file_path,api_count,code
config_test.py,0,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n\nbased on github.com/akanazawa/hmr\n""""""\n# Sets default args\n# Note all data format is NHWC because slim resnet wants NHWC.\nimport sys\nfrom absl import flags\n\nPRETRAINED_MODEL = \'./model/ring_6_68641\'\n\nflags.DEFINE_string(\'img_path\', \'/ps/project/face2d3d/face2mesh/website_release_testings/single_image_test/000001.jpg\', \'Image to run\')\nflags.DEFINE_string(\'out_folder\', \'./RingNet_output\',\n                     \'The output path to store images\')\n\nflags.DEFINE_boolean(\'save_obj_file\', False,\n                     \'If true the output meshes will be saved\')\n\nflags.DEFINE_boolean(\'save_flame_parameters\', False,\n                     \'If true the camera and flame parameters will be saved\')\n\nflags.DEFINE_boolean(\'neutralize_expression\', False,\n                     \'If true the camera and flame parameters will be saved\')\n\nflags.DEFINE_string(\'flame_model_path\', \'./flame_model/generic_model.pkl\', \'path to the neurtral flame model\')\n\nflags.DEFINE_string(\'load_path\', PRETRAINED_MODEL, \'path to trained model\')\n\nflags.DEFINE_integer(\'batch_size\', 1,\n                     \'Fixed to 1 for inference\')\n\n# Don\'t change if testing:\nflags.DEFINE_integer(\'img_size\', 224,\n                     \'Input image size to the network after preprocessing\')\nflags.DEFINE_string(\'data_format\', \'NHWC\', \'Data format\')\n\n# Flame parameters:\nflags.DEFINE_integer(\'pose_params\', 6,\n                     \'number of flame pose parameters\')\nflags.DEFINE_integer(\'shape_params\', 100,\n                     \'number of flame shape parameters\')\nflags.DEFINE_integer(\'expression_params\', 50,\n                     \'number of flame expression parameters\')\n\ndef get_config():\n    config = flags.FLAGS\n    config(sys.argv)\n    return config\n'"
demo.py,1,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n\nbased on github.com/akanazawa/hmr\n""""""\n## Demo of RingNet.\n## Note that RingNet requires a loose crop of the face in the image.\n## Sample usage:\n## Run the following command to generate check the RingNet predictions on loosely cropped face images\n# python -m demo --img_path *.jpg --out_folder ./RingNet_output\n## To output the meshes run the following command\n# python -m demo --img_path *.jpg --out_folder ./RingNet_output --save_obj_file=True\n## To output both meshes and flame parameters run the following command\n# python -m demo --img_path *.jpg --out_folder ./RingNet_output --save_obj_file=True --save_flame_parameters=True\n## To output both meshes and flame parameters and generate a neutralized mesh run the following command\n# python -m demo --img_path *.jpg --out_folder ./RingNet_output --save_obj_file=True --save_flame_parameters=True --neutralize_expression=True\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nfrom absl import flags\nimport numpy as np\nimport skimage.io as io\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom psbody.mesh import Mesh\nfrom smpl_webuser.serialization import load_model\n\nfrom util import renderer as vis_util\nfrom util import image as img_util\nfrom config_test import get_config\nfrom run_RingNet import RingNet_inference\n\ndef visualize(img, proc_param, verts, cam, img_name=\'test_image\'):\n    """"""\n    Renders the result in original image coordinate frame.\n    """"""\n    cam_for_render, vert_shifted = vis_util.get_original(\n        proc_param, verts, cam, img_size=img.shape[:2])\n\n    # Render results\n    rend_img_overlay = renderer(\n        vert_shifted*1.0, cam=cam_for_render, img=img, do_alpha=True)\n    rend_img = renderer(\n        vert_shifted*1.0, cam=cam_for_render, img_size=img.shape[:2])\n    rend_img_vp1 = renderer.rotated(\n        vert_shifted, 30, cam=cam_for_render, img_size=img.shape[:2])\n\n    import matplotlib.pyplot as plt\n    fig = plt.figure(1)\n    plt.clf()\n    plt.subplot(221)\n    plt.imshow(img)\n    plt.title(\'input\')\n    plt.axis(\'off\')\n    plt.subplot(222)\n    plt.imshow(rend_img_overlay)\n    plt.title(\'3D Mesh overlay\')\n    plt.axis(\'off\')\n    plt.subplot(223)\n    plt.imshow(rend_img)\n    plt.title(\'3D mesh\')\n    plt.axis(\'off\')\n    plt.subplot(224)\n    plt.imshow(rend_img_vp1)\n    plt.title(\'diff vp\')\n    plt.axis(\'off\')\n    plt.draw()\n    plt.show(block=False)\n    fig.savefig(img_name + \'.png\')\n    # import ipdb\n    # ipdb.set_trace()\n\ndef preprocess_image(img_path):\n    img = io.imread(img_path)\n    if np.max(img.shape[:2]) != config.img_size:\n        print(\'Resizing so the max image size is %d..\' % config.img_size)\n        scale = (float(config.img_size) / np.max(img.shape[:2]))\n    else:\n        scale = 1.0#scaling_factor\n    center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n    # image center in (x,y)\n    center = center[::-1]\n    crop, proc_param = img_util.scale_and_crop(img, scale, center,\n                                               config.img_size)\n    # import ipdb; ipdb.set_trace()\n    # Normalize image to [-1, 1]\n    # plt.imshow(crop/255.0)\n    # plt.show()\n    crop = 2 * ((crop / 255.) - 0.5)\n\n    return crop, proc_param, img\n\n\ndef main(config, template_mesh):\n    sess = tf.Session()\n    model = RingNet_inference(config, sess=sess)\n    input_img, proc_param, img = preprocess_image(config.img_path)\n    vertices, flame_parameters = model.predict(np.expand_dims(input_img, axis=0), get_parameters=True)\n    cams = flame_parameters[0][:3]\n    visualize(img, proc_param, vertices[0], cams, img_name=config.out_folder + \'/images/\' + config.img_path.split(\'/\')[-1][:-4])\n\n    if config.save_obj_file:\n        if not os.path.exists(config.out_folder + \'/mesh\'):\n            os.mkdir(config.out_folder + \'/mesh\')\n        mesh = Mesh(v=vertices[0], f=template_mesh.f)\n        mesh.write_obj(config.out_folder + \'/mesh/\' + config.img_path.split(\'/\')[-1][:-4] + \'.obj\')\n\n    if config.save_flame_parameters:\n        if not os.path.exists(config.out_folder + \'/params\'):\n            os.mkdir(config.out_folder + \'/params\')\n        flame_parameters_ = {\'cam\':  flame_parameters[0][:3], \'pose\': flame_parameters[0][3:3+config.pose_params], \'shape\': flame_parameters[0][3+config.pose_params:3+config.pose_params+config.shape_params],\n         \'expression\': flame_parameters[0][3+config.pose_params+config.shape_params:]}\n        np.save(config.out_folder + \'/params/\' + config.img_path.split(\'/\')[-1][:-4] + \'.npy\', flame_parameters_)\n\n    if config.neutralize_expression:\n        from util.using_flame_parameters import make_prdicted_mesh_neutral\n        if not os.path.exists(config.out_folder + \'/neutral_mesh\'):\n            os.mkdir(config.out_folder + \'/neutral_mesh\')\n        neutral_mesh = make_prdicted_mesh_neutral(config.out_folder + \'/params/\' + config.img_path.split(\'/\')[-1][:-4] + \'.npy\', config.flame_model_path)\n        neutral_mesh.write_obj(config.out_folder + \'/neutral_mesh/\' + config.img_path.split(\'/\')[-1][:-4] + \'.obj\')\n\n\n\n\nif __name__ == \'__main__\':\n    config = get_config()\n    template_mesh = Mesh(filename=\'./flame_model/FLAME_sample.ply\')\n    renderer = vis_util.SMPLRenderer(faces=template_mesh.f)\n\n    if not os.path.exists(config.out_folder):\n        os.makedirs(config.out_folder)\n\n    if not os.path.exists(config.out_folder + \'/images\'):\n        os.mkdir(config.out_folder + \'/images\')\n\n    main(config, template_mesh)\n'"
dynamic_contour_embedding.py,0,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n""""""\n\n## A function to load the dynamic contour and the static landmarks on a template mesh\n## Please cite the updated citaion from https://ringnet.is.tue.mpg.de if you use the dynamic contour for FLAME\n## The use of static and dynamic contours for any project follows the liscencing from FLAME (http://flame.is.tue.mpg.de/)\n\nimport numpy as np\nimport pyrender\nimport trimesh\nfrom smpl_webuser.serialization import load_model\nfrom psbody.mesh import Mesh\nimport cPickle as pickle\n\ndef load_static_embedding(static_embedding_path):\n    with open(static_embedding_path, \'rb\') as f:\n        lmk_indexes_dict = pickle.load(f)\n    lmk_face_idx = lmk_indexes_dict[ \'lmk_face_idx\' ].astype( np.uint32 )\n    lmk_b_coords = lmk_indexes_dict[ \'lmk_b_coords\' ]\n    return lmk_face_idx, lmk_b_coords\n\ndef mesh_points_by_barycentric_coordinates(mesh_verts, mesh_faces, lmk_face_idx, lmk_b_coords):\n    # function: evaluation 3d points given mesh and landmark embedding\n    # modified from https://github.com/Rubikplayer/flame-fitting/blob/master/fitting/landmarks.py\n    dif1 = np.vstack([(mesh_verts[mesh_faces[lmk_face_idx], 0] * lmk_b_coords).sum(axis=1),\n                    (mesh_verts[mesh_faces[lmk_face_idx], 1] * lmk_b_coords).sum(axis=1),\n                    (mesh_verts[mesh_faces[lmk_face_idx], 2] * lmk_b_coords).sum(axis=1)]).T\n    return dif1\n\ndef load_dynamic_contour(template_flame_path=\'None\', contour_embeddings_path=\'None\', static_embedding_path=\'None\', angle=0):\n    template_mesh = Mesh(filename=template_flame_path)\n    contour_embeddings_path = contour_embeddings_path\n    dynamic_lmks_embeddings = np.load(contour_embeddings_path, allow_pickle=True).item()\n    lmk_face_idx_static, lmk_b_coords_static = load_static_embedding(static_embedding_path)\n    lmk_face_idx_dynamic = dynamic_lmks_embeddings[\'lmk_face_idx\'][angle]\n    lmk_b_coords_dynamic = dynamic_lmks_embeddings[\'lmk_b_coords\'][angle]\n    dynamic_lmks = mesh_points_by_barycentric_coordinates(template_mesh.v, template_mesh.f, lmk_face_idx_dynamic, lmk_b_coords_dynamic)\n    static_lmks = mesh_points_by_barycentric_coordinates(template_mesh.v, template_mesh.f, lmk_face_idx_static, lmk_b_coords_static)\n    total_lmks = np.vstack([dynamic_lmks, static_lmks])\n\n    # Visualization of the pose dependent contour on the template mesh\n    vertex_colors = np.ones([template_mesh.v.shape[0], 4]) * [0.3, 0.3, 0.3, 0.8]\n    tri_mesh = trimesh.Trimesh(template_mesh.v, template_mesh.f,\n                               vertex_colors=vertex_colors)\n    mesh = pyrender.Mesh.from_trimesh(tri_mesh)\n    scene = pyrender.Scene()\n    scene.add(mesh)\n    sm = trimesh.creation.uv_sphere(radius=0.005)\n    sm.visual.vertex_colors = [0.9, 0.1, 0.1, 1.0]\n    tfs = np.tile(np.eye(4), (len(total_lmks), 1, 1))\n    tfs[:, :3, 3] = total_lmks\n    joints_pcl = pyrender.Mesh.from_trimesh(sm, poses=tfs)\n    scene.add(joints_pcl)\n    pyrender.Viewer(scene, use_raymond_lighting=True)\n\nif __name__ == \'__main__\':\n    # angle = 35.0 #in degrees\n    angle = 0.0 #in degrees\n    # angle = -16.0 #in degrees\n    if angle < 0:\n        angle = 39 - angle\n    contour_embeddings_path = \'./flame_model/flame_dynamic_embedding.npy\'\n    static_embedding_path = \'./flame_model/flame_static_embedding.pkl\'\n    load_dynamic_contour(template_flame_path=\'./flame_model/FLAME_sample.ply\', contour_embeddings_path=contour_embeddings_path, static_embedding_path=static_embedding_path, angle=int(angle))\n'"
run_RingNet.py,5,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n\nAll rights reserved.\nbased on github.com/akanazawa/hmr\n""""""\n# RingNet Inference for single image.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom os.path import exists\n\nclass RingNet_inference(object):\n    def __init__(self, config, sess=None):\n        self.config = config\n        self.load_path = config.load_path\n        if not config.load_path:\n            raise Exception(\n                ""provide a pretrained model path""\n            )\n        if not exists(config.load_path + \'.index\'):\n            print(\'%s couldnt find..\' % config.load_path)\n            import ipdb\n            ipdb.set_trace()\n\n        # Data\n        self.batch_size = config.batch_size\n        self.img_size = config.img_size\n        self.data_format = config.data_format\n        input_size = (self.batch_size, self.img_size, self.img_size, 3)\n        self.images_pl = tf.placeholder(tf.float32, shape=input_size, name=\'input_images\')\n\n        if sess is None:\n            self.sess = tf.Session()\n        else:\n            self.sess = sess\n\n        # Load graph.\n        self.saver = tf.train.import_meta_graph(self.load_path+\'.meta\')\n        self.graph = tf.get_default_graph()\n        self.prepare()\n\n\n    def prepare(self):\n        print(\'Restoring checkpoint %s..\' % self.load_path)\n        self.saver.restore(self.sess, self.load_path)\n\n\n    def predict(self, images, get_parameters=False):\n        """"""\n        images: batch_size, img_size, img_size, 3 # Here for inference the batch size is always set to 1\n        Preprocessed to range [-1, 1]\n        """"""\n        results = self.predict_dict(images)\n        if get_parameters:\n            return results[\'vertices\'], results[\'parameters\']\n        else:\n            return results[\'vertices\']\n\n\n    def predict_dict(self, images):\n        """"""\n        Runs the model with images.\n        """"""\n        images_ip = self.graph.get_tensor_by_name(u\'input_images_1:0\')\n        params = self.graph.get_tensor_by_name(u\'add_2:0\')\n        verts = self.graph.get_tensor_by_name(u\'Flamenetnormal_2/Add_9:0\')\n        feed_dict = {\n            images_ip: images,\n        }\n        fetch_dict = {\n            \'vertices\': verts,\n            \'parameters\': params,\n        }\n        results = self.sess.run(fetch_dict, feed_dict)\n        tf.reset_default_graph()\n        return results\n'"
smpl_webuser/__init__.py,0,"b""'''\n'''"""
smpl_webuser/lbs.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines linear blend skinning for the SMPL loader which \ndefines the effect of bones and blendshapes on the vertices of the template mesh.\n\nModules included:\n- global_rigid_transformation: \n  computes global rotation & translation of the model\n- verts_core: [overloaded function inherited from verts.verts_core]\n  computes the blending of joint-influences for each vertex based on type of skinning\n\n'''\n\nfrom posemapper import posemap\nimport chumpy\nimport numpy as np\n\ndef global_rigid_transformation(pose, J, kintree_table, xp):\n    results = {}\n    pose = pose.reshape((-1,3))\n    id_to_col = {kintree_table[1,i] : i for i in range(kintree_table.shape[1])}\n    parent = {i : id_to_col[kintree_table[0,i]] for i in range(1, kintree_table.shape[1])}\n\n    if xp == chumpy:\n        from posemapper import Rodrigues\n        rodrigues = lambda x : Rodrigues(x)\n    else:\n        import cv2\n        rodrigues = lambda x : cv2.Rodrigues(x)[0]\n\n    with_zeros = lambda x : xp.vstack((x, xp.array([[0.0, 0.0, 0.0, 1.0]])))\n    results[0] = with_zeros(xp.hstack((rodrigues(pose[0,:]), J[0,:].reshape((3,1)))))        \n        \n    for i in range(1, kintree_table.shape[1]):\n        results[i] = results[parent[i]].dot(with_zeros(xp.hstack((\n            rodrigues(pose[i,:]),\n            ((J[i,:] - J[parent[i],:]).reshape((3,1)))\n            ))))\n\n    pack = lambda x : xp.hstack([np.zeros((4, 3)), x.reshape((4,1))])\n    \n    results = [results[i] for i in sorted(results.keys())]\n    results_global = results\n\n    if True:\n        results2 = [results[i] - (pack(\n            results[i].dot(xp.concatenate( ( (J[i,:]), 0 ) )))\n            ) for i in range(len(results))]\n        results = results2\n    result = xp.dstack(results)\n    return result, results_global\n\n\ndef verts_core(pose, v, J, weights, kintree_table, want_Jtr=False, xp=chumpy):\n    A, A_global = global_rigid_transformation(pose, J, kintree_table, xp)\n    T = A.dot(weights.T)\n\n    rest_shape_h = xp.vstack((v.T, np.ones((1, v.shape[0]))))\n        \n    v =(T[:,0,:] * rest_shape_h[0, :].reshape((1, -1)) + \n        T[:,1,:] * rest_shape_h[1, :].reshape((1, -1)) + \n        T[:,2,:] * rest_shape_h[2, :].reshape((1, -1)) + \n        T[:,3,:] * rest_shape_h[3, :].reshape((1, -1))).T\n\n    v = v[:,:3] \n    \n    if not want_Jtr:\n        return v\n    Jtr = xp.vstack([g[:3,3] for g in A_global])\n    return (v, Jtr)\n    \n"""
smpl_webuser/posemapper.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis module defines the mapping of joint-angles to pose-blendshapes. \n\nModules included:\n- posemap:\n  computes the joint-to-pose blend shape mapping given a mapping type as input\n\n'''\n\nimport chumpy as ch\nimport numpy as np\nimport cv2\n\n\nclass Rodrigues(ch.Ch):\n    dterms = 'rt'\n    \n    def compute_r(self):\n        return cv2.Rodrigues(self.rt.r)[0]\n    \n    def compute_dr_wrt(self, wrt):\n        if wrt is self.rt:\n            return cv2.Rodrigues(self.rt.r)[1].T\n\n\ndef lrotmin(p): \n    if isinstance(p, np.ndarray):\n        p = p.ravel()[3:]\n        return np.concatenate([(cv2.Rodrigues(np.array(pp))[0]-np.eye(3)).ravel() for pp in p.reshape((-1,3))]).ravel()        \n    if p.ndim != 2 or p.shape[1] != 3:\n        p = p.reshape((-1,3))\n    p = p[1:]\n    return ch.concatenate([(Rodrigues(pp)-ch.eye(3)).ravel() for pp in p]).ravel()\n\ndef posemap(s):\n    if s == 'lrotmin':\n        return lrotmin\n    else:\n        raise Exception('Unknown posemapping: %s' % (str(s),))\n"""
smpl_webuser/serialization.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines the serialization functions of the SMPL model. \n\nModules included:\n- save_model:\n  saves the SMPL model to a given file location as a .pkl file\n- load_model:\n  loads the SMPL model from a given file location (i.e. a .pkl file location), \n  or a dictionary object.\n\n'''\n\n__all__ = ['load_model', 'save_model']\n\nimport numpy as np\nimport cPickle as pickle\nimport chumpy as ch\nfrom chumpy.ch import MatVecMult\nfrom posemapper import posemap\nfrom verts import verts_core\n    \ndef save_model(model, fname):\n    m0 = model\n    trainer_dict = {'v_template': np.asarray(m0.v_template),'J': np.asarray(m0.J),'weights': np.asarray(m0.weights),'kintree_table': m0.kintree_table,'f': m0.f, 'bs_type': m0.bs_type, 'posedirs': np.asarray(m0.posedirs)}    \n    if hasattr(model, 'J_regressor'):\n        trainer_dict['J_regressor'] = m0.J_regressor\n    if hasattr(model, 'J_regressor_prior'):\n        trainer_dict['J_regressor_prior'] = m0.J_regressor_prior\n    if hasattr(model, 'weights_prior'):\n        trainer_dict['weights_prior'] = m0.weights_prior\n    if hasattr(model, 'shapedirs'):\n        trainer_dict['shapedirs'] = m0.shapedirs\n    if hasattr(model, 'vert_sym_idxs'):\n        trainer_dict['vert_sym_idxs'] = m0.vert_sym_idxs\n    if hasattr(model, 'bs_style'):\n        trainer_dict['bs_style'] = model.bs_style\n    else:\n        trainer_dict['bs_style'] = 'lbs'\n    pickle.dump(trainer_dict, open(fname, 'w'), -1)\n\n\ndef backwards_compatibility_replacements(dd):\n\n    # replacements\n    if 'default_v' in dd:\n        dd['v_template'] = dd['default_v']\n        del dd['default_v']\n    if 'template_v' in dd:\n        dd['v_template'] = dd['template_v']\n        del dd['template_v']\n    if 'joint_regressor' in dd:\n        dd['J_regressor'] = dd['joint_regressor']\n        del dd['joint_regressor']\n    if 'blendshapes' in dd:\n        dd['posedirs'] = dd['blendshapes']\n        del dd['blendshapes']\n    if 'J' not in dd:\n        dd['J'] = dd['joints']\n        del dd['joints']\n\n    # defaults\n    if 'bs_style' not in dd:\n        dd['bs_style'] = 'lbs'\n\n\n\ndef ready_arguments(fname_or_dict):\n\n    if not isinstance(fname_or_dict, dict):\n        dd = pickle.load(open(fname_or_dict))\n    else:\n        dd = fname_or_dict\n        \n    backwards_compatibility_replacements(dd)\n        \n    want_shapemodel = 'shapedirs' in dd\n    nposeparms = dd['kintree_table'].shape[1]*3\n\n    if 'trans' not in dd:\n        dd['trans'] = np.zeros(3)\n    if 'pose' not in dd:\n        dd['pose'] = np.zeros(nposeparms)\n    if 'shapedirs' in dd and 'betas' not in dd:\n        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n\n    for s in ['v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs', 'betas', 'J']:\n        if (s in dd) and not hasattr(dd[s], 'dterms'):\n            dd[s] = ch.array(dd[s])\n\n    if want_shapemodel:\n        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas'])+dd['v_template']\n        v_shaped = dd['v_shaped']\n        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:,0])        \n        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:,1])        \n        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:,2])        \n        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T    \n        dd['v_posed'] = v_shaped + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n    else:    \n        dd['v_posed'] = dd['v_template'] + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n            \n    return dd\n\n\n\ndef load_model(fname_or_dict):\n    dd = ready_arguments(fname_or_dict)\n    \n    args = {\n        'pose': dd['pose'],\n        'v': dd['v_posed'],\n        'J': dd['J'],\n        'weights': dd['weights'],\n        'kintree_table': dd['kintree_table'],\n        'xp': ch,\n        'want_Jtr': True,\n        'bs_style': dd['bs_style']\n    }\n    \n    result, Jtr = verts_core(**args)\n    result = result + dd['trans'].reshape((1,3))\n    result.J_transformed = Jtr + dd['trans'].reshape((1,3))\n\n    for k, v in dd.items():\n        setattr(result, k, v)\n        \n    return result\n\n"""
smpl_webuser/verts.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines the basic skinning modules for the SMPL loader which \ndefines the effect of bones and blendshapes on the vertices of the template mesh.\n\nModules included:\n- verts_decorated: \n  creates an instance of the SMPL model which inherits model attributes from another \n  SMPL model.\n- verts_core: [overloaded function inherited by lbs.verts_core]\n  computes the blending of joint-influences for each vertex based on type of skinning\n\n'''\n\nimport chumpy\nimport lbs\nfrom posemapper import posemap\nimport scipy.sparse as sp\nfrom chumpy.ch import MatVecMult\n\ndef ischumpy(x): return hasattr(x, 'dterms')\n\ndef verts_decorated(trans, pose, \n    v_template, J, weights, kintree_table, bs_style, f,\n    bs_type=None, posedirs=None, betas=None, shapedirs=None, want_Jtr=False):\n\n    for which in [trans, pose, v_template, weights, posedirs, betas, shapedirs]:\n        if which is not None:\n            assert ischumpy(which)\n\n    v = v_template\n\n    if shapedirs is not None:\n        if betas is None:\n            betas = chumpy.zeros(shapedirs.shape[-1])\n        v_shaped = v + shapedirs.dot(betas)\n    else:\n        v_shaped = v\n        \n    if posedirs is not None:\n        v_posed = v_shaped + posedirs.dot(posemap(bs_type)(pose))\n    else:\n        v_posed = v_shaped\n        \n    v = v_posed\n        \n    if sp.issparse(J):\n        regressor = J\n        J_tmpx = MatVecMult(regressor, v_shaped[:,0])        \n        J_tmpy = MatVecMult(regressor, v_shaped[:,1])        \n        J_tmpz = MatVecMult(regressor, v_shaped[:,2])        \n        J = chumpy.vstack((J_tmpx, J_tmpy, J_tmpz)).T            \n    else:    \n        assert(ischumpy(J))\n        \n    assert(bs_style=='lbs')\n    result, Jtr = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr=True, xp=chumpy)\n     \n    tr = trans.reshape((1,3))\n    result = result + tr\n    Jtr = Jtr + tr\n\n    result.trans = trans\n    result.f = f\n    result.pose = pose\n    result.v_template = v_template\n    result.J = J\n    result.weights = weights\n    result.kintree_table = kintree_table\n    result.bs_style = bs_style\n    result.bs_type =bs_type\n    if posedirs is not None:\n        result.posedirs = posedirs\n        result.v_posed = v_posed\n    if shapedirs is not None:\n        result.shapedirs = shapedirs\n        result.betas = betas\n        result.v_shaped = v_shaped\n    if want_Jtr:\n        result.J_transformed = Jtr\n    return result\n\ndef verts_core(pose, v, J, weights, kintree_table, bs_style, want_Jtr=False, xp=chumpy):\n    \n    if xp == chumpy:\n        assert(hasattr(pose, 'dterms'))\n        assert(hasattr(v, 'dterms'))\n        assert(hasattr(J, 'dterms'))\n        assert(hasattr(weights, 'dterms'))\n     \n    assert(bs_style=='lbs')\n    result = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr, xp)\n\n    return result\n"""
util/__init__.py,0,b''
util/image.py,0,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n\nbased on github.com/akanazawa/hmr\n""""""\n# Preprocessing.\n\nimport numpy as np\nimport cv2\n\n\ndef resize_img(img, scale_factor):\n    new_size = (np.floor(np.array(img.shape[0:2]) * scale_factor)).astype(int)\n    new_img = cv2.resize(img, (new_size[1], new_size[0]))\n    # This is scale factor of [height, width] i.e. [y, x]\n    actual_factor = [\n        new_size[0] / float(img.shape[0]), new_size[1] / float(img.shape[1])\n    ]\n    return new_img, actual_factor\n\n\ndef scale_and_crop(image, scale, center, img_size):\n    image_scaled, scale_factors = resize_img(image, scale)\n    # Swap so it\'s [x, y]\n    scale_factors = [scale_factors[1], scale_factors[0]]\n    center_scaled = np.round(center * scale_factors).astype(np.int)\n\n    margin = int(img_size / 2)\n    image_pad = np.pad(\n        image_scaled, ((margin, ), (margin, ), (0, )), mode=\'edge\')\n    center_pad = center_scaled + margin\n    # figure out starting point\n    start_pt = center_pad - margin\n    end_pt = center_pad + margin\n    # crop:\n    crop = image_pad[start_pt[1]:end_pt[1], start_pt[0]:end_pt[0], :]\n    proc_param = {\n        \'scale\': scale,\n        \'start_pt\': start_pt,\n        \'end_pt\': end_pt,\n        \'img_size\': img_size\n    }\n\n    return crop, proc_param\n'"
util/renderer.py,0,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n\nbased on github.com/akanazawa/hmr\n""""""\n# Renders mesh using OpenDr for visualization.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\nfrom opendr.camera import ProjectPoints\nfrom opendr.renderer import ColoredRenderer\nfrom opendr.lighting import LambertianPointLight\n\ncolors = {\n    # colorbline/print/copy safe:\n    \'light_blue\': [0.65098039, 0.74117647, 0.85882353],\n    \'light_pink\': [.9, .7, .7],  # This is used to do no-3d\n}\n\n\nclass SMPLRenderer(object):\n    def __init__(self,\n                 img_size=256,\n                 flength=500.,\n                 faces=None):\n        self.faces = faces\n        self.w = img_size\n        self.h = img_size\n        self.flength = flength\n\n    def __call__(self,\n                 verts,\n                 cam=None,\n                 img=None,\n                 do_alpha=False,\n                 far=None,\n                 near=None,\n                 color_id=0,\n                 img_size=None):\n        """"""\n        cam is 3D [f, px, py]\n        """"""\n        if img is not None:\n            h, w = img.shape[:2]\n        elif img_size is not None:\n            h = img_size[0]\n            w = img_size[1]\n        else:\n            h = self.h\n            w = self.w\n\n        if cam is None:\n            cam = [self.flength, w / 2., h / 2.]\n\n        use_cam = ProjectPoints(\n            f=cam[0] * np.ones(2),\n            rt=np.zeros(3),\n            t=np.zeros(3),\n            k=np.zeros(5),\n            c=cam[1:3])\n\n        if near is None:\n            near = np.maximum(np.min(verts[:, 2]) - 25, 0.1)\n        if far is None:\n            far = np.maximum(np.max(verts[:, 2]) + 25, 25)\n\n        imtmp = render_model(\n            verts,\n            self.faces,\n            w,\n            h,\n            use_cam,\n            do_alpha=do_alpha,\n            img=img,\n            far=far,\n            near=near,\n            color_id=color_id)\n\n        return (imtmp * 255).astype(\'uint8\')\n\n    def rotated(self,\n                verts,\n                deg,\n                cam=None,\n                axis=\'y\',\n                img=None,\n                do_alpha=True,\n                far=None,\n                near=None,\n                color_id=0,\n                img_size=None):\n        import math\n        if axis == \'y\':\n            around = cv2.Rodrigues(np.array([0, math.radians(deg), 0]))[0]\n        elif axis == \'x\':\n            around = cv2.Rodrigues(np.array([math.radians(deg), 0, 0]))[0]\n        else:\n            around = cv2.Rodrigues(np.array([0, 0, math.radians(deg)]))[0]\n        center = verts.mean(axis=0)\n        new_v = np.dot((verts - center), around) + center\n\n        return self.__call__(\n            new_v,\n            cam,\n            img=img,\n            do_alpha=do_alpha,\n            far=far,\n            near=near,\n            img_size=img_size,\n            color_id=color_id)\n\n\ndef _create_renderer(w=640,\n                     h=480,\n                     rt=np.zeros(3),\n                     t=np.zeros(3),\n                     f=None,\n                     c=None,\n                     k=None,\n                     near=.5,\n                     far=10.):\n\n    f = np.array([w, w]) / 2. if f is None else f\n    c = np.array([w, h]) / 2. if c is None else c\n    k = np.zeros(5) if k is None else k\n\n    rn = ColoredRenderer()\n\n    rn.camera = ProjectPoints(rt=rt, t=t, f=f, c=c, k=k)\n    rn.frustum = {\'near\': near, \'far\': far, \'height\': h, \'width\': w}\n    return rn\n\n\ndef _rotateY(points, angle):\n    """"""Rotate the points by a specified angle.""""""\n    ry = np.array([[np.cos(angle), 0., np.sin(angle)], [0., 1., 0.],\n                   [-np.sin(angle), 0., np.cos(angle)]])\n    return np.dot(points, ry)\n\n\ndef simple_renderer(rn,\n                    verts,\n                    faces,\n                    yrot=np.radians(120),\n                    color=colors[\'light_pink\']):\n    # Rendered model color\n    rn.set(v=verts, f=faces, vc=color, bgcolor=np.ones(3))\n    albedo = rn.vc\n\n    # Construct Back Light (on back right corner)\n    rn.vc = LambertianPointLight(\n        f=rn.f,\n        v=rn.v,\n        num_verts=len(rn.v),\n        light_pos=_rotateY(np.array([-200, -100, -100]), yrot),\n        vc=albedo,\n        light_color=np.array([1, 1, 1]))\n\n    # Construct Left Light\n    rn.vc += LambertianPointLight(\n        f=rn.f,\n        v=rn.v,\n        num_verts=len(rn.v),\n        light_pos=_rotateY(np.array([800, 10, 300]), yrot),\n        vc=albedo,\n        light_color=np.array([1, 1, 1]))\n\n    # Construct Right Light\n    rn.vc += LambertianPointLight(\n        f=rn.f,\n        v=rn.v,\n        num_verts=len(rn.v),\n        light_pos=_rotateY(np.array([-500, 500, 1000]), yrot),\n        vc=albedo,\n        light_color=np.array([.7, .7, .7]))\n\n    return rn.r\n\n\ndef get_alpha(imtmp, bgval=1.):\n    h, w = imtmp.shape[:2]\n    alpha = (~np.all(imtmp == bgval, axis=2)).astype(imtmp.dtype)\n\n    b_channel, g_channel, r_channel = cv2.split(imtmp)\n\n    im_RGBA = cv2.merge((b_channel, g_channel, r_channel, alpha.astype(\n        imtmp.dtype)))\n    return im_RGBA\n\n\ndef append_alpha(imtmp):\n    alpha = np.ones_like(imtmp[:, :, 0]).astype(imtmp.dtype)\n    if np.issubdtype(imtmp.dtype, np.uint8):\n        alpha = alpha * 255\n    b_channel, g_channel, r_channel = cv2.split(imtmp)\n    im_RGBA = cv2.merge((b_channel, g_channel, r_channel, alpha))\n    return im_RGBA\n\n\ndef render_model(verts,\n                 faces,\n                 w,\n                 h,\n                 cam,\n                 near=0.5,\n                 far=25,\n                 img=None,\n                 do_alpha=False,\n                 color_id=None):\n    rn = _create_renderer(\n        w=w, h=h, near=near, far=far, rt=cam.rt, t=cam.t, f=cam.f, c=cam.c)\n\n    # Uses img as background, otherwise white background.\n    if img is not None:\n        rn.background_image = img / 255. if img.max() > 1 else img\n\n    if color_id is None:\n        color = colors[\'light_blue\']\n    else:\n        color_list = colors.values()\n        color = color_list[color_id % len(color_list)]\n\n    imtmp = simple_renderer(rn, verts, faces, color=color)\n\n    # If white bg, make transparent.\n    if img is None and do_alpha:\n        imtmp = get_alpha(imtmp)\n    elif img is not None and do_alpha:\n        imtmp = append_alpha(imtmp)\n\n    return imtmp\n\n\n# ------------------------------\n\n\ndef get_original(proc_param, verts, cam, img_size):\n    img_size = proc_param[\'img_size\']\n    undo_scale = 1. / np.array(proc_param[\'scale\'])\n\n    cam_s = cam[0]\n    cam_pos = cam[1:]\n    principal_pt = np.array([img_size, img_size]) / 2.\n    flength = 50000.0 #500.\n    tz = flength / (0.5 * img_size * cam_s)\n    trans = np.hstack([cam_pos, tz])\n    vert_shifted = verts + trans\n\n    start_pt = proc_param[\'start_pt\'] - 0.5 * img_size\n    final_principal_pt = (principal_pt + start_pt) * undo_scale\n    cam_for_render = np.hstack(\n        [np.mean(flength * undo_scale), final_principal_pt])\n\n    # This is in padded image.\n    # kp_original = (joints + proc_param[\'start_pt\']) * undo_scale\n    # Subtract padding from joints.\n    margin = int(img_size / 2)\n    # kp_original = (joints + proc_param[\'start_pt\'] - margin) * undo_scale\n\n    return cam_for_render, vert_shifted, #kp_original\n'"
util/using_flame_parameters.py,0,"b'""""""\nAuthor: Soubhik Sanyal\nCopyright (c) 2019, Soubhik Sanyal\nAll rights reserved.\n\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\n\nMore information about RingNet is available at https://ringnet.is.tue.mpg.de.\n""""""\n# This function Netralize the pose and expression of the predicted mesh and generates a template mesh with only the identity information\nimport numpy as np\nimport chumpy as ch\nfrom smpl_webuser.serialization import load_model\nfrom smpl_webuser.verts import verts_decorated\nfrom psbody.mesh import Mesh\n\n\ndef make_prdicted_mesh_neutral(predicted_params_path, flame_model_path):\n    params = np.load(predicted_params_path, allow_pickle=True)\n    params = params[()]\n    pose = np.zeros(15)\n    expression = np.zeros(100)\n    shape = np.hstack((params[\'shape\'], np.zeros(300-params[\'shape\'].shape[0])))\n    flame_genral_model = load_model(flame_model_path)\n    generated_neutral_mesh = verts_decorated(ch.array([0.0,0.0,0.0]),\n                        ch.array(pose),\n                        ch.array(flame_genral_model.r),\n                        flame_genral_model.J_regressor,\n                        ch.array(flame_genral_model.weights),\n                        flame_genral_model.kintree_table,\n                        flame_genral_model.bs_style,\n                        flame_genral_model.f,\n                        bs_type=flame_genral_model.bs_type,\n                        posedirs=ch.array(flame_genral_model.posedirs),\n                        betas=ch.array(np.hstack((shape,expression))),#betas=ch.array(np.concatenate((theta[0,75:85], np.zeros(390)))), #\n                        shapedirs=ch.array(flame_genral_model.shapedirs),\n                        want_Jtr=True)\n    neutral_mesh = Mesh(v=generated_neutral_mesh.r, f=generated_neutral_mesh.f)\n    return neutral_mesh\n'"
