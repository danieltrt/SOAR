file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\nwith open(""elasticdl/requirements.txt"") as f:\n    required_deps = f.read().splitlines()\n\nextras = {}\nwith open(""elasticdl/requirements-dev.txt"") as f:\n    extras[""develop""] = f.read().splitlines()\n\nsetup(\n    name=""elasticdl"",\n    version=""develop"",\n    description=""A Kubernetes-native Deep Learning Framework"",\n    long_description=""ElasticDL is a Kubernetes-native deep learning framework""\n    "" built on top of TensorFlow 2.0 that supports""\n    "" fault-tolerance and elastic scheduling."",\n    long_description_content_type=""text/markdown"",\n    author=""Ant Financial"",\n    url=""https://elasticdl.org"",\n    install_requires=required_deps,\n    extras_require=extras,\n    python_requires="">=3.5"",\n    packages=find_packages(exclude=[""*test*""]),\n    package_data={\n        """": [\n            ""proto/*.proto"",\n            ""docker/*"",\n            ""Makefile"",\n            ""requirements.txt"",\n            ""pkg/kernel/capi/*"",\n        ]\n    },\n    entry_points={\n        ""console_scripts"": [""elasticdl=elasticdl.python.elasticdl.client:main""]\n    },\n)\n'"
elasticdl/__init__.py,0,b''
elasticdl_preprocessing/__init__.py,0,b''
elasticdl_preprocessing/constants.py,0,"b'class AnalysisEnvTemplate:\n    MIN_ENV = ""_{}_min""\n    MAX_ENV = ""_{}_max""\n    AVG_ENV = ""_{}_avg""\n    STDDEV_ENV = ""_{}_stddev""\n    BUCKET_BOUNDARIES_ENV = ""_{}_boundaries""\n    DISTINCT_COUNT_ENV = ""_{}_distinct_count""\n    VOCABULARY_ENV = ""_{}_vocab""\n'"
elasticdl_preprocessing/setup.py,0,"b'from setuptools import setup\n\nwith open(""elasticdl_preprocessing/requirements.txt"") as f:\n    required_deps = f.read().splitlines()\n\nextras = {}\nwith open(""elasticdl_preprocessing/requirements-dev.txt"") as f:\n    extras[""develop""] = f.read().splitlines()\n\nsetup(\n    name=""elasticdl_preprocessing"",\n    version=""0.1.0"",\n    description=""A Kubernetes-native Deep Learning Framework"",\n    long_description=""This is an extension of the native Keras Preprocessing""\n    "" Layers and Feature Column API from TensorFlow. We can develop our model""\n    "" using the native high-level API from TensorFlow and our library.""\n    ""We can train this model using native TensorFlow or ElasticDL."",\n    long_description_content_type=""text/markdown"",\n    author=""Ant Financial"",\n    url=""https://elasticdl.org"",\n    install_requires=required_deps,\n    extras_require=extras,\n    python_requires="">=3.5"",\n    packages=[""elasticdl_preprocessing""],\n    package_data={"""": [""requirements.txt""]},\n)\n'"
model_zoo/__init__.py,0,b''
scripts/gen_mnist_checkpoint.py,10,"b'import argparse\n\nimport tensorflow as tf\n\nfrom elasticdl.python.tests.test_utils import save_checkpoint_without_embedding\n\n\ndef mnist_custom_model():\n    inputs = tf.keras.Input(shape=(28, 28), name=""image"")\n    x = tf.keras.layers.Reshape((28, 28, 1))(inputs)\n    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = tf.keras.layers.Flatten()(x)\n    outputs = tf.keras.layers.Dense(10)(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=""mnist_model"")\n\n\ndef add_params(parser):\n    parser.add_argument(\n        ""--checkpoint_dir"",\n        help=""The directory to store the mnist checkpoint"",\n        default="""",\n        type=str,\n    )\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    add_params(parser)\n    args, _ = parser.parse_known_args()\n    print(args)\n    model = mnist_custom_model()\n    save_checkpoint_without_embedding(model, args.checkpoint_dir)\n'"
scripts/validate_job_status.py,0,"b'import sys\nimport time\n\nfrom kubernetes import client, config\n\n\ndef print_tail_log(log, tail_num):\n    if log is not None:\n        log_lines = log.split(""\\n"")\n        tail_index = -1 * tail_num\n        print(""\\n"".join(log_lines[tail_index:]))\n\n\nclass Client(object):\n    def __init__(self, namespace):\n        self.namespace = namespace\n        config.load_kube_config()\n        self.client = client.CoreV1Api()\n\n    def get_pod_phase(self, pod_name):\n        try:\n            pod = self.client.read_namespaced_pod(\n                namespace=self.namespace, name=pod_name\n            )\n            return pod.status.phase\n        except Exception:\n            return ""Pod %s not found"" % pod_name\n\n    def get_pod_status(self, pod_name):\n        try:\n            pod = self.client.read_namespaced_pod(\n                namespace=self.namespace, name=pod_name\n            )\n            return pod.status.to_str()\n        except Exception:\n            return ""Pod %s not found"" % pod_name\n\n    def get_pod_label_status(self, pod_name):\n        try:\n            pod = self.client.read_namespaced_pod(\n                namespace=self.namespace, name=pod_name\n            )\n            return pod.metadata.labels[""status""]\n        except Exception:\n            return ""Pod %s not found"" % pod_name\n\n    def get_pod_log(self, pod_name):\n        try:\n            return self.client.read_namespaced_pod_log(\n                namespace=self.namespace, name=pod_name\n            )\n        except Exception:\n            return ""Pod %s not found"" % pod_name\n\n    def delete_pod(self, pod_name):\n        self.client.delete_namespaced_pod(\n            pod_name,\n            self.namespace,\n            body=client.V1DeleteOptions(grace_period_seconds=0),\n        )\n\n\ndef check_success(statuses):\n    for status in statuses:\n        if status != ""Succeeded"":\n            return False\n    return True\n\n\ndef check_failed(statuses):\n    for status in statuses:\n        if status == ""Failed"":\n            return True\n    return False\n\n\ndef validate_job_status(client, job_type, ps_num, worker_num):\n    ps_pod_names = [\n        ""elasticdl-test-"" + job_type + ""-ps-"" + str(i) for i in range(ps_num)\n    ]\n    worker_pod_names = [\n        ""elasticdl-test-"" + job_type + ""-worker-"" + str(i)\n        for i in range(worker_num)\n    ]\n    master_pod_name = ""elasticdl-test-"" + job_type + ""-master""\n\n    for step in range(200):\n        master_pod_phase = client.get_pod_phase(master_pod_name)\n        ps_pod_phases = [client.get_pod_phase(ps) for ps in ps_pod_names]\n        worker_pod_phases = [\n            client.get_pod_phase(worker) for worker in worker_pod_names\n        ]\n\n        if (\n            check_success([master_pod_phase])\n            and check_success(ps_pod_phases)\n            and check_success(worker_pod_phases)\n        ):\n            print(""ElasticDL job succeeded."")\n            client.delete_pod(master_pod_name)\n            exit(0)\n        elif (\n            check_success(ps_pod_phases)\n            and check_success(worker_pod_phases)\n            and client.get_pod_phase(master_pod_name) == ""Running""\n            and client.get_pod_label_status(master_pod_name) == ""Finished""\n        ):\n            print(\n                ""ElasticDL job succeeded""\n                ""(master pod keeps running for TensorBoard service).""\n            )\n            client.delete_pod(master_pod_name)\n            exit(0)\n        elif (\n            check_failed(ps_pod_phases)\n            or check_failed(worker_pod_phases)\n            or check_failed([master_pod_phase])\n        ):\n            print(""ElasticDL job failed."")\n            print(client.get_pod_status(master_pod_name))\n            print(""Master log:"")\n            print(client.get_pod_log(master_pod_name))\n            for ps, pod_phase in zip(ps_pod_names, ps_pod_phases):\n                if check_failed([pod_phase]):\n                    print(""PS %s log"" % ps)\n                    print_tail_log(client.get_pod_log(ps), 50)\n            for worker, pod_phase in zip(worker_pod_names, worker_pod_phases):\n                if check_failed([pod_phase]):\n                    print(""Worker %s log"" % worker)\n                    print_tail_log(client.get_pod_log(worker), 50)\n            client.delete_pod(master_pod_name)\n            exit(-1)\n        else:\n            print(\n                ""Master (status.phase): %s""\n                % client.get_pod_phase(master_pod_name)\n            )\n            print(\n                ""Master (metadata.labels.status): %s""\n                % client.get_pod_label_status(master_pod_name)\n            )\n            for i, ps in enumerate(ps_pod_names):\n                print(""PS%d: %s"" % (i, client.get_pod_phase(ps)))\n            for i, worker in enumerate(worker_pod_names):\n                print(""Worker%d: %s"" % (i, client.get_pod_phase(worker)))\n            time.sleep(10)\n\n    print(""ElasticDL job timed out."")\n    client.delete_pod(master_pod_name)\n    exit(-1)\n\n\nif __name__ == ""__main__"":\n    k8s_client = Client(namespace=""default"")\n    job_type = sys.argv[1]\n    ps_num = int(sys.argv[2])\n    worker_num = int(sys.argv[3])\n    validate_job_status(k8s_client, job_type, ps_num, worker_num)\n'"
elasticdl/proto/__init__.py,0,b''
elasticdl/python/__init__.py,0,b''
elasticdl_preprocessing/feature_column/__init__.py,0,b''
elasticdl_preprocessing/feature_column/feature_column.py,7,"b'import collections\nimport itertools\n\nimport tensorflow as tf\nfrom tensorflow.python.feature_column import feature_column as fc_old\nfrom tensorflow.python.feature_column import feature_column_v2 as fc_lib\n\n\ndef concatenated_categorical_column(categorical_columns):\n    """"""A `CategoricalColumn` to concatenate multiple categorical columns.\n\n    Use this when you have many categorical columns and want to map all\n    to embedding. It\'s recommended to concatenate the sparse id tensors\n    from the categorical columns above to one sparse id tensor using this\n    API, and then map the combined SparseTensor to Embedding. Because\n    there is much overhead to create embedding variables for each\n    categorical column, the model size will be huge. Using this way, we\n    can reduce the model size.\n\n    The output id range of source categorical columns are [0, num_buckets_0),\n    [0, num_buckets_1) ... [0, num_buckets_n). The ids will meet conflict\n    in the combined sparse id tensor because they all start from 0. In this\n    api, we will add offsets in sparse id tensors from each categorical column\n    to avoid this conflict. The id range of the concatenated column is\n    [0, num_bucket_0 + num_bucket_1 + ... + num_bucket_n).\n\n    Example\n\n    ```python\n    id = categorical_column_with_identity(""id"", num_buckets=32)\n    work_class = categorical_column_with_vocabulary_list(\n        ""work_class"",\n        vocabulary_list=[\n            ""Private"",\n            ""Self-emp-not-inc"",\n            ""Self-emp-inc"",\n            ""Federal-gov"",\n            ""Local-gov"",\n            ""State-gov"",\n            ""Without-pay"",\n            ""Never-worked"",\n        ],\n    )\n    concat = concatenated_categorical_column([id, work_class])\n    ```\n\n    For the feature inputs:\n    {\n        ""id"": tf.constant([[1], [-1], [8]]),\n        ""work_class"": tf.constant(\n            [\n                [""""],\n                [""Private""],\n                [""Self-emp-inc""]\n            ], tf.string)\n    }\n\n    The sparse id tensor from `id` column is:\n    shape = [3,1]\n    [0,0]: 1\n    [2,0]: 8\n\n    The sparse id tensor from `workclass` column is:\n    shape = [3,1]\n    [1,0]: 0\n    [2,0]: 2\n\n    The concatenated sparse id tensor from `concat` column is:\n    shape = [3,2]\n    [0,0]: 1\n    [1,0]: 32\n    [2,0]: 8\n    [2,1]: 34\n\n    Returns:\n        A `CategoricalColumn` to concatenate multiple categorical columns.\n\n    Raises:\n        ValueError: `categorical_columns` is missing or not a list.\n        ValueError: `categorical_columns` contains any element which\n            is not CategoricalColumn\n    """"""\n    if not isinstance(categorical_columns, list):\n        raise ValueError(""categorical_columns should be a list"")\n\n    if not categorical_columns:\n        raise ValueError(""categorical_columns shouldn\'t be empty"")\n\n    for column in categorical_columns:\n        if not isinstance(column, fc_lib.CategoricalColumn):\n            raise ValueError(\n                ""Items of categorical_columns should be CategoricalColumn.""\n                "" Given:{}"".format(column)\n            )\n\n    return ConcatenatedCategoricalColumn(\n        categorical_columns=tuple(categorical_columns)\n    )\n\n\nclass ConcatenatedCategoricalColumn(\n    fc_lib.CategoricalColumn,\n    fc_old._CategoricalColumn,\n    collections.namedtuple(\n        ""ConcatenatedCategoricalColumn"", (""categorical_columns"")\n    ),\n):\n    def __init__(self, **kwargs):\n        # Calculate the offset tensor\n        total_num_buckets = 0\n        leaf_column_num_buckets = []\n        for categorical_column in self.categorical_columns:\n            leaf_column_num_buckets.append(categorical_column.num_buckets)\n            total_num_buckets += categorical_column.num_buckets\n        self.accumulated_offsets = list(\n            itertools.accumulate([0] + leaf_column_num_buckets[:-1])\n        )\n        self.total_num_buckets = total_num_buckets\n\n    @property\n    def _is_v2_column(self):\n        for categorical_column in self.categorical_columns:\n            if not categorical_column._is_v2_column:\n                return False\n\n        return True\n\n    @property\n    def name(self):\n        feature_names = []\n        for categorical_column in self.categorical_columns:\n            feature_names.append(categorical_column.name)\n\n        return ""_C_"".join(sorted(feature_names))\n\n    @property\n    def num_buckets(self):\n        return self.total_num_buckets\n\n    @property\n    def _num_buckets(self):\n        return self.total_num_buckets\n\n    def transform_feature(self, transformation_cache, state_manager):\n        feature_tensors = []\n        for categorical_column in self.categorical_columns:\n            ids_and_weights = categorical_column.get_sparse_tensors(\n                transformation_cache, state_manager\n            )\n            feature_tensors.append(ids_and_weights.id_tensor)\n\n        feature_tensors_with_offset = []\n        for index, offset in enumerate(self.accumulated_offsets):\n            feature_tensor = feature_tensors[index]\n            feature_tensor_with_offset = tf.SparseTensor(\n                indices=feature_tensor.indices,\n                values=tf.cast(\n                    tf.add(feature_tensor.values, offset), tf.int64\n                ),\n                dense_shape=feature_tensor.dense_shape,\n            )\n            feature_tensors_with_offset.append(feature_tensor_with_offset)\n\n        return tf.sparse.concat(axis=-1, sp_inputs=feature_tensors_with_offset)\n\n    def get_sparse_tensors(self, transformation_cache, state_manager):\n        return fc_lib.CategoricalColumn.IdWeightPair(\n            transformation_cache.get(self, state_manager), None\n        )\n\n    @property\n    def parents(self):\n        return list(self.categorical_columns)\n\n    @property\n    def parse_example_spec(self):\n        config = {}\n        for categorical_column in self.categorical_columns:\n            config.update(categorical_column.parse_example_spec)\n\n        return config\n\n    @property\n    def _parse_example_spec(self):\n        return self.parse_example_spec\n\n    def get_config(self):\n        from tensorflow.python.feature_column.serialization import (\n            serialize_feature_column,\n        )  # pylint: disable=g-import-not-at-top\n\n        config = dict(zip(self._fields, self))\n        config[""categorical_columns""] = tuple(\n            [serialize_feature_column(fc) for fc in self.categorical_columns]\n        )\n\n        return config\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None, columns_by_name=None):\n        """"""See \'FeatureColumn` base class.""""""\n        from tensorflow.python.feature_column.serialization import (\n            deserialize_feature_column,\n        )  # pylint: disable=g-import-not-at-top\n\n        fc_lib._check_config_keys(config, cls._fields)\n        kwargs = fc_lib._standardize_and_copy_config(config)\n        kwargs[""categorical_columns""] = tuple(\n            [\n                deserialize_feature_column(c, custom_objects, columns_by_name)\n                for c in config[""categorical_columns""]\n            ]\n        )\n\n        return cls(**kwargs)\n'"
elasticdl_preprocessing/layers/__init__.py,0,"b'from __future__ import absolute_import\n\n# flake8: noqa\nfrom elasticdl_preprocessing.layers.concatenate_with_offset import (\n    ConcatenateWithOffset,\n)\nfrom elasticdl_preprocessing.layers.discretization import Discretization\nfrom elasticdl_preprocessing.layers.hashing import Hashing\nfrom elasticdl_preprocessing.layers.index_lookup import IndexLookup\nfrom elasticdl_preprocessing.layers.log_round import LogRound\nfrom elasticdl_preprocessing.layers.normalizer import Normalizer\nfrom elasticdl_preprocessing.layers.round_identity import RoundIdentity\nfrom elasticdl_preprocessing.layers.sparse_embedding import SparseEmbedding\nfrom elasticdl_preprocessing.layers.to_number import ToNumber\nfrom elasticdl_preprocessing.layers.to_ragged import ToRagged\nfrom elasticdl_preprocessing.layers.to_sparse import ToSparse\n'"
elasticdl_preprocessing/layers/concatenate_with_offset.py,11,"b'import tensorflow as tf\n\n\nclass ConcatenateWithOffset(tf.keras.layers.Layer):\n    """"""Layer that add offset to each id tensor in the input list and\n    then concatenate these tensors.\n\n    It takes as input a list of tensors and returns a single tensor.\n    Firstly, it will add an offset in offsets for each tensor in inputs.\n    Then concatenate them to a single tensor. The tensor in inputs\n    must have the same type, `Tensor` or `RaggedTensor` or `SparseTensor` and\n    the same shape.\n\n    Example :\n    ```python\n        a1 = tf.constant([[1], [1], [1]])\n        a2 = tf.constant([[2], [2], [2]])\n        offsets = [0, 10]\n        layer = ConcatenateWithOffset(offsets=offsets, axis=1)\n        layer([a1, a2])\n        [[ 1 12]\n         [ 1 12]\n         [ 1 12]]\n    ```\n\n    Arguments:\n        offsets: numeric list to add\n        axis: Axis along which to concatenate.\n        **kwargs: standard layer keyword arguments.\n    """"""\n\n    def __init__(self, offsets, axis=-1):\n        super(ConcatenateWithOffset, self).__init__()\n        self.offsets = offsets\n        self.axis = axis\n\n    def call(self, inputs):\n        if self.offsets is None:\n            return self._call_with_none_offsets(inputs)\n\n        return self._call_with_offsets(inputs)\n\n    def _call_with_offsets(self, inputs):\n        ids_with_offset = []\n        if not isinstance(inputs, list):\n            return inputs\n\n        if len(self.offsets) != len(inputs):\n            raise ValueError(\n                ""The offsets length is not equal to inputs length""\n                ""the inputs are {}, offsets are {}"".format(\n                    inputs, self.offsets\n                )\n            )\n        for i, tensor in enumerate(inputs):\n            if isinstance(tensor, tf.SparseTensor):\n                ids_with_offset.append(\n                    tf.SparseTensor(\n                        indices=tensor.indices,\n                        values=tensor.values + self.offsets[i],\n                        dense_shape=tensor.dense_shape,\n                    )\n                )\n            else:\n                ids_with_offset.append(tensor + self.offsets[i])\n\n        if isinstance(ids_with_offset[0], tf.SparseTensor):\n            result = tf.sparse.concat(\n                axis=self.axis, sp_inputs=ids_with_offset\n            )\n        else:\n            result = tf.concat(ids_with_offset, axis=self.axis)\n\n        return result\n\n    def _call_with_none_offsets(self, inputs):\n        if isinstance(inputs[0], tf.SparseTensor):\n            result = tf.sparse.concat(axis=self.axis, sp_inputs=inputs)\n        else:\n            result = tf.concat(inputs, axis=self.axis)\n\n        return result\n'"
elasticdl_preprocessing/layers/discretization.py,10,"b'from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\n\n\nclass Discretization(tf.keras.layers.Layer):\n    """"""Buckets data into discrete ranges.\n\n    TensorFlow 2.2 has developed `tf.keras.layers.preprocessing.Discretization`\n    but not released it yet. So the layer is a simple temporary version\n    `tensorflow.python.keras.layers.preprocessing.discretization.Discretization`\n\n    Input shape:\n        Any `tf.Tensor` or `tf.RaggedTensor` of dimension 2 or higher.\n\n    Output shape:\n        The same as the input shape with tf.int64.\n\n    Attributes:\n        bins: Optional boundary specification. Bins include the left boundary\n            and exclude the right boundary, so `bins=[0., 1., 2.]` generates\n            bins `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`.\n  """"""\n\n    def __init__(self, bins, **kwargs):\n        super(Discretization, self).__init__(**kwargs)\n        self._supports_ragged_inputs = True\n        self.bins = bins\n\n    def num_bins(self):\n        """"""The bins is a list with boundaries, so the number of bins is\n        len(bins) + 1.\n        """"""\n        return len(self.bins) + 1\n\n    def get_config(self):\n        config = {\n            ""bins"": self.bins,\n        }\n        base_config = super(Discretization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.RaggedTensor):\n            integer_buckets = tf.ragged.map_flat_values(\n                math_ops._bucketize, inputs, boundaries=self.bins\n            )\n            integer_buckets = tf.identity(integer_buckets)\n        elif isinstance(inputs, tf.SparseTensor):\n            integer_bucket_values = math_ops._bucketize(\n                inputs.values, boundaries=self.bins\n            )\n            integer_buckets = tf.SparseTensor(\n                indices=inputs.indices,\n                values=integer_bucket_values,\n                dense_shape=inputs.dense_shape,\n            )\n        else:\n            integer_buckets = math_ops._bucketize(inputs, boundaries=self.bins)\n\n        return tf.cast(integer_buckets, tf.int64)\n'"
elasticdl_preprocessing/layers/hashing.py,15,"b'from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\n\nclass Hashing(tf.keras.layers.Layer):\n    """"""Distribute categorical feature values into a finite number of buckets\n    by hashing.\n\n    This layer converts a sequence of int or string to a sequence of int.\n    output_id = Hash(input_feature_string) % num_bins for string type input.\n    For int type input, the layer converts the value to string and then\n    processes it by the same formula. TensorFlow 2.2 has developed\n    `tf.keras.layers.preprocessing.Hashing` but not released it yet. So the\n    layer is a simple temporary version.\n    https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/keras/layers/preprocessing/hashing.py\n\n    Note that the TensorFlow version with the layer must be greater than 2.0.0.\n\n    Example:\n    ```python\n    layer = Hashing(num_bins=3)\n    inp = np.asarray([[\'A\'], [\'B\'], [\'C\'], [\'D\'], [\'E\']])\n    layer(inp)\n    ```\n    The output will be `[[1], [0], [1], [1], [2]]`\n\n    Arguments:\n        num_bins: Number of hash bins.\n        **kwargs: Keyword arguments to construct a layer.\n\n    Input: A string, int32 or int64 `tf.Tensor`,\n        `tf.SparseTensor` or `tf.RaggedTensor`\n\n    Output: An int64 tensor with the same shape as input.\n\n    """"""\n\n    def __init__(self, num_bins, **kwargs):\n        if num_bins is None or num_bins <= 0:\n            raise ValueError(\n                ""`num_bins` cannot be `None` or non-positive values.""\n            )\n        super(Hashing, self).__init__(**kwargs)\n        self.num_bins = num_bins\n        self._supports_ragged_inputs = True\n\n    def call(self, inputs):\n        # Converts integer inputs to string.\n        if inputs.dtype.is_integer:\n            if isinstance(inputs, tf.SparseTensor):\n                inputs = tf.SparseTensor(\n                    indices=inputs.indices,\n                    values=tf.as_string(inputs.values),\n                    dense_shape=inputs.dense_shape,\n                )\n            else:\n                inputs = tf.as_string(inputs)\n        if isinstance(inputs, tf.RaggedTensor):\n            return tf.ragged.map_flat_values(\n                tf.strings.to_hash_bucket_fast,\n                inputs,\n                num_buckets=self.num_bins,\n                name=""hash"",\n            )\n        elif isinstance(inputs, tf.SparseTensor):\n            sparse_values = inputs.values\n            sparse_hashed_values = tf.strings.to_hash_bucket_fast(\n                sparse_values, self.num_bins, name=""hash""\n            )\n            return tf.SparseTensor(\n                indices=inputs.indices,\n                values=sparse_hashed_values,\n                dense_shape=inputs.dense_shape,\n            )\n        else:\n            return tf.strings.to_hash_bucket_fast(\n                inputs, self.num_bins, name=""hash""\n            )\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {""num_bins"": self.num_bins}\n        base_config = super(Hashing, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
elasticdl_preprocessing/layers/index_lookup.py,10,"b'from __future__ import absolute_import, division, print_function\n\nimport collections\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import lookup_ops\n\n\nclass IndexLookup(tf.keras.layers.Layer):\n    """"""Maps strings to integer indices by looking up a vocabulary.\n\n    This layer transforms categorical inputs to zero-based integer by\n    lookuping with a vocabulary list. TensorFlow 2.2 has developed\n    `tf.keras.layers.preprocessing.IndexLookup` but not released it yet.\n    So the layer is a simple temporary version. The codes in TensorFlow 2.2 is\n    `tensorflow.python.keras.layers.preprocessing.index_lookup.IndexLookup`.\n\n    Note that the TensorFlow version with the layer must be greater than 2.0.0.\n\n    Example:\n    ```python\n    layer = IndexLookup(vocabulary=[\'A\', \'B\', \'C\'])\n    inp = np.array([[\'A\'], [\'B\'], [\'C\'], [\'D\'], [\'E\']])\n    layer(inputs)\n    ```\n    Then output will be `[[0], [1], [2], [3], [3]]`\n\n    Attributes:\n    num_oov_tokens: The number of out-of-vocabulary tokens to use; defaults to\n        1. If this value is more than 1,\n        `hash(inputs) % num_oov_tokens + len(vocabulary)` converts OOV inputs\n        to integer values.\n    vocabulary: A list of vocabulary terms, or a path to a text file\n        containing a vocabulary to load into this layer. The file should\n        contain one token per line.\n\n    Input: A string `tf.Tensor`,`tf.SparseTensor` or\n        `tf.RaggedTensor`.\n\n    Output: An int64 tensor with the same type as input.\n\n    """"""\n\n    def __init__(self, vocabulary=None, num_oov_tokens=1, **kwargs):\n        super(IndexLookup, self).__init__()\n        self.num_oov_tokens = num_oov_tokens\n\n        if vocabulary is not None and isinstance(vocabulary, str):\n            vocabulary = self._get_vocabulary_from_file(vocabulary)\n            vocabulary_set = set(vocabulary)\n            if len(vocabulary) != len(vocabulary_set):\n                repeated_items = [\n                    item\n                    for item, count in collections.Counter(vocabulary).items()\n                    if count > 1\n                ]\n                raise ValueError(\n                    ""The passed vocabulary has at least one repeated ""\n                    ""term. Please uniquify your dataset before passing ""\n                    ""it to IndexLookup(). The repeated terms are %s""\n                    % repeated_items\n                )\n        self.vocabulary = vocabulary\n\n    def build(self, input_shape):\n        self._table = lookup_ops.index_table_from_tensor(\n            vocabulary_list=self.vocabulary,\n            num_oov_buckets=self.num_oov_tokens,\n        )\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.SparseTensor):\n            lookup_id = self._table.lookup(inputs.values)\n            output = tf.SparseTensor(\n                indices=inputs.indices,\n                values=lookup_id,\n                dense_shape=inputs.dense_shape,\n            )\n        elif isinstance(inputs, tf.RaggedTensor):\n            return tf.ragged.map_flat_values(self._table.lookup, inputs,)\n        else:\n            output = self._table.lookup(inputs)\n        return tf.cast(output, tf.int64)\n\n    def _get_vocabulary_from_file(self, vocabulary_path):\n        vocab = []\n        with tf.io.gfile.GFile(vocabulary_path, ""r"") as reader:\n            while True:\n                # Get the next line, and break if it is None.\n                text = reader.readline()\n                if not text:\n                    break\n\n                # Convert the raw text into UTF8 and strip whitespace.\n                if isinstance(text, str):\n                    token = text\n                elif isinstance(text, bytes):\n                    token = text.decode(""utf-8"", ""ignore"")\n                token = token.strip()\n                vocab.append(token)\n        return vocab\n\n    def vocab_size(self):\n        return self._table.size().numpy()\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            ""num_oov_tokens"": self.num_oov_tokens,\n            ""vocabulary"": None,\n        }\n        base_config = super(IndexLookup, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
elasticdl_preprocessing/layers/log_round.py,15,"b'import tensorflow as tf\nfrom tensorflow.python.ops.ragged import ragged_functional_ops, ragged_tensor\n\n\ndef log(x, base):\n    x = tf.cast(x, tf.float64)\n    numerator = tf.math.log(x)\n\n    if base is None:\n        return numerator\n\n    denominator = tf.math.log(tf.constant(base, dtype=numerator.dtype))\n    return numerator / denominator\n\n\nclass LogRound(tf.keras.layers.Layer):\n    """"""Cast a numeric value into a discrete integer value by\n    `round(log(x))`.\n\n\n    Example :\n    ```python\n        layer = LogRound(num_bins=16, base=2)\n        inp = np.asarray([[1.2], [1.6], [0.2], [3.1], [100]])\n        layer(inp)\n        [[0], [1], [0], [2], [7]]\n    ```\n\n    Arguments:\n        num_bins: Range of inputs and outputs is `[0, num_bins)`.\n        **kwargs: Keyword arguments to construct a layer.\n\n    Input shape: A numeric `Tensor`, `SparseTensor` or `RaggedTensor` of shape\n        `[batch_size, d1, ..., dm]`\n\n    Output shape: An int64 tensor of shape `[batch_size, d1, ..., dm]`\n\n    """"""\n\n    def __init__(self, num_bins, default_value=0, base=None):\n        super(LogRound, self).__init__()\n        self.num_bins = num_bins\n        self.default_value = default_value\n        self.base = base\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.SparseTensor):\n            id_values = self._log_round(inputs.values)\n            result = tf.SparseTensor(\n                indices=inputs.indices,\n                values=id_values,\n                dense_shape=inputs.dense_shape,\n            )\n        elif ragged_tensor.is_ragged(inputs):\n            result = ragged_functional_ops.map_flat_values(\n                self._log_round, inputs\n            )\n        else:\n            result = self._log_round(inputs)\n        return tf.cast(result, tf.int64)\n\n    def _log_round(self, values):\n        values = tf.cast(values, tf.float64)\n        values = tf.math.round(log(values, base=self.base))\n        values = tf.cast(values, tf.int64)\n        num_bins = tf.cast(self.num_bins, tf.int64)\n        default_value = tf.cast(self.default_value, tf.int64)\n        values = tf.where(\n            tf.logical_or(values < 0, values >= num_bins),\n            x=tf.fill(dims=tf.shape(values), value=default_value),\n            y=values,\n        )\n        return values\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            ""num_bins"": self.num_bins,\n            ""base"": self.base,\n            ""default_value"": self.default_value,\n        }\n        base_config = super(LogRound, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
elasticdl_preprocessing/layers/normalizer.py,8,"b'import tensorflow as tf\n\n\nclass Normalizer(tf.keras.layers.Layer):\n    """"""Normalize the numeric tensors by (x-subtractor)/divisor\n\n    Example :\n    ```python\n        layer = Normalizer(subtractor=1.0, divisor=2.0)\n        inp = np.asarray([[3.0], [5.0], [7.0]])\n        layer(inp)\n        [[1.0], [2.0], [3.0]]\n    ```\n\n    Arguments:\n        subtractor: A float value.\n        divisor: A float value.\n\n    Input shape: A numeric `Tensor`, `SparseTensor` or `RaggedTensor` of shape\n        `[batch_size, d1, ..., dm]`\n\n    Output shape: An float64 tensor of shape `[batch_size, d1, ..., dm]`\n\n    """"""\n\n    def __init__(self, subtractor, divisor, **kwargs):\n        super(Normalizer, self).__init__(**kwargs)\n        self._supports_ragged_inputs = True\n        self.subtractor = subtractor\n        self.divisor = divisor\n\n    def build(self, input_shape):\n        if self.divisor == 0:\n            raise ValueError(""The divisor cannot be 0"")\n\n    def get_config(self):\n        config = {""subtractor"": self.subtractor, ""divisor"": self.divisor}\n        base_config = super(Normalizer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.RaggedTensor):\n            normalized_tensor = tf.ragged.map_flat_values(\n                self._normalize_fn, inputs\n            )\n        elif isinstance(inputs, tf.SparseTensor):\n            normalize_values = self._normalize_fn(inputs.values)\n            normalized_tensor = tf.SparseTensor(\n                indices=inputs.indices,\n                values=normalize_values,\n                dense_shape=inputs.dense_shape,\n            )\n        else:\n            normalized_tensor = self._normalize_fn(inputs)\n\n        return normalized_tensor\n\n    def _normalize_fn(self, x):\n        x = tf.cast(x, tf.float32)\n        subtractor = tf.cast(self.subtractor, tf.float32)\n        divisor = tf.cast(self.divisor, tf.float32)\n        return (x - subtractor) / divisor\n'"
elasticdl_preprocessing/layers/round_identity.py,11,"b'import tensorflow as tf\nfrom tensorflow.python.ops.ragged import ragged_functional_ops, ragged_tensor\n\n\nclass RoundIdentity(tf.keras.layers.Layer):\n    """"""Cast a numeric feature into a discrete integer value.\n\n    This layer transforms numeric inputs to integer output. It is a special\n    case of bucketizing to bins. The max value in the layer is the number of\n    bins.\n\n    Example :\n    ```python\n        layer = RoundIdentity(max_value=5)\n        inp = np.asarray([[1.2], [1.6], [0.2], [3.1], [4.9]])\n        layer(inp)\n        [[1], [2], [0], [3], [5]]\n    ```\n\n    Arguments:\n        num_buckets: Range of inputs and outputs is `[0, num_buckets)`.\n        **kwargs: Keyword arguments to construct a layer.\n\n    Input shape: A numeric `Tensor`, `SparseTensor` or `RaggedTensor` of shape\n        `[batch_size, d1, ..., dm]`\n\n    Output shape: An int64 tensor of shape `[batch_size, d1, ..., dm]`\n\n    """"""\n\n    def __init__(self, num_buckets, default_value=0):\n        super(RoundIdentity, self).__init__()\n        self.num_buckets = num_buckets\n        self.default_value = default_value\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.SparseTensor):\n            id_values = self._round_and_truncate(inputs.values)\n            result = tf.SparseTensor(\n                indices=inputs.indices,\n                values=id_values,\n                dense_shape=inputs.dense_shape,\n            )\n        elif ragged_tensor.is_ragged(inputs):\n            result = ragged_functional_ops.map_flat_values(\n                self._round_and_truncate, inputs\n            )\n        else:\n            result = self._round_and_truncate(inputs)\n        return tf.cast(result, tf.int64)\n\n    def _round_and_truncate(self, values):\n        values = tf.keras.backend.round(values)\n        values = tf.cast(values, tf.int64)\n        num_buckets = tf.cast(self.num_buckets, tf.int64)\n        default_value = tf.cast(self.default_value, tf.int64)\n        values = tf.where(\n            tf.logical_or(values < 0, values >= num_buckets),\n            x=tf.fill(dims=tf.shape(values), value=default_value),\n            y=values,\n        )\n        return values\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            ""num_buckets"": self.num_buckets,\n            ""default_value"": self.default_value,\n        }\n        base_config = super(RoundIdentity, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
elasticdl_preprocessing/layers/sparse_embedding.py,6,"b'import tensorflow as tf\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras import initializers\nfrom tensorflow.python.ops import embedding_ops, math_ops\n\n\nclass SparseEmbedding(tf.keras.layers.Layer):\n    """"""\n    Input: indexes for the embedding entries with a shape of\n        (batch_size, input_length). Input is a SparseTensor.\n    Output:\n        embeddings with a shape (batch_size, output_dim)\n    Arguments:\n        input_dim: the max input id. If 0 or None, will not check the range of\n            input embedding ids.\n        output_dim: the dimension of the embedding vector\n        embeddings_initializer: Initializer for embedding table.\n        combiner: A string specifying the reduction op or None if not used.\n            ""mean"", ""sqrtn"" and ""sum"" are supported for the reduction op.\n            If input is SparseTensor, combiner must set as a reduction op.\n    """"""\n\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        embeddings_initializer=""uniform"",\n        combiner=""mean"",\n        **kwargs\n    ):\n        dtype = kwargs.pop(""dtype"", K.floatx())\n        super(SparseEmbedding, self).__init__(dtype=dtype, **kwargs)\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embeddings_initializer = initializers.get(embeddings_initializer)\n        self.combiner = combiner\n\n        if self.combiner not in [""sum"", ""mean"", ""sqrtn""]:\n            raise ValueError(\n                ""combiner must set sum, mean or sqrtn for sparse input""\n            )\n\n    def build(self, input_shape):\n        self.embeddings = self.add_weight(\n            shape=(self.input_dim, self.output_dim),\n            initializer=self.embeddings_initializer,\n            name=""embeddings"",\n            trainable=True,\n        )\n        self.built = True\n\n    def call(self, inputs):\n        # When saving a model with the layer, `tf.saved_model.save` will\n        # feed the inputs with a Tensor not a SparseTensor, so we should\n        # convert Tensor to `SparseTensor`.\n        if not isinstance(inputs, tf.SparseTensor):\n            idx = tf.where(tf.not_equal(inputs, 0))\n            inputs = tf.SparseTensor(\n                idx, tf.gather_nd(inputs, idx), (-1, self.input_dim)\n            )\n\n        dtype = K.dtype(inputs)\n        if dtype != ""int32"" and dtype != ""int64"":\n            inputs = math_ops.cast(inputs, ""int32"")\n        out = embedding_ops.safe_embedding_lookup_sparse(\n            embedding_weights=self.embeddings,\n            sparse_ids=inputs,\n            sparse_weights=None,\n            combiner=self.combiner,\n        )\n        return out\n'"
elasticdl_preprocessing/layers/to_number.py,21,"b'import tensorflow as tf\n\n_NUMBER_DTYPES = [\n    tf.int8,\n    tf.uint8,\n    tf.int16,\n    tf.uint16,\n    tf.int32,\n    tf.uint32,\n    tf.int64,\n    tf.uint64,\n    tf.float16,\n    tf.float32,\n    tf.float64,\n    tf.bfloat16,\n    tf.double,\n]\n\n\nclass ToNumber(tf.keras.layers.Layer):\n    """"""Convert the inputs to a number dtype (int, float, double)\n\n    Input Shape: Tensor or SparseTensor of any shape\n    Output Shape: Tensor or SparseTensor of the same shape with input\n    """"""\n\n    def __init__(self, out_type, default_value):\n        super(ToNumber, self).__init__()\n        if out_type not in _NUMBER_DTYPES:\n            raise ValueError(""{} is not a number type."".format(out_type))\n        self.out_type = out_type\n        self.default_value = default_value\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.SparseTensor):\n            number_value = self._cast_dense_to_number(inputs.values)\n            return tf.SparseTensor(\n                indices=inputs.indices,\n                values=number_value,\n                dense_shape=inputs.dense_shape,\n            )\n        else:\n            return self._cast_dense_to_number(inputs)\n\n    def _cast_dense_to_number(self, dense_inputs):\n        if dense_inputs.dtype is tf.string:\n            default_value = str(self.default_value)\n            outputs = tf.where(\n                tf.equal(dense_inputs, """"), x=default_value, y=dense_inputs\n            )\n            outputs = tf.strings.to_number(outputs, out_type=self.out_type)\n        else:\n            outputs = tf.cast(dense_inputs, self.out_type)\n\n        return outputs\n'"
elasticdl_preprocessing/layers/to_ragged.py,10,"b'import tensorflow as tf\n\n_COMMA_SEP = "",""\n\n\nclass ToRagged(tf.keras.layers.Layer):\n    """"""Converts a `Tensor` or `RaggedTensor` to a `RaggedTensor`,\n    dropping ignore_value cells. If the input\'s dtype is string, split\n    the string elements to convert the input to `RaggedTensor` firstly.\n\n    Note that the TensorFlow version with the layer must be greater than 2.0.0.\n\n    Example (Integer):\n    ```python\n        layer = ToRagged()\n        input_tensor = tf.constant([[1], [-1], [4]], tf.int64)\n        out = layer(input_tensor)\n        [[1], [], [4]]\n    ```\n\n    Example (String):\n    ```python\n        layer = ToRagged()\n        input_tensor = tf.constant([[""1"", ""2"", ""3""], [""4"", ""5""], [""""]])\n        out = layer(input_tensor)\n    ```\n    The expected output is `[[""1"", ""2"", ""3""], [""4"", ""5""], []]`\n\n    Arguments:\n        sep: Valid if the input\'s dtype is string.\n        ignore_value: Entries in inputs equal to this value will be\n            absent from the output `RaggedTensor`. If `None`, default value of\n            input\'s dtype will be used (\'\' for `str`, -1 for `int`).\n\n    Input shape: A numeric or string `Tensor` or `RaggedTensor` of shape\n        `[batch_size, d1, ..., dm]`\n\n    Output shape: An `RaggedTensor` with the same shape as inputs\n    """"""\n\n    def __init__(self, sep=_COMMA_SEP, ignore_value=None):\n        super(ToRagged, self).__init__()\n        self.sep = sep\n        self.ignore_value = ignore_value\n\n    def call(self, inputs):\n        if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):\n            raise TypeError(\n                ""The inputs must be a Tensor or RaggedTensor and ""\n                ""the type of inputs is {}"".format(type(inputs))\n            )\n\n        if isinstance(inputs, tf.Tensor):\n            inputs = tf.RaggedTensor.from_tensor(inputs)\n\n        ignore_value = self._get_ignore_value(inputs.dtype)\n        if ignore_value is None:\n            return inputs\n        else:\n            return tf.ragged.boolean_mask(\n                inputs, tf.not_equal(inputs, ignore_value)\n            )\n\n    def _get_ignore_value(self, input_dtype):\n        ignore_value = self.ignore_value\n\n        if ignore_value is None:\n            if input_dtype == tf.string:\n                ignore_value = """"\n            elif input_dtype.is_integer:\n                ignore_value = -1\n            else:\n                return None\n\n        return tf.cast(ignore_value, input_dtype)\n'"
elasticdl_preprocessing/layers/to_sparse.py,10,"b'import tensorflow as tf\n\n\nclass ToSparse(tf.keras.layers.Layer):\n    """"""Converts a `Tensor` to a `SparseTensor`, dropping ignore_value cells.\n    If the input is already a `SparseTensor`, just return it.\n\n    Example :\n    ```python\n        layer = ToSparse()\n        inp = tf.constant([[""A"", """"], [""B"", ""C""]], tf.string)\n        out = layer(inp)\n    ```\n    The expected output is\n    ```\n    tf.SparseTensor(\n            indices=np.array([[0, 0], [1, 0], [1, 1]]),\n            values=np.array([""A"", ""B"", ""C""]),\n            dense_shape=(2, 2),\n        )\n    ```\n\n    Arguments:\n        ignore_value: Entries in inputs equal to this value will be\n            absent from the output `SparseTensor`. If `None`, default value of\n            inputs dtype will be used (\'\' for `str`, -1 for `int`).\n\n    Input shape: A numeric or string `Tensor` of shape\n        `[batch_size, d1, ..., dm]`\n\n    Output shape: An `SparseTensor` with the same shape as inputs\n    """"""\n\n    def __init__(self, ignore_value=None):\n        super(ToSparse, self).__init__()\n        self.ignore_value = ignore_value\n\n    def call(self, inputs):\n        if isinstance(inputs, tf.SparseTensor):\n            return inputs\n\n        ignore_value = self.ignore_value\n        if ignore_value is None:\n            if inputs.dtype == tf.string:\n                ignore_value = """"\n            elif inputs.dtype.is_integer:\n                ignore_value = -1\n            else:\n                ignore_value = 0.0\n        ignore_value = tf.cast(ignore_value, inputs.dtype)\n        indices = tf.where(tf.not_equal(inputs, ignore_value))\n        values = tf.gather_nd(inputs, indices)\n        dense_shape = tf.shape(inputs, out_type=tf.int64)\n        return tf.SparseTensor(\n            indices=indices, values=values, dense_shape=dense_shape\n        )\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            ""ignore_value"": self.ignore_value,\n        }\n        base_config = super(ToSparse, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
elasticdl_preprocessing/tests/__init__.py,0,b''
elasticdl_preprocessing/tests/analyzer_util_test.py,0,"b'import os\nimport unittest\n\nfrom elasticdl_preprocessing.utils import analyzer_utils\n\n\nclass AnalyzerUtilTest(unittest.TestCase):\n    def test_methods_from_environment(self):\n        # using default value\n        self.assertEqual(analyzer_utils.get_min(""age"", 10), 10)\n        self.assertEqual(analyzer_utils.get_max(""age"", 100), 100)\n        self.assertEqual(analyzer_utils.get_avg(""age"", 10), 10)\n        self.assertEqual(analyzer_utils.get_stddev(""age"", 10), 10)\n        self.assertListEqual(\n            analyzer_utils.get_bucket_boundaries(""age"", [10, 78]), [10, 78]\n        )\n        self.assertEqual(analyzer_utils.get_distinct_count(""city"", 19), 19)\n        self.assertListEqual(\n            analyzer_utils.get_vocabulary(""city"", [""a"", ""b""]), [""a"", ""b""]\n        )\n\n        # Get value from environment\n        os.environ[""_age_min""] = ""11""\n        os.environ[""_age_max""] = ""100""\n        os.environ[""_age_avg""] = ""50.9""\n        os.environ[""_age_stddev""] = ""90.87""\n        os.environ[""_age_boundaries""] = ""15,67,89""\n        os.environ[""_city_distinct_count""] = ""50""\n        os.environ[""_city_vocab""] = ""./city.txt""\n\n        self.assertEqual(analyzer_utils.get_min(""age"", 10), 11)\n        self.assertEqual(analyzer_utils.get_max(""age"", 10), 100)\n        self.assertEqual(analyzer_utils.get_avg(""age"", 10), 50.9)\n        self.assertEqual(analyzer_utils.get_stddev(""age"", 10), 90.87)\n        self.assertListEqual(\n            analyzer_utils.get_bucket_boundaries(""age"", [10, 78]), [15, 67, 89]\n        )\n        self.assertEqual(analyzer_utils.get_distinct_count(""city"", 19), 50)\n\n        self.assertEqual(\n            analyzer_utils.get_vocabulary(""city"", [""a"", ""b""]), ""./city.txt"",\n        )\n'"
elasticdl_preprocessing/tests/concatenate_with_offset_test.py,6,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.concatenate_with_offset import (\n    ConcatenateWithOffset,\n)\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass ConcatenateWithOffsetTest(unittest.TestCase):\n    def test_concatenate_with_offset(self):\n        tensor_1 = tf.constant([[1], [1], [1]])\n        tensor_2 = tf.constant([[2], [2], [2]])\n        offsets = [0, 10]\n        concat_layer = ConcatenateWithOffset(offsets=offsets, axis=1)\n\n        output = concat_layer([tensor_1, tensor_2])\n        expected_out = np.array([[1, 12], [1, 12], [1, 12]])\n        self.assertTrue(np.array_equal(output.numpy(), expected_out))\n\n        ragged_tensor_1 = tf.ragged.constant([[1], [], [1]], dtype=tf.int64)\n        ragged_tensor_2 = tf.ragged.constant([[2], [2], []], dtype=tf.int64)\n        output = concat_layer([ragged_tensor_1, ragged_tensor_2])\n        expected_out = tf.ragged.constant([[1, 12], [12], [1]], dtype=tf.int64)\n        self.assertTrue(ragged_tensor_equal(output, expected_out))\n\n        sparse_tensor_1 = ragged_tensor_1.to_sparse()\n        sparse_tensor_2 = ragged_tensor_2.to_sparse()\n        output = concat_layer([sparse_tensor_1, sparse_tensor_2])\n        expected_out = tf.SparseTensor(\n            indices=np.array([[0, 0], [0, 1], [1, 1], [2, 0]]),\n            values=np.array([1, 12, 12, 1]),\n            dense_shape=(3, 2),\n        )\n        self.assertTrue(sparse_tensor_equal(output, expected_out))\n'"
elasticdl_preprocessing/tests/discretization_test.py,4,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.discretization import Discretization\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass DiscretizationTest(unittest.TestCase):\n    def test_discretize(self):\n        discretize_layer = Discretization(bins=[1, 5, 10])\n\n        dense_in = tf.constant([[0.2], [1.6], [4.2], [6.1], [10.9]])\n        dense_out = discretize_layer(dense_in)\n        expected_out = np.array([[0], [1], [1], [2], [3]])\n        self.assertTrue(np.array_equal(dense_out.numpy(), expected_out))\n\n        ragged_input = tf.ragged.constant([[0.2, 1.6, 4.2], [6.1], [10.9]])\n        ragged_output = discretize_layer(ragged_input)\n        expected_ragged_out = tf.ragged.constant(\n            [[0, 1, 1], [2], [3]], dtype=tf.int64\n        )\n        self.assertTrue(\n            ragged_tensor_equal(ragged_output, expected_ragged_out)\n        )\n\n        sparse_input = ragged_input.to_sparse()\n        sparse_output = discretize_layer(sparse_input)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(\n            sparse_tensor_equal(sparse_output, expected_sparse_out)\n        )\n'"
elasticdl_preprocessing/tests/feature_column_test.py,21,"b'import copy\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.feature_column.feature_column import (\n    concatenated_categorical_column,\n)\n\n\ndef call_feature_columns(feature_columns, input):\n    dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n    return dense_features(input)\n\n\nclass ConcatenatedCategoricalColumnTest(unittest.TestCase):\n    def test_name(self):\n        a = tf.feature_column.categorical_column_with_hash_bucket(\n            ""aaa"", hash_bucket_size=1024\n        )\n        b = tf.feature_column.categorical_column_with_identity(\n            ""bbb"", num_buckets=32\n        )\n        c = tf.feature_column.bucketized_column(\n            tf.feature_column.numeric_column(""ccc""), boundaries=[1, 2, 3, 4, 5]\n        )\n        concat = concatenated_categorical_column([a, b, c])\n        self.assertEqual(""aaa_C_bbb_C_ccc_bucketized"", concat.name)\n\n    def test_is_v2_column(self):\n        a = tf.feature_column.categorical_column_with_hash_bucket(\n            ""aaa"", hash_bucket_size=1024\n        )\n        b = tf.feature_column.categorical_column_with_identity(\n            ""bbb"", num_buckets=32\n        )\n        concat = concatenated_categorical_column([a, b])\n        self.assertTrue(concat._is_v2_column)\n\n    def test_num_buckets(self):\n        a = tf.feature_column.categorical_column_with_hash_bucket(\n            ""aaa"", hash_bucket_size=1024\n        )\n        b = tf.feature_column.categorical_column_with_identity(\n            ""bbb"", num_buckets=32\n        )\n        concat = concatenated_categorical_column([a, b])\n        self.assertEqual(1056, concat.num_buckets)\n\n    def test_parse_spec(self):\n        a = tf.feature_column.categorical_column_with_hash_bucket(\n            ""aaa"", hash_bucket_size=1024, dtype=tf.string\n        )\n        b = tf.feature_column.bucketized_column(\n            tf.feature_column.numeric_column(""bbb"", dtype=tf.int32),\n            boundaries=[1, 2, 3, 4, 5],\n        )\n        concat = concatenated_categorical_column([a, b])\n        self.assertEqual(\n            {\n                ""aaa"": tf.io.VarLenFeature(dtype=tf.string),\n                ""bbb"": tf.io.FixedLenFeature(shape=(1,), dtype=tf.int32),\n            },\n            concat.parse_example_spec,\n        )\n\n    def test_deep_copy(self):\n        a = tf.feature_column.categorical_column_with_hash_bucket(\n            ""aaa"", hash_bucket_size=1024\n        )\n        b = tf.feature_column.categorical_column_with_identity(\n            ""bbb"", num_buckets=32\n        )\n        concat = concatenated_categorical_column([a, b])\n        concat_copy = copy.deepcopy(concat)\n        self.assertEqual(""aaa_C_bbb"", concat_copy.name)\n        self.assertEqual(1056, concat_copy.num_buckets)\n\n    def test_call_column(self):\n        user_id = tf.feature_column.categorical_column_with_identity(\n            ""user_id"", num_buckets=32\n        )\n\n        item_id = tf.feature_column.categorical_column_with_identity(\n            ""item_id"", num_buckets=128\n        )\n\n        item_id_user_id_concat = concatenated_categorical_column(\n            [user_id, item_id]\n        )\n\n        concat_indicator = tf.feature_column.indicator_column(\n            item_id_user_id_concat\n        )\n\n        output = call_feature_columns(\n            [concat_indicator], {""user_id"": [10, 20], ""item_id"": [1, 120]},\n        )\n\n        expected_output = tf.one_hot(indices=[10, 20], depth=160) + tf.one_hot(\n            indices=[1 + 32, 120 + 32], depth=160\n        )\n\n        self.assertTrue(\n            np.array_equal(output.numpy(), expected_output.numpy())\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl_preprocessing/tests/hashing_test.py,3,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.hashing import Hashing\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass HashingTest(unittest.TestCase):\n    def test_hashing(self):\n        hash_layer = Hashing(num_bins=3)\n        inp = np.asarray([[""A""], [""B""], [""C""], [""D""], [""E""]])\n        hash_out = hash_layer(inp)\n        expected_out = np.array([[1], [0], [1], [1], [2]])\n        self.assertTrue(np.array_equal(hash_out.numpy(), expected_out))\n\n        ragged_in = tf.ragged.constant([[""A"", ""B""], [""C"", ""D""], [""E""], []])\n        hash_out = hash_layer(ragged_in)\n        expected_ragged_out = tf.ragged.constant(\n            [[1, 0], [1, 1], [2], []], dtype=tf.int64\n        )\n        self.assertTrue(ragged_tensor_equal(hash_out, expected_ragged_out))\n\n        sparse_in = ragged_in.to_sparse()\n        hash_out = hash_layer(sparse_in)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(sparse_tensor_equal(hash_out, expected_sparse_out))\n'"
elasticdl_preprocessing/tests/index_lookup_test.py,7,"b'import os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.index_lookup import IndexLookup\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass IndexLookupTest(unittest.TestCase):\n    def test_lookup_with_list(self):\n        lookup_layer = IndexLookup(vocabulary=[""A"", ""B"", ""C""])\n        self._check_lookup(lookup_layer)\n        self.assertEqual(lookup_layer.vocab_size(), 4)\n\n    def test_lookup_with_file(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            vocab_file = os.path.join(temp_dir, ""vocab_test.txt"")\n            with open(vocab_file, ""w"") as f:\n                f.write(""A\\n"")\n                f.write(""B\\n"")\n                f.write(""C\\n"")\n            lookup_layer = IndexLookup(vocabulary=vocab_file)\n            self._check_lookup(lookup_layer)\n\n    def test_model_with_lookup(self):\n        inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n        lookup_out = IndexLookup(vocabulary=[""A"", ""B"", ""C""])(inputs)\n        model = tf.keras.Model(inputs=inputs, outputs=lookup_out)\n        out = model.call(tf.constant([[""A""], [""C""], [""B""], [""D""], [""E""]]))\n        self.assertTrue(\n            np.array_equal(\n                np.array([[0], [2], [1], [3], [3]], dtype=int), out.numpy()\n            )\n        )\n\n    def _check_lookup(self, lookup_layer):\n        dense_input = tf.constant([[""A""], [""B""], [""C""], [""D""], [""E""]])\n        output = lookup_layer(dense_input)\n        expected_out = np.array([[0], [1], [2], [3], [3]])\n        self.assertTrue(np.array_equal(output.numpy(), expected_out))\n\n        ragged_input = tf.ragged.constant([[""A"", ""B"", ""C""], [""D"", ""E""]])\n        ragged_output = lookup_layer(ragged_input)\n        expected_ragged_out = tf.ragged.constant(\n            [[0, 1, 2], [3, 3]], dtype=tf.int64\n        )\n        self.assertTrue(\n            ragged_tensor_equal(ragged_output, expected_ragged_out)\n        )\n\n        sparse_input = ragged_input.to_sparse()\n        sparse_output = lookup_layer(sparse_input)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(\n            sparse_tensor_equal(sparse_output, expected_sparse_out)\n        )\n'"
elasticdl_preprocessing/tests/log_round_test.py,7,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers import LogRound\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass LogRoundTest(unittest.TestCase):\n    def test_round_indentity(self):\n        log_round = LogRound(num_bins=20, base=2)\n\n        dense_input = tf.constant([[0.0], [2.6], [0.2], [3.1], [1024]])\n        output = log_round(dense_input)\n        expected_out = np.array([[0], [1], [0], [2], [10]])\n\n        self.assertTrue(np.array_equal(output.numpy(), expected_out))\n\n        ragged_input = tf.ragged.constant([[1.1, 3.4], [1025]])\n        ragged_output = log_round(ragged_input)\n        expected_ragged_out = tf.ragged.constant(\n            [[0, 2], [10]], dtype=tf.int64\n        )\n        self.assertTrue(\n            ragged_tensor_equal(ragged_output, expected_ragged_out)\n        )\n\n        sparse_input = ragged_input.to_sparse()\n        sparse_output = log_round(sparse_input)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(\n            sparse_tensor_equal(sparse_output, expected_sparse_out)\n        )\n\n    def test_model_with_loground(self):\n        inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n        log_round = LogRound(num_bins=20, base=2)(inputs)\n        model = tf.keras.Model(inputs=inputs, outputs=log_round)\n        out = model.call(tf.constant([[1.2], [2.6], [0.2], [3.1], [1024]]))\n        self.assertTrue(\n            np.array_equal(\n                np.array([[0], [1], [0], [2], [10]], dtype=int), out.numpy()\n            )\n        )\n'"
elasticdl_preprocessing/tests/normalizer_test.py,7,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers import Normalizer\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass NormalizerTest(unittest.TestCase):\n    def test_normalizer(self):\n        normalizer = Normalizer(1.0, 2.0)\n\n        dense_input = tf.constant([[5.0], [7.0], [9.0], [11.0], [13.0]])\n        output = normalizer(dense_input)\n        expected_out = np.array([[2.0], [3.0], [4.0], [5.0], [6.0]])\n\n        self.assertTrue(np.array_equal(output.numpy(), expected_out))\n\n        ragged_input = tf.ragged.constant([[5.0, 7.0], [9.0]])\n        ragged_output = normalizer(ragged_input)\n        expected_ragged_out = tf.ragged.constant(\n            [[2.0, 3.0], [4.0]], dtype=tf.float32\n        )\n        self.assertTrue(\n            ragged_tensor_equal(ragged_output, expected_ragged_out)\n        )\n\n        sparse_input = ragged_input.to_sparse()\n        sparse_output = normalizer(sparse_input)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(\n            sparse_tensor_equal(sparse_output, expected_sparse_out)\n        )\n\n    def test_model_with_normalizer(self):\n        inputs = tf.keras.Input(shape=(1,), dtype=tf.float32)\n        normalize = Normalizer(1.0, 2.0)(inputs)\n        model = tf.keras.Model(inputs=inputs, outputs=normalize)\n        out = model.call(tf.constant([[5.0], [7.0], [9.0], [11.0], [13.0]]))\n        self.assertTrue(\n            np.array_equal(\n                np.array([[2.0], [3.0], [4.0], [5.0], [6.0]]), out.numpy()\n            )\n        )\n'"
elasticdl_preprocessing/tests/round_identity_test.py,3,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.round_identity import RoundIdentity\nfrom elasticdl_preprocessing.tests.test_utils import (\n    ragged_tensor_equal,\n    sparse_tensor_equal,\n)\n\n\nclass RoundIdentityTest(unittest.TestCase):\n    def test_round_indentity(self):\n        round_identity = RoundIdentity(num_buckets=10)\n\n        dense_input = tf.constant([[1.2], [1.6], [0.2], [3.1], [4.9]])\n        output = round_identity(dense_input)\n        expected_out = np.array([[1], [2], [0], [3], [5]])\n        self.assertTrue(np.array_equal(output.numpy(), expected_out))\n\n        ragged_input = tf.ragged.constant([[1.1, 3.4], [0.5]])\n        ragged_output = round_identity(ragged_input)\n        expected_ragged_out = tf.ragged.constant([[1, 3], [0]], dtype=tf.int64)\n        self.assertTrue(\n            ragged_tensor_equal(ragged_output, expected_ragged_out)\n        )\n\n        sparse_input = ragged_input.to_sparse()\n        sparse_output = round_identity(sparse_input)\n        expected_sparse_out = expected_ragged_out.to_sparse()\n        self.assertTrue(\n            sparse_tensor_equal(sparse_output, expected_sparse_out)\n        )\n'"
elasticdl_preprocessing/tests/test_utils.py,4,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef sparse_tensor_equal(sp_a, sp_b):\n    if not isinstance(sp_a, tf.SparseTensor) or not isinstance(\n        sp_b, tf.SparseTensor\n    ):\n        return False\n\n    if not np.array_equal(sp_a.dense_shape.numpy(), sp_b.dense_shape.numpy()):\n        return False\n\n    if not np.array_equal(sp_a.indices.numpy(), sp_b.indices.numpy()):\n        return False\n\n    if sp_a.values.dtype != sp_b.values.dtype:\n        return False\n\n    if not np.array_equal(sp_a.values.numpy(), sp_b.values.numpy()):\n        return False\n\n    return True\n\n\ndef ragged_tensor_equal(rt_a, rt_b):\n    if not isinstance(rt_a, tf.RaggedTensor) or not isinstance(\n        rt_b, tf.RaggedTensor\n    ):\n        return False\n\n    if rt_a.shape.as_list() != rt_b.shape.as_list():\n        return False\n\n    if rt_a.dtype != rt_b.dtype:\n        return False\n\n    if rt_a.to_list() != rt_b.to_list():\n        return False\n\n    return True\n'"
elasticdl_preprocessing/tests/to_number_test.py,19,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.to_number import ToNumber\nfrom elasticdl_preprocessing.tests.test_utils import sparse_tensor_equal\n\n\nclass ToNumberTest(unittest.TestCase):\n    def test_call_dense(self):\n        layer = ToNumber(out_type=tf.int32, default_value=-1)\n        input = tf.constant([[""123"", """"], [""456"", ""-789""]], tf.string)\n        output = layer.call(input)\n        expected_output = tf.constant([[123, -1], [456, -789]], tf.int32)\n        self.assertEqual(output.dtype, tf.int32)\n        self.assertTrue(\n            np.array_equal(output.numpy(), expected_output.numpy())\n        )\n\n        layer = ToNumber(out_type=tf.float32, default_value=0.0)\n        input = tf.constant([[""123.1"", """"], [""456"", ""-789.987""]], tf.string)\n        output = layer.call(input)\n        expected_output = tf.constant(\n            [[123.1, 0.0], [456.0, -789.987]], tf.float32\n        )\n        self.assertEqual(output.dtype, tf.float32)\n        self.assertTrue(\n            np.array_equal(output.numpy(), expected_output.numpy())\n        )\n\n    def test_call_sparse(self):\n        layer = ToNumber(out_type=tf.int32, default_value=-1)\n        input = tf.SparseTensor(\n            indices=[[0, 2], [2, 1], [2, 3], [5, 4]],\n            values=tf.constant([""123"", """", ""456"", ""-789""], tf.string),\n            dense_shape=[6, 5],\n        )\n        output = layer.call(input)\n        expected_output = tf.SparseTensor(\n            indices=[[0, 2], [2, 1], [2, 3], [5, 4]],\n            values=tf.constant([123, -1, 456, -789], tf.int32),\n            dense_shape=[6, 5],\n        )\n        self.assertTrue(sparse_tensor_equal(output, expected_output))\n\n        layer = ToNumber(out_type=tf.float32, default_value=0.0)\n        input = tf.SparseTensor(\n            indices=[[0, 2], [2, 1], [2, 3], [5, 4]],\n            values=tf.constant([""123.1"", """", ""456"", ""-789.987""], tf.string),\n            dense_shape=[6, 5],\n        )\n        output = layer.call(input)\n        expected_output = tf.SparseTensor(\n            indices=[[0, 2], [2, 1], [2, 3], [5, 4]],\n            values=tf.constant([123.1, 0.0, 456.0, -789.987], tf.float32),\n            dense_shape=[6, 5],\n        )\n        self.assertTrue(sparse_tensor_equal(output, expected_output))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl_preprocessing/tests/to_ragged_test.py,8,"b'import unittest\n\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.to_ragged import ToRagged\nfrom elasticdl_preprocessing.tests.test_utils import ragged_tensor_equal\n\n\nclass ToRaggedTest(unittest.TestCase):\n    def test_dense_to_ragged(self):\n        layer = ToRagged()\n        input_data = tf.constant([[1], [-1], [4]], tf.int64)\n        out = layer(input_data)\n        expected_out = tf.ragged.constant([[1], [], [4]], tf.int64)\n        self.assertTrue(ragged_tensor_equal(out, expected_out))\n\n    def test_string_split_to_ragged(self):\n        layer = ToRagged()\n        input_data = tf.ragged.constant([[""1"", ""2"", ""3""], [""4"", ""5""], [""""]])\n        out = layer(input_data)\n        expected_out = tf.ragged.constant([[""1"", ""2"", ""3""], [""4"", ""5""], []])\n        self.assertTrue(ragged_tensor_equal(out, expected_out))\n\n    def test_model_with_ragged(self):\n        inputs = tf.keras.Input(shape=(1,), dtype=tf.int32)\n        ragged = ToRagged(ignore_value=-1)(inputs)\n        sum_out = tf.reduce_sum(ragged)\n        model = tf.keras.Model(inputs=inputs, outputs=sum_out)\n        sum_value = model.call(tf.constant([[1], [-1], [4]])).numpy()\n        self.assertEqual(sum_value, 5.0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl_preprocessing/tests/to_sparse_test.py,10,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl_preprocessing.layers.to_sparse import ToSparse\nfrom elasticdl_preprocessing.tests.test_utils import sparse_tensor_equal\n\n\nclass ToSparseTest(unittest.TestCase):\n    def test_to_sparse(self):\n        layer = ToSparse()\n        inp = tf.constant([[""A"", """"], [""B"", ""C""]], tf.string)\n        output = layer.call(inp)\n        expected_out = tf.SparseTensor(\n            indices=np.array([[0, 0], [1, 0], [1, 1]]),\n            values=np.array([""A"", ""B"", ""C""]),\n            dense_shape=(2, 2),\n        )\n        self.assertTrue(sparse_tensor_equal(output, expected_out))\n\n        layer = ToSparse()\n        inp = tf.constant([[12, -1], [45, 78]], tf.int64)\n        output = layer.call(inp)\n        expected_out = tf.SparseTensor(\n            indices=np.array([[0, 0], [1, 0], [1, 1]]),\n            values=np.array([12, 45, 78]),\n            dense_shape=(2, 2),\n        )\n        self.assertTrue(sparse_tensor_equal(output, expected_out))\n\n    def test_model_with_to_sparse(self):\n        inputs = tf.keras.Input(shape=(1,), dtype=tf.int32)\n        sparse_inputs = ToSparse(ignore_value=-1)(inputs)\n        model = tf.keras.Model(inputs=inputs, outputs=sparse_inputs)\n        out = model.call(tf.constant([[1], [-1], [2], [3]]))\n\n        expect_out = tf.SparseTensor(\n            indices=tf.constant([[0, 0], [2, 0], [3, 0]], dtype=tf.int64),\n            values=tf.constant([1, 2, 3], dtype=tf.int32),\n            dense_shape=(4, 1),\n        )\n        self.assertTrue(sparse_tensor_equal(out, expect_out))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl_preprocessing/utils/__init__.py,0,b''
elasticdl_preprocessing/utils/analyzer_utils.py,0,"b'""""""This util gets statistics from the environment and return a default value\nif there is no statistics in the environment. So user can set a default value\nto perform unit tests.\n""""""\nimport os\n\nfrom elasticdl_preprocessing.constants import AnalysisEnvTemplate\n\n\ndef get_min(feature_name, default_value):\n    """"""Get the min value of numeric feature from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: Float.\n\n    Return:\n        Float\n    """"""\n    env_name = AnalysisEnvTemplate.MIN_ENV.format(feature_name)\n    min_value = os.getenv(env_name, None)\n    if min_value is None:\n        return default_value\n    else:\n        return float(min_value)\n\n\ndef get_max(feature_name, default_value):\n    """"""Get the max value of numeric feature from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: Float.\n\n    Return:\n        Float\n    """"""\n    env_name = AnalysisEnvTemplate.MAX_ENV.format(feature_name)\n    max_value = os.getenv(env_name, None)\n    if max_value is None:\n        return default_value\n    else:\n        return float(max_value)\n\n\ndef get_avg(feature_name, default_value):\n    """"""Get the average of numeric feature from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: Float.\n\n    Return:\n        Float\n    """"""\n    env_name = AnalysisEnvTemplate.AVG_ENV.format(feature_name)\n    mean = os.getenv(env_name, None)\n    if mean is None:\n        return default_value\n    else:\n        return float(mean)\n\n\ndef get_stddev(feature_name, default_value):\n    """"""Get the standard deviation from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: Float.\n\n    Return:\n        Float.\n    """"""\n    env_name = AnalysisEnvTemplate.STDDEV_ENV.format(feature_name)\n    std_dev = os.getenv(env_name, None)\n    if std_dev is None:\n        return default_value\n    else:\n        return float(std_dev)\n\n\ndef get_bucket_boundaries(feature_name, default_value):\n    """"""Get the bucket boundaries from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: List with float values.\n\n    Return:\n        List with float values.\n    """"""\n    env_name = AnalysisEnvTemplate.BUCKET_BOUNDARIES_ENV.format(feature_name)\n    boundaries = os.getenv(env_name, None)\n    if boundaries is None:\n        return default_value\n    else:\n        boundaries = list(map(float, boundaries.split("","")))\n        return sorted(set(boundaries))\n\n\ndef get_distinct_count(feature_name, default_value):\n    """"""Get the count of distinct feature values set from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: Integer.\n\n    Return:\n        Integer.\n    """"""\n    env_name = AnalysisEnvTemplate.DISTINCT_COUNT_ENV.format(feature_name)\n    count = os.getenv(env_name, None)\n    if count is None:\n        return default_value\n    else:\n        return int(count)\n\n\ndef get_vocabulary(feature_name, default_value):\n    """"""Get the feature vocabulary from the environment.\n    Return the default value if there is no the statistics in\n    the environment.\n\n    Args:\n        feature_name: String, feature name or column name in a table\n        default_value: List with strings or a path of vocabulary files.\n\n    Return:\n        List with strings.\n    """"""\n    env_name = AnalysisEnvTemplate.VOCABULARY_ENV.format(feature_name)\n    vocabulary_path = os.getenv(env_name, None)\n    if vocabulary_path is None:\n        return default_value\n    else:\n        return vocabulary_path\n'"
elasticdl_preprocessing/utils/decorators.py,1,"b'""""""\nThe decorator to declare the input tensors of a keras model.\nFor keras subclass model, it has a core method `def call(self, inputs)`.\nBut we don\'t know how many tensors the model accepts just from `inputs`.\nTo solve this, we can use this decorators just as follows:\n\nExample:\n@declare_model_inputs(""wide_embeddings,"", ""deep_embeddings"")\nclass WideAndDeepClassifier(tf.keras.Model):\n    def __init__(self):\n        pass\n\n    def call(self, inputs):\n        pass\n\nAnd then we can get the input tensor names from the property of the model class\n`WideAndDeepClassifier._model_inputs` => [""wide_embeddings"", ""deep_embeddings""]\n""""""\n\n\ndef declare_model_inputs(*args):\n    def decorator(clz):\n        input_names = list(args)\n        if not input_names:\n            raise ValueError(""Model input names should not be empty."")\n\n        if not all(isinstance(name, str) for name in input_names):\n            raise ValueError(""Model input names should be string type."")\n\n        setattr(clz, ""_model_inputs"", input_names)\n\n        return clz\n\n    return decorator\n'"
model_zoo/census_dnn_model/census_feature_columns.py,6,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.feature_column import feature_column\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\ndef get_feature_columns():\n    feature_columns = []\n\n    for numeric_feature_key in NUMERIC_FEATURE_KEYS:\n        numeric_feature = tf.feature_column.numeric_column(numeric_feature_key)\n        feature_columns.append(numeric_feature)\n\n    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:\n        embedding_feature = feature_column.embedding_column(\n            tf.feature_column.categorical_column_with_hash_bucket(\n                categorical_feature_key, hash_bucket_size=64\n            ),\n            dimension=16,\n        )\n        feature_columns.append(embedding_feature)\n\n    return feature_columns\n\n\ndef get_feature_input_layers():\n    feature_input_layers = {}\n\n    for numeric_feature_key in NUMERIC_FEATURE_KEYS:\n        feature_input_layers[numeric_feature_key] = tf.keras.Input(\n            shape=(1,), name=numeric_feature_key, dtype=tf.float32\n        )\n\n    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:\n        feature_input_layers[categorical_feature_key] = tf.keras.Input(\n            shape=(1,), name=categorical_feature_key, dtype=tf.string\n        )\n\n    return feature_input_layers\n'"
model_zoo/census_dnn_model/census_functional_api.py,14,"b'import tensorflow as tf\nfrom tensorflow.python.keras.metrics import accuracy\n\nfrom model_zoo.census_dnn_model.census_feature_columns import (\n    get_feature_columns,\n    get_feature_input_layers,\n)\n\n\ndef custom_model_def(feature_columns, feature_input_layers):\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    x = feature_layer(feature_input_layers)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    y = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)\n\n    model = tf.keras.Model(inputs=feature_input_layers, outputs=y)\n\n    return model\n\n\ndef custom_model():\n    feature_columns = get_feature_columns()\n    feature_input_layers = get_feature_input_layers()\n\n    return custom_model_def(\n        feature_columns=feature_columns,\n        feature_input_layers=feature_input_layers,\n    )\n\n\ndef loss(labels, predictions):\n    labels = tf.expand_dims(labels, axis=1)\n    return tf.keras.losses.binary_crossentropy(labels, predictions)\n\n\ndef optimizer():\n    return tf.keras.optimizers.Adam()\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: accuracy(\n            tf.cast(tf.squeeze(tf.round(predictions)), tf.int32),\n            tf.cast(labels, tf.int32),\n        )\n    }\n\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\n# TODO: The dataset_fn and the column names above is bound with\n# the input data source. We can consider move it out of the\n# model definition file. Currently ElasticDL framework has the\n# limitation that the dataset_fn is in the same file with model def.\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature([], tf.string))\n                for name in CATEGORICAL_FEATURE_KEYS\n            ]\n            + [\n                (name, tf.io.FixedLenFeature([], tf.float32))\n                for name in NUMERIC_FEATURE_KEYS\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n'"
model_zoo/census_dnn_model/census_sequential.py,14,"b'import tensorflow as tf\nfrom tensorflow.python.keras.metrics import accuracy\n\nfrom model_zoo.census_dnn_model.census_feature_columns import (\n    get_feature_columns,\n)\n\n\ndef custom_model_def(feature_columns):\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.DenseFeatures(feature_columns=feature_columns),\n            tf.keras.layers.Dense(16, activation=""relu""),\n            tf.keras.layers.Dense(16, activation=""relu""),\n            tf.keras.layers.Dense(1, activation=""sigmoid""),\n        ]\n    )\n\n    return model\n\n\ndef custom_model():\n    feature_columns = get_feature_columns()\n    return custom_model_def(feature_columns=feature_columns)\n\n\ndef loss(labels, predictions):\n    labels = tf.expand_dims(labels, axis=1)\n    return tf.keras.losses.binary_crossentropy(labels, predictions)\n\n\ndef optimizer():\n    return tf.keras.optimizers.Adam()\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: accuracy(\n            tf.cast(tf.squeeze(tf.round(predictions)), tf.int32),\n            tf.cast(labels, tf.int32),\n        )\n    }\n\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature([], tf.string))\n                for name in CATEGORICAL_FEATURE_KEYS\n            ]\n            + [\n                (name, tf.io.FixedLenFeature([], tf.float32))\n                for name in NUMERIC_FEATURE_KEYS\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n'"
model_zoo/census_dnn_model/census_subclass.py,14,"b'import tensorflow as tf\nfrom tensorflow.python.keras.metrics import accuracy\n\nfrom model_zoo.census_dnn_model.census_feature_columns import (\n    get_feature_columns,\n)\n\n\nclass CustomModel(tf.keras.Model):\n    def __init__(self, feature_columns):\n        super(CustomModel, self).__init__(name=""custom_model"")\n        self.dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n        self.dense_1 = tf.keras.layers.Dense(16, activation=""relu"")\n        self.dense_2 = tf.keras.layers.Dense(16, activation=""relu"")\n        self.dense_3 = tf.keras.layers.Dense(1, activation=""sigmoid"")\n\n    def call(self, inputs, training=False):\n        x = self.dense_features(inputs)\n        x = self.dense_1(x)\n        x = self.dense_2(x)\n        x = self.dense_3(x)\n\n        return x\n\n\ndef custom_model():\n    feature_columns = get_feature_columns()\n    return CustomModel(feature_columns=feature_columns)\n\n\ndef loss(labels, predictions):\n    labels = tf.expand_dims(labels, axis=1)\n    return tf.keras.losses.binary_crossentropy(labels, predictions)\n\n\ndef optimizer():\n    return tf.keras.optimizers.Adam()\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: accuracy(\n            tf.cast(tf.squeeze(tf.round(predictions)), tf.int32),\n            tf.cast(labels, tf.int32),\n        )\n    }\n\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature([], tf.string))\n                for name in CATEGORICAL_FEATURE_KEYS\n            ]\n            + [\n                (name, tf.io.FixedLenFeature([], tf.float32))\n                for name in NUMERIC_FEATURE_KEYS\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n'"
model_zoo/census_model_sqlflow/__init__.py,0,b''
model_zoo/census_wide_deep_model/feature_config.py,12,"b'import tensorflow as tf\n\nfrom model_zoo.census_wide_deep_model.feature_info_util import (\n    FeatureInfo,\n    TransformOp,\n)\n\nWORK_CLASS_VOCABULARY = [\n    ""Private"",\n    ""Self-emp-not-inc"",\n    ""Self-emp-inc"",\n    ""Federal-gov"",\n    ""Local-gov"",\n    ""State-gov"",\n    ""Without-pay"",\n    ""Never-worked"",\n]\n\nMARITAL_STATUS_VOCABULARY = [\n    ""Married-civ-spouse"",\n    ""Divorced"",\n    ""Never-married"",\n    ""Separated"",\n    ""Widowed"",\n    ""Married-spouse-absent"",\n    ""Married-AF-spouse"",\n]\n\nRELATION_SHIP_VOCABULARY = [\n    ""Wife"",\n    ""Own-child"",\n    ""Husband"",\n    ""Not-in-family"",\n    ""Other-relative"",\n    ""Unmarried"",\n]\n\nRACE_VOCABULARY = [\n    ""White"",\n    ""Asian-Pac-Islander"",\n    ""Amer-Indian-Eskimo"",\n    ""Other"",\n    ""Black"",\n]\n\nSEX_VOCABULARY = [""Female"", ""Male""]\n\nAGE_BOUNDARIES = [0, 20, 40, 60, 80]\nCAPITAL_GAIN_BOUNDARIES = [6000, 6500, 7000, 7500, 8000]\nCAPITAL_LOSS_BOUNDARIES = [2000, 2500, 3000, 3500, 4000]\nHOURS_BOUNDARIES = [10, 20, 30, 40, 50, 60]\n\neducation = FeatureInfo(""education"", TransformOp.HASH, tf.string, 30)\noccupation = FeatureInfo(""occupation"", TransformOp.HASH, tf.string, 30)\nnative_country = FeatureInfo(\n    ""native-country"", TransformOp.HASH, tf.string, 100\n)\n\nworkclass = FeatureInfo(\n    ""workclass"", TransformOp.LOOKUP, tf.string, WORK_CLASS_VOCABULARY\n)\nmarital_status = FeatureInfo(\n    ""marital-status"", TransformOp.LOOKUP, tf.string, MARITAL_STATUS_VOCABULARY\n)\nrelationship = FeatureInfo(\n    ""relationship"", TransformOp.LOOKUP, tf.string, RELATION_SHIP_VOCABULARY\n)\nrace = FeatureInfo(""race"", TransformOp.LOOKUP, tf.string, RACE_VOCABULARY)\nsex = FeatureInfo(""sex"", TransformOp.LOOKUP, tf.string, SEX_VOCABULARY)\n\nage = FeatureInfo(""age"", TransformOp.BUCKETIZE, tf.float32, AGE_BOUNDARIES)\ncapital_gain = FeatureInfo(\n    ""capital-gain"", TransformOp.BUCKETIZE, tf.float32, CAPITAL_GAIN_BOUNDARIES\n)\ncapital_loss = FeatureInfo(\n    ""capital-loss"", TransformOp.BUCKETIZE, tf.float32, CAPITAL_LOSS_BOUNDARIES\n)\nhours_per_week = FeatureInfo(\n    ""hours-per-week"", TransformOp.BUCKETIZE, tf.float32, HOURS_BOUNDARIES\n)\n\nFEATURE_GROUPS = {\n    ""group1"": [workclass, hours_per_week, capital_gain, capital_loss],\n    ""group2"": [education, marital_status, relationship, occupation],\n    ""group3"": [age, sex, race, native_country],\n}\n\nMODEL_INPUTS = {\n    ""wide"": [""group1"", ""group2""],\n    ""deep"": [""group1"", ""group2"", ""group3""],\n}\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n'"
model_zoo/census_wide_deep_model/feature_info_util.py,0,"b'from collections import namedtuple\n\nFeatureInfo = namedtuple(""FeatureInfo"", [""name"", ""op_name"", ""dtype"", ""param""])\n\n\nclass TransformOp(object):\n    HASH = ""HASH""\n    BUCKETIZE = ""BUCKETIZE""\n    LOOKUP = ""LOOKUP""\n\n\ndef get_id_boundaries(feature_group):\n    boundaries = [0]\n    for feature_info in feature_group:\n        boundaries.append(boundaries[-1] + get_max_id(feature_info))\n    return boundaries\n\n\ndef get_max_id(feature_info):\n    if feature_info.op_name == TransformOp.LOOKUP:\n        return len(feature_info.param) + 1\n    elif feature_info.op_name == TransformOp.HASH:\n        return feature_info.param\n    elif feature_info.op_name == TransformOp.BUCKETIZE:\n        return len(feature_info.param) + 1\n    else:\n        raise ValueError(""The op is not supported"")\n'"
model_zoo/census_wide_deep_model/keras_process_layer.py,13,"b'import tensorflow as tf\nfrom tensorflow.python.ops import lookup_ops, math_ops\n\n\nclass CategoryHash(tf.keras.layers.Layer):\n    """"""\n    Todo replace it with a preprocess layer after TF 2.2\n    https://github.com/tensorflow/community/pull/188/files?short_path=0657914#diff-0657914a8dc40e5fbca67680bf3fc45f\n    """"""\n\n    def __init__(self, bucket_size):\n        super(CategoryHash, self).__init__()\n        self.bucket_size = bucket_size\n\n    def call(self, inputs):\n        if inputs.dtype is not tf.string:\n            inputs = tf.strings.as_string(inputs)\n        bucket_id = tf.strings.to_hash_bucket_fast(inputs, self.bucket_size)\n        return tf.cast(bucket_id, tf.int64)\n\n\nclass NumericBucket(tf.keras.layers.Layer):\n    def __init__(self, boundaries):\n        super(NumericBucket, self).__init__()\n        self.boundaries = boundaries\n\n    def call(self, inputs):\n        if inputs.dtype is tf.string:\n            inputs = tf.strings.to_number(inputs, out_type=tf.float32)\n        else:\n            inputs = tf.cast(inputs, tf.float32)\n        bucket_id = math_ops._bucketize(inputs, boundaries=self.boundaries)\n        return tf.cast(bucket_id, tf.int64)\n\n\nclass CategoryLookup(tf.keras.layers.Layer):\n    """"""\n    Todo replace it with a preprocess layer after TF 2.2\n    https://github.com/tensorflow/community/pull/188/files?short_path=0657914#diff-0657914a8dc40e5fbca67680bf3fc45f\n    """"""\n\n    def __init__(self, vocabulary_list, num_oov_buckets=1, default_value=-1):\n        super(CategoryLookup, self).__init__()\n        self.vocabulary_list = vocabulary_list\n\n    def call(self, inputs):\n        table = lookup_ops.index_table_from_tensor(\n            vocabulary_list=self.vocabulary_list,\n            num_oov_buckets=1,\n            default_value=-1,\n        )\n        return tf.cast(table.lookup(inputs), tf.int64)\n\n\nclass AddIdOffset(tf.keras.layers.Layer):\n    def __init__(self, offsets):\n        super(AddIdOffset, self).__init__()\n        self.offsets = offsets\n\n    def call(self, inputs):\n        ids_with_offset = []\n        if len(self.offsets) != len(inputs):\n            raise ValueError(\n                ""The number of elements in offsets is not equal to inputs""\n            )\n        for i, value in enumerate(inputs):\n            ids_with_offset.append(value + self.offsets[i])\n        return ids_with_offset\n'"
model_zoo/census_wide_deep_model/wide_deep_functional_api.py,25,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom model_zoo.census_wide_deep_model.feature_config import (\n    CATEGORICAL_FEATURE_KEYS,\n    FEATURE_GROUPS,\n    LABEL_KEY,\n    MODEL_INPUTS,\n    NUMERIC_FEATURE_KEYS,\n    TransformOp,\n)\nfrom model_zoo.census_wide_deep_model.feature_info_util import (\n    get_id_boundaries,\n)\nfrom model_zoo.census_wide_deep_model.keras_process_layer import (\n    AddIdOffset,\n    CategoryHash,\n    CategoryLookup,\n    NumericBucket,\n)\n\n\ndef custom_model():\n    # The codes in the method can all be auto-generated\n    input_layers = get_input_layers(FEATURE_GROUPS)\n    transform_results = transform(input_layers, FEATURE_GROUPS)\n\n    params = {}\n    params[""id_group_dims""] = get_id_group_dims(FEATURE_GROUPS)\n    params[""embedding_config""] = MODEL_INPUTS\n    model = wide_deep_model(input_layers, transform_results, params)\n    return model\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.0003\n        elif model_version < 12000:\n            return 0.0002\n        else:\n            return 0.0001\n\n    return [LearningRateScheduler(_schedule)]\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature((1,), tf.string))\n                for name in CATEGORICAL_FEATURE_KEYS\n            ]\n            + [\n                (name, tf.io.FixedLenFeature((1,), tf.float32))\n                for name in NUMERIC_FEATURE_KEYS\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n\n\ndef get_input_layers(feature_groups):\n    input_layers = {}\n    for feature_group in feature_groups.values():\n        for feature_info in feature_group:\n            input_layers[feature_info.name] = tf.keras.layers.Input(\n                name=feature_info.name, shape=(1,), dtype=feature_info.dtype\n            )\n    return input_layers\n\n\ndef transform(inputs, feature_groups):\n    result = {}\n    for group_name, feature_group in feature_groups.items():\n        result[group_name] = transform_group(inputs, feature_group)\n    return result\n\n\ndef transform_group(inputs, feature_group):\n    """"""Transform the inputs and concatenate inputs in a group\n    to a dense tensor\n    """"""\n    group_items = []\n    for feature_info in feature_group:\n        layer = get_transform_layer(feature_info)\n        transform_output = layer(inputs[feature_info.name])\n        group_items.append(transform_output)\n\n    id_offsets = get_id_boundaries(feature_group)\n\n    if id_offsets is not None:\n        group_items = AddIdOffset(id_offsets[0:-1])(group_items)\n    group_stack = tf.keras.layers.concatenate(group_items, axis=-1)\n    return group_stack\n\n\ndef get_transform_layer(feature_info):\n    if feature_info.op_name == TransformOp.LOOKUP:\n        return CategoryLookup(feature_info.param)\n    elif feature_info.op_name == TransformOp.HASH:\n        return CategoryHash(feature_info.param)\n    elif feature_info.op_name == TransformOp.BUCKETIZE:\n        return NumericBucket(feature_info.param)\n    else:\n        raise ValueError(""The op is not supported"")\n\n\ndef get_id_group_dims(feature_groups):\n    id_group_dims = {}\n    for group_name, group_features in feature_groups.items():\n        id_boundaries = get_id_boundaries(group_features)\n        id_group_dims[group_name] = id_boundaries[-1]\n    return id_group_dims\n\n\ndef wide_deep_model(input_layers, input_tensors, params):\n    id_group_dims = params.get(""id_group_dims"", {})\n    embedding_config = params.get(""embedding_config"", {})\n    wide_embedding_groups = embedding_config[""wide""]\n    deep_embedding_groups = embedding_config[""deep""]\n    # wide part\n    wide_embeddings = []\n    for group_name in wide_embedding_groups:\n        wide_embedding_layer = tf.keras.layers.Embedding(\n            id_group_dims[group_name], 1\n        )\n        embedding = wide_embedding_layer(input_tensors[group_name])\n        embedding_sum = tf.keras.backend.sum(embedding, axis=1)\n        wide_embeddings.append(embedding_sum)\n\n    # deep part\n    deep_embeddings = []\n    for group_name in deep_embedding_groups:\n        deep_embedding_layer = tf.keras.layers.Embedding(\n            id_group_dims[group_name], 8\n        )\n        embedding = deep_embedding_layer(input_tensors[group_name])\n        embedding_sum = tf.keras.backend.sum(embedding, axis=1)\n        deep_embeddings.append(embedding_sum)\n\n    logits, probs = wide_deep(wide_embeddings, deep_embeddings)\n\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n\n\ndef wide_deep(wide_embeddings, deep_embeddings):\n    # Wide Part\n    wide = tf.keras.layers.Concatenate()(wide_embeddings)  # shape = (None, 3)\n\n    # Deep Part\n    dnn_input = tf.reshape(deep_embeddings, shape=(-1, 3 * 8))\n    for i in [16, 8, 4]:\n        dnn_input = tf.keras.layers.Dense(i)(dnn_input)\n\n    # Output Part\n    concat_input = tf.concat([wide, dnn_input], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n    return logits, probs\n'"
model_zoo/cifar10_functional_api/__init__.py,0,b''
model_zoo/cifar10_functional_api/cifar10_functional_api.py,41,"b'import os\n\nimport tensorflow as tf\n\nfrom elasticdl.python.common.constants import MaxComputeConfig, Mode\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.data.odps_io import ODPSWriter, is_odps_configured\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom elasticdl.python.worker.prediction_outputs_processor import (\n    BasePredictionOutputsProcessor,\n)\n\n\ndef custom_model():\n    inputs = tf.keras.layers.Input(shape=(32, 32, 3), name=""image"")\n    use_bias = True\n\n    conv = tf.keras.layers.Conv2D(\n        32,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(inputs)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    conv = tf.keras.layers.Conv2D(\n        32,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(activation)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    max_pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(activation)\n    dropout = tf.keras.layers.Dropout(0.2)(max_pool)\n\n    conv = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(dropout)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    conv = tf.keras.layers.Conv2D(\n        64,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(activation)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    max_pool = tf.keras.layers.MaxPooling2D()(activation)\n    dropout = tf.keras.layers.Dropout(0.3)(max_pool)\n\n    conv = tf.keras.layers.Conv2D(\n        128,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(dropout)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    conv = tf.keras.layers.Conv2D(\n        128,\n        kernel_size=(3, 3),\n        padding=""same"",\n        use_bias=use_bias,\n        activation=None,\n    )(activation)\n    bn = tf.keras.layers.BatchNormalization(\n        epsilon=1e-06, axis=-1, momentum=0.9\n    )(conv)\n    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)\n\n    max_pool = tf.keras.layers.MaxPooling2D()(activation)\n    dropout = tf.keras.layers.Dropout(0.4)(max_pool)\n\n    flatten = tf.keras.layers.Flatten()(dropout)\n    outputs = tf.keras.layers.Dense(10, name=""output"")(flatten)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=""cifar10_model"")\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=predictions, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.1\n        elif model_version < 15000:\n            return 0.01\n        else:\n            return 0.001\n\n    LearningRateScheduler(_schedule)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([32, 32, 3], tf.float32)\n            }\n        else:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([32, 32, 3], tf.float32),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {\n            ""image"": tf.math.divide(tf.cast(r[""image""], tf.float32), 255.0)\n        }\n        if mode == Mode.PREDICTION:\n            return features\n        else:\n            return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n\n\nclass PredictionOutputsProcessor(BasePredictionOutputsProcessor):\n    def __init__(self):\n        if is_odps_configured():\n            self.odps_writer = ODPSWriter(\n                os.environ[MaxComputeConfig.PROJECT_NAME],\n                os.environ[MaxComputeConfig.ACCESS_ID],\n                os.environ[MaxComputeConfig.ACCESS_KEY],\n                os.environ[MaxComputeConfig.ENDPOINT],\n                ""cifar10_prediction_outputs"",\n                # TODO: Print out helpful error message if the columns and\n                # column_types do not match with the prediction outputs\n                columns=[""f"" + str(i) for i in range(10)],\n                column_types=[""double"" for _ in range(10)],\n            )\n        else:\n            self.odps_writer = None\n\n    def process(self, predictions, worker_id):\n        if self.odps_writer:\n            self.odps_writer.from_iterator(\n                iter(predictions.numpy().tolist()), worker_id\n            )\n        else:\n            logger.info(predictions.numpy())\n'"
model_zoo/cifar10_subclass/__init__.py,0,b''
model_zoo/cifar10_subclass/cifar10_subclass.py,40,"b'import tensorflow as tf\n\nfrom elasticdl.python.common.constants import Mode\n\n\nclass CustomModel(tf.keras.Model):\n    def __init__(self, channel_last=True):\n        super(CustomModel, self).__init__(name=""cifar10_model"")\n\n        use_bias = True\n        self._conv_1 = tf.keras.layers.Conv2D(\n            32,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_1 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_1 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._conv_2 = tf.keras.layers.Conv2D(\n            32,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_2 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_2 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n        self._dropout_1 = tf.keras.layers.Dropout(0.2)\n\n        self._conv_3 = tf.keras.layers.Conv2D(\n            64,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_3 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_3 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._conv_4 = tf.keras.layers.Conv2D(\n            64,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_4 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_4 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n        self._dropout_2 = tf.keras.layers.Dropout(0.3)\n\n        self._conv_5 = tf.keras.layers.Conv2D(\n            128,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_5 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_5 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._conv_6 = tf.keras.layers.Conv2D(\n            128,\n            kernel_size=(3, 3),\n            padding=""same"",\n            use_bias=use_bias,\n            activation=None,\n        )\n        self._bn_6 = tf.keras.layers.BatchNormalization(\n            epsilon=1e-06, axis=-1, momentum=0.9\n        )\n        self._relu_6 = tf.keras.layers.Activation(tf.nn.relu)\n\n        self._max_pool_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n        self._dropout_3 = tf.keras.layers.Dropout(0.4)\n\n        self._flatten_1 = tf.keras.layers.Flatten()\n        self._dense_1 = tf.keras.layers.Dense(10, name=""output"")\n\n    def call(self, inputs, training=False):\n        x = self._conv_1(inputs[""image""])\n        x = self._bn_1(x)\n        x = self._relu_1(x)\n        x = self._conv_2(x)\n        x = self._bn_2(x)\n        x = self._relu_2(x)\n        x = self._max_pool_1(x)\n        x = self._dropout_1(x)\n        x = self._conv_3(x)\n        x = self._bn_3(x)\n        x = self._relu_3(x)\n        x = self._conv_4(x)\n        x = self._bn_4(x)\n        x = self._relu_4(x)\n        x = self._max_pool_2(x)\n        x = self._dropout_2(x)\n        x = self._conv_5(x)\n        x = self._bn_5(x)\n        x = self._relu_5(x)\n        x = self._conv_6(x)\n        x = self._bn_6(x)\n        x = self._relu_6(x)\n        x = self._max_pool_3(x)\n        x = self._dropout_3(x)\n        x = self._flatten_1(x)\n        return self._dense_1(x)\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=predictions, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([32, 32, 3], tf.float32)\n            }\n        else:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([32, 32, 3], tf.float32),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {\n            ""image"": tf.math.divide(tf.cast(r[""image""], tf.float32), 255.0)\n        }\n        if mode == Mode.PREDICTION:\n            return features\n        else:\n            return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n'"
model_zoo/dac_ctr/__init__.py,0,b''
model_zoo/dac_ctr/convert_to_recordio.py,7,"b'import argparse\nimport os\nimport pathlib\nimport sys\n\nimport recordio\nimport tensorflow as tf\n\nDAC_COLUMNS = [\n    ""label"",\n    ""I1"",\n    ""I2"",\n    ""I3"",\n    ""I4"",\n    ""I5"",\n    ""I6"",\n    ""I7"",\n    ""I8"",\n    ""I9"",\n    ""I10"",\n    ""I11"",\n    ""I12"",\n    ""I13"",\n    ""C1"",\n    ""C2"",\n    ""C3"",\n    ""C4"",\n    ""C5"",\n    ""C6"",\n    ""C7"",\n    ""C8"",\n    ""C9"",\n    ""C10"",\n    ""C11"",\n    ""C12"",\n    ""C13"",\n    ""C14"",\n    ""C15"",\n    ""C16"",\n    ""C17"",\n    ""C18"",\n    ""C19"",\n    ""C20"",\n    ""C21"",\n    ""C22"",\n    ""C23"",\n    ""C24"",\n    ""C25"",\n    ""C26"",\n]\n\nDAC_DTYPES = [""int64""] * 14 + [""str""] * 26\n\n\ndef convert_series_to_tf_feature(data_series, columns, dtype_series):\n    """"""\n    Convert pandas series to TensorFlow features.\n    Args:\n        data_series: Pandas series of data content.\n        columns: Column name array.\n        dtype_series: Pandas series of dtypes.\n    Return:\n        A dict of feature name -> tf.train.Feature\n    """"""\n    features = {}\n    data_series = data_series.split(""\\t"")\n    for i, column_name in enumerate(columns):\n        feature = None\n        value = data_series[i]\n        value = value.strip()\n        dtype = dtype_series[i]\n\n        if dtype == ""int64"":\n            if value == """":\n                value = 0\n            else:\n                value = int(value)\n            feature = tf.train.Feature(\n                int64_list=tf.train.Int64List(value=[value])\n            )\n        elif dtype == ""str"":\n            feature = tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[value.encode(""utf-8"")])\n            )\n        else:\n            assert False, ""Unrecoginize dtype: {}"".format(dtype)\n\n        features[column_name] = feature\n\n    return features\n\n\ndef convert_to_recordio_files(file_path, dir_name, records_per_shard):\n    """"""\n    Convert a pandas DataFrame to recordio files.\n    Args:\n        file_path: A path of the data file\n        dir_name: A directory to put the generated recordio files.\n        records_per_shard: The record number per shard.\n    """"""\n    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True)\n\n    writer = None\n    with open(file_path, ""r"") as f:\n        for index, row in enumerate(f):\n            if index % records_per_shard == 0:\n                if writer:\n                    writer.close()\n\n                shard = index // records_per_shard\n                file_path_name = os.path.join(dir_name, ""data-%05d"" % shard)\n                writer = recordio.Writer(file_path_name)\n\n            feature = convert_series_to_tf_feature(\n                row, DAC_COLUMNS, DAC_DTYPES\n            )\n            result_string = tf.train.Example(\n                features=tf.train.Features(feature=feature)\n            ).SerializeToString()\n            writer.write(result_string)\n\n        if writer:\n            writer.close()\n\n        print(""Finish data conversion in {}"".format(dir_name))\n\n\ndef split_to_train_test(data_path):\n    data_dir = os.path.dirname(data_path)\n\n    train_file_name = os.path.join(data_dir, ""edl_train.txt"")\n    train_file = open(train_file_name, ""w"")\n\n    eval_file_name = os.path.join(data_dir, ""edl_test.txt"")\n    eval_file = open(eval_file_name, ""w"")\n    with open(data_path, ""r"") as f:\n        for i, line in enumerate(f):\n            if i % 20 == 0:\n                eval_file.write(line)\n            else:\n                train_file.write(line)\n\n    train_file.close()\n    eval_file.close()\n    return train_file_name, eval_file_name\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--records_per_shard"",\n        type=int,\n        default=128,\n        help=""Record number per shard"",\n    )\n    parser.add_argument(\n        ""--output_dir"", help=""The directory for the generated recordio files""\n    )\n    parser.add_argument(""--data_path"", help=""The path of the origin data file"")\n\n    args = parser.parse_args(sys.argv[1:])\n\n    train, val = split_to_train_test(args.data_path)\n\n    convert_to_recordio_files(\n        train, os.path.join(args.output_dir, ""train""), args.records_per_shard\n    )\n    convert_to_recordio_files(\n        val, os.path.join(args.output_dir, ""val""), args.records_per_shard\n    )\n'"
model_zoo/dac_ctr/dcn_model.py,11,"b'import tensorflow as tf\nfrom deepctr.layers.interaction import CrossNet\n\nfrom model_zoo.dac_ctr.utils import DNN, lookup_embedding_func\n\n\ndef dcn_model(\n    input_layers, dense_tensor, id_tensors, max_ids, deep_embedding_dim=8,\n):\n    """"""\n    Args:\n        input_layers: dict, the key is feature name and\n            the value is tf.keras.Input.\n        dense_tensor: A 2-D tensor with float dtype\n        id_tensors: dict, the key is a string and the value\n            is a tensor with int64 dtype\n        max_ids: dict, the key is group name and the value is the max\n            integer id of this group.\n        deep_embedding_dim: The output dimension of embedding layer for\n            deep parts.\n    """"""\n    # linear part\n    linear_logits = lookup_embedding_func(id_tensors, max_ids, 1)\n\n    # deep part\n    deep_embeddings = lookup_embedding_func(\n        id_tensors, max_ids, deep_embedding_dim\n    )\n\n    model = dcn(\n        input_layers,\n        dense_tensor,\n        linear_logits,\n        deep_embeddings,\n        deep_embedding_dim,\n    )\n    return model\n\n\ndef dcn(\n    input_layers,\n    dense_tensor,\n    linear_logits,\n    deep_embeddings,\n    deep_embedding_dim=8,\n    l2_reg=1e-5,\n):\n    # Deep Part\n    dnn_input = tf.reshape(\n        deep_embeddings, shape=(-1, len(deep_embeddings) * deep_embedding_dim)\n    )\n    if dense_tensor is not None:\n        dnn_input = tf.keras.layers.Concatenate()([dense_tensor, dnn_input])\n        linear_logits.append(\n            tf.keras.layers.Dense(1, use_bias=False, activation=None)(\n                dense_tensor\n            )\n        )\n\n    if len(linear_logits) > 1:\n        linear_logit = tf.keras.layers.Concatenate()(linear_logits)\n    else:\n        linear_logit = linear_logits[0]\n\n    dnn_output = DNN(hidden_units=[16, 4], activation=""relu"")(dnn_input)\n\n    cross_out = CrossNet(2, l2_reg=l2_reg)(dnn_input)\n    # Output Part\n    concat_input = tf.concat([dnn_output, cross_out], 1)\n    deep_cross_logit = tf.keras.layers.Dense(\n        1, use_bias=False, activation=None\n    )(concat_input)\n\n    concat_input = tf.concat([linear_logit, deep_cross_logit], 1)\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""dcn"",\n    )\n'"
model_zoo/dac_ctr/deepfm_model.py,13,"b'import tensorflow as tf\nfrom deepctr.layers.interaction import FM\n\nfrom model_zoo.dac_ctr.utils import DNN, lookup_embedding_func\n\n\ndef deepfm_model(\n    input_layers,\n    dense_tensor,\n    id_tensors,\n    max_ids,\n    sequence_tensor=None,\n    deep_embedding_dim=8,\n):\n    """"""\n    Args:\n        input_layers: dict, the key is feature name and\n            the value is tf.keras.Input.\n        dense_tensor: A 2-D tensor with float dtype\n        id_tensors: dict, the key is a string and the value\n            is a tensor with int64 dtype\n        max_ids: dict, the key is group name and the value is the max\n            integer id of this group.\n        sequence_tensor: 2-D tensor with string\n        deep_embedding_dim: The output dimension of embedding layer for\n            deep parts.\n    """"""\n    # wide part\n    linear_logits = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=1,\n    )\n\n    # deep part\n    deep_embeddings = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=deep_embedding_dim,\n    )\n\n    model = deepfm(\n        input_layers,\n        dense_tensor,\n        linear_logits,\n        deep_embeddings,\n        deep_embedding_dim,\n    )\n    return model\n\n\ndef deepfm(\n    input_layers,\n    dense_tensor,\n    linear_logits,\n    deep_embeddings,\n    deep_embedding_dim=8,\n):\n    # Deep Part\n    dnn_input = tf.reshape(\n        deep_embeddings, shape=(-1, len(deep_embeddings) * deep_embedding_dim)\n    )\n    if dense_tensor is not None:\n        dnn_input = tf.keras.layers.Concatenate()([dense_tensor, dnn_input])\n        linear_logits.append(\n            tf.keras.layers.Dense(1, activation=None, use_bias=False)(\n                dense_tensor\n            )\n        )\n\n    # Linear Part\n    if len(linear_logits) > 1:\n        linear_logit = tf.keras.layers.Concatenate()(linear_logits)\n    else:\n        linear_logit = linear_logits[0]\n\n    dnn_output = DNN(hidden_units=[16, 4], activation=""relu"")(dnn_input)\n    dnn_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(\n        dnn_output\n    )\n\n    if len(deep_embeddings) > 1:\n        field_size = len(deep_embeddings)\n        embeddings = tf.concat(\n            deep_embeddings, 1\n        )  # shape = (None, field_size, 8)\n        embeddings = tf.reshape(embeddings, shape=(-1, field_size, 8))\n        FM_output = FM()(embeddings)\n        # Output Part\n        concat_input = tf.concat([linear_logit, dnn_logit, FM_output], 1)\n    else:\n        concat_input = tf.concat([linear_logit, dnn_logit], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""deepfm"",\n    )\n'"
model_zoo/dac_ctr/elasticdl_train.py,17,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.callbacks import MaxStepsStopping\nfrom model_zoo.dac_ctr.feature_config import (\n    FEATURE_GROUPS,\n    FEATURE_NAMES,\n    HASH_FEATURES,\n    LABEL_KEY,\n    STANDARDIZED_FEATURES,\n)\nfrom model_zoo.dac_ctr.feature_transform import transform_feature\nfrom model_zoo.dac_ctr.xdeepfm_model import xdeepfm_model as ctr_model\n\n\ndef custom_model():\n    # The codes in the method can all be auto-generated\n    input_layers = get_input_layers(FEATURE_NAMES)\n    standardized_tensor, id_tensors, max_ids = transform_feature(\n        input_layers, feature_groups=FEATURE_GROUPS\n    )\n    model = ctr_model(input_layers, standardized_tensor, id_tensors, max_ids,)\n    return model\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    return [\n        MaxStepsStopping(max_steps=150000),\n    ]\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature((1,), tf.int64))\n                for name in STANDARDIZED_FEATURES\n            ]\n            + [\n                (name, tf.io.FixedLenFeature((1,), tf.string))\n                for name in HASH_FEATURES\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.prefetch(10000)\n    dataset = dataset.shuffle(10000)\n    dataset = dataset.map(_parse_data, num_parallel_calls=8)\n\n    return dataset\n\n\ndef get_input_layers(feature_names):\n    input_layers = {}\n    for name in feature_names:\n        if name in STANDARDIZED_FEATURES:\n            dtype = tf.int64\n        else:\n            dtype = tf.string\n        input_layers[name] = tf.keras.layers.Input(\n            name=name, shape=(1,), dtype=dtype\n        )\n\n    return input_layers\n\n\nif __name__ == ""__main__"":\n    model = custom_model()\n    test_data = {}\n    for name in STANDARDIZED_FEATURES:\n        test_data[name] = tf.constant([[10]])\n    for name in HASH_FEATURES:\n        test_data[name] = tf.constant([[""aa""]])\n    print(model.call(test_data))\n'"
model_zoo/dac_ctr/feature_config.py,0,"b'STANDARDIZED_FEATURES = [\n    ""I1"",\n    ""I2"",\n    ""I3"",\n    ""I4"",\n    ""I5"",\n    ""I6"",\n    ""I7"",\n    ""I8"",\n    ""I9"",\n    ""I10"",\n    ""I11"",\n    ""I12"",\n    ""I13"",\n]\n\nFEATURES_AVGS = {\n    ""I1"": 1.913844818114358,\n    ""I2"": 105.85781137082337,\n    ""I3"": 21.179428578076866,\n    ""I4"": 5.735273873448716,\n    ""I5"": 18067.71807784242,\n    ""I6"": 90.08603360120591,\n    ""I7"": 15.626512199091756,\n    ""I8"": 12.509966404126569,\n    ""I9"": 101.53250047174322,\n    ""I10"": 0.3374528968790535,\n    ""I11"": 2.614521353031052,\n    ""I12"": 0.23277149534177055,\n    ""I13"": 6.436560081179827,\n}\n\nFEATURES_STDDEVS = {\n    ""I1"": 7.203044443387521,\n    ""I2"": 391.73147156506417,\n    ""I3"": 354.59360229869503,\n    ""I4"": 8.351369642571008,\n    ""I5"": 68611.11705989522,\n    ""I6"": 340.20415627271075,\n    ""I7"": 64.82617180501207,\n    ""I8"": 16.71389239615237,\n    ""I9"": 216.67850042198575,\n    ""I10"": 0.5918310609867024,\n    ""I11"": 5.115695237395591,\n    ""I12"": 2.7609291491203973,\n    ""I13"": 14.799688705863462,\n}\nFEATURE_BOUNDARIES = {\n    ""I1"": [0.0, 1.0, 2.0, 5.0],\n    ""I2"": [-1.0, 0.0, 1.0, 1.0, 3.0, 8.0, 23.0, 56.0, 184.0],\n    ""I3"": [0.0, 1.0, 2.0, 4.0, 6.0, 10.0, 17.0, 36.0],\n    ""I4"": [0.0, 1.0, 2.0, 3.0, 4.0, 6.0, 9.0, 16.0],\n    ""I5"": [5.0, 79.0, 622.0, 1408.0, 2687.0, 4363.0, 7381.0, 13433.0, 33163.0],\n    ""I6"": [0.0, 1.0, 7.0, 16.0, 30.0, 54.0, 98.0, 216.0],\n    ""I7"": [0.0, 1.0, 2.0, 3.0, 5.0, 8.0, 15.0, 32.0],\n    ""I8"": [0.0, 2.0, 3.0, 5.0, 7.0, 11.0, 16.0, 23.0, 34.0],\n    ""I9"": [1.0, 5.0, 12.0, 21.0, 35.0, 54.0, 82.0, 134.0, 255.0],\n    ""I10"": [0.0, 1.0],\n    ""I11"": [0.0, 1.0, 2.0, 3.0, 6.0],\n    ""I12"": [0.0],\n    ""I13"": [0.0, 1.0, 2.0, 3.0, 4.0, 6.0, 10.0, 18.0],\n}\n\n\nBUCKET_FEATURES = [\n    ""I1"",\n    ""I2"",\n    ""I3"",\n    ""I4"",\n    ""I5"",\n    ""I6"",\n    ""I7"",\n    ""I8"",\n    ""I9"",\n    ""I10"",\n    ""I11"",\n    ""I12"",\n    ""I13"",\n]\n\nHASH_FEATURES = [\n    ""C1"",\n    ""C2"",\n    ""C3"",\n    ""C4"",\n    ""C5"",\n    ""C6"",\n    ""C7"",\n    ""C8"",\n    ""C9"",\n    ""C10"",\n    ""C11"",\n    ""C12"",\n    ""C13"",\n    ""C14"",\n    ""C15"",\n    ""C16"",\n    ""C17"",\n    ""C18"",\n    ""C19"",\n    ""C20"",\n    ""C21"",\n    ""C22"",\n    ""C23"",\n    ""C24"",\n    ""C25"",\n    ""C26"",\n]\n\nFEATURE_DISTINCT_COUNT = {\n    ""C1"": 1460,\n    ""C2"": 582,\n    ""C3"": 9264260,\n    ""C4"": 2046299,\n    ""C5"": 305,\n    ""C6"": 24,\n    ""C7"": 12506,\n    ""C8"": 633,\n    ""C9"": 3,\n    ""C10"": 91211,\n    ""C11"": 5670,\n    ""C12"": 7659856,\n    ""C13"": 3194,\n    ""C14"": 27,\n    ""C15"": 14876,\n    ""C16"": 5031503,\n    ""C17"": 10,\n    ""C18"": 5624,\n    ""C19"": 2171,\n    ""C20"": 4,\n    ""C21"": 6477624,\n    ""C22"": 18,\n    ""C23"": 15,\n    ""C24"": 272811,\n    ""C25"": 105,\n    ""C26"": 138075,\n}\n\nFEATURE_NAMES = STANDARDIZED_FEATURES + HASH_FEATURES\n\nLABEL_KEY = ""label""\n\nFEATURE_GROUPS = [\n    [""I1""],\n    [""I2""],\n    [""I3""],\n    [""I5""],\n    [""I6""],\n    [""I7""],\n    [""I8""],\n    [""I9""],\n    [""I10""],\n    [""I11""],\n    [""I12""],\n    [""I13""],\n    [""C1""],\n    [""C2""],\n    [""C3""],\n    [""C4""],\n    [""C5""],\n    [""C6""],\n    [""C7""],\n    [""C8""],\n    [""C9""],\n    [""C10""],\n    [""C11""],\n    [""C12""],\n    [""C13""],\n    [""C14""],\n    [""C15""],\n    [""C16""],\n    [""C17""],\n    [""C18""],\n    [""C19""],\n    [""C20""],\n    [""C21""],\n    [""C22""],\n    [""C23""],\n    [""C24""],\n    [""C25""],\n    [""C26""],\n]\n'"
model_zoo/dac_ctr/feature_transform.py,1,"b'import tensorflow as tf\n\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl_preprocessing.layers import (\n    ConcatenateWithOffset,\n    Discretization,\n    Hashing,\n    Normalizer,\n)\nfrom model_zoo.dac_ctr.feature_config import (\n    BUCKET_FEATURES,\n    FEATURE_BOUNDARIES,\n    FEATURE_DISTINCT_COUNT,\n    FEATURES_AVGS,\n    FEATURES_STDDEVS,\n    HASH_FEATURES,\n    STANDARDIZED_FEATURES,\n)\n\nMAX_HASHING_BUCKET_SIZE = 1000000\n\n\ndef transform_feature(inputs, feature_groups):\n    """"""According to the FeatureConfig object and feature groups to\n    transform inputs to dense tensors.\n\n    Args:\n        inputs: A dict contains Keras inputs where the key is the\n            feature name and the value is the Keras input.\n        feature_groups: 2-D list. each sub-list contains feature names\n            of a group\n\n    Returns:\n        standardized_tensor: A float tensor\n        group_tensors: A dict where the key is group name like ""group_0""\n            and the value is the integer tensor.\n        group_max_ids: A dict which has the same keys as group_tensors and\n            the value is the max value of the integer tensor in group_tensors.\n    """"""\n    standardized_outputs = []\n    for feature in STANDARDIZED_FEATURES:\n        standardized_result = Normalizer(\n            subtractor=FEATURES_AVGS[feature],\n            divisor=FEATURES_STDDEVS[feature],\n        )(inputs[feature])\n        standardized_outputs.append(standardized_result)\n\n    numerical_tensor = (\n        tf.concat(standardized_outputs, -1) if standardized_outputs else None\n    )\n\n    if not feature_groups:\n        feature_names = BUCKET_FEATURES + HASH_FEATURES\n        feature_groups = [feature_names]\n\n    id_tensors = {}\n    max_ids = {}\n    for i, features in enumerate(feature_groups):\n        group_name = ""group_{}"".format(i)\n        id_tensor, max_id = transform_group(inputs, features)\n        id_tensors[group_name] = id_tensor\n        max_ids[group_name] = max_id\n\n    return numerical_tensor, id_tensors, max_ids\n\n\ndef transform_group(inputs, features):\n    """"""Transform the inputs and concatenate inputs in a group\n    to a dense tensor\n\n    Args:\n        inputs: A dict contains Keras inputs where the key is the\n            feature name and the value is the Keras input.\n        features: A list which contains feature names.\n\n    Returns:\n        A integer tensor,\n        max_id: The max value of the returned tensor\n    """"""\n    group_items = []\n    id_offsets = [0]\n    for feature in features:\n        if feature in BUCKET_FEATURES:\n            discretize_layer = Discretization(bins=FEATURE_BOUNDARIES[feature])\n            transform_output = discretize_layer(inputs[feature])\n            group_items.append(transform_output)\n            id_offsets.append(id_offsets[-1] + len(discretize_layer.bins) + 1)\n            logger.info(""{}:{}"".format(feature, discretize_layer.bins))\n        elif feature in HASH_FEATURES:\n            num_bins = FEATURE_DISTINCT_COUNT[feature]\n            if num_bins > MAX_HASHING_BUCKET_SIZE:\n                num_bins = MAX_HASHING_BUCKET_SIZE\n            hash_layer = Hashing(num_bins=num_bins)\n            transform_output = hash_layer(inputs[feature])\n            id_offsets.append(id_offsets[-1] + hash_layer.num_bins)\n            group_items.append(transform_output)\n            logger.info(""{}:{}"".format(feature, hash_layer.num_bins))\n        else:\n            logger.warning(\n                ""The preprocessing is not configured for the feature ""\n                ""{}"".format(feature)\n            )\n    concated = ConcatenateWithOffset(id_offsets[0:-1])(group_items)\n    max_id = id_offsets[-1]\n    return concated, max_id\n'"
model_zoo/dac_ctr/utils.py,5,"b'import tensorflow as tf\n\n\ndef lookup_embedding_func(\n    id_tensors, max_ids, embedding_dim,\n):\n    """"""\n    Args:\n        id_layers: dict, the key is a string and\n            the value is tf.keras.Input.\n        standardized_tensor:\n        input_tensors: dict, the key is a string and the value\n            is a tensor outputed by the transform function\n        max_ids: dict, the key is a string and the value is the max\n            integer id of this group.\n        deep_embedding_dim: The output dimension of embedding layer for\n            deep parts.\n    """"""\n\n    embeddings = []\n    for name, id_tensor in id_tensors.items():\n        wide_embedding_layer = tf.keras.layers.Embedding(\n            max_ids[name], embedding_dim\n        )\n        embedding = wide_embedding_layer(id_tensor)\n        embedding_sum = tf.keras.backend.sum(embedding, axis=1)\n        embeddings.append(embedding_sum)\n    return embeddings\n\n\nclass DNN(tf.keras.layers.Layer):\n    """""" DNN layer\n\n    Args:\n        hidden_units: A list with integers\n        activation: activation function name, like ""relu""\n    inputs: A Tensor\n    """"""\n\n    def __init__(self, hidden_units, activation=None, **kwargs):\n        super(DNN, self).__init__(**kwargs)\n        self.dense_layers = []\n        if not hidden_units:\n            raise ValueError(""The hidden units cannot be empty"")\n        for hidden_unit in hidden_units:\n            self.dense_layers.append(\n                tf.keras.layers.Dense(hidden_unit, activation=activation)\n            )\n\n    def call(self, inputs):\n        output = inputs\n        for layer in self.dense_layers:\n            output = layer(output)\n        return output\n'"
model_zoo/dac_ctr/wide_deep_model.py,10,"b'import tensorflow as tf\n\nfrom model_zoo.dac_ctr.utils import DNN, lookup_embedding_func\n\n\ndef wide_deep_model(\n    input_layers,\n    standardized_tensor,\n    id_tensors,\n    max_ids,\n    deep_embedding_dim=8,\n    dnn_hidden_units=[16, 4],\n    dnn_activation=""relu"",\n):\n    """"""\n    Args:\n        input_layers: dict, the key is feature name and\n            the value is tf.keras.Input.\n        standardized_tensor:\n        id_tensors: dict, the key is a string and the value\n            is a tensor outputed by the transform function\n        max_ids: dict, the key is group name and the value is the max\n            integer id of this group.\n        deep_embedding_dim: The output dimension of embedding layer for\n            deep parts.\n    """"""\n    # wide part\n    linear_logits = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=1,\n    )\n\n    # deep part\n    deep_embeddings = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=deep_embedding_dim,\n    )\n\n    model = wide_deep(\n        input_layers,\n        standardized_tensor,\n        linear_logits,\n        deep_embeddings,\n        deep_embedding_dim,\n        dnn_hidden_units,\n        dnn_activation,\n    )\n    return model\n\n\ndef wide_deep(\n    input_layers,\n    standardized_tensor,\n    linear_logits,\n    deep_embeddings,\n    deep_embedding_dim=8,\n    dnn_hidden_units=[16, 4],\n    dnn_activation=""relu"",\n):\n    # Deep Part\n    dnn_input = tf.reshape(\n        deep_embeddings, shape=(-1, len(deep_embeddings) * deep_embedding_dim)\n    )\n    if standardized_tensor is not None:\n        dnn_input = tf.keras.layers.Concatenate()(\n            [standardized_tensor, dnn_input]\n        )\n        linear_logits.append(\n            tf.keras.layers.Dense(1, activation=None, use_bias=False)(\n                standardized_tensor\n            )\n        )\n\n    # Linear Part\n    if len(linear_logits) > 1:\n        linear_logit = tf.keras.layers.Concatenate()(linear_logits)\n    else:\n        linear_logit = linear_logits[0]\n\n    dnn_output = DNN(hidden_units=dnn_hidden_units, activation=dnn_activation)(\n        dnn_input\n    )\n    dnn_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(\n        dnn_output\n    )\n\n    # Output Part\n    concat_input = tf.concat([linear_logit, dnn_logit], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n'"
model_zoo/dac_ctr/xdeepfm_model.py,14,"b'import tensorflow as tf\nfrom deepctr.layers.interaction import CIN\n\nfrom model_zoo.dac_ctr.utils import DNN, lookup_embedding_func\n\n\ndef xdeepfm_model(\n    input_layers, dense_tensor, id_tensors, max_ids, deep_embedding_dim=8,\n):\n    """"""\n    Args:\n        input_layers: dict, the key is feature name and\n            the value is tf.keras.Input.\n        dense_tensor: A 2-D tensor with float dtype\n        id_tensors: dict, the key is a string and the value\n            is a tensor with int64 dtype\n        max_ids: dict, the key is group name and the value is the max\n            integer id of this group.\n        deep_embedding_dim: The output dimension of embedding layer for\n            deep parts.\n    """"""\n    # wide part\n    linear_logits = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=1,\n    )\n\n    # deep part\n    deep_embeddings = lookup_embedding_func(\n        id_tensors, max_ids, embedding_dim=deep_embedding_dim,\n    )\n\n    model = xdeepfm(\n        input_layers,\n        dense_tensor,\n        linear_logits,\n        deep_embeddings,\n        deep_embedding_dim,\n    )\n    return model\n\n\ndef xdeepfm(\n    input_layers,\n    dense_tensor,\n    linear_logits,\n    deep_embeddings,\n    deep_embedding_dim=8,\n):\n    # Deep Part\n    dnn_input = tf.reshape(\n        deep_embeddings, shape=(-1, len(deep_embeddings) * deep_embedding_dim)\n    )\n    if dense_tensor is not None:\n        dnn_input = tf.keras.layers.Concatenate()([dense_tensor, dnn_input])\n        linear_logits.append(\n            tf.keras.layers.Dense(1, activation=None, use_bias=False)(\n                dense_tensor\n            )\n        )\n\n    # Linear Part\n    if len(linear_logits) > 1:\n        linear_logit = tf.keras.layers.Concatenate()(linear_logits)\n    else:\n        linear_logit = linear_logits[0]\n\n    dnn_output = DNN(hidden_units=[16, 4], activation=""relu"")(dnn_input)\n\n    dnn_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(\n        dnn_output\n    )\n\n    if len(deep_embeddings) > 1:\n        field_size = len(deep_embeddings)\n        embeddings = tf.concat(\n            deep_embeddings, 1\n        )  # shape = (None, field_size, 8)\n        embeddings = tf.reshape(embeddings, shape=(-1, field_size, 8))\n        exFM_out = CIN(\n            layer_size=(128, 128,), activation=""relu"", split_half=True\n        )(embeddings)\n        exFM_logit = tf.keras.layers.Dense(1, activation=None,)(exFM_out)\n        # Output Part\n        concat_input = tf.concat([linear_logit, dnn_logit, exFM_logit], 1)\n    else:\n        concat_input = tf.concat([linear_logit, dnn_output], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""xdeepfm"",\n    )\n'"
model_zoo/deepfm_edl_embedding/deepfm_edl_embedding.py,22,"b'import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense, Flatten, Layer, Multiply, Subtract\n\nfrom elasticdl.python.common.constants import Mode\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\n\nAUC_metric = None\n\n\nclass ApplyMask(Layer):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(ApplyMask, self).__init__(**kwargs)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        return Multiply()([x, tf.cast(K.expand_dims(mask, -1), tf.float32)])\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\ndef custom_model(\n    input_dim=5383, embedding_dim=64, input_length=10, fc_unit=64\n):\n    inputs = tf.keras.Input(shape=(input_length,))\n    embed_layer = Embedding(\n        output_dim=embedding_dim, mask_zero=True, input_length=input_length\n    )\n    embeddings = embed_layer(inputs)\n    embeddings = ApplyMask()(embeddings)\n\n    emb_sum = K.sum(embeddings, axis=1)\n    emb_sum_square = K.square(emb_sum)\n    emb_square = K.square(embeddings)\n    emb_square_sum = K.sum(emb_square, axis=1)\n    second_order = K.sum(\n        0.5 * Subtract()([emb_sum_square, emb_square_sum]), axis=1\n    )\n\n    id_bias = Embedding(output_dim=1, mask_zero=True)(inputs)\n    id_bias = ApplyMask()(id_bias)\n    first_order = K.sum(id_bias, axis=(1, 2))\n    fm_output = tf.keras.layers.Add()([first_order, second_order])\n\n    nn_input = Flatten()(embeddings)\n    nn_h = Dense(fc_unit)(nn_input)\n    deep_output = Dense(1)(nn_h)\n    deep_output = tf.reshape(deep_output, shape=(-1,))\n    logits = tf.keras.layers.Add()([fm_output, deep_output])\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1, 1))\n\n    m = tf.keras.Model(\n        inputs=inputs, outputs={""logits"": logits, ""probs"": probs}\n    )\n    return m\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    labels = tf.cast(tf.reshape(labels, [-1]), tf.dtypes.float32)\n    logits = tf.reshape(logits, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=logits, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""feature"": tf.io.FixedLenFeature([10], tf.int64)\n            }\n        else:\n            feature_description = {\n                ""feature"": tf.io.FixedLenFeature([10], tf.int64),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {""feature"": tf.cast(r[""feature""], tf.int64)}\n        if mode == Mode.PREDICTION:\n            return features\n        return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.0, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n'"
model_zoo/deepfm_functional_api/deepfm_functional_api.py,22,"b'import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import (\n    Dense,\n    Embedding,\n    Flatten,\n    Layer,\n    Multiply,\n    Subtract,\n)\n\nfrom elasticdl.python.common.constants import Mode\nfrom elasticdl.python.data.reader.recordio_reader import RecordIODataReader\nfrom elasticdl.python.elasticdl.callbacks import (\n    LearningRateScheduler,\n    MaxStepsStopping,\n)\n\nAUC_metric = None\n\n\nclass ApplyMask(Layer):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(ApplyMask, self).__init__(**kwargs)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        return Multiply()([x, tf.cast(K.expand_dims(mask, -1), tf.float32)])\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\ndef custom_model(\n    input_dim=5383, embedding_dim=64, input_length=10, fc_unit=64\n):\n    inputs = tf.keras.Input(shape=(input_length,))\n    embed_layer = Embedding(\n        input_dim=input_dim,\n        output_dim=embedding_dim,\n        mask_zero=True,\n        input_length=input_length,\n    )\n    embeddings = embed_layer(inputs)\n    embeddings = ApplyMask()(embeddings)\n\n    emb_sum = K.sum(embeddings, axis=1)\n    emb_sum_square = K.square(emb_sum)\n    emb_square = K.square(embeddings)\n    emb_square_sum = K.sum(emb_square, axis=1)\n    second_order = K.sum(\n        0.5 * Subtract()([emb_sum_square, emb_square_sum]), axis=1\n    )\n\n    id_bias = Embedding(input_dim=input_dim, output_dim=1, mask_zero=True)(\n        inputs\n    )\n    id_bias = ApplyMask()(id_bias)\n    first_order = K.sum(id_bias, axis=(1, 2))\n    fm_output = tf.keras.layers.Add()([first_order, second_order])\n\n    nn_input = Flatten()(embeddings)\n    nn_h = Dense(fc_unit)(nn_input)\n    deep_output = Dense(1)(nn_h)\n    deep_output = tf.reshape(deep_output, shape=(-1,))\n    logits = tf.keras.layers.Add()([fm_output, deep_output])\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1, 1))\n\n    m = tf.keras.Model(\n        inputs=inputs, outputs={""logits"": logits, ""probs"": probs}\n    )\n    return m\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    labels = tf.cast(tf.reshape(labels, [-1]), tf.dtypes.float32)\n    logits = tf.reshape(logits, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=logits, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""feature"": tf.io.FixedLenFeature([10], tf.int64)\n            }\n        else:\n            feature_description = {\n                ""feature"": tf.io.FixedLenFeature([10], tf.int64),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {""feature"": tf.cast(r[""feature""], tf.float32)}\n        if mode == Mode.PREDICTION:\n            return features\n        return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.0, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        return 0.5 if model_version < 100 else 0.2\n\n    learning_reate_scheduler = LearningRateScheduler(_schedule)\n    max_steps_stopping = MaxStepsStopping(max_steps=200)\n    return [max_steps_stopping, learning_reate_scheduler]\n\n\nCustomDataReader = RecordIODataReader\n\n\ndef custom_data_reader(data_origin, records_per_task=None, **kwargs):\n    return CustomDataReader(data_dir=data_origin)\n'"
model_zoo/heart_functional_api/heart_functional_api.py,30,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.feature_column import feature_column\n\n\ndef get_feature_columns_and_inputs():\n    feature_columns = []\n    feature_input_layers = {}\n\n    for header in [""trestbps"", ""chol"", ""thalach"", ""oldpeak"", ""slope"", ""ca""]:\n        feature_columns.append(tf.feature_column.numeric_column(header))\n        feature_input_layers[header] = tf.keras.Input(shape=(1,), name=header)\n\n    age = tf.feature_column.numeric_column(""age"")\n    age_buckets = tf.feature_column.bucketized_column(\n        age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    )\n    feature_columns.append(age_buckets)\n    feature_input_layers[""age""] = tf.keras.Input(shape=(1,), name=""age"")\n\n    thal_hashed = tf.feature_column.categorical_column_with_hash_bucket(\n        ""thal"", hash_bucket_size=100\n    )\n    thal_embedding = feature_column.embedding_column(thal_hashed, dimension=8)\n    feature_columns.append(thal_embedding)\n    feature_input_layers[""thal""] = tf.keras.Input(\n        shape=(1,), name=""thal"", dtype=tf.string\n    )\n\n    return feature_columns, feature_input_layers\n\n\ndef custom_model():\n    feature_columns, feature_inputs = get_feature_columns_and_inputs()\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    x = feature_layer(feature_inputs)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    y = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)\n\n    model = tf.keras.Model(inputs=feature_inputs, outputs=y)\n\n    return model\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    predictions = tf.reshape(predictions, [-1])\n    return tf.keras.losses.binary_crossentropy(labels, predictions)\n\n\ndef optimizer(lr=1e-6):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n\n        feature_description = {\n            ""age"": tf.io.FixedLenFeature([], tf.int64),\n            ""trestbps"": tf.io.FixedLenFeature([], tf.int64),\n            ""chol"": tf.io.FixedLenFeature([], tf.int64),\n            ""thalach"": tf.io.FixedLenFeature([], tf.int64),\n            ""oldpeak"": tf.io.FixedLenFeature([], tf.float32),\n            ""slope"": tf.io.FixedLenFeature([], tf.int64),\n            ""ca"": tf.io.FixedLenFeature([], tf.int64),\n            ""thal"": tf.io.FixedLenFeature([], tf.string),\n            ""target"": tf.io.FixedLenFeature([], tf.int64),\n        }\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(""target"")\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n'"
model_zoo/imagenet_resnet50/__init__.py,0,b''
model_zoo/imagenet_resnet50/imagenet_resnet50.py,6,"b'import tensorflow as tf\n\n\ndef prepare_data_for_a_single_file(file_object, filename):\n    """"""\n    :param filename: training data file name\n    The raw images should be packed into a TAR file and named as\n    <label_id>_xxx.JPEG, in which label_id is an integer representing\n    the category the image belongs to, and xxx can be anything.\n\n    :param file_object: a file object associated with filename\n    """"""\n    label = filename.split(""/"")[-1].split(""_"")[0]\n    int_label = int(label)\n    image_bytes = file_object.read()\n    feature_dict = {}\n    feature_dict[""image""] = tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[image_bytes])\n    )\n    feature_dict[""label""] = tf.train.Feature(\n        int64_list=tf.train.Int64List(value=[int_label])\n    )\n    example = tf.train.Example(\n        features=tf.train.Features(feature=feature_dict)\n    )\n    return example.SerializeToString()\n'"
model_zoo/mnist_functional_api/__init__.py,0,b''
model_zoo/mnist_functional_api/mnist_functional_api.py,29,"b'import numpy as np\nimport PIL.Image\nimport tensorflow as tf\n\nfrom elasticdl.python.common.constants import Mode\n\n\ndef custom_model():\n    inputs = tf.keras.Input(shape=(28, 28), name=""image"")\n    x = tf.keras.layers.Reshape((28, 28, 1))(inputs)\n    x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=""relu"")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = tf.keras.layers.Flatten()(x)\n    outputs = tf.keras.layers.Dense(10)(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=""mnist_model"")\n\n\ndef prepare_data_for_a_single_file(file_object, filename):\n    """"""\n    :param filename: training data file name\n    :param file_object: a file object associated with filename\n    """"""\n    label = int(filename.split(""/"")[-2])\n    image = PIL.Image.open(file_object)\n    numpy_image = np.array(image)\n    example_dict = {\n        ""image"": tf.train.Feature(\n            float_list=tf.train.FloatList(value=numpy_image.flatten())\n        ),\n        ""label"": tf.train.Feature(\n            int64_list=tf.train.Int64List(value=[label])\n        ),\n    }\n    example = tf.train.Example(\n        features=tf.train.Features(feature=example_dict)\n    )\n    return example.SerializeToString()\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=predictions, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([28, 28], tf.float32)\n            }\n        else:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([28, 28], tf.float32),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {\n            ""image"": tf.math.divide(tf.cast(r[""image""], tf.float32), 255.0)\n        }\n        if mode == Mode.PREDICTION:\n            return features\n        else:\n            return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n'"
model_zoo/mnist_subclass/__init__.py,0,b''
model_zoo/mnist_subclass/mnist_subclass.py,23,"b'import tensorflow as tf\n\nfrom elasticdl.python.common.constants import Mode\n\n\nclass CustomModel(tf.keras.Model):\n    def __init__(self, channel_last=True):\n        super(CustomModel, self).__init__(name=""mnist_model"")\n        if channel_last:\n            self._reshape = tf.keras.layers.Reshape((28, 28, 1))\n        else:\n            self._reshape = tf.keras.layers.Reshape((1, 28, 28))\n        self._conv1 = tf.keras.layers.Conv2D(\n            32, kernel_size=(3, 3), activation=""relu""\n        )\n        self._conv2 = tf.keras.layers.Conv2D(\n            64, kernel_size=(3, 3), activation=""relu""\n        )\n        self._batch_norm = tf.keras.layers.BatchNormalization()\n        self._maxpooling = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n        self._dropout = tf.keras.layers.Dropout(0.25)\n        self._flatten = tf.keras.layers.Flatten()\n        self._dense = tf.keras.layers.Dense(10)\n\n    def call(self, inputs, training=False):\n        x = self._reshape(inputs[""image""])\n        x = self._conv1(x)\n        x = self._conv2(x)\n        x = self._batch_norm(x, training=training)\n        x = self._maxpooling(x)\n        if training:\n            x = self._dropout(x, training=training)\n        x = self._flatten(x)\n        x = self._dense(x)\n        return x\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=predictions, labels=labels\n        )\n    )\n\n\ndef optimizer(lr=0.01):\n    return tf.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([28, 28], tf.float32)\n            }\n        else:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([28, 28], tf.float32),\n                ""label"": tf.io.FixedLenFeature([1], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = {\n            ""image"": tf.math.divide(tf.cast(r[""image""], tf.float32), 255.0)\n        }\n        if mode == Mode.PREDICTION:\n            return features\n        else:\n            return features, tf.cast(r[""label""], tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n'"
model_zoo/odps_iris_dnn_model/__init__.py,0,b''
model_zoo/odps_iris_dnn_model/odps_iris_dnn_model.py,13,"b'import tensorflow as tf\n\n\ndef custom_model():\n    inputs = tf.keras.layers.Input(shape=(4, 1), name=""input"")\n    x = tf.keras.layers.Flatten()(inputs)\n    outputs = tf.keras.layers.Dense(3, name=""output"")(x)\n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=""simple-model"")\n\n\ndef loss(labels, predictions):\n    return tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(\n            tf.cast(tf.reshape(labels, [-1]), tf.int32), predictions\n        )\n    )\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n\n\ndef dataset_fn(dataset, mode, metadata):\n    def _parse_data(record):\n        features = tf.strings.to_number(record[0:-1], tf.float32)\n        label = tf.strings.to_number(record[-1], tf.float32)\n        return features, label\n\n    dataset = dataset.map(_parse_data)\n    return dataset\n'"
model_zoo/resnet50_subclass/__init__.py,0,b''
model_zoo/resnet50_subclass/resnet50_model.py,2,"b'import tensorflow as tf\nfrom tensorflow.python.keras import backend, layers, regularizers\n\nL2_WEIGHT_DECAY = 1e-4\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 1e-5\n\n\nclass IdentityBlock(tf.keras.Model):\n    def __init__(self, kernel_size, filters, stage, block):\n        super(IdentityBlock, self).__init__(name=""identity_block"")\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == ""channels_last"":\n            bn_axis = 3\n        else:\n            bn_axis = 1\n        conv_name_base = ""res"" + str(stage) + block + ""_branch""\n        bn_name_base = ""bn"" + str(stage) + block + ""_branch""\n\n        self._conv2d_1 = layers.Conv2D(\n            filters1,\n            (1, 1),\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2a"",\n        )\n        self._bn_1 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2a"",\n        )\n        self.activation_1 = layers.Activation(""relu"")\n\n        self._conv2d_2 = layers.Conv2D(\n            filters2,\n            kernel_size,\n            padding=""same"",\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2b"",\n        )\n        self._bn_2 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2b"",\n        )\n        self._activation_2 = layers.Activation(""relu"")\n\n        self._conv2d_3 = layers.Conv2D(\n            filters3,\n            (1, 1),\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2c"",\n        )\n        self._bn_3 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2c"",\n        )\n\n        self._activation_e3 = layers.Activation(""relu"")\n\n    def call(self, inputs, training=False):\n        x = self._conv2d_1(inputs)\n        x = self._bn_1(x, training=training)\n        x = self.activation_1(x)\n        x = self._conv2d_2(x)\n        x = self._bn_2(x, training=training)\n        x = self._activation_2(x)\n        x = self._conv2d_3(x)\n        x = self._bn_3(x, training=training)\n        x = layers.add([x, inputs])\n        return self._activation_e3(x)\n\n\nclass ConvBlock(tf.keras.Model):\n    def __init__(self, kernel_size, filters, stage, block, strides=(2, 2)):\n        super(ConvBlock, self).__init__(name=""conv_block"")\n        filters1, filters2, filters3 = filters\n        if backend.image_data_format() == ""channels_last"":\n            bn_axis = 3\n        else:\n            bn_axis = 1\n        conv_name_base = ""res"" + str(stage) + block + ""_branch""\n        bn_name_base = ""bn"" + str(stage) + block + ""_branch""\n\n        self._conv2d_1 = layers.Conv2D(\n            filters1,\n            (1, 1),\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2a"",\n        )\n        self._bn_1 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2a"",\n        )\n        self._activation_1 = layers.Activation(""relu"")\n\n        self._conv2d_2 = layers.Conv2D(\n            filters2,\n            kernel_size,\n            strides=strides,\n            padding=""same"",\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2b"",\n        )\n        self._bn_2 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2b"",\n        )\n        self._activation_2 = layers.Activation(""relu"")\n\n        self._conv2d_3 = layers.Conv2D(\n            filters3,\n            (1, 1),\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""2c"",\n        )\n        self._bn_3 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""2c"",\n        )\n\n        self._shortcut = layers.Conv2D(\n            filters3,\n            (1, 1),\n            strides=strides,\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=conv_name_base + ""1"",\n        )\n        self._bn_4 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=bn_name_base + ""1"",\n        )\n\n        self._activation_4 = layers.Activation(""relu"")\n\n    def call(self, inputs, training=False):\n        x = self._conv2d_1(inputs)\n        x = self._bn_1(x, training=training)\n        x = self._activation_1(x)\n        x = self._conv2d_2(x)\n        x = self._bn_2(x, training=training)\n        x = self._activation_2(x)\n        x = self._conv2d_3(x)\n        x = self._bn_3(x, training=training)\n        shortcut = self._shortcut(inputs)\n        shortcut = self._bn_4(shortcut, training=training)\n        x = layers.add([x, shortcut])\n        return self._activation_4(x)\n'"
model_zoo/resnet50_subclass/resnet50_subclass.py,20,"b'import tensorflow as tf\nfrom tensorflow.python.keras import backend, layers, regularizers\n\nfrom elasticdl.python.common.constants import Mode\n\ntry:\n    from resnet50_subclass.resnet50_model import (\n        L2_WEIGHT_DECAY,\n        BATCH_NORM_DECAY,\n        BATCH_NORM_EPSILON,\n        IdentityBlock,\n        ConvBlock,\n    )\nexcept ImportError:\n    from model_zoo.resnet50_subclass.resnet50_model import (\n        L2_WEIGHT_DECAY,\n        BATCH_NORM_DECAY,\n        BATCH_NORM_EPSILON,\n        IdentityBlock,\n        ConvBlock,\n    )\n\n\nclass CustomModel(tf.keras.Model):\n    def __init__(self, num_classes=10, dtype=""float32"", batch_size=None):\n        super(CustomModel, self).__init__(name=""resnet50"")\n\n        if backend.image_data_format() == ""channels_first"":\n            self._lambda = layers.Lambda(\n                lambda x: backend.permute_dimensions(x, (0, 3, 1, 2)),\n                name=""transpose"",\n            )\n            bn_axis = 1\n            data_format = ""channels_first""\n        else:\n            bn_axis = 3\n            data_format = ""channels_last""\n\n        self._padding = layers.ZeroPadding2D(\n            padding=(3, 3), data_format=data_format, name=""zero_pad""\n        )\n        self._conv2d_1 = layers.Conv2D(\n            64,\n            (7, 7),\n            strides=(2, 2),\n            padding=""valid"",\n            use_bias=False,\n            kernel_initializer=""he_normal"",\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=""conv1"",\n        )\n        self._bn_1 = layers.BatchNormalization(\n            axis=bn_axis,\n            momentum=BATCH_NORM_DECAY,\n            epsilon=BATCH_NORM_EPSILON,\n            name=""bn_conv1"",\n        )\n        self._activation_1 = layers.Activation(""relu"")\n        self._maxpooling2d = layers.MaxPooling2D(\n            (3, 3), strides=(2, 2), padding=""same""\n        )\n\n        self._conv_block_1 = ConvBlock(\n            3, [64, 64, 256], stage=2, block=""a"", strides=(1, 1)\n        )\n        self._identity_block_1 = IdentityBlock(\n            3, [64, 64, 256], stage=2, block=""b""\n        )\n        self._identity_block_2 = IdentityBlock(\n            3, [64, 64, 256], stage=2, block=""c""\n        )\n\n        self._conv_block_2 = ConvBlock(3, [128, 128, 512], stage=3, block=""a"")\n        self._identity_block_3 = IdentityBlock(\n            3, [128, 128, 512], stage=3, block=""b""\n        )\n        self._identity_block_4 = IdentityBlock(\n            3, [128, 128, 512], stage=3, block=""c""\n        )\n        self._identity_block_5 = IdentityBlock(\n            3, [128, 128, 512], stage=3, block=""d""\n        )\n\n        self._conv_block_3 = ConvBlock(3, [256, 256, 1024], stage=4, block=""a"")\n        self._identity_block_6 = IdentityBlock(\n            3, [256, 256, 1024], stage=4, block=""b""\n        )\n        self._identity_block_7 = IdentityBlock(\n            3, [256, 256, 1024], stage=4, block=""c""\n        )\n        self._identity_block_8 = IdentityBlock(\n            3, [256, 256, 1024], stage=4, block=""d""\n        )\n        self._identity_block_9 = IdentityBlock(\n            3, [256, 256, 1024], stage=4, block=""e""\n        )\n        self._identity_block_10 = IdentityBlock(\n            3, [256, 256, 1024], stage=4, block=""f""\n        )\n\n        self._conv_block_4 = ConvBlock(3, [512, 512, 2048], stage=5, block=""a"")\n        self._identity_block_11 = IdentityBlock(\n            3, [512, 512, 2048], stage=5, block=""b""\n        )\n        self._identity_block_12 = IdentityBlock(\n            3, [512, 512, 2048], stage=5, block=""c""\n        )\n\n        rm_axes = (\n            [1, 2]\n            if backend.image_data_format() == ""channels_last""\n            else [2, 3]\n        )\n        self._lamba_2 = layers.Lambda(\n            lambda x: backend.mean(x, rm_axes), name=""reduce_mean""\n        )\n        self._dense = layers.Dense(\n            num_classes,\n            kernel_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            bias_regularizer=regularizers.l2(L2_WEIGHT_DECAY),\n            name=""fc1000"",\n        )\n        self._activation_2 = layers.Activation(""softmax"")\n\n    def call(self, inputs, training=True):\n\n        images = inputs[""image""]\n        if backend.image_data_format() == ""channels_first"":\n            x = self._lambda(images)\n        else:\n            x = images\n        x = self._padding(x)\n        x = self._conv2d_1(x)\n        x = self._bn_1(x, training=training)\n        x = self._activation_1(x)\n        x = self._maxpooling2d(x)\n        x = self._conv_block_1(x, training=training)\n        x = self._identity_block_1(x, training=training)\n        x = self._identity_block_2(x, training=training)\n        x = self._conv_block_2(x, training=training)\n        x = self._identity_block_3(x, training=training)\n        x = self._identity_block_4(x, training=training)\n        x = self._identity_block_5(x, training=training)\n        x = self._conv_block_3(x, training=training)\n        x = self._identity_block_6(x, training=training)\n        x = self._identity_block_7(x, training=training)\n        x = self._identity_block_8(x, training=training)\n        x = self._identity_block_9(x, training=training)\n        x = self._identity_block_10(x, training=training)\n        x = self._conv_block_4(x, training=training)\n        x = self._identity_block_11(x, training=training)\n        x = self._identity_block_12(x, training=training)\n        x = self._lamba_2(x)\n        x = self._dense(x)\n        x = backend.cast(x, ""float32"")\n        return self._activation_2(x)\n\n\ndef loss(labels, predictions):\n    labels = tf.reshape(labels, [-1])\n    return tf.reduce_mean(\n        input_tensor=tf.keras.losses.sparse_categorical_crossentropy(\n            labels, predictions\n        )\n    )\n\n\ndef optimizer(lr=0.02):\n    return tf.keras.optimizers.SGD(lr)\n\n\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n        if mode == Mode.PREDICTION:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([], tf.string)\n            }\n        else:\n            feature_description = {\n                ""image"": tf.io.FixedLenFeature([], tf.string),\n                ""label"": tf.io.FixedLenFeature([], tf.int64),\n            }\n        r = tf.io.parse_single_example(record, feature_description)\n        features = tf.image.resize(\n            tf.image.decode_jpeg(r[""image""]),\n            [224, 224],\n            method=tf.image.ResizeMethod.BILINEAR,\n        )\n        features = tf.cond(\n            tf.math.greater(tf.size(features), 244 * 244),\n            lambda: features,\n            lambda: tf.image.grayscale_to_rgb(features),\n        )\n        features = {\n            ""image"": tf.math.divide(tf.cast(features, tf.float32), 255.0)\n        }\n        if mode == Mode.PREDICTION:\n            return features\n        else:\n            return features, tf.cast(r[""label""] - 1, tf.int32)\n\n    dataset = dataset.map(_parse_data)\n\n    if mode == Mode.TRAINING:\n        dataset = dataset.shuffle(buffer_size=1024)\n    return dataset\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: tf.equal(\n            tf.argmax(predictions, 1, output_type=tf.int32),\n            tf.cast(tf.reshape(labels, [-1]), tf.int32),\n        )\n    }\n'"
tools/odps_table_tools/normalize_kv_udf.py,0,"b'from odps.udf import BaseUDTF\n\n\ndef parse_kv_string_to_dict(\n    kvs_string, output_key_names, inter_kv_sep, intra_kv_sep\n):\n    """""" Parse kv string to a dict like ""k1:v1,k2:v2"" to {k1:v1,k2:v2}\n    Args:\n        kvs_string: Key-value pairs string\n        output_key_names: All key names to be saved in the output dict\n        inter_kv_sep: Inter separator in the Key-value pair string\n        intra_kv_sep: Intra separator int Ker-value pairs string.\n\n    Returns:\n        dict\n    """"""\n    kv_dict = {}\n    kv_pairs = kvs_string.split(inter_kv_sep)\n    for kv in kv_pairs:\n        key_and_value = kv.split(intra_kv_sep)\n        if len(key_and_value) == 2:\n            kv_dict[key_and_value[0]] = key_and_value[1]\n    values = []\n    for name in output_key_names:\n        values.append(kv_dict.get(name, """"))\n\n    return values\n\n\nclass KVFlatter(BaseUDTF):\n    """"""Split string by separator to values\n    """"""\n\n    def process(self, *args):\n        """"""\n        Args:\n            args (list): args[0] is kv column value, args[-3] is the feature\n            names string other values in args is the append column values.\n            args[-1] is the intra key-value pair separator and args[-2]\n            it the inter key-value sparator.\n        """"""\n        if len(args) < 4:\n            raise ValueError(""The input values number can not be less than 4"")\n        feature_names = args[-3].split("","")\n        inter_kv_sep = args[-2]\n        intra_kv_sep = args[-1]\n        feature_values = parse_kv_string_to_dict(\n            args[0], feature_names, inter_kv_sep, intra_kv_sep\n        )\n        for value in args[1:-3]:\n            feature_values.append(str(value))\n        self.forward(*feature_values)\n'"
tools/odps_table_tools/transform_kv_table.py,0,"b'import argparse\nimport time\n\nimport odps\nfrom odps import ODPS, options\n\noptions.log_view_host = ""http://logview.odps.aliyun-inc.com:8080""\n\nUDF_CLASS_NAME = ""KVFlatter""\nANALYZE_FEATURE_RECORDS_COUNT = 100\n\nINTER_KV_SEPARATOR = "":""\nINTRA_KV_SEPARATOR = "",""\n\nTRANSFORM_SQL_TEMPLATE = ""CREATE TABLE IF NOT EXISTS {output_table} LIFECYCLE 7 AS \\n\\\n    SELECT \\n\\\n        {udf} \\n\\\n    FROM {input_table}""\n\n\ndef get_feature_names(\n    odps_entry, table_name, partition, kv_column, inter_kv_sep, intra_kv_sep\n):\n    """""" Parse the feature names from records the a table\n    Args:\n        table_name: ODPS table name\n        partition (string): The table partition\n        kv_column (string): The key-value column name\n\n    Returns:\n        list: A list with feature names\n    """"""\n    source_kv_table = odps_entry.get_table(table_name)\n    key_names = set()\n    for record in source_kv_table.head(\n        ANALYZE_FEATURE_RECORDS_COUNT, partition=partition\n    ):\n        kv_dict = parse_kv_string_to_dict(\n            record[kv_column], inter_kv_sep, intra_kv_sep\n        )\n        key_names.update(kv_dict.keys())\n    return sorted(key_names)\n\n\ndef parse_kv_string_to_dict(kvs_string, inter_kv_sep, intra_kv_sep):\n    """"""Parse a kv string to a dict. For example,\n    ""key1:value1,key2:value2"" => {key1: value1, key2: value2}\n    """"""\n    kv_dict = {}\n    kv_pairs = kvs_string.split(inter_kv_sep)\n    for kv in kv_pairs:\n        key_and_value = kv.split(intra_kv_sep)\n        if len(key_and_value) == 2:\n            kv_dict[key_and_value[0]] = key_and_value[1]\n\n    return kv_dict\n\n\ndef generate_sql(\n    input_table,\n    input_table_partition,\n    output_table,\n    output_columns,\n    kv_column,\n    udf_function,\n    append_columns,\n    inter_kv_sep,\n    intra_kv_sep,\n):\n    """"""Generate an ODPS SQL to transform the table\n    Args:\n        input_table: input table name\n        input_table_partition: input table partition\n        output_table: output table name\n        output_columns (list): feature names\n        kv_column: kv column name\n        udf_function: udf function name\n        append_columns (list): Append column names.\n        inter_kv_sep: Inter separator in the Key-value pair string.\n        intra_kv_sep: Intra separator int Ker-value pairs string.\n    """"""\n    feature_names_str = "","".join(output_columns)\n    output_columns.extend(append_columns)\n    output_columns_str = "","".join(output_columns)\n    input_columns = [kv_column]\n    input_columns.extend(append_columns)\n    input_columns_str = "","".join(input_columns)\n\n    udf = """"""{udf}({input_col_str},\n    ""{features_str}"", ""{inter_sep}"", ""{intra_sep}"")\n    as ({output_col_str})"""""".format(\n        udf=udf_function,\n        input_col_str=input_columns_str,\n        features_str=feature_names_str,\n        output_col_str=output_columns_str,\n        inter_sep=inter_kv_sep,\n        intra_sep=intra_kv_sep,\n    )\n\n    sql = TRANSFORM_SQL_TEMPLATE.format(\n        output_table=output_table, udf=udf, input_table=input_table,\n    )\n    if input_table_partition is not None:\n        sql += "" where {}"".format(input_table_partition)\n    return sql\n\n\ndef exec_sql(odps_entry, sql):\n    print(""====> execute_sql: "" + sql)\n    instance = odps_entry.run_sql(sql)\n    print(""====> logview: "" + instance.get_logview_address())\n    instance.wait_for_success()\n\n\ndef create_udf_function(odps_entry, udf_file_path):\n    udf_resource = ""sqlflow_flat_{}.py"".format(int(time.time()))\n    udf_function = ""sqlflow_flat_func_{}"".format(int(time.time()))\n\n    delete_udf_resource(odps_entry, udf_resource)\n    resource = odps_entry.create_resource(\n        udf_resource, type=""py"", file_obj=open(udf_file_path)\n    )\n    print(""Create python resource: {}"".format(udf_resource))\n\n    delete_udf_function(odps_entry, udf_function)\n    class_type = udf_resource[0:-2] + UDF_CLASS_NAME\n    odps_entry.create_function(\n        udf_function, class_type=class_type, resources=[resource]\n    )\n\n    return udf_resource, udf_function\n\n\ndef delete_udf_resource(odps_entry, udf_resource):\n    try:\n        py_resource = odps_entry.get_resource(udf_resource)\n        if py_resource:\n            py_resource.drop()\n    except odps.errors.NoSuchObject:\n        pass\n    finally:\n        print(""Drop resource if exists {}"".format(udf_resource))\n\n\ndef delete_udf_function(odps_entry, udf_function):\n    try:\n        function = odps_entry.get_function(udf_function)\n        function.drop()\n    except odps.errors.NoSuchObject:\n        pass\n    finally:\n        print(""Drop function is exists {}"".format(udf_function))\n\n\ndef flat_to_wide_table(\n    odps_entry,\n    input_table,\n    kv_column,\n    output_table,\n    udf_file_path,\n    inter_kv_sep,\n    intra_kv_sep,\n    input_table_partition=None,\n    append_columns=None,\n):\n    """"""Transform the kv column to wide table\n    Args:\n        odps_entry: ODPS entry instance\n        input_table: The input table name.\n        kv_column: The key-value pairs column name.\n        output_table: The output table name.\n        udf_file_path: The python udf file path.\n        input_table_partition: The input table partition.\n        append_columns: The columns appended to output table.\n        inter_kv_sep: Inter separator in the Key-value pair string.\n        intra_kv_sep: Intra separator int Ker-value pairs string.\n    """"""\n    try:\n        udf_resource, udf_function = create_udf_function(\n            odps_entry, udf_file_path\n        )\n        odps_entry.delete_table(output_table, if_exists=True)\n        feature_names = get_feature_names(\n            odps_entry,\n            input_table,\n            input_table_partition,\n            kv_column,\n            inter_kv_sep,\n            intra_kv_sep,\n        )\n        sql = generate_sql(\n            input_table,\n            input_table_partition,\n            output_table,\n            feature_names,\n            kv_column,\n            udf_function,\n            append_columns,\n            inter_kv_sep,\n            intra_kv_sep,\n        )\n        exec_sql(odps_entry, sql)\n    finally:\n        delete_udf_function(odps_entry, udf_function)\n        delete_udf_resource(odps_entry, udf_resource)\n\n\ndef add_params(parser):\n    parser.add_argument(\n        ""--udf_file_path"",\n        default="""",\n        type=str,\n        help=""The path of udf python file"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--input_table"",\n        default="""",\n        type=str,\n        help=""The input odps table name"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--input_table_partition"",\n        default=None,\n        type=str,\n        help=""The partition of input table"",\n    )\n    parser.add_argument(\n        ""--kv_column"",\n        default="""",\n        type=str,\n        help=""The name of kv column to transform"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--output_table"",\n        default="""",\n        type=str,\n        help=""The output table name"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--append_columns"",\n        default=None,\n        type=str,\n        help=""the append columns to output table like \'id,label\'"",\n    )\n    parser.add_argument(\n        ""--inter_kv_separator"",\n        default=INTER_KV_SEPARATOR,\n        type=str,\n        help=""The inter key-value separator in a key-value pair string"",\n    )\n    parser.add_argument(\n        ""--intra_kv_separator"",\n        default=INTRA_KV_SEPARATOR,\n        type=str,\n        help=""The intra key-value pairs separator\'"",\n    )\n    parser.add_argument(\n        ""--MAXCOMPUTE_AK"",\n        default=None,\n        required=True,\n        type=str,\n        help=""The intra key-value pairs separator\'"",\n    )\n    parser.add_argument(\n        ""--MAXCOMPUTE_SK"",\n        default=None,\n        required=True,\n        type=str,\n        help=""The intra key-value pairs separator\'"",\n    )\n    parser.add_argument(\n        ""--MAXCOMPUTE_PROJECT"",\n        default=None,\n        required=True,\n        type=str,\n        help=""The intra key-value pairs separator\'"",\n    )\n    parser.add_argument(\n        ""--MAXCOMPUTE_ENDPOINT"",\n        default=None,\n        type=str,\n        help=""The intra key-value pairs separator\'"",\n    )\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    add_params(parser)\n    args, _ = parser.parse_known_args()\n\n    odps_entry = ODPS(\n        access_id=args.MAXCOMPUTE_AK,\n        secret_access_key=args.MAXCOMPUTE_SK,\n        project=args.MAXCOMPUTE_PROJECT,\n        endpoint=args.MAXCOMPUTE_ENDPOINT,\n    )\n\n    append_columns = (\n        args.append_columns.strip().split("","")\n        if args.append_columns is not None\n        else None\n    )\n\n    flat_to_wide_table(\n        odps_entry,\n        args.input_table,\n        args.kv_column,\n        args.output_table,\n        args.udf_file_path,\n        args.inter_kv_separator,\n        args.intra_kv_separator,\n        args.input_table_partition,\n        append_columns,\n    )\n'"
elasticdl/python/collective_ops/__init__.py,0,b''
elasticdl/python/collective_ops/communicator.py,1,"b'import socket\n\nfrom elasticdl.python.common.constants import CollectiveCommunicatorStatus\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\ntry:\n    from ftlib import BasicFTLib\n    from ftlib.commlib.commlib_status import CommLibStatus\n    from ftlib.ftlib_status import FTCollectiveStatus\n\n    _FTLIB_INSTALLED = True\nexcept ImportError:\n    BasicFTLib = object\n    FTCollectiveStatus = object\n    _FTLIB_INSTALLED = False\n\n\n_SUPPORTED_ALLREDUCE_OPS = [""MEAN""]\n_FTLIB_UNINSTALLED_DEFAULT_STATUS_MESSAGE = (\n    ""FTLib is not installed. Default to succeeded for testing purposes""\n)\n\n\nclass CollectiveCommunicator(object):\n    def __init__(self, service_name=None):\n        if _FTLIB_INSTALLED:\n            peer_list = list(self._get_peer_set(service_name))\n            self._ftlib = BasicFTLib(\n                consensus=""gossip"",\n                commlib=""pytorch"",\n                consensus_init_kwargs={\n                    ""known_addr_list"": peer_list,\n                    ""custom_bind_addr"": socket.gethostbyname(\n                        socket.gethostname()\n                    ),\n                },\n            )\n            while peer_list and not self._ftlib.consensus_joined():\n                logger.warning(""Retry building consensus..."")\n                self._ftlib.manual_join(\n                    known_addr_list=list(self._get_peer_set(service_name))\n                )\n        else:\n            logger.warning(\n                ""FTLib is not installed. The CollectiveCommunicator ""\n                ""may not work as expected""\n            )\n            self._ftlib = None\n\n    def tf_allreduce(self, grads, op=""MEAN""):\n        if grads is None:\n            logger.error(""Grads is required for tf_allreduce operation"")\n            return CollectiveCommunicatorStatus.FAILED, grads\n        # convert tf.Tensor to numpy\n        numpy_data = [g.numpy() for g in grads]\n        return self.allreduce(numpy_data, op)\n\n    def allreduce(self, data, op=""MEAN""):\n        if data is None:\n            logger.error(""Data is required for allreduce operation"")\n            return CollectiveCommunicatorStatus.FAILED, data\n        if op not in _SUPPORTED_ALLREDUCE_OPS:\n            logger.error(\n                ""%s is not in list of supported allreduce operations: %s""\n                % (op, _SUPPORTED_ALLREDUCE_OPS)\n            )\n            return CollectiveCommunicatorStatus.FAILED, data\n        if self._ftlib is not None:\n            status, res = self._ftlib.wait_gradients_ready(params=data)\n            if (\n                status == FTCollectiveStatus.SUCCESS\n                and res == CommLibStatus.SUCCESS\n                or status == FTCollectiveStatus.NO_NEED\n            ):\n                return CollectiveCommunicatorStatus.SUCCEEDED, data\n            else:\n                return CollectiveCommunicatorStatus.FAILED, data\n        else:\n            logger.warning(_FTLIB_UNINSTALLED_DEFAULT_STATUS_MESSAGE)\n            return CollectiveCommunicatorStatus.SUCCEEDED, data\n\n    def tf_broadcast(self, params, src_rank):\n        for p in params:\n            data = p.numpy()\n            status, data = self.broadcast(p.numpy(), src_rank)\n            if status == CollectiveCommunicatorStatus.SUCCEEDED:\n                p.assign(data)\n            else:\n                return status\n        return CollectiveCommunicatorStatus.SUCCEEDED\n\n    def broadcast(self, data, src_rank):\n        if self._ftlib is not None:\n            status, _ = self._ftlib.broadcast(data, src_rank)\n            if status == FTCollectiveStatus.SUCCESS:\n                return CollectiveCommunicatorStatus.SUCCEEDED, data\n            else:\n                return CollectiveCommunicatorStatus.FAILED, data\n        else:\n            logger.warning(_FTLIB_UNINSTALLED_DEFAULT_STATUS_MESSAGE)\n            return CollectiveCommunicatorStatus.SUCCEEDED, data\n\n    def barrier(self):\n        if self._ftlib is not None:\n            status, _ = self._ftlib.barrier()\n            if status == FTCollectiveStatus.SUCCESS:\n                return CollectiveCommunicatorStatus.SUCCEEDED\n            else:\n                return CollectiveCommunicatorStatus.FAILED\n        else:\n            logger.warning(_FTLIB_UNINSTALLED_DEFAULT_STATUS_MESSAGE)\n            return CollectiveCommunicatorStatus.SUCCEEDED\n\n    def is_initialized(self):\n        """"""This will be `False` under three occasions:\n           * New workers report joining in\n           * Collective-communication operations fail or time out\n           * Liveness probe fails for existing workers\n        """"""\n        if self._ftlib is not None:\n            return self._ftlib.initialized\n        else:\n            return True\n\n    def _get_peer_set(self, svc_name):\n        if svc_name is None:\n            return None\n        my_ip = socket.gethostbyname(socket.gethostname())\n        temp_set = socket.getaddrinfo(svc_name, 0, proto=socket.IPPROTO_TCP)\n        peer_set = {peer[-1][0] for peer in temp_set if peer[-1][0] != my_ip}\n        return peer_set\n'"
elasticdl/python/common/__init__.py,0,b''
elasticdl/python/common/args.py,0,"b'import argparse\nfrom itertools import chain\n\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\nMODEL_SPEC_GROUP = [\n    ""dataset_fn"",\n    ""eval_metrics_fn"",\n    ""model_def"",\n    ""model_params"",\n    ""optimizer"",\n    ""loss"",\n    ""output"",\n    ""minibatch_size"",\n    ""grads_to_wait"",\n    ""num_epochs"",\n    ""tensorboard_log_dir"",\n    ""training_data"",\n]\n\nEVALUATION_GROUP = [\n    ""evaluation_steps"",\n    ""validation_data"",\n    ""evaluation_start_delay_secs"",\n    ""evaluation_throttle_secs"",\n]\n\nPREDICTION_GROUP = [""prediction_data"", ""prediction_outputs_processor""]\n\nCHECKPOINT_GROUP = [\n    ""checkpoint_dir_for_init"",\n    ""checkpoint_steps"",\n    ""keep_checkpoint_max"",\n    ""checkpoint_dir"",\n]\n\nALL_ARGS_GROUPS = [\n    MODEL_SPEC_GROUP,\n    EVALUATION_GROUP,\n    PREDICTION_GROUP,\n    CHECKPOINT_GROUP,\n]\n\n\ndef pos_int(arg):\n    res = int(arg)\n    if res <= 0:\n        raise ValueError(""Positive integer argument required. Got %s"" % res)\n    return res\n\n\ndef non_neg_int(arg):\n    res = int(arg)\n    if res < 0:\n        raise ValueError(\n            ""Non-negative integer argument required. Get %s"" % res\n        )\n    return res\n\n\ndef parse_envs(arg):\n    """"""Parse environment configs as a dict.\n\n    Support format \'k1=v1,k2=v2,k3=v3..\'. Note that comma is supported\n    in value field.\n    """"""\n    envs = {}\n    if not arg:\n        return envs\n\n    i = 0\n    fields = arg.split(""="")\n    if len(fields) < 2:\n        return envs\n    pre_key = """"\n    while i < len(fields):\n        if i == 0:\n            pre_key = fields[i]\n        elif i == len(fields) - 1:\n            envs[pre_key] = fields[i]\n        else:\n            r = fields[i].rfind("","")\n            envs[pre_key] = fields[i][:r]\n            pre_key = fields[i][r + 1 :]  # noqa: E203\n        i += 1\n    return envs\n\n\ndef add_bool_param(parser, name, default, help):\n    parser.add_argument(\n        name,  # should be in ""--foo"" format\n        nargs=""?"",\n        const=not default,\n        default=default,\n        type=lambda x: x.lower() in [""true"", ""yes"", ""t"", ""y""],\n        help=help,\n    )\n\n\ndef add_common_params(parser):\n    """"""Common arguments for training/prediction/evaluation""""""\n    add_common_args_between_master_and_worker(parser)\n    parser.add_argument(\n        ""--docker_image_repository"",\n        default="""",\n        help=""The repository for generated Docker images, if set, the image ""\n        ""is also pushed to the repository"",\n    )\n    parser.add_argument(\n        ""--image_base"",\n        default="""",\n        help=""Base Docker image. If set, a new image will be built each time""\n        ""while submitting the Elastic job."",\n    )\n    parser.add_argument(\n        ""--image_name"",\n        default="""",\n        help=""The pre-built image for this job. If set, ""\n        ""use this image instead of building a new one."",\n    )\n    parser.add_argument(""--job_name"", help=""ElasticDL job name"", required=True)\n    parser.add_argument(\n        ""--master_resource_request"",\n        default=""cpu=0.1,memory=1024Mi"",\n        type=str,\n        help=""The minimal resource required by master, ""\n        ""e.g. cpu=0.1,memory=1024Mi,disk=1024Mi,gpu=1"",\n    )\n    parser.add_argument(\n        ""--master_resource_limit"",\n        type=str,\n        default="""",\n        help=""The maximal resource required by master, ""\n        ""e.g. cpu=0.1,memory=1024Mi,disk=1024Mi,gpu=1, ""\n        ""default to master_resource_request"",\n    )\n    parser.add_argument(\n        ""--num_workers"", type=int, help=""Number of workers"", default=0\n    )\n    parser.add_argument(\n        ""--worker_resource_request"",\n        default=""cpu=1,memory=4096Mi"",\n        type=str,\n        help=""The minimal resource required by worker, ""\n        ""e.g. cpu=1,memory=1024Mi,disk=1024Mi,gpu=1"",\n    )\n    parser.add_argument(\n        ""--worker_resource_limit"",\n        type=str,\n        default="""",\n        help=""The maximal resource required by worker, ""\n        ""e.g. cpu=1,memory=1024Mi,disk=1024Mi,gpu=1,""\n        ""default to worker_resource_request"",\n    )\n    parser.add_argument(\n        ""--master_pod_priority"",\n        default="""",\n        help=""The requested priority of master pod"",\n    )\n    parser.add_argument(\n        ""--worker_pod_priority"",\n        default="""",\n        help=""The requested priority of worker pod, we support following""\n        ""configs: high/low/high=0.5. The high=0.5 means that half""\n        ""worker pods have high priority, and half worker pods have""\n        ""low priority. The default value is low"",\n    )\n    parser.add_argument(\n        ""--num_ps_pods"", type=int, help=""Number of PS pods"", default=1\n    )\n    parser.add_argument(\n        ""--ps_resource_request"",\n        default=""cpu=1,memory=4096Mi"",\n        type=str,\n        help=""The minimal resource required by worker, ""\n        ""e.g. cpu=1,memory=1024Mi,disk=1024Mi,gpu=1"",\n    )\n    parser.add_argument(\n        ""--ps_resource_limit"",\n        default="""",\n        type=str,\n        help=""The maximal resource required by worker, ""\n        ""e.g. cpu=1,memory=1024Mi,disk=1024Mi,gpu=1,""\n        ""default to worker_resource_request"",\n    )\n    parser.add_argument(\n        ""--ps_pod_priority"",\n        default="""",\n        help=""The requested priority of PS pod"",\n    )\n    parser.add_argument(\n        ""--volume"",\n        default="""",\n        type=str,\n        help=""The Kubernetes volume information, ""\n        ""the supported volumes are `persistentVolumeClaim` and `hostPath`,""\n        \'e.g. ""claim_name=c1,mount_path=/path1"" for `persistentVolumeClaim`,\'\n        \'""host_path=c0,mount_path=/path0"" for `hostPath`,\'\n        \'or ""host_path=c0,mount_path=/path0,type=Directory"" for `hostPath`,\'\n        \'""host_path=c0,mount_path=/path0;claim_name=c1,mount_path=/path1"" for\'\n        ""multiple volumes"",\n    )\n    parser.add_argument(\n        ""--image_pull_policy"",\n        default=""Always"",\n        help=""The image pull policy of master and worker"",\n        choices=[""Never"", ""IfNotPresent"", ""Always""],\n    )\n    parser.add_argument(\n        ""--restart_policy"",\n        default=""Never"",\n        help=""The pod restart policy when pod crashed"",\n        choices=[""Never"", ""OnFailure"", ""Always""],\n    )\n    parser.add_argument(\n        ""--envs"",\n        type=str,\n        default="""",\n        help=""Runtime environment variables. (key1=value1,key2=value2), ""\n        ""comma is supported in value field"",\n    )\n    parser.add_argument(\n        ""--extra_pypi_index"",\n        default=""https://pypi.org/simple"",\n        help=""The extra URLs of Python package repository indexes"",\n    )\n    parser.add_argument(\n        ""--namespace"",\n        default=""default"",\n        type=str,\n        help=""The name of the Kubernetes namespace where ElasticDL ""\n        ""pods will be created"",\n    )\n    parser.add_argument(\n        ""--num_minibatches_per_task"",\n        type=int,\n        help=""The number of minibatches per task"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--cluster_spec"",\n        help=""The file that contains user-defined cluster specification,""\n        ""the file path can be accessed by ElasticDL client."",\n        default="""",\n    )\n    parser.add_argument(\n        ""--docker_base_url"",\n        help=""URL to the Docker server"",\n        default=""unix://var/run/docker.sock"",\n    )\n    parser.add_argument(\n        ""--docker_tlscert"", help=""Path to Docker client cert"", default=""""\n    )\n    parser.add_argument(\n        ""--docker_tlskey"", help=""Path to Docker client key"", default=""""\n    )\n    parser.add_argument(\n        ""--yaml"",\n        type=str,\n        default="""",\n        help=""File path for dumping ElasticDL job YAML specification. ""\n        ""Note that, if users specify --yaml, the client wouldn\'t submit ""\n        ""the job automatically, and users need to launch the job through ""\n        ""command `kubectl create -f path_to_yaml_file`."",\n    )\n    add_bool_param(\n        parser=parser,\n        name=""--force_use_kube_config_file"",\n        default=False,\n        help=""If true, force to load the cluster config from ~/.kube/config ""\n        ""while submitting the ElasticDL job. Otherwise, if the client is in a ""\n        ""K8S environment, load the incluster config, if not, load the kube ""\n        ""config file."",\n    )\n    # delete this argument after finishing Go-based PS implementation\n    add_bool_param(\n        parser=parser,\n        name=""--use_go_ps"",\n        default=False,\n        help=""True for Go-based PS, False for Python-based PS"",\n    )\n    parser.add_argument(\n        ""--aux_params"",\n        type=str,\n        default="""",\n        help=""Auxiliary parameters for misc purposes such as debugging.""\n        ""The auxiliary parameters in a string separated ""\n        \'by semi-colon used to debug , e.g. ""param1=1; param2=2"" \'\n        ""Supported auxiliary parameters: disable_relaunch"",\n    )\n    parser.add_argument(\n        ""--log_file_path"",\n        type=str,\n        default="""",\n        help=""The path to save logs (e.g. stdout, stderr)"",\n    )\n\n\ndef add_train_params(parser):\n    parser.add_argument(\n        ""--tensorboard_log_dir"",\n        default="""",\n        type=str,\n        help=""Directory where TensorBoard will look to find ""\n        ""TensorFlow event files that it can display. ""\n        ""TensorBoard will recursively walk the directory ""\n        ""structure rooted at log dir, looking for .*tfevents.* ""\n        ""files. You may also pass a comma separated list of log ""\n        ""directories, and TensorBoard will watch each ""\n        ""directory."",\n    )\n    parser.add_argument(""--num_epochs"", type=int, default=1)\n    parser.add_argument(\n        ""--grads_to_wait"",\n        type=int,\n        help=""Number of gradients to wait before updating model"",\n        default=1,\n    )\n    parser.add_argument(\n        ""--training_data"",\n        help=""Either the data directory that contains RecordIO files ""\n        ""or an ODPS table name used for training."",\n        default="""",\n    )\n    parser.add_argument(\n        ""--validation_data"",\n        help=""Either the data directory that contains RecordIO files ""\n        ""or an ODPS table name used for evaluation."",\n        default="""",\n    )\n    parser.add_argument(\n        ""--evaluation_steps"",\n        type=int,\n        help=""Evaluate the model every this many steps.""\n        ""If 0, step-based evaluation is disabled"",\n        default=0,\n    )\n    parser.add_argument(\n        ""--evaluation_start_delay_secs"",\n        type=int,\n        help=""Start time-based evaluation only after waiting for ""\n        ""this many seconds"",\n        default=100,\n    )\n    parser.add_argument(\n        ""--evaluation_throttle_secs"",\n        type=int,\n        help=""Do not re-evaluate unless the last evaluation was started ""\n        ""at least this many seconds ago.""\n        ""If 0, time-based evaluation is disabled"",\n        default=0,\n    )\n    parser.add_argument(\n        ""--checkpoint_dir_for_init"",\n        help=""The checkpoint directory to initialize the training model"",\n        default="""",\n    )\n    parser.add_argument(\n        ""--sync_version_tolerance"",\n        type=int,\n        help=""The maximum model version difference between reported gradients ""\n        ""and PS that synchronous SGD can accepts."",\n        default=0,\n    )\n    parser.add_argument(\n        ""--log_loss_steps"",\n        type=int,\n        help=""The frequency, in number of global steps, that the global step ""\n        ""and the loss will be logged during training."",\n        default=100,\n    )\n    add_bool_param(\n        parser=parser,\n        name=""--use_async"",\n        default=False,\n        help=""True for asynchronous SGD, False for synchronous SGD"",\n    )\n    add_bool_param(\n        parser=parser,\n        name=""--lr_staleness_modulation"",\n        default=False,\n        help=""If True, PS will modulate the learning rate with staleness ""\n        ""in asynchronous SGD"",\n    )\n\n\ndef add_evaluate_params(parser):\n    parser.add_argument(\n        ""--validation_data"",\n        help=""Either the data directory that contains RecordIO files ""\n        ""or an ODPS table name used for evaluation."",\n        required=True,\n    )\n    parser.add_argument(\n        ""--checkpoint_dir_for_init"",\n        help=""The checkpoint directory to initialize the training model"",\n        required=True,\n    )\n\n\ndef add_predict_params(parser):\n    parser.add_argument(\n        ""--prediction_data"",\n        help=""Either the data directory that contains RecordIO files ""\n        ""or an ODPS table name used for prediction."",\n        required=True,\n    )\n    parser.add_argument(\n        ""--prediction_outputs_processor"",\n        help=""The name of the prediction output processor class ""\n        ""defined in the model definition file."",\n        default=""PredictionOutputsProcessor"",\n    )\n    parser.add_argument(\n        ""--checkpoint_dir_for_init"",\n        help=""The checkpoint directory to initialize the training model"",\n        required=True,\n    )\n\n\ndef add_clean_params(parser):\n    parser.add_argument(\n        ""--docker_image_repository"",\n        type=str,\n        help=""Clean docker images belonging to this repository."",\n    )\n    parser.add_argument(\n        ""--all"", action=""store_true"", help=""Clean all local docker images""\n    )\n    parser.add_argument(\n        ""--docker_base_url"",\n        help=""URL to the Docker server"",\n        default=""unix://var/run/docker.sock"",\n    )\n    parser.add_argument(\n        ""--docker_tlscert"", help=""Path to Docker client cert"", default=""""\n    )\n    parser.add_argument(\n        ""--docker_tlskey"", help=""Path to Docker client key"", default=""""\n    )\n\n\ndef print_args(args, groups=None):\n    """"""\n    Args:\n        args: parsing results returned from `parser.parse_args`\n        groups: It is a list of a list. It controls which options should be\n        printed together. For example, we expect all model specifications such\n        as `optimizer`, `loss` are better printed together.\n        groups = [[""optimizer"", ""loss""]]\n    """"""\n\n    def _get_attr(instance, attribute):\n        try:\n            return getattr(instance, attribute)\n        except AttributeError:\n            return None\n\n    dedup = set()\n    if groups:\n        for group in groups:\n            for element in group:\n                dedup.add(element)\n                logger.info(""%s = %s"", element, _get_attr(args, element))\n    other_options = [\n        (key, value)\n        for (key, value) in args.__dict__.items()\n        if key not in dedup\n    ]\n    for key, value in other_options:\n        logger.info(""%s = %s"", key, value)\n\n\ndef add_common_args_between_master_and_worker(parser):\n    parser.add_argument(\n        ""--minibatch_size"",\n        help=""Minibatch size for worker"",\n        type=int,\n        required=True,\n    )\n    parser.add_argument(\n        ""--model_zoo"",\n        help=""The directory that contains user-defined model files ""\n        ""or a specific model file. If set `image_base`, the path should""\n        ""be accessed by ElasticDL client. If set `image_name`, it is""\n        ""the path inside this pre-built image."",\n        required=True,\n    )\n    parser.add_argument(\n        ""--log_level"",\n        choices=[""DEBUG"", ""INFO"", ""WARNING"", ""ERROR"", ""CRITICAL""],\n        type=str.upper,\n        default=""INFO"",\n        help=""Set the logging level"",\n    )\n    parser.add_argument(\n        ""--dataset_fn"",\n        type=str,\n        default=""dataset_fn"",\n        help=""The name of the dataset function defined in the model file"",\n    )\n    parser.add_argument(\n        ""--loss"",\n        type=str,\n        default=""loss"",\n        help=""The name of the loss function defined in the model file"",\n    )\n    parser.add_argument(\n        ""--optimizer"",\n        type=str,\n        default=""optimizer"",\n        help=""The name of the optimizer defined in the model file"",\n    )\n    parser.add_argument(\n        ""--callbacks"",\n        type=str,\n        default=""callbacks"",\n        help=""Optional function to add callbacks to behavior during""\n        ""training, evaluation and inference."",\n    )\n    parser.add_argument(\n        ""--eval_metrics_fn"",\n        type=str,\n        default=""eval_metrics_fn"",\n        help=""The name of the evaluation metrics function defined ""\n        ""in the model file"",\n    )\n    parser.add_argument(\n        ""--custom_data_reader"",\n        type=str,\n        default=""custom_data_reader"",\n        help=""The custom data reader defined in the model file"",\n    )\n    parser.add_argument(\n        ""--model_def"",\n        type=str,\n        required=True,\n        help=""The import path to the model definition function/class in the ""\n        \'model zoo, e.g. ""cifar10_subclass.cifar10_subclass.CustomModel""\',\n    )\n    parser.add_argument(\n        ""--model_params"",\n        type=str,\n        default="""",\n        help=""The model parameters in a string separated by semi-colon ""\n        \'used to instantiate the model, e.g. ""param1=1; param2=2""\',\n    )\n    parser.add_argument(\n        ""--get_model_steps"",\n        type=int,\n        default=1,\n        help=""Worker will get_model from PS every this many steps"",\n    )\n    parser.add_argument(\n        ""--data_reader_params"",\n        type=str,\n        default="""",\n        help=""The data reader parameters in a string separated by semi-colon ""\n        \'used to instantiate the data reader, e.g. ""param1=1; param2=2""\',\n    )\n    parser.add_argument(\n        ""--distribution_strategy"",\n        type=str,\n        choices=[\n            """",\n            DistributionStrategy.LOCAL,\n            DistributionStrategy.PARAMETER_SERVER,\n            DistributionStrategy.ALLREDUCE,\n        ],\n        default="""",\n        help=""Master will use a distribution policy on a list of devices ""\n        ""according to the distributed strategy, ""\n        \'e.g. ""ParameterServerStrategy"" or ""AllreduceStrategy"" or ""Local""\',\n    )\n    parser.add_argument(\n        ""--checkpoint_steps"",\n        type=int,\n        help=""Save checkpoint every this many steps.""\n        ""If 0, no checkpoints to save."",\n        default=0,\n    )\n    parser.add_argument(\n        ""--checkpoint_dir"",\n        help=""The directory to store the checkpoint files"",\n        default="""",\n    )\n    parser.add_argument(\n        ""--keep_checkpoint_max"",\n        type=int,\n        help=""The maximum number of recent checkpoint files to keep.""\n        ""If 0, keep all."",\n        default=0,\n    )\n    parser.add_argument(\n        ""--output"",\n        type=str,\n        default="""",\n        help=""The path to save the final trained model"",\n    )\n\n\ndef parse_master_args(master_args=None):\n    parser = argparse.ArgumentParser(description=""ElasticDL Master"")\n    parser.add_argument(\n        ""--port"",\n        default=50001,\n        type=pos_int,\n        help=""The listening port of master"",\n    )\n    parser.add_argument(\n        ""--worker_image"", help=""Docker image for workers"", default=None\n    )\n    parser.add_argument(\n        ""--prediction_data"",\n        help=""Either the data directory that contains RecordIO files ""\n        ""or an ODPS table name used for prediction."",\n        default="""",\n    )\n    add_common_params(parser)\n    add_train_params(parser)\n\n    args, unknown_args = parser.parse_known_args(args=master_args)\n    print_args(args, groups=ALL_ARGS_GROUPS)\n    if unknown_args:\n        logger.warning(""Unknown arguments: %s"", unknown_args)\n\n    if all(\n        v == """" or v is None\n        for v in [\n            args.training_data,\n            args.validation_data,\n            args.prediction_data,\n        ]\n    ):\n        raise ValueError(\n            ""At least one of the data directories needs to be provided""\n        )\n\n    if args.prediction_data and (args.training_data or args.validation_data):\n        raise ValueError(\n            ""Running prediction together with training or evaluation ""\n            ""is not supported""\n        )\n    if args.prediction_data and not args.checkpoint_dir_for_init:\n        raise ValueError(\n            ""checkpoint_dir_for_init is required for running "" ""prediction job""\n        )\n    if not args.use_async and args.get_model_steps > 1:\n        args.get_model_steps = 1\n        logger.warning(\n            ""get_model_steps is set to 1 when using synchronous SGD.""\n        )\n    if args.use_async and args.grads_to_wait > 1:\n        args.grads_to_wait = 1\n        logger.warning(\n            ""grads_to_wait is set to 1 when using asynchronous SGD.""\n        )\n\n    return args\n\n\ndef parse_ps_args(ps_args=None):\n    parser = argparse.ArgumentParser(description=""ElasticDL PS"")\n    parser.add_argument(\n        ""--ps_id"", help=""ID unique to the PS"", type=int, required=True\n    )\n    parser.add_argument(\n        ""--port"", help=""Port used by the PS pod"", type=int, required=True\n    )\n    parser.add_argument(""--master_addr"", help=""Master ip:port"")\n\n    add_common_params(parser)\n    add_train_params(parser)\n    # TODO: add PS replica address for RPC stub creation\n\n    args, unknown_args = parser.parse_known_args(args=ps_args)\n    print_args(args, groups=ALL_ARGS_GROUPS)\n    if unknown_args:\n        logger.warning(""Unknown arguments: %s"", unknown_args)\n    if args.use_async and args.grads_to_wait > 1:\n        args.grads_to_wait = 1\n        logger.warning(\n            ""grads_to_wait is set to 1 when using asynchronous SGD.""\n        )\n    return args\n\n\ndef parse_worker_args(worker_args=None):\n    parser = argparse.ArgumentParser(description=""ElasticDL Worker"")\n    add_common_args_between_master_and_worker(parser)\n    add_train_params(parser)\n    parser.add_argument(\n        ""--worker_id"", help=""ID unique to the worker"", type=int, required=True\n    )\n    parser.add_argument(""--job_type"", help=""Job type"", required=True)\n    parser.add_argument(""--master_addr"", help=""Master ip:port"")\n    parser.add_argument(\n        ""--prediction_outputs_processor"",\n        help=""The name of the prediction output processor class ""\n        ""defined in the model definition file."",\n        default=""PredictionOutputsProcessor"",\n    )\n    parser.add_argument(\n        ""--ps_addrs"",\n        type=str,\n        help=""Addresses of parameter service pods, separated by comma"",\n    )\n    parser.add_argument(\n        ""--collective_communicator_service_name"",\n        default="""",\n        type=str,\n        help=""The name of the collective communicator k8s service for ""\n        ""allreduce-based training"",\n    )\n\n    if worker_args:\n        worker_args = list(map(str, worker_args))\n    args, unknown_args = parser.parse_known_args(args=worker_args)\n    print_args(args, groups=ALL_ARGS_GROUPS)\n    if unknown_args:\n        logger.warning(""Unknown arguments: %s"", unknown_args)\n    if args.distribution_strategy == DistributionStrategy.ALLREDUCE:\n        args.ps_addrs = """"\n    return args\n\n\ndef build_arguments_from_parsed_result(args, filter_args=None):\n    """"""Reconstruct arguments from parsed result\n    Args:\n        args: result from `parser.parse_args()`\n    Returns:\n        list of string: ready for parser to parse,\n        such as [""--foo"", ""3"", ""--bar"", False]\n    """"""\n    items = vars(args).items()\n    if filter_args:\n        items = filter(lambda item: item[0] not in filter_args, items)\n\n    def _str_ignore_none(s):\n        if s is None:\n            return s\n        return str(s)\n\n    arguments = map(_str_ignore_none, chain(*items))\n    arguments = [\n        ""--"" + k if i % 2 == 0 else k for i, k in enumerate(arguments)\n    ]\n    return arguments\n\n\ndef wrap_python_args_with_string(args):\n    """"""Wrap argument values with string\n    Args:\n        args: list like [""--foo"", ""3"", ""--bar"", False]\n\n    Returns:\n        list of string: like [""--foo"", ""\'3\'"", ""--bar"", ""\'False\'""]\n    """"""\n    result = []\n    for value in args:\n        if ""--"" not in value:\n            result.append(""\'{}\'"".format(value))\n        else:\n            result.append(value)\n    return result\n\n\ndef wrap_go_args_with_string(args):\n    """"""Wrap argument values with string\n    Args:\n        args: list like [""--foo=3"", ""--bar=False""]\n\n    Returns:\n        list of string: like [""--foo=\'3\'"", ""--bar=\'False\'""]\n    """"""\n    result = []\n    for value in args:\n        equal_mark_index = value.index(""="")\n        arg_value_index = equal_mark_index + 1\n        result.append(\n            value[0:equal_mark_index] + ""=\'{}\'"".format(value[arg_value_index:])\n        )\n    return result\n'"
elasticdl/python/common/constants.py,0,"b'class GRPC(object):\n    # gRPC limits the size of message by default to 4MB.\n    # It\'s too small to send model parameters.\n    MAX_SEND_MESSAGE_LENGTH = 256 * 1024 * 1024\n    MAX_RECEIVE_MESSAGE_LENGTH = 256 * 1024 * 1024\n\n\nclass InstanceManagerStatus(object):\n    PENDING = ""Pending""\n    RUNNING = ""Running""\n    FINISHED = ""Finished""\n\n\nclass MaxComputeConfig(object):\n    PROJECT_NAME = ""MAXCOMPUTE_PROJECT""\n    ACCESS_ID = ""MAXCOMPUTE_AK""\n    ACCESS_KEY = ""MAXCOMPUTE_SK""\n    ENDPOINT = ""MAXCOMPUTE_ENDPOINT""\n    TUNNEL_ENDPOINT = ""MAXCOMPUTE_TUNNEL_ENDPOINT""\n\n\nclass JobType(object):\n    TRAINING_ONLY = ""training_only""\n    EVALUATION_ONLY = ""evaluation_only""\n    PREDICTION_ONLY = ""prediction_only""\n    TRAINING_WITH_EVALUATION = ""training_with_evaluation""\n\n\nclass Mode(object):\n    TRAINING = ""training""\n    EVALUATION = ""evaluation""\n    PREDICTION = ""prediction""\n\n\nclass Redis(object):\n    MAX_COMMAND_RETRY_TIMES = 10\n\n\nclass MetricsDictKey(object):\n    MODEL_OUTPUT = ""output""\n    LABEL = ""label""\n\n\nclass DistributionStrategy(object):\n    LOCAL = ""Local""\n    PARAMETER_SERVER = ""ParameterServerStrategy""\n    ALLREDUCE = ""AllreduceStrategy""\n\n\nclass SaveModelConfig(object):\n    SAVED_MODEL_PATH = ""saved_model_path""\n\n\nclass TaskExecCounterKey(object):\n    FAIL_COUNT = ""fail_count""\n\n\nclass CollectiveCommunicatorStatus(object):\n    SUCCEEDED = ""succeeded""\n    FAILED = ""failed""\n\n\nclass PodStatus(object):\n    SUCCEEDED = ""Succeeded""\n    FAILED = ""Failed""\n    RUNNING = ""Running""\n    FINISHED = ""Finished""\n    PENDING = ""Pending""\n\n\nclass ReaderType(object):\n    CSV_READER = ""CSV""\n    ODPS_READER = ""ODPS""\n    RECORDIO_READER = ""RecordIO""\n\n\nclass BashCommandTemplate(object):\n    REDIRECTION = "" 2>&1 | tee {}""\n    SET_PIPEFAIL = ""set -o pipefail;""\n'"
elasticdl/python/common/dtypes.py,0,"b'import numpy as np\nfrom odps import types\nfrom tensorflow.core.framework import types_pb2\n\n\ndef dtype_tensor_to_numpy(dtype):\n    """"""Convert tensor dtype to numpy dtype object.""""""\n    np_dtype_name = _DT_TENSOR_TO_NP.get(dtype, None)\n    if not np_dtype_name:\n        raise ValueError(""Got wrong tensor PB dtype %s."", dtype)\n    return np.dtype(np_dtype_name)\n\n\ndef dtype_numpy_to_tensor(dtype):\n    """"""Convert numpy dtype object to tensor dtype.""""""\n    return _DT_NP_TO_TENSOR.get(dtype.type, types_pb2.DT_INVALID)\n\n\ndef is_numpy_dtype_allowed(dtype):\n    return dtype.type in _DT_NP_TO_TENSOR\n\n\n_DT_TENSOR_TO_NP = {\n    types_pb2.DT_INT8: np.int8,\n    types_pb2.DT_INT16: np.int16,\n    types_pb2.DT_INT32: np.int32,\n    types_pb2.DT_INT64: np.int64,\n    types_pb2.DT_FLOAT: np.float32,\n    types_pb2.DT_DOUBLE: np.float64,\n    types_pb2.DT_BOOL: np.bool,\n}\n\n_DT_NP_TO_TENSOR = {\n    np.int8: types_pb2.DT_INT8,\n    np.int16: types_pb2.DT_INT16,\n    np.int32: types_pb2.DT_INT32,\n    np.int64: types_pb2.DT_INT64,\n    np.float32: types_pb2.DT_FLOAT,\n    np.float64: types_pb2.DT_DOUBLE,\n    np.bool: types_pb2.DT_BOOL,\n}\n\n# TODO: There are many dtypes in MaxCompute and we can add them if needed.\nMAXCOMPUTE_DTYPE_TO_TF_DTYPE = {\n    types.bigint: types_pb2.DT_INT64,\n    types.double: types_pb2.DT_DOUBLE,\n    types.string: types_pb2.DT_STRING,\n}\n'"
elasticdl/python/common/evaluation_utils.py,5,"b'import numpy as np\nfrom tensorflow.python.keras import metrics as metrics_module\n\nfrom elasticdl.python.common.constants import MetricsDictKey\n\n\nclass EvaluationMetrics(object):\n    """"""Evaluation Metrics""""""\n\n    def __init__(self, metrics_dict):\n        """"""\n        Args:\n            metrics_dict: A python dictionary. If model has only one output,\n                `metrics_dict` is a dictionary of `{metric_name: metric}`,\n                i.e. `{""acc"": tf.keras.metrics.Accuracy()}`.\n                If model has multiple outputs, `metric_dict` is a dictionary of\n                `{output_name: {metric_name: metric}}`,\n                i.e. `{\n                    ""output_a"": {""acc"": tf.keras.metrics.Accuracy()},\n                    ""output_b"": {""auc"": tf.keras.metrics.AUC()},\n                }`. Note that for model with multiple outputs, each metric\n                only uses one output.\n        """"""\n        self._init_metrics_dict(metrics_dict)\n\n    def _init_metrics_dict(self, metrics_dict):\n        if not metrics_dict:\n            raise ValueError(\n                ""Evaluation metrics dictionary must not be empty.""\n            )\n        first_metrics = list(metrics_dict.values())[0]\n        if isinstance(first_metrics, dict):\n            self._model_have_multiple_outputs = True\n            self._metrics_dict = metrics_dict\n        else:\n            # When model has only one output, save it in a dict in order to\n            # keep the same data structure as the `metrics_dict` when model\n            # has multiple outputs.\n            self._model_have_multiple_outputs = False\n            self._metrics_dict = {MetricsDictKey.MODEL_OUTPUT: metrics_dict}\n        for output_name, metrics in self._metrics_dict.items():\n            for metric_name, metric in metrics.items():\n                if not isinstance(metric, metrics_module.Metric):\n                    # `tf.keras.metrics.MeanMetricWrapper` wraps stateless\n                    # functions into `tf.keras.metrics.Metric` instance.\n                    metrics[metric_name] = metrics_module.MeanMetricWrapper(\n                        metric, name=metric_name\n                    )\n\n    def update_evaluation_metrics(self, model_outputs, labels):\n        for key in model_outputs:\n            metrics = self._metrics_dict.get(key, {})\n            if not metrics:\n                continue\n            outputs = model_outputs.get(key)\n            for metric_inst in metrics.values():\n                self._update_metric_by_small_chunk(\n                    metric_inst, labels, outputs\n                )\n\n    def get_evaluation_summary(self):\n        if self._model_have_multiple_outputs:\n            return {\n                output_name: {\n                    metric_name: metric_inst.result().numpy()\n                    for metric_name, metric_inst in metrics.items()\n                }\n                for output_name, metrics in self._metrics_dict.items()\n            }\n        return {\n            metric_name: metric_inst.result().numpy()\n            for metric_name, metric_inst in self._metrics_dict[\n                MetricsDictKey.MODEL_OUTPUT\n            ].items()\n        }\n\n    def reset_metric_states(self):\n        """"""Resets all of the metric state variables.""""""\n        for metrics in self._metrics_dict.values():\n            for metric_inst in metrics.values():\n                metric_inst.reset_states()\n\n    @staticmethod\n    def _update_metric_by_small_chunk(\n        metric, labels, outputs, chunk_length=500\n    ):\n        """"""The metric updates state in a thread launched by grpc. The memory will\n        increase greatly if we update the metric with large size outputs. So\n        we split the outputs and labels to small chunks then update the metric\n        with those small chunks. The [issue 35044](https://github.com/\n        tensorflow/tensorflow/issues/35044) has been submitted to tensorflow.\n        """"""\n        chunk_boundaries = np.asarray(range(0, len(labels), chunk_length))\n        label_chunks = np.array_split(labels, chunk_boundaries)\n        output_chunks = np.array_split(outputs, chunk_boundaries)\n        for label, output in zip(label_chunks, output_chunks):\n            metric.update_state(label, output)\n'"
elasticdl/python/common/file_utils.py,0,"b'import os\nimport shutil\n\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n\ndef copy_if_not_exists(src, dst, is_dir):\n    if os.path.exists(dst):\n        logger.info(\n            ""Skip copying from %s to %s since the destination already exists""\n            % (src, dst)\n        )\n    else:\n        if is_dir:\n            shutil.copytree(src, dst)\n        else:\n            shutil.copy(src, dst)\n'"
elasticdl/python/common/grpc_utils.py,0,"b'import grpc\n\nfrom elasticdl.python.common.constants import GRPC\n\n\ndef build_channel(addr):\n    channel = grpc.insecure_channel(\n        addr,\n        options=[\n            (""grpc.max_send_message_length"", GRPC.MAX_SEND_MESSAGE_LENGTH),\n            (\n                ""grpc.max_receive_message_length"",\n                GRPC.MAX_RECEIVE_MESSAGE_LENGTH,\n            ),\n        ],\n    )\n    return channel\n'"
elasticdl/python/common/hash_utils.py,0,"b'import hashlib\n\n\ndef string_to_id(name, bucket_num):\n    h = hashlib.sha256(name.encode(""utf-8""))\n    return int(h.hexdigest(), base=32) % bucket_num\n\n\ndef int_to_id(number, bucket_num):\n    return number % bucket_num\n\n\ndef scatter_embedding_vector(values, indices, bucket_num):\n    """"""\n    Scatter embedding vectors to different parameter servers.\n    There are two steps to process <id, embedding vector> pairs:\n    1. hash the item id to a parameter server id\n    2. put corresponding item embedding vector to the parameter server id\n\n    For example, we scatter following embedding vectors into two parameter\n    servers:\n        values = np.array([[1, 2], [3, 4], [5, 6]])\n        indices = np.array([8, 1, 7])\n\n    ID 8 will be hashed to parameter server 0, ID 1 and ID 7 will be hashed to\n    parameter server 1. So, parameter server 0 has embedding vectors\n    np.array([[1, 2]]), parameter server 1 has embedding vectors,\n    np.array([[3, 4], [5, 6]).\n\n    The function return a dictionary:\n    {\n        0: (np.array([[1, 2]]), [8]),\n        1: (np.array([[3, 4], [5, 6]]), [1, 7])\n    }\n    """"""\n    ps_ids = {}\n    indices_list = indices.tolist()\n    for i, item_id in enumerate(indices_list):\n        ps_id = int_to_id(item_id, bucket_num)\n        if ps_id not in ps_ids:\n            ps_ids[ps_id] = [(i, item_id)]\n        else:\n            ps_ids[ps_id].append((i, item_id))\n    results = {}\n    for ps_id, i_item_id in ps_ids.items():\n        i = [v[0] for v in i_item_id]\n        item_id = [v[1] for v in i_item_id]\n        results[ps_id] = (values[i, :], item_id)\n    return results\n'"
elasticdl/python/common/k8s_client.py,0,"b'import os\nimport threading\nimport time\nimport traceback\n\nimport yaml\nfrom kubernetes import client, config, watch\nfrom kubernetes.client import V1EnvVar, V1EnvVarSource, V1ObjectFieldSelector\n\nfrom elasticdl.python.common.k8s_resource import parse as parse_resource\nfrom elasticdl.python.common.k8s_volume import parse_volume_and_mount\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.model_utils import load_module\n\nELASTICDL_APP_NAME = ""elasticdl""\nELASTICDL_JOB_KEY = ""elasticdl-job-name""\nELASTICDL_REPLICA_TYPE_KEY = ""elasticdl-replica-type""\nELASTICDL_REPLICA_INDEX_KEY = ""elasticdl-replica-index""\n_PS_SERVICE_PORT = 2222\n_WORKER_SERVICE_PORT = 3333\n_FTLIB_GOSSIP_CONTAINER_PORT = 7946\n_FTLIB_SSH_CONTAINER_PORT = 22\n\n\ndef get_master_pod_name(job_name):\n    return ""elasticdl-%s-master"" % job_name\n\n\ndef get_worker_pod_name(job_name, worker_id):\n    return ""elasticdl-%s-worker-%s"" % (job_name, str(worker_id))\n\n\ndef get_ps_pod_name(job_name, ps_id):\n    return ""elasticdl-%s-ps-%s"" % (job_name, str(ps_id))\n\n\ndef append_pod_ip_to_env(env):\n    pod_ip_var = V1EnvVar(\n        name=""MY_POD_IP"",\n        value_from=V1EnvVarSource(\n            field_ref=V1ObjectFieldSelector(field_path=""status.podIP"")\n        ),\n    )\n    if env:\n        env.append(pod_ip_var)\n    else:\n        env = [pod_ip_var]\n    return env\n\n\nclass Client(object):\n    def __init__(\n        self,\n        *,\n        image_name,\n        namespace,\n        job_name,\n        event_callback=None,\n        cluster_spec="""",\n        force_use_kube_config_file=False\n    ):\n        """"""\n        ElasticDL k8s client.\n\n        Args:\n            image_name: Docker image path for ElasticDL pod.\n            namespace: The name of the Kubernetes namespace where ElasticDL\n                pods will be created.\n            job_name: ElasticDL job name, should be unique in the namespace.\n                Used as pod name prefix and value for ""elasticdl"" label.\n            event_callback: If not None, an event watcher will be created and\n                events passed to the callback.\n            force_use_kube_config_file: If true, force to load the cluster\n                config from ~/.kube/config. Otherwise, if it\'s in a process\n                running in a K8S environment, it loads the incluster config,\n                if not, it loads the kube config file.\n        """"""\n        try:\n            if (\n                os.getenv(""KUBERNETES_SERVICE_HOST"")\n                and not force_use_kube_config_file\n            ):\n                # We are running inside a k8s cluster\n                config.load_incluster_config()\n                logger.info(""Load the incluster config."")\n            else:\n                # Use user\'s kube config\n                config.load_kube_config()\n                logger.info(""Load the kube config file."")\n        except Exception as ex:\n            traceback.print_exc()\n            raise Exception(\n                ""Failed to load configuration for Kubernetes:\\n%s"" % str(ex)\n            )\n\n        self.client = client.CoreV1Api()\n        self.namespace = namespace\n        self.job_name = job_name\n        self._image_name = image_name\n        self._event_cb = event_callback\n        if self._event_cb:\n            threading.Thread(\n                target=self._watch, name=""event_watcher"", daemon=True\n            ).start()\n        self.cluster = None\n        if cluster_spec:\n            cluster_spec_module = load_module(cluster_spec)\n            self.cluster = cluster_spec_module.cluster\n\n    def _watch(self):\n        while True:\n            try:\n                stream = watch.Watch().stream(\n                    self.client.list_namespaced_pod,\n                    self.namespace,\n                    label_selector=ELASTICDL_JOB_KEY + ""="" + self.job_name,\n                )\n                for event in stream:\n                    self._event_cb(event)\n            except Exception as e:\n                logger.debug(e)\n            # In case of any flaky issue causing exceptions, we wait for little\n            # time and retry.\n            time.sleep(5)\n\n    def _get_service_address(self, service_name, port):\n        return ""%s.%s.svc:%d"" % (service_name, self.namespace, port)\n\n    def get_master_pod_name(self):\n        return get_master_pod_name(self.job_name)\n\n    def get_worker_pod_name(self, worker_id):\n        return get_worker_pod_name(self.job_name, worker_id)\n\n    def get_worker_service_name(self, worker_id):\n        return self.get_worker_pod_name(worker_id)\n\n    def get_worker_service_address(self, worker_id):\n        return self._get_service_address(\n            self.get_worker_service_name(worker_id), _WORKER_SERVICE_PORT\n        )\n\n    def get_ps_pod_name(self, ps_id):\n        return get_ps_pod_name(self.job_name, ps_id)\n\n    def get_ps_service_name(self, ps_id):\n        return self.get_ps_pod_name(ps_id)\n\n    def get_ps_service_address(self, ps_id):\n        return self._get_service_address(\n            self.get_ps_service_name(ps_id), _PS_SERVICE_PORT\n        )\n\n    def patch_labels_to_pod(self, pod_name, labels_dict):\n        body = {""metadata"": {""labels"": labels_dict}}\n        try:\n            return self.client.patch_namespaced_pod(\n                name=pod_name, namespace=self.namespace, body=body\n            )\n        except client.api_client.ApiException as e:\n            logger.warning(""Exception when patching labels to pod: %s\\n"" % e)\n            return None\n\n    def get_master_pod(self):\n        return self.get_pod(self.get_master_pod_name())\n\n    def get_worker_pod(self, worker_id):\n        return self.get_pod(self.get_worker_pod_name(worker_id))\n\n    def get_ps_pod(self, ps_id):\n        return self.get_pod(self.get_ps_pod_name(ps_id))\n\n    def get_pod(self, pod_name):\n        try:\n            return self.client.read_namespaced_pod(\n                namespace=self.namespace, name=pod_name\n            )\n        except client.api_client.ApiException as e:\n            logger.warning(\n                ""Exception when reading pod %s: %s\\n"" % (pod_name, e)\n            )\n            return None\n\n    def get_ps_service(self, ps_id):\n        try:\n            return self.client.read_namespaced_service(\n                # PS service has the same name as pod name\n                name=self.get_ps_service_name(ps_id),\n                namespace=self.namespace,\n            )\n        except client.api_client.ApiException as e:\n            logger.warning(""Exception when reading PS service: %s\\n"" % e)\n            return None\n\n    def get_worker_service(self, worker_id):\n        try:\n            return self.client.read_namespaced_service(\n                # Worker service has the same name as pod name\n                name=self.get_worker_service_name(worker_id),\n                namespace=self.namespace,\n            )\n        except client.api_client.ApiException as e:\n            logger.warning(""Exception when reading worker service: %s\\n"" % e)\n            return None\n\n    @staticmethod\n    def create_owner_reference(owner_pod):\n        owner_ref = (\n            [\n                client.V1OwnerReference(\n                    api_version=""v1"",\n                    block_owner_deletion=True,\n                    kind=""Pod"",\n                    name=owner_pod.metadata.name,\n                    uid=owner_pod.metadata.uid,\n                )\n            ]\n            if owner_pod\n            else None\n        )\n        return owner_ref\n\n    def create_pod(self, **kargs):\n        # Container\n        pod_resource_requests = kargs[""resource_requests""]\n        pod_resource_limits = kargs[""resource_limits""]\n        pod_resource_limits = (\n            pod_resource_limits\n            if pod_resource_limits\n            else pod_resource_requests\n        )\n        ports = (\n            [\n                client.V1ContainerPort(\n                    container_port=_FTLIB_GOSSIP_CONTAINER_PORT, name=""gossip""\n                ),\n            ]\n            if ""expose_ports"" in kargs and kargs[""expose_ports""]\n            else None\n        )\n        container = client.V1Container(\n            name=kargs[""pod_name""],\n            image=kargs[""image_name""],\n            command=kargs[""command""],\n            resources=client.V1ResourceRequirements(\n                requests=parse_resource(pod_resource_requests),\n                limits=parse_resource(pod_resource_limits),\n            ),\n            args=kargs[""container_args""],\n            image_pull_policy=kargs[""image_pull_policy""],\n            env=kargs[""env""],\n            ports=ports,\n        )\n\n        # Pod\n        spec = client.V1PodSpec(\n            containers=[container],\n            restart_policy=kargs[""restart_policy""],\n            priority_class_name=kargs[""pod_priority""],\n            termination_grace_period_seconds=kargs.get(\n                ""termination_period"", None\n            ),\n        )\n\n        # Mount data path\n        if kargs[""volume""]:\n            volumes, volume_mounts = parse_volume_and_mount(\n                kargs[""volume""], kargs[""pod_name""]\n            )\n            spec.volumes = volumes\n            container.volume_mounts = volume_mounts\n\n        pod = client.V1Pod(\n            spec=spec,\n            metadata=client.V1ObjectMeta(\n                name=kargs[""pod_name""],\n                labels=self._get_common_labels(),\n                owner_references=self.create_owner_reference(\n                    kargs[""owner_pod""]\n                ),\n                namespace=self.namespace,\n            ),\n        )\n        if self.cluster:\n            pod = self.cluster.with_pod(pod)\n\n        return pod\n\n    def create_master(self, **kargs):\n        pod = self._create_master_pod_obj(**kargs)\n        self.client.create_namespaced_pod(self.namespace, pod)\n        logger.info(""Master launched."")\n\n    def dump_master_yaml(self, **kargs):\n        pod = self._create_master_pod_obj(**kargs)\n        pod_dict = self.client.api_client.sanitize_for_serialization(pod)\n        with open(kargs[""yaml""], ""w"") as f:\n            yaml.safe_dump(pod_dict, f, default_flow_style=False)\n\n    def _create_master_pod_obj(self, **kargs):\n        env = []\n        if ""envs"" in kargs:\n            for key in kargs[""envs""]:\n                env.append(V1EnvVar(name=key, value=kargs[""envs""][key]))\n        env = append_pod_ip_to_env(env)\n\n        pod = self.create_pod(\n            pod_name=self.get_master_pod_name(),\n            job_name=self.job_name,\n            image_name=self._image_name,\n            command=[""/bin/bash""],\n            resource_requests=kargs[""resource_requests""],\n            resource_limits=kargs[""resource_limits""],\n            container_args=kargs[""args""],\n            pod_priority=kargs[""pod_priority""],\n            image_pull_policy=kargs[""image_pull_policy""],\n            restart_policy=kargs[""restart_policy""],\n            volume=kargs[""volume""],\n            owner_pod=None,\n            env=env,\n        )\n        # Add replica type and index\n        pod.metadata.labels[ELASTICDL_REPLICA_TYPE_KEY] = ""master""\n        pod.metadata.labels[ELASTICDL_REPLICA_INDEX_KEY] = ""0""\n        pod.api_version = ""v1""\n        pod.kind = ""Pod""\n        return pod\n\n    def _create_ps_worker_pod(self, pod_name, type_key, index_key, **kargs):\n        # Find that master pod that will be used as the owner reference\n        # for the ps or worker pod.\n        master_pod = self.get_master_pod()\n        env = kargs[""envs""] if ""envs"" in kargs else None\n        env = append_pod_ip_to_env(env)\n        pod = self.create_pod(\n            pod_name=pod_name,\n            job_name=self.job_name,\n            image_name=self._image_name,\n            command=kargs[""command""],\n            resource_requests=kargs[""resource_requests""],\n            resource_limits=kargs[""resource_limits""],\n            container_args=kargs[""args""],\n            pod_priority=kargs[""pod_priority""],\n            image_pull_policy=kargs[""image_pull_policy""],\n            restart_policy=kargs[""restart_policy""],\n            volume=kargs[""volume""],\n            owner_pod=master_pod,\n            ps_addrs=kargs.get(""ps_addrs"", """"),\n            termination_period=kargs.get(""termination_period"", None),\n            env=env,\n            expose_ports=kargs[""expose_ports""],\n        )\n        # Add replica type and index\n        pod.metadata.labels[ELASTICDL_REPLICA_TYPE_KEY] = type_key\n        pod.metadata.labels[ELASTICDL_REPLICA_INDEX_KEY] = str(index_key)\n        return self.client.create_namespaced_pod(self.namespace, pod)\n\n    def create_worker(self, **kargs):\n        pod_name = self.get_worker_pod_name(kargs[""worker_id""])\n        return self._create_ps_worker_pod(\n            pod_name, ""worker"", kargs[""worker_id""], **kargs\n        )\n\n    def create_ps(self, **kargs):\n        pod_name = self.get_ps_pod_name(kargs[""ps_id""])\n        return self._create_ps_worker_pod(\n            pod_name, ""ps"", kargs[""ps_id""], **kargs\n        )\n\n    def delete_master(self):\n        logger.info(""pod name is %s"" % self.get_master_pod_name())\n        self.delete_pod(self.get_master_pod_name())\n\n    def delete_worker(self, worker_id):\n        self.delete_pod(self.get_worker_pod_name(worker_id))\n\n    def delete_ps(self, ps_id):\n        self.delete_pod(self.get_ps_pod_name(ps_id))\n\n    def delete_pod(self, pod_name):\n        self.client.delete_namespaced_pod(\n            pod_name,\n            self.namespace,\n            body=client.V1DeleteOptions(grace_period_seconds=0),\n        )\n\n    def get_tensorboard_service_name(self):\n        return ""tensorboard-"" + self.job_name\n\n    def create_tensorboard_service(\n        self,\n        port=80,\n        target_port=6006,\n        replica_type=""master"",\n        replica_index=""0"",\n        service_type=""LoadBalancer"",\n    ):\n        return self._create_service(\n            name=self.get_tensorboard_service_name(),\n            port=port,\n            target_port=target_port,\n            replica_type=replica_type,\n            replica_index=replica_index,\n            service_type=service_type,\n            owner=self.get_master_pod(),\n        )\n\n    def get_collective_communicator_service_name(self):\n        return self.job_name + ""-ftlib-consensus""\n\n    def create_ftlib_consensus_service(self):\n        return self._create_service(\n            name=self.get_collective_communicator_service_name(),\n            port=_FTLIB_GOSSIP_CONTAINER_PORT,\n            target_port=_FTLIB_GOSSIP_CONTAINER_PORT,\n            replica_type=""worker"",\n            replica_index=None,\n            owner=self.get_master_pod(),\n        )\n\n    def create_ps_service(self, ps_id):\n        return self._create_service(\n            name=self.get_ps_service_name(ps_id),\n            port=_PS_SERVICE_PORT,\n            target_port=_PS_SERVICE_PORT,\n            replica_type=""ps"",\n            replica_index=ps_id,\n            owner=self.get_ps_pod(ps_id),\n        )\n\n    def create_worker_service(self, worker_id):\n        return self._create_service(\n            name=self.get_worker_service_name(worker_id),\n            port=_WORKER_SERVICE_PORT,\n            target_port=_WORKER_SERVICE_PORT,\n            replica_type=""worker"",\n            replica_index=worker_id,\n            owner=self.get_worker_pod(worker_id),\n        )\n\n    def _create_service(self, **kargs):\n        labels = self._get_common_labels()\n\n        metadata = client.V1ObjectMeta(\n            name=kargs[""name""],\n            labels=labels,\n            # Note: We have to add at least one annotation here.\n            # Otherwise annotation is `None` and cannot be modified\n            # using `with_service()` for cluster specific information.\n            annotations=labels,\n            owner_references=self.create_owner_reference(kargs[""owner""])\n            if ""owner"" in kargs\n            else None,\n            namespace=self.namespace,\n        )\n        selector = {\n            ""app"": ELASTICDL_APP_NAME,\n            ELASTICDL_JOB_KEY: self.job_name,\n            ELASTICDL_REPLICA_TYPE_KEY: kargs[""replica_type""],\n        }\n        if kargs[""replica_index""] is not None:\n            selector[ELASTICDL_REPLICA_INDEX_KEY] = str(kargs[""replica_index""])\n        spec = client.V1ServiceSpec(\n            ports=[\n                client.V1ServicePort(\n                    port=kargs[""port""], target_port=kargs[""target_port""]\n                )\n            ],\n            selector=selector,\n            type=kargs.get(""service_type"", None),\n        )\n        service = client.V1Service(\n            api_version=""v1"", kind=""Service"", metadata=metadata, spec=spec\n        )\n        if self.cluster:\n            service = self.cluster.with_service(service)\n        return self.client.create_namespaced_service(self.namespace, service)\n\n    def _get_common_labels(self):\n        """"""Labels that should be attached to all k8s objects belong to\n           current job.\n        """"""\n        return {""app"": ELASTICDL_APP_NAME, ELASTICDL_JOB_KEY: self.job_name}\n\n    def get_master_log(self):\n        return self.get_pod_log(self.get_master_pod_name())\n\n    def get_worker_log(self, worker_id):\n        return self.get_pod_log(self.get_worker_pod_name(worker_id))\n\n    def get_ps_log(self, ps_id):\n        return self.get_pod_log(self.get_ps_pod_name(ps_id))\n\n    def get_pod_log(self, pod_name):\n        try:\n            return self.client.read_namespaced_pod_log(\n                namespace=self.namespace, name=pod_name\n            )\n        except client.api_client.ApiException as e:\n            logger.warning(\n                ""Exception when reading log of pod %s: %s\\n"" % (pod_name, e)\n            )\n            return None\n'"
elasticdl/python/common/k8s_job_monitor.py,0,"b'import time\n\nfrom kubernetes import client\n\nfrom elasticdl.python.common.constants import PodStatus\nfrom elasticdl.python.common.k8s_client import Client\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\nMAX_READ_POD_RETRIES = 6\n\n\ndef print_tail_log(log, tail_num):\n    if log is not None:\n        log_lines = log.split(""\\n"")\n        tail_index = -1 * tail_num\n        logger.info(""\\n"".join(log_lines[tail_index:]))\n\n\nclass PodMonitor:\n    def __init__(self, namespace, pod_name, use_kube_config=True):\n        """"""\n        k8s Pod Monitor. During data transformation, we may launch a pod\n        on k8s cluster to perform data analysis and need to monitor the\n        pod execution status.\n\n        Args:\n            namespace: The name of the Kubernetes namespace where the pod\n                will be created.\n            pod_name: Pod name, should be unique in the namespace.\n            use_kube_config: If true, load the cluster config from\n                ~/.kube/config. Otherwise, if it\'s in a process running in\n                a K8S environment, it loads the incluster config.\n        """"""\n        self.namespace = namespace\n        self.pod_name = pod_name\n        self.client = Client(\n            image_name=None,\n            namespace=namespace,\n            job_name=None,\n            force_use_kube_config_file=use_kube_config,\n        )\n\n    def monitor_status(self):\n        retry_num = 0\n        pod_succeeded = False\n\n        while True:\n            try:\n                pod = self.client.get_pod(self.pod_name)\n                if pod is None:\n                    retry_num += 1\n                    if retry_num > MAX_READ_POD_RETRIES:\n                        logger.error(""{} Not Found"".format(self.pod_name))\n                        break\n                    time.sleep(10)\n                    continue\n\n                retry_num = 0\n\n                logger.info(""Pod Status : %s"" % pod.status.phase)\n                if pod.status.phase == PodStatus.SUCCEEDED:\n                    pod_succeeded = True\n                    break\n                elif pod.status.phase == PodStatus.FAILED:\n                    logger.info(self.client.get_pod_log(self.pod_name))\n                    break\n                else:\n                    time.sleep(30)\n            except client.api_client.ApiException:\n                time.sleep(60)\n        return pod_succeeded\n\n    def delete_pod(self):\n        if self.client.get_pod(self.pod_name):\n            self.client.delete_pod(self.pod_name)\n        # Wait until the pod is deleted\n        while self.client.get_pod(self.pod_name):\n            time.sleep(5)\n\n\nclass EdlJobMonitor:\n    def __init__(\n        self, namespace, job_name, worker_num, ps_num, use_kube_config=True\n    ):\n        """"""\n        ElasticDL job monitor. After launching an ElasticDL job, the user\n        may want to monitor the job status.\n\n        Args:\n            namespace: The name of the Kubernetes namespace where the pod\n                will be created.\n            job_name: ElasticDL job name, should be unique in the namespace.\n                Used as pod name prefix and value for ""elasticdl"" label.\n            worker_num: Integer, worker number of the job.\n            ps_num: Integer, parameter server number of the job.\n            use_kube_config: If true, load the cluster config from\n                ~/.kube/config. Otherwise, if it\'s in a process running in\n                a K8S environment, it loads the incluster config.\n        """"""\n        self.worker_num = worker_num\n        self.ps_num = ps_num\n        self.job_name = job_name\n        self.client = Client(\n            image_name=None,\n            namespace=namespace,\n            job_name=job_name,\n            force_use_kube_config_file=use_kube_config,\n        )\n\n    def check_worker_status(self):\n        for i in range(self.worker_num):\n            worker_pod = self.client.get_worker_pod(i)\n            worker_pod_name = self.client.get_worker_pod_name(i)\n            if worker_pod is None:\n                logger.error(""Worker {} Not Found"".format(worker_pod_name))\n            elif worker_pod.status.phase == PodStatus.FAILED:\n                logger.error(\n                    ""Worker {} {}"".format(\n                        worker_pod_name, worker_pod.status.phase\n                    )\n                )\n\n    def check_ps_status(self):\n        for i in range(self.ps_num):\n            ps_pod = self.client.get_ps_pod(i)\n            ps_pod_name = self.client.get_ps_pod_name(i)\n            if ps_pod is None:\n                logger.error(""PS {} Not Found"".format(ps_pod_name))\n            elif ps_pod.status.phase == PodStatus.FAILED:\n                logger.error(\n                    ""PS {} {}"".format(ps_pod_name, ps_pod.status.phase)\n                )\n\n    def show_evaluation_and_task_log(self, new_log, old_log):\n        """"""Show the master\'s incremental logs about evaluation task and\n        latest completed task compared with the last query.\n        """"""\n        if new_log is None:\n            return\n        increment_log = new_log.replace(old_log, """")\n        task_log = """"\n        for log_line in increment_log.split(""\\n""):\n            if ""Evaluation"" in log_line:\n                logger.info(log_line)\n            if ""Task"" in log_line:\n                task_log = log_line\n        logger.info(task_log)\n        return new_log\n\n    def monitor_status(self):\n        retry_num = 0\n        job_succeed = False\n        master_old_log = """"\n        while True:\n            try:\n                master_pod = self.client.get_master_pod()\n                if master_pod is None:\n                    retry_num += 1\n                    if retry_num > MAX_READ_POD_RETRIES:\n                        logger.error(\n                            ""{} Not Found"".format(\n                                self.client.get_master_pod_name()\n                            )\n                        )\n                        break\n                    time.sleep(10)\n                    continue\n                retry_num = 0\n\n                logger.info(\n                    ""Master status: {}"".format(master_pod.status.phase)\n                )\n                if master_pod.status.phase == PodStatus.SUCCEEDED:\n                    job_succeed = True\n                    break\n                elif master_pod.status.phase == PodStatus.PENDING:\n                    time.sleep(10)\n                elif master_pod.status.phase == PodStatus.FAILED:\n                    log = self.client.get_master_log()\n                    print_tail_log(log, tail_num=100)\n                    logger.error(""Job {} Failed"".format(self.job_name))\n                    break\n                else:\n                    master_new_log = self.client.get_master_log()\n                    self.show_evaluation_and_task_log(\n                        master_new_log, master_old_log\n                    )\n                    master_old_log = master_new_log\n                    time.sleep(60)\n            except client.api_client.ApiException:\n                time.sleep(60)\n        return job_succeed\n\n    def delete_job(self):\n        if self.client.get_master_pod():\n            self.client.delete_master()\n\n        # Wait until the master is deleted\n        while self.client.get_master_pod():\n            time.sleep(5)\n'"
elasticdl/python/common/k8s_resource.py,0,"b'import re\n\n_ALLOWED_RESOURCE_TYPES = [""memory"", ""disk"", ""ephemeral-storage"", ""cpu"", ""gpu""]\n# Any domain name is (syntactically) valid if it\'s a dot-separated list of\n# identifiers, each no longer than 63 characters, and made up of letters,\n# digits and dashes (no underscores).\n_GPU_VENDOR_REGEX_STR = r""^[a-zA-Z\\d-]{,63}(\\.[a-zA-Z\\d-]{,63})*/gpu$""\n\n\ndef _is_numeric(n):\n    try:\n        float(n)\n    except ValueError:\n        return False\n    return True\n\n\ndef _valid_gpu_spec(gpu_str):\n    if not gpu_str.isnumeric():\n        raise ValueError(""invalid gpu request spec: "" + gpu_str)\n    return gpu_str\n\n\ndef _valid_cpu_spec(cpu_str):\n    regexp = re.compile(""([1-9]{1})([0-9]*)m$"")\n    if not regexp.match(cpu_str) and not _is_numeric(cpu_str):\n        raise ValueError(""invalid cpu request spec: "" + cpu_str)\n    return cpu_str\n\n\ndef _valid_mem_spec(mem_str):\n    regexp = re.compile(""([1-9]{1})([0-9]*)(E|P|T|G|M|K|Ei|Pi|Ti|Gi|Mi|Ki)$"")\n    if not regexp.match(mem_str):\n        raise ValueError(""invalid memory request spec: "" + mem_str)\n    return mem_str\n\n\ndef parse(resource_str):\n    """"""Parse combined k8s resource string into a dict.\n\n    Args:\n        resource_str: The string representation for k8s resource,\n            e.g. ""cpu=250m,memory=32Mi,disk=64Mi,gpu=1,ephemeral-storage=32Mi"".\n\n    Return:\n        A Python dictionary parsed from the given resource string.\n    """"""\n    kvs = resource_str.strip().split("","")\n    resource_names = []\n    parsed_res_dict = {}\n    for kv in kvs:\n        k, v = kv.strip().split(""="")\n        k = k.strip()\n        v = v.strip()\n        if k not in resource_names:\n            resource_names.append(k)\n        else:\n            raise ValueError(\n                ""The resource string contains duplicate resource names: %s"" % k\n            )\n        if k in [""memory"", ""disk"", ""ephemeral-storage""]:\n            _valid_mem_spec(v)\n        elif k == ""cpu"":\n            _valid_cpu_spec(v)\n        elif ""gpu"" in k:\n            if k == ""gpu"":\n                k = ""nvidia.com/gpu""\n            elif not re.compile(_GPU_VENDOR_REGEX_STR).match(k):\n                raise ValueError(\n                    ""gpu resource name does not have a valid vendor name: %s""\n                    % k\n                )\n            _valid_gpu_spec(v)\n        else:\n            raise ValueError(\n                ""%s is not in the allowed list of resource types: %s""\n                % (k, _ALLOWED_RESOURCE_TYPES)\n            )\n        parsed_res_dict[k] = v\n    return parsed_res_dict\n'"
elasticdl/python/common/k8s_tensorboard_client.py,0,"b'import time\n\nfrom kubernetes import client\n\nfrom elasticdl.python.common import k8s_client as k8s\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n\nclass TensorBoardClient(object):\n    def __init__(self, **kwargs):\n        """"""\n        ElasticDL k8s TensorBoard client.\n\n        Args:\n            **kwargs: Additional keyword arguments passed to the\n                `elasticdl.python.common.k8s_client.Client` object.\n        """"""\n        self._k8s_client = k8s.Client(**kwargs)\n\n    def start_tensorboard_service(self):\n        self._k8s_client.create_tensorboard_service()\n        logger.info(""Waiting for the URL for TensorBoard service..."")\n        tb_url = self._get_tensorboard_url()\n        if tb_url:\n            logger.info(""TensorBoard service is available at: %s"" % tb_url)\n        else:\n            logger.warning(""Unable to get the URL for TensorBoard service"")\n\n    def _get_tensorboard_service(self):\n        try:\n            return self._k8s_client.client.read_namespaced_service(\n                name=self._k8s_client.get_tensorboard_service_name(),\n                namespace=self._k8s_client.namespace,\n            ).to_dict()\n        except client.api_client.ApiException as e:\n            logger.warning(\n                ""Exception when reading TensorBoard service: %s\\n"" % e\n            )\n            return None\n\n    def _get_tensorboard_url(self, check_interval=5, wait_timeout=120):\n        start_time = time.time()\n        while True:\n            if time.time() - start_time > wait_timeout:\n                return None\n            service = self._get_tensorboard_service()\n            if (\n                service is None\n                or service[""status""][""load_balancer""][""ingress""] is None\n            ):\n                time.sleep(check_interval)\n            else:\n                return service[""status""][""load_balancer""][""ingress""][0][""ip""]\n'"
elasticdl/python/common/k8s_volume.py,0,"b'from kubernetes import client\n\n_ALLOWED_VOLUME_KEYS = [\n    ""claim_name"",\n    ""host_path"",\n    ""type"",\n    ""mount_path"",\n    ""sub_path"",\n]\n_ALLOWED_VOLUME_TYPES = [\n    ""pvc"",\n    ""host_path"",\n]\n\n\ndef parse_volume_and_mount(volume_conf, pod_name):\n    """"""Get k8s volumes list and volume mounts list from\n    the volume config string.\n\n    Args:\n        volume_conf (string): the volumes config string,\n        e.g. ""host_path=c0,mount_path=/path0;claim_name=c1,mount_path=/path1"".\n        pod_name (string): the pod name\n\n    Return:\n        volumes (List): a Python list contains k8s volumes.\n        volume_mounts (List): a Python list contains k8s volume mounts.\n    """"""\n    volumes_per_type = {type_name: {} for type_name in _ALLOWED_VOLUME_TYPES}\n    volume_mounts = []\n    volume_dicts = parse(volume_conf)\n    num_volumes = 0\n    for volume_dict in volume_dicts:\n        if ""claim_name"" in volume_dict:\n            """"""For the volume configuration string\n            \'claim_name=c1,mount_path=/path1;\n             claim_name=c1,mount_path=/path2,sub_path=/sub_path0\'\n            We don\'t need create PVC volume for `c1` twice and\n            prefer to reuse it by getting from volumes_per_type.\n            """"""\n            volume = volumes_per_type[""pvc""].get(\n                volume_dict[""claim_name""], None\n            )\n            if volume is None:\n                pvc_volume_source = client.V1PersistentVolumeClaimVolumeSource(\n                    claim_name=volume_dict[""claim_name""], read_only=False\n                )\n                volume_name = pod_name + ""-volume-%d"" % num_volumes\n                volume = client.V1Volume(\n                    name=volume_name, persistent_volume_claim=pvc_volume_source\n                )\n                volumes_per_type[""pvc""][volume_dict[""claim_name""]] = volume\n                num_volumes += 1\n        elif ""host_path"" in volume_dict:\n            volume = volumes_per_type[""host_path""].get(\n                volume_dict[""host_path""], None\n            )\n            if volume is None:\n                volume_name = pod_name + ""-volume-%d"" % num_volumes\n                volume = client.V1Volume(\n                    name=volume_name,\n                    host_path=client.V1HostPathVolumeSource(\n                        path=volume_dict[""host_path""],\n                        type=volume_dict.get(""type"", None),\n                    ),\n                )\n                volumes_per_type[""host_path""][\n                    volume_dict[""host_path""]\n                ] = volume\n                num_volumes += 1\n        else:\n            continue\n\n        volume_mounts.append(\n            client.V1VolumeMount(\n                name=volume.name,\n                mount_path=volume_dict[""mount_path""],\n                sub_path=volume_dict.get(""sub_path"", None),\n            )\n        )\n\n    volumes = []\n    for volumes_one_type in volumes_per_type.values():\n        volumes.extend(volumes_one_type.values())\n\n    return volumes, volume_mounts\n\n\ndef parse(volume_str):\n    """"""Parse combined k8s volume strings separated by\n    semicolons to a list of python dictionaries.\n\n    Args:\n        volume_str: The string representation for k8s volume,\n        e.g. ""host_path=c0,mount_path=/path0;claim_name=c1,mount_path=/path1"".\n\n    Return:\n        A Python list which contains dictionaries and each dictionary is\n        representation for a k8s volume.\n    """"""\n    volumes = volume_str.strip().split("";"")\n    volume_mount_pairs = []\n    for volume_str in volumes:\n        if volume_str:\n            volume_mount_pairs.append(parse_single_volume(volume_str))\n    return volume_mount_pairs\n\n\ndef parse_single_volume(volume_str):\n    """"""Parse combined k8s volume string into a dict.\n\n    Args:\n        volume_str: The string representation for k8s volume,\n            e.g. ""claim_name=c1,mount_path=/path1"".\n\n    Return:\n        A Python dictionary parsed from the given volume string.\n    """"""\n    kvs = volume_str.strip().split("","")\n    volume_keys = []\n    parsed_volume_dict = {}\n    for kv in kvs:\n        k, v = kv.strip().split(""="")\n        k = k.strip()\n        v = v.strip()\n        if k not in volume_keys:\n            volume_keys.append(k)\n        else:\n            raise ValueError(\n                ""The volume string contains duplicate volume key: %s"" % k\n            )\n        if k not in _ALLOWED_VOLUME_KEYS:\n            raise ValueError(\n                ""%s is not in the allowed list of volume keys: %s""\n                % (k, _ALLOWED_VOLUME_KEYS)\n            )\n        parsed_volume_dict[k] = v\n    return parsed_volume_dict\n'"
elasticdl/python/common/log_utils.py,0,"b'import logging\nimport sys\nimport typing\n\n_DEFAULT_LOGGER = ""elastic.logger""\n\n_DEFAULT_FORMATTER = logging.Formatter(\n    ""[%(asctime)s] [%(levelname)s] ""\n    ""[%(filename)s:%(lineno)d:%(funcName)s] %(message)s""\n)\n\n_ch = logging.StreamHandler(stream=sys.stderr)\n_ch.setFormatter(_DEFAULT_FORMATTER)\n\n_DEFAULT_HANDLERS = [_ch]\n\n_LOGGER_CACHE = {}  # type: typing.Dict[str, logging.Logger]\n\n\ndef get_logger(name, level=""INFO"", handlers=None, update=False):\n    if name in _LOGGER_CACHE and not update:\n        return _LOGGER_CACHE[name]\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.handlers = handlers or _DEFAULT_HANDLERS\n    logger.propagate = False\n    return logger\n\n\ndefault_logger = get_logger(_DEFAULT_LOGGER)\n'"
elasticdl/python/common/model_handler.py,31,"b'import abc\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.feature_column import feature_column_v2 as fc_lib\n\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.elasticdl.feature_column.feature_column import (\n    EmbeddingColumn,\n    embedding_column,\n)\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\nfrom elasticdl.python.ps.embedding_table import EmbeddingTable\nfrom elasticdl_preprocessing.layers import SparseEmbedding\n\n\ndef _get_trained_params_from_checkpoint(checkpoint_dir):\n    """"""Get parameters from a checkpoint directory saved by ElasticDL""""""\n    parameters = CheckpointSaver.restore_params_from_checkpoint(\n        checkpoint_dir, 0, 1\n    )\n\n    trained_params = parameters.non_embedding_params\n    for name, table in parameters.embedding_params.items():\n        trained_params[name] = table\n    return trained_params\n\n\ndef _convert_embedding_table_to_numpy_array(embedding_table, embedding_shape):\n    """"""Convert an embedding table to a np.ndarray which can be assigned\n    to trainable weights in keras embedding layers.\n\n    Args:\n        embedding_table: A `EmbeddingTable` instance.\n        embedding_shape: a tuple with two elements\n\n    Returns:\n        A np.ndarray\n    """"""\n    embedding_ids = list(embedding_table.embedding_vectors.keys())\n    embedding_values = list(embedding_table.embedding_vectors.values())\n    embedding_weights = np.zeros(embedding_shape)\n    embedding_weights[embedding_ids] = embedding_values\n    return embedding_weights\n\n\ndef _get_embedding_column_input_dim(embedding_column):\n    if type(embedding_column) != fc_lib.EmbeddingColumn:\n        raise Exception(""The input should be EmbeddingColumn type."")\n\n    default_num_buckets = (\n        embedding_column.categorical_column.num_buckets\n        if embedding_column._is_v2_column\n        else embedding_column.categorical_column._num_buckets\n    )  # pylint: disable=protected-access\n    num_buckets = getattr(\n        embedding_column.categorical_column, ""num_buckets"", default_num_buckets\n    )\n\n    return num_buckets\n\n\ndef _need_partition_embedding(embedding_object):\n    """"""The embedding layer will be partitioned on multiple\n    PS instances if the memory of the layer.train_weights is\n    bigger than 2MB.\n    """"""\n    if isinstance(embedding_object, tf.keras.layers.Layer):\n        return _need_partition_embedding_from_shape_info(\n            embedding_object.input_dim, embedding_object.output_dim\n        )\n    elif isinstance(embedding_object, fc_lib.EmbeddingColumn):\n        return _need_partition_embedding_from_shape_info(\n            _get_embedding_column_input_dim(embedding_object),\n            embedding_object.dimension,\n        )\n    else:\n        raise Exception(\n            ""Unsupported type {} for embedding"".format(type(embedding_object))\n        )\n\n\ndef _need_partition_embedding_from_shape_info(input_dim, output_dim):\n    EMBEDDING_SIZE_THRESHOLD_FOR_PARTITION = 2 * 1024 * 1024  # 2MB\n    FLOAT32_BYTES = 4\n    weights_memory = input_dim * output_dim * FLOAT32_BYTES\n    return weights_memory > EMBEDDING_SIZE_THRESHOLD_FOR_PARTITION\n\n\ndef _replace_tf_embedding_column_with_edl(dense_features_layer):\n    new_feature_columns = []\n    for column in dense_features_layer._feature_columns:\n        if isinstance(\n            column, fc_lib.EmbeddingColumn\n        ) and _need_partition_embedding(column):\n            logger.info(\n                ""Replace embedding_column {} from TensorFlow ""\n                ""version to ElasticDL version"".format(column.name)\n            )\n            new_column = embedding_column(\n                column.categorical_column, dimension=column.dimension\n            )\n            new_column.set_dense_features_layer_name(dense_features_layer.name)\n            new_feature_columns.append(new_column)\n        else:\n            new_feature_columns.append(column)\n\n    return tf.keras.layers.DenseFeatures(\n        feature_columns=new_feature_columns, name=dense_features_layer.name\n    )\n\n\ndef _replace_edl_embedding_column_with_tf(dense_features_layer):\n    new_feature_columns = []\n    for column in dense_features_layer._feature_columns:\n        if isinstance(column, EmbeddingColumn):\n            logger.info(\n                ""Replace embedding_column {} from ElasticDL ""\n                ""version to TF version"".format(column.name)\n            )\n            new_column = fc_lib.embedding_column(\n                column.categorical_column, dimension=column.dimension\n            )\n            new_feature_columns.append(new_column)\n        else:\n            new_feature_columns.append(column)\n\n    return tf.keras.layers.DenseFeatures(\n        feature_columns=new_feature_columns, name=dense_features_layer.name\n    )\n\n\nclass ModelHandler(metaclass=abc.ABCMeta):\n    """"""Generate the model to train in ElasticDL for different distributed\n    strategies and export trained model in ElasticDL to SavedModel.\n    """"""\n\n    @abc.abstractmethod\n    def get_model_to_train(self, model):\n        """"""Generate a model to train in ElasticDL.\n\n        Args:\n            model: A native keras model instance.\n\n        Returns:\n            A keras model instance for ElasticDL training.\n        """"""\n\n    @abc.abstractmethod\n    def get_model_to_export(self, model, dataset):\n        """"""Get the model which can be exported a SavedModel\n        by tf.saved_model.save.\n\n        Args:\n            model: A keras model instance trained by ElasticDL and\n            it may contains `elasticdl.layers.Embedding` layers.\n            dataset: A `tf.data.Dataset` instance which has the same outputs as\n                the training dataset.\n\n        Returns:\n            A keras model instance trained by ElasticDL.\n        """"""\n\n    @classmethod\n    def get_model_handler(\n        cls, distribution_strategy=None, checkpoint_dir=None\n    ):\n        """"""Create a model handler to process the model for the\n        distributed strategy.\n\n        Args:\n            distribution_strategy (string): distribution strategy name\n            checkpoint_dir: Checkpoint directory to save model parametes\n                during training.\n\n        Return:\n            ModelHandler subclass instance.\n        """"""\n        if distribution_strategy == DistributionStrategy.PARAMETER_SERVER:\n            return ParameterServerModelHandler(checkpoint_dir=checkpoint_dir)\n        elif distribution_strategy == DistributionStrategy.ALLREDUCE:\n            logger.warning(\n                ""Allreduce distribution strategy is not supported yet. ""\n                ""Switching to use the default distribution strategy.""\n            )\n        return DefaultModelHandler()\n\n\nclass DefaultModelHandler(ModelHandler):\n    """"""Return the origin model to train and export.""""""\n\n    def get_model_to_train(self, model):\n        return model\n\n    def get_model_to_export(self, model, dataset):\n        """"""\n        Get model with inputs and trained parameters to export.\n        """"""\n        if not model.inputs:\n            model._build_model_with_inputs(inputs=dataset, targets=None)\n        return model\n\n\nclass ParameterServerModelHandler(ModelHandler):\n    """"""Model handler for parameter server strategy.\n    For training, The handler will replace `tf.keras.layers.Embedding`\n    layers with`elasticdl.layers.Embedding` for training.\n    For saving model, the handler will restore Keras model definition and\n    pull trained parameters from parameter server(s) for the model.\n    """"""\n\n    def __init__(self, checkpoint_dir=None):\n        """"""\n        Arguments:\n            checkpoint_dir: A checkpoint directory to save all model\n                parameters during training.\n        """"""\n        self._checkpoint_dir = checkpoint_dir\n\n    def get_model_to_train(self, model):\n        """"""Replace the tf.keras.layers.Embedding layer in the model with\n        an elasticdl.layers.Embedding layer in ParameterServerStrategy.\n        """"""\n        # clear keras model session to avoid clutter from old models/layers.\n        tf.keras.backend.clear_session()\n        if type(model) == tf.keras.Sequential or model._is_graph_network:\n            model = self._clone_model_with_edl_embedding(model)\n        else:\n            model = self._replace_attr_with_edl_embedding(model)\n        return model\n\n    def get_model_to_export(self, model, dataset):\n        """"""Get the model which can be exported to a SavedModel by\n        `tf.saved_model.save`.\n        """"""\n        model = self._restore_keras_model_def(model)\n        if not model.inputs:\n            # build model to add inputs and outputs that\n            # can be consumed by tf-serving\n            model._build_model_with_inputs(inputs=dataset, targets=None)\n\n        checkpoint_dir = CheckpointSaver.get_valid_lastest_version_dir(\n            self._checkpoint_dir\n        )\n        if checkpoint_dir is None:\n            logger.warning(""No available checkpoint to export model"")\n            return model\n\n        trained_params = _get_trained_params_from_checkpoint(checkpoint_dir)\n        for var in model.trainable_variables:\n            if isinstance(trained_params[var.name], EmbeddingTable):\n                embedding_params = _convert_embedding_table_to_numpy_array(\n                    trained_params[var.name], var.shape\n                )\n                var.assign(embedding_params)\n            else:\n                var.assign(trained_params[var.name].numpy())\n        return model\n\n    def _restore_keras_model_def(self, model):\n        """"""Restore Keras model definition by replacing\n        `elasticdl.layers.Embedding` layers with\n        `tf.keras.layers.Embedding` layers.\n        """"""\n        # clear keras model session to avoid clutter from old models/layers.\n        tf.keras.backend.clear_session()\n        if (\n            isinstance(model, tf.keras.models.Model)\n            and not model._is_graph_network\n        ):\n            model = self._replace_attr_with_keras_embedding(model)\n        else:\n            model = self._clone_model_with_keras_embedding(model)\n        return model\n\n    @staticmethod\n    def _clone_model_with_edl_embedding(model):\n        """"""Clone a new model and replace keras embedding layers including\n        `tf.keras.layers.Embedding` and `SparseEmbedding` with\n        `elasticdl.layers.Embedding`\n        """"""\n\n        def _clone_function(layer):\n            if type(layer) in [\n                tf.keras.layers.Embedding,\n                SparseEmbedding,\n            ] and _need_partition_embedding(layer):\n                logger.debug(\n                    ""Replace {} with {}"".format(layer.name, Embedding)\n                )\n                # ElasticDL embedding only accept a string type initializer\n                init = tf.keras.initializers.serialize(\n                    layer.embeddings_initializer\n                )[""class_name""]\n\n                if type(layer) == tf.keras.layers.Embedding:\n                    embedding_layer = Embedding(\n                        output_dim=layer.output_dim,\n                        input_dim=layer.input_dim,\n                        embeddings_initializer=init,\n                        mask_zero=layer.mask_zero,\n                        input_length=layer.input_length,\n                        name=layer.name,\n                    )\n                else:\n                    embedding_layer = Embedding(\n                        output_dim=layer.output_dim,\n                        input_dim=layer.input_dim,\n                        embeddings_initializer=init,\n                        name=layer.name,\n                        combiner=layer.combiner,\n                    )\n                embedding_layer.set_embedding_weight_name(\n                    layer.trainable_weights[0].name\n                )\n                return embedding_layer\n            elif type(layer) == tf.keras.layers.DenseFeatures:\n                return _replace_tf_embedding_column_with_edl(layer)\n            return layer\n\n        return tf.keras.models.clone_model(\n            model, clone_function=_clone_function\n        )\n\n    @staticmethod\n    def _clone_model_with_keras_embedding(model):\n        """"""Clone a new model and replace the `elasticdl.layers.Embedding`\n        layers with `tf.keras.layers.Embedding` or `SparseEmbedding` layers\n        """"""\n\n        def _clone_function(layer):\n            if type(layer) == Embedding:\n                logger.info(\n                    ""Replace embedding layer with ""\n                    ""elasticdl.layers.Embedding""\n                )\n                # The combiner is not None only for SparseEmbedding,\n                if layer.combiner is not None:\n                    embedding_layer = SparseEmbedding(\n                        output_dim=layer.output_dim,\n                        input_dim=layer.input_dim,\n                        embeddings_initializer=layer.embeddings_initializer,\n                        name=layer.name,\n                        combiner=layer.combiner,\n                    )\n                else:\n                    embedding_layer = tf.keras.layers.Embedding(\n                        output_dim=layer.output_dim,\n                        input_dim=layer.input_dim,\n                        embeddings_initializer=layer.embeddings_initializer,\n                        mask_zero=layer.mask_zero,\n                        input_length=layer.input_length,\n                        name=layer.name,\n                    )\n                return embedding_layer\n            elif type(layer) == tf.keras.layers.DenseFeatures:\n                return _replace_edl_embedding_column_with_tf(layer)\n            return layer\n\n        return tf.keras.models.clone_model(\n            model, clone_function=_clone_function\n        )\n\n    @staticmethod\n    def _replace_attr_with_edl_embedding(model):\n        """"""Replace the keras embedding attributes in the model with\n        `elasticdl.layers.Embedding` layers.\n        """"""\n        for name, value in model.__dict__.items():\n            if type(\n                value\n            ) == tf.keras.layers.Embedding and _need_partition_embedding(\n                value\n            ):\n                logger.info(\n                    ""Replace {} layer with ""\n                    ""elasticdl.layers.Embedding"".format(value)\n                )\n                initializer_name = tf.keras.initializers.serialize(\n                    value.embeddings_initializer\n                )[""class_name""]\n                embedding_layer = Embedding(\n                    output_dim=value.output_dim,\n                    input_dim=value.input_dim,\n                    embeddings_initializer=initializer_name,\n                    mask_zero=value.mask_zero,\n                    input_length=value.input_length,\n                    name=value.name,\n                )\n                # The weights of subclass model is None, so we need to create\n                # the weight name which is ""{layer_name}/embeddings:0"" in\n                # tf.keras.layers.Embedding.\n                embedding_layer.set_embedding_weight_name(\n                    value.name + ""/embeddings:0""\n                )\n                setattr(model, name, embedding_layer)\n            elif type(value) == SparseEmbedding and _need_partition_embedding(\n                value\n            ):\n                logger.info(\n                    ""Replace {} layer with ""\n                    ""elasticdl.layers.Embedding"".format(value)\n                )\n                embedding_layer = Embedding(\n                    output_dim=value.output_dim,\n                    input_dim=value.input_dim,\n                    embeddings_initializer=initializer_name,\n                    combiner=value.combiner,\n                    name=value.name,\n                )\n                embedding_layer.set_embedding_weight_name(\n                    value.name + ""/embeddings:0""\n                )\n                setattr(model, name, embedding_layer)\n            elif type(value) == tf.keras.layers.DenseFeatures:\n                feature_layer = _replace_tf_embedding_column_with_edl(value)\n                setattr(model, name, feature_layer)\n        return model\n\n    @staticmethod\n    def _replace_attr_with_keras_embedding(model):\n        """"""Replace the elasticdl.layers.Embedding attributes in the model\n        with `tf.keras.layers.Embedding` or `SparseEmbedding` layers.\n        """"""\n        for name, value in model.__dict__.items():\n            if type(value) == Embedding:\n                # The combiner is not None only for SparseEmbedding,\n                if value.combiner is not None:\n                    logger.info(""Replace elasticdl with SparseEmbedding"")\n                    embedding_layer = SparseEmbedding(\n                        output_dim=value.output_dim,\n                        input_dim=value.input_dim,\n                        embeddings_initializer=value.embeddings_initializer,\n                        combiner=value.combiner,\n                    )\n                else:\n                    logger.info(\n                        ""Replace elasticdl with tf.kerasl.layers.Embedding""\n                    )\n                    embedding_layer = tf.keras.layers.Embedding(\n                        output_dim=value.output_dim,\n                        input_dim=value.input_dim,\n                        embeddings_initializer=value.embeddings_initializer,\n                        mask_zero=value.mask_zero,\n                        input_length=value.input_length,\n                    )\n                setattr(model, name, embedding_layer)\n            elif type(value) == tf.keras.layers.DenseFeatures:\n                feature_layer = _replace_edl_embedding_column_with_tf(value)\n                setattr(model, name, feature_layer)\n        return model\n'"
elasticdl/python/common/model_utils.py,3,"b'import importlib.util\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.callbacks import CallbackList\n\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.data.odps_io import is_odps_configured\nfrom elasticdl.python.worker.prediction_outputs_processor import (\n    BasePredictionOutputsProcessor,\n)\n\n\ndef load_module(module_file):\n    spec = importlib.util.spec_from_file_location(module_file, module_file)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n\ndef load_model_from_module(model_def, model_module, model_params):\n    model_def_name = model_def.split(""."")[-1]\n    if model_def_name in model_module:\n        custom_model_name = model_def_name\n    else:\n        raise ValueError(\n            ""Cannot find the custom model function/class ""\n            ""in model definition files""\n        )\n    if model_params:\n        model_params_dict = get_dict_from_params_str(model_params)\n        return model_module[custom_model_name](**model_params_dict)\n    else:\n        return model_module[custom_model_name]()\n\n\ndef load_callbacks_from_module(callbacks_def, model_module):\n    callbacks_def_name = callbacks_def.split(""."")[-1]\n    callbacks_fn = _get_spec_value(callbacks_def_name, None, model_module)\n    callbacks = [] if callbacks_fn is None else callbacks_fn()\n    return CallbackList(callbacks)\n\n\ndef set_callback_parameters(\n    callback_list,\n    batch_size=None,\n    saved_model_path=None,\n    checkpoint_path=None,\n):\n    """"""Sets callback parameters.\n\n    Arguments:\n        callback_list: CallbackList instance.\n        batch_size: Number of samples per batch\n        saved_model_path: Path to export SavedModel\n        checkpoint_path: Path to save checkpoint\n    """"""\n    callback_params = {\n        ""batch_size"": batch_size,\n        ""saved_model_path"": saved_model_path,\n        ""checkpoint_path"": checkpoint_path,\n    }\n    callback_list.set_params(callback_params)\n\n\ndef get_dict_from_params_str(params_str):\n    """"""Get the dictionary of kv pairs in a string separated\n    by semi-colon.""""""\n    params_dict = {}\n    if params_str:\n        kvs = params_str.split("";"")\n        for kv in kvs:\n            splitted = kv.strip().split(""="")\n            k = splitted[0]\n            # if there is \'=\' in value, need to restore it.\n            v = ""="".join(splitted[1:])\n            try:\n                params_dict[k] = eval(v)\n            except Exception:\n                params_dict[k] = v\n    return params_dict\n\n\ndef get_module_file_path(model_zoo, spec_key):\n    """"""Get the path to module file from model zoo and the spec string.\n\n    For example, if `model_zoo = ""model_zoo""` and\n    `spec_key = ""test_module.custom_model""`, the function returns\n    ""model_zoo/test_module.py"".\n    """"""\n    return os.path.join(model_zoo, ""/"".join(spec_key.split(""."")[:-1]) + "".py"")\n\n\ndef _get_spec_value(spec_key, model_zoo, default_module, required=False):\n    """"""Get the value to the given spec key.\n\n    Notes:\n\n    * If the dot-splitted spec key (e.g. ""test_module.custom_model""\n      is splitted into ""test_module"" and ""custom_model"") is of length 1\n      (e.g. `spec_key` is ""custom_model""), return the value in the\n      specified `default_module`.\n    * If the spec key does not exist in the module, return `None`.\n    """"""\n    spec_key_items = spec_key.split(""."")\n    spec_key_base = spec_key_items[-1]\n    if len(spec_key_items) == 1:\n        spec_key_module = default_module\n    else:\n        spec_key_module = load_module(\n            get_module_file_path(model_zoo, spec_key)\n        ).__dict__\n    spec_value = (\n        spec_key_module[spec_key_base]\n        if spec_key_base in spec_key_module\n        else None\n    )\n    if required and spec_value is None:\n        raise Exception(\n            ""Missing required spec key %s in the module: %s""\n            % (spec_key_base, spec_key)\n        )\n    return spec_value\n\n\ndef get_model_spec(\n    model_zoo,\n    model_def,\n    model_params,\n    dataset_fn,\n    loss,\n    optimizer,\n    eval_metrics_fn,\n    prediction_outputs_processor,\n    custom_data_reader,\n    callbacks,\n):\n    """"""Get the model spec items in a tuple.\n\n    The model spec tuple contains the following items in order:\n\n    * The model object instantiated with parameters specified\n      in `model_params`,\n    * The `dataset_fn`,\n    * The `loss`,\n    * The `optimizer`,\n    * The `eval_metrics_fn`,\n    * The `prediction_outputs_processor`. Note that it will print\n      warning if it\'s not inherited from `BasePredictionOutputsProcessor`.\n    * The `custom_data_reader`\n    * The `callbacks`\n    """"""\n    model_def_module_file = get_module_file_path(model_zoo, model_def)\n    default_module = load_module(model_def_module_file).__dict__\n    model = load_model_from_module(model_def, default_module, model_params)\n    prediction_outputs_processor = _get_spec_value(\n        prediction_outputs_processor, model_zoo, default_module\n    )\n    if prediction_outputs_processor and not isinstance(\n        prediction_outputs_processor, BasePredictionOutputsProcessor\n    ):\n        logger.warning(\n            ""prediction_outputs_processor is not ""\n            ""inherited from BasePredictionOutputsProcessor. ""\n            ""Prediction outputs may not be processed correctly.""\n        )\n\n    # If ODPS data source is used, dataset_fn is optional\n    dataset_fn_required = not is_odps_configured()\n    callbacks_list = load_callbacks_from_module(callbacks, default_module)\n\n    return (\n        model,\n        _get_spec_value(\n            dataset_fn, model_zoo, default_module, required=dataset_fn_required\n        ),\n        _get_spec_value(loss, model_zoo, default_module, required=True),\n        _get_spec_value(optimizer, model_zoo, default_module, required=True),\n        _get_spec_value(\n            eval_metrics_fn, model_zoo, default_module, required=True\n        ),\n        prediction_outputs_processor,\n        _get_spec_value(custom_data_reader, model_zoo, default_module),\n        callbacks_list,\n    )\n\n\ndef find_layer(model, layer_class):\n    """"""\n    Find all layers in model that are instances of layer_class\n    """"""\n    layers = []\n    for layer in model.layers:\n        if isinstance(layer, layer_class):\n            layers.append(layer)\n        elif hasattr(layer, ""layers""):\n            # search in nested layers\n            layers += find_layer(layer, layer_class)\n    return layers\n\n\ndef get_non_embedding_trainable_vars(model, embedding_layers):\n    """"""\n    Get trainable variables which are not from ElasticDL embedding layers.\n    """"""\n    embedding_items = []\n    for layer in embedding_layers:\n        embedding_items.extend(layer.trainable_variables)\n    non_embedding_trainable_vars = []\n    for var in model.trainable_variables:\n        is_embedding_item = False\n        for embedding_item in embedding_items:\n            if var is embedding_item:\n                is_embedding_item = True\n                break\n        if not is_embedding_item:\n            non_embedding_trainable_vars.append(var)\n    return non_embedding_trainable_vars\n\n\ndef get_optimizer_info(optimizer):\n    """"""\n    Get optimizer type and its argument values\n    """"""\n    # TODO: add more supported optimizers\n    OPT_ARGUMENTS = {\n        ""SGD"": [""learning_rate"", ""momentum"", ""nesterov""],\n        ""Adam"": [""learning_rate"", ""beta_1"", ""beta_2"", ""epsilon"", ""amsgrad""],\n        ""Adagrad"": [""learning_rate"", ""epsilon""],\n        ""unkown"": [],\n    }\n    opt_type = ""unknown""\n    opt_argument = """"\n\n    if isinstance(optimizer, tf.keras.optimizers.SGD):\n        opt_type = ""SGD""\n    elif isinstance(optimizer, tf.keras.optimizers.Adam):\n        opt_type = ""Adam""\n    elif isinstance(optimizer, tf.keras.optimizers.Adagrad):\n        opt_type = ""Adagrad""\n    opt_config = optimizer.get_config()\n    for arg_name in OPT_ARGUMENTS[opt_type]:\n        arg_value = opt_config[arg_name]\n        # For callable, only get the value from 1st call\n        if callable(arg_value):\n            arg_value = arg_value()\n        opt_argument += arg_name + ""="" + str(arg_value) + "";""\n    return opt_type, opt_argument\n'"
elasticdl/python/common/save_utils.py,2,"b'import contextlib\nimport os\nimport shutil\nimport tempfile\n\nimport tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.hash_utils import int_to_id, string_to_id\nfrom elasticdl.python.common.tensor_utils import (\n    pb_to_indexed_slices,\n    pb_to_ndarray,\n)\nfrom elasticdl.python.ps.embedding_table import create_embedding_table\nfrom elasticdl.python.ps.parameters import Parameters\n\n\ndef save_pb_to_file(pb_obj, file_name):\n    """"""Save a protobuf object to file""""""\n    encoded_model = pb_obj.SerializeToString()\n    with open(file_name, ""wb"") as f:\n        f.write(encoded_model)\n\n\ndef load_pb_from_file(pb_obj, file_name):\n    """"""Load a protobuf object from a file""""""\n    with open(file_name, ""rb"") as f:\n        pb_obj.ParseFromString(f.read())\n    return pb_obj\n\n\ndef _get_params_shard_from_pb(model_pb, shard_index, shard_num):\n    """"""Get parameters including variables values and embedding table\n    from a model protobuf.\n    Args:\n        model_pb: A Model protobuf instance.\n        shard_index: Model shard index.\n        shard_num: The total number of model shards.\n    Return:\n        non_embedding_vars: A Python dict in which the key is a variable\n            name and the value is a `tf.Variable` object.\n        embedding_table_values: A Python dict in which the key is an embedding\n            table name and the value is a tuple with 2 elements. The value[0]\n            is indices and value[1] is the corresponding embedding vector.\n    """"""\n    non_embedding_vars = {}\n    embedding_table_values = {}\n\n    for name, pb in model_pb.dense_parameters.items():\n        if string_to_id(name, shard_num) == shard_index:\n            non_embedding_vars[name] = tf.Variable(\n                initial_value=pb_to_ndarray(pb), trainable=True\n            )\n    for name, pb in model_pb.embedding_tables.items():\n        embedding_table_values.setdefault(name, ([], []))\n        t = pb_to_indexed_slices(pb)\n        for embedding_id, vector in zip(t.indices, t.values):\n            if int_to_id(embedding_id, shard_num) == shard_index:\n                embedding_table_values[name][0].append(embedding_id)\n                embedding_table_values[name][1].append(vector)\n    return non_embedding_vars, embedding_table_values\n\n\nclass Checkpoint(object):\n    def __init__(self, version, file):\n        self.version = version\n        self.file = file\n\n\nclass CheckpointSaver(object):\n    """"""Checkpoint Saver implementation""""""\n\n    def __init__(\n        self,\n        checkpoint_dir,\n        checkpoint_steps,\n        keep_checkpoint_max,\n        include_evaluation,\n    ):\n        """"""\n        Arguments:\n            checkpoint_dir: The directory to store the checkpoint files.\n                            Directory will be created if not exist.\n            checkpoint_steps: Save checkpoint every this many steps.\n            keep_checkpoint_max: The maximum number of recent checkpoint\n                                 files to keep.\n        """"""\n        self._directory = checkpoint_dir\n        self._steps = checkpoint_steps\n        self._max_versions = keep_checkpoint_max\n        if not self._directory:\n            self._directory = os.getcwd() + ""/checkpoint_dir""\n        if self._steps:\n            os.makedirs(self._directory, exist_ok=True)\n        self._checkpoint_dir_list = []\n        self._include_evaluation = include_evaluation\n        self._eval_checkpoint_dir = (\n            tempfile.mkdtemp() if include_evaluation else """"\n        )\n\n    def _get_checkpoint_file(\n        self, version, is_eval_checkpoint=False, shard_index=0, shard_num=1\n    ):\n        checkpoint_dir = (\n            self._eval_checkpoint_dir\n            if is_eval_checkpoint\n            else self._directory\n        )\n        checkpoint_version_dir = os.path.join(\n            checkpoint_dir, ""version-%s"" % str(version)\n        )\n        with contextlib.suppress(FileExistsError):\n            os.makedirs(checkpoint_version_dir, exist_ok=True)\n        return ""%s/variables-%s-of-%s.ckpt"" % (\n            checkpoint_version_dir,\n            str(shard_index),\n            str(shard_num),\n        )\n\n    def is_enabled(self):\n        """"""Checkpoint is enabled or not""""""\n        return self._steps\n\n    def need_to_checkpoint(self, version):\n        """"""Check if the given model version needs to be checkpointed""""""\n        return self.is_enabled() and version % self._steps == 0\n\n    def save(\n        self, version, model, is_eval_checkpoint, shard_index=0, shard_num=1\n    ):\n        """"""Checkpoint the given model\n\n        Args:\n            version (int): iteration steps\n            model: a model protobuf instance\n            is_eval_checkpoint (bool): if True, the model will be saved to\n                a temporary directory.\n            shard_index (int): default 0. The shard index in all\n                model shard files, e.g. the shard_index is PS instance index\n                using ParameterServerStrategy.\n            shard_number (int): default 1. The number of model shards,\n                e.g. shard_number is the number of PS instances using\n                ParameterServerStrategy.\n        """"""\n        filename = self._get_checkpoint_file(\n            version, is_eval_checkpoint, shard_index, shard_num\n        )\n        save_pb_to_file(model, filename)\n        if not is_eval_checkpoint:\n            self._checkpoint_dir_list.append(os.path.dirname(filename))\n            if self._max_versions:\n                self._delete_old_checkpoints_if_needed()\n\n    def _delete_old_checkpoints_if_needed(self):\n        """"""Delete the oldest checkpoint files and keep the number of\n        checkpoints is not beyond max_version.\n        """"""\n        if len(self._checkpoint_dir_list) > self._max_versions:\n            old_version_dir = self._checkpoint_dir_list[0]\n\n            # Some PS instances have not saved checkpoint shard files of\n            # the version if invalid. And the slowest PS will remove the\n            # old version checkpoint.\n            if self.check_checkpoint_valid(old_version_dir):\n                self._checkpoint_dir_list.pop(0)\n                with contextlib.suppress(FileNotFoundError):\n                    shutil.rmtree(old_version_dir)\n\n    @staticmethod\n    def get_valid_lastest_version_dir(checkpoint_dir):\n        """"""Get the valid and lastest version checkpoint directory""""""\n        if not checkpoint_dir or not os.path.exists(checkpoint_dir):\n            return None\n\n        version_folders = os.listdir(checkpoint_dir)\n        if not version_folders:\n            return None\n        version_num = [int(v.split(""-"")[-1]) for v in version_folders]\n        version_folder_pairs = sorted(\n            zip(version_num, version_folders), reverse=True\n        )\n        for version, folder in version_folder_pairs:\n            folder_dir = os.path.join(checkpoint_dir, folder)\n            if CheckpointSaver.check_checkpoint_valid(folder_dir):\n                return folder_dir\n        return None\n\n    @staticmethod\n    def check_checkpoint_valid(checkpoint_dir):\n        """"""Check whether the checkpoint directory is valid. The filename template\n        in the checkpoint directory like ""variables-{i}-of-{N}.ckpt"". We will\n        parse any filename to get N which is the total number of parameters\n        shards. It is valid if the number of files in the directory N.\n        """"""\n        if not os.path.exists(checkpoint_dir):\n            return False\n\n        shard_files = os.listdir(checkpoint_dir)\n        if not shard_files:\n            return False\n\n        shard_file_prefix = shard_files[0].split(""."")[0]\n        expected_shard_num = int(shard_file_prefix.split(""-"")[-1])\n        return expected_shard_num == len(shard_files)\n\n    @staticmethod\n    def restore_params_from_checkpoint(checkpoint_dir, shard_index, shard_num):\n        """"""Restore a shard parameters from the checkpoint directory.\n        If shard_num=1, a entire model parameters will be restored.\n\n        Args:\n            checkpoint_dir: a directory with checkpoint files.\n            shard_index: Model shard index, e.g. the PS instance index\n                using ParameterServerStrategy with multiple PS instances.\n            shard_num: The total number of model shards, e.g. the total PS\n                instancecount using ParameterServerStrategy with multiple\n                PS instances.\n\n        Return:\n            parameters: A Parameter object which contains model version,\n                non-embedding parameters and embedding tables for the\n                PS instance with ps_id.\n        """"""\n\n        variable_shard_files = os.listdir(checkpoint_dir)\n        non_embedding_vars = {}\n        embedding_tables = {}\n        version = None\n        for shard_file in variable_shard_files:\n            shard_file_path = os.path.join(checkpoint_dir, shard_file)\n            model_pb = elasticdl_pb2.Model()\n            model_pb = load_pb_from_file(model_pb, shard_file_path)\n            if version is None:\n                version = model_pb.version\n            elif version != model_pb.version:\n                raise ValueError(\n                    ""The versions in model shards are not consistent""\n                )\n\n            for embedding_info_pb in model_pb.embedding_table_infos:\n                embedding_table = create_embedding_table(embedding_info_pb)\n                embedding_tables.setdefault(\n                    embedding_table.name, embedding_table\n                )\n\n            (\n                shard_non_embedding_vars,\n                shard_embedding_table_values,\n            ) = _get_params_shard_from_pb(model_pb, shard_index, shard_num)\n\n            non_embedding_vars.update(shard_non_embedding_vars)\n            for name, pair in shard_embedding_table_values.items():\n                embedding_tables[name].set(pair[0], pair[1])\n\n        parameters = Parameters()\n        parameters.non_embedding_params.update(non_embedding_vars)\n        parameters.embedding_params.update(embedding_tables)\n        parameters.version = version\n        return parameters\n\n    @staticmethod\n    def get_version_from_checkpoint(checkpoint_dir):\n        """"""Get model version from the checkpoint. There may be several shard\n        files in the checkpoint directory. The model versions of shard files\n        are same, so we only need to read one shard file to get model version.\n        """"""\n        variable_shard_files = os.listdir(checkpoint_dir)\n        shard_file_path = os.path.join(checkpoint_dir, variable_shard_files[0])\n        model_pb = elasticdl_pb2.Model()\n        model_pb = load_pb_from_file(model_pb, shard_file_path)\n        return model_pb.version\n'"
elasticdl/python/common/tensor_utils.py,7,"b'from collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.core.framework import tensor_pb2\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.dtypes import (\n    dtype_numpy_to_tensor,\n    dtype_tensor_to_numpy,\n)\n\nTensor = namedtuple(""Tensor"", (""name"", ""values"", ""indices""))\n\n\ndef merge_indexed_slices(*args):\n    return tf.IndexedSlices(\n        tf.concat([i.values for i in args], axis=0),\n        tf.concat([i.indices for i in args], axis=0),\n    )\n\n\ndef deduplicate_indexed_slices(values, indices):\n    """"""\n    Sum up the values associated with duplicated indices and\n    return unique indices with corresponding summed values.\n    Args:\n        values: A Tensor with rank >= 1.\n        indices: A one-dimension integer of Tensor.\n    Returns:\n        A tuple of (`sum_combined_values`, `unique_indices`).\n        `sum_combined_values` contains the sum of `values` associated\n        with each unique indice.\n        `unique_indices` is a de-duplicated version of `indices`.\n    """"""\n    unique_indices, new_index_positions = tf.unique(indices)\n    sum_combined_values = tf.math.unsorted_segment_sum(\n        values, new_index_positions, tf.shape(unique_indices)[0]\n    )\n\n    return (sum_combined_values, unique_indices)\n\n\ndef serialize_ndarray(array, pb):\n    dtype = dtype_numpy_to_tensor(array.dtype)\n    if not dtype:\n        raise ValueError(""Dtype of ndarray %s is not supported"", array.dtype)\n    pb.dtype = dtype\n    pb.tensor_content = array.tobytes()\n    for d in array.shape:\n        pb_d = pb.tensor_shape.dim.add()\n        pb_d.size = d\n\n\ndef ndarray_to_pb(array):\n    pb = tensor_pb2.TensorProto()\n    serialize_ndarray(array, pb)\n    return pb\n\n\ndef pb_to_ndarray(pb):\n    if not pb.tensor_shape:\n        raise ValueError(""PB has no dim defined"")\n    dtype = dtype_tensor_to_numpy(pb.dtype)\n    size = dtype.itemsize\n    shape = [d.size for d in pb.tensor_shape.dim]\n    for d in shape:\n        size *= d\n    if size != len(pb.tensor_content):\n        raise ValueError(\n            ""PB size mismatch, dim: %s, len(content): %d"",\n            str(shape),\n            len(pb.tensor_content),\n        )\n    array = np.ndarray(shape=shape, dtype=dtype, buffer=pb.tensor_content)\n    return array\n\n\ndef pb_to_indexed_slices(pb):\n    concat_tensors = pb_to_ndarray(pb.concat_tensors)\n    ids = np.array([int(i) for i in pb.ids])\n    return tf.IndexedSlices(concat_tensors, ids)\n\n\ndef serialize_indexed_slices(slices, pb):\n    serialize_ndarray(slices.values, pb.concat_tensors)\n    if (\n        isinstance(slices.indices, np.ndarray)\n        and len(slices.indices.shape) > 1\n    ):\n        raise ValueError(\n            ""IndexedSlices pb only accepts indices with one dimension, got %d"",\n            len(slices.indices.shape),\n        )\n    pb.ids.extend(slices.indices)\n\n\ndef indexed_slices_to_pb(slices):\n    pb = elasticdl_pb2.IndexedSlicesProto()\n    serialize_indexed_slices(slices, pb)\n    return pb\n'"
elasticdl/python/common/timing_utils.py,0,"b'import time\n\nTIMING_GROUP = [\n    ""task_process"",\n    ""batch_process"",\n    ""get_model"",\n    ""report_gradient"",\n]\n\n\nclass Timing(object):\n    def __init__(self, enable, logger=None):\n        self.enable = enable\n        self.logger = logger\n        self.timings = {}\n        self.start_time = {}\n        self.reset()\n\n    def reset(self):\n        if not self.enable:\n            return\n        for timing_type in TIMING_GROUP:\n            self.timings[timing_type] = 0\n\n    def start_record_time(self, timing_type):\n        if not self.enable:\n            return\n        self.start_time[timing_type] = time.time()\n\n    def end_record_time(self, timing_type):\n        if not self.enable:\n            return\n        self.timings[timing_type] += time.time() - self.start_time[timing_type]\n\n    def report_timing(self, reset=False):\n        if not self.enable:\n            return\n        for timing_type in TIMING_GROUP:\n            self.logger.debug(\n                ""%s time is %.6g seconds""\n                % (timing_type, self.timings[timing_type])\n            )\n        if reset:\n            self.reset()\n'"
elasticdl/python/data/__init__.py,0,b''
elasticdl/python/data/odps_io.py,0,"b'import os\nimport queue\nimport random\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor as Executor\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport odps\nfrom odps import ODPS\nfrom odps.models import Schema\n\nfrom elasticdl.python.common.constants import MaxComputeConfig\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n\ndef _nested_list_size(nested_list):\n    """"""\n    Obtains the memory size for the nested list.\n    """"""\n    total = sys.getsizeof(nested_list)\n    for i in nested_list:\n        if isinstance(i, list):\n            total += _nested_list_size(i)\n        else:\n            total += sys.getsizeof(i)\n\n    return total\n\n\ndef _configure_odps_options(endpoint, options=None):\n    if endpoint is not None and endpoint != """":\n        odps.options.retry_times = options.get(""odps.options.retry_times"", 5)\n        odps.options.read_timeout = options.get(\n            ""odps.options.read_timeout"", 200\n        )\n        odps.options.connect_timeout = options.get(\n            ""odps.options.connect_timeout"", 200\n        )\n        odps.options.tunnel.endpoint = options.get(\n            ""odps.options.tunnel.endpoint"", None\n        )\n        if (\n            odps.options.tunnel.endpoint is None\n            and ""service.odps.aliyun-inc.com/api"" in endpoint\n        ):\n            odps.options.tunnel.endpoint = ""http://dt.odps.aliyun-inc.com""\n\n\ndef is_odps_configured():\n    return all(\n        k in os.environ\n        for k in (\n            MaxComputeConfig.PROJECT_NAME,\n            MaxComputeConfig.ACCESS_ID,\n            MaxComputeConfig.ACCESS_KEY,\n        )\n    )\n\n\nclass ODPSReader(object):\n    def __init__(\n        self,\n        project,\n        access_id,\n        access_key,\n        endpoint,\n        table,\n        partition=None,\n        num_processes=None,\n        options=None,\n        transform_fn=None,\n        columns=None,\n    ):\n        """"""\n        Constructs a `ODPSReader` instance.\n\n        Args:\n            project: Name of the ODPS project.\n            access_id: ODPS user access ID.\n            access_key: ODPS user access key.\n            endpoint: ODPS cluster endpoint.\n            table: ODPS table name.\n            tunnel_endpoint: ODPS tunnel endpoint.\n            partition: ODPS table\'s partition.\n            options: Other options passed to ODPS context.\n            num_processes: Number of parallel processes on this worker.\n                If `None`, use the number of cores.\n            transform_fn: Customized transfrom function\n            columns: list of table column names\n        """"""\n        super(ODPSReader, self).__init__()\n\n        if table.find(""."") > 0:\n            project, table = table.split(""."")\n        if options is None:\n            options = {}\n        self._project = project\n        self._access_id = access_id\n        self._access_key = access_key\n        self._endpoint = endpoint\n        self._table = table\n        self._partition = partition\n        self._num_processes = num_processes\n        _configure_odps_options(self._endpoint, options)\n        self._odps_table = ODPS(\n            self._access_id, self._access_key, self._project, self._endpoint,\n        ).get_table(self._table)\n\n        self._transform_fn = transform_fn\n        self._columns = columns\n\n    def reset(self, shards, shard_size):\n        """"""\n        The parallel reader launches multiple worker processes to read\n        records from an ODPS table and applies `transform_fn` to each record.\n        If `transform_fn` is not set, the transform stage will be skipped.\n\n        Worker process:\n        1. get a shard from index queue, the shard is a pair (start, count)\n            of the ODPS table\n        2. reads the records from the ODPS table\n        3. apply `transform_fn` to each record\n        4. put records to the result queue\n\n        Main process:\n        1. call `reset` to create a number of shards given a input shard\n        2. put shard to index queue of workers in round-robin way\n        3. call `get_records`  to get transformed data from result queue\n        4. call `stop` to stop the workers\n        """"""\n        self._result_queue = Queue()\n        self._index_queues = []\n        self._workers = []\n\n        self._shards = []\n        self._shard_idx = 0\n        self._worker_idx = 0\n\n        for i in range(self._num_processes):\n            index_queue = Queue()\n            self._index_queues.append(index_queue)\n\n            p = Process(target=self._worker_loop, args=(i,))\n            p.daemon = True\n            p.start()\n            self._workers.append(p)\n\n        self._create_shards(shards, shard_size)\n        for i in range(2 * self._num_processes):\n            self._put_index()\n\n    def get_shards_count(self):\n        return len(self._shards)\n\n    def get_records(self):\n        data = self._result_queue.get()\n        self._put_index()\n        return data\n\n    def stop(self):\n        for q in self._index_queues:\n            q.put((None, None))\n\n    def _worker_loop(self, worker_id):\n        while True:\n            index = self._index_queues[worker_id].get()\n            if index[0] is None and index[1] is None:\n                break\n\n            records = []\n            for record in self.record_generator_with_retry(\n                start=index[0],\n                end=index[0] + index[1],\n                columns=self._columns,\n                transform_fn=self._transform_fn,\n            ):\n                records.append(record)\n            self._result_queue.put(records)\n\n    def _create_shards(self, shards, shard_size):\n        start = shards[0]\n        count = shards[1]\n        m = count // shard_size\n        n = count % shard_size\n\n        for i in range(m):\n            self._shards.append((start + i * shard_size, shard_size))\n        if n != 0:\n            self._shards.append((start + m * shard_size, n))\n\n    def _next_worker_id(self):\n        cur_id = self._worker_idx\n        self._worker_idx += 1\n        if self._worker_idx == self._num_processes:\n            self._worker_idx = 0\n        return cur_id\n\n    def _put_index(self):\n        # put index to the index queue of each worker\n        # with Round-Robin way\n        if self._shard_idx < len(self._shards):\n            worker_id = self._next_worker_id()\n            shard = self._shards[self._shard_idx]\n            self._index_queues[worker_id].put(shard)\n            self._shard_idx += 1\n\n    def to_iterator(\n        self,\n        num_workers,\n        worker_index,\n        batch_size,\n        epochs=1,\n        shuffle=False,\n        columns=None,\n        cache_batch_count=None,\n        limit=-1,\n    ):\n        """"""\n        Load slices of ODPS table (partition of table if `partition`\n        was specified) data with Python Generator.\n        Args:\n            num_workers: Total number of worker in the cluster.\n            worker_index: Current index of the worker in the cluster.\n            batch_size: Size of a slice.\n            epochs: Repeat the data for this many times.\n            shuffle: Whether to shuffle the data or rows.\n            columns: The list of columns to load. If `None`,\n                use all schema names of ODPS table.\n            cache_batch_count: The cache batch count.\n            limit: The limit for the table size to load.\n        """"""\n        if not worker_index < num_workers:\n            raise ValueError(\n                ""index of worker should be less than number of worker""\n            )\n        if not batch_size > 0:\n            raise ValueError(""batch_size should be positive"")\n\n        table_size = self.get_table_size()\n        if 0 < limit < table_size:\n            table_size = limit\n        if columns is None:\n            columns = self._odps_table.schema.names\n\n        if cache_batch_count is None:\n            cache_batch_count = self._estimate_cache_batch_count(\n                columns=columns, table_size=table_size, batch_size=batch_size\n            )\n\n        large_batch_size = batch_size * cache_batch_count\n\n        overall_items = range(0, table_size, large_batch_size)\n\n        if len(overall_items) < num_workers:\n            overall_items = range(0, table_size, int(table_size / num_workers))\n\n        worker_items = list(\n            np.array_split(np.asarray(overall_items), num_workers)[\n                worker_index\n            ]\n        )\n        if shuffle:\n            random.shuffle(worker_items)\n        worker_items_with_epoch = worker_items * epochs\n\n        # `worker_items_with_epoch` is the total number of batches\n        # that needs to be read and the worker number should not\n        # be larger than `worker_items_with_epoch`\n        if self._num_processes is None:\n            self._num_processes = min(8, len(worker_items_with_epoch))\n        else:\n            self._num_processes = min(\n                self._num_processes, len(worker_items_with_epoch)\n            )\n\n        if self._num_processes == 0:\n            raise ValueError(\n                ""Total worker number is 0. Please check if table has data.""\n            )\n\n        with Executor(max_workers=self._num_processes) as executor:\n\n            futures = queue.Queue()\n            # Initialize concurrently running processes according\n            # to `num_processes`\n            for i in range(self._num_processes):\n                range_start = worker_items_with_epoch[i]\n                range_end = min(range_start + large_batch_size, table_size)\n                future = executor.submit(\n                    self.read_batch, range_start, range_end, columns\n                )\n                futures.put(future)\n\n            worker_items_index = self._num_processes\n\n            while not futures.empty():\n                if worker_items_index < len(worker_items_with_epoch):\n                    range_start = worker_items_with_epoch[worker_items_index]\n                    range_end = min(range_start + large_batch_size, table_size)\n                    future = executor.submit(\n                        self.read_batch, range_start, range_end, columns\n                    )\n                    futures.put(future)\n                    worker_items_index = worker_items_index + 1\n\n                head_future = futures.get()\n                records = head_future.result()\n                for i in range(0, len(records), batch_size):\n                    yield records[i : i + batch_size]  # noqa: E203\n\n    def read_batch(self, start, end, columns=None, max_retries=3):\n        """"""\n        Read ODPS table in chosen row range [ `start`, `end` ) with the\n        specified columns `columns`.\n        Args:\n            start: The row index to start reading.\n            end: The row index to end reading.\n            columns: The list of column to read.\n            max_retries : The maximum number of retries in case of exceptions.\n        Returns:\n            Two-dimension python list with shape: (end - start, len(columns))\n        """"""\n        retry_count = 0\n        if columns is None:\n            columns = self._odps_table.schema.names\n        while retry_count < max_retries:\n            try:\n                record_gen = self.record_generator(start, end, columns)\n                return [record for record in record_gen]\n            except Exception as e:\n                if retry_count >= max_retries:\n                    raise Exception(""Exceeded maximum number of retries"")\n                logger.warning(\n                    ""ODPS read exception {} for {} in {}.""\n                    ""Retrying time: {}"".format(\n                        e, columns, self._table, retry_count\n                    )\n                )\n                time.sleep(5)\n                retry_count += 1\n\n    def record_generator_with_retry(\n        self, start, end, columns=None, max_retries=3, transform_fn=None\n    ):\n        """"""Wrap record_generator with retry to avoid ODPS table read failure\n        due to network instability.\n        """"""\n        retry_count = 0\n        while retry_count < max_retries:\n            try:\n                for record in self.record_generator(start, end, columns):\n                    if transform_fn:\n                        record = transform_fn(record)\n                    yield record\n                break\n            except Exception as e:\n                if retry_count >= max_retries:\n                    raise Exception(""Exceeded maximum number of retries"")\n                logger.warning(\n                    ""ODPS read exception {} for {} in {}.""\n                    ""Retrying time: {}"".format(\n                        e, columns, self._table, retry_count\n                    )\n                )\n                time.sleep(5)\n                retry_count += 1\n\n    def record_generator(self, start, end, columns=None):\n        """"""Generate records from an ODPS table\n        """"""\n        if columns is None:\n            columns = self._odps_table.schema.names\n        with self._odps_table.open_reader(\n            partition=self._partition, reopen=False\n        ) as reader:\n            for record in reader.read(\n                start=start, count=end - start, columns=columns\n            ):\n                yield [str(record[column]) for column in columns]\n\n    def get_table_size(self, max_retries=3):\n        retry_count = 0\n        while retry_count < max_retries:\n            try:\n                with self._odps_table.open_reader(\n                    partition=self._partition\n                ) as reader:\n                    return reader.count\n            except Exception as e:\n                if retry_count >= max_retries:\n                    raise Exception(""Exceeded maximum number of retries"")\n                logger.warning(\n                    ""ODPS read exception {} to get table size.""\n                    ""Retrying time: {}"".format(e, retry_count)\n                )\n                time.sleep(5)\n                retry_count += 1\n\n    def _estimate_cache_batch_count(self, columns, table_size, batch_size):\n        """"""\n        This function calculates the appropriate cache batch size\n        when we download from ODPS, if batch size is small, we will\n        repeatedly create http connection and download small chunk of\n        data. To read more efficiently, we will read\n        `batch_size * cache_batch_count` lines of data.\n        However, determining a proper `cache_batch_count` is non-trivial.\n        Our heuristic now is to set a per download upper bound.\n        """"""\n\n        sample_size = 10\n        max_cache_batch_count = 50\n        upper_bound = 20 * 1000000\n\n        if table_size < sample_size:\n            return 1\n\n        batch = self.read_batch(start=0, end=sample_size, columns=columns)\n\n        size_sample = _nested_list_size(batch)\n        size_per_batch = size_sample * batch_size / sample_size\n\n        # `size_per_batch * cache_batch_count` will\n        # not exceed upper bound but will always greater than 0\n        cache_batch_count_estimate = max(int(upper_bound / size_per_batch), 1)\n\n        return min(cache_batch_count_estimate, max_cache_batch_count)\n\n\nclass ODPSWriter(object):\n    def __init__(\n        self,\n        project,\n        access_id,\n        access_key,\n        endpoint,\n        table,\n        columns=None,\n        column_types=None,\n        options=None,\n    ):\n        """"""\n        Constructs a `ODPSWriter` instance.\n\n        Args:\n            project: Name of the ODPS project.\n            access_id: ODPS user access ID.\n            access_key: ODPS user access key.\n            endpoint: ODPS cluster endpoint.\n            table: ODPS table name.\n            columns: The list of column names in the table,\n                which will be inferred if the table exits.\n            column_types"" The list of column types in the table,\n                which will be inferred if the table exits.\n            options: Other options passed to ODPS context.\n        """"""\n        super(ODPSWriter, self).__init__()\n\n        if table.find(""."") > 0:\n            project, table = table.split(""."")\n        if options is None:\n            options = {}\n        self._project = project\n        self._access_id = access_id\n        self._access_key = access_key\n        self._endpoint = endpoint\n        self._table = table\n        self._columns = columns\n        self._column_types = column_types\n        self._odps_table = None\n        _configure_odps_options(self._endpoint, options)\n        self._odps_client = ODPS(\n            self._access_id, self._access_key, self._project, self._endpoint\n        )\n\n    def _initialize_table(self):\n        if self._odps_client.exist_table(self._table, self._project):\n            self._odps_table = self._odps_client.get_table(\n                self._table, self._project\n            )\n        else:\n            if self._columns is None or self._column_types is None:\n                raise ValueError(\n                    ""columns and column_types need to be ""\n                    ""specified for non-existing table.""\n                )\n            schema = Schema.from_lists(\n                self._columns, self._column_types, [""worker""], [""string""]\n            )\n            self._odps_table = self._odps_client.create_table(\n                self._table, schema\n            )\n\n    def from_iterator(self, records_iter, worker_index):\n        if self._odps_table is None:\n            self._initialize_table()\n        with self._odps_table.open_writer(\n            partition=""worker="" + str(worker_index), create_partition=True\n        ) as writer:\n            for records in records_iter:\n                writer.write(records)\n'"
elasticdl/python/data/odps_recordio_conversion_utils.py,1,"b'import os\nfrom collections import namedtuple\n\nimport numpy as np\nimport recordio\nimport tensorflow as tf\n\n\ndef _find_features_indices(\n    features_list, int_features, float_features, bytes_features\n):\n    """"""Finds the indices for different types of features.""""""\n    FeatureIndices = namedtuple(\n        ""FeatureIndices"", [""int_features"", ""float_features"", ""bytes_features""]\n    )\n    int_features_indices = [features_list.index(key) for key in int_features]\n    float_features_indices = [\n        features_list.index(key) for key in float_features\n    ]\n    bytes_features_indices = [\n        features_list.index(key) for key in bytes_features\n    ]\n    return FeatureIndices(\n        int_features_indices, float_features_indices, bytes_features_indices\n    )\n\n\ndef _parse_row_to_example(record, features_list, feature_indices):\n    """"""\n    Parses one row (a flat list or one-dimensional numpy array)\n    to a TensorFlow Example.\n    """"""\n    if isinstance(record, list):\n        record = np.array(record, dtype=object)\n\n    example = tf.train.Example()\n    # Note: these cannot be constructed dynamically since\n    # we cannot assign a value to an embedded message\n    # field in protobuf\n    for feature_ind in feature_indices.int_features:\n        example.features.feature[\n            features_list[feature_ind]\n        ].int64_list.value.append(\n            int(_maybe_encode_unicode_string(record[feature_ind]) or 0)\n        )\n    for feature_ind in feature_indices.float_features:\n        example.features.feature[\n            features_list[feature_ind]\n        ].float_list.value.append(\n            float(_maybe_encode_unicode_string(record[feature_ind]) or 0.0)\n        )\n    for feature_ind in feature_indices.bytes_features:\n        example.features.feature[\n            features_list[feature_ind]\n        ].bytes_list.value.append(\n            _maybe_encode_unicode_string(record[feature_ind])\n        )\n    return example\n\n\ndef _maybe_encode_unicode_string(record):\n    """"""Encodes unicode strings if needed.""""""\n    if isinstance(record, str):\n        record = bytes(record, ""utf-8"").strip()\n    return record\n\n\ndef _find_feature_indices_from_record(record):\n    """"""Find the indices of different feature types.""""""\n    feature_types = [type(value) for value in record]\n    FeatureIndices = namedtuple(\n        ""FeatureIndices"", [""int_features"", ""float_features"", ""bytes_features""]\n    )\n    return FeatureIndices(\n        [i for i, x in enumerate(feature_types) if x == int],\n        [i for i, x in enumerate(feature_types) if x == float],\n        [i for i, x in enumerate(feature_types) if x == str],\n    )\n\n\ndef write_recordio_shards_from_iterator(\n    records_iter, features_list, output_dir, records_per_shard\n):\n    """"""Writes RecordIO files from Python iterator of numpy arrays.""""""\n    # Take the first record batch to check whether it contains multiple items\n    first_record_batch = next(records_iter)\n    is_first_record_batch_consumed = False\n    is_multi_items_per_batch = any(\n        isinstance(i, list) for i in first_record_batch\n    )\n\n    # Find the features of different types that will be used\n    # in `_parse_row_to_example()` later\n    record = (\n        first_record_batch[0]\n        if is_multi_items_per_batch\n        else first_record_batch\n    )\n    feature_indices = _find_feature_indices_from_record(record)\n\n    writer = None\n    rows_written = 0\n    shards_written = 0\n    while True:\n        try:\n            # Make sure to consume the first record batch\n            if is_first_record_batch_consumed:\n                record_batch = next(records_iter)\n            else:\n                record_batch = first_record_batch\n                is_first_record_batch_consumed = True\n            if not is_multi_items_per_batch:\n                record_batch = [record_batch]\n\n            # Write each record in the batch to a RecordIO shard\n            for record in record_batch:\n                # Initialize the writer for the new shard\n                if rows_written % records_per_shard == 0:\n                    if writer is not None:\n                        writer.close()\n                    shard_file_path = os.path.join(\n                        output_dir, ""data-%05d"" % shards_written\n                    )\n                    writer = recordio.Writer(shard_file_path)\n                    shards_written += 1\n\n                writer.write(\n                    _parse_row_to_example(\n                        record, features_list, feature_indices\n                    ).SerializeToString()\n                )\n                rows_written += 1\n        except StopIteration:\n            break\n\n    writer.close()\n'"
elasticdl/python/elasticdl/__init__.py,0,b'from elasticdl.python.elasticdl import layers  # noqa: F401\n'
elasticdl/python/elasticdl/api.py,0,"b'import os\n\nfrom elasticdl.python.common import k8s_client as k8s\nfrom elasticdl.python.common.args import (\n    build_arguments_from_parsed_result,\n    parse_envs,\n    wrap_python_args_with_string,\n)\nfrom elasticdl.python.common.constants import (\n    BashCommandTemplate,\n    DistributionStrategy,\n)\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.elasticdl.image_builder import (\n    build_and_push_docker_image,\n    remove_images,\n)\nfrom elasticdl.python.elasticdl.local_executor import LocalExecutor\n\n\ndef train(args):\n    model_zoo = os.path.normpath(args.model_zoo)\n\n    if args.distribution_strategy == DistributionStrategy.LOCAL:\n        local_executor = LocalExecutor(args)\n        local_executor.run()\n        return\n\n    image_pre_built = bool(args.image_name)\n\n    image_name = (\n        args.image_name\n        if image_pre_built\n        else build_and_push_docker_image(\n            model_zoo=model_zoo,\n            base_image=args.image_base,\n            docker_image_repository=args.docker_image_repository,\n            extra_pypi=args.extra_pypi_index,\n            cluster_spec=args.cluster_spec,\n            docker_base_url=args.docker_base_url,\n            docker_tlscert=args.docker_tlscert,\n            docker_tlskey=args.docker_tlskey,\n        )\n    )\n\n    container_args = [\n        ""--worker_image"",\n        image_name,\n        ""--model_zoo"",\n        _model_zoo_in_docker(model_zoo, image_pre_built),\n        ""--cluster_spec"",\n        _cluster_spec_def_in_docker(args.cluster_spec),\n    ]\n    container_args.extend(\n        build_arguments_from_parsed_result(\n            args,\n            filter_args=[\n                ""model_zoo"",\n                ""cluster_spec"",\n                ""worker_image"",\n                ""force_use_kube_config_file"",\n                ""func"",\n            ],\n        )\n    )\n\n    _submit_job(image_name, args, container_args)\n    # TODO: print dashboard url after launching the master pod\n\n\ndef evaluate(args):\n    model_zoo = os.path.normpath(args.model_zoo)\n\n    image_pre_built = bool(args.image_name)\n\n    image_name = (\n        args.image_name\n        if image_pre_built\n        else build_and_push_docker_image(\n            model_zoo=model_zoo,\n            base_image=args.image_base,\n            docker_image_repository=args.docker_image_repository,\n            extra_pypi=args.extra_pypi_index,\n            cluster_spec=args.cluster_spec,\n            docker_base_url=args.docker_base_url,\n            docker_tlscert=args.docker_tlscert,\n            docker_tlskey=args.docker_tlskey,\n        )\n    )\n    container_args = [\n        ""--worker_image"",\n        image_name,\n        ""--model_zoo"",\n        _model_zoo_in_docker(model_zoo, image_pre_built),\n        ""--cluster_spec"",\n        _cluster_spec_def_in_docker(args.cluster_spec),\n    ]\n    container_args.extend(\n        build_arguments_from_parsed_result(\n            args,\n            filter_args=[\n                ""model_zoo"",\n                ""cluster_spec"",\n                ""worker_image"",\n                ""force_use_kube_config_file"",\n                ""func"",\n            ],\n        )\n    )\n\n    _submit_job(image_name, args, container_args)\n\n\ndef predict(args):\n    model_zoo = os.path.normpath(args.model_zoo)\n\n    image_pre_built = bool(args.image_name)\n\n    image_name = (\n        args.image_name\n        if image_pre_built\n        else build_and_push_docker_image(\n            model_zoo=model_zoo,\n            base_image=args.image_base,\n            docker_image_repository=args.docker_image_repository,\n            extra_pypi=args.extra_pypi_index,\n            cluster_spec=args.cluster_spec,\n            docker_base_url=args.docker_base_url,\n            docker_tlscert=args.docker_tlscert,\n            docker_tlskey=args.docker_tlskey,\n        )\n    )\n    container_args = [\n        ""--worker_image"",\n        image_name,\n        ""--model_zoo"",\n        _model_zoo_in_docker(model_zoo, image_pre_built),\n        ""--cluster_spec"",\n        _cluster_spec_def_in_docker(args.cluster_spec),\n    ]\n    container_args.extend(\n        build_arguments_from_parsed_result(\n            args,\n            filter_args=[\n                ""model_zoo"",\n                ""cluster_spec"",\n                ""worker_image"",\n                ""force_use_kube_config_file"",\n            ],\n        )\n    )\n\n    _submit_job(image_name, args, container_args)\n\n\ndef clean(args):\n    if args.docker_image_repository and args.all:\n        raise ValueError(\n            ""--docker_image_repository and --all cannot ""\n            ""be specified at the same time""\n        )\n    if not (args.docker_image_repository or args.all):\n        raise ValueError(\n            ""Either --docker_image_repository or --all ""\n            ""needs to be configured""\n        )\n    remove_images(\n        docker_image_repository=args.docker_image_repository,\n        docker_base_url=args.docker_base_url,\n        docker_tlscert=args.docker_tlscert,\n        docker_tlskey=args.docker_tlskey,\n    )\n\n\ndef _submit_job(image_name, client_args, container_args):\n    client = k8s.Client(\n        image_name=image_name,\n        namespace=client_args.namespace,\n        job_name=client_args.job_name,\n        event_callback=None,\n        cluster_spec=client_args.cluster_spec,\n        force_use_kube_config_file=client_args.force_use_kube_config_file,\n    )\n\n    container_args = wrap_python_args_with_string(container_args)\n\n    master_client_command = (\n        BashCommandTemplate.SET_PIPEFAIL\n        + "" python -m elasticdl.python.master.main""\n    )\n    container_args.insert(0, master_client_command)\n    if client_args.log_file_path:\n        container_args.append(\n            BashCommandTemplate.REDIRECTION.format(client_args.log_file_path)\n        )\n\n    python_command = "" "".join(container_args)\n    container_args = [""-c"", python_command]\n\n    if client_args.yaml:\n        client.dump_master_yaml(\n            resource_requests=client_args.master_resource_request,\n            resource_limits=client_args.master_resource_limit,\n            args=container_args,\n            pod_priority=client_args.master_pod_priority,\n            image_pull_policy=client_args.image_pull_policy,\n            restart_policy=client_args.restart_policy,\n            volume=client_args.volume,\n            envs=parse_envs(client_args.envs),\n            yaml=client_args.yaml,\n        )\n        logger.info(\n            ""ElasticDL job %s YAML has been dumped into file %s.""\n            % (client_args.job_name, client_args.yaml)\n        )\n    else:\n        client.create_master(\n            resource_requests=client_args.master_resource_request,\n            resource_limits=client_args.master_resource_limit,\n            args=container_args,\n            pod_priority=client_args.master_pod_priority,\n            image_pull_policy=client_args.image_pull_policy,\n            restart_policy=client_args.restart_policy,\n            volume=client_args.volume,\n            envs=parse_envs(client_args.envs),\n        )\n        logger.info(\n            ""ElasticDL job %s was successfully submitted. ""\n            ""The master pod is: %s.""\n            % (client_args.job_name, client.get_master_pod_name())\n        )\n\n\ndef _model_zoo_in_docker(model_zoo, image_pre_built):\n    if image_pre_built:\n        return model_zoo\n\n    MODEL_ROOT_PATH = ""/""\n    return os.path.join(MODEL_ROOT_PATH, os.path.basename(model_zoo))\n\n\ndef _cluster_spec_def_in_docker(cluster_spec):\n    CLUSTER_SPEC_ROOT_PATH = ""/cluster_spec""\n    return (\n        os.path.join(CLUSTER_SPEC_ROOT_PATH, os.path.basename(cluster_spec))\n        if cluster_spec\n        else """"\n    )\n'"
elasticdl/python/elasticdl/callbacks.py,6,"b'import os\nimport shutil\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.constants import Mode\n\n\nclass SavedModelExporter(tf.keras.callbacks.Callback):\n    """"""Export model using SavedModel after training.\n    Args:\n        task_data_service: TaskDataService to process data according the task\n        dataset_fn: function to process dataset\n        model_handler: to transform the trained model with ElasticDL embedding\n            layer to Keras native model.\n    """"""\n\n    def __init__(self, task_data_service, dataset_fn, model_handler):\n        self._model_handler = model_handler\n        self._task_data_service = task_data_service\n        self._dataset_fn = dataset_fn\n\n    def on_train_end(self, logs=None):\n        """"""Call on the train job end\n        Args:\n            logs: dict. Currently no data is passed to this argument for this\n                method but that may change in the future.\n        """"""\n        saved_model_path = self.params.get(""saved_model_path"", None)\n        if saved_model_path is None or saved_model_path == """":\n            return\n        batch_size = self.params.get(""batch_size"")\n        task = self._task_data_service.get_train_end_callback_task()\n        dataset = self._task_data_service.get_dataset_by_task(task)\n        if dataset is not None:\n            dataset = self._dataset_fn(\n                dataset,\n                Mode.PREDICTION,\n                self._task_data_service.data_reader.metadata,\n            )\n            dataset = dataset.batch(batch_size)\n            model = self._model_handler.get_model_to_export(\n                self.model, dataset\n            )\n            if os.path.exists(saved_model_path):\n                shutil.rmtree(saved_model_path)\n\n            # tf.saved_model cannot save the model if model.optimizer is not\n            # None and the model is not complied.\n            model.optimizer = None\n            tf.saved_model.save(model, saved_model_path)\n\n\nclass MaxStepsStopping(tf.keras.callbacks.Callback):\n    """"""Stop training if the training steps exceed the maximum.\n\n    Args:\n        max_steps:\n\n    Example:\n    ```python\n    from elasticdl.python.elasticdl.callbacks import MaxStepsStopping\n\n    def callbacks():\n        # This callback will stop the training when the training steps\n        # exceed the max_steps.\n        max_steps_stopping = MaxStepsStopping(max_steps=1000)\n        return [max_steps_stopping]\n    ```\n    """"""\n\n    def __init__(self, max_steps):\n        self._max_steps = max_steps\n        self._completed_steps = 0\n\n    def set_completed_steps(self, completed_steps):\n        """"""We need to set completed steps if we load the model from\n        a checkpoint where the model has been trained.\n        """"""\n        self._completed_steps = completed_steps\n\n    def on_task_end(self, task, logs=None):\n        """"""Call on the task end\n        Args:\n            task: A completed task.\n            logs: dict. Currently no data is passed to this argument for this\n                method but that may change in the future.\n        """"""\n        batch_size = self.params.get(""batch_size"", None)\n        if task.type == elasticdl_pb2.TRAINING:\n            task_records = task.end - task.start\n            task_batch_count = int(task_records / batch_size)\n            self._completed_steps += task_batch_count\n            if self._completed_steps > self._max_steps:\n                self.model.stop_training = True\n\n\nclass LearningRateScheduler(tf.keras.callbacks.Callback):\n    """"""Learning rate scheduler schedule the learning rate according\n    to the iteration steps.\n\n    Args:\n        schedule: A function that takes a batch index as input\n        (integer, indexed from 0) and returns a new learning rate\n        as output (float).\n\n    Example:\n    ```python\n    from elasticdl.python.elasticdl.callbacks import LearningRateScheduler\n\n    def callbacks():\n        # This callback will schedule the learning rate for each step.\n        def _schedule(batch):\n            return 0.002 if batch < 1000 else 0.001\n        learning_rate_scheduler = LearningRateScheduler(_schedule)\n        return [learning_rate_scheduler]\n    ```\n    """"""\n\n    def __init__(self, schedule):\n        self._schedule = schedule\n\n    def on_train_batch_begin(self, batch, logs=None):\n        """"""\n        Args:\n            batch: integer, the model version requested from PS.\n            logs: dict. Has keys batch and size representing the current batch\n                number and the size of the batch.\n        """"""\n        if not hasattr(self.model.optimizer, ""lr""):\n            raise ValueError(\'Optimizer must have a ""lr"" attribute.\')\n\n        lr = self._schedule(batch)\n        if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):\n            raise ValueError(\n                \'The output of the ""schedule"" function should be float.\'\n            )\n        K.set_value(self.model.optimizer.lr, K.get_value(lr))\n'"
elasticdl/python/elasticdl/client.py,0,"b'import argparse\n\nfrom elasticdl.python.common.args import (\n    add_clean_params,\n    add_common_params,\n    add_evaluate_params,\n    add_predict_params,\n    add_train_params,\n)\nfrom elasticdl.python.elasticdl.api import clean, evaluate, predict, train\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest=""cmd"")\n    subparsers.required = True\n\n    train_parser = subparsers.add_parser(\n        ""train"", help=""Submit a ElasticDL distributed training job""\n    )\n    train_parser.set_defaults(func=train)\n    add_common_params(train_parser)\n    add_train_params(train_parser)\n\n    evaluate_parser = subparsers.add_parser(\n        ""evaluate"", help=""Submit a ElasticDL distributed evaluation job""\n    )\n    evaluate_parser.set_defaults(func=evaluate)\n    add_common_params(evaluate_parser)\n    add_evaluate_params(evaluate_parser)\n\n    predict_parser = subparsers.add_parser(\n        ""predict"", help=""Submit a ElasticDL distributed prediction job""\n    )\n    predict_parser.set_defaults(func=predict)\n    add_common_params(predict_parser)\n    add_predict_params(predict_parser)\n\n    clean_parser = subparsers.add_parser(\n        ""clean"", help=""Clean up local docker images""\n    )\n    clean_parser.set_defaults(func=clean)\n    add_clean_params(clean_parser)\n\n    args, _ = parser.parse_known_args()\n    args.func(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
elasticdl/python/elasticdl/embedding_delegate.py,32,"b'import collections\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops, math_ops, sparse_ops\n\nEmbeddingAndIds = collections.namedtuple(\n    ""EmbeddingAndIds"", [""batch_embedding"", ""batch_ids""]\n)\n\n\nclass EmbeddingDelegate(object):\n    """"""\n    The common component to interact the external embedding\n    storage such as the parameter server.\n    Both ElasticDL Embedding Layer and Embedding Column will\n    use this component.\n    """"""\n\n    def __init__(self, input_dim, output_dim, name=None):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.name = name\n        self._lookup_embedding_func = None\n        self._embedding_and_ids_eagerly = []\n        # BET\'s shape and ids\' shape in `self._embedding_and_ids_graph` have\n        # `None` dimension. This is because they have different shapes in\n        # different iterations.\n        # `tf.Variable` requires initial value if shape has `None` dimension.\n        self._embedding_and_ids_graph = []\n        self.tape = None\n\n    def _init_for_graph_mode_if_necessary(self):\n        if (\n            tf.executing_eagerly()\n            or self._embedding_and_ids_graph\n            or not self.tape\n        ):\n            return\n\n        self._embedding_and_ids_graph = [\n            EmbeddingAndIds(\n                batch_embedding=tf.Variable(\n                    # In some cases, `tf.Variable` requires that initial value\n                    # is callable.\n                    initial_value=lambda: tf.zeros((1, self.output_dim)),\n                    shape=tf.TensorShape((None, self.output_dim)),\n                    dtype=tf.float32,\n                    trainable=True,\n                ),\n                batch_ids=tf.Variable(\n                    initial_value=lambda: tf.zeros((1, 1), dtype=tf.int64),\n                    shape=tf.TensorShape(None),\n                    dtype=tf.int64,\n                    trainable=False,\n                ),\n            )\n        ]\n\n    def embedding_lookup(self, ids):\n        """"""Looks up `ids` in a list of embedding tensors. The result of this\n        function is the same as Tensorflow `embedding_ops.embedding_lookup`.\n        But, this function is implemented to support lookup embedding using\n        ParameterServer distribution strategy.\n        """"""\n        self._init_for_graph_mode_if_necessary()\n\n        ids = tf.cast(ids, tf.int64)\n        ids = tf.convert_to_tensor(ids, name=self.name + ""_ids"")\n        flat_ids = tf.reshape(ids, [-1])\n        unique_ids, idx = tf.unique(flat_ids)\n        batch_embedding = self._get_embeddings_by_id(unique_ids)\n\n        # Gradient for `batch_embedding` is SparseTensor here due to\n        # `tf.gather` op. `tf.gather` accesses tensor slices, resulting in\n        # sparse tensor gradient.\n        # TODO: use tf.cond rather than python if statement\n        if self.tape:\n            batch_embedding = self._record_gradients(batch_embedding, flat_ids)\n\n        result = tf.gather(batch_embedding, idx)\n        # tf.reshape does not support shape with None. Replace None with -1.\n        if ids.get_shape().rank == 2:\n            input_length = ids.get_shape()[1]\n            if input_length is None:\n                result.set_shape(shape=(None, None, self.output_dim))\n                return result\n            output_shape = (-1, input_length, self.output_dim)\n        else:\n            output_shape = ids.get_shape().concatenate(self.output_dim)\n        result = tf.reshape(result, output_shape)\n        return result\n\n    def safe_embedding_lookup_sparse(\n        self, sparse_ids, sparse_weights=None, combiner=""mean"", default_id=None\n    ):\n        """"""Lookup embedding results, accounting for invalid IDs and empty\n        features. The result of this function is the same as\n        tf.nn.safe_embeddding_lookup_sparse`. But, this function is implemented\n        to support lookup embedding using ParameterServer distribution\n        strategy.\n        """"""\n        self._init_for_graph_mode_if_necessary()\n\n        sparse_ids = _prune_invalid_ids(sparse_ids)\n        # Fill in dummy values for empty features, if necessary.\n        sparse_ids, is_row_empty = sparse_ops.sparse_fill_empty_rows(\n            sparse_ids, 0\n        )\n        unique_ids, idx = tf.unique(sparse_ids.values)\n\n        segment_ids = sparse_ids.indices[:, 0]\n        if segment_ids.dtype != tf.int32:\n            segment_ids = tf.cast(segment_ids, tf.int32)\n\n        ids = sparse_ids.values\n        unique_ids, idx = tf.unique(ids)\n        batch_embedding = self._get_embeddings_by_id(unique_ids)\n\n        if sparse_weights is not None:\n            if self.tape:\n                batch_embedding = self._record_gradients(\n                    batch_embedding=batch_embedding, ids=ids\n                )\n\n            weights = sparse_weights.values\n            if weights.dtype != batch_embedding.dtype:\n                weights = math_ops.cast(weights, batch_embedding.dtype)\n\n            batch_embedding = array_ops.gather(batch_embedding, idx)\n            # Reshape weights to allow broadcast\n            ones = array_ops.fill(\n                array_ops.expand_dims(array_ops.rank(batch_embedding) - 1, 0),\n                1,\n            )\n            bcast_weights_shape = array_ops.concat(\n                [array_ops.shape(weights), ones], 0\n            )\n\n            orig_weights_shape = weights.get_shape()\n            weights = array_ops.reshape(weights, bcast_weights_shape)\n\n            # Set the weight shape, since after reshaping to\n            # bcast_weights_shape, the shape becomes None.\n            if batch_embedding.get_shape().ndims is not None:\n                weights.set_shape(\n                    orig_weights_shape.concatenate(\n                        [\n                            1\n                            for _ in range(\n                                batch_embedding.get_shape().ndims - 1\n                            )\n                        ]\n                    )\n                )\n\n            batch_embedding *= weights\n\n            if combiner == ""sum"":\n                batch_embedding = math_ops.segment_sum(\n                    batch_embedding, segment_ids\n                )\n            elif combiner == ""mean"":\n                batch_embedding = math_ops.segment_sum(\n                    batch_embedding, segment_ids\n                )\n                weight_sum = math_ops.segment_sum(weights, segment_ids)\n                batch_embedding = math_ops.div(batch_embedding, weight_sum)\n            elif combiner == ""sqrtn"":\n                batch_embedding = math_ops.segment_sum(\n                    batch_embedding, segment_ids\n                )\n                weights_squared = math_ops.pow(weights, 2)\n                weight_sum = math_ops.segment_sum(weights_squared, segment_ids)\n                weight_sum_sqrt = math_ops.sqrt(weight_sum)\n                batch_embedding = math_ops.div(\n                    batch_embedding, weight_sum_sqrt\n                )\n            else:\n                assert False, ""Unrecognized combiner""\n        else:\n            if self.tape:\n                batch_embedding = self._record_gradients(\n                    batch_embedding=batch_embedding, ids=unique_ids,\n                )\n\n            assert idx is not None\n            if combiner == ""sum"":\n                batch_embedding = math_ops.sparse_segment_sum(\n                    batch_embedding, idx, segment_ids\n                )\n            elif combiner == ""mean"":\n                batch_embedding = math_ops.sparse_segment_mean(\n                    batch_embedding, idx, segment_ids\n                )\n            elif combiner == ""sqrtn"":\n                batch_embedding = math_ops.sparse_segment_sqrt_n(\n                    batch_embedding, idx, segment_ids\n                )\n            else:\n                assert False, ""Unrecognized combiner""\n\n        # Broadcast is_row_empty to the same shape as embedding_lookup_result,\n        # for use in Select.\n        is_row_empty = array_ops.tile(\n            array_ops.reshape(is_row_empty, [-1, 1]),\n            array_ops.stack([1, array_ops.shape(batch_embedding)[1]]),\n        )\n\n        batch_embedding = array_ops.where(\n            is_row_empty,\n            array_ops.zeros_like(batch_embedding),\n            batch_embedding,\n        )\n        batch_embedding.set_shape((None, self.output_dim))\n        return batch_embedding\n\n    def _get_embeddings_by_id(self, unique_ids):\n        """"""There is a memory leak when using tf.py_function with eager\n        execution. So, we only use tf.py_function in graph mode.\n        """"""\n        if isinstance(unique_ids, ops.EagerTensor):\n            batch_embedding = self._gather_embedding_vectors(unique_ids)\n            batch_embedding = tf.constant(batch_embedding, dtype=tf.float32)\n        else:\n            batch_embedding = tf.py_function(\n                self._gather_embedding_vectors,\n                inp=[unique_ids],\n                Tout=tf.float32,\n            )\n        return batch_embedding\n\n    def _gather_embedding_vectors(self, unique_ids):\n        ids = unique_ids.numpy()\n        self._check_id_valid(ids)\n        if self._lookup_embedding_func:\n            embedding_vectors = self._lookup_embedding_func(self.name, ids)\n            return embedding_vectors\n\n    def _check_id_valid(self, ids):\n        if not self.input_dim:\n            return\n\n        first_may_exceed_id = ids[np.argmax(ids >= self.input_dim)]\n        if self.input_dim is not None and first_may_exceed_id > self.input_dim:\n            raise ValueError(\n                "" The embedding id cannot be bigger ""\n                ""than input_dim. id = %d is not in [0, %d)""\n                % (first_may_exceed_id, self.input_dim)\n            )\n\n    def _record_gradients(self, batch_embedding, ids):\n        if tf.executing_eagerly():\n            self.tape.watch(batch_embedding)\n            self._embedding_and_ids_eagerly.append(\n                EmbeddingAndIds(batch_embedding, ids)\n            )\n        else:\n            # In graph mode, assigning tensors to trainable variables is\n            # allowed and tape can record the gradients of trainable\n            # variables automatically.\n            embedding_and_ids = self._embedding_and_ids_graph[0]\n            embedding_and_ids.batch_embedding.assign(batch_embedding)\n            embedding_and_ids.batch_ids.assign(ids)\n            batch_embedding = embedding_and_ids.batch_embedding\n\n        return batch_embedding\n\n    def reset(self):\n        self.tape = None\n        self._embedding_and_ids_eagerly = []\n\n    def set_name(self, name):\n        self.name = name\n\n    def set_tape(self, tape):\n        self.tape = tape\n\n    def set_lookup_embedding_func(self, lookup_embedding_func):\n        self._lookup_embedding_func = lookup_embedding_func\n\n    @property\n    def embedding_and_ids(self):\n        """"""\n        Return bet and ids pairs.\n        """"""\n        if self._embedding_and_ids_eagerly:\n            return self._embedding_and_ids_eagerly\n        return self._embedding_and_ids_graph\n\n\ndef _prune_invalid_ids(sparse_ids):\n    """"""Prune invalid IDs (< 0) from the input ids.""""""\n    is_id_valid = tf.greater_equal(sparse_ids.values, 0)\n    sparse_ids = sparse_ops.sparse_retain(sparse_ids, is_id_valid)\n    return sparse_ids\n'"
elasticdl/python/elasticdl/image_builder.py,0,"b'import os\nimport tempfile\nimport uuid\nfrom urllib.parse import urlparse\n\nimport docker\n\nfrom elasticdl.python.common.file_utils import copy_if_not_exists\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n\ndef build_and_push_docker_image(\n    model_zoo,\n    docker_image_repository,\n    base_image="""",\n    extra_pypi="""",\n    cluster_spec="""",\n    docker_base_url=""unix://var/run/docker.sock"",\n    docker_tlscert="""",\n    docker_tlskey="""",\n):\n    """"""Build and push a Docker image containing ElasticDL and the model\nzoo.  The parameter model_zoo could be a local directory or an URL.\nIn the later case, we do git clone.\n\n    The basename of the Docker image is auto-generated and is globally\nunique.  The fullname of the Docker image is docker_image_repository + "":"" +\nbasename.  Unless repository is None or """", _push_docker_image is called\nafter _build_docker_image.\n\n    Returns the full Docker image name.  So the caller can docker rmi\n    fullname later.\n\n    """"""\n    with tempfile.TemporaryDirectory() as ctx_dir:\n\n        # Copy ElasticDL Python source tree into the context directory.\n        elasticdl = _find_elasticdl_root()\n        edl_dest = os.path.join(ctx_dir, os.path.basename(elasticdl))\n        copy_if_not_exists(elasticdl, edl_dest, is_dir=True)\n\n        # Copy model zoo source tree into the context directory.\n        model_zoo_dest = os.path.join(ctx_dir, os.path.basename(model_zoo))\n        copy_if_not_exists(model_zoo, model_zoo_dest, is_dir=True)\n\n        # Copy cluster specification file into the context directory.\n        if cluster_spec:\n            copy_if_not_exists(\n                cluster_spec,\n                os.path.join(ctx_dir, os.path.basename(cluster_spec)),\n                is_dir=False,\n            )\n\n        # Create the Dockerfile.\n        with tempfile.NamedTemporaryFile(mode=""w+"", delete=False) as df:\n            df.write(\n                _create_dockerfile(\n                    os.path.basename(elasticdl),\n                    # Note that we need `abspath` here since `urlparse`\n                    # does not handle directory names correctly sometimes\n                    os.path.basename(os.path.abspath(model_zoo)),\n                    os.path.basename(cluster_spec),\n                    base_image,\n                    extra_pypi,\n                )\n            )\n\n        image_name = _generate_unique_image_name(docker_image_repository)\n        client = _get_docker_client(\n            docker_base_url=docker_base_url,\n            docker_tlscert=docker_tlscert,\n            docker_tlskey=docker_tlskey,\n        )\n        _build_docker_image(client, ctx_dir, df.name, image_name)\n\n        if docker_image_repository:\n            _push_docker_image(client, image_name)\n\n    return image_name\n\n\ndef remove_images(\n    docker_image_repository="""",\n    docker_base_url=""unix://var/run/docker.sock"",\n    docker_tlscert="""",\n    docker_tlskey="""",\n):\n    """"""Remove all docker images with repository name equal to\n    docker_image_repository. If docker_image_repository is empty, it\n    will remove all images.\n    """"""\n    client = _get_docker_client(\n        docker_base_url=docker_base_url,\n        docker_tlscert=docker_tlscert,\n        docker_tlskey=docker_tlskey,\n    )\n    # Use repository tags to delete images\n    images = client.images(name=docker_image_repository, quiet=False)\n    for image in images:\n        repo_tags = image.get(""RepoTags"") or []\n        for repo_tag in repo_tags:\n            if repo_tag == ""<none>:<none>"":\n                # A special case where both repository and tag are none,\n                # and we need to delete it through Id in the following code.\n                continue\n            logger.info(""Removing image %s"" % repo_tag)\n            try:\n                client.remove_image(repo_tag)\n            except docker.errors.APIError as e:\n                logger.warning(""Failed to delete image %s: %s"" % (repo_tag, e))\n    # For image not having full repository tags, use ID instead.\n    # Note that, here we need to re-list images\n    images = client.images(name=docker_image_repository, quiet=False)\n    for image in images:\n        image_id = image.get(""Id"")\n        if image_id:\n            logger.info(""Removing image %s"" % image_id)\n            try:\n                client.remove_image(image_id)\n            except docker.errors.APIError as e:\n                logger.warning(""Failed to delete image %s: %s"" % (image_id, e))\n    # If removing all images, we also run prune to remove untagged images\n    if not docker_image_repository:\n        logger.info(""Pruning unused images"")\n        try:\n            client.prune_images()\n        except docker.errors.APIError as e:\n            logger.warning(""Failed to prune images: %s"" % e)\n\n\ndef _find_elasticdl_root():\n    return os.path.abspath(\n        os.path.join(os.path.dirname(__file__), ""../../../"")\n    )\n\n\ndef _create_dockerfile(\n    elasticdl, model_zoo, cluster_spec="""", base_image="""", extra_pypi_index=""""\n):\n    HEAD = """"""\nFROM {BASE_IMAGE} as base\nENV PYTHONPATH=/\n""""""\n    if elasticdl:\n        HEAD = """"""\n%s\nCOPY %s/elasticdl /elasticdl\nCOPY %s/elasticdl_preprocessing /elasticdl_preprocessing\n"""""" % (\n            HEAD,\n            elasticdl,\n            elasticdl,\n        )\n\n    LOCAL_ZOO = """"""\nRUN pip install -r /elasticdl/requirements.txt \\\n  --extra-index-url=""${EXTRA_PYPI_INDEX}""\nRUN make -f /elasticdl/Makefile\nCOPY {MODEL_ZOO} /{MODEL_ZOO}\n""""""\n    REMOTE_ZOO = """"""\nRUN pip install -r /elasticdl/requirements.txt \\\n  --extra-index-url=""${EXTRA_PYPI_INDEX}""\nRUN make -f /elasticdl/Makefile\nRUN apt-get update && apt-get install -y git\nRUN git clone --recursive {MODEL_ZOO} /\n""""""\n    pr = urlparse(model_zoo)\n    if not pr.path:\n        raise RuntimeError(\n            ""urlparse(model_zoo) {} has no path field"".format(model_zoo)\n        )\n    if pr.scheme in [""file"", """"]:\n        tmpl = HEAD + LOCAL_ZOO\n        model_zoo = pr.path  # Remove the ""file://"" prefix if any.\n    else:\n        tmpl = HEAD + REMOTE_ZOO\n\n    if cluster_spec:\n        tmpl = """"""\n%s\nCOPY %s /cluster_spec/%s\n"""""" % (\n            tmpl,\n            cluster_spec,\n            cluster_spec,\n        )\n\n    tmpl = (\n        """"""\n%s\nARG REQS=/{MODEL_ZOO}/requirements.txt\nRUN if [ -f $REQS ]; then \\\n      pip install -r $REQS --extra-index-url=""${EXTRA_PYPI_INDEX}""; \\\n    fi\n# Check whether TensorFlow has been installed in the base image\n# so we can fail earlier if the specified base image is problematic\nRUN python -c \'import sys, pkgutil; exit_code = 0 if \\\n    pkgutil.find_loader(""tensorflow"") else 1; \\\n    print(""TensorFlow is installed"") if exit_code == 0 \\\n    else print(""TensorFlow must be installed in the base image""); \\\n    sys.exit(exit_code)\'\n""""""\n        % tmpl\n    )\n\n    return tmpl.format(\n        BASE_IMAGE=base_image\n        if base_image\n        else ""tensorflow/tensorflow:2.0.0-py3"",\n        ELASTIC_DL=elasticdl,\n        MODEL_ZOO=model_zoo,\n        EXTRA_PYPI_INDEX=extra_pypi_index,\n    )\n\n\ndef _generate_unique_image_name(repository):\n    return os.path.join(\n        repository if repository else """", ""elasticdl:"" + uuid.uuid4().hex\n    )\n\n\ndef _print_docker_progress(line):\n    error = line.get(""error"", None)\n    if error:\n        raise RuntimeError(""Docker image build: "" + error)\n    stream = line.get(""stream"", None)\n    if stream:\n        logger.info(stream)\n    else:\n        logger.info(line)\n\n\ndef _build_docker_image(client, ctx_dir, dockerfile, image_name):\n    logger.info(""===== Building Docker Image ====="")\n    for line in client.build(\n        dockerfile=dockerfile,\n        path=ctx_dir,\n        rm=True,\n        tag=image_name,\n        decode=True,\n    ):\n        _print_docker_progress(line)\n\n\ndef _push_docker_image(client, image_name):\n    logger.info(""===== Pushing Docker Image ====="")\n    for line in client.push(image_name, stream=True, decode=True):\n        _print_docker_progress(line)\n\n\ndef _get_docker_client(docker_base_url, docker_tlscert, docker_tlskey):\n    if docker_tlscert and docker_tlskey:\n        tls_config = docker.tls.TLSConfig(\n            client_cert=(docker_tlscert, docker_tlskey)\n        )\n        return docker.APIClient(base_url=docker_base_url, tls=tls_config)\n    else:\n        return docker.APIClient(base_url=docker_base_url)\n'"
elasticdl/python/elasticdl/local_executor.py,5,"b'import os\nimport sys\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nfrom elasticdl.python.common.args import parse_envs\nfrom elasticdl.python.common.constants import MetricsDictKey\nfrom elasticdl.python.common.evaluation_utils import EvaluationMetrics\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.model_utils import (\n    get_dict_from_params_str,\n    get_model_spec,\n)\nfrom elasticdl.python.data.reader.csv_reader import CSVDataReader\nfrom elasticdl.python.data.reader.data_reader_factory import create_data_reader\nfrom elasticdl.python.data.reader.odps_reader import ODPSDataReader\nfrom elasticdl.python.data.reader.recordio_reader import RecordIODataReader\n\n_MockedTask = namedtuple(""Task"", [""shard_name"", ""start"", ""end""])\n\n\nclass LocalExecutor:\n    """"""Train the Keras model defined in ElasticDL model zoo locally\n    using the custom training loop with Tensorflow native low-level API.\n    """"""\n\n    def __init__(self, args):\n        envs = parse_envs(args.envs)\n        self._init_environment(envs)\n\n        (\n            self.model_inst,\n            self.dataset_fn,\n            self.loss_fn,\n            self.opt_fn,\n            self.eval_metrics_fn,\n            self.prediction_outputs_processor,\n            self.custom_data_reader,\n            self.callback_list,\n        ) = get_model_spec(\n            model_zoo=args.model_zoo,\n            model_def=args.model_def,\n            dataset_fn=args.dataset_fn,\n            loss=args.loss,\n            optimizer=args.optimizer,\n            eval_metrics_fn=args.eval_metrics_fn,\n            model_params=args.model_params,\n            prediction_outputs_processor="""",\n            custom_data_reader=args.custom_data_reader,\n            callbacks=args.callbacks,\n        )\n        self.opt = self.opt_fn()\n        self.epoch = args.num_epochs\n        self.evaluation_steps = args.evaluation_steps\n        self.batch_size = args.minibatch_size\n        self.data_reader_params = get_dict_from_params_str(\n            args.data_reader_params\n        )\n        self.records_per_task = (\n            args.minibatch_size * args.num_minibatches_per_task\n        )\n\n        create_data_reader_fn = (\n            create_data_reader\n            if self.custom_data_reader is None\n            else self.custom_data_reader\n        )\n        self.data_reader = create_data_reader_fn(\n            data_origin=args.training_data,\n            records_per_task=self.records_per_task,\n            **self.data_reader_params\n        )\n        self.training_data = args.training_data\n        self.validation_data = args.validation_data\n        self.save_model_dir = args.output\n\n    def _init_environment(self, envs):\n        for key, value in envs.items():\n            os.environ[key] = value\n\n    def run(self):\n        """"""Execute the training loop""""""\n        epoch = 0\n        step = 0\n\n        train_tasks = self._gen_tasks(self.training_data)\n        validation_tasks = self._gen_tasks(self.validation_data)\n        train_dataset = self._get_dataset(train_tasks)\n        validation_dataset = self._get_dataset(validation_tasks)\n\n        while epoch < self.epoch:\n            for features, labels in train_dataset:\n                loss = self._train(features, labels)\n                logger.info(""step {}, Loss = {}"".format(step, loss))\n                step += 1\n                if (\n                    self.evaluation_steps > 0\n                    and step % self.evaluation_steps == 0\n                ):\n                    self._evaluate(validation_dataset)\n            self._evaluate(validation_dataset)\n            logger.info(""Epoch {} end"".format(epoch))\n            epoch += 1\n        if self.save_model_dir != """":\n            tf.saved_model.save(self.model_inst, self.save_model_dir)\n\n    def _train(self, features, labels):\n        with tf.GradientTape() as tape:\n            outputs = self.model_inst.call(features, training=True)\n            loss = self.loss_fn(labels, outputs)\n            if self.model_inst.losses:\n                loss += tf.math.add_n(self.model_inst.losses)\n        grads = tape.gradient(loss, self.model_inst.trainable_variables)\n        grads_and_vars = zip(grads, self.model_inst.trainable_variables)\n        self.opt.apply_gradients(grads_and_vars, name=None)\n        return loss\n\n    def _evaluate(self, dataset):\n        if dataset is None:\n            logger.info(""No validation dataset is configured"")\n            return\n        eval_metrics = EvaluationMetrics(self.eval_metrics_fn())\n        for features, labels in dataset:\n            outputs = self.model_inst.call(features)\n            if not isinstance(outputs, dict):\n                outputs = {MetricsDictKey.MODEL_OUTPUT: outputs}\n            eval_metrics.update_evaluation_metrics(outputs, labels)\n        metrics = eval_metrics.get_evaluation_summary()\n        logger.info(""Evaluation metrics : {}"".format(metrics))\n        return metrics\n\n    def _get_dataset(self, tasks):\n        """"""\n        Utilize tasks to creates a generator, which could be used to\n        creating a `tf.data.Dataset` object in further.\n        """"""\n\n        def gen():\n            for task in tasks:\n                for data in self.data_reader.read_records(task):\n                    if data:\n                        yield data\n\n        dataset = tf.data.Dataset.from_generator(\n            gen, self.data_reader.records_output_types\n        )\n        dataset = self.dataset_fn(dataset, None, None)\n        dataset = dataset.batch(self.batch_size)\n        return dataset\n\n    def _gen_tasks(self, data_dir):\n        """"""Generate tasks to create a dataset.\n        For CSVDataReader, a task will contains all records in a file.\n        For RecordIODataReader, a task contains all records in a shard file.\n        For ODPSDataReader, a task contains a portion of records in the table.\n        """"""\n        tasks = []\n        if isinstance(self.data_reader, CSVDataReader):\n            if os.path.exists(data_dir):\n                # A task contains all records for a csv file\n                tasks.append(_MockedTask(data_dir, 0, sys.maxsize))\n        else:\n            shards = self._create_shards(data_dir)\n            if isinstance(self.data_reader, RecordIODataReader):\n                for shard_name, (start_index, end_index) in shards.items():\n                    tasks.append(\n                        _MockedTask(shard_name, start_index, end_index)\n                    )\n            elif isinstance(self.data_reader, ODPSDataReader):\n                for shard_name, (start_index, end_index) in shards.items():\n                    tasks.append(\n                        _MockedTask(\n                            shard_name, start_index, end_index + start_index\n                        )\n                    )\n        return tasks\n\n    def _create_shards(self, data_origin):\n        """"""Create shards\n        Args:\n            data_origin: A recordIO directory or a csv file path\n            or an ODPS table name.\n        """"""\n        partition = self.data_reader_params.get(""partition"", None)\n        return (\n            create_data_reader(\n                data_origin=data_origin,\n                records_per_task=self.records_per_task,\n                partition=partition,\n                **self.data_reader_params\n            ).create_shards()\n            if data_origin\n            else {}\n        )\n'"
elasticdl/python/master/__init__.py,0,b''
elasticdl/python/master/evaluation_service.py,3,"b'import threading\nimport time\nfrom threading import Thread\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.evaluation_utils import EvaluationMetrics\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.tensor_utils import pb_to_ndarray\n\n\nclass EvaluationJob(object):\n    """"""Representation of an evaluation job""""""\n\n    def __init__(self, metrics_dict, model_version, total_tasks=-1):\n        """"""\n        Args:\n            metrics_dict: A python dictionary. If model has only one output,\n                `metrics_dict` is a dictionary of `{metric_name: metric}`,\n                i.e. `{""acc"": tf.keras.metrics.Accuracy()}`.\n                If model has multiple outputs, `metric_dict` is a dictionary of\n                `{output_name: {metric_name: metric}}`,\n                i.e. `{\n                    ""output_a"": {""acc"": tf.keras.metrics.Accuracy()},\n                    ""output_b"": {""auc"": tf.keras.metrics.AUC()},\n                }`. Note that for model with multiple outputs, each metric\n                only uses one output.\n            model_version: The version of the model to be evaluated.\n            total_tasks: The number of evaluation tasks.\n        """"""\n\n        self.model_version = model_version\n        self._total_tasks = total_tasks\n        self._completed_tasks = 0\n        self.evaluation_metrics = EvaluationMetrics(metrics_dict)\n\n    def complete_task(self):\n        self._completed_tasks += 1\n\n    def finished(self):\n        return self._completed_tasks >= self._total_tasks\n\n    def report_evaluation_metrics(self, model_outputs_pb, labels):\n        labels = pb_to_ndarray(labels)\n        model_outputs = {}\n        for name, tensor_pb in model_outputs_pb.items():\n            model_outputs[name] = pb_to_ndarray(tensor_pb)\n        self.evaluation_metrics.update_evaluation_metrics(\n            model_outputs, labels\n        )\n\n\nclass _EvaluationTrigger(Thread):\n    """"""A trigger which generates evaluation tasks periodically""""""\n\n    def __init__(self, eval_service, start_delay_secs, throttle_secs):\n        Thread.__init__(self)\n        self._eval_service = eval_service\n        self._stopper = threading.Event()\n        self._throttle_secs = throttle_secs\n        self._eval_min_time = time.time() + start_delay_secs\n\n    def stop(self):\n        self._stopper.set()\n\n    def _wait_enough_time(self, cur_time_secs, previous_round_start_secs):\n        if cur_time_secs < self._eval_min_time:\n            return False\n        if (\n            previous_round_start_secs != -1\n            and cur_time_secs - previous_round_start_secs < self._throttle_secs\n        ):\n            return False\n        return True\n\n    def run(self):\n        previous_round_start_secs = -1\n\n        while not self._stopper.is_set():\n            time_now = time.time()\n            if self._wait_enough_time(time_now, previous_round_start_secs):\n                # Time is up, add an evaluation task\n                self._eval_service.add_evaluation_task(is_time_based_eval=True)\n                previous_round_start_secs = time_now\n            time.sleep(5)\n\n\nclass EvaluationService(object):\n    """"""Evaluation service""""""\n\n    def __init__(\n        self,\n        tensorboard_service,\n        task_d,\n        start_delay_secs,\n        throttle_secs,\n        eval_steps,\n        eval_only,\n        eval_metrics_fn,\n    ):\n        self._tensorboard_service = tensorboard_service\n        self._task_d = task_d\n        self._lock = threading.Lock()\n        self._eval_job = None\n        self.trigger = _EvaluationTrigger(\n            self, start_delay_secs, throttle_secs\n        )\n        self._time_based_eval = throttle_secs > 0\n        self._eval_steps = eval_steps\n        self._eval_checkpoint_versions = []\n        self._last_eval_checkpoint_version = -1\n        self._eval_only = eval_only\n        self._eval_metrics_fn = eval_metrics_fn\n\n    def start(self):\n        if self._time_based_eval and not self._eval_only:\n            self.trigger.start()\n\n    def stop(self):\n        if self._time_based_eval and not self._eval_only:\n            self.trigger.stop()\n\n    def set_master_servicer(self, master_servicer):\n        self._master_servicer = master_servicer\n\n    def init_eval_only_job(self, num_task):\n        self._eval_job = EvaluationJob(self._eval_metrics_fn(), -1, num_task)\n\n    def add_evaluation_task(\n        self, is_time_based_eval, master_locking=True, model_version=None\n    ):\n        """"""\n        Add evaluation task with current model_version.\n        """"""\n        # Do not create time-based eval after all tasks are done\n        if is_time_based_eval and self._task_d.finished():\n            return\n        if not model_version:\n            model_version = self._master_servicer.get_model_version()\n        if model_version == self._last_eval_checkpoint_version:\n            return\n\n        checkpoint_version = model_version\n        with self._lock:\n            self._eval_checkpoint_versions.append(checkpoint_version)\n        self._last_eval_checkpoint_version = checkpoint_version\n        self.try_to_create_new_job()\n\n    def try_to_create_new_job(self):\n        """"""\n        Add eval task into task dispatcher if current eval_job is done\n        and there are pending eval tasks\n        """"""\n        with self._lock:\n            if self._eval_job is None and self._eval_checkpoint_versions:\n                checkpoint_version = self._eval_checkpoint_versions.pop(0)\n                self._task_d.create_tasks(\n                    elasticdl_pb2.EVALUATION, checkpoint_version\n                )\n                task_count = len(self._task_d._eval_todo)\n                if self._eval_job is None:\n                    self._eval_job = EvaluationJob(\n                        self._eval_metrics_fn(), checkpoint_version, task_count\n                    )\n                else:\n                    self._eval_job.model_version = checkpoint_version\n                    self._eval_job._total_tasks = task_count\n                    self._eval_job.reset_metric_states()\n                return True\n        return False\n\n    def add_evaluation_task_if_needed(self, master_locking, model_version):\n        """"""\n        Add step-based evaluation task\n        """"""\n        if not model_version:\n            model_version = self._master_servicer.get_model_version()\n        if (\n            self._eval_steps\n            and model_version % self._eval_steps == 0\n            and model_version > self._last_eval_checkpoint_version\n        ):\n            self.add_evaluation_task(\n                is_time_based_eval=False,\n                master_locking=master_locking,\n                model_version=model_version,\n            )\n\n    def report_evaluation_metrics(self, model_outputs, labels):\n        if self._eval_job is None:\n            return False\n        with self._lock:\n            return self._eval_job.report_evaluation_metrics(\n                model_outputs, labels\n            )\n\n    def complete_task(self):\n        if self._eval_job is None:\n            return\n        self._eval_job.complete_task()\n        if self._eval_job.finished():\n            evaluation_metrics = (\n                self._eval_job.evaluation_metrics.get_evaluation_summary()\n            )\n            if self._tensorboard_service and evaluation_metrics:\n                self._tensorboard_service.write_dict_to_summary(\n                    evaluation_metrics, version=self._eval_job.model_version\n                )\n            logger.info(\n                ""Evaluation metrics[v=%d]: %s""\n                % (\n                    self._eval_job.model_version\n                    if self._eval_job.model_version >= 0\n                    else self._master_servicer.get_model_version(),\n                    str(evaluation_metrics),\n                )\n            )\n            if not self._eval_only:\n                # delete checkpoint file\n                self._eval_job = None\n                # create new eval job if possible\n                self.try_to_create_new_job()\n            return evaluation_metrics\n'"
elasticdl/python/master/k8s_instance_manager.py,0,"b'import copy\nimport itertools\nimport threading\nfrom collections import Counter\n\nfrom elasticdl.python.common import k8s_client as k8s\nfrom elasticdl.python.common.constants import BashCommandTemplate, PodStatus\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n_SERVICE_ADDR_SEP = "",""\n\n\ndef _parse_worker_pod_priority(num_workers, worker_pod_priority):\n    res = {}\n    if isinstance(worker_pod_priority, str) and ""high="" in worker_pod_priority:\n        try:\n            fraction = float(worker_pod_priority.split(""="")[1])\n            high_count = int(num_workers * fraction)\n            for i in range(num_workers):\n                if i < high_count:\n                    res[i] = ""high""\n                else:\n                    res[i] = ""low""\n        except Exception:\n            logger.warning(\n                ""Please check the input worker pod priority format,""\n                ""e.g. high=0.5  The config is no use, and ElasticDL sets ""\n                ""low priority for all worker pods by default.""\n            )\n            for i in range(num_workers):\n                res[i] = None\n    else:\n        for i in range(num_workers):\n            res[i] = worker_pod_priority\n    return res\n\n\nclass InstanceManager(object):\n    def __init__(\n        self,\n        task_d,\n        num_workers=1,\n        worker_command=None,\n        worker_args=None,\n        worker_resource_request=""cpu=1,memory=4096Mi"",\n        worker_resource_limit=""cpu=1,memory=4096Mi"",\n        worker_pod_priority=None,\n        num_ps=0,\n        ps_command=None,\n        ps_args=None,\n        ps_resource_request=""cpu=1,memory=4096Mi"",\n        ps_resource_limit=""cpu=1,memory=4096Mi"",\n        ps_pod_priority=None,\n        volume=None,\n        image_pull_policy=None,\n        restart_policy=""Never"",\n        envs=None,\n        expose_ports=False,\n        disable_relaunch=False,\n        log_file_path=None,\n        **kwargs\n    ):\n        self._num_workers = num_workers\n        self._worker_command = worker_command\n        self._worker_args = worker_args\n        self._worker_resource_request = worker_resource_request\n        self._worker_resource_limit = worker_resource_limit\n        self._worker_pod_priority = _parse_worker_pod_priority(\n            self._num_workers, worker_pod_priority\n        )\n        self._expose_ports = expose_ports\n\n        self._num_ps = num_ps\n        self._ps_command = ps_command\n        self._ps_args = ps_args\n        self._ps_resource_request = ps_resource_request\n        self._ps_resource_limit = ps_resource_limit\n        self._ps_pod_priority = ps_pod_priority\n\n        self._restart_policy = restart_policy\n        self._volume = volume\n        self._image_pull_policy = image_pull_policy\n        self._envs = envs\n        self._task_d = task_d\n        self._next_worker_id = itertools.count().__next__\n        self._log_file_path = log_file_path\n\n        # Protects followed variables, which are accessed from event_cb.\n        self._lock = threading.Lock()\n        # worker id to (pod name, phase) mapping\n        # phase: None/Pending/Running/Succeeded/Failed/Unknown\n        #   None: worker was just launched, haven\'t received event yet.\n        #   Pending: worker pod not started yet\n        #   Running: worker pod is running\n        #   Succeeded: worker pod finishes all tasks and terminates with\n        #       no issue.\n        #   Failed: worker pod is killed for some reason\n        #   Unknown: unknown\n        self._worker_pods_phase = {}\n        # pod name to worker id mapping\n        self._worker_pod_name_to_id = {}\n\n        self._relaunch_deleted_live_worker = True\n\n        self._ps_pods_phase = {}\n        self._ps_pod_name_to_id = {}\n        self._relaunch_deleted_live_ps = True\n\n        self._failed_pods = []\n        self.all_workers_failed = False\n\n        if disable_relaunch:\n            self._k8s_client = k8s.Client(**kwargs)\n        else:\n            self._k8s_client = k8s.Client(\n                event_callback=self._event_cb, **kwargs\n            )\n        self._ps_addrs = self._get_addrs(\n            self._num_ps, self._k8s_client.get_ps_service_address\n        )\n        # TODO: Select a worker address to be used for broadcasting model\n        # parameters under allreduce-strategy.\n        self._worker_addrs = self._get_addrs(\n            self._num_workers, self._k8s_client.get_worker_service_address\n        )\n        if expose_ports:\n            self._worker_args += [\n                ""--collective_communicator_service_name"",\n                self._k8s_client.get_collective_communicator_service_name(),\n            ]\n\n    def _start_worker(self, worker_id):\n        logger.info(""Starting worker: %d"" % worker_id)\n        bash_command = self._worker_args[1]\n        bash_command += "" --worker_id {}"".format(worker_id)\n        bash_command += "" --ps_addrs {}"".format(self._ps_addrs)\n        if self._log_file_path:\n            bash_command += BashCommandTemplate.REDIRECTION.format(\n                self._log_file_path\n            )\n        worker_args = [self._worker_args[0], bash_command]\n        with self._lock:\n            pod = self._k8s_client.create_worker(\n                worker_id=worker_id,\n                resource_requests=self._worker_resource_request,\n                resource_limits=self._worker_resource_limit,\n                pod_priority=self._worker_pod_priority[worker_id],\n                termination_period=1,\n                volume=self._volume,\n                image_pull_policy=self._image_pull_policy,\n                command=self._worker_command,\n                args=worker_args,\n                restart_policy=self._restart_policy,\n                ps_addrs=self._ps_addrs,\n                envs=copy.deepcopy(self._envs),\n                expose_ports=self._expose_ports,\n            )\n            name = pod.metadata.name\n            self._worker_pod_name_to_id[name] = worker_id\n            self._worker_pods_phase[worker_id] = (name, None)\n            self._k8s_client.create_worker_service(worker_id)\n\n    def _start_ps(self, ps_id):\n        logger.info(""Starting PS: %d"" % ps_id)\n        bash_command = self._ps_args[1]\n        bash_command += "" --ps_id {}"".format(ps_id)\n        if self._log_file_path:\n            bash_command += BashCommandTemplate.REDIRECTION.format(\n                self._log_file_path\n            )\n        ps_args = [self._ps_args[0], bash_command]\n        with self._lock:\n            pod = self._k8s_client.create_ps(\n                ps_id=ps_id,\n                resource_requests=self._ps_resource_request,\n                resource_limits=self._ps_resource_limit,\n                pod_priority=self._ps_pod_priority,\n                volume=self._volume,\n                image_pull_policy=self._image_pull_policy,\n                command=self._ps_command,\n                args=ps_args,\n                restart_policy=self._restart_policy,\n                envs=copy.deepcopy(self._envs),\n                expose_ports=False,\n            )\n            name = pod.metadata.name\n            self._ps_pod_name_to_id[name] = ps_id\n            self._ps_pods_phase[ps_id] = (name, None)\n            self._k8s_client.create_ps_service(ps_id)\n\n    def _get_addrs(self, num_addrs, addr_get_fn):\n        addrs = []\n        for addr_id in range(num_addrs):\n            addrs.append(addr_get_fn(addr_id))\n        return _SERVICE_ADDR_SEP.join(addrs)\n\n    @staticmethod\n    def _update_addr(old_addr, new_addr, addrs, addr_get_fn):\n        addrs_list = addrs.split(_SERVICE_ADDR_SEP)\n        addrs_list[addrs_list.index(addr_get_fn(old_addr))] = addr_get_fn(\n            new_addr\n        )\n        return _SERVICE_ADDR_SEP.join(addrs_list)\n\n    def _update_worker_addr(self, old_worker_id, new_worker_id):\n        new_addr = self._update_addr(\n            old_worker_id,\n            new_worker_id,\n            self._worker_addrs,\n            addr_get_fn=self._k8s_client.get_worker_service_address,\n        )\n        self._worker_addrs = new_addr\n\n    def update_status(self, status):\n        master_name = self._k8s_client.get_master_pod_name()\n        self._k8s_client.patch_labels_to_pod(\n            master_name, labels_dict={""status"": status}\n        )\n\n    def start_workers(self):\n        for _ in range(self._num_workers):\n            self._start_worker(self._next_worker_id())\n\n    def start_ftlib_consensus_service(self):\n        self._k8s_client.create_ftlib_consensus_service()\n\n    def start_parameter_servers(self):\n        for i in range(self._num_ps):\n            self._start_ps(i)\n\n    def _remove_worker(self, worker_id):\n        logger.info(""Removing worker: %d"", worker_id)\n        with self._lock:\n            if worker_id not in self._worker_pods_phase:\n                logger.error(""Unknown worker id: %s"" % worker_id)\n                return\n\n        # TODO: change _k8s_client to accept pod name instead of worker id.\n        self._k8s_client.delete_worker(worker_id)\n\n    def _remove_ps(self, ps_id):\n        logger.info(""Removing PS: %d"", ps_id)\n        with self._lock:\n            if ps_id not in self._ps_pods_phase:\n                logger.error(""Unknown PS id: %s"" % ps_id)\n                return\n\n        self._k8s_client.delete_ps(ps_id)\n\n    def stop_relaunch_and_remove_workers(self):\n        with self._lock:\n            self._relaunch_deleted_live_worker = False\n            for worker_id in self._worker_pods_phase:\n                self._k8s_client.delete_worker(worker_id)\n\n    def stop_relaunch_and_remove_all_ps(self):\n        with self._lock:\n            self._relaunch_deleted_live_ps = False\n            for ps_id in self._ps_pods_phase:\n                self._k8s_client.delete_ps(ps_id)\n\n    def get_worker_counter(self):\n        with self._lock:\n            return Counter([v for _, v in self._worker_pods_phase.values()])\n\n    def get_ps_counter(self):\n        with self._lock:\n            return Counter([v for _, v in self._ps_pods_phase.values()])\n\n    def _event_cb(self, event):\n        evt_obj = event.get(""object"")\n        evt_type = event.get(""type"")\n        if not evt_obj or not evt_type:\n            logger.error(""Event doesn\'t have object or type: %s"" % event)\n            return\n\n        if evt_obj.kind != ""Pod"":\n            # We only care about pod related events\n            return\n\n        pod_name = evt_obj.metadata.name\n        phase = evt_obj.status.phase\n        if pod_name == self._k8s_client.get_master_pod_name():\n            # No need to care about master pod\n            return\n\n        relaunch_worker = False\n        relaunch_ps = False\n        worker_id = -1\n        ps_id = -1\n        with self._lock:\n            if pod_name in self._failed_pods:\n                return\n\n            # When a pod fails with exit_code == 137, it may be deleted,\n            # preempted, or OOMkilled. Master will try to relaunch it.\n            # For OOMkilled, the relaunch is a workaround for memory leak\n            # issues in tf eager mode.\n            relaunch_failed_pod = False\n            if (\n                evt_type == ""MODIFIED""\n                and phase == ""Failed""\n                and evt_obj.status.container_statuses\n                and evt_obj.status.container_statuses[0].state.terminated\n                and evt_obj.status.container_statuses[\n                    0\n                ].state.terminated.exit_code\n                == 137\n            ):\n                self._failed_pods.append(pod_name)\n                relaunch_failed_pod = True\n                logger.info(\n                    ""Pod %s is killed with reason %s.""\n                    % (\n                        pod_name,\n                        evt_obj.status.container_statuses[\n                            0\n                        ].state.terminated.reason,\n                    )\n                )\n\n            if pod_name in self._worker_pod_name_to_id:\n                worker_id = self._worker_pod_name_to_id.get(pod_name)\n                self._worker_pods_phase[worker_id] = (pod_name, phase)\n                if evt_type == ""DELETED"" or relaunch_failed_pod:\n                    del self._worker_pods_phase[worker_id]\n                    del self._worker_pod_name_to_id[pod_name]\n                    self._task_d.recover_tasks(worker_id)\n\n                    # If a deleted pod was not ""Succeeded"", relaunch a worker.\n                    relaunch_worker = (\n                        self._relaunch_deleted_live_worker\n                        and phase != ""Succeeded""\n                    )\n                else:\n                    workers_failed = []\n                    for pod_name, phase in self._worker_pods_phase.values():\n                        workers_failed.append(phase == PodStatus.FAILED)\n                    self.all_workers_failed = all(workers_failed)\n\n            elif pod_name in self._ps_pod_name_to_id:\n                ps_id = self._ps_pod_name_to_id.get(pod_name)\n                self._ps_pods_phase[ps_id] = (pod_name, phase)\n                if evt_type == ""DELETED"" or relaunch_failed_pod:\n                    del self._ps_pods_phase[ps_id]\n                    del self._ps_pod_name_to_id[pod_name]\n                    relaunch_ps = self._relaunch_deleted_live_ps\n            else:\n                logger.error(""Unknown pod name: %s"" % pod_name)\n                return\n\n        if relaunch_worker and worker_id >= 0:\n            logger.info(""Relaunching worker."")\n            new_worker_id = self._next_worker_id()\n            with self._lock:\n                self._worker_pod_priority[\n                    new_worker_id\n                ] = self._worker_pod_priority[worker_id]\n            self._start_worker(new_worker_id)\n            with self._lock:\n                self._update_worker_addr(worker_id, new_worker_id)\n        elif relaunch_ps:\n            logger.info(""Relaunching ps."")\n            # Note: the ID and service address for relaunched parameter\n            # server are intentionally left unchanged to support fault\n            # tolerance.\n            self._start_ps(ps_id)\n\n    @property\n    def ps_addrs(self):\n        return self._ps_addrs\n'"
elasticdl/python/master/main.py,0,"b'import sys\n\nfrom elasticdl.python.common.args import parse_master_args\nfrom elasticdl.python.master.master import Master\n\n\ndef main():\n    args = parse_master_args()\n    master = Master(args)\n    master.prepare()\n    return master.run()\n\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
elasticdl/python/master/master.py,0,"b'import os\nimport threading\nimport time\nfrom concurrent import futures\n\nimport grpc\nfrom kubernetes.client import V1EnvVar\n\nfrom elasticdl.proto import elasticdl_pb2, elasticdl_pb2_grpc\nfrom elasticdl.python.common.args import (\n    build_arguments_from_parsed_result,\n    parse_envs,\n    wrap_go_args_with_string,\n    wrap_python_args_with_string,\n)\nfrom elasticdl.python.common.constants import (\n    GRPC,\n    BashCommandTemplate,\n    DistributionStrategy,\n    InstanceManagerStatus,\n    JobType,\n)\nfrom elasticdl.python.common.k8s_tensorboard_client import TensorBoardClient\nfrom elasticdl.python.common.log_utils import get_logger\nfrom elasticdl.python.common.model_utils import (\n    get_dict_from_params_str,\n    get_module_file_path,\n    get_optimizer_info,\n    load_callbacks_from_module,\n    load_model_from_module,\n    load_module,\n    set_callback_parameters,\n)\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.data.reader.data_reader_factory import create_data_reader\nfrom elasticdl.python.elasticdl.callbacks import MaxStepsStopping\nfrom elasticdl.python.master.evaluation_service import EvaluationService\nfrom elasticdl.python.master.k8s_instance_manager import InstanceManager\nfrom elasticdl.python.master.servicer import MasterServicer\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\nfrom elasticdl.python.master.tensorboard_service import TensorboardService\n\n\ndef _make_task_dispatcher(\n    training_data,\n    validation_data,\n    prediction_data,\n    records_per_task,\n    num_epochs,\n    data_reader_params,\n    create_data_reader_fn,\n    callbacks_list,\n):\n    def _maybe_create_shards(data_origin):\n        kwargs = get_dict_from_params_str(data_reader_params)\n        partition = kwargs.get(""partition"", None) if kwargs else None\n        return (\n            create_data_reader_fn(\n                data_origin=data_origin,\n                records_per_task=records_per_task,\n                partition=partition,\n            ).create_shards()\n            if data_origin\n            else {}\n        )\n\n    prediction_f_records = _maybe_create_shards(prediction_data)\n\n    return _TaskDispatcher(\n        _maybe_create_shards(training_data),\n        _maybe_create_shards(validation_data),\n        prediction_f_records,\n        records_per_task,\n        # Only generate prediction tasks for 1 epoch\n        1 if prediction_f_records else num_epochs,\n        callbacks_list,\n    )\n\n\nclass Master(object):\n    def __init__(self, args):\n        self.logger = get_logger(""master"", level=args.log_level.upper())\n\n        self.num_ps_pods = args.num_ps_pods\n        self.checkpoint_output_path = args.checkpoint_dir\n        self.distribution_strategy = args.distribution_strategy\n\n        # Master addr\n        master_ip = os.getenv(""MY_POD_IP"", ""localhost"")\n        self.master_addr = ""%s:%d"" % (master_ip, args.port)\n        self.job_type = Master._get_job_type(args)\n\n        # Initialize TensorBoard service if requested\n        self.tb_service = self._create_tensorboard_service(\n            args.tensorboard_log_dir, master_ip\n        )\n        if self.tb_service:\n            self.tb_client = TensorBoardClient(\n                job_name=args.job_name,\n                image_name=args.worker_image,\n                namespace=args.namespace,\n            )\n\n        # Initialize the components from the model definition\n        self.model_module = load_module(\n            get_module_file_path(args.model_zoo, args.model_def)\n        ).__dict__\n        self.model_inst = load_model_from_module(\n            args.model_def, self.model_module, args.model_params\n        )\n        self.optimizer = self.model_module[args.optimizer]()\n        self._create_data_reader_fn = create_data_reader\n        if args.custom_data_reader in self.model_module:\n            self._create_data_reader_fn = self.model_module[\n                args.custom_data_reader\n            ]\n\n        # Initialize the callbacks\n        self.callbacks_list = load_callbacks_from_module(\n            args.callbacks, self.model_module\n        )\n        self.callbacks_list.set_model(self.model_inst)\n        set_callback_parameters(\n            self.callbacks_list,\n            batch_size=args.minibatch_size,\n            saved_model_path=args.output,\n            checkpoint_path=args.checkpoint_dir,\n        )\n        self._set_completed_steps_by_checkpoint(args.checkpoint_dir_for_init)\n\n        # Start task queue\n        records_per_task = args.minibatch_size * args.num_minibatches_per_task\n        self.task_d = _make_task_dispatcher(\n            args.training_data,\n            args.validation_data,\n            args.prediction_data,\n            records_per_task,\n            args.num_epochs,\n            args.data_reader_params,\n            self._create_data_reader_fn,\n            self.callbacks_list,\n        )\n\n        self.task_d.add_deferred_callback_create_train_end_task()\n        self.evaluation_service = self._create_evaluation_service(args)\n\n        # Initialize master service\n        self.master_servicer, self.server = self._create_master_service(args)\n\n        # Initialize instance manager\n        self.instance_manager = self._create_instance_manager(args)\n\n        self._should_stop = False\n        self._exit_code = 0\n        threading.Thread(\n            target=self._check_timeout_tasks,\n            name=""check_timeout_tasks"",\n            daemon=True,\n        ).start()\n\n    def _set_completed_steps_by_checkpoint(self, checkpoint_dir_for_init):\n        if not checkpoint_dir_for_init:\n            return\n\n        if not CheckpointSaver.check_checkpoint_valid(checkpoint_dir_for_init):\n            raise ValueError(\n                ""Invalid checkpoint directory {}"".format(\n                    checkpoint_dir_for_init\n                )\n            )\n\n        model_verion = CheckpointSaver.get_version_from_checkpoint(\n            checkpoint_dir_for_init\n        )\n        for callback in self.callbacks_list.callbacks:\n            if isinstance(callback, MaxStepsStopping):\n                callback.set_completed_steps(model_verion)\n\n    def request_stop(self, err_msg=None):\n        """"""Request master to quit""""""\n        self._should_stop = True\n        if err_msg:\n            self.logger.error(err_msg)\n            # TODO (chengfu.wcy) create meaningful status codes\n            self._exit_code = -1\n\n    def prepare(self):\n        """"""\n        Start the components one by one. Make sure that it is ready to run.\n        """"""\n        # Start the evaluation service if requested\n        if self.evaluation_service:\n            self.logger.info(""Starting evaluation service"")\n            self.evaluation_service.start()\n            self.logger.info(""Evaluation service started"")\n\n        # Start the master GRPC server\n        self.logger.info(""Starting master RPC server"")\n        self.server.start()\n        self.logger.info(""Master RPC server started"")\n\n        # Start the worker manager if requested\n        if self.instance_manager:\n            self.instance_manager.update_status(InstanceManagerStatus.PENDING)\n            if self.distribution_strategy == DistributionStrategy.ALLREDUCE:\n                # Exposes the consensus service for allreduce-based training\n                self.instance_manager.start_ftlib_consensus_service()\n            else:\n                self.instance_manager.start_parameter_servers()\n            self.instance_manager.start_workers()\n            self.instance_manager.update_status(InstanceManagerStatus.RUNNING)\n\n        # Start TensorBoard k8s Service if requested\n        if self.tb_service and self.tb_client:\n            self.logger.info(""Starting tensorboard service"")\n            self.tb_service.start()\n            self.tb_client.start_tensorboard_service()\n            self.logger.info(""Tensorboard service started"")\n\n    def run(self):\n        """"""\n        The main loop of master.\n        Dispatch the tasks to the workers until all the tasks are completed.\n        """"""\n        try:\n            while True:\n                if self.instance_manager.all_workers_failed:\n                    raise Exception(\n                        ""All workers fail with unrecoverable errors""\n                    )\n                    break\n                if self.task_d.finished():\n                    if self.instance_manager:\n                        self.instance_manager.update_status(\n                            InstanceManagerStatus.FINISHED\n                        )\n                    break\n                if self._should_stop:\n                    break\n                time.sleep(30)\n        except KeyboardInterrupt:\n            self.logger.warning(""Server stopping"")\n        finally:\n            self._stop()\n        return self._exit_code\n\n    def _stop(self):\n        """"""\n        Stop all the components.\n        Make sure that the created services and components are shut down.\n        """"""\n        self.logger.info(""Stopping master"")\n\n        if self.evaluation_service:\n            self.logger.info(""Stopping evaluation service"")\n            self.evaluation_service.stop()\n            self.logger.info(""Evaluation service stopped"")\n\n        self.logger.info(""Stopping RPC server"")\n        self.server.stop(None)  # grace = None\n        self.logger.info(""RPC server stopped"")\n\n        # Keep TensorBoard running when all the tasks are finished\n        if self.tb_service:\n            self.logger.info(\n                ""All tasks finished. Keeping TensorBoard service running...""\n            )\n            while True:\n                if self.tb_service.is_active():\n                    time.sleep(10)\n                else:\n                    self.logger.warning(\n                        ""Unable to keep TensorBoard running. ""\n                        ""It has already terminated""\n                    )\n                    break\n        self.logger.info(""Master stopped"")\n\n    @staticmethod\n    def _get_job_type(args):\n        if all(\n            (\n                args.training_data,\n                args.validation_data,\n                args.evaluation_throttle_secs or args.evaluation_steps,\n            )\n        ):\n            job_type = JobType.TRAINING_WITH_EVALUATION\n        elif all(\n            (\n                args.validation_data,\n                not args.training_data,\n                not args.prediction_data,\n            )\n        ):\n            job_type = JobType.EVALUATION_ONLY\n        elif all(\n            (\n                args.prediction_data,\n                not args.validation_data,\n                not args.training_data,\n            )\n        ):\n            job_type = JobType.PREDICTION_ONLY\n        else:\n            job_type = JobType.TRAINING_ONLY\n\n        return job_type\n\n    def _create_tensorboard_service(self, tensorboard_log_dir, master_ip):\n        tb_service = None\n        if tensorboard_log_dir:\n            self.logger.info(\n                ""Creating TensorBoard service with log directory %s"",\n                tensorboard_log_dir,\n            )\n            # Start TensorBoard CLI\n            tb_service = TensorboardService(tensorboard_log_dir, master_ip)\n\n        return tb_service\n\n    def _create_evaluation_service(self, args):\n        evaluation_service = None\n        if (\n            self.job_type == JobType.TRAINING_WITH_EVALUATION\n            or self.job_type == JobType.EVALUATION_ONLY\n        ):\n            self.logger.info(\n                ""Creating evaluation service with throttle seconds %d ""\n                "" and evaluation steps %d"",\n                args.evaluation_throttle_secs,\n                args.evaluation_steps,\n            )\n            evaluation_service = EvaluationService(\n                self.tb_service,\n                self.task_d,\n                args.evaluation_start_delay_secs,\n                args.evaluation_throttle_secs,\n                args.evaluation_steps,\n                self.job_type == JobType.EVALUATION_ONLY,\n                self.model_module[args.eval_metrics_fn],\n            )\n            self.task_d.set_evaluation_service(evaluation_service)\n\n        return evaluation_service\n\n    def _create_master_service(self, args):\n        self.logger.info(""Creating master service"")\n        server = grpc.server(\n            futures.ThreadPoolExecutor(max_workers=64),\n            options=[\n                (""grpc.max_send_message_length"", GRPC.MAX_SEND_MESSAGE_LENGTH),\n                (\n                    ""grpc.max_receive_message_length"",\n                    GRPC.MAX_RECEIVE_MESSAGE_LENGTH,\n                ),\n            ],\n        )\n        master_servicer = MasterServicer(\n            args.minibatch_size,\n            self.task_d,\n            evaluation_service=self.evaluation_service,\n        )\n        elasticdl_pb2_grpc.add_MasterServicer_to_server(\n            master_servicer, server\n        )\n        server.add_insecure_port(""[::]:{}"".format(args.port))\n        self.logger.info(""The port of the master server is: %d"", args.port)\n\n        return master_servicer, server\n\n    def _create_instance_manager(self, args):\n        instance_manager = None\n\n        container_command = [""/bin/bash""]\n        if args.num_workers:\n            assert args.worker_image, ""Worker image cannot be empty""\n\n            worker_client_command = (\n                BashCommandTemplate.SET_PIPEFAIL\n                + "" python -m elasticdl.python.worker.main""\n            )\n            worker_args = [\n                ""--master_addr"",\n                self.master_addr,\n                ""--job_type"",\n                self.job_type,\n            ]\n            worker_args.extend(\n                build_arguments_from_parsed_result(args, filter_args=[""envs""])\n            )\n            worker_args = wrap_python_args_with_string(worker_args)\n            worker_args.insert(0, worker_client_command)\n\n            if args.use_go_ps:\n                opt_type, opt_args = get_optimizer_info(self.optimizer)\n                # TODO: rename the Go PS executable using a meaningful filename\n                ps_client_command = ""main""\n                ps_args = [\n                    ""-job_name="" + args.job_name,\n                    ""-namespace="" + args.namespace,\n                    ""-master_addr="" + self.master_addr,\n                    ""-port=2222"",\n                    ""-use_async="" + (""true"" if args.use_async else ""false""),\n                    ""-grads_to_wait="" + str(args.grads_to_wait),\n                    ""-lr_staleness_modulation=""\n                    + (""true"" if args.lr_staleness_modulation else ""false""),\n                    ""-sync_version_tolerance=""\n                    + str(args.sync_version_tolerance),\n                    ""-evaluation_steps="" + str(args.evaluation_steps),\n                    ""-num_ps_pods="" + str(args.num_ps_pods),\n                    ""-num_workers="" + str(args.num_workers),\n                    ""-checkpoint_dir="" + str(args.checkpoint_dir),\n                    ""-checkpoint_steps="" + str(args.checkpoint_steps),\n                    ""-keep_checkpoint_max="" + str(args.keep_checkpoint_max),\n                    ""-checkpoint_dir_for_init=""\n                    + str(args.checkpoint_dir_for_init),\n                    ""-opt_type="" + opt_type,\n                    ""-opt_args="" + opt_args,\n                ]\n                ps_args = wrap_go_args_with_string(ps_args)\n                ps_args.insert(0, ps_client_command)\n            else:\n                ps_client_command = (\n                    BashCommandTemplate.SET_PIPEFAIL\n                    + "" python -m elasticdl.python.ps.main""\n                )\n                ps_args = [\n                    ""--grads_to_wait"",\n                    str(args.grads_to_wait),\n                    ""--lr_staleness_modulation"",\n                    str(args.lr_staleness_modulation),\n                    ""--sync_version_tolerance"",\n                    str(args.sync_version_tolerance),\n                    ""--use_async"",\n                    str(args.use_async),\n                    ""--model_zoo"",\n                    args.model_zoo,\n                    ""--model_def"",\n                    args.model_def,\n                    ""--job_name"",\n                    args.job_name,\n                    ""--port"",\n                    ""2222"",\n                    ""--master_addr"",\n                    self.master_addr,\n                    ""--namespace"",\n                    args.namespace,\n                    ""--evaluation_steps"",\n                    str(args.evaluation_steps),\n                    ""--checkpoint_dir"",\n                    str(args.checkpoint_dir),\n                    ""--checkpoint_steps"",\n                    str(args.checkpoint_steps),\n                    ""--keep_checkpoint_max"",\n                    str(args.keep_checkpoint_max),\n                    ""--num_ps_pods"",\n                    str(args.num_ps_pods),\n                    ""--checkpoint_dir_for_init"",\n                    str(args.checkpoint_dir_for_init),\n                    ""--num_workers"",\n                    str(args.num_workers),\n                    ""--log_level"",\n                    str(args.log_level),\n                    ""--minibatch_size"",\n                    str(args.minibatch_size),\n                    ""--num_minibatches_per_task"",\n                    str(args.num_minibatches_per_task),\n                ]\n                ps_args = wrap_python_args_with_string(ps_args)\n                ps_args.insert(0, ps_client_command)\n\n            worker_args = [""-c"", "" "".join(worker_args)]\n            ps_args = [""-c"", "" "".join(ps_args)]\n\n            env_dict = parse_envs(args.envs)\n            env = []\n            for key in env_dict:\n                env.append(V1EnvVar(name=key, value=env_dict[key]))\n\n            kwargs = get_dict_from_params_str(args.aux_params)\n            disable_relaunch = kwargs.get(""disable_relaunch"", False)\n\n            instance_manager = InstanceManager(\n                self.task_d,\n                job_name=args.job_name,\n                image_name=args.worker_image,\n                worker_command=container_command,\n                worker_args=worker_args,\n                namespace=args.namespace,\n                num_workers=args.num_workers,\n                worker_resource_request=args.worker_resource_request,\n                worker_resource_limit=args.worker_resource_limit,\n                worker_pod_priority=args.worker_pod_priority,\n                num_ps=args.num_ps_pods,\n                ps_command=container_command,\n                ps_args=ps_args,\n                ps_resource_request=args.ps_resource_request,\n                ps_resource_limit=args.ps_resource_limit,\n                ps_pod_priority=args.ps_pod_priority,\n                volume=args.volume,\n                image_pull_policy=args.image_pull_policy,\n                restart_policy=args.restart_policy,\n                cluster_spec=args.cluster_spec,\n                envs=env,\n                expose_ports=self.distribution_strategy\n                == DistributionStrategy.ALLREDUCE,\n                disable_relaunch=disable_relaunch,\n                log_file_path=args.log_file_path,\n            )\n\n        return instance_manager\n\n    def _check_timeout_tasks(self):\n        while True:\n            doing_tasks = self.task_d._doing.copy()\n            cur_time = time.time()\n            avg_time = self.master_servicer.get_average_task_complete_time()\n            for task_id, (worker_id, task, start_time) in doing_tasks.items():\n                if task.type == elasticdl_pb2.TRAINING:\n                    start_time = self.master_servicer.get_worker_liveness_time(\n                        worker_id\n                    )\n                if task.type in [\n                    elasticdl_pb2.TRAINING,\n                    elasticdl_pb2.EVALUATION,\n                ]:\n                    if (cur_time - start_time) > 3 * avg_time[task.type]:\n                        self.logger.info(\n                            ""worker %d timeout, relaunch it"" % worker_id\n                        )\n                        self.task_d.recover_tasks(worker_id)\n                        # TODO: save worker logs before remove it\n                        self.instance_manager._remove_worker(worker_id)\n                        break\n            time.sleep(30)\n'"
elasticdl/python/master/servicer.py,0,"b'import statistics\nimport threading\nimport time\n\nfrom google.protobuf import empty_pb2\n\nfrom elasticdl.proto import elasticdl_pb2, elasticdl_pb2_grpc\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n\nclass MasterServicer(elasticdl_pb2_grpc.MasterServicer):\n    """"""Master service implementation""""""\n\n    def __init__(\n        self, minibatch_size, task_d, evaluation_service,\n    ):\n        # TODO: group params together into a single object.\n        self._task_d = task_d\n        self._lock = threading.Lock()\n        self._minibatch_size = minibatch_size\n        self._version = 0\n\n        self._evaluation_service = evaluation_service\n        self._task_complete_times = {\n            elasticdl_pb2.EVALUATION: [],\n            elasticdl_pb2.TRAINING: [],\n        }\n        self._worker_liveness_time = {}\n        if evaluation_service:\n            evaluation_service.set_master_servicer(self)\n\n    @staticmethod\n    def var_name_encode(name):\n        return name.replace("":"", ""-"")\n\n    def get_model_version(self):\n        return self._version\n\n    def get_task(self, request, _):\n        res = elasticdl_pb2.Task()\n        res.model_version = self._version\n        res.minibatch_size = self._minibatch_size\n        if request.task_type == elasticdl_pb2.EVALUATION:\n            task_id, task = self._task_d.get_eval_task(request.worker_id)\n        else:\n            task_id, task = self._task_d.get(request.worker_id)\n\n        if task:\n            res.task_id = task_id\n            res.shard_name = task.shard_name\n            res.start = task.start\n            res.end = task.end\n            res.type = task.type\n            for k, v in task.extended_config.items():\n                res.extended_config[k] = v\n\n            # For evaluation task, it will use the fixed version model\n            if task.type == elasticdl_pb2.EVALUATION:\n                res.model_version = task.model_version\n        elif (not self._task_d.finished()) or (\n            self._task_d.invoke_deferred_callback()\n        ):\n            # If the todo and doing tasks are not empty,\n            # Otherwise if the callback list is not empty,\n            # we are trying to pop and invoke the callback.\n            # Then the master tells the worker to wait\n            # in case of new tasks later.\n            res.type = elasticdl_pb2.WAIT\n        with self._lock:\n            self._worker_liveness_time[request.worker_id] = time.time()\n        return res\n\n    def report_task_result(self, request, _):\n        if request.err_message:\n            logger.warning(""Worker reported error: "" + request.err_message)\n            self._task_d.report(request, False)\n        else:\n            complete_time, task, worker_id = self._task_d.report(request, True)\n            if task:\n                with self._lock:\n                    self._worker_liveness_time[worker_id] = time.time()\n                    if task.type in [\n                        elasticdl_pb2.TRAINING,\n                        elasticdl_pb2.EVALUATION,\n                    ]:\n                        self._task_complete_times[task.type].append(\n                            complete_time\n                        )\n        return empty_pb2.Empty()\n\n    def report_evaluation_metrics(self, request, _):\n        with self._lock:\n            self._worker_liveness_time[request.worker_id] = time.time()\n        self._evaluation_service.report_evaluation_metrics(\n            request.model_outputs, request.labels\n        )\n        return empty_pb2.Empty()\n\n    def report_version(self, request, _):\n        self._version = request.model_version\n        if self._evaluation_service:\n            self._evaluation_service.add_evaluation_task_if_needed(\n                master_locking=False, model_version=request.model_version\n            )\n        return empty_pb2.Empty()\n\n    def get_average_task_complete_time(self):\n        if len(self._task_complete_times) < 20:\n            return {\n                elasticdl_pb2.TRAINING: 300,\n                elasticdl_pb2.EVALUATION: 300,\n            }\n        else:\n            return {\n                elasticdl_pb2.TRAINING: statistics.mean(\n                    self._task_complete_times[elasticdl_pb2.TRAINING]\n                ),\n                elasticdl_pb2.EVALUATION: statistics.mean(\n                    self._task_complete_times[elasticdl_pb2.EVALUATION]\n                ),\n            }\n\n    def get_worker_liveness_time(self, worker_id):\n        return self._worker_liveness_time[worker_id]\n'"
elasticdl/python/master/task_dispatcher.py,3,"b'""""""TaskQueue Implementation""""""\n\nimport random\nimport threading\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.callbacks import CallbackList\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.constants import TaskExecCounterKey\nfrom elasticdl.python.common.log_utils import default_logger as logger\n\n_MAX_TASK_RETRIES = 3\n\n\nclass _Task(object):\n    """"""Internal representation of a task""""""\n\n    def __init__(\n        self, shard_name, start, end, type, model_version=-1, **kwargs\n    ):\n        self.shard_name = shard_name\n        self.start = start\n        self.end = end\n        self.type = type\n        self.model_version = model_version\n        self.extended_config = kwargs\n\n    def _info(self):\n        return (\n            self.shard_name,\n            self.start,\n            self.end,\n            self.type,\n            self.model_version,\n        )\n\n\nclass JobCounter(object):\n    """"""Counters for job""""""\n\n    def __init__(self, total_records=0, failed_records=0):\n        self._total_records = total_records\n        self._failed_records = failed_records\n\n    @property\n    def total_records(self):\n        return self._total_records\n\n    @total_records.setter\n    def total_records(self, total_records):\n        self._total_records = total_records\n\n    @property\n    def failed_records(self):\n        return self._failed_records\n\n    @failed_records.setter\n    def failed_records(self, failed_records):\n        self._failed_records = failed_records\n\n\nclass _TaskDispatcher(object):\n    """"""Creates and dispatches Tasks. Keep track of a Task\'s lifecycle.""""""\n\n    def __init__(\n        self,\n        training_shards,\n        evaluation_shards,\n        prediction_shards,\n        records_per_task,\n        num_epochs,\n        callbacks_list=None,\n    ):\n        """"""\n        Arguments:\n            training_shards: A dictionary from RecordIO file name to the\n                number of training records.\n            evaluation_shards: A dictionary from RecordIO file name to\n                the number of evaluation records.\n            prediction_shards: A dictionary from RecordIO file name to\n                the number of prediction records.\n            records_per_task: The number of records per task.\n            num_epochs: The total number of epochs for the tasks where\n                an epoch is a complete iteration over the shards.\n            callbacks_list: The Keras CallbacksList object to contain all\n                callback instances.\n        """"""\n        self._lock = threading.Lock()\n\n        self._num_epochs = num_epochs\n        self._epoch = 0\n        self._training_shards = training_shards\n        self._evaluation_shards = evaluation_shards\n        self._prediction_shards = prediction_shards\n        self._records_per_task = records_per_task\n        self._init_callbacks(callbacks_list)\n\n        self._todo = []\n        # dictionary from task id to Task.\n        self._doing = {}\n        self._task_id = 0\n        self._eval_todo = []\n        self._evaluation_service = None\n\n        # Callback list to invoke after all tasks complete.\n        self._tasks_done_deferred_callbacks = []\n\n        self._job_counters = {}\n        self._task_retry_count = {}\n\n        if self._training_shards:\n            logger.info(""Starting epoch %d"", self._epoch)\n            self.create_tasks(elasticdl_pb2.TRAINING)\n        elif self._evaluation_shards:\n            self.create_tasks(elasticdl_pb2.EVALUATION)\n        elif self._prediction_shards:\n            self.create_tasks(elasticdl_pb2.PREDICTION)\n\n    def _init_callbacks(self, callbacks_list):\n        if callbacks_list is None:\n            self._callbacks_list = CallbackList([])\n            self._callbacks_list.set_model(tf.keras.Model())\n        else:\n            self._callbacks_list = callbacks_list\n\n        self._callbacks_list.model.stop_training = False\n\n    def reset_job_counters(self, task_type):\n        """"""Return record number in specific task_type""""""\n        self._job_counters[task_type] = JobCounter()\n\n    def create_tasks(self, task_type, model_version=-1):\n        logger.info(\n            ""Creating a new set of %s tasks for model version %d"",\n            elasticdl_pb2._TASKTYPE.values_by_number[task_type].name.lower(),\n            model_version,\n        )\n        self.reset_job_counters(task_type)\n        if task_type == elasticdl_pb2.TRAINING:\n            shards = self._training_shards\n        elif task_type == elasticdl_pb2.EVALUATION:\n            shards = self._evaluation_shards\n        else:\n            shards = self._prediction_shards\n        tasks = []\n        num_records_before_create = self._job_counters[task_type].total_records\n        # Note that a shard may contain records for multiple tasks.\n        for (\n            shard_name,\n            (start_ind_this_shard, num_records_this_shard),\n        ) in shards.items():\n            max_ind_this_shard = start_ind_this_shard + num_records_this_shard\n            self._job_counters[\n                task_type\n            ].total_records += num_records_this_shard\n            for start_ind_this_task in range(\n                start_ind_this_shard,\n                max_ind_this_shard,\n                self._records_per_task,\n            ):\n                end_ind_this_task = min(\n                    start_ind_this_task + self._records_per_task,\n                    max_ind_this_shard,\n                )\n\n                # Note that only records in [start, end) of this task\n                # will be consumed later in the worker that handles\n                # this task.\n                tasks.append(\n                    _Task(\n                        shard_name=shard_name,\n                        start=start_ind_this_task,\n                        end=end_ind_this_task,\n                        type=task_type,\n                        model_version=model_version,\n                    )\n                )\n        if task_type == elasticdl_pb2.TRAINING:\n            random.shuffle(tasks)\n            self._todo.extend(tasks)\n        elif task_type == elasticdl_pb2.EVALUATION:\n            self._eval_todo.extend(tasks)\n        else:\n            self._todo.extend(tasks)\n        logger.info(\n            ""%d tasks created with total of %d records.""\n            % (\n                len(tasks),\n                self._job_counters[task_type].total_records\n                - num_records_before_create,\n            )\n        )\n\n    def get_eval_task(self, worker_id):\n        """"""Return next evaluation (task_id, Task) tuple""""""\n        with self._lock:\n            if not self._eval_todo:\n                return -1, None\n            self._task_id += 1\n            task = self._eval_todo.pop()\n            self._doing[self._task_id] = (worker_id, task, time.time())\n            return self._task_id, task\n\n    def _create_train_end_callback_task(self):\n        """"""\n        Build one instance of training end task and add it to todo list.\n        Because we need create a dataset to build the model for\n        SavedModelExporter to execute on_train_end,we include\n        a shard of data in this task.\n        """"""\n        if not self._training_shards:\n            return\n\n        self.reset_job_counters(elasticdl_pb2.TRAIN_END_CALLBACK)\n        shards = self._training_shards\n        assert shards is not None\n\n        (shard_name, (start_ind_this_shard, num_records_this_shard)) = next(\n            iter(shards.items())\n        )\n        start_ind_this_task = start_ind_this_shard\n        end_ind_this_task = start_ind_this_shard + min(\n            self._records_per_task, num_records_this_shard\n        )\n\n        # Use the first shard of data to do the SavedModel work\n        train_end_callback_task = _Task(\n            shard_name=shard_name,\n            start=start_ind_this_task,\n            end=end_ind_this_task,\n            type=elasticdl_pb2.TRAIN_END_CALLBACK,\n        )\n\n        self._todo.append(train_end_callback_task)\n\n    def add_deferred_callback_create_train_end_task(self):\n        self._tasks_done_deferred_callbacks.append(\n            lambda: self._create_train_end_callback_task()\n        )\n\n    def invoke_deferred_callback(self):\n        """"""\n        Pop a callback from the list and invoke it.\n        If the callback list is empty, return False directly.\n        """"""\n        if not self._tasks_done_deferred_callbacks:\n            return False\n\n        with self._lock:\n            if not self._tasks_done_deferred_callbacks:\n                return False\n\n            callback = self._tasks_done_deferred_callbacks.pop()\n            callback()\n            return True\n\n    def get(self, worker_id):\n        """"""Return next (task_id, Task) tuple""""""\n\n        with self._lock:\n            # TODO: check if task queue doesn\'t have training task,\n            #       to avoid the queue is overwhelmed by evaluation tasks.\n            if (\n                not self._todo\n                and not self._callbacks_list.model.stop_training\n                and self._epoch < self._num_epochs - 1\n            ):\n                # Start a new epoch\n                self._epoch += 1\n                self.create_tasks(elasticdl_pb2.TRAINING)\n                logger.info(""Starting epoch %d"", self._epoch)\n\n            if not self._todo:\n                # No more tasks\n                return -1, None\n\n            self._task_id += 1\n            task = self._todo.pop()\n            # TODO: Handle timeout of tasks.\n            self._doing[self._task_id] = (worker_id, task, time.time())\n\n            return self._task_id, task\n\n    def report(self, request, success):\n        """"""Report if the task is successful or not""""""\n\n        task_id = request.task_id\n        evaluation_task_completed = False\n        with self._lock:\n            worker_id, task, start_time = self._doing.pop(\n                task_id, (-1, None, -1)\n            )\n            if task:\n                self._job_counters[\n                    task.type\n                ].failed_records += request.exec_counters.get(\n                    TaskExecCounterKey.FAIL_COUNT, 0\n                )\n            if not task:\n                logger.warning(""Unknown task_id: %d"" % task_id)\n            elif not success:\n                logger.warning(""Task %d of %s failed "" % (task_id, task.type))\n                if not self.check_exceed_max_task_retries(task):\n                    if task.type in [\n                        elasticdl_pb2.TRAINING,\n                        elasticdl_pb2.TRAIN_END_CALLBACK,\n                    ]:\n                        self._todo.append(task)\n                    else:\n                        self._eval_todo.append(task)\n            elif (\n                task.type == elasticdl_pb2.EVALUATION\n                and self._evaluation_service is not None\n            ):\n                evaluation_task_completed = True\n            else:\n                self._call_on_task_end(task)\n                logger.info(\n                    ""Task:%d completed, %d remaining tasks"",\n                    task_id,\n                    len(self._todo) + len(self._doing),\n                )\n            if evaluation_task_completed:\n                self._evaluation_service.complete_task()\n\n            if success:\n                if task in self._task_retry_count:\n                    del self._task_retry_count[task]\n                if self._callbacks_list.model.stop_training:\n                    # Clear todo list to stop training\n                    self._todo = []\n\n        return (time.time() - start_time), task, worker_id\n\n    def check_exceed_max_task_retries(self, task):\n        self._task_retry_count.setdefault(task, 1)\n        self._task_retry_count[task] += 1\n        if self._task_retry_count[task] > _MAX_TASK_RETRIES:\n            logger.error(\n                ""A %s task failed with %d retries ""\n                % (task.type, _MAX_TASK_RETRIES)\n            )\n            return True\n        return False\n\n    def finished(self):\n        """"""Return if all tasks are done""""""\n        return all([not self._todo, not self._eval_todo, not self._doing])\n\n    def recover_tasks(self, worker_id):\n        """"""Recover doing tasks for a dead worker""""""\n\n        with self._lock:\n            ids = [\n                id\n                for id, (wid, _, _) in self._doing.items()\n                if wid == worker_id\n            ]\n        request = elasticdl_pb2.ReportTaskResultRequest()\n        for id in ids:\n            request.task_id = id\n            self.report(request, False)\n\n    # TODO: need to re-check after refactoring servicer.py\n    def set_evaluation_service(self, evaluation_service):\n        with self._lock:\n            self._evaluation_service = evaluation_service\n            if self._evaluation_shards and not self._training_shards:\n                evaluation_service.init_eval_only_job(len(self._eval_todo))\n\n    def _call_on_task_end(self, task):\n        # The on_task_end is not a method of tf.keras.callbacks.Callback\n        # and tf.keras.callbacks.CallbackList. So, we need to check\n        # before calling the method.\n        for callback in self._callbacks_list.callbacks:\n            if hasattr(callback, ""on_task_end""):\n                callback.on_task_end(task)\n'"
elasticdl/python/master/tensorboard_service.py,2,"b'import datetime\nimport subprocess\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass TensorboardService(object):\n    """"""Tensorboard Service implementation""""""\n\n    def __init__(self, tensorboard_log_dir, master_ip):\n        """"""\n        Arguments:\n            tensorboard_log_dir: The log directory for Tensorboard.\n        """"""\n        _current_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")\n        self._tensorboard_log_dir = tensorboard_log_dir + _current_time\n        self._initialize_summary_writer()\n        self._master_ip = master_ip\n        self.tb_process = None\n\n    def _initialize_summary_writer(self):\n        self.summary_writer = tf.summary.create_file_writer(\n            self._tensorboard_log_dir\n        )\n\n    def write_dict_to_summary(self, dictionary, version):\n        with self.summary_writer.as_default():\n            for k, v in dictionary.items():\n                if isinstance(v, np.ndarray) and len(v) == 1:\n                    v = v[0]\n                elif isinstance(v, dict) and v:\n                    v = list(v.values())[0]\n                tf.summary.scalar(k, v, step=version)\n\n    def start(self):\n        # TODO: Find a good way to catch the exception if any.\n        # `tb_process.poll()` is unreliable as TensorBoard won\'t\n        # exit immediately in some cases, e.g. when host is missing.\n        self.tb_process = subprocess.Popen(\n            [\n                ""tensorboard --logdir %s --host %s""\n                % (self._tensorboard_log_dir, self._master_ip)\n            ],\n            shell=True,\n            stdout=subprocess.DEVNULL,\n        )\n\n    def is_active(self):\n        return self.tb_process.poll() is None\n'"
elasticdl/python/ps/__init__.py,0,b''
elasticdl/python/ps/embedding_table.py,3,"b'import threading\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.proto.elasticdl_pb2 import EmbeddingTableInfo\nfrom elasticdl.python.common.dtypes import dtype_numpy_to_tensor\n\n\nclass EmbeddingTable(object):\n    """"""\n    EmbeddingTable is used to store embedding parameters of an embedding\n    layer. The name of an embedding table is actually the embedding layer\n    name. It uses a dictionary to store embedding vectors, the key is the\n    item id, the value is a 1-D numpy.ndarray.\n\n    Embedding vectors are lazily initialized in parameter server.\n    EmbeddingTable also has dim and initializer fields. Inside the get\n    interface of EmbeddingTable, if the id is not in the embedding_vectors\n    dictionary, the corresponding value will be initialized.\n    """"""\n\n    def __init__(self, name, dim=None, initializer=None, is_slot=False):\n        """"""\n        Args:\n            name: The embedding table name.\n            dim: The dimension of embeddings in this embedding table.\n            initializer: The initializer to initialize new embeddings. If this\n                embedding table is for slots, `initializer` is a float and this\n                table will initialize with constant initializer. Otherwise\n                `initializer` is the name of Keras initializer.\n            is_slot: A bool. True for storing slot variable, otherwise false.\n        """"""\n        self.name = name\n        self.dim = dim\n        self.initializer_value = initializer\n        # set dtype to float32\n        self.dtype = np.dtype(""float32"")\n        if is_slot:\n            self.initializer = tf.keras.initializers.Constant(\n                float(self.initializer_value)\n            )\n        else:\n            self.initializer = tf.keras.initializers.get(\n                self.initializer_value\n            )\n        self.is_slot = is_slot\n        self.embedding_vectors = {}\n        self._lock = threading.Lock()\n\n    def get(self, indices):\n        if len(indices) == 0:\n            return None\n        values = []\n        for i in indices:\n            value = self.embedding_vectors.get(i, None)\n            if value is None:\n                with self._lock:\n                    value = self.initializer(shape=(self.dim,)).numpy()\n                    self.embedding_vectors[i] = value\n            values.append(value)\n        return np.stack(values)\n\n    def set(self, indices, values):\n        # TODO(qijun) need to add a RWLock in Sync-SGD\n        for index, i in enumerate(indices):\n            embedding_vector = values[index]\n            self.embedding_vectors[i] = embedding_vector\n\n    def clear(self):\n        self.embedding_vectors.clear()\n\n    def to_indexed_slices(self):\n        indices = []\n        embedding_vectors = []\n        with self._lock:\n            for id, embedding_vector in self.embedding_vectors.items():\n                indices.append(id)\n                embedding_vectors.append(embedding_vector)\n        return tf.IndexedSlices(\n            values=np.array(embedding_vectors), indices=np.array(indices)\n        )\n\n    def to_embedding_table_info_pb(self):\n        """"""Convert the embedding table information to a protobuf""""""\n        embedding_pb = EmbeddingTableInfo()\n        embedding_pb.name = self.name\n        embedding_pb.dim = self.dim\n        embedding_pb.initializer = str(self.initializer_value)\n        embedding_pb.dtype = dtype_numpy_to_tensor(self.dtype)\n        return embedding_pb\n\n    def get_table_size(self):\n        """"""Get the element count of an embedding table""""""\n        if len(self.embedding_vectors) > 0:\n            element_size = list(self.embedding_vectors.values())[0].itemsize\n            size = self.dim * len(self.embedding_vectors) * element_size\n            return size\n        return 0\n\n    def debug_info(self):\n        return (\n            ""Embedding param name: %s\\n  shape: [%d, %d]\\n  size: %d bytes\\n""\n            % (\n                self.name,\n                len(self.embedding_vectors),\n                self.dim,\n                self.get_table_size(),\n            )\n        )\n\n\n# TODO(bug): create_embedding_table does not create EmbeddingTable correctly\n#     if it is a slot table.\ndef create_embedding_table(embedding_table_info_pb):\n    name = embedding_table_info_pb.name\n    dim = embedding_table_info_pb.dim\n    initializer = embedding_table_info_pb.initializer\n    return EmbeddingTable(name, dim, initializer)\n\n\ndef get_slot_table_name(embedding_name, slot_name):\n    return embedding_name + ""-"" + slot_name\n'"
elasticdl/python/ps/learning_rate_modulator.py,0,"b'import threading\n\n\nclass LearningRateModulator:\n    """"""Modulates the learning rate with a multiplier.\n\n    Note:\n        This class supports concurrent usage by using\n        thread local storage.\n    """"""\n\n    def __init__(self, learning_rate):\n        """"""Constructs a `LearningRateModulator` instance.\n\n        Args:\n            learning_rate: The learning rate to be modulated.\n                This can be either a numeric value or a callable.\n        """"""\n        self._learning_rate = learning_rate\n        self._tls = threading.local()\n        self._tls.multiplier = 1\n\n    def set_multiplier(self, multiplier):\n        """"""Sets the multiplier.\n\n        Args:\n            multiplier: The multiplier used to modulate the learning rate.\n        """"""\n        self._tls.multiplier = multiplier\n\n    def get_learning_rate(self):\n        """"""Gets the modulated learning rate.\n\n        Returns:\n            The learning rate modulated by the multiplier.\n        """"""\n        lr = self._learning_rate\n        if callable(lr):\n            lr = lr()\n        lr *= self._tls.multiplier\n        return lr\n\n\ndef add_lr_modulation_to_optimizer(optimizer):\n    """"""Adds learning rate modulation to the given optimizer.\n\n    Args:\n      optimizer: The optimizer to add learning rate modulation to.\n\n    Returns:\n      A `LearningRateModulator` instance.\n    """"""\n    # Get learning rate from optimizer\n    learning_rate = optimizer._hyper[""learning_rate""]\n\n    # Replace the learning rate in optimizer with a callable\n    lr_modulation = LearningRateModulator(learning_rate)\n    optimizer.learning_rate = lr_modulation.get_learning_rate\n\n    return lr_modulation\n'"
elasticdl/python/ps/main.py,0,"b'from elasticdl.python.common.args import parse_ps_args\nfrom elasticdl.python.ps.parameter_server import ParameterServer\n\n\ndef main():\n    args = parse_ps_args()\n    pserver = ParameterServer(args)\n    pserver.prepare()\n    pserver.run()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
elasticdl/python/ps/optimizer_wrapper.py,4,"b'""""""Optimizer Wrapper for ElasticDL""""""\n\nimport threading\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import (\n    SGD,\n    Adadelta,\n    Adagrad,\n    Adam,\n    Adamax,\n    Ftrl,\n    Nadam,\n    RMSprop,\n)\n\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.ps.embedding_table import get_slot_table_name\n\n\ndef _get_embedding_layer_name_from_var(var):\n    """"""Get name for ElasticDL embedding layer from variable.""""""\n    # Assumes that for ElasticDL embedding layer, variable will be a\n    # string representing its layer name\n    if isinstance(var, str):\n        return var\n    return None\n\n\n# This function is taken from `tensorflow.keras.optimizers.Optimizer._var_key`.\n# https://github.com/tensorflow/tensorflow/blob/71d73e56a2e66e4a6805d967cfa48ea\n# 594f8c54e/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1033\ndef _var_key(var):\n    """"""Key for representing a primary variable, for looking up slots.\n\n    In graph mode the name is derived from the var shared name.\n    In eager mode the name is derived from the var unique id.\n    If distribution strategy exists, get the primary variable first.\n\n    Arguments:\n        var: the variable.\n\n    Returns:\n        the unique name of the variable.\n    """"""\n\n    # pylint: disable=protected-access\n    # Get the distributed variable if it exists.\n    if getattr(var, ""_distributed_container"", None) is not None:\n        var = var._distributed_container()\n    if var._in_graph_mode:\n        return var._shared_name\n    return var._unique_id\n\n\nclass OptimizerWrapper(object):\n    """""" ElasticDL optimizer wrapper.\n\n    If model does not use ElasticDL embedding layer, `OptimizerWrapper`\n    does nothing but calls `apply_gradients` function of TensorFlow optimizer.\n    Otherwise, `OptimizerWrapper` looks up embedding vectors and slot values\n    from external kv store before updating variables, and updates embedding\n    vectors and slot values in kv store after updating variables.\n    """"""\n\n    def __init__(\n        self,\n        opt,\n        use_async=False,\n        lookup_embedding_func=None,\n        update_embedding_func=None,\n    ):\n        """"""\n        Arguments:\n            opt: A TensorFlow optimizer instance.\n            kv_store_endpoint: The endpoint to kv store.\n            embedding_dims: A python dictionary of\n                {layer name: `embedding_dim`} where layer name is the\n                name of ElasticDL embedding layer and `embedding_dim`\n                is the output dimension of corresponding embedding layer.\n            use_async: A python bool. True if using asynchronous updates. When\n                using asynchronous updates, `OptimizerWrapper` is thread-safe\n                for non-embedding variables and is not thread-safe for\n                embedding table.\n            lookup_embedding_func: The function to lookup embeddings. The\n                argument of this function is a list of keys.\n            update_embedding_func: The function to update embeddings. The\n                arguments of this function is a key list and a value list.\n        """"""\n        self._opt = opt\n        self._use_async = use_async\n        self._lookup_embedding_func = lookup_embedding_func\n        self._update_embedding_func = update_embedding_func\n        self._slot_initial_value = {}\n\n        self._update_gradient_lock = threading.Lock()\n        self._tls = threading.local()\n        self._init_thread_local()\n        self._has_embedding = False\n\n        # ""-"" in slot name is not supported\n        if isinstance(opt, SGD):\n            self._allowed_slot_names = []\n            if opt._momentum:\n                self._allowed_slot_names.append(""momentum"")\n\n        elif isinstance(opt, (Adam, Adamax, Nadam)):\n            self._allowed_slot_names = [""m"", ""v""]\n            if isinstance(opt, Adam) and self._opt.amsgrad:\n                self._allowed_slot_names.append(""vhat"")\n\n        elif isinstance(opt, Adadelta):\n            self._allowed_slot_names = [""accum_grad"", ""accum_var""]\n\n        elif isinstance(opt, (Adagrad, Ftrl)):\n            self._allowed_slot_names = [""accumulator""]\n            if isinstance(opt, Ftrl):\n                self._allowed_slot_names.append(""linear"")\n            accumu_init = opt._initial_accumulator_value\n            self._slot_initial_value[""accumulator""] = accumu_init\n\n        elif isinstance(opt, RMSprop):\n            self._allowed_slot_names = [""rms""]\n            if self._opt._momentum:\n                self._allowed_slot_names.append(""momentum"")\n            if self._opt.centered:\n                self._allowed_slot_names.append(""mg"")\n\n        else:\n            raise NotImplementedError(\n                ""Optimizer %s is not supported in ElasticDL."" % type(opt)\n            )\n\n        for slot in self._allowed_slot_names:\n            self._slot_initial_value.setdefault(slot, 0.0)\n\n    def _init_thread_local(self):\n        self._tls._unique_ids_all_layers = {}\n        self._tls._embed_variables = {}\n        self._tls._slot_variables = {}\n\n    def apply_gradients(self, grads_and_vars):\n        """"""Update variable values.\n\n        Args:\n            grads_and_vars: A list of (gradient, variable) pairs. If the\n                variable is from ElasticDL embedding layer, it should be a\n                ElasticDL `Tensor` object. Otherwise it is a TensorFlow\n                variable.\n        """"""\n        if not hasattr(self._tls, ""_embed_variables""):\n            self._init_thread_local()\n\n        if self._has_embedding:\n            with self._update_gradient_lock:\n                self._update_parameters_by_gradients(grads_and_vars)\n        else:\n            self._update_parameters_by_gradients(grads_and_vars)\n\n    def _update_parameters_by_gradients(self, grads_and_vars):\n        """"""Update parameters by gradients received by GRPC""""""\n        grads_and_vars_new = []\n        for grad, var in grads_and_vars:\n            # If var is a string, create the grad var pair for\n            # ElasticDL embedding\n            if isinstance(var, str):\n                grads_and_vars_new.append(\n                    self._get_embedding_var_and_grad(grad, var)\n                )\n                self._has_embedding = True\n            else:\n                grads_and_vars_new.append((grad, var))\n        self._opt.apply_gradients(grads_and_vars_new)\n        self._update_embedding_param()\n        self._delete_slots_and_weights_in_optimizer()\n\n    def _get_embedding_var_and_grad(self, grad, layer_name):\n        unique_ids, indices = tf.unique(grad.indices)\n        unique_ids = unique_ids.numpy()\n        if layer_name in self._tls._unique_ids_all_layers:\n            # TODO: support grads_and_vars with duplicated layer name\n            logger.warning(\n                ""grads_and_vars has duplicated layer name %s."" % layer_name\n            )\n        self._tls._unique_ids_all_layers[layer_name] = unique_ids\n        new_grad = tf.IndexedSlices(values=grad.values, indices=indices)\n\n        embed_value = self._lookup_embedding_func(layer_name, unique_ids)\n        embed_var = self._create_embedding_variable(layer_name, embed_value)\n        self._get_slot_and_set_to_optimizer(layer_name)\n        return new_grad, embed_var\n\n    def _create_embedding_variable(self, name, initial_value):\n        """"""Creates a TensorFlow variable using given initial value.\n\n        Note that this function saves the created variable to\n        `self._tls._embed_variables`.\n        """"""\n        if name not in self._tls._embed_variables:\n            embed_var = tf.Variable(\n                initial_value,\n                name=name + str(threading.get_ident()),\n                shape=(None, None),\n                dtype=tf.float32,\n                trainable=False,\n            )\n            self._tls._embed_variables[name] = embed_var\n        else:\n            embed_var = self._tls._embed_variables[name]\n            embed_var.assign(initial_value)\n        return embed_var\n\n    def _get_slot_and_set_to_optimizer(self, layer_name):\n        """"""Looks up slot value and set it to TensorFlow optimizer.""""""\n        for slot_name in self._allowed_slot_names:\n            param_name = get_slot_table_name(layer_name, slot_name)\n            indices = self._tls._unique_ids_all_layers[layer_name]\n            slot_value = self._lookup_embedding_func(param_name, indices)\n            # self._create_slot_variable creates a slot variable in tf\n            # optimizer and set slot_value to it.\n            self._create_slot_variable(layer_name, slot_name, slot_value)\n\n    def _get_slot_variable(self, layer_name, slot_name):\n        """"""Get the variable for specified slot.""""""\n        return self._tls._slot_variables.get(layer_name, {}).get(\n            slot_name, None\n        )\n\n    def _get_embedding_variable(self, layer_name):\n        """"""Get the variable for the specified ElasticDL embedding layer.""""""\n        return self._tls._embed_variables.get(layer_name, None)\n\n    def _create_slot_variable(self, layer_name, slot_name, initial_value):\n        """"""Creates a slot variable in TensorFlow optimizer using given\n        value.\n        """"""\n        embed_var = self._get_embedding_variable(layer_name)\n        if embed_var is None:\n            raise RuntimeError(\n                ""Embedding variable for layer %s should be already created.""\n                % (layer_name)\n            )\n        slot_var = self._init_slot_variable(\n            layer_name, embed_var, slot_name, initial_value\n        )\n        self._update_slot_variable_to_optimizer(slot_name, embed_var, slot_var)\n\n        return slot_var\n\n    def _init_slot_variable(\n        self, layer_name, embed_var, slot_name, initial_value\n    ):\n        """"""Initialize a variable for a slot""""""\n        if (\n            layer_name not in self._tls._slot_variables\n            or slot_name not in self._tls._slot_variables[layer_name]\n        ):\n            slot_var_name = ""%s/%s"" % (embed_var._shared_name, slot_name)\n            slot_var = self._opt.add_weight(\n                name=slot_var_name,\n                shape=(None, None),\n                dtype=embed_var.dtype,\n                initializer=initial_value,\n                trainable=False,\n            )\n            slot_variables_dict = self._tls._slot_variables.setdefault(\n                layer_name, {}\n            )\n            slot_variables_dict[slot_name] = slot_var\n        else:\n            slot_var = self._tls._slot_variables[layer_name][slot_name]\n            slot_var.assign(initial_value)\n        return slot_var\n\n    # This is a function modified from TensorFlow optimizers.\n    # https://github.com/tensorflow/tensorflow/blob/\n    # 69b1feac62276edcc509ac88af229c6236e645fe/tensorflow/python\n    # /keras/optimizer_v2/optimizer_v2.py#L567\n    def _update_slot_variable_to_optimizer(\n        self, slot_name, embed_var, slot_var\n    ):\n        if slot_name not in self._opt._slot_names:\n            self._opt._slot_names.append(slot_name)\n        var_key = _var_key(embed_var)\n        slot_dict = self._opt._slots.setdefault(var_key, {})\n        if slot_name not in slot_dict:\n            slot_dict[slot_name] = slot_var\n            self._opt._weights.append(slot_var)\n        else:\n            raise RuntimeError(\n                ""Variable with var_key %s and slot_name %s is not expected to ""\n                ""be in self._opt."" % (var_key, slot_name)\n            )\n\n    def _update_embedding_param(self):\n        """"""Report updated embedding vectors and slots to kv store.""""""\n        for layer, ids in self._tls._unique_ids_all_layers.items():\n            value = self._get_embedding_variable(layer).numpy()\n            self._update_embedding_func(layer, ids, value)\n\n            for slot in self._allowed_slot_names:\n                value = self._get_slot_variable(layer, slot).numpy()\n                slot_table_name = get_slot_table_name(layer, slot)\n                self._update_embedding_func(slot_table_name, ids, value)\n\n    def _delete_slots_and_weights_in_optimizer(self):\n        """"""Clear the slots and weights in the optimizer according\n        to Embedding layers.\n        """"""\n        for layer_name, slots in self._tls._slot_variables.items():\n            embed_var = self._get_embedding_variable(layer_name)\n            embed_var_key = _var_key(embed_var)\n            if embed_var_key in self._opt._slots:\n                del self._opt._slots[embed_var_key]\n            for _, var in slots.items():\n                opt_weight_iter = 0\n                while opt_weight_iter < len(self._opt._weights):\n                    if var is self._opt._weights[opt_weight_iter]:\n                        self._opt._weights.pop(opt_weight_iter)\n                        break\n                    else:\n                        opt_weight_iter += 1\n\n        # Delete variables in unique_ids_all_layers.\n        for key in list(self._tls._unique_ids_all_layers.keys()):\n            del self._tls._unique_ids_all_layers[key]\n\n    def set_learning_rate(self, learning_rate):\n        K.set_value(self._opt.lr, K.get_value(learning_rate))\n\n    @property\n    def allowed_slot_names(self):\n        return self._allowed_slot_names\n\n    @property\n    def slot_initial_value(self):\n        return self._slot_initial_value\n'"
elasticdl/python/ps/parameter_server.py,0,"b'import time\nfrom concurrent import futures\n\nimport grpc\nfrom kubernetes import client, config\n\nfrom elasticdl.proto import elasticdl_pb2_grpc\nfrom elasticdl.python.common.constants import GRPC, PodStatus\nfrom elasticdl.python.common.grpc_utils import build_channel\nfrom elasticdl.python.common.k8s_client import get_master_pod_name\nfrom elasticdl.python.common.log_utils import get_logger\nfrom elasticdl.python.common.model_utils import (\n    get_module_file_path,\n    load_module,\n)\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.ps.parameters import Parameters\nfrom elasticdl.python.ps.servicer import PserverServicer\n\n\nclass ParameterServer(object):\n    def __init__(self, args):\n        self.logger = get_logger(""PS"", level=args.log_level.upper())\n        self.grads_to_wait = args.grads_to_wait\n        self.lr_staleness_modulation = args.lr_staleness_modulation\n        self.sync_version_tolerance = args.sync_version_tolerance\n        self.use_async = args.use_async\n        self.port = args.port\n        model_module = load_module(\n            get_module_file_path(args.model_zoo, args.model_def)\n        ).__dict__\n        self.optimizer = model_module[args.optimizer]()\n        self.ps_id = args.ps_id\n        self.num_ps_pods = args.num_ps_pods\n        self.num_workers = args.num_workers\n        # Create Parameters instance\n        self.parameters = Parameters()\n        if args.master_addr is None:\n            raise ValueError(""master_addr is missing for parameter servers"")\n        self.master_channel = build_channel(args.master_addr)\n        self.evaluation_steps = args.evaluation_steps\n\n        self.master_name = get_master_pod_name(args.job_name)\n        self.namespace = args.namespace\n        self._init_checkpoint_saver(args)\n        self._restore_params_from_checkpoint(args.checkpoint_dir_for_init)\n        self._debug_info_needed = args.log_level.upper() == ""DEBUG""\n\n    def _restore_params_from_checkpoint(self, checkpoint_dir_for_init):\n        """"""Restore parameters from a checkpint directory for the PS instance\n        """"""\n        if not checkpoint_dir_for_init:\n            self.logger.info(""checkpoint directory for init is None"")\n            return\n\n        if not CheckpointSaver.check_checkpoint_valid(checkpoint_dir_for_init):\n            raise ValueError(""Invalid checkpoint directory"")\n\n        self.parameters = CheckpointSaver.restore_params_from_checkpoint(\n            checkpoint_dir_for_init, self.ps_id, self.num_ps_pods\n        )\n        self.parameters.initialized = True\n        self.logger.info(\n            ""The version of restored parameters is %d""\n            % self.parameters.version\n        )\n\n    def _init_checkpoint_saver(self, args):\n        if all([args.checkpoint_dir, args.checkpoint_steps]):\n            self.checkpoint_saver = CheckpointSaver(\n                args.checkpoint_dir,\n                args.checkpoint_steps,\n                args.keep_checkpoint_max,\n                include_evaluation=False,\n            )\n        else:\n            self.checkpoint_saver = None\n            self.logger.warning(\n                ""Invalid checkpoint config and no model will be saved""\n            )\n\n    def prepare(self):\n        max_workers = min(self.num_workers, 64)\n        self.logger.info(""The max threads in PS servers is %d"" % max_workers)\n        server = grpc.server(\n            futures.ThreadPoolExecutor(max_workers=max_workers),\n            options=[\n                (""grpc.max_send_message_length"", GRPC.MAX_SEND_MESSAGE_LENGTH),\n                (\n                    ""grpc.max_receive_message_length"",\n                    GRPC.MAX_RECEIVE_MESSAGE_LENGTH,\n                ),\n            ],\n        )\n        pserver_servicer = PserverServicer(\n            self.parameters,\n            self.grads_to_wait,\n            self.optimizer,\n            lr_staleness_modulation=self.lr_staleness_modulation,\n            sync_version_tolerance=self.sync_version_tolerance,\n            use_async=self.use_async,\n            evaluation_steps=self.evaluation_steps,\n            master_channel=self.master_channel,\n            checkpoint_saver=self.checkpoint_saver,\n            ps_id=self.ps_id,\n            num_ps_pods=self.num_ps_pods,\n        )\n        elasticdl_pb2_grpc.add_PserverServicer_to_server(\n            pserver_servicer, server\n        )\n        server.add_insecure_port(""[::]:{}"".format(self.port))\n        server.start()\n        self.server = server\n        self.logger.info(""RPC Server started at port: %d"", self.port)\n\n    def run(self):\n        config.load_incluster_config()\n        api = client.CoreV1Api()\n        try:\n            while True:\n                time.sleep(30)\n                master_pod = api.read_namespaced_pod(\n                    namespace=self.namespace, name=self.master_name\n                )\n                if master_pod.status.phase == PodStatus.SUCCEEDED:\n                    self.logger.info(""Master pod is Succeeded"")\n                    break\n                elif master_pod.status.phase == PodStatus.FAILED:\n                    self.logger.info(""Master pod is Failed"")\n                    break\n                elif (\n                    master_pod.status.phase == PodStatus.RUNNING\n                    and master_pod.metadata.labels[""status""]\n                    == PodStatus.FINISHED\n                ):\n                    self.logger.info(\n                        ""Task is finished, ""\n                        ""master pod is still running tensorboard service""\n                    )\n                    break\n\n                if self._debug_info_needed:\n                    self.logger.debug(\n                        ""Parameters info:\\n%s"" % self.parameters.debug_info()\n                    )\n        except KeyboardInterrupt:\n            self.logger.warning(""Server stopping"")\n\n        self.server.stop(0)\n        self.logger.info(""RPC server stopped"")\n'"
elasticdl/python/ps/parameters.py,7,"b'import tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.tensor_utils import (\n    pb_to_indexed_slices,\n    pb_to_ndarray,\n    serialize_indexed_slices,\n    serialize_ndarray,\n)\nfrom elasticdl.python.ps.embedding_table import (\n    EmbeddingTable,\n    create_embedding_table,\n    get_slot_table_name,\n)\n\n\nclass Parameters(object):\n    """"""\n    There are two kinds of parameters:\n\n    1. non-embedding parameters, or dense tensor parameters. We save it\n       in a hashmap `non-embedding_params`, the key is the parameter name,\n       the value is a tf.Variable` object.\n    2. embedding parameters, or row-sparse parameters. We save it in a\n       hashmap `embedding_params`, the key is the embedding layer name,\n       the value is an `EmbeddingTable` object.\n\n    """"""\n\n    def __init__(self):\n        self.version = 0\n        self.initialized = False\n        self.non_embedding_params = {}\n        self.embedding_params = {}\n\n    def reset(self):\n        self.version = 0\n        self.initialized = False\n        self.non_embedding_params.clear()\n        self.embedding_params.clear()\n\n    def get_non_embedding_param(self, name, default_value=None):\n        return self.non_embedding_params.get(name, default_value)\n\n    def get_embedding_param(self, name, indices):\n        if name not in self.embedding_params:\n            raise ValueError(\n                ""Please initialize embedding param %s first!"" % name\n            )\n        return self.embedding_params[name].get(indices)\n\n    def set_embedding_param(self, name, indices, values):\n        if name not in self.embedding_params:\n            raise ValueError(\n                ""Please initialize embedding param %s first!"" % name\n            )\n        self.embedding_params[name].set(indices, values)\n\n    def check_grad(self, grad):\n        name = grad.name\n        if name in self.non_embedding_params:\n            param_shape = tuple(\n                self.non_embedding_params[name].get_shape().as_list()\n            )\n            if grad.indices is not None:\n                dim0 = tf.math.reduce_max(grad.indices).numpy()\n                dim1 = grad.values.shape[1]\n                if dim0 > param_shape[0] or dim1 != param_shape[1]:\n                    raise ValueError(\n                        ""Keras embedding param error: ""\n                        ""the shape of gradient %s is (%d, %d), ""\n                        ""the shape of parameter %s is (%d, %d), ""\n                        ""which is incompatible""\n                        % (\n                            name,\n                            dim0,\n                            dim1,\n                            name,\n                            param_shape[0],\n                            param_shape[1],\n                        )\n                    )\n            else:\n                if grad.values.shape != param_shape:\n                    raise ValueError(\n                        ""Non embedding param error: ""\n                        ""the shape of gradient %s is %s, ""\n                        ""the shape of parameter %s is %s, ""\n                        ""which is incompatible""\n                        % (\n                            name,\n                            str(grad.values.shape),\n                            name,\n                            str(param_shape),\n                        )\n                    )\n        elif name in self.embedding_params:\n            if grad.values.shape[1] != self.embedding_params[name].dim:\n                raise ValueError(\n                    ""ElasticDL embedding param error: ""\n                    ""the shape of gradient %s is (None, %d), ""\n                    ""the shape of parameter %s is (None, %d), ""\n                    ""which is incompatible""\n                    % (\n                        name,\n                        grad.values.shape[1],\n                        name,\n                        self.embedding_params[name].dim,\n                    )\n                )\n        else:\n            raise ValueError(\n                ""Name error: Gradient %s is not in Parameters"" % name\n            )\n\n    def init_from_model_pb(self, model_pb):\n        """"""Initializes `Parameters` with model protocol buffer.\n\n        The `Parameters` accepts model pb and initialize only when it is\n        not initialized. Otherwise, it ignores the model pb.\n\n        Args:\n            model_pb: The model protocol buffer used for initialization.\n\n        Returns:\n            A bool indicates whether `Parameters` accepts this model pb or not.\n        """"""\n        if not self.initialized:\n            infos = model_pb.embedding_table_infos\n            self.init_embedding_params(infos)\n            for name, pb in model_pb.dense_parameters.items():\n                # Please note that `tf.Variable` will do something with magic.\n                # If you pass a name ""somename"" to a `tf.Variable`, the final\n                # variable name will be ""somename:0"". So the `tf.Variable.name`\n                # is meaningless, we must avoid use it in PS side.\n                arr = pb_to_ndarray(pb)\n                var = tf.Variable(initial_value=arr, trainable=True)\n                self.non_embedding_params[name] = var\n\n            for name, pb in model_pb.embedding_tables.items():\n                s = pb_to_indexed_slices(pb)\n                self.embedding_params[name].set(s.indices, s.values)\n            self.version = max(0, model_pb.version)\n            self.initialized = True\n            return True\n        return False\n\n    def init_embedding_params(self, embeddings_pb):\n        for pb in embeddings_pb:\n            if pb.name not in self.embedding_params:\n                self.embedding_params[pb.name] = create_embedding_table(pb)\n\n    def has_embedding_params(self):\n        return len(self.embedding_params) > 0\n\n    def create_slot_params(self, slot_names, init_values):\n        embed_layer_names = list(self.embedding_params.keys())\n        for layer_name in embed_layer_names:\n            for slot_name in slot_names:\n                key = get_slot_table_name(layer_name, slot_name)\n                if key in self.embedding_params:\n                    raise ValueError(\n                        ""An embedding layer has unexpected name %s"" % key\n                    )\n                self.embedding_params[key] = EmbeddingTable(\n                    key,\n                    self.embedding_params[layer_name].dim,\n                    init_values[slot_name],\n                    True,\n                )\n\n    def to_model_pb(self):\n        """""" Convert all parameters including embedding and non-embedding\n        parameters to `elasticdl_pb2.Model` which can be serialized.\n        """"""\n        model_pb = elasticdl_pb2.Model()\n        model_pb.version = self.version\n        for name, var in self.non_embedding_params.items():\n            serialize_ndarray(var.numpy(), model_pb.dense_parameters[name])\n\n        for name, embedding_table in self.embedding_params.items():\n            # Slot embedding table is not weights in the model, so we don\'t\n            # save it to checkpoint.\n            if not embedding_table.is_slot:\n                serialize_indexed_slices(\n                    embedding_table.to_indexed_slices(),\n                    model_pb.embedding_tables[name],\n                )\n                embedding_info = embedding_table.to_embedding_table_info_pb()\n                model_pb.embedding_table_infos.append(embedding_info)\n        return model_pb\n\n    def debug_info(self):\n        info = """"\n        total_size = 0\n        for param in self.embedding_params:\n            info += self.embedding_params[param].debug_info()\n            total_size += self.embedding_params[param].get_table_size()\n        for param in self.non_embedding_params:\n            shape = self.non_embedding_params[param].get_shape().as_list()\n            size = (\n                tf.size(self.non_embedding_params[param])\n                * self.non_embedding_params[param].dtype.size\n            )\n            info += (\n                ""Non-embedding param name: %s\\n  shape: %s\\n  size: %d\\n""\n                % (param, str(shape), size)\n            )\n            total_size += size\n        info += ""Total parameters size: %d bytes"" % total_size\n        return info\n'"
elasticdl/python/ps/servicer.py,3,"b'import threading\n\nimport tensorflow as tf\nfrom google.protobuf import empty_pb2\nfrom tensorflow.core.framework import tensor_pb2\nfrom tensorflow.keras import backend as K\n\nfrom elasticdl.proto import elasticdl_pb2, elasticdl_pb2_grpc\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.tensor_utils import (\n    Tensor,\n    merge_indexed_slices,\n    pb_to_indexed_slices,\n    pb_to_ndarray,\n    serialize_ndarray,\n)\nfrom elasticdl.python.ps.optimizer_wrapper import OptimizerWrapper\n\n\nclass PserverServicer(elasticdl_pb2_grpc.PserverServicer):\n    """"""PS service implementation""""""\n\n    def __init__(\n        self,\n        parameters,\n        grads_to_wait,\n        optimizer,\n        lr_staleness_modulation=False,\n        sync_version_tolerance=0,\n        use_async=False,\n        evaluation_steps=0,\n        master_channel=None,\n        checkpoint_saver=None,\n        ps_id=None,\n        num_ps_pods=None,\n    ):\n        if master_channel is None:\n            self._master_stub = None\n        else:\n            self._master_stub = elasticdl_pb2_grpc.MasterStub(master_channel)\n\n        self._parameters = parameters\n        self._grads_to_wait = grads_to_wait\n        self._optimizer = optimizer\n        self._lr_staleness_modulation = lr_staleness_modulation\n        self._sync_version_tolerance = sync_version_tolerance\n        self._use_async = use_async\n        self._eval_steps = evaluation_steps\n        self._checkpoint_saver = checkpoint_saver\n        self._ps_id = ps_id\n        self._num_ps_pods = num_ps_pods\n        self._version_lock = threading.Lock()\n        self._lock = threading.Lock()\n        self._use_wrap_opt = False\n\n        self._grads_n = 0\n        self._grads_buffer = {}\n\n    def pull_dense_parameters(self, request, _):\n        """"""\n        Response with all non-embedding parameters if initialized.\n        """"""\n        res = elasticdl_pb2.PullDenseParametersResponse()\n        if not self._parameters.initialized:\n            res.initialized = False\n            return res\n\n        # Only sync-SGD needs lock\n        # TODO: use a read-write lock to support multiple concurrent reads\n        if not self._use_async:\n            self._lock.acquire()\n        res.version = self._parameters.version\n        # No need to send variables if the requester has the latest version.\n        if self._parameters.version > request.version:\n            for name, var in self._parameters.non_embedding_params.items():\n                serialize_ndarray(var.numpy(), res.dense_parameters[name])\n        if not self._use_async:\n            self._lock.release()\n        res.initialized = True\n        return res\n\n    def pull_embedding_vectors(self, request, _):\n        result = tensor_pb2.TensorProto()\n        if not request.ids:\n            return result\n        embedding_vectors = self._parameters.get_embedding_param(\n            request.name, request.ids\n        )\n        serialize_ndarray(embedding_vectors, result)\n        return result\n\n    def push_model(self, request, _):\n        with self._lock:\n            accepted = self._parameters.init_from_model_pb(request)\n        if accepted and self._parameters.has_embedding_params():\n            self.wrap_optimizer_and_set_slot()\n        return empty_pb2.Empty()\n\n    def push_embedding_table_infos(self, request, _):\n        with self._lock:\n            self._parameters.init_embedding_params(\n                request.embedding_table_infos\n            )\n            self.wrap_optimizer_and_set_slot()\n        return empty_pb2.Empty()\n\n    def push_gradients(self, request, _):\n        res = elasticdl_pb2.PushGradientsResponse()\n        if self._use_async:\n            grad_vars = []\n\n            for name, pb in request.gradients.dense_parameters.items():\n                grad = pb_to_ndarray(pb)\n                self._parameters.check_grad(Tensor(name, grad, None))\n                grad = tf.constant(grad)\n                var = self._parameters.get_non_embedding_param(name)\n                grad_vars.append((grad, var))\n\n            for name, pb in request.gradients.embedding_tables.items():\n                grad = pb_to_indexed_slices(pb)\n                self._parameters.check_grad(\n                    Tensor(name, grad.values, grad.indices)\n                )\n                if name in self._parameters.non_embedding_params:\n                    var = self._parameters.get_non_embedding_param(name)\n                    grad_vars.append((grad, var))\n                else:\n                    grad_vars.append((grad, name))\n\n            learning_rate = request.learning_rate\n            # TODO: if request.learning_rate == 0.0, modulate learning_rate\n            #       in self._optimizer with staleness\n            if self._lr_staleness_modulation and learning_rate > 0.0:\n                staleness = max(\n                    1, self._parameters.version - request.gradients.version\n                )\n                # Modulate learning rate by staleness\n                learning_rate /= staleness\n\n            self._set_optimizer_learning_rate(learning_rate)\n            self._optimizer.apply_gradients(grad_vars)\n            with self._version_lock:\n                self._parameters.version += 1\n                self._save_params_to_checkpoint_if_needed()\n                version = self._parameters.version\n            self._report_version_if_needed(version)\n\n            res.accepted = True\n            res.version = self._parameters.version\n            return res\n        else:\n            if (\n                request.gradients.version\n                < self._parameters.version - self._sync_version_tolerance\n            ):\n                res.accepted = False\n                res.version = self._parameters.version\n                return res\n\n            with self._lock:\n                for name, pb in request.gradients.dense_parameters.items():\n                    grad = pb_to_ndarray(pb)\n                    self._parameters.check_grad(Tensor(name, grad, None))\n                    if name in self._grads_buffer:\n                        self._grads_buffer[name] = (\n                            self._grads_buffer[name] + grad\n                        )\n                    else:\n                        self._grads_buffer[name] = grad\n\n                for name, pb in request.gradients.embedding_tables.items():\n                    grad = pb_to_indexed_slices(pb)\n                    self._parameters.check_grad(\n                        Tensor(name, grad.values, grad.indices)\n                    )\n                    if name in self._grads_buffer:\n                        self._grads_buffer[name] = merge_indexed_slices(\n                            self._grads_buffer[name], grad\n                        )\n                    else:\n                        self._grads_buffer[name] = grad\n\n                self._grads_n += 1\n                res.accepted = True\n\n                updated_version = False\n                version = self._parameters.version\n                if self._grads_n == self._grads_to_wait:\n                    grad_vars = []\n                    for name, grad in self._grads_buffer.items():\n                        # Dense gradients are averaged,\n                        # while sparse gradients are summed\n                        if not isinstance(grad, tf.IndexedSlices):\n                            grad = grad / self._grads_to_wait\n                            grad = tf.constant(grad)\n                        var = self._parameters.get_non_embedding_param(name)\n                        if var is None:\n                            grad_vars.append((grad, name))\n                        else:\n                            grad_vars.append((grad, var))\n\n                    self._set_optimizer_learning_rate(request.learning_rate)\n                    self._optimizer.apply_gradients(grad_vars)\n                    self._grads_n = 0\n                    self._grads_buffer.clear()\n                    self._parameters.version += 1\n                    self._save_params_to_checkpoint_if_needed()\n                    version = self._parameters.version\n                    updated_version = True\n\n            if updated_version:\n                self._report_version_if_needed(version)\n            res.version = version\n            return res\n\n    def wrap_optimizer(self):\n        self._optimizer = OptimizerWrapper(\n            self._optimizer,\n            self._use_async,\n            self._parameters.get_embedding_param,\n            self._parameters.set_embedding_param,\n        )\n\n    def _report_version_if_needed(self, version):\n        if self._eval_steps and version % self._eval_steps == 0:\n            self._report_version(version)\n\n    def _report_version(self, version):\n        req = elasticdl_pb2.ReportVersionRequest()\n        req.model_version = version\n        self._master_stub.report_version(req)\n\n    def wrap_optimizer_and_set_slot(self):\n        if not self._use_wrap_opt:\n            self.wrap_optimizer()\n            self._parameters.create_slot_params(\n                self._optimizer.allowed_slot_names,\n                self._optimizer.slot_initial_value,\n            )\n            self._use_wrap_opt = True\n\n    def _save_params_to_checkpoint_if_needed(self):\n        """"""Save a checkpoint of parameters to a protobuf file""""""\n        if (\n            self._checkpoint_saver\n            and self._parameters.version % self._checkpoint_saver._steps == 0\n        ):\n            model_pb = self._parameters.to_model_pb()\n\n            logger.info(""Save checkpoint for version %s"" % model_pb.version)\n            self._checkpoint_saver.save(\n                model_pb.version,\n                model_pb,\n                is_eval_checkpoint=False,\n                shard_index=self._ps_id,\n                shard_num=self._num_ps_pods,\n            )\n\n    def _set_optimizer_learning_rate(self, learning_rate):\n        if learning_rate == 0.0:\n            return\n\n        if self._use_wrap_opt:\n            self._optimizer.set_learning_rate(learning_rate)\n        else:\n            K.set_value(self._optimizer.lr, K.get_value(learning_rate))\n'"
elasticdl/python/tests/__init__.py,0,b''
elasticdl/python/tests/args_test.py,0,"b'import argparse\nimport unittest\n\nfrom elasticdl.python.common.args import (\n    add_bool_param,\n    build_arguments_from_parsed_result,\n    parse_ps_args,\n    wrap_go_args_with_string,\n    wrap_python_args_with_string,\n)\n\n\nclass ArgsTest(unittest.TestCase):\n    def setUp(self):\n        self._parser = argparse.ArgumentParser()\n        self._parser.add_argument(""--foo"", default=3, type=int)\n        add_bool_param(self._parser, ""--bar"", False, """")\n\n    def test_build_arguments_from_parsed_result(self):\n        args = [""--foo"", ""4"", ""--bar""]\n        results = self._parser.parse_args(args=args)\n        original_arguments = build_arguments_from_parsed_result(results)\n        value = ""\\t"".join(sorted(original_arguments))\n        target = ""\\t"".join(sorted([""--foo"", ""4"", ""--bar"", ""True""]))\n        self.assertEqual(value, target)\n\n        original_arguments = build_arguments_from_parsed_result(\n            results, filter_args=[""foo""]\n        )\n        value = ""\\t"".join(sorted(original_arguments))\n        target = ""\\t"".join(sorted([""--bar"", ""True""]))\n        self.assertEqual(value, target)\n\n        args = [""--foo"", ""4""]\n        results = self._parser.parse_args(args=args)\n        original_arguments = build_arguments_from_parsed_result(results)\n        value = ""\\t"".join(sorted(original_arguments))\n        target = ""\\t"".join(sorted([""--bar"", ""False"", ""--foo"", ""4""]))\n        self.assertEqual(value, target)\n\n    def test_parse_ps_args(self):\n        minibatch_size = 16\n        num_minibatches_per_task = 8\n        model_zoo = ""dummy_zoo""\n        model_def = ""dummy_def""\n\n        original_args = [\n            ""--ps_id"",\n            str(0),\n            ""--port"",\n            str(2222),\n            ""--minibatch_size"",\n            str(minibatch_size),\n            ""--model_zoo"",\n            model_zoo,\n            ""--model_def"",\n            model_def,\n            ""--job_name"",\n            ""test _args"",\n            ""--num_minibatches_per_task"",\n            str(num_minibatches_per_task),\n        ]\n        parsed_args = parse_ps_args(original_args)\n        self.assertEqual(parsed_args.ps_id, 0)\n        self.assertEqual(parsed_args.port, 2222)\n        self.assertEqual(parsed_args.minibatch_size, minibatch_size)\n        self.assertEqual(\n            parsed_args.num_minibatches_per_task, num_minibatches_per_task\n        )\n        self.assertEqual(parsed_args.model_zoo, model_zoo)\n        self.assertEqual(parsed_args.model_def, model_def)\n\n    def test_wrap_python_args_with_string(self):\n        args = [\n            ""--ps_id"",\n            str(0),\n            ""--job_name"",\n            ""test_args"",\n            ""--checkpoint_dir"",\n            """",\n        ]\n        args = wrap_python_args_with_string(args)\n        expected_args = [\n            ""--ps_id"",\n            ""\'0\'"",\n            ""--job_name"",\n            ""\'test_args\'"",\n            ""--checkpoint_dir"",\n            ""\'\'"",\n        ]\n        self.assertListEqual(args, expected_args)\n\n    def test_wrap_go_args_with_string(self):\n        args = [\n            ""-ps_id=0"",\n            ""-job_name=test_args"",\n            ""-opt_args=learning_rate=0.1;momentum=0.0;nesterov=False"",\n        ]\n        args = wrap_go_args_with_string(args)\n        expected_args = [\n            ""-ps_id=\'0\'"",\n            ""-job_name=\'test_args\'"",\n            ""-opt_args=\'learning_rate=0.1;momentum=0.0;nesterov=False\'"",\n        ]\n        self.assertListEqual(args, expected_args)\n'"
elasticdl/python/tests/callbacks_test.py,11,"b'import os\nimport tempfile\nimport unittest\nfrom unittest import mock\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.constants import DistributionStrategy, JobType\nfrom elasticdl.python.common.model_handler import ModelHandler\nfrom elasticdl.python.elasticdl.callbacks import (\n    LearningRateScheduler,\n    MaxStepsStopping,\n    SavedModelExporter,\n)\nfrom elasticdl.python.master.task_dispatcher import _Task\nfrom elasticdl.python.tests.test_utils import save_checkpoint_without_embedding\nfrom elasticdl.python.worker.task_data_service import TaskDataService\n\n\ndef custom_model_with_embedding_layer():\n    inputs = tf.keras.layers.Input(shape=(4,), name=""x"")\n    embedding = tf.keras.layers.Embedding(300000, 2)(inputs)\n    outputs = tf.keras.layers.Dense(1)(embedding)\n    return tf.keras.models.Model(inputs, outputs)\n\n\ndef dataset_fn(dataset, mode, metadata):\n    return dataset\n\n\nclass MockWorker:\n    def __init__(self):\n        self._custom_data_reader = None\n\n\nclass SavedModelExporterTest(unittest.TestCase):\n    def setUp(self):\n        tf.keras.backend.clear_session()\n\n    def test_on_train_end(self):\n        worker = MockWorker()\n        task_data_service = TaskDataService(\n            worker, JobType.TRAINING_WITH_EVALUATION\n        )\n        dataset = tf.data.Dataset.from_tensor_slices(\n            np.array([[1.0, 2.0, 3.0, 4.0], [1.0, 2.0, 3.0, 4.0]])\n        )\n        task_data_service._pending_train_end_callback_task = (\n            """",\n            0,\n            1,\n            elasticdl_pb2.TRAIN_END_CALLBACK,\n        )\n        task_data_service.get_dataset_by_task = mock.Mock(return_value=dataset)\n\n        with tempfile.TemporaryDirectory() as temp_dir_name:\n            checkpoint_dir = os.path.join(temp_dir_name, ""checkpoint"")\n            model = custom_model_with_embedding_layer()\n            save_checkpoint_without_embedding(model, checkpoint_dir)\n            model_handler = ModelHandler.get_model_handler(\n                distribution_strategy=DistributionStrategy.PARAMETER_SERVER,\n                checkpoint_dir=checkpoint_dir,\n            )\n            saved_model_exporter = SavedModelExporter(\n                task_data_service, dataset_fn, model_handler\n            )\n            saved_model_path = os.path.join(temp_dir_name, ""test_exporter"")\n            params = {""batch_size"": 10, ""saved_model_path"": saved_model_path}\n            saved_model_exporter.set_params(params)\n            saved_model_exporter.set_model(model)\n            saved_model_exporter.on_train_end()\n            self.assertTrue(os.path.exists(saved_model_path))\n            self.assertTrue(\n                os.path.exists(\n                    os.path.join(saved_model_path, ""saved_model.pb"")\n                )\n            )\n\n\nclass MaxStepsStoppingTest(unittest.TestCase):\n    def test_on_task_end(self):\n        max_steps_stopping = MaxStepsStopping(max_steps=20)\n        max_steps_stopping.set_model(tf.keras.Model())\n        max_steps_stopping.model.stop_training = False\n        max_steps_stopping.set_params({""batch_size"": 128})\n        for i in range(6):\n            start = i * 512\n            end = (i + 1) * 512\n            task = _Task(\n                shard_name=""test"",\n                start=start,\n                end=end,\n                type=elasticdl_pb2.TRAINING,\n            )\n            max_steps_stopping.on_task_end(task)\n        self.assertTrue(max_steps_stopping.model.stop_training)\n\n\nclass LearningRateSchedulerTest(unittest.TestCase):\n    def _schedule(self, model_version):\n        return 0.2 if model_version < 2 else 0.1\n\n    def test_learning_rate_scheduler(self):\n        learning_rate_scheduler = LearningRateScheduler(self._schedule)\n        model = tf.keras.Model()\n        model.optimizer = tf.optimizers.SGD(0.1)\n        learning_rate_scheduler.set_model(model)\n\n        learning_rate_scheduler.on_train_batch_begin(batch=1)\n        self.assertEqual(model.optimizer.lr.numpy(), np.float32(0.2))\n        learning_rate_scheduler.on_train_batch_begin(batch=2)\n        self.assertEqual(model.optimizer.lr.numpy(), np.float32(0.1))\n\n        model_versions = [0, 1, 2]\n        variables = []\n        grads = []\n        original_values = [1.2, 0.8]\n        grad_values = [0.2, 0.1]\n\n        for i in range(len(model_versions)):\n            variables.append([tf.Variable(v) for v in original_values])\n            grads.append([tf.convert_to_tensor(g) for g in grad_values])\n\n        results = []\n        for i in range(len(model_versions)):\n            result = self.apply_gradients_with_scheduler(\n                learning_rate_scheduler,\n                model.optimizer,\n                model_versions[i],\n                variables[i],\n                grads[i],\n            )\n            results.append(result)\n\n        place = 5\n        for i in range(0, len(model_versions)):\n            i_diff = [\n                original_values[j] - results[i][j]\n                for j in range(len(original_values))\n            ]\n            for j in range(len(original_values)):\n                # variable value change ratio equals the learning rate ratio\n                # for SGD without momentum\n                self.assertAlmostEqual(\n                    i_diff[j],\n                    grad_values[j] * self._schedule(model_versions[i]),\n                    place,\n                )\n\n    @staticmethod\n    def apply_gradients_with_scheduler(\n        lr_scheduler, opt, model_version, variables, grads\n    ):\n        lr_scheduler.on_train_batch_begin(model_version)\n        grads_and_vars = zip(grads, variables)\n        opt.apply_gradients(grads_and_vars)\n        return [v.numpy() for v in variables]\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/collective_communicator_test.py,0,"b'import unittest\n\nfrom elasticdl.python.collective_ops.communicator import CollectiveCommunicator\nfrom elasticdl.python.common.constants import CollectiveCommunicatorStatus\n\n\nclass CollectiveCommunicatorTest(unittest.TestCase):\n    def test_collective_communicator(self):\n        communicator = CollectiveCommunicator()\n        data = [1]\n        self.assertEqual(\n            communicator.allreduce(data),\n            (CollectiveCommunicatorStatus.SUCCEEDED, data),\n        )\n        self.assertEqual(\n            communicator.allreduce(None),\n            (CollectiveCommunicatorStatus.FAILED, None),\n        )\n        data = {""param1"": 1}\n        self.assertEqual(\n            communicator.broadcast(data, 0),\n            (CollectiveCommunicatorStatus.SUCCEEDED, data),\n        )\n        self.assertEqual(\n            communicator.broadcast(None, 0),\n            (CollectiveCommunicatorStatus.SUCCEEDED, None),\n        )\n        self.assertEqual(\n            communicator.barrier(), CollectiveCommunicatorStatus.SUCCEEDED\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/data_reader_test.py,10,"b'import os\nimport random\nimport tempfile\nimport time\nimport unittest\nfrom collections import namedtuple\n\nimport numpy as np\nimport odps\nimport tensorflow as tf\nfrom odps import ODPS\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.constants import MaxComputeConfig\nfrom elasticdl.python.common.model_utils import load_module\nfrom elasticdl.python.data.odps_io import is_odps_configured\nfrom elasticdl.python.data.reader.csv_reader import CSVDataReader\nfrom elasticdl.python.data.reader.data_reader import Metadata\nfrom elasticdl.python.data.reader.data_reader_factory import create_data_reader\nfrom elasticdl.python.data.reader.odps_reader import ODPSDataReader\nfrom elasticdl.python.data.reader.recordio_reader import RecordIODataReader\nfrom elasticdl.python.tests.test_utils import (\n    IRIS_TABLE_COLUMN_NAMES,\n    DatasetName,\n    create_iris_csv_file,\n    create_iris_odps_table,\n    create_recordio_file,\n)\n\n_MockedTask = namedtuple(""Task"", [""start"", ""end"", ""shard_name"", ""type""])\n\n\nclass RecordIODataReaderTest(unittest.TestCase):\n    def test_recordio_data_reader(self):\n        num_records = 128\n        with tempfile.TemporaryDirectory() as temp_dir_name:\n            shard_name = create_recordio_file(\n                num_records, DatasetName.TEST_MODULE, 1, temp_dir=temp_dir_name\n            )\n\n            # Test shards creation\n            expected_shards = {shard_name: (0, num_records)}\n            reader = RecordIODataReader(data_dir=temp_dir_name)\n            self.assertEqual(expected_shards, reader.create_shards())\n\n            # Test records reading\n            records = list(\n                reader.read_records(\n                    _MockedTask(\n                        0, num_records, shard_name, elasticdl_pb2.TRAINING\n                    )\n                )\n            )\n            self.assertEqual(len(records), num_records)\n            for record in records:\n                parsed_record = tf.io.parse_single_example(\n                    record,\n                    {\n                        ""x"": tf.io.FixedLenFeature([1], tf.float32),\n                        ""y"": tf.io.FixedLenFeature([1], tf.float32),\n                    },\n                )\n                for k, v in parsed_record.items():\n                    self.assertEqual(len(v.numpy()), 1)\n\n\nclass CSVDataReaderTest(unittest.TestCase):\n    def test_csv_data_reader(self):\n        with tempfile.TemporaryDirectory() as temp_dir_name:\n            num_records = 128\n            columns = [\n                ""sepal_length"",\n                ""sepal_width"",\n                ""petal_length"",\n                ""petal_width"",\n                ""class"",\n            ]\n            iris_file_name = create_iris_csv_file(\n                size=num_records, columns=columns, temp_dir=temp_dir_name\n            )\n            csv_data_reader = CSVDataReader(columns=columns, sep="","")\n            task = _MockedTask(\n                0, num_records, iris_file_name, elasticdl_pb2.TRAINING\n            )\n\n            def _gen():\n                for record in csv_data_reader.read_records(task):\n                    yield record\n\n            def _dataset_fn(dataset, mode, metadata):\n                def _parse_data(record):\n                    features = tf.strings.to_number(record[0:-1], tf.float32)\n                    label = tf.strings.to_number(record[-1], tf.float32)\n                    return features, label\n\n                dataset = dataset.map(_parse_data)\n                dataset = dataset.batch(10)\n                return dataset\n\n            dataset = tf.data.Dataset.from_generator(\n                _gen, csv_data_reader.records_output_types\n            )\n            dataset = _dataset_fn(dataset, None, None)\n            for features, labels in dataset:\n                self.assertEqual(features.shape.as_list(), [10, 4])\n                self.assertEqual(labels.shape.as_list(), [10])\n                break\n\n\n@unittest.skipIf(\n    not is_odps_configured(), ""ODPS environment is not configured"",\n)\nclass ODPSDataReaderTest(unittest.TestCase):\n    def setUp(self):\n        self.project = os.environ[MaxComputeConfig.PROJECT_NAME]\n        access_id = os.environ[MaxComputeConfig.ACCESS_ID]\n        access_key = os.environ[MaxComputeConfig.ACCESS_KEY]\n        endpoint = os.environ.get(MaxComputeConfig.ENDPOINT)\n        tunnel_endpoint = os.environ.get(\n            MaxComputeConfig.TUNNEL_ENDPOINT, None\n        )\n        self.test_table = ""test_odps_data_reader_%d_%d"" % (\n            int(time.time()),\n            random.randint(1, 101),\n        )\n        self.odps_client = ODPS(access_id, access_key, self.project, endpoint)\n        create_iris_odps_table(self.odps_client, self.project, self.test_table)\n        self.records_per_task = 50\n\n        self.reader = ODPSDataReader(\n            project=self.project,\n            access_id=access_id,\n            access_key=access_key,\n            endpoint=endpoint,\n            table=self.test_table,\n            tunnel_endpoint=tunnel_endpoint,\n            num_processes=1,\n            records_per_task=self.records_per_task,\n        )\n\n    def test_odps_data_reader_shards_creation(self):\n        expected_shards = {\n            self.test_table + "":shard_0"": (0, self.records_per_task),\n            self.test_table + "":shard_1"": (50, self.records_per_task),\n            self.test_table + "":shard_2"": (100, 10),\n        }\n        self.assertEqual(expected_shards, self.reader.create_shards())\n\n    def test_odps_data_reader_records_reading(self):\n        records = list(\n            self.reader.read_records(\n                _MockedTask(\n                    0, 2, self.test_table + "":shard_0"", elasticdl_pb2.TRAINING\n                )\n            )\n        )\n        records = np.array(records, dtype=""float"").tolist()\n        self.assertEqual(\n            [[6.4, 2.8, 5.6, 2.2, 2], [5.0, 2.3, 3.3, 1.0, 1]], records\n        )\n        self.assertEqual(\n            self.reader.metadata.column_names, IRIS_TABLE_COLUMN_NAMES\n        )\n        self.assertListEqual(\n            list(self.reader.metadata.column_dtypes.values()),\n            [\n                odps.types.double,\n                odps.types.double,\n                odps.types.double,\n                odps.types.double,\n                odps.types.bigint,\n            ],\n        )\n        self.assertEqual(\n            self.reader.metadata.get_tf_dtype_from_maxcompute_column(\n                self.reader.metadata.column_names[0]\n            ),\n            tf.float64,\n        )\n        self.assertEqual(\n            self.reader.metadata.get_tf_dtype_from_maxcompute_column(\n                self.reader.metadata.column_names[-1]\n            ),\n            tf.int64,\n        )\n\n    def test_create_data_reader(self):\n        reader = create_data_reader(\n            data_origin=self.test_table,\n            records_per_task=10,\n            **{\n                ""columns"": [""sepal_length"", ""sepal_width""],\n                ""label_col"": ""class"",\n            }\n        )\n        self.assertEqual(\n            reader._kwargs[""columns""], [""sepal_length"", ""sepal_width""]\n        )\n        self.assertEqual(reader._kwargs[""label_col""], ""class"")\n        self.assertEqual(reader._kwargs[""records_per_task""], 10)\n        reader = create_data_reader(\n            data_origin=self.test_table, records_per_task=10\n        )\n        self.assertEqual(reader._kwargs[""records_per_task""], 10)\n        self.assertTrue(""columns"" not in reader._kwargs)\n\n    def test_odps_data_reader_integration_with_local_keras(self):\n        num_records = 2\n        model_spec = load_module(\n            os.path.join(\n                os.path.dirname(os.path.realpath(__file__)),\n                ""../../../model_zoo"",\n                ""odps_iris_dnn_model/odps_iris_dnn_model.py"",\n            )\n        ).__dict__\n        model = model_spec[""custom_model""]()\n        optimizer = model_spec[""optimizer""]()\n        loss = model_spec[""loss""]\n        reader = create_data_reader(\n            data_origin=self.test_table,\n            records_per_task=10,\n            **{""columns"": IRIS_TABLE_COLUMN_NAMES, ""label_col"": ""class""}\n        )\n        dataset_fn = reader.default_dataset_fn()\n\n        def _gen():\n            for data in self.reader.read_records(\n                _MockedTask(\n                    0,\n                    num_records,\n                    self.test_table + "":shard_0"",\n                    elasticdl_pb2.TRAINING,\n                )\n            ):\n                if data is not None:\n                    yield data\n\n        dataset = tf.data.Dataset.from_generator(_gen, tf.string)\n        dataset = dataset_fn(\n            dataset, None, Metadata(column_names=IRIS_TABLE_COLUMN_NAMES)\n        )\n        dataset = dataset.batch(1)\n\n        loss_history = []\n        grads = None\n        for features, labels in dataset:\n            with tf.GradientTape() as tape:\n                logits = model(features, training=True)\n                loss_value = loss(labels, logits)\n            loss_history.append(loss_value.numpy())\n            grads = tape.gradient(loss_value, model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        self.assertEqual(len(loss_history), num_records)\n        self.assertEqual(len(grads), num_records)\n        self.assertEqual(len(model.trainable_variables), num_records)\n\n    def tearDown(self):\n        self.odps_client.delete_table(\n            self.test_table, self.project, if_exists=True\n        )\n'"
elasticdl/python/tests/embedding_table_test.py,2,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.proto.elasticdl_pb2 import EmbeddingTableInfo\nfrom elasticdl.python.ps.embedding_table import (\n    EmbeddingTable,\n    create_embedding_table,\n    get_slot_table_name,\n)\n\n\nclass EmbeddingTableTest(unittest.TestCase):\n    def setUp(self):\n        self.name = ""embedding_1""\n        self.dim = 10\n        self.initializer = ""uniform""\n        self.table = EmbeddingTable(self.name, self.dim, self.initializer)\n\n    def test_embedding_table_init(self):\n        self.assertIsNotNone(self.table)\n        self.assertEqual(self.table.name, self.name)\n        self.assertEqual(self.table.dim, self.dim)\n        self.assertEqual(\n            tf.keras.initializers.get(self.initializer).__class__,\n            self.table.initializer.__class__,\n        )\n\n    def test_embedding_table_get(self):\n        self.table.clear()\n        indices = [0, 3, 7]\n        res = self.table.get(indices)\n        self.assertTupleEqual(res.shape, (3, 10))\n\n        res = self.table.get([])\n        self.assertIsNone(res)\n\n        self.table.get([0, 3, 8])\n        self.assertEqual(len(self.table.embedding_vectors), 4)\n\n    def test_embedding_table_set(self):\n        self.table.clear()\n        indices = [0, 1, 4]\n        x = len(indices)\n        values = np.random.uniform(size=x * self.dim).reshape((x, self.dim))\n        self.table.set(indices, values)\n\n        row0 = self.table.get([0])\n        row1 = self.table.get([1])\n        row4 = self.table.get([4])\n\n        rows = [row0, row1, row4]\n        rows = np.concatenate(rows)\n        np.testing.assert_array_equal(rows, values)\n\n    def test_create_embedding_table(self):\n        embedding_pb = EmbeddingTableInfo()\n        embedding_pb.name = self.name\n        embedding_pb.dim = self.dim\n        embedding_pb.initializer = self.initializer\n        table = create_embedding_table(embedding_pb)\n        self.assertIsNotNone(table)\n        self.assertEqual(table.name, self.name)\n        self.assertEqual(\n            tf.keras.initializers.get(self.initializer).__class__,\n            table.initializer.__class__,\n        )\n        self.assertEqual(table.dim, self.dim)\n\n    def test_create_embedding_table_for_slots(self):\n        slot_name = ""momentum""\n        init_value = 3.5\n        table = EmbeddingTable(\n            get_slot_table_name(self.name, slot_name),\n            dim=self.dim,\n            initializer=init_value,\n            is_slot=True,\n        )\n        self.assertIsNotNone(table)\n        self.assertEqual(table.name, get_slot_table_name(self.name, slot_name))\n        self.assertEqual(table.dim, self.dim)\n        # test initialize\n        embedding = table.get([2])\n        self.assertTrue((embedding - init_value < 0.0001).all())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/embedding_test_module.py,12,"b'import tensorflow as tf\nfrom tensorflow.keras.layers import Concatenate, Dense, Flatten\n\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\n\n\nclass EdlEmbeddingModel(tf.keras.Model):\n    def __init__(self, output_dim=16, weights=None):\n        """"""\n        Arguments:\n            output_dim: An Integer. It is the output dimension of embedding\n                layers in `EdlEmbeddingModel`.\n            weights: A numpy ndarray list. If `weights` is not None, dense\n                layer initializes its weights using `weights`.\n        """"""\n        super(EdlEmbeddingModel, self).__init__(name=""EdlEmbeddingModel"")\n        self.output_dim = output_dim\n        if weights:\n            if len(weights) != 2:\n                raise ValueError(\n                    ""EdlEmbeddingModel constructor receives weights with ""\n                    ""length %d, expected %d"" % (len(weights), 2)\n                )\n\n        self.embedding_1 = Embedding(output_dim)\n        self.embedding_2 = Embedding(output_dim)\n        self.concat = Concatenate()\n        self.dense = Dense(1, weights=weights)\n        self.flatten = Flatten()\n\n    def call(self, inputs, training=False):\n        x = self.concat(\n            [\n                self.embedding_1(inputs[""f1""]),\n                self.embedding_1(inputs[""f2""]),\n                self.embedding_2(inputs[""f3""]),\n            ]\n        )\n        x = self.flatten(x)\n        x = self.dense(x)\n        return x\n\n\n# The model structure of KerasEmbeddingModel should keep same with\n# EdlEmbeddingModel.\nclass KerasEmbeddingModel(tf.keras.Model):\n    def __init__(self, input_dim, output_dim=16, weights=None):\n        """"""\n        Arguments:\n            input_dim: An Integer. It is the input dimension of embedding\n                layers in `KerasEmbeddingModel`.\n            output_dim: An Integer. It is the output dimension of embedding\n                layers in `KerasEmbeddingModel`.\n            weights: A numpy ndarray list. Unless `weights` is None, embedding\n                layer and dense layer initialize their weights using `weights`.\n        """"""\n        super(KerasEmbeddingModel, self).__init__(name=""KerasEmbeddingModel"")\n        self.output_dim = output_dim\n        if weights:\n            weight_1 = [weights[0]]\n            weight_2 = [weights[1]]\n            linear_weights = weights[2:]\n        else:\n            weight_1, weight_2, linear_weights = None, None, None\n        self.embedding_1 = tf.keras.layers.Embedding(\n            input_dim, output_dim, weights=weight_1\n        )\n        self.embedding_2 = tf.keras.layers.Embedding(\n            input_dim, output_dim, weights=weight_2\n        )\n        self.concat = Concatenate()\n        self.dense = Dense(1, weights=linear_weights)\n        self.flatten = Flatten()\n\n    def call(self, inputs, training=False):\n        x = self.concat(\n            [\n                self.embedding_1(inputs[""f1""]),\n                self.embedding_1(inputs[""f2""]),\n                self.embedding_2(inputs[""f3""]),\n            ]\n        )\n        x = self.flatten(x)\n        x = self.dense(x)\n        return x\n\n\ndef loss(labels, predictions):\n    return tf.reduce_mean(tf.square(predictions - labels))\n\n\ndef dataset_fn(dataset, mode, metadata):\n    def _parse_data(record):\n        feature_description = {\n            ""f1"": tf.io.FixedLenFeature([1], tf.int64),\n            ""f2"": tf.io.FixedLenFeature([1], tf.int64),\n            ""f3"": tf.io.FixedLenFeature([1], tf.int64),\n            ""label"": tf.io.FixedLenFeature([1], tf.int64),\n        }\n        r = tf.io.parse_single_example(record, feature_description)\n        return {""f1"": r[""f1""], ""f2"": r[""f2""], ""f3"": r[""f3""]}, r[""label""]\n\n    dataset = dataset.map(_parse_data)\n    return dataset\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef eval_metrics_fn(predictions, labels):\n    return {""mse"": tf.reduce_mean(tf.square(predictions - labels))}\n'"
elasticdl/python/tests/evaluation_service_test.py,3,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Accuracy, MeanSquaredError\n\nfrom elasticdl.python.common.constants import MetricsDictKey\nfrom elasticdl.python.common.evaluation_utils import EvaluationMetrics\nfrom elasticdl.python.common.tensor_utils import ndarray_to_pb\nfrom elasticdl.python.master.evaluation_service import (\n    EvaluationJob,\n    EvaluationService,\n)\nfrom elasticdl.python.master.servicer import MasterServicer\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\n\n\ndef _eval_metrics_fn():\n    return {\n        ""acc"": Accuracy(),\n        ""mse"": MeanSquaredError(),\n        ""acc_fn"": lambda labels, outputs: tf.equal(\n            tf.cast(outputs, tf.int32), tf.cast(labels, tf.int32)\n        ),\n    }\n\n\nclass EvaluationServiceTest(unittest.TestCase):\n    @staticmethod\n    def ok_to_new_job(job, latest_chkp_version):\n        return job.finished() and latest_chkp_version > job.model_version\n\n    def testEvaluationJob(self):\n        model_version = 1\n        total_tasks = 5\n        latest_chkp_version = 2\n        job = EvaluationJob(_eval_metrics_fn(), model_version, total_tasks)\n        self.assertEqual(0, job._completed_tasks)\n        self.assertFalse(job.finished())\n        self.assertFalse(self.ok_to_new_job(job, latest_chkp_version))\n\n        # Now make 4 tasks finished\n        for i in range(4):\n            job.complete_task()\n        self.assertEqual(4, job._completed_tasks)\n        self.assertFalse(job.finished())\n        self.assertFalse(self.ok_to_new_job(job, latest_chkp_version))\n\n        # One more task finishes\n        job.complete_task()\n        self.assertEqual(5, job._completed_tasks)\n        self.assertTrue(job.finished())\n        self.assertTrue(self.ok_to_new_job(job, latest_chkp_version))\n\n        # No new model checkpoint\n        latest_chkp_version = job.model_version\n        self.assertFalse(self.ok_to_new_job(job, latest_chkp_version))\n        latest_chkp_version = job.model_version + 1\n        self.assertTrue(self.ok_to_new_job(job, latest_chkp_version))\n\n        model_outputs = {}\n        model_outputs[MetricsDictKey.MODEL_OUTPUT] = ndarray_to_pb(\n            np.array([[1], [6], [3]], np.float32)\n        )\n        labels = ndarray_to_pb(np.array([[1], [0], [3]], np.float32))\n        job.report_evaluation_metrics(model_outputs, labels)\n        job.report_evaluation_metrics(\n            {\n                MetricsDictKey.MODEL_OUTPUT: ndarray_to_pb(\n                    np.array([[4], [5], [6], [7], [8]], np.float32)\n                )\n            },\n            ndarray_to_pb(np.array([[7], [8], [9], [10], [11]], np.float32)),\n        )\n        expected_acc = 0.25\n        evaluation_metrics = job.evaluation_metrics.get_evaluation_summary()\n        self.assertAlmostEqual(expected_acc, evaluation_metrics.get(""acc""))\n        self.assertAlmostEqual(expected_acc, evaluation_metrics.get(""acc_fn""))\n        self.assertAlmostEqual(10.125, evaluation_metrics.get(""mse""))\n\n    def testEvaluationService(self):\n        task_d = _TaskDispatcher(\n            {""f1"": (0, 10), ""f2"": (0, 10)},\n            {""f1"": (0, 10), ""f2"": (0, 10)},\n            {},\n            3,\n            1,\n        )\n\n        # Evaluation metrics will not be accepted if no evaluation ongoing\n        evaluation_service = EvaluationService(\n            None, task_d, 10, 20, 0, False, _eval_metrics_fn,\n        )\n\n        _ = MasterServicer(2, task_d, evaluation_service=evaluation_service,)\n\n        # No checkpoint available\n        self.assertFalse(evaluation_service.try_to_create_new_job())\n\n        # Add an evaluation task and we can start evaluation\n        self.assertEqual(8, len(task_d._todo))\n        evaluation_service.add_evaluation_task(False)\n        self.assertEqual(8, len(task_d._eval_todo))\n        self.assertFalse(evaluation_service._eval_job.finished())\n\n        for i in range(8):\n            self.assertFalse(evaluation_service._eval_job.finished())\n            evaluation_service.complete_task()\n        self.assertTrue(evaluation_service._eval_job is None)\n        self.assertFalse(evaluation_service.try_to_create_new_job())\n\n    def testEvaluationOnly(self):\n        task_d = _TaskDispatcher({}, {""f1"": (0, 10), ""f2"": (0, 10)}, {}, 3, 1)\n\n        evaluation_service = EvaluationService(\n            None, task_d, 0, 0, 0, True, _eval_metrics_fn\n        )\n        task_d.set_evaluation_service(evaluation_service)\n\n        _ = MasterServicer(2, task_d, evaluation_service=evaluation_service,)\n\n        self.assertEqual(8, len(task_d._eval_todo))\n        for i in range(8):\n            self.assertFalse(evaluation_service._eval_job.finished())\n            evaluation_service.complete_task()\n        self.assertTrue(evaluation_service._eval_job.finished())\n\n    def testNeedEvaluation(self):\n        task_d = _TaskDispatcher(\n            {""f1"": (0, 10), ""f2"": (0, 10)},\n            {""f1"": (0, 10), ""f2"": (0, 10)},\n            {},\n            3,\n            1,\n        )\n\n        evaluation_service = EvaluationService(\n            None, task_d, 10, 0, 10, False, _eval_metrics_fn,\n        )\n\n        # Should add evaluation task and create eval job\n        evaluation_service.add_evaluation_task_if_needed(\n            master_locking=False, model_version=10\n        )\n        self.assertTrue(evaluation_service._eval_job is not None)\n        self.assertEqual(evaluation_service._eval_checkpoint_versions, [])\n\n        # Should ignore because version 10 is in the eval list\n        evaluation_service.add_evaluation_task_if_needed(\n            master_locking=False, model_version=10\n        )\n        self.assertEqual(evaluation_service._eval_checkpoint_versions, [])\n\n        # Should append version 20 to the eval list\n        evaluation_service.add_evaluation_task_if_needed(\n            master_locking=False, model_version=20\n        )\n        self.assertEqual(evaluation_service._eval_checkpoint_versions, [20])\n\n        # Should ignore version 10 because version 20 is already in eval list\n        evaluation_service.add_evaluation_task_if_needed(\n            master_locking=False, model_version=10\n        )\n        self.assertEqual(evaluation_service._eval_checkpoint_versions, [20])\n\n        # Should append version 30 to the eval list\n        evaluation_service.add_evaluation_task_if_needed(\n            master_locking=False, model_version=30\n        )\n        self.assertEqual(\n            evaluation_service._eval_checkpoint_versions, [20, 30]\n        )\n\n    def test_update_metric_by_small_chunks(self):\n        labels = np.random.randint(0, 2, 1234)\n        preds = np.random.random(1234)\n        auc = tf.keras.metrics.AUC()\n        auc.update_state(labels, preds)\n        auc_value_0 = auc.result()\n\n        auc.reset_states()\n        EvaluationMetrics._update_metric_by_small_chunk(auc, labels, preds)\n        auc_value_1 = auc.result()\n        self.assertEquals(auc_value_0, auc_value_1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/example_test.py,0,"b'import os\nimport unittest\n\nfrom elasticdl.python.tests.test_utils import (\n    DatasetName,\n    create_pserver,\n    distributed_train_and_evaluate,\n)\n\n_model_zoo_path = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), ""../../../model_zoo""\n)\n\n\nclass ExampleTest(unittest.TestCase):\n    def _test_train(\n        self,\n        feature_shape,\n        model_def,\n        model_params="""",\n        dataset_name=DatasetName.IMAGE_DEFAULT,\n    ):\n        num_ps_pods = 2\n        use_asyncs = [False, True]\n        model_versions = []\n        for use_async in use_asyncs:\n            grads_to_wait = 1 if use_async else 2\n            _, ps_channels, pservers = create_pserver(\n                _model_zoo_path,\n                model_def,\n                grads_to_wait,\n                use_async,\n                num_ps_pods,\n            )\n            try:\n                model_version = distributed_train_and_evaluate(\n                    feature_shape,\n                    _model_zoo_path,\n                    model_def,\n                    model_params=model_params,\n                    training=True,\n                    dataset_name=dataset_name,\n                    use_async=use_async,\n                    ps_channels=ps_channels,\n                    pservers=pservers,\n                )\n            finally:\n                for pserver in pservers:\n                    pserver.server.stop(0)\n            model_versions.append(model_version)\n        return model_versions\n\n    def _test_evaluate(\n        self,\n        feature_shape,\n        model_def,\n        model_params="""",\n        dataset_name=DatasetName.IMAGE_DEFAULT,\n    ):\n        num_ps_pods = 2\n        grads_to_wait = 1\n        _, ps_channels, pservers = create_pserver(\n            _model_zoo_path, model_def, grads_to_wait, False, num_ps_pods\n        )\n        try:\n            model_version = distributed_train_and_evaluate(\n                feature_shape,\n                _model_zoo_path,\n                model_def,\n                model_params=model_params,\n                training=False,\n                dataset_name=dataset_name,\n                ps_channels=ps_channels,\n                pservers=pservers,\n            )\n        finally:\n            for pserver in pservers:\n                pserver.server.stop(0)\n        return model_version\n\n    def test_deepfm_functional_train(self):\n        self._test_train(\n            10,\n            ""deepfm_functional_api.deepfm_functional_api.custom_model"",\n            ""input_dim=5383;embedding_dim=4;input_length=10;fc_unit=4"",\n            dataset_name=DatasetName.FRAPPE,\n        )\n\n    def test_deepfm_functional_evaluate(self):\n        self._test_evaluate(\n            10,\n            ""deepfm_functional_api.deepfm_functional_api.custom_model"",\n            ""input_dim=5383;embedding_dim=4;input_length=10;fc_unit=4"",\n            dataset_name=DatasetName.FRAPPE,\n        )\n\n    def test_mnist_train(self):\n        model_defs = [\n            ""mnist_functional_api.mnist_functional_api.custom_model"",\n            ""mnist_subclass.mnist_subclass.CustomModel"",\n        ]\n\n        model_versions = []\n        for model_def in model_defs:\n            versions = self._test_train(\n                feature_shape=[28, 28], model_def=model_def,\n            )\n\n            model_versions.extend(versions)\n        # async model version = sync model version * 2\n        self.assertEqual(model_versions[0] * 2, model_versions[1])\n        self.assertEqual(model_versions[2] * 2, model_versions[3])\n\n    def test_mnist_evaluate(self):\n        model_defs = [\n            ""mnist_functional_api.mnist_functional_api.custom_model"",\n            ""mnist_subclass.mnist_subclass.CustomModel"",\n        ]\n        for model_def in model_defs:\n            self._test_evaluate([28, 28], model_def)\n\n    def test_cifar10_train(self):\n        model_defs = [\n            ""cifar10_functional_api.cifar10_functional_api.custom_model"",\n            ""cifar10_subclass.cifar10_subclass.CustomModel"",\n        ]\n\n        model_versions = []\n        for model_def in model_defs:\n            versions = self._test_train([32, 32, 3], model_def,)\n            model_versions.extend(versions)\n        # async model version = sync model version * 2\n        self.assertEqual(model_versions[0] * 2, model_versions[1])\n        self.assertEqual(model_versions[2] * 2, model_versions[3])\n\n    def test_cifar10_evaluate(self):\n        model_defs = [\n            ""cifar10_functional_api.cifar10_functional_api.custom_model"",\n            ""cifar10_subclass.cifar10_subclass.CustomModel"",\n        ]\n        for model_def in model_defs:\n            self._test_evaluate(\n                [32, 32, 3], model_def,\n            )\n\n    def test_resnet50_subclass_train(self):\n        self._test_train(\n            [224, 224, 3],\n            ""resnet50_subclass.resnet50_subclass.CustomModel"",\n            dataset_name=DatasetName.IMAGENET,\n        )\n\n    def test_resnet50_subclass_evaluate(self):\n        self._test_evaluate(\n            [224, 224, 3],\n            ""resnet50_subclass.resnet50_subclass.CustomModel"",\n            model_params=\'num_classes=10;dtype=""float32""\',\n            dataset_name=DatasetName.IMAGENET,\n        )\n\n    def test_wide_deep_train(self):\n        self._test_train(\n            10,\n            ""census_wide_deep_model.wide_deep_functional_api.custom_model"",\n            dataset_name=DatasetName.CENSUS,\n        )\n\n    def test_wide_deep_evaluate(self):\n        self._test_evaluate(\n            10,\n            ""census_wide_deep_model.wide_deep_functional_api.custom_model"",\n            dataset_name=DatasetName.CENSUS,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/feature_column_test.py,19,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.python.elasticdl.feature_column.feature_column import (\n    embedding_column,\n)\n\n\ndef call_feature_columns(feature_columns, input):\n    dense_features = tf.keras.layers.DenseFeatures(feature_columns)\n    return dense_features(input)\n\n\ndef generate_vectors_with_one_hot_value(ids, dimension):\n    if isinstance(ids, tf.Tensor):\n        ids = ids.numpy()\n\n    return np.array(\n        [\n            (np.arange(dimension) == (id % dimension)).astype(np.int)\n            for id in ids\n        ]\n    )\n\n\ndef generate_vectors_fill_with_id_value(ids, dimension):\n    if isinstance(ids, tf.Tensor):\n        ids = ids.numpy()\n\n    return np.array([np.full((dimension), id) for id in ids])\n\n\nclass EmbeddingColumnTest(unittest.TestCase):\n    def test_call_embedding_column(self):\n        dimension = 32\n\n        item_id_embedding = embedding_column(\n            tf.feature_column.categorical_column_with_identity(\n                ""item_id"", num_buckets=128\n            ),\n            dimension=dimension,\n        )\n\n        def _mock_gather_embedding(name, ids):\n            return generate_vectors_with_one_hot_value(ids, dimension)\n\n        item_id_embedding.set_lookup_embedding_func(_mock_gather_embedding)\n\n        output = call_feature_columns(\n            [item_id_embedding], {""item_id"": [1, 2, 3]}\n        )\n\n        self.assertTrue(\n            np.array_equal(\n                output.numpy(),\n                generate_vectors_with_one_hot_value([1, 2, 3], dimension),\n            )\n        )\n\n    def test_call_embedding_column_with_weights(self):\n        dimension = 8\n\n        item_id_embedding = embedding_column(\n            tf.feature_column.weighted_categorical_column(\n                tf.feature_column.categorical_column_with_identity(\n                    ""item_id"", num_buckets=128\n                ),\n                weight_feature_key=""frequency"",\n            ),\n            dimension=dimension,\n            initializer=tf.initializers.identity,\n            combiner=""sum"",\n        )\n\n        def _mock_gather_embedding(name, ids):\n            return generate_vectors_with_one_hot_value(ids, dimension)\n\n        item_id_embedding.set_lookup_embedding_func(_mock_gather_embedding)\n\n        output = call_feature_columns(\n            [item_id_embedding],\n            {\n                ""item_id"": [[2, 6, 5], [3, 1, 1]],\n                ""frequency"": [[0.33, 5.0, 1.024], [2.048, 0.5, 1.0]],\n            },\n        )\n\n        expected_output = np.array(\n            [\n                [0.0, 0.0, 0.33, 0.0, 0.0, 1.024, 5.0, 0.0],\n                [0.0, 1.5, 0.0, 2.048, 0.0, 0.0, 0.0, 0.0],\n            ],\n            dtype=np.float32,\n        )\n\n        self.assertTrue(np.array_equal(output.numpy(), expected_output))\n\n    def test_embedding_column_gradients(self):\n        dimension = 8\n\n        inputs = {""item_id"": [[1, 2], [10, 10], [6, 3], [10, 2]]}\n\n        # unique_ids: 1, 2, 10, 6, 3\n        expected_sum_grads = [1, 2, 3, 1, 1]\n        expected_mean_grads = [1 / 2.0, 2 / 2.0, 3 / 2.0, 1 / 2.0, 1 / 2.0]\n        expected_sqrtn_grads = [\n            1 / np.sqrt(2.0),\n            2 / np.sqrt(2.0),\n            3 / np.sqrt(2.0),\n            1 / np.sqrt(2.0),\n            1 / np.sqrt(2.0),\n        ]\n\n        combiner_to_expected_grads = {\n            ""sum"": expected_sum_grads,\n            ""mean"": expected_mean_grads,\n            ""sqrtn"": expected_sqrtn_grads,\n        }\n\n        for combiner, expected_grads in combiner_to_expected_grads.items():\n            item_id_embedding = embedding_column(\n                tf.feature_column.categorical_column_with_identity(\n                    ""item_id"", num_buckets=128\n                ),\n                dimension=dimension,\n                combiner=combiner,\n            )\n\n            def _mock_gather_embedding(name, ids):\n                return generate_vectors_fill_with_id_value(ids, dimension)\n\n            item_id_embedding.set_lookup_embedding_func(_mock_gather_embedding)\n\n            dense_features = tf.keras.layers.DenseFeatures([item_id_embedding])\n            call_fns = [dense_features.call, tf.function(dense_features.call)]\n\n            for call_fn in call_fns:\n                with tf.GradientTape() as tape:\n                    item_id_embedding.set_tape(tape)\n                    output = call_fn(inputs)\n                    batch_embedding = item_id_embedding.embedding_and_ids[\n                        0\n                    ].batch_embedding\n                    grads = tape.gradient(output, batch_embedding)\n\n                    item_id_embedding.reset()\n\n                    grads = grads.numpy()\n                    for i in range(5):\n                        self.assertTrue(\n                            np.isclose(\n                                grads[i],\n                                np.full(dimension, expected_grads[i]),\n                            ).all()\n                        )\n\n    def test_embedding_column_gradients_with_weights(self):\n        dimension = 8\n\n        inputs = {\n            ""item_id"": [[1, 2], [10, 10], [6, 3], [10, 2]],\n            ""frequency"": [[0.3, 0.6], [0.1, 0.2], [0.8, 0.1], [0.9, 0.6]],\n        }\n\n        item_ids = tf.reshape(inputs[""item_id""], shape=[-1])\n        _, item_id_idx = tf.unique(item_ids)\n\n        # unique_ids: 1, 2, 10, 6, 3\n        expected_sum_grads = np.array(\n            [\n                np.full(dimension, value)\n                for value in [0.3, 0.6, 0.1, 0.2, 0.8, 0.1, 0.9, 0.6]\n            ]\n        )\n        expected_mean_grads = np.array(\n            [\n                np.full(dimension, value)\n                for value in [\n                    0.3 / 0.9,\n                    0.6 / 0.9,\n                    0.1 / 0.3,\n                    0.2 / 0.3,\n                    0.8 / 0.9,\n                    0.1 / 0.9,\n                    0.9 / 1.5,\n                    0.6 / 1.5,\n                ]\n            ]\n        )\n        expected_sqrtn_grads = np.array(\n            [\n                np.full(dimension, value)\n                for value in [\n                    0.3 / np.sqrt(np.power(0.3, 2) + np.power(0.6, 2)),\n                    0.6 / np.sqrt(np.power(0.3, 2) + np.power(0.6, 2)),\n                    0.1 / np.sqrt(np.power(0.1, 2) + np.power(0.2, 2)),\n                    0.2 / np.sqrt(np.power(0.1, 2) + np.power(0.2, 2)),\n                    0.8 / np.sqrt(np.power(0.8, 2) + np.power(0.1, 2)),\n                    0.1 / np.sqrt(np.power(0.8, 2) + np.power(0.1, 2)),\n                    0.9 / np.sqrt(np.power(0.9, 2) + np.power(0.6, 2)),\n                    0.6 / np.sqrt(np.power(0.9, 2) + np.power(0.6, 2)),\n                ]\n            ]\n        )\n\n        combiner_to_expected_grads = {\n            ""sum"": expected_sum_grads,\n            ""mean"": expected_mean_grads,\n            ""sqrtn"": expected_sqrtn_grads,\n        }\n\n        for combiner, expected_grads in combiner_to_expected_grads.items():\n            item_id_embedding = embedding_column(\n                tf.feature_column.weighted_categorical_column(\n                    tf.feature_column.categorical_column_with_identity(\n                        ""item_id"", num_buckets=128\n                    ),\n                    weight_feature_key=""frequency"",\n                ),\n                dimension=dimension,\n                initializer=tf.initializers.identity,\n                combiner=combiner,\n            )\n\n            def _mock_gather_embedding(name, ids):\n                return generate_vectors_fill_with_id_value(ids, dimension)\n\n            item_id_embedding.set_lookup_embedding_func(_mock_gather_embedding)\n\n            dense_features = tf.keras.layers.DenseFeatures([item_id_embedding])\n            call_fns = [dense_features.call, tf.function(dense_features.call)]\n\n            for call_fn in call_fns:\n                with tf.GradientTape() as tape:\n                    item_id_embedding.set_tape(tape)\n                    output = call_fn(inputs)\n                    batch_embedding = item_id_embedding.embedding_and_ids[\n                        0\n                    ].batch_embedding\n                    grads = tape.gradient(output, batch_embedding)\n\n                    item_id_embedding.reset()\n\n                    grad_indices = grads.indices\n                    grad_values = grads.values\n\n                    self.assertTrue(\n                        np.array_equal(\n                            grad_indices.numpy(), item_id_idx.numpy()\n                        )\n                    )\n                    self.assertTrue(\n                        np.isclose(grad_values.numpy(), expected_grads).all()\n                    )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/hash_utils_test.py,0,"b'import unittest\n\nimport numpy as np\n\nfrom elasticdl.python.common.hash_utils import (\n    int_to_id,\n    scatter_embedding_vector,\n)\n\n\nclass HashUtilTest(unittest.TestCase):\n    def test_scatter_embedding_vector(self):\n        vectors = np.array([[1, 2], [3, 4], [5, 6], [1, 7], [3, 9]])\n        indices = np.array([0, 1, 2, 3, 4])\n        num = 2\n\n        expected_results = {}\n        for i, item_id in enumerate(indices):\n            ps_id = int_to_id(item_id, num)\n            if ps_id not in expected_results:\n                item_list = [item_id]\n                expected_results[ps_id] = [\n                    np.expand_dims(vectors[i, :], axis=0),\n                    item_list,\n                ]\n            else:\n                expected_results[ps_id][0] = np.concatenate(\n                    (\n                        expected_results[ps_id][0],\n                        np.expand_dims(vectors[i, :], axis=0),\n                    ),\n                    axis=0,\n                )\n                expected_results[ps_id][1].append(item_id)\n        results = scatter_embedding_vector(vectors, indices, num)\n\n        for ps_id in range(num):\n            np.testing.assert_array_equal(\n                results[ps_id][0], expected_results[ps_id][0]\n            )\n            self.assertListEqual(results[ps_id][1], expected_results[ps_id][1])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/image_builder_test.py,0,"b'import os\nimport unittest\n\nfrom elasticdl.python.elasticdl.image_builder import (\n    _create_dockerfile,\n    _find_elasticdl_root,\n    _generate_unique_image_name,\n)\n\n\nclass DockerTest(unittest.TestCase):\n    def test_find_elasticdl_root(self):\n        rdm = os.path.join(_find_elasticdl_root(), ""README.md"")\n        self.assertTrue(os.path.exists(rdm))\n        with open(rdm, ""r"") as f:\n            self.assertTrue(f.read().startswith(""# ElasticDL:""))\n\n    def test_generate_unique_image_name(self):\n        self.assertTrue(\n            _generate_unique_image_name(None).startswith(""elasticdl:"")\n        )\n        self.assertTrue(\n            _generate_unique_image_name("""").startswith(""elasticdl:"")\n        )\n        self.assertTrue(\n            _generate_unique_image_name(""proj"").startswith(""proj/elasticdl:"")\n        )\n        self.assertTrue(\n            _generate_unique_image_name(""gcr.io/proj"").startswith(\n                ""gcr.io/proj/elasticdl:""\n            )\n        )\n\n    def test_create_dockerfile(self):\n        self.assertTrue(\n            ""COPY"" in _create_dockerfile(""elasticdl"", ""/home/me/models"")\n        )\n        self.assertTrue(\n            ""COPY"" in _create_dockerfile(""elasticdl"", ""file:///home/me/models"")\n        )\n        self.assertTrue(\n            ""git clone""\n            in _create_dockerfile(""elasticdl"", ""https://github.com/me/models"")\n        )\n        with self.assertRaises(RuntimeError):\n            _create_dockerfile(""elasticdl"", """")\n        with self.assertRaises(RuntimeError):\n            _create_dockerfile(""elasticdl"", None)\n'"
elasticdl/python/tests/in_process_master.py,0,"b'""""""In process master for unittests""""""\nfrom elasticdl.python.tests import test_call_back\n\n\nclass InProcessMaster(object):\n    def __init__(self, master, callbacks=[]):\n        self._m = master\n        self._callbacks = callbacks\n\n    def get_task(self, req):\n        return self._m.get_task(req, None)\n\n    """"""\n    def ReportGradient(self, req):\n        for callback in self._callbacks:\n            if test_call_back.ON_REPORT_GRADIENT_BEGIN in callback.call_times:\n                callback()\n        return self._m.ReportGradient(req, None)\n    """"""\n\n    def report_evaluation_metrics(self, req):\n        for callback in self._callbacks:\n            if test_call_back.ON_REPORT_EVALUATION_METRICS_BEGIN in (\n                callback.call_times\n            ):\n                callback()\n        return self._m.report_evaluation_metrics(req, None)\n\n    def report_task_result(self, req):\n        return self._m.report_task_result(req, None)\n\n    def report_version(self, req):\n        return self._m.report_version(req, None)\n'"
elasticdl/python/tests/k8s_client_test.py,0,"b'import os\nimport random\nimport time\nimport unittest\n\nfrom elasticdl.python.common import k8s_client as k8s\n\n\nclass WorkerTracker(object):\n    def __init__(self):\n        self._count = 0\n\n    def event_cb(self, event):\n        if event[""type""] == ""ADDED"":\n            self._count += 1\n        elif event[""type""] == ""DELETED"":\n            self._count -= 1\n\n\n@unittest.skipIf(\n    os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n    ""No Kubernetes cluster available"",\n)\nclass K8sClientTest(unittest.TestCase):\n    def test_client(self):\n        tracker = WorkerTracker()\n\n        c = k8s.Client(\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            namespace=""default"",\n            job_name=""test-job-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            event_callback=tracker.event_cb,\n        )\n\n        # Start master\n        resource = ""cpu=100m,memory=64M""\n        c.create_master(\n            resource_requests=resource,\n            resource_limits=resource,\n            pod_priority=None,\n            args=None,\n            volume=None,\n            image_pull_policy=""Never"",\n            restart_policy=""Never"",\n        )\n        while tracker._count < 1:\n            time.sleep(1)\n\n        # Check master pod labels\n        master = c.get_master_pod()\n        self.assertEqual(\n            master.metadata.labels[k8s.ELASTICDL_JOB_KEY], c.job_name\n        )\n        self.assertEqual(\n            master.metadata.labels[k8s.ELASTICDL_REPLICA_TYPE_KEY], ""master""\n        )\n        self.assertEqual(\n            master.metadata.labels[k8s.ELASTICDL_REPLICA_INDEX_KEY], ""0""\n        )\n\n        # Start 3 workers\n        for i in range(3):\n            _ = c.create_worker(\n                worker_id=str(i),\n                resource_requests=resource,\n                resource_limits=resource,\n                command=[""echo""],\n                pod_priority=None,\n                args=None,\n                volume=None,\n                image_pull_policy=""Never"",\n                restart_policy=""Never"",\n                expose_ports=False,\n            )\n            time.sleep(5)\n\n        # Wait for workers to be added\n        while tracker._count < 4:\n            time.sleep(1)\n\n        # Check worker pods labels\n        for i in range(3):\n            worker = c.get_worker_pod(i)\n            self.assertEqual(\n                worker.metadata.labels[k8s.ELASTICDL_JOB_KEY], c.job_name\n            )\n            self.assertEqual(\n                worker.metadata.labels[k8s.ELASTICDL_REPLICA_TYPE_KEY],\n                ""worker"",\n            )\n            self.assertEqual(\n                worker.metadata.labels[k8s.ELASTICDL_REPLICA_INDEX_KEY], str(i)\n            )\n\n        # Start 3 worker services\n        for i in range(3):\n            c.create_worker_service(i)\n\n        # Check worker services\n        for i in range(3):\n            service = c.get_worker_service(i)\n            self.assertIsNotNone(service)\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_JOB_KEY], c.job_name\n            )\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_REPLICA_TYPE_KEY], ""worker""\n            )\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_REPLICA_INDEX_KEY], str(i)\n            )\n\n        # Start 2 ps pods\n        for i in range(2):\n            _ = c.create_ps(\n                ps_id=str(i),\n                resource_requests=resource,\n                resource_limits=resource,\n                command=[""echo""],\n                pod_priority=None,\n                args=None,\n                volume=None,\n                image_pull_policy=""Never"",\n                restart_policy=""Never"",\n                expose_ports=False,\n            )\n            time.sleep(5)\n\n        # Wait for ps to be added\n        while tracker._count < 6:\n            time.sleep(1)\n\n        # Check ps pods labels\n        for i in range(2):\n            ps = c.get_ps_pod(i)\n            self.assertEqual(\n                ps.metadata.labels[k8s.ELASTICDL_JOB_KEY], c.job_name\n            )\n            self.assertEqual(\n                ps.metadata.labels[k8s.ELASTICDL_REPLICA_TYPE_KEY], ""ps""\n            )\n            self.assertEqual(\n                ps.metadata.labels[k8s.ELASTICDL_REPLICA_INDEX_KEY], str(i)\n            )\n\n        # Start 2 ps services\n        for i in range(2):\n            c.create_ps_service(i)\n\n        # Check ps services\n        for i in range(2):\n            service = c.get_ps_service(i)\n            self.assertIsNotNone(service)\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_JOB_KEY], c.job_name\n            )\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_REPLICA_TYPE_KEY], ""ps""\n            )\n            self.assertEqual(\n                service.spec.selector[k8s.ELASTICDL_REPLICA_INDEX_KEY], str(i)\n            )\n\n        # Delete master and all ps and workers should also be deleted\n        c.delete_master()\n\n        # wait for all ps, workers and services to be deleted\n        while tracker._count > 0:\n            time.sleep(1)\n\n    def test_patch_labels_to_pod(self):\n        tracker = WorkerTracker()\n\n        c = k8s.Client(\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            namespace=""default"",\n            job_name=""test-job-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            event_callback=tracker.event_cb,\n        )\n\n        # Start 1 worker\n        resource = ""cpu=100m,memory=64M""\n        worker_name = ""worker-1""\n        worker_pod = c.create_worker(\n            worker_id=worker_name,\n            resource_requests=resource,\n            resource_limits=resource,\n            command=[""echo""],\n            pod_priority=None,\n            args=None,\n            volume=None,\n            image_pull_policy=""Never"",\n            restart_policy=""Never"",\n            expose_ports=False,\n        )\n\n        label_k = ""status""\n        label_v = ""finished""\n        modified_pod = c.patch_labels_to_pod(\n            pod_name=worker_pod.metadata.name, labels_dict={label_k: label_v}\n        )\n\n        # Wait for the worker to be added\n        while tracker._count != 1:\n            time.sleep(1)\n\n        # Patching labels to an existing pod should work correctly\n        self.assertEqual(modified_pod.metadata.labels[label_k], label_v)\n\n        # Delete the worker\n        c.delete_worker(worker_name)\n\n        # Wait for the worker to be deleted\n        while tracker._count == 1:\n            time.sleep(1)\n\n        # Patching a non-existent pod should return None\n        modified_pod = c.patch_labels_to_pod(\n            pod_name=worker_pod.metadata.name, labels_dict={label_k: label_v}\n        )\n        self.assertEqual(modified_pod, None)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/k8s_instance_manager_test.py,0,"b'import os\nimport random\nimport time\nimport unittest\nfrom unittest.mock import MagicMock, call\n\nfrom elasticdl.python.master.k8s_instance_manager import InstanceManager\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\n\n\nclass InstanceManagerTest(unittest.TestCase):\n    @unittest.skipIf(\n        os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n        ""No Kubernetes cluster available"",\n    )\n    def testCreateDeleteWorkerPod(self):\n        task_d = _TaskDispatcher({""f"": (0, 10)}, {}, {}, 1, 1)\n        task_d.recover_tasks = MagicMock()\n        instance_manager = InstanceManager(\n            task_d,\n            job_name=""test-create-worker-pod-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            worker_command=[""/bin/bash""],\n            worker_args=[""-c"", ""echo""],\n            namespace=""default"",\n            num_workers=3,\n        )\n\n        instance_manager.start_workers()\n        max_check_num = 20\n        for _ in range(max_check_num):\n            time.sleep(3)\n            counters = instance_manager.get_worker_counter()\n            if counters[""Succeeded""] == 3:\n                break\n\n        instance_manager.stop_relaunch_and_remove_workers()\n        for _ in range(max_check_num):\n            time.sleep(3)\n            counters = instance_manager.get_worker_counter()\n            if not counters:\n                break\n        task_d.recover_tasks.assert_has_calls(\n            [call(0), call(1), call(2)], any_order=True\n        )\n\n    @unittest.skipIf(\n        os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n        ""No Kubernetes cluster available"",\n    )\n    def testFailedWorkerPod(self):\n        """"""\n        Start a pod running a python program destined to fail with\n        restart_policy=""Never"" to test failed_worker_count\n        """"""\n        task_d = _TaskDispatcher({""f"": (0, 10)}, {}, {}, 1, 1)\n        task_d.recover_tasks = MagicMock()\n        instance_manager = InstanceManager(\n            task_d,\n            job_name=""test-failed-worker-pod-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            worker_command=[""/bin/bash""],\n            worker_args=[""-c"", ""badcommand""],\n            namespace=""default"",\n            num_workers=3,\n            restart_policy=""Never"",\n        )\n        instance_manager.start_workers()\n        max_check_num = 20\n        for _ in range(max_check_num):\n            time.sleep(3)\n            counters = instance_manager.get_worker_counter()\n            if counters[""Failed""] == 3:\n                break\n\n        instance_manager.stop_relaunch_and_remove_workers()\n        for _ in range(max_check_num):\n            time.sleep(3)\n            counters = instance_manager.get_worker_counter()\n            if not counters:\n                break\n        task_d.recover_tasks.assert_has_calls(\n            [call(0), call(1), call(2)], any_order=True\n        )\n\n    @unittest.skipIf(\n        os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n        ""No Kubernetes cluster available"",\n    )\n    def testRelaunchWorkerPod(self):\n        num_workers = 3\n        task_d = _TaskDispatcher({""f"": (0, 10)}, {}, {}, 1, 1)\n        instance_manager = InstanceManager(\n            task_d,\n            job_name=""test-relaunch-worker-pod-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            worker_command=[""/bin/bash""],\n            worker_args=[""-c"", ""sleep 10""],\n            namespace=""default"",\n            num_workers=num_workers,\n        )\n\n        instance_manager.start_workers()\n\n        max_check_num = 60\n        for _ in range(max_check_num):\n            time.sleep(1)\n            counters = instance_manager.get_worker_counter()\n            if counters[""Running""] + counters[""Pending""] > 0:\n                break\n        # Note: There is a slight chance of race condition.\n        # Hack to find a worker to remove\n        current_workers = set()\n        live_workers = set()\n        with instance_manager._lock:\n            for k, (_, phase) in instance_manager._worker_pods_phase.items():\n                current_workers.add(k)\n                if phase in [""Running"", ""Pending""]:\n                    live_workers.add(k)\n        self.assertTrue(live_workers)\n\n        instance_manager._remove_worker(live_workers.pop())\n        # verify a new worker get launched\n        found = False\n        for _ in range(max_check_num):\n            if found:\n                break\n            time.sleep(1)\n            with instance_manager._lock:\n                for k in instance_manager._worker_pods_phase:\n                    if k not in range(num_workers, num_workers * 2):\n                        found = True\n        else:\n            self.fail(""Failed to find newly launched worker."")\n\n        instance_manager.stop_relaunch_and_remove_workers()\n\n    @unittest.skipIf(\n        os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n        ""No Kubernetes cluster available"",\n    )\n    def testRelaunchPsPod(self):\n        num_ps = 3\n        instance_manager = InstanceManager(\n            task_d=None,\n            job_name=""test-relaunch-ps-pod-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            image_name=""gcr.io/google-samples/hello-app:1.0"",\n            ps_command=[""/bin/bash""],\n            ps_args=[""-c"", ""sleep 10""],\n            namespace=""default"",\n            num_ps=num_ps,\n        )\n\n        instance_manager.start_parameter_servers()\n\n        # Check we also have ps services started\n        for i in range(num_ps):\n            service = instance_manager._k8s_client.get_ps_service(i)\n            self.assertTrue(service.metadata.owner_references)\n            owner = service.metadata.owner_references[0]\n            self.assertEqual(owner.kind, ""Pod"")\n            self.assertEqual(\n                owner.name, instance_manager._k8s_client.get_ps_pod_name(i)\n            )\n\n        max_check_num = 60\n        for _ in range(max_check_num):\n            time.sleep(1)\n            counters = instance_manager.get_ps_counter()\n            if counters[""Running""] + counters[""Pending""] > 0:\n                break\n        # Note: There is a slight chance of race condition.\n        # Hack to find a ps to remove\n        all_current_ps = set()\n        all_live_ps = set()\n        with instance_manager._lock:\n            for k, (_, phase) in instance_manager._ps_pods_phase.items():\n                all_current_ps.add(k)\n                if phase in [""Running"", ""Pending""]:\n                    all_live_ps.add(k)\n        self.assertTrue(all_live_ps)\n\n        ps_to_be_removed = all_live_ps.pop()\n        all_current_ps.remove(ps_to_be_removed)\n        instance_manager._remove_ps(ps_to_be_removed)\n        # Verify a new ps gets launched\n        found = False\n        for _ in range(max_check_num):\n            if found:\n                break\n            time.sleep(1)\n            with instance_manager._lock:\n                for k in instance_manager._ps_pods_phase:\n                    if k not in all_current_ps:\n                        found = True\n        else:\n            self.fail(""Failed to find newly launched ps."")\n\n        instance_manager.stop_relaunch_and_remove_all_ps()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/k8s_resource_test.py,0,"b'import unittest\n\nfrom elasticdl.python.common.k8s_resource import parse\n\n\nclass K8SResourceTest(unittest.TestCase):\n    def test_k8s_resource_parse(self):\n        # parse works as expected on the allowed list of resources\n        self.assertEqual(\n            {\n                ""cpu"": ""250m"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""nvidia.com/gpu"": ""1"",\n                ""ephemeral-storage"": ""32Mi"",\n            },\n            parse(\n                ""cpu=250m,memory=32Mi,disk=64Mi,gpu=1,ephemeral-storage=32Mi""\n            ),\n        )\n        # parse works as expected with redundant spaces\n        self.assertEqual(\n            {\n                ""cpu"": ""250m"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""nvidia.com/gpu"": ""1"",\n                ""ephemeral-storage"": ""32Mi"",\n            },\n            parse(\n                "" cpu=250m, memory=32Mi,disk =64Mi,""\n                ""gpu= 1,ephemeral-storage=32Mi ""\n            ),\n        )\n        # When cpu is non-numeric, parse works as expected\n        self.assertEqual(\n            {\n                ""cpu"": ""250m"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""nvidia.com/gpu"": ""1"",\n            },\n            parse(""cpu=250m,memory=32Mi,disk=64Mi,gpu=1""),\n        )\n        # When cpu is integer, parse works as expected\n        self.assertEqual(\n            {\n                ""cpu"": ""1"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""nvidia.com/gpu"": ""1"",\n            },\n            parse(""cpu=1,memory=32Mi,disk=64Mi,gpu=1""),\n        )\n        # When cpu is float, parse works as expected\n        self.assertEqual(\n            {\n                ""cpu"": ""0.1"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""nvidia.com/gpu"": ""1"",\n            },\n            parse(""cpu=0.1,memory=32Mi,disk=64Mi,gpu=1""),\n        )\n        # When cpu is non-numeric, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""invalid cpu request spec: 250Mi"",\n            parse,\n            ""cpu=250Mi,memory=32Mi,disk=64Mi,gpu=1"",\n        )\n        # When gpu is non-numeric, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""invalid gpu request spec: 1Mi"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,gpu=1Mi"",\n        )\n        # When gpu is not integer, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""invalid gpu request spec: 0.1"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,gpu=0.1"",\n        )\n        # When gpu resource name has a valid vendor name,\n        # parse works as expected\n        self.assertEqual(\n            {\n                ""cpu"": ""0.1"",\n                ""memory"": ""32Mi"",\n                ""disk"": ""64Mi"",\n                ""amd.com/gpu"": ""1"",\n            },\n            parse(""cpu=0.1,memory=32Mi,disk=64Mi,amd.com/gpu=1""),\n        )\n        # When gpu resource name does not have a valid vendor name,\n        # raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""gpu resource name does not have a valid vendor name: blah-gpu"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,blah-gpu=1"",\n        )\n        # When gpu resource name does not have a valid vendor name,\n        # raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""gpu resource name does not have a valid vendor name: @#/gpu"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,@#/gpu=1"",\n        )\n        self.assertRaisesRegex(\n            ValueError,\n            ""gpu resource name does not have a valid vendor name: a_d.com/gpu"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,a_d.com/gpu=1"",\n        )\n        self.assertRaisesRegex(\n            ValueError,\n            ""gpu resource name does not have a valid vendor name: *"",\n            parse,\n            ""cpu=2,memory=32Mi,disk=64Mi,"" + ""a"" * 64 + ""/gpu=1"",\n        )\n        # When memory does not contain expected regex, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""invalid memory request spec: 32blah"",\n            parse,\n            ""cpu=250m,memory=32blah,disk=64Mi,gpu=1"",\n        )\n        # When resource name is unknown, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""unknown is not in the allowed list of resource types:"",\n            parse,\n            ""cpu=250m,unknown=32Mi,disk=64Mi,gpu=1"",\n        )\n        # When resource name is duplicated, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""The resource string contains duplicate resource names: cpu"",\n            parse,\n            ""cpu=250m,cpu=32Mi,disk=64Mi,gpu=1"",\n        )\n'"
elasticdl/python/tests/k8s_tensorboard_client_test.py,0,"b'import os\nimport random\nimport time\nimport unittest\n\nfrom elasticdl.python.common.k8s_tensorboard_client import TensorBoardClient\n\n\n@unittest.skipIf(\n    os.environ.get(""K8S_TESTS"", ""True"") == ""False"",\n    ""No Kubernetes cluster available"",\n)\nclass K8sTensorBoardClientTest(unittest.TestCase):\n    def test_create_tensorboard_service(self):\n        tb_client = TensorBoardClient(\n            image_name=None,\n            namespace=""default"",\n            job_name=""test-job-%d-%d""\n            % (int(time.time()), random.randint(1, 101)),\n            event_callback=None,\n        )\n        tb_client._k8s_client.create_tensorboard_service(\n            port=80, service_type=""LoadBalancer""\n        )\n        time.sleep(1)\n        service = tb_client._get_tensorboard_service()\n        self.assertTrue(""load_balancer"" in service[""status""])\n        self.assertEqual(service[""spec""][""ports""][0][""port""], 80)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/k8s_volume_test.py,0,"b'import unittest\n\nfrom elasticdl.python.common.k8s_volume import parse, parse_volume_and_mount\n\n\nclass K8SVolumeTest(unittest.TestCase):\n    def test_k8s_volume_parse(self):\n        # parse works as expected on the allowed list of volume keys\n        self.assertEqual(\n            [{""claim_name"": ""c1"", ""mount_path"": ""/path1""}],\n            parse(""claim_name=c1,mount_path=/path1""),\n        )\n\n        # parse works as expected on the allowed list of volume dictionaries\n        # with multiple volume configs\n        self.assertEqual(\n            [\n                {""host_path"": ""c0"", ""mount_path"": ""/path0""},\n                {""claim_name"": ""c1"", ""mount_path"": ""/path1""},\n            ],\n            parse(\n                """"""host_path=c0,mount_path=/path0;\n                claim_name=c1,mount_path=/path1""""""\n            ),\n        )\n\n        # parse works as expected with redundant semicolons\n        self.assertEqual(\n            [{""claim_name"": ""c1"", ""mount_path"": ""/path1""}],\n            parse(""claim_name=c1,mount_path=/path1;""),\n        )\n\n        # parse works as expected with redundant spaces\n        self.assertEqual(\n            [{""claim_name"": ""c1"", ""mount_path"": ""/path1""}],\n            parse(""  claim_name=c1,   mount_path = /path1 ""),\n        )\n        # When volume key is unknown, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""unknown is not in the allowed list of volume keys:"",\n            parse,\n            ""claim_name=c1,unknown=v1,mount_path=/path1"",\n        )\n        # When volume key is duplicated, raise an error\n        self.assertRaisesRegex(\n            ValueError,\n            ""The volume string contains duplicate volume key: mount_path"",\n            parse,\n            ""claim_name=c1,mount_path=/path1,mount_path=/path2"",\n        )\n\n    def test_parse_volume_and_mount(self):\n        volume_config = """"""host_path=c0,mount_path=/path0;\n        claim_name=c1,mount_path=/path1""""""\n        volumes, volume_mounts = parse_volume_and_mount(volume_config, ""test"")\n        self.assertEqual(len(volumes), 2)\n        self.assertEqual(volumes[0].persistent_volume_claim.claim_name, ""c1"")\n        self.assertEqual(volumes[1].host_path.path, ""c0"")\n        self.assertEqual(volume_mounts[0].mount_path, ""/path0"")\n        self.assertEqual(volume_mounts[1].mount_path, ""/path1"")\n\n    def test_parse_multiple_mount_on_one_volume(self):\n        volume_config = """"""host_path=c0,mount_path=/path0;\n        claim_name=c1,mount_path=/path1;\n        claim_name=c1,mount_path=/path2,sub_path=/sub_path0""""""\n        volumes, volume_mounts = parse_volume_and_mount(volume_config, ""test"")\n        self.assertEqual(len(volumes), 2)\n        self.assertEqual(volumes[0].persistent_volume_claim.claim_name, ""c1"")\n        self.assertEqual(volumes[1].host_path.path, ""c0"")\n        self.assertEqual(volume_mounts[0].mount_path, ""/path0"")\n        self.assertEqual(volume_mounts[1].mount_path, ""/path1"")\n        self.assertEqual(volume_mounts[2].mount_path, ""/path2"")\n        self.assertEqual(volume_mounts[2].sub_path, ""/sub_path0"")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/layer_test.py,33,"b'import os\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.python.common.model_utils import (\n    find_layer,\n    get_module_file_path,\n    load_model_from_module,\n    load_module,\n)\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\n\n\ndef _get_model_zoo_path():\n    return os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), ""../../../model_zoo""\n    )\n\n\ndef _create_model_instance(model_def):\n    module_file = get_module_file_path(_get_model_zoo_path(), model_def)\n    model_module = load_module(module_file).__dict__\n    return load_model_from_module(model_def, model_module, None)\n\n\nclass FindLayerTest(unittest.TestCase):\n    def test_find_layer(self):\n        model_def = ""mnist_functional_api.mnist_functional_api.custom_model""\n        model = _create_model_instance(model_def)\n\n        layer_num = {\n            tf.keras.layers.Conv2D: 2,\n            tf.keras.layers.Dropout: 1,\n            tf.keras.layers.Embedding: 0,\n        }\n\n        for layer_class in layer_num:\n            layers = find_layer(model, layer_class)\n            self.assertEqual(layer_num[layer_class], len(layers))\n\n    def test_find_layer_nested(self):\n        model_def = ""resnet50_subclass.resnet50_subclass.CustomModel""\n        model = _create_model_instance(model_def)\n\n        layer_num = {\n            tf.keras.layers.Conv2D: 53,\n            tf.keras.layers.Activation: 50,\n            tf.keras.layers.Embedding: 0,\n        }\n\n        for layer_class in layer_num:\n            layers = find_layer(model, layer_class)\n            self.assertEqual(layer_num[layer_class], len(layers))\n\n\nclass mock_worker:\n    def __init__(self, embedding_size, output_dim):\n        self.embedding_size = embedding_size\n        self.output_dim = output_dim\n        self.embedding = np.ndarray(\n            shape=(embedding_size, output_dim), dtype=np.float32\n        )\n        for i in range(embedding_size):\n            self.embedding[i].fill(i)\n\n    def lookup_embedding(self, name, ids):\n        values = np.take(self.embedding, ids, axis=0)\n        return values\n\n\ndef create_embedding_layer(\n    embedding_size,\n    output_dim,\n    input_length=None,\n    combiner=None,\n    mask_zero=False,\n):\n    layer = Embedding(\n        output_dim,\n        input_length=input_length,\n        combiner=combiner,\n        mask_zero=mask_zero,\n    )\n    worker = mock_worker(embedding_size, output_dim)\n    layer.set_lookup_embedding_func(worker.lookup_embedding)\n    return layer\n\n\ndef get_correct_values_for_sparse_test(indices, values, dense_shape, combiner):\n    results = []\n    for i in range(dense_shape[0]):\n        embedding_values = []\n        for n, idx in enumerate(indices):\n            if idx[0] == i:\n                embedding_values.append(values[n])\n        if combiner == ""sum"":\n            combined_value = np.sum(embedding_values, dtype=np.float32)\n        elif combiner == ""mean"":\n            combined_value = np.sum(embedding_values, dtype=np.float32) / len(\n                embedding_values\n            )\n        elif combiner == ""sqrtn"":\n            combined_value = np.sum(\n                embedding_values, dtype=np.float32\n            ) / np.sqrt(len(embedding_values), dtype=np.float32)\n        results.append(combined_value)\n    return results\n\n\nclass EmbeddingLayerTest(unittest.TestCase):\n    def _run_forward_pass_and_compare(\n        self, call_fns, inputs, correct_values, output_dim\n    ):\n        for call_fn in call_fns:\n            values = call_fn(inputs)\n            values = values.numpy().reshape(-1, output_dim)\n            for v, correct_v in zip(values, correct_values):\n                self.assertTrue(np.isclose(v, correct_v).all())\n\n    def test_embedding_layer(self):\n        output_dim = 8\n        embedding_size = 16\n        layer = create_embedding_layer(embedding_size, output_dim)\n\n        input_shape = (2, 6)\n        output_shape = layer.compute_output_shape(input_shape)\n        self.assertEqual(output_shape, input_shape + (output_dim,))\n\n        ids = [0, 1, 3, 8, 3, 2, 3]\n        call_fns = [layer.call, tf.function(layer.call)]\n        correct_values = np.array(\n            [np.array([idx] * output_dim, dtype=np.float32) for idx in ids]\n        )\n        self._run_forward_pass_and_compare(\n            call_fns, ids, correct_values, output_dim\n        )\n\n        # Keras model without/with input_layer\n        model_without_input_layer = tf.keras.models.Sequential([layer])\n        inputs = tf.keras.Input(shape=(7,))\n        embeddings = layer(inputs)\n        model_with_input_layer = tf.keras.Model(\n            inputs=inputs, outputs=embeddings\n        )\n        models = [model_without_input_layer, model_with_input_layer]\n        for model in models:\n            inputs = tf.constant([ids])\n            call_fns = [model.call, tf.function(model.call)]\n            self._run_forward_pass_and_compare(\n                call_fns, inputs, correct_values, output_dim\n            )\n\n    def test_embedding_layer_with_input_length(self):\n        output_dim = 8\n        embedding_size = 16\n        input_length = 4\n        layer = create_embedding_layer(\n            embedding_size, output_dim, input_length=input_length\n        )\n        ids = [[0, 1, 3, 8], [5, 3, 2, 3]]\n        flatten_ids = ids[0] + ids[1]\n        call_fns = [layer.call, tf.function(layer.call)]\n        correct_values = np.array(\n            [\n                np.array([idx] * output_dim, dtype=np.float32)\n                for idx in flatten_ids\n            ]\n        )\n        self._run_forward_pass_and_compare(\n            call_fns, ids, correct_values, output_dim\n        )\n\n    def test_embedding_layer_with_sparse_input(self):\n        output_dim = 8\n        embedding_size = 16\n        combiners = [""sum"", ""mean"", ""sqrtn""]\n        indices = [[0, 0], [1, 1], [1, 2], [2, 1], [3, 1], [3, 3], [3, 5]]\n        values = [1, 1, 3, 2, 0, 2, 6]\n        dense_shape = [4, 6]\n\n        for combiner in combiners:\n            layer = create_embedding_layer(\n                embedding_size, output_dim, combiner=combiner\n            )\n            inputs = tf.SparseTensor(\n                indices=indices, values=values, dense_shape=dense_shape\n            )\n            call_fns = [layer.call, tf.function(layer.call)]\n            correct_value_single_line = get_correct_values_for_sparse_test(\n                indices, values, dense_shape, combiner\n            )\n            correct_values = np.array(\n                [\n                    np.array([idx] * output_dim, dtype=np.float32)\n                    for idx in correct_value_single_line\n                ]\n            )\n            self._run_forward_pass_and_compare(\n                call_fns, inputs, correct_values, output_dim\n            )\n\n    def test_embedding_layer_with_mask_zero(self):\n        output_dim = 8\n        embedding_size = 16\n        mask_zero = True\n        layer = create_embedding_layer(\n            embedding_size, output_dim, mask_zero=mask_zero\n        )\n        ids = [[0, 1, 3, 8], [5, 0, 2, 3]]\n        correct_masks = np.ones((2, 4), dtype=np.bool)\n        correct_masks[0][0] = False\n        correct_masks[1][1] = False\n\n        masks = layer.compute_mask(ids)\n        self.assertTrue((masks.numpy() == correct_masks).all())\n\n        # SparseTensor inputs will raise error\n        indices = [[0, 0], [1, 1], [1, 2], [2, 1], [3, 1], [3, 3], [3, 5]]\n        values = [1, 1, 3, 2, 0, 2, 6]\n        dense_shape = [4, 6]\n        inputs = tf.SparseTensor(\n            indices=indices, values=values, dense_shape=dense_shape\n        )\n        self.assertRaises(ValueError, layer.compute_mask, inputs)\n\n    def test_embedding_layer_gradient(self):\n        output_dim = 8\n        embedding_size = 16\n        layer = create_embedding_layer(embedding_size, output_dim)\n        inputs_list = [\n            tf.keras.backend.constant([[0, 1, 3], [1, 2, 0]], dtype=tf.int64),\n            tf.keras.backend.constant(\n                [[0, 10, 3], [11, 2, 1]], dtype=tf.int64\n            ),\n        ]\n        multiply_values = np.ndarray(shape=(6, output_dim), dtype=np.float32)\n        for i in range(6):\n            multiply_values[i].fill(i)\n        multiply_tensor = tf.convert_to_tensor(multiply_values)\n        multiply_tensor = tf.reshape(multiply_tensor, [2, 3, 8])\n\n        for inputs in inputs_list:\n            call_fns = [layer.call, tf.function(layer.call)]\n            for call_fn in call_fns:\n                with tf.GradientTape() as tape:\n                    layer.set_tape(tape)\n                    output = call_fn(inputs)\n                    output = output * multiply_tensor\n                batch_embedding = layer.embedding_and_ids[0].batch_embedding\n                grads = tape.gradient(output, batch_embedding)\n                self.assertTrue(\n                    (grads.values.numpy() == multiply_values).all()\n                )\n                self.assertTrue(\n                    (\n                        layer.embedding_and_ids[0].batch_ids.numpy()\n                        == inputs.numpy().reshape(-1)\n                    ).all()\n                )\n                layer.reset()\n\n        # Test when an embedding layer is called more than once.\n        # When an embedding layer is called more than once, `tf.function` is\n        # not supported.\n        correct_ids_list = [[0, 1, 3, 1, 2, 0], [0, 10, 3, 11, 2, 1]]\n        with tf.GradientTape() as tape:\n            layer.set_tape(tape)\n            for inputs in inputs_list:\n                _ = layer.call(inputs)\n            self.assertTrue(len(layer.embedding_and_ids) == len(inputs_list))\n            for i, correct_ids in enumerate(correct_ids_list):\n                self.assertTrue(\n                    (\n                        layer.embedding_and_ids[i].batch_ids.numpy()\n                        == correct_ids\n                    ).all()\n                )\n\n    def test_embedding_layer_gradient_with_sparse_inputs(self):\n        output_dim = 8\n        embedding_size = 16\n        combiners = [""sum"", ""mean"", ""sqrtn""]\n        indices = [[0, 0], [1, 1], [1, 2], [2, 1], [3, 1], [3, 3], [3, 5]]\n        values = [1, 1, 3, 2, 0, 2, 6]\n        dense_shape = [4, 6]\n        inputs = tf.SparseTensor(\n            indices=indices, values=values, dense_shape=dense_shape\n        )\n        inputs = tf.dtypes.cast(inputs, tf.int64)\n\n        multiply_values = np.ndarray(shape=(4, output_dim), dtype=np.float32)\n        for i in range(4):\n            multiply_values[i].fill(i)\n        multiply_tensor = tf.convert_to_tensor(multiply_values)\n\n        # grads for unique ids (1, 3, 2, 0, 6)\n        sum_correct_grads = [0 + 1, 1, 2 + 3, 3, 3]\n        mean_correct_grads = [1 / 2.0, 1 / 2.0, 2 + 3 / 3.0, 3 / 3.0, 3 / 3.0]\n        sqrtn_correct_grads = [\n            1 / np.sqrt(2.0),\n            1 / np.sqrt(2.0),\n            2 + 3 / np.sqrt(3.0),\n            3 / np.sqrt(3.0),\n            3 / np.sqrt(3.0),\n        ]\n        correct_grads = {\n            ""sum"": sum_correct_grads,\n            ""mean"": mean_correct_grads,\n            ""sqrtn"": sqrtn_correct_grads,\n        }\n\n        for combiner in combiners:\n            layer = create_embedding_layer(\n                embedding_size, output_dim, combiner=combiner\n            )\n            call_fns = [tf.function(layer.call), layer.call]\n            for call_fn in call_fns:\n                with tf.GradientTape() as tape:\n                    layer.set_tape(tape)\n                    output = call_fn(inputs)\n                    output = output * multiply_tensor\n                batch_embedding = layer.embedding_and_ids[0].batch_embedding\n                grads = tape.gradient(output, batch_embedding)\n                grads = grads.numpy()\n                place = 8 if combiner == ""sum"" else 5\n                for n, v in enumerate(correct_grads[combiner]):\n                    self.assertAlmostEqual(grads[n][0], v, place)\n                self.assertTrue(\n                    np.isclose(\n                        layer.embedding_and_ids[0].batch_ids.numpy(),\n                        np.array([1, 3, 2, 0, 6]),\n                    ).all()\n                )\n                layer.reset()\n\n        # Test when an embedding layer is called more than once.\n        # When an embedding layer is called more than once, `tf.function` is\n        # not supported.\n        correct_ids = [1, 3, 2, 0, 6]\n        for combiner in combiners:\n            layer = create_embedding_layer(\n                embedding_size, output_dim, combiner=combiner\n            )\n            with tf.GradientTape() as tape:\n                layer.set_tape(tape)\n                _ = layer.call(inputs)\n                _ = layer.call(inputs)\n                self.assertTrue(len(layer.embedding_and_ids) == 2)\n                for i in range(2):\n                    self.assertTrue(\n                        (\n                            layer.embedding_and_ids[i].batch_ids.numpy()\n                            == correct_ids\n                        ).all()\n                    )\n\n    def test_embedding_layer_with_none_shape_input(self):\n        output_dim = 8\n        embedding_size = 16\n        layer = create_embedding_layer(embedding_size, output_dim)\n        input = tf.keras.layers.Input(shape=(None,))\n        embedding_output = layer(input)\n        embedding_output_shape = embedding_output.get_shape().as_list()\n        self.assertEqual(embedding_output_shape, [None, None, 8])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/local_executor_test.py,0,"b'import os\nimport tempfile\nimport unittest\nfrom collections import namedtuple\n\nimport numpy as np\n\nfrom elasticdl.python.elasticdl.local_executor import LocalExecutor\nfrom elasticdl.python.tests.test_utils import create_iris_csv_file\n\n_MockedTask = namedtuple(""Task"", [""start"", ""end"", ""shard_name""])\n\n_model_zoo_path = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), ""../../../model_zoo""\n)\n\n_iris_dnn_def = ""odps_iris_dnn_model.odps_iris_dnn_model.custom_model""\n\n\nclass LocalExecutorArgs(object):\n    def __init__(\n        self,\n        num_epochs,\n        minibatch_size,\n        training_data,\n        validation_data,\n        evaluation_steps,\n        model_zoo,\n        model_def,\n        dataset_fn,\n        loss,\n        optimizer,\n        eval_metrics_fn,\n        model_params=None,\n        prediction_outputs_processor=None,\n        envs=None,\n        data_reader_params=None,\n        num_minibatches_per_task=None,\n        custom_data_reader="""",\n        output="""",\n        callbacks="""",\n    ):\n        self.num_epochs = num_epochs\n        self.minibatch_size = minibatch_size\n        self.training_data = training_data\n        self.validation_data = validation_data\n        self.evaluation_steps = evaluation_steps\n        self.model_zoo = model_zoo\n        self.model_def = model_def\n        self.dataset_fn = dataset_fn\n        self.loss = loss\n        self.optimizer = optimizer\n        self.eval_metrics_fn = eval_metrics_fn\n        self.model_params = model_params\n        self.prediction_outputs_processor = prediction_outputs_processor\n        self.envs = envs\n        self.data_reader_params = data_reader_params\n        self.num_minibatches_per_task = num_minibatches_per_task\n        self.custom_data_reader = custom_data_reader\n        self.output = output\n        self.callbacks = callbacks\n\n\nclass LocalExectorTest(unittest.TestCase):\n    def test_train_model_local(self):\n        with tempfile.TemporaryDirectory() as temp_dir_name:\n            num_records = 1000\n            columns = [\n                ""sepal_length"",\n                ""sepal_width"",\n                ""petal_length"",\n                ""petal_width"",\n                ""class"",\n            ]\n            training_data = create_iris_csv_file(\n                size=num_records, columns=columns, temp_dir=temp_dir_name\n            )\n            validation_data = create_iris_csv_file(\n                size=num_records, columns=columns, temp_dir=temp_dir_name\n            )\n\n            data_reader_params = (\n                \'columns=[""sepal_length"", ""sepal_width"", ""petal_length"",\'\n                \'""petal_width"", ""class""]; sep="",""\'\n            )\n            args = LocalExecutorArgs(\n                num_epochs=1,\n                minibatch_size=32,\n                training_data=training_data,\n                validation_data=validation_data,\n                evaluation_steps=10,\n                model_zoo=_model_zoo_path,\n                model_def=_iris_dnn_def,\n                dataset_fn=""dataset_fn"",\n                loss=""loss"",\n                optimizer=""optimizer"",\n                eval_metrics_fn=""eval_metrics_fn"",\n                model_params="""",\n                prediction_outputs_processor=""PredictionOutputsProcessor"",\n                data_reader_params=data_reader_params,\n                num_minibatches_per_task=5,\n                callbacks=""callbacks"",\n            )\n            local_executor = LocalExecutor(args)\n            train_tasks = local_executor._gen_tasks(\n                local_executor.training_data\n            )\n            validation_tasks = local_executor._gen_tasks(\n                local_executor.validation_data\n            )\n\n            train_dataset = local_executor._get_dataset(train_tasks)\n            features, labels = next(iter(train_dataset))\n            loss = local_executor._train(features, labels)\n            self.assertEqual(type(loss.numpy()), np.float32)\n\n            validation_dataset = local_executor._get_dataset(validation_tasks)\n            metrics = local_executor._evaluate(validation_dataset)\n            self.assertEqual(list(metrics.keys()), [""accuracy""])\n'"
elasticdl/python/tests/model_handler_test.py,35,"b'import os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.feature_column import feature_column_v2 as fc_lib\n\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.common.model_handler import ModelHandler\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.elasticdl.feature_column.feature_column import (\n    EmbeddingColumn,\n)\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\nfrom elasticdl.python.ps.embedding_table import EmbeddingTable\nfrom elasticdl.python.ps.parameters import Parameters\nfrom elasticdl_preprocessing.layers import SparseEmbedding\n\nEMBEDDING_INPUT_DIM = 300000\n\n\nclass CustomModel(tf.keras.models.Model):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.embedding = tf.keras.layers.Embedding(EMBEDDING_INPUT_DIM, 2)\n        self.dense = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        embedding = self.embedding(inputs)\n        output = self.dense(embedding)\n        return output\n\n\ndef custom_model_with_embedding_layer():\n    inputs = tf.keras.layers.Input(shape=(4,), name=""x"")\n    embedding = tf.keras.layers.Embedding(EMBEDDING_INPUT_DIM, 2)(inputs)\n    outputs = tf.keras.layers.Dense(1)(embedding)\n    return tf.keras.models.Model(inputs, outputs)\n\n\ndef custom_model_with_embedding_column():\n    inputs = {\n        ""age"": tf.keras.layers.Input(shape=(1,), name=""age"", dtype=tf.float32),\n        ""user_id"": tf.keras.layers.Input(\n            shape=(1,), name=""user_id"", dtype=tf.string\n        ),\n    }\n    age = tf.feature_column.numeric_column(""age"", dtype=tf.int64)\n    user_id_embedding = tf.feature_column.embedding_column(\n        tf.feature_column.categorical_column_with_hash_bucket(\n            ""user_id"", hash_bucket_size=EMBEDDING_INPUT_DIM\n        ),\n        dimension=2,\n    )\n    feature_columns = [age, user_id_embedding]\n    dense = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)(\n        inputs\n    )\n    output = tf.keras.layers.Dense(1)(dense)\n    return tf.keras.models.Model(inputs, output)\n\n\ndef custom_model_with_sparse_embedding():\n    sparse_input = tf.keras.layers.Input(\n        shape=(4,), dtype=""int64"", sparse=True, name=""sparse_feature""\n    )\n    embedding = SparseEmbedding(\n        EMBEDDING_INPUT_DIM, 2, combiner=""sum"", name=""embedding""\n    )(sparse_input)\n    outputs = tf.keras.layers.Dense(1)(embedding)\n    return tf.keras.models.Model(sparse_input, outputs)\n\n\ndef custom_sequential_model(feature_columns):\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.DenseFeatures(feature_columns=feature_columns),\n            tf.keras.layers.Dense(10, activation=""relu""),\n            tf.keras.layers.Dense(1, activation=""sigmoid""),\n        ]\n    )\n    return model\n\n\ndef feature_columns_fn():\n    age = tf.feature_column.numeric_column(""age"", dtype=tf.int64)\n    education = tf.feature_column.categorical_column_with_hash_bucket(\n        ""education"", hash_bucket_size=4\n    )\n    education_one_hot = tf.feature_column.indicator_column(education)\n    return [age, education_one_hot]\n\n\ndef _get_dataset():\n    y_labels = np.array([1, 1, 0, 0, 1])\n    x_data = {\n        ""age"": [14, 56, 78, 38, 80],\n        ""education"": [\n            ""Bachelors"",\n            ""Master"",\n            ""Some-college"",\n            ""Bachelors"",\n            ""Master"",\n        ],\n    }\n    dataset = tf.data.Dataset.from_tensor_slices((dict(x_data), y_labels))\n    dataset = dataset.shuffle(len(x_data)).batch(4)\n    return dataset\n\n\ndef _mock_model_trained_params(model):\n    trained_params = {}\n    for var in model.trainable_variables:\n        trained_params[var.name] = np.ones(\n            var.shape.as_list(), dtype=""float32""\n        )\n    return trained_params\n\n\nclass DefaultModelHandlerTest(unittest.TestCase):\n    def setUp(self):\n        self.model_handler = ModelHandler.get_model_handler()\n\n    def test_get_model_to_ps(self):\n        model_inst = custom_model_with_embedding_layer()\n        model_inst = self.model_handler.get_model_to_train(model_inst)\n        self.assertEqual(type(model_inst.layers[1]), tf.keras.layers.Embedding)\n\n    def test_get_model_to_export(self):\n        dataset = _get_dataset()\n        feature_columns = feature_columns_fn()\n        model_inst = custom_sequential_model(feature_columns)\n        model_inst._build_model_with_inputs(inputs=dataset, targets=None)\n        model_inst = self.model_handler.get_model_to_export(\n            model_inst, dataset\n        )\n        self.assertEqual(list(model_inst.inputs.keys()), [""age"", ""education""])\n        self.assertEqual(len(model_inst.outputs), 1)\n\n        mock_params = _mock_model_trained_params(model_inst)\n        for var in model_inst.trainable_variables:\n            var.assign(mock_params[var.name])\n\n        test_data = {\n            ""age"": [14, 56, 78, 38, 80],\n            ""education"": [\n                ""Bachelors"",\n                ""Master"",\n                ""Some-college"",\n                ""Bachelors"",\n                ""Master"",\n            ],\n        }\n        result = model_inst.call(test_data).numpy()\n        self.assertEqual(result.tolist(), np.ones((5, 1)).tolist())\n\n\nclass ParameterSeverModelHandlerTest(unittest.TestCase):\n    def setUp(self):\n        tf.keras.backend.clear_session()\n        self.model_handler = ModelHandler.get_model_handler(\n            distribution_strategy=DistributionStrategy.PARAMETER_SERVER,\n            checkpoint_dir="""",\n        )\n\n    def _mock_model_parameters(self, model):\n        params = Parameters()\n        for weight in model.trainable_variables:\n            if ""embedding"" in weight.name:\n                embedding_table = EmbeddingTable(\n                    name=weight.name,\n                    dim=weight.shape[1],\n                    initializer=""RandomUniform"",\n                )\n                embedding_table.set(\n                    np.arange(weight.shape[0]), np.ones(weight.shape)\n                )\n                params.embedding_params[weight.name] = embedding_table\n            else:\n                params.non_embedding_params[weight.name] = tf.ones(\n                    weight.shape\n                )\n        params.version = 100\n        return params\n\n    def _mock_model_weights_and_save_checkpoint(self, model):\n        ckpt_dir = self.model_handler._checkpoint_dir\n        checkpoint_saver = CheckpointSaver(ckpt_dir, 0, 0, False)\n        params = self._mock_model_parameters(model)\n        model_pb = params.to_model_pb()\n        checkpoint_saver.save(100, model_pb, False)\n\n    def test_get_model_with_embedding_layer_to_train(self):\n        model_inst = custom_model_with_embedding_layer()\n        model_inst = self.model_handler.get_model_to_train(model_inst)\n        self.assertEqual(type(model_inst.layers[1]), Embedding)\n\n    def test_get_model_with_embedding_column_to_train(self):\n        model_inst = custom_model_with_embedding_column()\n        dense_features_layer_index = 2\n        embedding_column_index = 1\n        self.assertEqual(\n            type(\n                model_inst.layers[dense_features_layer_index]._feature_columns[\n                    embedding_column_index\n                ]\n            ),\n            fc_lib.EmbeddingColumn,\n        )\n        model_inst = self.model_handler.get_model_to_train(model_inst)\n        self.assertEqual(\n            type(\n                model_inst.layers[dense_features_layer_index]._feature_columns[\n                    embedding_column_index\n                ]\n            ),\n            EmbeddingColumn,\n        )\n\n    def test_get_model_with_embedding_column_to_export(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            self.model_handler._checkpoint_dir = os.path.join(\n                temp_dir, ""test_export""\n            )\n            model_inst = custom_model_with_embedding_column()\n            self._mock_model_weights_and_save_checkpoint(model_inst)\n\n            model_inst = self.model_handler.get_model_to_train(model_inst)\n            export_model = self.model_handler.get_model_to_export(\n                model_inst, dataset=None\n            )\n            result = export_model.call(\n                {""age"": tf.constant([[1]]), ""user_id"": tf.constant([[""134""]])}\n            ).numpy()\n            self.assertEqual(result[0][0], 4.0)\n\n    def test_get_model_to_export(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            self.model_handler._checkpoint_dir = os.path.join(\n                temp_dir, ""test_export""\n            )\n            model_inst = custom_model_with_embedding_layer()\n            train_model = self.model_handler.get_model_to_train(model_inst)\n\n            self._mock_model_weights_and_save_checkpoint(model_inst)\n            export_model = self.model_handler.get_model_to_export(\n                train_model, dataset=None\n            )\n\n            test_data = tf.constant([0])\n            result = export_model.call(test_data).numpy()\n            self.assertEqual(result[0][0], 3.0)\n\n    def test_get_subclass_model_to_export(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            self.model_handler._checkpoint_dir = os.path.join(\n                temp_dir, ""test_export""\n            )\n\n            def _get_dataset():\n                dataset = tf.data.Dataset.from_tensor_slices(\n                    np.random.randint(0, 10, (10, 4))\n                )\n                dataset = dataset.batch(2)\n                return dataset\n\n            model_inst = CustomModel()\n            dataset = _get_dataset()\n            model_inst._build_model_with_inputs(inputs=dataset, targets=None)\n            self._mock_model_weights_and_save_checkpoint(model_inst)\n\n            model_inst.inputs = None  # Reset model inputs\n            train_model = self.model_handler.get_model_to_train(model_inst)\n            self.assertEqual(type(train_model.embedding), Embedding)\n\n            export_model = self.model_handler.get_model_to_export(\n                train_model, dataset=dataset\n            )\n\n            test_data = tf.constant([0])\n            result = export_model.call(test_data).numpy()\n            self.assertEqual(result[0][0], 3.0)\n\n    def test_get_model_with_sparse_to_train(self):\n        model_inst = custom_model_with_sparse_embedding()\n        model_inst = self.model_handler.get_model_to_train(model_inst)\n        self.assertEqual(type(model_inst.layers[1]), Embedding)\n\n    def test_get_model_with_sparse_to_export(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            self.model_handler._checkpoint_dir = os.path.join(\n                temp_dir, ""test_export""\n            )\n            model_inst = custom_model_with_sparse_embedding()\n            train_model = self.model_handler.get_model_to_train(model_inst)\n\n            self._mock_model_weights_and_save_checkpoint(model_inst)\n            # Model handler will restore model parameters from the checkpoint\n            # directory and assign parameters to train_model.\n            export_model = self.model_handler.get_model_to_export(\n                train_model, dataset=None\n            )\n            test_data = tf.SparseTensor(\n                indices=[[0, 0]], values=[0], dense_shape=(1, 1)\n            )\n            result = export_model.call(test_data).numpy()\n\n            # The embedding table in checkpoint file is\n            # [[1.0, 1.0], [1.0, 1.0], [1.0,1.0], [1.0, 1.0]], weights in the\n            # dense layer is [[1.0],[1.0]], bias is [1.0]. So the result\n            # is 3.0.\n            self.assertEqual(result[0][0], 3.0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/model_utils_test.py,2,"b'import os\nimport unittest\n\nimport tensorflow as tf\n\nfrom elasticdl.python.common.model_utils import (\n    _get_spec_value,\n    get_dict_from_params_str,\n    get_model_spec,\n    get_module_file_path,\n    get_optimizer_info,\n)\n\n_model_zoo_path = os.path.dirname(os.path.realpath(__file__))\n\n\nclass ModelHelperTest(unittest.TestCase):\n    def test_get_model_spec(self):\n        (\n            model,\n            dataset_fn,\n            loss,\n            optimizer,\n            eval_metrics_fn,\n            prediction_outputs_processor,\n            custom_data_reader,\n            callback_list,\n        ) = get_model_spec(\n            model_zoo=_model_zoo_path,\n            model_def=""test_module.custom_model"",\n            dataset_fn=""dataset_fn"",\n            loss=""loss"",\n            optimizer=""optimizer"",\n            eval_metrics_fn=""eval_metrics_fn"",\n            model_params="""",\n            prediction_outputs_processor=""PredictionOutputsProcessor"",\n            custom_data_reader=""custom_data_reader"",\n            callbacks=""callbacks"",\n        )\n\n        self.assertTrue(model is not None)\n        self.assertTrue(dataset_fn is not None)\n        self.assertTrue(loss is not None)\n        self.assertTrue(optimizer is not None)\n        self.assertTrue(eval_metrics_fn is not None)\n        self.assertTrue(prediction_outputs_processor is not None)\n        self.assertTrue(custom_data_reader is not None)\n        self.assertTrue(callback_list is not None)\n        self.assertEqual(len(callback_list.callbacks), 1)\n        self.assertRaisesRegex(\n            Exception,\n            ""Cannot find the custom model function/class ""\n            ""in model definition files"",\n            get_model_spec,\n            model_zoo=_model_zoo_path,\n            model_def=""test_module.undefined"",\n            dataset_fn=""dataset_fn"",\n            loss=""loss"",\n            optimizer=""optimizer"",\n            eval_metrics_fn=""eval_metrics_fn"",\n            model_params="""",\n            prediction_outputs_processor=""PredictionOutputsProcessor"",\n            custom_data_reader=""custom_data_reader"",\n            callbacks=""callbacks"",\n        )\n\n    def test_get_module_file_path(self):\n        self.assertEqual(\n            get_module_file_path(_model_zoo_path, ""test_module.custom_model""),\n            os.path.join(_model_zoo_path, ""test_module.py""),\n        )\n\n    def test_get_spec_value(self):\n        self.assertTrue(\n            _get_spec_value(\n                ""custom_model"", _model_zoo_path, {""custom_model"": 1}\n            )\n            is not None\n        )\n        self.assertTrue(\n            _get_spec_value(""test_module.custom_model"", _model_zoo_path, {})\n            is not None\n        )\n        self.assertTrue(\n            _get_spec_value(""test_module.unknown_model"", _model_zoo_path, {})\n            is None\n        )\n        self.assertRaisesRegex(\n            Exception,\n            ""Missing required spec key unknown_model ""\n            ""in the module: test_module.unknown_model"",\n            _get_spec_value,\n            ""test_module.unknown_model"",\n            _model_zoo_path,\n            {},\n            True,\n        )\n\n    def test_get_dict_from_params_str(self):\n        self.assertEqual(\n            get_dict_from_params_str(\'ls=[""a"", ""b""]\'), {""ls"": [""a"", ""b""]}\n        )\n        self.assertEqual(\n            get_dict_from_params_str(\'ls=[""a"", ""b""]; d={""a"": 3}\'),\n            {""ls"": [""a"", ""b""], ""d"": {""a"": 3}},\n        )\n        self.assertEqual(\n            get_dict_from_params_str(\'ls=[""a"", ""b""];partition=dt=20190011\'),\n            {""ls"": [""a"", ""b""], ""partition"": ""dt=20190011""},\n        )\n        self.assertEqual(get_dict_from_params_str(""""), {})\n\n    def test_get_optimizer_info(self):\n        learning_rate = 0.1\n        momentum = 0.0\n        nesterov = False\n        expected_args = (\n            ""learning_rate=""\n            + str(learning_rate)\n            + "";momentum=""\n            + str(momentum)\n            + "";nesterov=False;""\n        )\n        opt = tf.keras.optimizers.SGD(\n            learning_rate=learning_rate, momentum=momentum, nesterov=nesterov\n        )\n        opt_type, opt_args = get_optimizer_info(opt)\n        self.assertEqual(opt_type, ""SGD"")\n        self.assertEqual(opt_args, expected_args)\n\n        beta_1 = 0.8\n        beta_2 = 0.6\n        epsilon = 1e-08\n        amsgrad = False\n        expected_args = (\n            ""learning_rate=""\n            + str(learning_rate)\n            + "";beta_1=""\n            + str(beta_1)\n            + "";beta_2=""\n            + str(beta_2)\n            + "";epsilon=""\n            + str(epsilon)\n            + "";amsgrad=False;""\n        )\n        opt = tf.keras.optimizers.Adam(\n            learning_rate=learning_rate,\n            beta_1=beta_1,\n            beta_2=beta_2,\n            epsilon=epsilon,\n            amsgrad=amsgrad,\n        )\n        opt_type, opt_args = get_optimizer_info(opt)\n        self.assertEqual(opt_type, ""Adam"")\n        self.assertEqual(opt_args, expected_args)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/odps_io_test.py,0,"b'import os\nimport random\nimport tempfile\nimport time\nimport unittest\n\nfrom odps import ODPS\n\nfrom elasticdl.python.common.constants import MaxComputeConfig\nfrom elasticdl.python.data.odps_io import (\n    ODPSReader,\n    ODPSWriter,\n    is_odps_configured,\n)\nfrom elasticdl.python.data.odps_recordio_conversion_utils import (\n    write_recordio_shards_from_iterator,\n)\nfrom elasticdl.python.tests.test_utils import create_iris_odps_table\n\n\n@unittest.skipIf(\n    not is_odps_configured(), ""ODPS environment is not configured"",\n)\nclass ODPSIOTest(unittest.TestCase):\n    def setUp(self):\n        self._project = os.environ[MaxComputeConfig.PROJECT_NAME]\n        self._access_id = os.environ[MaxComputeConfig.ACCESS_ID]\n        self._access_key = os.environ[MaxComputeConfig.ACCESS_KEY]\n        self._endpoint = os.environ.get(MaxComputeConfig.ENDPOINT)\n        self._test_read_table = ""test_odps_reader_%d_%d"" % (\n            int(time.time()),\n            random.randint(1, 101),\n        )\n        self._test_write_table = ""test_odps_writer_%d_%d"" % (\n            int(time.time()),\n            random.randint(1, 101),\n        )\n        self._odps_client = ODPS(\n            self._access_id, self._access_key, self._project, self._endpoint\n        )\n        create_iris_odps_table(\n            self._odps_client, self._project, self._test_read_table\n        )\n\n    def test_parallel_read(self):\n        def transform(record):\n            return float(record[0]) + 1\n\n        start = 0\n        end = 100\n        shard_size = (end - start) // 4\n\n        pd = ODPSReader(\n            access_id=self._access_id,\n            access_key=self._access_key,\n            project=self._project,\n            endpoint=self._endpoint,\n            table=self._test_read_table,\n            num_processes=2,\n            transform_fn=transform,\n        )\n\n        results = []\n        pd.reset((start, end - start), shard_size)\n        shard_count = pd.get_shards_count()\n        for i in range(shard_count):\n            records = pd.get_records()\n            for record in records:\n                results.append(record)\n        pd.stop()\n\n        self.assertEqual(len(results), 100)\n\n    def test_read_to_iterator(self):\n        reader = ODPSReader(\n            self._project,\n            self._access_id,\n            self._access_key,\n            self._endpoint,\n            self._test_read_table,\n            None,\n            4,\n            None,\n        )\n        records_iter = reader.to_iterator(1, 0, 50, 2, False, None)\n        records = list(records_iter)\n        self.assertEqual(\n            len(records), 6, ""Unexpected number of batches: %d"" % len(records)\n        )\n        flattened_records = [record for batch in records for record in batch]\n        self.assertEqual(\n            len(flattened_records),\n            220,\n            ""Unexpected number of total records: %d"" % len(flattened_records),\n        )\n\n    def test_write_odps_to_recordio_shards_from_iterator(self):\n        reader = ODPSReader(\n            self._project,\n            self._access_id,\n            self._access_key,\n            self._endpoint,\n            self._test_read_table,\n            None,\n            4,\n            None,\n        )\n        records_iter = reader.to_iterator(1, 0, 50, 2, False, None)\n        with tempfile.TemporaryDirectory() as output_dir:\n            write_recordio_shards_from_iterator(\n                records_iter,\n                [""f"" + str(i) for i in range(5)],\n                output_dir,\n                records_per_shard=50,\n            )\n            self.assertEqual(len(os.listdir(output_dir)), 5)\n\n    def test_write_from_iterator(self):\n        columns = [""num"", ""num2""]\n        column_types = [""bigint"", ""double""]\n\n        # If the table doesn\'t exist yet\n        writer = ODPSWriter(\n            self._project,\n            self._access_id,\n            self._access_key,\n            self._endpoint,\n            self._test_write_table,\n            columns,\n            column_types,\n        )\n        writer.from_iterator(iter([[1, 0.5], [2, 0.6]]), 2)\n        table = self._odps_client.get_table(\n            self._test_write_table, self._project\n        )\n        self.assertEqual(table.schema.names, columns)\n        self.assertEqual(table.schema.types, column_types)\n        self.assertEqual(table.to_df().count(), 1)\n\n        # If the table already exists\n        writer = ODPSWriter(\n            self._project,\n            self._access_id,\n            self._access_key,\n            self._endpoint,\n            self._test_write_table,\n        )\n        writer.from_iterator(iter([[1, 0.5], [2, 0.6]]), 2)\n        table = self._odps_client.get_table(\n            self._test_write_table, self._project\n        )\n        self.assertEqual(table.schema.names, columns)\n        self.assertEqual(table.schema.types, column_types)\n        self.assertEqual(table.to_df().count(), 2)\n\n    def tearDown(self):\n        self._odps_client.delete_table(\n            self._test_write_table, self._project, if_exists=True\n        )\n        self._odps_client.delete_table(\n            self._test_read_table, self._project, if_exists=True\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/odps_recordio_conversion_utils_test.py,0,"b'import inspect\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\n\nfrom elasticdl.python.data.odps_recordio_conversion_utils import (\n    _find_features_indices,\n    _maybe_encode_unicode_string,\n    _parse_row_to_example,\n    write_recordio_shards_from_iterator,\n)\n\n\nclass TestODPSRecordIOConversionUtils(unittest.TestCase):\n\n    row1 = [61, 5.65, ""Cash""]\n    row2 = [50, 1.2, ""Credit Card""]\n    float_features = [""float_col""]\n    bytes_features = [""bytes_col""]\n    int_features = [""int_col""]\n    features_list = [""int_col"", ""float_col"", ""bytes_col""]\n    feature_indices = _find_features_indices(\n        features_list, int_features, float_features, bytes_features\n    )\n\n    def test_parse_row_to_example(self):\n        expected = inspect.cleandoc(\n            """"""features {\n      feature {\n        key: ""bytes_col""\n        value {\n          bytes_list {\n            value: ""Cash""\n          }\n        }\n      }\n      feature {\n        key: ""float_col""\n        value {\n          float_list {\n            value: 5.650000095367432\n          }\n        }\n      }\n      feature {\n        key: ""int_col""\n        value {\n          int64_list {\n            value: 61\n          }\n        }\n      }\n    }\n    """"""\n        )\n\n        result = _parse_row_to_example(\n            np.array(self.row1, dtype=object),\n            self.features_list,\n            self.feature_indices,\n        )\n        self.assertEqual(str(result), expected + ""\\n"")\n\n        result = _parse_row_to_example(\n            self.row1, self.features_list, self.feature_indices\n        )\n        self.assertEqual(str(result), expected + ""\\n"")\n\n    def test_parse_row_to_example_unicode_mapping(self):\n        self.assertEqual(\n            _maybe_encode_unicode_string(u""some_unicode_string""),\n            b""some_unicode_string"",\n        )\n        self.assertEqual(\n            _maybe_encode_unicode_string(""some_string""), b""some_string""\n        )\n\n        _parse_row_to_example(\n            np.array([u""40"", u""2.5"", ""Chinese\xe4\xb8\xad\xe6\x96\x87""], dtype=object),\n            self.features_list,\n            self.feature_indices,\n        )\n\n    def test_write_recordio_shards_from_iterator(self):\n        features_list = [""Float1"", ""Float2"", ""Str1"", ""Int1""]\n        # Each batch contains single item\n        records_iter = iter(\n            [[8.0, 10.65, ""Cash"", 6], [7.5, 17.8, ""Credit Card"", 3]]\n        )\n        with tempfile.TemporaryDirectory() as output_dir:\n            write_recordio_shards_from_iterator(\n                records_iter, features_list, output_dir, records_per_shard=1\n            )\n            self.assertEqual(\n                sorted(os.listdir(output_dir)), [""data-00000"", ""data-00001""]\n            )\n\n        # Each batch contains multiple items\n        records_iter = iter(\n            [\n                [[1.0, 10.65, ""Cash"", 6], [2.5, 17.8, ""Credit Card"", 3]],\n                [[3.0, 10.65, ""Cash"", 6], [4.5, 17.8, ""Credit Card"", 3]],\n            ]\n        )\n        with tempfile.TemporaryDirectory() as output_dir:\n            write_recordio_shards_from_iterator(\n                records_iter, features_list, output_dir, records_per_shard=1\n            )\n            self.assertEqual(len(os.listdir(output_dir)), 4)\n\n        # Each batch contains multiple items with fixed length\n        records_iter = iter(\n            [\n                [[1.0, 10.65, ""Cash"", 6], [2.5, 17.8, ""Credit Card"", 3]],\n                [[3.0, 10.65, ""Cash"", 6], [4.5, 17.8, ""Credit Card"", 3]],\n            ]\n        )\n        with tempfile.TemporaryDirectory() as output_dir:\n            write_recordio_shards_from_iterator(\n                records_iter, features_list, output_dir, records_per_shard=2\n            )\n            self.assertEqual(len(os.listdir(output_dir)), 2)\n\n        # Each batch contains multiple items with variable length\n        records_iter = iter(\n            [\n                [[1.0, 10.65, ""Cash"", 6], [2.5, 17.8, ""Credit Card"", 3]],\n                [[3.0, 10.65, ""Cash"", 6], [4.5, 17.8, ""Credit Card"", 3]],\n                [[3.0, 10.65, ""Cash"", 6]],\n            ]\n        )\n        with tempfile.TemporaryDirectory() as output_dir:\n            write_recordio_shards_from_iterator(\n                records_iter, features_list, output_dir, records_per_shard=2\n            )\n            self.assertEqual(len(os.listdir(output_dir)), 3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/optimizer_wrapper_test.py,24,"b'import copy\nimport os\nimport random\nimport time\nimport unittest\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import (\n    SGD,\n    Adadelta,\n    Adagrad,\n    Adam,\n    Adamax,\n    Ftrl,\n    Nadam,\n    RMSprop,\n)\n\nfrom elasticdl.python.common.model_utils import (\n    find_layer,\n    get_module_file_path,\n    get_non_embedding_trainable_vars,\n    load_module,\n)\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\nfrom elasticdl.python.ps.embedding_table import (\n    EmbeddingTable,\n    get_slot_table_name,\n)\nfrom elasticdl.python.ps.optimizer_wrapper import OptimizerWrapper\nfrom elasticdl.python.ps.parameters import Parameters\n\n\ndef _prepare_random_data(\n    iters_per_epoch,\n    batch_size,\n    input_length,\n    input_dim,\n    is_sparse,\n    random_seed,\n):\n    """"""\n    Generate data for training. `is_sparse=False` means we require that all\n    embedding ids appear in every batch data.\n    """"""\n    random.seed(random_seed)\n    X, Y = [], []\n    if not is_sparse:\n        assert input_length > input_dim, ""`input_length` should be larger ""\n        ""than `input_dim` when dense data required.""\n\n    def _gen_single_data(choices, input_length, is_sparse):\n        if not is_sparse:\n            data = copy.copy(choices)\n        else:\n            data = []\n        while len(data) < input_length:\n            data.append(random.choice(choices))\n        random.shuffle(data)\n        return data\n\n    for i in range(iters_per_epoch):\n        f1_batch, f2_batch, f3_batch, y_batch = [], [], [], []\n        if is_sparse:\n            choices = random.choices(range(input_dim), k=input_dim // 2)\n        else:\n            choices = list(range(input_dim))\n        for j in range(batch_size):\n            f1_batch.append(_gen_single_data(choices, input_length, is_sparse))\n            f2_batch.append(_gen_single_data(choices, input_length, is_sparse))\n            f3_batch.append(_gen_single_data(choices, input_length, is_sparse))\n            y_batch.append(random.randint(0, 1))\n        X.append(\n            {\n                ""f1"": np.array(f1_batch),\n                ""f2"": np.array(f2_batch),\n                ""f3"": np.array(f3_batch),\n            }\n        )\n        Y.append(y_batch)\n    return X, Y\n\n\ndef _train(model, optimizer, X, Y, loss_fn, random_seed):\n    """"""Train model with TensorFlow optimizer.""""""\n    tf.random.set_seed(random_seed)\n    for features, labels in zip(X, Y):\n        with tf.GradientTape() as tape:\n            outputs = model.call(features)\n            loss = loss_fn(outputs, labels)\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\ndef _train_edl_embedding_with_optimizer_wrapper(\n    model, opt_keras, X, Y, loss_fn, params, random_seed\n):\n    """"""Train model with optimizer wrapper.""""""\n    tf.random.set_seed(random_seed)\n    opt_wrapper = OptimizerWrapper(\n        opt_keras,\n        lookup_embedding_func=params.get_embedding_param,\n        update_embedding_func=params.set_embedding_param,\n    )\n\n    embed_layers = find_layer(model, Embedding)\n\n    # initialize slot params\n    params.create_slot_params(\n        opt_wrapper.allowed_slot_names, opt_wrapper.slot_initial_value\n    )\n\n    # initialize ElasticDL embedding layer\n    for layer in embed_layers:\n        layer.set_lookup_embedding_func(params.get_embedding_param)\n\n    # training process\n    for train_iter, (features, labels) in enumerate(zip(X, Y)):\n        with tf.GradientTape() as tape:\n            for layer in embed_layers:\n                layer.set_tape(tape)\n            outputs = model.call(features)\n            loss = loss_fn(outputs, labels)\n\n        # Need to get non-embedding variables inside for loop because model\n        # creates variables after the first time `model.call` is called\n        if not train_iter:\n            non_embed_vars = get_non_embedding_trainable_vars(\n                model, embed_layers\n            )\n        embed_items = []\n        for layer in embed_layers:\n            embed_items.extend(\n                [\n                    (bet, layer.embedding_weight_name, ids)\n                    for bet, ids in layer.embedding_and_ids\n                ]\n            )\n\n        grads = tape.gradient(\n            loss, non_embed_vars + [var for var, _, _ in embed_items]\n        )\n\n        # TODO: do not need to merge gradient from the same embedding layer\n        # after `optimizer_wrapper` support grads_and_vars with duplicated\n        # layer name\n        non_embed_vars_n = len(non_embed_vars)\n        non_embed_grads = grads[:non_embed_vars_n]\n        embed_grads_dict = {}\n        for (_, layer_name, ids), grad in zip(\n            embed_items, grads[non_embed_vars_n:]\n        ):\n            if layer_name in embed_grads_dict:\n                merged_grads = embed_grads_dict[layer_name]\n                embed_grads_dict[layer_name] = tf.IndexedSlices(\n                    tf.concat([merged_grads.values, grad.values], axis=0),\n                    tf.concat([merged_grads.indices, ids], axis=0),\n                )\n            else:\n                embed_grads_dict[layer_name] = tf.IndexedSlices(\n                    grad.values, ids\n                )\n\n        opt_wrapper.apply_gradients(\n            list(zip(non_embed_grads, non_embed_vars))\n            + [\n                (grad, layer_name)\n                for layer_name, grad in embed_grads_dict.items()\n            ]\n        )\n\n        for layer in embed_layers:\n            layer.reset()\n\n\nclass OptimizerWrapperTest(unittest.TestCase):\n    def _compare_slot_names(self, opt, expected):\n        tmp = OptimizerWrapper(opt)\n        self.assertTrue(sorted(tmp.allowed_slot_names) == sorted(expected))\n\n    def test_allowed_slot_names(self):\n        opt_and_slots_pairs = [\n            (SGD(), []),\n            (SGD(momentum=0.2), [""momentum""]),\n            (Adam(), [""m"", ""v""]),\n            (Adam(amsgrad=True), [""m"", ""v"", ""vhat""]),\n            (Adamax(), [""m"", ""v""]),\n            (Nadam(), [""m"", ""v""]),\n            (Adadelta(), [""accum_grad"", ""accum_var""]),\n            (Adagrad(), [""accumulator""]),\n            (Ftrl(), [""accumulator"", ""linear""]),\n            (RMSprop(), [""rms""]),\n            (RMSprop(momentum=0.2), [""rms"", ""momentum""]),\n            (RMSprop(centered=True), [""rms"", ""mg""]),\n            (RMSprop(momentum=0.2, centered=True), [""rms"", ""momentum"", ""mg""]),\n        ]\n        for opt, expected_slots in opt_and_slots_pairs:\n            self._compare_slot_names(opt, expected_slots)\n\n    def test_set_slot_to_optimizer(self):\n        embed_name = ""test_emb""\n        indices = np.ndarray([2], dtype=np.int32)\n        embed_values = np.ndarray([2, 2], dtype=np.float32)\n        slot_values = {\n            ""m"": np.ndarray([2, 2], dtype=np.float32),\n            ""v"": np.ndarray([2, 2], dtype=np.float32),\n        }\n        params = Parameters()\n        params.embedding_params[embed_name] = EmbeddingTable(embed_name, 8)\n        for slot in [""m"", ""v""]:\n            slot_table_name = get_slot_table_name(embed_name, slot)\n            params.embedding_params[slot_table_name] = EmbeddingTable(\n                slot_table_name, 2, ""0.0"", True\n            )\n\n        opt = Adam()\n        opt_wrapper = OptimizerWrapper(opt, None, params.get_embedding_param)\n        opt_wrapper._init_thread_local()\n\n        opt_wrapper._tls._unique_ids_all_layers[embed_name] = indices\n        opt_wrapper._create_embedding_variable(embed_name, embed_values)\n        opt_wrapper._get_slot_and_set_to_optimizer(embed_name)\n\n        self.assertEqual(len(opt._slots), 1)\n        opt_slots = list(opt._slots.values())[0]\n        self.assertEqual(sorted(opt_slots.keys()), [""m"", ""v""])\n        for name in [""m"", ""v""]:\n            self.assertTrue(\n                np.allclose(opt_slots[name].numpy(), slot_values[name])\n            )\n\n    def test_update_embedding_param(self):\n        params = Parameters()\n        for name in [""test_1"", ""test_2""]:\n            params.embedding_params[name] = EmbeddingTable(name, 8)\n            slot_key = get_slot_table_name(name, ""momentum"")\n            params.embedding_params[slot_key] = EmbeddingTable(\n                slot_key, 8, ""0.0"", True\n            )\n\n        indices = {\n            ""test_1"": np.array([1, 5]),\n            ""test_2"": np.array([10]),\n        }\n        embed_vars = {\n            ""test_1"": tf.Variable(np.random.rand(2, 8).astype(np.float32)),\n            ""test_2"": tf.Variable(np.random.rand(1, 8).astype(np.float32)),\n        }\n        slot_vars = {\n            ""test_1"": {\n                ""momentum"": tf.Variable(\n                    np.random.rand(2, 8).astype(np.float32)\n                )\n            },\n            ""test_2"": {\n                ""momentum"": tf.Variable(\n                    np.random.rand(1, 8).astype(np.float32)\n                )\n            },\n        }\n\n        opt = SGD(momentum=0.1)\n        opt_wrapper = OptimizerWrapper(\n            opt, None, None, params.set_embedding_param\n        )\n        opt_wrapper._tls._unique_ids_all_layers = indices\n        opt_wrapper._tls._embed_variables = embed_vars\n        opt_wrapper._tls._slot_variables = slot_vars\n        opt_wrapper._update_embedding_param()\n\n        for name in [""test_1"", ""test_2""]:\n            self.assertTrue(\n                np.allclose(\n                    embed_vars[name].numpy(),\n                    params.get_embedding_param(name, indices[name]),\n                )\n            )\n\n            slot = ""momentum""\n            slot_table_name = get_slot_table_name(name, slot)\n            self.assertTrue(\n                np.allclose(\n                    slot_vars[name][slot].numpy(),\n                    params.get_embedding_param(slot_table_name, indices[name]),\n                )\n            )\n\n    def test_delete_variables(self):\n        params = Parameters()\n        embed_layers = [""test_1"", ""test_2""]\n        slot_names = [""m"", ""v""]\n        dim = 8\n        for layer in embed_layers:\n            params.embedding_params[layer] = EmbeddingTable(layer, dim)\n            for slot in slot_names:\n                slot_key = get_slot_table_name(layer, slot)\n                params.embedding_params[slot_key] = EmbeddingTable(\n                    slot_key, dim, ""0.0"", True\n                )\n\n        opt = Adam()\n        opt_wrapper = OptimizerWrapper(\n            opt, None, params.get_embedding_param, params.set_embedding_param\n        )\n\n        opt_wrapper._init_thread_local()\n        for name in embed_layers:\n            opt_wrapper._tls._unique_ids_all_layers[name] = np.ndarray(\n                [2], np.int32\n            )\n            opt_wrapper._create_embedding_variable(\n                name, np.ndarray([2, dim], np.float32)\n            )\n            opt_wrapper._get_slot_and_set_to_optimizer(name)\n\n        self.assertTrue(len(opt._weights) == 4)\n        self.assertTrue(len(opt._slots) == 2)\n        for slot_dict in opt._slots.values():\n            self.assertTrue(len(slot_dict) == 2)\n\n        opt_wrapper._delete_slots_and_weights_in_optimizer()\n        self.assertTrue(len(opt._weights) == 0)\n        self.assertTrue(len(opt._slots) == 0)\n\n    def _random_init_model_weight(self, shapes, random_seed):\n        np.random.seed(random_seed)\n        return [np.random.rand(*shape).astype(np.float32) for shape in shapes]\n\n    def _test_correctness(self, optimizer_class, X, Y, seed, **opt_kwargs):\n        """"""Test the correctness of specific TensorFlow optimizer.""""""\n        _model_file = get_module_file_path(\n            os.path.dirname(os.path.realpath(__file__)),\n            ""embedding_test_module.KerasEmbeddingModel"",\n        )\n        model_module = load_module(_model_file).__dict__\n\n        # train model with TensorFlow optimizer\n        dim = 4\n        weights = self._random_init_model_weight(\n            [(4, dim), (4, dim), (72, 1), (1,)], seed\n        )\n        loss_fn = model_module[""loss""]\n        model1 = model_module[""KerasEmbeddingModel""](4, dim, weights)\n        opt1 = optimizer_class(**opt_kwargs)\n        _train(model1, opt1, X, Y, loss_fn, random_seed=seed)\n\n        model2 = model_module[""EdlEmbeddingModel""](dim, weights[2:])\n        opt2 = optimizer_class(**opt_kwargs)\n\n        embedding_weight_names = [\n            layer.embedding_weight_name\n            for layer in find_layer(model2, Embedding)\n        ]\n\n        # create Parameters object and initialize embedding vectors\n        params = Parameters()\n        for weight_name, embed_value in zip(\n            embedding_weight_names, weights[:2]\n        ):\n            embed_table = EmbeddingTable(weight_name, dim)\n            embed_table.set(range(len(embed_value)), embed_value)\n            params.embedding_params[weight_name] = embed_table\n\n        _train_edl_embedding_with_optimizer_wrapper(\n            model2, opt2, X, Y, loss_fn, params, random_seed=seed\n        )\n\n        # compare trained parameters\n        wrong_msg = (\n            ""The updated parameters of Optimizer Wrapper and TensorFlow ""\n            ""optimizer %s differ."" % opt1.get_config()[""name""]\n        )\n\n        for layer1, layer2 in zip(model1.layers, model2.layers):\n            if ""embedding"" in layer2.name:\n                w1 = layer1.weights[0].numpy()\n                w2 = params.get_embedding_param(\n                    layer2.embedding_weight_name, range(4)\n                )\n                self.assertTrue(np.isclose(w1, w2).all(), msg=wrong_msg)\n            else:\n                for w1, w2 in zip(layer1.weights, layer2.weights):\n                    self.assertTrue(\n                        np.isclose(w1.numpy(), w2.numpy()).all(), msg=wrong_msg\n                    )\n\n    def test_correctness(self):\n        """"""\n        Test the correctness of Optimizer Wrapper for all TensorFlow\n        optimizers.\n        """"""\n        optimizer_kargs = {\n            SGD: {""momentum"": 0.5},\n            Adadelta: {},\n            Adagrad: {},\n            Adamax: {},\n            Ftrl: {},\n            Adam: {""amsgrad"": True},\n            Nadam: {},\n            RMSprop: {""momentum"": 0.5, ""centered"": True},\n        }\n        learning_rate = 0.1\n        for key in optimizer_kargs.keys():\n            optimizer_kargs[key][""learning_rate""] = learning_rate\n\n        # TensorFlow implements these optimizers in densely updating style,\n        # i.e. update all parameters even if some parameters do not used in\n        # forward pass. `OptimizerWrapper` only supports sparsely updating\n        # style. So we test these optimizers using dense data for many\n        # iterations and sparse data for one iteration.\n        tf_dense_optimizers = [Adam, Nadam, RMSprop]\n\n        seed = 1\n        _prepare_data_common_args = {\n            ""batch_size"": 4,\n            ""input_length"": 6,\n            ""input_dim"": 4,\n            ""random_seed"": seed,\n        }\n        X_sparse, Y_sparse = _prepare_random_data(\n            iters_per_epoch=4, is_sparse=True, **_prepare_data_common_args\n        )\n        X_sparse_one_iter, Y_sparse_one_iter = _prepare_random_data(\n            iters_per_epoch=1, is_sparse=True, **_prepare_data_common_args\n        )\n        X_dense, Y_dense = _prepare_random_data(\n            iters_per_epoch=4, is_sparse=False, **_prepare_data_common_args\n        )\n\n        for opt, kargs in optimizer_kargs.items():\n            if opt not in tf_dense_optimizers:\n                self._test_correctness(opt, X_sparse, Y_sparse, seed, **kargs)\n            else:\n                self._test_correctness(opt, X_dense, Y_dense, seed, **kargs)\n                self._test_correctness(\n                    opt, X_sparse_one_iter, Y_sparse_one_iter, seed, **kargs\n                )\n\n    def _test_async_correctness(\n        self,\n        grads_and_vars_batches,\n        embed_values,\n        expected_non_embed_values,\n        expected_embed_values=None,\n    ):\n        """"""Checks the correctness of async OptimizerWrapper. This function\n        creates many threads and these threads call\n        `OptimizerWrapper.apply_gradients` simultaneously.\n\n        Args:\n            grads_and_vars_batches: A python list of `grads_and_vars`. Every\n                thread takes a `grads_and_vars` and calls `apply_gradients`.\n            embed_values: A python dictionary of\n                `(layer_name, embedding table)`.\n            expected_non_embed_values: A python list of expected non-embdding\n                values after applying gradients.\n            expected_embed_values: A python dictionary of expected embedding\n                values after applying gradients. None means no need to check\n                embedding values.\n        """"""\n        thread_num = len(grads_and_vars_batches)\n        input_dims = {}\n        embed_var_n = len(embed_values)\n        params = Parameters()\n        for layer, values in embed_values.items():\n            embed_dim = values.shape[1]\n            input_dims[layer] = values.shape[0]\n            embed_table = EmbeddingTable(layer, embed_dim)\n            embed_table.set(range(input_dims[layer]), values)\n            params.embedding_params[layer] = embed_table\n\n        opt = SGD(0.1)\n        opt_wrapper = OptimizerWrapper(\n            opt,\n            True,\n            lookup_embedding_func=params.get_embedding_param,\n            update_embedding_func=params.set_embedding_param,\n        )\n\n        # call optimizer_wrapper.apply_gradients asynchronously\n        def _apply_gradients(opt_wrapper, grads_and_vars):\n            # sleep 1s to wait that all threads are in this method call\n            time.sleep(1)\n            opt_wrapper.apply_gradients(grads_and_vars)\n\n        executor = ThreadPoolExecutor(max_workers=thread_num)\n        tasks = [\n            executor.submit(_apply_gradients, opt_wrapper, grads_and_vars)\n            for grads_and_vars in grads_and_vars_batches\n        ]\n        _ = [task.result() for task in tasks]\n\n        # check updated results of non-embedding variables\n        non_embed_vars = [\n            var for grad, var in grads_and_vars_batches[0][:-embed_var_n]\n        ]\n        for var, expected_value in zip(\n            non_embed_vars, expected_non_embed_values\n        ):\n            self.assertTrue(np.isclose(var.numpy(), expected_value).all())\n\n        # `expected_embed_values=None` means that no need to check\n        # embedding table\n        if not expected_embed_values:\n            return\n        # check updated results of embedding table\n        for layer, expected_values in expected_embed_values.items():\n            value = params.get_embedding_param(layer, range(input_dims[layer]))\n\n            self.assertTrue(\n                any(\n                    [\n                        np.isclose(value, expected).all()\n                        for expected in expected_values\n                    ]\n                )\n            )\n\n    def test_async_correctness(self):\n        """"""Tests the correctness of async updates in `OptimizerWrapper`.\n\n        Testing the correctness is not simple because OptimizerWrapper is not\n        thread-safe for embedding table. This test case lists all the possible\n        results when `thread_number=2` and test the correctness. This test case\n        also tests that OptimizerWrapper does not raise Error with a large\n        thread number(8).\n        """"""\n        max_thread_num = 8\n        input_dim = 4\n        output_dim = 3\n        non_embed_vars = [\n            tf.Variable([0.0] * output_dim),\n            tf.Variable([1.0] * output_dim),\n        ]\n        non_embed_vars_copy = copy.deepcopy(non_embed_vars)\n        non_embed_grads_batches = [\n            [\n                tf.constant([i + 1] * output_dim, dtype=tf.float32),\n                tf.constant([-i - 1] * output_dim, dtype=tf.float32),\n            ]\n            for i in range(max_thread_num)\n        ]\n        embed_shape = (input_dim, output_dim)\n        embed_value_count = output_dim * input_dim\n        embed_layers = [""embed_1"", ""embed_2""]\n        embed_values = {\n            embed_layers[0]: np.arange(\n                embed_value_count, dtype=np.float32\n            ).reshape(embed_shape),\n            embed_layers[1]: np.arange(\n                embed_value_count, dtype=np.float32\n            ).reshape(embed_shape),\n        }\n\n        embed_grads_batches = [\n            [\n                tf.IndexedSlices(\n                    tf.reshape(\n                        tf.constant([i + 1.0] * embed_value_count), embed_shape\n                    ),\n                    tf.constant(list(range(input_dim))),\n                ),\n                tf.IndexedSlices(\n                    tf.reshape(\n                        tf.constant([-i - 1.0] * embed_value_count),\n                        embed_shape,\n                    ),\n                    tf.constant(list(range(input_dim))),\n                ),\n            ]\n            for i in range(max_thread_num)\n        ]\n\n        # thread number = 2\n        expected_non_embed_values = [[-0.3, -0.3, -0.3], [1.3, 1.3, 1.3]]\n        expected_embed_values = {\n            embed_layers[0]: [\n                (np.arange(12) - 0.1).reshape(embed_shape),\n                (np.arange(12) - 0.2).reshape(embed_shape),\n                (np.arange(12) - 0.3).reshape(embed_shape),\n            ],\n            embed_layers[1]: [\n                (np.arange(12) + 0.1).reshape(embed_shape),\n                (np.arange(12) + 0.2).reshape(embed_shape),\n                (np.arange(12) + 0.3).reshape(embed_shape),\n            ],\n        }\n        grads_and_vars_batches = [\n            list(zip(non_embed_grads_batches[i], non_embed_vars))\n            + list(zip(embed_grads_batches[i], embed_layers))\n            for i in range(2)\n        ]\n\n        self._test_async_correctness(\n            grads_and_vars_batches,\n            embed_values,\n            expected_non_embed_values,\n            expected_embed_values,\n        )\n\n        # thread number = 8\n        grads_sum = max_thread_num * (max_thread_num + 1) / 2 / 10.0\n        expected_non_embed_values = [[-grads_sum] * 3, [1 + grads_sum] * 3]\n        grads_and_vars_batches = [\n            list(zip(non_embed_grads_batches[i], non_embed_vars_copy))\n            + list(zip(embed_grads_batches[i], embed_layers))\n            for i in range(max_thread_num)\n        ]\n        # Do not check updating results of embedding table when `thread_num>2`.\n        # Because there are too many possible results.\n        self._test_async_correctness(\n            grads_and_vars_batches, embed_values, expected_non_embed_values\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/parameters_test.py,2,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.proto.elasticdl_pb2 import Model\nfrom elasticdl.python.common.tensor_utils import (\n    Tensor,\n    serialize_indexed_slices,\n    serialize_ndarray,\n)\nfrom elasticdl.python.ps.embedding_table import get_slot_table_name\nfrom elasticdl.python.ps.parameters import Parameters\n\n\nclass ParametersTest(unittest.TestCase):\n    def setUp(self):\n        self.params = Parameters()\n\n        self.model_pb = Model()\n        self.infos_pb = self.model_pb.embedding_table_infos\n        self.tensors_pb = self.model_pb.dense_parameters\n        self.embedding_tables_pb = self.model_pb.embedding_tables\n\n        self.embedding_table_name = ""embedding_1""\n        self.embedding_dim = 10\n        embedding_pb = self.infos_pb.add()\n        embedding_pb.name = self.embedding_table_name\n        embedding_pb.dim = self.embedding_dim\n        embedding_pb.initializer = ""uniform""\n\n        arr1 = np.random.uniform(size=(3, 4))\n        serialize_ndarray(arr1, self.tensors_pb[""x""])\n        arr2 = np.random.uniform(size=(4, 5))\n        serialize_ndarray(arr2, self.tensors_pb[""y""])\n\n        embedding_vectors = np.random.uniform(size=(2, 10))\n        embedding_indices = np.array([0, 8])\n        serialize_indexed_slices(\n            Tensor(None, embedding_vectors, embedding_indices),\n            self.embedding_tables_pb[self.embedding_table_name],\n        )\n\n    def _test_get_embedding_param(self, slot_names=[], slot_init_value={}):\n        indices = [0, 3, 7]\n\n        res = self.params.get_embedding_param(\n            self.embedding_table_name, indices\n        )\n        self.assertTupleEqual(res.shape, (3, 10))\n        for slot in slot_names:\n            res = self.params.get_embedding_param(\n                get_slot_table_name(self.embedding_table_name, slot), indices\n            )\n            self.assertTrue(((res - slot_init_value[slot]) < 0.0001).all())\n\n        res = self.params.get_embedding_param(self.embedding_table_name, [])\n        self.assertIsNone(res)\n\n        with self.assertRaises(ValueError):\n            self.params.get_embedding_param(""tom"", indices)\n\n    def test_init_from_model_pb(self):\n        self.params.reset()\n        self.params.init_from_model_pb(self.model_pb)\n\n        res = self.params.non_embedding_params\n        self.assertTrue(""x"" in res)\n        self.assertTrue(""y"" in res)\n        self.assertTrue(res[""x""].trainable)\n        self.assertTupleEqual(tuple(res[""y""].shape.as_list()), (4, 5))\n\n        self._test_get_embedding_param()\n\n    def test_non_embedding_params(self):\n        self.params.reset()\n\n        res = self.params.non_embedding_params\n        self.assertFalse(any(res))\n\n        variables = {\n            ""x"": tf.Variable(1, name=""x""),\n            ""y"": tf.Variable(2, name=""y""),\n        }\n\n        self.params.non_embedding_params = variables\n        self.assertTrue(""x"" in self.params.non_embedding_params)\n        self.assertTrue(""y"" in self.params.non_embedding_params)\n\n    def test_get_embedding_param(self):\n        self.params.reset()\n        self.params.init_embedding_params(self.infos_pb)\n        self._test_get_embedding_param()\n\n    def test_set_embedding_param(self):\n        self.params.reset()\n        self.params.init_embedding_params(self.infos_pb)\n        indices = [100, 34, 8]\n        x = len(indices)\n        values = np.random.uniform(size=x * self.embedding_dim).reshape(\n            (x, self.embedding_dim)\n        )\n\n        self.params.set_embedding_param(\n            self.embedding_table_name, indices, values\n        )\n\n        row0 = self.params.get_embedding_param(\n            self.embedding_table_name, [100]\n        )\n        row1 = self.params.get_embedding_param(self.embedding_table_name, [34])\n        row2 = self.params.get_embedding_param(self.embedding_table_name, [8])\n\n        rows = [row0, row1, row2]\n        rows = np.concatenate(rows)\n        np.testing.assert_array_equal(rows, values)\n\n        with self.assertRaises(ValueError):\n            self.params.set_embedding_param(""tom"", [0, 1, 2], values)\n\n    def test_check_grad(self):\n        self.params.reset()\n        self.params.init_from_model_pb(self.model_pb)\n\n        grad0 = Tensor(""z"", None, None)\n        with self.assertRaisesRegex(ValueError, ""Name error""):\n            self.params.check_grad(grad0)\n\n        grad1 = Tensor(""x"", np.random.uniform(size=(3, 5)), None)\n        with self.assertRaisesRegex(ValueError, ""Non embedding param error""):\n            self.params.check_grad(grad1)\n\n        grad2 = Tensor(\n            name=""embedding_1"",\n            values=np.random.uniform(size=(3, 11)),\n            indices=np.array([1, 2, 3]),\n        )\n        with self.assertRaisesRegex(\n            ValueError, ""ElasticDL embedding param error""\n        ):\n            self.params.check_grad(grad2)\n\n        grad3 = Tensor(\n            name=""x"",\n            values=np.random.uniform(size=(4, 4)),\n            indices=np.array([1, 2, 3, 4]),\n        )\n        with self.assertRaisesRegex(ValueError, ""Keras embedding param error""):\n            self.params.check_grad(grad3)\n\n    def test_create_slot_params(self):\n        # At first, no embedding table are in the parameters\n        self.assertFalse(self.params.has_embedding_params())\n\n        # create embedding tables in the parameters\n        self.params.init_embedding_params(self.infos_pb)\n        self.assertTrue(self.params.has_embedding_params())\n\n        slot_names = [""accumulator"", ""linear""]\n        slot_init_value = {slot_names[0]: 3.5, slot_names[1]: 0.0}\n        self.params.create_slot_params(slot_names, slot_init_value)\n        self._test_get_embedding_param(slot_names, slot_init_value)\n\n    def test_export_to_model_pb(self):\n        self.params.init_from_model_pb(self.model_pb)\n        self.params.version = 15\n        model_pb = self.params.to_model_pb()\n\n        params = Parameters()\n        params.init_from_model_pb(model_pb)\n        self.assertEqual(params.version, self.params.version)\n        self.assertEqual(\n            params.non_embedding_params.keys(),\n            self.params.non_embedding_params.keys(),\n        )\n        self.assertEqual(\n            params.embedding_params[""embedding_1""].get([0]).tolist(),\n            self.params.embedding_params[""embedding_1""].get([0]).tolist(),\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/pserver_servicer_test.py,9,"b'import os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\nfrom google.protobuf import empty_pb2\n\nfrom elasticdl.proto import elasticdl_pb2, elasticdl_pb2_grpc\nfrom elasticdl.python.common.grpc_utils import build_channel\nfrom elasticdl.python.common.model_utils import (\n    get_module_file_path,\n    load_module,\n)\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.common.tensor_utils import (\n    pb_to_ndarray,\n    serialize_indexed_slices,\n    serialize_ndarray,\n)\nfrom elasticdl.python.ps.embedding_table import (\n    EmbeddingTable,\n    get_slot_table_name,\n)\nfrom elasticdl.python.ps.parameter_server import ParameterServer\nfrom elasticdl.python.ps.parameters import Parameters\nfrom elasticdl.python.ps.servicer import PserverServicer\nfrom elasticdl.python.tests.test_utils import PserverArgs\n\n_test_model_zoo_path = os.path.dirname(os.path.realpath(__file__))\n_module_file = get_module_file_path(\n    _test_model_zoo_path, ""test_module.custom_model""\n)\n\n\nclass PserverServicerTest(unittest.TestCase):\n    def setUp(self):\n        self._port = 9999\n        addr = ""localhost:%d"" % self._port\n        self._channel = build_channel(addr)\n        embedding_info = elasticdl_pb2.EmbeddingTableInfo()\n        embedding_info.name = ""layer_a""\n        embedding_info.dim = 32\n        embedding_info.initializer = ""normal""\n        self._embedding_info = embedding_info\n        self._server = None\n\n    def tearDown(self):\n        if self._server:\n            self._server.stop(0)\n\n    def create_server_and_stub(\n        self, grads_to_wait, lr_staleness_modulation, use_async, **kwargs\n    ):\n        args = PserverArgs(\n            grads_to_wait=grads_to_wait,\n            lr_staleness_modulation=lr_staleness_modulation,\n            use_async=use_async,\n            port=self._port,\n            model_zoo=_test_model_zoo_path,\n            model_def=""test_module.custom_model"",\n            **kwargs\n        )\n        pserver = ParameterServer(args)\n        pserver.prepare()\n        self._parameters = pserver.parameters\n        self._server = pserver.server\n        self._stub = elasticdl_pb2_grpc.PserverStub(self._channel)\n\n        self._lr = 0.1\n\n    def create_default_server_and_stub(self, **kwargs):\n        grads_to_wait = 8\n        lr_staleness_modulation = False\n        use_async = True\n\n        self.create_server_and_stub(\n            grads_to_wait, lr_staleness_modulation, use_async, **kwargs\n        )\n\n    def get_embedding_vectors(self, name, ids):\n        pull_req = elasticdl_pb2.PullEmbeddingVectorRequest()\n        pull_req.name = name\n        pull_req.ids.extend(ids)\n        res = self._stub.pull_embedding_vectors(pull_req)\n        if res.tensor_content:\n            return pb_to_ndarray(res)\n        else:\n            return None\n\n    def test_push_model(self):\n        opt_func_name = ""ftrl_optimizer""\n        opt = load_module(_module_file).__dict__[opt_func_name]()\n        opt_config = opt.get_config()\n        slot_names = [""accumulator"", ""linear""]\n        slot_init_value = {\n            ""accumulator"": opt_config[""initial_accumulator_value""],\n            ""linear"": 0.0,\n        }\n\n        self.create_default_server_and_stub(optimizer=opt_func_name)\n        param0 = {\n            ""v0"": np.random.rand(3, 2).astype(np.float32),\n            ""v1"": np.random.rand(10, 32).astype(np.float32),\n        }\n        param1 = {\n            ""v0"": np.ones([3, 2], dtype=np.float32),\n            ""v1"": np.ones([10, 32], dtype=np.float32),\n        }\n\n        models = [param0, param1]\n\n        for idx, model in enumerate(models):\n            req = elasticdl_pb2.Model()\n            req.version = idx + 1\n            for name in model:\n                serialize_ndarray(model[name], req.dense_parameters[name])\n            req.embedding_table_infos.append(self._embedding_info)\n            res = self._stub.push_model(req)\n            self.assertEqual(res, empty_pb2.Empty())\n            # self._parameters is initialized with the first push_model call\n            # and the second push_model has no effect\n            self.assertEqual(self._parameters.version, 1)\n            for name in param0:\n                self.assertTrue(\n                    np.allclose(\n                        param0[name],\n                        self._parameters.non_embedding_params[name].numpy(),\n                    )\n                )\n            self.assertEqual(\n                self._embedding_info.name,\n                self._parameters.embedding_params[\n                    self._embedding_info.name\n                ].name,\n            )\n            self.assertEqual(\n                self._embedding_info.dim,\n                self._parameters.embedding_params[\n                    self._embedding_info.name\n                ].dim,\n            )\n            self.assertEqual(\n                tf.keras.initializers.get(\n                    self._embedding_info.initializer\n                ).__class__,\n                self._parameters.embedding_params[\n                    self._embedding_info.name\n                ].initializer.__class__,\n            )\n\n            for slot_name in slot_names:\n                name = get_slot_table_name(\n                    self._embedding_info.name, slot_name\n                )\n                table = self._parameters.embedding_params[name]\n                self.assertTrue(name, table.name)\n                self.assertTrue(self._embedding_info.dim, table.dim)\n                embedding = table.get([2])\n                self.assertTrue(\n                    (embedding - slot_init_value[slot_name] < 0.0001).all()\n                )\n\n    def test_pull_dense_parameters(self):\n        self.create_default_server_and_stub()\n        param0 = {\n            ""v0"": np.random.rand(3, 2).astype(np.float32),\n            ""v1"": np.random.rand(10, 32).astype(np.float32),\n        }\n        pull_req = elasticdl_pb2.PullDenseParametersRequest()\n        pull_req.version = -1\n        # try to pull variable\n        res = self._stub.pull_dense_parameters(pull_req)\n        # not initialized\n        self.assertFalse(res.initialized)\n\n        # init variable\n        req = elasticdl_pb2.Model()\n        req.version = 1\n        for name, var in param0.items():\n            serialize_ndarray(var, req.dense_parameters[name])\n        res = self._stub.push_model(req)\n        self.assertEqual(res, empty_pb2.Empty())\n\n        # pull variable back\n        res = self._stub.pull_dense_parameters(pull_req)\n        self.assertTrue(res.initialized)\n        self.assertEqual(res.version, req.version)\n        for name, pb in res.dense_parameters.items():\n            tensor = pb_to_ndarray(pb)\n            self.assertTrue(np.allclose(param0[name], tensor))\n\n        # pull variable again, no param as no updated version\n        pull_req.version = res.version\n        res = self._stub.pull_dense_parameters(pull_req)\n        self.assertTrue(res.initialized)\n        self.assertEqual(res.version, pull_req.version)\n        self.assertTrue(not res.dense_parameters)\n\n    def test_pull_embedding_vectors(self):\n        self.create_default_server_and_stub()\n\n        id_list_0 = [1, 3, 9, 6]\n        id_list_1 = [8, 9, 1, 0, 6]\n\n        req = elasticdl_pb2.Model()\n        req.version = 1\n        req.embedding_table_infos.append(self._embedding_info)\n        another_embedding_info = elasticdl_pb2.EmbeddingTableInfo()\n        another_embedding_info.name = ""layer_b""\n        another_embedding_info.dim = 16\n        another_embedding_info.initializer = ""normal""\n        req.embedding_table_infos.append(another_embedding_info)\n        res = self._stub.push_model(req)\n        self.assertEqual(res, empty_pb2.Empty())\n\n        vectors_a_0 = self.get_embedding_vectors(""layer_a"", id_list_0)\n        self.assertEqual(vectors_a_0.shape[0], len(id_list_0))\n        self.assertEqual(vectors_a_0.shape[1], 32)\n\n        vectors_a_1 = self.get_embedding_vectors(""layer_a"", id_list_1)\n        self.assertEqual(vectors_a_1.shape[0], len(id_list_1))\n        self.assertEqual(vectors_a_1.shape[1], 32)\n\n        vectors_b_1 = self.get_embedding_vectors(""layer_b"", id_list_1)\n        self.assertEqual(vectors_b_1.shape[0], len(id_list_1))\n        self.assertEqual(vectors_b_1.shape[1], 16)\n\n        vectors_b_0 = self.get_embedding_vectors(""layer_b"", id_list_0)\n        self.assertEqual(vectors_b_0.shape[0], len(id_list_0))\n        self.assertEqual(vectors_b_0.shape[1], 16)\n\n        for idx0, id0 in enumerate(id_list_0):\n            for idx1, id1 in enumerate(id_list_1):\n                if id0 == id1:\n                    self.assertTrue(\n                        np.array_equal(vectors_a_0[idx0], vectors_a_1[idx1])\n                    )\n                    self.assertTrue(\n                        np.array_equal(vectors_b_0[idx0], vectors_b_1[idx1])\n                    )\n\n        vectors = self.get_embedding_vectors(""layer_a"", [])\n        self.assertEqual(vectors, None)\n\n    def push_gradient_test_setup(self):\n        self.var_names = [""test_1"", ""test_2""]\n        self.var_values = [\n            np.array([10.0, 20.0, 30.0], np.float32),\n            np.array([20.0, 40.0, 60.0], np.float32),\n        ]\n        self.grad_values0 = [\n            np.array([1.0, 2.0, 3.0], np.float32),\n            np.array([2.0, 4.0, 6.0], np.float32),\n        ]\n        self.grad_values1 = [\n            np.array([0.0, 0.0, 7.0], np.float32),\n            np.array([9.0, 9.0, 6.0], np.float32),\n        ]\n\n        dim = self._embedding_info.dim\n        self.embedding_table = (\n            np.random.rand(4 * dim).reshape((4, dim)).astype(np.float32)\n        )\n        self.embedding_grads0 = tf.IndexedSlices(\n            values=np.random.rand(3 * dim)\n            .reshape((3, dim))\n            .astype(np.float32),\n            indices=(3, 1, 3),\n        )\n        self.embedding_grads1 = tf.IndexedSlices(\n            values=np.random.rand(3 * dim)\n            .reshape((3, dim))\n            .astype(np.float32),\n            indices=(2, 2, 3),\n        )\n        push_model_req = elasticdl_pb2.Model()\n        push_model_req.version = self._parameters.version\n        for name, value in zip(self.var_names, self.var_values):\n            serialize_ndarray(value, push_model_req.dense_parameters[name])\n        push_model_req.embedding_table_infos.append(self._embedding_info)\n        self._stub.push_model(push_model_req)\n\n        for name, var in zip(self.var_names, self.var_values):\n            self._parameters.non_embedding_params[name] = tf.Variable(var)\n\n        self._parameters.embedding_params[self._embedding_info.name].set(\n            range(len(self.embedding_table)), self.embedding_table\n        )\n\n    def test_push_gradient_async_update(self):\n        self.create_default_server_and_stub()\n        self.push_gradient_test_setup()\n\n        # Test applying gradients to embedding and non-embedding parameters\n        req = elasticdl_pb2.PushGradientsRequest()\n        for g, name in zip(self.grad_values0, self.var_names):\n            serialize_ndarray(g, req.gradients.dense_parameters[name])\n        serialize_indexed_slices(\n            self.embedding_grads0,\n            req.gradients.embedding_tables[self._embedding_info.name],\n        )\n        res = self._stub.push_gradients(req)\n        self.assertEqual(res.accepted, True)\n        self.assertEqual(res.version, 1)\n        expected_values = [\n            v - self._lr * g\n            for v, g in zip(self.var_values, self.grad_values0)\n        ]\n        for name, expected_value in zip(self.var_names, expected_values):\n            self.assertTrue(\n                np.allclose(\n                    expected_value,\n                    self._parameters.non_embedding_params[name].numpy(),\n                )\n            )\n\n        expected_embed_table = np.copy(self.embedding_table)\n        for gv, gi in zip(\n            self.embedding_grads0.values, self.embedding_grads0.indices\n        ):\n            expected_embed_table[gi] -= self._lr * gv\n\n        actual_embed_table = self._parameters.get_embedding_param(\n            self._embedding_info.name, range(len(expected_embed_table))\n        )\n        self.assertTrue(np.allclose(expected_embed_table, actual_embed_table))\n\n        # Test applying gradients with same name\n        for name, var in zip(self.var_names, self.var_values):\n            self._parameters.non_embedding_params[name] = tf.Variable(var)\n        req = elasticdl_pb2.PushGradientsRequest()\n        serialize_ndarray(\n            self.grad_values1[1],\n            req.gradients.dense_parameters[self.var_names[0]],\n        )\n        res = self._stub.push_gradients(req)\n        self.assertEqual(res.accepted, True)\n        self.assertEqual(res.version, 2)\n        expected_values = [\n            self.var_values[0] - self._lr * self.grad_values1[1],\n            self.var_values[1],\n        ]\n        for expected_value, name in zip(expected_values, self.var_names):\n            self.assertTrue(\n                np.allclose(\n                    expected_value,\n                    self._parameters.non_embedding_params[name].numpy(),\n                )\n            )\n\n    def test_push_gradient_sync_update(self):\n        self.create_server_and_stub(\n            grads_to_wait=2, lr_staleness_modulation=False, use_async=False\n        )\n        self.push_gradient_test_setup()\n\n        req = elasticdl_pb2.PushGradientsRequest()\n        req.gradients.version = 0\n        for g, name in zip(self.grad_values0, self.var_names):\n            serialize_ndarray(g, req.gradients.dense_parameters[name])\n        serialize_indexed_slices(\n            self.embedding_grads0,\n            req.gradients.embedding_tables[self._embedding_info.name],\n        )\n\n        res = self._stub.push_gradients(req)\n        self.assertEqual(res.accepted, True)\n        self.assertEqual(res.version, 0)\n\n        req = elasticdl_pb2.PushGradientsRequest()\n        req.gradients.version = 0\n        for g, name in zip(self.grad_values1, self.var_names):\n            serialize_ndarray(g, req.gradients.dense_parameters[name])\n        serialize_indexed_slices(\n            self.embedding_grads1,\n            req.gradients.embedding_tables[self._embedding_info.name],\n        )\n        res = self._stub.push_gradients(req)\n        self.assertEqual(res.accepted, True)\n        self.assertEqual(res.version, 1)\n\n        req = elasticdl_pb2.PushGradientsRequest()\n        req.gradients.version = 0\n        for g, name in zip(self.grad_values1, self.var_names):\n            serialize_ndarray(g, req.gradients.dense_parameters[name])\n        res = self._stub.push_gradients(req)\n        self.assertEqual(res.accepted, False)\n        self.assertEqual(res.version, 1)\n\n        expected_values = [\n            self.var_values[0]\n            - self._lr * (self.grad_values0[0] + self.grad_values1[0]) / 2,\n            self.var_values[1]\n            - self._lr * (self.grad_values0[1] + self.grad_values1[1]) / 2,\n        ]\n        for expected_value, name in zip(expected_values, self.var_names):\n            self.assertTrue(\n                np.allclose(\n                    expected_value,\n                    self._parameters.non_embedding_params[name].numpy(),\n                )\n            )\n\n        expected_embed_table = np.copy(self.embedding_table)\n        for gv, gi in zip(\n            self.embedding_grads0.values, self.embedding_grads0.indices\n        ):\n            expected_embed_table[gi] -= self._lr * gv\n        for gv, gi in zip(\n            self.embedding_grads1.values, self.embedding_grads1.indices\n        ):\n            expected_embed_table[gi] -= self._lr * gv\n\n        actual_embed_table = self._parameters.get_embedding_param(\n            self._embedding_info.name, range(len(expected_embed_table))\n        )\n        self.assertTrue(np.allclose(expected_embed_table, actual_embed_table))\n\n    def test_save_parameters_to_checkpoint_file(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            checkpoint_saver = CheckpointSaver(\n                checkpoint_dir=os.path.join(tempdir, ""ckpt/""),\n                checkpoint_steps=5,\n                keep_checkpoint_max=3,\n                include_evaluation=False,\n            )\n            pserver_servicer = PserverServicer(\n                parameters=Parameters(),\n                grads_to_wait=0,\n                optimizer=""optimizer"",\n                checkpoint_saver=checkpoint_saver,\n                ps_id=0,\n                num_ps_pods=1,\n            )\n            model_params = {\n                ""v0"": tf.Variable([[1, 1, 1], [1, 1, 1]]),\n                ""v1"": tf.Variable([[2, 2, 2], [2, 2, 2]]),\n            }\n\n            server_params = pserver_servicer._parameters\n            for var_name, var_value in model_params.items():\n                server_params.non_embedding_params[var_name] = var_value\n\n            embedding_table = EmbeddingTable(\n                name=""embedding_0"", dim=3, initializer=""random_uniform""\n            )\n            server_params.embedding_params[""embedding_0""] = embedding_table\n            server_params.set_embedding_param(\n                name=""embedding_0"",\n                indices=np.array([0, 1]),\n                values=np.array([[1, 1, 1], [2, 2, 2]]),\n            )\n\n            for i in range(100):\n                pserver_servicer._parameters.version += 1\n                pserver_servicer._save_params_to_checkpoint_if_needed()\n\n            self.assertEqual(len(os.listdir(checkpoint_saver._directory)), 3)\n            self.assertEqual(\n                sorted(os.listdir(checkpoint_saver._directory)),\n                [""version-100"", ""version-90"", ""version-95""],\n            )\n            self.assertEqual(\n                os.listdir(checkpoint_saver._directory + ""/version-100""),\n                [""variables-0-of-1.ckpt""],\n            )\n\n    def test_restore_parameters_from_checkpoint(self):\n        checkpoint_dir = ""elasticdl/python/tests/testdata/ps_ckpt""\n        checkpoint_saver = CheckpointSaver(checkpoint_dir, 0, 0, False)\n        params = Parameters()\n        table = EmbeddingTable(""embedding"", 2, ""random_uniform"")\n        table.set([0, 1, 2, 3], np.ones((4, 2), dtype=np.float32))\n        params.embedding_params[""embedding""] = table\n        params.non_embedding_params[""dense/kernel:0""] = tf.Variable(\n            [[1.0], [1.0]]\n        )\n        params.non_embedding_params[""dense/bias:0""] = tf.Variable([1.0])\n        params.version = 100\n        model_pb = params.to_model_pb()\n        checkpoint_saver.save(100, model_pb, False)\n\n        checkpoint_dir_for_init = checkpoint_dir + ""/version-100""\n        args = PserverArgs(\n            ps_id=0,\n            num_ps_pods=2,\n            model_zoo=_test_model_zoo_path,\n            model_def=""test_module.custom_model"",\n            checkpoint_dir_for_init=checkpoint_dir_for_init,\n        )\n        pserver_0 = ParameterServer(args)\n\n        embedding_table = pserver_0.parameters.embedding_params[""embedding""]\n        self.assertEqual(\n            list(embedding_table.embedding_vectors.keys()), [0, 2]\n        )\n        self.assertEqual(\n            list(pserver_0.parameters.non_embedding_params.keys()),\n            [""dense/kernel:0""],\n        )\n        self.assertTrue(\n            np.array_equal(\n                pserver_0.parameters.non_embedding_params[\n                    ""dense/kernel:0""\n                ].numpy(),\n                np.array([[1], [1]], dtype=int),\n            )\n        )\n        self.assertEqual(pserver_0.parameters.version, 100)\n\n        args = PserverArgs(\n            ps_id=1,\n            num_ps_pods=2,\n            model_zoo=_test_model_zoo_path,\n            model_def=""test_module.custom_model"",\n            checkpoint_dir_for_init=checkpoint_dir_for_init,\n        )\n        pserver_1 = ParameterServer(args)\n\n        embedding_table = pserver_1.parameters.embedding_params[""embedding""]\n        self.assertEqual(\n            list(embedding_table.embedding_vectors.keys()), [1, 3]\n        )\n        self.assertEqual(\n            list(pserver_1.parameters.non_embedding_params.keys()),\n            [""dense/bias:0""],\n        )\n        self.assertTrue(\n            np.array_equal(\n                pserver_1.parameters.non_embedding_params[\n                    ""dense/bias:0""\n                ].numpy(),\n                np.array([1], dtype=int),\n            )\n        )\n        self.assertEqual(pserver_1.parameters.version, 100)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/save_utils_test.py,0,"b'import os\nimport tempfile\nimport unittest\n\nimport numpy as np\n\nfrom elasticdl.python.common.model_utils import (\n    get_module_file_path,\n    load_module,\n)\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.ps.parameters import Parameters\n\n_model_zoo_path = os.path.dirname(os.path.realpath(__file__))\n_model_file = get_module_file_path(_model_zoo_path, ""test_module.custom_model"")\nm = load_module(_model_file).__dict__\n\n\ndef save_variables_to_checkpoint(root_dir, params):\n    ckpt_dir = os.path.join(root_dir, ""testSaveLoadCheckpoint"")\n    os.makedirs(ckpt_dir)\n    checkpoint_saver = CheckpointSaver(ckpt_dir, 3, 5, False)\n    model_pb = params.to_model_pb()\n    checkpoint_saver.save(params.version, model_pb, False)\n    return ckpt_dir\n\n\nclass SaveUtilsTest(unittest.TestCase):\n    def setUp(self):\n        init_var = m[""custom_model""]().trainable_variables\n        self.params = Parameters()\n        for var in init_var:\n            self.params.non_embedding_params[var.name] = var\n\n    def testNeedToCheckpoint(self):\n        checkpointer = CheckpointSaver("""", 0, 5, False)\n        self.assertFalse(checkpointer.is_enabled())\n        checkpointer._steps = 3\n        self.assertTrue(checkpointer.is_enabled())\n\n        self.assertFalse(checkpointer.need_to_checkpoint(1))\n        self.assertFalse(checkpointer.need_to_checkpoint(2))\n        self.assertTrue(checkpointer.need_to_checkpoint(3))\n        self.assertFalse(checkpointer.need_to_checkpoint(4))\n        self.assertFalse(checkpointer.need_to_checkpoint(5))\n        self.assertTrue(checkpointer.need_to_checkpoint(6))\n\n    def testGetCheckpointPath(self):\n        ckpt_dir = ""test/checkpoint_dir""\n        checkpoint_saver = CheckpointSaver(ckpt_dir, 3, 5, False)\n        checkpint_path = checkpoint_saver._get_checkpoint_file(100)\n        self.assertEqual(\n            checkpint_path,\n            ""test/checkpoint_dir/version-100/variables-0-of-1.ckpt"",\n        )\n\n    def testSaveLoadCheckpoint(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            self.params.version = 0\n            ckpt_dir = save_variables_to_checkpoint(tempdir, self.params)\n            ckpt_version_dir = os.path.join(ckpt_dir, ""version-0"")\n            restore_params = CheckpointSaver.restore_params_from_checkpoint(\n                ckpt_version_dir, 0, 1\n            )\n            self.assertEqual(restore_params.version, self.params.version)\n            for var_name in self.params.non_embedding_params:\n                self.assertTrue(\n                    np.array_equal(\n                        self.params.non_embedding_params[var_name].numpy(),\n                        restore_params.non_embedding_params[var_name].numpy(),\n                    )\n                )\n\n    def testGetVersionFromCheckpoint(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            self.params.version = 100\n            ckpt_dir = save_variables_to_checkpoint(tempdir, self.params)\n            ckpt_version_dir = os.path.join(ckpt_dir, ""version-100"")\n            model_version = CheckpointSaver.get_version_from_checkpoint(\n                ckpt_version_dir\n            )\n            self.assertTrue(model_version, 100)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/servicer_test.py,4,"b'import random\nimport unittest\nfrom collections import defaultdict\n\nimport tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.master.servicer import MasterServicer\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\n\n\ndef _get_variable_names(model_pb):\n    return [v.name for v in model_pb.param]\n\n\nclass SimpleModel(tf.keras.Model):\n    def __init__(self):\n        super(SimpleModel, self).__init__(name=""test_model"")\n        self.dense_1 = tf.keras.layers.Dense(\n            32, activation=""relu"", name=""dense_1""\n        )\n        self.dense_2 = tf.keras.layers.Dense(\n            1, activation=""sigmoid"", name=""dense_2""\n        )\n\n    def call(self, inputs):\n        x = self.dense_1(inputs)\n        return self.dense_2(x)\n\n    @staticmethod\n    def input_shapes():\n        return 10, 10\n\n    @staticmethod\n    def optimizer(lr=0.1):\n        return tf.optimizers.SGD(lr)\n\n\nclass ServicerTest(unittest.TestCase):\n    def testGetEmptyTask(self):\n        master = MasterServicer(\n            3,\n            _TaskDispatcher({}, {}, {}, records_per_task=3, num_epochs=2),\n            evaluation_service=None,\n        )\n\n        req = elasticdl_pb2.GetTaskRequest()\n\n        # No task yet, make sure the returned versions are as expected.\n        req.worker_id = 1\n        task = master.get_task(req, None)\n        self.assertEqual("""", task.shard_name)\n        self.assertEqual(0, task.model_version)\n\n        master._version = 1\n        task = master.get_task(req, None)\n        self.assertEqual("""", task.shard_name)\n        self.assertEqual(1, task.model_version)\n\n    def testReportTaskResult(self):\n        task_d = _TaskDispatcher(\n            {""shard_1"": (0, 10), ""shard_2"": (0, 9)},\n            {},\n            {},\n            records_per_task=3,\n            num_epochs=2,\n        )\n        master = MasterServicer(3, task_d, evaluation_service=None,)\n\n        # task to number of runs.\n        tasks = defaultdict(int)\n        while True:\n            req = elasticdl_pb2.GetTaskRequest()\n            req.worker_id = random.randint(1, 10)\n            task = master.get_task(req, None)\n            if not task.shard_name:\n                break\n            self.assertEqual(task_d._doing[task.task_id][0], req.worker_id)\n            task_key = (task.shard_name, task.start, task.end)\n            tasks[task_key] += 1\n            report = elasticdl_pb2.ReportTaskResultRequest()\n            report.task_id = task.task_id\n            if task.start == 0 and tasks[task_key] == 1:\n                # Simulate error reports.\n                report.err_message = ""Worker error""\n            master.report_task_result(report, None)\n\n        self.assertDictEqual(\n            {\n                (""shard_1"", 0, 3): 3,\n                (""shard_1"", 3, 6): 2,\n                (""shard_1"", 6, 9): 2,\n                (""shard_1"", 9, 10): 2,\n                (""shard_2"", 0, 3): 3,\n                (""shard_2"", 3, 6): 2,\n                (""shard_2"", 6, 9): 2,\n            },\n            tasks,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/staleness_aware_test.py,3,"b'import time\nimport unittest\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport tensorflow as tf\n\nfrom elasticdl.python.ps.learning_rate_modulator import (\n    add_lr_modulation_to_optimizer,\n)\n\n\nclass LearningRateTest(unittest.TestCase):\n    @staticmethod\n    def get_lr(lr_modulation, opt, multiplier):\n        lr_modulation.set_multiplier(multiplier)\n        # sleep 1s to wait that all threads are in this method call\n        time.sleep(1)\n        return opt.learning_rate\n\n    @staticmethod\n    def apply_gradients_with_modulation(\n        lr_modulation, opt, multiplier, variables, grads\n    ):\n        grads_and_vars = zip(grads, variables)\n        lr_modulation.set_multiplier(multiplier)\n        # sleep 1s to wait that all threads are in this method call\n        time.sleep(1)\n        opt.apply_gradients(grads_and_vars)\n        return [v.numpy() for v in variables]\n\n    def test_lr_modulation(self):\n        lr = 0.1\n        multipliers = [1, 0.5, 0.1, 0.01]\n        counts = len(multipliers)\n        opt = tf.optimizers.SGD(lr)\n        lr_modulation = add_lr_modulation_to_optimizer(opt)\n\n        executor = ThreadPoolExecutor(max_workers=counts)\n        tasks = [\n            executor.submit(self.get_lr, lr_modulation, opt, m)\n            for m in multipliers\n        ]\n        results = [tasks[i].result() for i in range(counts)]\n        for i in range(counts):\n            self.assertAlmostEqual(results[i], lr * multipliers[i])\n\n        variables = []\n        grads = []\n        original_values = [1.2, 0.8]\n        grad_values = [0.2, 0.1]\n\n        for i in range(counts):\n            variables.append([tf.Variable(v) for v in original_values])\n            grads.append([tf.convert_to_tensor(g) for g in grad_values])\n\n        tasks = [\n            executor.submit(\n                self.apply_gradients_with_modulation,\n                lr_modulation,\n                opt,\n                multipliers[i],\n                variables[i],\n                grads[i],\n            )\n            for i in range(counts)\n        ]\n        results = [tasks[i].result() for i in range(counts)]\n        place = 5\n        for i in range(0, counts):\n            i_diff = [\n                original_values[j] - results[i][j]\n                for j in range(len(original_values))\n            ]\n            for j in range(len(original_values)):\n                # variable value change ratio equals the learning rate ratio\n                # for SGD without momentum\n                self.assertAlmostEqual(\n                    i_diff[j], grad_values[j] * lr * multipliers[i], place\n                )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/task_dispatcher_test.py,0,"b'import unittest\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\n\n\nclass TaskQueueTest(unittest.TestCase):\n    def test_create_tasks_with_zero_start_ind(self):\n        task_d = _TaskDispatcher({""f1"": (0, 10), ""f2"": (0, 10)}, {}, {}, 3, 1)\n\n        all_tasks = [\n            (""f1"", 0, 3, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 3, 6, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 6, 9, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 9, 10, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 0, 3, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 3, 6, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 6, 9, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 9, 10, elasticdl_pb2.TRAINING, -1),\n        ]\n\n        # get all tasks out, each worker is assigned 2 tasks.\n        got_tasks = [task_d.get(i // 2) for i in range(8)]\n\n        # verify ids ranges from 1 to 8\n        self.assertEqual(list(range(1, 9)), [k for k, _ in got_tasks])\n\n        # verify tasks\n        self.assertEqual(sorted([v._info() for _, v in got_tasks]), all_tasks)\n\n        # no todo tasks, should return None\n        self.assertEqual((-1, None), task_d.get(10))\n\n        request = elasticdl_pb2.ReportTaskResultRequest()\n        # report 6 task successes.\n        for t in (1, 3, 5, 7, 2, 8):\n            request.task_id = t\n            task_d.report(request, True)\n\n        # there should be 2 doing tasks left.\n        self.assertEqual(2, len(task_d._doing))\n\n        # report a task failure\n        request.task_id = list(task_d._doing.items())[0][0]\n        task_d.report(request, False)\n        self.assertEqual(1, len(task_d._doing))\n\n        # recover tasks from a dead worker\n        task_d.recover_tasks(list(task_d._doing.items())[0][1][0])\n        self.assertEqual(0, len(task_d._doing))\n\n        self.assertEqual(2, len(task_d._todo))\n\n        id1, t1 = task_d.get(11)\n        id2, t2 = task_d.get(12)\n        request.task_id = id1\n        task_d.report(request, True)\n        request.task_id = id2\n        task_d.report(request, True)\n\n        self.assertTrue(task_d.finished())\n\n    def test_create_tasks_with_non_zero_start_ind(self):\n        task_d = _TaskDispatcher({""f1"": (0, 10), ""f2"": (10, 10)}, {}, {}, 3, 1)\n\n        all_tasks = [\n            (""f1"", 0, 3, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 3, 6, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 6, 9, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 9, 10, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 10, 13, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 13, 16, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 16, 19, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 19, 20, elasticdl_pb2.TRAINING, -1),\n        ]\n\n        # get all tasks out, each worker is assigned 2 tasks.\n        got_tasks = [task_d.get(i // 2) for i in range(8)]\n\n        # verify ids ranges from 1 to 8\n        self.assertEqual(list(range(1, 9)), [k for k, _ in got_tasks])\n\n        # verify tasks\n        self.assertEqual(sorted([v._info() for _, v in got_tasks]), all_tasks)\n\n    def test_epoch(self):\n        task_d = _TaskDispatcher({""f1"": (0, 10), ""f2"": (0, 10)}, {}, {}, 3, 2)\n\n        epoch_tasks = [\n            (""f1"", 0, 3, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 3, 6, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 6, 9, elasticdl_pb2.TRAINING, -1),\n            (""f1"", 9, 10, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 0, 3, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 3, 6, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 6, 9, elasticdl_pb2.TRAINING, -1),\n            (""f2"", 9, 10, elasticdl_pb2.TRAINING, -1),\n        ]\n\n        # get first epoch tasks\n        got_tasks = [task_d.get(i // 2) for i in range(8)]\n        self.assertEqual(\n            sorted([v._info() for _, v in got_tasks]), epoch_tasks\n        )\n\n        # get second epoch tasks\n        got_tasks = [task_d.get(i // 2) for i in range(8)]\n        self.assertEqual(\n            sorted([v._info() for _, v in got_tasks]), epoch_tasks\n        )\n\n    def test_invoke_train_end_callback(self):\n        task_d = _TaskDispatcher({""f1"": (0, 10), ""f2"": (0, 10)}, {}, {}, 3, 1)\n        task_d.add_deferred_callback_create_train_end_task()\n        task_d._todo.clear()\n        task_d.invoke_deferred_callback()\n        self.assertEqual(len(task_d._todo), 1)\n        self.assertEqual(\n            task_d._todo[0].type, elasticdl_pb2.TRAIN_END_CALLBACK\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/tensor_utils_test.py,3,"b'import unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.python.common.tensor_utils import (\n    indexed_slices_to_pb,\n    ndarray_to_pb,\n    pb_to_indexed_slices,\n    pb_to_ndarray,\n)\n\n\nclass TensorUtilsTest(unittest.TestCase):\n    def test_round_trip(self):\n        def verify(array):\n            pb = ndarray_to_pb(array)\n            new_array = pb_to_ndarray(pb)\n            np.testing.assert_array_equal(array, new_array)\n\n        # dtype = np.float32\n        # 1-D array\n        verify(np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32))\n        # 4-D random array\n        verify(np.ndarray(shape=[2, 1, 3, 4], dtype=np.float32))\n\n        # dtype = np.int64\n        # 1-D random array\n        verify(np.array([1, 2, 3, 4], dtype=np.int64))\n        # 4-D random array\n        verify(np.ndarray(shape=[2, 1, 3, 4], dtype=np.int64))\n\n    def test_indexed_slices_round_trip(self):\n        def verify(slices):\n            pb = indexed_slices_to_pb(slices)\n            new_slices = pb_to_indexed_slices(pb)\n            np.testing.assert_array_equal(slices.values, new_slices.values)\n            np.testing.assert_array_equal(slices.indices, new_slices.indices)\n\n        # dtype = np.float32\n        verify(\n            tf.IndexedSlices(\n                np.array([1.0, 2.0, 3.0], dtype=np.float32),\n                np.array([0, 2, 1]),\n            )\n        )\n        # dtype = np.int64\n        verify(\n            tf.IndexedSlices(\n                np.array([1, 2, 3], dtype=np.int64), np.array([0, 2, 1])\n            )\n        )\n\n        slices = tf.IndexedSlices(\n            np.array([1, 2, 3], dtype=np.int64),\n            np.array([[0, 1], [1, 2], [2, 3]]),\n        )\n        self.assertRaisesRegex(\n            ValueError,\n            ""IndexedSlices pb only accepts indices with one dimension"",\n            verify,\n            slices,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/test_call_back.py,0,"b'""""""Callback for unittests.""""""\nfrom abc import ABC, abstractmethod\n\nON_REPORT_GRADIENT_BEGIN = ""on_report_gradient_begin""\nON_REPORT_EVALUATION_METRICS_BEGIN = ""on_report_evaluation_metrics_begin""\n\n\nclass BaseCallback(ABC):\n    """"""Base class of callbacks used for testing.""""""\n\n    def __init__(self, master, worker, call_times):\n        self._master = master\n        self._worker = worker\n        self.call_times = call_times\n\n    @abstractmethod\n    def __call__(self):\n        pass\n'"
elasticdl/python/tests/test_module.py,9,"b'import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Input\n\nfrom elasticdl.python.data.reader.recordio_reader import RecordIODataReader\nfrom elasticdl.python.worker.prediction_outputs_processor import (\n    BasePredictionOutputsProcessor,\n)\n\n\ndef custom_model():\n    inputs = Input(shape=(1, 1), name=""x"")\n    outputs = Dense(1)(inputs)\n    return Model(inputs, outputs)\n\n\ndef loss(labels, predictions):\n    return tf.reduce_mean(tf.square(predictions - labels))\n\n\ndef keras_loss(labels, predictions):\n    return tf.keras.losses.mean_squared_error(labels, predictions)\n\n\ndef dataset_fn(dataset, mode, metadata):\n    def _parse_data(record):\n        feature_description = {\n            ""x"": tf.io.FixedLenFeature([1], tf.float32),\n            ""y"": tf.io.FixedLenFeature([1], tf.float32),\n        }\n        r = tf.io.parse_single_example(record, feature_description)\n        return {""x"": r[""x""]}, r[""y""]\n\n    dataset = dataset.map(_parse_data)\n    return dataset\n\n\ndef optimizer(lr=0.1):\n    return tf.optimizers.SGD(lr)\n\n\ndef ftrl_optimizer(lr=0.1):\n    return tf.optimizers.Ftrl(lr)\n\n\ndef eval_metrics_fn():\n    return {""mse"": lambda labels, outputs: tf.square(outputs - labels)}\n\n\ndef callbacks():\n    return [tf.keras.callbacks.Callback()]\n\n\nclass PredictionOutputsProcessor(BasePredictionOutputsProcessor):\n    def __init__(self):\n        pass\n\n    def process(self, predictions, worker_id):\n        pass\n\n\nclass CustomDataReader(RecordIODataReader):\n    def __init__(self, **kwargs):\n        RecordIODataReader.__init__(self, **kwargs)\n\n    def custom_method(self):\n        return ""custom_method""\n\n\ndef custom_data_reader(data_origin, records_per_task=None, **kwargs):\n    return CustomDataReader(data_dir=data_origin)\n'"
elasticdl/python/tests/test_utils.py,61,"b'import csv\nimport os\nimport tempfile\nfrom collections.__init__ import namedtuple\nfrom contextlib import closing\nfrom pathlib import Path\n\nimport grpc\nimport numpy as np\nimport recordio\nimport tensorflow as tf\nfrom odps import ODPS\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.args import parse_worker_args\nfrom elasticdl.python.common.constants import (\n    DistributionStrategy,\n    JobType,\n    MaxComputeConfig,\n)\nfrom elasticdl.python.common.grpc_utils import build_channel\nfrom elasticdl.python.common.model_utils import (\n    get_module_file_path,\n    load_module,\n)\nfrom elasticdl.python.common.save_utils import CheckpointSaver\nfrom elasticdl.python.data.recordio_gen.frappe_recordio_gen import (\n    load_raw_data,\n)\nfrom elasticdl.python.master.evaluation_service import EvaluationService\nfrom elasticdl.python.master.servicer import MasterServicer\nfrom elasticdl.python.master.task_dispatcher import _TaskDispatcher\nfrom elasticdl.python.ps.parameter_server import Parameters, ParameterServer\nfrom elasticdl.python.tests.in_process_master import InProcessMaster\nfrom elasticdl.python.worker.worker import Worker\n\n\nclass PserverArgs(object):\n    def __init__(\n        self,\n        grads_to_wait=8,\n        lr_staleness_modulation=0,\n        sync_version_tolerance=0,\n        use_async=False,\n        model_zoo=None,\n        model_def=None,\n        optimizer=""optimizer"",\n        port=9999,\n        log_level=""INFO"",\n        job_name=""test_pserver"",\n        namespace=""default"",\n        master_addr=""test:1111"",\n        evaluation_steps=0,\n        checkpoint_dir=None,\n        checkpoint_steps=None,\n        keep_checkpoint_max=0,\n        ps_id=0,\n        num_ps_pods=1,\n        num_workers=2,\n        checkpoint_dir_for_init=None,\n    ):\n        self.grads_to_wait = grads_to_wait\n        self.lr_staleness_modulation = lr_staleness_modulation\n        self.sync_version_tolerance = sync_version_tolerance\n        self.use_async = use_async\n        self.model_zoo = model_zoo\n        self.model_def = model_def\n        self.optimizer = optimizer\n        self.port = port\n        self.log_level = log_level\n        self.job_name = job_name\n        self.namespace = namespace\n        self.master_addr = master_addr\n        self.evaluation_steps = evaluation_steps\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint_steps = checkpoint_steps\n        self.keep_checkpoint_max = keep_checkpoint_max\n        self.ps_id = ps_id\n        self.num_ps_pods = num_ps_pods\n        self.num_workers = num_workers\n        self.checkpoint_dir_for_init = checkpoint_dir_for_init\n\n\nclass DatasetName(object):\n    IMAGENET = ""imagenet1""\n    FRAPPE = ""frappe1""\n    TEST_MODULE = ""test_module1""\n    IMAGE_DEFAULT = ""image_default1""\n    CENSUS = ""census1""\n\n\ndef create_recordio_file(size, dataset_name, shape, temp_dir=None):\n    """"""Creates a temporary file containing data of `recordio` format.\n\n    Args:\n        size: The number of records in the temporary file.\n        dataset_name: A dataset name from `DatasetName`.\n        shape: The shape of records to be created.\n        temp_dir: The storage path of the temporary file.\n\n    Returns:\n        A python string indicating the temporary file name.\n    """"""\n    temp_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir)\n    with closing(recordio.Writer(temp_file.name)) as f:\n        for _ in range(size):\n            if dataset_name == DatasetName.IMAGENET:\n                image = np.random.randint(255, size=shape, dtype=np.uint8)\n                image = tf.image.encode_jpeg(tf.convert_to_tensor(value=image))\n                image = image.numpy()\n                label = np.ndarray([1], dtype=np.int64)\n                label[0] = np.random.randint(1, 11)\n                example_dict = {\n                    ""image"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[image])\n                    ),\n                    ""label"": tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[label])\n                    ),\n                }\n            elif dataset_name == DatasetName.FRAPPE:\n                feature = np.random.randint(5383, size=(shape,))\n                label = np.random.randint(2, size=(1,))\n                example_dict = {\n                    ""feature"": tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=feature)\n                    ),\n                    ""label"": tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[label])\n                    ),\n                }\n            elif dataset_name == DatasetName.TEST_MODULE:\n                x = np.random.rand(shape).astype(np.float32)\n                y = 2 * x + 1\n                example_dict = {\n                    ""x"": tf.train.Feature(\n                        float_list=tf.train.FloatList(value=x)\n                    ),\n                    ""y"": tf.train.Feature(\n                        float_list=tf.train.FloatList(value=y)\n                    ),\n                }\n            elif dataset_name == DatasetName.IMAGE_DEFAULT:\n                image = np.random.rand(np.prod(shape)).astype(np.float32)\n                label = np.ndarray([1], dtype=np.int64)\n                label[0] = np.random.randint(0, 10)\n                example_dict = {\n                    ""image"": tf.train.Feature(\n                        float_list=tf.train.FloatList(value=image)\n                    ),\n                    ""label"": tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=[label])\n                    ),\n                }\n            elif dataset_name == DatasetName.CENSUS:\n                example_dict = {\n                    ""workclass"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""Private""])\n                    ),\n                    ""education"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""HS-grad""])\n                    ),\n                    ""marital-status"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""Widowed""])\n                    ),\n                    ""occupation"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(\n                            value=[b""Exec-managerial""]\n                        )\n                    ),\n                    ""relationship"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""Not-in-family""])\n                    ),\n                    ""race"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""White""])\n                    ),\n                    ""sex"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""Female""])\n                    ),\n                    ""native-country"": tf.train.Feature(\n                        bytes_list=tf.train.BytesList(value=[b""United-States""])\n                    ),\n                    ""age"": tf.train.Feature(\n                        float_list=tf.train.FloatList(\n                            value=[np.random.randint(10, 100)]\n                        )\n                    ),\n                    ""capital-gain"": tf.train.Feature(\n                        float_list=tf.train.FloatList(\n                            value=[np.random.randint(100, 4000)]\n                        )\n                    ),\n                    ""capital-loss"": tf.train.Feature(\n                        float_list=tf.train.FloatList(\n                            value=[np.random.randint(2000, 7000)]\n                        )\n                    ),\n                    ""hours-per-week"": tf.train.Feature(\n                        float_list=tf.train.FloatList(\n                            value=[np.random.randint(10, 70)]\n                        )\n                    ),\n                    ""label"": tf.train.Feature(\n                        int64_list=tf.train.Int64List(\n                            value=[np.random.randint(0, 2)]\n                        )\n                    ),\n                }\n            else:\n                raise ValueError(""Unknown dataset name %s."" % dataset_name)\n\n            example = tf.train.Example(\n                features=tf.train.Features(feature=example_dict)\n            )\n            f.write(example.SerializeToString())\n    return temp_file.name\n\n\ndef create_iris_csv_file(size, columns, temp_dir=None):\n    """"""Creates a temporary CSV file.\n\n    Args:\n        size: The number of records in the CSV file.\n        columns: The names of columns in the CSV file.\n        temp_dir: The storage path of the CSV file.\n\n    Returns:\n        A python string indicating the temporary file name.\n    """"""\n    temp_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir)\n\n    features = np.random.random((size, 4))\n    features = np.round(features, 4)\n    labels = np.random.randint(0, 2, (size, 1))\n    value_data = np.concatenate((features, labels), axis=1)\n\n    csv_file_name = temp_file.name + "".csv""\n    with open(csv_file_name, ""w"", newline="""") as csv_file:\n        csv_writer = csv.writer(csv_file)\n        csv_writer.writerow(columns)\n        csv_writer.writerows(value_data)\n\n    return csv_file_name\n\n\ndef create_pserver(\n    model_zoo_path, model_def, grads_to_wait, use_async, num_ps_pods\n):\n    ports = [i + 12345 for i in range(num_ps_pods)]\n    channels = []\n    for port in ports:\n        addr = ""localhost:%d"" % port\n        channel = build_channel(addr)\n        channels.append(channel)\n\n    pservers = []\n    for port in ports:\n        args = PserverArgs(\n            grads_to_wait=grads_to_wait,\n            use_async=True,\n            port=port,\n            model_zoo=model_zoo_path,\n            model_def=model_def,\n        )\n        pserver = ParameterServer(args)\n        pserver.prepare()\n        pservers.append(pserver)\n    return ports, channels, pservers\n\n\ndef distributed_train_and_evaluate(\n    feature_shape,\n    model_zoo_path,\n    model_def,\n    model_params="""",\n    eval_metrics_fn=""eval_metrics_fn"",\n    loss=""loss"",\n    training=True,\n    dataset_name=DatasetName.IMAGE_DEFAULT,\n    callback_classes=[],\n    use_async=False,\n    get_model_steps=1,\n    ps_channels=None,\n    pservers=None,\n    distribution_strategy=DistributionStrategy.PARAMETER_SERVER,\n):\n    """"""Runs distributed training and evaluation with a local master. Grpc\n    calls are mocked by local master call.\n\n    Args:\n        feature_shape: The shape of model input.\n        model_zoo_path: The directory that contains user-defined model files\n            or a specific model file.\n        model_def: The import path to the model definition function/class in\n            the model zoo, e.g.  ""cifar10_subclass.CustomModel"".\n        model_params: The dictionary of model parameters in a string that will\n            be used to instantiate the model, e.g. ""param1=1,param2=2"".\n        eval_metrics_fn: The name of the evaluation metrics function defined\n            in the model file.\n        loss: The name of the loss function defined in the model file.\n        training: True for job type `TRAIN_WITH_EVALUATION`, False for\n            job type `EVALUATION`.\n        dataset_name: A dataset name from `DatasetName`.\n        callback_classes: A List of callbacks that will be called at given\n            stages of the training procedure.\n        use_async: A bool. True if using asynchronous updates.\n        get_model_steps: Worker will perform `get_model` from the parameter\n            server every this many steps.\n        ps_channels: A channel list to all parameter server pods.\n        pservers: A list of parameter server pods.\n        distribution_strategy: The distribution startegy used by workers, e.g.\n            DistributionStrategy.PARAMETER_SERVER or\n            DistributionStrategy.AllreduceStrategy.\n\n    Returns:\n        An integer indicating the model version after the distributed training\n        and evaluation.\n    """"""\n    job_type = (\n        JobType.TRAINING_WITH_EVALUATION\n        if training\n        else JobType.EVALUATION_ONLY\n    )\n    evaluation_steps = 1 if job_type == JobType.TRAINING_WITH_EVALUATION else 0\n    batch_size = 8 if dataset_name == DatasetName.IMAGENET else 16\n    pservers = pservers or []\n    ps_channels = ps_channels or []\n\n    model_module = load_module(\n        get_module_file_path(model_zoo_path, model_def)\n    ).__dict__\n\n    for channel in ps_channels:\n        grpc.channel_ready_future(channel).result()\n    worker_arguments = [\n        ""--worker_id"",\n        ""1"",\n        ""--job_type"",\n        job_type,\n        ""--minibatch_size"",\n        batch_size,\n        ""--model_zoo"",\n        model_zoo_path,\n        ""--model_def"",\n        model_def,\n        ""--model_params"",\n        model_params,\n        ""--loss"",\n        loss,\n        ""--get_model_steps"",\n        get_model_steps,\n        ""--distribution_strategy"",\n        distribution_strategy,\n    ]\n    args = parse_worker_args(worker_arguments)\n    worker = Worker(args, ps_channels=ps_channels)\n\n    if dataset_name in [DatasetName.IMAGENET, DatasetName.FRAPPE]:\n        record_num = batch_size\n    else:\n        record_num = 128\n    shards = {\n        create_recordio_file(record_num, dataset_name, feature_shape): (\n            0,\n            record_num,\n        )\n    }\n    if training:\n        training_shards = shards\n        evaluation_shards = shards\n    else:\n        training_shards = {}\n        evaluation_shards = shards\n    task_d = _TaskDispatcher(\n        training_shards,\n        evaluation_shards,\n        {},\n        records_per_task=64,\n        num_epochs=1,\n    )\n\n    if training:\n        evaluation_service = EvaluationService(\n            None,\n            task_d,\n            0,\n            0,\n            evaluation_steps,\n            False,\n            model_module[eval_metrics_fn],\n        )\n    else:\n        evaluation_service = EvaluationService(\n            None,\n            task_d,\n            0,\n            0,\n            evaluation_steps,\n            True,\n            model_module[eval_metrics_fn],\n        )\n    task_d.set_evaluation_service(evaluation_service)\n\n    master = MasterServicer(\n        batch_size, task_d, evaluation_service=evaluation_service,\n    )\n    callbacks = [\n        callback_class(master, worker) for callback_class in callback_classes\n    ]\n\n    in_process_master = InProcessMaster(master, callbacks)\n    worker._stub = in_process_master\n    for pservicer in pservers:\n        pservicer._master_stub = in_process_master\n\n    worker.run()\n\n    req = elasticdl_pb2.GetTaskRequest()\n    req.worker_id = 1\n    task = master.get_task(req, None)\n    # No more task.\n    if task.shard_name:\n        raise RuntimeError(\n            ""There are some tasks unfinished after worker exits.""\n        )\n    return master._version\n\n\nIRIS_TABLE_COLUMN_NAMES = [\n    ""sepal_length"",\n    ""sepal_width"",\n    ""petal_length"",\n    ""petal_width"",\n    ""class"",\n]\n\n\ndef create_iris_odps_table(odps_client, project_name, table_name):\n    sql_tmpl = """"""\n    DROP TABLE IF EXISTS {PROJECT_NAME}.{TABLE_NAME};\n    CREATE TABLE {PROJECT_NAME}.{TABLE_NAME} (\n           sepal_length DOUBLE,\n           sepal_width  DOUBLE,\n           petal_length DOUBLE,\n           petal_width  DOUBLE,\n           class BIGINT);\n\n    INSERT INTO {PROJECT_NAME}.{TABLE_NAME} VALUES\n    (6.4,2.8,5.6,2.2,2),\n    (5.0,2.3,3.3,1.0,1),\n    (4.9,2.5,4.5,1.7,2),\n    (4.9,3.1,1.5,0.1,0),\n    (5.7,3.8,1.7,0.3,0),\n    (4.4,3.2,1.3,0.2,0),\n    (5.4,3.4,1.5,0.4,0),\n    (6.9,3.1,5.1,2.3,2),\n    (6.7,3.1,4.4,1.4,1),\n    (5.1,3.7,1.5,0.4,0),\n    (5.2,2.7,3.9,1.4,1),\n    (6.9,3.1,4.9,1.5,1),\n    (5.8,4.0,1.2,0.2,0),\n    (5.4,3.9,1.7,0.4,0),\n    (7.7,3.8,6.7,2.2,2),\n    (6.3,3.3,4.7,1.6,1),\n    (6.8,3.2,5.9,2.3,2),\n    (7.6,3.0,6.6,2.1,2),\n    (6.4,3.2,5.3,2.3,2),\n    (5.7,4.4,1.5,0.4,0),\n    (6.7,3.3,5.7,2.1,2),\n    (6.4,2.8,5.6,2.1,2),\n    (5.4,3.9,1.3,0.4,0),\n    (6.1,2.6,5.6,1.4,2),\n    (7.2,3.0,5.8,1.6,2),\n    (5.2,3.5,1.5,0.2,0),\n    (5.8,2.6,4.0,1.2,1),\n    (5.9,3.0,5.1,1.8,2),\n    (5.4,3.0,4.5,1.5,1),\n    (6.7,3.0,5.0,1.7,1),\n    (6.3,2.3,4.4,1.3,1),\n    (5.1,2.5,3.0,1.1,1),\n    (6.4,3.2,4.5,1.5,1),\n    (6.8,3.0,5.5,2.1,2),\n    (6.2,2.8,4.8,1.8,2),\n    (6.9,3.2,5.7,2.3,2),\n    (6.5,3.2,5.1,2.0,2),\n    (5.8,2.8,5.1,2.4,2),\n    (5.1,3.8,1.5,0.3,0),\n    (4.8,3.0,1.4,0.3,0),\n    (7.9,3.8,6.4,2.0,2),\n    (5.8,2.7,5.1,1.9,2),\n    (6.7,3.0,5.2,2.3,2),\n    (5.1,3.8,1.9,0.4,0),\n    (4.7,3.2,1.6,0.2,0),\n    (6.0,2.2,5.0,1.5,2),\n    (4.8,3.4,1.6,0.2,0),\n    (7.7,2.6,6.9,2.3,2),\n    (4.6,3.6,1.0,0.2,0),\n    (7.2,3.2,6.0,1.8,2),\n    (5.0,3.3,1.4,0.2,0),\n    (6.6,3.0,4.4,1.4,1),\n    (6.1,2.8,4.0,1.3,1),\n    (5.0,3.2,1.2,0.2,0),\n    (7.0,3.2,4.7,1.4,1),\n    (6.0,3.0,4.8,1.8,2),\n    (7.4,2.8,6.1,1.9,2),\n    (5.8,2.7,5.1,1.9,2),\n    (6.2,3.4,5.4,2.3,2),\n    (5.0,2.0,3.5,1.0,1),\n    (5.6,2.5,3.9,1.1,1),\n    (6.7,3.1,5.6,2.4,2),\n    (6.3,2.5,5.0,1.9,2),\n    (6.4,3.1,5.5,1.8,2),\n    (6.2,2.2,4.5,1.5,1),\n    (7.3,2.9,6.3,1.8,2),\n    (4.4,3.0,1.3,0.2,0),\n    (7.2,3.6,6.1,2.5,2),\n    (6.5,3.0,5.5,1.8,2),\n    (5.0,3.4,1.5,0.2,0),\n    (4.7,3.2,1.3,0.2,0),\n    (6.6,2.9,4.6,1.3,1),\n    (5.5,3.5,1.3,0.2,0),\n    (7.7,3.0,6.1,2.3,2),\n    (6.1,3.0,4.9,1.8,2),\n    (4.9,3.1,1.5,0.1,0),\n    (5.5,2.4,3.8,1.1,1),\n    (5.7,2.9,4.2,1.3,1),\n    (6.0,2.9,4.5,1.5,1),\n    (6.4,2.7,5.3,1.9,2),\n    (5.4,3.7,1.5,0.2,0),\n    (6.1,2.9,4.7,1.4,1),\n    (6.5,2.8,4.6,1.5,1),\n    (5.6,2.7,4.2,1.3,1),\n    (6.3,3.4,5.6,2.4,2),\n    (4.9,3.1,1.5,0.1,0),\n    (6.8,2.8,4.8,1.4,1),\n    (5.7,2.8,4.5,1.3,1),\n    (6.0,2.7,5.1,1.6,1),\n    (5.0,3.5,1.3,0.3,0),\n    (6.5,3.0,5.2,2.0,2),\n    (6.1,2.8,4.7,1.2,1),\n    (5.1,3.5,1.4,0.3,0),\n    (4.6,3.1,1.5,0.2,0),\n    (6.5,3.0,5.8,2.2,2),\n    (4.6,3.4,1.4,0.3,0),\n    (4.6,3.2,1.4,0.2,0),\n    (7.7,2.8,6.7,2.0,2),\n    (5.9,3.2,4.8,1.8,1),\n    (5.1,3.8,1.6,0.2,0),\n    (4.9,3.0,1.4,0.2,0),\n    (4.9,2.4,3.3,1.0,1),\n    (4.5,2.3,1.3,0.3,0),\n    (5.8,2.7,4.1,1.0,1),\n    (5.0,3.4,1.6,0.4,0),\n    (5.2,3.4,1.4,0.2,0),\n    (5.3,3.7,1.5,0.2,0),\n    (5.0,3.6,1.4,0.2,0),\n    (5.6,2.9,3.6,1.3,1),\n    (4.8,3.1,1.6,0.2,0);\n    """"""\n    odps_client.execute_sql(\n        sql_tmpl.format(PROJECT_NAME=project_name, TABLE_NAME=table_name),\n        hints={""odps.sql.submit.mode"": ""script""},\n    )\n\n\ndef get_odps_client_from_env():\n    project = os.environ[MaxComputeConfig.PROJECT_NAME]\n    access_id = os.environ[MaxComputeConfig.ACCESS_ID]\n    access_key = os.environ[MaxComputeConfig.ACCESS_KEY]\n    endpoint = os.environ.get(MaxComputeConfig.ENDPOINT)\n    return ODPS(access_id, access_key, project, endpoint)\n\n\ndef create_iris_odps_table_from_env():\n    project = os.environ[MaxComputeConfig.PROJECT_NAME]\n    table_name = os.environ[""MAXCOMPUTE_TABLE""]\n    create_iris_odps_table(get_odps_client_from_env(), project, table_name)\n\n\ndef delete_iris_odps_table_from_env():\n    project = os.environ[MaxComputeConfig.PROJECT_NAME]\n    table_name = os.environ[""MAXCOMPUTE_TABLE""]\n    get_odps_client_from_env().delete_table(\n        table_name, project, if_exists=True\n    )\n\n\ndef get_random_batch(batch_size):\n    shape = (28, 28)\n    shape = (batch_size,) + shape\n    num_classes = 10\n    images = tf.random.uniform(shape)\n    labels = tf.random.uniform(\n        [batch_size], minval=0, maxval=num_classes, dtype=tf.int32\n    )\n    return images, labels\n\n\ndef get_mnist_dataset(batch_size):\n    (\n        (x_train, y_train),\n        (x_test, y_test),\n    ) = tf.keras.datasets.mnist.load_data()\n    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32) / 255.0\n    y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n\n    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.0\n    y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n\n    db = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    db = db.batch(batch_size).repeat(2)\n    test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    test_db = test_db.batch(batch_size)\n\n    return db, test_db\n\n\ndef get_frappe_dataset(batch_size):\n    home = str(Path.home())\n    Args = namedtuple(""Args"", [""data""])\n    args = Args(data=os.path.join(home, "".keras/datasets""))\n    x_train, y_train, x_val, y_val, x_test, y_test = load_raw_data(args)\n    x_train = tf.convert_to_tensor(x_train, dtype=tf.int64)\n    x_test = tf.convert_to_tensor(x_test, dtype=tf.int64)\n    y_train = tf.convert_to_tensor(y_train, dtype=tf.int64)\n    y_test = tf.convert_to_tensor(y_test, dtype=tf.int64)\n\n    db = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    db = db.batch(batch_size).repeat(2)\n    test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    test_db = test_db.batch(batch_size)\n    return db, test_db\n\n\ndef save_checkpoint_without_embedding(model, checkpoint_dir, version=100):\n    checkpoint_saver = CheckpointSaver(checkpoint_dir, 0, 0, False)\n    params = Parameters()\n    for var in model.trainable_variables:\n        params.non_embedding_params[var.name] = var\n    params.version = version\n    model_pb = params.to_model_pb()\n    checkpoint_saver.save(version, model_pb, False)\n'"
elasticdl/python/tests/worker_allreduce_strategy_test.py,0,"b'import os\nimport unittest\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.args import parse_worker_args\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.tests.test_utils import get_mnist_dataset\nfrom elasticdl.python.worker.worker import Worker\n\n\nclass WorkerAllReduceStrategyTest(unittest.TestCase):\n    def setUp(self):\n        self._model_zoo_path = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)), ""../../../model_zoo""\n        )\n        self._model_def = (\n            ""mnist_functional_api.mnist_functional_api.custom_model""\n        )\n        self._batch_size = 16\n        self._test_steps = 10\n        self._workers = []\n        self._create_worker(2)\n\n    def _create_worker(self, worker_num, max_allreduce_retry_num=0):\n        for i in range(worker_num):\n            arguments = [\n                ""--worker_id"",\n                i,\n                ""--job_type"",\n                elasticdl_pb2.TRAINING,\n                ""--minibatch_size"",\n                self._batch_size,\n                ""--model_zoo"",\n                self._model_zoo_path,\n                ""--model_def"",\n                self._model_def,\n                ""--distribution_strategy"",\n                DistributionStrategy.ALLREDUCE,\n            ]\n            args = parse_worker_args(arguments)\n            worker = Worker(\n                args, max_allreduce_retry_num=max_allreduce_retry_num\n            )\n            self._workers.append(worker)\n\n    def test_collect_gradients_with_allreduce_success_case(self):\n        worker = self._workers[0]\n        train_db, _ = get_mnist_dataset(self._batch_size)\n        for step, (x, y) in enumerate(train_db):\n            if step == 0:\n                worker._run_model_call_before_training(x)\n            w_loss, w_grads = worker.training_process_eagerly(x, y)\n            if step == self._test_steps:\n                break\n            self.assertEqual(\n                worker._collect_gradients_with_allreduce_robust(w_grads), True,\n            )\n\n    def test_collect_gradients_with_allreduce_failure_case(self):\n        worker = self._workers[1]\n        train_db, _ = get_mnist_dataset(self._batch_size)\n        for step, (x, y) in enumerate(train_db):\n            if step == 0:\n                worker._run_model_call_before_training(x)\n            if step == self._test_steps:\n                break\n            self.assertEqual(\n                worker._calculate_grads_and_report_with_allreduce(None),\n                False,\n                ""Should fail when no data is received"",\n            )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/tests/worker_ps_interaction_test.py,21,"b'import os\nimport unittest\nfrom threading import Thread\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.args import parse_worker_args\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.common.hash_utils import int_to_id, string_to_id\nfrom elasticdl.python.common.model_utils import get_model_spec\nfrom elasticdl.python.ps.embedding_table import EmbeddingTable\nfrom elasticdl.python.tests.test_utils import (\n    create_pserver,\n    get_frappe_dataset,\n    get_mnist_dataset,\n    get_random_batch,\n)\nfrom elasticdl.python.worker.worker import Worker\n\n\nclass WorkerPSInteractionTest(unittest.TestCase):\n    def setUp(self):\n        self._model_zoo_path = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)), ""../../../model_zoo""\n        )\n        self._batch_size = 16\n        self._channels = []\n        self._pservers = []\n        self._workers = []\n\n    def tearDown(self):\n        for pserver in self._pservers:\n            pserver.server.stop(0)\n\n    def _create_pserver(self, model_def, num):\n        self._ports, self._channels, self._pservers = create_pserver(\n            self._model_zoo_path,\n            model_def,\n            grads_to_wait=1,\n            use_async=True,\n            num_ps_pods=num,\n        )\n        self._model_def = model_def\n\n    def _reset_pserver(self):\n        for ps in self._pservers:\n            ps.parameters.reset()\n\n    def _create_worker(self, worker_num):\n        for i in range(worker_num):\n            tf.keras.backend.clear_session()\n            tf.random.set_seed(22)\n            arguments = [\n                ""--worker_id"",\n                i,\n                ""--job_type"",\n                elasticdl_pb2.TRAINING,\n                ""--minibatch_size"",\n                self._batch_size,\n                ""--model_zoo"",\n                self._model_zoo_path,\n                ""--model_def"",\n                self._model_def,\n                ""--distribution_strategy"",\n                DistributionStrategy.PARAMETER_SERVER,\n            ]\n            args = parse_worker_args(arguments)\n            worker = Worker(args, ps_channels=self._channels)\n            self._workers.append(worker)\n\n    def _worker_train(\n        self, worker_id, train_db, test_db, stop_step, use_tf_function=False\n    ):\n        worker = self._workers[worker_id]\n        acc_meter = tf.keras.metrics.Accuracy()\n        worker_results = []\n        for step, (x, y) in enumerate(train_db):\n            if step == 0:\n                worker._run_model_call_before_training(x)\n\n            worker.get_model()\n            if use_tf_function:\n                w_loss, w_grads = worker.training_process_with_acceleration(\n                    x, y\n                )\n            else:\n                w_loss, w_grads = worker.training_process_eagerly(x, y)\n            worker.report_gradient(w_grads)\n\n            if step % 20 == 0:\n                worker.get_model()\n                for (x, y) in test_db:\n                    out = worker.forward_process(x)\n                    if ""mnist"" in self._model_def:\n                        acc_meter.update_state(tf.argmax(out, axis=1), y)\n                    else:\n                        out[""probs""] = tf.reshape(out[""probs""], [-1])\n                        acc_meter.update_state(\n                            tf.where(\n                                out[""probs""] < 0.5,\n                                x=tf.zeros_like(y),\n                                y=tf.ones_like(y),\n                            ),\n                            y,\n                        )\n                worker_results.append(\n                    (float(w_loss.numpy()), float(acc_meter.result().numpy()))\n                )\n                acc_meter.reset_states()\n\n            if step > stop_step:\n                break\n        return worker_results\n\n    def _test_deepfm_train(self, num_ps, num_worker, stop_step):\n        model_def = ""deepfm_functional_api.deepfm_functional_api.custom_model""\n        self._create_pserver(model_def, num_ps)\n        db, test_db = get_frappe_dataset(self._batch_size)\n\n        self._create_worker(num_worker)\n        threads = []\n        for w in range(num_worker):\n            t = Thread(\n                target=self._worker_train, args=(w, db, test_db, stop_step)\n            )\n            t.start()\n            threads.append(t)\n        for t in threads:\n            t.join()\n\n    def test_worker_pull_embedding(self):\n        model_def = ""mnist_functional_api.mnist_functional_api.custom_model""\n        self._create_pserver(model_def, 2)\n        arguments = [\n            ""--worker_id"",\n            0,\n            ""--job_type"",\n            elasticdl_pb2.TRAINING,\n            ""--minibatch_size"",\n            self._batch_size,\n            ""--model_zoo"",\n            self._model_zoo_path,\n            ""--model_def"",\n            model_def,\n            ""--distribution_strategy"",\n            DistributionStrategy.PARAMETER_SERVER,\n        ]\n        args = parse_worker_args(arguments)\n        worker = Worker(args, ps_channels=self._channels)\n\n        # Test lookup embedding vectors that do not exist\n        layers = [""test-2"", ""test-2-slot""]\n        ids = [3, 5, 1, 6, 10, 2, 1, 2, 4, 7, 9]\n        embedding_table_args = [\n            (layers[0], 8, ""uniform"", False),\n            (layers[1], 8, 3.3, True),\n        ]\n\n        # initialize embedding table object\n        for pserver in self._pservers:\n            for layer, table_args in zip(layers, embedding_table_args):\n                pserver.parameters.embedding_params[layer] = EmbeddingTable(\n                    *table_args\n                )\n\n        result_dict = {}\n        for layer in layers:\n            embedding = worker.pull_embedding_vectors(layer, ids)\n            result_dict[layer] = embedding\n\n        for layer in layers:\n            expected_result = []\n            for embedding_id in ids:\n                ps_id = int_to_id(embedding_id, len(self._pservers))\n                table = self._pservers[ps_id].parameters.embedding_params[\n                    layer\n                ]\n                expected_result.append(table.get([embedding_id]))\n            expected_result = np.concatenate(expected_result)\n            self.assertTrue(np.allclose(expected_result, result_dict[layer]))\n\n    def test_compare_onebatch_train(self):\n        model_def = ""mnist_functional_api.mnist_functional_api.custom_model""\n        self._create_pserver(model_def, 2)\n        images, labels = get_random_batch(self._batch_size)\n        # TODO(yunjian.lmh): test optimizer wrapper\n        arguments = [\n            ""--worker_id"",\n            0,\n            ""--job_type"",\n            elasticdl_pb2.TRAINING,\n            ""--minibatch_size"",\n            self._batch_size,\n            ""--model_zoo"",\n            self._model_zoo_path,\n            ""--model_def"",\n            model_def,\n            ""--distribution_strategy"",\n            DistributionStrategy.PARAMETER_SERVER,\n        ]\n        args = parse_worker_args(arguments)\n\n        tf.keras.backend.clear_session()\n        tf.random.set_seed(22)\n\n        worker = Worker(args, ps_channels=self._channels)\n        worker._run_model_call_before_training(images)\n        worker.get_model()\n        w_loss, w_grads = worker.training_process_eagerly(images, labels)\n        worker.report_gradient(w_grads)\n\n        tf.keras.backend.clear_session()\n        tf.random.set_seed(22)\n\n        (\n            model,\n            dataset_fn,\n            loss_fn,\n            opt_fn,\n            eval_metrics_fn,\n            prediction_outputs_processor,\n            create_data_reader_fn,\n            callback_list,\n        ) = get_model_spec(\n            model_zoo=self._model_zoo_path,\n            model_def=model_def,\n            dataset_fn=""dataset_fn"",\n            model_params=None,\n            loss=""loss"",\n            optimizer=""optimizer"",\n            eval_metrics_fn=""eval_metrics_fn"",\n            prediction_outputs_processor=""PredictionOutputsProcessor"",\n            custom_data_reader=""custom_data_reader"",\n            callbacks=""callbacks"",\n        )\n\n        with tf.GradientTape() as tape:\n            output = model.call(images, training=True)\n            labels = tf.reshape(labels, [-1])\n            loss = loss_fn(labels, output)\n        grads = tape.gradient(loss, model.trainable_variables)\n        opt_fn().apply_gradients(zip(grads, model.trainable_variables))\n\n        for v in model.trainable_variables:\n            ps_id = string_to_id(v.name, len(self._channels))\n            ps_v = self._pservers[ps_id].parameters.get_non_embedding_param(\n                v.name\n            )\n            np.testing.assert_array_equal(ps_v.numpy(), v.numpy())\n\n    def test_compare_mnist_train(self):\n        model_def = ""mnist_functional_api.mnist_functional_api.custom_model""\n        self._create_pserver(model_def, 2)\n        db, test_db = get_mnist_dataset(self._batch_size)\n        stop_step = 20\n\n        self._create_worker(1)\n        worker_results = self._worker_train(\n            0, train_db=db, test_db=test_db, stop_step=stop_step\n        )\n\n        tf.keras.backend.clear_session()\n        tf.random.set_seed(22)\n\n        acc_meter = tf.keras.metrics.Accuracy()\n\n        (\n            model,\n            dataset_fn,\n            loss_fn,\n            opt_fn,\n            eval_metrics_fn,\n            prediction_outputs_processor,\n            create_data_reader_fn,\n            callbacks_list,\n        ) = get_model_spec(\n            model_zoo=self._model_zoo_path,\n            model_def=model_def,\n            dataset_fn=""dataset_fn"",\n            model_params=None,\n            loss=""loss"",\n            optimizer=""optimizer"",\n            eval_metrics_fn=""eval_metrics_fn"",\n            prediction_outputs_processor=""PredictionOutputsProcessor"",\n            custom_data_reader=""custom_data_reader"",\n            callbacks=""callbacks"",\n        )\n        local_results = []\n        for step, (x, y) in enumerate(db):\n            with tf.GradientTape() as tape:\n                out = model.call(x, training=True)\n                ll = loss_fn(y, out)\n            grads = tape.gradient(ll, model.trainable_variables)\n            opt_fn().apply_gradients(zip(grads, model.trainable_variables))\n\n            if step % 20 == 0:\n                for (x, y) in test_db:\n                    out = model.call(x, training=False)\n                    acc_meter.update_state(tf.argmax(out, axis=1), y)\n\n                local_results.append(\n                    (float(ll.numpy()), float(acc_meter.result().numpy()))\n                )\n                acc_meter.reset_states()\n\n            if step > stop_step:\n                break\n\n        for w, l in zip(worker_results, local_results):\n            self.assertTupleEqual(w, l)\n\n    def test_deepfm_train(self):\n        model_def = ""deepfm_functional_api.deepfm_functional_api.custom_model""\n        self._create_pserver(model_def, 2)\n        db, test_db = get_frappe_dataset(self._batch_size)\n        self._create_worker(1)\n        worker_results = self._worker_train(\n            0, train_db=db, test_db=test_db, stop_step=100\n        )\n        acc = max([r[1] for r in worker_results])\n        self.assertLess(0.65, acc)\n\n    def test_deepfm_two_worker_train(self):\n        num_ps = 2\n        num_worker = 2\n        stop_step = 10\n        self._test_deepfm_train(num_ps, num_worker, stop_step)\n\n    def test_deepfm_four_worker_train(self):\n        num_ps = 4\n        num_worker = 1\n        stop_step = 10\n        self._test_deepfm_train(num_ps, num_worker, stop_step)\n\n    def test_restart_ps(self):\n        model_def = ""mnist_functional_api.mnist_functional_api.custom_model""\n        num_data = 8\n        training_data = [\n            get_random_batch(self._batch_size) for _ in range(num_data)\n        ]\n        workers = []\n        self._create_pserver(model_def, 2)\n        for w in range(2):\n            self._reset_pserver()\n            arguments = [\n                ""--worker_id"",\n                0,\n                ""--job_type"",\n                elasticdl_pb2.TRAINING,\n                ""--minibatch_size"",\n                self._batch_size,\n                ""--model_zoo"",\n                self._model_zoo_path,\n                ""--model_def"",\n                model_def,\n                ""--distribution_strategy"",\n                DistributionStrategy.PARAMETER_SERVER,\n            ]\n            args = parse_worker_args(arguments)\n            tf.keras.backend.clear_session()\n            tf.random.set_seed(22)\n            worker = Worker(args, ps_channels=self._channels)\n            workers.append(worker)\n            worker._run_model_call_before_training(training_data[0][0])\n            for i in range(num_data):\n                worker.get_model()\n                w_loss, w_grads = worker.training_process_eagerly(\n                    training_data[i][0], training_data[i][1]\n                )\n                worker.report_gradient(w_grads)\n                if w == 1 and i == 3:\n                    # Restart ps for the 2nd worker at i==3\n                    # self._restart_pserver(model_def)\n                    self._reset_pserver()\n                    # `report_variable` will be called in `get_model` to\n                    # initialize variables on ps with worker variables\n                    worker.get_model()\n                    # send the grads again as these grads are not applied\n                    # on worker variables\n                    worker.report_gradient(w_grads)\n\n        for var_name in workers[0]._non_embed_vars:\n            np.testing.assert_array_equal(\n                workers[0]._non_embed_vars[var_name].numpy(),\n                workers[1]._non_embed_vars[var_name].numpy(),\n            )\n\n    def test_train_acceleration_with_embedding(self):\n        model_def = ""deepfm_functional_api.deepfm_functional_api.custom_model""\n        self._create_pserver(model_def, 2)\n        db, test_db = get_frappe_dataset(self._batch_size)\n        self._create_worker(1)\n        worker_results = self._worker_train(\n            0,\n            train_db=db,\n            test_db=test_db,\n            stop_step=100,\n            use_tf_function=True,\n        )\n        acc = max([r[1] for r in worker_results])\n        self.assertLess(0.65, acc)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
elasticdl/python/worker/__init__.py,0,b''
elasticdl/python/worker/main.py,0,"b'import time\n\nimport grpc\n\nfrom elasticdl.python.common import log_utils\nfrom elasticdl.python.common.args import parse_worker_args\nfrom elasticdl.python.common.constants import DistributionStrategy\nfrom elasticdl.python.common.grpc_utils import build_channel\nfrom elasticdl.python.worker.worker import Worker\n\nCONNECT_PS_MAX_RETRIES = 3\nCONNECT_PS_TIMEOUT = 300\n# The number of seconds we wait for allreduce strategy\'s\n# FTLib consensus service to detect the worker pod\n_ALLREDUCE_STRATEGY_WARM_UP_SECS = 20\n\n\ndef main():\n    args = parse_worker_args()\n    logger = log_utils.get_logger(__name__)\n    logger.info(""Starting worker %d"", args.worker_id)\n    if args.master_addr is None:\n        raise ValueError(""master_addr is missing for worker"")\n\n    master_channel = build_channel(args.master_addr)\n\n    ps_channels = []\n    if args.ps_addrs:\n        ps_addrs = args.ps_addrs.split("","")\n\n        for addr in ps_addrs:\n            # addr is in the form as ""ps-pod-name.namespace.svc:port""\n            channel = build_channel(addr)\n\n            succeeded = False\n            for i in range(CONNECT_PS_MAX_RETRIES):\n                try:\n                    grpc.channel_ready_future(channel).result(\n                        timeout=CONNECT_PS_TIMEOUT\n                    )\n                    logger.info(\n                        ""grpc channel %s to connect pod %s is ready""\n                        % (addr, addr.split(""."")[0])\n                    )\n                    ps_channels.append(channel)\n                    succeeded = True\n                    break\n                except grpc.FutureTimeoutError:\n                    logger.warning(\n                        ""Failed to connect pod %s with %d retry""\n                        % (addr.split(""."")[0], i)\n                    )\n            if not succeeded:\n                raise TimeoutError(\n                    ""Time out to connect pod %s with 3 retries""\n                    % addr.split(""."")[0]\n                )\n\n    if args.distribution_strategy == DistributionStrategy.ALLREDUCE:\n        logger.info(\n            ""Wait for %s seconds for FTLib consensus service to ""\n            ""detect the worker pod"" % str(_ALLREDUCE_STRATEGY_WARM_UP_SECS)\n        )\n        time.sleep(_ALLREDUCE_STRATEGY_WARM_UP_SECS)\n\n    worker = Worker(\n        args,\n        channel=master_channel,\n        ps_channels=ps_channels,\n        set_parallelism=True,\n    )\n    worker.run()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
elasticdl/python/worker/prediction_outputs_processor.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass BasePredictionOutputsProcessor(ABC):\n    """"""\n    This is the base processor for prediction outputs.\n    Users need to implement the abstract methods in order\n    to process the prediction outputs.\n    """"""\n\n    @abstractmethod\n    def process(self, predictions, worker_id):\n        """"""\n        The method that uses to process the prediction outputs produced\n        from a single worker.\n\n        Arguments:\n            predictions: The raw prediction outputs from the model.\n            worker_id: The ID of the worker that produces this\n                batch of predictions.\n        """"""\n        pass\n'"
elasticdl/python/worker/task_data_service.py,5,"b'import threading\nimport time\nfrom collections import deque\n\nimport tensorflow as tf\n\nfrom elasticdl.proto import elasticdl_pb2\nfrom elasticdl.python.common.constants import TaskExecCounterKey\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.data.reader.data_reader_factory import create_data_reader\n\n\nclass TaskDataService(object):\n    def __init__(\n        self,\n        worker,\n        training_with_evaluation,\n        data_reader_params=None,\n        data_origin=None,\n    ):\n        self._worker = worker\n        self._create_data_reader_fn = create_data_reader\n        if self._worker._custom_data_reader is not None:\n            self._create_data_reader_fn = self._worker._custom_data_reader\n        self._training_with_evaluation = training_with_evaluation\n        self._lock = threading.Lock()\n        self._pending_dataset = True\n        self._pending_train_end_callback_task = None\n        if data_reader_params:\n            self.data_reader = self._create_data_reader_fn(\n                data_origin=data_origin, **data_reader_params\n            )\n        else:\n            self.data_reader = self._create_data_reader_fn(\n                data_origin=data_origin\n            )\n        self._warm_up_task = None\n        self._has_warmed_up = False\n        self._failed_record_count = 0\n        self._reported_record_count = 0\n        self._current_task = None\n        self._pending_tasks = deque()\n\n    def _reset(self):\n        """"""\n        Reset pending tasks and record counts\n        """"""\n        self._reported_record_count = 0\n        self._failed_record_count = 0\n        self._pending_tasks = deque()\n        self._current_task = None\n\n    def get_current_task(self):\n        return self._current_task\n\n    def _do_report_task(self, task, err_msg=""""):\n        if self._failed_record_count != 0:\n            exec_counters = {\n                TaskExecCounterKey.FAIL_COUNT: self._failed_record_count\n            }\n        else:\n            exec_counters = None\n        self._worker.report_task_result(\n            task.task_id, err_msg, exec_counters=exec_counters\n        )\n\n    def _log_fail_records(self, task, err_msg):\n        task_len = task.end - task.start\n        msg = (\n            ""records ({f}/{t}) failure, possible ""\n            ""in task_id: {task_id} ""\n            \'reason ""{err_msg}""\'\n        ).format(\n            task_id=task.task_id,\n            err_msg=err_msg,\n            f=self._failed_record_count,\n            t=task_len,\n        )\n        logger.warning(msg)\n\n    def report_record_done(self, count, err_msg=""""):\n        """"""\n        Report the number of records in the latest processed batch,\n        so TaskDataService knows if some pending tasks are finished\n        and report_task_result to the master.\n        Return True if there are some finished tasks, False otherwise.\n        """"""\n        self._reported_record_count += count\n        if err_msg:\n            self._failed_record_count += count\n\n        # TODO(qijun) This is a workaround for #1829\n        if not self._pending_tasks:\n            return False\n        task = self._pending_tasks[0]\n        total_record_num = task.end - task.start\n        if self._reported_record_count >= total_record_num:\n            if err_msg:\n                self._log_fail_records(task, err_msg)\n\n            # Keep popping tasks until the reported record count is less\n            # than the size of the current data since `batch_size` may be\n            # larger than `task.end - task.start`\n            with self._lock:\n                while self._pending_tasks and self._reported_record_count >= (\n                    self._pending_tasks[0].end - self._pending_tasks[0].start\n                ):\n                    task = self._pending_tasks[0]\n                    self._reported_record_count -= task.end - task.start\n                    self._pending_tasks.popleft()\n                    self._do_report_task(task, err_msg)\n                    self._failed_record_count = 0\n                if self._pending_tasks:\n                    self._current_task = self._pending_tasks[0]\n            return True\n        return False\n\n    def get_dataset_gen(self, task):\n        """"""\n        If a task exists, this creates a generator, which could be used to\n        creating a `tf.data.Dataset` object in further.\n        """"""\n        if not task:\n            return None\n        tasks = [task]\n\n        def gen():\n            for task in tasks:\n                for data in self.data_reader.read_records(task):\n                    if data:\n                        yield data\n\n        return gen\n\n    def get_dataset_by_task(self, task):\n        if task is None:\n            return None\n        gen = self.get_dataset_gen(task)\n        dataset = tf.data.Dataset.from_generator(\n            gen, self.data_reader.records_output_types\n        )\n        return dataset\n\n    def get_train_end_callback_task(self):\n        return self._pending_train_end_callback_task\n\n    def clear_train_end_callback_task(self):\n        self._pending_train_end_callback_task = None\n\n    def get_dataset(self):\n        """"""\n        If there\'s more data, this creates a `tf.data.Dataset` object.\n        Otherwise, this returns `None`.\n        """"""\n        if self._pending_dataset:\n            if self._pending_tasks:\n                logger.error(\n                    ""Cannot get new dataset when there are pending tasks""\n                )\n                return None\n            self._reset()\n            # We use a task to perform warm-up for data reader in order\n            # to collect useful metadata. Note that we only performs\n            # data fetching for this task and `break` instantly to make\n            # sure `read_records()` is executed without iterating all the\n            # records so this should not be time consuming.\n            if self._warm_up_task is None and not self._has_warmed_up:\n                while True:\n                    task = self._worker.get_task()\n                    if task.type != elasticdl_pb2.WAIT:\n                        break\n                    time.sleep(2)\n                if task.type == elasticdl_pb2.TRAIN_END_CALLBACK:\n                    self._pending_train_end_callback_task = task\n                    return None\n                elif not task.shard_name:\n                    logger.info(""No more task, stopping"")\n                    return None\n                else:\n                    self._warm_up_task = task\n                    for _ in self.data_reader.read_records(task):\n                        break\n                    self._has_warmed_up = True\n            ds = tf.data.Dataset.from_generator(\n                self._gen, self.data_reader.records_output_types\n            )\n            self._pending_dataset = False\n            return ds\n        else:\n            return None\n\n    def _gen(self):\n        """"""\n        A generator supports the iter() protocol (e.g. a generator function),\n        used to create a `tf.data.Dataset` object from a list of tasks.\n        """"""\n        while True:\n            # Make sure we also generate data from the warm-up task.\n            if self._warm_up_task is not None and self._has_warmed_up:\n                task = self._warm_up_task\n                self._warm_up_task = None\n            else:\n                task = self._worker.get_task()\n            if not task.shard_name:\n                if task.type == elasticdl_pb2.WAIT:\n                    self._pending_dataset = True\n                    logger.info(""No tasks for now, maybe more later"")\n                    # There are too many requests to get task from the master\n                    # if the worker does not sleep.\n                    time.sleep(5)\n                else:\n                    logger.info(""No more task, stopping"")\n                break\n            with self._lock:\n                if task.type == elasticdl_pb2.TRAIN_END_CALLBACK:\n                    self._pending_train_end_callback_task = task\n                    continue\n\n                self._pending_tasks.append(task)\n                if len(self._pending_tasks) == 1:\n                    self._current_task = task\n            for data in self.data_reader.read_records(task):\n                if data:\n                    yield data\n'"
elasticdl/python/worker/worker.py,20,"b'import os\nimport time\nimport traceback\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nfrom elasticdl.proto import elasticdl_pb2, elasticdl_pb2_grpc\nfrom elasticdl.python.collective_ops.communicator import CollectiveCommunicator\nfrom elasticdl.python.common.constants import (\n    CollectiveCommunicatorStatus,\n    DistributionStrategy,\n    JobType,\n    MetricsDictKey,\n    Mode,\n)\nfrom elasticdl.python.common.dtypes import dtype_numpy_to_tensor\nfrom elasticdl.python.common.hash_utils import (\n    int_to_id,\n    scatter_embedding_vector,\n    string_to_id,\n)\nfrom elasticdl.python.common.log_utils import get_logger\nfrom elasticdl.python.common.model_handler import ModelHandler\nfrom elasticdl.python.common.model_utils import (\n    find_layer,\n    get_dict_from_params_str,\n    get_model_spec,\n    get_non_embedding_trainable_vars,\n    set_callback_parameters,\n)\nfrom elasticdl.python.common.tensor_utils import (\n    Tensor,\n    deduplicate_indexed_slices,\n    merge_indexed_slices,\n    pb_to_ndarray,\n    serialize_indexed_slices,\n    serialize_ndarray,\n)\nfrom elasticdl.python.common.timing_utils import Timing\nfrom elasticdl.python.elasticdl.callbacks import SavedModelExporter\nfrom elasticdl.python.elasticdl.feature_column import feature_column\nfrom elasticdl.python.elasticdl.layers.embedding import Embedding\nfrom elasticdl.python.worker.task_data_service import TaskDataService\n\n# The default maximum number of a minibatch retry as its results\n# (e.g. gradients) are not accepted by master.\nDEFAULT_MAX_MINIBATCH_RETRY_NUM = 64\n\n# The default maximum number of retries for allreduce operation\n# if allreduce-based distributed training strategy is used.\nDEFAULT_MAX_ALLREDUCE_RETRY_NUM = 5\n# The default timeout in seconds allowed for reinitializing the\n# collective communicator.\nDEFAULT_COMMUNICATOR_REINITIALIZING_TIMEOUT = 20\n\n\nclass Worker(object):\n    """"""ElasticDL worker""""""\n\n    def __init__(\n        self,\n        args,\n        channel=None,\n        ps_channels=None,\n        max_minibatch_retry_num=DEFAULT_MAX_MINIBATCH_RETRY_NUM,\n        max_allreduce_retry_num=DEFAULT_MAX_ALLREDUCE_RETRY_NUM,\n        set_parallelism=False,\n    ):\n        """"""\n        Arguments:\n            channel: The channel for the gRPC master service.\n            ps_channels: The PS channels for PS service\n            max_minibatch_retry_num: The maximum number of a minibatch retry\n                as its results (e.g. gradients) are not accepted by master.\n            max_allreduce_retry_num: The maximum number of retries for\n                allreduce operation if allreduce-based distributed\n                training strategy is used.\n        """"""\n        self._args = args\n        self.logger = get_logger(""Worker"", level=args.log_level.upper())\n\n        if set_parallelism:\n            # Explicitly setting the parallelism will avoid multi-process hangs\n            # Maybe due to an unknown bug in Tensorflow?\n            # Must called before TensorFlow is initialized.\n            # Not set_parallelism by default to make unittests happy.\n            num_threads = os.cpu_count()\n            tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n            tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n\n        if channel is None:\n            self._stub = None\n        else:\n            self._stub = elasticdl_pb2_grpc.MasterStub(channel)\n\n        self._use_multi_ps = False\n        self._ps_vars = {}\n        if isinstance(ps_channels, list):\n            if len(ps_channels) > 0:\n                self._use_multi_ps = True\n                self._ps_stubs = [\n                    elasticdl_pb2_grpc.PserverStub(c) for c in ps_channels\n                ]\n                self._var_to_ps = {}\n                self._ps_num = len(self._ps_stubs)\n        else:\n            self._ps_num = 0\n        self._distribution_strategy = args.distribution_strategy\n        if (\n            self._distribution_strategy\n            == DistributionStrategy.PARAMETER_SERVER\n            and self._use_multi_ps is False\n        ):\n            raise ValueError(\n                ""PS channels are not set up under parameter server strategy""\n            )\n\n        self._max_minibatch_retry_num = max_minibatch_retry_num\n        self._max_allreduce_retry_num = max_allreduce_retry_num\n        self._init_from_args(args)\n        self._timing = Timing(args.log_level.upper() == ""DEBUG"", self.logger)\n        self._log_loss_count = 0\n\n    def _init_from_args(self, args):\n        """"""\n        Please refer to elastic/python/common/args.py for more\n        details about arguments of a worker.\n        """"""\n        self._worker_id = args.worker_id\n        self._job_type = args.job_type\n        self._minibatch_size = args.minibatch_size\n        self._log_loss_steps = args.log_loss_steps\n        (\n            model_inst,\n            self._dataset_fn,\n            self._loss,\n            self._opt_fn,\n            self._eval_metrics_fn,\n            self._prediction_outputs_processor,\n            self._custom_data_reader,\n            self._callbacks_list,\n        ) = get_model_spec(\n            model_zoo=args.model_zoo,\n            model_def=args.model_def,\n            dataset_fn=args.dataset_fn,\n            loss=args.loss,\n            optimizer=args.optimizer,\n            eval_metrics_fn=args.eval_metrics_fn,\n            model_params=args.model_params,\n            prediction_outputs_processor=args.prediction_outputs_processor,\n            custom_data_reader=args.custom_data_reader,\n            callbacks=args.callbacks,\n        )\n\n        self._collective_communicator = (\n            CollectiveCommunicator(\n                service_name=args.collective_communicator_service_name\n            )\n            if self._distribution_strategy == DistributionStrategy.ALLREDUCE\n            else None\n        )\n        self._model_handler = ModelHandler.get_model_handler(\n            self._distribution_strategy, checkpoint_dir=args.checkpoint_dir\n        )\n        model_inst = self._model_handler.get_model_to_train(model_inst)\n        self.set_model(model_inst)\n\n        self._model_version = -1\n        if self._distribution_strategy != DistributionStrategy.ALLREDUCE:\n            self._model_versions_from_ps = [-1 for _ in range(self._ps_num)]\n        self._task_data_service = TaskDataService(\n            self,\n            self._job_type == JobType.TRAINING_WITH_EVALUATION,\n            data_reader_params=get_dict_from_params_str(\n                args.data_reader_params\n            ),\n            data_origin=args.training_data,\n        )\n        if self._dataset_fn is None:\n            if hasattr(\n                self._task_data_service.data_reader, ""default_dataset_fn""\n            ):\n                self._dataset_fn = (\n                    self._task_data_service.data_reader.default_dataset_fn()\n                )\n            else:\n                raise ValueError(\n                    ""dataset_fn is required if the data_reader used does ""\n                    ""not provide default implementation of dataset_fn""\n                )\n        self._get_model_steps = args.get_model_steps\n        self._opt = self._opt_fn()\n        self._model.optimizer = self._opt\n        self._non_embed_grads = {}\n        self._evaluation_result = {}\n\n        saved_model_exporter = SavedModelExporter(\n            self._task_data_service, self._dataset_fn, self._model_handler\n        )\n        # Place default callbacks at the head to execute them firstly\n        self._callbacks_list.callbacks.insert(0, saved_model_exporter)\n        self._callbacks_list.set_model(model_inst)\n        set_callback_parameters(\n            self._callbacks_list,\n            batch_size=args.minibatch_size,\n            saved_model_path=args.output,\n            checkpoint_path=args.checkpoint_dir,\n        )\n\n    # TODO: Multiple tests are currently using this function to initialize\n    # self._model, where the initialization should be done via constructor.\n    def set_model(self, model_inst):\n        """"""Set model instance to worker.""""""\n        self._model = model_inst\n        self._train_eagerly = False\n        self._init_embeddings()\n        self._var_created = self._model.built\n        self._non_embed_vars = {}\n        if self._var_created:\n            for var in get_non_embedding_trainable_vars(\n                self._model, self._embedding_layers\n            ):\n                self._non_embed_vars[var.name] = var\n            if self._use_multi_ps:\n                self.init_ps_var_partition()\n\n    def _init_embedding_layer(self):\n        """"""\n        Init elasticdl.layers.embedding layer list and assign worker to them\n        """"""\n        self._embedding_layers = find_layer(self._model, Embedding)\n        if self._use_multi_ps:\n            for layer in self._embedding_layers:\n                layer.set_lookup_embedding_func(self.pull_embedding_vectors)\n\n    def _init_embedding_column(self):\n        self._embedding_columns = []\n        for layer in self._model.layers:\n            if isinstance(layer, tf.keras.layers.DenseFeatures):\n                for column in layer._feature_columns:\n                    if isinstance(column, feature_column.EmbeddingColumn):\n                        self._embedding_columns.append(column)\n                        self.logger.info(\n                            ""Initialize ElasticDL EmbeddingColumn:{}"".format(\n                                column.name\n                            )\n                        )\n\n        if self._use_multi_ps:\n            for column in self._embedding_columns:\n                column.set_lookup_embedding_func(self.pull_embedding_vectors)\n\n    def _check_name_conflict_of_embedding_layer_and_column(self):\n        if not self._embedding_layers or not self._embedding_columns:\n            return\n\n        embedding_layer_name_set = set(\n            [layer.name for layer in self._embedding_layers]\n        )\n        embedding_column_name_set = set(\n            [column.name for column in self._embedding_columns]\n        )\n        conflict_name_set = embedding_column_name_set.union(\n            embedding_layer_name_set\n        )\n        if conflict_name_set:\n            raise Exception(\n                ""Name conflict between embedding layer and column: {}"".format(\n                    conflict_name_set\n                )\n            )\n\n    def _init_embeddings(self):\n        self._init_embedding_layer()\n        self._init_embedding_column()\n        self._check_name_conflict_of_embedding_layer_and_column()\n\n        if self._use_multi_ps:\n            self.report_embedding_info()\n\n        self._need_embedding_layer_check = (\n            True\n            if self._embedding_layers or self._embedding_columns\n            else False\n        )\n\n    def _set_tape_for_embedding(self, tape):\n        for layer in self._embedding_layers:\n            layer.set_tape(tape)\n        for column in self._embedding_columns:\n            column.set_tape(tape)\n\n    def _reset_embedding(self):\n        for layer in self._embedding_layers:\n            layer.reset()\n        for column in self._embedding_columns:\n            column.reset()\n\n    def _update_local_model(self):\n        if not self._non_embed_grads:\n            return\n        # Take care of the order of grads and vars if worker modifies\n        # `_non_embed_vars` during training.\n        self._opt.apply_gradients(\n            zip(self._non_embed_grads, self._non_embed_vars.values())\n        )\n        self._non_embed_grads = None\n\n    def get_task(self, task_type=None):\n        """"""\n        get task from master\n        """"""\n        req = elasticdl_pb2.GetTaskRequest()\n        req.worker_id = self._worker_id\n        if task_type is not None:\n            req.task_type = task_type\n\n        try:\n            res = self._stub.get_task(req)\n        except Exception:\n            # Master may have stopped GRPC service when there are no more\n            # tasks. This will result in a GRPC call exception.\n            self.logger.info(\n                ""Cannot connect to master, assuming no more tasks""\n            )\n            res = elasticdl_pb2.Task()\n        return res\n\n    def get_model(self):\n        self._timing.start_record_time(""get_model"")\n        if self._distribution_strategy != DistributionStrategy.ALLREDUCE:\n            variable_future_and_id_pairs = []\n            if self._use_multi_ps:\n                self.init_ps_var_partition()\n            for ps_id, stub in enumerate(self._ps_stubs):\n                if ps_id not in self._ps_vars:\n                    continue\n                # async grpc call\n                req = elasticdl_pb2.PullDenseParametersRequest()\n                req.version = self._model_versions_from_ps[ps_id]\n                var_future = stub.pull_dense_parameters.future(req)\n                variable_future_and_id_pairs.append((var_future, ps_id))\n\n            for var_future, ps_id in variable_future_and_id_pairs:\n                res = var_future.result()\n                if not res.initialized:\n                    # push variable to ps for initialization\n                    self.report_variable_to_ps(ps_id)\n                    req = elasticdl_pb2.PullDenseParametersRequest()\n                    req.version = self._model_versions_from_ps[ps_id]\n                    res = self._ps_stubs[ps_id].pull_dense_parameters(req)\n                    if not res.initialized:\n                        # TODO: support PS fault-tolerance\n                        raise RuntimeError(\n                            ""PS pod %d cannot be initialized"" % ps_id\n                        )\n\n                for name, pb in res.dense_parameters.items():\n                    self._non_embed_vars[name].assign(pb_to_ndarray(pb))\n                self._model_versions_from_ps[ps_id] = res.version\n\n            self._model_version = max(self._model_versions_from_ps)\n        self._timing.end_record_time(""get_model"")\n\n    def pull_embedding_vectors(self, layer_name, embedding_ids):\n        """"""Pulls and returns embedding vectors ordered by the embedding ids.""""""\n        ps_ids = {}\n        ps_ids_index = {}\n        for idx, embedding_id in enumerate(embedding_ids):\n            ps_id = int_to_id(embedding_id, self._ps_num)\n            ps_ids.setdefault(ps_id, []).append(embedding_id)\n            ps_ids_index.setdefault(ps_id, []).append(idx)\n\n        embeddings = []\n        index = []\n        pb_future_and_id_pairs = []\n        for ps_id, embedding_ids in ps_ids.items():\n            req = elasticdl_pb2.PullEmbeddingVectorRequest()\n            req.name = layer_name\n            req.ids.extend(embedding_ids)\n            pb_future = self._ps_stubs[ps_id].pull_embedding_vectors.future(\n                req\n            )\n            pb_future_and_id_pairs.append((pb_future, ps_id))\n        for pb_future, ps_id in pb_future_and_id_pairs:\n            pb = pb_future.result()\n            embeddings.append(pb_to_ndarray(pb))\n            index.extend(ps_ids_index[ps_id])\n        embeddings = np.concatenate(embeddings)\n\n        # adjust the order of embedding vectors\n        new_embeddings = np.empty_like(embeddings)\n        new_embeddings[index] = embeddings\n        return new_embeddings\n\n    def report_task_result(self, task_id, err_msg, exec_counters=None):\n        """"""\n        report task result to master\n        """"""\n        report = elasticdl_pb2.ReportTaskResultRequest()\n        report.task_id = task_id\n        report.err_message = err_msg\n        if isinstance(exec_counters, dict):\n            report.exec_counters.update(exec_counters)\n        return self._stub.report_task_result(report)\n\n    def init_ps_var_partition(self):\n        ps_vars = {}\n        for v in self._non_embed_vars.values():\n            if v.name not in self._var_to_ps:\n                self._var_to_ps[v.name] = string_to_id(v.name, self._ps_num)\n            ps_id = self._var_to_ps[v.name]\n            if ps_id not in ps_vars:\n                ps_vars[ps_id] = [v]\n            else:\n                ps_vars[ps_id].append(v)\n        self._ps_vars = ps_vars\n\n    def report_embedding_info(self):\n        model = elasticdl_pb2.Model()\n        if self._embedding_layers:\n            embedding_infos = model.embedding_table_infos\n            for layer in self._embedding_layers:\n                embedding_info = embedding_infos.add()\n                embedding_info.name = layer.embedding_weight_name\n                embedding_info.dim = layer.output_dim\n                embedding_info.initializer = layer.embeddings_initializer\n                # set to float32\n                embedding_info.dtype = dtype_numpy_to_tensor(\n                    np.dtype(""float32"")\n                )\n\n        if self._embedding_columns:\n            embedding_infos = model.embedding_table_infos\n            for column in self._embedding_columns:\n                embedding_info = embedding_infos.add()\n                embedding_info.name = column.embedding_weight_name\n                embedding_info.dim = column.dimension\n                # TODO(brightcoder01): The initializer in embedding column is\n                # a variable initializer function. For embedding layer, it\'s a\n                # tf.keras.initializers. Keep aligned between these two.\n                embedding_info.initializer = ""uniform""\n                # set to float32\n                embedding_info.dtype = dtype_numpy_to_tensor(\n                    np.dtype(""float32"")\n                )\n\n        for ps_id in range(self._ps_num):\n            self._ps_stubs[ps_id].push_embedding_table_infos(model)\n\n    def report_variable_to_ps(self, ps_id):\n        model = elasticdl_pb2.Model()\n        model.version = self._model_versions_from_ps[ps_id]\n        if ps_id in self._ps_vars:\n            vars = self._ps_vars[ps_id]\n            for var in vars:\n                serialize_ndarray(\n                    var.numpy(), model.dense_parameters[var.name]\n                )\n        self._ps_stubs[ps_id].push_model(model)\n\n    def report_variable(self):\n        # TODO: call `push_model` in parallel\n        for ps_id in range(self._ps_num):\n            self.report_variable_to_ps(ps_id)\n\n    def _collect_edl_embedding_name_values(self):\n        """"""\n        Collect the information of ElasticDL customized\n        embeddings such as EDL embedding layer and EDL embedding column.\n        Return:\n            An array of key-value pair.\n            Key is embedding names, layer name for embedding layer\n            and column name for embedding column.\n            Value is the EmbeddingAndIds tuple.\n        """"""\n\n        embedding_name_values = []\n        for layer in self._embedding_layers:\n            embedding_name_values.append(\n                (layer.embedding_weight_name, layer.embedding_and_ids)\n            )\n        for column in self._embedding_columns:\n            embedding_name_values.append(\n                (column.embedding_weight_name, column.embedding_and_ids)\n            )\n\n        return embedding_name_values\n\n    def report_gradient_to_ps(self, grads):\n        self._timing.start_record_time(""report_gradient"")\n        reqs = [\n            elasticdl_pb2.PushGradientsRequest() for i in range(self._ps_num)\n        ]\n        ps_grads = {}\n        non_embed_vars_n = len(self._non_embed_vars)\n        for g, v in zip(\n            grads[:non_embed_vars_n], self._non_embed_vars.values()\n        ):\n            ps_id = self._var_to_ps[v.name]\n            if ps_id not in ps_grads:\n                ps_grads[ps_id] = {v.name: g}\n            else:\n                if v.name not in ps_grads[ps_id]:\n                    ps_grads[ps_id][v.name] = g\n                else:\n                    if isinstance(g, tf.IndexedSlices):\n                        ps_grads[ps_id][v.name] = merge_indexed_slices(\n                            ps_grads[ps_id][v.name], g\n                        )\n                    else:\n                        ps_grads[ps_id][v.name] += g\n\n        for ps_id, pair in ps_grads.items():\n            for name, g in pair.items():\n                if isinstance(g, tf.IndexedSlices):\n                    v, i = deduplicate_indexed_slices(g.values, g.indices)\n                    ps_grads[ps_id][name] = tf.IndexedSlices(v, i)\n\n        for ps_id in ps_grads:\n            req = reqs[ps_id]\n            for name, g in ps_grads[ps_id].items():\n                # Keras embedding layer has a dense parameter,\n                # but an indexed slices type gradient\n                if isinstance(g, tf.IndexedSlices):\n                    serialize_indexed_slices(\n                        Tensor(None, g.values.numpy(), g.indices.numpy()),\n                        req.gradients.embedding_tables[name],\n                    )\n                else:\n                    serialize_ndarray(\n                        g.numpy(), req.gradients.dense_parameters[name]\n                    )\n\n        edl_embedding_name_values = self._collect_edl_embedding_name_values()\n\n        if edl_embedding_name_values:\n            edl_embedding_grads = grads[non_embed_vars_n:]\n            bet_number = 0\n            for name, embedding_and_ids in edl_embedding_name_values:\n                bet_number += len(embedding_and_ids)\n            if len(edl_embedding_grads) != bet_number:\n                raise ValueError(\n                    ""elasticdl.layers.embedding related gradient number %d ""\n                    ""does not match the number of its output tensor %d.""\n                    % (len(edl_embedding_grads), bet_number)\n                )\n\n            grad_accum_iter = 0\n            for name, embedding_and_ids in edl_embedding_name_values:\n                g_values = None\n                g_indices = None\n                for _, ids in embedding_and_ids:\n                    grad = edl_embedding_grads[grad_accum_iter]\n                    grad_accum_iter += 1\n                    # ElasticDL embedding layer with Sparse Gradients\n                    if isinstance(grad, tf.IndexedSlices):\n                        grad = grad.values\n                    if g_values is not None:\n                        g_values = tf.concat([g_values, grad], axis=0)\n                        g_indices = tf.concat([g_indices, ids], axis=0)\n                    else:\n                        g_values = grad\n                        g_indices = ids\n\n                # Sum up the values of the duplicated indices in the\n                # gradients. It can reduce the gradient payload of the\n                # dense embedding.\n                g_values, g_indices = deduplicate_indexed_slices(\n                    values=g_values, indices=g_indices\n                )\n\n                results = scatter_embedding_vector(\n                    g_values.numpy(), g_indices.numpy(), self._ps_num\n                )\n\n                for ps_id in results:\n                    req = reqs[ps_id]\n                    gv, gi = results[ps_id]\n                    serialize_indexed_slices(\n                        Tensor(None, gv, gi),\n                        req.gradients.embedding_tables[name],\n                    )\n\n        report_futures = []\n        for ps_id in range(self._ps_num):\n            req = reqs[ps_id]\n            req.gradients.version = self._model_versions_from_ps[ps_id]\n            req.learning_rate = K.get_value(self._model.optimizer.lr)\n            report_future = self._ps_stubs[ps_id].push_gradients.future(req)\n            report_futures.append(report_future)\n\n        accepted = False\n        max_version = -1\n        for report_future in report_futures:\n            res = report_future.result()\n            if res.accepted:\n                accepted = True\n            if res.version > max_version:\n                max_version = res.version\n        self._timing.end_record_time(""report_gradient"")\n        return accepted, max_version\n\n    def report_gradient_locally(self, grads):\n        if self._embedding_layers or self._embedding_columns:\n            raise ValueError(\n                ""ElasticDL embedding layer is not supported when""\n                ""reporting gradients locally""\n            )\n        self._non_embed_grads = grads[: len(self._non_embed_vars)]\n        return True, None\n\n    def report_gradient(self, grads):\n        if self._distribution_strategy == DistributionStrategy.ALLREDUCE:\n            self.report_gradient_locally(grads)\n            self._update_local_model()\n            return True, None\n        else:\n            if self._use_multi_ps:\n                return self.report_gradient_to_ps(grads)\n            raise RuntimeError(""Only support report gradients to PS"")\n\n    def report_evaluation_metrics(self, model_outputs, labels):\n        """"""\n        report evaluation metrics to ps.\n        """"""\n        req = elasticdl_pb2.ReportEvaluationMetricsRequest()\n        for name, output in model_outputs.items():\n            output = np.concatenate(output)\n            serialize_ndarray(output, req.model_outputs[name])\n        labels = np.concatenate(labels)\n        serialize_ndarray(labels, req.labels)\n        req.worker_id = self._worker_id\n        self._stub.report_evaluation_metrics(req)\n\n    def report_prediction_outputs(self, predictions):\n        if self._prediction_outputs_processor:\n            self._prediction_outputs_processor.process(\n                predictions, self._worker_id\n            )\n        else:\n            self.logger.warning(\n                ""prediction_outputs_processor is not ""\n                ""defined in the model definition. Prediction outputs ""\n                ""are not processed.""\n            )\n        return True\n\n    def _run_model_call_before_training(self, features):\n        """"""Call `self._model.call` before training for two things:\n            * Create variables and report to ps if not created.\n            * Check whether there is an embedding layer that is called\n              more than once during one forward-pass.\n        """"""\n        if self._embedding_layers:\n            with tf.GradientTape() as tape:\n                self._set_tape_for_embedding(tape)\n                _ = self._model.call(features)\n        else:\n            _ = self._model.call(features)\n        self._non_embed_vars = {}\n        for var in get_non_embedding_trainable_vars(\n            self._model, self._embedding_layers\n        ):\n            self._non_embed_vars[var.name] = var\n\n        if not self._var_created:\n            if self._use_multi_ps:\n                self.init_ps_var_partition()\n            else:\n                self.report_variable()\n            self._var_created = True\n\n        if self._need_embedding_layer_check:\n            self._train_eagerly = False\n            for layer in self._embedding_layers:\n                if len(layer.embedding_and_ids) > 1:\n                    self._train_eagerly = True\n                    self.logger.warning(\n                        ""ElasticDL embedding layer %s is called more than ""\n                        ""once, this will make the training process unable ""\n                        ""to accelerate with tf.function."" % (layer.name)\n                    )\n            self._need_embedding_layer_check = False\n\n        self._reset_embedding()\n\n    def get_trainable_items(self):\n        """"""\n        return all trainable variables list, including batch embedding\n        tensor (BET) if exists. take care to keep the same order as in\n        self.report_gradient()\n        """"""\n        bets = []\n        if self._embedding_layers:\n            for layer in self._embedding_layers:\n                bets.extend(\n                    [\n                        batch_embedding\n                        for (batch_embedding, _) in layer.embedding_and_ids\n                    ]\n                )\n\n        if self._embedding_columns:\n            for column in self._embedding_columns:\n                bets.extend(\n                    [\n                        batch_embedding\n                        for (batch_embedding, _) in column.embedding_and_ids\n                    ]\n                )\n\n        return list(self._non_embed_vars.values()) + bets\n\n    def training_process(self, features, labels):\n        """"""\n        training for models with elasticdl.layers.embedding does not\n        support tf.function decorator\n        """"""\n        if self._train_eagerly:\n            return self.training_process_eagerly(features, labels)\n        else:\n            return self.training_process_with_acceleration(features, labels)\n\n    @tf.function\n    def training_process_with_acceleration(self, features, labels):\n        return self.training_process_eagerly(features, labels)\n\n    def training_process_eagerly(self, features, labels):\n        with tf.GradientTape() as tape:\n            self._set_tape_for_embedding(tape)\n            outputs = self._model.call(features, training=True)\n            loss = self._loss(labels, outputs)\n            # Add regularization loss if any\n            if self._model.losses:\n                loss += tf.math.add_n(self._model.losses)\n        grads = tape.gradient(loss, self.get_trainable_items())\n        return loss, grads\n\n    @tf.function\n    def forward_process(self, features):\n        """"""Calculates model outputs in non-training mode.""""""\n        outputs = self._model.call(features, training=False)\n        return outputs\n\n    def _get_local_model_params(self):\n        return [v for v in self._non_embed_vars.values()]\n\n    @staticmethod\n    def _get_rank_of_broadcast_src_worker():\n        return 0\n\n    def _broadcast_model_params(self):\n        status = self._collective_communicator.barrier()\n        if status == CollectiveCommunicatorStatus.FAILED:\n            self.logger.warning(""Failed to perform barrier operation"")\n            return False\n        broadcast_root_worker_rank = self._get_rank_of_broadcast_src_worker()\n        model_params = self._get_local_model_params()\n        status = self._collective_communicator.tf_broadcast(\n            model_params, broadcast_root_worker_rank\n        )\n        if status == CollectiveCommunicatorStatus.FAILED:\n            self.logger.warning(""Failed to broadcast model parameters"")\n            return False\n        return True\n\n    def _calculate_grads_and_report_with_allreduce(self, grads):\n        status, averaged_grads = self._collective_communicator.tf_allreduce(\n            grads\n        )\n        accepted = False\n        if status == CollectiveCommunicatorStatus.SUCCEEDED:\n            accepted, _ = self.report_gradient(averaged_grads)\n            if not accepted:\n                self.logger.warning(""Failed to report the averaged gradients"")\n        return accepted\n\n    def _collect_gradients_with_allreduce_robust(self, grads):\n        accepted = self._calculate_grads_and_report_with_allreduce(grads)\n        if not accepted:\n            start_time = time.time()\n            while not self._collective_communicator.is_initialized():\n                if (\n                    time.time() - start_time\n                    < DEFAULT_COMMUNICATOR_REINITIALIZING_TIMEOUT\n                ):\n                    self.logger.info(\n                        ""(Re-)initializing the collective communicator...""\n                    )\n                    time.sleep(3)\n                else:\n                    self.logger.warning(\n                        ""Failed to (re-)initializing the ""\n                        ""collective communicator""\n                    )\n                    return False\n            succeeded = self._broadcast_model_params()\n            if succeeded:\n                return self._calculate_grads_and_report_with_allreduce(grads)\n            else:\n                self.logger.warning(""Failed to broadcast model parameters"")\n                return False\n        else:\n            return True\n\n    def _collect_gradients_without_allreduce(self, grads):\n        accepted, min_model_version = self.report_gradient(grads)\n        if accepted and self._get_model_steps > 1:\n            non_embed_vars_n = len(self._non_embed_vars)\n            self._non_embed_grads = grads[:non_embed_vars_n]\n        self._reset_embedding()\n        return accepted, min_model_version\n\n    def _run_training_task(self, features, labels):\n        loss, grads = self.training_process(features, labels)\n        if self._distribution_strategy == DistributionStrategy.ALLREDUCE:\n            # TODO: Delay certain amount of time before retrying\n            for _ in range(self._max_allreduce_retry_num + 1):\n                accepted = self._collect_gradients_with_allreduce_robust(grads)\n                if accepted:\n                    return accepted, None, loss\n                else:\n                    self.logger.warning(\n                        ""Failed to perform allreduce operation on""\n                        ""the gradients. Retrying...""\n                    )\n        else:\n            return (*self._collect_gradients_without_allreduce(grads), loss)\n\n    def _collect_evaluation_result(self, outputs, labels):\n        key = MetricsDictKey.MODEL_OUTPUT\n        if key not in self._evaluation_result:\n            outputs = {k: [v.numpy()] for k, v in outputs.items()}\n            self._evaluation_result[key] = outputs\n        else:\n            for k, v in outputs.items():\n                self._evaluation_result[key][k].append(v.numpy())\n        key = MetricsDictKey.LABEL\n        if key not in self._evaluation_result:\n            self._evaluation_result[key] = [labels.numpy()]\n        else:\n            self._evaluation_result[key].append(labels.numpy())\n\n    def _run_evaluation_task(self, features, labels):\n        outputs = self.forward_process(features)\n        if not isinstance(outputs, dict):\n            outputs = {MetricsDictKey.MODEL_OUTPUT: outputs}\n        self._collect_evaluation_result(outputs, labels)\n\n    def _run_prediction_task(self, features):\n        predictions = self.forward_process(features)\n        return self.report_prediction_outputs(predictions)\n\n    def _process_minibatch(\n        self,\n        task_type,\n        features,\n        labels,\n        min_model_version,\n        train_with_local_model=False,\n    ):\n        if self._need_embedding_layer_check or not self._var_created:\n            self._run_model_call_before_training(features)\n        self._timing.start_record_time(""batch_process"")\n        for _ in range(self._max_minibatch_retry_num):\n            if task_type == elasticdl_pb2.EVALUATION:\n                self._run_evaluation_task(features, labels)\n                break\n            elif task_type == elasticdl_pb2.TRAINING:\n                # TODO: optimize the logic to avoid unnecessary\n                #       get_model call.\n                if not train_with_local_model:\n                    self.get_model()\n                self._callbacks_list.on_train_batch_begin(self._model_version)\n                *accepted, min_model_version, loss = self._run_training_task(\n                    features, labels\n                )\n                if (\n                    self._model_version\n                    >= self._log_loss_count * self._log_loss_steps\n                ):\n                    self.logger.info(\n                        ""Loss = {}, steps = {}"".format(\n                            loss.numpy(), self._model_version\n                        )\n                    )\n                    self._log_loss_count = (\n                        int(self._model_version / self._log_loss_steps) + 1\n                    )\n                if accepted:\n                    break\n            elif task_type == elasticdl_pb2.PREDICTION:\n                if self._model_version != min_model_version:\n                    self.get_model()\n                accepted = self._run_prediction_task(features)\n                if accepted:\n                    break\n            else:\n                raise RuntimeError(""Unrecognized task type, %s"" % task_type)\n        else:\n            # Worker got stuck, fail the task.\n            # TODO: stop the worker if it fails to make any\n            #       progress for some time.\n            raise RuntimeError(""Worker got stuck"")\n        self._timing.end_record_time(""batch_process"")\n        return min_model_version\n\n    def _process_eval_task(self, task):\n        """"""\n        Check if there are evaluation tasks and process the tasks if any.\n        Return:\n            A python bool indicating whether worker processed some evaluation\n            tasks.\n        """"""\n        self.logger.info(""the evaluation task_id: %d"" % task.task_id)\n\n        gen = self._task_data_service.get_dataset_gen(task)\n        if not gen:\n            return None\n\n        def create_dataset():\n            eval_dataset = tf.data.Dataset.from_generator(\n                gen, self._task_data_service.data_reader.records_output_types\n            )\n            eval_dataset = self._dataset_fn(\n                eval_dataset,\n                Mode.EVALUATION,\n                self._task_data_service.data_reader.metadata,\n            )\n            eval_dataset = eval_dataset.batch(self._minibatch_size).prefetch(1)\n            return eval_dataset\n\n        with tf.device(""/device:cpu:0""):\n            eval_dataset = create_dataset()\n        model_version = task.model_version\n        task_id = task.task_id\n        err_msg = """"\n        for dataset_batch in eval_dataset:\n            data_err_msg = self._process_minibatch_and_report(\n                dataset_batch, elasticdl_pb2.EVALUATION, model_version\n            )\n            if data_err_msg:\n                err_msg = data_err_msg\n                break\n        del eval_dataset\n        self.report_evaluation_metrics(\n            model_outputs=self._evaluation_result[MetricsDictKey.MODEL_OUTPUT],\n            labels=self._evaluation_result[MetricsDictKey.LABEL],\n        )\n        self.report_task_result(task_id, err_msg)\n        self._evaluation_result = {}\n\n    def _process_train_end_callback_task_if_needed(self):\n        train_end_task = self._task_data_service.get_train_end_callback_task()\n        if train_end_task:\n            self._callbacks_list.on_train_end()\n            self._task_data_service.clear_train_end_callback_task()\n            self.report_task_result(task_id=train_end_task.task_id, err_msg="""")\n\n    def _process_minibatch_and_report(\n        self,\n        dataset_batch,\n        task_type,\n        model_version,\n        train_with_local_model=False,\n    ):\n        err_msg = """"\n        try:\n            if self._job_type == JobType.PREDICTION_ONLY:\n                features = dataset_batch\n                labels = None\n            else:\n                features = dataset_batch[0]\n                labels = dataset_batch[1]\n            self._process_minibatch(\n                task_type,\n                features,\n                labels,\n                model_version,\n                train_with_local_model,\n            )\n        except RuntimeError as err:\n            err_msg = str(err)\n            traceback.print_exc()\n        except Exception as ex:\n            err_msg = str(ex)\n            traceback.print_exc()\n            raise ex\n        return err_msg\n\n    def _train_and_evaluate(self):\n        """"""\n        Train and evaluate the model on the worker\n        """"""\n\n        # The worker needs to get model from PS if\n        # `train_with_local_model=False`. This happens when:\n        #     processing first minibatch\n        #     any evaluation task has been executed just before this minibatch\n        #     last minibatch is training task and failed\n        #     local_update_count >= worker._get_model_steps\n        # Otherwise, worker trains with local model, i.e.\n        # `train_with_local_model=True`\n        train_with_local_model = False\n\n        # Initialize `local_update_count=get_model_steps` in order to set\n        # `train_with_local_model` to False inside for-loop for the first\n        # minibatch.\n\n        local_update_count = self._get_model_steps\n        last_training_minibatch_failed = False\n        evaluation_task_executed = False\n        while True:\n            dataset = self._task_data_service.get_dataset()\n            if not dataset:\n                self._process_train_end_callback_task_if_needed()\n                break\n            dataset = self._dataset_fn(\n                dataset,\n                Mode.TRAINING,\n                self._task_data_service.data_reader.metadata,\n            )\n            dataset = dataset.batch(self._minibatch_size).prefetch(1)\n            self._timing.start_record_time(""task_process"")\n            for dataset_batch in dataset:\n                if self._job_type == JobType.TRAINING_WITH_EVALUATION:\n                    # Give the worker a chance to process an evaluation task\n                    # during training if the task exists\n                    evaluation_task_executed = (\n                        True\n                        if self._evaluate_only()\n                        else evaluation_task_executed\n                    )\n\n                task = self._task_data_service.get_current_task()\n                if (\n                    evaluation_task_executed\n                    or last_training_minibatch_failed\n                    or local_update_count >= self._get_model_steps\n                ):\n                    local_update_count = 0\n                    train_with_local_model = False\n                else:\n                    train_with_local_model = True\n\n                err_msg = self._process_minibatch_and_report(\n                    dataset_batch,\n                    task.type,\n                    task.model_version,\n                    train_with_local_model,\n                )\n\n                local_update_count += 1\n                if err_msg:\n                    last_training_minibatch_failed = True\n                else:\n                    last_training_minibatch_failed = False\n                    if local_update_count < self._get_model_steps:\n                        self._update_local_model()\n                if self._task_data_service.report_record_done(\n                    self._minibatch_size, err_msg\n                ):\n                    self._timing.end_record_time(""task_process"")\n                    self._timing.report_timing(reset=True)\n                    self._timing.start_record_time(""task_process"")\n            del dataset\n            # New evaluation tasks may be created after this worker\'s\n            # training tasks are done, as other workers\' may still\n            # have pending training tasks.\n            if self._job_type == JobType.TRAINING_WITH_EVALUATION:\n                evaluation_task_executed = self._evaluate_only()\n\n            self._process_train_end_callback_task_if_needed()\n\n    def _evaluate_only(self):\n        """"""\n        Only evaluate the model on the worker.\n        """"""\n        evaluation_task_executed = False\n        # should not get model before finishing some training tasks, because\n        # variables of subclass models are not created.\n        is_model_got = False\n        while True:\n            task = self.get_task(elasticdl_pb2.EVALUATION)\n            # no evaluation task in eval_todo of master\n            if not task.shard_name:\n                break\n            # get the latest model before processing eval tasks\n            if not is_model_got:\n                self.get_model()\n                is_model_got = True\n            self._process_eval_task(task)\n            evaluation_task_executed = True\n        return evaluation_task_executed\n\n    def _predict_only(self):\n        """"""\n        Only predict outputs of the model with data in tasks on the worker.\n        """"""\n        while True:\n            dataset = self._task_data_service.get_dataset()\n            if not dataset:\n                break\n            dataset = self._dataset_fn(\n                dataset,\n                Mode.PREDICTION,\n                self._task_data_service.data_reader.metadata,\n            )\n            dataset = dataset.batch(self._minibatch_size).prefetch(1)\n            for dataset_batch in dataset:\n                task = self._task_data_service.get_current_task()\n\n                err_msg = self._process_minibatch_and_report(\n                    dataset_batch, task.type, task.model_version\n                )\n                self._task_data_service.report_record_done(\n                    self._minibatch_size, err_msg\n                )\n            del dataset\n\n    def run(self):\n        """"""\n        Fetches task from master with and performs training, evaluation\n        or prediction.\n        """"""\n        if self._job_type == JobType.PREDICTION_ONLY:\n            self._predict_only()\n        elif self._job_type == JobType.EVALUATION_ONLY:\n            self._evaluate_only()\n        else:\n            self._train_and_evaluate()\n'"
model_zoo/census_model_sqlflow/dnn/__init__.py,0,b''
model_zoo/census_model_sqlflow/dnn/census_feature_column.py,7,"b'import tensorflow as tf\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\ndef get_feature_columns():\n    feature_columns = []\n\n    for numeric_feature_key in NUMERIC_FEATURE_KEYS:\n        numeric_feature = tf.feature_column.numeric_column(numeric_feature_key)\n        feature_columns.append(numeric_feature)\n\n    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:\n        embedding_feature = tf.feature_column.embedding_column(\n            tf.feature_column.categorical_column_with_hash_bucket(\n                categorical_feature_key, hash_bucket_size=64\n            ),\n            dimension=16,\n        )\n        feature_columns.append(embedding_feature)\n\n    return feature_columns\n\n\ndef get_feature_input_layers():\n    feature_input_layers = {}\n\n    for numeric_feature_key in NUMERIC_FEATURE_KEYS:\n        feature_input_layers[numeric_feature_key] = tf.keras.Input(\n            shape=(1,), name=numeric_feature_key, dtype=tf.float32\n        )\n\n    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:\n        feature_input_layers[categorical_feature_key] = tf.keras.Input(\n            shape=(1,), name=categorical_feature_key, dtype=tf.string\n        )\n\n    return feature_input_layers\n'"
model_zoo/census_model_sqlflow/dnn/census_functional.py,14,"b'import tensorflow as tf\nfrom tensorflow.python.keras.metrics import accuracy\n\nfrom model_zoo.census_model_sqlflow.dnn.census_feature_column import (\n    get_feature_columns,\n    get_feature_input_layers,\n)\n\n\n# The model definition from model zoo\n# Input Params:\n#   feature_columns: The feature column array.\n#       It can be generated from `COLUMN` clause.\n#   feature_input_layers: The input layers specify the feature inputs.\ndef dnn_classifier(feature_columns, feature_input_layers):\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    x = feature_layer(feature_input_layers)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    x = tf.keras.layers.Dense(16, activation=""relu"")(x)\n    y = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)\n\n    model = tf.keras.Model(inputs=feature_input_layers, outputs=y)\n\n    return model\n\n\n# The entry point of the submitter program\ndef custom_model():\n    feature_columns = get_feature_columns()\n    feature_input_layers = get_feature_input_layers()\n\n    return dnn_classifier(\n        feature_columns=feature_columns,\n        feature_input_layers=feature_input_layers,\n    )\n\n\ndef loss(labels, predictions):\n    labels = tf.expand_dims(labels, axis=1)\n    return tf.keras.losses.binary_crossentropy(labels, predictions)\n\n\ndef optimizer():\n    return tf.keras.optimizers.Adam()\n\n\ndef eval_metrics_fn():\n    return {\n        ""accuracy"": lambda labels, predictions: accuracy(\n            tf.cast(tf.squeeze(tf.round(predictions)), tf.int32),\n            tf.cast(labels, tf.int32),\n        )\n    }\n\n\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nLABEL_KEY = ""label""\n\n\n# TODO: The dataset_fn and the column names above is bound with\n# the input data source. We can consider move it out of the\n# model definition file. Currently ElasticDL framework has the\n# limitation that the dataset_fn is in the same file with model def.\ndef dataset_fn(dataset, mode, _):\n    def _parse_data(record):\n\n        feature_description = dict(\n            [\n                (name, tf.io.FixedLenFeature([], tf.string))\n                for name in CATEGORICAL_FEATURE_KEYS\n            ]\n            + [\n                (name, tf.io.FixedLenFeature([], tf.float32))\n                for name in NUMERIC_FEATURE_KEYS\n            ]\n            + [(LABEL_KEY, tf.io.FixedLenFeature([], tf.int64))]\n        )\n\n        parsed_record = tf.io.parse_single_example(record, feature_description)\n        label = parsed_record.pop(LABEL_KEY)\n\n        return parsed_record, label\n\n    dataset = dataset.map(_parse_data)\n\n    return dataset\n'"
model_zoo/census_model_sqlflow/wide_and_deep/__init__.py,0,b''
model_zoo/census_model_sqlflow/wide_and_deep/feature_configs.py,12,"b'import itertools\n\nimport tensorflow as tf\n\nfrom model_zoo.census_model_sqlflow.wide_and_deep.transform_ops import (\n    Array,\n    Bucketize,\n    Concat,\n    Embedding,\n    Hash,\n    SchemaInfo,\n    Vocabularize,\n)\n\n# The following objects can be generated from the meta parsed from\n# the `COLUMN` clause in SQLFlow statement and the analysis result.\n\nWORK_CLASS_VOCABULARY = [\n    ""Private"",\n    ""Self-emp-not-inc"",\n    ""Self-emp-inc"",\n    ""Federal-gov"",\n    ""Local-gov"",\n    ""State-gov"",\n    ""Without-pay"",\n    ""Never-worked"",\n]\n\nMARITAL_STATUS_VOCABULARY = [\n    ""Married-civ-spouse"",\n    ""Divorced"",\n    ""Never-married"",\n    ""Separated"",\n    ""Widowed"",\n    ""Married-spouse-absent"",\n    ""Married-AF-spouse"",\n]\n\nRELATION_SHIP_VOCABULARY = [\n    ""Wife"",\n    ""Own-child"",\n    ""Husband"",\n    ""Not-in-family"",\n    ""Other-relative"",\n    ""Unmarried"",\n]\n\nRACE_VOCABULARY = [\n    ""White"",\n    ""Asian-Pac-Islander"",\n    ""Amer-Indian-Eskimo"",\n    ""Other"",\n    ""Black"",\n]\n\nSEX_VOCABULARY = [""Female"", ""Male""]\n\nAGE_BOUNDARIES = [0, 20, 40, 60, 80]\nCAPITAL_GAIN_BOUNDARIES = [6000, 6500, 7000, 7500, 8000]\nCAPITAL_LOSS_BOUNDARIES = [2000, 2500, 3000, 3500, 4000]\nHOURS_BOUNDARIES = [10, 20, 30, 40, 50, 60]\n\neducation_hash = Hash(\n    ""education_hash"", ""education"", ""education_hash"", hash_bucket_size=30,\n)\noccupation_hash = Hash(\n    ""occupation_hash"", ""occupation"", ""occupation_hash"", hash_bucket_size=30,\n)\nnative_country_hash = Hash(\n    ""native_country_hash"",\n    ""native_country"",\n    ""native_country_hash"",\n    hash_bucket_size=100,\n)\n\nworkclass_lookup = Vocabularize(\n    ""workclass_lookup"",\n    ""workclass"",\n    ""workclass_lookup"",\n    vocabulary_list=WORK_CLASS_VOCABULARY,\n)\nmarital_status_lookup = Vocabularize(\n    ""marital_status_lookup"",\n    ""marital_status"",\n    ""marital_status_lookup"",\n    vocabulary_list=MARITAL_STATUS_VOCABULARY,\n)\nrelationship_lookup = Vocabularize(\n    ""relationship_lookup"",\n    ""relationship"",\n    ""relationship_lookup"",\n    vocabulary_list=RELATION_SHIP_VOCABULARY,\n)\nrace_lookup = Vocabularize(\n    ""race_lookup"", ""race"", ""race_lookup"", vocabulary_list=RACE_VOCABULARY,\n)\nsex_lookup = Vocabularize(\n    ""sex_lookup"", ""sex"", ""sex_lookup"", vocabulary_list=SEX_VOCABULARY,\n)\n\nage_bucketize = Bucketize(\n    ""age_bucketize"", ""age"", ""age_bucketize"", boundaries=AGE_BOUNDARIES,\n)\ncapital_gain_bucketize = Bucketize(\n    ""capital_gain_bucketize"",\n    ""capital_gain"",\n    ""capital_gain_bucketize"",\n    boundaries=CAPITAL_GAIN_BOUNDARIES,\n)\ncapital_loss_bucketize = Bucketize(\n    ""capital_loss_bucketize"",\n    ""capital_loss"",\n    ""capital_loss_bucketize"",\n    boundaries=CAPITAL_LOSS_BOUNDARIES,\n)\nhours_per_week_bucketize = Bucketize(\n    ""hours_per_week_bucketize"",\n    ""hours_per_week"",\n    ""hours_per_week_bucketize"",\n    boundaries=HOURS_BOUNDARIES,\n)\n\n\ndef _get_id_offsets_from_dependency_bucket_num(num_buckets):\n    return list(itertools.accumulate([0] + num_buckets[:-1]))\n\n\ngroup1 = Concat(\n    ""group1"",\n    [\n        ""workclass_lookup"",\n        ""hours_per_week_bucketize"",\n        ""capital_gain_bucketize"",\n        ""capital_loss_bucketize"",\n    ],\n    ""group1"",\n    id_offsets=_get_id_offsets_from_dependency_bucket_num([8, 7, 6, 6]),\n)\ngroup2 = Concat(\n    ""group2"",\n    [\n        ""education_hash"",\n        ""marital_status_lookup"",\n        ""relationship_lookup"",\n        ""occupation_hash"",\n    ],\n    ""group2"",\n    id_offsets=_get_id_offsets_from_dependency_bucket_num([30, 7, 6, 30]),\n)\ngroup3 = Concat(\n    ""group3"",\n    [""age_bucketize"", ""sex_lookup"", ""race_lookup"", ""native_country_hash""],\n    ""group3"",\n    id_offsets=_get_id_offsets_from_dependency_bucket_num([6, 2, 5, 100]),\n)\n\ngroup1_embedding_wide = Embedding(\n    ""group1_embedding_wide"",\n    ""group1"",\n    ""group1_embedding_wide"",\n    input_dim=sum([8, 7, 6, 6]),\n    output_dim=1,\n)\ngroup2_embedding_wide = Embedding(\n    ""group2_embedding_wide"",\n    ""group2"",\n    ""group2_embedding_wide"",\n    input_dim=sum([30, 7, 6, 30]),\n    output_dim=1,\n)\n\ngroup1_embedding_deep = Embedding(\n    ""group1_embedding_deep"",\n    ""group1"",\n    ""group1_embedding_deep"",\n    input_dim=sum([8, 7, 6, 6]),\n    output_dim=8,\n)\ngroup2_embedding_deep = Embedding(\n    ""group2_embedding_deep"",\n    ""group2"",\n    ""group2_embedding_deep"",\n    input_dim=sum([30, 7, 6, 30]),\n    output_dim=8,\n)\ngroup3_embedding_deep = Embedding(\n    ""group3_embedding_deep"",\n    ""group3"",\n    ""group3_embedding_deep"",\n    input_dim=sum([6, 2, 5, 100]),\n    output_dim=8,\n)\n\nwide_embeddings = Array(\n    ""wide_embeddings"",\n    [""group1_embedding_wide"", ""group2_embedding_wide""],\n    ""wide_embeddings"",\n)\n\ndeep_embeddings = Array(\n    ""deep_embeddings"",\n    [\n        ""group1_embedding_deep"",\n        ""group2_embedding_deep"",\n        ""group3_embedding_deep"",\n    ],\n    ""deep_embeddings"",\n)\n\nTRANSFORM_OUTPUTS = [""wide_embeddings"", ""deep_embeddings""]\n\n# Get this execution order by topological sort\nFEATURE_TRANSFORM_INFO_EXECUTE_ARRAY = [\n    education_hash,\n    occupation_hash,\n    native_country_hash,\n    workclass_lookup,\n    marital_status_lookup,\n    relationship_lookup,\n    race_lookup,\n    sex_lookup,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    hours_per_week_bucketize,\n    group1,\n    group2,\n    group3,\n    group1_embedding_wide,\n    group2_embedding_wide,\n    group1_embedding_deep,\n    group2_embedding_deep,\n    group3_embedding_deep,\n    wide_embeddings,\n    deep_embeddings,\n]\n\n# The schema information can be gotten from the\n# source table in SQLFlow statement\n# For example, {table_name} from\n# SELECT * FROM {table_name}\nINPUT_SCHEMAS = [\n    SchemaInfo(""education"", tf.string),\n    SchemaInfo(""occupation"", tf.string),\n    SchemaInfo(""native_country"", tf.string),\n    SchemaInfo(""workclass"", tf.string),\n    SchemaInfo(""marital_status"", tf.string),\n    SchemaInfo(""relationship"", tf.string),\n    SchemaInfo(""race"", tf.string),\n    SchemaInfo(""sex"", tf.string),\n    SchemaInfo(""age"", tf.float32),\n    SchemaInfo(""capital_gain"", tf.float32),\n    SchemaInfo(""capital_loss"", tf.float32),\n    SchemaInfo(""hours_per_week"", tf.float32),\n]\n'"
model_zoo/census_model_sqlflow/wide_and_deep/transform_ops.py,5,"b'from enum import Enum\n\n\nclass TransformOpType(Enum):\n    HASH = (1,)\n    BUCKETIZE = (2,)\n    LOOKUP = (3,)\n    EMBEDDING = (4,)\n    CONCAT = (5,)\n    ARRAY = 6\n\n\nclass TransformOp(object):\n    def __init__(self, name, input, output):\n        self.name = name\n        # The input name or a list of input names\n        self.input = input\n        # The output name\n        self.output = output\n        # The type of the TransformOp.\n        # The value is one of `TransformOpType`.\n        self.op_type = None\n\n\nclass Vocabularize(TransformOp):\n    """"""\n    feature_column:\n        tf.feature_column.categorical_column_with_vocabulary_file\n        tf.feature_column.categorical_column_with_vocabulary_list\n    keras preprocessing:\n        elasticdl_preprocessing.layers.index_lookup.IndexLookup\n    """"""\n\n    def __init__(\n        self, name, input, output, vocabulary_list=None, vocabulary_file=None\n    ):\n        super(Vocabularize, self).__init__(name, input, output)\n        self.op_type = TransformOpType.LOOKUP\n        self.vocabulary_list = vocabulary_list\n        self.vocabulary_file = vocabulary_file\n\n\nclass Bucketize(TransformOp):\n    """"""\n    feature_column:\n        tf.feature_column.bucketized_column\n    keras preprocessing:\n        elasticdl_preprocessing.layers.discretization.Discretization\n    """"""\n\n    def __init__(self, name, input, output, num_buckets=None, boundaries=None):\n        super(Bucketize, self).__init__(name, input, output)\n        self.op_type = TransformOpType.BUCKETIZE\n        self.num_buckets = num_buckets\n        self.boundaries = boundaries\n\n\nclass Embedding(TransformOp):\n    """"""\n    feature_column:\n        tf.feature_column.embedding_column\n    keras preprocessing:\n        elasticdl_preprocessing.layers.embedding.SparseEmbedding\n    """"""\n\n    def __init__(self, name, input, output, input_dim, output_dim):\n        super(Embedding, self).__init__(name, input, output)\n        self.op_type = TransformOpType.EMBEDDING\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n\nclass Hash(TransformOp):\n    """"""\n    feature_column:\n        tf.feature_column.categorical_column_with_hash_bucket\n    keras preprocessing:\n        elasticdl_preprocessing.layers.hashing.Hashing\n    """"""\n\n    def __init__(self, name, input, output, hash_bucket_size):\n        super().__init__(name, input, output)\n        self.op_type = TransformOpType.HASH\n        self.hash_bucket_size = hash_bucket_size\n\n\nclass Concat(TransformOp):\n    """"""\n    feature_column:\n        elasticdl_preprocessing.feature_column \\\n            .concatenated_categorical_column\n    keras preprocessing:\n        elasticdl_preprocessing.layers.concatenate_with_offset \\\n            .ConcatenateWithOffset\n    """"""\n\n    def __init__(self, name, input, output, id_offsets):\n        super().__init__(name, input, output)\n        self.op_type = TransformOpType.CONCAT\n        self.id_offsets = id_offsets\n\n\nclass Array(TransformOp):\n    def __init__(self, name, input, output):\n        super().__init__(name, input, output)\n        self.op_type = TransformOpType.ARRAY\n\n\nclass SchemaInfo(object):\n    def __init__(self, name, dtype):\n        self.name = name\n        self.dtype = dtype\n'"
model_zoo/census_model_sqlflow/wide_and_deep/wide_deep_functional_fc.py,34,"b'import tensorflow as tf\nfrom tensorflow import feature_column as fc\n\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom elasticdl_preprocessing.feature_column import feature_column as edl_fc\nfrom model_zoo.census_model_sqlflow.wide_and_deep.feature_configs import (\n    FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY,\n    INPUT_SCHEMAS,\n    TRANSFORM_OUTPUTS,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    education_hash,\n    group1_embedding_deep,\n    group1_embedding_wide,\n    group2_embedding_deep,\n    group2_embedding_wide,\n    group3_embedding_deep,\n    hours_per_week_bucketize,\n    marital_status_lookup,\n    native_country_hash,\n    occupation_hash,\n    race_lookup,\n    relationship_lookup,\n    sex_lookup,\n    workclass_lookup,\n)\nfrom model_zoo.census_model_sqlflow.wide_and_deep.transform_ops import (\n    TransformOpType,\n)\n\n\n# The model definition from model zoo. It\'s functional style.\n# Input Params:\n#   input_layers: The input layers dict of feature inputs\n#   wide_feature_columns: The feature columns for the wide part\n#   deep_feature_columns: The feature columns for the deep part\ndef wide_and_deep_classifier(\n    input_layers, wide_feature_columns, deep_feature_columns\n):\n    wide_embeddings = []\n    for wide_feature_column in wide_feature_columns:\n        if not isinstance(wide_feature_column, list):\n            wide_feature_column = [wide_feature_column]\n        wide_embedding = tf.keras.layers.DenseFeatures(wide_feature_column)(\n            input_layers\n        )\n        wide_embeddings.append(wide_embedding)\n\n    deep_embeddings = []\n    for deep_feature_column in deep_feature_columns:\n        if not isinstance(deep_feature_column, list):\n            deep_feature_column = [deep_feature_column]\n        deep_embedding = tf.keras.layers.DenseFeatures(deep_feature_column)(\n            input_layers\n        )\n        deep_embeddings.append(deep_embedding)\n\n    # Wide Part\n    wide = tf.keras.layers.Concatenate()(wide_embeddings)  # shape = (None, 3)\n\n    # Deep Part\n    dnn_input = tf.reshape(deep_embeddings, shape=(-1, 3 * 8))\n    for i in [16, 8, 4]:\n        dnn_input = tf.keras.layers.Dense(i)(dnn_input)\n\n    # Output Part\n    concat_input = tf.concat([wide, dnn_input], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n\n\n# Build the input layers from the schema of the input features\ndef get_input_layers(input_schemas):\n    input_layers = {}\n\n    for schema_info in input_schemas:\n        input_layers[schema_info.name] = tf.keras.layers.Input(\n            name=schema_info.name, shape=(1,), dtype=schema_info.dtype\n        )\n\n    return input_layers\n\n\n# Build the transform logic from the metadata in feature_configs.py.\ndef transform(inputs):\n    feature_column_dict = {}\n\n    for feature_transform_info in FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY:\n        if feature_transform_info.op_type == TransformOpType.HASH:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.categorical_column_with_hash_bucket(\n                feature_transform_info.input,\n                hash_bucket_size=feature_transform_info.hash_bucket_size,\n            )\n        elif feature_transform_info.op_type == TransformOpType.BUCKETIZE:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.bucketized_column(\n                fc.numeric_column(feature_transform_info.input),\n                boundaries=feature_transform_info.boundaries,\n            )\n        elif feature_transform_info.op_type == TransformOpType.LOOKUP:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.categorical_column_with_vocabulary_list(\n                feature_transform_info.input,\n                vocabulary_list=workclass_lookup.vocabulary_list,\n            )\n        elif feature_transform_info.op_type == TransformOpType.CONCAT:\n            concat_inputs = [\n                feature_column_dict[name]\n                for name in feature_transform_info.input\n            ]\n            concat_column = edl_fc.concatenated_categorical_column(\n                concat_inputs\n            )\n            feature_column_dict[feature_transform_info.output] = concat_column\n        elif feature_transform_info.op_type == TransformOpType.EMBEDDING:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.embedding_column(\n                feature_column_dict[feature_transform_info.input],\n                dimension=feature_transform_info.output_dim,\n            )\n        elif feature_transform_info.op_type == TransformOpType.ARRAY:\n            feature_column_dict[feature_transform_info.output] = [\n                feature_column_dict[name]\n                for name in feature_transform_info.input\n            ]\n\n    return tuple([feature_column_dict[name] for name in TRANSFORM_OUTPUTS])\n\n\n# The following code has the same logic with the `transform` function above.\n# It can be generated from the parsed meta in feature_configs using code_gen.\ndef transform_from_code_gen(source_inputs):\n    education_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""education"", hash_bucket_size=education_hash.hash_bucket_size\n    )\n\n    occupation_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""occupation"", hash_bucket_size=occupation_hash.hash_bucket_size\n    )\n\n    native_country_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""native_country"", hash_bucket_size=native_country_hash.hash_bucket_size\n    )\n\n    workclass_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""workclass"", vocabulary_list=workclass_lookup.vocabulary_list\n    )\n\n    marital_status_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""marital_status"", vocabulary_list=marital_status_lookup.vocabulary_list\n    )\n\n    relationship_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""relationship"", vocabulary_list=relationship_lookup.vocabulary_list\n    )\n\n    race_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""race"", vocabulary_list=race_lookup.vocabulary_list\n    )\n\n    sex_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""sex"", vocabulary_list=sex_lookup.vocabulary_list\n    )\n\n    age_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""age""), boundaries=age_bucketize.boundaries\n    )\n\n    capital_gain_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""capital_gain""),\n        boundaries=capital_gain_bucketize.boundaries,\n    )\n\n    capital_loss_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""capital_loss""),\n        boundaries=capital_loss_bucketize.boundaries,\n    )\n\n    hours_per_week_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""hours_per_week""),\n        boundaries=hours_per_week_bucketize.boundaries,\n    )\n\n    group1_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            workclass_lookup_fc,\n            hours_per_week_bucketize_fc,\n            capital_gain_bucketize_fc,\n            capital_loss_bucketize_fc,\n        ]\n    )\n\n    group2_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            education_hash_fc,\n            marital_status_lookup_fc,\n            relationship_lookup_fc,\n            occupation_hash_fc,\n        ]\n    )\n\n    group3_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            age_bucketize_fc,\n            sex_lookup_fc,\n            race_lookup_fc,\n            native_country_hash_fc,\n        ]\n    )\n\n    group1_wide_embedding_fc = fc.embedding_column(\n        group1_fc, dimension=group1_embedding_wide.output_dim,\n    )\n\n    group2_wide_embedding_fc = fc.embedding_column(\n        group2_fc, dimension=group2_embedding_wide.output_dim,\n    )\n\n    group1_deep_embedding_fc = fc.embedding_column(\n        group1_fc, dimension=group1_embedding_deep.output_dim,\n    )\n\n    group2_deep_embedding_fc = fc.embedding_column(\n        group2_fc, dimension=group2_embedding_deep.output_dim,\n    )\n\n    group3_deep_embedding_fc = fc.embedding_column(\n        group3_fc, dimension=group3_embedding_deep.output_dim,\n    )\n\n    wide_feature_columns = [\n        [group1_wide_embedding_fc],\n        [group2_wide_embedding_fc],\n    ]\n\n    deep_feature_columns = [\n        [group1_deep_embedding_fc],\n        [group2_deep_embedding_fc],\n        [group3_deep_embedding_fc],\n    ]\n\n    return wide_feature_columns, deep_feature_columns\n\n\n# The entry point of the submitter program\ndef custom_model():\n    input_layers = get_input_layers(input_schemas=INPUT_SCHEMAS)\n    wide_feature_columns, deep_feature_columns = transform_from_code_gen(\n        input_layers\n    )\n\n    return wide_and_deep_classifier(\n        input_layers, wide_feature_columns, deep_feature_columns\n    )\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.0003\n        elif model_version < 12000:\n            return 0.0002\n        else:\n            return 0.0001\n\n    return [LearningRateScheduler(_schedule)]\n\n\nif __name__ == ""__main__"":\n    model = custom_model()\n    print(model.summary())\n\n    output = model.call(\n        {\n            ""education"": tf.constant([[""Bachelors""]], tf.string),\n            ""occupation"": tf.constant([[""Tech-support""]], tf.string),\n            ""native_country"": tf.constant([[""United-States""]], tf.string),\n            ""workclass"": tf.constant([[""Private""]], tf.string),\n            ""marital_status"": tf.constant([[""Separated""]], tf.string),\n            ""relationship"": tf.constant([[""Husband""]], tf.string),\n            ""race"": tf.constant([[""White""]], tf.string),\n            ""sex"": tf.constant([[""Female""]], tf.string),\n            ""age"": tf.constant([[18]], tf.float32),\n            ""capital_gain"": tf.constant([[100.0]], tf.float32),\n            ""capital_loss"": tf.constant([[1.0]], tf.float32),\n            ""hours_per_week"": tf.constant([[40]], tf.float32),\n        }\n    )\n\n    print(output)\n'"
model_zoo/census_model_sqlflow/wide_and_deep/wide_deep_functional_keras.py,28,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom elasticdl_preprocessing.layers import SparseEmbedding\nfrom elasticdl_preprocessing.layers.concatenate_with_offset import (\n    ConcatenateWithOffset,\n)\nfrom elasticdl_preprocessing.layers.discretization import Discretization\nfrom elasticdl_preprocessing.layers.hashing import Hashing\nfrom elasticdl_preprocessing.layers.index_lookup import IndexLookup\nfrom elasticdl_preprocessing.layers.to_sparse import ToSparse\nfrom model_zoo.census_model_sqlflow.wide_and_deep.feature_configs import (\n    FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY,\n    INPUT_SCHEMAS,\n    TRANSFORM_OUTPUTS,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    education_hash,\n    group1,\n    group1_embedding_deep,\n    group1_embedding_wide,\n    group2,\n    group2_embedding_deep,\n    group2_embedding_wide,\n    group3,\n    group3_embedding_deep,\n    hours_per_week_bucketize,\n    marital_status_lookup,\n    native_country_hash,\n    occupation_hash,\n    race_lookup,\n    relationship_lookup,\n    sex_lookup,\n    workclass_lookup,\n)\nfrom model_zoo.census_model_sqlflow.wide_and_deep.transform_ops import (\n    TransformOpType,\n)\n\n\n# The model definition from model zoo. It\'s functional style.\n# Input Params:\n#   input_layers: The input layers dict of feature inputs\n#   wide_embeddings: The embedding list for the wide part\n#   deep_embeddings: The embedding list for the deep part\ndef wide_and_deep_classifier(input_layers, wide_embeddings, deep_embeddings):\n    # Wide Part\n    wide = tf.keras.layers.Concatenate()(wide_embeddings)  # shape = (None, 3)\n\n    # Deep Part\n    dnn_input = tf.reshape(deep_embeddings, shape=(-1, 3 * 8))\n    for i in [16, 8, 4]:\n        dnn_input = tf.keras.layers.Dense(i)(dnn_input)\n\n    # Output Part\n    concat_input = tf.concat([wide, dnn_input], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n\n\n# Build the input layers from the schema of the input features\ndef get_input_layers(input_schemas):\n    input_layers = {}\n\n    for schema_info in input_schemas:\n        input_layers[schema_info.name] = tf.keras.layers.Input(\n            name=schema_info.name, shape=(1,), dtype=schema_info.dtype\n        )\n\n    return input_layers\n\n\n# Build the transform logic from the metadata in feature_configs.py.\ndef transform(inputs):\n    transformed = inputs.copy()\n\n    for feature_transform_info in FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY:\n        if feature_transform_info.op_type == TransformOpType.HASH:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = Hashing(\n                feature_transform_info.hash_bucket_size\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.BUCKETIZE:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = Discretization(\n                feature_transform_info.boundaries\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.LOOKUP:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = IndexLookup(\n                feature_transform_info.vocabulary_list\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.CONCAT:\n            inputs_to_concat = [\n                transformed[name] for name in feature_transform_info.input\n            ]\n            transformed[feature_transform_info.output] = ConcatenateWithOffset(\n                feature_transform_info.id_offsets\n            )(inputs_to_concat)\n        elif feature_transform_info.op_type == TransformOpType.EMBEDDING:\n            transformed[feature_transform_info.output] = SparseEmbedding(\n                input_dim=feature_transform_info.input_dim,\n                output_dim=feature_transform_info.output_dim,\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.ARRAY:\n            transformed[feature_transform_info.output] = [\n                transformed[name] for name in feature_transform_info.input\n            ]\n\n    return tuple([transformed[name] for name in TRANSFORM_OUTPUTS])\n\n\n# The following code has the same logic with the `transform` function above.\n# It can be generated from the parsed meta in feature_configs using code_gen.\ndef transform_from_code_gen(source_inputs):\n    inputs = source_inputs.copy()\n\n    education_hash_out = Hashing(education_hash.hash_bucket_size)(\n        ToSparse()(inputs[""education""])\n    )\n    occupation_hash_out = Hashing(occupation_hash.hash_bucket_size)(\n        ToSparse()(inputs[""occupation""])\n    )\n    native_country_hash_out = Hashing(native_country_hash.hash_bucket_size)(\n        ToSparse()(inputs[""native_country""])\n    )\n    workclass_lookup_out = IndexLookup(workclass_lookup.vocabulary_list)(\n        ToSparse()(inputs[""workclass""])\n    )\n    marital_status_lookup_out = IndexLookup(\n        marital_status_lookup.vocabulary_list\n    )(ToSparse()(inputs[""marital_status""]))\n    relationship_lookup_out = IndexLookup(relationship_lookup.vocabulary_list)(\n        ToSparse()(inputs[""relationship""])\n    )\n    race_lookup_out = IndexLookup(race_lookup.vocabulary_list)(\n        ToSparse()(inputs[""race""])\n    )\n    sex_lookup_out = IndexLookup(sex_lookup.vocabulary_list)(\n        ToSparse()(inputs[""sex""])\n    )\n    age_bucketize_out = Discretization(age_bucketize.boundaries)(\n        ToSparse()(inputs[""age""])\n    )\n    capital_gain_bucketize_out = Discretization(\n        capital_gain_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_gain""]))\n    capital_loss_bucketize_out = Discretization(\n        capital_loss_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_loss""]))\n    hours_per_week_bucketize_out = Discretization(\n        hours_per_week_bucketize.boundaries\n    )(ToSparse()(inputs[""hours_per_week""]))\n\n    group1_out = ConcatenateWithOffset(group1.id_offsets)(\n        [\n            workclass_lookup_out,\n            hours_per_week_bucketize_out,\n            capital_gain_bucketize_out,\n            capital_loss_bucketize_out,\n        ]\n    )\n    group2_out = ConcatenateWithOffset(group2.id_offsets)(\n        [\n            education_hash_out,\n            marital_status_lookup_out,\n            relationship_lookup_out,\n            occupation_hash_out,\n        ]\n    )\n    group3_out = ConcatenateWithOffset(group3.id_offsets)(\n        [\n            age_bucketize_out,\n            sex_lookup_out,\n            race_lookup_out,\n            native_country_hash_out,\n        ]\n    )\n\n    group1_embedding_wide_out = SparseEmbedding(\n        input_dim=group1_embedding_wide.input_dim,\n        output_dim=group1_embedding_wide.output_dim,\n    )(group1_out)\n    group2_embedding_wide_out = SparseEmbedding(\n        input_dim=group2_embedding_wide.input_dim,\n        output_dim=group2_embedding_wide.output_dim,\n    )(group2_out)\n\n    group1_embedding_deep_out = SparseEmbedding(\n        input_dim=group1_embedding_deep.input_dim,\n        output_dim=group1_embedding_deep.output_dim,\n    )(group1_out)\n    group2_embedding_deep_out = SparseEmbedding(\n        input_dim=group2_embedding_deep.input_dim,\n        output_dim=group2_embedding_deep.output_dim,\n    )(group2_out)\n    group3_embedding_deep_out = SparseEmbedding(\n        input_dim=group3_embedding_deep.input_dim,\n        output_dim=group3_embedding_deep.output_dim,\n    )(group3_out)\n\n    wide_embeddings_out = [\n        group1_embedding_wide_out,\n        group2_embedding_wide_out,\n    ]\n\n    deep_embeddings_out = [\n        group1_embedding_deep_out,\n        group2_embedding_deep_out,\n        group3_embedding_deep_out,\n    ]\n\n    return wide_embeddings_out, deep_embeddings_out\n\n\n# The entry point of the submitter program\ndef custom_model():\n    input_layers = get_input_layers(input_schemas=INPUT_SCHEMAS)\n    wide_embeddings, deep_embeddings = transform_from_code_gen(input_layers)\n\n    return wide_and_deep_classifier(\n        input_layers, wide_embeddings, deep_embeddings\n    )\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.0003\n        elif model_version < 12000:\n            return 0.0002\n        else:\n            return 0.0001\n\n    return [LearningRateScheduler(_schedule)]\n\n\nif __name__ == ""__main__"":\n    model = custom_model()\n    print(model.summary())\n\n    output = model.call(\n        {\n            ""education"": tf.constant([[""Bachelors""]], tf.string),\n            ""occupation"": tf.constant([[""Tech-support""]], tf.string),\n            ""native_country"": tf.constant([[""United-States""]], tf.string),\n            ""workclass"": tf.constant([[""Private""]], tf.string),\n            ""marital_status"": tf.constant([[""Separated""]], tf.string),\n            ""relationship"": tf.constant([[""Husband""]], tf.string),\n            ""race"": tf.constant([[""White""]], tf.string),\n            ""sex"": tf.constant([[""Female""]], tf.string),\n            ""age"": tf.constant([[18]], tf.float32),\n            ""capital_gain"": tf.constant([[100.0]], tf.float32),\n            ""capital_loss"": tf.constant([[1.0]], tf.float32),\n            ""hours_per_week"": tf.constant([[40]], tf.float32),\n        }\n    )\n\n    print(output)\n'"
model_zoo/census_model_sqlflow/wide_and_deep/wide_deep_functional_tensor_interface_fc.py,33,"b'import tensorflow as tf\nfrom tensorflow import feature_column as fc\n\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom elasticdl_preprocessing.feature_column import feature_column as edl_fc\nfrom model_zoo.census_model_sqlflow.wide_and_deep.feature_configs import (\n    FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY,\n    INPUT_SCHEMAS,\n    TRANSFORM_OUTPUTS,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    education_hash,\n    group1_embedding_deep,\n    group1_embedding_wide,\n    group2_embedding_deep,\n    group2_embedding_wide,\n    group3_embedding_deep,\n    hours_per_week_bucketize,\n    marital_status_lookup,\n    native_country_hash,\n    occupation_hash,\n    race_lookup,\n    relationship_lookup,\n    sex_lookup,\n    workclass_lookup,\n)\nfrom model_zoo.census_model_sqlflow.wide_and_deep.transform_ops import (\n    TransformOpType,\n)\n\n\n# The model definition from model zoo. It\'s functional style.\n# Input Params:\n#   input_layers: The input layers dict of feature inputs\n#   wide_embedding: A tensor. Embedding for the wide part.\n#   deep_embedding: A tensor. Embedding for the deep part.\ndef wide_and_deep_classifier(input_layers, wide_embedding, deep_embedding):\n    # Wide Part\n    wide = wide_embedding  # shape = (None, 3)\n\n    # Deep Part\n    dnn_input = deep_embedding\n    for i in [16, 8, 4]:\n        dnn_input = tf.keras.layers.Dense(i)(dnn_input)\n\n    # Output Part\n    concat_input = tf.concat([wide, dnn_input], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n\n\n# Build the input layers from the schema of the input features\ndef get_input_layers(input_schemas):\n    input_layers = {}\n\n    for schema_info in input_schemas:\n        input_layers[schema_info.name] = tf.keras.layers.Input(\n            name=schema_info.name, shape=(1,), dtype=schema_info.dtype\n        )\n\n    return input_layers\n\n\n# Build the transform logic from the metadata in feature_configs.py.\ndef transform(inputs):\n    feature_column_dict = {}\n\n    for feature_transform_info in FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY:\n        if feature_transform_info.op_type == TransformOpType.HASH:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.categorical_column_with_hash_bucket(\n                feature_transform_info.input,\n                hash_bucket_size=feature_transform_info.hash_bucket_size,\n            )\n        elif feature_transform_info.op_type == TransformOpType.BUCKETIZE:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.bucketized_column(\n                fc.numeric_column(feature_transform_info.input),\n                boundaries=feature_transform_info.boundaries,\n            )\n        elif feature_transform_info.op_type == TransformOpType.LOOKUP:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.categorical_column_with_vocabulary_list(\n                feature_transform_info.input,\n                vocabulary_list=workclass_lookup.vocabulary_list,\n            )\n        elif feature_transform_info.op_type == TransformOpType.CONCAT:\n            concat_inputs = [\n                feature_column_dict[name]\n                for name in feature_transform_info.input\n            ]\n            concat_column = edl_fc.concatenated_categorical_column(\n                concat_inputs\n            )\n            feature_column_dict[feature_transform_info.output] = concat_column\n        elif feature_transform_info.op_type == TransformOpType.EMBEDDING:\n            feature_column_dict[\n                feature_transform_info.output\n            ] = tf.feature_column.embedding_column(\n                feature_column_dict[feature_transform_info.input],\n                dimension=feature_transform_info.output_dim,\n            )\n        elif feature_transform_info.op_type == TransformOpType.ARRAY:\n            feature_column_dict[feature_transform_info.output] = [\n                feature_column_dict[name]\n                for name in feature_transform_info.input\n            ]\n\n    return tuple(\n        [\n            tf.keras.layers.DenseFeatures(feature_column_dict[name])(inputs)\n            for name in TRANSFORM_OUTPUTS\n        ]\n    )\n\n\n# The following code has the same logic with the `transform` function above.\n# It can be generated from the parsed meta in feature_configs using code_gen.\ndef transform_from_code_gen(source_inputs):\n    education_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""education"", hash_bucket_size=education_hash.hash_bucket_size\n    )\n\n    occupation_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""occupation"", hash_bucket_size=occupation_hash.hash_bucket_size\n    )\n\n    native_country_hash_fc = fc.categorical_column_with_hash_bucket(\n        ""native_country"", hash_bucket_size=native_country_hash.hash_bucket_size\n    )\n\n    workclass_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""workclass"", vocabulary_list=workclass_lookup.vocabulary_list\n    )\n\n    marital_status_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""marital_status"", vocabulary_list=marital_status_lookup.vocabulary_list\n    )\n\n    relationship_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""relationship"", vocabulary_list=relationship_lookup.vocabulary_list\n    )\n\n    race_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""race"", vocabulary_list=race_lookup.vocabulary_list\n    )\n\n    sex_lookup_fc = fc.categorical_column_with_vocabulary_list(\n        ""sex"", vocabulary_list=sex_lookup.vocabulary_list\n    )\n\n    age_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""age""), boundaries=age_bucketize.boundaries\n    )\n\n    capital_gain_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""capital_gain""),\n        boundaries=capital_gain_bucketize.boundaries,\n    )\n\n    capital_loss_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""capital_loss""),\n        boundaries=capital_loss_bucketize.boundaries,\n    )\n\n    hours_per_week_bucketize_fc = fc.bucketized_column(\n        fc.numeric_column(""hours_per_week""),\n        boundaries=hours_per_week_bucketize.boundaries,\n    )\n\n    group1_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            workclass_lookup_fc,\n            hours_per_week_bucketize_fc,\n            capital_gain_bucketize_fc,\n            capital_loss_bucketize_fc,\n        ]\n    )\n\n    group2_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            education_hash_fc,\n            marital_status_lookup_fc,\n            relationship_lookup_fc,\n            occupation_hash_fc,\n        ]\n    )\n\n    group3_fc = edl_fc.concatenated_categorical_column(\n        categorical_columns=[\n            age_bucketize_fc,\n            sex_lookup_fc,\n            race_lookup_fc,\n            native_country_hash_fc,\n        ]\n    )\n\n    group1_wide_embedding_fc = fc.embedding_column(\n        group1_fc, dimension=group1_embedding_wide.output_dim,\n    )\n\n    group2_wide_embedding_fc = fc.embedding_column(\n        group2_fc, dimension=group2_embedding_wide.output_dim,\n    )\n\n    group1_deep_embedding_fc = fc.embedding_column(\n        group1_fc, dimension=group1_embedding_deep.output_dim,\n    )\n\n    group2_deep_embedding_fc = fc.embedding_column(\n        group2_fc, dimension=group2_embedding_deep.output_dim,\n    )\n\n    group3_deep_embedding_fc = fc.embedding_column(\n        group3_fc, dimension=group3_embedding_deep.output_dim,\n    )\n\n    wide_feature_columns = [\n        group1_wide_embedding_fc,\n        group2_wide_embedding_fc,\n    ]\n\n    deep_feature_columns = [\n        group1_deep_embedding_fc,\n        group2_deep_embedding_fc,\n        group3_deep_embedding_fc,\n    ]\n\n    return (\n        tf.keras.layers.DenseFeatures(wide_feature_columns)(source_inputs),\n        tf.keras.layers.DenseFeatures(deep_feature_columns)(source_inputs),\n    )\n\n\n# The entry point of the submitter program\ndef custom_model():\n    input_layers = get_input_layers(input_schemas=INPUT_SCHEMAS)\n    wide_embedding, deep_embedding = transform_from_code_gen(input_layers)\n\n    return wide_and_deep_classifier(\n        input_layers, wide_embedding, deep_embedding\n    )\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.0003\n        elif model_version < 12000:\n            return 0.0002\n        else:\n            return 0.0001\n\n    return [LearningRateScheduler(_schedule)]\n\n\nif __name__ == ""__main__"":\n    model = custom_model()\n    print(model.summary())\n\n    output = model.call(\n        {\n            ""education"": tf.constant([[""Bachelors""]], tf.string),\n            ""occupation"": tf.constant([[""Tech-support""]], tf.string),\n            ""native_country"": tf.constant([[""United-States""]], tf.string),\n            ""workclass"": tf.constant([[""Private""]], tf.string),\n            ""marital_status"": tf.constant([[""Separated""]], tf.string),\n            ""relationship"": tf.constant([[""Husband""]], tf.string),\n            ""race"": tf.constant([[""White""]], tf.string),\n            ""sex"": tf.constant([[""Female""]], tf.string),\n            ""age"": tf.constant([[18]], tf.float32),\n            ""capital_gain"": tf.constant([[100.0]], tf.float32),\n            ""capital_loss"": tf.constant([[1.0]], tf.float32),\n            ""hours_per_week"": tf.constant([[40]], tf.float32),\n        }\n    )\n\n    print(output)\n'"
model_zoo/census_model_sqlflow/wide_and_deep/wide_deep_functional_tensor_interface_keras.py,26,"b'import tensorflow as tf\n\nfrom elasticdl.python.elasticdl.callbacks import LearningRateScheduler\nfrom elasticdl_preprocessing.layers import SparseEmbedding\nfrom elasticdl_preprocessing.layers.concatenate_with_offset import (\n    ConcatenateWithOffset,\n)\nfrom elasticdl_preprocessing.layers.discretization import Discretization\nfrom elasticdl_preprocessing.layers.hashing import Hashing\nfrom elasticdl_preprocessing.layers.index_lookup import IndexLookup\nfrom elasticdl_preprocessing.layers.to_sparse import ToSparse\nfrom model_zoo.census_model_sqlflow.wide_and_deep.feature_configs import (\n    FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY,\n    INPUT_SCHEMAS,\n    TRANSFORM_OUTPUTS,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    education_hash,\n    group1,\n    group1_embedding_deep,\n    group1_embedding_wide,\n    group2,\n    group2_embedding_deep,\n    group2_embedding_wide,\n    group3,\n    group3_embedding_deep,\n    hours_per_week_bucketize,\n    marital_status_lookup,\n    native_country_hash,\n    occupation_hash,\n    race_lookup,\n    relationship_lookup,\n    sex_lookup,\n    workclass_lookup,\n)\nfrom model_zoo.census_model_sqlflow.wide_and_deep.transform_ops import (\n    TransformOpType,\n)\n\n\n# The model definition from model zoo. It\'s functional style.\n# Input Params:\n#   input_layers: The input layers dict of feature inputs\n#   wide_embedding: A tensor. Embedding for the wide part.\n#   deep_embedding: A tensor. Embedding for the deep part.\ndef wide_and_deep_classifier(input_layers, wide_embedding, deep_embedding):\n    # Wide Part\n    wide = wide_embedding  # shape = (None, 3)\n\n    # Deep Part\n    dnn_input = deep_embedding\n    for i in [16, 8, 4]:\n        dnn_input = tf.keras.layers.Dense(i)(dnn_input)\n\n    # Output Part\n    concat_input = tf.concat([wide, dnn_input], 1)\n\n    logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n    probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n\n    return tf.keras.Model(\n        inputs=input_layers,\n        outputs={""logits"": logits, ""probs"": probs},\n        name=""wide_deep"",\n    )\n\n\n# Build the input layers from the schema of the input features\ndef get_input_layers(input_schemas):\n    input_layers = {}\n\n    for schema_info in input_schemas:\n        input_layers[schema_info.name] = tf.keras.layers.Input(\n            name=schema_info.name, shape=(1,), dtype=schema_info.dtype\n        )\n\n    return input_layers\n\n\n# Build the transform logic from the metadata in feature_configs.py.\ndef transform(inputs):\n    transformed = inputs.copy()\n\n    for feature_transform_info in FEATURE_TRANSFORM_INFO_EXECUTE_ARRAY:\n        if feature_transform_info.op_type == TransformOpType.HASH:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = Hashing(\n                feature_transform_info.hash_bucket_size\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.BUCKETIZE:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = Discretization(\n                feature_transform_info.boundaries\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.LOOKUP:\n            transformed[feature_transform_info.input] = ToSparse()(\n                transformed[feature_transform_info.input]\n            )\n            transformed[feature_transform_info.output] = IndexLookup(\n                feature_transform_info.vocabulary_list\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.CONCAT:\n            inputs_to_concat = [\n                transformed[name] for name in feature_transform_info.input\n            ]\n            transformed[feature_transform_info.output] = ConcatenateWithOffset(\n                feature_transform_info.id_offsets\n            )(inputs_to_concat)\n        elif feature_transform_info.op_type == TransformOpType.EMBEDDING:\n            transformed[feature_transform_info.output] = SparseEmbedding(\n                input_dim=feature_transform_info.input_dim,\n                output_dim=feature_transform_info.output_dim,\n            )(transformed[feature_transform_info.input])\n        elif feature_transform_info.op_type == TransformOpType.ARRAY:\n            transformed[feature_transform_info.output] = ConcatenateWithOffset(\n                offsets=None\n            )([transformed[name] for name in feature_transform_info.input])\n\n    return tuple([transformed[name] for name in TRANSFORM_OUTPUTS])\n\n\n# The following code has the same logic with the `transform` function above.\n# It can be generated from the parsed meta in feature_configs using code_gen.\ndef transform_from_code_gen(source_inputs):\n    inputs = source_inputs.copy()\n\n    education_hash_out = Hashing(education_hash.hash_bucket_size)(\n        ToSparse()(inputs[""education""])\n    )\n    occupation_hash_out = Hashing(occupation_hash.hash_bucket_size)(\n        ToSparse()(inputs[""occupation""])\n    )\n    native_country_hash_out = Hashing(native_country_hash.hash_bucket_size)(\n        ToSparse()(inputs[""native_country""])\n    )\n    workclass_lookup_out = IndexLookup(workclass_lookup.vocabulary_list)(\n        ToSparse()(inputs[""workclass""])\n    )\n    marital_status_lookup_out = IndexLookup(\n        marital_status_lookup.vocabulary_list\n    )(ToSparse()(inputs[""marital_status""]))\n    relationship_lookup_out = IndexLookup(relationship_lookup.vocabulary_list)(\n        ToSparse()(inputs[""relationship""])\n    )\n    race_lookup_out = IndexLookup(race_lookup.vocabulary_list)(\n        ToSparse()(inputs[""race""])\n    )\n    sex_lookup_out = IndexLookup(sex_lookup.vocabulary_list)(\n        ToSparse()(inputs[""sex""])\n    )\n    age_bucketize_out = Discretization(age_bucketize.boundaries)(\n        ToSparse()(inputs[""age""])\n    )\n    capital_gain_bucketize_out = Discretization(\n        capital_gain_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_gain""]))\n    capital_loss_bucketize_out = Discretization(\n        capital_loss_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_loss""]))\n    hours_per_week_bucketize_out = Discretization(\n        hours_per_week_bucketize.boundaries\n    )(ToSparse()(inputs[""hours_per_week""]))\n\n    group1_out = ConcatenateWithOffset(group1.id_offsets)(\n        [\n            workclass_lookup_out,\n            hours_per_week_bucketize_out,\n            capital_gain_bucketize_out,\n            capital_loss_bucketize_out,\n        ]\n    )\n    group2_out = ConcatenateWithOffset(group2.id_offsets)(\n        [\n            education_hash_out,\n            marital_status_lookup_out,\n            relationship_lookup_out,\n            occupation_hash_out,\n        ]\n    )\n    group3_out = ConcatenateWithOffset(group3.id_offsets)(\n        [\n            age_bucketize_out,\n            sex_lookup_out,\n            race_lookup_out,\n            native_country_hash_out,\n        ]\n    )\n\n    group1_embedding_wide_out = SparseEmbedding(\n        input_dim=group1_embedding_wide.input_dim,\n        output_dim=group1_embedding_wide.output_dim,\n    )(group1_out)\n    group2_embedding_wide_out = SparseEmbedding(\n        input_dim=group2_embedding_wide.input_dim,\n        output_dim=group2_embedding_wide.output_dim,\n    )(group2_out)\n\n    group1_embedding_deep_out = SparseEmbedding(\n        input_dim=group1_embedding_deep.input_dim,\n        output_dim=group1_embedding_deep.output_dim,\n    )(group1_out)\n    group2_embedding_deep_out = SparseEmbedding(\n        input_dim=group2_embedding_deep.input_dim,\n        output_dim=group2_embedding_deep.output_dim,\n    )(group2_out)\n    group3_embedding_deep_out = SparseEmbedding(\n        input_dim=group3_embedding_deep.input_dim,\n        output_dim=group3_embedding_deep.output_dim,\n    )(group3_out)\n\n    wide_embeddings_out = ConcatenateWithOffset(offsets=None)(\n        [group1_embedding_wide_out, group2_embedding_wide_out]\n    )\n\n    deep_embeddings_out = ConcatenateWithOffset(offsets=None)(\n        [\n            group1_embedding_deep_out,\n            group2_embedding_deep_out,\n            group3_embedding_deep_out,\n        ]\n    )\n\n    return wide_embeddings_out, deep_embeddings_out\n\n\n# The entry point of the submitter program\ndef custom_model():\n    input_layers = get_input_layers(input_schemas=INPUT_SCHEMAS)\n    wide_embedding, deep_embedding = transform_from_code_gen(input_layers)\n\n    return wide_and_deep_classifier(\n        input_layers, wide_embedding, deep_embedding\n    )\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\ndef callbacks():\n    def _schedule(model_version):\n        if model_version < 5000:\n            return 0.0003\n        elif model_version < 12000:\n            return 0.0002\n        else:\n            return 0.0001\n\n    return [LearningRateScheduler(_schedule)]\n\n\nif __name__ == ""__main__"":\n    model = custom_model()\n    print(model.summary())\n\n    output = model.call(\n        {\n            ""education"": tf.constant([[""Bachelors""]], tf.string),\n            ""occupation"": tf.constant([[""Tech-support""]], tf.string),\n            ""native_country"": tf.constant([[""United-States""]], tf.string),\n            ""workclass"": tf.constant([[""Private""]], tf.string),\n            ""marital_status"": tf.constant([[""Separated""]], tf.string),\n            ""relationship"": tf.constant([[""Husband""]], tf.string),\n            ""race"": tf.constant([[""White""]], tf.string),\n            ""sex"": tf.constant([[""Female""]], tf.string),\n            ""age"": tf.constant([[18]], tf.float32),\n            ""capital_gain"": tf.constant([[100.0]], tf.float32),\n            ""capital_loss"": tf.constant([[1.0]], tf.float32),\n            ""hours_per_week"": tf.constant([[40]], tf.float32),\n        }\n    )\n\n    print(output)\n'"
model_zoo/census_model_sqlflow/wide_and_deep/wide_deep_subclass_keras.py,27,"b'import tensorflow as tf\n\nfrom elasticdl_preprocessing.layers import SparseEmbedding\nfrom elasticdl_preprocessing.layers.concatenate_with_offset import (\n    ConcatenateWithOffset,\n)\nfrom elasticdl_preprocessing.layers.discretization import Discretization\nfrom elasticdl_preprocessing.layers.hashing import Hashing\nfrom elasticdl_preprocessing.layers.index_lookup import IndexLookup\nfrom elasticdl_preprocessing.layers.to_sparse import ToSparse\nfrom elasticdl_preprocessing.utils.decorators import declare_model_inputs\nfrom model_zoo.census_model_sqlflow.wide_and_deep.feature_configs import (\n    INPUT_SCHEMAS,\n    age_bucketize,\n    capital_gain_bucketize,\n    capital_loss_bucketize,\n    education_hash,\n    group1,\n    group1_embedding_deep,\n    group1_embedding_wide,\n    group2,\n    group2_embedding_deep,\n    group2_embedding_wide,\n    group3,\n    group3_embedding_deep,\n    hours_per_week_bucketize,\n    marital_status_lookup,\n    native_country_hash,\n    occupation_hash,\n    race_lookup,\n    relationship_lookup,\n    sex_lookup,\n    workclass_lookup,\n)\n\n\n# The model definition in model zoo\n# Add `@declare_model_inputs` to declare that this model\n# need two input tensors: `wide_embeddings` and `deep_embeddings`.\n@declare_model_inputs(""wide_embeddings"", ""deep_embeddings"")\nclass WideAndDeepClassifier(tf.keras.Model):\n    def __init__(self, hidden_units=[16, 8, 4]):\n        super(WideAndDeepClassifier, self).__init__()\n        self.dense_layers = [tf.keras.layers.Dense(i) for i in hidden_units]\n\n    def call(self, inputs):\n        wide_input, dnn_input = inputs\n\n        for dense_layer in self.dense_layers:\n            dnn_input = dense_layer(dnn_input)\n\n        # Output Part\n        concat_input = tf.concat([wide_input, dnn_input], 1)\n\n        logits = tf.reduce_sum(concat_input, 1, keepdims=True)\n        probs = tf.reshape(tf.sigmoid(logits), shape=(-1,))\n\n        return {""logits"": logits, ""probs"": probs}\n\n\n# Build the input layers from the schema of the input features\ndef get_input_layers(input_schemas):\n    input_layers = {}\n\n    for schema_info in input_schemas:\n        input_layers[schema_info.name] = tf.keras.layers.Input(\n            name=schema_info.name, shape=(1,), dtype=schema_info.dtype\n        )\n\n    return input_layers\n\n\n# The following code has the same logic with the `transform` function above.\n# It can be generated from the parsed meta in feature_configs using code_gen.\ndef transform_from_code_gen(inputs):\n    education_hash_out = Hashing(education_hash.hash_bucket_size)(\n        ToSparse()(inputs[""education""])\n    )\n    occupation_hash_out = Hashing(occupation_hash.hash_bucket_size)(\n        ToSparse()(inputs[""occupation""])\n    )\n    native_country_hash_out = Hashing(native_country_hash.hash_bucket_size)(\n        ToSparse()(inputs[""native_country""])\n    )\n    workclass_lookup_out = IndexLookup(workclass_lookup.vocabulary_list)(\n        ToSparse()(inputs[""workclass""])\n    )\n    marital_status_lookup_out = IndexLookup(\n        marital_status_lookup.vocabulary_list\n    )(ToSparse()(inputs[""marital_status""]))\n    relationship_lookup_out = IndexLookup(relationship_lookup.vocabulary_list)(\n        ToSparse()(inputs[""relationship""])\n    )\n    race_lookup_out = IndexLookup(race_lookup.vocabulary_list)(\n        ToSparse()(inputs[""race""])\n    )\n    sex_lookup_out = IndexLookup(sex_lookup.vocabulary_list)(\n        ToSparse()(inputs[""sex""])\n    )\n    age_bucketize_out = Discretization(age_bucketize.boundaries)(\n        ToSparse()(inputs[""age""])\n    )\n    capital_gain_bucketize_out = Discretization(\n        capital_gain_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_gain""]))\n    capital_loss_bucketize_out = Discretization(\n        capital_loss_bucketize.boundaries\n    )(ToSparse()(inputs[""capital_loss""]))\n    hours_per_week_bucketize_out = Discretization(\n        hours_per_week_bucketize.boundaries\n    )(ToSparse()(inputs[""hours_per_week""]))\n\n    group1_out = ConcatenateWithOffset(group1.id_offsets)(\n        [\n            workclass_lookup_out,\n            hours_per_week_bucketize_out,\n            capital_gain_bucketize_out,\n            capital_loss_bucketize_out,\n        ]\n    )\n    group2_out = ConcatenateWithOffset(group2.id_offsets)(\n        [\n            education_hash_out,\n            marital_status_lookup_out,\n            relationship_lookup_out,\n            occupation_hash_out,\n        ]\n    )\n    group3_out = ConcatenateWithOffset(group3.id_offsets)(\n        [\n            age_bucketize_out,\n            sex_lookup_out,\n            race_lookup_out,\n            native_country_hash_out,\n        ]\n    )\n\n    group1_embedding_wide_out = SparseEmbedding(\n        input_dim=group1_embedding_wide.input_dim,\n        output_dim=group1_embedding_wide.output_dim,\n    )(group1_out)\n    group2_embedding_wide_out = SparseEmbedding(\n        input_dim=group2_embedding_wide.input_dim,\n        output_dim=group2_embedding_wide.output_dim,\n    )(group2_out)\n\n    group1_embedding_deep_out = SparseEmbedding(\n        input_dim=group1_embedding_deep.input_dim,\n        output_dim=group1_embedding_deep.output_dim,\n    )(group1_out)\n    group2_embedding_deep_out = SparseEmbedding(\n        input_dim=group2_embedding_deep.input_dim,\n        output_dim=group2_embedding_deep.output_dim,\n    )(group2_out)\n    group3_embedding_deep_out = SparseEmbedding(\n        input_dim=group3_embedding_deep.input_dim,\n        output_dim=group3_embedding_deep.output_dim,\n    )(group3_out)\n\n    wide_embeddings_out = ConcatenateWithOffset(offsets=None)(\n        [group1_embedding_wide_out, group2_embedding_wide_out]\n    )\n\n    deep_embeddings_out = ConcatenateWithOffset(offsets=None)(\n        [\n            group1_embedding_deep_out,\n            group2_embedding_deep_out,\n            group3_embedding_deep_out,\n        ]\n    )\n\n    return wide_embeddings_out, deep_embeddings_out\n\n\n# The entry point of the submitter program.\n# This should be generated by SQLFlow.\ndef custom_model():\n    input_layers = get_input_layers(input_schemas=INPUT_SCHEMAS)\n    transformed = transform_from_code_gen(input_layers)\n    main_model = WideAndDeepClassifier()\n    outputs = main_model(transformed)\n    return tf.keras.Model(inputs=input_layers, outputs=outputs)\n\n\ndef loss(labels, predictions):\n    logits = predictions[""logits""]\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=tf.cast(tf.reshape(labels, (-1, 1)), tf.float32),\n            logits=logits,\n        )\n    )\n\n\ndef optimizer(lr=0.001):\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef eval_metrics_fn():\n    return {\n        ""logits"": {\n            ""accuracy"": lambda labels, predictions: tf.equal(\n                tf.cast(tf.reshape(predictions, [-1]) > 0.5, tf.int32),\n                tf.cast(tf.reshape(labels, [-1]), tf.int32),\n            )\n        },\n        ""probs"": {""auc"": tf.keras.metrics.AUC()},\n    }\n\n\nif __name__ == ""__main__"":\n    print(WideAndDeepClassifier._model_inputs)\n\n    model = custom_model()\n    print(model.summary())\n\n    inputs = {\n        ""education"": tf.constant([[""Bachelors""]], tf.string),\n        ""occupation"": tf.constant([[""Tech-support""]], tf.string),\n        ""native_country"": tf.constant([[""United-States""]], tf.string),\n        ""workclass"": tf.constant([[""Private""]], tf.string),\n        ""marital_status"": tf.constant([[""Separated""]], tf.string),\n        ""relationship"": tf.constant([[""Husband""]], tf.string),\n        ""race"": tf.constant([[""White""]], tf.string),\n        ""sex"": tf.constant([[""Female""]], tf.string),\n        ""age"": tf.constant([[18]], tf.float32),\n        ""capital_gain"": tf.constant([[100.0]], tf.float32),\n        ""capital_loss"": tf.constant([[1.0]], tf.float32),\n        ""hours_per_week"": tf.constant([[40]], tf.float32),\n    }\n\n    output = model.call(inputs)\n    print(output)\n'"
elasticdl/python/data/reader/__init__.py,0,b''
elasticdl/python/data/reader/csv_reader.py,1,"b'import csv\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom elasticdl.python.data.reader.data_reader import (\n    AbstractDataReader,\n    Metadata,\n    check_required_kwargs,\n)\n\n\nclass CSVDataReader(AbstractDataReader):\n    """"""This reader is used to read data from a csv file. It is convenient for\n    user to locally run and debug a Keras model by using this reader.\n    However, it cannot be used with distribution strategy because it cannot\n    read data by line indices.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            kwargs should contains ""sep"" and ""columns"" like\n            \'sep="","",column=[""sepal.length"", ""sepal.width"", ""variety""]\'\n        """"""\n        AbstractDataReader.__init__(self, **kwargs)\n        check_required_kwargs([""sep"", ""columns""], kwargs)\n        self.sep = kwargs.get(""sep"", "","")\n        self.selected_columns = kwargs.get(""columns"", None)\n\n    def read_records(self, task):\n        with open(task.shard_name, ""r"", encoding=""utf-8"") as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=self.sep)\n            csv_columns = next(csv_reader)\n            selected_columns = (\n                csv_columns\n                if self.selected_columns is None\n                else self.selected_columns\n            )\n            if not set(selected_columns).issubset(set(csv_columns)):\n                raise ValueError(\n                    ""The first line in the csv file must be column names and ""\n                    ""the selected columns are not in the file. The selected ""\n                    ""columns are {} and the columns in {} are {}"".format(\n                        selected_columns, task.shard_name, csv_columns\n                    )\n                )\n            column_indices = [csv_columns.index(e) for e in selected_columns]\n            for line in csv_reader:\n                line_elements = np.array(line, dtype=np.str)\n                yield line_elements[column_indices].tolist()\n\n    def create_shards(self):\n        pass\n\n    @property\n    def records_output_types(self):\n        return tf.string\n\n    @property\n    def metadata(self):\n        return Metadata(column_names=self.selected_columns)\n'"
elasticdl/python/data/reader/data_reader.py,3,"b'from abc import ABC, abstractmethod\n\nfrom elasticdl.python.common.dtypes import MAXCOMPUTE_DTYPE_TO_TF_DTYPE\n\n\nclass Metadata(object):\n    """""" Metadata of a dataset containing column name and dtype\n\n    Attributes:\n        column_names: A list with column names\n        column_dtypes: A dict where the key is a column name\n            and the value is a dtype. The dtypes are MaxCompute dtypes for a\n            MaxCompute table or numpy dtypes for a CSV file.\n    """"""\n\n    def __init__(self, column_names, column_dtypes=None):\n        self.column_names = column_names\n        self.column_dtypes = column_dtypes\n\n    @property\n    def column_dtypes(self):\n        return self.__column_dtypes\n\n    @column_dtypes.setter\n    def column_dtypes(self, column_dtypes):\n        self.__column_dtypes = column_dtypes\n\n    def get_tf_dtype_from_maxcompute_column(self, column_name):\n        """"""Get TensorFlow dtype according to the column name in\n        a MaxCompute table\n\n        Args:\n            column_name: The column name in a MaxCompute table.\n\n        Returns:\n            TensorFlow dtype\n        """"""\n        if self.column_dtypes is None:\n            raise ValueError(""The column dtypes has not been configured"")\n\n        maxcompute_dtype = self.column_dtypes.get(column_name, None)\n\n        if maxcompute_dtype not in MAXCOMPUTE_DTYPE_TO_TF_DTYPE:\n            raise ValueError(\n                ""Not support {} and only support {}"".format(\n                    maxcompute_dtype, list(MAXCOMPUTE_DTYPE_TO_TF_DTYPE.keys())\n                )\n            )\n        return MAXCOMPUTE_DTYPE_TO_TF_DTYPE[maxcompute_dtype]\n\n\nclass AbstractDataReader(ABC):\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def read_records(self, task):\n        """"""This method will be used in `TaskDataService` to read the records\n        based on the information provided for a given task into a Python\n        generator/iterator.\n\n        Arguments:\n            task: The current `Task` object that provides information on where\n                to read the data for this task.\n        """"""\n        pass\n\n    @abstractmethod\n    def create_shards(self):\n        """"""This method creates the dictionary of shards where the keys\n        are the shard names and the values are tuples of the starting\n        index and the number of records in each shard.\n        """"""\n        pass\n\n    @property\n    def records_output_types(self):\n        """"""This method returns the output data types used for\n        `tf.data.Dataset.from_generator` when creating the\n        `tf.data.Dataset` object from the generator created\n        by `read_records()`. Note that the returned output types\n        should be a nested structure of `tf.DType` objects corresponding\n        to each component of an element yielded by the created generator.\n        """"""\n        return None\n\n    @property\n    def metadata(self):\n        """"""This method returns the `Metadata` object that contains\n         some metadata collected for the read records, such as the\n         list of column names.""""""\n        return Metadata(column_names=None)\n\n\ndef check_required_kwargs(required_args, kwargs):\n    missing_args = [k for k in required_args if k not in kwargs]\n    if missing_args:\n        raise ValueError(\n            ""The following required arguments are missing: %s""\n            % "", "".join(missing_args)\n        )\n'"
elasticdl/python/data/reader/data_reader_factory.py,0,"b'import os\n\nfrom elasticdl.python.common.constants import MaxComputeConfig, ReaderType\nfrom elasticdl.python.data.odps_io import is_odps_configured\nfrom elasticdl.python.data.reader.csv_reader import CSVDataReader\nfrom elasticdl.python.data.reader.odps_reader import ODPSDataReader\nfrom elasticdl.python.data.reader.recordio_reader import RecordIODataReader\n\n\ndef create_data_reader(data_origin, records_per_task=None, **kwargs):\n    """"""Create a data reader to read records\n    Args:\n        data_origin: The origin of the data, e.g. location to files,\n            table name in the database, etc.\n        records_per_task: The number of records to create a task\n        kwargs: data reader params, the supported keys are\n            ""columns"", ""partition"", ""reader_type""\n    """"""\n    reader_type = kwargs.get(""reader_type"", None)\n    if reader_type is None:\n        if is_odps_configured():\n            return ODPSDataReader(\n                project=os.environ[MaxComputeConfig.PROJECT_NAME],\n                access_id=os.environ[MaxComputeConfig.ACCESS_ID],\n                access_key=os.environ[MaxComputeConfig.ACCESS_KEY],\n                table=data_origin,\n                endpoint=os.environ.get(MaxComputeConfig.ENDPOINT),\n                tunnel_endpoint=os.environ.get(\n                    MaxComputeConfig.TUNNEL_ENDPOINT, None\n                ),\n                records_per_task=records_per_task,\n                **kwargs,\n            )\n        elif data_origin and data_origin.endswith("".csv""):\n            return CSVDataReader(data_dir=data_origin, **kwargs)\n        else:\n            return RecordIODataReader(data_dir=data_origin)\n    elif reader_type == ReaderType.CSV_READER:\n        return CSVDataReader(data_dir=data_origin, **kwargs)\n    elif reader_type == ReaderType.ODPS_READER:\n        if not is_odps_configured:\n            raise ValueError(\n                ""MAXCOMPUTE_AK, MAXCOMPUTE_SK and MAXCOMPUTE_PROJECT "",\n                ""must be configured in envs"",\n            )\n        return ODPSDataReader(\n            project=os.environ[MaxComputeConfig.PROJECT_NAME],\n            access_id=os.environ[MaxComputeConfig.ACCESS_ID],\n            access_key=os.environ[MaxComputeConfig.ACCESS_KEY],\n            table=data_origin,\n            endpoint=os.environ.get(MaxComputeConfig.ENDPOINT),\n            records_per_task=records_per_task,\n            **kwargs,\n        )\n    elif reader_type == ReaderType.RECORDIO_READER:\n        return RecordIODataReader(data_dir=data_origin)\n    else:\n        raise ValueError(\n            ""The reader type {} is not supported"".format(reader_type)\n        )\n'"
elasticdl/python/data/reader/odps_reader.py,7,"b'import tensorflow as tf\nfrom odps import ODPS\n\nfrom elasticdl.python.common.constants import Mode\nfrom elasticdl.python.data.odps_io import ODPSReader\nfrom elasticdl.python.data.reader.data_reader import (\n    AbstractDataReader,\n    Metadata,\n    check_required_kwargs,\n)\n\n\nclass ODPSDataReader(AbstractDataReader):\n    def __init__(self, **kwargs):\n        AbstractDataReader.__init__(self, **kwargs)\n        self._kwargs = kwargs\n        self._metadata = Metadata(column_names=None)\n        self._table = self._kwargs[""table""]\n        self._columns = self._kwargs.get(""columns"")\n        self._init_metadata()\n        # Initialize an ODPS IO reader for each table with task type\n        self._table_readers = dict()\n\n    def _init_metadata(self):\n        table_schema = self._get_table_schema()\n        if self._metadata.column_names is None:\n            self._metadata.column_names = (\n                table_schema.names if self._columns is None else self._columns\n            )\n\n        if self._metadata.column_names:\n            column_dtypes = {\n                column_name: table_schema[column_name].type\n                for column_name in self._metadata.column_names\n            }\n            self.metadata.column_dtypes = column_dtypes\n\n    def read_records(self, task):\n        task_table_name = self._get_odps_table_name(task.shard_name)\n        self._init_reader(task_table_name, task.type)\n\n        reader = self._table_readers[task_table_name][task.type]\n        for record in reader.record_generator_with_retry(\n            start=task.start, end=task.end, columns=self._metadata.column_names\n        ):\n            yield record\n\n    def create_shards(self):\n        check_required_kwargs([""table"", ""records_per_task""], self._kwargs)\n        reader = self.get_odps_reader(self._kwargs[""table""])\n        shard_name_prefix = self._kwargs[""table""] + "":shard_""\n        table_size = reader.get_table_size()\n        records_per_task = self._kwargs[""records_per_task""]\n        shards = {}\n        num_shards = table_size // records_per_task\n        start_ind = 0\n        for shard_id in range(num_shards):\n            shards[shard_name_prefix + str(shard_id)] = (\n                start_ind,\n                records_per_task,\n            )\n            start_ind += records_per_task\n        num_records_left = table_size % records_per_task\n        if num_records_left != 0:\n            shards[shard_name_prefix + str(num_shards)] = (\n                start_ind,\n                num_records_left,\n            )\n        return shards\n\n    @property\n    def records_output_types(self):\n        return tf.string\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n    def _init_reader(self, table_name, task_type):\n        if (\n            table_name in self._table_readers\n            and task_type in self._table_readers[table_name]\n        ):\n            return\n\n        self._table_readers.setdefault(table_name, {})\n\n        check_required_kwargs(\n            [""project"", ""access_id"", ""access_key""], self._kwargs\n        )\n        reader = self.get_odps_reader(table_name)\n\n        # There may be weird errors if tasks with the same table\n        # and different type use the same reader.\n        self._table_readers[table_name][task_type] = reader\n\n    def get_odps_reader(self, table_name):\n        return ODPSReader(\n            project=self._kwargs[""project""],\n            access_id=self._kwargs[""access_id""],\n            access_key=self._kwargs[""access_key""],\n            table=table_name,\n            endpoint=self._kwargs.get(""endpoint""),\n            partition=self._kwargs.get(""partition"", None),\n            num_processes=self._kwargs.get(""num_processes"", 1),\n            options={\n                ""odps.options.tunnel.endpoint"": self._kwargs.get(\n                    ""tunnel_endpoint"", None\n                )\n            },\n        )\n\n    def _get_table_schema(self):\n        odps_client = ODPS(\n            access_id=self._kwargs[""access_id""],\n            secret_access_key=self._kwargs[""access_key""],\n            project=self._kwargs[""project""],\n            endpoint=self._kwargs.get(""endpoint""),\n        )\n        odps_table = odps_client.get_table(self._kwargs[""table""])\n        return odps_table.schema\n\n    @staticmethod\n    def _get_odps_table_name(shard_name):\n        return shard_name.split("":"")[0]\n\n    def default_dataset_fn(self):\n        check_required_kwargs([""label_col""], self._kwargs)\n\n        def dataset_fn(dataset, mode, metadata):\n            def _parse_data(record):\n                label_col_name = self._kwargs[""label_col""]\n                record = tf.strings.to_number(record, tf.float32)\n\n                def _get_features_without_labels(\n                    record, label_col_idx, features_shape\n                ):\n                    features = [\n                        record[:label_col_idx],\n                        record[label_col_idx + 1 :],  # noqa: E203\n                    ]\n                    features = tf.concat(features, -1)\n                    return tf.reshape(features, features_shape)\n\n                features_shape = (len(metadata.column_names) - 1, 1)\n                labels_shape = (1,)\n                if mode == Mode.PREDICTION:\n                    if label_col_name in metadata.column_names:\n                        label_col_idx = metadata.column_names.index(\n                            label_col_name\n                        )\n                        return _get_features_without_labels(\n                            record, label_col_idx, features_shape\n                        )\n                    else:\n                        return tf.reshape(record, features_shape)\n                else:\n                    if label_col_name not in metadata.column_names:\n                        raise ValueError(\n                            ""Missing the label column \'%s\' in the retrieved ""\n                            ""ODPS table during %s mode.""\n                            % (label_col_name, mode)\n                        )\n                    label_col_idx = metadata.column_names.index(label_col_name)\n                    labels = tf.reshape(record[label_col_idx], labels_shape)\n                    return (\n                        _get_features_without_labels(\n                            record, label_col_idx, features_shape\n                        ),\n                        labels,\n                    )\n\n            dataset = dataset.map(_parse_data)\n\n            if mode == Mode.TRAINING:\n                dataset = dataset.shuffle(buffer_size=200)\n            return dataset\n\n        return dataset_fn\n\n\nclass ParallelODPSDataReader(ODPSDataReader):\n    """"""Use multi-process to download records from a MaxCompute table\n    """"""\n\n    def __init__(self, parse_fn, **kwargs):\n        ODPSDataReader.__init__(self, **kwargs)\n        self.py_parse_data = parse_fn\n\n    def parallel_record_records(\n        self, task, num_processes, shard_size, transform_fn\n    ):\n        check_required_kwargs(\n            [""project"", ""access_id"", ""access_key""], self._kwargs\n        )\n        start = task.start\n        end = task.end\n        table = self._get_odps_table_name(task.shard_name)\n        table = table.split(""."")[1]\n        project = self._kwargs[""project""]\n        access_id = self._kwargs[""access_id""]\n        access_key = self._kwargs[""access_key""]\n        endpoint = self._kwargs.get(""endpoint"")\n        partition = self._kwargs.get(""partition"", None)\n        columns = self._kwargs.get(""columns"", None)\n        pd = ODPSReader(\n            access_id=access_id,\n            access_key=access_key,\n            project=project,\n            endpoint=endpoint,\n            table=table,\n            partition=partition,\n            num_processes=num_processes,\n            transform_fn=transform_fn,\n            columns=columns,\n        )\n        pd.reset((start, end - start), shard_size)\n        shard_count = pd.get_shards_count()\n        for i in range(shard_count):\n            records = pd.get_records()\n            for record in records:\n                yield record\n        pd.stop()\n\n    def read_records(self, task):\n        shard_size = (task.end - task.start) // 4\n        record_gen = self.parallel_record_records(\n            task=task,\n            num_processes=4,\n            shard_size=shard_size,\n            transform_fn=self.py_parse_data,\n        )\n        for record in record_gen:\n            yield record\n\n    @property\n    def records_output_types(self):\n        return tf.string\n'"
elasticdl/python/data/reader/recordio_reader.py,1,"b'import os\nfrom contextlib import closing\n\nimport recordio\nimport tensorflow as tf\n\nfrom elasticdl.python.data.reader.data_reader import (\n    AbstractDataReader,\n    Metadata,\n    check_required_kwargs,\n)\n\n\nclass RecordIODataReader(AbstractDataReader):\n    def __init__(self, **kwargs):\n        AbstractDataReader.__init__(self, **kwargs)\n        self._kwargs = kwargs\n        check_required_kwargs([""data_dir""], self._kwargs)\n\n    def read_records(self, task):\n        with closing(\n            recordio.Scanner(\n                task.shard_name, task.start, task.end - task.start\n            )\n        ) as reader:\n            while True:\n                record = reader.record()\n                if record:\n                    yield record\n                else:\n                    break\n\n    def create_shards(self):\n        data_dir = self._kwargs[""data_dir""]\n        start_ind = 0\n        f_records = {}\n        for f in os.listdir(data_dir):\n            p = os.path.join(data_dir, f)\n            with closing(recordio.Index(p)) as rio:\n                f_records[p] = (start_ind, rio.num_records())\n        return f_records\n\n    @property\n    def records_output_types(self):\n        return tf.string\n\n    @property\n    def metadata(self):\n        return Metadata(column_names=None)\n'"
elasticdl/python/data/recordio_gen/__init__.py,0,b''
elasticdl/python/data/recordio_gen/census_recordio_gen.py,9,"b'import argparse\nimport os\nimport pathlib\nimport sys\nimport urllib\n\nimport pandas as pd\nimport recordio\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nDATA_URL = (\n    ""https://archive.ics.uci.edu/ml/machine-learning-databases/adult/""\n    ""adult.data""\n)\n\n__COLUMN_NAMES = [\n    ""age"",\n    ""workclass"",\n    ""fnlwgt"",\n    ""education"",\n    ""education-num"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n    ""native-country"",\n    ""label"",\n]\nCATEGORICAL_FEATURE_KEYS = [\n    ""workclass"",\n    ""education"",\n    ""marital-status"",\n    ""occupation"",\n    ""relationship"",\n    ""race"",\n    ""sex"",\n    ""native-country"",\n]\nNUMERIC_FEATURE_KEYS = [\n    ""age"",\n    ""capital-gain"",\n    ""capital-loss"",\n    ""hours-per-week"",\n]\nOPTIONAL_NUMERIC_FEATURE_KEYS = [\n    ""education-num"",\n]\nLABEL_KEY = ""label""\n\n\ndef convert_series_to_tf_feature(data_series, columns, dtype_series):\n    """"""\n    Convert pandas series to TensorFlow features.\n    Args:\n        data_series: Pandas series of data content.\n        columns: Column name array.\n        dtype_series: Pandas series of dtypes.\n    Return:\n        A dict of feature name -> tf.train.Feature\n    """"""\n    if data_series.hasnans:\n        return\n\n    features = {}\n    for numeric_feature_key in NUMERIC_FEATURE_KEYS:\n        feature = tf.train.Feature(\n            float_list=tf.train.FloatList(\n                value=[data_series[numeric_feature_key]]\n            )\n        )\n        features[numeric_feature_key] = feature\n\n    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:\n        feature = tf.train.Feature(\n            bytes_list=tf.train.BytesList(\n                value=[data_series[categorical_feature_key].encode(""utf-8"")]\n            )\n        )\n        features[categorical_feature_key] = feature\n\n    feature = tf.train.Feature(\n        int64_list=tf.train.Int64List(value=[data_series[LABEL_KEY]])\n    )\n    features[LABEL_KEY] = feature\n\n    return features\n\n\ndef convert_to_recordio_files(data_frame, dir_name, records_per_shard):\n    """"""\n    Convert a pandas DataFrame to recordio files.\n    Args:\n        data_frame: A pandas DataFrame to convert_to_recordio_files.\n        dir_name: A directory to put the generated recordio files.\n        records_per_shard: The record number per shard.\n    """"""\n    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True)\n\n    row_num = 0\n    writer = None\n    for index, row in data_frame.iterrows():\n        if row_num % records_per_shard == 0:\n            if writer:\n                writer.close()\n\n            shard = row_num // records_per_shard\n            file_path_name = os.path.join(dir_name, ""data-%05d"" % shard)\n            writer = recordio.Writer(file_path_name)\n\n        feature = convert_series_to_tf_feature(\n            row, data_frame.columns, data_frame.dtypes\n        )\n        result_string = tf.train.Example(\n            features=tf.train.Features(feature=feature)\n        ).SerializeToString()\n        writer.write(result_string)\n\n        row_num += 1\n\n    if writer:\n        writer.close()\n\n    print(""Finish data conversion in {}"".format(dir_name))\n\n\ndef load_raw_data(source_data_url, data_dir, skiprows=None):\n    file_name = os.path.basename(source_data_url)\n    file_path = os.path.join(data_dir, file_name)\n    pathlib.Path(data_dir).mkdir(parents=True, exist_ok=True)\n    if not os.path.exists(file_path):\n        urllib.request.urlretrieve(source_data_url, file_path)\n    census = pd.read_csv(\n        file_path, skiprows=skiprows, header=None, skipinitialspace=True\n    )\n    census.columns = __COLUMN_NAMES\n\n    census[LABEL_KEY] = census[LABEL_KEY].apply(\n        lambda label: 0 if label == ""<=50K"" else 1\n    )\n    for optional_key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n        census.pop(optional_key)\n\n    return census\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--data_dir"",\n        help=""The cache directory to put the data downloaded from the web"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--records_per_shard"",\n        type=int,\n        default=1024 * 8,\n        help=""Record number per shard"",\n    )\n    parser.add_argument(\n        ""--output_dir"",\n        help=""The directory for the generated recordio files"",\n        required=True,\n    )\n\n    args = parser.parse_args(sys.argv[1:])\n\n    data_frame = load_raw_data(DATA_URL, args.data_dir)\n    train_data_frame, test_data_frame = train_test_split(\n        data_frame, test_size=0.25, shuffle=False\n    )\n\n    convert_to_recordio_files(\n        train_data_frame,\n        os.path.join(args.output_dir, ""train""),\n        args.records_per_shard,\n    )\n    convert_to_recordio_files(\n        test_data_frame,\n        os.path.join(args.output_dir, ""test""),\n        args.records_per_shard,\n    )\n'"
elasticdl/python/data/recordio_gen/frappe_recordio_gen.py,6,"b'#!/usr/bin/env python\n\nimport argparse\nimport logging\nimport os\nimport sys\n\nimport numpy as np\nimport recordio\nimport requests\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nurls_template = (\n    ""https://raw.githubusercontent.com/hexiangnan/neural_factorization""\n    ""_machine/master/data/frappe/frappe.%s.libfm""\n)\n\nurls = {\n    ""train"": urls_template % ""train"",\n    ""validation"": urls_template % ""validation"",\n    ""test"": urls_template % ""test"",\n}\n\n\nclass LoadFrappe(object):\n    def __init__(self, path):\n        self.trainfile = os.path.join(path, ""train.libfm"")\n        self.testfile = os.path.join(path, ""test.libfm"")\n        self.validationfile = os.path.join(path, ""validation.libfm"")\n        self.feature_num = self.gen_feature_map()\n        print(""feature_num:%d"" % self.feature_num)\n\n        self.train = self.read_data(self.trainfile)\n        maxlen_train = max([len(i) for i in self.train[0]])\n\n        self.validation = self.read_data(self.validationfile)\n        maxlen_val = max([len(i) for i in self.validation[0]])\n\n        self.test = self.read_data(self.testfile)\n        maxlen_test = max([len(i) for i in self.test[0]])\n\n        self.maxlen = max(maxlen_train, maxlen_val, maxlen_test)\n        print(""maxlen:%d"" % self.maxlen)\n\n        self.train = self.to_numpy(self.train, self.maxlen)\n        self.validation = self.to_numpy(self.validation, self.maxlen)\n        self.test = self.to_numpy(self.test, self.maxlen)\n\n    def gen_feature_map(self):\n        self.features = {}\n        self._read_features(self.trainfile)\n        self._read_features(self.testfile)\n        self._read_features(self.validationfile)\n        return len(self.features) + 1\n\n    def _read_features(self, filepath):\n        with open(filepath, ""r"") as fp:\n            for line in fp:\n                for item in line.strip().split("" "")[1:]:\n                    # 0 for pad_sequences\n                    self.features.setdefault(item, len(self.features) + 1)\n\n    def read_data(self, datafile):\n        x, y = [], []\n        with open(datafile, ""r"") as fp:\n            for line in fp:\n                arr = line.strip().split("" "")\n                if float(arr[0]) > 0:\n                    y.append(1)\n                else:\n                    y.append(0)\n                x.append([self.features[item] for item in arr[1:]])\n        return x, y\n\n    def to_numpy(self, data, maxlen):\n        x, y = data\n        maxlen = max([len(i) for i in x])\n        x = pad_sequences(x, maxlen=maxlen)\n        return (np.array(x, dtype=np.int64), np.array(y, dtype=np.int64))\n\n\ndef convert(x, y, args, subdir):\n    """"""Convert pairs of feature and label in NumPy arrays into a set of\n    RecordIO files.\n    """"""\n    row = 0\n    shard = 0\n    w = None\n    while row < x.shape[0] * args.fraction:\n        if row % args.records_per_shard == 0:\n            if w:\n                w.close()\n            dn = os.path.join(args.output_dir, subdir)\n            fn = os.path.join(dn, ""data-%05d"" % (shard))\n            if not os.path.exists(dn):\n                os.makedirs(os.path.dirname(fn))\n            logging.info(""Writing {} ..."".format(fn))\n            w = recordio.Writer(fn)\n            shard = shard + 1\n\n        w.write(\n            tf.train.Example(\n                features=tf.train.Features(\n                    feature={\n                        ""feature"": tf.train.Feature(\n                            int64_list=tf.train.Int64List(\n                                value=x[row].flatten()\n                            )\n                        ),\n                        ""label"": tf.train.Feature(\n                            int64_list=tf.train.Int64List(\n                                value=y[row].flatten()\n                            )\n                        ),\n                    }\n                )\n            ).SerializeToString()\n        )\n        row = row + 1\n    w.close()\n    print(""done"")\n    logging.info(\n        ""Wrote {} of total {} records into {} files"".format(\n            row, x.shape[0], shard\n        )\n    )\n\n\ndef download(url, dest_path):\n\n    req = requests.get(url, stream=True)\n    req.raise_for_status()\n\n    with open(dest_path, ""wb"") as fd:\n        for chunk in req.iter_content(chunk_size=2 ** 20):\n            fd.write(chunk)\n\n\ndef load_raw_data(args):\n    for phase in [""train"", ""validation"", ""test""]:\n        filepath = os.path.join(args.data, phase + "".libfm"")\n        if not os.path.exists(filepath):\n            download(urls[phase], filepath)\n    data = LoadFrappe(args.data)\n    return (\n        data.train[0],\n        data.train[1],\n        data.validation[0],\n        data.validation[1],\n        data.test[0],\n        data.test[1],\n    )\n\n\ndef main(args):\n    x_train, y_train, x_val, y_val, x_test, y_test = load_raw_data(args)\n\n    convert(x_train, y_train, args, ""train"")\n    convert(x_val, y_val, args, ""val"")\n    convert(x_test, y_test, args, ""test"")\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=(\n            ""Convert TensorFlow feature datasets into RecordIO format.""\n        )\n    )\n    parser.add_argument(""--data"", help=""Data file path"")\n    parser.add_argument(\n        ""--records_per_shard"",\n        default=16 * 1024,\n        type=int,\n        help=""Maximum number of records per shard file."",\n    )\n    parser.add_argument(""--output_dir"", help=""Output directory"")\n    parser.add_argument(\n        ""--fraction"",\n        default=1.0,\n        type=float,\n        help=""The fraction of the dataset to be converted"",\n    )\n    args = parser.parse_args(sys.argv[1:])\n    main(args)\n'"
elasticdl/python/data/recordio_gen/heart_recordio_gen.py,11,"b'import argparse\nimport os\nimport pathlib\nimport sys\nimport urllib\n\nimport pandas as pd\nimport recordio\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nURL = ""https://storage.googleapis.com/applied-dl/heart.csv""\n\n\ndef convert_series_to_tf_feature(data_series, columns, dtype_series):\n    """"""\n    Convert pandas series to TensorFlow features.\n    Args:\n        data_series: Pandas series of data content.\n        columns: Column name array.\n        dtype_series: Pandas series of dtypes.\n    Return:\n        A dict of feature name -> tf.train.Feature\n    """"""\n    features = {}\n    for column_name in columns:\n        feature = None\n        value = data_series[column_name]\n        dtype = dtype_series[column_name]\n\n        if dtype == ""int64"":\n            feature = tf.train.Feature(\n                int64_list=tf.train.Int64List(value=[value])\n            )\n        elif dtype == ""float64"":\n            feature = tf.train.Feature(\n                float_list=tf.train.FloatList(value=[value])\n            )\n        elif dtype == ""str"":\n            feature = tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[value.encode(""utf-8"")])\n            )\n        elif dtype == ""object"":\n            feature = tf.train.Feature(\n                bytes_list=tf.train.BytesList(\n                    value=[str(value).encode(""utf-8"")]\n                )\n            )\n        else:\n            assert False, ""Unrecoginize dtype: {}"".format(dtype)\n\n        features[column_name] = feature\n\n    return features\n\n\ndef convert_to_recordio_files(data_frame, dir_name, records_per_shard):\n    """"""\n    Convert a pandas DataFrame to recordio files.\n    Args:\n        data_frame: A pandas DataFrame to convert_to_recordio_files.\n        dir_name: A directory to put the generated recordio files.\n        records_per_shard: The record number per shard.\n    """"""\n    pathlib.Path(dir_name).mkdir(parents=True, exist_ok=True)\n\n    row_num = 0\n    writer = None\n    for index, row in data_frame.iterrows():\n        if row_num % records_per_shard == 0:\n            if writer:\n                writer.close()\n\n            shard = row_num // records_per_shard\n            file_path_name = os.path.join(dir_name, ""data-%05d"" % shard)\n            writer = recordio.Writer(file_path_name)\n\n        feature = convert_series_to_tf_feature(\n            row, data_frame.columns, data_frame.dtypes\n        )\n        result_string = tf.train.Example(\n            features=tf.train.Features(feature=feature)\n        ).SerializeToString()\n        writer.write(result_string)\n\n        row_num += 1\n\n    if writer:\n        writer.close()\n\n    print(""Finish data conversion in {}"".format(dir_name))\n\n\ndef load_raw_data(data_dir):\n    file_name = os.path.basename(URL)\n    file_path = os.path.join(data_dir, file_name)\n    pathlib.Path(data_dir).mkdir(parents=True, exist_ok=True)\n    if not os.path.exists(file_path):\n        urllib.request.urlretrieve(URL, file_path)\n    return pd.read_csv(file_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--data_dir"",\n        help=""The cache directory to put the data downloaded from the web"",\n    )\n    parser.add_argument(\n        ""--records_per_shard"",\n        type=int,\n        default=128,\n        help=""Record number per shard"",\n    )\n    parser.add_argument(\n        ""--output_dir"", help=""The directory for the generated recordio files""\n    )\n\n    args = parser.parse_args(sys.argv[1:])\n\n    data_frame = load_raw_data(args.data_dir)\n\n    train, test = train_test_split(data_frame, test_size=0.2)\n    train, val = train_test_split(train, test_size=0.2)\n\n    convert_to_recordio_files(\n        train, os.path.join(args.output_dir, ""train""), args.records_per_shard\n    )\n    convert_to_recordio_files(\n        val, os.path.join(args.output_dir, ""val""), args.records_per_shard\n    )\n    convert_to_recordio_files(\n        test, os.path.join(args.output_dir, ""test""), args.records_per_shard\n    )\n'"
elasticdl/python/data/recordio_gen/image_label.py,6,"b'#!/usr/bin/env python\n\nimport argparse\nimport logging\nimport os\nimport sys\n\nimport recordio\nimport tensorflow as tf\n\n\ndef convert(x, y, args, subdir):\n    """"""Convert pairs of image and label in NumPy arrays into a set of\n    RecordIO files.\n    """"""\n    logger = logging.getLogger(""image_label::convert"")\n    logger.setLevel(""INFO"")\n    row = 0\n    shard = 0\n    w = None\n    while row < x.shape[0] * args.fraction:\n        if row % args.records_per_shard == 0:\n            if w:\n                w.close()\n            dn = os.path.join(args.dir, args.dataset, subdir)\n            fn = os.path.join(dn, ""data-%05d"" % (shard))\n            if not os.path.exists(dn):\n                os.makedirs(os.path.dirname(fn))\n            logger.info(""Writing {} ..."".format(fn))\n            w = recordio.Writer(fn)\n            shard = shard + 1\n\n        w.write(\n            tf.train.Example(\n                features=tf.train.Features(\n                    feature={\n                        ""image"": tf.train.Feature(\n                            float_list=tf.train.FloatList(\n                                value=x[row].flatten()\n                            )\n                        ),\n                        ""label"": tf.train.Feature(\n                            int64_list=tf.train.Int64List(\n                                value=y[row].flatten()\n                            )\n                        ),\n                    }\n                )\n            ).SerializeToString()\n        )\n        row = row + 1\n    w.close()\n    logger.info(\n        ""Wrote {} of total {} records into {} files"".format(\n            row, x.shape[0], shard\n        )\n    )\n\n\ndef main(args):\n    if args.dataset == ""mnist"":\n        from tensorflow.python.keras.datasets import mnist\n\n        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    elif args.dataset == ""fashion_mnist"":\n        from tensorflow.python.keras.datasets import fashion_mnist\n\n        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    elif args.dataset == ""cifar10"":\n        from tensorflow.python.keras.datasets import cifar10\n\n        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    else:\n        sys.exit(""Unknown dataset {}"".format(args.dataset))\n\n    convert(x_train, y_train, args, ""train"")\n    convert(x_test, y_test, args, ""test"")\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        description=(""Convert TensorFlow image datasets into RecordIO format."")\n    )\n    parser.add_argument(""dir"", help=""Output directory"")\n    parser.add_argument(\n        ""--records_per_shard"",\n        default=16 * 1024,\n        type=int,\n        help=""Maximum number of records per shard file."",\n    )\n    parser.add_argument(\n        ""--dataset"",\n        choices=[""mnist"", ""fashion_mnist"", ""cifar10""],\n        default=""mnist"",\n        help=""Dataset name: mnist or fashion_mnist or cifar10"",\n    )\n    parser.add_argument(\n        ""--fraction"",\n        default=1.0,\n        type=float,\n        help=""The fraction of the dataset to be converted"",\n    )\n    args = parser.parse_args(sys.argv[1:])\n    main(args)\n'"
elasticdl/python/elasticdl/feature_column/__init__.py,0,b''
elasticdl/python/elasticdl/feature_column/feature_column.py,2,"b'import collections\nimport math\n\nfrom tensorflow.python.feature_column import feature_column as fc_old\nfrom tensorflow.python.feature_column import feature_column_v2 as fc_lib\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import init_ops\n\nfrom elasticdl.python.elasticdl.embedding_delegate import EmbeddingDelegate\n\n\ndef embedding_column(\n    categorical_column,\n    dimension,\n    combiner=""mean"",\n    initializer=None,\n    max_norm=None,\n    trainable=True,\n):\n    """"""\n    Create a customized EmbeddingColumn for ElasticDL.\n    The native EmbeddingColumn will create a variable to\n    store the entire embedding table. It can\'t leverage the\n    benefit from the ElasticDL parameter server to partition\n    the embedding table. Create this ElasticDL EmbeddingColumn\n    to interact with ElasticDL parameter server.\n    The API signature is based on the native\n    tf.feature_column.embedding_column and\n    remove some unused parameters.\n\n    Args:\n      categorical_column: A `CategoricalColumn` created by a\n        `categorical_column_with_*` function. This column produces\n        the sparse IDs that are inputs to the embedding lookup.\n      dimension: An integer specifying dimension of the embedding, must be > 0.\n      combiner: A string specifying how to reduce if there are multiple entries\n        in a single row. Currently \'mean\', \'sqrtn\' and \'sum\' are supported,\n        with \'mean\' the default. \'sqrtn\' often achieves good accuracy, in\n        particular with bag-of-words columns. Each of this can be thought as\n        example level normalizations on the column. For more information, see\n        `tf.embedding_lookup_sparse`.\n      initializer: A variable initializer function to be used in embedding\n        variable initialization. If not specified, defaults to\n        `truncated_normal_initializer` with mean `0.0` and\n        standard deviation `1/sqrt(dimension)`.\n      max_norm: If not `None`, embedding values are l2-normalized\n        to this value.\n      trainable: Whether or not the embedding is trainable. Default is True.\n\n    Returns:\n        `DenseColumn` that converts from sparse input.\n\n    Raises:\n        ValueError: if `dimension` not > 0.\n        ValueError: if `initializer` is specified and is not callable.\n    """"""\n    if (dimension is None) or (dimension < 1):\n        raise ValueError(""Invalid dimension {}."".format(dimension))\n\n    if (initializer is not None) and (not callable(initializer)):\n        raise ValueError(\n            ""initializer must be callable if specified. ""\n            ""Embedding of column_name: {}"".format(categorical_column.name)\n        )\n    if initializer is None:\n        initializer = init_ops.truncated_normal_initializer(\n            mean=0.0, stddev=1 / math.sqrt(dimension)\n        )\n\n    return EmbeddingColumn(\n        categorical_column=categorical_column,\n        dimension=dimension,\n        combiner=combiner,\n        initializer=initializer,\n        max_norm=max_norm,\n        trainable=trainable,\n    )\n\n\nclass EmbeddingColumn(\n    fc_lib.DenseColumn,\n    fc_lib.SequenceDenseColumn,\n    fc_old._DenseColumn,\n    fc_old._SequenceDenseColumn,\n    collections.namedtuple(\n        ""EmbeddingColumn"",\n        (\n            ""categorical_column"",\n            ""dimension"",\n            ""combiner"",\n            ""initializer"",\n            ""max_norm"",\n            ""trainable"",\n        ),\n    ),\n):\n    def __init__(self, **kwargs):\n        # Get the input dimension of the embedding variable\n        default_num_buckets = (\n            self.categorical_column.num_buckets\n            if self._is_v2_column\n            else self.categorical_column._num_buckets\n        )  # pylint: disable=protected-access\n        num_buckets = getattr(\n            self.categorical_column, ""num_buckets"", default_num_buckets\n        )\n\n        self._embedding_delegate = EmbeddingDelegate(\n            input_dim=num_buckets, output_dim=self.dimension,\n        )\n\n    @property\n    def _is_v2_column(self):\n        return (\n            isinstance(self.categorical_column, fc_lib.FeatureColumn)\n            and self.categorical_column._is_v2_column\n        )\n\n    @property\n    def name(self):\n        """"""See `FeatureColumn` base class.""""""\n        return ""{}_embedding"".format(self.categorical_column.name)\n\n    @property\n    def parse_example_spec(self):\n        """"""See `FeatureColumn` base class.""""""\n        return self.categorical_column.parse_example_spec\n\n    @property\n    def variable_shape(self):\n        """"""See `DenseColumn` base class.""""""\n        return tensor_shape.TensorShape([self.dimension])\n\n    def create_state(self, state_manager):\n        dense_features_layer_name = state_manager._layer.name\n        self.set_dense_features_layer_name(dense_features_layer_name)\n\n    def get_dense_tensor(self, transformation_cache, state_manager):\n        if isinstance(\n            self.categorical_column, fc_lib.SequenceCategoricalColumn\n        ):\n            raise ValueError(\n                ""In embedding_column: {}. ""\n                ""categorical_column must not be of ""\n                ""type SequenceCategoricalColumn. ""\n                ""Suggested fix A: If you wish to use DenseFeatures, use a ""\n                ""non-sequence categorical_column_with_*. ""\n                ""Suggested fix B: If you wish to create sequence input, use ""\n                ""SequenceFeatures instead of DenseFeatures. ""\n                ""Given (type {}): {}"".format(\n                    self.name,\n                    type(self.categorical_column),\n                    self.categorical_column,\n                )\n            )\n\n        # Get sparse IDs and weights.\n        sparse_tensors = self.categorical_column.get_sparse_tensors(\n            transformation_cache, state_manager\n        )\n\n        # Look up the embedding from the sparse input\n        sparse_ids = sparse_tensors.id_tensor\n        sparse_weights = sparse_tensors.weight_tensor\n        result = self._embedding_delegate.safe_embedding_lookup_sparse(\n            sparse_ids, sparse_weights=sparse_weights, combiner=self.combiner\n        )\n        return result\n\n    def lookup_embedding(self, unique_ids):\n        return self._embedding_delegate.lookup_embedding(unique_ids)\n\n    def set_tape(self, tape):\n        self._embedding_delegate.set_tape(tape)\n\n    def set_lookup_embedding_func(self, func):\n        """"""Sets function for looking up embeddings in the PS.\n\n        Args:\n            func: The function used to look up embeddings. The arguments of\n                are `(column_name, embedding_id_list)`, where `column_name` is\n                the name of embedding column, and `embedding_id_list` is a list\n                of embedding ids to be looked up.\n        """"""\n        self._embedding_delegate.set_lookup_embedding_func(func)\n\n    def set_dense_features_layer_name(self, dense_features_layer_name):\n        # Get the name of the embedding variable\n        EMBEDDING_VARIABLE_NAME_FORMAT = (\n            ""{dense_features_layer_name}/{column_name}/embedding_weights:0""\n        )\n        embedding_variable_name = EMBEDDING_VARIABLE_NAME_FORMAT.format(\n            dense_features_layer_name=dense_features_layer_name,\n            column_name=self.name,\n        )\n\n        self._embedding_delegate.set_name(embedding_variable_name)\n\n    def reset(self):\n        self._embedding_delegate.reset()\n\n    @property\n    def embedding_and_ids(self):\n        return self._embedding_delegate.embedding_and_ids\n\n    @property\n    def embedding_weight_name(self):\n        return self._embedding_delegate.name\n'"
elasticdl/python/elasticdl/layers/__init__.py,0,b''
elasticdl/python/elasticdl/layers/embedding.py,7,"b'import tensorflow as tf\nfrom tensorflow.python.keras.utils import tf_utils\n\nfrom elasticdl.python.elasticdl.embedding_delegate import EmbeddingDelegate\n\n\nclass Embedding(tf.keras.layers.Layer):\n    """"""\n    Input: indexes for the embedding entries with a shape of\n      (batch_size, input_length). Input can be either dense tensor\n      or SparseTensor.\n    Output:\n      corresponding (combined) embeddings with a shape of\n      (batch_size, input_length, output_dim) if combiner is None\n      (batch_size, output_dim) if combiner is not None\n    Arguments:\n      output_dim: the dimension of the embedding vector\n      input_dim: the max input id. If 0 or None, will not check the range of\n        input embedding ids.\n      embeddings_initializer: Initializer for embedding table\n      mask_zero: Whether or not the input value 0 is a special ""padding""\n        value that should be masked out.\n        If input is SparseTensor, mask_zero must be False.\n      input_length: Length of input sequences, when it is constant.\n        This argument is required if you are going to connect\n        `Flatten` then `Dense` layers upstream\n        (without it, the shape of the dense outputs cannot be computed).\n      combiner: A string specifying the reduction op or None if not used.\n        ""mean"", ""sqrtn"" and ""sum"" are supported for the reduction op.\n        If input is SparseTensor, combiner must set as a reduction op.\n    """"""\n\n    def __init__(\n        self,\n        output_dim,\n        input_dim=None,\n        embeddings_initializer=""uniform"",\n        mask_zero=False,\n        input_length=None,\n        combiner=None,\n        **kwargs\n    ):\n        if ""input_shape"" not in kwargs and input_length:\n            kwargs[""input_shape""] = (input_length,)\n        super(Embedding, self).__init__(**kwargs)\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embeddings_initializer = embeddings_initializer\n        self.mask_zero = mask_zero\n        self.supports_masking = mask_zero\n        self.input_length = input_length\n        self.combiner = combiner\n        self._embedding_and_ids_eagerly = []\n\n        # BET\'s shape and ids\' shape in `self._embedding_and_ids_graph` have\n        # `None` dimension. This is because they have different shapes in\n        # different iterations.\n        # `tf.Variable` requires initial value if shape has `None` dimension.\n        self._embedding_and_ids_graph = []\n        self.embedding_weight_name = self.name + ""/embeddings:0""\n        self.embedding_delegate = EmbeddingDelegate(\n            self.input_dim, self.output_dim, self.embedding_weight_name\n        )\n\n    @tf_utils.shape_type_conversion\n    def compute_output_shape(self, input_shape):\n        # this function is taken from\n        # tf.keras.layers.Embedding.compute_output_shape\n        # https://github.com/tensorflow/tensorflow/blob/3f3c728bf80e0fd6653744318cbbfe1454c6ddca/tensorflow/python/keras/layers/embeddings.py#L156\n        if self.input_length is None:\n            return input_shape + (self.output_dim,)\n        else:\n            if isinstance(self.input_length, (list, tuple)):\n                in_lens = list(self.input_length)\n            else:\n                in_lens = [self.input_length]\n            if len(in_lens) != len(input_shape) - 1:\n                raise ValueError(\n                    \'""input_length"" is %s, \'\n                    ""but received input has shape %s""\n                    % (str(self.input_length), str(input_shape))\n                )\n            else:\n                for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n                    if s1 is not None and s2 is not None and s1 != s2:\n                        raise ValueError(\n                            \'""input_length"" is %s, \'\n                            ""but received input has shape %s""\n                            % (str(self.input_length), str(input_shape))\n                        )\n                    elif s1 is None:\n                        in_lens[i] = s2\n            return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n\n    @property\n    def name(self):\n        return self._name\n\n    @staticmethod\n    def get_key(name_list):\n        return ""-"".join(map(str, name_list))\n\n    def call(self, input):\n        input = tf.cast(input, tf.int64)\n        if isinstance(input, tf.SparseTensor):\n            return self._sparse_input_call(input)\n        else:\n            return self.embedding_delegate.embedding_lookup(input)\n\n    def _sparse_input_call(self, sparse_input):\n        if self.combiner not in [""sum"", ""mean"", ""sqrtn""]:\n            raise ValueError(\n                ""combiner must set sum, mean or sqrtn for sparse input""\n            )\n        batch_embedding = self.embedding_delegate.safe_embedding_lookup_sparse(\n            sparse_input, combiner=self.combiner\n        )\n        return batch_embedding\n\n    def compute_mask(self, inputs, mask=None):\n        if isinstance(input, tf.SparseTensor):\n            raise ValueError(""SparseTensor inputs do not support mask_zero"")\n        if not self.supports_masking:\n            return None\n        return tf.math.not_equal(inputs, 0)\n\n    def reset(self):\n        self.embedding_delegate.reset()\n\n    def set_tape(self, tape):\n        self.embedding_delegate.set_tape(tape)\n\n    def set_lookup_embedding_func(self, func):\n        """"""Sets function for looking up embeddings in the PS.\n        Args:\n            func: The function used to look up embeddings. The arguments of\n                are `(layer_name, embedding_id_list)`, where `layer_name` is\n                the name of embedding layer, and `embedding_id_list` is a list\n                of embedding ids to be looked up.\n        """"""\n        self.embedding_delegate.set_lookup_embedding_func(func)\n\n    @property\n    def embedding_and_ids(self):\n        return self.embedding_delegate.embedding_and_ids\n\n    def set_embedding_weight_name(self, name):\n        self.embedding_weight_name = name\n'"
elasticdl/python/data/recordio_gen/sample_pyspark_recordio_gen/__init__.py,0,b''
elasticdl/python/data/recordio_gen/sample_pyspark_recordio_gen/spark_gen_recordio.py,0,"b'import argparse\nimport glob\nimport os\nimport tarfile\nfrom contextlib import closing\n\nimport recordio\nfrom pyspark import SparkContext, TaskContext\n\nfrom elasticdl.python.common.log_utils import default_logger as logger\nfrom elasticdl.python.common.model_utils import load_module\n\n\ndef write_to_recordio(filename, data_list):\n    logger.info(""Writing to file:"", filename)\n    with closing(recordio.Writer(filename)) as f:\n        for d in data_list:\n            f.write(d)\n\n\ndef process_data(\n    single_file_preparation_func,\n    training_data_tar_file,\n    output_dir,\n    records_per_file,\n):\n    def _process_data(filename_list):\n        filename_set = set()\n        for filename in filename_list:\n            filename_set.add(filename)\n\n        tar = tarfile.open(training_data_tar_file)\n        tar_info_list = tar.getmembers()\n        filename_to_object = {}\n        for tar_info in tar_info_list:\n            if tar_info.name in filename_set:\n                f = tar.extractfile(tar_info)\n                assert f is not None\n                filename_to_object[tar_info.name] = f\n\n        partition = TaskContext().partitionId()\n        counter = 0\n        data_list = []\n        for filename in glob.glob(output_dir + ""/data-%s*"" % partition):\n            os.remove(filename)\n        for filename in filename_set:\n            data = single_file_preparation_func(\n                filename_to_object[filename], filename\n            )\n            data_list.append(data)\n            if len(data_list) == records_per_file:\n                filename = output_dir + ""/data-%s-%04d"" % (partition, counter)\n                counter += 1\n\n                write_to_recordio(filename, data_list)\n                data_list.clear()\n\n        if data_list:\n            filename = output_dir + ""/data-%s-%04d"" % (partition, counter)\n            write_to_recordio(filename, data_list)\n        return filename_list\n\n    return _process_data\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=""Spark job to convert training data to RecordIO format""\n    )\n    parser.add_argument(\n        ""--training_data_tar_file"",\n        help=""Tar file that contains all training data"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--output_dir"", help=""Directory of output RecordIO data"", required=True\n    )\n    parser.add_argument(\n        ""--model_file"",\n        required=True,\n        help=""User-defined model file which data processing logic is in"",\n    )\n    parser.add_argument(\n        ""--records_per_file"", default=1024, type=int, help=""Record per file""\n    )\n    parser.add_argument(\n        ""--num_workers"",\n        default=2,\n        type=int,\n        help=""Number of workers of Spark job"",\n    )\n\n    args = parser.parse_args()\n\n    # Get training data file names from training_data_tar_file\n    tar = tarfile.open(args.training_data_tar_file)\n    tar_info_list = tar.getmembers()\n    filename_list = []\n    for tar_info in tar_info_list:\n        f = tar.extractfile(tar_info)\n        if f is not None and not tar_info.name.split(""/"")[-1].startswith("".""):\n            filename_list.append(tar_info.name)\n\n    # Load user-defined model\n    model_module = load_module(args.model_file)\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    # Start the Spark job\n    sc = SparkContext()\n    rdd = sc.parallelize(filename_list, args.num_workers)\n    rdd.mapPartitions(\n        process_data(\n            model_module.prepare_data_for_a_single_file,\n            args.training_data_tar_file,\n            args.output_dir,\n            args.records_per_file,\n        )\n    ).collect()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
