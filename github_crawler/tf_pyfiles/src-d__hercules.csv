file_path,api_count,code
fix_yaml_unicode.py,0,"b'import sys\n\nimport yaml\n\n\nyaml_invalid = yaml.reader.Reader.NON_PRINTABLE\n\nfor line in sys.stdin:\n    sys.stdout.write(yaml_invalid.sub("""", line))\n'"
internal/__init__.py,0,b''
python/setup.py,0,"b'import os\n\nfrom setuptools import setup\n\n\ntry:\n    with open(\n        os.path.join(os.path.dirname(__file__), ""README.md""), encoding=""utf-8""\n    ) as f:\n        long_description = f.read()\nexcept FileNotFoundError:\n    long_description = """"\n\nwith open(\n    os.path.join(os.path.dirname(__file__), ""requirements.in""), encoding=""utf-8""\n) as f:\n    requirements = f.readlines()\n\n\nsetup(\n    name=""labours"",\n    description=""Python companion for github.com/src-d/hercules to visualize the results."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    version=""10.7.2"",\n    license=""Apache-2.0"",\n    author=""source{d}"",\n    author_email=""machine-learning@sourced.tech"",\n    url=""https://github.com/src-d/hercules"",\n    download_url=""https://github.com/src-d/hercules"",\n    packages=[""labours"", ""labours._vendor"", ""labours.modes""],\n    keywords=[""git"", ""mloncode"", ""mining software repositories"", ""hercules""],\n    install_requires=requirements,\n    package_data={""labours"": [""../LICENSE.md"", ""../README.md"", ""../requirements.txt""]},\n    entry_points={""console_scripts"": [""labours=labours.__main__:main""]},\n    classifiers=[\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""Environment :: Console"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n    ],\n)\n'"
contrib/_plugin_example/plot_churn.py,0,"b'import argparse\nfrom datetime import datetime, timedelta\nimport os\nimport re\nimport sys\n\nfrom matplotlib import pyplot\nimport matplotlib.dates as mdates\nimport numpy\nimport pandas\nimport yaml\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""input"", help=""Path to the input YAML file. \\""-\\"" means stdin."")\n    parser.add_argument(""-o"", ""--output"", help=""Output directory. If empty, display the plots."")\n    parser.add_argument(""-f"", ""--format"", choices=(""png"", ""svg""), default=""png"",\n                        help=""Output format"")\n    parser.add_argument(""--tick-days"", type=int, default=7, help=""Ticks interval in days."")\n    args = parser.parse_args()\n    return args\n\n\ndef parse_input(file):\n    yaml.reader.Reader.NON_PRINTABLE = re.compile(r""(?!x)x"")\n    try:\n        loader = yaml.CLoader\n    except AttributeError:\n        print(""Warning: failed to import yaml.CLoader, falling back to slow yaml.Loader"")\n        loader = yaml.Loader\n    try:\n        if file != ""-"":\n            with open(file) as fin:\n                return yaml.load(fin, Loader=loader)\n        else:\n            return yaml.load(sys.stdin, Loader=loader)\n    except (UnicodeEncodeError, yaml.reader.ReaderError) as e:\n        print(""\\nInvalid unicode in the input: %s\\nPlease filter it through ""\n              ""fix_yaml_unicode.py"" % e)\n        sys.exit(1)\n\n\ndef plot_churn(name, data, url, beginTime, endTime, output, fmt, tick_interval):\n    days, adds, dels = data[""days""], data[""additions""], data[""removals""]\n    dates = [beginTime + timedelta(days=d) for d in days]\n    df = pandas.DataFrame(data=list(zip(adds, dels)),\n                          index=dates,\n                          columns=(""additions"", ""removals""))\n    df[""removals""] = -df[""removals""]\n    df = df.reindex(pandas.date_range(beginTime, endTime, freq=""D""))\n    effective = df[""additions""] + df[""removals""]\n    effective = effective.cumsum()\n    effective.fillna(method=""ffill"", inplace=True)\n    scale = numpy.maximum(df.max(), -df.min()).max()\n    effective = effective / effective.max() * scale\n    pyplot.figure(figsize=(16, 9))\n    for spine in pyplot.gca().spines.values():\n        spine.set_visible(False)\n    pyplot.gca().xaxis.set_major_locator(mdates.DayLocator(interval=tick_interval))\n    pyplot.gca().xaxis.set_major_formatter(mdates.DateFormatter(""%Y-%m-%d""))\n    pyplot.tick_params(top=""off"", bottom=""off"", left=""off"", right=""off"",\n                       labelleft=""off"", labelbottom=""on"")\n    pyplot.bar(df.index, df[""additions""], label=""additions"")\n    pyplot.bar(df.index, df[""removals""], label=""removals"")\n    pyplot.plot(df.index, effective, ""black"", label=""effective"")\n    pyplot.xticks(rotation=""vertical"")\n    pyplot.legend(loc=2, fontsize=18)\n    pyplot.title(""%s churn plot, %s"" % (name, url), fontsize=24)\n    if not output:\n        pyplot.show()\n    else:\n        os.makedirs(output, exist_ok=True)\n        pyplot.savefig(os.path.join(output, name.replace(""/"", ""_"") + ""."" + fmt),\n                       bbox_inches=""tight"", transparent=True)\n\n\ndef main():\n    args = parse_args()\n    data = parse_input(args.input)\n    beginTime, endTime = (datetime.fromtimestamp(data[""hercules""][t])\n                          for t in (""begin_unix_time"", ""end_unix_time""))\n    for key, val in data[""ChurnAnalysis""].items():\n        plot_churn(key, val, data[""hercules""][""repository""], beginTime, endTime,\n                   args.output, args.format, args.tick_days)\n\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
internal/pb/__init__.py,0,b''
internal/pb/pb_pb2.py,0,"b'# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: pb.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'pb.proto\',\n  package=\'\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x08pb.proto\\""\\x81\\x02\\n\\x08Metadata\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x0c\\n\\x04hash\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nrepository\\x18\\x03 \\x01(\\t\\x12\\x17\\n\\x0f\\x62\\x65gin_unix_time\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rend_unix_time\\x18\\x05 \\x01(\\x03\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x06 \\x01(\\x05\\x12\\x10\\n\\x08run_time\\x18\\x07 \\x01(\\x03\\x12\\x38\\n\\x11run_time_per_item\\x18\\x08 \\x03(\\x0b\\x32\\x1d.Metadata.RunTimePerItemEntry\\x1a\\x35\\n\\x13RunTimePerItemEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\""*\\n\\x17\\x42urndownSparseMatrixRow\\x12\\x0f\\n\\x07\\x63olumns\\x18\\x01 \\x03(\\r\\""\\x7f\\n\\x14\\x42urndownSparseMatrix\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x16\\n\\x0enumber_of_rows\\x18\\x02 \\x01(\\x05\\x12\\x19\\n\\x11number_of_columns\\x18\\x03 \\x01(\\x05\\x12&\\n\\x04rows\\x18\\x04 \\x03(\\x0b\\x32\\x18.BurndownSparseMatrixRow\\""i\\n\\x0e\\x46ilesOwnership\\x12)\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32\\x1a.FilesOwnership.ValueEntry\\x1a,\\n\\nValueEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\""\\xaa\\x02\\n\\x17\\x42urndownAnalysisResults\\x12\\x13\\n\\x0bgranularity\\x18\\x01 \\x01(\\x05\\x12\\x10\\n\\x08sampling\\x18\\x02 \\x01(\\x05\\x12&\\n\\x07project\\x18\\x03 \\x01(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12$\\n\\x05\\x66iles\\x18\\x04 \\x03(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12%\\n\\x06people\\x18\\x05 \\x03(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12\\x36\\n\\x12people_interaction\\x18\\x06 \\x01(\\x0b\\x32\\x1a.CompressedSparseRowMatrix\\x12(\\n\\x0f\\x66iles_ownership\\x18\\x07 \\x03(\\x0b\\x32\\x0f.FilesOwnership\\x12\\x11\\n\\ttick_size\\x18\\x08 \\x01(\\x03\\""}\\n\\x19\\x43ompressedSparseRowMatrix\\x12\\x16\\n\\x0enumber_of_rows\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x11number_of_columns\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x03 \\x03(\\x03\\x12\\x0f\\n\\x07indices\\x18\\x04 \\x03(\\x05\\x12\\x0e\\n\\x06indptr\\x18\\x05 \\x03(\\x03\\""D\\n\\x07\\x43ouples\\x12\\r\\n\\x05index\\x18\\x01 \\x03(\\t\\x12*\\n\\x06matrix\\x18\\x02 \\x01(\\x0b\\x32\\x1a.CompressedSparseRowMatrix\\""\\x1d\\n\\x0cTouchedFiles\\x12\\r\\n\\x05\\x66iles\\x18\\x01 \\x03(\\x05\\""\\x94\\x01\\n\\x16\\x43ouplesAnalysisResults\\x12\\x1e\\n\\x0c\\x66ile_couples\\x18\\x06 \\x01(\\x0b\\x32\\x08.Couples\\x12 \\n\\x0epeople_couples\\x18\\x07 \\x01(\\x0b\\x32\\x08.Couples\\x12#\\n\\x0cpeople_files\\x18\\x08 \\x03(\\x0b\\x32\\r.TouchedFiles\\x12\\x13\\n\\x0b\\x66iles_lines\\x18\\t \\x03(\\x05\\""o\\n\\nUASTChange\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nsrc_before\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tsrc_after\\x18\\x03 \\x01(\\t\\x12\\x13\\n\\x0buast_before\\x18\\x04 \\x01(\\t\\x12\\x12\\n\\nuast_after\\x18\\x05 \\x01(\\t\\""7\\n\\x17UASTChangesSaverResults\\x12\\x1c\\n\\x07\\x63hanges\\x18\\x01 \\x03(\\x0b\\x32\\x0b.UASTChange\\""\\x9c\\x01\\n\\x0eShotnessRecord\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04\\x66ile\\x18\\x03 \\x01(\\t\\x12/\\n\\x08\\x63ounters\\x18\\x04 \\x03(\\x0b\\x32\\x1d.ShotnessRecord.CountersEntry\\x1a/\\n\\rCountersEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\"";\\n\\x17ShotnessAnalysisResults\\x12 \\n\\x07records\\x18\\x01 \\x03(\\x0b\\x32\\x0f.ShotnessRecord\\""\\xa9\\x01\\n\\x0b\\x46ileHistory\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x01 \\x03(\\t\\x12\\x42\\n\\x14\\x63hanges_by_developer\\x18\\x02 \\x03(\\x0b\\x32$.FileHistory.ChangesByDeveloperEntry\\x1a\\x45\\n\\x17\\x43hangesByDeveloperEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats:\\x02\\x38\\x01\\""\\x8b\\x01\\n\\x18\\x46ileHistoryResultMessage\\x12\\x33\\n\\x05\\x66iles\\x18\\x01 \\x03(\\x0b\\x32$.FileHistoryResultMessage.FilesEntry\\x1a:\\n\\nFilesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x0c.FileHistory:\\x02\\x38\\x01\\""<\\n\\tLineStats\\x12\\r\\n\\x05\\x61\\x64\\x64\\x65\\x64\\x18\\x01 \\x01(\\x05\\x12\\x0f\\n\\x07removed\\x18\\x02 \\x01(\\x05\\x12\\x0f\\n\\x07\\x63hanged\\x18\\x03 \\x01(\\x05\\""\\x9f\\x01\\n\\x07\\x44\\x65vTick\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05stats\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats\\x12*\\n\\tlanguages\\x18\\x03 \\x03(\\x0b\\x32\\x17.DevTick.LanguagesEntry\\x1a<\\n\\x0eLanguagesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats:\\x02\\x38\\x01\\""d\\n\\x08TickDevs\\x12!\\n\\x04\\x64\\x65vs\\x18\\x01 \\x03(\\x0b\\x32\\x13.TickDevs.DevsEntry\\x1a\\x35\\n\\tDevsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x17\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x08.DevTick:\\x02\\x38\\x01\\""\\x91\\x01\\n\\x13\\x44\\x65vsAnalysisResults\\x12.\\n\\x05ticks\\x18\\x01 \\x03(\\x0b\\x32\\x1f.DevsAnalysisResults.TicksEntry\\x12\\x11\\n\\tdev_index\\x18\\x02 \\x03(\\t\\x1a\\x37\\n\\nTicksEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x18\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\t.TickDevs:\\x02\\x38\\x01\\""=\\n\\tSentiment\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\x12\\x10\\n\\x08\\x63omments\\x18\\x02 \\x03(\\t\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x03 \\x03(\\t\\""\\xa7\\x01\\n\\x17\\x43ommentSentimentResults\\x12H\\n\\x11sentiment_by_tick\\x18\\x01 \\x03(\\x0b\\x32-.CommentSentimentResults.SentimentByTickEntry\\x1a\\x42\\n\\x14SentimentByTickEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.Sentiment:\\x02\\x38\\x01\\""G\\n\\nCommitFile\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08language\\x18\\x03 \\x01(\\t\\x12\\x19\\n\\x05stats\\x18\\x04 \\x01(\\x0b\\x32\\n.LineStats\\""Z\\n\\x06\\x43ommit\\x12\\x0c\\n\\x04hash\\x18\\x01 \\x01(\\t\\x12\\x16\\n\\x0ewhen_unix_time\\x18\\x02 \\x01(\\x03\\x12\\x0e\\n\\x06\\x61uthor\\x18\\x03 \\x01(\\x05\\x12\\x1a\\n\\x05\\x66iles\\x18\\x04 \\x03(\\x0b\\x32\\x0b.CommitFile\\""H\\n\\x16\\x43ommitsAnalysisResults\\x12\\x18\\n\\x07\\x63ommits\\x18\\x01 \\x03(\\x0b\\x32\\x07.Commit\\x12\\x14\\n\\x0c\\x61uthor_index\\x18\\x02 \\x03(\\t\\""R\\n\\x04Typo\\x12\\r\\n\\x05wrong\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63orrect\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x63ommit\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04\\x66ile\\x18\\x04 \\x01(\\t\\x12\\x0c\\n\\x04line\\x18\\x05 \\x01(\\x05\\""$\\n\\x0cTyposDataset\\x12\\x14\\n\\x05typos\\x18\\x01 \\x03(\\x0b\\x32\\x05.Typo\\""\\x8f\\x01\\n\\x0f\\x41nalysisResults\\x12\\x19\\n\\x06header\\x18\\x01 \\x01(\\x0b\\x32\\t.Metadata\\x12\\x30\\n\\x08\\x63ontents\\x18\\x02 \\x03(\\x0b\\x32\\x1e.AnalysisResults.ContentsEntry\\x1a/\\n\\rContentsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x0c:\\x02\\x38\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_METADATA_RUNTIMEPERITEMENTRY = _descriptor.Descriptor(\n  name=\'RunTimePerItemEntry\',\n  full_name=\'Metadata.RunTimePerItemEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'Metadata.RunTimePerItemEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'Metadata.RunTimePerItemEntry.value\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=217,\n  serialized_end=270,\n)\n\n_METADATA = _descriptor.Descriptor(\n  name=\'Metadata\',\n  full_name=\'Metadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'Metadata.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hash\', full_name=\'Metadata.hash\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'repository\', full_name=\'Metadata.repository\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'begin_unix_time\', full_name=\'Metadata.begin_unix_time\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end_unix_time\', full_name=\'Metadata.end_unix_time\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'Metadata.commits\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'run_time\', full_name=\'Metadata.run_time\', index=6,\n      number=7, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'run_time_per_item\', full_name=\'Metadata.run_time_per_item\', index=7,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_METADATA_RUNTIMEPERITEMENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=13,\n  serialized_end=270,\n)\n\n\n_BURNDOWNSPARSEMATRIXROW = _descriptor.Descriptor(\n  name=\'BurndownSparseMatrixRow\',\n  full_name=\'BurndownSparseMatrixRow\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'columns\', full_name=\'BurndownSparseMatrixRow.columns\', index=0,\n      number=1, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=272,\n  serialized_end=314,\n)\n\n\n_BURNDOWNSPARSEMATRIX = _descriptor.Descriptor(\n  name=\'BurndownSparseMatrix\',\n  full_name=\'BurndownSparseMatrix\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'BurndownSparseMatrix.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_rows\', full_name=\'BurndownSparseMatrix.number_of_rows\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_columns\', full_name=\'BurndownSparseMatrix.number_of_columns\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'rows\', full_name=\'BurndownSparseMatrix.rows\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=316,\n  serialized_end=443,\n)\n\n\n_FILESOWNERSHIP_VALUEENTRY = _descriptor.Descriptor(\n  name=\'ValueEntry\',\n  full_name=\'FilesOwnership.ValueEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FilesOwnership.ValueEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FilesOwnership.ValueEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=506,\n  serialized_end=550,\n)\n\n_FILESOWNERSHIP = _descriptor.Descriptor(\n  name=\'FilesOwnership\',\n  full_name=\'FilesOwnership\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FilesOwnership.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILESOWNERSHIP_VALUEENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=445,\n  serialized_end=550,\n)\n\n\n_BURNDOWNANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'BurndownAnalysisResults\',\n  full_name=\'BurndownAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'granularity\', full_name=\'BurndownAnalysisResults.granularity\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sampling\', full_name=\'BurndownAnalysisResults.sampling\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'project\', full_name=\'BurndownAnalysisResults.project\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'BurndownAnalysisResults.files\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people\', full_name=\'BurndownAnalysisResults.people\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_interaction\', full_name=\'BurndownAnalysisResults.people_interaction\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files_ownership\', full_name=\'BurndownAnalysisResults.files_ownership\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tick_size\', full_name=\'BurndownAnalysisResults.tick_size\', index=7,\n      number=8, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=553,\n  serialized_end=851,\n)\n\n\n_COMPRESSEDSPARSEROWMATRIX = _descriptor.Descriptor(\n  name=\'CompressedSparseRowMatrix\',\n  full_name=\'CompressedSparseRowMatrix\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'number_of_rows\', full_name=\'CompressedSparseRowMatrix.number_of_rows\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_columns\', full_name=\'CompressedSparseRowMatrix.number_of_columns\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'CompressedSparseRowMatrix.data\', index=2,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'indices\', full_name=\'CompressedSparseRowMatrix.indices\', index=3,\n      number=4, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'indptr\', full_name=\'CompressedSparseRowMatrix.indptr\', index=4,\n      number=5, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=853,\n  serialized_end=978,\n)\n\n\n_COUPLES = _descriptor.Descriptor(\n  name=\'Couples\',\n  full_name=\'Couples\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'Couples.index\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'matrix\', full_name=\'Couples.matrix\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=980,\n  serialized_end=1048,\n)\n\n\n_TOUCHEDFILES = _descriptor.Descriptor(\n  name=\'TouchedFiles\',\n  full_name=\'TouchedFiles\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'TouchedFiles.files\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1050,\n  serialized_end=1079,\n)\n\n\n_COUPLESANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'CouplesAnalysisResults\',\n  full_name=\'CouplesAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_couples\', full_name=\'CouplesAnalysisResults.file_couples\', index=0,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_couples\', full_name=\'CouplesAnalysisResults.people_couples\', index=1,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_files\', full_name=\'CouplesAnalysisResults.people_files\', index=2,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files_lines\', full_name=\'CouplesAnalysisResults.files_lines\', index=3,\n      number=9, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1082,\n  serialized_end=1230,\n)\n\n\n_UASTCHANGE = _descriptor.Descriptor(\n  name=\'UASTChange\',\n  full_name=\'UASTChange\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_name\', full_name=\'UASTChange.file_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'src_before\', full_name=\'UASTChange.src_before\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'src_after\', full_name=\'UASTChange.src_after\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'uast_before\', full_name=\'UASTChange.uast_before\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'uast_after\', full_name=\'UASTChange.uast_after\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1232,\n  serialized_end=1343,\n)\n\n\n_UASTCHANGESSAVERRESULTS = _descriptor.Descriptor(\n  name=\'UASTChangesSaverResults\',\n  full_name=\'UASTChangesSaverResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'changes\', full_name=\'UASTChangesSaverResults.changes\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1345,\n  serialized_end=1400,\n)\n\n\n_SHOTNESSRECORD_COUNTERSENTRY = _descriptor.Descriptor(\n  name=\'CountersEntry\',\n  full_name=\'ShotnessRecord.CountersEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'ShotnessRecord.CountersEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'ShotnessRecord.CountersEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1512,\n  serialized_end=1559,\n)\n\n_SHOTNESSRECORD = _descriptor.Descriptor(\n  name=\'ShotnessRecord\',\n  full_name=\'ShotnessRecord\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'ShotnessRecord.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'ShotnessRecord.name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'file\', full_name=\'ShotnessRecord.file\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'counters\', full_name=\'ShotnessRecord.counters\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SHOTNESSRECORD_COUNTERSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1403,\n  serialized_end=1559,\n)\n\n\n_SHOTNESSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'ShotnessAnalysisResults\',\n  full_name=\'ShotnessAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'records\', full_name=\'ShotnessAnalysisResults.records\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1561,\n  serialized_end=1620,\n)\n\n\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY = _descriptor.Descriptor(\n  name=\'ChangesByDeveloperEntry\',\n  full_name=\'FileHistory.ChangesByDeveloperEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FileHistory.ChangesByDeveloperEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FileHistory.ChangesByDeveloperEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1723,\n  serialized_end=1792,\n)\n\n_FILEHISTORY = _descriptor.Descriptor(\n  name=\'FileHistory\',\n  full_name=\'FileHistory\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'FileHistory.commits\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'changes_by_developer\', full_name=\'FileHistory.changes_by_developer\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILEHISTORY_CHANGESBYDEVELOPERENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1623,\n  serialized_end=1792,\n)\n\n\n_FILEHISTORYRESULTMESSAGE_FILESENTRY = _descriptor.Descriptor(\n  name=\'FilesEntry\',\n  full_name=\'FileHistoryResultMessage.FilesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FileHistoryResultMessage.FilesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FileHistoryResultMessage.FilesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1876,\n  serialized_end=1934,\n)\n\n_FILEHISTORYRESULTMESSAGE = _descriptor.Descriptor(\n  name=\'FileHistoryResultMessage\',\n  full_name=\'FileHistoryResultMessage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'FileHistoryResultMessage.files\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILEHISTORYRESULTMESSAGE_FILESENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1795,\n  serialized_end=1934,\n)\n\n\n_LINESTATS = _descriptor.Descriptor(\n  name=\'LineStats\',\n  full_name=\'LineStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'added\', full_name=\'LineStats.added\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'removed\', full_name=\'LineStats.removed\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'changed\', full_name=\'LineStats.changed\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1936,\n  serialized_end=1996,\n)\n\n\n_DEVTICK_LANGUAGESENTRY = _descriptor.Descriptor(\n  name=\'LanguagesEntry\',\n  full_name=\'DevTick.LanguagesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'DevTick.LanguagesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'DevTick.LanguagesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2098,\n  serialized_end=2158,\n)\n\n_DEVTICK = _descriptor.Descriptor(\n  name=\'DevTick\',\n  full_name=\'DevTick\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'DevTick.commits\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stats\', full_name=\'DevTick.stats\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'languages\', full_name=\'DevTick.languages\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEVTICK_LANGUAGESENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1999,\n  serialized_end=2158,\n)\n\n\n_TICKDEVS_DEVSENTRY = _descriptor.Descriptor(\n  name=\'DevsEntry\',\n  full_name=\'TickDevs.DevsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'TickDevs.DevsEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'TickDevs.DevsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2207,\n  serialized_end=2260,\n)\n\n_TICKDEVS = _descriptor.Descriptor(\n  name=\'TickDevs\',\n  full_name=\'TickDevs\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'devs\', full_name=\'TickDevs.devs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TICKDEVS_DEVSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2160,\n  serialized_end=2260,\n)\n\n\n_DEVSANALYSISRESULTS_TICKSENTRY = _descriptor.Descriptor(\n  name=\'TicksEntry\',\n  full_name=\'DevsAnalysisResults.TicksEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'DevsAnalysisResults.TicksEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'DevsAnalysisResults.TicksEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2353,\n  serialized_end=2408,\n)\n\n_DEVSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'DevsAnalysisResults\',\n  full_name=\'DevsAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ticks\', full_name=\'DevsAnalysisResults.ticks\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dev_index\', full_name=\'DevsAnalysisResults.dev_index\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEVSANALYSISRESULTS_TICKSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2263,\n  serialized_end=2408,\n)\n\n\n_SENTIMENT = _descriptor.Descriptor(\n  name=\'Sentiment\',\n  full_name=\'Sentiment\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'Sentiment.value\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'comments\', full_name=\'Sentiment.comments\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'Sentiment.commits\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2410,\n  serialized_end=2471,\n)\n\n\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY = _descriptor.Descriptor(\n  name=\'SentimentByTickEntry\',\n  full_name=\'CommentSentimentResults.SentimentByTickEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'CommentSentimentResults.SentimentByTickEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'CommentSentimentResults.SentimentByTickEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2575,\n  serialized_end=2641,\n)\n\n_COMMENTSENTIMENTRESULTS = _descriptor.Descriptor(\n  name=\'CommentSentimentResults\',\n  full_name=\'CommentSentimentResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentiment_by_tick\', full_name=\'CommentSentimentResults.sentiment_by_tick\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2474,\n  serialized_end=2641,\n)\n\n\n_COMMITFILE = _descriptor.Descriptor(\n  name=\'CommitFile\',\n  full_name=\'CommitFile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'CommitFile.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'language\', full_name=\'CommitFile.language\', index=1,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stats\', full_name=\'CommitFile.stats\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2643,\n  serialized_end=2714,\n)\n\n\n_COMMIT = _descriptor.Descriptor(\n  name=\'Commit\',\n  full_name=\'Commit\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'hash\', full_name=\'Commit.hash\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'when_unix_time\', full_name=\'Commit.when_unix_time\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author\', full_name=\'Commit.author\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'Commit.files\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2716,\n  serialized_end=2806,\n)\n\n\n_COMMITSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'CommitsAnalysisResults\',\n  full_name=\'CommitsAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'CommitsAnalysisResults.commits\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author_index\', full_name=\'CommitsAnalysisResults.author_index\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2808,\n  serialized_end=2880,\n)\n\n\n_TYPO = _descriptor.Descriptor(\n  name=\'Typo\',\n  full_name=\'Typo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'wrong\', full_name=\'Typo.wrong\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'correct\', full_name=\'Typo.correct\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commit\', full_name=\'Typo.commit\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'file\', full_name=\'Typo.file\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'line\', full_name=\'Typo.line\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2882,\n  serialized_end=2964,\n)\n\n\n_TYPOSDATASET = _descriptor.Descriptor(\n  name=\'TyposDataset\',\n  full_name=\'TyposDataset\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'typos\', full_name=\'TyposDataset.typos\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2966,\n  serialized_end=3002,\n)\n\n\n_ANALYSISRESULTS_CONTENTSENTRY = _descriptor.Descriptor(\n  name=\'ContentsEntry\',\n  full_name=\'AnalysisResults.ContentsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'AnalysisResults.ContentsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'AnalysisResults.ContentsEntry.value\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3101,\n  serialized_end=3148,\n)\n\n_ANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'AnalysisResults\',\n  full_name=\'AnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'header\', full_name=\'AnalysisResults.header\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'contents\', full_name=\'AnalysisResults.contents\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ANALYSISRESULTS_CONTENTSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3005,\n  serialized_end=3148,\n)\n\n_METADATA_RUNTIMEPERITEMENTRY.containing_type = _METADATA\n_METADATA.fields_by_name[\'run_time_per_item\'].message_type = _METADATA_RUNTIMEPERITEMENTRY\n_BURNDOWNSPARSEMATRIX.fields_by_name[\'rows\'].message_type = _BURNDOWNSPARSEMATRIXROW\n_FILESOWNERSHIP_VALUEENTRY.containing_type = _FILESOWNERSHIP\n_FILESOWNERSHIP.fields_by_name[\'value\'].message_type = _FILESOWNERSHIP_VALUEENTRY\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'project\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'files\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'people\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'people_interaction\'].message_type = _COMPRESSEDSPARSEROWMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'files_ownership\'].message_type = _FILESOWNERSHIP\n_COUPLES.fields_by_name[\'matrix\'].message_type = _COMPRESSEDSPARSEROWMATRIX\n_COUPLESANALYSISRESULTS.fields_by_name[\'file_couples\'].message_type = _COUPLES\n_COUPLESANALYSISRESULTS.fields_by_name[\'people_couples\'].message_type = _COUPLES\n_COUPLESANALYSISRESULTS.fields_by_name[\'people_files\'].message_type = _TOUCHEDFILES\n_UASTCHANGESSAVERRESULTS.fields_by_name[\'changes\'].message_type = _UASTCHANGE\n_SHOTNESSRECORD_COUNTERSENTRY.containing_type = _SHOTNESSRECORD\n_SHOTNESSRECORD.fields_by_name[\'counters\'].message_type = _SHOTNESSRECORD_COUNTERSENTRY\n_SHOTNESSANALYSISRESULTS.fields_by_name[\'records\'].message_type = _SHOTNESSRECORD\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY.fields_by_name[\'value\'].message_type = _LINESTATS\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY.containing_type = _FILEHISTORY\n_FILEHISTORY.fields_by_name[\'changes_by_developer\'].message_type = _FILEHISTORY_CHANGESBYDEVELOPERENTRY\n_FILEHISTORYRESULTMESSAGE_FILESENTRY.fields_by_name[\'value\'].message_type = _FILEHISTORY\n_FILEHISTORYRESULTMESSAGE_FILESENTRY.containing_type = _FILEHISTORYRESULTMESSAGE\n_FILEHISTORYRESULTMESSAGE.fields_by_name[\'files\'].message_type = _FILEHISTORYRESULTMESSAGE_FILESENTRY\n_DEVTICK_LANGUAGESENTRY.fields_by_name[\'value\'].message_type = _LINESTATS\n_DEVTICK_LANGUAGESENTRY.containing_type = _DEVTICK\n_DEVTICK.fields_by_name[\'stats\'].message_type = _LINESTATS\n_DEVTICK.fields_by_name[\'languages\'].message_type = _DEVTICK_LANGUAGESENTRY\n_TICKDEVS_DEVSENTRY.fields_by_name[\'value\'].message_type = _DEVTICK\n_TICKDEVS_DEVSENTRY.containing_type = _TICKDEVS\n_TICKDEVS.fields_by_name[\'devs\'].message_type = _TICKDEVS_DEVSENTRY\n_DEVSANALYSISRESULTS_TICKSENTRY.fields_by_name[\'value\'].message_type = _TICKDEVS\n_DEVSANALYSISRESULTS_TICKSENTRY.containing_type = _DEVSANALYSISRESULTS\n_DEVSANALYSISRESULTS.fields_by_name[\'ticks\'].message_type = _DEVSANALYSISRESULTS_TICKSENTRY\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY.fields_by_name[\'value\'].message_type = _SENTIMENT\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY.containing_type = _COMMENTSENTIMENTRESULTS\n_COMMENTSENTIMENTRESULTS.fields_by_name[\'sentiment_by_tick\'].message_type = _COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY\n_COMMITFILE.fields_by_name[\'stats\'].message_type = _LINESTATS\n_COMMIT.fields_by_name[\'files\'].message_type = _COMMITFILE\n_COMMITSANALYSISRESULTS.fields_by_name[\'commits\'].message_type = _COMMIT\n_TYPOSDATASET.fields_by_name[\'typos\'].message_type = _TYPO\n_ANALYSISRESULTS_CONTENTSENTRY.containing_type = _ANALYSISRESULTS\n_ANALYSISRESULTS.fields_by_name[\'header\'].message_type = _METADATA\n_ANALYSISRESULTS.fields_by_name[\'contents\'].message_type = _ANALYSISRESULTS_CONTENTSENTRY\nDESCRIPTOR.message_types_by_name[\'Metadata\'] = _METADATA\nDESCRIPTOR.message_types_by_name[\'BurndownSparseMatrixRow\'] = _BURNDOWNSPARSEMATRIXROW\nDESCRIPTOR.message_types_by_name[\'BurndownSparseMatrix\'] = _BURNDOWNSPARSEMATRIX\nDESCRIPTOR.message_types_by_name[\'FilesOwnership\'] = _FILESOWNERSHIP\nDESCRIPTOR.message_types_by_name[\'BurndownAnalysisResults\'] = _BURNDOWNANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'CompressedSparseRowMatrix\'] = _COMPRESSEDSPARSEROWMATRIX\nDESCRIPTOR.message_types_by_name[\'Couples\'] = _COUPLES\nDESCRIPTOR.message_types_by_name[\'TouchedFiles\'] = _TOUCHEDFILES\nDESCRIPTOR.message_types_by_name[\'CouplesAnalysisResults\'] = _COUPLESANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'UASTChange\'] = _UASTCHANGE\nDESCRIPTOR.message_types_by_name[\'UASTChangesSaverResults\'] = _UASTCHANGESSAVERRESULTS\nDESCRIPTOR.message_types_by_name[\'ShotnessRecord\'] = _SHOTNESSRECORD\nDESCRIPTOR.message_types_by_name[\'ShotnessAnalysisResults\'] = _SHOTNESSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'FileHistory\'] = _FILEHISTORY\nDESCRIPTOR.message_types_by_name[\'FileHistoryResultMessage\'] = _FILEHISTORYRESULTMESSAGE\nDESCRIPTOR.message_types_by_name[\'LineStats\'] = _LINESTATS\nDESCRIPTOR.message_types_by_name[\'DevTick\'] = _DEVTICK\nDESCRIPTOR.message_types_by_name[\'TickDevs\'] = _TICKDEVS\nDESCRIPTOR.message_types_by_name[\'DevsAnalysisResults\'] = _DEVSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'Sentiment\'] = _SENTIMENT\nDESCRIPTOR.message_types_by_name[\'CommentSentimentResults\'] = _COMMENTSENTIMENTRESULTS\nDESCRIPTOR.message_types_by_name[\'CommitFile\'] = _COMMITFILE\nDESCRIPTOR.message_types_by_name[\'Commit\'] = _COMMIT\nDESCRIPTOR.message_types_by_name[\'CommitsAnalysisResults\'] = _COMMITSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'Typo\'] = _TYPO\nDESCRIPTOR.message_types_by_name[\'TyposDataset\'] = _TYPOSDATASET\nDESCRIPTOR.message_types_by_name[\'AnalysisResults\'] = _ANALYSISRESULTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMetadata = _reflection.GeneratedProtocolMessageType(\'Metadata\', (_message.Message,), dict(\n\n  RunTimePerItemEntry = _reflection.GeneratedProtocolMessageType(\'RunTimePerItemEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _METADATA_RUNTIMEPERITEMENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:Metadata.RunTimePerItemEntry)\n    ))\n  ,\n  DESCRIPTOR = _METADATA,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Metadata)\n  ))\n_sym_db.RegisterMessage(Metadata)\n_sym_db.RegisterMessage(Metadata.RunTimePerItemEntry)\n\nBurndownSparseMatrixRow = _reflection.GeneratedProtocolMessageType(\'BurndownSparseMatrixRow\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNSPARSEMATRIXROW,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownSparseMatrixRow)\n  ))\n_sym_db.RegisterMessage(BurndownSparseMatrixRow)\n\nBurndownSparseMatrix = _reflection.GeneratedProtocolMessageType(\'BurndownSparseMatrix\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNSPARSEMATRIX,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownSparseMatrix)\n  ))\n_sym_db.RegisterMessage(BurndownSparseMatrix)\n\nFilesOwnership = _reflection.GeneratedProtocolMessageType(\'FilesOwnership\', (_message.Message,), dict(\n\n  ValueEntry = _reflection.GeneratedProtocolMessageType(\'ValueEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILESOWNERSHIP_VALUEENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FilesOwnership.ValueEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILESOWNERSHIP,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FilesOwnership)\n  ))\n_sym_db.RegisterMessage(FilesOwnership)\n_sym_db.RegisterMessage(FilesOwnership.ValueEntry)\n\nBurndownAnalysisResults = _reflection.GeneratedProtocolMessageType(\'BurndownAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownAnalysisResults)\n  ))\n_sym_db.RegisterMessage(BurndownAnalysisResults)\n\nCompressedSparseRowMatrix = _reflection.GeneratedProtocolMessageType(\'CompressedSparseRowMatrix\', (_message.Message,), dict(\n  DESCRIPTOR = _COMPRESSEDSPARSEROWMATRIX,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CompressedSparseRowMatrix)\n  ))\n_sym_db.RegisterMessage(CompressedSparseRowMatrix)\n\nCouples = _reflection.GeneratedProtocolMessageType(\'Couples\', (_message.Message,), dict(\n  DESCRIPTOR = _COUPLES,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Couples)\n  ))\n_sym_db.RegisterMessage(Couples)\n\nTouchedFiles = _reflection.GeneratedProtocolMessageType(\'TouchedFiles\', (_message.Message,), dict(\n  DESCRIPTOR = _TOUCHEDFILES,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TouchedFiles)\n  ))\n_sym_db.RegisterMessage(TouchedFiles)\n\nCouplesAnalysisResults = _reflection.GeneratedProtocolMessageType(\'CouplesAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _COUPLESANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CouplesAnalysisResults)\n  ))\n_sym_db.RegisterMessage(CouplesAnalysisResults)\n\nUASTChange = _reflection.GeneratedProtocolMessageType(\'UASTChange\', (_message.Message,), dict(\n  DESCRIPTOR = _UASTCHANGE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:UASTChange)\n  ))\n_sym_db.RegisterMessage(UASTChange)\n\nUASTChangesSaverResults = _reflection.GeneratedProtocolMessageType(\'UASTChangesSaverResults\', (_message.Message,), dict(\n  DESCRIPTOR = _UASTCHANGESSAVERRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:UASTChangesSaverResults)\n  ))\n_sym_db.RegisterMessage(UASTChangesSaverResults)\n\nShotnessRecord = _reflection.GeneratedProtocolMessageType(\'ShotnessRecord\', (_message.Message,), dict(\n\n  CountersEntry = _reflection.GeneratedProtocolMessageType(\'CountersEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SHOTNESSRECORD_COUNTERSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:ShotnessRecord.CountersEntry)\n    ))\n  ,\n  DESCRIPTOR = _SHOTNESSRECORD,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ShotnessRecord)\n  ))\n_sym_db.RegisterMessage(ShotnessRecord)\n_sym_db.RegisterMessage(ShotnessRecord.CountersEntry)\n\nShotnessAnalysisResults = _reflection.GeneratedProtocolMessageType(\'ShotnessAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _SHOTNESSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ShotnessAnalysisResults)\n  ))\n_sym_db.RegisterMessage(ShotnessAnalysisResults)\n\nFileHistory = _reflection.GeneratedProtocolMessageType(\'FileHistory\', (_message.Message,), dict(\n\n  ChangesByDeveloperEntry = _reflection.GeneratedProtocolMessageType(\'ChangesByDeveloperEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILEHISTORY_CHANGESBYDEVELOPERENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FileHistory.ChangesByDeveloperEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILEHISTORY,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FileHistory)\n  ))\n_sym_db.RegisterMessage(FileHistory)\n_sym_db.RegisterMessage(FileHistory.ChangesByDeveloperEntry)\n\nFileHistoryResultMessage = _reflection.GeneratedProtocolMessageType(\'FileHistoryResultMessage\', (_message.Message,), dict(\n\n  FilesEntry = _reflection.GeneratedProtocolMessageType(\'FilesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILEHISTORYRESULTMESSAGE_FILESENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FileHistoryResultMessage.FilesEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILEHISTORYRESULTMESSAGE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FileHistoryResultMessage)\n  ))\n_sym_db.RegisterMessage(FileHistoryResultMessage)\n_sym_db.RegisterMessage(FileHistoryResultMessage.FilesEntry)\n\nLineStats = _reflection.GeneratedProtocolMessageType(\'LineStats\', (_message.Message,), dict(\n  DESCRIPTOR = _LINESTATS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:LineStats)\n  ))\n_sym_db.RegisterMessage(LineStats)\n\nDevTick = _reflection.GeneratedProtocolMessageType(\'DevTick\', (_message.Message,), dict(\n\n  LanguagesEntry = _reflection.GeneratedProtocolMessageType(\'LanguagesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DEVTICK_LANGUAGESENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:DevTick.LanguagesEntry)\n    ))\n  ,\n  DESCRIPTOR = _DEVTICK,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:DevTick)\n  ))\n_sym_db.RegisterMessage(DevTick)\n_sym_db.RegisterMessage(DevTick.LanguagesEntry)\n\nTickDevs = _reflection.GeneratedProtocolMessageType(\'TickDevs\', (_message.Message,), dict(\n\n  DevsEntry = _reflection.GeneratedProtocolMessageType(\'DevsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _TICKDEVS_DEVSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:TickDevs.DevsEntry)\n    ))\n  ,\n  DESCRIPTOR = _TICKDEVS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TickDevs)\n  ))\n_sym_db.RegisterMessage(TickDevs)\n_sym_db.RegisterMessage(TickDevs.DevsEntry)\n\nDevsAnalysisResults = _reflection.GeneratedProtocolMessageType(\'DevsAnalysisResults\', (_message.Message,), dict(\n\n  TicksEntry = _reflection.GeneratedProtocolMessageType(\'TicksEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DEVSANALYSISRESULTS_TICKSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:DevsAnalysisResults.TicksEntry)\n    ))\n  ,\n  DESCRIPTOR = _DEVSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:DevsAnalysisResults)\n  ))\n_sym_db.RegisterMessage(DevsAnalysisResults)\n_sym_db.RegisterMessage(DevsAnalysisResults.TicksEntry)\n\nSentiment = _reflection.GeneratedProtocolMessageType(\'Sentiment\', (_message.Message,), dict(\n  DESCRIPTOR = _SENTIMENT,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Sentiment)\n  ))\n_sym_db.RegisterMessage(Sentiment)\n\nCommentSentimentResults = _reflection.GeneratedProtocolMessageType(\'CommentSentimentResults\', (_message.Message,), dict(\n\n  SentimentByTickEntry = _reflection.GeneratedProtocolMessageType(\'SentimentByTickEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:CommentSentimentResults.SentimentByTickEntry)\n    ))\n  ,\n  DESCRIPTOR = _COMMENTSENTIMENTRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommentSentimentResults)\n  ))\n_sym_db.RegisterMessage(CommentSentimentResults)\n_sym_db.RegisterMessage(CommentSentimentResults.SentimentByTickEntry)\n\nCommitFile = _reflection.GeneratedProtocolMessageType(\'CommitFile\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMITFILE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommitFile)\n  ))\n_sym_db.RegisterMessage(CommitFile)\n\nCommit = _reflection.GeneratedProtocolMessageType(\'Commit\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMIT,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Commit)\n  ))\n_sym_db.RegisterMessage(Commit)\n\nCommitsAnalysisResults = _reflection.GeneratedProtocolMessageType(\'CommitsAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMITSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommitsAnalysisResults)\n  ))\n_sym_db.RegisterMessage(CommitsAnalysisResults)\n\nTypo = _reflection.GeneratedProtocolMessageType(\'Typo\', (_message.Message,), dict(\n  DESCRIPTOR = _TYPO,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Typo)\n  ))\n_sym_db.RegisterMessage(Typo)\n\nTyposDataset = _reflection.GeneratedProtocolMessageType(\'TyposDataset\', (_message.Message,), dict(\n  DESCRIPTOR = _TYPOSDATASET,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TyposDataset)\n  ))\n_sym_db.RegisterMessage(TyposDataset)\n\nAnalysisResults = _reflection.GeneratedProtocolMessageType(\'AnalysisResults\', (_message.Message,), dict(\n\n  ContentsEntry = _reflection.GeneratedProtocolMessageType(\'ContentsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _ANALYSISRESULTS_CONTENTSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:AnalysisResults.ContentsEntry)\n    ))\n  ,\n  DESCRIPTOR = _ANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:AnalysisResults)\n  ))\n_sym_db.RegisterMessage(AnalysisResults)\n_sym_db.RegisterMessage(AnalysisResults.ContentsEntry)\n\n\n_METADATA_RUNTIMEPERITEMENTRY._options = None\n_FILESOWNERSHIP_VALUEENTRY._options = None\n_SHOTNESSRECORD_COUNTERSENTRY._options = None\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY._options = None\n_FILEHISTORYRESULTMESSAGE_FILESENTRY._options = None\n_DEVTICK_LANGUAGESENTRY._options = None\n_TICKDEVS_DEVSENTRY._options = None\n_DEVSANALYSISRESULTS_TICKSENTRY._options = None\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY._options = None\n_ANALYSISRESULTS_CONTENTSENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
python/labours/__init__.py,0,b''
python/labours/__main__.py,0,"b'import sys\n\nfrom labours.cli import main\n\nif __name__ == ""__main__"":\n    sys.exit(main())\n'"
python/labours/cli.py,0,"b'import argparse\nfrom argparse import Namespace\nimport os\nimport subprocess\nimport sys\nimport time\nfrom typing import List\n\nimport numpy\n\nfrom labours.cors_web_server import web_server\nfrom labours.embeddings import train_embeddings, write_embeddings\nfrom labours.modes.burndown import load_burndown, plot_burndown, plot_many_burndown\nfrom labours.modes.devs import show_devs, show_devs_efforts\nfrom labours.modes.devs_parallel import load_devs_parallel, show_devs_parallel\nfrom labours.modes.languages import show_languages\nfrom labours.modes.old_vs_new import show_old_vs_new\nfrom labours.modes.overwrites import load_overwrites_matrix, plot_overwrites_matrix\nfrom labours.modes.ownership import load_ownership, plot_ownership\nfrom labours.modes.sentiment import show_sentiment_stats\nfrom labours.modes.shotness import show_shotness_stats\nfrom labours.readers import read_input\nfrom labours.utils import import_pandas\n\n# NB: this value is modified within the Dockerfile.\nDEFAULT_MATPLOTLIB_BACKEND = None\n\n\ndef list_matplotlib_styles() -> List[str]:\n    script = (\n        ""import sys; from matplotlib import pyplot; ""\n        ""sys.stdout.write(repr(pyplot.style.available))""\n    )\n    styles = eval(subprocess.check_output([sys.executable, ""-c"", script]))\n    styles.remove(""classic"")\n    return [""default"", ""classic""] + styles\n\n\ndef parse_args() -> Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""-o"",\n        ""--output"",\n        default="""",\n        help=""Path to the output file/directory (empty for display). ""\n        ""If the extension is JSON, the data is saved instead of ""\n        ""the real image."",\n    )\n    parser.add_argument(\n        ""-i"", ""--input"", default=""-"", help=""Path to the input file (- for stdin).""\n    )\n    parser.add_argument(\n        ""-f"", ""--input-format"", default=""auto"", choices=[""yaml"", ""pb"", ""auto""]\n    )\n    parser.add_argument(\n        ""--font-size"", default=12, type=int, help=""Size of the labels and legend.""\n    )\n    parser.add_argument(\n        ""--style"",\n        default=""ggplot"",\n        choices=list_matplotlib_styles(),\n        help=""Plot style to use."",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=DEFAULT_MATPLOTLIB_BACKEND,\n        help=""Matplotlib backend to use."",\n    )\n    parser.add_argument(\n        ""--background"",\n        choices=[""black"", ""white""],\n        default=""white"",\n        help=""Plot\'s general color scheme."",\n    )\n    parser.add_argument(""--size"", help=""Axes\' size in inches, for example \\""12,9\\"""")\n    parser.add_argument(\n        ""--relative"",\n        action=""store_true"",\n        help=""Occupy 100%% height for every measurement."",\n    )\n    parser.add_argument(""--tmpdir"", help=""Temporary directory for intermediate files."")\n    parser.add_argument(\n        ""-m"",\n        ""--mode"",\n        dest=""modes"",\n        default=[],\n        action=""append"",\n        choices=[\n            ""burndown-project"",\n            ""burndown-file"",\n            ""burndown-person"",\n            ""overwrites-matrix"",\n            ""ownership"",\n            ""couples-files"",\n            ""couples-people"",\n            ""couples-shotness"",\n            ""shotness"",\n            ""sentiment"",\n            ""devs"",\n            ""devs-efforts"",\n            ""old-vs-new"",\n            ""run-times"",\n            ""languages"",\n            ""devs-parallel"",\n            ""all"",\n        ],\n        help=""What to plot. Can be repeated, e.g. "" ""-m burndown-project -m run-times"",\n    )\n    parser.add_argument(\n        ""--resample"",\n        default=""year"",\n        help=""The way to resample the time series. Possible values are: ""\n        ""\\""month\\"", \\""year\\"", \\""no\\"", \\""raw\\"" and pandas offset aliases (""\n        ""http://pandas.pydata.org/pandas-docs/stable/timeseries.html""\n        ""#offset-aliases)."",\n    )\n    dateutil_url = (\n        ""https://dateutil.readthedocs.io/en/stable/parser.html#dateutil.parser.parse""\n    )\n    parser.add_argument(\n        ""--start-date"",\n        help=""Start date of time-based plots. Any format is accepted which is ""\n        ""supported by %s"" % dateutil_url,\n    )\n    parser.add_argument(\n        ""--end-date"",\n        help=""End date of time-based plots. Any format is accepted which is ""\n        ""supported by %s"" % dateutil_url,\n    )\n    parser.add_argument(\n        ""--disable-projector"",\n        action=""store_true"",\n        help=""Do not run Tensorflow Projector on couples."",\n    )\n    parser.add_argument(\n        ""--max-people"",\n        default=20,\n        type=int,\n        help=""Maximum number of developers in overwrites matrix and people plots."",\n    )\n    parser.add_argument(\n        ""--order-ownership-by-time"",\n        action=""store_true"",\n        help=""Sort developers in the ownership plot according to their first ""\n        ""appearance in the history. The default is sorting by the number of ""\n        ""commits."",\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef main() -> None:\n    args = parse_args()\n    reader = read_input(args)\n    header = reader.get_header()\n    name = reader.get_name()\n\n    burndown_warning = (\n        ""Burndown stats were not collected. Re-run hercules with --burndown.""\n    )\n    burndown_files_warning = (\n        ""Burndown stats for files were not collected. Re-run hercules with ""\n        ""--burndown --burndown-files.""\n    )\n    burndown_people_warning = (\n        ""Burndown stats for people were not collected. Re-run hercules with ""\n        ""--burndown --burndown-people.""\n    )\n    couples_warning = (\n        ""Coupling stats were not collected. Re-run hercules with --couples.""\n    )\n    shotness_warning = (\n        ""Structural hotness stats were not collected. Re-run hercules with ""\n        ""--shotness. Also check --languages - the output may be empty.""\n    )\n    sentiment_warning = (\n        ""Sentiment stats were not collected. Re-run hercules with --sentiment.""\n    )\n    devs_warning = ""Devs stats were not collected. Re-run hercules with --devs.""\n\n    def run_times():\n        rt = reader.get_run_times()\n        pandas = import_pandas()\n        series = pandas.to_timedelta(\n            pandas.Series(rt).sort_values(ascending=False), unit=""s""\n        )\n        df = pandas.concat([series, series / series.sum()], axis=1)\n        df.columns = [""time"", ""ratio""]\n        print(df)\n\n    def project_burndown():\n        try:\n            full_header = header + reader.get_burndown_parameters()\n        except KeyError:\n            print(""project: "" + burndown_warning)\n            return\n        plot_burndown(\n            args,\n            ""project"",\n            *load_burndown(\n                full_header,\n                *reader.get_project_burndown(),\n                resample=args.resample,\n                interpolation_progress=True,\n            ),\n        )\n\n    def files_burndown():\n        try:\n            full_header = header + reader.get_burndown_parameters()\n        except KeyError:\n            print(burndown_warning)\n            return\n        try:\n            plot_many_burndown(args, ""file"", full_header, reader.get_files_burndown())\n        except KeyError:\n            print(""files: "" + burndown_files_warning)\n\n    def people_burndown():\n        try:\n            full_header = header + reader.get_burndown_parameters()\n        except KeyError:\n            print(burndown_warning)\n            return\n        try:\n            plot_many_burndown(\n                args, ""person"", full_header, reader.get_people_burndown()\n            )\n        except KeyError:\n            print(""people: "" + burndown_people_warning)\n\n    def overwrites_matrix():\n        try:\n\n            plot_overwrites_matrix(\n                args,\n                name,\n                *load_overwrites_matrix(\n                    *reader.get_people_interaction(), max_people=args.max_people\n                ),\n            )\n            people, matrix = load_overwrites_matrix(\n                *reader.get_people_interaction(), max_people=1000000, normalize=False\n            )\n            from scipy.sparse import csr_matrix\n\n            matrix = matrix[:, 1:]\n            matrix = numpy.triu(matrix) + numpy.tril(matrix).T\n            matrix = matrix + matrix.T\n            matrix = csr_matrix(matrix)\n            try:\n                write_embeddings(\n                    ""overwrites"",\n                    args.output,\n                    not args.disable_projector,\n                    *train_embeddings(people, matrix, tmpdir=args.tmpdir),\n                )\n            except AttributeError as e:\n                print(\n                    ""Training the embeddings is not possible: %s: %s"",\n                    type(e).__name__,\n                    e,\n                )\n        except KeyError:\n            print(""overwrites_matrix: "" + burndown_people_warning)\n\n    def ownership_burndown():\n        try:\n            full_header = header + reader.get_burndown_parameters()\n        except KeyError:\n            print(burndown_warning)\n            return\n        try:\n            plot_ownership(\n                args,\n                name,\n                *load_ownership(\n                    full_header,\n                    *reader.get_ownership_burndown(),\n                    max_people=args.max_people,\n                    order_by_time=args.order_ownership_by_time,\n                ),\n            )\n        except KeyError:\n            print(""ownership: "" + burndown_people_warning)\n\n    def couples_files():\n        try:\n            write_embeddings(\n                ""files"",\n                args.output,\n                not args.disable_projector,\n                *train_embeddings(*reader.get_files_coocc(), tmpdir=args.tmpdir),\n            )\n        except KeyError:\n            print(couples_warning)\n\n    def couples_people():\n        try:\n            write_embeddings(\n                ""people"",\n                args.output,\n                not args.disable_projector,\n                *train_embeddings(*reader.get_people_coocc(), tmpdir=args.tmpdir),\n            )\n        except KeyError:\n            print(couples_warning)\n\n    def couples_shotness():\n        try:\n            write_embeddings(\n                ""shotness"",\n                args.output,\n                not args.disable_projector,\n                *train_embeddings(*reader.get_shotness_coocc(), tmpdir=args.tmpdir),\n            )\n        except KeyError:\n            print(shotness_warning)\n\n    def shotness():\n        try:\n            data = reader.get_shotness()\n        except KeyError:\n            print(shotness_warning)\n            return\n        show_shotness_stats(data)\n\n    def sentiment():\n        try:\n            data = reader.get_sentiment()\n        except KeyError:\n            print(sentiment_warning)\n            return\n        show_sentiment_stats(\n            args, reader.get_name(), args.resample, reader.get_header()[0], data\n        )\n\n    def devs():\n        try:\n            data = reader.get_devs()\n        except KeyError:\n            print(devs_warning)\n            return\n        show_devs(\n            args,\n            reader.get_name(),\n            *reader.get_header(),\n            *data,\n            max_people=args.max_people,\n        )\n\n    def devs_efforts():\n        try:\n            data = reader.get_devs()\n        except KeyError:\n            print(devs_warning)\n            return\n        show_devs_efforts(\n            args,\n            reader.get_name(),\n            *reader.get_header(),\n            *data,\n            max_people=args.max_people,\n        )\n\n    def old_vs_new():\n        try:\n            data = reader.get_devs()\n        except KeyError:\n            print(devs_warning)\n            return\n        show_old_vs_new(args, reader.get_name(), *reader.get_header(), *data)\n\n    def languages():\n        try:\n            data = reader.get_devs()\n        except KeyError:\n            print(devs_warning)\n            return\n        show_languages(args, reader.get_name(), *reader.get_header(), *data)\n\n    def devs_parallel():\n        try:\n            ownership = reader.get_ownership_burndown()\n        except KeyError:\n            print(burndown_people_warning)\n            return\n        try:\n            couples = reader.get_people_coocc()\n        except KeyError:\n            print(couples_warning)\n            return\n        try:\n            devs = reader.get_devs()\n        except KeyError:\n            print(devs_warning)\n            return\n        show_devs_parallel(\n            args,\n            reader.get_name(),\n            *reader.get_header(),\n            load_devs_parallel(ownership, couples, devs, args.max_people),\n        )\n\n    modes = {\n        ""run-times"": run_times,\n        ""burndown-project"": project_burndown,\n        ""burndown-file"": files_burndown,\n        ""burndown-person"": people_burndown,\n        ""overwrites-matrix"": overwrites_matrix,\n        ""ownership"": ownership_burndown,\n        ""couples-files"": couples_files,\n        ""couples-people"": couples_people,\n        ""couples-shotness"": couples_shotness,\n        ""shotness"": shotness,\n        ""sentiment"": sentiment,\n        ""devs"": devs,\n        ""devs-efforts"": devs_efforts,\n        ""old-vs-new"": old_vs_new,\n        ""languages"": languages,\n        ""devs-parallel"": devs_parallel,\n    }\n\n    if ""all"" in args.modes:\n        all_mode = True\n        args.modes = [\n            ""burndown-project"",\n            ""overwrites-matrix"",\n            ""ownership"",\n            ""couples-files"",\n            ""couples-people"",\n            ""couples-shotness"",\n            ""shotness"",\n            ""devs"",\n            ""devs-efforts"",\n        ]\n    else:\n        all_mode = False\n\n    for mode in args.modes:\n        if mode not in modes:\n            print(""Unknown mode: %s"" % mode)\n            continue\n\n        print(""Running: %s"" % mode)\n        # `args.mode` is required for path determination in the mode functions\n        args.mode = ""all"" if all_mode else mode\n        try:\n            modes[mode]()\n        except ImportError as ie:\n            print(""A module required by the %s mode was not found: %s"" % (mode, ie))\n            if not all_mode:\n                raise\n\n    if web_server.running:\n        secs = int(os.getenv(""COUPLES_SERVER_TIME"", ""60""))\n        print(""Sleeping for %d seconds, safe to Ctrl-C"" % secs)\n        sys.stdout.flush()\n        try:\n            time.sleep(secs)\n        except KeyboardInterrupt:\n            pass\n        web_server.stop()\n'"
python/labours/cors_web_server.py,0,"b'import threading\n\n\nclass CORSWebServer(object):\n    def __init__(self) -> None:\n        self.thread = threading.Thread(target=self.serve)\n        self.server = None\n\n    def serve(self):\n        outer = self\n\n        from http.server import HTTPServer, SimpleHTTPRequestHandler, test\n\n        class ClojureServer(HTTPServer):\n            def __init__(self, *args, **kwargs):\n                HTTPServer.__init__(self, *args, **kwargs)\n                outer.server = self\n\n        class CORSRequestHandler(SimpleHTTPRequestHandler):\n            def end_headers(self):\n                self.send_header(""Access-Control-Allow-Origin"", ""*"")\n                SimpleHTTPRequestHandler.end_headers(self)\n\n        test(CORSRequestHandler, ClojureServer)\n\n    def start(self) -> None:\n        self.thread.start()\n\n    def stop(self) -> None:\n        if self.running:\n            self.server.shutdown()\n            self.thread.join()\n\n    @property\n    def running(self) -> bool:\n        return self.server is not None\n\n\nweb_server = CORSWebServer()\n'"
python/labours/embeddings.py,6,"b'import os\nimport shutil\nimport sys\nimport tempfile\nfrom typing import List, Tuple\n\nimport numpy\nfrom scipy.sparse.csr import csr_matrix\n\nfrom labours.cors_web_server import web_server\n\nIDEAL_SHARD_SIZE = 4096\n\n\ndef train_embeddings(\n    index: List[str],\n    matrix: csr_matrix,\n    tmpdir: None,\n    shard_size: int = IDEAL_SHARD_SIZE,\n) -> Tuple[List[Tuple[str, numpy.int64]], List[numpy.ndarray]]:\n    import tensorflow as tf\n    from labours._vendor import swivel\n\n    assert matrix.shape[0] == matrix.shape[1]\n    assert len(index) <= matrix.shape[0]\n    outlier_threshold = numpy.percentile(matrix.data, 99)\n    matrix.data[matrix.data > outlier_threshold] = outlier_threshold\n    nshards = len(index) // shard_size\n    if nshards * shard_size < len(index):\n        nshards += 1\n        shard_size = len(index) // nshards\n        nshards = len(index) // shard_size\n    remainder = len(index) - nshards * shard_size\n    if remainder > 0:\n        lengths = matrix.indptr[1:] - matrix.indptr[:-1]\n        filtered = sorted(numpy.argsort(lengths)[remainder:])\n    else:\n        filtered = list(range(len(index)))\n    if len(filtered) < matrix.shape[0]:\n        print(""Truncating the sparse matrix..."")\n        matrix = matrix[filtered, :][:, filtered]\n    meta_index = []\n    for i, j in enumerate(filtered):\n        meta_index.append((index[j], matrix[i, i]))\n    index = [mi[0] for mi in meta_index]\n    with tempfile.TemporaryDirectory(\n        prefix=""hercules_labours_"", dir=tmpdir or None\n    ) as tmproot:\n        print(""Writing Swivel metadata..."")\n        vocabulary = ""\\n"".join(index)\n        with open(os.path.join(tmproot, ""row_vocab.txt""), ""w"") as out:\n            out.write(vocabulary)\n        with open(os.path.join(tmproot, ""col_vocab.txt""), ""w"") as out:\n            out.write(vocabulary)\n        del vocabulary\n        bool_sums = matrix.indptr[1:] - matrix.indptr[:-1]\n        bool_sums_str = ""\\n"".join(map(str, bool_sums.tolist()))\n        with open(os.path.join(tmproot, ""row_sums.txt""), ""w"") as out:\n            out.write(bool_sums_str)\n        with open(os.path.join(tmproot, ""col_sums.txt""), ""w"") as out:\n            out.write(bool_sums_str)\n        del bool_sums_str\n        reorder = numpy.argsort(-bool_sums)\n\n        print(""Writing Swivel shards..."")\n        for row in range(nshards):\n            for col in range(nshards):\n\n                def _int64s(xs):\n                    return tf.train.Feature(\n                        int64_list=tf.train.Int64List(value=list(xs))\n                    )\n\n                def _floats(xs):\n                    return tf.train.Feature(\n                        float_list=tf.train.FloatList(value=list(xs))\n                    )\n\n                indices_row = reorder[row::nshards]\n                indices_col = reorder[col::nshards]\n                shard = matrix[indices_row][:, indices_col].tocoo()\n\n                example = tf.train.Example(\n                    features=tf.train.Features(\n                        feature={\n                            ""global_row"": _int64s(indices_row),\n                            ""global_col"": _int64s(indices_col),\n                            ""sparse_local_row"": _int64s(shard.row),\n                            ""sparse_local_col"": _int64s(shard.col),\n                            ""sparse_value"": _floats(shard.data),\n                        }\n                    )\n                )\n\n                with open(\n                    os.path.join(tmproot, ""shard-%03d-%03d.pb"" % (row, col)), ""wb""\n                ) as out:\n                    out.write(example.SerializeToString())\n        print(""Training Swivel model..."")\n        swivel.FLAGS.submatrix_rows = shard_size\n        swivel.FLAGS.submatrix_cols = shard_size\n        if len(meta_index) <= IDEAL_SHARD_SIZE / 16:\n            embedding_size = 50\n            num_epochs = 100000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE:\n            embedding_size = 50\n            num_epochs = 50000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE * 2:\n            embedding_size = 60\n            num_epochs = 10000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE * 4:\n            embedding_size = 70\n            num_epochs = 8000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE * 10:\n            embedding_size = 80\n            num_epochs = 5000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE * 25:\n            embedding_size = 100\n            num_epochs = 1000\n        elif len(meta_index) <= IDEAL_SHARD_SIZE * 100:\n            embedding_size = 200\n            num_epochs = 600\n        else:\n            embedding_size = 300\n            num_epochs = 300\n        if os.getenv(""CI""):\n            # Travis, AppVeyor etc. during the integration tests\n            num_epochs /= 10\n        swivel.FLAGS.embedding_size = embedding_size\n        swivel.FLAGS.input_base_path = tmproot\n        swivel.FLAGS.output_base_path = tmproot\n        swivel.FLAGS.loss_multiplier = 1.0 / shard_size\n        swivel.FLAGS.num_epochs = num_epochs\n        # Tensorflow 1.5 parses sys.argv unconditionally *applause*\n        argv_backup = sys.argv[1:]\n        del sys.argv[1:]\n        swivel.main(None)\n        sys.argv.extend(argv_backup)\n        print(""Reading Swivel embeddings..."")\n        embeddings = []\n        with open(os.path.join(tmproot, ""row_embedding.tsv"")) as frow:\n            with open(os.path.join(tmproot, ""col_embedding.tsv"")) as fcol:\n                for i, (lrow, lcol) in enumerate(zip(frow, fcol)):\n                    prow, pcol = (l.split(""\\t"", 1) for l in (lrow, lcol))\n                    assert prow[0] == pcol[0]\n                    erow, ecol = (\n                        numpy.fromstring(p[1], dtype=numpy.float32, sep=""\\t"")\n                        for p in (prow, pcol)\n                    )\n                    embeddings.append((erow + ecol) / 2)\n    return meta_index, embeddings\n\n\ndef write_embeddings(\n    name: str,\n    output: str,\n    run_server: bool,\n    index: List[Tuple[str, numpy.int64]],\n    embeddings: List[numpy.ndarray],\n) -> None:\n    print(""Writing Tensorflow Projector files..."")\n    if not output:\n        output = ""couples""\n    if output.endswith("".json""):\n        output = os.path.join(output[:-5], ""couples"")\n        run_server = False\n    metaf = ""%s_%s_meta.tsv"" % (output, name)\n    with open(metaf, ""w"") as fout:\n        fout.write(""name\\tcommits\\n"")\n        for pair in index:\n            fout.write(""%s\\t%s\\n"" % pair)\n    print(""Wrote"", metaf)\n    dataf = ""%s_%s_data.tsv"" % (output, name)\n    with open(dataf, ""w"") as fout:\n        for vec in embeddings:\n            fout.write(""\\t"".join(str(v) for v in vec))\n            fout.write(""\\n"")\n    print(""Wrote"", dataf)\n    jsonf = ""%s_%s.json"" % (output, name)\n    with open(jsonf, ""w"") as fout:\n        fout.write(\n            """"""{\n  ""embeddings"": [\n    {\n      ""tensorName"": ""%s %s coupling"",\n      ""tensorShape"": [%s, %s],\n      ""tensorPath"": ""http://0.0.0.0:8000/%s"",\n      ""metadataPath"": ""http://0.0.0.0:8000/%s""\n    }\n  ]\n}\n""""""\n            % (output, name, len(embeddings), len(embeddings[0]), dataf, metaf)\n        )\n    print(""Wrote %s"" % jsonf)\n    if run_server and not web_server.running:\n        web_server.start()\n    url = ""http://projector.tensorflow.org/?config=http://0.0.0.0:8000/"" + jsonf\n    print(url)\n    if run_server:\n        if shutil.which(""xdg-open"") is not None:\n            os.system(""xdg-open "" + url)\n        else:\n            browser = os.getenv(""BROWSER"", """")\n            if browser:\n                os.system(browser + "" "" + url)\n            else:\n                print(""\\t"" + url)\n'"
python/labours/objects.py,0,"b'from collections import defaultdict, namedtuple\n\n\nclass DevDay(\n    namedtuple(""DevDay"", (""Commits"", ""Added"", ""Removed"", ""Changed"", ""Languages""))\n):\n    def add(self, dd: \'DevDay\') -> \'DevDay\':\n        langs = defaultdict(lambda: [0] * 3)\n        for key, val in self.Languages.items():\n            for i in range(3):\n                langs[key][i] += val[i]\n        for key, val in dd.Languages.items():\n            for i in range(3):\n                langs[key][i] += val[i]\n        return DevDay(\n            Commits=self.Commits + dd.Commits,\n            Added=self.Added + dd.Added,\n            Removed=self.Removed + dd.Removed,\n            Changed=self.Changed + dd.Changed,\n            Languages=dict(langs),\n        )\n\n\nclass ParallelDevData:\n    def __init__(self):\n        self.commits_rank = -1\n        self.commits = -1\n        self.lines_rank = -1\n        self.lines = -1\n        self.ownership_rank = -1\n        self.ownership = -1\n        self.couples_index = -1\n        self.couples_cluster = -1\n        self.commit_coocc_index = -1\n        self.commit_coocc_cluster = -1\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    def __repr__(self):\n        return str(self)\n'"
python/labours/pb_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: pb.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'pb.proto\',\n  package=\'\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x08pb.proto\\""\\x81\\x02\\n\\x08Metadata\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x0c\\n\\x04hash\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nrepository\\x18\\x03 \\x01(\\t\\x12\\x17\\n\\x0f\\x62\\x65gin_unix_time\\x18\\x04 \\x01(\\x03\\x12\\x15\\n\\rend_unix_time\\x18\\x05 \\x01(\\x03\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x06 \\x01(\\x05\\x12\\x10\\n\\x08run_time\\x18\\x07 \\x01(\\x03\\x12\\x38\\n\\x11run_time_per_item\\x18\\x08 \\x03(\\x0b\\x32\\x1d.Metadata.RunTimePerItemEntry\\x1a\\x35\\n\\x13RunTimePerItemEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01:\\x02\\x38\\x01\\""*\\n\\x17\\x42urndownSparseMatrixRow\\x12\\x0f\\n\\x07\\x63olumns\\x18\\x01 \\x03(\\r\\""\\x7f\\n\\x14\\x42urndownSparseMatrix\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x16\\n\\x0enumber_of_rows\\x18\\x02 \\x01(\\x05\\x12\\x19\\n\\x11number_of_columns\\x18\\x03 \\x01(\\x05\\x12&\\n\\x04rows\\x18\\x04 \\x03(\\x0b\\x32\\x18.BurndownSparseMatrixRow\\""i\\n\\x0e\\x46ilesOwnership\\x12)\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32\\x1a.FilesOwnership.ValueEntry\\x1a,\\n\\nValueEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\""\\xaa\\x02\\n\\x17\\x42urndownAnalysisResults\\x12\\x13\\n\\x0bgranularity\\x18\\x01 \\x01(\\x05\\x12\\x10\\n\\x08sampling\\x18\\x02 \\x01(\\x05\\x12&\\n\\x07project\\x18\\x03 \\x01(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12$\\n\\x05\\x66iles\\x18\\x04 \\x03(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12%\\n\\x06people\\x18\\x05 \\x03(\\x0b\\x32\\x15.BurndownSparseMatrix\\x12\\x36\\n\\x12people_interaction\\x18\\x06 \\x01(\\x0b\\x32\\x1a.CompressedSparseRowMatrix\\x12(\\n\\x0f\\x66iles_ownership\\x18\\x07 \\x03(\\x0b\\x32\\x0f.FilesOwnership\\x12\\x11\\n\\ttick_size\\x18\\x08 \\x01(\\x03\\""}\\n\\x19\\x43ompressedSparseRowMatrix\\x12\\x16\\n\\x0enumber_of_rows\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x11number_of_columns\\x18\\x02 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x03 \\x03(\\x03\\x12\\x0f\\n\\x07indices\\x18\\x04 \\x03(\\x05\\x12\\x0e\\n\\x06indptr\\x18\\x05 \\x03(\\x03\\""D\\n\\x07\\x43ouples\\x12\\r\\n\\x05index\\x18\\x01 \\x03(\\t\\x12*\\n\\x06matrix\\x18\\x02 \\x01(\\x0b\\x32\\x1a.CompressedSparseRowMatrix\\""\\x1d\\n\\x0cTouchedFiles\\x12\\r\\n\\x05\\x66iles\\x18\\x01 \\x03(\\x05\\""\\x94\\x01\\n\\x16\\x43ouplesAnalysisResults\\x12\\x1e\\n\\x0c\\x66ile_couples\\x18\\x06 \\x01(\\x0b\\x32\\x08.Couples\\x12 \\n\\x0epeople_couples\\x18\\x07 \\x01(\\x0b\\x32\\x08.Couples\\x12#\\n\\x0cpeople_files\\x18\\x08 \\x03(\\x0b\\x32\\r.TouchedFiles\\x12\\x13\\n\\x0b\\x66iles_lines\\x18\\t \\x03(\\x05\\""o\\n\\nUASTChange\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nsrc_before\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tsrc_after\\x18\\x03 \\x01(\\t\\x12\\x13\\n\\x0buast_before\\x18\\x04 \\x01(\\t\\x12\\x12\\n\\nuast_after\\x18\\x05 \\x01(\\t\\""7\\n\\x17UASTChangesSaverResults\\x12\\x1c\\n\\x07\\x63hanges\\x18\\x01 \\x03(\\x0b\\x32\\x0b.UASTChange\\""\\x9c\\x01\\n\\x0eShotnessRecord\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04\\x66ile\\x18\\x03 \\x01(\\t\\x12/\\n\\x08\\x63ounters\\x18\\x04 \\x03(\\x0b\\x32\\x1d.ShotnessRecord.CountersEntry\\x1a/\\n\\rCountersEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x05:\\x02\\x38\\x01\\"";\\n\\x17ShotnessAnalysisResults\\x12 \\n\\x07records\\x18\\x01 \\x03(\\x0b\\x32\\x0f.ShotnessRecord\\""\\xa9\\x01\\n\\x0b\\x46ileHistory\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x01 \\x03(\\t\\x12\\x42\\n\\x14\\x63hanges_by_developer\\x18\\x02 \\x03(\\x0b\\x32$.FileHistory.ChangesByDeveloperEntry\\x1a\\x45\\n\\x17\\x43hangesByDeveloperEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats:\\x02\\x38\\x01\\""\\x8b\\x01\\n\\x18\\x46ileHistoryResultMessage\\x12\\x33\\n\\x05\\x66iles\\x18\\x01 \\x03(\\x0b\\x32$.FileHistoryResultMessage.FilesEntry\\x1a:\\n\\nFilesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x0c.FileHistory:\\x02\\x38\\x01\\""<\\n\\tLineStats\\x12\\r\\n\\x05\\x61\\x64\\x64\\x65\\x64\\x18\\x01 \\x01(\\x05\\x12\\x0f\\n\\x07removed\\x18\\x02 \\x01(\\x05\\x12\\x0f\\n\\x07\\x63hanged\\x18\\x03 \\x01(\\x05\\""\\x9f\\x01\\n\\x07\\x44\\x65vTick\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05stats\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats\\x12*\\n\\tlanguages\\x18\\x03 \\x03(\\x0b\\x32\\x17.DevTick.LanguagesEntry\\x1a<\\n\\x0eLanguagesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.LineStats:\\x02\\x38\\x01\\""d\\n\\x08TickDevs\\x12!\\n\\x04\\x64\\x65vs\\x18\\x01 \\x03(\\x0b\\x32\\x13.TickDevs.DevsEntry\\x1a\\x35\\n\\tDevsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x17\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x08.DevTick:\\x02\\x38\\x01\\""\\xa4\\x01\\n\\x13\\x44\\x65vsAnalysisResults\\x12.\\n\\x05ticks\\x18\\x01 \\x03(\\x0b\\x32\\x1f.DevsAnalysisResults.TicksEntry\\x12\\x11\\n\\tdev_index\\x18\\x02 \\x03(\\t\\x12\\x11\\n\\ttick_size\\x18\\x08 \\x01(\\x03\\x1a\\x37\\n\\nTicksEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x18\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\t.TickDevs:\\x02\\x38\\x01\\""=\\n\\tSentiment\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\x12\\x10\\n\\x08\\x63omments\\x18\\x02 \\x03(\\t\\x12\\x0f\\n\\x07\\x63ommits\\x18\\x03 \\x03(\\t\\""\\xa7\\x01\\n\\x17\\x43ommentSentimentResults\\x12H\\n\\x11sentiment_by_tick\\x18\\x01 \\x03(\\x0b\\x32-.CommentSentimentResults.SentimentByTickEntry\\x1a\\x42\\n\\x14SentimentByTickEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\x19\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\n.Sentiment:\\x02\\x38\\x01\\""G\\n\\nCommitFile\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08language\\x18\\x03 \\x01(\\t\\x12\\x19\\n\\x05stats\\x18\\x04 \\x01(\\x0b\\x32\\n.LineStats\\""Z\\n\\x06\\x43ommit\\x12\\x0c\\n\\x04hash\\x18\\x01 \\x01(\\t\\x12\\x16\\n\\x0ewhen_unix_time\\x18\\x02 \\x01(\\x03\\x12\\x0e\\n\\x06\\x61uthor\\x18\\x03 \\x01(\\x05\\x12\\x1a\\n\\x05\\x66iles\\x18\\x04 \\x03(\\x0b\\x32\\x0b.CommitFile\\""H\\n\\x16\\x43ommitsAnalysisResults\\x12\\x18\\n\\x07\\x63ommits\\x18\\x01 \\x03(\\x0b\\x32\\x07.Commit\\x12\\x14\\n\\x0c\\x61uthor_index\\x18\\x02 \\x03(\\t\\""R\\n\\x04Typo\\x12\\r\\n\\x05wrong\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63orrect\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x63ommit\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04\\x66ile\\x18\\x04 \\x01(\\t\\x12\\x0c\\n\\x04line\\x18\\x05 \\x01(\\x05\\""$\\n\\x0cTyposDataset\\x12\\x14\\n\\x05typos\\x18\\x01 \\x03(\\x0b\\x32\\x05.Typo\\""l\\n\\x0eImportsPerTick\\x12+\\n\\x06\\x63ounts\\x18\\x01 \\x03(\\x0b\\x32\\x1b.ImportsPerTick.CountsEntry\\x1a-\\n\\x0b\\x43ountsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x03:\\x02\\x38\\x01\\""\\x82\\x01\\n\\x12ImportsPerLanguage\\x12-\\n\\x05ticks\\x18\\x01 \\x03(\\x0b\\x32\\x1e.ImportsPerLanguage.TicksEntry\\x1a=\\n\\nTicksEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\x1e\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x0f.ImportsPerTick:\\x02\\x38\\x01\\""\\x94\\x01\\n\\x13ImportsPerDeveloper\\x12\\x36\\n\\tlanguages\\x18\\x01 \\x03(\\x0b\\x32#.ImportsPerDeveloper.LanguagesEntry\\x1a\\x45\\n\\x0eLanguagesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\""\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x13.ImportsPerLanguage:\\x02\\x38\\x01\\""l\\n\\x1aImportsPerDeveloperResults\\x12%\\n\\x07imports\\x18\\x01 \\x03(\\x0b\\x32\\x14.ImportsPerDeveloper\\x12\\x14\\n\\x0c\\x61uthor_index\\x18\\x02 \\x03(\\t\\x12\\x11\\n\\ttick_size\\x18\\x03 \\x01(\\x03\\""\\x8f\\x01\\n\\x0f\\x41nalysisResults\\x12\\x19\\n\\x06header\\x18\\x01 \\x01(\\x0b\\x32\\t.Metadata\\x12\\x30\\n\\x08\\x63ontents\\x18\\x02 \\x03(\\x0b\\x32\\x1e.AnalysisResults.ContentsEntry\\x1a/\\n\\rContentsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x0c:\\x02\\x38\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_METADATA_RUNTIMEPERITEMENTRY = _descriptor.Descriptor(\n  name=\'RunTimePerItemEntry\',\n  full_name=\'Metadata.RunTimePerItemEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'Metadata.RunTimePerItemEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'Metadata.RunTimePerItemEntry.value\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=217,\n  serialized_end=270,\n)\n\n_METADATA = _descriptor.Descriptor(\n  name=\'Metadata\',\n  full_name=\'Metadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'Metadata.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hash\', full_name=\'Metadata.hash\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'repository\', full_name=\'Metadata.repository\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'begin_unix_time\', full_name=\'Metadata.begin_unix_time\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end_unix_time\', full_name=\'Metadata.end_unix_time\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'Metadata.commits\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'run_time\', full_name=\'Metadata.run_time\', index=6,\n      number=7, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'run_time_per_item\', full_name=\'Metadata.run_time_per_item\', index=7,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_METADATA_RUNTIMEPERITEMENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=13,\n  serialized_end=270,\n)\n\n\n_BURNDOWNSPARSEMATRIXROW = _descriptor.Descriptor(\n  name=\'BurndownSparseMatrixRow\',\n  full_name=\'BurndownSparseMatrixRow\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'columns\', full_name=\'BurndownSparseMatrixRow.columns\', index=0,\n      number=1, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=272,\n  serialized_end=314,\n)\n\n\n_BURNDOWNSPARSEMATRIX = _descriptor.Descriptor(\n  name=\'BurndownSparseMatrix\',\n  full_name=\'BurndownSparseMatrix\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'BurndownSparseMatrix.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_rows\', full_name=\'BurndownSparseMatrix.number_of_rows\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_columns\', full_name=\'BurndownSparseMatrix.number_of_columns\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'rows\', full_name=\'BurndownSparseMatrix.rows\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=316,\n  serialized_end=443,\n)\n\n\n_FILESOWNERSHIP_VALUEENTRY = _descriptor.Descriptor(\n  name=\'ValueEntry\',\n  full_name=\'FilesOwnership.ValueEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FilesOwnership.ValueEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FilesOwnership.ValueEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=506,\n  serialized_end=550,\n)\n\n_FILESOWNERSHIP = _descriptor.Descriptor(\n  name=\'FilesOwnership\',\n  full_name=\'FilesOwnership\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FilesOwnership.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILESOWNERSHIP_VALUEENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=445,\n  serialized_end=550,\n)\n\n\n_BURNDOWNANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'BurndownAnalysisResults\',\n  full_name=\'BurndownAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'granularity\', full_name=\'BurndownAnalysisResults.granularity\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sampling\', full_name=\'BurndownAnalysisResults.sampling\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'project\', full_name=\'BurndownAnalysisResults.project\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'BurndownAnalysisResults.files\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people\', full_name=\'BurndownAnalysisResults.people\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_interaction\', full_name=\'BurndownAnalysisResults.people_interaction\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files_ownership\', full_name=\'BurndownAnalysisResults.files_ownership\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tick_size\', full_name=\'BurndownAnalysisResults.tick_size\', index=7,\n      number=8, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=553,\n  serialized_end=851,\n)\n\n\n_COMPRESSEDSPARSEROWMATRIX = _descriptor.Descriptor(\n  name=\'CompressedSparseRowMatrix\',\n  full_name=\'CompressedSparseRowMatrix\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'number_of_rows\', full_name=\'CompressedSparseRowMatrix.number_of_rows\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number_of_columns\', full_name=\'CompressedSparseRowMatrix.number_of_columns\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'CompressedSparseRowMatrix.data\', index=2,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'indices\', full_name=\'CompressedSparseRowMatrix.indices\', index=3,\n      number=4, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'indptr\', full_name=\'CompressedSparseRowMatrix.indptr\', index=4,\n      number=5, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=853,\n  serialized_end=978,\n)\n\n\n_COUPLES = _descriptor.Descriptor(\n  name=\'Couples\',\n  full_name=\'Couples\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'Couples.index\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'matrix\', full_name=\'Couples.matrix\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=980,\n  serialized_end=1048,\n)\n\n\n_TOUCHEDFILES = _descriptor.Descriptor(\n  name=\'TouchedFiles\',\n  full_name=\'TouchedFiles\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'TouchedFiles.files\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1050,\n  serialized_end=1079,\n)\n\n\n_COUPLESANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'CouplesAnalysisResults\',\n  full_name=\'CouplesAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_couples\', full_name=\'CouplesAnalysisResults.file_couples\', index=0,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_couples\', full_name=\'CouplesAnalysisResults.people_couples\', index=1,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'people_files\', full_name=\'CouplesAnalysisResults.people_files\', index=2,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files_lines\', full_name=\'CouplesAnalysisResults.files_lines\', index=3,\n      number=9, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1082,\n  serialized_end=1230,\n)\n\n\n_UASTCHANGE = _descriptor.Descriptor(\n  name=\'UASTChange\',\n  full_name=\'UASTChange\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_name\', full_name=\'UASTChange.file_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'src_before\', full_name=\'UASTChange.src_before\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'src_after\', full_name=\'UASTChange.src_after\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'uast_before\', full_name=\'UASTChange.uast_before\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'uast_after\', full_name=\'UASTChange.uast_after\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1232,\n  serialized_end=1343,\n)\n\n\n_UASTCHANGESSAVERRESULTS = _descriptor.Descriptor(\n  name=\'UASTChangesSaverResults\',\n  full_name=\'UASTChangesSaverResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'changes\', full_name=\'UASTChangesSaverResults.changes\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1345,\n  serialized_end=1400,\n)\n\n\n_SHOTNESSRECORD_COUNTERSENTRY = _descriptor.Descriptor(\n  name=\'CountersEntry\',\n  full_name=\'ShotnessRecord.CountersEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'ShotnessRecord.CountersEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'ShotnessRecord.CountersEntry.value\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1512,\n  serialized_end=1559,\n)\n\n_SHOTNESSRECORD = _descriptor.Descriptor(\n  name=\'ShotnessRecord\',\n  full_name=\'ShotnessRecord\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'ShotnessRecord.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'ShotnessRecord.name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'file\', full_name=\'ShotnessRecord.file\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'counters\', full_name=\'ShotnessRecord.counters\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SHOTNESSRECORD_COUNTERSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1403,\n  serialized_end=1559,\n)\n\n\n_SHOTNESSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'ShotnessAnalysisResults\',\n  full_name=\'ShotnessAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'records\', full_name=\'ShotnessAnalysisResults.records\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1561,\n  serialized_end=1620,\n)\n\n\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY = _descriptor.Descriptor(\n  name=\'ChangesByDeveloperEntry\',\n  full_name=\'FileHistory.ChangesByDeveloperEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FileHistory.ChangesByDeveloperEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FileHistory.ChangesByDeveloperEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1723,\n  serialized_end=1792,\n)\n\n_FILEHISTORY = _descriptor.Descriptor(\n  name=\'FileHistory\',\n  full_name=\'FileHistory\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'FileHistory.commits\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'changes_by_developer\', full_name=\'FileHistory.changes_by_developer\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILEHISTORY_CHANGESBYDEVELOPERENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1623,\n  serialized_end=1792,\n)\n\n\n_FILEHISTORYRESULTMESSAGE_FILESENTRY = _descriptor.Descriptor(\n  name=\'FilesEntry\',\n  full_name=\'FileHistoryResultMessage.FilesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'FileHistoryResultMessage.FilesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'FileHistoryResultMessage.FilesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1876,\n  serialized_end=1934,\n)\n\n_FILEHISTORYRESULTMESSAGE = _descriptor.Descriptor(\n  name=\'FileHistoryResultMessage\',\n  full_name=\'FileHistoryResultMessage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'FileHistoryResultMessage.files\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_FILEHISTORYRESULTMESSAGE_FILESENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1795,\n  serialized_end=1934,\n)\n\n\n_LINESTATS = _descriptor.Descriptor(\n  name=\'LineStats\',\n  full_name=\'LineStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'added\', full_name=\'LineStats.added\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'removed\', full_name=\'LineStats.removed\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'changed\', full_name=\'LineStats.changed\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1936,\n  serialized_end=1996,\n)\n\n\n_DEVTICK_LANGUAGESENTRY = _descriptor.Descriptor(\n  name=\'LanguagesEntry\',\n  full_name=\'DevTick.LanguagesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'DevTick.LanguagesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'DevTick.LanguagesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2098,\n  serialized_end=2158,\n)\n\n_DEVTICK = _descriptor.Descriptor(\n  name=\'DevTick\',\n  full_name=\'DevTick\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'DevTick.commits\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stats\', full_name=\'DevTick.stats\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'languages\', full_name=\'DevTick.languages\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEVTICK_LANGUAGESENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1999,\n  serialized_end=2158,\n)\n\n\n_TICKDEVS_DEVSENTRY = _descriptor.Descriptor(\n  name=\'DevsEntry\',\n  full_name=\'TickDevs.DevsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'TickDevs.DevsEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'TickDevs.DevsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2207,\n  serialized_end=2260,\n)\n\n_TICKDEVS = _descriptor.Descriptor(\n  name=\'TickDevs\',\n  full_name=\'TickDevs\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'devs\', full_name=\'TickDevs.devs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TICKDEVS_DEVSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2160,\n  serialized_end=2260,\n)\n\n\n_DEVSANALYSISRESULTS_TICKSENTRY = _descriptor.Descriptor(\n  name=\'TicksEntry\',\n  full_name=\'DevsAnalysisResults.TicksEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'DevsAnalysisResults.TicksEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'DevsAnalysisResults.TicksEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2372,\n  serialized_end=2427,\n)\n\n_DEVSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'DevsAnalysisResults\',\n  full_name=\'DevsAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ticks\', full_name=\'DevsAnalysisResults.ticks\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dev_index\', full_name=\'DevsAnalysisResults.dev_index\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tick_size\', full_name=\'DevsAnalysisResults.tick_size\', index=2,\n      number=8, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEVSANALYSISRESULTS_TICKSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2263,\n  serialized_end=2427,\n)\n\n\n_SENTIMENT = _descriptor.Descriptor(\n  name=\'Sentiment\',\n  full_name=\'Sentiment\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'Sentiment.value\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'comments\', full_name=\'Sentiment.comments\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'Sentiment.commits\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2429,\n  serialized_end=2490,\n)\n\n\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY = _descriptor.Descriptor(\n  name=\'SentimentByTickEntry\',\n  full_name=\'CommentSentimentResults.SentimentByTickEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'CommentSentimentResults.SentimentByTickEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'CommentSentimentResults.SentimentByTickEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2594,\n  serialized_end=2660,\n)\n\n_COMMENTSENTIMENTRESULTS = _descriptor.Descriptor(\n  name=\'CommentSentimentResults\',\n  full_name=\'CommentSentimentResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentiment_by_tick\', full_name=\'CommentSentimentResults.sentiment_by_tick\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2493,\n  serialized_end=2660,\n)\n\n\n_COMMITFILE = _descriptor.Descriptor(\n  name=\'CommitFile\',\n  full_name=\'CommitFile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'CommitFile.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'language\', full_name=\'CommitFile.language\', index=1,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'stats\', full_name=\'CommitFile.stats\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2662,\n  serialized_end=2733,\n)\n\n\n_COMMIT = _descriptor.Descriptor(\n  name=\'Commit\',\n  full_name=\'Commit\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'hash\', full_name=\'Commit.hash\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'when_unix_time\', full_name=\'Commit.when_unix_time\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author\', full_name=\'Commit.author\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'files\', full_name=\'Commit.files\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2735,\n  serialized_end=2825,\n)\n\n\n_COMMITSANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'CommitsAnalysisResults\',\n  full_name=\'CommitsAnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'commits\', full_name=\'CommitsAnalysisResults.commits\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author_index\', full_name=\'CommitsAnalysisResults.author_index\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2827,\n  serialized_end=2899,\n)\n\n\n_TYPO = _descriptor.Descriptor(\n  name=\'Typo\',\n  full_name=\'Typo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'wrong\', full_name=\'Typo.wrong\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'correct\', full_name=\'Typo.correct\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'commit\', full_name=\'Typo.commit\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'file\', full_name=\'Typo.file\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'line\', full_name=\'Typo.line\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2901,\n  serialized_end=2983,\n)\n\n\n_TYPOSDATASET = _descriptor.Descriptor(\n  name=\'TyposDataset\',\n  full_name=\'TyposDataset\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'typos\', full_name=\'TyposDataset.typos\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2985,\n  serialized_end=3021,\n)\n\n\n_IMPORTSPERTICK_COUNTSENTRY = _descriptor.Descriptor(\n  name=\'CountsEntry\',\n  full_name=\'ImportsPerTick.CountsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'ImportsPerTick.CountsEntry.key\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'ImportsPerTick.CountsEntry.value\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3086,\n  serialized_end=3131,\n)\n\n_IMPORTSPERTICK = _descriptor.Descriptor(\n  name=\'ImportsPerTick\',\n  full_name=\'ImportsPerTick\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'counts\', full_name=\'ImportsPerTick.counts\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_IMPORTSPERTICK_COUNTSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3023,\n  serialized_end=3131,\n)\n\n\n_IMPORTSPERLANGUAGE_TICKSENTRY = _descriptor.Descriptor(\n  name=\'TicksEntry\',\n  full_name=\'ImportsPerLanguage.TicksEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'ImportsPerLanguage.TicksEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'ImportsPerLanguage.TicksEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3203,\n  serialized_end=3264,\n)\n\n_IMPORTSPERLANGUAGE = _descriptor.Descriptor(\n  name=\'ImportsPerLanguage\',\n  full_name=\'ImportsPerLanguage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ticks\', full_name=\'ImportsPerLanguage.ticks\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_IMPORTSPERLANGUAGE_TICKSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3134,\n  serialized_end=3264,\n)\n\n\n_IMPORTSPERDEVELOPER_LANGUAGESENTRY = _descriptor.Descriptor(\n  name=\'LanguagesEntry\',\n  full_name=\'ImportsPerDeveloper.LanguagesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'ImportsPerDeveloper.LanguagesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'ImportsPerDeveloper.LanguagesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3346,\n  serialized_end=3415,\n)\n\n_IMPORTSPERDEVELOPER = _descriptor.Descriptor(\n  name=\'ImportsPerDeveloper\',\n  full_name=\'ImportsPerDeveloper\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'languages\', full_name=\'ImportsPerDeveloper.languages\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_IMPORTSPERDEVELOPER_LANGUAGESENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3267,\n  serialized_end=3415,\n)\n\n\n_IMPORTSPERDEVELOPERRESULTS = _descriptor.Descriptor(\n  name=\'ImportsPerDeveloperResults\',\n  full_name=\'ImportsPerDeveloperResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'imports\', full_name=\'ImportsPerDeveloperResults.imports\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author_index\', full_name=\'ImportsPerDeveloperResults.author_index\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tick_size\', full_name=\'ImportsPerDeveloperResults.tick_size\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3417,\n  serialized_end=3525,\n)\n\n\n_ANALYSISRESULTS_CONTENTSENTRY = _descriptor.Descriptor(\n  name=\'ContentsEntry\',\n  full_name=\'AnalysisResults.ContentsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'AnalysisResults.ContentsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'AnalysisResults.ContentsEntry.value\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3624,\n  serialized_end=3671,\n)\n\n_ANALYSISRESULTS = _descriptor.Descriptor(\n  name=\'AnalysisResults\',\n  full_name=\'AnalysisResults\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'header\', full_name=\'AnalysisResults.header\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'contents\', full_name=\'AnalysisResults.contents\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ANALYSISRESULTS_CONTENTSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3528,\n  serialized_end=3671,\n)\n\n_METADATA_RUNTIMEPERITEMENTRY.containing_type = _METADATA\n_METADATA.fields_by_name[\'run_time_per_item\'].message_type = _METADATA_RUNTIMEPERITEMENTRY\n_BURNDOWNSPARSEMATRIX.fields_by_name[\'rows\'].message_type = _BURNDOWNSPARSEMATRIXROW\n_FILESOWNERSHIP_VALUEENTRY.containing_type = _FILESOWNERSHIP\n_FILESOWNERSHIP.fields_by_name[\'value\'].message_type = _FILESOWNERSHIP_VALUEENTRY\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'project\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'files\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'people\'].message_type = _BURNDOWNSPARSEMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'people_interaction\'].message_type = _COMPRESSEDSPARSEROWMATRIX\n_BURNDOWNANALYSISRESULTS.fields_by_name[\'files_ownership\'].message_type = _FILESOWNERSHIP\n_COUPLES.fields_by_name[\'matrix\'].message_type = _COMPRESSEDSPARSEROWMATRIX\n_COUPLESANALYSISRESULTS.fields_by_name[\'file_couples\'].message_type = _COUPLES\n_COUPLESANALYSISRESULTS.fields_by_name[\'people_couples\'].message_type = _COUPLES\n_COUPLESANALYSISRESULTS.fields_by_name[\'people_files\'].message_type = _TOUCHEDFILES\n_UASTCHANGESSAVERRESULTS.fields_by_name[\'changes\'].message_type = _UASTCHANGE\n_SHOTNESSRECORD_COUNTERSENTRY.containing_type = _SHOTNESSRECORD\n_SHOTNESSRECORD.fields_by_name[\'counters\'].message_type = _SHOTNESSRECORD_COUNTERSENTRY\n_SHOTNESSANALYSISRESULTS.fields_by_name[\'records\'].message_type = _SHOTNESSRECORD\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY.fields_by_name[\'value\'].message_type = _LINESTATS\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY.containing_type = _FILEHISTORY\n_FILEHISTORY.fields_by_name[\'changes_by_developer\'].message_type = _FILEHISTORY_CHANGESBYDEVELOPERENTRY\n_FILEHISTORYRESULTMESSAGE_FILESENTRY.fields_by_name[\'value\'].message_type = _FILEHISTORY\n_FILEHISTORYRESULTMESSAGE_FILESENTRY.containing_type = _FILEHISTORYRESULTMESSAGE\n_FILEHISTORYRESULTMESSAGE.fields_by_name[\'files\'].message_type = _FILEHISTORYRESULTMESSAGE_FILESENTRY\n_DEVTICK_LANGUAGESENTRY.fields_by_name[\'value\'].message_type = _LINESTATS\n_DEVTICK_LANGUAGESENTRY.containing_type = _DEVTICK\n_DEVTICK.fields_by_name[\'stats\'].message_type = _LINESTATS\n_DEVTICK.fields_by_name[\'languages\'].message_type = _DEVTICK_LANGUAGESENTRY\n_TICKDEVS_DEVSENTRY.fields_by_name[\'value\'].message_type = _DEVTICK\n_TICKDEVS_DEVSENTRY.containing_type = _TICKDEVS\n_TICKDEVS.fields_by_name[\'devs\'].message_type = _TICKDEVS_DEVSENTRY\n_DEVSANALYSISRESULTS_TICKSENTRY.fields_by_name[\'value\'].message_type = _TICKDEVS\n_DEVSANALYSISRESULTS_TICKSENTRY.containing_type = _DEVSANALYSISRESULTS\n_DEVSANALYSISRESULTS.fields_by_name[\'ticks\'].message_type = _DEVSANALYSISRESULTS_TICKSENTRY\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY.fields_by_name[\'value\'].message_type = _SENTIMENT\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY.containing_type = _COMMENTSENTIMENTRESULTS\n_COMMENTSENTIMENTRESULTS.fields_by_name[\'sentiment_by_tick\'].message_type = _COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY\n_COMMITFILE.fields_by_name[\'stats\'].message_type = _LINESTATS\n_COMMIT.fields_by_name[\'files\'].message_type = _COMMITFILE\n_COMMITSANALYSISRESULTS.fields_by_name[\'commits\'].message_type = _COMMIT\n_TYPOSDATASET.fields_by_name[\'typos\'].message_type = _TYPO\n_IMPORTSPERTICK_COUNTSENTRY.containing_type = _IMPORTSPERTICK\n_IMPORTSPERTICK.fields_by_name[\'counts\'].message_type = _IMPORTSPERTICK_COUNTSENTRY\n_IMPORTSPERLANGUAGE_TICKSENTRY.fields_by_name[\'value\'].message_type = _IMPORTSPERTICK\n_IMPORTSPERLANGUAGE_TICKSENTRY.containing_type = _IMPORTSPERLANGUAGE\n_IMPORTSPERLANGUAGE.fields_by_name[\'ticks\'].message_type = _IMPORTSPERLANGUAGE_TICKSENTRY\n_IMPORTSPERDEVELOPER_LANGUAGESENTRY.fields_by_name[\'value\'].message_type = _IMPORTSPERLANGUAGE\n_IMPORTSPERDEVELOPER_LANGUAGESENTRY.containing_type = _IMPORTSPERDEVELOPER\n_IMPORTSPERDEVELOPER.fields_by_name[\'languages\'].message_type = _IMPORTSPERDEVELOPER_LANGUAGESENTRY\n_IMPORTSPERDEVELOPERRESULTS.fields_by_name[\'imports\'].message_type = _IMPORTSPERDEVELOPER\n_ANALYSISRESULTS_CONTENTSENTRY.containing_type = _ANALYSISRESULTS\n_ANALYSISRESULTS.fields_by_name[\'header\'].message_type = _METADATA\n_ANALYSISRESULTS.fields_by_name[\'contents\'].message_type = _ANALYSISRESULTS_CONTENTSENTRY\nDESCRIPTOR.message_types_by_name[\'Metadata\'] = _METADATA\nDESCRIPTOR.message_types_by_name[\'BurndownSparseMatrixRow\'] = _BURNDOWNSPARSEMATRIXROW\nDESCRIPTOR.message_types_by_name[\'BurndownSparseMatrix\'] = _BURNDOWNSPARSEMATRIX\nDESCRIPTOR.message_types_by_name[\'FilesOwnership\'] = _FILESOWNERSHIP\nDESCRIPTOR.message_types_by_name[\'BurndownAnalysisResults\'] = _BURNDOWNANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'CompressedSparseRowMatrix\'] = _COMPRESSEDSPARSEROWMATRIX\nDESCRIPTOR.message_types_by_name[\'Couples\'] = _COUPLES\nDESCRIPTOR.message_types_by_name[\'TouchedFiles\'] = _TOUCHEDFILES\nDESCRIPTOR.message_types_by_name[\'CouplesAnalysisResults\'] = _COUPLESANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'UASTChange\'] = _UASTCHANGE\nDESCRIPTOR.message_types_by_name[\'UASTChangesSaverResults\'] = _UASTCHANGESSAVERRESULTS\nDESCRIPTOR.message_types_by_name[\'ShotnessRecord\'] = _SHOTNESSRECORD\nDESCRIPTOR.message_types_by_name[\'ShotnessAnalysisResults\'] = _SHOTNESSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'FileHistory\'] = _FILEHISTORY\nDESCRIPTOR.message_types_by_name[\'FileHistoryResultMessage\'] = _FILEHISTORYRESULTMESSAGE\nDESCRIPTOR.message_types_by_name[\'LineStats\'] = _LINESTATS\nDESCRIPTOR.message_types_by_name[\'DevTick\'] = _DEVTICK\nDESCRIPTOR.message_types_by_name[\'TickDevs\'] = _TICKDEVS\nDESCRIPTOR.message_types_by_name[\'DevsAnalysisResults\'] = _DEVSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'Sentiment\'] = _SENTIMENT\nDESCRIPTOR.message_types_by_name[\'CommentSentimentResults\'] = _COMMENTSENTIMENTRESULTS\nDESCRIPTOR.message_types_by_name[\'CommitFile\'] = _COMMITFILE\nDESCRIPTOR.message_types_by_name[\'Commit\'] = _COMMIT\nDESCRIPTOR.message_types_by_name[\'CommitsAnalysisResults\'] = _COMMITSANALYSISRESULTS\nDESCRIPTOR.message_types_by_name[\'Typo\'] = _TYPO\nDESCRIPTOR.message_types_by_name[\'TyposDataset\'] = _TYPOSDATASET\nDESCRIPTOR.message_types_by_name[\'ImportsPerTick\'] = _IMPORTSPERTICK\nDESCRIPTOR.message_types_by_name[\'ImportsPerLanguage\'] = _IMPORTSPERLANGUAGE\nDESCRIPTOR.message_types_by_name[\'ImportsPerDeveloper\'] = _IMPORTSPERDEVELOPER\nDESCRIPTOR.message_types_by_name[\'ImportsPerDeveloperResults\'] = _IMPORTSPERDEVELOPERRESULTS\nDESCRIPTOR.message_types_by_name[\'AnalysisResults\'] = _ANALYSISRESULTS\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMetadata = _reflection.GeneratedProtocolMessageType(\'Metadata\', (_message.Message,), dict(\n\n  RunTimePerItemEntry = _reflection.GeneratedProtocolMessageType(\'RunTimePerItemEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _METADATA_RUNTIMEPERITEMENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:Metadata.RunTimePerItemEntry)\n    ))\n  ,\n  DESCRIPTOR = _METADATA,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Metadata)\n  ))\n_sym_db.RegisterMessage(Metadata)\n_sym_db.RegisterMessage(Metadata.RunTimePerItemEntry)\n\nBurndownSparseMatrixRow = _reflection.GeneratedProtocolMessageType(\'BurndownSparseMatrixRow\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNSPARSEMATRIXROW,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownSparseMatrixRow)\n  ))\n_sym_db.RegisterMessage(BurndownSparseMatrixRow)\n\nBurndownSparseMatrix = _reflection.GeneratedProtocolMessageType(\'BurndownSparseMatrix\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNSPARSEMATRIX,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownSparseMatrix)\n  ))\n_sym_db.RegisterMessage(BurndownSparseMatrix)\n\nFilesOwnership = _reflection.GeneratedProtocolMessageType(\'FilesOwnership\', (_message.Message,), dict(\n\n  ValueEntry = _reflection.GeneratedProtocolMessageType(\'ValueEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILESOWNERSHIP_VALUEENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FilesOwnership.ValueEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILESOWNERSHIP,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FilesOwnership)\n  ))\n_sym_db.RegisterMessage(FilesOwnership)\n_sym_db.RegisterMessage(FilesOwnership.ValueEntry)\n\nBurndownAnalysisResults = _reflection.GeneratedProtocolMessageType(\'BurndownAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _BURNDOWNANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:BurndownAnalysisResults)\n  ))\n_sym_db.RegisterMessage(BurndownAnalysisResults)\n\nCompressedSparseRowMatrix = _reflection.GeneratedProtocolMessageType(\'CompressedSparseRowMatrix\', (_message.Message,), dict(\n  DESCRIPTOR = _COMPRESSEDSPARSEROWMATRIX,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CompressedSparseRowMatrix)\n  ))\n_sym_db.RegisterMessage(CompressedSparseRowMatrix)\n\nCouples = _reflection.GeneratedProtocolMessageType(\'Couples\', (_message.Message,), dict(\n  DESCRIPTOR = _COUPLES,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Couples)\n  ))\n_sym_db.RegisterMessage(Couples)\n\nTouchedFiles = _reflection.GeneratedProtocolMessageType(\'TouchedFiles\', (_message.Message,), dict(\n  DESCRIPTOR = _TOUCHEDFILES,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TouchedFiles)\n  ))\n_sym_db.RegisterMessage(TouchedFiles)\n\nCouplesAnalysisResults = _reflection.GeneratedProtocolMessageType(\'CouplesAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _COUPLESANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CouplesAnalysisResults)\n  ))\n_sym_db.RegisterMessage(CouplesAnalysisResults)\n\nUASTChange = _reflection.GeneratedProtocolMessageType(\'UASTChange\', (_message.Message,), dict(\n  DESCRIPTOR = _UASTCHANGE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:UASTChange)\n  ))\n_sym_db.RegisterMessage(UASTChange)\n\nUASTChangesSaverResults = _reflection.GeneratedProtocolMessageType(\'UASTChangesSaverResults\', (_message.Message,), dict(\n  DESCRIPTOR = _UASTCHANGESSAVERRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:UASTChangesSaverResults)\n  ))\n_sym_db.RegisterMessage(UASTChangesSaverResults)\n\nShotnessRecord = _reflection.GeneratedProtocolMessageType(\'ShotnessRecord\', (_message.Message,), dict(\n\n  CountersEntry = _reflection.GeneratedProtocolMessageType(\'CountersEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SHOTNESSRECORD_COUNTERSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:ShotnessRecord.CountersEntry)\n    ))\n  ,\n  DESCRIPTOR = _SHOTNESSRECORD,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ShotnessRecord)\n  ))\n_sym_db.RegisterMessage(ShotnessRecord)\n_sym_db.RegisterMessage(ShotnessRecord.CountersEntry)\n\nShotnessAnalysisResults = _reflection.GeneratedProtocolMessageType(\'ShotnessAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _SHOTNESSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ShotnessAnalysisResults)\n  ))\n_sym_db.RegisterMessage(ShotnessAnalysisResults)\n\nFileHistory = _reflection.GeneratedProtocolMessageType(\'FileHistory\', (_message.Message,), dict(\n\n  ChangesByDeveloperEntry = _reflection.GeneratedProtocolMessageType(\'ChangesByDeveloperEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILEHISTORY_CHANGESBYDEVELOPERENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FileHistory.ChangesByDeveloperEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILEHISTORY,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FileHistory)\n  ))\n_sym_db.RegisterMessage(FileHistory)\n_sym_db.RegisterMessage(FileHistory.ChangesByDeveloperEntry)\n\nFileHistoryResultMessage = _reflection.GeneratedProtocolMessageType(\'FileHistoryResultMessage\', (_message.Message,), dict(\n\n  FilesEntry = _reflection.GeneratedProtocolMessageType(\'FilesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _FILEHISTORYRESULTMESSAGE_FILESENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:FileHistoryResultMessage.FilesEntry)\n    ))\n  ,\n  DESCRIPTOR = _FILEHISTORYRESULTMESSAGE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:FileHistoryResultMessage)\n  ))\n_sym_db.RegisterMessage(FileHistoryResultMessage)\n_sym_db.RegisterMessage(FileHistoryResultMessage.FilesEntry)\n\nLineStats = _reflection.GeneratedProtocolMessageType(\'LineStats\', (_message.Message,), dict(\n  DESCRIPTOR = _LINESTATS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:LineStats)\n  ))\n_sym_db.RegisterMessage(LineStats)\n\nDevTick = _reflection.GeneratedProtocolMessageType(\'DevTick\', (_message.Message,), dict(\n\n  LanguagesEntry = _reflection.GeneratedProtocolMessageType(\'LanguagesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DEVTICK_LANGUAGESENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:DevTick.LanguagesEntry)\n    ))\n  ,\n  DESCRIPTOR = _DEVTICK,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:DevTick)\n  ))\n_sym_db.RegisterMessage(DevTick)\n_sym_db.RegisterMessage(DevTick.LanguagesEntry)\n\nTickDevs = _reflection.GeneratedProtocolMessageType(\'TickDevs\', (_message.Message,), dict(\n\n  DevsEntry = _reflection.GeneratedProtocolMessageType(\'DevsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _TICKDEVS_DEVSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:TickDevs.DevsEntry)\n    ))\n  ,\n  DESCRIPTOR = _TICKDEVS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TickDevs)\n  ))\n_sym_db.RegisterMessage(TickDevs)\n_sym_db.RegisterMessage(TickDevs.DevsEntry)\n\nDevsAnalysisResults = _reflection.GeneratedProtocolMessageType(\'DevsAnalysisResults\', (_message.Message,), dict(\n\n  TicksEntry = _reflection.GeneratedProtocolMessageType(\'TicksEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _DEVSANALYSISRESULTS_TICKSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:DevsAnalysisResults.TicksEntry)\n    ))\n  ,\n  DESCRIPTOR = _DEVSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:DevsAnalysisResults)\n  ))\n_sym_db.RegisterMessage(DevsAnalysisResults)\n_sym_db.RegisterMessage(DevsAnalysisResults.TicksEntry)\n\nSentiment = _reflection.GeneratedProtocolMessageType(\'Sentiment\', (_message.Message,), dict(\n  DESCRIPTOR = _SENTIMENT,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Sentiment)\n  ))\n_sym_db.RegisterMessage(Sentiment)\n\nCommentSentimentResults = _reflection.GeneratedProtocolMessageType(\'CommentSentimentResults\', (_message.Message,), dict(\n\n  SentimentByTickEntry = _reflection.GeneratedProtocolMessageType(\'SentimentByTickEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:CommentSentimentResults.SentimentByTickEntry)\n    ))\n  ,\n  DESCRIPTOR = _COMMENTSENTIMENTRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommentSentimentResults)\n  ))\n_sym_db.RegisterMessage(CommentSentimentResults)\n_sym_db.RegisterMessage(CommentSentimentResults.SentimentByTickEntry)\n\nCommitFile = _reflection.GeneratedProtocolMessageType(\'CommitFile\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMITFILE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommitFile)\n  ))\n_sym_db.RegisterMessage(CommitFile)\n\nCommit = _reflection.GeneratedProtocolMessageType(\'Commit\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMIT,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Commit)\n  ))\n_sym_db.RegisterMessage(Commit)\n\nCommitsAnalysisResults = _reflection.GeneratedProtocolMessageType(\'CommitsAnalysisResults\', (_message.Message,), dict(\n  DESCRIPTOR = _COMMITSANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:CommitsAnalysisResults)\n  ))\n_sym_db.RegisterMessage(CommitsAnalysisResults)\n\nTypo = _reflection.GeneratedProtocolMessageType(\'Typo\', (_message.Message,), dict(\n  DESCRIPTOR = _TYPO,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:Typo)\n  ))\n_sym_db.RegisterMessage(Typo)\n\nTyposDataset = _reflection.GeneratedProtocolMessageType(\'TyposDataset\', (_message.Message,), dict(\n  DESCRIPTOR = _TYPOSDATASET,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:TyposDataset)\n  ))\n_sym_db.RegisterMessage(TyposDataset)\n\nImportsPerTick = _reflection.GeneratedProtocolMessageType(\'ImportsPerTick\', (_message.Message,), dict(\n\n  CountsEntry = _reflection.GeneratedProtocolMessageType(\'CountsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _IMPORTSPERTICK_COUNTSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:ImportsPerTick.CountsEntry)\n    ))\n  ,\n  DESCRIPTOR = _IMPORTSPERTICK,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ImportsPerTick)\n  ))\n_sym_db.RegisterMessage(ImportsPerTick)\n_sym_db.RegisterMessage(ImportsPerTick.CountsEntry)\n\nImportsPerLanguage = _reflection.GeneratedProtocolMessageType(\'ImportsPerLanguage\', (_message.Message,), dict(\n\n  TicksEntry = _reflection.GeneratedProtocolMessageType(\'TicksEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _IMPORTSPERLANGUAGE_TICKSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:ImportsPerLanguage.TicksEntry)\n    ))\n  ,\n  DESCRIPTOR = _IMPORTSPERLANGUAGE,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ImportsPerLanguage)\n  ))\n_sym_db.RegisterMessage(ImportsPerLanguage)\n_sym_db.RegisterMessage(ImportsPerLanguage.TicksEntry)\n\nImportsPerDeveloper = _reflection.GeneratedProtocolMessageType(\'ImportsPerDeveloper\', (_message.Message,), dict(\n\n  LanguagesEntry = _reflection.GeneratedProtocolMessageType(\'LanguagesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _IMPORTSPERDEVELOPER_LANGUAGESENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:ImportsPerDeveloper.LanguagesEntry)\n    ))\n  ,\n  DESCRIPTOR = _IMPORTSPERDEVELOPER,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ImportsPerDeveloper)\n  ))\n_sym_db.RegisterMessage(ImportsPerDeveloper)\n_sym_db.RegisterMessage(ImportsPerDeveloper.LanguagesEntry)\n\nImportsPerDeveloperResults = _reflection.GeneratedProtocolMessageType(\'ImportsPerDeveloperResults\', (_message.Message,), dict(\n  DESCRIPTOR = _IMPORTSPERDEVELOPERRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:ImportsPerDeveloperResults)\n  ))\n_sym_db.RegisterMessage(ImportsPerDeveloperResults)\n\nAnalysisResults = _reflection.GeneratedProtocolMessageType(\'AnalysisResults\', (_message.Message,), dict(\n\n  ContentsEntry = _reflection.GeneratedProtocolMessageType(\'ContentsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _ANALYSISRESULTS_CONTENTSENTRY,\n    __module__ = \'pb_pb2\'\n    # @@protoc_insertion_point(class_scope:AnalysisResults.ContentsEntry)\n    ))\n  ,\n  DESCRIPTOR = _ANALYSISRESULTS,\n  __module__ = \'pb_pb2\'\n  # @@protoc_insertion_point(class_scope:AnalysisResults)\n  ))\n_sym_db.RegisterMessage(AnalysisResults)\n_sym_db.RegisterMessage(AnalysisResults.ContentsEntry)\n\n\n_METADATA_RUNTIMEPERITEMENTRY._options = None\n_FILESOWNERSHIP_VALUEENTRY._options = None\n_SHOTNESSRECORD_COUNTERSENTRY._options = None\n_FILEHISTORY_CHANGESBYDEVELOPERENTRY._options = None\n_FILEHISTORYRESULTMESSAGE_FILESENTRY._options = None\n_DEVTICK_LANGUAGESENTRY._options = None\n_TICKDEVS_DEVSENTRY._options = None\n_DEVSANALYSISRESULTS_TICKSENTRY._options = None\n_COMMENTSENTIMENTRESULTS_SENTIMENTBYTICKENTRY._options = None\n_IMPORTSPERTICK_COUNTSENTRY._options = None\n_IMPORTSPERLANGUAGE_TICKSENTRY._options = None\n_IMPORTSPERDEVELOPER_LANGUAGESENTRY._options = None\n_ANALYSISRESULTS_CONTENTSENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
python/labours/plotting.py,0,"b'import os\nfrom pathlib import Path\n\n\ndef import_pyplot(backend, style):\n    import matplotlib\n\n    if backend:\n        matplotlib.use(backend)\n    from matplotlib import pyplot\n\n    pyplot.style.use(style)\n    print(""matplotlib: backend is"", matplotlib.get_backend())\n    return matplotlib, pyplot\n\n\ndef apply_plot_style(figure, axes, legend, background, font_size, axes_size):\n    foreground = ""black"" if background == ""white"" else ""white""\n    if axes_size is None:\n        axes_size = (16, 12)\n    else:\n        axes_size = tuple(float(p) for p in axes_size.split("",""))\n    figure.set_size_inches(*axes_size)\n    for side in (""bottom"", ""top"", ""left"", ""right""):\n        axes.spines[side].set_color(foreground)\n    for axis in (axes.xaxis, axes.yaxis):\n        axis.label.update(dict(fontsize=font_size, color=foreground))\n    for axis in (""x"", ""y""):\n        getattr(axes, axis + ""axis"").get_offset_text().set_size(font_size)\n        axes.tick_params(axis=axis, colors=foreground, labelsize=font_size)\n    try:\n        axes.ticklabel_format(axis=""y"", style=""sci"", scilimits=(0, 3))\n    except AttributeError:\n        pass\n    figure.patch.set_facecolor(background)\n    axes.set_facecolor(background)\n    if legend is not None:\n        frame = legend.get_frame()\n        for setter in (frame.set_facecolor, frame.set_edgecolor):\n            setter(background)\n        for text in legend.get_texts():\n            text.set_color(foreground)\n\n\ndef get_plot_path(base: str, name: str) -> str:\n    root, ext = os.path.splitext(base)\n    if not ext:\n        ext = "".png""\n    output = os.path.join(root, name + ext)\n    os.makedirs(os.path.dirname(output), exist_ok=True)\n    return output\n\n\ndef deploy_plot(title: str, output: str, background: str, tight: bool = True) -> None:\n    import matplotlib.pyplot as pyplot\n\n    if not output:\n        pyplot.gcf().canvas.set_window_title(title)\n        pyplot.show()\n    else:\n        po = Path(output)\n        if len(po.name) > 64:\n            suffix = po.suffix[:5]\n            output = str(po.with_name(po.stem[:64 - len(suffix)] + suffix))\n        if title:\n            pyplot.title(title, color=""black"" if background == ""white"" else ""white"")\n        if tight:\n            try:\n                pyplot.tight_layout()\n            except:  # noqa: E722\n                print(""Warning: failed to set the tight layout"")\n        print(""Writing plot to %s"" % output)\n        pyplot.savefig(output, transparent=True)\n    pyplot.clf()\n'"
python/labours/readers.py,0,"b'from argparse import Namespace\nfrom importlib import import_module\nimport io\nimport re\nimport sys\nfrom typing import Any, BinaryIO, Dict, List, Tuple, TYPE_CHECKING\n\nimport numpy\nimport yaml\n\nfrom labours.objects import DevDay\n\nif TYPE_CHECKING:\n    from scipy.sparse.csr import csr_matrix\n\n\nclass Reader(object):\n    def read(self, fileobj: BinaryIO):\n        raise NotImplementedError\n\n    def get_name(self):\n        raise NotImplementedError\n\n    def get_header(self):\n        raise NotImplementedError\n\n    def get_burndown_parameters(self):\n        raise NotImplementedError\n\n    def get_project_burndown(self):\n        raise NotImplementedError\n\n    def get_files_burndown(self):\n        raise NotImplementedError\n\n    def get_people_burndown(self):\n        raise NotImplementedError\n\n    def get_ownership_burndown(self):\n        raise NotImplementedError\n\n    def get_people_interaction(self):\n        raise NotImplementedError\n\n    def get_files_coocc(self):\n        raise NotImplementedError\n\n    def get_people_coocc(self):\n        raise NotImplementedError\n\n    def get_shotness_coocc(self):\n        raise NotImplementedError\n\n    def get_shotness(self):\n        raise NotImplementedError\n\n    def get_sentiment(self):\n        raise NotImplementedError\n\n    def get_devs(self):\n        raise NotImplementedError\n\n\nclass YamlReader(Reader):\n    def read(self, fileobj: BinaryIO):\n        yaml.reader.Reader.NON_PRINTABLE = re.compile(r""(?!x)x"")\n        try:\n            loader = yaml.CLoader\n        except AttributeError:\n            print(\n                ""Warning: failed to import yaml.CLoader, falling back to slow yaml.Loader""\n            )\n            loader = yaml.Loader\n        try:\n            wrapper = io.TextIOWrapper(fileobj, encoding=""utf-8"")\n            data = yaml.load(wrapper, Loader=loader)\n        except (UnicodeEncodeError, UnicodeDecodeError, yaml.reader.ReaderError) as e:\n            print(\n                ""\\nInvalid unicode in the input: %s\\nPlease filter it through ""\n                ""fix_yaml_unicode.py"" % e\n            )\n            sys.exit(1)\n        if data is None:\n            print(""\\nNo data has been read - has Hercules crashed?"")\n            sys.exit(1)\n        self.data = data\n\n    def get_run_times(self):\n        return {}\n\n    def get_name(self):\n        return self.data[""hercules""][""repository""]\n\n    def get_header(self):\n        header = self.data[""hercules""]\n        return header[""begin_unix_time""], header[""end_unix_time""]\n\n    def get_burndown_parameters(self):\n        header = self.data[""Burndown""]\n        return header[""sampling""], header[""granularity""], header[""tick_size""]\n\n    def get_project_burndown(self):\n        return (\n            self.data[""hercules""][""repository""],\n            self._parse_burndown_matrix(self.data[""Burndown""][""project""]).T,\n        )\n\n    def get_files_burndown(self):\n        return [\n            (p[0], self._parse_burndown_matrix(p[1]).T)\n            for p in self.data[""Burndown""][""files""].items()\n        ]\n\n    def get_people_burndown(self):\n        return [\n            (p[0], self._parse_burndown_matrix(p[1]).T)\n            for p in self.data[""Burndown""][""people""].items()\n        ]\n\n    def get_ownership_burndown(self):\n        return (\n            self.data[""Burndown""][""people_sequence""].copy(),\n            {\n                p[0]: self._parse_burndown_matrix(p[1])\n                for p in self.data[""Burndown""][""people""].items()\n            },\n        )\n\n    def get_people_interaction(self):\n        return (\n            self.data[""Burndown""][""people_sequence""].copy(),\n            self._parse_burndown_matrix(self.data[""Burndown""][""people_interaction""]),\n        )\n\n    def get_files_coocc(self):\n        coocc = self.data[""Couples""][""files_coocc""]\n        return coocc[""index""], self._parse_coocc_matrix(coocc[""matrix""])\n\n    def get_people_coocc(self):\n        coocc = self.data[""Couples""][""people_coocc""]\n        return coocc[""index""], self._parse_coocc_matrix(coocc[""matrix""])\n\n    def get_shotness_coocc(self):\n        shotness = self.data[""Shotness""]\n        index = [""%s:%s"" % (i[""file""], i[""name""]) for i in shotness]\n        indptr = numpy.zeros(len(shotness) + 1, dtype=numpy.int64)\n        indices = []\n        data = []\n        for i, record in enumerate(shotness):\n            pairs = [(int(k), v) for k, v in record[""counters""].items()]\n            pairs.sort()\n            indptr[i + 1] = indptr[i] + len(pairs)\n            for k, v in pairs:\n                indices.append(k)\n                data.append(v)\n        indices = numpy.array(indices, dtype=numpy.int32)\n        data = numpy.array(data, dtype=numpy.int32)\n        from scipy.sparse import csr_matrix\n\n        return index, csr_matrix((data, indices, indptr), shape=(len(shotness),) * 2)\n\n    def get_shotness(self):\n        from munch import munchify\n\n        obj = munchify(self.data[""Shotness""])\n        # turn strings into ints\n        for item in obj:\n            item.counters = {int(k): v for k, v in item.counters.items()}\n        if len(obj) == 0:\n            raise KeyError\n        return obj\n\n    def get_sentiment(self):\n        from munch import munchify\n\n        return munchify(\n            {\n                int(key): {\n                    ""Comments"": vals[2].split(""|""),\n                    ""Commits"": vals[1],\n                    ""Value"": float(vals[0]),\n                }\n                for key, vals in self.data[""Sentiment""].items()\n            }\n        )\n\n    def get_devs(self):\n        people = self.data[""Devs""][""people""]\n        days = {\n            int(d): {\n                int(dev): DevDay(*(int(x) for x in day[:-1]), day[-1])\n                for dev, day in devs.items()\n            }\n            for d, devs in self.data[""Devs""][""ticks""].items()\n        }\n        return people, days\n\n    def _parse_burndown_matrix(self, matrix):\n        return numpy.array(\n            [numpy.fromstring(line, dtype=int, sep="" "") for line in matrix.split(""\\n"")]\n        )\n\n    def _parse_coocc_matrix(self, matrix):\n        from scipy.sparse import csr_matrix\n\n        data = []\n        indices = []\n        indptr = [0]\n        for row in matrix:\n            for k, v in sorted(row.items()):\n                data.append(v)\n                indices.append(k)\n            indptr.append(indptr[-1] + len(row))\n        return csr_matrix((data, indices, indptr), shape=(len(matrix),) * 2)\n\n\nclass ProtobufReader(Reader):\n    def read(self, fileobj: BinaryIO) -> None:\n        try:\n            from labours.pb_pb2 import AnalysisResults\n        except ImportError as e:\n            print(\n                ""\\n\\n>>> You need to generate python/hercules/pb/pb_pb2.py - run \\""make\\""\\n"",\n                file=sys.stderr,\n            )\n            raise e from None\n        self.data = AnalysisResults()\n        all_bytes = fileobj.read()\n        if not all_bytes:\n            raise ValueError(""empty input"")\n        self.data.ParseFromString(all_bytes)\n        self.contents = {}\n        for key, val in self.data.contents.items():\n            try:\n                mod, name = PB_MESSAGES[key].rsplit(""."", 1)\n            except KeyError:\n                sys.stderr.write(\n                    ""Warning: there is no registered PB decoder for %s\\n"" % key\n                )\n                continue\n            cls = getattr(import_module(mod), name)\n            self.contents[key] = msg = cls()\n            msg.ParseFromString(val)\n\n    def get_run_times(self):\n        return {key: val for key, val in self.data.header.run_time_per_item.items()}\n\n    def get_name(self) -> str:\n        return self.data.header.repository\n\n    def get_header(self) -> Tuple[int, int]:\n        header = self.data.header\n        return header.begin_unix_time, header.end_unix_time\n\n    def get_burndown_parameters(self) -> Tuple[int, int, float]:\n        burndown = self.contents[""Burndown""]\n        return burndown.sampling, burndown.granularity, burndown.tick_size / 1000000000\n\n    def get_project_burndown(self) -> Tuple[str, numpy.ndarray]:\n        return self._parse_burndown_matrix(self.contents[""Burndown""].project)\n\n    def get_files_burndown(self):\n        return [self._parse_burndown_matrix(i) for i in self.contents[""Burndown""].files]\n\n    def get_people_burndown(self) -> List[Any]:\n        return [\n            self._parse_burndown_matrix(i) for i in self.contents[""Burndown""].people\n        ]\n\n    def get_ownership_burndown(self) -> Tuple[List[Any], Dict[Any, Any]]:\n        people = self.get_people_burndown()\n        return [p[0] for p in people], {p[0]: p[1].T for p in people}\n\n    def get_people_interaction(self):\n        burndown = self.contents[""Burndown""]\n        return (\n            [i.name for i in burndown.people],\n            self._parse_sparse_matrix(burndown.people_interaction).toarray(),\n        )\n\n    def get_files_coocc(self) -> Tuple[List[str], \'csr_matrix\']:\n        node = self.contents[""Couples""].file_couples\n        return list(node.index), self._parse_sparse_matrix(node.matrix)\n\n    def get_people_coocc(self) -> Tuple[List[str], \'csr_matrix\']:\n        node = self.contents[""Couples""].people_couples\n        return list(node.index), self._parse_sparse_matrix(node.matrix)\n\n    def get_shotness_coocc(self):\n        shotness = self.get_shotness()\n        index = [""%s:%s"" % (i.file, i.name) for i in shotness]\n        indptr = numpy.zeros(len(shotness) + 1, dtype=numpy.int32)\n        indices = []\n        data = []\n        for i, record in enumerate(shotness):\n            pairs = list(record.counters.items())\n            pairs.sort()\n            indptr[i + 1] = indptr[i] + len(pairs)\n            for k, v in pairs:\n                indices.append(k)\n                data.append(v)\n        indices = numpy.array(indices, dtype=numpy.int32)\n        data = numpy.array(data, dtype=numpy.int32)\n        from scipy.sparse import csr_matrix\n\n        return index, csr_matrix((data, indices, indptr), shape=(len(shotness),) * 2)\n\n    def get_shotness(self):\n        records = self.contents[""Shotness""].records\n        if len(records) == 0:\n            raise KeyError\n        return records\n\n    def get_sentiment(self):\n        byday = self.contents[""Sentiment""].SentimentByDay\n        if len(byday) == 0:\n            raise KeyError\n        return byday\n\n    def get_devs(self) -> Tuple[List[str], Dict[int, Dict[int, DevDay]]]:\n        people = list(self.contents[""Devs""].dev_index)\n        days = {\n            d: {\n                dev: DevDay(\n                    stats.commits,\n                    stats.stats.added,\n                    stats.stats.removed,\n                    stats.stats.changed,\n                    {\n                        k: [v.added, v.removed, v.changed]\n                        for k, v in stats.languages.items()\n                    },\n                )\n                for dev, stats in day.devs.items()\n            }\n            for d, day in self.contents[""Devs""].ticks.items()\n        }\n        return people, days\n\n    def _parse_burndown_matrix(self, matrix):\n        dense = numpy.zeros(\n            (matrix.number_of_rows, matrix.number_of_columns), dtype=int\n        )\n        for y, row in enumerate(matrix.rows):\n            for x, col in enumerate(row.columns):\n                dense[y, x] = col\n        return matrix.name, dense.T\n\n    def _parse_sparse_matrix(self, matrix):\n        from scipy.sparse import csr_matrix\n\n        return csr_matrix(\n            (list(matrix.data), list(matrix.indices), list(matrix.indptr)),\n            shape=(matrix.number_of_rows, matrix.number_of_columns),\n        )\n\n\nREADERS = {""yaml"": YamlReader, ""yml"": YamlReader, ""pb"": ProtobufReader}\nPB_MESSAGES = {\n    ""Burndown"": ""labours.pb_pb2.BurndownAnalysisResults"",\n    ""Couples"": ""labours.pb_pb2.CouplesAnalysisResults"",\n    ""Shotness"": ""labours.pb_pb2.ShotnessAnalysisResults"",\n    ""Devs"": ""labours.pb_pb2.DevsAnalysisResults"",\n}\n\n\ndef chain_streams(streams, buffer_size=io.DEFAULT_BUFFER_SIZE):\n    """"""\n    Chain an iterable of streams together into a single buffered stream.\n    Source: https://stackoverflow.com/a/50770511\n\n    Usage:\n        f = chain_streams(open(f, ""rb"") for f in filenames)\n        f.read()\n    """"""\n\n    class ChainStream(io.RawIOBase):\n        def __init__(self):\n            self.leftover = b""""\n            self.stream_iter = iter(streams)\n            try:\n                self.stream = next(self.stream_iter)\n            except StopIteration:\n                self.stream = None\n\n        def readable(self):\n            return True\n\n        def _read_next_chunk(self, max_length):\n            # Return 0 or more bytes from the current stream, first returning all\n            # leftover bytes. If the stream is closed returns b\'\'\n            if self.leftover:\n                return self.leftover\n            elif self.stream is not None:\n                return self.stream.read(max_length)\n            else:\n                return b""""\n\n        def readinto(self, b):\n            buffer_length = len(b)\n            chunk = self._read_next_chunk(buffer_length)\n            while len(chunk) == 0:\n                # move to next stream\n                if self.stream is not None:\n                    self.stream.close()\n                try:\n                    self.stream = next(self.stream_iter)\n                    chunk = self._read_next_chunk(buffer_length)\n                except StopIteration:\n                    # No more streams to chain together\n                    self.stream = None\n                    return 0  # indicate EOF\n            output, self.leftover = chunk[:buffer_length], chunk[buffer_length:]\n            b[:len(output)] = output\n            return len(output)\n\n    return io.BufferedReader(ChainStream(), buffer_size=buffer_size)\n\n\ndef read_input(args: Namespace) -> ProtobufReader:\n    sys.stdout.write(""Reading the input... "")\n    sys.stdout.flush()\n    if args.input != ""-"":\n        stream = open(args.input, ""rb"")\n    else:\n        stream = sys.stdin.buffer\n    try:\n        if args.input_format == ""auto"":\n            buffer = stream.read(1 << 16)\n            try:\n                buffer.decode(""utf-8"")\n                args.input_format = ""yaml""\n            except UnicodeDecodeError:\n                args.input_format = ""pb""\n            ins = chain_streams((io.BytesIO(buffer), stream), len(buffer))\n        else:\n            ins = stream\n        reader = READERS[args.input_format]()\n        reader.read(ins)\n    finally:\n        if args.input != ""-"":\n            stream.close()\n    print(""done"")\n    return reader\n'"
python/labours/utils.py,0,"b'from datetime import datetime\nfrom numbers import Number\nfrom typing import TYPE_CHECKING\n\nimport numpy\n\nif TYPE_CHECKING:\n    from pandas import Timestamp\n\n\ndef floor_datetime(dt: datetime, duration: float) -> datetime:\n    return datetime.fromtimestamp(dt.timestamp() - dt.timestamp() % duration)\n\n\ndef default_json(x):\n    if hasattr(x, ""tolist""):\n        return x.tolist()\n    if hasattr(x, ""isoformat""):\n        return x.isoformat()\n    return x\n\n\ndef parse_date(text: None, default: \'Timestamp\') -> \'Timestamp\':\n    if not text:\n        return default\n    from dateutil.parser import parse\n\n    return parse(text)\n\n\ndef _format_number(n: Number) -> str:\n    if n == 0:\n        return ""0""\n    power = int(numpy.log10(abs(n)))\n    if power >= 6:\n        n = n / 1000000\n        if n >= 10:\n            n = str(int(n))\n        else:\n            n = ""%.1f"" % n\n            if n.endswith(""0""):\n                n = n[:-2]\n        suffix = ""M""\n    elif power >= 3:\n        n = n / 1000\n        if n >= 10:\n            n = str(int(n))\n        else:\n            n = ""%.1f"" % n\n            if n.endswith(""0""):\n                n = n[:-2]\n        suffix = ""K""\n    else:\n        n = str(n)\n        suffix = """"\n    return n + suffix\n\n\ndef import_pandas():\n    import pandas\n\n    try:\n        from pandas.plotting import register_matplotlib_converters\n\n        register_matplotlib_converters()\n    except ImportError:\n        pass\n    return pandas\n'"
python/labours/_vendor/__init__.py,0,b''
python/labours/_vendor/swivel.py,73,"b'#!/usr/bin/env python3\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n# Copyright 2017 Sourced Technologies S. L.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Submatrix-wise Vector Embedding Learner.\n\nImplementation of SwiVel algorithm described at:\nhttp://arxiv.org/abs/1602.02215\n\nThis program expects an input directory that contains the following files.\n\n  row_vocab.txt, col_vocab.txt\n\n    The row an column vocabulary files.  Each file should contain one token per\n    line; these will be used to generate a tab-separate file containing the\n    trained embeddings.\n\n  row_sums.txt, col_sum.txt\n\n    The matrix row and column marginal sums.  Each file should contain one\n    decimal floating point number per line which corresponds to the marginal\n    count of the matrix for that row or column.\n\n  shards.recs\n\n    A file containing the sub-matrix shards, stored as TFRecords. Each shard is\n    expected to be a serialzed tf.Example protocol buffer with the following\n    properties:\n\n      global_row: the global row indicies contained in the shard\n      global_col: the global column indicies contained in the shard\n      sparse_local_row, sparse_local_col, sparse_value: three parallel arrays\n      that are a sparse representation of the submatrix counts.\n\nIt will generate embeddings, training from the input directory for\nthe specified number of epochs.  When complete, it will output the trained\nvectors to a tab-separated file that contains one line per embedding.  Row and\ncolumn embeddings are stored in separate files.\n\n""""""\n\nimport glob\nimport math\nimport os\nimport threading\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nflags = tf.app.flags\n\nflags.DEFINE_string(""input_base_path"", None,\n                    ""Directory containing input shards, vocabularies, ""\n                    ""and marginals."")\nflags.DEFINE_string(""output_base_path"", None,\n                    ""Path where to write the trained embeddings."")\nflags.DEFINE_integer(""embedding_size"", 300, ""Size of the embeddings"")\nflags.DEFINE_boolean(""trainable_bias"", False, ""Biases are trainable"")\nflags.DEFINE_integer(""submatrix_rows"", 4096,\n                     ""Rows in each training submatrix. This must match ""\n                     ""the training data."")\nflags.DEFINE_integer(""submatrix_cols"", 4096,\n                     ""Rows in each training submatrix. This must match ""\n                     ""the training data."")\nflags.DEFINE_float(""loss_multiplier"", 1.0 / 4096,\n                   ""constant multiplier on loss."")\nflags.DEFINE_float(""confidence_exponent"", 0.5,\n                   ""Exponent for l2 confidence function"")\nflags.DEFINE_float(""confidence_scale"", 0.25,\n                   ""Scale for l2 confidence function"")\nflags.DEFINE_float(""confidence_base"", 0.1, ""Base for l2 confidence function"")\nflags.DEFINE_float(""learning_rate"", 1.0, ""Initial learning rate"")\nflags.DEFINE_string(""optimizer"", ""Adagrad"",\n                    ""SGD optimizer (tf.train.*Optimizer)"")\nflags.DEFINE_integer(""num_concurrent_steps"", 2,\n                     ""Number of threads to train with"")\nflags.DEFINE_integer(""num_readers"", 4,\n                     ""Number of threads to read the input data and feed it"")\nflags.DEFINE_float(""num_epochs"", 40, ""Number epochs to train for"")\nflags.DEFINE_float(""per_process_gpu_memory_fraction"", 0,\n                   ""Fraction of GPU memory to use, 0 means allow_growth"")\nflags.DEFINE_integer(""num_gpus"", 0,\n                     ""Number of GPUs to use, 0 means all available"")\nflags.DEFINE_string(""logs"", """",\n                    ""Path for TensorBoard logs (empty value disables them)"")\n\nFLAGS = flags.FLAGS\n\n\ndef log(message, *args, **kwargs):\n    tf.logging.info(message, *args, **kwargs)\n\n\ndef get_available_gpus():\n    return [d.name for d in device_lib.list_local_devices()\n            if d.device_type == ""GPU""]\n\n\ndef embeddings_with_init(vocab_size, embedding_dim, name):\n    """"""Creates and initializes the embedding tensors.""""""\n    return tf.get_variable(name=name,\n                           shape=[vocab_size, embedding_dim],\n                           initializer=tf.random_normal_initializer(\n                               stddev=math.sqrt(1.0 / embedding_dim)))\n\n\ndef count_matrix_input(filenames, submatrix_rows, submatrix_cols):\n    """"""Reads submatrix shards from disk.""""""\n    filename_queue = tf.train.string_input_producer(filenames)\n    reader = tf.WholeFileReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            ""global_row"": tf.FixedLenFeature([submatrix_rows], dtype=tf.int64),\n            ""global_col"": tf.FixedLenFeature([submatrix_cols], dtype=tf.int64),\n            ""sparse_local_row"": tf.VarLenFeature(dtype=tf.int64),\n            ""sparse_local_col"": tf.VarLenFeature(dtype=tf.int64),\n            ""sparse_value"": tf.VarLenFeature(dtype=tf.float32)\n        })\n\n    global_row = features[""global_row""]\n    global_col = features[""global_col""]\n\n    sparse_local_row = features[""sparse_local_row""].values\n    sparse_local_col = features[""sparse_local_col""].values\n    sparse_count = features[""sparse_value""].values\n\n    sparse_indices = tf.concat(axis=1, values=[tf.expand_dims(sparse_local_row, 1),\n                                               tf.expand_dims(sparse_local_col, 1)])\n    count = tf.sparse_to_dense(sparse_indices, [submatrix_rows, submatrix_cols],\n                               sparse_count, validate_indices=False)\n\n    queued_global_row, queued_global_col, queued_count = tf.train.batch(\n        [global_row, global_col, count],\n        batch_size=1,\n        num_threads=FLAGS.num_readers,\n        capacity=32)\n\n    queued_global_row = tf.reshape(queued_global_row, [submatrix_rows])\n    queued_global_col = tf.reshape(queued_global_col, [submatrix_cols])\n    queued_count = tf.reshape(queued_count, [submatrix_rows, submatrix_cols])\n\n    return queued_global_row, queued_global_col, queued_count\n\n\ndef read_marginals_file(filename):\n    """"""Reads text file with one number per line to an array.""""""\n    with open(filename) as lines:\n        return [float(line) for line in lines]\n\n\ndef write_embedding_tensor_to_disk(vocab_path, output_path, sess, embedding):\n    """"""Writes tensor to output_path as tsv""""""\n    # Fetch the embedding values from the model\n    embeddings = sess.run(embedding)\n\n    with open(output_path, ""w"") as out_f:\n        with open(vocab_path) as vocab_f:\n            for index, word in enumerate(vocab_f):\n                word = word.strip()\n                embedding = embeddings[index]\n                out_f.write(word + ""\\t"" + ""\\t"".join(\n                    [str(x) for x in embedding]) + ""\\n"")\n\n\ndef write_embeddings_to_disk(config, model, sess):\n    """"""Writes row and column embeddings disk""""""\n    # Row Embedding\n    row_vocab_path = config.input_base_path + ""/row_vocab.txt""\n    row_embedding_output_path = config.output_base_path + ""/row_embedding.tsv""\n    log(""Writing row embeddings to: %s"", row_embedding_output_path)\n    write_embedding_tensor_to_disk(row_vocab_path, row_embedding_output_path,\n                                   sess, model.row_embedding)\n\n    # Column Embedding\n    col_vocab_path = config.input_base_path + ""/col_vocab.txt""\n    col_embedding_output_path = config.output_base_path + ""/col_embedding.tsv""\n    log(""Writing column embeddings to: %s"", col_embedding_output_path)\n    write_embedding_tensor_to_disk(col_vocab_path, col_embedding_output_path,\n                                   sess, model.col_embedding)\n\n\nclass SwivelModel:\n    """"""Small class to gather needed pieces from a Graph being built.""""""\n\n    def __init__(self, config):\n        """"""Construct graph for dmc.""""""\n        self._config = config\n\n        # Create paths to input data files\n        log(""Reading model from: %s"", config.input_base_path)\n        count_matrix_files = glob.glob(os.path.join(config.input_base_path, ""shard-*.pb""))\n        row_sums_path = os.path.join(config.input_base_path, ""row_sums.txt"")\n        col_sums_path = os.path.join(config.input_base_path, ""col_sums.txt"")\n\n        # Read marginals\n        row_sums = read_marginals_file(row_sums_path)\n        col_sums = read_marginals_file(col_sums_path)\n\n        self.n_rows = len(row_sums)\n        self.n_cols = len(col_sums)\n        log(""Matrix dim: (%d,%d) SubMatrix dim: (%d,%d)"",\n            self.n_rows, self.n_cols, config.submatrix_rows,\n            config.submatrix_cols)\n        if self.n_cols < config.submatrix_cols:\n            raise ValueError(\n                ""submatrix_cols={0} can not be bigger than columns number={1} ""\n                ""(specify submatrix_cols={1})"".format(config.submatrix_cols, self.n_cols))\n        if self.n_rows < config.submatrix_rows:\n            raise ValueError(\n                ""submatrix_rows={0} can not be bigger than rows number={1} ""\n                ""(specify submatrix_rows={1})"".format(config.submatrix_rows, self.n_cols))\n        self.n_submatrices = (\n            self.n_rows * self.n_cols / (config.submatrix_rows * config.submatrix_cols))\n        log(""n_submatrices: %d"", self.n_submatrices)\n\n        with tf.device(""/cpu:0""):\n            # ===== CREATE VARIABLES ======\n            # Get input\n            global_row, global_col, count = count_matrix_input(\n                count_matrix_files, config.submatrix_rows,\n                config.submatrix_cols)\n\n            # Embeddings\n            self.row_embedding = embeddings_with_init(\n                embedding_dim=config.embedding_size,\n                vocab_size=self.n_rows,\n                name=""row_embedding"")\n            self.col_embedding = embeddings_with_init(\n                embedding_dim=config.embedding_size,\n                vocab_size=self.n_cols,\n                name=""col_embedding"")\n            tf.summary.histogram(""row_emb"", self.row_embedding)\n            tf.summary.histogram(""col_emb"", self.col_embedding)\n\n            matrix_log_sum = math.log(np.sum(row_sums) + 1)\n            row_bias_init = [math.log(x + 1) for x in row_sums]\n            col_bias_init = [math.log(x + 1) for x in col_sums]\n            self.row_bias = tf.Variable(\n                row_bias_init, trainable=config.trainable_bias)\n            self.col_bias = tf.Variable(\n                col_bias_init, trainable=config.trainable_bias)\n            tf.summary.histogram(""row_bias"", self.row_bias)\n            tf.summary.histogram(""col_bias"", self.col_bias)\n\n            # Add optimizer\n            l2_losses = []\n            sigmoid_losses = []\n            self.global_step = tf.Variable(0, name=""global_step"")\n            learning_rate = tf.Variable(config.learning_rate,\n                                        name=""learning_rate"")\n            opt = getattr(tf.train, FLAGS.optimizer + ""Optimizer"")(\n                learning_rate)\n            tf.summary.scalar(""learning_rate"", learning_rate)\n\n            all_grads = []\n\n        devices = [""/gpu:%d"" % i for i in range(FLAGS.num_gpus)] \\\n            if FLAGS.num_gpus > 0 else get_available_gpus()\n        self.devices_number = len(devices)\n        if not self.devices_number:\n            devices = [""/cpu:0""]\n            self.devices_number = 1\n        for dev in devices:\n            with tf.device(dev):\n                with tf.name_scope(dev[1:].replace("":"", ""_"")):\n                    # ===== CREATE GRAPH =====\n                    # Fetch embeddings.\n                    selected_row_embedding = tf.nn.embedding_lookup(\n                        self.row_embedding, global_row)\n                    selected_col_embedding = tf.nn.embedding_lookup(\n                        self.col_embedding, global_col)\n\n                    # Fetch biases.\n                    selected_row_bias = tf.nn.embedding_lookup(\n                        [self.row_bias], global_row)\n                    selected_col_bias = tf.nn.embedding_lookup(\n                        [self.col_bias], global_col)\n\n                    # Multiply the row and column embeddings to generate\n                    # predictions.\n                    predictions = tf.matmul(\n                        selected_row_embedding, selected_col_embedding,\n                        transpose_b=True)\n\n                    # These binary masks separate zero from non-zero values.\n                    count_is_nonzero = tf.to_float(tf.cast(count, tf.bool))\n                    count_is_zero = 1 - count_is_nonzero\n\n                    objectives = count_is_nonzero * tf.log(count + 1e-30)\n                    objectives -= tf.reshape(\n                        selected_row_bias, [config.submatrix_rows, 1])\n                    objectives -= selected_col_bias\n                    objectives += matrix_log_sum\n\n                    err = predictions - objectives\n\n                    # The confidence function scales the L2 loss based on\n                    # the raw co-occurrence count.\n                    l2_confidence = config.confidence_base + config.confidence_scale * tf.pow(\n                        count, config.confidence_exponent)\n\n                    l2_loss = config.loss_multiplier * tf.reduce_sum(\n                        0.5 * l2_confidence * err * err * count_is_nonzero)\n                    l2_losses.append(tf.expand_dims(l2_loss, 0))\n\n                    sigmoid_loss = config.loss_multiplier * tf.reduce_sum(\n                        tf.nn.softplus(err) * count_is_zero)\n                    sigmoid_losses.append(tf.expand_dims(sigmoid_loss, 0))\n\n                    loss = l2_loss + sigmoid_loss\n                    grads = opt.compute_gradients(loss)\n                    all_grads.append(grads)\n\n        with tf.device(""/cpu:0""):\n            # ===== MERGE LOSSES =====\n            l2_loss = tf.reduce_mean(tf.concat(axis=0, values=l2_losses), 0,\n                                     name=""l2_loss"")\n            sigmoid_loss = tf.reduce_mean(\n                tf.concat(axis=0, values=sigmoid_losses), 0,\n                name=""sigmoid_loss"")\n            overall_loss = l2_loss + sigmoid_loss\n            average = tf.train.ExponentialMovingAverage(0.999)\n            loss_average_op = average.apply(\n                (overall_loss, l2_loss, sigmoid_loss))\n            self.loss = average.average(overall_loss)\n            tf.summary.scalar(""overall_loss"", self.loss)\n            tf.summary.scalar(""l2_loss"", average.average(l2_loss))\n            tf.summary.scalar(""sigmoid_loss"", average.average(sigmoid_loss))\n\n            # Apply the gradients to adjust the shared variables.\n            apply_gradient_ops = []\n            for grads in all_grads:\n                apply_gradient_ops.append(opt.apply_gradients(\n                    grads, global_step=self.global_step))\n\n            self.train_op = tf.group(loss_average_op, *apply_gradient_ops)\n            self.saver = tf.train.Saver(sharded=True)\n\n    def initialize_summary(self, sess):\n        log(""creating TensorBoard stuff..."")\n        self.summary = tf.summary.merge_all()\n        self.writer = tf.summary.FileWriter(FLAGS.logs, sess.graph)\n        projector_config = \\\n            tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n        embedding_config = projector_config.embeddings.add()\n        length = min(10000, self.n_rows, self.n_cols)\n        self.embedding10k = tf.Variable(\n            tf.zeros((length, self._config.embedding_size)),\n            name=""top10k_embedding"")\n        embedding_config.tensor_name = self.embedding10k.name\n        embedding_config.metadata_path = os.path.join(\n            self._config.input_base_path, ""row_vocab.txt"")\n        tf.contrib.tensorboard.plugins.projector.visualize_embeddings(\n            self.writer, projector_config)\n        self.saver = tf.train.Saver((self.embedding10k,), max_to_keep=1)\n\n    def write_summary(self, sess):\n        log(""writing the summary..."")\n        length = min(10000, self.n_rows, self.n_cols)\n        assignment = self.embedding10k.assign(\n            (self.row_embedding[:length] + self.col_embedding[:length]) / 2)\n        summary, _, global_step = sess.run(\n            (self.summary, assignment, self.global_step))\n        self.writer.add_summary(summary, global_step)\n        self.saver.save(\n            sess, os.path.join(FLAGS.logs, ""embeddings10k.checkpoint""),\n            global_step)\n\n\ndef main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    start_time = time.time()\n\n    # Create the output path.  If this fails, it really ought to fail now. :)\n    if not os.path.isdir(FLAGS.output_base_path):\n        os.makedirs(FLAGS.output_base_path)\n\n    # Create and run model\n    with tf.Graph().as_default():\n        log(""creating the model..."")\n        model = SwivelModel(FLAGS)\n\n        # Create a session for running Ops on the Graph.\n        gpu_opts = {}\n        if FLAGS.per_process_gpu_memory_fraction > 0:\n            gpu_opts[""per_process_gpu_memory_fraction""] = \\\n                FLAGS.per_process_gpu_memory_fraction\n        else:\n            gpu_opts[""allow_growth""] = True\n        gpu_options = tf.GPUOptions(**gpu_opts)\n        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n        if FLAGS.logs:\n            model.initialize_summary(sess)\n\n        # Run the Op to initialize the variables.\n        log(""initializing the variables..."")\n        sess.run(tf.global_variables_initializer())\n\n        # Start feeding input\n        log(""starting the input threads..."")\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        # Calculate how many steps each thread should run\n        n_total_steps = int(FLAGS.num_epochs * model.n_rows * model.n_cols) / (\n            FLAGS.submatrix_rows * FLAGS.submatrix_cols)\n        n_steps_per_thread = n_total_steps / (\n            FLAGS.num_concurrent_steps * model.devices_number)\n        n_submatrices_to_train = model.n_submatrices * FLAGS.num_epochs\n        t0 = [time.time()]\n        n_steps_between_status_updates = 100\n        n_steps_between_summary_updates = 10000\n        status_i = [0, 0]\n        status_lock = threading.Lock()\n        msg = (""%%%dd/%%d submatrices trained (%%.1f%%%%), ""\n               ""%%5.1f submatrices/sec | loss %%f"") % \\\n            len(str(n_submatrices_to_train))\n\n        def TrainingFn():\n            for _ in range(int(n_steps_per_thread)):\n                _, global_step, loss = sess.run((\n                    model.train_op, model.global_step, model.loss))\n\n                show_status = False\n                update_summary = False\n                with status_lock:\n                    new_i = global_step // n_steps_between_status_updates\n                    if new_i > status_i[0]:\n                        status_i[0] = new_i\n                        show_status = True\n                    new_i = global_step // n_steps_between_summary_updates\n                    if new_i > status_i[1]:\n                        status_i[1] = new_i\n                        update_summary = True\n                if show_status:\n                    elapsed = float(time.time() - t0[0])\n                    log(msg, global_step, n_submatrices_to_train,\n                        100.0 * global_step / n_submatrices_to_train,\n                        n_steps_between_status_updates / elapsed, loss)\n                    t0[0] = time.time()\n            if update_summary and FLAGS.logs:\n                model.write_summary(sess)\n\n    # Start training threads\n    train_threads = []\n    for _ in range(FLAGS.num_concurrent_steps):\n        t = threading.Thread(target=TrainingFn)\n        train_threads.append(t)\n        t.start()\n\n    # Wait for threads to finish.\n    for t in train_threads:\n        t.join()\n\n    coord.request_stop()\n    coord.join(threads)\n\n    # Write out vectors\n    write_embeddings_to_disk(FLAGS, model, sess)\n\n    # Shutdown\n    sess.close()\n    log(""Elapsed: %s"", time.time() - start_time)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
python/labours/modes/__init__.py,0,b''
python/labours/modes/burndown.py,0,"b'from argparse import Namespace\nimport contextlib\nfrom datetime import datetime, timedelta\nimport io\nimport json\nimport sys\nfrom typing import List, Tuple, TYPE_CHECKING\nimport warnings\n\nimport numpy\nimport tqdm\n\nfrom labours.plotting import apply_plot_style, deploy_plot, get_plot_path, import_pyplot\nfrom labours.utils import default_json, floor_datetime, import_pandas, parse_date\n\nif TYPE_CHECKING:\n    from lifelines import KaplanMeierFitter\n    from pandas.core.indexes.datetimes import DatetimeIndex\n\n\ndef plot_burndown(\n    args: Namespace,\n    target: str,\n    name: str,\n    matrix: numpy.ndarray,\n    date_range_sampling: \'DatetimeIndex\',\n    labels: List[int],\n    granularity: int,\n    sampling: int,\n    resample: str,\n) -> None:\n    if args.output and args.output.endswith("".json""):\n        data = locals().copy()\n        del data[""args""]\n        data[""type""] = ""burndown""\n        if args.mode == ""project"" and target == ""project"":\n            output = args.output\n        else:\n            if target == ""project"":\n                name = ""project""\n            output = get_plot_path(args.output, name)\n        with open(output, ""w"") as fout:\n            json.dump(data, fout, sort_keys=True, default=default_json)\n        return\n\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n\n    pyplot.stackplot(date_range_sampling, matrix, labels=labels)\n    if args.relative:\n        for i in range(matrix.shape[1]):\n            matrix[:, i] /= matrix[:, i].sum()\n        pyplot.ylim(0, 1)\n        legend_loc = 3\n    else:\n        legend_loc = 2\n    legend = pyplot.legend(loc=legend_loc, fontsize=args.font_size)\n    pyplot.ylabel(""Lines of code"")\n    pyplot.xlabel(""Time"")\n    apply_plot_style(\n        pyplot.gcf(), pyplot.gca(), legend, args.background, args.font_size, args.size\n    )\n    pyplot.xlim(\n        parse_date(args.start_date, date_range_sampling[0]),\n        parse_date(args.end_date, date_range_sampling[-1]),\n    )\n    locator = pyplot.gca().xaxis.get_major_locator()\n    # set the optimal xticks locator\n    if ""M"" not in resample:\n        pyplot.gca().xaxis.set_major_locator(matplotlib.dates.YearLocator())\n    locs = pyplot.gca().get_xticks().tolist()\n    if len(locs) >= 16:\n        pyplot.gca().xaxis.set_major_locator(matplotlib.dates.YearLocator())\n        locs = pyplot.gca().get_xticks().tolist()\n        if len(locs) >= 16:\n            pyplot.gca().xaxis.set_major_locator(locator)\n    if locs[0] < pyplot.xlim()[0]:\n        del locs[0]\n    endindex = -1\n    if len(locs) >= 2 and pyplot.xlim()[1] - locs[-1] > (locs[-1] - locs[-2]) / 2:\n        locs.append(pyplot.xlim()[1])\n        endindex = len(locs) - 1\n    startindex = -1\n    if len(locs) >= 2 and locs[0] - pyplot.xlim()[0] > (locs[1] - locs[0]) / 2:\n        locs.append(pyplot.xlim()[0])\n        startindex = len(locs) - 1\n    pyplot.gca().set_xticks(locs)\n    # hacking time!\n    labels = pyplot.gca().get_xticklabels()\n    if startindex >= 0:\n        labels[startindex].set_text(date_range_sampling[0].date())\n        labels[startindex].set_text = lambda _: None\n        labels[startindex].set_rotation(30)\n        labels[startindex].set_ha(""right"")\n    if endindex >= 0:\n        labels[endindex].set_text(date_range_sampling[-1].date())\n        labels[endindex].set_text = lambda _: None\n        labels[endindex].set_rotation(30)\n        labels[endindex].set_ha(""right"")\n    title = ""%s %d x %d (granularity %d, sampling %d)"" % (\n        (name,) + matrix.shape + (granularity, sampling)\n    )\n    output = args.output\n    if output:\n        if args.mode == ""project"" and target == ""project"":\n            output = args.output\n        else:\n            if target == ""project"":\n                name = ""project""\n            output = get_plot_path(args.output, name)\n    deploy_plot(title, output, args.background)\n\n\ndef plot_many_burndown(args: Namespace, target: str, header, parts):\n    if not args.output:\n        print(""Warning: output not set, showing %d plots."" % len(parts))\n    stdout = io.StringIO()\n    for name, matrix in tqdm.tqdm(parts):\n        with contextlib.redirect_stdout(stdout):\n            plot_burndown(\n                args, target, *load_burndown(header, name, matrix, args.resample)\n            )\n    sys.stdout.write(stdout.getvalue())\n\n\ndef fit_kaplan_meier(matrix: numpy.ndarray) -> \'KaplanMeierFitter\':\n    from lifelines import KaplanMeierFitter\n\n    T = []\n    W = []\n    indexes = numpy.arange(matrix.shape[0], dtype=int)\n    entries = numpy.zeros(matrix.shape[0], int)\n    dead = set()\n    for i in range(1, matrix.shape[1]):\n        diff = matrix[:, i - 1] - matrix[:, i]\n        entries[diff < 0] = i\n        mask = diff > 0\n        deaths = diff[mask]\n        T.append(numpy.full(len(deaths), i) - entries[indexes[mask]])\n        W.append(deaths)\n        entered = entries > 0\n        entered[0] = True\n        dead = dead.union(set(numpy.where((matrix[:, i] == 0) & entered)[0]))\n    # add the survivors as censored\n    nnzind = entries != 0\n    nnzind[0] = True\n    nnzind[sorted(dead)] = False\n    T.append(numpy.full(nnzind.sum(), matrix.shape[1]) - entries[nnzind])\n    W.append(matrix[nnzind, -1])\n    T = numpy.concatenate(T)\n    E = numpy.ones(len(T), bool)\n    E[-nnzind.sum() :] = 0\n    W = numpy.concatenate(W)\n    if T.size == 0:\n        return None\n    kmf = KaplanMeierFitter().fit(T, E, weights=W)\n    return kmf\n\n\ndef print_survival_function(kmf: \'KaplanMeierFitter\', sampling: int) -> None:\n    sf = kmf.survival_function_\n    sf.index = [timedelta(days=d) for d in sf.index * sampling]\n    sf.columns = [""Ratio of survived lines""]\n    try:\n        print(sf[len(sf) // 6 :: len(sf) // 6].append(sf.tail(1)))\n    except ValueError:\n        pass\n\n\ndef interpolate_burndown_matrix(\n    matrix: numpy.ndarray, granularity: int, sampling: int, progress: bool = False\n) -> numpy.ndarray:\n    daily = numpy.zeros(\n        (matrix.shape[0] * granularity, matrix.shape[1] * sampling), dtype=numpy.float32\n    )\n    """"""\n    ----------> samples, x\n    |\n    |\n    |\n    \xe2\x8c\x84\n    bands, y\n    """"""\n    for y in tqdm.tqdm(range(matrix.shape[0]), disable=(not progress)):\n        for x in range(matrix.shape[1]):\n            if y * granularity > (x + 1) * sampling:\n                # the future is zeros\n                continue\n\n            def decay(start_index: int, start_val: float):\n                if start_val == 0:\n                    return\n                k = matrix[y][x] / start_val  # <= 1\n                scale = (x + 1) * sampling - start_index\n                for i in range(y * granularity, (y + 1) * granularity):\n                    initial = daily[i][start_index - 1]\n                    for j in range(start_index, (x + 1) * sampling):\n                        daily[i][j] = initial * (\n                            1 + (k - 1) * (j - start_index + 1) / scale\n                        )\n\n            def grow(finish_index: int, finish_val: float):\n                initial = matrix[y][x - 1] if x > 0 else 0\n                start_index = x * sampling\n                if start_index < y * granularity:\n                    start_index = y * granularity\n                if finish_index == start_index:\n                    return\n                avg = (finish_val - initial) / (finish_index - start_index)\n                for j in range(x * sampling, finish_index):\n                    for i in range(start_index, j + 1):\n                        daily[i][j] = avg\n                # copy [x*g..y*s)\n                for j in range(x * sampling, finish_index):\n                    for i in range(y * granularity, x * sampling):\n                        daily[i][j] = daily[i][j - 1]\n\n            if (y + 1) * granularity >= (x + 1) * sampling:\n                # x*granularity <= (y+1)*sampling\n                # 1. x*granularity <= y*sampling\n                #    y*sampling..(y+1)sampling\n                #\n                #       x+1\n                #        /\n                #       /\n                #      / y+1  -|\n                #     /        |\n                #    / y      -|\n                #   /\n                #  / x\n                #\n                # 2. x*granularity > y*sampling\n                #    x*granularity..(y+1)sampling\n                #\n                #       x+1\n                #        /\n                #       /\n                #      / y+1  -|\n                #     /        |\n                #    / x      -|\n                #   /\n                #  / y\n                if y * granularity <= x * sampling:\n                    grow((x + 1) * sampling, matrix[y][x])\n                elif (x + 1) * sampling > y * granularity:\n                    grow((x + 1) * sampling, matrix[y][x])\n                    avg = matrix[y][x] / ((x + 1) * sampling - y * granularity)\n                    for j in range(y * granularity, (x + 1) * sampling):\n                        for i in range(y * granularity, j + 1):\n                            daily[i][j] = avg\n            elif (y + 1) * granularity >= x * sampling:\n                # y*sampling <= (x+1)*granularity < (y+1)sampling\n                # y*sampling..(x+1)*granularity\n                # (x+1)*granularity..(y+1)sampling\n                #        x+1\n                #         /\\\n                #        /  \\\n                #       /    \\\n                #      /    y+1\n                #     /\n                #    y\n                v1 = matrix[y][x - 1]\n                v2 = matrix[y][x]\n                delta = (y + 1) * granularity - x * sampling\n                previous = 0\n                if x > 0 and (x - 1) * sampling >= y * granularity:\n                    # x*g <= (y-1)*s <= y*s <= (x+1)*g <= (y+1)*s\n                    #           |________|.......^\n                    if x > 1:\n                        previous = matrix[y][x - 2]\n                    scale = sampling\n                else:\n                    # (y-1)*s < x*g <= y*s <= (x+1)*g <= (y+1)*s\n                    #            |______|.......^\n                    scale = sampling if x == 0 else x * sampling - y * granularity\n                peak = v1 + (v1 - previous) / scale * delta\n                if v2 > peak:\n                    # we need to adjust the peak, it may not be less than the decayed value\n                    if x < matrix.shape[1] - 1:\n                        # y*s <= (x+1)*g <= (y+1)*s < (y+2)*s\n                        #           ^.........|_________|\n                        k = (v2 - matrix[y][x + 1]) / sampling  # > 0\n                        peak = matrix[y][x] + k * (\n                            (x + 1) * sampling - (y + 1) * granularity\n                        )\n                        # peak > v2 > v1\n                    else:\n                        peak = v2\n                        # not enough data to interpolate; this is at least not restricted\n                grow((y + 1) * granularity, peak)\n                decay((y + 1) * granularity, peak)\n            else:\n                # (x+1)*granularity < y*sampling\n                # y*sampling..(y+1)sampling\n                decay(x * sampling, matrix[y][x - 1])\n    return daily\n\n\ndef load_burndown(\n    header: Tuple[int, int, int, int, float],\n    name: str,\n    matrix: numpy.ndarray,\n    resample: str,\n    report_survival: bool = True,\n    interpolation_progress: bool = False,\n) -> Tuple[str, numpy.ndarray, \'DatetimeIndex\', List[int], int, int, str]:\n    pandas = import_pandas()\n\n    start, last, sampling, granularity, tick = header\n    assert sampling > 0\n    assert granularity > 0\n    start = floor_datetime(datetime.fromtimestamp(start), tick)\n    last = datetime.fromtimestamp(last)\n    if report_survival:\n        kmf = fit_kaplan_meier(matrix)\n        if kmf is not None:\n            print_survival_function(kmf, sampling)\n    finish = start + timedelta(seconds=matrix.shape[1] * sampling * tick)\n    if resample not in (""no"", ""raw""):\n        print(""resampling to %s, please wait..."" % resample)\n        # Interpolate the day x day matrix.\n        # Each day brings equal weight in the granularity.\n        # Sampling\'s interpolation is linear.\n        daily = interpolate_burndown_matrix(\n            matrix=matrix,\n            granularity=granularity,\n            sampling=sampling,\n            progress=interpolation_progress,\n        )\n        daily[(last - start).days :] = 0\n        # Resample the bands\n        aliases = {""year"": ""A"", ""month"": ""M"", ""day"": ""D""}\n        resample = aliases.get(resample, resample)\n        periods = 0\n        date_granularity_sampling = [start]\n        while date_granularity_sampling[-1] < finish:\n            periods += 1\n            date_granularity_sampling = pandas.date_range(\n                start, periods=periods, freq=resample\n            )\n        if date_granularity_sampling[0] > finish:\n            if resample == ""A"":\n                print(""too loose resampling - by year, trying by month"")\n                return load_burndown(\n                    header, name, matrix, ""month"", report_survival=False\n                )\n            elif resample == ""M"":\n                print(""too loose resampling - by month, trying by day"")\n                return load_burndown(\n                    header, name, matrix, ""day"", report_survival=False\n                )\n            else:\n                raise ValueError(""Too loose resampling: %s. Try finer."" % resample)\n        date_range_sampling = pandas.date_range(\n            date_granularity_sampling[0],\n            periods=(finish - date_granularity_sampling[0]).days,\n            freq=""1D"",\n        )\n        # Fill the new square matrix\n        matrix = numpy.zeros(\n            (len(date_granularity_sampling), len(date_range_sampling)),\n            dtype=numpy.float32,\n        )\n        for i, gdt in enumerate(date_granularity_sampling):\n            istart = (date_granularity_sampling[i - 1] - start).days if i > 0 else 0\n            ifinish = (gdt - start).days\n\n            for j, sdt in enumerate(date_range_sampling):\n                if (sdt - start).days >= istart:\n                    break\n            matrix[i, j:] = daily[istart:ifinish, (sdt - start).days :].sum(axis=0)\n        # Hardcode some cases to improve labels\' readability\n        if resample in (""year"", ""A""):\n            labels = [dt.year for dt in date_granularity_sampling]\n        elif resample in (""month"", ""M""):\n            labels = [dt.strftime(""%Y %B"") for dt in date_granularity_sampling]\n        else:\n            labels = [dt.date() for dt in date_granularity_sampling]\n    else:\n        labels = [\n            ""%s - %s""\n            % (\n                (start + timedelta(seconds=i * granularity * tick)).date(),\n                (start + timedelta(seconds=(i + 1) * granularity * tick)).date(),\n            )\n            for i in range(matrix.shape[0])\n        ]\n        if len(labels) > 18:\n            warnings.warn(""Too many labels - consider resampling."")\n        resample = ""M""  # fake resampling type is checked while plotting\n        date_range_sampling = pandas.date_range(\n            start + timedelta(seconds=sampling * tick),\n            periods=matrix.shape[1],\n            freq=""%dD"" % sampling,\n        )\n    return name, matrix, date_range_sampling, labels, granularity, sampling, resample\n'"
python/labours/modes/devs.py,0,"b'from argparse import Namespace\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nimport sys\nfrom typing import Dict, List, Set, Tuple\n\nimport numpy\nimport tqdm\n\nfrom labours.objects import DevDay\nfrom labours.plotting import apply_plot_style, deploy_plot, get_plot_path, import_pyplot\nfrom labours.utils import _format_number\n\n\ndef show_devs(\n    args: Namespace,\n    name: str,\n    start_date: int,\n    end_date: int,\n    people: List[str],\n    days: Dict[int, Dict[int, DevDay]],\n    max_people: int = 50,\n) -> None:\n    from scipy.signal import convolve, slepian\n\n    if len(people) > max_people:\n        print(""Picking top %s developers by commit count"" % max_people)\n        # pick top N developers by commit count\n        commits = defaultdict(int)\n        for devs in days.values():\n            for dev, stats in devs.items():\n                commits[dev] += stats.Commits\n        commits = sorted(((v, k) for k, v in commits.items()), reverse=True)\n        chosen_people = {people[k] for _, k in commits[:max_people]}\n    else:\n        chosen_people = set(people)\n    dists, devseries, devstats, route = order_commits(chosen_people, days, people)\n    route_map = {v: i for i, v in enumerate(route)}\n    # determine clusters\n    clusters = hdbscan_cluster_routed_series(dists, route)\n    keys = list(devseries.keys())\n    route = [keys[node] for node in route]\n    print(""Plotting"")\n    # smooth time series\n    start_date = datetime.fromtimestamp(start_date)\n    start_date = datetime(start_date.year, start_date.month, start_date.day)\n    end_date = datetime.fromtimestamp(end_date)\n    end_date = datetime(end_date.year, end_date.month, end_date.day)\n    size = (end_date - start_date).days + 1\n    plot_x = [start_date + timedelta(days=i) for i in range(size)]\n    resolution = 64\n    window = slepian(size // resolution, 0.5)\n    final = numpy.zeros((len(devseries), size), dtype=numpy.float32)\n    for i, s in enumerate(devseries.values()):\n        arr = numpy.array(s).transpose()\n        full_history = numpy.zeros(size, dtype=numpy.float32)\n        mask = arr[0] < size\n        full_history[arr[0][mask]] = arr[1][mask]\n        final[route_map[i]] = convolve(full_history, window, ""same"")\n\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n    pyplot.rcParams[""figure.figsize""] = (32, 16)\n    pyplot.rcParams[""font.size""] = args.font_size\n    prop_cycle = pyplot.rcParams[""axes.prop_cycle""]\n    colors = prop_cycle.by_key()[""color""]\n    fig, axes = pyplot.subplots(final.shape[0], 1)\n    try:\n        axes = tuple(axes)\n    except TypeError:\n        axes = axes,\n    backgrounds = (\n        (""#C4FFDB"", ""#FFD0CD"") if args.background == ""white"" else (""#05401C"", ""#40110E"")\n    )\n    max_cluster = numpy.max(clusters)\n    for ax, series, cluster, dev_i in zip(axes, final, clusters, route):\n        if cluster >= 0:\n            color = colors[cluster % len(colors)]\n            i = 1\n            while color == ""#777777"":\n                color = colors[(max_cluster + i) % len(colors)]\n                i += 1\n        else:\n            # outlier\n            color = ""#777777""\n        ax.fill_between(plot_x, series, color=color)\n        ax.set_axis_off()\n        author = people[dev_i]\n        ax.text(\n            0.03,\n            0.5,\n            author[:36] + (author[36:] and ""...""),\n            horizontalalignment=""right"",\n            verticalalignment=""center"",\n            transform=ax.transAxes,\n            fontsize=args.font_size,\n            color=""black"" if args.background == ""white"" else ""white"",\n        )\n        ds = devstats[dev_i]\n        stats = ""%5d %8s %8s"" % (\n            ds[0],\n            _format_number(ds[1] - ds[2]),\n            _format_number(ds[3]),\n        )\n        ax.text(\n            0.97,\n            0.5,\n            stats,\n            horizontalalignment=""left"",\n            verticalalignment=""center"",\n            transform=ax.transAxes,\n            fontsize=args.font_size,\n            family=""monospace"",\n            backgroundcolor=backgrounds[ds[1] <= ds[2]],\n            color=""black"" if args.background == ""white"" else ""white"",\n        )\n    axes[0].text(\n        0.97,\n        1.75,\n        "" cmts    delta  changed"",\n        horizontalalignment=""left"",\n        verticalalignment=""center"",\n        transform=axes[0].transAxes,\n        fontsize=args.font_size,\n        family=""monospace"",\n        color=""black"" if args.background == ""white"" else ""white"",\n    )\n    axes[-1].set_axis_on()\n    target_num_labels = 12\n    num_months = (\n        (end_date.year - start_date.year) * 12 + end_date.month - start_date.month\n    )\n    interval = int(numpy.ceil(num_months / target_num_labels))\n    if interval >= 8:\n        interval = int(numpy.ceil(num_months / (12 * target_num_labels)))\n        axes[-1].xaxis.set_major_locator(\n            matplotlib.dates.YearLocator(base=max(1, interval // 12))\n        )\n        axes[-1].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(""%Y""))\n    else:\n        axes[-1].xaxis.set_major_locator(\n            matplotlib.dates.MonthLocator(interval=interval)\n        )\n        axes[-1].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(""%Y-%m""))\n    for tick in axes[-1].xaxis.get_major_ticks():\n        tick.label.set_fontsize(args.font_size)\n    axes[-1].spines[""left""].set_visible(False)\n    axes[-1].spines[""right""].set_visible(False)\n    axes[-1].spines[""top""].set_visible(False)\n    axes[-1].get_yaxis().set_visible(False)\n    axes[-1].set_facecolor((1.0,) * 3 + (0.0,))\n\n    title = (""%s commits"" % name) if not args.output else """"\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""time_series"")\n    else:\n        output = args.output\n    deploy_plot(title, output, args.background)\n\n\ndef order_commits(\n    chosen_people: Set[str], days: Dict[int, Dict[int, DevDay]], people: List[str]\n) -> Tuple[numpy.ndarray, defaultdict, defaultdict, List[int]]:\n    from seriate import seriate\n\n    try:\n        from fastdtw import fastdtw\n    except ImportError as e:\n        print(\n            ""Cannot import fastdtw: %s\\nInstall it from https://github.com/slaypni/fastdtw""\n            % e\n        )\n        sys.exit(1)\n    # FIXME(vmarkovtsev): remove once https://github.com/slaypni/fastdtw/pull/28 is merged&released\n    try:\n        sys.modules[\n            ""fastdtw.fastdtw""\n        ].__norm = lambda p: lambda a, b: numpy.linalg.norm(\n            numpy.atleast_1d(a) - numpy.atleast_1d(b), p\n        )\n    except KeyError:\n        # the native extension does not have this bug\n        pass\n\n    devseries = defaultdict(list)\n    devstats = defaultdict(lambda: DevDay(0, 0, 0, 0, {}))\n    for day, devs in sorted(days.items()):\n        for dev, stats in devs.items():\n            if people[dev] in chosen_people:\n                devseries[dev].append((day, stats.Commits))\n                devstats[dev] = devstats[dev].add(stats)\n    print(""Calculating the distance matrix"")\n    # max-normalize the time series using a sliding window\n    series = list(devseries.values())\n    for i, s in enumerate(series):\n        arr = numpy.array(s).transpose().astype(numpy.float32)\n        arr[1] /= arr[1].sum()\n        series[i] = arr.transpose()\n    # calculate the distance matrix using dynamic time warping\n    dists = numpy.full((len(series),) * 2, -100500, dtype=numpy.float32)\n    # TODO: what\'s the total for this progress bar?\n    with tqdm.tqdm() as pb:\n        for x, serx in enumerate(series):\n            dists[x, x] = 0\n            for y, sery in enumerate(series[x + 1 :], start=x + 1):\n                min_day = int(min(serx[0][0], sery[0][0]))\n                max_day = int(max(serx[-1][0], sery[-1][0]))\n                arrx = numpy.zeros(max_day - min_day + 1, dtype=numpy.float32)\n                arry = numpy.zeros_like(arrx)\n                arrx[serx[:, 0].astype(int) - min_day] = serx[:, 1]\n                arry[sery[:, 0].astype(int) - min_day] = sery[:, 1]\n                # L1 norm\n                dist, _ = fastdtw(arrx, arry, radius=5, dist=1)\n                dists[x, y] = dists[y, x] = dist\n                pb.update()\n    print(""Ordering the series"")\n    route = seriate(dists)\n    return dists, devseries, devstats, route\n\n\ndef hdbscan_cluster_routed_series(\n    dists: numpy.ndarray, route: List[int]\n) -> numpy.ndarray:\n    try:\n        from hdbscan import HDBSCAN\n    except ImportError as e:\n        print(""Cannot import hdbscan: %s"" % e)\n        sys.exit(1)\n\n    opt_dist_chain = numpy.cumsum(\n        numpy.array(\n            [0] + [dists[route[i], route[i + 1]] for i in range(len(route) - 1)]\n        )\n    )\n    if len(route) < 2:\n        clusters = numpy.zeros(len(route), dtype=int)\n    else:\n        clusters = HDBSCAN(min_cluster_size=2).fit_predict(opt_dist_chain[:, numpy.newaxis])\n    return clusters\n\n\ndef show_devs_efforts(\n    args: Namespace,\n    name: str,\n    start_date: int,\n    end_date: int,\n    people: List[str],\n    days: Dict[int, Dict[int, DevDay]],\n    max_people: int,\n) -> None:\n    from scipy.signal import convolve, slepian\n\n    start_date = datetime.fromtimestamp(start_date)\n    start_date = datetime(start_date.year, start_date.month, start_date.day)\n    end_date = datetime.fromtimestamp(end_date)\n    end_date = datetime(end_date.year, end_date.month, end_date.day)\n\n    efforts_by_dev = defaultdict(int)\n    for day, devs in days.items():\n        for dev, stats in devs.items():\n            efforts_by_dev[dev] += stats.Added + stats.Removed + stats.Changed\n    if len(efforts_by_dev) > max_people:\n        chosen = {\n            v\n            for k, v in sorted(\n                ((v, k) for k, v in efforts_by_dev.items()), reverse=True\n            )[:max_people]\n        }\n        print(""Warning: truncated people to the most active %d"" % max_people)\n    else:\n        chosen = set(efforts_by_dev)\n    chosen_efforts = sorted(((efforts_by_dev[k], k) for k in chosen), reverse=True)\n    chosen_order = {k: i for i, (_, k) in enumerate(chosen_efforts)}\n\n    efforts = numpy.zeros(\n        (len(chosen) + 1, (end_date - start_date).days + 1), dtype=numpy.float32\n    )\n    for day, devs in days.items():\n        if day < efforts.shape[1]:\n            for dev, stats in devs.items():\n                dev = chosen_order.get(dev, len(chosen_order))\n                efforts[dev][day] += stats.Added + stats.Removed + stats.Changed\n    efforts_cum = numpy.cumsum(efforts, axis=1)\n    window = slepian(10, 0.5)\n    window /= window.sum()\n    for e in (efforts, efforts_cum):\n        for i in range(e.shape[0]):\n            ending = e[i][-len(window) * 2 :].copy()\n            e[i] = convolve(e[i], window, ""same"")\n            e[i][-len(ending) :] = ending\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n    plot_x = [start_date + timedelta(days=i) for i in range(efforts.shape[1])]\n\n    people = [people[k] for _, k in chosen_efforts] + [""others""]\n    for i, name in enumerate(people):\n        if len(name) > 40:\n            people[i] = name[:37] + ""...""\n\n    polys = pyplot.stackplot(plot_x, efforts_cum, labels=people)\n    if len(polys) == max_people + 1:\n        polys[-1].set_hatch(""/"")\n    polys = pyplot.stackplot(plot_x, -efforts * efforts_cum.max() / efforts.max())\n    if len(polys) == max_people + 1:\n        polys[-1].set_hatch(""/"")\n    yticks = []\n    for tick in pyplot.gca().yaxis.iter_ticks():\n        if tick[1] >= 0:\n            yticks.append(tick[1])\n    pyplot.gca().yaxis.set_ticks(yticks)\n    legend = pyplot.legend(loc=2, ncol=2, fontsize=args.font_size)\n    apply_plot_style(\n        pyplot.gcf(),\n        pyplot.gca(),\n        legend,\n        args.background,\n        args.font_size,\n        args.size or ""16,10"",\n    )\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""efforts"")\n    else:\n        output = args.output\n    deploy_plot(""Efforts through time (changed lines of code)"", output, args.background)\n'"
python/labours/modes/devs_parallel.py,0,"b'from collections import defaultdict\nimport sys\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy\nfrom scipy.sparse.csr import csr_matrix\n\nfrom labours.modes.devs import hdbscan_cluster_routed_series, order_commits\nfrom labours.objects import DevDay, ParallelDevData\nfrom labours.plotting import deploy_plot, import_pyplot\n\n\ndef load_devs_parallel(\n    ownership: Tuple[List[Any], Dict[Any, Any]],\n    couples: Tuple[List[str], csr_matrix],\n    devs: Tuple[List[str], Dict[int, Dict[int, DevDay]]],\n    max_people: int,\n):\n    from seriate import seriate\n\n    try:\n        from hdbscan import HDBSCAN\n    except ImportError as e:\n        print(\n            ""Cannot import ortools: %s\\nInstall it from ""\n            ""https://developers.google.com/optimization/install/python/"" % e\n        )\n        sys.exit(1)\n\n    people, owned = ownership\n    _, cmatrix = couples\n    _, days = devs\n\n    print(""calculating - commits"")\n    commits = defaultdict(int)\n    for day, devs in days.items():\n        for dev, stats in devs.items():\n            commits[people[dev]] += stats.Commits\n    chosen = [\n        k\n        for v, k in sorted(((v, k) for k, v in commits.items()), reverse=True)[\n            :max_people\n        ]\n    ]\n    result = {k: ParallelDevData() for k in chosen}\n    for k, v in result.items():\n        v.commits_rank = chosen.index(k)\n        v.commits = commits[k]\n\n    print(""calculating - lines"")\n    lines = defaultdict(int)\n    for day, devs in days.items():\n        for dev, stats in devs.items():\n            lines[people[dev]] += stats.Added + stats.Removed + stats.Changed\n    lines_index = {\n        k: i\n        for i, (_, k) in enumerate(\n            sorted(((v, k) for k, v in lines.items() if k in chosen), reverse=True)\n        )\n    }\n    for k, v in result.items():\n        v.lines_rank = lines_index[k]\n        v.lines = lines[k]\n\n    print(""calculating - ownership"")\n    owned_index = {\n        k: i\n        for i, (_, k) in enumerate(\n            sorted(((owned[k][-1].sum(), k) for k in chosen), reverse=True)\n        )\n    }\n    for k, v in result.items():\n        v.ownership_rank = owned_index[k]\n        v.ownership = owned[k][-1].sum()\n\n    print(""calculating - couples"")\n    embeddings = numpy.genfromtxt(fname=""couples_people_data.tsv"", delimiter=""\\t"")[\n        [people.index(k) for k in chosen]\n    ]\n    embeddings /= numpy.linalg.norm(embeddings, axis=1)[:, None]\n    cos = embeddings.dot(embeddings.T)\n    cos[cos > 1] = 1  # tiny precision faults\n    dists = numpy.arccos(cos)\n    clusters = HDBSCAN(min_cluster_size=2, metric=""precomputed"").fit_predict(dists)\n    for k, v in result.items():\n        v.couples_cluster = clusters[chosen.index(k)]\n\n    couples_order = seriate(dists)\n    roll_options = []\n    for i in range(len(couples_order)):\n        loss = 0\n        for k, v in result.items():\n            loss += abs(\n                v.ownership_rank\n                - (couples_order.index(chosen.index(k)) + i) % len(chosen)\n            )\n        roll_options.append(loss)\n    best_roll = numpy.argmin(roll_options)\n    couples_order = list(numpy.roll(couples_order, best_roll))\n    for k, v in result.items():\n        v.couples_index = couples_order.index(chosen.index(k))\n\n    print(""calculating - commit series"")\n    dists, devseries, _, orig_route = order_commits(chosen, days, people)\n    keys = list(devseries.keys())\n    route = [keys[node] for node in orig_route]\n    for roll in range(len(route)):\n        loss = 0\n        for k, v in result.items():\n            i = route.index(people.index(k))\n            loss += abs(v.couples_index - ((i + roll) % len(route)))\n        roll_options[roll] = loss\n    best_roll = numpy.argmin(roll_options)\n    route = list(numpy.roll(route, best_roll))\n    orig_route = list(numpy.roll(orig_route, best_roll))\n    clusters = hdbscan_cluster_routed_series(dists, orig_route)\n    for k, v in result.items():\n        v.commit_coocc_index = route.index(people.index(k))\n        v.commit_coocc_cluster = clusters[v.commit_coocc_index]\n\n    return result\n\n\ndef show_devs_parallel(args, name, start_date, end_date, devs):\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n    from matplotlib.collections import LineCollection\n\n    def solve_equations(x1, y1, x2, y2):\n        xcube = (x1 - x2) ** 3\n        a = 2 * (y2 - y1) / xcube\n        b = 3 * (y1 - y2) * (x1 + x2) / xcube\n        c = 6 * (y2 - y1) * x1 * x2 / xcube\n        d = y1 - a * x1 ** 3 - b * x1 ** 2 - c * x1\n        return a, b, c, d\n\n    # biggest = {k: max(getattr(d, k) for d in devs.values())\n    #            for k in (""commits"", ""lines"", ""ownership"")}\n    for k, dev in devs.items():\n        points = numpy.array(\n            [\n                (1, dev.commits_rank),\n                (2, dev.lines_rank),\n                (3, dev.ownership_rank),\n                (4, dev.couples_index),\n                (5, dev.commit_coocc_index),\n            ],\n            dtype=float,\n        )\n        points[:, 1] = points[:, 1] / len(devs)\n        splines = []\n        for i in range(len(points) - 1):\n            a, b, c, d = solve_equations(*points[i], *points[i + 1])\n            x = numpy.linspace(i + 1, i + 2, 100)\n            smooth_points = numpy.array(\n                [x, a * x ** 3 + b * x ** 2 + c * x + d]\n            ).T.reshape(-1, 1, 2)\n            splines.append(smooth_points)\n        points = numpy.concatenate(splines)\n        segments = numpy.concatenate([points[:-1], points[1:]], axis=1)\n        lc = LineCollection(segments)\n        lc.set_array(numpy.linspace(0, 0.1, segments.shape[0]))\n        pyplot.gca().add_collection(lc)\n\n    pyplot.xlim(0, 6)\n    pyplot.ylim(-0.1, 1.1)\n    deploy_plot(""Developers"", args.output, args.background)\n'"
python/labours/modes/languages.py,0,"b'from argparse import Namespace\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport numpy\n\nfrom labours.objects import DevDay\n\n\ndef show_languages(\n    args: Namespace,\n    name: str,\n    start_date: int,\n    end_date: int,\n    people: List[str],\n    days: Dict[int, Dict[int, DevDay]],\n) -> None:\n    devlangs = defaultdict(lambda: defaultdict(lambda: numpy.zeros(3, dtype=int)))\n    for day, devs in days.items():\n        for dev, stats in devs.items():\n            for lang, vals in stats.Languages.items():\n                devlangs[dev][lang] += vals\n    devlangs = sorted(\n        devlangs.items(), key=lambda p: -sum(x.sum() for x in p[1].values())\n    )\n    for dev, ls in devlangs:\n        print()\n        print(""#"", people[dev])\n        ls = sorted(((vals.sum(), lang) for lang, vals in ls.items()), reverse=True)\n        for vals, lang in ls:\n            if lang:\n                print(""%s: %d"" % (lang, vals))\n'"
python/labours/modes/old_vs_new.py,0,"b'from argparse import Namespace\nfrom datetime import datetime, timedelta\nfrom itertools import chain\nfrom typing import Dict, List\n\nimport numpy\n\nfrom labours.objects import DevDay\nfrom labours.plotting import deploy_plot, get_plot_path, import_pyplot\n\n\ndef show_old_vs_new(\n    args: Namespace,\n    name: str,\n    start_date: int,\n    end_date: int,\n    people: List[str],\n    days: Dict[int, Dict[int, DevDay]],\n) -> None:\n    from scipy.signal import convolve, slepian\n\n    start_date = datetime.fromtimestamp(start_date)\n    start_date = datetime(start_date.year, start_date.month, start_date.day)\n    end_date = datetime.fromtimestamp(end_date)\n    end_date = datetime(end_date.year, end_date.month, end_date.day)\n    new_lines = numpy.zeros((end_date - start_date).days + 2)\n    old_lines = numpy.zeros_like(new_lines)\n    for day, devs in days.items():\n        for stats in devs.values():\n            new_lines[day] += stats.Added\n            old_lines[day] += stats.Removed + stats.Changed\n    resolution = 32\n    window = slepian(max(len(new_lines) // resolution, 1), 0.5)\n    new_lines = convolve(new_lines, window, ""same"")\n    old_lines = convolve(old_lines, window, ""same"")\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n    plot_x = [start_date + timedelta(days=i) for i in range(len(new_lines))]\n    pyplot.fill_between(plot_x, new_lines, color=""#8DB843"", label=""Changed new lines"")\n    pyplot.fill_between(\n        plot_x, old_lines, color=""#E14C35"", label=""Changed existing lines""\n    )\n    pyplot.legend(loc=2, fontsize=args.font_size)\n    for tick in chain(\n        pyplot.gca().xaxis.get_major_ticks(), pyplot.gca().yaxis.get_major_ticks()\n    ):\n        tick.label.set_fontsize(args.font_size)\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""old_vs_new"")\n    else:\n        output = args.output\n    deploy_plot(""Additions vs changes"", output, args.background)\n'"
python/labours/modes/overwrites.py,0,"b'import json\n\nimport numpy\n\nfrom labours.plotting import apply_plot_style, deploy_plot, get_plot_path, import_pyplot\nfrom labours.utils import default_json\n\n\ndef load_overwrites_matrix(people, matrix, max_people, normalize=True):\n    matrix = matrix.astype(float)\n    if matrix.shape[0] > max_people:\n        order = numpy.argsort(-matrix[:, 0])\n        matrix = matrix[order[:max_people]][:, [0, 1] + list(2 + order[:max_people])]\n        people = [people[i] for i in order[:max_people]]\n        print(""Warning: truncated people to most productive %d"" % max_people)\n    if normalize:\n        zeros = matrix[:, 0] == 0\n        matrix[zeros, :] = 1\n        matrix /= matrix[:, 0][:, None]\n        matrix[zeros, :] = 0\n    matrix = -matrix[:, 1:]\n    for i, name in enumerate(people):\n        if len(name) > 40:\n            people[i] = name[:37] + ""...""\n    return people, matrix\n\n\ndef plot_overwrites_matrix(args, repo, people, matrix):\n    if args.output and args.output.endswith("".json""):\n        data = locals().copy()\n        del data[""args""]\n        data[""type""] = ""overwrites_matrix""\n        if args.mode == ""all"":\n            output = get_plot_path(args.output, ""matrix"")\n        else:\n            output = args.output\n        with open(output, ""w"") as fout:\n            json.dump(data, fout, sort_keys=True, default=default_json)\n        return\n\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n\n    s = 4 + matrix.shape[1] * 0.3\n    fig = pyplot.figure(figsize=(s, s))\n    ax = fig.add_subplot(111)\n    ax.xaxis.set_label_position(""top"")\n    ax.matshow(matrix, cmap=pyplot.cm.OrRd)\n    ax.set_xticks(numpy.arange(0, matrix.shape[1]))\n    ax.set_yticks(numpy.arange(0, matrix.shape[0]))\n    ax.set_yticklabels(people, va=""center"")\n    ax.set_xticks(numpy.arange(0.5, matrix.shape[1] + 0.5), minor=True)\n    ax.set_xticklabels(\n        [""Unidentified""] + people,\n        rotation=45,\n        ha=""left"",\n        va=""bottom"",\n        rotation_mode=""anchor"",\n    )\n    ax.set_yticks(numpy.arange(0.5, matrix.shape[0] + 0.5), minor=True)\n    ax.grid(False)\n    ax.grid(which=""minor"")\n    apply_plot_style(fig, ax, None, args.background, args.font_size, args.size)\n    if not args.output:\n        pos1 = ax.get_position()\n        pos2 = (pos1.x0 + 0.15, pos1.y0 - 0.1, pos1.width * 0.9, pos1.height * 0.9)\n        ax.set_position(pos2)\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""matrix"")\n    else:\n        output = args.output\n    title = ""%s %d developers overwrite"" % (repo, matrix.shape[0])\n    if args.output:\n        # FIXME(vmarkovtsev): otherwise the title is screwed in savefig()\n        title = """"\n    deploy_plot(title, output, args.background)\n'"
python/labours/modes/ownership.py,0,"b'from datetime import datetime, timedelta\nimport json\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy\n\nfrom labours.plotting import apply_plot_style, deploy_plot, get_plot_path, import_pyplot\nfrom labours.utils import default_json, floor_datetime, import_pandas, parse_date\n\n\ndef load_ownership(\n    header: Tuple[int, int, int, int, float],\n    sequence: List[Any],\n    contents: Dict[Any, Any],\n    max_people: int,\n    order_by_time: bool,\n):\n    pandas = import_pandas()\n\n    start, last, sampling, _, tick = header\n    start = datetime.fromtimestamp(start)\n    start = floor_datetime(start, tick)\n    last = datetime.fromtimestamp(last)\n    people = []\n    for name in sequence:\n        people.append(contents[name].sum(axis=1))\n    people = numpy.array(people)\n    date_range_sampling = pandas.date_range(\n        start + timedelta(seconds=sampling * tick),\n        periods=people[0].shape[0],\n        freq=""%dD"" % sampling,\n    )\n\n    if people.shape[0] > max_people:\n        chosen = numpy.argpartition(-numpy.sum(people, axis=1), max_people)\n        others = people[chosen[max_people:]].sum(axis=0)\n        people = people[chosen[: max_people + 1]]\n        people[max_people] = others\n        sequence = [sequence[i] for i in chosen[:max_people]] + [""others""]\n        print(""Warning: truncated people to the most owning %d"" % max_people)\n\n    if order_by_time:\n        appearances = numpy.argmax(people > 0, axis=1)\n        if people.shape[0] > max_people:\n            appearances[-1] = people.shape[1]\n    else:\n        appearances = -people.sum(axis=1)\n        if people.shape[0] > max_people:\n            appearances[-1] = 0\n    order = numpy.argsort(appearances)\n    people = people[order]\n    sequence = [sequence[i] for i in order]\n\n    for i, name in enumerate(sequence):\n        if len(name) > 40:\n            sequence[i] = name[:37] + ""...""\n    return sequence, people, date_range_sampling, last\n\n\ndef plot_ownership(args, repo, names, people, date_range, last):\n    if args.output and args.output.endswith("".json""):\n        data = locals().copy()\n        del data[""args""]\n        data[""type""] = ""ownership""\n        if args.mode == ""all"" and args.output:\n            output = get_plot_path(args.output, ""people"")\n        else:\n            output = args.output\n        with open(output, ""w"") as fout:\n            json.dump(data, fout, sort_keys=True, default=default_json)\n        return\n\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n\n    polys = pyplot.stackplot(date_range, people, labels=names)\n    if names[-1] == ""others"":\n        polys[-1].set_hatch(""/"")\n    pyplot.xlim(\n        parse_date(args.start_date, date_range[0]), parse_date(args.end_date, last)\n    )\n\n    if args.relative:\n        for i in range(people.shape[1]):\n            people[:, i] /= people[:, i].sum()\n        pyplot.ylim(0, 1)\n        legend_loc = 3\n    else:\n        legend_loc = 2\n    ncol = 1 if len(names) < 15 else 2\n    legend = pyplot.legend(loc=legend_loc, fontsize=args.font_size, ncol=ncol)\n    apply_plot_style(\n        pyplot.gcf(), pyplot.gca(), legend, args.background, args.font_size, args.size\n    )\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""people"")\n    else:\n        output = args.output\n    deploy_plot(""%s code ownership through time"" % repo, output, args.background)\n'"
python/labours/modes/sentiment.py,0,"b'from datetime import datetime, timedelta\n\nimport numpy\n\nfrom labours.plotting import apply_plot_style, deploy_plot, get_plot_path, import_pyplot\nfrom labours.utils import parse_date\n\n\ndef show_sentiment_stats(args, name, resample, start_date, data):\n    from scipy.signal import convolve, slepian\n\n    matplotlib, pyplot = import_pyplot(args.backend, args.style)\n\n    start_date = datetime.fromtimestamp(start_date)\n    data = sorted(data.items())\n    mood = numpy.zeros(data[-1][0] + 1, dtype=numpy.float32)\n    timeline = numpy.array(\n        [start_date + timedelta(days=i) for i in range(mood.shape[0])]\n    )\n    for d, val in data:\n        mood[d] = (0.5 - val.Value) * 2\n    resolution = 32\n    window = slepian(len(timeline) // resolution, 0.5)\n    window /= window.sum()\n    mood_smooth = convolve(mood, window, ""same"")\n    pos = mood_smooth.copy()\n    pos[pos < 0] = 0\n    neg = mood_smooth.copy()\n    neg[neg >= 0] = 0\n    resolution = 4\n    window = numpy.ones(len(timeline) // resolution)\n    window /= window.sum()\n    avg = convolve(mood, window, ""same"")\n    pyplot.fill_between(timeline, pos, color=""#8DB843"", label=""Positive"")\n    pyplot.fill_between(timeline, neg, color=""#E14C35"", label=""Negative"")\n    pyplot.plot(timeline, avg, color=""grey"", label=""Average"", linewidth=5)\n    legend = pyplot.legend(loc=1, fontsize=args.font_size)\n    pyplot.ylabel(""Comment sentiment"")\n    pyplot.xlabel(""Time"")\n    apply_plot_style(\n        pyplot.gcf(), pyplot.gca(), legend, args.background, args.font_size, args.size\n    )\n    pyplot.xlim(\n        parse_date(args.start_date, timeline[0]),\n        parse_date(args.end_date, timeline[-1]),\n    )\n    locator = pyplot.gca().xaxis.get_major_locator()\n    # set the optimal xticks locator\n    if ""M"" not in resample:\n        pyplot.gca().xaxis.set_major_locator(matplotlib.dates.YearLocator())\n    locs = pyplot.gca().get_xticks().tolist()\n    if len(locs) >= 16:\n        pyplot.gca().xaxis.set_major_locator(matplotlib.dates.YearLocator())\n        locs = pyplot.gca().get_xticks().tolist()\n        if len(locs) >= 16:\n            pyplot.gca().xaxis.set_major_locator(locator)\n    if locs[0] < pyplot.xlim()[0]:\n        del locs[0]\n    endindex = -1\n    if len(locs) >= 2 and pyplot.xlim()[1] - locs[-1] > (locs[-1] - locs[-2]) / 2:\n        locs.append(pyplot.xlim()[1])\n        endindex = len(locs) - 1\n    startindex = -1\n    if len(locs) >= 2 and locs[0] - pyplot.xlim()[0] > (locs[1] - locs[0]) / 2:\n        locs.append(pyplot.xlim()[0])\n        startindex = len(locs) - 1\n    pyplot.gca().set_xticks(locs)\n    # hacking time!\n    labels = pyplot.gca().get_xticklabels()\n    if startindex >= 0:\n        labels[startindex].set_text(timeline[0].date())\n        labels[startindex].set_text = lambda _: None\n        labels[startindex].set_rotation(30)\n        labels[startindex].set_ha(""right"")\n    if endindex >= 0:\n        labels[endindex].set_text(timeline[-1].date())\n        labels[endindex].set_text = lambda _: None\n        labels[endindex].set_rotation(30)\n        labels[endindex].set_ha(""right"")\n    overall_pos = sum(2 * (0.5 - d[1].Value) for d in data if d[1].Value < 0.5)\n    overall_neg = sum(2 * (d[1].Value - 0.5) for d in data if d[1].Value > 0.5)\n    title = ""%s sentiment +%.1f -%.1f \xce\xb4=%.1f"" % (\n        name,\n        overall_pos,\n        overall_neg,\n        overall_pos - overall_neg,\n    )\n    if args.mode == ""all"" and args.output:\n        output = get_plot_path(args.output, ""sentiment"")\n    else:\n        output = args.output\n    deploy_plot(title, output, args.background)\n'"
python/labours/modes/shotness.py,0,"b'def show_shotness_stats(data):\n    top = sorted(((r.counters[i], i) for i, r in enumerate(data)), reverse=True)\n    for count, i in top:\n        r = data[i]\n        print(""%8d  %s:%s [%s]"" % (count, r.file, r.name, r.internal_role))\n'"
