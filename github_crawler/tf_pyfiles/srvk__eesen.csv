file_path,api_count,code
asr_egs/librispeech/utils/ctc_token_fst.py,0,"b""#!/usr/bin/env python\n\n# Apache 2.0\n\nimport sys\n\nfread = open(sys.argv[1], 'r')\n\nprint '0 1 <eps> <eps>'\nprint '1 1 <blk> <eps>'\nprint '2 2 <blk> <eps>'\nprint '2 0 <eps> <eps>'\n\nnodeX = 3\nfor entry in fread.readlines():\n    entry = entry.replace('\\n','').strip()\n    fields = entry.split(' ')\n    phone = fields[0]\n    if phone == '<eps>' or phone == '<blk>':\n      continue\n   \n    if '#' in phone:\n      print str(0) + ' ' + str(0) + ' ' +  '<eps>' + ' ' + phone;\n    else:\n      print str(1) + ' ' + str(nodeX) + ' ' +  phone + ' ' + phone;\n      print str(nodeX) + ' ' + str(nodeX) + ' ' +  phone + ' <eps>';\n      print str(nodeX) + ' ' + str(2) + ' ' + '<eps> <eps>';\n    nodeX += 1\nprint '0'\n\nfread.close()\n"""
asr_egs/librispeech/utils/prep_ctc_trans.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015       Yajie Miao    (Carnegie Mellon University)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# This python script converts the word-based transcripts into label sequences. The labels are\n# represented by their indices. \n\nimport sys\n\nif __name__ == \'__main__\':\n\n    if len(sys.argv) < 4 or len(sys.argv) > 5:\n        print ""Usage: {0} <lexicon_file> <trans_file> <unk_word> [space_word]"".format(sys.argv[0])\n        print ""e.g., utils/prep_ctc_trans.py data/lang/lexicon_numbers.txt data/train/text <UNK>""\n        print ""<lexicon_file> - the lexicon file in which entries have been represented by indices""\n        print ""<trans_file>   - the word-based transcript file""\n        print ""<unk_word>     - the word which represents OOVs in transcripts""\n        print ""[space_word]   - optional, the word representing spaces in the transcripts""\n        exit(1)\n\n    dict_file = sys.argv[1]\n    trans_file = sys.argv[2]\n    unk_word = sys.argv[3]\n\n    is_char = False\n    if len(sys.argv) == 5:\n        is_char = True\n        space_word = sys.argv[4]\n\n    # read the lexicon into a dictionary data structure\n    fread = open(dict_file,\'r\')\n    dict = {}\n    for line in fread.readlines():\n        line = line.replace(\'\\n\',\'\')\n        splits = line.split(\' \')  # assume there are no multiple spaces\n        word = splits[0]\n        letters = \'\'\n        for n in range(1, len(splits)):\n            letters += splits[n] + \' \'\n        dict[word] = letters.strip()\n    fread.close()\n\n    # assume that each line is formatted as ""uttid word1 word2 word3 ..."", with no multiple spaces appearing\n    fread = open(trans_file,\'r\')\n    for line in fread.readlines():\n        out_line = \'\'\n        line = line.replace(\'\\n\',\'\').strip()\n        while \'  \' in line:\n            line = line.replace(\'  \', \' \')   # remove multiple spaces in the transcripts\n        \n        uttid = line.split(\' \')[0]  # the first field is always utterance id\n        trans = line.replace(uttid, \'\').strip()\n        if is_char:\n            trans = trans.replace(\' \', \' \' + space_word + \' \')\n        splits = trans.split(\' \')    \n\n        out_line += uttid + \' \'\n        for n in range(0, len(splits)):\n            try:\n              out_line += dict[splits[n]] + \' \'\n            except Exception:\n              out_line += dict[unk_word] + \' \'\n        print out_line.strip()\n'"
asr_egs/wsj/utils/ctc_token_fst.py,0,"b""#!/usr/bin/env python\n\n# Apache 2.0\n\nimport sys\n\nfread = open(sys.argv[1], 'r')\n\nprint '0 1 <eps> <eps>'\nprint '1 1 <blk> <eps>'\nprint '2 2 <blk> <eps>'\nprint '2 0 <eps> <eps>'\n\nnodeX = 3\nfor entry in fread.readlines():\n    entry = entry.replace('\\n','').strip()\n    fields = entry.split(' ')\n    phone = fields[0]\n    if phone == '<eps>' or phone == '<blk>':\n      continue\n   \n    if '#' in phone:\n      print str(0) + ' ' + str(0) + ' ' +  '<eps>' + ' ' + phone;\n    else:\n      print str(1) + ' ' + str(nodeX) + ' ' +  phone + ' ' + phone;\n      print str(nodeX) + ' ' + str(nodeX) + ' ' +  phone + ' <eps>';\n      print str(nodeX) + ' ' + str(2) + ' ' + '<eps> <eps>';\n    nodeX += 1\nprint '0'\n\nfread.close()\n"""
asr_egs/wsj/utils/model_topo.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015       Yajie Miao    (Carnegie Mellon University)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\n\ndef parse_arguments(arg_elements):\n    args = {}\n    arg_num = len(arg_elements) / 2\n    for i in xrange(arg_num):\n        key = arg_elements[2*i].replace(""--"","""").replace(""-"", ""_"");\n        args[key] = arg_elements[2*i+1]\n    return args\n\n\nif __name__ == \'__main__\':\n\n    """"""\n    Python script to generate the network topology. Parameters:\n    ------------------\n    --input-feat-dim : int\n        Dimension of the input features\n        Required.\n    --lstm-layer-num : int\n        Number of LSTM layers\n        Required.\n    --lstm-cell-dim : int\n        Number of memory cells in LSTM. For the bi-directional case, this is the number of cells\n        in either the forward or the backward sub-layer.\n        Required.\n    --target-num : int\n        Number of labels as the targets\n        Required.\n    --param-range : float\n        Range to randomly draw the initial values of model parameters. For example, setting it to\n        0.1 means model parameters are drawn uniformly from [-0.1, 0.1]\n        Optional. By default it is set to 0.1.\n    --lstm-type : string\n        Type of LSTMs. Optional. Either ""bi"" (bi-directional) or ""uni"" (uni-directional). By default,\n        ""bi"" (bi-directional).\n    --fgate-bias-init : float\n        Initial value of the forget-gate bias. Not specifying this option means the forget-gate bias\n        will be initialized randomly, in the same way as the other parameters. \n    --input-dim : int\n        Reduce the input feature to a given dimensionality before passing to the LSTM.\n        Optional.\n    --projection-dim : int\n        Project the feature vector down to a given dimensionality between LSTM layers.\n        Optional.\n    --max-grad : int\n        Clip the gradient of all layers to the specified value.\n        Optional\n    """"""\n\n    # parse arguments\n    arg_elements = [sys.argv[i] for i in range(1, len(sys.argv))]\n    arguments = parse_arguments(arg_elements)\n\n    # these 4 arguments are mandatory\n    input_feat_dim=int(arguments[\'input_feat_dim\'])\n    lstm_layer_num=int(arguments[\'lstm_layer_num\'])\n    lstm_cell_dim=int(arguments[\'lstm_cell_dim\'])\n    target_num=int(arguments[\'target_num\'])\n\n    # by default, the range of the parameters is set to 0.1; however, you can change it by specifying ""--param-range""\n    # this means for initialization, model parameters are drawn uniformly from the interval [-0.1, 0.1]\n    param_range=\'0.1\'\n    if arguments.has_key(\'param_range\'):\n        param_range = arguments[\'param_range\']\n\n    actual_cell_dim = 2*lstm_cell_dim\n    model_type = \'<BiLstmParallel>\'   # by default\n    if arguments.has_key(\'lstm_type\') and arguments[\'lstm_type\'] == \'uni\':\n        actual_cell_dim = lstm_cell_dim\n        model_type = \'<LstmParallel>\'\n\n    max_grad = 50.0\n    if arguments.has_key(\'max_grad\'):\n        max_grad=float(arguments[\'max_grad\'])\n\n    # add the option to set the initial value of the forget-gate bias\n    lstm_comm = \' <ParamRange> \' + param_range + \' <LearnRateCoef> 1.0 <MaxGrad> \' + str(max_grad)\n    if arguments.has_key(\'fgate_bias_init\'):\n        lstm_comm = lstm_comm + \' <FgateBias> \' + arguments[\'fgate_bias_init\']\n\n    # add the option to specify projection layers\n    if arguments.has_key(\'projection_dim\'):\n        proj_dim = arguments[\'projection_dim\']\n    else:\n        proj_dim = 0\n\n    # add the option to reduce the dimensionality of the input features\n    if arguments.has_key(\'input_dim\'):\n        input_dim = arguments[\'input_dim\']\n    else:\n        input_dim = 0\n\n\n    # pre-amble\n    print \'<Nnet>\'\n\n    # optional dimensionality reduction layer\n    if input_dim > 0:\n        print \'<AffineTransform> <InputDim> \' + str(input_feat_dim) + \' <OutputDim> \' + str(input_dim) + \' <ParamRange>\' + param_range + \' <MaxGrad> \' + str(max_grad)\n        input_feat_dim = input_dim\n\n    # the first layer takes input features\n    print model_type + \' <InputDim> \' + str(input_feat_dim) + \' <CellDim> \' + str(actual_cell_dim) + lstm_comm\n    # the following bidirectional LSTM layers\n    for n in range(1, lstm_layer_num):\n        if proj_dim > 0:\n            print \'<AffineTransform> <InputDim> \' + str(actual_cell_dim) + \' <OutputDim> \' + str(proj_dim) + \'<ParamRange> \' + param_range + \' <MaxGrad> \' + max_grad\n            print model_type + \' <InputDim> \' +        str(proj_dim) + \' <CellDim> \' + str(actual_cell_dim) + lstm_comm\n        else:\n            print model_type + \' <InputDim> \' + str(actual_cell_dim) + \' <CellDim> \' + str(actual_cell_dim) + lstm_comm\n\n    # the final affine-transform and softmax layer\n    print \'<AffineTransform> <InputDim> \' + str(actual_cell_dim) + \' <OutputDim> \' + str(target_num) + \' <ParamRange> \' + param_range + \' <MaxGrad> \' + str(max_grad)\n    print \'<Softmax> <InputDim> \' + str(target_num) + \' <OutputDim> \' + str(target_num)\n    print \'</Nnet>\'\n'"
asr_egs/wsj/utils/prep_ctc_trans.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015       Yajie Miao    (Carnegie Mellon University)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# This python script converts the word-based transcripts into label sequences. The labels are\n# represented by their indices. \n\nimport sys\n\nif __name__ == \'__main__\':\n\n    if len(sys.argv) < 4 or len(sys.argv) > 5:\n        print ""Usage: {0} <lexicon_file> <trans_file> <unk_word> [space_word]"".format(sys.argv[0])\n        print ""e.g., utils/prep_ctc_trans.py data/lang/lexicon_numbers.txt data/train/text <UNK>""\n        print ""<lexicon_file> - the lexicon file in which entries have been represented by indices""\n        print ""<trans_file>   - the word-based transcript file""\n        print ""<unk_word>     - the word which represents OOVs in transcripts""\n        print ""[space_word]   - optional, the word representing spaces in the transcripts""\n        exit(1)\n\n    dict_file = sys.argv[1]\n    trans_file = sys.argv[2]\n    unk_word = sys.argv[3]\n\n    is_char = False\n    if len(sys.argv) == 5:\n        is_char = True\n        space_word = sys.argv[4]\n\n    # read the lexicon into a dictionary data structure\n    fread = open(dict_file,\'r\')\n    dict = {}\n    for line in fread.readlines():\n        line = line.replace(\'\\n\',\'\')\n        splits = line.split(\' \')  # assume there are no multiple spaces\n        word = splits[0]\n        letters = \'\'\n        for n in range(1, len(splits)):\n            letters += splits[n] + \' \'\n        dict[word] = letters.strip()\n    fread.close()\n\n    # assume that each line is formatted as ""uttid word1 word2 word3 ..."", with no multiple spaces appearing\n    fread = open(trans_file,\'r\')\n    for line in fread.readlines():\n        out_line = \'\'\n        line = line.replace(\'\\n\',\'\').strip()\n        while \'  \' in line:\n            line = line.replace(\'  \', \' \')   # remove multiple spaces in the transcripts\n        \n        uttid = line.split(\' \')[0]  # the first field is always utterance id\n        trans = line.replace(uttid, \'\').strip()\n        if is_char:\n            trans = trans.replace(\' \', \' \' + space_word + \' \')\n        splits = trans.split(\' \')    \n\n        out_line += uttid + \' \'\n        for n in range(0, len(splits)):\n            try:\n              out_line += dict[splits[n]] + \' \'\n            except Exception:\n              out_line += dict[unk_word] + \' \'\n        print out_line.strip()\n'"
asr_egs/wsj/utils/prep_ctc_trans_bkup.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015       Yajie Miao    (Carnegie Mellon University)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# This python script converts the word-based transcripts into label sequences. The labels are\n# represented by their indices. \n\nimport sys\n\nif __name__ == \'__main__\':\n\n    if len(sys.argv) != 4:\n        print ""Usage: {0} <lexicon_file> <trans_file> <unk_word>"".format(sys.argv[0])\n        print ""e.g., utils/prep_ctc_trans.py data/lang/lexicon_numbers.txt data/train/text <UNK>""\n        print ""<lexicon_file> - the lexicon file in which entries have been represented by indices""\n        print ""<trans_file>   - the word-based transcript file""\n        print ""<unk_word>     - the word which represents OOVs in transcripts""\n        exit(1)\n\n    dict_file = sys.argv[1]\n    trans_file = sys.argv[2]\n    unk_word = sys.argv[3]\n\n    # read the lexicon into a dictionary data structure\n    fread = open(dict_file,\'r\')\n    dict = {}\n    for line in fread.readlines():\n        line = line.replace(\'\\n\',\'\')\n        splits = line.split(\' \')  # assume there are no multiple spaces\n        word = splits[0]\n        letters = \'\'\n        for n in range(1, len(splits)):\n            letters += splits[n] + \' \'\n        dict[word] = letters.strip()\n    fread.close()\n\n    # assume that each line is formatted as ""uttid word1 word2 word3 ..."", with no multiple spaces appearing\n    fread = open(trans_file,\'r\')\n    for line in fread.readlines():\n        out_line = \'\'\n        line = line.replace(\'\\n\',\'\').strip();\n        splits = line.split(\' \');\n    \n        out_line += splits[0] + \' \'\n        for n in range(1, len(splits)):\n            try:\n              out_line += dict[splits[n]] + \' \'\n            except Exception:\n              out_line += dict[unk_word] + \' \'\n        print out_line.strip()\n'"
asr_egs/wsj/utils/training_trans_fst.py,0,"b""#!/usr/bin/env python\n\n# Apache 2.0\n\nimport sys\n\nfread = open(sys.argv[1], 'r')\n\nfor entry in fread.readlines():\n    entry = entry.replace('\\n','').strip()\n    fields = entry.split(' ')\n    uttid = fields[0]\n   \n    for n in range(1, len(fields)):\n      print str(n-1) + ' ' + str(n) + ' ' + fields[n] + ' ' + fields[n]\n\n    print str(n) + ' ' + '0' + ' ' + '0' + ' ' + '0'  # assume that <eps> is 0 in words.txt    \n\nprint '0'\n\nfread.close()\n"""
asr_egs/hkust/v1/local/hkust_segment.py,0,"b'#!/usr/bin/env python\n#coding:utf-8\n#!/usr/bin/env python\nimport sys\nfrom mmseg import seg_txt\nfor line in sys.stdin:\n  blks = str.split(line)\n  out_line = blks[0]\n  for i in range(1, len(blks)):\n    if blks[i] == ""[VOCALIZED-NOISE]"" or blks[i] == ""[NOISE]"" or blks[i] == ""[LAUGHTER]"":\n      out_line += "" "" + blks[i]\n      continue\n    for j in seg_txt(blks[i]):\n      out_line += "" "" + j\n  print out_line     \n'"
asr_egs/tedlium/v1/local/join_suffix.py,0,"b'#!/usr/bin/env python\n#\n# Copyright  2014 Nickolay V. Shmyrev \n# Apache 2.0\n\n\nimport sys\n\nwords = set()\nfor line in open(sys.argv[1]):\n    items = line.split()\n    words.add(items[0])\n\nfor line in sys.stdin:\n    items = line.split()\n    new_items = []\n    i = 1\n    while i < len(items):\n\tif i < len(items) - 1 and items[i+1][0] == \'\\\'\' and items[i] + items[i+1] in words:\n\t    new_items.append(items[i] + items[i+1])\n\t    i = i + 1\n\telse:\n\t    new_items.append(items[i])\n\ti = i + 1\n\t\n    print items[0], "" "".join(new_items)\n    \n'"
