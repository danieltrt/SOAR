file_path,api_count,code
config.py,0,"b'import os\nimport argparse\n\n###################################### configuration ######################################\nclass Config(object):\n\n    typeFilters = [[], [""1_query_size_"",\n                ""1_query_material_"",\n                ""2_equal_color_"",\n                ""2_equal_shape_""],\n                [""1_query_color_"",\n                ""1_query_shape_"",\n                ""2_equal_size_"",\n                ""2_equal_material_""]]\n\n    #### files interface\n    ## data files\n    dataPath = """" # dataset specific\n    datasetFilename = """" # dataset specific\n\n    # file names\n    imagesFilename = ""{tier}.h5"" # Images\n    instancesFilename = ""{tier}Instances.json""\n    # symbols dictionaries\n    questionDictFilename = ""questionDict.pkl""\n    answerDictFilename = ""answerDict.pkl""\n    qaDictFilename = ""qaDict.pkl""\n    \n    ## experiment files\n    expPathname = ""{expName}""\n    expName = """" #  will be assigned through argparse\n\n    weightsPath = ""./weights""\n    weightsFilename = ""weights{epoch}.ckpt""\n\n    # model predictions and optionally attention maps\n    predsPath = ""./preds""\n    predsFilename = ""{tier}Predictions-{expName}.json""\n    answersFilename = ""{tier}Answers-{expName}.txt""\n\n    # logging of accuracy, loss etc. per epoch\n    logPath = ""./results""\n    logFilename = ""results-{expName}.csv""\n\n    # configuration file of the used flags to run the experiment\n    configPath = ""./results""\n    configFilename = ""config-{expName}.json""\n\n    def toString(self):\n        return self.expName\n\n    # make directories of experiment if not exist yet\n    def makedirs(self, directory):\n        directory = os.path.join(directory, self.expPath())\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        return directory\n\n    ### filename builders\n    ## data files\n    def dataFile(self, filename): \n        return os.path.join(self.dataPath, filename)\n\n    def generatedFile(self, filename): \n        return self.dataFile(self.generatedPrefix + filename)\n\n    datasetFile     = lambda self, tier: self.dataFile(self.datasetFilename.format(tier = tier))\n    imagesIdsFile   = lambda self, tier: self.dataFile(self.imgIdsFilename.format(tier = tier)) #\n    imagesFile      = lambda self, tier: self.dataFile(self.imagesFilename.format(tier = tier))\n    instancesFile   = lambda self, tier: self.generatedFile(self.instancesFilename.format(tier = tier))\n\n    questionDictFile    = lambda self: self.generatedFile(self.questionDictFilename)\n    answerDictFile      = lambda self: self.generatedFile(self.answerDictFilename)\n    qaDictFile          = lambda self: self.generatedFile(self.qaDictFilename)\n\n    ## experiment files\n    expPath     = lambda self: self.expPathname.format(expName = self.toString())\n    \n    weightsDir  = lambda self: self.makedirs(self.weightsPath)\n    predsDir    = lambda self: self.makedirs(self.predsPath)\n    logDir      = lambda self: self.makedirs(self.logPath)\n    configDir   = lambda self: self.makedirs(self.configPath)\n\n    weightsFile     = lambda self, epoch: os.path.join(self.weightsDir(), self.weightsFilename.format(epoch = str(epoch)))\n    predsFile       = lambda self, tier: os.path.join(self.predsDir(), self.predsFilename.format(tier = tier, expName = self.expName))\n    answersFile     = lambda self, tier: os.path.join(self.predsDir(), self.answersFilename.format(tier = tier, expName = self.expName))\n    logFile         = lambda self: os.path.join(self.logDir(), self.logFilename.format(expName = self.expName))\n    configFile      = lambda self: os.path.join(self.configDir(), self.configFilename.format(expName = self.expName))\n\n\n# global configuration variable. Holds file paths and program parameters\nconfig = Config()\n\n###################################### arguments ######################################\ndef parseArgs():\n    parser = argparse.ArgumentParser(fromfile_prefix_chars = ""@"")\n\n    ################ systems\n\n    # gpus and memory\n    parser.add_argument(""--gpus"",           default = """", type = str,       help = ""comma-separated list of gpus to use"")\n    parser.add_argument(""--gpusNum"",        default = 1, type = int,        help = ""number of gpus to use"")\n    \n    parser.add_argument(""--allowGrowth"",    action = ""store_true"",          help = ""allow gpu memory growth"")\n    parser.add_argument(""--maxMemory"",      default = 1.0, type = float,    help = ""set maximum gpu memory usage"")\n    \n    parser.add_argument(""--parallel"",       action = ""store_true"",          help = ""load images in parallel to batch running"")\n    parser.add_argument(""--workers"",        default = 1, type = int,        help = ""number of workers to load images"")\n    parser.add_argument(""--taskSize"",       default = 8, type = int,        help = ""number of image batches to load in advance"") # 40\n    # parser.add_argument(""--tasksNum"",       default = 20, type = int,       help = ""maximal queue size for tasks (to constrain ram usage)"") # 2\n\n    parser.add_argument(""--useCPU"",         action = ""store_true"",          help = ""put word embeddings on cpu"")\n\n    # weight loading and training\n    parser.add_argument(""-r"", ""--restore"",  action = ""store_true"",          help = ""restore last epoch (based on results file)"") \n    parser.add_argument(""--restoreEpoch"",   default = 0, type = int,        help = ""if positive, specific epoch to restore"")\n    parser.add_argument(""--weightsToKeep"",  default = 2, type = int,        help = ""number of previous epochs\' weights keep"")\n    parser.add_argument(""--saveEvery"",     default = 3000, type = int,      help = ""number of iterations to save weights after"")\n    parser.add_argument(""--calleEvery"",     default = 1500, type = int,     help = ""number of iterations to call custom function after"")\n\n    parser.add_argument(""--saveSubset"",     action = ""store_true"",          help = ""save only subset of the weights"")\n    parser.add_argument(""--trainSubset"",    action = ""store_true"",          help = ""train only subset of the weights"")\n    parser.add_argument(""--varSubset"",      default = [], nargs = ""*"",      type = str, help = ""list of namespaces to train on"")    \n    \n    # trainReader = [""questionEmbeddings"", ""questionReader""]\n    # saveControl = [""questionEmbeddings"", ""programEmbeddings"", ""seqReader"", ""programControl""]\n    \n    # experiment files\n    parser.add_argument(""--expName"",        default = ""experiment"", type = str,    help = ""experiment name"") \n\n    # data files\n    parser.add_argument(""--dataset"",         default = ""CLEVR"", choices = [""CLEVR"", ""NLVR""], type = str) # \n    parser.add_argument(""--dataBasedir"",     default = ""./"", type = str,            help = ""data base directory"") # /jagupard14/scr1/dorarad/\n    parser.add_argument(""--generatedPrefix"", default = ""gennew"", type = str,           help = ""prefix for generated data files"") \n    parser.add_argument(""--featureType"",     default = ""norm_128x32"", type = str,   help = ""features type"") #   \n    # resnet101_512x128, norm_400x100, none_80x20, normPerImage_80x20, norm_80x20\n    \n    ################ optimization\n\n    # training/testing\n    parser.add_argument(""--train"",          action = ""store_true"",      help = ""run training"")\n    parser.add_argument(""--evalTrain"",      action = ""store_true"",      help = ""run eval with ema on train dataset"") #   \n    parser.add_argument(""--test"",           action = ""store_true"",      help = ""run testing every epoch and generate predictions file"") #\n    parser.add_argument(""--finalTest"",      action = ""store_true"",      help = ""run testing on final epoch"")\n    parser.add_argument(""--retainVal"",      action = ""store_true"",      help = ""retain validation order between runs"") #     \n\n    parser.add_argument(""--getPreds"",       action = ""store_true"",      help = ""store prediction"")\n    parser.add_argument(""--getAtt"",         action = ""store_true"",      help = ""store attention maps"")\n    parser.add_argument(""--analysisType"",   default = """", type = str,   choices = ["""", ""questionLength, programLength"",""type"", ""arity""], help = ""show breakdown of results according to type"") #\n\n    parser.add_argument(""--trainedNum"",     default = 0, type = int,    help = ""if positive, train on subset of the data"")    \n    parser.add_argument(""--testedNum"",      default = 0, type = int,    help = ""if positive, test on subset of the data"")  \n    \n    # bucketing\n    parser.add_argument(""--noBucket"",       action = ""store_true"",      help = ""bucket data according to question length"")        \n    parser.add_argument(""--noRebucket"",     action = ""store_true"",      help = ""bucket data according to question and program length"") #\n    \n    # filtering\n    parser.add_argument(""--tOnlyChain"",     action = ""store_true"",      help = ""train only chain questions"")\n    parser.add_argument(""--vOnlyChain"",     action = ""store_true"",      help = ""test only chain questions"")\n    parser.add_argument(""--tMaxQ"",          default = 0, type = int,    help = ""if positive, train on questions up to this length"")\n    parser.add_argument(""--tMaxP"",          default = 0, type = int,    help = ""if positive, test on questions up to this length"")\n    parser.add_argument(""--vMaxQ"",          default = 0, type = int,    help = ""if positive, train on questions with programs up to this length"")\n    parser.add_argument(""--vMaxP"",          default = 0, type = int,    help = ""if positive, test on questions with programs up to this length"")\n    parser.add_argument(""--tFilterOp"",      default = 0, type = int,    help = ""train questions by to be included in the types listed"")\n    parser.add_argument(""--vFilterOp"",      default = 0, type = int,    help = ""test questions by to be included in the types listed"")\n\n    # extra and extraVal\n    parser.add_argument(""--extra"",          action = ""store_true"",      help = ""prepare extra data (add to vocabulary"") #\n    parser.add_argument(""--trainExtra"",     action = ""store_true"",      help = ""train (only) on extra data"") #\n    parser.add_argument(""--alterExtra"",     action = ""store_true"",      help = ""alter main data training with extra dataset"") # \n    parser.add_argument(""--alterNum"",       default = 1, type = int,    help = ""alteration rate"") #\n    parser.add_argument(""--extraVal"",       action = ""store_true"",      help = ""only extra validation data (for compositional clevr)"") # \n    parser.add_argument(""--finetuneNum"",    default = 0, type = int,    help = ""if positive, finetune on that subset of val (for compositional clevr)"") #\n\n    # exponential moving average\n    parser.add_argument(""--useEMA"",         action = ""store_true"",           help = ""use exponential moving average for weights"")\n    parser.add_argument(""--emaDecayRate"",   default = 0.999, type = float,   help = ""decay rate for exponential moving average"")\n    \n    # sgd optimizer\n    parser.add_argument(""--batchSize"",      default = 64, type = int,       help = ""batch size"")    \n    parser.add_argument(""--epochs"",         default = 100, type = int,      help = ""number of epochs to run"")    \n    parser.add_argument(""--lr"",             default = 0.0001, type = float, help = ""learning rate"")\n    parser.add_argument(""--lrReduce"",       action = ""store_true"",          help = ""reduce learning rate if training loss doesn\'t go down (manual annealing)"")    \n    parser.add_argument(""--lrDecayRate"",    default = 0.5, type = float,    help = ""learning decay rate if training loss doesn\'t go down"")\n    parser.add_argument(""--earlyStopping"",  default = 0, type = int,        help = ""if positive, stop if no improvement for that number of epochs"")\n\n    parser.add_argument(""--adam"",           action = ""store_true"",          help = ""use adam"")   \n    parser.add_argument(""--l2"",             default = 0, type = float,      help = ""if positive, add l2 loss term"")    \n    parser.add_argument(""--clipGradients"",  action = ""store_true"",          help = ""clip gradients"")\n    parser.add_argument(""--gradMaxNorm"",    default = 8, type = int,        help = ""clipping value"") \n\n    # batch normalization\n    parser.add_argument(""--memoryBN"",   action = ""store_true"",              help = ""use batch normalization on the recurrent memory"")\n    parser.add_argument(""--stemBN"",     action = ""store_true"",              help = ""use batch normalization in the image input unit (stem)"")\n    parser.add_argument(""--outputBN"",   action = ""store_true"",              help = ""use batch normalization in the output unit"")\n    parser.add_argument(""--bnDecay"",    default = 0.999, type = float,      help = ""batch norm decay rate"")\n    parser.add_argument(""--bnCenter"",   action = ""store_true"",              help = ""batch norm with centering"")\n    parser.add_argument(""--bnScale"",    action = ""store_true"",              help = ""batch norm with scaling"")\n\n    ## dropouts\n    parser.add_argument(""--encInputDropout"", default = 0.85, type = float,   help = ""dropout of the rnn inputs to the Question Input Unit"") \n    parser.add_argument(""--encStateDropout"", default = 1.0, type = float,   help = ""dropout of the rnn states of the Question Input Unit"") \n    parser.add_argument(""--stemDropout"",     default = 0.82, type = float,  help = ""dropout of the Image Input Unit (the stem)"")\n\n    parser.add_argument(""--qDropout"",       default = 0.92, type = float,    help = ""dropout on the question vector"") \n    # parser.add_argument(""--qDropoutOut"",    default = 1.0, type = float,    help = ""dropout on the question vector the goes to the output unit"") \n    # parser.add_argument(""--qDropoutMAC"",    default = 1.0, type = float,    help = ""dropout on the question vector the goes to MAC"") \n\n    parser.add_argument(""--memoryDropout"",  default = 0.85, type = float,    help = ""dropout on the recurrent memory"") \n    parser.add_argument(""--readDropout"",    default = 0.85, type = float,    help = ""dropout of the read unit"")     \n    parser.add_argument(""--writeDropout"",   default = 1.0, type = float,    help = ""dropout of the write unit"") \n    parser.add_argument(""--outputDropout"",  default = 0.85, type = float,   help = ""dropout of the output unit"") \n    \n    parser.add_argument(""--parametricDropout"",        action = ""store_true"", help = ""use parametric dropout"") #\n    parser.add_argument(""--encVariationalDropout"",    action = ""store_true"", help = ""use variational dropout in the RNN input unit"") \n    parser.add_argument(""--memoryVariationalDropout"", action = ""store_true"", help = ""use variational dropout across the MAC network"") \n\n    ## nonlinearities\n    parser.add_argument(""--relu"",       default = ""STD"", choices = [""STD"", ""PRM"", ""ELU"", ""LKY"", ""SELU""], type = str, help = ""type of ReLU to use: standard, parametric, ELU, or leaky"")\n    # parser.add_argument(""--reluAlpha"",  default = 0.2, type = float,    help = ""alpha value for the leaky ReLU"")\n\n    parser.add_argument(""--mulBias"",    default = 0.0, type = float,   help = ""bias to add in multiplications (x + b) * (y + b) for better training"") #\n\n    parser.add_argument(""--imageLinPool"",   default = 2, type = int,   help = ""pooling for image linearizion"") \n\n    ################ baseline model parameters\n    \n    parser.add_argument(""--useBaseline"",    action = ""store_true"",     help = ""run the baseline model"")    \n    parser.add_argument(""--baselineLSTM"",   action = ""store_true"",     help = ""use LSTM in baseline"")    \n    parser.add_argument(""--baselineCNN"",    action = ""store_true"",     help = ""use CNN in baseline"")       \n    parser.add_argument(""--baselineAtt"",    action = ""store_true"",     help = ""use stacked attention baseline"")\n    \n    parser.add_argument(""--baselineProjDim"", default = 64, type = int, help = ""projection dimension for image linearizion"")    \n\n    parser.add_argument(""--baselineAttNumLayers"", default = 2, type = int, help = ""number of stacked attention layers"") \n    parser.add_argument(""--baselineAttType"", default = ""ADD"", type = str, choices = [""MUL"", ""DIAG"", ""BL"", ""ADD""], help = ""attention type (multiplicative, additive, etc)"") \n\n    ################ image input unit (the ""stem"")\n\n    parser.add_argument(""--stemDim"",         default = 512, type = int,               help = ""dimension of stem CNNs"") \n    parser.add_argument(""--stemNumLayers"",   default = 2, type = int,                 help = ""number of stem layers"")\n    parser.add_argument(""--stemKernelSize"",  default = 3, type = int,                 help = ""kernel size for stem (same for all the stem layers)"")\n    parser.add_argument(""--stemKernelSizes"", default = None, nargs = ""*"", type = int, help = ""kernel sizes for stem (per layer)"")\n    parser.add_argument(""--stemStrideSizes"", default = None, nargs = ""*"", type = int, help = ""stride sizes for stem (per layer)"")\n\n    parser.add_argument(""--stemLinear"",             action = ""store_true"",          help = ""use a linear stem (instead of CNNs)"") #\n    # parser.add_argument(""--stemProjDim"",          default = 64, type = int,       help = ""projection dimension of in image linearization"") #\n    # parser.add_argument(""--stemProjPooling"",      default = 2, type = int,        help = ""pooling for the image linearization"") #\n\n    parser.add_argument(""--stemGridRnn"",            action = ""store_true"",          help = ""use grid RNN layer"") #\n    parser.add_argument(""--stemGridRnnMod"",         default = ""RNN"", type = str,    choices = [""RNN"", ""GRU""], help = ""RNN type for grid"") #\n    parser.add_argument(""--stemGridAct"",            default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""nonlinearity type for grid"") #\n\n    ## location\n    parser.add_argument(""--locationAware"",          action = ""store_true"",          help = ""add positional features to image representation (linear meshgrid by default)"") \n    parser.add_argument(""--locationType"",           default = ""L"", type = str,      choices = [""L"", ""PE""], help = ""L: linear features, PE: Positional Encoding"") \n    parser.add_argument(""--locationBias"",           default = 1.0, type = float,    help = ""the scale of the positional features"")\n    parser.add_argument(""--locationDim"",            default = 32, type = int,       help = ""the number of PE dimensions"")\n\n    ################ question input unit (the ""encoder"")\n    parser.add_argument(""--encType"",                default = ""LSTM"",               choices = [""RNN"", ""GRU"", ""LSTM"", ""MiGRU"", ""MiLSTM""], help = ""encoder RNN type"")\n    parser.add_argument(""--encDim"",                 default = 512, type = int,      help = ""dimension of encoder RNN"")    \n    parser.add_argument(""--encNumLayers"",           default = 1, type = int,        help = ""number of encoder RNN layers"")  \n    parser.add_argument(""--encBi"",                  action = ""store_true"",          help = ""use bi-directional encoder"")    \n    # parser.add_argument(""--encOutProj"",           action = ""store_true"",          help = ""add projection layer for encoder outputs"") \n    # parser.add_argument(""--encOutProjDim"",        default = 256, type = int,      help = ""dimension of the encoder projection layer"") \n    # parser.add_argument(""--encQProj"",             action = ""store_true"",          help = ""add projection for the question representation"")\n    parser.add_argument(""--encProj"",                action = ""store_true"",          help = ""project encoder outputs and question"")\n    parser.add_argument(""--encProjQAct"",            default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""project question vector with this activation"")\n\n    ##### word embeddings \n    parser.add_argument(""--wrdEmbDim"",              default = 300, type = int,      help = ""word embeddings dimension"") \n    parser.add_argument(""--wrdEmbRandom"",           action = ""store_true"",          help = ""initialize word embeddings to random (normal)"")\n    parser.add_argument(""--wrdEmbUniform"",          action = ""store_true"",          help = ""initialize with uniform distribution"")\n    parser.add_argument(""--wrdEmbScale"",            default = 1.0, type = float,    help = ""word embeddings initialization scale"")\n    parser.add_argument(""--wrdEmbFixed"",            action = ""store_true"",          help = ""set word embeddings fixed (don\'t train)"")\n    parser.add_argument(""--wrdEmbUnknown"",          action = ""store_true"",          help = ""set words outside of training set to <UNK>"")\n\n    parser.add_argument(""--ansEmbMod"",              default = ""NON"", choices = [""NON"", ""SHARED"", ""BOTH""], type = str,   help = ""BOTH: create word embeddings for answers. SHARED: share them with question embeddings."") #\n    parser.add_argument(""--answerMod"",              default = ""NON"", choices = [""NON"", ""MUL"", ""DIAG"", ""BL""], type = str, help = ""operation for multiplication with answer embeddings: direct multiplication, scalar weighting, or bilinear"") #\n\n    ################ output unit (classifier)\n    parser.add_argument(""--outClassifierDims"",      default = [512], nargs = ""*"",   type = int, help = ""dimensions of the classifier"") \n    parser.add_argument(""--outImage"",               action = ""store_true"",          help = ""feed the image to the output unit"") \n    parser.add_argument(""--outImageDim"",            default = 1024, type = int,     help = ""dimension of linearized image fed to the output unit"") \n    parser.add_argument(""--outQuestion"",            action = ""store_true"",          help = ""feed the question to the output unit"") \n    parser.add_argument(""--outQuestionMul"",         action = ""store_true"",          help = ""feed the multiplication of question and memory to the output unit"") \n\n    ################ network\n    \n    parser.add_argument(""--netLength"",              default = 16, type = int,        help = ""network length (number of cells)"")      \n    # parser.add_argument(""--netDim"", default = 512, type = int)\n    parser.add_argument(""--memDim"",                 default = 512, type = int,      help = ""dimension of memory state"")\n    parser.add_argument(""--ctrlDim"",                default = 512, type = int,      help = ""dimension of control state"")\n    parser.add_argument(""--attDim"",                 default = 512, type = int,      help = ""dimension of pre-attention interactions space"")\n    parser.add_argument(""--unsharedCells"",          default = False, type = bool,   help = ""unshare weights between cells "") \n\n    # initialization\n    parser.add_argument(""--initCtrl"",               default = ""PRM"", type = str,    choices = [""PRM"", ""ZERO"", ""Q""], help = ""initialization mod for control"")\n    parser.add_argument(""--initMem"",                default = ""PRM"", type = str,    choices = [""PRM"", ""ZERO"", ""Q""], help = ""initialization mod for memory"")\n    parser.add_argument(""--initKBwithQ"",            default = ""NON"", type = str,    choices = [""NON"", ""CNCT"", ""MUL""], help = ""merge question with knowledge base"")\n    parser.add_argument(""--addNullWord"",            action = ""store_true"",          help = ""add parametric word in the beginning of the question"") \n\n    ################ control unit\n    # control ablations (use whole question or pre-attention continuous vectors as control)\n    parser.add_argument(""--controlWholeQ"",          action = ""store_true"",          help = ""use whole question vector as control"") \n    parser.add_argument(""--controlContinuous"",      action = ""store_true"",          help = ""use continuous representation of control (without attention)"")\n\n    # step 0: inputs to control unit (word embeddings or encoder outputs, with optional projection)  \n    parser.add_argument(""--controlContextual"",      action = ""store_true"",          help = ""use contextual words for attention (otherwise will use word embeddings)"")\n    parser.add_argument(""--controlInWordsProj"",     action = ""store_true"",          help = ""apply linear projection over words for attention computation"") \n    parser.add_argument(""--controlOutWordsProj"",    action = ""store_true"",          help = ""apply linear projection over words for summary computation"") \n\n    parser.add_argument(""--controlInputUnshared"",   action = ""store_true"",          help = ""use different question representation for each cell"") \n    parser.add_argument(""--controlInputAct"",        default = ""TANH"", type = str,   choices = [""NON"", ""RELU"", ""TANH""], help = ""activation for question projection"")\n\n    # step 1: merging previous control and whole question  \n    parser.add_argument(""--controlFeedPrev"",        action = ""store_true"",          help = ""feed previous control state"") \n    parser.add_argument(""--controlFeedPrevAtt"",     action = ""store_true"",          help = ""feed previous control post word attention (otherwise will feed continuous control)"")\n    parser.add_argument(""--controlFeedInputs"",      action = ""store_true"",          help = ""feed question representation"")\n    parser.add_argument(""--controlContAct"",         default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""activation on the words interactions"")\n    \n    # step 2: word attention and optional projection \n    parser.add_argument(""--controlConcatWords"",     action = ""store_true"",          help = ""concatenate words to interaction when computing attention"") \n    parser.add_argument(""--controlProj"",            action = ""store_true"",          help = ""apply linear projection on words interactions"")\n    parser.add_argument(""--controlProjAct"",         default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""activation for control interactions"")\n\n    # parser.add_argument(""--controlSelfAtt"", default = False, type = bool) \n\n    # parser.add_argument(""--controlCoverage"", default = False, type = bool)\n    # parser.add_argument(""--controlCoverageBias"", default = 1.0, type = float)\n\n    # parser.add_argument(""--controlPostRNN"", default = False, type = bool) \n    # parser.add_argument(""--controlPostRNNmod"", default = ""RNN"", type = str) # GRU\n\n    # parser.add_argument(""--selfAttShareInter"", default = False, type = bool)\n\n    # parser.add_argument(""--wordControl"", default = False, type = bool)\n    # parser.add_argument(""--gradualControl"", default = False, type = bool)\n\n    ################ read unit\n    # step 1: KB-memory interactions\n    parser.add_argument(""--readProjInputs"",         action = ""store_true"",         help = ""project read unit inputs"")\n    parser.add_argument(""--readProjShared"",         action = ""store_true"",         help = ""use shared projection for all read unit inputs"")\n\n    parser.add_argument(""--readMemAttType"",         default = ""MUL"", type = str,   choices = [""MUL"", ""DIAG"", ""BL"", ""ADD""], help = ""attention type for interaction with memory"")\n    parser.add_argument(""--readMemConcatKB"",        action = ""store_true"",         help = ""concatenate KB elements to memory interaction"")\n    parser.add_argument(""--readMemConcatProj"",      action = ""store_true"",         help = ""concatenate projected values instead or original to memory interaction"")\n    parser.add_argument(""--readMemProj"",            action = ""store_true"",         help = ""project interactions with memory"")\n    parser.add_argument(""--readMemAct"",             default = ""RELU"", type = str,  choices = [""NON"", ""RELU"", ""TANH""], help = ""activation for memory interaction"")\n\n    # step 2: interaction with control\n    parser.add_argument(""--readCtrl"",               action = ""store_true"",         help = ""compare KB-memory interactions to control"")\n    parser.add_argument(""--readCtrlAttType"",        default = ""MUL"", type = str,   choices = [""MUL"", ""DIAG"", ""BL"", ""ADD""], help = ""attention type for interaction with control"")\n    parser.add_argument(""--readCtrlConcatKB"",       action = ""store_true"",         help = ""concatenate KB elements to control interaction"")\n    parser.add_argument(""--readCtrlConcatProj"",     action = ""store_true"",         help = ""concatenate projected values instead or original to control interaction"")\n    parser.add_argument(""--readCtrlConcatInter"",    action = ""store_true"",         help = ""concatenate memory interactions to control interactions"")\n    parser.add_argument(""--readCtrlAct"",            default = ""RELU"", type = str,  choices = [""NON"", ""RELU"", ""TANH""], help = ""activation for control interaction"")\n\n    # step 3: summarize attention over knowledge base\n    parser.add_argument(""--readSmryKBProj"",       action = ""store_true"",        help = ""use knowledge base projections when summing attention up (should be used only if KB is projected."")\n    \n    # parser.add_argument(""--saAllMultiplicative"", default = False, type = bool) \n    # parser.add_argument(""--saSumMultiplicative"", default = False, type = bool)\n\n    ################ write unit\n    # step 1: input to the write unit (only previous memory, or new information, or both)\n    parser.add_argument(""--writeInputs"",            default = ""BOTH"", type = str,   choices = [""MEM"", ""INFO"", ""BOTH"", ""SUM""], help = ""inputs to the write unit"")\n    parser.add_argument(""--writeConcatMul"",         action = ""store_true"",          help = ""add multiplicative integration between inputs"")\n    \n    parser.add_argument(""--writeInfoProj"",          action = ""store_true"",          help = ""project retrieved info"")\n    parser.add_argument(""--writeInfoAct"",           default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""new info activation"")\n\n    # step 2: self attention and following projection\n    parser.add_argument(""--writeSelfAtt"",           action = ""store_true"",          help = ""use self attention"") \n    parser.add_argument(""--writeSelfAttMod"",        default = ""NON"", type = str,    choices = [""NON"", ""CONT""], help = ""control version to compare to"")\n\n    parser.add_argument(""--writeMergeCtrl"",           action = ""store_true"",          help = ""merge control with memory"") \n\n    parser.add_argument(""--writeMemProj"",           action = ""store_true"",          help = ""project new memory"")\n    parser.add_argument(""--writeMemAct"",            default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""new memory activation"")\n\n    # step 3: gate between new memory and previous value \n    parser.add_argument(""--writeGate"",              action = ""store_true"",          help = ""add gate to write unit"") \n    parser.add_argument(""--writeGateShared"",        action = ""store_true"",          help = ""use one gate value for all dimensions of the memory state"") \n    parser.add_argument(""--writeGateBias"",          default = 1.0, type = float,    help = ""bias for the write unit gate (positive to bias for taking new memory)"") \n\n    ## modular\n    # parser.add_argument(""--modulesNum"", default = 10, type = int) \n    # parser.add_argument(""--controlBoth"", default = False, type = bool)\n    # parser.add_argument(""--addZeroModule"", default = False, type = bool) \n    # parser.add_argument(""--endModule"", default = False, type = bool) \n\n    ## hybrid\n    # parser.add_argument(""--hybrid"",      default = False, type = bool, help = ""hybrid attention cnn model"") \n    # parser.add_argument(""--earlyHybrid"", default = False, type = bool) \n    # parser.add_argument(""--lateHybrid"",  default = False, type = bool) \n\n    ## autoencoders\n    # parser.add_argument(""--autoEncMem"",         action = ""store_true"",          help = ""add memory2control auto-encoder loss"")\n    # parser.add_argument(""--autoEncMemW"",        default = 0.0001, type = float, help = ""weight for auto-encoder loss"")\n    # parser.add_argument(""--autoEncMemInputs"",   default = ""INFO"", type = str,   choices = [""MEM"", ""INFO""], help = ""inputs to auto-encoder"")\n    # parser.add_argument(""--autoEncMemAct"",      default = ""NON"", type = str,    choices = [""NON"", ""RELU"", ""TANH""], help = ""activation type in the auto-encoder"")\n    # parser.add_argument(""--autoEncMemLoss"",     default = ""CONT"", type = str,   choices = [""CONT"", ""PROB"", ""SMRY""], help = ""target for the auto-encoder loss"")\n    # parser.add_argument(""--autoEncMemCnct"",     action = ""store_true"",          help = ""concat word attentions to auto-encoder features"")\n\n    # parser.add_argument(""--autoEncCtrl"",        action = ""store_true"")\n    # parser.add_argument(""--autoEncCtrlW"",       default = 0.0001, type = float)\n    # parser.add_argument(""--autoEncCtrlGRU"",     action = ""store_true"")\n \n    ## temperature\n    # parser.add_argument(""--temperature"",    default = 1.0, type = float,        help = ""temperature for modules softmax"") #\n    # parser.add_argument(""--tempParametric"", action = ""store_true"",              help = ""parametric temperature"") #\n    # parser.add_argument(""--tempDynamic"",    action = ""store_true"",              help = ""dynamic temperature"") #\n    # parser.add_argument(""--tempAnnealRate"", default = 0.000004, type = float,   help = ""temperature annealing rate"") #\n    # parser.add_argument(""--tempMin"",        default = 0.5, type = float,        help = ""minimum temperature"") #\n\n    ## gumbel\n    # parser.add_argument(""--gumbelSoftmax"",      action = ""store_true"", help = ""use gumbel for the module softmax (soft for training and hard for testing)"") #\n    # parser.add_argument(""--gumbelSoftmaxBoth"",  action = ""store_true"", help = ""use softmax for training and testing"") #\n    # parser.add_argument(""--gumbelArgmaxBoth"",   action = ""store_true"", help = ""use argmax for training and testing"") #\n    \n    parser.parse_args(namespace = config) \n\n###################################### dataset configuration ######################################\n\ndef configCLEVR():\n    config.dataPath = ""{dataBasedir}/CLEVR_v1/data"".format(dataBasedir = config.dataBasedir)\n    config.datasetFilename = ""CLEVR_{tier}_questions.json""\n    config.wordVectorsFile = ""./CLEVR_v1/data/glove/glove.6B.{dim}d.txt"".format(dim = config.wrdEmbDim) #\n\n    config.imageDims = [14, 14, 1024]\n    config.programLims = [5, 10, 15, 20]\n    config.questionLims = [10, 15, 20, 25]        \n\ndef configNLVR():\n    config.dataPath = ""{dataBasedir}/nlvr"".format(dataBasedir = config.dataBasedir)\n    config.datasetFilename = ""{tier}.json""\n    config.imagesFilename = ""{{tier}}_{featureType}.h5"".format(featureType = config.featureType)\n    config.imgIdsFilename = ""{tier}ImgIds.json""\n    config.wordVectorsFile = ""./CLEVR_v1/data/glove/glove.6B.{dim}d.txt"".format(dim = config.wrdEmbDim) #\n\n    config.questionLims = [12]\n    # config.noRebucket = True \n\n    # if config.stemKernelSizes == []:\n    #     if config.featureType.endsWith(""128x32""):\n    #         config.stemKernelSizes = [8, 4, 4]\n    #         config.stemStrideSizes = [2, 2, 1]\n    #         config.stemNumLayers = 3\n    #     if config.featureType.endsWith(""512x128""): \n    #         config.stemKernelSizes = [8, 4, 4, 2]\n    #         config.stemStrideSizes = [4, 2, 2, 1]\n    #         config.stemNumLayers = 4\n    # config.stemDim = 64\n\n    if config.featureType == ""resnet101_512x128"":\n        config.imageDims = [8, 32, 1024]\n    else:\n        stridesOverall = 1\n        if stemStrideSizes is not None:\n            for s in config.stemStrideSizes:\n                stridesOverall *= int(s)\n        size = config.featureType.split(""_"")[-1].split(""x"")\n        config.imageDims = [int(size[1]) / stridesOverall, int(size[0]) / stridesOverall, 3]\n\n## dataset specific configs\nloadDatasetConfig = {\n    ""CLEVR"": configCLEVR,\n    ""NLVR"": configNLVR\n}\n'"
extract_features.py,0,"b'# Copyright 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse, os, json\nimport h5py\nimport numpy as np\nfrom scipy.misc import imread, imresize\n\nimport torch\nimport torchvision\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input_image_dir\', required=True)\nparser.add_argument(\'--max_images\', default=None, type=int)\nparser.add_argument(\'--output_h5_file\', required=True)\n\nparser.add_argument(\'--image_height\', default=224, type=int)\nparser.add_argument(\'--image_width\', default=224, type=int)\n\nparser.add_argument(\'--model\', default=\'resnet101\')\nparser.add_argument(\'--model_stage\', default=3, type=int)\nparser.add_argument(\'--batch_size\', default=128, type=int)\n\n\ndef build_model(args):\n  if not hasattr(torchvision.models, args.model):\n    raise ValueError(\'Invalid model ""%s""\' % args.model)\n  if not \'resnet\' in args.model:\n    raise ValueError(\'Feature extraction only supports ResNets\')\n  cnn = getattr(torchvision.models, args.model)(pretrained=True)\n  layers = [\n    cnn.conv1,\n    cnn.bn1,\n    cnn.relu,\n    cnn.maxpool,\n  ]\n  for i in range(args.model_stage):\n    name = \'layer%d\' % (i + 1)\n    layers.append(getattr(cnn, name))\n  model = torch.nn.Sequential(*layers)\n  model.cuda()\n  model.eval()\n  return model\n\n\ndef run_batch(cur_batch, model):\n  mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n  std = np.array([0.229, 0.224, 0.224]).reshape(1, 3, 1, 1)\n\n  image_batch = np.concatenate(cur_batch, 0).astype(np.float32)\n  image_batch = (image_batch / 255.0 - mean) / std\n  image_batch = torch.FloatTensor(image_batch).cuda()\n  image_batch = torch.autograd.Variable(image_batch, volatile=True)\n\n  feats = model(image_batch)\n  feats = feats.data.cpu().clone().numpy()\n\n  return feats\n\n\ndef main(args):\n  input_paths = []\n  idx_set = set()\n  for fn in os.listdir(args.input_image_dir):\n    if not fn.endswith(\'.png\'): continue\n    idx = int(os.path.splitext(fn)[0].split(\'_\')[-1])\n    input_paths.append((os.path.join(args.input_image_dir, fn), idx))\n    idx_set.add(idx)\n  input_paths.sort(key=lambda x: x[1])\n  assert len(idx_set) == len(input_paths)\n  assert min(idx_set) == 0 and max(idx_set) == len(idx_set) - 1\n  if args.max_images is not None:\n    input_paths = input_paths[:args.max_images]\n  print(input_paths[0])\n  print(input_paths[-1])\n\n  model = build_model(args)\n\n  img_size = (args.image_height, args.image_width)\n  with h5py.File(args.output_h5_file, \'w\') as f:\n    feat_dset = None\n    i0 = 0\n    cur_batch = []\n    for i, (path, idx) in enumerate(input_paths):\n      img = imread(path, mode=\'RGB\')\n      img = imresize(img, img_size, interp=\'bicubic\')\n      img = img.transpose(2, 0, 1)[None]\n      cur_batch.append(img)\n      if len(cur_batch) == args.batch_size:\n        feats = run_batch(cur_batch, model)\n        if feat_dset is None:\n          N = len(input_paths)\n          _, C, H, W = feats.shape\n          feat_dset = f.create_dataset(\'features\', (N, C, H, W),\n                                       dtype=np.float32)\n        i1 = i0 + len(cur_batch)\n        feat_dset[i0:i1] = feats\n        i0 = i1\n        print(\'Processed %d / %d images\' % (i1, len(input_paths)))\n        cur_batch = []\n    if len(cur_batch) > 0:\n      feats = run_batch(cur_batch, model)\n      i1 = i0 + len(cur_batch)\n      feat_dset[i0:i1] = feats\n      print(\'Processed %d / %d images\' % (i1, len(input_paths)))\n\n\nif __name__ == \'__main__\':\n  args = parser.parse_args()\n  main(args)'"
mac_cell.py,44,"b'import collections\nimport numpy as np\nimport tensorflow as tf\n\nimport ops\nfrom config import config\n\nMACCellTuple = collections.namedtuple(""MACCellTuple"", (""control"", ""memory""))\n\n\'\'\'\nThe MAC cell.\n\nRecurrent cell for multi-step reasoning. Presented in https://arxiv.org/abs/1803.03067.\nThe cell has recurrent control and memory states that interact with the question \nand knowledge base (image) respectively.\n\nThe hidden state structure is MACCellTuple(control, memory)\n\nAt each step the cell performs by calling to three subunits: control, read and write.\n\n1. The Control Unit computes the control state by computing attention over the question words.\nThe control state represents the current reasoning operation the cell performs.\n\n2. The Read Unit retrieves information from the knowledge base, given the control and previous\nmemory values, by computing 2-stages attention over the knowledge base.\n\n3. The Write Unit integrates the retrieved information to the previous hidden memory state,\ngiven the value of the control state, to perform the current reasoning operation.\n\'\'\'\nclass MACCell(tf.nn.rnn_cell.RNNCell):\n\n    \'\'\'Initialize the MAC cell. \n    (Note that in the current version the cell is stateful -- \n    updating its own internals when being called) \n    \n    Args:\n        vecQuestions: the vector representation of the questions. \n        [batchSize, ctrlDim]\n\n        questionWords: the question words embeddings. \n        [batchSize, questionLength, ctrlDim]\n\n        questionCntxWords: the encoder outputs -- the ""contextual"" question words. \n        [batchSize, questionLength, ctrlDim]\n\n        questionLengths: the length of each question.\n        [batchSize]\n        \n        memoryDropout: dropout on the memory state (Tensor scalar).\n        readDropout: dropout inside the read unit (Tensor scalar).\n        writeDropout: dropout on the new information that gets into the write unit (Tensor scalar).\n        \n        batchSize: batch size (Tensor scalar).\n        train: train or test mod (Tensor boolean).\n        reuse: reuse cell\n\n        knowledgeBase: \n    \'\'\'\n    def __init__(self, vecQuestions, questionWords, questionCntxWords, questionLengths, \n            knowledgeBase, memoryDropout, readDropout, writeDropout, \n            batchSize, train, reuse = None):\n        \n        self.vecQuestions = vecQuestions\n        self.questionWords = questionWords\n        self.questionCntxWords = questionCntxWords\n        self.questionLengths = questionLengths\n\n        self.knowledgeBase = knowledgeBase\n\n        self.dropouts = {}\n        self.dropouts[""memory""] = memoryDropout \n        self.dropouts[""read""] = readDropout \n        self.dropouts[""write""] = writeDropout\n\n        self.none = tf.zeros((batchSize, 1), dtype = tf.float32)\n\n        self.batchSize = batchSize\n        self.train = train\n        self.reuse = reuse\n\n    \'\'\' \n    Cell state size. \n    \'\'\'\n    @property\n    def state_size(self):\n        return MACCellTuple(config.ctrlDim, config.memDim)\n\n    \'\'\'\n    Cell output size. Currently it doesn\'t have any outputs. \n    \'\'\'\n    @property\n    def output_size(self):\n        return 1\n\n    # pass encoder hidden states to control?\n    \'\'\'\n    The Control Unit: computes the new control state -- the reasoning operation,\n    by summing up the word embeddings according to a computed attention distribution.\n    \n    The unit is recurrent: it receives the whole question and the previous control state,\n    merge them together (resulting in the ""continuous control""), and then uses that \n    to compute attentions over the question words. Finally, it combines the words \n    together according to the attention distribution to get the new control state. \n    \n    Args:\n        controlInput: external inputs to control unit (the question vector).\n        [batchSize, ctrlDim]\n\n        inWords: the representation of the words used to compute the attention.\n        [batchSize, questionLength, ctrlDim]\n\n        outWords: the representation of the words that are summed up. \n                  (by default inWords == outWords)\n        [batchSize, questionLength, ctrlDim]\n\n        questionLengths: the length of each question.\n        [batchSize]\n\n        control: the previous control hidden state value.\n        [batchSize, ctrlDim]\n\n        contControl: optional corresponding continuous control state\n        (before casting the attention over the words).\n        [batchSize, ctrlDim]\n\n    Returns:\n        the new control state\n        [batchSize, ctrlDim]\n\n        the continuous (pre-attention) control\n        [batchSize, ctrlDim]\n    \'\'\'\n    def control(self, controlInput, inWords, outWords, questionLengths,\n        control, contControl = None, name = """", reuse = None):\n\n        with tf.variable_scope(""control"" + name, reuse = reuse):\n            dim = config.ctrlDim\n\n            ## Step 1: compute ""continuous"" control state given previous control and question.\n            # control inputs: question and previous control\n            newContControl = controlInput\n            if config.controlFeedPrev:\n                newContControl = control if config.controlFeedPrevAtt else contControl\n                if config.controlFeedInputs:\n                    newContControl = tf.concat([newContControl, controlInput], axis = -1)\n                    dim += config.ctrlDim\n\n                # merge inputs together\n                newContControl = ops.linear(newContControl, dim, config.ctrlDim,\n                    act = config.controlContAct, name = ""contControl"")\n                dim = config.ctrlDim\n\n            ## Step 2: compute attention distribution over words and sum them up accordingly.\n            # compute interactions with question words\n            interactions = tf.expand_dims(newContControl, axis = 1) * inWords\n            \n            # optionally concatenate words\n            if config.controlConcatWords:\n                interactions = tf.concat([interactions, inWords], axis = -1)\n                dim += config.ctrlDim                                              \n\n            # optional projection\n            if config.controlProj:\n                interactions = ops.linear(interactions, dim, config.ctrlDim, \n                    act = config.controlProjAct) \n                dim = config.ctrlDim\n\n            # compute attention distribution over words and summarize them accordingly \n            logits = ops.inter2logits(interactions, dim)\n            # self.interL = (interW, interb)\n\n            # if config.controlCoverage:\n            #     logits += coverageBias * coverage\n\n            attention = tf.nn.softmax(ops.expMask(logits, questionLengths))\n            self.attentions[""question""].append(attention)\n\n            # if config.controlCoverage:\n            #     coverage += attention # Add logits instead?               \n\n            newControl = ops.att2Smry(attention, outWords) \n            \n            # ablation: use continuous control (pre-attention) instead\n            if config.controlContinuous:\n                newControl = newContControl\n\n        return newControl, newContControl\n\n    \'\'\'\n    The read unit extracts relevant information from the knowledge base given the\n    cell\'s memory and control states. It computes attention distribution over\n    the knowledge base by comparing it first to the memory and then to the control.\n    Finally, it uses the attention distribution to sum up the knowledge base accordingly,\n    resulting in an extraction of relevant information. \n\n    Args:\n        knowledge base: representation of the knowledge base (image). \n        [batchSize, kbSize (Height * Width), memDim]\n\n        memory: the cell\'s memory state\n        [batchSize, memDim]\n\n        control: the cell\'s control state\n        [batchSize, ctrlDim]\n\n    Returns the information extracted.\n    [batchSize, memDim]\n    \'\'\'\n    def read(self, knowledgeBase, memory, control, name = """", reuse = None):\n        with tf.variable_scope(""read"" + name, reuse = reuse):\n            dim = config.memDim \n\n            ## memory dropout\n            if config.memoryVariationalDropout:\n                memory = ops.applyVarDpMask(memory, self.memDpMask, self.dropouts[""memory""])\n            else:\n                memory = tf.nn.dropout(memory, self.dropouts[""memory""])\n\n            ## Step 1: knowledge base / memory interactions \n            # parameters for knowledge base and memory projection \n            proj = None\n            if config.readProjInputs:\n                proj = {""dim"": config.attDim, ""shared"": config.readProjShared, ""dropout"": self.dropouts[""read""] }\n                dim = config.attDim\n\n            # parameters for concatenating knowledge base elements\n            concat = {""x"": config.readMemConcatKB, ""proj"": config.readMemConcatProj}\n\n            # compute interactions between knowledge base and memory\n            interactions, interDim = ops.mul(x = knowledgeBase, y = memory, dim = config.memDim, \n                proj = proj, concat = concat, interMod = config.readMemAttType, name = ""memInter"")\n\n            projectedKB = proj.get(""x"") if proj else None\n\n            # project memory interactions back to hidden dimension\n            if config.readMemProj:\n                interactions = ops.linear(interactions, interDim, dim, act = config.readMemAct, \n                    name = ""memKbProj"")\n            else: \n                dim = interDim\n\n            ## Step 2: compute interactions with control\n            if config.readCtrl:\n                # compute interactions with control\n                if config.ctrlDim != dim:\n                    control = ops.linear(control, ctrlDim, dim, name = ""ctrlProj"")\n\n                interactions, interDim = ops.mul(interactions, control, dim, \n                    interMod = config.readCtrlAttType, concat = {""x"": config.readCtrlConcatInter}, \n                    name = ""ctrlInter"")\n\n                # optionally concatenate knowledge base elements\n                if config.readCtrlConcatKB:\n                    if config.readCtrlConcatProj:\n                        addedInp, addedDim = projectedKB, config.attDim\n                    else:\n                        addedInp, addedDim = knowledgeBase, config.memDim\n                    interactions = tf.concat([interactions, addedInp], axis = -1)\n                    dim += addedDim   \n\n                # optional nonlinearity \n                interactions = ops.activations[config.readCtrlAct](interactions)\n\n            ## Step 3: sum attentions up over the knowledge base\n            # transform vectors to attention distribution\n            attention = ops.inter2att(interactions, dim, dropout = self.dropouts[""read""])\n\n            self.attentions[""kb""].append(attention)\n\n            # optionally use projected knowledge base instead of original\n            if config.readSmryKBProj:\n                knowledgeBase = projectedKB\n            \n            # sum up the knowledge base according to the distribution\n            information = ops.att2Smry(attention, knowledgeBase)\n\n        return information \n\n    \'\'\'\n    The write unit integrates newly retrieved information (from the read unit),\n    with the cell\'s previous memory hidden state, resulting in a new memory value.\n    The unit optionally supports:\n    1. Self-attention to previous control / memory states, in order to consider previous steps\n    in the reasoning process.\n    2. Gating between the new memory and previous memory states, to allow dynamic adjustment\n    of the reasoning process length.\n\n    Args:\n        memory: the cell\'s memory state\n        [batchSize, memDim]\n\n        info: the information to integrate with the memory\n        [batchSize, memDim]\n\n        control: the cell\'s control state\n        [batchSize, ctrlDim]\n\n        contControl: optional corresponding continuous control state \n        (before casting the attention over the words).\n        [batchSize, ctrlDim]\n\n    Return the new memory \n    [batchSize, memDim]\n    \'\'\'\n    def write(self, memory, info, control, contControl = None, name = """", reuse = None):\n        with tf.variable_scope(""write"" + name, reuse = reuse):\n\n            # optionally project info\n            if config.writeInfoProj:\n                info = ops.linear(info, config.memDim, config.memDim, name = ""info"")\n\n            # optional info nonlinearity\n            info = ops.activations[config.writeInfoAct](info) \n\n            # compute self-attention vector based on previous controls and memories\n            if config.writeSelfAtt:\n                selfControl = control\n                if config.writeSelfAttMod == ""CONT"":\n                    selfControl = contControl\n                # elif config.writeSelfAttMod == ""POST"":\n                #     selfControl = postControl\n                selfControl = ops.linear(selfControl, config.ctrlDim, config.ctrlDim, name = ""ctrlProj"")\n                \n                interactions = self.controls * tf.expand_dims(selfControl, axis = 1)\n\n                # if config.selfAttShareInter: \n                #     selfAttlogits = self.linearP(selfAttInter, config.encDim, 1, self.interL[0], self.interL[1], name = ""modSelfAttInter"")\n                attention = ops.inter2att(interactions, config.ctrlDim, name = ""selfAttention"")\n                self.attentions[""self""].append(attention) \n                selfSmry = ops.att2Smry(attention, self.memories)\n\n            # get write unit inputs: previous memory, the new info, optionally self-attention / control\n            newMemory, dim = memory, config.memDim\n            if config.writeInputs == ""INFO"":\n                newMemory = info\n            elif config.writeInputs == ""SUM"":\n                newMemory += info\n            elif config.writeInputs == ""BOTH"":\n                newMemory, dim = ops.concat(newMemory, info, dim, mul = config.writeConcatMul)\n            # else: MEM\n\n            if config.writeSelfAtt:\n                newMemory = tf.concat([newMemory, selfSmry], axis = -1)\n                dim += config.memDim\n\n            if config.writeMergeCtrl:\n                newMemory = tf.concat([newMemory, control], axis = -1)\n                dim += config.memDim                \n\n            # project memory back to memory dimension\n            if config.writeMemProj or (dim != config.memDim):\n                newMemory = ops.linear(newMemory, dim, config.memDim, name = ""newMemory"")\n\n            # optional memory nonlinearity\n            newMemory = ops.activations[config.writeMemAct](newMemory)\n\n            # write unit gate\n            if config.writeGate:\n                gateDim = config.memDim\n                if config.writeGateShared:\n                    gateDim = 1\n                \n                z = tf.sigmoid(ops.linear(control, config.ctrlDim, gateDim, name = ""gate"", bias = config.writeGateBias))\n                \n                self.attentions[""gate""].append(z)\n\n                newMemory = newMemory * z + memory * (1 - z)                \n\n            # optional batch normalization\n            if config.memoryBN:\n                newMemory = tf.contrib.layers.batch_norm(newMemory, decay = config.bnDecay, \n                    center = config.bnCenter, scale = config.bnScale, \n                    is_training = self.train, updates_collections = None)\n\n        return newMemory\n\n    def memAutoEnc(newMemory, info, control, name = """", reuse = None):\n        with tf.variable_scope(""memAutoEnc"" + name, reuse = reuse):\n            # inputs to auto encoder\n            features = info if config.autoEncMemInputs == ""INFO"" else newMemory\n            features = ops.linear(features, config.memDim, config.ctrlDim, \n                act = config.autoEncMemAct, name = ""aeMem"")\n\n            # reconstruct control\n            if config.autoEncMemLoss == ""CONT"":\n                loss = tf.reduce_mean(tf.squared_difference(control, features))\n            else:                    \n                interactions, dim = ops.mul(self.questionCntxWords, features, config.ctrlDim, \n                    concat = {""x"": config.autoEncMemCnct}, mulBias = config.mulBias, name = ""aeMem"")\n                \n                logits = ops.inter2logits(interactions, dim)\n                logits = self.expMask(logits, self.questionLengths)\n\n                # reconstruct word attentions\n                if config.autoEncMemLoss == ""PROB"":\n                    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                        labels = self.attentions[""question""][-1], logits = logits))\n                \n                # reconstruct control through words attentions\n                else:\n                    attention = tf.nn.softmax(logits)\n                    summary = ops.att2Smry(attention, self.questionCntxWords)\n                    loss = tf.reduce_mean(tf.squared_difference(control, summary))\n        \n        return loss\n\n    \'\'\'\n    Call the cell to get new control and memory states.\n\n    Args:\n        inputs: in the current implementation the cell don\'t get recurrent inputs\n        every iteration (argument for comparability with rnn interface).\n            \n        state: the cell current state (control, memory)\n        MACCellTuple([batchSize, ctrlDim],[batchSize, memDim])\n\n    Returns the new state -- the new memory and control values.\n    MACCellTuple([batchSize, ctrlDim],[batchSize, memDim])\n    \'\'\'\n    def __call__(self, inputs, state, scope = None):\n        scope = scope or type(self).__name__\n        with tf.variable_scope(scope, reuse = self.reuse): #  as tfscope\n            control = state.control\n            memory = state.memory\n\n            # cell sharing\n            inputName = ""qInput""\n            inputNameU = ""qInputU""\n            inputReuseU = inputReuse = (self.iteration > 0)\n            if config.controlInputUnshared:\n                inputNameU = ""qInput%d"" % self.iteration\n                inputReuseU = None\n\n            cellName = """"\n            cellReuse = (self.iteration > 0)\n            if config.unsharedCells:\n                cellName = str(self.iteration)\n                cellReuse = None \n\n            ## control unit\n            # prepare question input to control \n            controlInput = ops.linear(self.vecQuestions, config.ctrlDim, config.ctrlDim, \n                name = inputName, reuse = inputReuse)\n\n            controlInput = ops.activations[config.controlInputAct](controlInput)\n\n            controlInput = ops.linear(controlInput, config.ctrlDim, config.ctrlDim, \n                name = inputNameU, reuse = inputReuseU)\n\n            newControl, self.contControl = self.control(controlInput, self.inWords, self.outWords, \n                self.questionLengths, control, self.contControl, name = cellName, reuse = cellReuse)\n            \n            # read unit\n            # ablation: use whole question as control\n            if config.controlWholeQ:                    \n                newControl = self.vecQuestions\n                # ops.linear(self.vecQuestions, config.ctrlDim, projDim, name = ""qMod"") \n\n            info = self.read(self.knowledgeBase, memory, newControl, name = cellName, reuse = cellReuse) \n\n            if config.writeDropout < 1.0:\n                # write unit\n                info = tf.nn.dropout(info, self.dropouts[""write""])\n            \n            newMemory = self.write(memory, info, newControl, self.contControl, name = cellName, reuse = cellReuse)\n\n            # add auto encoder loss for memory\n            # if config.autoEncMem:\n            #     self.autoEncLosses[""memory""] += memAutoEnc(newMemory, info, newControl)\n        \n            # append as standard list?\n            self.controls = tf.concat([self.controls, tf.expand_dims(newControl, axis = 1)], axis = 1)\n            self.memories = tf.concat([self.memories, tf.expand_dims(newMemory, axis = 1)], axis = 1)\n            self.infos = tf.concat([self.infos, tf.expand_dims(info, axis = 1)], axis = 1)\n\n            # self.contControls = tf.concat([self.contControls, tf.expand_dims(contControl, axis = 1)], axis = 1)\n            # self.postControls = tf.concat([self.controls, tf.expand_dims(postControls, axis = 1)], axis = 1)\n\n        newState = MACCellTuple(newControl, newMemory)\n        return self.none, newState\n\n    \'\'\'\n    Initializes the a hidden state to based on the value of the initType:\n    ""PRM"" for parametric initialization\n    ""ZERO"" for zero initialization  \n    ""Q"" to initialize to question vectors.\n\n    Args:\n        name: the state variable name.\n        dim: the dimension of the state.\n        initType: the type of the initialization\n        batchSize: the batch size\n\n    Returns the initialized hidden state.\n    \'\'\'\n    def initState(self, name, dim, initType, batchSize):\n        if initType == ""PRM"":\n            prm = tf.get_variable(name, shape = (dim, ),\n                    initializer = tf.random_normal_initializer())                \n            initState = tf.tile(tf.expand_dims(prm, axis = 0), [batchSize, 1])\n        elif initType == ""ZERO"":\n            initState = tf.zeros((batchSize, dim), dtype = tf.float32)\n        else: # ""Q""\n            initState = self.vecQuestions\n        return initState\n\n    \'\'\'\n    Add a parametric null word to the questions.\n\n    Args:\n        words: the words to add a null word to.\n        [batchSize, questionLentgth]\n\n        lengths: question lengths.\n        [batchSize] \n\n    Returns the updated word sequence and lengths.  \n    \'\'\'\n    def addNullWord(words, lengths):\n        nullWord = tf.get_variable(""zeroWord"", shape = (1 , config.ctrlDim), initializer = tf.random_normal_initializer())                    \n        nullWord = tf.tile(tf.expand_dims(nullWord, axis = 0), [self.batchSize, 1, 1])\n        words = tf.concat([nullWord, words], axis = 1)\n        lengths += 1\n        return words, lengths\n\n    \'\'\'\n    Initializes the cell internal state (currently it\'s stateful). In particular,\n    1. Data-structures (lists of attention maps and accumulated losses).\n    2. The memory and control states.\n    3. The knowledge base (optionally merging it with the question vectors)\n    4. The question words used by the cell (either the original word embeddings, or the \n       encoder outputs, with optional projection).\n\n    Args:\n        batchSize: the batch size\n\n    Returns the initial cell state.\n    \'\'\'\n    def zero_state(self, batchSize, dtype = tf.float32):\n        ## initialize data-structures\n        self.attentions = {""kb"": [], ""question"": [], ""self"": [], ""gate"": []}\n        self.autoEncLosses = {""control"": tf.constant(0.0), ""memory"": tf.constant(0.0)}\n\n\n        ## initialize state\n        initialControl = self.initState(""initCtrl"", config.ctrlDim, config.initCtrl, batchSize)\n        initialMemory = self.initState(""initMem"", config.memDim, config.initMem, batchSize)\n\n        self.controls = tf.expand_dims(initialControl, axis = 1)\n        self.memories = tf.expand_dims(initialMemory, axis = 1)\n        self.infos = tf.expand_dims(initialMemory, axis = 1)\n        \n        self.contControl = initialControl\n        # self.contControls = tf.expand_dims(initialControl, axis = 1)\n        # self.postControls = tf.expand_dims(initialControl, axis = 1)\n\n\n        ## initialize knowledge base\n        # optionally merge question into knowledge base representation\n        if config.initKBwithQ != ""NON"":\n            iVecQuestions = ops.linear(self.vecQuestions, config.ctrlDim, config.memDim, name = ""questions"") \n\n            concatMul = (config.initKBwithQ == ""MUL"") \n            cnct, dim = ops.concat(self.knowledgeBase, iVecQuestions, config.memDim, mul = concatMul, expandY = True)\n            self.knowledgeBase = ops.linear(cnct, dim, config.memDim, name = ""initKB"")\n\n\n        ## initialize question words\n        # choose question words to work with (original embeddings or encoder outputs)\n        words = self.questionCntxWords if config.controlContextual else self.questionWords\n        \n        # optionally add parametric ""null"" word in the to all questions\n        if config.addNullWord:\n            words, questionLengths = self.addNullWord(words, questionLengths)\n\n        # project words\n        self.inWords = self.outWords = words\n        if config.controlInWordsProj or config.controlOutWordsProj:\n            pWords = ops.linear(words, config.ctrlDim, config.ctrlDim, name = ""wordsProj"") \n            self.inWords = pWords if config.controlInWordsProj else words\n            self.outWords = pWords if config.controlOutWordsProj else words\n\n        # if config.controlCoverage:\n        #     self.coverage = tf.zeros((batchSize, tf.shape(words)[1]), dtype = tf.float32)\n        #     self.coverageBias = tf.get_variable(""coverageBias"", shape = (),\n        #         initializer = config.controlCoverageBias)  \n\n        ## initialize memory variational dropout mask\n        if config.memoryVariationalDropout:\n            self.memDpMask = ops.generateVarDpMask((batchSize, config.memDim), self.dropouts[""memory""])\n\n        return MACCellTuple(initialControl, initialMemory)     \n'"
main.py,8,"b'from __future__ import division\nimport warnings\nwarnings.filterwarnings(""ignore"", category=FutureWarning)\nwarnings.filterwarnings(""ignore"", message=""size changed"")\nimport sys \nimport os\nimport time\nimport math\nimport random\ntry:\n    import Queue as queue\nexcept ImportError:\n    import queue\nimport threading\nimport h5py\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom termcolor import colored, cprint\n\nfrom config import config, loadDatasetConfig, parseArgs\nfrom preprocess import Preprocesser, bold, bcolored, writeline, writelist\nfrom model import MACnet\nfrom collections import defaultdict\n\n############################################# loggers #############################################\n\n# Writes log header to file \ndef logInit():\n    with open(config.logFile(), ""a+"") as outFile:\n        writeline(outFile, config.expName)\n        headers = [""epoch"", ""trainAcc"", ""valAcc"", ""trainLoss"", ""valLoss""]\n        if config.evalTrain:\n            headers += [""evalTrainAcc"", ""evalTrainLoss""]\n        if config.extra:\n            if config.evalTrain:\n                headers += [""thAcc"", ""thLoss""]\n            headers += [""vhAcc"", ""vhLoss""]\n        headers += [""time"", ""lr""]\n\n        writelist(outFile, headers)\n        # lr assumed to be last\n\n# Writes log record to file \ndef logRecord(epoch, epochTime, lr, trainRes, evalRes, extraEvalRes):\n    with open(config.logFile(), ""a+"") as outFile:\n        record = [epoch, trainRes[""acc""], evalRes[""val""][""acc""], trainRes[""loss""], evalRes[""val""][""loss""]]\n        if config.evalTrain:\n            record += [evalRes[""evalTrain""][""acc""], evalRes[""evalTrain""][""loss""]]\n        if config.extra:\n            if config.evalTrain:\n                record += [extraEvalRes[""evalTrain""][""acc""], extraEvalRes[""evalTrain""][""loss""]]\n            record += [extraEvalRes[""val""][""acc""], extraEvalRes[""val""][""loss""]]\n        record += [epochTime, lr]\n\n        writelist(outFile, record)\n\n# Gets last logged epoch and learning rate\ndef lastLoggedEpoch():\n    with open(config.logFile(), ""r"") as inFile:\n        lastLine = list(inFile)[-1].split("","") \n    epoch = int(lastLine[0])\n    lr = float(lastLine[-1])   \n    return epoch, lr \n\n################################## printing, output and analysis ##################################\n\n# Analysis by type\nanalysisQuestionLims = [(0,18),(19,float(""inf""))]\nanalysisProgramLims = [(0,12),(13,float(""inf""))]\n\ntoArity = lambda instance: instance[""programSeq""][-1].split(""_"", 1)[0]\ntoType = lambda instance: instance[""programSeq""][-1].split(""_"", 1)[1]\n\ndef fieldLenIsInRange(field):\n    return lambda instance, group: \\\n        (len(instance[field]) >= group[0] and\n        len(instance[field]) <= group[1])\n\n# Groups instances based on a key\ndef grouperKey(toKey):\n    def grouper(instances):\n        res = defaultdict(list)\n        for instance in instances:\n            res[toKey(instance)].append(instance)\n        return res\n    return grouper\n\n# Groups instances according to their match to condition\ndef grouperCond(groups, isIn):\n    def grouper(instances):\n        res = {}\n        for group in groups:\n            res[group] = (instance for instance in instances if isIn(instance, group))\n        return res\n    return grouper \n\ngroupers = {\n    ""questionLength"": grouperCond(analysisQuestionLims, fieldLenIsInRange(""questionSeq"")),\n    ""programLength"": grouperCond(analysisProgramLims, fieldLenIsInRange(""programSeq"")),\n    ""arity"": grouperKey(toArity),\n    ""type"": grouperKey(toType)\n}\n\n# Computes average\ndef avg(instances, field):\n    if len(instances) == 0:\n        return 0.0\n    return sum(instances[field]) / len(instances)\n\n# Prints analysis of questions loss and accuracy by their group \ndef printAnalysis(res):\n    if config.analysisType != """":\n        print(""Analysis by {type}"".format(type = config.analysisType))\n        groups = groupers[config.analysisType](res[""preds""])\n        for key in groups:\n            instances = groups[key]\n            avgLoss = avg(instances, ""loss"")\n            avgAcc = avg(instances, ""acc"")\n            num = len(instances)\n            print(""Group {key}: Loss: {loss}, Acc: {acc}, Num: {num}"".format(key, avgLoss, avgAcc, num))\n\n# Print results for a tier\ndef printTierResults(tierName, res, color):\n    if res is None:\n        return\n\n    print(""{tierName} Loss: {loss}, {tierName} accuracy: {acc}"".format(tierName = tierName,\n        loss = bcolored(res[""loss""], color), \n        acc = bcolored(res[""acc""], color)))\n    \n    printAnalysis(res)\n\n# Prints dataset results (for several tiers)\ndef printDatasetResults(trainRes, evalRes, extraEvalRes):\n    printTierResults(""Training"", trainRes, ""magenta"")\n    printTierResults(""Training EMA"", evalRes[""evalTrain""], ""red"")\n    printTierResults(""Validation"", evalRes[""val""], ""cyan"")\n    printTierResults(""Extra Training EMA"", extraEvalRes[""evalTrain""], ""red"")\n    printTierResults(""Extra Validation"", extraEvalRes[""val""], ""cyan"")    \n\n# Writes predictions for several tiers\ndef writePreds(preprocessor, evalRes, extraEvalRes):\n    preprocessor.writePreds(evalRes[""evalTrain""], ""evalTrain"")\n    preprocessor.writePreds(evalRes[""val""], ""val"")\n    preprocessor.writePreds(evalRes[""test""], ""test"")\n    preprocessor.writePreds(extraEvalRes[""evalTrain""], ""evalTrain"", ""H"")\n    preprocessor.writePreds(extraEvalRes[""val""], ""val"", ""H"")\n    preprocessor.writePreds(extraEvalRes[""test""], ""test"", ""H"")\n\n############################################# session #############################################\n# Initializes TF session. Sets GPU memory configuration.\ndef setSession():\n    sessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)\n    if config.allowGrowth:\n        sessionConfig.gpu_options.allow_growth = True\n    if config.maxMemory < 1.0:\n        sessionConfig.gpu_options.per_process_gpu_memory_fraction = config.maxMemory\n    return sessionConfig\n\n############################################## savers #############################################\n# Initializes savers (standard, optional exponential-moving-average and optional for subset of variables)\ndef setSavers(model):\n    saver = tf.train.Saver(max_to_keep = config.weightsToKeep)\n\n    subsetSaver = None\n    if config.saveSubset:\n        isRelevant = lambda var: any(s in var.name for s in config.varSubset)\n        relevantVars = [var for var in tf.global_variables() if isRelevant(var)]\n        subsetSaver = tf.train.Saver(relevantVars, max_to_keep = config.weightsToKeep, allow_empty = True)\n    \n    emaSaver = None\n    if config.useEMA: \n        emaSaver = tf.train.Saver(model.emaDict, max_to_keep = config.weightsToKeep)\n\n    return {\n        ""saver"": saver,\n        ""subsetSaver"": subsetSaver,\n        ""emaSaver"": emaSaver\n    }\n\n################################### restore / initialize weights ##################################\n# Restores weights of specified / last epoch if on restore mod.\n# Otherwise, initializes weights.  \ndef loadWeights(sess, saver, init):\n    if config.restoreEpoch > 0 or config.restore:\n        # restore last epoch only if restoreEpoch isn\'t set\n        if config.restoreEpoch == 0:\n            # restore last logged epoch\n            config.restoreEpoch, config.lr = lastLoggedEpoch()\n        print(bcolored(""Restoring epoch {} and lr {}"".format(config.restoreEpoch, config.lr),""cyan""))\n        print(bcolored(""Restoring weights"", ""blue""))\n        saver.restore(sess, config.weightsFile(config.restoreEpoch))\n        epoch = config.restoreEpoch\n    else:\n        print(bcolored(""Initializing weights"", ""blue""))\n        sess.run(init)\n        logInit()\n        epoch = 0\n\n    return epoch \n\n###################################### training / evaluation ######################################\n# Chooses data to train on (main / extra) data. \ndef chooseTrainingData(data):\n    trainingData = data[""main""][""train""]\n    alterData = None\n\n    if config.extra:\n        if config.trainExtra:\n            if config.extraVal:\n                trainingData = data[""extra""][""val""]\n            else:\n                trainingData = data[""extra""][""train""]                  \n        if config.alterExtra:\n            alterData = data[""extra""][""train""]\n\n    return trainingData, alterData\n\n#### evaluation\n# Runs evaluation on train / val / test datasets.\ndef runEvaluation(sess, model, data, epoch, evalTrain = True, evalTest = False, getAtt = None):\n    if getAtt is None:\n        getAtt = config.getAtt\n    res = {""evalTrain"": None, ""val"": None, ""test"": None}\n    \n    if data is not None:\n        if evalTrain and config.evalTrain:\n            res[""evalTrain""] = runEpoch(sess, model, data[""evalTrain""], train = False, epoch = epoch, getAtt = getAtt)\n\n        res[""val""] = runEpoch(sess, model, data[""val""], train = False, epoch = epoch, getAtt = getAtt)\n        \n        if evalTest or config.test:\n            res[""test""] = runEpoch(sess, model, data[""test""], train = False, epoch = epoch, getAtt = getAtt)    \n        \n    return res\n\n## training conditions (comparing current epoch result to prior ones)\ndef improveEnough(curr, prior, lr):\n    prevRes = prior[""prev""][""res""]\n    currRes = curr[""res""]\n\n    if prevRes is None:\n        return True\n\n    prevTrainLoss = prevRes[""train""][""loss""]\n    currTrainLoss = currRes[""train""][""loss""]\n    lossDiff = prevTrainLoss - currTrainLoss\n    \n    notImprove = ((lossDiff < 0.015 and prevTrainLoss < 0.5 and lr > 0.00002) or \\\n                  (lossDiff < 0.008 and prevTrainLoss < 0.15 and lr > 0.00001) or \\\n                  (lossDiff < 0.003 and prevTrainLoss < 0.10 and lr > 0.000005))\n                  #(prevTrainLoss < 0.2 and config.lr > 0.000015)\n    \n    return not notImprove\n\ndef better(currRes, bestRes):\n    return currRes[""val""][""acc""] > bestRes[""val""][""acc""]\n\n############################################## data ###############################################\n#### instances and batching \n# Trims sequences based on their max length.\ndef trim2DVectors(vectors, vectorsLengths):\n    maxLength = np.max(vectorsLengths)\n    return vectors[:,:maxLength]\n\n# Trims batch based on question length.\ndef trimData(data):\n    data[""questions""] = trim2DVectors(data[""questions""], data[""questionLengths""])\n    return data\n\n# Gets batch / bucket size.\ndef getLength(data):\n    return len(data[""instances""])\n\n# Selects the data entries that match the indices. \ndef selectIndices(data, indices):\n    def select(field, indices): \n        if type(field) is np.ndarray:\n            return field[indices]\n        if type(field) is list:\n            return [field[i] for i in indices]\n        else:\n            return field\n    selected = {k : select(d, indices) for k,d in data.items()}\n    return selected\n\n# Batches data into a a list of batches of batchSize. \n# Shuffles the data by default.\ndef getBatches(data, batchSize = None, shuffle = True):\n    batches = []\n\n    dataLen = getLength(data)\n    if batchSize is None or batchSize > dataLen:\n        batchSize = dataLen\n    \n    indices = np.arange(dataLen)\n    if shuffle:\n        np.random.shuffle(indices)\n\n    for batchStart in range(0, dataLen, batchSize):\n        batchIndices = indices[batchStart : batchStart + batchSize]\n        # if len(batchIndices) == batchSize?\n        if len(batchIndices) >= config.gpusNum:\n            batch = selectIndices(data, batchIndices)\n            batches.append(batch)\n            # batchesIndices.append((data, batchIndices))\n\n    return batches\n\n#### image batches\n# Opens image files.\ndef openImageFiles(images):\n    images[""imagesFile""] = h5py.File(images[""imagesFilename""], ""r"")\n    images[""imagesIds""] = None\n    if config.dataset == ""NLVR"":\n        with open(images[""imageIdsFilename""], ""r"") as imageIdsFile:\n            images[""imagesIds""] = json.load(imageIdsFile)  \n\n# Closes image files.\ndef closeImageFiles(images): \n    images[""imagesFile""].close()\n\n# Loads an images from file for a given data batch.\ndef loadImageBatch(images, batch):\n    imagesFile = images[""imagesFile""]\n    id2idx = images[""imagesIds""]\n\n    toIndex = lambda imageId: imageId\n    if id2idx is not None:\n        toIndex = lambda imageId: id2idx[imageId]\n    imageBatch = np.stack([imagesFile[""features""][toIndex(imageId)] for imageId in batch[""imageIds""]], axis = 0)\n    \n    return {""images"": imageBatch, ""imageIds"": batch[""imageIds""]}\n\n# Loads images for several num batches in the batches list from start index. \ndef loadImageBatches(images, batches, start, num):\n    batches = batches[start: start + num]\n    return [loadImageBatch(images, batch) for batch in batches]\n\n#### data alternation\n# Alternates main training batches with extra data.\ndef alternateData(batches, alterData, dataLen):\n    alterData = alterData[""data""][0] # data isn\'t bucketed for altered data\n\n    # computes number of repetitions\n    needed = math.ceil(len(batches) / config.alterNum) \n    print(bold(""Extra batches needed: %d"") % needed)\n    perData = math.ceil(getLength(alterData) / config.batchSize)\n    print(bold(""Batches per extra data: %d"") % perData)\n    repetitions = math.ceil(needed / perData)\n    print(bold(""reps: %d"") % repetitions)\n    \n    # make alternate batches\n    alterBatches = []\n    for _ in range(repetitions):\n        repBatches = getBatches(alterData, batchSize = config.batchSize)\n        random.shuffle(repBatches)\n        alterBatches += repBatches\n    print(bold(""Batches num: %d"") + len(alterBatches))\n    \n    # alternate data with extra data\n    curr = len(batches) - 1\n    for alterBatch in alterBatches:\n        if curr < 0:\n            # print(colored(""too many"" + str(curr) + "" "" + str(len(batches)),""red""))\n            break\n        batches.insert(curr, alterBatch)\n        dataLen += getLength(alterBatch)\n        curr -= config.alterNum\n\n    return batches, dataLen\n\n############################################ threading ############################################\n\nimagesQueue = queue.Queue(maxsize = 20) # config.tasksNum\ninQueue = queue.Queue(maxsize = 1)\noutQueue = queue.Queue(maxsize = 1)\n\n# Runs a worker thread(s) to load images while training .\nclass StoppableThread(threading.Thread):\n    # Thread class with a stop() method. The thread itself has to check\n    # regularly for the stopped() condition.\n\n    def __init__(self, images, batches): # i\n        super(StoppableThread, self).__init__()\n        # self.i = i\n        self.images = images\n        self.batches = batches\n        self._stop_event = threading.Event()\n\n    # def __init__(self, args):\n    #     super(StoppableThread, self).__init__(args = args)\n    #     self._stop_event = threading.Event()\n\n    # def __init__(self, target, args):\n    #     super(StoppableThread, self).__init__(target = target, args = args)\n    #     self._stop_event = threading.Event()\n\n    def stop(self):\n        self._stop_event.set()\n\n    def stopped(self):\n        return self._stop_event.is_set()\n\n    def run(self):\n        while not self.stopped():\n            try:\n                batchNum = inQueue.get(timeout = 60)\n                nextItem = loadImageBatches(self.images, self.batches, batchNum, int(config.taskSize / 2))\n                outQueue.put(nextItem)\n                # inQueue.task_done()\n            except:\n                pass\n        # print(""worker %d done"", self.i)\n\ndef loaderRun(images, batches):\n    batchNum = 0\n\n    # if config.workers == 2:           \n    #     worker = StoppableThread(images, batches) # i, \n    #     worker.daemon = True\n    #     worker.start() \n\n    #     while batchNum < len(batches):\n    #         inQueue.put(batchNum + int(config.taskSize / 2))\n    #         nextItem1 = loadImageBatches(images, batches, batchNum, int(config.taskSize / 2))\n    #         nextItem2 = outQueue.get()\n\n    #         nextItem = nextItem1 + nextItem2\n    #         assert len(nextItem) == min(config.taskSize, len(batches) - batchNum)\n    #         batchNum += config.taskSize\n            \n    #         imagesQueue.put(nextItem)\n\n    #     worker.stop()\n    # else:\n    while batchNum < len(batches):\n        nextItem = loadImageBatches(images, batches, batchNum, config.taskSize)\n        assert len(nextItem) == min(config.taskSize, len(batches) - batchNum)\n        batchNum += config.taskSize                    \n        imagesQueue.put(nextItem)\n\n    # print(""manager loader done"")\n\n########################################## stats tracking #########################################\n# Computes exponential moving average.\ndef emaAvg(avg, value):\n    if avg is None:\n        return value\n    emaRate = 0.98\n    return avg * emaRate + value * (1 - emaRate)\n\n# Initializes training statistics.\ndef initStats():\n    return {\n        ""totalBatches"": 0,\n        ""totalData"": 0,\n        ""totalLoss"": 0.0,\n        ""totalCorrect"": 0,\n        ""loss"": 0.0,\n        ""acc"": 0.0,\n        ""emaLoss"": None,\n        ""emaAcc"": None,\n    }\n\n# Updates statistics with training results of a batch\ndef updateStats(stats, res, batch):\n    stats[""totalBatches""] += 1\n    stats[""totalData""] += getLength(batch)\n\n    stats[""totalLoss""] += res[""loss""]\n    stats[""totalCorrect""] += res[""correctNum""]\n\n    stats[""loss""] = stats[""totalLoss""] / stats[""totalBatches""]\n    stats[""acc""] = stats[""totalCorrect""] / stats[""totalData""]\n    \n    stats[""emaLoss""] = emaAvg(stats[""emaLoss""], res[""loss""])\n    stats[""emaAcc""] = emaAvg(stats[""emaAcc""], res[""acc""])\n                                                    \n    return stats \n\n# auto-encoder ae = {:2.4f} autoEncLoss,\n# Translates training statistics into a string to print\ndef statsToStr(stats, res, epoch, batchNum, dataLen, startTime):\n    formatStr = ""\\reb {epoch},{batchNum} ({dataProcessed} / {dataLen:5d}), "" + \\\n                             ""t = {time} ({loadTime:2.2f}+{trainTime:2.2f}), "" + \\\n                             ""lr {lr}, l = {loss}, a = {acc}, avL = {avgLoss}, "" + \\\n                             ""avA = {avgAcc}, g = {gradNorm:2.4f}, "" + \\\n                             ""emL = {emaLoss:2.4f}, emA = {emaAcc:2.4f}; "" + \\\n                             ""{expname}"" # {machine}/{gpu}""\n\n    s_epoch = bcolored(""{:2d}"".format(epoch),""green"")\n    s_batchNum = ""{:3d}"".format(batchNum)\n    s_dataProcessed = bcolored(""{:5d}"".format(stats[""totalData""]),""green"")\n    s_dataLen = dataLen\n    s_time = bcolored(""{:2.2f}"".format(time.time() - startTime),""green"")\n    s_loadTime = res[""readTime""] \n    s_trainTime = res[""trainTime""]\n    s_lr = bold(config.lr)\n    s_loss = bcolored(""{:2.4f}"".format(res[""loss""]), ""blue"")\n    s_acc = bcolored(""{:2.4f}"".format(res[""acc""]),""blue"")\n    s_avgLoss = bcolored(""{:2.4f}"".format(stats[""loss""]), ""blue"")\n    s_avgAcc = bcolored(""{:2.4f}"".format(stats[""acc""]),""red"")\n    s_gradNorm = res[""gradNorm""]  \n    s_emaLoss = stats[""emaLoss""]\n    s_emaAcc = stats[""emaAcc""]\n    s_expname = config.expName \n    # s_machine = bcolored(config.dataPath[9:11],""green"") \n    # s_gpu = bcolored(config.gpus,""green"")\n\n    return formatStr.format(epoch = s_epoch, batchNum = s_batchNum, dataProcessed = s_dataProcessed,\n                            dataLen = s_dataLen, time = s_time, loadTime = s_loadTime,\n                            trainTime = s_trainTime, lr = s_lr, loss = s_loss, acc = s_acc,\n                            avgLoss = s_avgLoss, avgAcc = s_avgAcc, gradNorm = s_gradNorm,\n                            emaLoss = s_emaLoss, emaAcc = s_emaAcc, expname = s_expname)\n                            # machine = s_machine, gpu = s_gpu)\n\n# collectRuntimeStats, writer = None,  \n\'\'\'\nRuns an epoch with model and session over the data.\n1. Batches the data and optionally mix it with the extra alterData.\n2. Start worker threads to load images in parallel to training.\n3. Runs model for each batch, and gets results (e.g. loss,  accuracy).\n4. Updates and prints statistics based on batch results.\n5. Once in a while (every config.saveEvery), save weights. \n\nArgs:\n    sess: TF session to run with.\n    \n    model: model to process data. Has runBatch method that process a given batch.\n    (See model.py for further details).\n    \n    data: data to use for training/evaluation.\n    \n    epoch: epoch number.\n\n    saver: TF saver to save weights\n\n    calle: a method to call every number of iterations (config.calleEvery)\n\n    alterData: extra data to mix with main data while training.\n\n    getAtt: True to return model attentions.  \n\'\'\'\ndef runEpoch(sess, model, data, train, epoch, saver = None, calle = None, \n    alterData = None, getAtt = False):\n    # train = data[""train""] better than outside argument\n\n    # initialization\n    startTime0 = time.time()\n\n    stats = initStats()\n    preds = []\n\n    # open image files\n    openImageFiles(data[""images""])\n\n    ## prepare batches\n    buckets = data[""data""]\n    dataLen = sum(getLength(bucket) for bucket in buckets)\n    \n    # make batches and randomize\n    batches = []\n    for bucket in buckets:\n        batches += getBatches(bucket, batchSize = config.batchSize)\n    random.shuffle(batches)\n\n    # alternate with extra data\n    if train and alterData is not None:\n        batches, dataLen = alternateData(batches, alterData, dataLen)\n\n    # start image loaders\n    if config.parallel:\n        loader = threading.Thread(target = loaderRun, args = (data[""images""], batches))\n        loader.daemon = True\n        loader.start()\n\n    for batchNum, batch in enumerate(batches):   \n        startTime = time.time()\n\n        # prepare batch \n        batch = trimData(batch)\n\n        # load images batch\n        if config.parallel:\n            if batchNum % config.taskSize == 0:\n                imagesBatches = imagesQueue.get()\n            imagesBatch = imagesBatches[batchNum % config.taskSize] # len(imagesBatches)     \n        else:\n            imagesBatch = loadImageBatch(data[""images""], batch)\n        for i, imageId in enumerate(batch[""imageIds""]):\n            assert imageId == imagesBatch[""imageIds""][i]   \n        \n        # run batch\n        res = model.runBatch(sess, batch, imagesBatch, train, getAtt) \n\n        # update stats\n        stats = updateStats(stats, res, batch)\n        preds += res[""preds""]\n\n        # if config.summerize and writer is not None:\n        #     writer.add_summary(res[""summary""], epoch)\n\n        sys.stdout.write(statsToStr(stats, res, epoch, batchNum, dataLen, startTime))\n        sys.stdout.flush()\n\n        # save weights\n        if saver is not None:\n            if batchNum > 0 and batchNum % config.saveEvery == 0:\n                print("""")\n                print(bold(""saving weights""))\n                saver.save(sess, config.weightsFile(epoch))\n\n        # calle\n        if calle is not None:            \n            if batchNum > 0 and batchNum % config.calleEvery == 0:\n                calle()\n    \n    sys.stdout.write(""\\r"")\n    sys.stdout.flush()\n\n    print("""")\n\n    closeImageFiles(data[""images""])\n\n    if config.parallel:\n        loader.join() # should work\n\n    return {""loss"": stats[""loss""], \n            ""acc"": stats[""acc""],\n            ""preds"": preds\n            }\n\n\'\'\'\nTrains/evaluates the model:\n1. Set GPU configurations.\n2. Preprocess data: reads from datasets, and convert into numpy arrays.\n3. Builds the TF computational graph for the MAC model.\n4. Starts a session and initialize / restores weights.\n5. If config.train is True, trains the model for number of epochs:\n    a. Trains the model on training data\n    b. Evaluates the model on training / validation data, optionally with \n       exponential-moving-average weights.\n    c. Prints and logs statistics, and optionally saves model predictions.\n    d. Optionally reduces learning rate if losses / accuracies don\'t improve,\n       and applies early stopping.\n6. If config.test is True, runs a final evaluation on the dataset and print\n   final results!\n\'\'\'\ndef main():\n    with open(config.configFile(), ""a+"") as outFile:\n        json.dump(vars(config), outFile)\n\n    # set gpus\n    if config.gpus != """":\n        config.gpusNum = len(config.gpus.split("",""))\n        os.environ[""CUDA_VISIBLE_DEVICES""] = config.gpus\n\n    tf.logging.set_verbosity(tf.logging.ERROR)\n\n    # process data\n    print(bold(""Preprocess data...""))\n    start = time.time()\n    preprocessor = Preprocesser()\n    data, embeddings, answerDict = preprocessor.preprocessData()\n    print(""took {} seconds"".format(bcolored(""{:.2f}"".format(time.time() - start), ""blue"")))\n\n    # build model\n    print(bold(""Building model...""))\n    start = time.time()\n    model = MACnet(embeddings, answerDict)\n    print(""took {} seconds"".format(bcolored(""{:.2f}"".format(time.time() - start), ""blue"")))\n\n    # initializer\n    init = tf.global_variables_initializer()\n\n    # savers\n    savers = setSavers(model)\n    saver, emaSaver = savers[""saver""], savers[""emaSaver""]\n\n    # sessionConfig\n    sessionConfig = setSession()\n    \n    with tf.Session(config = sessionConfig) as sess:\n\n        # ensure no more ops are added after model is built\n        sess.graph.finalize()\n\n        # restore / initialize weights, initialize epoch variable\n        epoch = loadWeights(sess, saver, init)\n\n        if config.train:\n            start0 = time.time()\n\n            bestEpoch = epoch \n            bestRes = None\n            prevRes = None\n\n            # epoch in [restored + 1, epochs]\n            for epoch in range(config.restoreEpoch + 1, config.epochs + 1):\n                print(bcolored(""Training epoch {}..."".format(epoch), ""green""))\n                start = time.time()\n                \n                # train\n                # calle = lambda: model.runEpoch(), collectRuntimeStats, writer\n                trainingData, alterData = chooseTrainingData(data)\n                trainRes = runEpoch(sess, model, trainingData, train = True, epoch = epoch, \n                    saver = saver, alterData = alterData)\n                \n                # save weights\n                saver.save(sess, config.weightsFile(epoch))\n                if config.saveSubset:\n                    subsetSaver.save(sess, config.subsetWeightsFile(epoch))                   \n                \n                # load EMA weights \n                if config.useEMA:\n                    print(bold(""Restoring EMA weights""))\n                    emaSaver.restore(sess, config.weightsFile(epoch))\n\n                # evaluation                \n                evalRes = runEvaluation(sess, model, data[""main""], epoch)\n                extraEvalRes = runEvaluation(sess, model, data[""extra""], epoch, \n                    evalTrain = not config.extraVal)\n\n                # restore standard weights\n                if config.useEMA:\n                    print(bold(""Restoring standard weights""))\n                    saver.restore(sess, config.weightsFile(epoch))\n\n                print("""")\n\n                epochTime = time.time() - start\n                print(""took {:.2f} seconds"".format(epochTime))\n\n                # print results\n                printDatasetResults(trainRes, evalRes, extraEvalRes)\n   \n                # stores predictions and optionally attention maps\n                if config.getPreds:\n                    print(bcolored(""Writing predictions..."", ""white""))\n                    writePreds(preprocessor, evalRes, extraEvalRes)\n\n                logRecord(epoch, epochTime, config.lr, trainRes, evalRes, extraEvalRes)\n\n                # update best result\n                # compute curr and prior\n                currRes = {""train"": trainRes, ""val"": evalRes[""val""]}\n                curr = {""res"": currRes, ""epoch"": epoch} \n\n                if bestRes is None or better(currRes, bestRes):\n                    bestRes = currRes\n                    bestEpoch = epoch\n                \n                prior = {""best"": {""res"": bestRes, ""epoch"": bestEpoch}, \n                         ""prev"": {""res"": prevRes, ""epoch"": epoch - 1}}\n\n                # lr reducing\n                if config.lrReduce:\n                    if not improveEnough(curr, prior, config.lr):\n                        config.lr *= config.lrDecayRate\n                        print(colored(""Reducing LR to {}"".format(config.lr), ""red""))   \n\n                # early stopping\n                if config.earlyStopping > 0:\n                    if epoch - bestEpoch > config.earlyStopping:\n                        break\n\n                # update previous result\n                prevRes = currRes\n\n            # reduce epoch back to the last one we trained on\n            epoch -= 1\n            print(""Training took {:.2f} seconds ({:} epochs)"".format(time.time() - start0, \n                epoch - config.restoreEpoch))\n        \n        if config.finalTest:\n            print(""Testing on epoch {}..."".format(epoch))\n            \n            start = time.time()\n            if epoch > 0:\n                if config.useEMA:\n                    emaSaver.restore(sess, config.weightsFile(epoch))\n                else:\n                    saver.restore(sess, config.weightsFile(epoch))\n\n            evalRes = runEvaluation(sess, model, data[""main""], epoch, evalTest = True)\n            extraEvalRes = runEvaluation(sess, model, data[""extra""], epoch, \n                evalTrain = not config.extraVal, evalTest = True)\n                        \n            print(""took {:.2f} seconds"".format(time.time() - start))\n            printDatasetResults(None, evalRes, extraEvalRes)\n\n            print(""Writing predictions..."")\n            writePreds(preprocessor, evalRes, extraEvalRes)\n\n        print(bcolored(""Done!"",""white""))\n\nif __name__ == \'__main__\':\n    parseArgs()    \n    loadDatasetConfig[config.dataset]()        \n    main()\n'"
mi_gru_cell.py,19,"b'import tensorflow as tf\nimport numpy as np\n\nclass MiGRUCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, num_units, input_size = None, activation = tf.tanh, reuse = None):\n        self.numUnits = num_units\n        self.activation = activation\n        self.reuse = reuse\n\n    @property\n    def state_size(self):\n        return self.numUnits\n\n    @property\n    def output_size(self):\n        return self.numUnits\n\n    def mulWeights(self, inp, inDim, outDim, name = """"): \n        with tf.variable_scope(""weights"" + name):\n            W = tf.get_variable(""weights"", shape = (inDim, outDim),\n                initializer = tf.contrib.layers.xavier_initializer())\n\n        output = tf.matmul(inp, W)        \n        return output\n\n    def addBiases(self, inp1, inp2, dim, bInitial = 0, name = """"):\n        with tf.variable_scope(""additiveBiases"" + name):\n            b = tf.get_variable(""biases"", shape = (dim,), \n                initializer = tf.zeros_initializer()) + bInitial\n        with tf.variable_scope(""multiplicativeBias"" + name):\n            beta = tf.get_variable(""biases"", shape = (3 * dim,), \n                initializer = tf.ones_initializer())\n\n        Wx, Uh, inter = tf.split(beta * tf.concat([inp1, inp2, inp1 * inp2], axis = 1), \n            num_or_size_splits = 3, axis = 1)\n        output = Wx + Uh + inter + b        \n        return output\n\n    def __call__(self, inputs, state, scope = None):\n        scope = scope or type(self).__name__\n        with tf.variable_scope(scope, reuse = self.reuse):\n            inputSize = int(inputs.shape[1])\n            \n            Wxr = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxr"")\n            Uhr = self.mulWeights(state, self.numUnits, self.numUnits, name = ""Uhr"")\n            \n            r = tf.nn.sigmoid(self.addBiases(Wxr, Uhr, self.numUnits, bInitial = 1, name = ""r""))\n            \n            Wxu = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxu"")\n            Uhu = self.mulWeights(state, self.numUnits, self.numUnits, name = ""Uhu"")\n            \n            u = tf.nn.sigmoid(self.addBiases(Wxu, Uhu, self.numUnits, bInitial = 1, name = ""u""))\n            # r, u = tf.split(gates, num_or_size_splits = 2, axis = 1)\n\n            Wx = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxl"")\n            Urh = self.mulWeights(r * state, self.numUnits, self.numUnits, name = ""Uhl"")\n            c = self.activation(self.addBiases(Wx, Urh, self.numUnits, name = ""2""))\n\n            newH = u * state + (1 - u) * c # switch u and 1-u?\n        return newH, newH\n\n    def zero_state(self, batchSize, dtype = tf.float32):\n        return tf.zeros((batchSize, self.numUnits), dtype = dtype)\n        '"
mi_lstm_cell.py,22,"b'import tensorflow as tf\nimport numpy as np\n\nclass MiLSTMCell(tf.nn.rnn_cell.RNNCell):\n    def __init__(self, num_units, forget_bias = 1.0, input_size = None,\n               state_is_tuple = True, activation = tf.tanh, reuse = None):\n        self.numUnits = num_units\n        self.forgetBias = forget_bias\n        self.activation = activation\n        self.reuse = reuse\n\n    @property\n    def state_size(self):\n        return tf.nn.rnn_cell.LSTMStateTuple(self.numUnits, self.numUnits)          \n\n    @property\n    def output_size(self):\n        return self.numUnits\n\n    def mulWeights(self, inp, inDim, outDim, name = """"):\n        with tf.variable_scope(""weights"" + name):\n            W = tf.get_variable(""weights"", shape = (inDim, outDim),\n                initializer = tf.contrib.layers.xavier_initializer())\n        output = tf.matmul(inp, W)        \n        return output\n\n    def addBiases(self, inp1, inp2, dim, name = """"):\n        with tf.variable_scope(""additiveBiases"" + name):\n            b = tf.get_variable(""biases"", shape = (dim,), \n                initializer = tf.zeros_initializer())\n        with tf.variable_scope(""multiplicativeBias"" + name):\n            beta = tf.get_variable(""biases"", shape = (3 * dim,), \n                initializer = tf.ones_initializer())\n\n        Wx, Uh, inter = tf.split(beta * tf.concat([inp1, inp2, inp1 * inp2], axis = 1), \n            num_or_size_splits = 3, axis = 1)\n        output = Wx + Uh + inter + b\n        return output\n\n    def __call__(self, inputs, state, scope = None):\n        scope = scope or type(self).__name__\n        with tf.variable_scope(scope, reuse = self.reuse):\n            c, h = state        \n            inputSize = int(inputs.shape[1])\n\n            Wx = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxi"")\n            Uh = self.mulWeights(h, self.numUnits, self.numUnits, name = ""Uhi"")\n            \n            i = self.addBiases(Wx, Uh, self.numUnits, name = ""i"")\n\n            Wx = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxj"")\n            Uh = self.mulWeights(h, self.numUnits, self.numUnits, name = ""Uhj"")\n            \n            j = self.addBiases(Wx, Uh, self.numUnits, name = ""l"")\n\n            Wx = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxf"")\n            Uh = self.mulWeights(h, self.numUnits, self.numUnits, name = ""Uhf"")\n            \n            f = self.addBiases(Wx, Uh, self.numUnits, name = ""f"")\n\n            Wx = self.mulWeights(inputs, inputSize, self.numUnits, name = ""Wxo"")\n            Uh = self.mulWeights(h, self.numUnits, self.numUnits, name = ""Uho"")\n            \n            o = self.addBiases(Wx, Uh, self.numUnits, name = ""o"")\n            # i, j, f, o = tf.split(value = concat, num_or_size_splits = 4, axis = 1)\n\n            newC = (c * tf.nn.sigmoid(f + self.forgetBias) + tf.nn.sigmoid(i) *\n                    self.activation(j))\n            newH = self.activation(newC) * tf.nn.sigmoid(o)\n\n            newState = tf.nn.rnn_cell.LSTMStateTuple(newC, newH)\n        return newH, newState\n\n    def zero_state(self, batchSize, dtype = tf.float32):\n        return tf.nn.rnn_cell.LSTMStateTuple(tf.zeros((batchSize, self.numUnits), dtype = dtype),\n                                        tf.zeros((batchSize, self.numUnits), dtype = dtype))\n        '"
model.py,93,"b'import time\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nimport ops\nfrom config import config\nfrom mac_cell import MACCell\n\'\'\'\nThe MAC network model. It performs reasoning processes to answer a question over\nknowledge base (the image) by decomposing it into attention-based computational steps,\neach perform by a recurrent MAC cell.\n\nThe network has three main components. \nInput unit: processes the network inputs: raw question strings and image into\ndistributional representations.\n\nThe MAC network: calls the MACcells (mac_cell.py) config.netLength number of times,\nto perform the reasoning process over the question and image.\n\nThe output unit: a classifier that receives the question and final state of the MAC\nnetwork and uses them to compute log-likelihood over the possible one-word answers.       \n\'\'\'\nclass MACnet(object):\n\n    \'\'\'Initialize the class.\n    \n    Args: \n        embeddingsInit: initialization for word embeddings (random / glove).\n        answerDict: answers dictionary (mapping between integer id and symbol).\n    \'\'\'\n    def __init__(self, embeddingsInit, answerDict):\n        self.embeddingsInit = embeddingsInit\n        self.answerDict = answerDict\n        self.build()\n\n    \'\'\'\n    Initializes placeholders.\n        questionsIndicesAll: integer ids of question words. \n        [batchSize, questionLength]\n        \n        questionLengthsAll: length of each question. \n        [batchSize]\n        \n        imagesPlaceholder: image features. \n        [batchSize, channels, height, width]\n        (converted internally to [batchSize, height, width, channels])\n        \n        answersIndicesAll: integer ids of answer words. \n        [batchSize]\n\n        lr: learning rate (tensor scalar)\n        train: train / evaluation (tensor boolean)\n\n        dropout values dictionary (tensor scalars)\n    \'\'\'\n    # change to H x W x C?\n    def addPlaceholders(self):\n        with tf.variable_scope(""Placeholders""):\n            ## data\n            # questions            \n            self.questionsIndicesAll = tf.placeholder(tf.int32, shape = (None, None))\n            self.questionLengthsAll = tf.placeholder(tf.int32, shape = (None, ))\n\n            # images\n            # put image known dimension as last dim?\n            self.imagesPlaceholder = tf.placeholder(tf.float32, shape = (None, None, None, None))\n            self.imagesAll = tf.transpose(self.imagesPlaceholder, (0, 2, 3, 1))\n            # self.imageH = tf.shape(self.imagesAll)[1]\n            # self.imageW = tf.shape(self.imagesAll)[2]\n\n            # answers\n            self.answersIndicesAll = tf.placeholder(tf.int32, shape = (None, ))\n\n            ## optimization\n            self.lr = tf.placeholder(tf.float32, shape = ())\n            self.train = tf.placeholder(tf.bool, shape = ())\n            self.batchSizeAll = tf.shape(self.questionsIndicesAll)[0]\n\n            ## dropouts\n            # TODO: change dropouts to be 1 - current\n            self.dropouts = {\n                ""encInput"": tf.placeholder(tf.float32, shape = ()),\n                ""encState"": tf.placeholder(tf.float32, shape = ()),\n                ""stem"": tf.placeholder(tf.float32, shape = ()),\n                ""question"": tf.placeholder(tf.float32, shape = ()),\n                # self.dropouts[""question""]Out = tf.placeholder(tf.float32, shape = ())\n                # self.dropouts[""question""]MAC = tf.placeholder(tf.float32, shape = ())\n                ""read"": tf.placeholder(tf.float32, shape = ()),\n                ""write"": tf.placeholder(tf.float32, shape = ()),\n                ""memory"": tf.placeholder(tf.float32, shape = ()),\n                ""output"": tf.placeholder(tf.float32, shape = ())\n            }\n\n            # batch norm params\n            self.batchNorm = {""decay"": config.bnDecay, ""train"": self.train}\n\n            # if config.parametricDropout:\n            #     self.dropouts[""question""] = parametricDropout(""qDropout"", self.train)\n            #     self.dropouts[""read""] = parametricDropout(""readDropout"", self.train)\n            # else:\n            #     self.dropouts[""question""] = self.dropouts[""_q""]\n            #     self.dropouts[""read""] = self.dropouts[""_read""]\n            \n            # if config.tempDynamic:\n            #     self.tempAnnealRate = tf.placeholder(tf.float32, shape = ())\n\n            self.H, self.W, self.imageInDim = config.imageDims\n\n    # Feeds data into placeholders. See addPlaceholders method for further details.\n    def createFeedDict(self, data, images, train):\n        feedDict = {\n            self.questionsIndicesAll: data[""questions""],\n            self.questionLengthsAll: data[""questionLengths""],\n            self.imagesPlaceholder: images[""images""],\n            self.answersIndicesAll: data[""answers""],\n            \n            self.dropouts[""encInput""]: config.encInputDropout if train else 1.0,\n            self.dropouts[""encState""]: config.encStateDropout if train else 1.0,\n            self.dropouts[""stem""]: config.stemDropout if train else 1.0,\n            self.dropouts[""question""]: config.qDropout if train else 1.0, #_\n            self.dropouts[""memory""]: config.memoryDropout if train else 1.0,\n            self.dropouts[""read""]: config.readDropout if train else 1.0, #_\n            self.dropouts[""write""]: config.writeDropout if train else 1.0,\n            self.dropouts[""output""]: config.outputDropout if train else 1.0,\n            # self.dropouts[""question""]Out: config.qDropoutOut if train else 1.0,\n            # self.dropouts[""question""]MAC: config.qDropoutMAC if train else 1.0,\n\n            self.lr: config.lr,\n            self.train: train\n        }\n\n        # if config.tempDynamic:\n        #     feedDict[self.tempAnnealRate] = tempAnnealRate          \n\n        return feedDict\n\n    # Splits data to a specific GPU (tower) for parallelization\n    def initTowerBatch(self, towerI, towersNum, dataSize):\n        towerBatchSize = tf.floordiv(dataSize, towersNum)\n        start = towerI * towerBatchSize\n        end = (towerI + 1) * towerBatchSize if towerI < towersNum - 1 else dataSize\n\n        self.questionsIndices = self.questionsIndicesAll[start:end]\n        self.questionLengths = self.questionLengthsAll[start:end]\n        self.images = self.imagesAll[start:end]\n        self.answersIndices = self.answersIndicesAll[start:end]\n\n        self.batchSize = end - start\n\n    \'\'\'\n    The Image Input Unit (stem). Passes the image features through a CNN-network\n    Optionally adds position encoding (doesn\'t in the default behavior).\n    Flatten the image into Height * Width ""Knowledge base"" array.\n\n    Args:\n        images: image input. [batchSize, height, width, inDim]\n        inDim: input image dimension\n        outDim: image out dimension\n        addLoc: if not None, adds positional encoding to the image\n\n    Returns preprocessed images. \n    [batchSize, height * width, outDim]\n    \'\'\'\n    def stem(self, images, inDim, outDim, addLoc = None):\n\n        with tf.variable_scope(""stem""):        \n            if addLoc is None:\n                addLoc = config.locationAware\n\n            if config.stemLinear:\n                features = ops.linear(images, inDim, outDim)\n            else:\n                dims = [inDim] + ([config.stemDim] * (config.stemNumLayers - 1)) + [outDim]\n\n                if addLoc:\n                    images, inDim = ops.addLocation(images, inDim, config.locationDim, \n                        h = self.H, w = self.W, locType = config.locationType) \n                    dims[0] = inDim\n\n                    # if config.locationType == ""PE"":\n                    #     dims[-1] /= 4\n                    #     dims[-1] *= 3\n                    # else:\n                    #     dims[-1] -= 2\n                features = ops.CNNLayer(images, dims, \n                    batchNorm = self.batchNorm if config.stemBN else None,\n                    dropout = self.dropouts[""stem""],\n                    kernelSizes = config.stemKernelSizes, \n                    strides = config.stemStrideSizes)\n\n                # if addLoc:\n                #     lDim = outDim / 4\n                #     lDim /= 4\n                #     features, _ = addLocation(features, dims[-1], lDim, h = H, w = W, \n                #         locType = config.locationType) \n\n                if config.stemGridRnn:\n                    features = ops.multigridRNNLayer(features, H, W, outDim)\n\n            # flatten the 2d images into a 1d KB\n            features = tf.reshape(features, (self.batchSize, -1, outDim))\n\n        return features  \n\n    # Embed question using parametrized word embeddings.\n    # The embedding are initialized to the values supported to the class initialization\n    def qEmbeddingsOp(self, qIndices, embInit):\n        with tf.variable_scope(""qEmbeddings""):\n            # if config.useCPU:\n            #     with tf.device(\'/cpu:0\'):\n            #         embeddingsVar = tf.Variable(self.embeddingsInit, name = ""embeddings"", dtype = tf.float32)\n            # else:\n            #     embeddingsVar = tf.Variable(self.embeddingsInit, name = ""embeddings"", dtype = tf.float32)\n            embeddingsVar = tf.get_variable(""emb"", initializer = tf.to_float(embInit), \n                dtype = tf.float32, trainable = (not config.wrdEmbFixed))\n            embeddings = tf.concat([tf.zeros((1, config.wrdEmbDim)), embeddingsVar], axis = 0)\n            questions = tf.nn.embedding_lookup(embeddings, qIndices)\n\n        return questions, embeddings\n\n    # Embed answer words\n    def aEmbeddingsOp(self, embInit):\n        with tf.variable_scope(""aEmbeddings""):\n            if embInit is None:\n                return None\n            answerEmbeddings = tf.get_variable(""emb"", initializer = tf.to_float(embInit), \n                dtype = tf.float32)\n        return answerEmbeddings\n\n    # Embed question and answer words with tied embeddings\n    def qaEmbeddingsOp(self, qIndices, embInit):\n        questions, qaEmbeddings = self.qEmbeddingsOp(qIndices, embInit[""qa""])\n        aEmbeddings = tf.nn.embedding_lookup(qaEmbeddings, embInit[""ansMap""])\n\n        return questions, qaEmbeddings, aEmbeddings \n\n    \'\'\'\n    Embed question (and optionally answer) using parametrized word embeddings.\n    The embedding are initialized to the values supported to the class initialization \n    \'\'\'\n    def embeddingsOp(self, qIndices, embInit):\n        if config.ansEmbMod == ""SHARED"":\n            questions, qEmb, aEmb = self.qaEmbeddingsOp(qIndices, embInit)                                \n        else:\n            questions, qEmb = self.qEmbeddingsOp(qIndices, embInit[""q""])\n            aEmb = self.aEmbeddingsOp(embInit[""a""])\n\n        return questions, qEmb, aEmb\n\n    \'\'\'\n    The Question Input Unit embeds the questions to randomly-initialized word vectors,\n    and runs a recurrent bidirectional encoder (RNN/LSTM etc.) that gives back\n    vector representations for each question (the RNN final hidden state), and\n    representations for each of the question words (the RNN outputs for each word). \n\n    The method uses bidirectional LSTM, by default.\n    Optionally projects the outputs of the LSTM (with linear projection / \n    optionally with some activation).\n    \n    Args:\n        questions: question word embeddings  \n        [batchSize, questionLength, wordEmbDim]\n\n        questionLengths: the question lengths.\n        [batchSize]\n\n        projWords: True to apply projection on RNN outputs.\n        projQuestion: True to apply projection on final RNN state.\n        projDim: projection dimension in case projection is applied.  \n\n    Returns:\n        Contextual Words: RNN outputs for the words.\n        [batchSize, questionLength, ctrlDim]\n\n        Vectorized Question: Final hidden state representing the whole question.\n        [batchSize, ctrlDim]\n    \'\'\'\n    def encoder(self, questions, questionLengths, projWords = False, \n        projQuestion = False, projDim = None):\n        \n        with tf.variable_scope(""encoder""):\n            # variational dropout option\n            varDp = None\n            if config.encVariationalDropout:\n                varDp = {""stateDp"": self.dropouts[""stateInput""], \n                         ""inputDp"": self.dropouts[""encInput""], \n                         ""inputSize"": config.wrdEmbDim}\n\n            # rnns\n            for i in range(config.encNumLayers):\n                questionCntxWords, vecQuestions = ops.RNNLayer(questions, questionLengths, \n                    config.encDim, bi = config.encBi, cellType = config.encType, \n                    dropout = self.dropouts[""encInput""], varDp = varDp, name = ""rnn%d"" % i)\n\n            # dropout for the question vector\n            vecQuestions = tf.nn.dropout(vecQuestions, self.dropouts[""question""])\n            \n            # projection of encoder outputs \n            if projWords:\n                questionCntxWords = ops.linear(questionCntxWords, config.encDim, projDim, \n                    name = ""projCW"")\n            if projQuestion:\n                vecQuestions = ops.linear(vecQuestions, config.encDim, projDim, \n                    act = config.encProjQAct, name = ""projQ"")\n\n        return questionCntxWords, vecQuestions        \n\n    \'\'\'\n    Stacked Attention Layer for baseline. Computes interaction between images\n    and the previous memory, and casts it back to compute attention over the \n    image, which in turn is summed up with the previous memory to result in the\n    new one. \n\n    Args:\n        images: input image.\n        [batchSize, H * W, inDim]\n\n        memory: previous memory value\n        [batchSize, inDim]\n\n        inDim: inputs dimension\n        hDim: hidden dimension to compute interactions between image and memory\n\n    Returns the new memory value.\n    \'\'\'\n    def baselineAttLayer(self, images, memory, inDim, hDim, name = """", reuse = None):\n        with tf.variable_scope(""attLayer"" + name, reuse = reuse):         \n            # projImages = ops.linear(images, inDim, hDim, name = ""projImage"")\n            # projMemory = tf.expand_dims(ops.linear(memory, inDim, hDim, name = ""projMemory""), axis = -2)       \n            # if config.saMultiplicative:\n            #     interactions = projImages * projMemory\n            # else:\n            #     interactions = tf.tanh(projImages + projMemory) \n            interactions, _ = ops.mul(images, memory, inDim, proj = {""dim"": hDim, ""shared"": False}, \n                interMod = config.baselineAttType)\n            \n            attention = ops.inter2att(interactions, hDim)\n            summary = ops.att2Smry(attention, images)            \n            newMemory = memory + summary\n        \n        return newMemory\n\n    \'\'\'\n    Baseline approach:\n    If baselineAtt is True, applies several layers (baselineAttNumLayers)\n    of stacked attention to image and memory, when memory is initialized\n    to the vector questions. See baselineAttLayer for further details.\n\n    Otherwise, computes result output features based on image representation\n    (baselineCNN), or question (baselineLSTM) or both.\n\n    Args:\n        vecQuestions: question vector representation\n        [batchSize, questionDim]\n\n        questionDim: dimension of question vectors\n\n        images: (flattened) image representation\n        [batchSize, imageDim]\n\n        imageDim: dimension of image representations.\n        \n        hDim: hidden dimension to compute interactions between image and memory\n        (for attention-based baseline).\n\n    Returns final features to use in later classifier.\n    [batchSize, outDim] (out dimension depends on baseline method)\n    \'\'\'\n    def baseline(self, vecQuestions, questionDim, images, imageDim, hDim):\n        with tf.variable_scope(""baseline""):\n            if config.baselineAtt:  \n                memory = self.linear(vecQuestions, questionDim, hDim, name = ""qProj"")\n                images = self.linear(images, imageDim, hDim, name = ""iProj"")\n\n                for i in range(config.baselineAttNumLayers):\n                    memory = self.baselineAttLayer(images, memory, hDim, hDim, \n                        name = ""baseline%d"" % i)\n                memDim = hDim\n            else:      \n                images, imagesDim = ops.linearizeFeatures(images, self.H, self.W, \n                    imageDim, projDim = config.baselineProjDim)\n                if config.baselineLSTM and config.baselineCNN:\n                    memory = tf.concat([vecQuestions, images], axis = -1)\n                    memDim = questionDim + imageDim\n                elif config.baselineLSTM:\n                    memory = vecQuestions\n                    memDim = questionDim\n                else: # config.baselineCNN\n                    memory = images\n                    memDim = imageDim \n                \n        return memory, memDim\n\n    \'\'\'\n    Runs the MAC recurrent network to perform the reasoning process.\n    Initializes a MAC cell and runs netLength iterations.\n    \n    Currently it passes the question and knowledge base to the cell during\n    its creating, such that it doesn\'t need to interact with it through \n    inputs / outputs while running. The recurrent computation happens \n    by working iteratively over the hidden (control, memory) states.  \n    \n    Args:\n        images: flattened image features. Used as the ""Knowledge Base"".\n        (Received by default model behavior from the Image Input Units).\n        [batchSize, H * W, memDim]\n\n        vecQuestions: vector questions representations.\n        (Received by default model behavior from the Question Input Units\n        as the final RNN state).\n        [batchSize, ctrlDim]\n\n        questionWords: question word embeddings.\n        [batchSize, questionLength, ctrlDim]\n\n        questionCntxWords: question contextual words.\n        (Received by default model behavior from the Question Input Units\n        as the series of RNN output states).\n        [batchSize, questionLength, ctrlDim]\n\n        questionLengths: question lengths.\n        [batchSize]\n\n    Returns the final control state and memory state resulted from the network.\n    ([batchSize, ctrlDim], [bathSize, memDim])\n    \'\'\'\n    def MACnetwork(self, images, vecQuestions, questionWords, questionCntxWords, \n        questionLengths, name = """", reuse = None):\n\n        with tf.variable_scope(""MACnetwork"" + name, reuse = reuse):\n            \n            self.macCell = MACCell(\n                vecQuestions = vecQuestions,\n                questionWords = questionWords,\n                questionCntxWords = questionCntxWords, \n                questionLengths = questionLengths,\n                knowledgeBase = images,\n                memoryDropout = self.dropouts[""memory""],\n                readDropout = self.dropouts[""read""],\n                writeDropout = self.dropouts[""write""],\n                # qDropoutMAC = self.qDropoutMAC,\n                batchSize = self.batchSize,\n                train = self.train,\n                reuse = reuse)           \n\n            state = self.macCell.zero_state(self.batchSize, tf.float32)\n\n            # inSeq = tf.unstack(inSeq, axis = 1)        \n            none = tf.zeros((self.batchSize, 1), dtype = tf.float32)\n\n            # for i, inp in enumerate(inSeq):\n            for i in range(config.netLength):\n                self.macCell.iteration = i\n                # if config.unsharedCells:\n                    # with tf.variable_scope(""iteration%d"" % i):\n                    # macCell.myNameScope = ""iteration%d"" % i\n                _, state = self.macCell(none, state)                     \n                # else:\n                    # _, state = macCell(none, state)\n                    # macCell.reuse = True\n\n            # self.autoEncMMLoss = macCell.autoEncMMLossI\n            # inputSeqL = None\n            # _, lastOutputs = tf.nn.dynamic_rnn(macCell, inputSeq, # / static\n            #     sequence_length = inputSeqL, \n            #     initial_state = initialState, \n            #     swap_memory = True)           \n\n            # self.postModules = None\n            # if (config.controlPostRNN or config.selfAttentionMod == ""POST""): # may not work well with dlogits\n            #     self.postModules, _ = self.RNNLayer(cLogits, None, config.encDim, bi = False, \n            #         name = ""decPostRNN"", cellType = config.controlPostRNNmod)\n            #     if config.controlPostRNN:\n            #         logits = self.postModules\n            #     self.postModules = tf.unstack(self.postModules, axis = 1)\n\n            # self.autoEncCtrlLoss = tf.constant(0.0)\n            # if config.autoEncCtrl:\n            #     autoEncCtrlCellType = (""GRU"" if config.autoEncCtrlGRU else ""RNN"")\n            #     autoEncCtrlinp = logits\n            #     _, autoEncHid = self.RNNLayer(autoEncCtrlinp, None, config.encDim, \n            #       bi = True, name = ""autoEncCtrl"", cellType = autoEncCtrlCellType)\n            #     self.autoEncCtrlLoss = (tf.nn.l2_loss(vecQuestions - autoEncHid)) / tf.to_float(self.batchSize)\n\n            finalControl = state.control\n            finalMemory = state.memory\n\n        return finalControl, finalMemory         \n\n    \'\'\'\n    Output Unit (step 1): chooses the inputs to the output classifier.\n    \n    By default the classifier input will be the the final memory state of the MAC network.\n    If outQuestion is True, concatenate the question representation to that.\n    If outImage is True, concatenate the image flattened representation.\n\n    Args:\n        memory: (final) memory state of the MAC network.\n        [batchSize, memDim]\n\n        vecQuestions: question vector representation.\n        [batchSize, ctrlDim]\n\n        images: image features.\n        [batchSize, H, W, imageInDim]\n\n        imageInDim: images dimension.\n\n    Returns the resulted features and their dimension. \n    \'\'\'  \n    def outputOp(self, memory, vecQuestions, images, imageInDim):\n        with tf.variable_scope(""outputUnit""):            \n            features = memory\n            dim = config.memDim\n\n            if config.outQuestion:\n                eVecQuestions = ops.linear(vecQuestions, config.ctrlDim, config.memDim, name = ""outQuestion"") \n                features, dim = ops.concat(features, eVecQuestions, config.memDim, mul = config.outQuestionMul)\n            \n            if config.outImage:\n                images, imagesDim = ops.linearizeFeatures(images, self.H, self.W, self.imageInDim, \n                    outputDim = config.outImageDim)\n                images = ops.linear(images, config.memDim, config.outImageDim, name = ""outImage"")\n                features = tf.concat([features, images], axis = -1)\n                dim += config.outImageDim\n\n        return features, dim        \n\n    \'\'\'\n    Output Unit (step 2): Computes the logits for the answers. Passes the features\n    through fully-connected network to get the logits over the possible answers.\n    Optionally uses answer word embeddings in computing the logits (by default, it doesn\'t).\n\n    Args:\n        features: features used to compute logits\n        [batchSize, inDim]\n\n        inDim: features dimension\n\n        aEmbedding: supported word embeddings for answer words in case answerMod is not NON.\n        Optionally computes logits by computing dot-product with answer embeddings.\n    \n    Returns: the computed logits.\n    [batchSize, answerWordsNum]\n    \'\'\'\n    def classifier(self, features, inDim, aEmbeddings = None):\n        with tf.variable_scope(""classifier""):                    \n            outDim = config.answerWordsNum\n            dims = [inDim] + config.outClassifierDims + [outDim]\n            if config.answerMod != ""NON"":\n                dims[-1] = config.wrdEmbDim                \n\n\n            logits = ops.FCLayer(features, dims, \n                batchNorm = self.batchNorm if config.outputBN else None, \n                dropout = self.dropouts[""output""]) \n            \n            if config.answerMod != ""NON"":\n                logits = tf.nn.dropout(logits, self.dropouts[""output""])\n                interactions = ops.mul(aEmbeddings, logits, dims[-1], interMod = config.answerMod)\n                logits = ops.inter2logits(interactions, dims[-1], sumMod = ""SUM"")\n                logits += ops.getBias((outputDim, ), ""ans"")\n\n                # answersWeights = tf.transpose(aEmbeddings)\n\n                # if config.answerMod == ""BL"":\n                #     Wans = ops.getWeight((dims[-1], config.wrdEmbDim), ""ans"")\n                #     logits = tf.matmul(logits, Wans)\n                # elif config.answerMod == ""DIAG"":\n                #     Wans = ops.getWeight((config.wrdEmbDim, ), ""ans"")\n                #     logits = logits * Wans\n                \n                # logits = tf.matmul(logits, answersWeights) \n\n        return logits\n\n    # def getTemp():\n    #     with tf.variable_scope(""temperature""):\n    #         if config.tempParametric:\n    #             self.temperatureVar = tf.get_variable(""temperature"", shape = (), \n    #                 initializer = tf.constant_initializer(5), dtype = tf.float32)\n    #             temperature = tf.sigmoid(self.temperatureVar)\n    #         else:\n    #             temperature = config.temperature\n            \n    #         if config.tempDynamic:\n    #             temperature *= self.tempAnnealRate\n\n    #     return temperature \n\n    # Computes mean cross entropy loss between logits and answers.\n    def addAnswerLossOp(self, logits, answers):\n        with tf.variable_scope(""answerLoss""):\n            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = answers, logits = logits)\n            loss = tf.reduce_mean(losses)\n            self.answerLossList.append(loss)\n\n        return loss, losses\n\n    # Computes predictions (by finding maximal logit value, corresponding to highest probability)\n    # and mean accuracy between predictions and answers. \n    def addPredOp(self, logits, answers):\n        with tf.variable_scope(""pred""):\n            preds = tf.to_int32(tf.argmax(logits, axis = -1)) # tf.nn.softmax( \n            corrects = tf.equal(preds, answers) \n            correctNum = tf.reduce_sum(tf.to_int32(corrects))\n            acc = tf.reduce_mean(tf.to_float(corrects))\n            self.correctNumList.append(correctNum) \n            self.answerAccList.append(acc)\n\n        return preds, corrects, correctNum\n\n    # Creates optimizer (adam)\n    def addOptimizerOp(self): \n        with tf.variable_scope(""trainAddOptimizer""):            \n            self.globalStep = tf.Variable(0, dtype = tf.int32, trainable = False, name = ""globalStep"") # init to 0 every run?\n            optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n\n        return optimizer\n\n    \'\'\'\n    Computes gradients for all variables or subset of them, based on provided loss, \n    using optimizer.\n    \'\'\'\n    def computeGradients(self, optimizer, loss, trainableVars = None): # tf.trainable_variables()\n        with tf.variable_scope(""computeGradients""):            \n            if config.trainSubset:\n                trainableVars = []\n                allVars = tf.trainable_variables()\n                for var in allVars:\n                    if any((s in var.name) for s in config.varSubset):\n                        trainableVars.append(var)\n\n            gradients_vars = optimizer.compute_gradients(loss, trainableVars) \n        return gradients_vars\n\n    \'\'\'\n    Apply gradients. Optionally clip them, and update exponential moving averages \n    for parameters.\n    \'\'\'\n    def addTrainingOp(self, optimizer, gradients_vars):\n        with tf.variable_scope(""train""):\n            gradients, variables = zip(*gradients_vars)\n            norm = tf.global_norm(gradients)\n\n            # gradient clipping\n            if config.clipGradients:            \n                clippedGradients, _ = tf.clip_by_global_norm(gradients, config.gradMaxNorm, use_norm = norm)\n                gradients_vars = zip(clippedGradients, variables)\n\n            # updates ops (for batch norm) and train op\n            updateOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(updateOps):\n                train = optimizer.apply_gradients(gradients_vars, global_step = self.globalStep)\n\n            # exponential moving average\n            if config.useEMA:\n                ema = tf.train.ExponentialMovingAverage(decay = config.emaDecayRate)\n                maintainAveragesOp = ema.apply(tf.trainable_variables())\n\n                with tf.control_dependencies([train]):\n                    trainAndUpdateOp = tf.group(maintainAveragesOp)\n                \n                train = trainAndUpdateOp\n\n                self.emaDict = ema.variables_to_restore()\n\n        return train, norm\n\n    # TODO (add back support for multi-gpu..)\n    def averageAcrossTowers(self, gpusNum):\n        self.lossAll = self.lossList[0]\n\n        self.answerLossAll = self.answerLossList[0]\n        self.correctNumAll = self.correctNumList[0]\n        self.answerAccAll = self.answerAccList[0]\n        self.predsAll = self.predsList[0]\n        self.gradientVarsAll = self.gradientVarsList[0]\n\n    def trim2DVectors(self, vectors, vectorsLengths):\n        maxLength = np.max(vectorsLengths)\n        return vectors[:,:maxLength]\n\n    def trimData(self, data):\n        data[""questions""] = self.trim2DVectors(data[""questions""], data[""questionLengths""])\n        return data\n\n    \'\'\'\n    Builds predictions JSON, by adding the model\'s predictions and attention maps \n    back to the original data JSON.\n    \'\'\'\n    def buildPredsList(self, data, predictions, attentionMaps):\n        predsList = []\n        \n        for i, instance in enumerate(data[""instances""]):\n\n            if predictions is not None:\n                pred = self.answerDict.decodeId(predictions[i])\n                instance[""prediction""] = pred          \n\n            # aggregate np attentions of instance i in the batch into 2d list\n            attMapToList = lambda attMap: [step[i].tolist() for step in attMap]\n            if attentionMaps is not None:\n                attentions = {k: attMapToList(attentionMaps[k]) for k in attentionMaps}\n                instance[""attentions""] = attentions\n\n            predsList.append(instance)\n\n        return predsList\n\n    \'\'\'\n    Processes a batch of data with the model.\n\n    Args:\n        sess: TF session\n        \n        data: Data batch. Dictionary that contains numpy array for:\n        questions, questionLengths, answers. \n        See preprocess.py for further information of the batch structure.\n\n        images: batch of image features, as numpy array. images[""images""] contains\n        [batchSize, channels, h, w]\n\n        train: True to run batch for training.\n\n        getAtt: True to return attention maps for question and image (and optionally \n        self-attention and gate values).\n\n    Returns results: e.g. loss, accuracy, running time.\n    \'\'\'\n    def runBatch(self, sess, data, images, train, getAtt = False):     \n        data = self.trimData(data)\n\n        trainOp = self.trainOp if train else self.noOp\n        gradNormOp = self.gradNorm if train else self.noOp\n\n        predsOp = (self.predsAll, self.correctNumAll, self.answerAccAll)\n        \n        attOp = self.macCell.attentions\n        \n        time0 = time.time()\n        feed = self.createFeedDict(data, images, train) \n\n        time1 = time.time()\n        _, loss, predsInfo, gradNorm, attentionMaps = sess.run(\n            [trainOp, self.lossAll, predsOp, gradNormOp, attOp], \n            feed_dict = feed)\n        \n        time2 = time.time()  \n\n        predsList = self.buildPredsList(data, predsInfo[0], attentionMaps if getAtt else None)\n\n        return {""loss"": loss,\n                ""correctNum"": predsInfo[1],\n                ""acc"": predsInfo[2], \n                ""preds"": predsList,\n                ""gradNorm"": gradNorm if train else -1,\n                ""readTime"": time1 - time0,\n                ""trainTime"": time2 - time1}\n\n    def build(self):\n        self.addPlaceholders()\n        self.optimizer = self.addOptimizerOp()\n\n        self.gradientVarsList = []\n        self.lossList = []\n\n        self.answerLossList = []\n        self.correctNumList = []\n        self.answerAccList = []\n        self.predsList = []\n\n        with tf.variable_scope(""macModel""):\n            for i in range(config.gpusNum):\n                with tf.device(""/gpu:{}"".format(i)):\n                    with tf.name_scope(""tower{}"".format(i)) as scope:\n                        self.initTowerBatch(i, config.gpusNum, self.batchSizeAll)\n\n                        self.loss = tf.constant(0.0)\n\n                        # embed questions words (and optionally answer words)\n                        questionWords, qEmbeddings, aEmbeddings = \\\n                            self.embeddingsOp(self.questionsIndices, self.embeddingsInit)\n\n                        projWords = projQuestion = ((config.encDim != config.ctrlDim) or config.encProj)\n                        questionCntxWords, vecQuestions = self.encoder(questionWords, \n                            self.questionLengths, projWords, projQuestion, config.ctrlDim)\n\n                        # Image Input Unit (stem)\n                        imageFeatures = self.stem(self.images, self.imageInDim, config.memDim)\n\n                        # baseline model\n                        if config.useBaseline:\n                            output, dim = self.baseline(vecQuestions, config.ctrlDim, \n                                self.images, self.imageInDim, config.attDim)\n                        # MAC model\n                        else:      \n                            # self.temperature = self.getTemp()\n                            \n                            finalControl, finalMemory = self.MACnetwork(imageFeatures, vecQuestions, \n                                questionWords, questionCntxWords, self.questionLengths)\n                            \n                            # Output Unit - step 1 (preparing classifier inputs)\n                            output, dim = self.outputOp(finalMemory, vecQuestions, \n                                self.images, self.imageInDim)\n\n                        # Output Unit - step 2 (classifier)\n                        logits = self.classifier(output, dim, aEmbeddings)\n\n                        # compute loss, predictions, accuracy\n                        answerLoss, self.losses = self.addAnswerLossOp(logits, self.answersIndices)\n                        self.preds, self.corrects, self.correctNum = self.addPredOp(logits, self.answersIndices)\n                        self.loss += answerLoss\n                        self.predsList.append(self.preds)\n\n                        self.lossList.append(self.loss)\n\n                        # compute gradients\n                        gradient_vars = self.computeGradients(self.optimizer, self.loss, trainableVars = None)\n                        self.gradientVarsList.append(gradient_vars)\n\n                        # reuse variables in next towers\n                        tf.get_variable_scope().reuse_variables()\n\n        self.averageAcrossTowers(config.gpusNum)\n\n        self.trainOp, self.gradNorm = self.addTrainingOp(self.optimizer, self.gradientVarsAll)\n        self.noOp = tf.no_op()\n'"
ops.py,163,"b'from __future__ import division\nimport math\nimport tensorflow as tf\n\nfrom mi_gru_cell import MiGRUCell\nfrom mi_lstm_cell import MiLSTMCell\nfrom config import config\n\neps = 1e-20\ninf = 1e30\n\n####################################### variables ########################################\n\n\'\'\'\nInitializes a weight matrix variable given a shape and a name. \nUses random_normal initialization if 1d, otherwise uses xavier. \n\'\'\'\ndef getWeight(shape, name = """"):\n    with tf.variable_scope(""weights""):               \n        initializer = tf.contrib.layers.xavier_initializer()\n        # if len(shape) == 1: # good?\n        #     initializer = tf.random_normal_initializer()        \n        W = tf.get_variable(""weight"" + name, shape = shape, initializer = initializer)\n    return W\n\n\'\'\'\nInitializes a weight matrix variable given a shape and a name. Uses xavier\n\'\'\'\ndef getKernel(shape, name = """"):\n    with tf.variable_scope(""kernels""):               \n        initializer = tf.contrib.layers.xavier_initializer()\n        W = tf.get_variable(""kernel"" + name, shape = shape, initializer = initializer)\n    return W\n\n\'\'\'\nInitializes a bias variable given a shape and a name.\n\'\'\'\ndef getBias(shape, name = """"):\n    with tf.variable_scope(""biases""):              \n        initializer = tf.zeros_initializer()\n        b = tf.get_variable(""bias"" + name, shape = shape, initializer = initializer)\n    return b\n\n######################################### basics #########################################\n\n\'\'\'\nMultiplies input inp of any depth by a 2d weight matrix.  \n\'\'\'\n# switch with conv 1?\ndef multiply(inp, W):\n    inDim = tf.shape(W)[0]\n    outDim = tf.shape(W)[1] \n    newDims = tf.concat([tf.shape(inp)[:-1], tf.fill((1,), outDim)], axis = 0)\n    \n    inp = tf.reshape(inp, (-1, inDim))\n    output = tf.matmul(inp, W)\n    output = tf.reshape(output, newDims)\n\n    return output\n\n\'\'\'\nConcatenates x and y. Support broadcasting. \nOptionally concatenate multiplication of x * y\n\'\'\'\ndef concat(x, y, dim, mul = False, extendY = False):\n    if extendY:\n        y = tf.expand_dims(y, axis = -2)\n        # broadcasting to have the same shape\n        y = tf.zeros_like(x) + y\n\n    if mul:\n        out = tf.concat([x, y, x * y], axis = -1)\n        dim *= 3\n    else:\n        out = tf.concat([x, y], axis = -1)\n        dim *= 2\n    \n    return out, dim\n\n\'\'\'\nAdds L2 regularization for weight and kernel variables.\n\'\'\'\n# add l2 in the tf way\ndef L2RegularizationOp(l2 = None):\n    if l2 is None:\n        l2 = config.l2\n    l2Loss = 0\n    names = [""weight"", ""kernel""]\n    for var in tf.trainable_variables():\n        if any((name in var.name.lower()) for name in names):\n            l2Loss += tf.nn.l2_loss(var)\n    return l2 * l2Loss\n\n######################################### attention #########################################\n\n\'\'\'\nTransform vectors to scalar logits.\n\nArgs:\n    interactions: input vectors\n    [batchSize, N, dim]\n\n    dim: dimension of input vectors\n\n    sumMod: LIN for linear transformation to scalars.\n            SUM to sum up vectors entries to get scalar logit.\n\n    dropout: dropout value over inputs (for linear case)\n\nReturn matching scalar for each interaction.\n[batchSize, N]\n\'\'\'\nsumMod = [""LIN"", ""SUM""]\ndef inter2logits(interactions, dim, sumMod = ""LIN"", dropout = 1.0, name = """", reuse = None):\n    with tf.variable_scope(""inter2logits"" + name, reuse = reuse): \n        if sumMod == ""SUM"":\n            logits = tf.reduce_sum(interactions, axis = -1)\n        else: # ""LIN""\n            logits = linear(interactions, dim, 1, dropout = dropout, name = ""logits"")\n    return logits\n\n\'\'\'\nTransforms vectors to probability distribution. \nCalls inter2logits and then softmax over these.\n\nArgs:\n    interactions: input vectors\n    [batchSize, N, dim]\n\n    dim: dimension of input vectors\n\n    sumMod: LIN for linear transformation to scalars.\n            SUM to sum up vectors entries to get scalar logit.\n\n    dropout: dropout value over inputs (for linear case)\n\nReturn attention distribution over interactions.\n[batchSize, N]\n\'\'\'\ndef inter2att(interactions, dim, dropout = 1.0, name = """", reuse = None):\n    with tf.variable_scope(""inter2att"" + name, reuse = reuse): \n        logits = inter2logits(interactions, dim, dropout = dropout)\n        attention = tf.nn.softmax(logits)    \n    return attention\n\n\'\'\'\nSums up features using attention distribution to get a weighted average over them. \n\'\'\'\ndef att2Smry(attention, features):\n    return tf.reduce_sum(tf.expand_dims(attention, axis = -1) * features, axis = -2)\n\n####################################### activations ########################################\n\n\'\'\'\nPerforms a variant of ReLU based on config.relu\n    PRM for PReLU\n    ELU for ELU\n    LKY for Leaky ReLU\n    otherwise, standard ReLU\n\'\'\'\ndef relu(inp):                  \n    if config.relu == ""PRM"":\n        with tf.variable_scope(None, default_name = ""prelu""):\n            alpha = tf.get_variable(""alpha"", shape = inp.get_shape()[-1], \n                initializer = tf.constant_initializer(0.25))\n            pos = tf.nn.relu(inp)\n            neg = - (alpha * tf.nn.relu(-inp))\n            output = pos + neg\n    elif config.relu == ""ELU"":\n        output = tf.nn.elu(inp)\n    # elif config.relu == ""SELU"":\n    #     output = tf.nn.selu(inp) \n    elif config.relu == ""LKY"":\n        # output = tf.nn.leaky_relu(inp, config.reluAlpha)\n        output = tf.maximum(inp, config.reluAlpha * inp)\n    elif config.relu == ""STD"": # STD\n        output = tf.nn.relu(inp)\n    \n    return output\n\nactivations = {\n    ""NON"":      tf.identity, # lambda inp: inp    \n    ""TANH"":     tf.tanh,\n    ""SIGMOID"":  tf.sigmoid,\n    ""RELU"":     relu,\n    ""ELU"":      tf.nn.elu\n}    \n\n# Sample from Gumbel(0, 1)\ndef sampleGumbel(shape): \n    U = tf.random_uniform(shape, minval = 0, maxval = 1)\n    return -tf.log(-tf.log(U + eps) + eps)\n\n# Draw a sample from the Gumbel-Softmax distribution\ndef gumbelSoftmaxSample(logits, temperature): \n    y = logits + sampleGumbel(tf.shape(logits))\n    return tf.nn.softmax(y / temperature)\n\ndef gumbelSoftmax(logits, temperature, train): # hard = False\n    # Sample from the Gumbel-Softmax distribution and optionally discretize.\n    # Args:\n    #    logits: [batch_size, n_class] unnormalized log-probs\n    #    temperature: non-negative scalar\n    #    hard: if True, take argmax, but differentiate w.r.t. soft sample y\n    # Returns:\n    #    [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n    #    If hard=True, then the returned sample will be one-hot, otherwise it will\n    #    be a probabilitiy distribution that sums to 1 across classes\n\n    y = gumbelSoftmaxSample(logits, temperature)\n\n    # k = tf.shape(logits)[-1]\n    # yHard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)\n    yHard = tf.cast(tf.equal(y, tf.reduce_max(y, 1, keep_dims = True)), y.dtype)\n    yNew = tf.stop_gradient(yHard - y) + y\n\n    if config.gumbelSoftmaxBoth:\n        return y\n    if config.gumbelArgmaxBoth:\n        return yNew\n    ret = tf.cond(train, lambda: y, lambda: yNew)\n    \n    return ret \n\ndef softmaxDiscrete(logits, temperature, train):\n    if config.gumbelSoftmax:\n        return gumbelSoftmax(logits, temperature = temperature, train = train)\n    else:\n        return tf.nn.softmax(logits)\n\ndef parametricDropout(name, train):\n    var = tf.get_variable(""varDp"" + name, shape = (), initializer = tf.constant_initializer(2), \n        dtype = tf.float32)\n    dropout = tf.cond(train, lambda: tf.sigmoid(var), lambda: 1.0)\n    return dropout\n\n###################################### sequence helpers ######################################\n\n\'\'\'\nCasts exponential mask over a sequence with sequence length.\nUsed to prepare logits before softmax.\n\'\'\'\ndef expMask(seq, seqLength):\n    maxLength = tf.shape(seq)[-1]\n    mask = (1 - tf.cast(tf.sequence_mask(seqLength, maxLength), tf.float32)) * (-inf)\n    masked = seq + mask\n    return masked\n\n\'\'\'\nComputes seq2seq loss between logits and target sequences, with given lengths.\n\'\'\'\ndef seq2SeqLoss(logits, targets, lengths):\n    mask = tf.sequence_mask(lengths, maxlen = tf.shape(targets)[1])\n    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.to_float(mask))\n    return loss\n\n\'\'\'\nComputes seq2seq loss between logits and target sequences, with given lengths.\n    acc1: accuracy per symbol \n    acc2: accuracy per sequence\n\'\'\'\ndef seq2seqAcc(preds, targets, lengths):\n    mask = tf.sequence_mask(lengths, maxlen = tf.shape(targets)[1])\n    corrects = tf.logical_and(tf.equal(preds, targets), mask)\n    numCorrects = tf.reduce_sum(tf.to_int32(corrects), axis = 1)\n    \n    acc1 = tf.to_float(numCorrects) / (tf.to_float(lengths) + eps) # add small eps instead?\n    acc1 = tf.reduce_mean(acc1)  \n    \n    acc2 = tf.to_float(tf.equal(numCorrects, lengths))\n    acc2 = tf.reduce_mean(acc2)      \n\n    return acc1, acc2\n\n########################################### linear ###########################################\n\n\'\'\'\nlinear transformation.\n\nArgs:\n    inp: input to transform\n    inDim: input dimension\n    outDim: output dimension\n    dropout: dropout over input\n    batchNorm: if not None, applies batch normalization to inputs\n    addBias: True to add bias\n    bias: initial bias value\n    act: if not None, activation to use after linear transformation\n    actLayer: if True and act is not None, applies another linear transformation on top of previous\n    actDropout: dropout to apply in the optional second linear transformation\n    retVars: if True, return parameters (weight and bias) \n\nReturns linear transformation result.\n\'\'\'\n# batchNorm = {""decay"": float, ""train"": Tensor}\n# actLayer: if activation is not non, stack another linear layer\n# maybe change naming scheme such that if name = """" than use it as default_name (-->unique?)\ndef linear(inp, inDim, outDim, dropout = 1.0, \n    batchNorm = None, addBias = True, bias = 0.0,\n    act = ""NON"", actLayer = True, actDropout = 1.0, \n    retVars = False, name = """", reuse = None):\n    \n    with tf.variable_scope(""linearLayer"" + name, reuse = reuse):        \n        W = getWeight((inDim, outDim) if outDim > 1 else (inDim, ))\n        b = getBias((outDim, ) if outDim > 1 else ()) + bias\n        \n        if batchNorm is not None:\n            inp = tf.contrib.layers.batch_norm(inp, decay = batchNorm[""decay""], \n                center = True, scale = True, is_training = batchNorm[""train""], updates_collections = None)\n            # tf.layers.batch_normalization, axis -1 ?\n\n        inp = tf.nn.dropout(inp, dropout)                \n        \n        if outDim > 1:\n            output = multiply(inp, W)\n        else:\n            output = tf.reduce_sum(inp * W, axis = -1)\n        \n        if addBias:\n            output += b\n\n        output = activations[act](output)\n\n        # good?\n        if act != ""NON"" and actLayer:\n            output = linear(output, outDim, outDim, dropout = actDropout, batchNorm = batchNorm,  \n                addBias = addBias, act = ""NON"", actLayer = False, \n                name = name + ""_2"", reuse = reuse)\n\n    if retVars:\n        return (output, (W, b))\n\n    return output\n\n\'\'\'\nComputes Multi-layer feed-forward network.\n\nArgs:\n    features: input features\n    dims: list with dimensions of network. \n          First dimension is of the inputs, final is of the outputs.\n    batchNorm: if not None, applies batchNorm\n    dropout: dropout value to apply for each layer\n    act: activation to apply between layers.\n    NON, TANH, SIGMOID, RELU, ELU\n\'\'\'\n# no activation after last layer\n# batchNorm = {""decay"": float, ""train"": Tensor}\ndef FCLayer(features, dims, batchNorm = None, dropout = 1.0, act = ""RELU""):\n    layersNum = len(dims) - 1\n    \n    for i in range(layersNum):\n        features = linear(features, dims[i], dims[i+1], name = ""fc_%d"" % i, \n            batchNorm = batchNorm, dropout = dropout)\n        # not the last layer\n        if i < layersNum - 1: \n            features = activations[act](features)\n    \n    return features   \n\n###################################### cnns ######################################\n\n\'\'\'\nComputes convolution.\n\nArgs:\n    inp: input features\n    inDim: input dimension\n    outDim: output dimension\n    batchNorm: if not None, applies batchNorm on inputs\n    dropout: dropout value to apply on inputs\n    addBias: True to add bias\n    kernelSize: kernel size\n    stride: stride size\n    act: activation to apply on outputs\n    NON, TANH, SIGMOID, RELU, ELU\n\'\'\'\n# batchNorm = {""decay"": float, ""train"": Tensor, ""center"": bool, ""scale"": bool}\n# collections.namedtuple(""batchNorm"", (""decay"", ""train""))\ndef cnn(inp, inDim, outDim, batchNorm = None, dropout = 1.0, addBias = True, \n    kernelSize = None, stride = 1, act = ""NON"", name = """", reuse = None):\n    \n    with tf.variable_scope(""cnnLayer"" + name, reuse = reuse):\n        \n        if kernelSize is None:\n            kernelSize = config.stemKernelSize            \n        kernelH = kernelW = kernelSize\n        \n        kernel = getKernel((kernelH, kernelW, inDim, outDim))\n        b = getBias((outDim, ))\n        \n        if batchNorm is not None:\n            inp = tf.contrib.layers.batch_norm(inp, decay = batchNorm[""decay""], center = batchNorm[""center""], \n                scale = batchNorm[""scale""], is_training = batchNorm[""train""], updates_collections = None)   \n\n        inp = tf.nn.dropout(inp, dropout)                \n        \n        output = tf.nn.conv2d(inp, filter = kernel, strides = [1, stride, stride, 1], padding = ""SAME"")\n        \n        if addBias:\n            output += b\n\n        output = activations[act](output)\n\n    return output\n\n\'\'\'\nComputes Multi-layer convolutional network.\n\nArgs:\n    features: input features\n    dims: list with dimensions of network. \n          First dimension is of the inputs. Final is of the outputs.\n    batchNorm: if not None, applies batchNorm\n    dropout: dropout value to apply for each layer\n    kernelSizes: list of kernel sizes for each layer. Default to config.stemKernelSize\n    strides: list of strides for each layer. Default to 1.\n    act: activation to apply between layers.\n    NON, TANH, SIGMOID, RELU, ELU\n\'\'\'\n# batchNorm = {""decay"": float, ""train"": Tensor, ""center"": bool, ""scale"": bool}\n# activation after last layer\ndef CNNLayer(features, dims, batchNorm = None, dropout = 1.0, \n    kernelSizes = None, strides = None, act = ""RELU""):\n    \n    layersNum = len(dims) - 1\n    \n    if kernelSizes is None:\n        kernelSizes = [config.stemKernelSize for i in range(layersNum)]\n    \n    if strides is None:\n        strides = [1 for i in range(layersNum)]\n    \n    for i in range(layersNum):\n        features = cnn(features, dims[i], dims[i+1], name = ""cnn_%d"" % i, batchNorm = batchNorm, \n            dropout = dropout, kernelSize = kernelSizes[i], stride = strides[i], act = act)\n\n    return features\n\n######################################## location ########################################\n\n\'\'\'\nComputes linear positional encoding for h x w grid. \nIf outDim positive, casts positions to that dimension.\n\'\'\'\n# ignores dim\n# h,w can be tensor scalars\ndef locationL(h, w, dim, outDim = -1, addBias = True):\n    dim = 2\n    grid = tf.stack(tf.meshgrid(tf.linspace(-config.locationBias, config.locationBias, w), \n                                tf.linspace(-config.locationBias, config.locationBias, h)), axis = -1)\n\n    if outDim > 0:\n        grid = linear(grid, dim, outDim, addBias = addBias, name = ""locationL"")\n        dim = outDim\n\n    return grid, dim\n\n\'\'\'\nComputes sin/cos positional encoding for h x w x (4*dim). \nIf outDim positive, casts positions to that dimension.\nBased on positional encoding presented in ""Attention is all you need""\n\'\'\'\n# dim % 4 = 0\n# h,w can be tensor scalars\ndef locationPE(h, w, dim, outDim = -1, addBias = True):    \n    x = tf.expand_dims(tf.to_float(tf.linspace(-config.locationBias, config.locationBias, w)), axis = -1)\n    y = tf.expand_dims(tf.to_float(tf.linspace(-config.locationBias, config.locationBias, h)), axis = -1)\n    i = tf.expand_dims(tf.to_float(tf.range(dim)), axis = 0)\n\n    peSinX = tf.sin(x / (tf.pow(10000.0, i / dim)))\n    peCosX = tf.cos(x / (tf.pow(10000.0, i / dim)))\n    peSinY = tf.sin(y / (tf.pow(10000.0, i / dim)))\n    peCosY = tf.cos(y / (tf.pow(10000.0, i / dim)))\n\n    peSinX = tf.tile(tf.expand_dims(peSinX, axis = 0), [h, 1, 1])\n    peCosX = tf.tile(tf.expand_dims(peCosX, axis = 0), [h, 1, 1])\n    peSinY = tf.tile(tf.expand_dims(peSinY, axis = 1), [1, w, 1])\n    peCosY = tf.tile(tf.expand_dims(peCosY, axis = 1), [1, w, 1]) \n\n    grid = tf.concat([peSinX, peCosX, peSinY, peCosY], axis = -1)\n    dim *= 4\n    \n    if outDim > 0:\n        grid = linear(grid, dim, outDim, addBias = addBias, name = ""locationPE"")\n        dim = outDim\n\n    return grid, dim\n\nlocations = {\n    ""L"": locationL,\n    ""PE"": locationPE\n}\n\n\'\'\'\nAdds positional encoding to features. May ease spatial reasoning.\n(although not used in the default model). \n\nArgs:\n    features: features to add position encoding to.\n    [batchSize, h, w, c]\n\n    inDim: number of features\' channels\n    lDim: dimension for positional encodings\n    outDim: if positive, cast enhanced features (with positions) to that dimension\n    h: features\' height\n    w: features\' width\n    locType: L for linear encoding, PE for cos/sin based positional encoding\n    mod: way to add positional encoding: concatenation (CNCT), addition (ADD), \n            multiplication (MUL), linear transformation (LIN).\n\'\'\'\nmods = [""CNCT"", ""ADD"", ""LIN"", ""MUL""]\n# if outDim = -1, then will be set based on inDim, lDim\ndef addLocation(features, inDim, lDim, outDim = -1, h = None, w = None, \n    locType = ""L"", mod = ""CNCT"", name = """", reuse = None): # h,w not needed\n    \n    with tf.variable_scope(""addLocation"" + name, reuse = reuse):\n        batchSize = tf.shape(features)[0]\n        if h is None:\n            h = tf.shape(features)[1]\n        if w is None:\n            w = tf.shape(features)[2]\n        dim = inDim\n\n        if mod == ""LIN"":\n            if outDim < 0:\n                outDim = dim\n\n            grid, _ = locations[locType](h, w, lDim, outDim = outDim, addBias = False)\n            features = linear(features, dim, outDim, name = ""LIN"")\n            features += grid  \n            return features, outDim\n\n        if mod == ""CNCT"":\n            grid, lDim = locations[locType](h, w, lDim)\n            # grid = tf.zeros_like(features) + grid\n            grid = tf.tile(tf.expand_dims(grid, axis = 0), [batchSize, 1, 1, 1])\n            features = tf.concat([features, grid], axis = -1)\n            dim += lDim\n\n        elif mod == ""ADD"":\n            grid, _ = locations[locType](h, w, lDim, outDim = dim)\n            features += grid    \n        \n        elif mod == ""MUL"": # MUL\n            grid, _ = locations[locType](h, w, lDim, outDim = dim)\n\n            if outDim < 0:\n                outDim = dim\n\n            grid = tf.tile(tf.expand_dims(grid, axis = 0), [batchSize, 1, 1, 1])\n            features = tf.concat([features, grid, features * grid], axis = -1)\n            dim *= 3                \n\n        if outDim > 0:\n            features = linear(features, dim, outDim)\n            dim = outDim \n\n    return features, dim\n\n# config.locationAwareEnd\n# H, W, _ = config.imageDims\n# projDim = config.stemProjDim\n# k = config.stemProjPooling\n# projDim on inDim or on out\n# inDim = tf.shape(features)[3]\n\n\'\'\'\nLinearize 2d image to linear vector.\n\nArgs:\n    features: batch of 2d images. \n    [batchSize, h, w, inDim]\n\n    h: image height\n\n    w: image width\n\n    inDim: number of channels\n\n    projDim: if not None, project image to that dimension before linearization\n\n    outDim: if not None, project image to that dimension after linearization\n\n    loc: if not None, add positional encoding:\n        locType: L for linear encoding, PE for cos/sin based positional encoding\n        mod: way to add positional encoding: concatenation (CNCT), addition (ADD), \n            multiplication (MUL), linear transformation (LIN).\n        pooling: number to pool image with before linearization.\n\nReturns linearized image:\n[batchSize, outDim] (or [batchSize, (h / pooling) * (w /pooling) * projDim] if outDim not supported) \n\'\'\'\n# loc = {""locType"": str, ""mod"": str}\ndef linearizeFeatures(features, h, w, inDim, projDim = None, outDim = None, \n    loc = None, pooling = None):\n    \n    if pooling is None:\n        pooling = config.imageLinPool\n    \n    if loc is not None:\n        features = addLocation(features, inDim, lDim = inDim, outDim = inDim, \n            locType = loc[""locType""], mod = loc[""mod""])\n\n    if projDim is not None:\n        features = linear(features, dim, projDim)\n        features = relu(features)\n        dim = projDim\n\n    if pooling > 1:\n        poolingDims = [1, pooling, pooling, 1]\n        features = tf.nn.max_pool(features, ksize = poolingDims, strides = poolingDims, \n            padding = ""SAME"")\n        h /= pooling\n        w /= pooling\n  \n    dim = h * w * dim  \n    features = tf.reshape(features, (-1, dim))\n    \n    if outDim is not None:\n        features = linear(features, dim, outDim)\n        dim = outDim\n\n    return features, dim\n\n################################### multiplication ###################################\n# specific dim / proj for x / y\n\'\'\'\n""Enhanced"" hadamard product between x and y:\n1. Supports optional projection of x, and y prior to multiplication.\n2. Computes simple multiplication, or a parametrized one, using diagonal of complete matrix (bi-linear) \n3. Optionally concatenate x or y or their projection to the multiplication result.\n\nSupport broadcasting\n\nArgs:\n    x: left-hand side argument\n    [batchSize, dim]\n\n    y: right-hand side argument\n    [batchSize, dim]\n\n    dim: input dimension of x and y\n    \n    dropout: dropout value to apply on x and y\n\n    proj: if not None, project x and y:\n        dim: projection dimension\n        shared: use same projection for x and y\n        dropout: dropout to apply to x and y if projected\n\n    interMod: multiplication type:\n        ""MUL"": x * y\n        ""DIAG"": x * W * y for a learned diagonal parameter W\n        ""BL"": x\' W y for a learned matrix W\n\n    concat: if not None, concatenate x or y or their projection. \n    \n    mulBias: optional bias to stabilize multiplication (x * bias) (y * bias)\n\nReturns the multiplication result\n[batchSize, outDim] when outDim depends on the use of proj and cocnat arguments.\n\'\'\'\n# proj = {""dim"": int, ""shared"": bool, ""dropout"": float} # ""act"": str, ""actDropout"": float\n## interMod = [""direct"", ""scalarW"", ""bilinear""] # ""additive""\n# interMod = [""MUL"", ""DIAG"", ""BL"", ""ADD""]\n# concat = {""x"": bool, ""y"": bool, ""proj"": bool}\ndef mul(x, y, dim, dropout = 1.0, proj = None, interMod = ""MUL"", concat = None, mulBias = None,\n    extendY = True, name = """", reuse = None):\n    \n    with tf.variable_scope(""mul"" + name, reuse = reuse):                \n        origVals = {""x"": x, ""y"": y, ""dim"": dim}\n\n        x = tf.nn.dropout(x, dropout)\n        y = tf.nn.dropout(y, dropout)\n        # projection\n        if proj is not None:\n            x = tf.nn.dropout(x, proj.get(""dropout"", 1.0))\n            y = tf.nn.dropout(y, proj.get(""dropout"", 1.0))\n\n            if proj[""shared""]:\n                xName, xReuse = ""proj"", None\n                yName, yReuse = ""proj"", True\n            else:\n                xName, xReuse = ""projX"", None\n                yName, yReuse = ""projY"", None\n\n            x = linear(x, dim, proj[""dim""], name = xName, reuse = xReuse)\n            y = linear(y, dim, proj[""dim""], name = yName, reuse = yReuse)\n            dim = proj[""dim""]\n            projVals = {""x"": x, ""y"": y, ""dim"": dim}\n            proj[""x""], proj[""y""] = x, y\n\n        if extendY:\n            y = tf.expand_dims(y, axis = -2)\n            # broadcasting to have the same shape\n            y = tf.zeros_like(x) + y\n\n        # multiplication\n        if interMod == ""MUL"":\n            if mulBias is None:\n                mulBias = config.mulBias             \n            output = (x + mulBias) * (y + mulBias)\n        elif interMod == ""DIAG"":\n            W = getWeight((dim, )) # change initialization?\n            b = getBias((dim, ))\n            activations = x * W * y + b\n        elif interMod == ""BL"":\n            W = getWeight((dim, dim))\n            b = getBias((dim, ))            \n            output = multiply(x, W) * y + b\n        else: # ""ADD""\n            output = tf.tanh(x + y)\n        # concatenation\n        if concat is not None:\n            concatVals = projVals if concat.get(""proj"", False) else origVals\n            if concat.get(""x"", False):\n                output = tf.concat([output, concatVals[""x""]], axis = -1)\n                dim += concatVals[""dim""]\n\n            if concat.get(""y"", False):\n                output = ops.concat(output, concatVals[""y""], extendY = extendY)\n                dim += concatVals[""dim""]\n\n    return output, dim\n\n######################################## rnns ########################################\n\n\'\'\'\nCreates an RNN cell.\n\nArgs:\n    hdim: the hidden dimension of the RNN cell.\n    \n    reuse: whether the cell should reuse parameters or create new ones.\n    \n    cellType: the cell type \n    RNN, GRU, LSTM, MiGRU, MiLSTM, ProjLSTM\n\n    act: the cell activation\n    NON, TANH, SIGMOID, RELU, ELU\n\n    projDim: if ProjLSTM, the dimension for the states projection\n\nReturns the cell.\n\'\'\'\n# tf.nn.rnn_cell.MultiRNNCell([cell(hDim, reuse = reuse) for _ in config.encNumLayers])\n# note that config.enc params not general \ndef createCell(hDim, reuse, cellType = None, act = None, projDim = None):\n    if cellType is None:\n        cellType = config.encType\n\n    activation = activations.get(act, None) \n\n    if cellType == ""ProjLSTM"":\n        cell = tf.nn.rnn_cell.LSTMCell\n        if projDim is None:\n            projDim = config.cellDim\n        cell = cell(hDim, num_proj = projDim, reuse = reuse, activation = activation)\n        return cell        \n\n    cells = {\n        ""RNN"": tf.nn.rnn_cell.BasicRNNCell,\n        ""GRU"": tf.nn.rnn_cell.GRUCell,\n        ""LSTM"": tf.nn.rnn_cell.BasicLSTMCell,\n        ""MiGRU"": MiGRUCell,\n        ""MiLSTM"": MiLSTMCell\n    }\n\n    cell = cells[cellType](hDim, reuse = reuse, activation = activation)\n\n    return cell\n\n\'\'\'\nRuns an forward RNN layer.\n\nArgs:\n    inSeq: the input sequence to run the RNN over.\n    [batchSize, sequenceLength, inDim]\n    \n    seqL: the sequence matching lengths.\n    [batchSize, 1]\n\n    hDim: hidden dimension of the RNN.\n\n    cellType: the cell type \n    RNN, GRU, LSTM, MiGRU, MiLSTM, ProjLSTM\n\n    dropout: value for dropout over input sequence\n\n    varDp: if not None, state and input variational dropouts to apply.\n    dimension of input has to be supported (inputSize). \n\nReturns the outputs sequence and final RNN state.  \n\'\'\'\n# varDp = {""stateDp"": float, ""inputDp"": float, ""inputSize"": int}\n# proj = {""output"": bool, ""state"": bool, ""dim"": int, ""dropout"": float, ""act"": str}\ndef fwRNNLayer(inSeq, seqL, hDim, cellType = None, dropout = 1.0, varDp = None, \n    name = """", reuse = None): # proj = None\n    \n    with tf.variable_scope(""rnnLayer"" + name, reuse = reuse):\n        batchSize = tf.shape(inSeq)[0]\n\n        cell = createCell(hDim, reuse, cellType) # passing reuse isn\'t mandatory\n\n        if varDp is not None:\n            cell = tf.contrib.rnn.DropoutWrapper(cell, \n                state_keep_prob = varDp[""stateDp""],\n                input_keep_prob = varDp[""inputDp""],\n                variational_recurrent = True, input_size = varDp[""inputSize""], dtype = tf.float32)\n        else:           \n            inSeq = tf.nn.dropout(inSeq, dropout)\n        \n        initialState = cell.zero_state(batchSize, tf.float32)\n\n        outSeq, lastState = tf.nn.dynamic_rnn(cell, inSeq, \n            sequence_length = seqL, \n            initial_state = initialState,\n            swap_memory = True)\n            \n        if isinstance(lastState, tf.nn.rnn_cell.LSTMStateTuple):\n            lastState = lastState.h\n\n        # if proj is not None:\n        #     if proj[""output""]:\n        #         outSeq = linear(outSeq, cell.output_size, proj[""dim""], act = proj[""act""],  \n        #             dropout = proj[""dropout""], name = ""projOutput"")\n\n        #     if proj[""state""]:\n        #         lastState = linear(lastState, cell.state_size, proj[""dim""], act = proj[""act""],  \n        #             dropout = proj[""dropout""], name = ""projState"")\n\n    return outSeq, lastState\n\n\'\'\'\nRuns an bidirectional RNN layer.\n\nArgs:\n    inSeq: the input sequence to run the RNN over.\n    [batchSize, sequenceLength, inDim]\n    \n    seqL: the sequence matching lengths.\n    [batchSize, 1]\n\n    hDim: hidden dimension of the RNN.\n\n    cellType: the cell type \n    RNN, GRU, LSTM, MiGRU, MiLSTM\n\n    dropout: value for dropout over input sequence\n\n    varDp: if not None, state and input variational dropouts to apply.\n    dimension of input has to be supported (inputSize).   \n\nReturns the outputs sequence and final RNN state.     \n\'\'\'\n# varDp = {""stateDp"": float, ""inputDp"": float, ""inputSize"": int}\n# proj = {""output"": bool, ""state"": bool, ""dim"": int, ""dropout"": float, ""act"": str}\ndef biRNNLayer(inSeq, seqL, hDim, cellType = None, dropout = 1.0, varDp = None, \n    name = """", reuse = None): # proj = None, \n\n    with tf.variable_scope(""birnnLayer"" + name, reuse = reuse):\n        batchSize = tf.shape(inSeq)[0]\n\n        with tf.variable_scope(""fw""):\n            cellFw = createCell(hDim, reuse, cellType)\n        with tf.variable_scope(""bw""):\n            cellBw = createCell(hDim, reuse, cellType)\n        \n        if varDp is not None:\n            cellFw = tf.contrib.rnn.DropoutWrapper(cellFw, \n                state_keep_prob = varDp[""stateDp""],\n                input_keep_prob = varDp[""inputDp""],\n                variational_recurrent = True, input_size = varDp[""inputSize""], dtype = tf.float32)\n            \n            cellBw = tf.contrib.rnn.DropoutWrapper(cellBw, \n                state_keep_prob = varDp[""stateDp""],\n                input_keep_prob = varDp[""inputDp""],\n                variational_recurrent = True, input_size = varDp[""inputSize""], dtype = tf.float32)            \n        else:\n            inSeq = tf.nn.dropout(inSeq, dropout)\n\n        initialStateFw = cellFw.zero_state(batchSize, tf.float32)\n        initialStateBw = cellBw.zero_state(batchSize, tf.float32)\n\n        (outSeqFw, outSeqBw), (lastStateFw, lastStateBw) = tf.nn.bidirectional_dynamic_rnn(\n            cellFw, cellBw, inSeq, \n            sequence_length = seqL, \n            initial_state_fw = initialStateFw, \n            initial_state_bw = initialStateBw,\n            swap_memory = True)\n\n        if isinstance(lastStateFw, tf.nn.rnn_cell.LSTMStateTuple):\n            lastStateFw = lastStateFw.h # take c? \n            lastStateBw = lastStateBw.h  \n\n        outSeq = tf.concat([outSeqFw, outSeqBw], axis = -1)\n        lastState = tf.concat([lastStateFw, lastStateBw], axis = -1)\n\n        # if proj is not None:\n        #     if proj[""output""]:\n        #         outSeq = linear(outSeq, cellFw.output_size + cellFw.output_size, \n        #             proj[""dim""], act = proj[""act""], dropout = proj[""dropout""], \n        #             name = ""projOutput"")\n\n        #     if proj[""state""]:\n        #         lastState = linear(lastState, cellFw.state_size + cellFw.state_size, \n        #             proj[""dim""], act = proj[""act""], dropout = proj[""dropout""], \n        #             name = ""projState"")\n\n    return outSeq, lastState\n\n# int(hDim / 2) for biRNN?\n\'\'\'\nRuns an RNN layer by calling biRNN or fwRNN.\n\nArgs:\n    inSeq: the input sequence to run the RNN over.\n    [batchSize, sequenceLength, inDim]\n    \n    seqL: the sequence matching lengths.\n    [batchSize, 1]\n\n    hDim: hidden dimension of the RNN.\n\n    bi: true to run bidirectional rnn.\n\n    cellType: the cell type \n    RNN, GRU, LSTM, MiGRU, MiLSTM\n\n    dropout: value for dropout over input sequence\n\n    varDp: if not None, state and input variational dropouts to apply.\n    dimension of input has to be supported (inputSize).   \n\nReturns the outputs sequence and final RNN state.     \n\'\'\'\n# proj = {""output"": bool, ""state"": bool, ""dim"": int, ""dropout"": float, ""act"": str}\n# varDp = {""stateDp"": float, ""inputDp"": float, ""inputSize"": int}\ndef RNNLayer(inSeq, seqL, hDim, bi = None, cellType = None, dropout = 1.0, varDp = None, \n    name = """", reuse = None): # proj = None\n    \n    with tf.variable_scope(""rnnLayer"" + name, reuse = reuse):\n        if bi is None:\n            bi = config.encBi\n        \n        rnn = biRNNLayer if bi else fwRNNLayer\n        \n        if bi:\n            hDim = int(hDim / 2)\n\n    return rnn(inSeq, seqL, hDim, cellType = cellType, dropout = dropout, varDp = varDp) # , proj = proj\n\n# tf counterpart?\n# hDim = config.moduleDim\ndef multigridRNNLayer(featrues, h, w, dim, name = """", reuse = None):\n    with tf.variable_scope(""multigridRNNLayer"" + name, reuse = reuse):\n        featrues = linear(featrues, dim, dim / 2, name = ""i"")\n\n        output0 = gridRNNLayer(featrues, h, w, dim, right = True, down = True, name = ""rd"")\n        output1 = gridRNNLayer(featrues, h, w, dim, right = True, down = False, name = ""r"")\n        output2 = gridRNNLayer(featrues, h, w, dim, right = False, down = True, name = ""d"")\n        output3 = gridRNNLayer(featrues, h, w, dim, right = False, down = False, name = ""NON"")\n\n        output = tf.concat([output0, output1, output2, output3], axis = -1)\n        output = linear(output, 2 * dim, dim, name = ""o"")\n\n    return outputs\n\n# h,w should be constants\ndef gridRNNLayer(features, h, w, dim, right, down, name = """", reuse = None):\n    with tf.variable_scope(""gridRNNLayer"" + name):\n        batchSize = tf.shape(features)[0]\n\n        cell = createCell(dim, reuse = reuse, cellType = config.stemGridRnnMod, \n            act = config.stemGridAct)\n        \n        initialState = cell.zero_state(batchSize, tf.float32)\n        \n        inputs = [tf.unstack(row, w, axis = 1) for row in tf.unstack(features, h, axis = 1)]\n        states = [[None for _ in range(w)] for _ in range(h)]\n\n        iAxis = range(h) if down else (range(h)[::-1])\n        jAxis = range(w) if right else (range(w)[::-1])\n\n        iPrev = -1 if down else 1\n        jPrev = -1 if right else 1\n\n        prevState = lambda i,j: states[i][j] if (i >= 0 and i < h and j >= 0 and j < w) else initialState\n        \n        for i in iAxis:\n            for j in jAxis:\n                prevs = tf.concat((prevState(i + iPrev, j), prevState(i, j + jPrev)), axis = -1)\n                curr = inputs[i][j]\n                _, states[i][j] = cell(prevs, curr)\n\n        outputs = [tf.stack(row, axis = 1) for row in states]\n        outputs = tf.stack(outputs, axis = 1)\n\n    return outputs\n\n# tf seq2seq?\n# def projRNNLayer(inSeq, seqL, hDim, labels, labelsNum, labelsDim, labelsEmb, name = """", reuse = None):\n#     with tf.variable_scope(""projRNNLayer"" + name):\n#         batchSize = tf.shape(features)[0]\n\n#         cell = createCell(hDim, reuse = reuse)\n\n#         projCell = ProjWrapper(cell, labelsNum, labelsDim, labelsEmb, # config.wrdEmbDim\n#             feedPrev = True, dropout = 1.0, config,\n#             temperature = 1.0, sample = False, reuse)\n        \n#         initialState = projCell.zero_state(batchSize, tf.float32)\n        \n#         if config.soft:\n#             inSeq = inSeq\n\n#             # outputs, _ = tf.nn.static_rnn(projCell, inputs, \n#             #     sequence_length = seqL, \n#             #     initial_state = initialState)\n\n#             inSeq = tf.unstack(inSeq, axis = 1)                        \n#             state = initialState\n#             logitsList = []\n#             chosenList = []\n\n#             for inp in inSeq:\n#                 (logits, chosen), state = projCell(inp, state)\n#                 logitsList.append(logits)\n#                 chosenList.append(chosen)\n#                 projCell.reuse = True\n\n#             logitsOut = tf.stack(logitsList, axis = 1)\n#             chosenOut = tf.stack(chosenList, axis = 1)\n#             outputs = (logitsOut, chosenOut)\n#         else:\n#             labels = tf.to_float(labels)\n#             labels = tf.concat([tf.zeros((batchSize, 1)), labels], axis = 1)[:, :-1] # ,newaxis\n#             inSeq = tf.concat([inSeq, tf.expand_dims(labels, axis = -1)], axis = -1)\n\n#             outputs, _ = tf.nn.dynamic_rnn(projCell, inSeq, \n#                 sequence_length = seqL, \n#                 initial_state = initialState,\n#                 swap_memory = True)\n\n#     return outputs #, labelsEmb\n\n############################### variational dropout ###############################\n\n\'\'\'\nGenerates a variational dropout mask for a given shape and a dropout \nprobability value.\n\'\'\'\ndef generateVarDpMask(shape, keepProb):\n    randomTensor = tf.to_float(keepProb)\n    randomTensor += tf.random_uniform(shape, minval = 0, maxval = 1)\n    binaryTensor = tf.floor(randomTensor)\n    mask = tf.to_float(binaryTensor)\n    return mask\n\n\'\'\'\nApplies the a variational dropout over an input, given dropout mask \nand a dropout probability value. \n\'\'\'\ndef applyVarDpMask(inp, mask, keepProb):\n    ret = (tf.div(inp, tf.to_float(keepProb))) * mask\n    return ret   \n'"
preprocess.py,0,"b'import time\nimport os\nimport random\nimport json\nimport pickle\nimport numpy as np\nfrom tqdm import tqdm\nfrom termcolor import colored\nfrom program_translator import ProgramTranslator #\nfrom config import config\n\n# Print bold tex\ndef bold(txt):\n    return colored(str(txt),attrs = [""bold""])\n\n# Print bold and colored text \ndef bcolored(txt, color):\n    return colored(str(txt), color, attrs = [""bold""])\n\n# Write a line to file\ndef writeline(f, line):\n    f.write(str(line) + ""\\n"")\n\n# Write a list to file\ndef writelist(f, l):\n    writeline(f, "","".join(map(str, l)))\n\n# 2d list to numpy\ndef vectorize2DList(items, minX = 0, minY = 0, dtype = np.int):\n    maxX = max(len(items), minX)\n    maxY = max([len(item) for item in items] + [minY])\n    t = np.zeros((maxX, maxY), dtype = dtype)\n    tLengths = np.zeros((maxX, ), dtype = np.int)\n    for i, item in enumerate(items):\n        t[i, 0:len(item)] = np.array(item, dtype = dtype)\n        tLengths[i] = len(item)\n    return t, tLengths\n\n# 3d list to numpy\ndef vectorize3DList(items, minX = 0, minY = 0, minZ = 0, dtype = np.int):\n    maxX = max(len(items), minX)\n    maxY = max([len(item) for item in items] + [minY])\n    maxZ = max([len(subitem) for item in items for subitem in item] + [minZ])\n    t = np.zeros((maxX, maxY, maxZ), dtype = dtype)\n    tLengths = np.zeros((maxX, maxY), dtype = np.int)\n    for i, item in enumerate(items):\n        for j, subitem in enumerate(item):\n            t[i, j, 0:len(subitem)] = np.array(subitem, dtype = dtype)\n            tLengths[i, j] = len(subitem)\n    return t, tLengths\n\n\'\'\'\nEncodes text into integers. Keeps dictionary between string words (symbols) \nand their matching integers. Supports encoding and decoding.\n\'\'\'\nclass SymbolDict(object):\n    def __init__(self, empty = False):\n        self.padding = ""<PAD>""\n        self.unknown = ""<UNK>""\n        self.start = ""<START>""\n        self.end = ""<END>""\n\n        self.invalidSymbols = [self.padding, self.unknown, self.start, self.end]\n\n        if empty:\n            self.sym2id = {} \n            self.id2sym = []            \n        else:\n            self.sym2id = {self.padding: 0, self.unknown: 1, self.start: 2, self.end: 3} \n            self.id2sym = [self.padding, self.unknown, self.start, self.end]\n        self.allSeqs = []\n\n    def getNumSymbols(self):\n        return len(self.sym2id)\n\n    def isPadding(self, enc):\n        return enc == 0\n\n    def isUnknown(self, enc):\n        return enc == 1\n\n    def isStart(self, enc):\n        return enc == 2\n\n    def isEnd(self, enc):\n        return enc == 3\n\n    def isValid(self, enc):\n        return enc < self.getNumSymbols() and enc >= len(self.invalidSymbols)\n\n    def resetSeqs(self):\n        self.allSeqs = []\n\n    def addSeq(self, seq):\n        self.allSeqs += seq\n\n    # Call to create the words-to-integers vocabulary after (reading word sequences with addSeq).\n    def createVocab(self, minCount = 0):\n        counter = {}\n        for symbol in self.allSeqs:\n            counter[symbol] = counter.get(symbol, 0) + 1\n        for symbol in counter:\n            if counter[symbol] > minCount and (symbol not in self.sym2id):\n                self.sym2id[symbol] = self.getNumSymbols()\n                self.id2sym.append(symbol)\n\n    # Encodes a symbol. Returns the matching integer.\n    def encodeSym(self, symbol):\n        if symbol not in self.sym2id:\n            symbol = self.unknown\n        return self.sym2id[symbol]\n\n    \'\'\'\n    Encodes a sequence of symbols.\n    Optionally add start, or end symbols. \n    Optionally reverse sequence \n    \'\'\'\n    def encodeSequence(self, decoded, addStart = False, addEnd = False, reverse = False):\n        if reverse:\n            decoded.reverse()\n        if addStart:\n            decoded = [self.start] + decoded\n        if addEnd:\n            decoded = decoded + [self.end]\n        encoded = [self.encodeSym(symbol) for symbol in decoded]\n        return encoded\n\n    # Decodes an integer into its symbol \n    def decodeId(self, enc):\n        return self.id2sym[enc] if enc < self.getNumSymbols() else self.unknown\n\n    \'\'\'\n    Decodes a sequence of integers into their symbols.\n    If delim is given, joins the symbols using delim,\n    Optionally reverse the resulted sequence \n    \'\'\'\n    def decodeSequence(self, encoded, delim = None, reverse = False, stopAtInvalid = True):\n        length = 0\n        for i in range(len(encoded)):\n            if not self.isValid(encoded[i]) and stopAtInvalid:\n                break\n            length += 1\n        encoded = encoded[:length]\n\n        decoded = [self.decodeId(enc) for enc in encoded]\n        if reverse:\n            decoded.reverse()\n\n        if delim is not None:\n            return delim.join(decoded)\n        \n        return decoded\n\n\'\'\'\nPreprocesses a given dataset into numpy arrays.\nBy calling preprocess, the class:\n1. Reads the input data files into dictionary. \n2. Saves the results jsons in files and loads them instead of parsing input if files exist/\n3. Initializes word embeddings to random / GloVe.\n4. Optionally filters data according to given filters.\n5. Encodes and vectorize the data into numpy arrays.\n6. Buckets the data according to the instances length.\n\'\'\'\nclass Preprocesser(object):\n    def __init__(self):\n        self.questionDict = SymbolDict()\n        self.answerDict = SymbolDict(empty = True)\n        self.qaDict = SymbolDict()\n\n        self.specificDatasetDicts = None\n\n        self.programDict = SymbolDict()\n        self.programTranslator = ProgramTranslator(self.programDict, 2)\n    \'\'\'\n    Tokenizes string into list of symbols.\n\n    Args:\n        text: raw string to tokenize.\n        ignorePuncts: punctuation to ignore\n        keptPunct: punctuation to keep (as symbol)\n        endPunct: punctuation to remove if appears at the end\n        delim: delimiter between symbols\n        clean: True to replace text in string\n        replacelistPre: dictionary of replacement to perform on the text before tokanization\n        replacelistPost: dictionary of replacement to perform on the text after tokanization\n    \'\'\'\n    # sentence tokenizer\n    allPunct = [""?"", ""!"", ""\\\\"", ""/"", "")"", ""("", ""."", "","", "";"", "":""]\n    def tokenize(self, text, ignoredPuncts = [""?"", ""!"", ""\\\\"", ""/"", "")"", ""(""], \n        keptPuncts = [""."", "","", "";"", "":""], endPunct = ["">"", ""<"", "":""], delim = "" "", \n        clean = False, replacelistPre = dict(), replacelistPost = dict()):\n\n        if clean:\n            for word in replacelistPre:\n                origText = text\n                text = text.replace(word, replacelistPre[word])\n                if (origText != text):\n                    print(origText)\n                    print(text)\n                    print("""")\n\n            for punct in endPunct:\n                if text[-1] == punct:\n                    print(text)\n                    text = text[:-1]\n                    print(text)\n                    print("""")\n\n        for punct in keptPuncts:\n            text = text.replace(punct, delim + punct + delim)           \n        \n        for punct in ignoredPuncts:\n            text = text.replace(punct, """")\n\n        ret = text.lower().split(delim)\n\n        if clean:\n            origRet = ret\n            ret = [replacelistPost.get(word, word) for word in ret]\n            if origRet != ret:\n                print(origRet)\n                print(ret)\n\n        ret = [t for t in ret if t != """"]\n        return ret\n\n\n    # Read class\' generated files.\n    # files interface\n    def readFiles(self, instancesFilename):\n        with open(instancesFilename, ""r"") as inFile:\n            instances = json.load(inFile)\n        \n        with open(config.questionDictFile(), ""rb"") as inFile:\n            self.questionDict = pickle.load(inFile)\n\n        with open(config.answerDictFile(), ""rb"") as inFile:\n            self.answerDict = pickle.load(inFile)\n\n        with open(config.qaDictFile(), ""rb"") as inFile:\n            self.qaDict = pickle.load(inFile)\n\n        return instances \n    \n    \'\'\'\n    Generate class\' files. Save json representation of instances and\n    symbols-to-integers dictionaries.  \n    \'\'\'\n    def writeFiles(self, instances, instancesFilename):\n        with open(instancesFilename, ""w"") as outFile:\n            json.dump(instances, outFile)\n\n        with open(config.questionDictFile(), ""wb"") as outFile:\n            pickle.dump(self.questionDict, outFile)\n\n        with open(config.answerDictFile(), ""wb"") as outFile:\n            pickle.dump(self.answerDict, outFile)\n\n        with open(config.qaDictFile(), ""wb"") as outFile:\n            pickle.dump(self.qaDict, outFile)\n\n    # Write prediction json to file and optionally a one-answer-per-line output file\n    def writePreds(self, res, tier, suffix = """"):\n        if res is None:\n            return\n        preds = res[""preds""]\n        sortedPreds = sorted(preds, key = lambda instance: instance[""index""]) \n        with open(config.predsFile(tier + suffix), ""w"") as outFile:\n            outFile.write(json.dumps(sortedPreds))\n        with open(config.answersFile(tier + suffix), ""w"") as outFile:\n            for instance in sortedPreds:\n                writeline(outFile, instance[""prediction""])\n    \n    # Reads NLVR data entries and create a json dictionary.\n    def readNLVR(self, datasetFilename, instancesFilename, train):\n        instances = []\n        i = 0 \n\n        if os.path.exists(instancesFilename):\n            instances = self.readFiles(instancesFilename)\n        else:\n            with open(datasetFilename, ""r"") as datasetFile:               \n                for line in datasetFile:\n                    instance = json.loads(line)\n                    question = instance[""sentence""]\n                    questionSeq = self.tokenize(question, \n                        ignoredPuncts = Preprocesser.allPunct, keptPuncts = [])\n\n                    if train or (not config.wrdEmbUnknown):\n                        self.questionDict.addSeq(question)\n                        self.qaDict.addSeq(question)\n\n                    answer = instance[""label""]\n                    self.answerDict.addSeq([answer])\n                    self.qaDict.addSeq([answer])\n\n                    for k in range(6):\n                        instances.append({\n                            ""question"": question,\n                            ""questionSeq"": questionSeq,\n                            ""answer"": answer,\n                            ""imageId"": instance[""identifier""] + ""-"" + str(k),\n                            ""index"": i\n                            })\n                        i += 1\n\n                random.shuffle(instances)\n\n                self.questionDict.createVocab()\n                self.answerDict.createVocab()\n                self.qaDict.createVocab()\n\n                self.writeFiles(instances, instancesFilename)\n\n        return instances\n\n    # Reads CLEVR data entries and create a json dictionary.\n    def readCLEVR(self, datasetFilename, instancesFilename, train):\n        instances = []\n\n        if os.path.exists(instancesFilename):\n            instances = self.readFiles(instancesFilename)\n        else:\n            with open(datasetFilename, ""r"") as datasetFile:\n                data = json.load(datasetFile)[""questions""]            \n            for i in tqdm(range(len(data)), desc = ""Preprocessing""):\n                instance = data[i]\n\n                question = instance[""question""]\n                questionSeq = self.tokenize(question)\n\n                if train or (not config.wrdEmbUnknown):\n                    self.questionDict.addSeq(questionSeq)\n                    self.qaDict.addSeq(questionSeq)\n\n                answer = instance.get(""answer"", ""yes"") # DUMMY_ANSWER\n                self.answerDict.addSeq([answer])\n                self.qaDict.addSeq([answer])\n\n                dummyProgram = [{""function"": ""FUNC"", ""value_inputs"": [], ""inputs"": []}]\n                program = instance.get(""program"", dummyProgram)\n                postfixProgram = self.programTranslator.programToPostfixProgram(program)\n                programSeq = self.programTranslator.programToSeq(postfixProgram)\n                programInputs = self.programTranslator.programToInputs(postfixProgram, \n                    offset = 2)\n\n                # pass other fields to instance?\n                instances.append({\n                        ""question"": question,\n                        ""questionSeq"": questionSeq,\n                        ""answer"": answer,\n                        ""imageId"": instance[""image_index""],\n                        ""program"": program,\n                        ""programSeq"": programSeq,\n                        ""programInputs"": programInputs,\n                        ""index"": i\n                        })\n\n            random.shuffle(instances)\n\n            self.questionDict.createVocab()\n            self.answerDict.createVocab()\n            self.qaDict.createVocab()\n\n            self.writeFiles(instances, instancesFilename)\n\n        return instances\n\n    \'\'\'\n    Reads data in datasetFilename, and creates json dictionary.\n    If instancesFilename exists, restore dictionary from this file.\n    Otherwise, save created dictionary to instancesFilename.\n    \'\'\'\n    def readData(self, datasetFilename, instancesFilename, train):\n        # data extraction\n        datasetReader = {\n            ""CLEVR"": self.readCLEVR,\n            ""NLVR"": self.readNLVR\n        }\n\n        return datasetReader[config.dataset](datasetFilename, instancesFilename, train)\n\n    # Reads dataset tier (train, val, test) and returns the loaded instances \n    # and image relevant filenames\n    def readTier(self, tier, train):\n        imagesFilename = config.imagesFile(tier)\n        datasetFilename = config.datasetFile(tier)\n        instancesFilename = config.instancesFile(tier)\n        \n        instances = self.readData(datasetFilename, instancesFilename, train)\n\n        images = {""imagesFilename"": imagesFilename}\n        if config.dataset == ""NLVR"":\n            images[""imageIdsFilename""] = config.imagesIdsFile(tier)\n            \n        return {""instances"": instances, ""images"": images, ""train"": train}\n\n    \'\'\'\n    Reads all tiers of a dataset (train if exists, val, test).\n    Creates also evalTrain tier which will optionally be used for evaluation. \n    \'\'\'\n    def readDataset(self, suffix = """", hasTrain = True):\n        dataset = {""train"": None, ""evalTrain"": None, ""val"": None, ""test"": None}        \n        if hasTrain:\n            dataset[""train""] = self.readTier(""train"" + suffix, train = True)\n        dataset[""val""] = self.readTier(""val"" + suffix, train = False)\n        dataset[""test""] = self.readTier(""test"" + suffix, train = False)\n        \n        if hasTrain:\n            dataset[""evalTrain""] = {}\n            for k in dataset[""train""]:\n                dataset[""evalTrain""][k] = dataset[""train""][k]\n            dataset[""evalTrain""][""train""] = False\n\n        return dataset\n\n    # Transform symbols to corresponding integers and vectorize into numpy array\n    def vectorizeData(self, data):\n        # if ""SHARED"" tie symbol representations in questions and answers \n        if config.ansEmbMod == ""SHARED"":\n            qDict = self.qaDict\n        else:\n            qDict = self.questionDict\n\n        encodedQuestions = [qDict.encodeSequence(d[""questionSeq""]) for d in data]\n        questions, questionsL = vectorize2DList(encodedQuestions)\n\n        answers = np.array([self.answerDict.encodeSym(d[""answer""]) for d in data])\n        \n        # pass the whole instances? if heavy then not good\n        imageIds = [d[""imageId""] for d in data]\n        indices = [d[""index""] for d in data]\n        instances = data\n\n        return {    ""questions"": questions,\n                    ""questionLengths"": questionsL,\n                    ""answers"": answers,\n                    ""imageIds"": imageIds,\n                    ""indices"": indices,\n                    ""instances"": instances\n                }\n\n    # Separates data based on a field length\n    def lseparator(self, key, lims):\n        maxI = len(lims)\n        def separatorFn(x):\n            v = x[key]\n            for i, lim in enumerate(lims):\n                if len(v) < lim:\n                    return i\n            return maxI\n        return {""separate"": separatorFn, ""groupsNum"": maxI + 1}\n\n    # # separate data based on a field type\n    # def tseparator(self, key, types):\n    #     typesNum = len(types) + 1\n    #     def separatorFn(x):\n    #         v = str(x[key][-1])\n    #         return types.get(v, len(types))\n    #     return {""separate"": separatorFn, ""groupsNum"": typesNum}\n\n    # # separate data based on field arity\n    # def bseparator(self, key):\n    #     def separatorFn(x):\n    #         cond = (len(x[key][-1]) == 2)\n    #         return (1 if cond else 0)\n    #     return {""separate"": separatorFn, ""groupsNum"": 2}\n\n    # Buckets data to groups using a separator\n    def bucket(self, instances, separator):\n        buckets = [[] for i in range(separator[""groupsNum""])]\n        for instance in instances:\n            bucketI = separator[""separate""](instance)\n            buckets[bucketI].append(instance)\n        return [bucket for bucket in buckets if len(bucket) > 0]\n\n    # Re-buckets bucket list given a seperator\n    def rebucket(self, buckets, separator):\n        res = []\n        for bucket in buckets:\n            res += self.bucket(bucket, separator)\n        return res\n\n    # Buckets data based on question / program length \n    def bucketData(self, data, noBucket = False):\n        if noBucket:\n            buckets = [data]\n        else:\n            if config.noBucket:\n                buckets = [data]\n            elif config.noRebucket:\n                questionSep = self.lseparator(""questionSeq"", config.questionLims)\n                buckets = self.bucket(data, questionSep)\n            else:\n                programSep = self.lseparator(""programSeq"", config.programLims)\n                questionSep = self.lseparator(""questionSeq"", config.questionLims)\n                buckets = self.bucket(data, programSep)\n                buckets = self.rebucket(buckets, questionSep)\n        return buckets\n\n    \'\'\' \n    Prepares data: \n    1. Filters data according to above arguments.\n    2. Takes only a subset of the data based on config.trainedNum / config.testedNum\n    3. Buckets data according to question / program length\n    4. Vectorizes data into numpy arrays\n    \'\'\'\n    def prepareData(self, data, train, filterKey = None, noBucket = False):\n        filterDefault = {""maxQLength"": 0, ""maxPLength"": 0, ""onlyChain"": False, ""filterOp"": 0}\n\n        filterTrain = {""maxQLength"": config.tMaxQ, ""maxPLength"": config.tMaxP,\n                       ""onlyChain"": config.tOnlyChain, ""filterOp"": config.tFilterOp}\n\n        filterVal = {""maxQLength"": config.vMaxQ, ""maxPLength"": config.vMaxP,\n                     ""onlyChain"": config.vOnlyChain, ""filterOp"": config.vFilterOp}\n\n        filters = {""train"": filterTrain, ""evalTrain"": filterTrain, \n                   ""val"": filterVal, ""test"": filterDefault}\n\n        if filterKey is None:\n            fltr = filterDefault\n        else:\n            fltr = filters[filterKey]\n\n        # split data when finetuning on validation set \n        if config.trainExtra and config.extraVal and (config.finetuneNum > 0):\n            if train:\n                data = data[:config.finetuneNum]\n            else: \n                data = data[config.finetuneNum:]\n\n        typeFilter = config.typeFilters[fltr[""filterOp""]]\n        # filter specific settings\n        if fltr[""onlyChain""]:\n            data = [d for d in data if all((len(inputNum) < 2) for inputNum in d[""programInputs""])]\n        if fltr[""maxQLength""] > 0:\n            data = [d for d in data if len(d[""questionSeq""]) <= fltr[""maxQLength""]]\n        if fltr[""maxPLength""] > 0:\n            data = [d for d in data if len(d[""programSeq""]) <= fltr[""maxPLength""]]\n        if len(typeFilter) > 0:\n            data = [d for d in data if d[""programSeq""][-1] not in typeFilter]\n\n        # run on subset of the data. If 0 then use all data \n        num = config.trainedNum if train else config.testedNum\n        # retainVal = True to retain same sample of validation across runs  \n        if (not train) and (not config.retainVal):\n            random.shuffle(data)\n        if num > 0:\n            data = data[:num]\n        # set number to match dataset size \n        if train:\n            config.trainedNum = len(data)\n        else:\n            config.testedNum = len(data)\n\n        # bucket\n        buckets = self.bucketData(data, noBucket = noBucket)\n        \n        # vectorize\n        return [self.vectorizeData(bucket) for bucket in buckets]\n\n    # Prepares all the tiers of a dataset. See prepareData method for further details.\n    def prepareDataset(self, dataset, noBucket = False):\n        if dataset is None:\n            return None\n\n        for tier in dataset:\n            if dataset[tier] is not None:\n                dataset[tier][""data""] = self.prepareData(dataset[tier][""instances""], \n                    train = dataset[tier][""train""], filterKey = tier, noBucket = noBucket)\n        \n        for tier in dataset:\n            if dataset[tier] is not None:\n                del dataset[tier][""instances""]\n\n        return dataset\n\n    # Initializes word embeddings to random uniform / random normal / GloVe. \n    def initializeWordEmbeddings(self, wordsDict = None, noPadding = False):\n        # default dictionary to use for embeddings\n        if wordsDict is None:\n            wordsDict = self.questionDict\n\n        # uniform initialization\n        if config.wrdEmbUniform:\n            lowInit = -1.0 * config.wrdEmbScale\n            highInit = 1.0 * config.wrdEmbScale\n            embeddings = np.random.uniform(low = lowInit, high = highInit, \n                size = (wordsDict.getNumSymbols(), config.wrdEmbDim))\n        # normal initialization\n        else:\n            embeddings = config.wrdEmbScale * np.random.randn(wordsDict.getNumSymbols(), \n                config.wrdEmbDim)\n\n        # if wrdEmbRandom = False, use GloVE\n        counter = 0\n        if (not config.wrdEmbRandom): \n            with open(config.wordVectorsFile, \'r\') as inFile:\n                for line in inFile:\n                    line = line.strip().split()\n                    word = line[0].lower()\n                    vector = [float(x) for x in line[1:]]\n                    index = wordsDict.sym2id.get(word)\n                    if index is not None:\n                        embeddings[index] = vector\n                        counter += 1\n        \n        print(counter)\n        print(self.questionDict.sym2id)\n        print(len(self.questionDict.sym2id))\n        print(self.answerDict.sym2id)      \n        print(len(self.answerDict.sym2id))\n        print(self.qaDict.sym2id)      \n        print(len(self.qaDict.sym2id))\n\n        if noPadding:            \n            return embeddings # no embedding for padding symbol\n        else:\n            return embeddings[1:]\n\n    \'\'\'\n    Initializes words embeddings for question words and optionally for answer words\n    (when config.ansEmbMod == ""BOTH""). If config.ansEmbMod == ""SHARED"", tie embeddings for\n    question and answer same symbols. \n    \'\'\'\n    def initializeQAEmbeddings(self):\n        # use same embeddings for questions and answers\n        if config.ansEmbMod == ""SHARED"":\n            qaEmbeddings = self.initializeWordEmbeddings(self.qaDict)\n            ansMap = np.array([self.qaDict.sym2id[sym] for sym in self.answerDict.id2sym])\n            embeddings = {""qa"": qaEmbeddings, ""ansMap"": ansMap}\n        # use different embeddings for questions and answers\n        else:\n            qEmbeddings = self.initializeWordEmbeddings(self.questionDict)\n            aEmbeddings = None\n            if config.ansEmbMod == ""BOTH"":\n                aEmbeddings = self.initializeWordEmbeddings(self.answerDict, noPadding = True)\n            embeddings = {""q"": qEmbeddings, ""a"": aEmbeddings}\n        return embeddings\n\n    \'\'\'\n    Preprocesses a given dataset into numpy arrays:\n    1. Reads the input data files into dictionary. \n    2. Saves the results jsons in files and loads them instead of parsing input if files exist/\n    3. Initializes word embeddings to random / GloVe.\n    4. Optionally filters data according to given filters.\n    5. Encodes and vectorize the data into numpy arrays.\n    5. Buckets the data according to the instances length.\n    \'\'\'\n    def preprocessData(self, debug = False):\n        # Read data into json and symbols\' dictionaries\n        print(bold(""Loading data...""))\n        start = time.time()\n        mainDataset = self.readDataset(hasTrain = True)\n\n        extraDataset = None\n        if config.extra:\n            # compositionalClevr doesn\'t have training dataset\n            extraDataset = self.readDataset(suffix = ""H"", hasTrain = (not config.extraVal))          \n            # extra dataset uses the same images\n            if not config.extraVal:\n                for tier in extraDataset:\n                    extraDataset[tier][""images""] = mainDataset[tier][""images""]\n\n        print(""took {:.2f} seconds"".format(time.time() - start))\n\n        # Initialize word embeddings (random / glove)\n        print(bold(""Loading word vectors...""))\n        start = time.time()\n        embeddings = self.initializeQAEmbeddings()\n        print(""took {:.2f} seconds"".format(time.time() - start))\n\n        # Prepare data: filter, bucket, and vectorize into numpy arrays\n        print(bold(""Vectorizing data...""))\n        start = time.time()       \n\n        mainDataset = self.prepareDataset(mainDataset)\n        # don\'t bucket for alternated data and also for humans data (small dataset)\n        extraDataset = self.prepareDataset(extraDataset, \n            noBucket = (not config.extraVal) or (not config.alterExtra))\n\n        data = {""main"": mainDataset, ""extra"": extraDataset}\n        print(""took {:.2f} seconds"".format(time.time() - start))\n\n        config.questionWordsNum = self.questionDict.getNumSymbols()\n        config.answerWordsNum = self.answerDict.getNumSymbols()\n        \n        return data, embeddings, self.answerDict\n'"
program_translator.py,0,"b'\nclass ProgramTranslator(object):\n    def __init__(self, programDict, maxArity):\n        self.programDict = programDict\n        self.maxArity = maxArity\n\n        self.maxStack = 0\n\n    def functionToKey(self, function, withValInputs = True):\n        valInputs = """"\n        if withValInputs:\n            valInputs = ""_"" + "","".join(function[""value_inputs""])\n        functionKey = function[""function""] if ""_"" in function[""function""] else \\\n                      ""_"".join([function[""function""], function[""function""]])\n        return str(len(function[""inputs""])) + ""_"" + functionKey + valInputs\n\n    def keyToFunction(self, key):\n        assert key not in self.programDict.invalidSymbols\n        function = {}\n        parts = key.split(""_"")\n        arity = int(parts[0])\n        function[""function""] = ""_"".join([parts[1], parts[2]])\n        function[""value_inputs""] = []\n        if len(parts) == 4:\n            function[""value_inputs""] = parts[3].split("","")\n        function[""inputs""] = []\n        return function, arity\n    \n    def keyToArity(self, key):\n        if key in self.programDict.invalidSymbols:\n            return 0\n        return int(key.split(""_"")[0])\n\n    def keyToType(self, key):\n        if key in self.programDict.invalidSymbols:\n            return [""0"", ""0"", ""0""]\n        return [""0:"" + key.split(""_"")[0], ""1:"" + key.split(""_"")[1], ""2:"" + key.split(""_"")[2]]\n\n    def programToPostfixProgram(self, program):\n        newProgram = []\n        \n        def programToPostfixAux(currIndex = -1):\n            childrenIndices = program[currIndex][""inputs""]\n            #[int(child) for child in program[currIndex][""inputs""]]\n            childrenNewIndices = []\n            for child in childrenIndices:\n                programToPostfixAux(child)\n                childrenNewIndices.append(len(newProgram) - 1)\n            program[currIndex][""inputs""] = childrenNewIndices\n            newProgram.append(program[currIndex])\n        \n        programToPostfixAux()\n        return newProgram\n\n    def programToSeq(self, program):\n        return [self.functionToKey(function) for function in program]\n\n    def programToInputs(self, program, offset = 0):\n        inputs = [function[""inputs""] for function in program]\n        offsetedInputs = [[FuncInput + offset for FuncInput in FuncInputs] for FuncInputs in inputs]\n        return offsetedInputs\n\n    # def seqToProgram(self, seq, enforceValidPrograms = True):\n    #     program = []\n\n    #     def seqToProgramAux(currIndex = len(seq) - 1):\n    #         if currIndex < 0:\n    #             program = None\n    #             return\n    #         currFunc, arity = self.keyToFunction(seq[currIndex])\n    #         nextIndex = currIndex - 1\n    #         program.append(currFunc)\n    #         for _ in arity:\n    #             currFunc[""inputs""].append(nextIndex)\n    #             nextIndex = seqToProgramAux(nextIndex)\n    #         currFunc[""inputs""].reverse()\n    #         return nextIndex\n\n    #     if enforceValidPrograms:\n    #         seqToProgramAux()\n    #         if program is not None:\n    #             program.reverse()\n    #     else:\n    #         stack = [0] * self.maxArity\n    #         for i in range(len(seq)):\n    #             func, arity = self.keyToFunction(seq[i])\n    #             func[""inputs""] = stack[len(stack) - arity:]\n    #             newLength = max(len(stack) - arity, self.maxArity)\n    #             stack = stack[:newLength] + [i + self.maxArity]\n    #             self.maxStack = max(len(stack), self.maxStack)\n    #             program.append(func)\n\n    #     return program\n'"
visualization.py,0,"b'import json\nimport pandas\nimport argparse \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom scipy.misc import imread, imresize\nfrom matplotlib.colors import Normalize, LinearSegmentedColormap\n\nflatten = lambda ll: [e for l in ll for e in l]\n\nparser = argparse.ArgumentParser()\n\n# experiment settings\nparser.add_argument(""--tier"",  default = ""val"", choices = [""train"", ""val"", ""test""], type = str)\nparser.add_argument(""--expName"",  default = ""experiment"", type = str)\n\n# plotting\nparser.add_argument(""--cmap"", default = ""custom"", type = str) # ""gnuplot2"", ""GreysT"" \n\nparser.add_argument(""--trans"", help = ""transpose question attention"", action = ""store_true"")\nparser.add_argument(""--sa"", action = ""store_true"")\nparser.add_argument(""--gate"", action = ""store_true"")\n\n# filtering\nparser.add_argument(""--instances"", nargs = ""*"", type = int)\nparser.add_argument(""--maxNum"", default = 0, type = int)\n\nparser.add_argument(""--filter"", default = [], nargs = ""*"", choices = [""mod"", ""length"", ""field""])\nparser.add_argument(""--filterMod"", action = ""store_true"")\nparser.add_argument(""--filterLength"", type = int) # 19\nparser.add_argument(""--filterField"", type = str)\nparser.add_argument(""--filterIn"", action = ""store_true"")\nparser.add_argument(""--filterList"", nargs = ""*"") # [""how many"", ""more""], numbers\n\nargs = parser.parse_args()\n\nisRight = lambda instance: instance[""answer""] == instance[""prediction""]\nisRightStr = lambda instance: ""RIGHT"" if isRight(instance) else ""WRONG""\n\n# files\n# jsonFilename = ""valHPredictions.json"" if args.humans else ""valPredictions.json""\nimagesDir = ""./CLEVR_v1/images/{tier}"".format(\n    tier = args.tier)\n\ndataFile = ""./preds/{expName}/{tier}Predictions-{expName}.json"".format(\n    tier = args.tier, \n    expName = args.expName)\n\ninImgName = lambda index: ""{dir}/CLEVR_{tier}_{index}.png"".format(\n    dir = imagesDir, \n    index = (""000000%d"" % index)[-6:],\n    tier = args.tier)\n\noutImgAttName = lambda instance, j: ""./preds/{expName}/{tier}{id}Img_{step}.png"".format(\n    expName = args.expName, \n    tier = args.tier, \n    id = instance[""index""], \n    step = j + 1)\n\noutTableAttName = lambda instance, name: ""./preds/{expName}/{tier}{id}{tableName}_{right}{orientation}.png"".format(\n    expName = args.expName, \n    tier = args.tier, \n    id = instance[""index""], \n    tableName = name, \n    right = isRightStr(instance), \n    orientation = ""_t"" if args.trans else """")\n\n# plotting\nimageDims = (14,14)\nfigureImageDims = (2,3)\nfigureTableDims = (5,4)\nfontScale = 1\n\n# set transparent mask for low attention areas  \n# cdict = plt.get_cmap(""gnuplot2"")._segmentdata\ncdict = {""red"": ((0.0, 0.0, 0.0), (0.6, 0.8, 0.8), (1.0, 1, 1)), \n    ""green"": ((0.0, 0.0, 0.0), (0.6, 0.8, 0.8), (1.0, 1, 1)), \n    ""blue"": ((0.0, 0.0, 0.0), (0.6, 0.8, 0.8), (1.0, 1, 1))}\ncdict[""alpha""] = ((0.0, 0.35, 0.35),\n                  (1.0,0.65, 0.65))\nplt.register_cmap(name = ""custom"", data = cdict)\n\ndef savePlot(fig, fileName):\n    plt.savefig(fileName, dpi = 720)\n    plt.close(fig) \n    del fig\n\ndef filter(instance):\n    if ""length"" in args.filter: \n        if len(instance[""question""].split("" "")) > args.filterLength:\n             return True\n\n    if ""field"" in args.filter:\n        if args.filterIn:  \n            if not (instance[args.filterField] in args.filterList):\n                return True\n        else:\n            if not any((l in instance[args.filterField]) for l in args.filterList):\n                return True            \n\n    if ""mod"" in args.filter:\n        if (not isRight(instance)) and args.filterMod:\n            return True\n\n        if isRight(instance) and (not args.filterMod):\n            return True\n\n    return False\n\ndef showImgAtt(img, instance, step, ax):\n    dx, dy = 0.05, 0.05\n    x = np.arange(-1.5, 1.5, dx)\n    y = np.arange(-1.0, 1.0, dy)\n    X, Y = np.meshgrid(x, y)\n    extent = np.min(x), np.max(x), np.min(y), np.max(y)\n\n    ax.cla()\n\n    img1 = ax.imshow(img, interpolation = ""nearest"", extent = extent)\n    ax.imshow(np.array(instance[""attentions""][""kb""][step]).reshape(imageDims), cmap = plt.get_cmap(args.cmap), \n        interpolation = ""bicubic"", extent = extent)\n\n    ax.set_axis_off()\n    plt.axis(""off"")\n\n    ax.set_aspect(""auto"")\n\n\ndef showImgAtts(instance):\n    img = imread(inImgName(instance[""imageId""]))\n\n    length = len(instance[""attentions""][""kb""])\n    \n    # show images\n    for j in range(length):\n        fig, ax = plt.subplots()\n        fig.set_figheight(figureImageDims[0])\n        fig.set_figwidth(figureImageDims[1])              \n        \n        showImgAtt(img, instance, j, ax)\n        \n        plt.subplots_adjust(bottom = 0, top = 1, left = 0, right = 1)\n        savePlot(fig, outImgAttName(instance, j))\n\ndef showTableAtt(instance, table, x, y, name):\n    # if args.trans:\n    #     figureTableDims = (len(y) / 2 + 4, len(x) + 2)\n    # else:\n    #     figureTableDims = (len(y) / 2, len(x) / 2)\n    # xx = np.arange(0, len(x), 1)\n    # yy = np.arange(0, len(y), 1)\n    # extent2 = np.min(xx), np.max(xx), np.min(yy), np.max(yy)\n    \n    fig2, bx = plt.subplots(1, 1) # figsize = figureTableDims\n    bx.cla()\n\n    sns.set(font_scale = fontScale)\n\n    if args.trans:\n        table = np.transpose(table)\n        x, y = y, x\n    \n    tableMap = pandas.DataFrame(data = table, index = x, columns = y)\n    \n    bx = sns.heatmap(tableMap, cmap = ""Purples"", cbar = False, linewidths = .5, linecolor = ""gray"", square = True)\n    \n    # x ticks\n    if args.trans:\n        bx.xaxis.tick_top()\n    locs, labels = plt.xticks()\n    if args.trans:\n        plt.setp(labels, rotation = 0)\n    else:\n        plt.setp(labels, rotation = 60)\n\n    # y ticks\n    locs, labels = plt.yticks()\n    plt.setp(labels, rotation = 0)\n\n    plt.savefig(outTableAttName(instance, name), dpi = 720)\n\ndef main():\n    with open(dataFile) as inFile:\n        results = json.load(inFile)\n\n    # print(args.exp)\n\n    count = 0\n    if args.instances is None:\n        args.instances = range(len(results))\n\n    for i in args.instances:        \n        if filter(results[i]):\n            continue\n\n        if count > args.maxNum and args.maxNum > 0:\n            break\n        count += 1\n\n        length = len(results[i][""attentions""][""kb""])\n        showImgAtts(results[i])\n\n        iterations = range(1, length + 1)\n        questionList = results[i][""question""].split("" "")\n        table = np.array(results[i][""attentions""][""question""])[:,:(len(questionList) + 1)]        \n        showTableAtt(results[i], table, iterations, questionList, ""text"")\n\n        if args.sa:\n            iterations = range(length)\n            sa = np.zeros((length, length))\n            for i in range(length):\n                for j in range(i+1):\n                    sa[i][j] = results[i][""attentions""][""self""][i][j]\n            \n            showTableAtt(results[i], sa[i][j], iterations, iterations, ""sa"")                    \n\n        print(i)\n        print(""id:"", results[i][""index""])        \n        print(""img:"", results[i][""imageId""])\n        print(""Q:"", results[i][""question""])\n        print(""G:"", results[i][""answer""])\n        print(""P:"", results[i][""prediction""])\n        print(isRightStr(results[i]))\n\n        if args.gate:\n            print(results[i][""attentions""][""gate""])\n\n        print(""________________________________________________________________________"")\n\nif __name__ == ""__main__"":\n    main()\n'"
