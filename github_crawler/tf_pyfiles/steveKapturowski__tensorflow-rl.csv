file_path,api_count,code
launch_cluster.py,0,"b""#!/usr/bin/env python\nimport subprocess\nimport argparse\nimport signal\nimport yaml\n\n\ndef launch_proc(proc_type, args):\n\tcmd = ['python', 'main.py'] + args \\\n\t\t+ ['--job_name', proc_type, '--task_index', str(i)]\n\treturn subprocess.Popen(cmd)\n\n\ndef launch_cluster(spec, arg_string, daemonize=False):\n\tparameter_servers = [launch_proc('ps', arg_string) for i in spec['ps']]\n\tworkers = [launch_proc('worker', arg_string) for i in spec['worker']]\n\n\tif not daemonize:\n\t\tprocs = parameter_servers + workers\n\t\ttry:\n\t\t\tfor p in procs:\n\t\t\t\tp.wait()\n\t\texcept KeyboardInterrupt:\n\t\t\tfor p in streams:\n\t\t\t\tp.send_signal(signal.SIGINT)\n\n\nif __name__ == '__main__':\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-n', '--num_workers', default=8, type=int, help='number of workers to use if in local mode', dest='num_workers')\n\tparser.add_argument('--cluster_config', default=None, help='load cluster config from yaml file', dest='cluster_config')\n\tparser.add_argument('--daemonize', action='store_true', help='detach after cluster creation', dest='daemonize')\n\targs, unused_args = parser.parse_known_args()\n\n\tif args.cluster_config:\n\t\twith open(args.cluster_config, 'r') as f:\n\t\t\tspec = yaml.load(f)\n\telse:\n\t\tspec = {\n\t\t\t'ps': ['localhost:2048'],\n\t\t\t'worker': [\n\t\t\t\t'localhost:{}'.format(4096+i)\n\t\t\t\tfor i in range(args.num_workers)\n\t\t\t]}\n\n\tlaunch_cluster(spec, unused_args, daemonize=args.daemonize)\n\n\n"""
main.py,2,"b'# -*- coding: utf-8 -*-\r\nimport os\r\nimport sys\r\nimport time\r\nimport yaml\r\nimport argparse\r\nimport numpy as np\r\nimport utils.logger\r\nimport multiprocessing\r\nimport tensorflow as tf\r\n\r\nfrom networks.q_network import QNetwork\r\nfrom multiprocessing import Process, Queue\r\nfrom networks.dueling_network import DuelingNetwork\r\nfrom networks.continuous_actions import ContinuousPolicyNetwork, ContinuousPolicyValueNetwork\r\nfrom networks.policy_v_network import PolicyNetwork, PolicyValueNetwork, PolicyRepeatNetwork, SequencePolicyVNetwork\r\nfrom utils.shared_memory import SharedCounter, SharedVars, SharedFlags, Barrier\r\nfrom algorithms.policy_based_actor_learner import A3CLearner, A3CLSTMLearner\r\nfrom algorithms.sequence_decoder_actor_learner import ActionSequenceA3CLearner, ARA3CLearner\r\nfrom algorithms.value_based_actor_learner import NStepQLearner, DuelingLearner, OneStepSARSALearner\r\nfrom algorithms.intrinsic_motivation_actor_learner import PseudoCountA3CLearner, PseudoCountA3CLSTMLearner, PseudoCountQLearner\r\nfrom algorithms.trpo_actor_learner import TRPOLearner\r\nfrom algorithms.pgq_actor_learner import PGQLearner\r\nfrom algorithms.cem_actor_learner import CEMLearner\r\n\r\nlogger = utils.logger.getLogger(\'main\')\r\n\r\n\r\nALGORITHMS = {\r\n    \'q\': (NStepQLearner, QNetwork),\r\n    \'sarsa\': (OneStepSARSALearner, QNetwork),\r\n    \'dueling\': (DuelingLearner, DuelingNetwork),\r\n    \'a3c\': (A3CLearner, PolicyValueNetwork),\r\n    \'a3c-lstm\': (A3CLSTMLearner, PolicyValueNetwork),\r\n    \'a3c-sequence-decoder\': (ActionSequenceA3CLearner, SequencePolicyVNetwork),\r\n    \'pgq\': (PGQLearner, PolicyValueNetwork),\r\n    \'trpo\': (TRPOLearner, PolicyNetwork),\r\n    \'cem\': (CEMLearner, PolicyNetwork),\r\n    \'dqn-cts\': (PseudoCountQLearner, QNetwork),\r\n    \'a3c-cts\': (PseudoCountA3CLearner, PolicyValueNetwork),\r\n    \'a3c-lstm-cts\': (PseudoCountA3CLSTMLearner, PolicyValueNetwork),\r\n    \'a3c-repeat\': (ARA3CLearner, PolicyRepeatNetwork),\r\n    \'a3c-continuous\': (A3CLearner, ContinuousPolicyValueNetwork),\r\n    \'a3c-lstm-continuous\': (A3CLSTMLearner, ContinuousPolicyValueNetwork),\r\n    \'cem-continuous\': (CEMLearner, ContinuousPolicyNetwork),\r\n    \'trpo-continuous\': (TRPOLearner, ContinuousPolicyNetwork),\r\n}\r\n\r\ndef get_num_actions(rom_path, rom_name):\r\n    from ale_python_interface import ALEInterface\r\n    filename = \'{0}/{1}.bin\'.format(rom_path, rom_name)\r\n    ale = ALEInterface()\r\n    ale.loadROM(filename)\r\n    return len(ale.getMinimalActionSet())\r\n\r\ndef main(args):\r\n    args.batch_size = None\r\n    logger.debug(\'CONFIGURATION: {}\'.format(args))\r\n    \r\n    """""" Set up the graph, the agents, and run the agents in parallel. """"""\r\n    if args.env == \'GYM\':\r\n        from environments import atari_environment\r\n        num_actions, action_space, _ = atari_environment.get_actions(args.game)\r\n        input_shape = atari_environment.get_input_shape(args.game)\r\n    else:\r\n        num_actions = get_num_actions(args.rom_path, args.game)\r\n    \r\n    args.action_space = action_space\r\n    args.summ_base_dir = \'/tmp/summary_logs/{}/{}\'.format(args.game, time.strftime(\'%m.%d/%H.%M\'))\r\n    logger.info(\'logging summaries to {}\'.format(args.summ_base_dir))\r\n\r\n    Learner, Network = ALGORITHMS[args.alg_type]\r\n    network = Network({\r\n        \'name\': \'shared_vars_network\',\r\n        \'input_shape\': input_shape,\r\n        \'num_act\': num_actions,\r\n        \'args\': args\r\n    })\r\n    args.network = Network\r\n\r\n    #initialize shared variables\r\n    args.learning_vars = SharedVars(network.params)\r\n    args.opt_state = SharedVars(\r\n        network.params, opt_type=args.opt_type, lr=args.initial_lr\r\n    ) if args.opt_mode == \'shared\' else None\r\n    args.batch_opt_state = SharedVars(\r\n        network.params, opt_type=args.opt_type, lr=args.initial_lr\r\n    ) if args.opt_mode == \'shared\' else None\r\n\r\n    #TODO: need to refactor so TRPO+GAE doesn\'t need special treatment\r\n    if args.alg_type in [\'trpo\', \'trpo-continuous\']:\r\n        if args.arch == \'FC\': #add timestep feature\r\n            vf_input_shape = [input_shape[0]+1]\r\n        else:\r\n            vf_input_shape = input_shape\r\n\r\n        baseline_network = PolicyValueNetwork({\r\n            \'name\': \'shared_value_network\',\r\n            \'input_shape\': vf_input_shape,\r\n            \'num_act\': num_actions,\r\n            \'args\': args\r\n        }, use_policy_head=False)\r\n        args.baseline_vars = SharedVars(baseline_network.params)\r\n        args.vf_input_shape = vf_input_shape\r\n        \r\n    if args.alg_type in [\'q\', \'sarsa\', \'dueling\', \'dqn-cts\']:\r\n        args.target_vars = SharedVars(network.params)\r\n        args.target_update_flags = SharedFlags(args.num_actor_learners)\r\n    if args.alg_type in [\'dqn-cts\', \'a3c-cts\', \'a3c-lstm-cts\']:\r\n        args.density_model_update_flags = SharedFlags(args.num_actor_learners)\r\n\r\n    tf.reset_default_graph()\r\n    args.barrier = Barrier(args.num_actor_learners)\r\n    args.global_step = SharedCounter(0)\r\n    args.num_actions = num_actions\r\n\r\n    cuda_visible_devices = os.getenv(\'CUDA_VISIBLE_DEVICES\')\r\n    num_gpus = 0\r\n    if cuda_visible_devices:\r\n        num_gpus = len(cuda_visible_devices.split())\r\n\r\n    #spin up processes and block\r\n    if (args.visualize == 2): args.visualize = 0        \r\n    actor_learners = []\r\n    task_queue = Queue()\r\n    experience_queue = Queue()\r\n    seed = args.seed or np.random.randint(2**32)\r\n    np.random.seed(seed)\r\n    tf.set_random_seed(seed)\r\n    for i in xrange(args.num_actor_learners):\r\n        if (args.visualize == 2) and (i == args.num_actor_learners - 1):\r\n            args.args.visualize = 1\r\n\r\n        args.actor_id = i\r\n        args.device = \'/gpu:{}\'.format(i % num_gpus) if num_gpus else \'/cpu:0\'\r\n        \r\n        args.random_seed = seed + i\r\n            \r\n        #only used by TRPO\r\n        args.task_queue = task_queue\r\n        args.experience_queue = experience_queue\r\n\r\n        args.input_shape = input_shape\r\n        actor_learners.append(Learner(args))\r\n        actor_learners[-1].start()\r\n\r\n    try:\r\n        for t in actor_learners:\r\n            t.join()\r\n    except KeyboardInterrupt:\r\n        #Terminate with extreme prejudice\r\n        for t in actor_learners:\r\n            t.terminate()\r\n    \r\n    logger.info(\'All training threads finished!\')\r\n    logger.info(\'Use seed={} to reproduce\'.format(seed))\r\n\r\n\r\ndef get_validated_params(args):\r\n    #validate param\r\n    if args.env==\'ALE\' and args.rom_path is None:\r\n        raise argparse.ArgumentTypeError(\'Need to specify the directory where the game roms are located, via --rom_path\')         \r\n    if args.reward_clip_val <= 0:\r\n        raise argparse.ArgumentTypeError(\'value of --reward_clip_val option must be non-negative\')\r\n    if args.alg_type not in ALGORITHMS:\r\n        raise argparse.ArgumentTypeError(\'alg_type `{}` not implemented\'.format(args.alg_type))\r\n\r\n    # fix up frame_skip depending on whether it was an int or tuple \r\n    if len(args.frame_skip) == 1:\r\n        args.frame_skip = args.frame_skip[0]\r\n    elif len(args.frame_skip) > 2:\r\n        raise argparse.ArgumentTypeError(\'Expected tuple of length two or int for param `frame_skip`\')\r\n\r\n    return args\r\n\r\n\r\ndef get_config():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\'--load_config\', default=None, help=\'load config defaults from yaml file\', dest=\'config_path\')\r\n    args, unprocessed_args = parser.parse_known_args()\r\n\r\n    #override defaults\r\n    parser.add_argument(\'game\', help=\'Name of game\')\r\n    parser.add_argument(\'--alg_type\', default=""a3c"", help=\'Type of algorithm: q (for Q-learning), sarsa, a3c (for actor-critic)\', dest=\'alg_type\')\r\n    parser.add_argument(\'--arch\', default=\'NIPS\', help=\'Which network architecture to use: NIPS, NATURE, ATARI-TRPO, or FC (fully connected)\', dest=\'arch\')\r\n    parser.add_argument(\'--env\', default=\'GYM\', help=\'Type of environment: ALE or GYM\', dest=\'env\')\r\n    parser.add_argument(\'--rom_path\', help=\'Directory where the game roms are located (needed for ALE environment)\', dest=\'rom_path\')\r\n    parser.add_argument(\'-n\', \'--num_actor_learners\', default=8, type=int, help=\'number of actors (processes)\', dest=\'num_actor_learners\')\r\n    parser.add_argument(\'-v\', \'--visualize\', default=0, type=int, help=\'0: no visualization of emulator; 1: all emulators, for all actors, are visualized; 2: only 1 emulator (for one of the actors) is visualized\', dest=\'visualize\')\r\n    parser.add_argument(\'--gamma\', default=0.99, type=float, help=\'Discount factor\', dest=\'gamma\')\r\n    parser.add_argument(\'--frame_skip\', default=[4], type=int, nargs=\'+\', help=\'number of frames to repeat action\', dest=\'frame_skip\')\r\n    parser.add_argument(\'--history_length\', default=4, type=int, help=\'number of frames to stack as input state\', dest=\'history_length\')\r\n    parser.add_argument(\'--single_life_episodes\', action=\'store_true\', help=\'if true, training episodes will be terminated when a life is lost (for games)\', dest=\'single_life_episodes\')\r\n    parser.add_argument(\'--max_decoder_steps\', default=20, type=int, help=\'max number of steps that sequence decoder will be allowed to take\', dest=\'max_decoder_steps\')\r\n    parser.add_argument(\'--test\', action=\'store_false\', help=\'if not set train agents in parallel, otherwise follow optimal policy with single agent\', dest=\'is_train\')\r\n    parser.add_argument(\'--restore_checkpoint\', action=\'store_true\', help=\'resume training from last checkpoint\', dest=\'restore_checkpoint\')\r\n    parser.add_argument(\'--use_monitor\', action=\'store_true\', help=\'Record video / episode stats if set\', dest=\'use_monitor\')\r\n    parser.add_argument(\'--pgq_fraction\', default=0.5, type=float, help=\'fraction by which to multiply q gradients\', dest=\'pgq_fraction\')\r\n    parser.add_argument(\'--activation\', default=\'relu\', type=str, help=\'specify relu, softplus, or tanh activations\', dest=\'activation\')\r\n    parser.add_argument(\'--use_rgb\', action=\'store_true\', help=\'If set use rgb image channels instead of stacked luninance frames\', dest=\'use_rgb\')\r\n    parser.add_argument(\'--no_share_weights\', action=\'store_false\', help=\'If set don\\\'t share parameters between policy and value function\', dest=\'share_encoder_weights\')\r\n    parser.add_argument(\'--fc_layer_sizes\', default=[60, 60], type=int, nargs=\'+\', help=\'width of layers in fully connected architecture\', dest=\'fc_layer_sizes\')\r\n    parser.add_argument(\'--seed\', default=None, type=int, help=\'Specify random seed. Each process will get its own unique seed computed as seed+actor_id. Due to race conditions only 1 worker process should be used to get deterministic results\', dest=\'seed\')\r\n\r\n    #optimizer args\r\n    parser.add_argument(\'--opt_type\', default=\'rmsprop\', help=\'Type of optimizer: rmsprop, momentum, adam, adamax\', dest=\'opt_type\')\r\n    parser.add_argument(\'--opt_mode\', default=\'shared\', help=\'Whether to use \\""local\\"" or \\""shared\\"" vector(s) for the momemtum/optimizer statistics\', dest=\'opt_mode\')\r\n    parser.add_argument(\'--b1\', default=0.9, type=float, help=\'Beta1 for the Adam optimizer\', dest=\'b1\')\r\n    parser.add_argument(\'--b2\', default=0.999, type=float, help=\'Beta2 for the Adam optimizer\', dest=\'b2\')\r\n    parser.add_argument(\'--e\', default=0.1, type=float, help=\'Epsilon for the Rmsprop and Adam optimizers\', dest=\'e\')\r\n    parser.add_argument(\'--momentum\', default=0.99, type=float, help=\'Discount factor for the history/coming gradient, for the Rmsprop optimizer\', dest=\'momentum\')\r\n    parser.add_argument(\'-lr\', \'--initial_lr\', default=0.001, type=float, help=\'Initial value for the learning rate. Default = LogUniform(10**-4, 10**-2)\', dest=\'initial_lr\')\r\n    parser.add_argument(\'-lra\', \'--lr_annealing_steps\', default=200000000, type=int, help=\'Nr. of global steps during which the learning rate will be linearly annealed towards zero\', dest=\'lr_annealing_steps\')\r\n    parser.add_argument(\'--max_episode_steps\', default=None, type=int, help=\'max rollout steps per trpo episode\', dest=\'max_episode_steps\')\r\n\r\n    #clipping args\r\n    parser.add_argument(\'--clip_loss\', default=0.0, type=float, help=\'If bigger than 0.0, the loss will be clipped at +/-clip_loss\', dest=\'clip_loss_delta\')\r\n    parser.add_argument(\'--clip_norm\', default=40, type=float, help=\'If clip_norm_type is local/global, grads will be clipped at the specified maximum (avaerage) L2-norm\', dest=\'clip_norm\')\r\n    parser.add_argument(\'--clip_norm_type\', default=\'global\', help=\'Whether to clip grads by their norm or not. Values: ignore (no clipping), local (layer-wise norm), global (global norm)\', dest=\'clip_norm_type\')\r\n    parser.add_argument(\'--rescale_rewards\', action=\'store_true\', help=\'If True, rewards will be rescaled (dividing by the max. possible reward) to be in the range [-1, 1]. If False, rewards will be clipped to be in the range [-REWARD_CLIP, REWARD_CLIP]\', dest=\'rescale_rewards\')\r\n    parser.add_argument(\'--reward_clip_val\', default=1.0, type=float, help=\'Clip rewards outside of [-REWARD_CLIP, REWARD_CLIP]\', dest=\'reward_clip_val\')\r\n    \r\n    #q-learning args\r\n    parser.add_argument(\'-ea\', \'--epsilon_annealing_steps\', default=1000000, type=int, help=\'Nr. of global steps during which the exploration epsilon will be annealed\', dest=\'epsilon_annealing_steps\')\r\n    parser.add_argument(\'--final_epsilon\', default=0.1, type=float, help=\'Final epsilon after annealing is complete. Only used for dqn-cts\', dest=\'final_epsilon\')\r\n    parser.add_argument(\'--grads_update_steps\', default=5, type=int, help=\'Nr. of local steps during which grads are accumulated before applying them to the shared network parameters (needed for 1-step Q/Sarsa learning)\', dest=\'grads_update_steps\')\r\n    parser.add_argument(\'--q_target_update_steps\', default=10000, type=int, help=\'Interval (in nr. of global steps) at which the parameters of the Q target network are updated (obs! 1 step = 4 video frames) (needed for Q-learning and Sarsa)\', dest=\'q_target_update_steps\') \r\n    parser.add_argument(\'--replay_size\', default=100000, type=int, help=\'Maximum capacity of replay memory\', dest=\'replay_size\')\r\n    parser.add_argument(\'--batch_update_size\', default=32, type=int, help=\'Minibatch size for q-learning updates\', dest=\'batch_update_size\')\r\n    parser.add_argument(\'--exploration_strategy\', default=\'epsilon-greedy\', type=str, help=\'boltzmann or epsilon-greedy\', dest=\'exploration_strategy\')\r\n    parser.add_argument(\'--temperature\', default=1.0, type=float, help=\'temperature to use for boltzmann exploration\', dest=\'bolzmann_temperature\')\r\n\r\n    #a3c args \r\n    parser.add_argument(\'--entropy\', default=0.01, type=float, help=\'Strength of the entropy regularization term (needed for actor-critic)\', dest=\'entropy_regularisation_strength\')\r\n    parser.add_argument(\'--max_global_steps\', default=200000000, type=int, help=\'Max. number of training steps\', dest=\'max_global_steps\')\r\n    parser.add_argument(\'--max_local_steps\', default=5, type=int, help=\'Number of steps to gain experience from before every update for the Q learning/A3C algorithm\', dest=\'max_local_steps\')\r\n    \r\n    #trpo args\r\n    parser.add_argument(\'--num_epochs\', default=1000, type=int, help=\'number of epochs for which to run TRPO\', dest=\'num_epochs\')\r\n    parser.add_argument(\'--episodes_per_batch\', default=50, type=int, help=\'number of episodes to batch for TRPO updates\', dest=\'episodes_per_batch\')\r\n    parser.add_argument(\'--trpo_max_rollout\', default=1000, type=int, help=\'max rollout steps per trpo episode\', dest=\'max_rollout\')\r\n    parser.add_argument(\'--cg_subsample\', default=0.1, type=float, help=\'rate at which to subsample data for TRPO conjugate gradient iteration\', dest=\'cg_subsample\')\r\n    parser.add_argument(\'--cg_damping\', default=0.001, type=float, help=\'conjugate gradient damping weight\', dest=\'cg_damping\')   \r\n    parser.add_argument(\'--max_kl\', default=0.01, type=float, help=\'max kl divergence for TRPO updates\', dest=\'max_kl\')\r\n    parser.add_argument(\'--td_lambda\', default=1.0, type=float, help=\'lambda parameter for GAE\', dest=\'td_lambda\')\r\n    \r\n    #cts args\r\n    parser.add_argument(\'--cts_bins\', default=8, type=int, help=\'number of bins to assign pixel values\', dest=\'cts_bins\')\r\n    parser.add_argument(\'--cts_rescale_dim\', default=42, type=int, help=\'rescaled image size to use with cts density model\', dest=\'cts_rescale_dim\')\r\n    parser.add_argument(\'--cts_beta\', default=.05, type=float, help=\'weight by which to scale novelty bonuses\', dest=\'cts_beta\')\r\n    parser.add_argument(\'--cts_eta\', default=.9, type=float, help=\'mixing param between 1-step TD-Error and Monte-Carlo Error\', dest=\'cts_eta\')\r\n    parser.add_argument(\'--density_model\', default=\'cts\', type=str, help=\'density model to use for generating novelty bonuses: cts, or pixel-counts\', dest=\'density_model\')\r\n    parser.add_argument(\'--q_update_interval\', default=4, type=int, help=\'Number of steps between successive batch q-learning updates\', dest=\'q_update_interval\')\r\n\r\n    if args.config_path:\r\n        with open(args.config_path, \'r\') as f:\r\n            parser.set_defaults(**yaml.load(f))\r\n\r\n    args = parser.parse_args(unprocessed_args)\r\n    return get_validated_params(args)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main(get_config())\r\n\r\n '"
setup.py,0,"b""#!/usr/bin/env python\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy as np\n\nextensions = [\n\tExtension('utils.fast_cts', sources=['utils/fast_cts.pyx']),\n\tExtension('utils.hogupdatemv', sources=['utils/hogupdatemv.pyx']),\n]\n\nsetup(\n\tname='async-rl-extensions',\n\tinclude_dirs=[np.get_include()],   \n\text_modules=cythonize(extensions)\n)\n"""
algorithms/__init__.py,0,b''
algorithms/actor_learner.py,16,"b'# -*- encoding: utf-8 -*-\r\nimport gym\r\nimport time\r\nimport ctypes\r\nimport tempfile\r\nimport utils.logger\r\nimport multiprocessing\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom utils import checkpoint_utils\r\nfrom utils.decorators import only_on_train\r\nfrom utils.hogupdatemv import apply_grads_mom_rmsprop, apply_grads_adam, apply_grads_adamax\r\nfrom contextlib import contextmanager\r\nfrom multiprocessing import Process\r\n\r\n\r\nCHECKPOINT_INTERVAL = 100000\r\nONE_LIFE_GAMES = [\r\n    #Atari\r\n    \'Bowling-v0\',\r\n    \'Boxing-v0\',\r\n    \'Carnival-v0\',\r\n    \'DoubleDunk-v0\',\r\n    \'Enduro-v0\',\r\n    \'FishingDerby-v0\',\r\n    \'Freeway-v0\',\r\n    \'IceHockey-v0\',\r\n    \'JourneyEscape-v0\',\r\n    \'Pong-v0\',\r\n    \'PrivateEye-v0\',\r\n    \'Skiing-v0\',\r\n    \'Tennis-v0\',\r\n    #Classic Control\r\n    \'CartPole-v0\',\r\n    \'CartPole-v1\',\r\n    \'Pendulum-v0\',\r\n    \'MountainCar-v0\',\r\n    \'Acrobot-v1\',\r\n    \'MountainCarContinuous-v0\',\r\n    \'Pendulum-v0\',\r\n    #Box2D\r\n    \'LunarLander-v2\',\r\n    \'LunarLanderContinuous-v2\',\r\n    \'BipedalWalker-v2\',\r\n    \'BipedalWalkerHardcore-v2\',\r\n    \'CarRacing-v0\',\r\n    #MuJoCo\r\n    \'InvertedPendulum-v1\',\r\n    \'IvertedDoublePendulum-v1\',\r\n    \'Reacher-v1\',\r\n    \'HalfCheetah-v1\',\r\n    \'Swimmer-v1\',\r\n    \'Hopper-v1\',\r\n    \'Walker2d-v1\',\r\n    \'Ant-v1\',\r\n    \'Humanoid-v1\',\r\n    \'HumanoidStandup-v1\',\r\n]\r\n \r\nlogger = utils.logger.getLogger(\'actor_learner\')\r\n\r\n\r\nclass ActorLearner(Process):\r\n    \r\n    def __init__(self, args):\r\n        super(ActorLearner, self).__init__()\r\n       \r\n        self.summ_base_dir = args.summ_base_dir\r\n        \r\n        self.local_step = 0\r\n        self.global_step = args.global_step\r\n        self.local_episode = 0\r\n        self.last_saving_step = 0\r\n\r\n        self.saver = None\r\n        self.actor_id = args.actor_id\r\n        self.alg_type = args.alg_type\r\n        self.use_monitor = args.use_monitor\r\n        self.max_local_steps = args.max_local_steps\r\n        self.optimizer_type = args.opt_type\r\n        self.optimizer_mode = args.opt_mode\r\n        self.num_actions = args.num_actions\r\n        self.initial_lr = args.initial_lr\r\n        self.lr_annealing_steps = args.lr_annealing_steps\r\n        self.num_actor_learners = args.num_actor_learners\r\n        self.is_train = args.is_train\r\n        self.input_shape = args.input_shape\r\n        self.reward_clip_val = args.reward_clip_val\r\n        self.q_update_interval = args.q_update_interval\r\n        self.restore_checkpoint = args.restore_checkpoint\r\n        self.random_seed = args.random_seed\r\n        \r\n        # Shared mem vars\r\n        self.learning_vars = args.learning_vars\r\n            \r\n        if self.optimizer_mode == \'local\':\r\n            if self.optimizer_type == \'rmsprop\':\r\n                self.opt_st = np.ones(self.learning_vars.size, dtype=ctypes.c_float)\r\n            else:\r\n                self.opt_st = np.zeros(self.learning_vars.size, dtype=ctypes.c_float)\r\n        elif self.optimizer_mode == \'shared\':\r\n                self.opt_st = args.opt_state\r\n\r\n        # rmsprop/momentum\r\n        self.alpha = args.momentum\r\n        # adam\r\n        self.b1 = args.b1\r\n        self.b2 = args.b2\r\n        self.e = args.e\r\n        \r\n        if args.env == \'GYM\':\r\n            from environments.atari_environment import AtariEnvironment\r\n            self.emulator = AtariEnvironment(\r\n                args.game,\r\n                self.random_seed,\r\n                args.visualize,\r\n                use_rgb=args.use_rgb,\r\n                frame_skip=args.frame_skip,\r\n                agent_history_length=args.history_length,\r\n                max_episode_steps=args.max_episode_steps,\r\n                single_life_episodes=args.single_life_episodes,\r\n            )\r\n        elif args.env == \'ALE\':\r\n            from environments.emulator import Emulator\r\n            self.emulator = Emulator(\r\n                args.rom_path, \r\n                args.game, \r\n                args.visualize, \r\n                self.actor_id,\r\n                self.random_seed,\r\n                args.single_life_episodes)\r\n        else:\r\n            raise Exception(\'Invalid environment `{}`\'.format(args.env))\r\n            \r\n        self.grads_update_steps = args.grads_update_steps\r\n        self.max_global_steps = args.max_global_steps\r\n        self.gamma = args.gamma\r\n\r\n        self.rescale_rewards = args.rescale_rewards\r\n        self.max_achieved_reward = -float(\'inf\')\r\n        if self.rescale_rewards:\r\n            self.thread_max_reward = 1.0\r\n\r\n        # Barrier to synchronize all actors after initialization is done\r\n        self.barrier = args.barrier\r\n        self.game = args.game\r\n\r\n        # Initizlize Tensorboard summaries\r\n        self.summary_ph, self.update_ops, self.summary_ops = self.setup_summaries()\r\n        self.summary_op = tf.summary.merge_all()\r\n\r\n\r\n    def compute_targets(self, rewards, R):\r\n        size = len(rewards)\r\n        y_batch = list()\r\n\r\n        for i in reversed(xrange(size)):\r\n            R = rewards[i] + self.gamma * R\r\n            y_batch.append(R)\r\n\r\n        y_batch.reverse()\r\n        return y_batch\r\n\r\n\r\n    def reset_hidden_state(self):\r\n        """"""\r\n        Override in subclass if needed\r\n        """"""\r\n        pass\r\n\r\n\r\n    def is_master(self):\r\n        return self.actor_id == 0\r\n\r\n\r\n    def test(self, num_episodes=100):\r\n        """"""\r\n        Run test monitor for `num_episodes`\r\n        """"""\r\n        rewards = list()\r\n        for episode in range(num_episodes):\r\n            s = self.emulator.get_initial_state()\r\n            self.reset_hidden_state()\r\n            total_episode_reward = 0\r\n            episode_over = False\r\n\r\n            while not episode_over:\r\n                a = self.choose_next_action(s)[0]\r\n                s, reward, episode_over = self.emulator.next(a)\r\n\r\n                total_episode_reward += reward\r\n\r\n            else:\r\n                rewards.append(total_episode_reward)\r\n                logger.info(""EPISODE {0} -- REWARD: {1}, RUNNING AVG: {2:.1f}\xc2\xb1{3:.1f}, BEST: {4}"".format(\r\n                    episode,\r\n                    total_episode_reward,\r\n                    np.array(rewards).mean(),\r\n                    2*np.array(rewards).std(),\r\n                    max(rewards),\r\n                ))\r\n\r\n\r\n    def synchronize_workers(self):\r\n        if self.is_master():\r\n            # Initialize network parameters\r\n            g_step = checkpoint_utils.restore_vars(self.saver, self.session, self.game, self.alg_type, self.max_local_steps, self.restore_checkpoint)\r\n            self.global_step.val.value = g_step\r\n            self.last_saving_step = g_step   \r\n            logger.debug(""T{}: Initializing shared memory..."".format(self.actor_id))\r\n            self.update_shared_memory()\r\n\r\n        # Wait until actor 0 finishes initializing shared memory\r\n        self.barrier.wait()\r\n\r\n        if not self.is_master():\r\n            logger.debug(""T{}: Syncing with shared memory..."".format(self.actor_id))\r\n            self.sync_net_with_shared_memory(self.local_network, self.learning_vars)  \r\n            if hasattr(self, \'target_network\'):\r\n                self.sync_net_with_shared_memory(self.target_network, self.learning_vars)\r\n            elif hasattr(self, \'batch_network\'):\r\n                self.sync_net_with_shared_memory(self.batch_network, self.learning_vars)\r\n\r\n        # Ensure we don\'t add any more nodes to the graph\r\n        self.session.graph.finalize()\r\n        self.start_time = time.time()\r\n\r\n\r\n    def get_gpu_options(self):\r\n        return tf.GPUOptions(allow_growth=True)\r\n\r\n\r\n    @contextmanager\r\n    def monitored_environment(self):\r\n        if self.use_monitor:\r\n            self.log_dir = tempfile.mkdtemp()\r\n            self.emulator.env = gym.wrappers.Monitor(self.emulator.env, self.log_dir)\r\n\r\n        yield\r\n        self.emulator.env.close()\r\n\r\n\r\n    def run(self):\r\n        #set random seeds so we can reproduce runs\r\n        np.random.seed(self.random_seed)\r\n        tf.set_random_seed(self.random_seed)\r\n\r\n        num_cpus = multiprocessing.cpu_count()\r\n        self.supervisor = tf.train.Supervisor(\r\n            init_op=tf.global_variables_initializer(),\r\n            local_init_op=tf.global_variables_initializer(),\r\n            logdir=self.summ_base_dir,\r\n            saver=self.saver,\r\n            summary_op=None)\r\n        session_context = self.supervisor.managed_session(config=tf.ConfigProto(\r\n            intra_op_parallelism_threads=num_cpus,\r\n            inter_op_parallelism_threads=num_cpus,\r\n            gpu_options=self.get_gpu_options(),\r\n            allow_soft_placement=True))\r\n\r\n        with self.monitored_environment(), session_context as self.session:\r\n            self.synchronize_workers()\r\n\r\n            if self.is_train:\r\n                self.train()\r\n            else:\r\n                self.test()\r\n\r\n\r\n    def save_vars(self):\r\n        if self.is_master() and self.global_step.value()-self.last_saving_step >= CHECKPOINT_INTERVAL:\r\n            self.last_saving_step = self.global_step.value()\r\n            checkpoint_utils.save_vars(self.saver, self.session, self.game, self.alg_type, self.max_local_steps, self.last_saving_step) \r\n    \r\n    def update_shared_memory(self):\r\n        # Initialize shared memory with tensorflow var values\r\n        params = self.session.run(self.local_network.params)\r\n\r\n        # Merge all param matrices into a single 1-D array\r\n        params = np.hstack([p.reshape(-1) for p in params])\r\n        np.frombuffer(self.learning_vars.vars, ctypes.c_float)[:] = params\r\n        # if hasattr(self, \'target_vars\'):\r\n            # target_params = self.session.run(self.target_network.params)\r\n            # np.frombuffer(self.target_vars.vars, ctypes.c_float)[:] = params\r\n                \r\n    \r\n    @only_on_train(return_val=0.0)\r\n    def decay_lr(self):\r\n        if self.global_step.value() <= self.lr_annealing_steps:            \r\n            return self.initial_lr - (self.global_step.value() * self.initial_lr / self.lr_annealing_steps)\r\n        else:\r\n            return 0.0\r\n\r\n    def apply_gradients_to_shared_memory_vars(self, grads):\r\n        self._apply_gradients_to_shared_memory_vars(grads, self.learning_vars)\r\n\r\n\r\n    @only_on_train()\r\n    def _apply_gradients_to_shared_memory_vars(self, grads, shared_vars):\r\n            opt_st = self.opt_st\r\n            self.flat_grads = np.empty(shared_vars.size, dtype=ctypes.c_float)\r\n\r\n            #Flatten grads\r\n            offset = 0\r\n            for g in grads:\r\n                self.flat_grads[offset:offset + g.size] = g.reshape(-1)\r\n                offset += g.size\r\n            g = self.flat_grads\r\n\r\n            shared_vars.step.value += 1\r\n            T = shared_vars.step.value\r\n\r\n            if self.optimizer_type == ""adam"" and self.optimizer_mode == ""shared"":\r\n                p = np.frombuffer(shared_vars.vars, ctypes.c_float)\r\n                p_size = shared_vars.size\r\n                m = np.frombuffer(opt_st.ms, ctypes.c_float)\r\n                v = np.frombuffer(opt_st.vs, ctypes.c_float)\r\n                lr =  1.0 * opt_st.lr.value * (1 - self.b2**T)**0.5 / (1 - self.b1**T) \r\n                \r\n                apply_grads_adam(m, v, g, p, p_size, lr, self.b1, self.b2, self.e)\r\n\r\n            elif self.optimizer_type == ""adamax"" and self.optimizer_mode == ""shared"":\r\n                beta_1 = .9\r\n                beta_2 = .999\r\n                lr = opt_st.lr.value\r\n\r\n                p = np.frombuffer(shared_vars.vars, ctypes.c_float)\r\n                p_size = shared_vars.size\r\n                m = np.frombuffer(opt_st.ms, ctypes.c_float)\r\n                u = np.frombuffer(opt_st.vs, ctypes.c_float)\r\n\r\n                apply_grads_adamax(m, u, g, p, p_size, lr, beta_1, beta_2, T)\r\n                    \r\n            else: #local or shared rmsprop/momentum\r\n                lr = self.decay_lr()\r\n                if (self.optimizer_mode == ""local""):\r\n                    m = opt_st\r\n                else: #shared\r\n                    m = np.frombuffer(opt_st.vars, ctypes.c_float)\r\n                \r\n                p = np.frombuffer(shared_vars.vars, ctypes.c_float)\r\n                p_size = shared_vars.size\r\n                _type = 0 if self.optimizer_type == ""momentum"" else 1\r\n                \r\n                apply_grads_mom_rmsprop(m, g, p, p_size, _type, lr, self.alpha, self.e)\r\n\r\n    def rescale_reward(self, reward):\r\n        if self.rescale_rewards:\r\n            # Rescale immediate reward by max reward encountered thus far\r\n            if np.abs(reward) > self.thread_max_reward:\r\n                self.thread_max_reward = np.abs(reward)\r\n            return reward/self.thread_max_reward\r\n        else:\r\n            # Clip immediate reward\r\n            return np.sign(reward) * np.minimum(self.reward_clip_val, np.abs(reward))\r\n            \r\n\r\n    def assign_vars(self, dest_net, params):\r\n        feed_dict = {}\r\n        offset = 0\r\n\r\n        for i, var in enumerate(dest_net.params):\r\n            shape = var.get_shape().as_list()\r\n            size = np.prod(shape)\r\n            if type(params) == list:\r\n                feed_dict[dest_net.params_ph[i]] = params[i]\r\n            else:\r\n                feed_dict[dest_net.params_ph[i]] = \\\r\n                    params[offset:offset+size].reshape(shape)\r\n            offset += size\r\n        \r\n        self.session.run(dest_net.sync_with_shared_memory, \r\n            feed_dict=feed_dict)\r\n\r\n\r\n    def sync_net_with_shared_memory(self, dest_net, shared_mem_vars):\r\n        feed_dict = {}\r\n        offset = 0\r\n        params = np.frombuffer(shared_mem_vars.vars, \r\n                                  ctypes.c_float)\r\n        for i in xrange(len(dest_net.params)):\r\n            shape = shared_mem_vars.var_shapes[i]\r\n            size = np.prod(shape)\r\n            feed_dict[dest_net.params_ph[i]] = \\\r\n                    params[offset:offset+size].reshape(shape)\r\n            offset += size\r\n        \r\n        self.session.run(dest_net.sync_with_shared_memory, \r\n            feed_dict=feed_dict)\r\n\r\n    \r\n    def _get_summary_vars(self):\r\n        episode_reward = tf.Variable(0., name=\'episode_reward\')\r\n        s1 = tf.summary.scalar(\'Episode_Reward_{}\'.format(self.actor_id), episode_reward)\r\n\r\n        mean_value = tf.Variable(0., name=\'mean_value\')\r\n        s2 = tf.summary.scalar(\'Mean_Value_{}\'.format(self.actor_id), mean_value)\r\n\r\n        mean_entropy = tf.Variable(0., name=\'mean_entropy\')\r\n        s3 = tf.summary.scalar(\'Mean_Entropy_{}\'.format(self.actor_id), mean_entropy)\r\n\r\n        return [episode_reward, mean_value, mean_entropy]\r\n\r\n\r\n    def setup_summaries(self):\r\n        summary_vars = self._get_summary_vars()\r\n\r\n        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\r\n        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\r\n        with tf.control_dependencies(update_ops):\r\n            summary_ops = tf.summary.merge_all()\r\n\r\n        return summary_placeholders, update_ops, summary_ops\r\n\r\n\r\n    @only_on_train()\r\n    def log_summary(self, *args):\r\n        if self.is_master():\r\n            feed_dict = {ph: val for ph, val in zip(self.summary_ph, args)}\r\n            summaries = self.session.run(self.update_ops + [self.summary_op], feed_dict=feed_dict)[-1]\r\n            self.supervisor.summary_computed(self.session, summaries, global_step=self.global_step.value())\r\n    \r\n\r\n'"
algorithms/cem_actor_learner.py,1,"b""# -*- coding: utf-8 -*-\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\nfrom networks.policy_v_network import PolicyNetwork\nfrom policy_based_actor_learner import BaseA3CLearner\n\n\nlogger = utils.logger.getLogger('cross_entropy_actor_learner')\n\n\nclass CEMLearner(BaseA3CLearner):\n\t'''\n\tImplementation of Cross-Entropy Method, Useful as a baseline for simple environments\n\t'''\n\tdef __init__(self, args):\n\t\tsuper(CEMLearner, self).__init__(args)\n\n\t\tpolicy_conf = {'name': 'local_learning_{}'.format(self.actor_id),\n\t\t\t\t\t   'input_shape': self.input_shape,\n\t\t\t\t\t   'num_act': self.num_actions,\n\t\t\t\t\t   'args': args}\n\n\t\tself.local_network = args.network(policy_conf)\n\t\tself.num_params = np.sum([\n\t\t\tnp.prod(v.get_shape().as_list())\n\t\t\tfor v in self.local_network.params])\n\n\t\tlogger.info('Parameter count: {}'.format(self.num_params))\n\t\tself.mu = np.zeros(self.num_params)\n\t\tself.sigma = np.ones(self.num_params)\n\t\tself.num_samples = args.episodes_per_batch\n\t\tself.num_epochs = args.num_epochs\n\n\t\tif self.is_master():\n\t\t\tvar_list = self.local_network.params\n\t\t\tself.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\n                                        keep_checkpoint_every_n_hours=2)\n\n\n\tdef choose_next_action(self, state):\n\t\treturn self.local_network.get_action(self.session, state)\n\n\n\tdef sample_theta(self, N):\n\t\treturn self.mu + np.random.randn(N, self.num_params)*self.sigma\n\n\n\tdef update_sample_distribution(self, population, rewards, keep_ratio=0.25, noise=0.01):\n\t\tnum_to_keep = int(keep_ratio * len(population))\n\t\telite = np.array(rewards).argsort()[-num_to_keep:]\n\n\t\tself.mu = population[elite].mean(axis=0)\n\t\tself.sigma = population[elite].std(axis=0) + noise\n\n\t\treturn np.array(rewards)[elite].mean(), population[elite]\n\n\n\tdef train(self):\n\t\tconsecutive_successes = 0\n\n\t\tfor epoch in xrange(self.num_epochs):\n\t\t\tepisode_rewards = list()\n\t\t\tpopulation = self.sample_theta(self.num_samples)\n\n\t\t\tfor theta in population:\n\t\t\t\tself.assign_vars(self.local_network, theta)\n\t\t\t\ts = self.emulator.get_initial_state()\n\n\t\t\t\ttotal_episode_reward = 0.0\n\t\t\t\tepisode_over = False\n\n\t\t\t\twhile not episode_over:\n\t\t\t\t\ta, pi = self.choose_next_action(s)\n\t\t\t\t\ts, reward, episode_over = self.emulator.next(a)\n\t\t\t\t\ttotal_episode_reward += reward\n\n\t\t\t\tepisode_rewards.append(total_episode_reward)\n\n\t\t\tpopulation_mean_reward = np.array(episode_rewards).mean()\n\t\t\telite_mean_reward, elite_set = self.update_sample_distribution(\n\t\t\t\tpopulation, episode_rewards, noise=.1/(1+epoch))\n\t\t\tlogger.info('Epoch {} / Population Mean {} / Elite Mean {}'.format(\n\t\t\t\tepoch+1, population_mean_reward, elite_mean_reward))\n\n\t\t\tif self.emulator.env.spec.reward_threshold and elite_mean_reward > self.emulator.env.spec.reward_threshold:\n\t\t\t\tconsecutive_successes += 1\n\t\t\telse:\n\t\t\t\tconsecutive_successes = 0\n\n\t\t\t#if the elite set is consistently good enough we'll stop updating the parameters\n\t\t\tif consecutive_successes > 10:\n\t\t\t\tlogger.info('switching to testing mode...')\n\t\t\t\tepisode_rewards = list()\n\n\t\t\t\tfor _ in xrange(200):\n\t\t\t\t\ti = np.random.choice(elite_set.shape[0])\n\t\t\t\t\tself.assign_vars(self.local_network, elite_set[i])\n\t\t\t\t\ts = self.emulator.get_initial_state()\n\t\t\t\t\ttotal_episode_reward = 0.0\n\t\t\t\t\tepisode_over = False\n\n\t\t\t\t\twhile not episode_over:\n\t\t\t\t\t\ta, pi = self.choose_next_action(s)\n\t\t\t\t\t\ts, reward, episode_over = self.emulator.next(a)\n\t\t\t\t\t\ttotal_episode_reward += reward\n\n\t\t\t\t\tepisode_rewards.append(total_episode_reward)\n\t\t\t\t\n\t\t\t\tlogger.info('Average Test Reward: {}'.format(np.array(episode_rewards).mean()))\n\t\t\t\treturn\n\n\n\n\n\t\t"""
algorithms/intrinsic_motivation_actor_learner.py,16,"b'# -*- encoding: utf-8 -*-\nimport time\nimport cPickle\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\n\nfrom skimage.transform import resize\nfrom collections import deque\nfrom utils import checkpoint_utils\nfrom actor_learner import ONE_LIFE_GAMES\nfrom utils.decorators import Experimental\nfrom utils.fast_cts import CTSDensityModel\nfrom utils.replay_memory import ReplayMemory\nfrom policy_based_actor_learner import A3CLearner, A3CLSTMLearner\nfrom value_based_actor_learner import ValueBasedLearner\n\n\nlogger = utils.logger.getLogger(\'intrinsic_motivation_actor_learner\')\n\n\nclass PixelCNNDensityModel(object):\n    pass\n\n\nclass PerPixelDensityModel(object):\n    """"""\n    Calculates image probability according to per-pixel counts: P(X) = \xe2\x88\x8f p(x_ij)\n    Mostly here for debugging purposes as CTSDensityModel is much more expressive\n    """"""\n    def __init__(self, height=42, width=42, num_bins=8, beta=0.05):\n        self.counts = np.zeros((width, height, num_bins))\n        self.height = height\n        self.width = width\n        self.beta = beta\n        self.num_bins = num_bins\n\n    def update(self, obs):\n        obs = resize(obs, (self.height, self.width), preserve_range=True)\n        obs = np.floor((obs*self.num_bins)).astype(np.int32)\n        \n        log_prob, log_recoding_prob = self._update(obs)\n        return self.exploration_bonus(log_prob, log_recoding_prob)\n\n    def _update(self, obs):\n        log_prob = 0.0\n        log_recoding_prob = 0.0\n\n        for i in range(self.height):\n            for j in range(self.height):\n                self.counts[i, j, obs[i, j]] += 1\n\n                bin_count = self.counts[i, j, obs[i, j]]\n                pixel_mass = self.counts[i, j].sum()\n                log_prob += np.log(bin_count / pixel_mass)\n                log_recoding_prob += np.log((bin_count + 1) / (pixel_mass + 1))\n        \n        return log_prob, log_recoding_prob\n\n    def exploration_bonus(self, log_prob, log_recoding_prob):\n        recoding_prob = np.exp(log_recoding_prob)\n        prob_ratio = np.exp(log_recoding_prob - log_prob)\n\n        pseudocount = (1 - recoding_prob) / np.maximum(prob_ratio - 1, 1e-10)\n        return self.beta / np.sqrt(pseudocount + .01)\n\n    def get_state(self):\n        return self.num_bins, self.height, self.width, self.beta, self.counts\n\n    def set_state(self, state):\n        self.num_bins, self.height, self.width, self.beta, self.counts = state\n\n\nclass DensityModelMixin(object):\n    """"""\n    Mixin to provide initialization and synchronization methods for density models\n    """"""\n    def _init_density_model(self, args):\n        self.density_model_update_steps = 20*args.q_target_update_steps\n        self.density_model_update_flags = args.density_model_update_flags\n\n        model_args = {\n            \'height\': args.cts_rescale_dim,\n            \'width\': args.cts_rescale_dim,\n            \'num_bins\': args.cts_bins,\n            \'beta\': args.cts_beta\n        }\n        if args.density_model == \'cts\':\n            self.density_model = CTSDensityModel(**model_args)\n        else:\n            self.density_model = PerPixelDensityModel(**model_args)\n\n\n    def write_density_model(self):\n        logger.info(\'T{} Writing Pickled Density Model to File...\'.format(self.actor_id))\n        raw_data = cPickle.dumps(self.density_model.get_state(), protocol=2)\n        with self.barrier.counter.lock, open(\'/tmp/density_model.pkl\', \'wb\') as f:\n            f.write(raw_data)\n\n        for i in xrange(len(self.density_model_update_flags.updated)):\n            self.density_model_update_flags.updated[i] = 1\n\n    def read_density_model(self):\n        logger.info(\'T{} Synchronizing Density Model...\'.format(self.actor_id))\n        with self.barrier.counter.lock, open(\'/tmp/density_model.pkl\', \'rb\') as f:\n            raw_data = f.read()\n\n        self.density_model.set_state(cPickle.loads(raw_data))\n\n\nclass A3CDensityModelMixin(DensityModelMixin):\n    """"""\n    Mixin to share _train method between A3C and A3C-LSTM models\n    """"""\n    def _train(self):\n        """""" Main actor learner loop for advantage actor critic learning. """"""\n        logger.debug(""Actor {} resuming at Step {}"".format(self.actor_id, \n            self.global_step.value()))\n        \n        bonuses = deque(maxlen=100)\n        while (self.global_step.value() < self.max_global_steps):\n            # Sync local learning net with shared mem\n            s = self.emulator.get_initial_state()\n            self.reset_hidden_state()\n            self.local_episode += 1\n            episode_over = False\n            total_episode_reward = 0.0\n            episode_start_step = self.local_step\n            \n            while not episode_over:\n                self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n                self.save_vars()\n\n                rewards = list()\n                states  = list()\n                actions = list()\n                values  = list()\n                local_step_start = self.local_step\n                self.set_local_lstm_state()\n\n                while self.local_step - local_step_start < self.max_local_steps and not episode_over:\n                    # Choose next action and execute it\n                    a, readout_v_t, readout_pi_t = self.choose_next_action(s)                    \n                    new_s, reward, episode_over = self.emulator.next(a)\n                    total_episode_reward += reward\n                    \n                    # Update density model\n                    current_frame = new_s[...,-1]\n                    bonus = self.density_model.update(current_frame)\n                    bonuses.append(bonus)\n\n                    if self.is_master() and (self.local_step % 400 == 0):\n                        bonus_array = np.array(bonuses)\n                        logger.debug(\'\xcf\x80_a={:.4f} / V={:.4f} / Mean Bonus={:.4f} / Max Bonus={:.4f}\'.format(\n                            readout_pi_t[a.argmax()], readout_v_t, bonus_array.mean(), bonus_array.max()))\n\n                    # Rescale or clip immediate reward\n                    reward = self.rescale_reward(self.rescale_reward(reward) + bonus)\n                    rewards.append(reward)\n                    states.append(s)\n                    actions.append(a)\n                    values.append(readout_v_t)\n\n                    s = new_s\n                    self.local_step += 1\n\n                    global_step, _ = self.global_step.increment()\n                    if global_step % self.density_model_update_steps == 0:\n                        self.write_density_model()\n                    if self.density_model_update_flags.updated[self.actor_id] == 1:\n                        self.read_density_model()\n                        self.density_model_update_flags.updated[self.actor_id] = 0  \n                \n                next_val = self.bootstrap_value(new_s, episode_over)\n                advantages = self.compute_gae(rewards, values, next_val)\n                targets = self.compute_targets(rewards, next_val)\n                # Compute gradients on the local policy/V network and apply them to shared memory \n                entropy = self.apply_update(states, actions, targets, advantages)\n\n            elapsed_time = time.time() - self.start_time\n            steps_per_sec = self.global_step.value() / elapsed_time\n            perf = ""{:.0f}"".format(steps_per_sec)\n            logger.info(""T{} / EPISODE {} / STEP {}k / REWARD {} / {} STEPS/s"".format(\n                self.actor_id,\n                self.local_episode,\n                self.global_step.value()/1000,\n                total_episode_reward,\n                perf))\n\n            self.log_summary(total_episode_reward, np.array(values).mean(), entropy)\n\n\n@Experimental\nclass PseudoCountA3CLearner(A3CLearner, A3CDensityModelMixin):\n    """"""\n    Attempt at replicating the A3C+ model from the paper \'Unifying Count-Based Exploration and Intrinsic Motivation\' (https://arxiv.org/abs/1606.01868)\n    """"""\n    def __init__(self, args):\n        super(PseudoCountA3CLearner, self).__init__(args)\n        self._init_density_model(args)\n\n    def train(self):\n        self._train()\n\n\n@Experimental\nclass PseudoCountA3CLSTMLearner(A3CLSTMLearner, A3CDensityModelMixin):\n    def __init__(self, args):\n        super(PseudoCountA3CLSTMLearner, self).__init__(args)\n        self._init_density_model(args)\n\n    def train(self):\n        self._train()\n\n\nclass PseudoCountQLearner(ValueBasedLearner, DensityModelMixin):\n    """"""\n    Based on DQN+CTS model from the paper \'Unifying Count-Based Exploration and Intrinsic Motivation\' (https://arxiv.org/abs/1606.01868)\n    Presently the implementation differs from the paper in that the novelty bonuses are computed online rather than by computing the\n    prediction gains after the model has been updated with all frames from the episode. Async training with different final epsilon values\n    tends to produce better results than just using a single actor-learner.\n    """"""\n    def __init__(self, args):\n        self.args = args\n        super(PseudoCountQLearner, self).__init__(args)\n\n        self.cts_eta = args.cts_eta\n        self.cts_beta = args.cts_beta\n        self.batch_size = args.batch_update_size\n        self.replay_memory = ReplayMemory(\n            args.replay_size,\n            self.local_network.get_input_shape(),\n            self.num_actions)\n\n        self._init_density_model(args)\n        self._double_dqn_op()\n\n\n    def generate_final_epsilon(self):\n        if self.num_actor_learners == 1:\n            return self.args.final_epsilon\n        else:\n            return super(PseudoCountQLearner, self).generate_final_epsilon()\n\n\n    def _get_summary_vars(self):\n        q_vars = super(PseudoCountQLearner, self)._get_summary_vars()\n\n        bonus_q05 = tf.Variable(0., name=\'novelty_bonus_q05\')\n        s1 = tf.summary.scalar(\'Novelty_Bonus_q05_{}\'.format(self.actor_id), bonus_q05)\n        bonus_q50 = tf.Variable(0., name=\'novelty_bonus_q50\')\n        s2 = tf.summary.scalar(\'Novelty_Bonus_q50_{}\'.format(self.actor_id), bonus_q50)\n        bonus_q95 = tf.Variable(0., name=\'novelty_bonus_q95\')\n        s3 = tf.summary.scalar(\'Novelty_Bonus_q95_{}\'.format(self.actor_id), bonus_q95)\n\n        augmented_reward = tf.Variable(0., name=\'augmented_episode_reward\')\n        s4 = tf.summary.scalar(\'Augmented_Episode_Reward_{}\'.format(self.actor_id), augmented_reward)       \n\n        return q_vars + [bonus_q05, bonus_q50, bonus_q95, augmented_reward]\n\n\n    #TODO: refactor to make this cleaner\n    def prepare_state(self, state, total_episode_reward, steps_at_last_reward,\n                      ep_t, episode_ave_max_q, episode_over, bonuses, total_augmented_reward):\n        # Start a new game on reaching terminal state\n        if episode_over:\n            T = self.global_step.value() * self.max_local_steps\n            t = self.local_step\n            e_prog = float(t)/self.epsilon_annealing_steps\n            episode_ave_max_q = episode_ave_max_q/float(ep_t)\n            s1 = ""Q_MAX {0:.4f}"".format(episode_ave_max_q)\n            s2 = ""EPS {0:.4f}"".format(self.epsilon)\n\n            self.scores.insert(0, total_episode_reward)\n            if len(self.scores) > 100:\n                self.scores.pop()\n\n            logger.info(\'T{0} / STEP {1} / REWARD {2} / {3} / {4}\'.format(\n                self.actor_id, T, total_episode_reward, s1, s2))\n            logger.info(\'ID: {0} -- RUNNING AVG: {1:.0f} \xc2\xb1 {2:.0f} -- BEST: {3:.0f}\'.format(\n                self.actor_id,\n                np.array(self.scores).mean(),\n                2*np.array(self.scores).std(),\n                max(self.scores),\n            ))\n\n            self.log_summary(\n                total_episode_reward,\n                episode_ave_max_q,\n                self.epsilon,\n                np.percentile(bonuses, 5),\n                np.percentile(bonuses, 50),\n                np.percentile(bonuses, 95),\n                total_augmented_reward,\n            )\n\n            state = self.emulator.get_initial_state()\n            ep_t = 0\n            total_episode_reward = 0\n            episode_ave_max_q = 0\n            episode_over = False\n\n        return (\n            state,\n            total_episode_reward,\n            steps_at_last_reward,\n            ep_t,\n            episode_ave_max_q,\n            episode_over\n        )\n\n\n    def _double_dqn_op(self):\n        q_local_action = tf.cast(tf.argmax(\n            self.local_network.output_layer, axis=1), tf.int32)\n        q_target_max = utils.ops.slice_2d(\n            self.target_network.output_layer,\n            tf.range(0, self.batch_size),\n            q_local_action,\n        )\n        self.one_step_reward = tf.placeholder(tf.float32, self.batch_size, name=\'one_step_reward\')\n        self.is_terminal = tf.placeholder(tf.bool, self.batch_size, name=\'is_terminal\')\n\n        self.y_target = self.one_step_reward + self.cts_eta*self.gamma*q_target_max \\\n            * (1 - tf.cast(self.is_terminal, tf.float32))\n\n        self.double_dqn_loss = self.local_network._value_function_loss(\n            self.local_network.q_selected_action\n            - tf.stop_gradient(self.y_target))\n\n        self.double_dqn_grads = tf.gradients(self.double_dqn_loss, self.local_network.params)\n\n\n    # def batch_update(self):\n    #     if len(self.replay_memory) < self.replay_memory.maxlen//10:\n    #         return\n\n    #     s_i, a_i, r_i, s_f, is_terminal = self.replay_memory.sample_batch(self.batch_size)\n\n    #     feed_dict={\n    #         self.one_step_reward: r_i,\n    #         self.target_network.input_ph: s_f,\n    #         self.local_network.input_ph: np.vstack([s_i, s_f]),\n    #         self.local_network.selected_action_ph: np.vstack([a_i, a_i]),\n    #         self.is_terminal: is_terminal\n    #     }\n    #     grads = self.session.run(self.double_dqn_grads, feed_dict=feed_dict)\n    #     self.apply_gradients_to_shared_memory_vars(grads)\n\n\n    def batch_update(self):\n        if len(self.replay_memory) < self.replay_memory.maxlen//10:\n            return\n\n        s_i, a_i, r_i, s_f, is_terminal = self.replay_memory.sample_batch(self.batch_size)\n\n        feed_dict={\n            self.local_network.input_ph: s_f,\n            self.target_network.input_ph: s_f,\n            self.is_terminal: is_terminal,\n            self.one_step_reward: r_i,\n        }\n        y_target = self.session.run(self.y_target, feed_dict=feed_dict)\n\n        feed_dict={\n            self.local_network.input_ph: s_i,\n            self.local_network.target_ph: y_target,\n            self.local_network.selected_action_ph: a_i\n        }\n        grads = self.session.run(self.local_network.get_gradients,\n                                 feed_dict=feed_dict)\n        self.apply_gradients_to_shared_memory_vars(grads)\n\n\n    def train(self):\n        """""" Main actor learner loop for n-step Q learning. """"""\n        logger.debug(""Actor {} resuming at Step {}, {}"".format(self.actor_id, \n            self.global_step.value(), time.ctime()))\n\n        s = self.emulator.get_initial_state()\n        \n        s_batch = list()\n        a_batch = list()\n        y_batch = list()\n        bonuses = deque(maxlen=1000)\n        episode_over = False\n        \n        t0 = time.time()\n        global_steps_at_last_record = self.global_step.value()\n        while (self.global_step.value() < self.max_global_steps):\n            # # Sync local learning net with shared mem\n            # self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n            # self.save_vars()\n            rewards =      list()\n            states =       list()\n            actions =      list()\n            max_q_values = list()\n            local_step_start = self.local_step\n            total_episode_reward = 0\n            total_augmented_reward = 0\n            episode_ave_max_q = 0\n            ep_t = 0\n\n            while not episode_over:\n                # Sync local learning net with shared mem\n                self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n                self.save_vars()\n\n                # Choose next action and execute it\n                a, q_values = self.choose_next_action(s)\n\n                new_s, reward, episode_over = self.emulator.next(a)\n                total_episode_reward += reward\n                max_q = np.max(q_values)\n\n                current_frame = new_s[...,-1]\n                bonus = self.density_model.update(current_frame)\n                bonuses.append(bonus)\n\n                # Rescale or clip immediate reward\n                reward = self.rescale_reward(self.rescale_reward(reward) + bonus)\n                total_augmented_reward += reward\n                ep_t += 1\n                \n                rewards.append(reward)\n                states.append(s)\n                actions.append(a)\n                max_q_values.append(max_q)\n                \n                s = new_s\n                self.local_step += 1\n                episode_ave_max_q += max_q\n                \n                global_step, _ = self.global_step.increment()\n\n                if global_step % self.q_target_update_steps == 0:\n                    self.update_target()\n                if global_step % self.density_model_update_steps == 0:\n                    self.write_density_model()\n\n                # Sync local tensorflow target network params with shared target network params\n                if self.target_update_flags.updated[self.actor_id] == 1:\n                    self.sync_net_with_shared_memory(self.target_network, self.target_vars)\n                    self.target_update_flags.updated[self.actor_id] = 0\n                if self.density_model_update_flags.updated[self.actor_id] == 1:\n                    self.read_density_model()\n                    self.density_model_update_flags.updated[self.actor_id] = 0\n\n                if self.local_step % self.q_update_interval == 0:\n                    self.batch_update()\n\n                if self.is_master() and (self.local_step % 500 == 0):\n                    bonus_array = np.array(bonuses)\n                    steps = global_step - global_steps_at_last_record\n                    global_steps_at_last_record = global_step\n\n                    logger.debug(\'Mean Bonus={:.4f} / Max Bonus={:.4f} / STEPS/s={}\'.format(\n                        bonus_array.mean(), bonus_array.max(), steps/float(time.time()-t0)))\n                    t0 = time.time()\n\n\n            else:\n                #compute monte carlo return\n                mc_returns = np.zeros((len(rewards),), dtype=np.float32)\n                running_total = 0.0\n                for i, r in enumerate(reversed(rewards)):\n                    running_total = r + self.gamma*running_total\n                    mc_returns[len(rewards)-i-1] = running_total\n\n                mixed_returns = self.cts_eta*np.asarray(rewards) + (1-self.cts_eta)*mc_returns\n\n                #update replay memory\n                states.append(new_s)\n                episode_length = len(rewards)\n                for i in range(episode_length):\n                    self.replay_memory.append(\n                        states[i],\n                        actions[i],\n                        mixed_returns[i],\n                        i+1 == episode_length)\n\n            s, total_episode_reward, _, ep_t, episode_ave_max_q, episode_over = \\\n                self.prepare_state(s, total_episode_reward, self.local_step, ep_t, episode_ave_max_q, episode_over, bonuses, total_augmented_reward)\n\n'"
algorithms/pgq_actor_learner.py,13,"b""# -*- coding: utf-8 -*-\nimport time\nimport ctypes\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\nfrom utils.hogupdatemv import copy\nfrom actor_learner import ONE_LIFE_GAMES\nfrom utils.decorators import Experimental\nfrom utils.replay_memory import ReplayMemory\nfrom networks.policy_v_network import PolicyValueNetwork\nfrom policy_based_actor_learner import BaseA3CLearner\n\n\nlogger = utils.logger.getLogger('pgq_actor_learner')\n\n\nclass BasePGQLearner(BaseA3CLearner):\n    def __init__(self, args):\n\n        super(BasePGQLearner, self).__init__(args)\n\n        self.q_update_counter = 0\n        self.replay_size = args.replay_size\n        self.pgq_fraction = args.pgq_fraction\n        self.batch_update_size = args.batch_update_size\n        scope_name = 'local_learning_{}'.format(self.actor_id)\n        conf_learning = {'name': scope_name,\n                         'input_shape': self.input_shape,\n                         'num_act': self.num_actions,\n                         'args': args}\n\n        with tf.device('/cpu:0'):\n            self.local_network = PolicyValueNetwork(conf_learning)\n        with tf.device('/gpu:0'), tf.variable_scope('', reuse=True):\n            self.batch_network = PolicyValueNetwork(conf_learning)\n            self._build_q_ops()\n\n        self.reset_hidden_state()\n        self.replay_memory = ReplayMemory(\n            self.replay_size,\n            self.local_network.get_input_shape(),\n            self.num_actions)\n            \n        if self.is_master():\n            var_list = self.local_network.params\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3, \n                                        keep_checkpoint_every_n_hours=2)\n\n\n    def _build_q_ops(self):\n        # pgq specific initialization\n        self.pgq_fraction = self.pgq_fraction\n        self.batch_size = self.batch_update_size\n        self.q_tilde = self.batch_network.beta * (\n            self.batch_network.log_output_layer_pi\n            + tf.expand_dims(self.batch_network.output_layer_entropy, 1)\n        ) + self.batch_network.output_layer_v\n\n        self.Qi, self.Qi_plus_1 = tf.split(axis=0, num_or_size_splits=2, value=self.q_tilde)\n        self.V, _ = tf.split(axis=0, num_or_size_splits=2, value=self.batch_network.output_layer_v)\n        self.log_pi, _ = tf.split(axis=0, num_or_size_splits=2, value=tf.expand_dims(self.batch_network.log_output_selected_action, 1))\n        self.R = tf.placeholder('float32', [None], name='1-step_reward')\n\n        self.terminal_indicator = tf.placeholder(tf.float32, [None], name='terminal_indicator')\n        self.max_TQ = self.gamma*tf.reduce_max(self.Qi_plus_1, 1) * (1 - self.terminal_indicator)\n        self.Q_a = tf.reduce_sum(self.Qi * tf.split(axis=0, num_or_size_splits=2, value=self.batch_network.selected_action_ph)[0], 1)\n\n        self.q_objective = - self.pgq_fraction * tf.reduce_mean(tf.stop_gradient(self.R + self.max_TQ - self.Q_a) * (0.5 * self.V[:, 0] + self.log_pi[:, 0]))\n\n        self.V_params = self.batch_network.params\n        self.q_gradients = tf.gradients(self.q_objective, self.V_params)\n        self.q_gradients = self.batch_network._clip_grads(self.q_gradients)\n\n\n    def batch_q_update(self):\n        if len(self.replay_memory) < self.replay_memory.maxlen//10:\n            return\n\n        s_i, a_i, r_i, s_f, is_terminal = self.replay_memory.sample_batch(self.batch_size)\n\n        batch_grads = self.session.run(\n            self.q_gradients,\n            feed_dict={\n                self.R: r_i,\n                self.batch_network.selected_action_ph: np.vstack([a_i, a_i]),\n                self.batch_network.input_ph: np.vstack([s_i, s_f]),\n                self.terminal_indicator: is_terminal.astype(np.int),\n            }\n        )\n        self.apply_gradients_to_shared_memory_vars(batch_grads)\n\n\nclass PGQLearner(BasePGQLearner):\n    def choose_next_action(self, state):\n        network_output_v, network_output_pi = self.session.run(\n                [self.local_network.output_layer_v,\n                 self.local_network.output_layer_pi], \n                feed_dict={self.local_network.input_ph: [state]})\n\n        network_output_pi = network_output_pi.reshape(-1)\n        network_output_v = np.asscalar(network_output_v)\n\n        action_index = self.sample_policy_action(network_output_pi)\n        new_action = np.zeros([self.num_actions])\n        new_action[action_index] = 1\n\n        return new_action, network_output_v, network_output_pi\n\n\n    def apply_update(self, states, actions, targets, advantages):\n        feed_dict={\n            self.local_network.input_ph: states,\n            self.local_network.selected_action_ph: actions,\n            self.local_network.critic_target_ph: targets,\n            self.local_network.adv_actor_ph: advantages,\n        }\n        grads, entropy = self.session.run(\n            [self.local_network.get_gradients, self.local_network.entropy],\n            feed_dict=feed_dict)\n        self.apply_gradients_to_shared_memory_vars(grads)\n\n        self.q_update_counter += 1\n        if self.q_update_counter % self.q_update_interval == 0:\n            self.batch_q_update()\n\n        return entropy\n\n"""
algorithms/policy_based_actor_learner.py,3,"b'# -*- encoding: utf-8 -*-\nimport time\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\nfrom collections import deque\nfrom utils import checkpoint_utils\nfrom utils.decorators import only_on_train\nfrom actor_learner import ActorLearner, ONE_LIFE_GAMES\nfrom networks.policy_v_network import PolicyValueNetwork\n\n\nlogger = utils.logger.getLogger(\'policy_based_actor_learner\')\n\n\nclass BaseA3CLearner(ActorLearner):\n    def __init__(self, args):\n        super(BaseA3CLearner, self).__init__(args)\n\n        self.td_lambda = args.td_lambda\n        self.action_space = args.action_space\n        self.learning_vars = args.learning_vars\n        self.beta = args.entropy_regularisation_strength\n        self.q_target_update_steps = args.q_target_update_steps\n\n\n    def sample_policy_action(self, probs, temperature=0.5):\n        probs = probs - np.finfo(np.float32).epsneg\n    \n        histogram = np.random.multinomial(1, probs)\n        action_index = int(np.nonzero(histogram)[0])\n        return action_index\n\n\n    def bootstrap_value(self, state, episode_over):\n        if episode_over:\n            R = 0\n        else:\n            R = self.session.run(\n                self.local_network.output_layer_v,\n                feed_dict={self.local_network.input_ph:[state]})[0][0]\n\n        return R\n\n\n    def compute_gae(self, rewards, values, next_val):\n        values = values + [next_val]\n        size = len(rewards)\n        adv_batch = list()\n        td_i = 0.0\n\n        for i in reversed(xrange(size)):\n            td_i = rewards[i] + self.gamma*values[i+1] - values[i] + self.td_lambda*self.gamma*td_i \n            adv_batch.append(td_i)\n\n        adv_batch.reverse()\n        return adv_batch\n\n\n    def set_local_lstm_state(self):\n        pass\n\n\n    def apply_update(self, states, actions, targets, advantages):\n        feed_dict={\n            self.local_network.input_ph: states,\n            self.local_network.selected_action_ph: actions,\n            self.local_network.critic_target_ph: targets,\n            self.local_network.adv_actor_ph: advantages,\n        }\n        grads, entropy = self.session.run(\n            [self.local_network.get_gradients, self.local_network.entropy],\n            feed_dict=feed_dict)\n\n        self.apply_gradients_to_shared_memory_vars(grads)\n        return entropy\n\n\n    def train(self):\n        """""" Main actor learner loop for advantage actor critic learning. """"""\n        logger.debug(""Actor {} resuming at Step {}"".format(self.actor_id, \n            self.global_step.value()))\n        \n        episode_rewards = deque(maxlen=1000)\n        while (self.global_step.value() < self.max_global_steps):\n            # Sync local learning net with shared mem\n            s = self.emulator.get_initial_state()\n            self.reset_hidden_state()\n            self.local_episode += 1\n            episode_over = False\n            total_episode_reward = 0.0\n            episode_start_step = self.local_step\n            \n            while not episode_over:\n                self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n                self.save_vars()\n\n                rewards = list()\n                states  = list()\n                actions = list()\n                values  = list()\n                local_step_start = self.local_step\n                self.set_local_lstm_state()\n\n                while self.local_step - local_step_start < self.max_local_steps and not episode_over:\n                    # Choose next action and execute it\n                    a, readout_v_t, readout_pi_t = self.choose_next_action(s)\n                    if self.is_master() and (self.local_step % 400 == 0):\n                        logger.debug(""\xcf\x80_a={:.4f} / V={:.4f}"".format(readout_pi_t[a.argmax()], readout_v_t))\n                    \n                    new_s, reward, episode_over = self.emulator.next(a)\n                    total_episode_reward += reward\n                    # Rescale or clip immediate reward\n                    reward = self.rescale_reward(reward)\n                \n                    rewards.append(reward)\n                    states.append(s)\n                    actions.append(a)\n                    values.append(readout_v_t)\n\n                    s = new_s\n                    self.local_step += 1\n                    self.global_step.increment()\n                \n                next_val = self.bootstrap_value(new_s, episode_over)\n                advantages = self.compute_gae(rewards, values, next_val)\n                targets = self.compute_targets(rewards, next_val)\n                # Compute gradients on the local policy/V network and apply them to shared memory \n                entropy = self.apply_update(states, actions, targets, advantages)\n\n\n            episode_rewards.append(total_episode_reward)\n            elapsed_time = time.time() - self.start_time\n            steps_per_sec = self.global_step.value() / elapsed_time\n            perf = ""{:.0f}"".format(steps_per_sec)\n            logger.info(""T{} / EPISODE {} / STEP {}k / MEAN REWARD {:.1f} / {} STEPS/s"".format(\n                self.actor_id,\n                self.local_episode,\n                self.global_step.value()/1000,\n                np.array(episode_rewards).mean(),\n                perf))\n\n            self.log_summary(total_episode_reward, np.array(values).mean(), entropy)\n\n\nclass A3CLearner(BaseA3CLearner):\n    def __init__(self, args):\n        super(A3CLearner, self).__init__(args)\n\n        conf_learning = {\'name\': \'local_learning_{}\'.format(self.actor_id),\n                         \'input_shape\': self.input_shape,\n                         \'num_act\': self.num_actions,\n                         \'args\': args}\n\n        self.local_network = args.network(conf_learning)\n        self.reset_hidden_state()\n\n        if self.is_master():\n            var_list = self.local_network.params\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\n                                        keep_checkpoint_every_n_hours=2)\n\n\n    def choose_next_action(self, state):\n        return self.local_network.get_action_and_value(self.session, state)\n\n\nclass A3CLSTMLearner(BaseA3CLearner):\n    def __init__(self, args):\n        super(A3CLSTMLearner, self).__init__(args)\n\n        conf_learning = {\'name\': \'local_learning_{}\'.format(self.actor_id),\n                         \'input_shape\': self.input_shape,\n                         \'num_act\': self.num_actions,\n                         \'args\': args}\n\n        self.local_network = args.network(conf_learning)\n        self.reset_hidden_state()\n\n        if self.is_master():\n            var_list = self.local_network.params\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\n                                        keep_checkpoint_every_n_hours=2)\n\n\n    def reset_hidden_state(self):\n        self.lstm_state_out = np.zeros([1, 2*self.local_network.hidden_state_size])\n        # self.lstm_state_out = tf.contrib.rnn.LSTMStateTuple(np.zeros([1, 256]),\n        #                                                     np.zeros([1, 256]))\n\n\n    def set_local_lstm_state(self):\n        self.local_lstm_state = np.copy(self.lstm_state_out)\n\n\n    def choose_next_action(self, state):\n        action, v, dist, self.lstm_state_out = self.local_network.get_action_and_value(\n            self.session, state, lstm_state=self.lstm_state_out)\n        return action, v, dist\n\n\n    def bootstrap_value(self, state, episode_over):\n        if episode_over:\n            R = 0\n        else:\n            R = self.session.run(\n                self.local_network.output_layer_v,\n                feed_dict={\n                    self.local_network.input_ph:[state],\n                    self.local_network.step_size: [1],\n                    self.local_network.initial_lstm_state: self.lstm_state_out,\n                }\n            )[0][0]\n\n        return R\n\n\n    def apply_update(self, states, actions, targets, advantages):\n        feed_dict={\n            self.local_network.input_ph: states,\n            self.local_network.selected_action_ph: actions,\n            self.local_network.critic_target_ph: targets,\n            self.local_network.adv_actor_ph: advantages,\n            self.local_network.step_size : [len(states)],\n            self.local_network.initial_lstm_state: self.local_lstm_state,\n        }\n        grads, entropy = self.session.run(\n            [self.local_network.get_gradients, self.local_network.entropy],\n            feed_dict=feed_dict)\n\n        self.apply_gradients_to_shared_memory_vars(grads)\n        return entropy\n\n\n\n'"
algorithms/sequence_decoder_actor_learner.py,2,"b'# -*- encoding: utf-8 -*-\nimport time\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\n\nfrom utils.forked_debugger import ForkedPdb as Pdb\nfrom actor_learner import ActorLearner, ONE_LIFE_GAMES\nfrom networks.policy_v_network import SequencePolicyVNetwork, PolicyRepeatNetwork\nfrom algorithms.policy_based_actor_learner import BaseA3CLearner\n\n\nlogger = utils.logger.getLogger(\'action_sequence_actor_learner\')\n\n\nclass ActionSequenceA3CLearner(BaseA3CLearner):\n    def __init__(self, args):\n\n        super(ActionSequenceA3CLearner, self).__init__(args)\n        \n        # Shared mem vars\n        self.learning_vars = args.learning_vars\n\n        conf_learning = {\'name\': \'local_learning_{}\'.format(self.actor_id),\n                         \'input_shape\': args.input_shape,\n                         \'num_act\': self.num_actions,\n                         \'args\': args}\n        \n        self.local_network = SequencePolicyVNetwork(conf_learning)\n        self.reset_hidden_state()\n            \n        if self.is_master():\n            var_list = self.local_network.params\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3, \n                                        keep_checkpoint_every_n_hours=2)\n\n\n    def sample_action_sequence(self, state):\n        allowed_actions = np.ones((self.local_network.max_decoder_steps, self.local_network.num_actions+1))\n        allowed_actions[0, -1] = 0\n\n        action_inputs = np.zeros((1,self.local_network.max_decoder_steps,self.num_actions+1))\n        action_inputs[0, 0, -1] = 1\n\n        actions, value = self.session.run(\n            [\n                self.local_network.actions,\n                self.local_network.output_layer_v,\n            ],\n            feed_dict={\n                self.local_network.input_ph:              [state],\n                self.local_network.decoder_seq_lengths:   [self.local_network.max_decoder_steps],\n                self.local_network.allowed_actions:       [allowed_actions],\n                self.local_network.use_fixed_action:      False,\n                self.local_network.temperature:           1.0,\n                self.local_network.action_outputs:        np.zeros((1,self.local_network.max_decoder_steps,self.num_actions+1)),\n                self.local_network.action_inputs:         action_inputs,\n                self.local_network.decoder_initial_state: np.zeros((1, self.local_network.decoder_hidden_state_size*2)),\n            }\n        )\n\n        return actions[0], value[0, 0]\n\n\n    def train(self):\n        """""" Main actor learner loop for advantage actor critic learning. """"""\n        logger.debug(""Actor {} resuming at Step {}"".format(self.actor_id, \n            self.global_step.value()))\n\n        s = self.emulator.get_initial_state()\n        total_episode_reward = 0\n\n        s_batch = []\n        a_batch = []\n        y_batch = []\n        adv_batch = []\n        seq_len_batch = []\n        \n        reset_game = False\n        episode_over = False\n        start_time = time.time()\n        steps_at_last_reward = self.local_step\n        \n        while (self.global_step.value() < self.max_global_steps):\n            # Sync local learning net with shared mem\n            self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n            self.save_vars()\n\n            local_step_start = self.local_step \n            \n            rewards = []\n            states = []\n            actions = []\n            values = []\n            seq_lengths = []\n            \n            while not (episode_over \n                or (self.local_step - local_step_start \n                    == self.max_local_steps)):\n                \n                # Choose next action and execute it\n                action_sequence, readout_v_t = self.sample_action_sequence(s)\n                # if self.is_master() and (self.local_step % 100 == 0):\n                #     logger.debug(""pi={}, V={}"".format(readout_pi_t, readout_v_t))\n                \n                acc_reward = 0.0\n                length = 0\n\n                for action in action_sequence:\n                    length += 1\n                    a = np.argmax(action)\n                    if a == self.num_actions or episode_over:\n                        break\n\n                    new_s, reward, episode_over = self.emulator.next(action[:self.num_actions])\n                    acc_reward += reward\n\n                reward = acc_reward\n                if reward != 0.0:\n                    steps_at_last_reward = self.local_step\n\n\n                total_episode_reward += reward\n                # Rescale or clip immediate reward\n                reward = self.rescale_reward(reward)\n                \n                rewards.append(reward)\n                seq_lengths.append(length)\n                states.append(s)\n                actions.append(action_sequence)\n                values.append(readout_v_t)\n                \n\n                s = new_s\n                self.local_step += 1\n                self.global_step.increment()\n\n                if self.local_step % 1000 == 0:\n                    pass\n                    # Pdb().set_trace()\n                \n            \n            # Calculate the value offered by critic in the new state.\n            if episode_over:\n                R = 0\n            else:\n                R = self.session.run(\n                    self.local_network.output_layer_v,\n                    feed_dict={self.local_network.input_ph:[new_s]})[0][0]\n                            \n             \n            sel_actions = []\n            for i in reversed(xrange(len(states))):\n                R = rewards[i] + self.gamma * R\n\n                y_batch.append(R)\n                a_batch.append(actions[i])\n                s_batch.append(states[i])\n                adv_batch.append(R - values[i])\n                seq_len_batch.append(seq_lengths[i])\n                \n                sel_actions.append(np.argmax(actions[i]))\n            \n            padded_output_sequences = np.array([\n                np.vstack([seq[:length, :], np.zeros((max(seq_len_batch)-length, self.num_actions+1))])\n                for length, seq in zip(seq_len_batch, a_batch)\n            ])\n\n            go_input = np.zeros((len(s_batch), 1, self.num_actions+1))\n            go_input[:,:,self.num_actions] = 1\n            padded_input_sequences = np.hstack([go_input, padded_output_sequences[:,:-1,:]])\n\n            print \'Sequence lengths:\', seq_lengths\n            print \'Actions:\', [np.argmax(a) for a in a_batch[0]]\n\n            allowed_actions = np.ones((len(s_batch), max(seq_len_batch), self.num_actions+1))\n            allowed_actions[:, 0, -1] = 0 #empty sequence is not a valid action\n\n            feed_dict={\n                self.local_network.input_ph:              s_batch, \n                self.local_network.critic_target_ph:      y_batch,\n                self.local_network.adv_actor_ph:          adv_batch,\n                self.local_network.decoder_initial_state: np.zeros((len(s_batch), self.local_network.decoder_hidden_state_size*2)),\n                self.local_network.action_inputs:         padded_input_sequences,\n                self.local_network.action_outputs:        padded_output_sequences,\n                self.local_network.allowed_actions:       allowed_actions,\n                self.local_network.use_fixed_action:      True,\n                self.local_network.decoder_seq_lengths:   seq_lengths,\n                self.local_network.temperature:           1.0,\n            }\n            entropy, grads = self.session.run(\n                [\n                    self.local_network.entropy,\n                    # self.local_network.adv_critic,\n                    # self.local_network.output_layer_v,\n                    self.local_network.get_gradients\n                ],\n                feed_dict=feed_dict)\n\n            print \'Entropy:\', entropy #, \'Adv:\', advantage #, \'Value:\', value\n            self.apply_gradients_to_shared_memory_vars(grads)     \n            \n            s_batch = []\n            a_batch = []\n            y_batch = []          \n            adv_batch = []\n            seq_len_batch = []\n\n            \n            # prevent the agent from getting stuck\n            if (self.local_step - steps_at_last_reward > 5000\n                or (self.emulator.get_lives() == 0\n                    and self.emulator.game not in ONE_LIFE_GAMES)):\n\n                steps_at_last_reward = self.local_step\n                episode_over = True\n                reset_game = True\n\n\n            # Start a new game on reaching terminal state\n            if episode_over:\n                elapsed_time = time.time() - start_time\n                global_t = self.global_step.value()\n                steps_per_sec = global_t / elapsed_time\n                perf = ""{:.0f}"".format(steps_per_sec)\n                logger.info(""T{} / STEP {} / REWARD {} / {} STEPS/s, Actions {}"".format(self.actor_id, global_t, total_episode_reward, perf, sel_actions))\n                \n                self.log_summary(total_episode_reward, entropy)\n\n                episode_over = False\n                total_episode_reward = 0\n                steps_at_last_reward = self.local_step\n\n                if reset_game or self.emulator.game in ONE_LIFE_GAMES:\n                    s = self.emulator.get_initial_state()\n                    reset_game = False\n\n\nclass ARA3CLearner(BaseA3CLearner):\n    def __init__(self, args):\n        super(ARA3CLearner, self).__init__(args)\n\n        conf_learning = {\'name\': \'local_learning_{}\'.format(self.actor_id),\n                         \'input_shape\': self.input_shape,\n                         \'num_act\': self.num_actions,\n                         \'args\': args}\n\n        self.local_network = PolicyRepeatNetwork(conf_learning)\n        self.reset_hidden_state()\n\n        if self.is_master():\n            var_list = self.local_network.params\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\n                                        keep_checkpoint_every_n_hours=2)\n\n\n    def choose_next_action(self, state):\n        network_output_v, network_output_pi, action_repeat_probs = self.session.run(\n            [\n                self.local_network.output_layer_v,\n                self.local_network.output_layer_pi,\n                self.local_network.action_repeat_probs,\n            ],\n            feed_dict={\n                self.local_network.input_ph: [state],\n            })\n\n        network_output_pi = network_output_pi.reshape(-1)\n        network_output_v = np.asscalar(network_output_v)\n\n        action_index = self.sample_policy_action(network_output_pi)\n        new_action = np.zeros([self.num_actions])\n        new_action[action_index] = 1\n\n        action_repeat = 1 + self.sample_policy_action(action_repeat_probs[0])\n\n        return new_action, network_output_v, network_output_pi, action_repeat\n\n\n    def train(self):\n        """""" Main actor learner loop for advantage actor critic learning. """"""\n        logger.debug(""Actor {} resuming at Step {}"".format(self.actor_id, \n            self.global_step.value()))\n\n        s = self.emulator.get_initial_state()\n        steps_at_last_reward = self.local_step\n        total_episode_reward = 0.0\n        mean_entropy = 0.0\n        episode_start_step = 0\n        \n        while (self.global_step.value() < self.max_global_steps):\n            # Sync local learning net with shared mem\n            self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n            self.save_vars()\n\n            local_step_start = self.local_step \n            \n            reset_game = False\n            episode_over = False\n\n            rewards        = list()\n            states         = list()\n            actions        = list()\n            values         = list()\n            s_batch        = list()\n            a_batch        = list()\n            y_batch        = list()\n            ar_batch       = list()\n            adv_batch      = list()\n            action_repeats = list()\n            \n            while not (episode_over \n                or (self.local_step - local_step_start \n                    == self.max_local_steps)):\n                \n                # Choose next action and execute it\n                a, readout_v_t, readout_pi_t, action_repeat = self.choose_next_action(s)\n                \n                if self.is_master() and (self.local_step % 100 == 0):\n                    logger.debug(""\xcf\x80_a={:.4f} / V={:.4f} repeat={}"".format(\n                        readout_pi_t[a.argmax()], readout_v_t, action_repeat))\n\n                reward = 0.0\n                for _ in range(action_repeat):\n                    new_s, reward_i, episode_over = self.emulator.next(a)\n                    reward += reward_i\n\n                if reward != 0.0:\n                    steps_at_last_reward = self.local_step\n\n\n                total_episode_reward += reward\n                # Rescale or clip immediate reward\n                reward = self.rescale_reward(reward)\n                \n                rewards.append(reward)\n                states.append(s)\n                actions.append(a)\n                values.append(readout_v_t)\n                action_repeats.append(action_repeat)\n                \n                s = new_s\n                self.local_step += 1\n                self.global_step.increment()\n                \n            \n            # Calculate the value offered by critic in the new state.\n            if episode_over:\n                R = 0\n            else:\n                R = self.session.run(\n                    self.local_network.output_layer_v,\n                    feed_dict={self.local_network.input_ph:[new_s]})[0][0]\n                            \n             \n            sel_actions = []\n            for i in reversed(xrange(len(states))):\n                R = rewards[i] + self.gamma * R\n\n                y_batch.append(R)\n                a_batch.append(actions[i])\n                s_batch.append(states[i])\n                adv_batch.append(R - values[i])\n                ar_batch.append(action_repeats[i])\n                \n                sel_actions.append(np.argmax(actions[i]))\n                \n\n            # Compute gradients on the local policy/V network and apply them to shared memory  \n            feed_dict={\n                self.local_network.input_ph: s_batch, \n                self.local_network.critic_target_ph: y_batch,\n                self.local_network.selected_action_ph: a_batch,\n                self.local_network.adv_actor_ph: adv_batch,\n                self.local_network.selected_repeat: ar_batch,\n            }\n            grads, entropy = self.session.run(\n                [self.local_network.get_gradients, self.local_network.entropy],\n                feed_dict=feed_dict)\n\n            self.apply_gradients_to_shared_memory_vars(grads)     \n\n            delta_old = local_step_start - episode_start_step\n            delta_new = self.local_step -  local_step_start\n            mean_entropy = (mean_entropy*delta_old + entropy*delta_new) / (delta_old + delta_new)\n\n            s, mean_entropy, episode_start_step, total_episode_reward, steps_at_last_reward = self.prepare_state(\n                s, mean_entropy, episode_start_step, total_episode_reward, steps_at_last_reward, sel_actions, episode_over)\n\n\n'"
algorithms/trpo_actor_learner.py,28,"b""# -*- coding: utf-8 -*-\nimport time\nimport utils.ops\nimport utils.stats\nimport numpy as np\nimport utils.logger\nimport tensorflow as tf\n\nfrom utils.distributions import DiagNormal\nfrom policy_based_actor_learner import BaseA3CLearner\nfrom networks.policy_v_network import PolicyValueNetwork\n\n\nlogger = utils.logger.getLogger('trpo_actor_learner')\n\n\nclass TRPOLearner(BaseA3CLearner):\n\t'''\n\tImplementation of Trust Region Policy Optimization + Generalized Advantage Estimation \n\tas described in https://arxiv.org/pdf/1506.02438.pdf\n\n\t\xe2\x88\x82'\xcf\x80 = F^-1 \xe2\x88\x82\xcf\x80 where F is the Fischer Information Matrix\n\tWe can't compute F^-1 directly except for very small networks\n\tso we'll use either conjugate gradient descent to approximate F^-1 \xe2\x88\x82\xcf\x80\n\t'''\n\n\tdef __init__(self, args):\n\t\targs.entropy_regularisation_strength = 0.0\n\t\tsuper(TRPOLearner, self).__init__(args)\n\n\t\tself.batch_size = 512\n\t\tself.max_cg_iters = 20\n\t\tself.num_epochs = args.num_epochs\n\t\tself.cg_damping = args.cg_damping\n\t\tself.cg_subsample = args.cg_subsample\n\t\tself.max_kl = args.max_kl\n\t\tself.max_rollout = args.max_rollout\n\t\tself.episodes_per_batch = args.episodes_per_batch\n\t\tself.baseline_vars = args.baseline_vars\n\t\tself.experience_queue = args.experience_queue\n\t\tself.task_queue = args.task_queue\n\t\tself.history_length = args.history_length\n\t\tself.append_timestep = args.arch == 'FC'\n\n\n\t\tpolicy_conf = {'name': 'policy_network_{}'.format(self.actor_id),\n\t\t\t\t\t   'input_shape': self.input_shape,\n\t\t\t\t\t   'num_act': self.num_actions,\n\t\t\t\t\t   'args': args}\n\t\tvalue_conf = policy_conf.copy()\n\t\tvalue_conf['name'] = 'value_network_{}'.format(self.actor_id)\n\t\tvalue_conf['input_shape'] = args.vf_input_shape\n\n\t\tself.device = '/gpu:0' if self.is_master() else '/cpu:0'\n\t\twith tf.device(self.device):\n\t\t\t#we use separate networks as in the paper since so we don't do damage to the trust region updates\n\t\t\tself.policy_network = args.network(policy_conf)\n\t\t\tself.value_network = PolicyValueNetwork(value_conf, use_policy_head=False)\n\t\t\tself.local_network = self.policy_network\n\t\t\tself._build_ops()\n\n\t\tif self.is_master():\n\t\t\tvar_list = self.policy_network.params + self.value_network.params\n\t\t\tself.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\n                                        keep_checkpoint_every_n_hours=2)\n\n\n\tdef _build_ops(self):\n\t\teps = 1e-10\n\t\tself.dist_params = self.policy_network.dist.params()\n\t\tnum_params = self.dist_params.get_shape().as_list()[1]\n\t\tself.old_params = tf.placeholder(tf.float32, shape=[None, num_params], name='old_params')\n\n\t\tselected_prob = tf.exp(self.policy_network.log_output_selected_action)\n\t\told_dist = self.policy_network.dist.__class__(self.old_params)\n\t\told_selected_prob = tf.exp(old_dist.log_likelihood(self.policy_network.selected_action_ph))\n\n\t\tself.theta = self.policy_network.flat_vars\n\t\tself.policy_loss = -tf.reduce_mean(tf.multiply(\n\t\t\tself.policy_network.adv_actor_ph,\n\t\t\tselected_prob / old_selected_prob\n\t\t))\n\t\tself.pg = utils.ops.flatten_vars(\n\t\t\ttf.gradients(self.policy_loss, self.policy_network.params))\n\n\t\tself.kl = tf.reduce_mean(self.policy_network.dist.kl_divergence(self.old_params))\n\t\tself.kl_firstfixed = tf.reduce_mean(self.policy_network.dist.kl_divergence(\n\t\t\ttf.stop_gradient(self.dist_params)))\n\n\n\t\tkl_grads = tf.gradients(self.kl_firstfixed, self.policy_network.params)\n\t\tflat_kl_grads = utils.ops.flatten_vars(kl_grads)\n\n\t\tself.pg_placeholder = tf.placeholder(tf.float32, shape=self.pg.get_shape().as_list(), name='pg_placeholder')\n\t\tself.fullstep, self.neggdotstepdir = self._conjugate_gradient_ops(\n\t\t\t-self.pg_placeholder, flat_kl_grads, max_iterations=self.max_cg_iters)\n\n\n\tdef _conjugate_gradient_ops(self, pg_grads, kl_grads, max_iterations=20, residual_tol=1e-10):\n\t\t'''\n\t\tConstruct conjugate gradient descent algorithm inside computation graph for improved efficiency\n\t\t'''\n\t\ti0 = tf.constant(0, dtype=tf.int32)\n\t\tloop_condition = lambda i, r, p, x, rdotr: tf.logical_and(\n\t\t\ttf.greater(rdotr, residual_tol), tf.less(i, max_iterations))\n\n\n\t\tdef body(i, r, p, x, rdotr):\n\t\t\tfvp = utils.ops.flatten_vars(tf.gradients(\n\t\t\t\ttf.reduce_sum(tf.stop_gradient(p)*kl_grads),\n\t\t\t\tself.policy_network.params))\n\n\t\t\tz = fvp + self.cg_damping * p\n\n\t\t\talpha = rdotr / (tf.reduce_sum(p*z) + 1e-8)\n\t\t\tx += alpha * p\n\t\t\tr -= alpha * z\n\n\t\t\tnew_rdotr = tf.reduce_sum(r*r)\n\t\t\tbeta = new_rdotr / (rdotr + 1e-8)\n\t\t\tp = r + beta * p\n\n\t\t\tnew_rdotr = tf.Print(new_rdotr, [i, new_rdotr], 'Iteration / Residual: ')\n\n\t\t\treturn i+1, r, p, x, new_rdotr\n\n\t\t_, r, p, stepdir, rdotr = tf.while_loop(\n\t\t\tloop_condition,\n\t\t\tbody,\n\t\t\tloop_vars=[i0,\n\t\t\t\t\t   pg_grads,\n\t\t\t\t\t   pg_grads,\n\t\t\t\t\t   tf.zeros_like(pg_grads),\n\t\t\t\t\t   tf.reduce_sum(pg_grads*pg_grads)])\n\n\t\tfvp = utils.ops.flatten_vars(tf.gradients(\n\t\t\ttf.reduce_sum(tf.stop_gradient(stepdir)*kl_grads),\n\t\t\tself.policy_network.params))\n\n\t\tshs = 0.5 * tf.reduce_sum(stepdir*fvp)\n\t\tlm = tf.sqrt((shs + 1e-8) / self.max_kl)\n\t\tfullstep = stepdir / lm\n\t\tneggdotstepdir = tf.reduce_sum(pg_grads*stepdir) / lm\n\n\t\treturn fullstep, neggdotstepdir\n\n\n\tdef choose_next_action(self, state):\n\t\treturn self.policy_network.get_action(self.session, state)\n\n\n\tdef run_minibatches(self, data, *ops):\n\t\toutputs = [np.zeros(op.get_shape().as_list(), dtype=np.float32) for op in ops]\n\n\t\tdata_size = len(data['state'])\n\t\tfor start in range(0, data_size, self.batch_size):\n\t\t\tend = start + np.minimum(self.batch_size, data_size-start)\n\t\t\tfeed_dict={\n\t\t\t\tself.policy_network.input_ph:           data['state'][start:end],\n\t\t\t\tself.policy_network.selected_action_ph: data['action'][start:end],\n\t\t\t\tself.policy_network.adv_actor_ph:       data['reward'][start:end],\n\t\t\t\tself.old_params:                        data['pi'][start:end]\n\t\t\t}\n\t\t\tfor i, output_i in enumerate(self.session.run(ops, feed_dict=feed_dict)):\n\t\t\t\toutputs[i] += output_i * (end-start)/float(data_size)\n\n\t\treturn outputs\n\n\n\tdef linesearch(self, data, x, fullstep, expected_improve_rate):\n\t\taccept_ratio = .1\n\t\tbacktrack_ratio = .7\n\t\tmax_backtracks = 15\n    \n\t\tfval = self.run_minibatches(data, self.policy_loss)\n\n\t\tfor (_n_backtracks, stepfrac) in enumerate(backtrack_ratio**np.arange(max_backtracks)):\n\t\t    xnew = x + stepfrac * fullstep\n\t\t    self.assign_vars(self.policy_network, xnew)\n\t\t    newfval, kl = self.run_minibatches(data, self.policy_loss, self.kl)\n\n\t\t    improvement = fval - newfval\n\t\t    logger.debug('Improvement {} / Mean KL {}'.format(improvement, kl))\n\n\t\t    expected_improve = expected_improve_rate * stepfrac\n\t\t    ratio = improvement / expected_improve\n\t\t    # if ratio > accept_ratio and improvement > 0:\n\t\t    if kl < self.max_kl and improvement > 0:\n\t\t        return xnew\n\n\t\tlogger.debug('No update')\n\t\treturn x\n\n\n\tdef fit_baseline(self, data, mix_old=.9):\n\t\tdata_size = len(data['state'])\n\t\tproc_state = self.preprocess_value_state(data)\n\t\tprint 'diffs', (data['mc_return'] - data['values']).mean()\n\t\ttarget = (1-mix_old)*data['mc_return'] + mix_old*data['values']\n\t\tgrads = [np.zeros(g.get_shape().as_list(), dtype=np.float32) for g in self.value_network.get_gradients]\n\n\t\t#permute data in minibatches so we don't introduce bias\n\t\tperm = np.random.permutation(data_size)\n\t\tfor start in range(0, data_size, self.batch_size):\n\t\t\tend = start + np.minimum(self.batch_size, data_size-start)\n\t\t\tbatch_idx = perm[start:end]\n\t\t\tfeed_dict={\n\t\t\t\tself.value_network.input_ph:         proc_state[batch_idx],\n\t\t\t\tself.value_network.critic_target_ph: target[batch_idx]\n\t\t\t}\n\t\t\toutput_i = self.session.run(self.value_network.get_gradients, feed_dict=feed_dict)\n\t\t\t\n\t\t\tfor i, g in enumerate(output_i):\n\t\t\t\tgrads[i] += g * (end-start)/float(data_size)\n\n\t\t\tself._apply_gradients_to_shared_memory_vars(output_i, self.baseline_vars)\n\t\t\tself.sync_net_with_shared_memory(self.value_network, self.baseline_vars)\n\n\n\tdef preprocess_value_state(self, data):\n\t\tif self.append_timestep: #this is particularly helpful on MuJoCo environments\n\t\t\treturn np.hstack([data['state'], np.tile(data['timestep'].reshape(-1, 1, 1), self.history_length)])\n\t\telse:\n\t\t\treturn data['state']\n\n\n\tdef predict_values(self, data):\n\t\tstate = self.preprocess_value_state({\n\t\t\t'state': np.array(data['state']),\n\t\t\t'timestep': np.array(data['timestep'])})\n\t\treturn self.session.run(\n\t\t\tself.value_network.output_layer_v,\n\t\t\tfeed_dict={self.value_network.input_ph: state})[:, 0]\n\n\n\tdef update_grads(self, data):\n\t\t#we need to compute the policy gradient in minibatches to avoid GPU OOM errors on Atari\n\t\tprint 'fitting baseline...'\n\t\tself.fit_baseline(data)\n\n\t\tnormalized_advantage = (data['advantage'] - data['advantage'].mean())/(data['advantage'].std() + 1e-8)\n\t\tdata['reward'] = normalized_advantage\n\n\t\tprint 'running policy gradient...'\n\t\tpg = self.run_minibatches(data, self.pg)[0]\n\n\t\tdata_size = len(data['state'])\n\t\tsubsample = np.random.choice(data_size, int(data_size*self.cg_subsample), replace=False)\n\t\tfeed_dict={\n\t\t\tself.policy_network.input_ph:           data['state'][subsample],\n\t\t\tself.policy_network.selected_action_ph: data['action'][subsample],\n\t\t\tself.policy_network.adv_actor_ph:       data['reward'][subsample],\n\t\t\tself.old_params:                        data['pi'][subsample],\n\t\t\tself.pg_placeholder:                    pg\n\t\t}\n\n\t\tprint 'running conjugate gradient descent...'\n\t\ttheta_prev, fullstep, neggdotstepdir = self.session.run(\n\t\t\t[self.theta, self.fullstep, self.neggdotstepdir], feed_dict=feed_dict)\n\n\t\tprint 'running linesearch...'\n\t\tnew_theta = self.linesearch(data, theta_prev, fullstep, neggdotstepdir)\n\t\tself.assign_vars(self.policy_network, new_theta)\n\n\t\treturn self.session.run(self.kl, feed_dict)\n\n\n\tdef _run_worker(self):\n\t\twhile True:\n\t\t\tsignal = self.task_queue.get()\n\t\t\tif signal == 'EXIT':\n\t\t\t\tbreak\n\n\t\t\tself.sync_net_with_shared_memory(self.local_network, self.learning_vars)\n\t\t\tself.sync_net_with_shared_memory(self.value_network, self.baseline_vars)\n\t\t\ts = self.emulator.get_initial_state()\n\n\t\t\tdata = {\n\t\t\t\t'state':     list(),\n\t\t\t\t'pi':        list(),\n\t\t\t\t'action':    list(),\n\t\t\t\t'reward':    list(),\n\t\t\t}\n\t\t\tepisode_over = False\n\t\t\taccumulated_rewards = list()\n\t\t\twhile not episode_over and len(accumulated_rewards) < self.max_rollout:\n\t\t\t\ta, pi = self.choose_next_action(s)\n\t\t\t\tnew_s, reward, episode_over = self.emulator.next(a)\n\t\t\t\t\n\t\t\t\taccumulated_rewards.append(reward)\n\n\t\t\t\tdata['state'].append(s)\n\t\t\t\tdata['pi'].append(pi)\n\t\t\t\tdata['action'].append(a)\n\t\t\t\tdata['reward'].append(reward)\n\n\t\t\t\ts = new_s\n\n\t\t\tmc_returns = list()\n\t\t\trunning_total = 0.0\n\t\t\tfor r in reversed(accumulated_rewards):\n\t\t\t\trunning_total = r + self.gamma*running_total\n\t\t\t\tmc_returns.insert(0, running_total)\n\n\t\t\ttimestep = np.arange(len(mc_returns), dtype=np.float32)/self.emulator.env.spec.timestep_limit\n\t\t\tdata['timestep'] = timestep\n\t\t\tdata['mc_return'] = mc_returns\n\t\t\tepisode_reward = sum(accumulated_rewards)\n\t\t\tlogger.debug('T{} / Episode Reward {}'.format(\n\t\t\t\tself.actor_id, episode_reward))\n\n\t\t\tself.experience_queue.put((data, episode_reward))\n\t\t\t\n\n\tdef _run_master(self):\n\t\tfor epoch in range(self.num_epochs):\n\t\t\tdata = {\n\t\t\t\t'state':     list(),\n\t\t\t\t'pi':        list(),\n\t\t\t\t'action':    list(),\n\t\t\t\t'reward':    list(),\n\t\t\t\t'advantage': list(),\n\t\t\t\t'mc_return': list(),\n\t\t\t\t'timestep':  list(),\n\t\t\t\t'values':    list(),\n\t\t\t}\n\t\t\t#launch worker tasks\n\t\t\tfor i in xrange(self.episodes_per_batch):\n\t\t\t\tself.task_queue.put(i)\n\n\t\t\t#collect worker experience\n\t\t\tepisode_rewards = list()\n\t\t\tt0 = time.time()\n\t\t\tfor _ in xrange(self.episodes_per_batch):\n\t\t\t\tworker_data, reward = self.experience_queue.get()\n\t\t\t\tepisode_rewards.append(reward)\n\n\t\t\t\tvalues = self.predict_values(worker_data)\n\t\t\t\tadvantages = self.compute_gae(worker_data['reward'], values.tolist(), 0)\n\t\t\t\t# advantages = worker_data['mc_return'] - values\n\t\t\t\tworker_data['values'] = values\n\t\t\t\tworker_data['advantage'] = advantages\n\t\t\t\tfor key, value in worker_data.items():\n\t\t\t\t\tdata[key].extend(value)\n\t\t\t\t\t\n\t\t\tt1 = time.time()\n\t\t\tkl = self.update_grads({\n\t\t\t\tk: np.array(v) for k, v in data.items()})\n\t\t\tself.update_shared_memory()\n\t\t\tt2 = time.time()\n\n\t\t\tmean_episode_reward = np.array(episode_rewards).mean()\n\t\t\tlogger.info('Epoch {} / Mean KL Divergence {} / Mean Reward {} / Experience Time {:.2f}s / Training Time {:.2f}s'.format(\n\t\t\t\tepoch+1, kl, mean_episode_reward, t1-t0, t2-t1))\n\n\n\tdef train(self):\n\t\tif self.is_master():\n\t\t\tself._run_master()\n\t\t\tfor _ in xrange(self.num_actor_learners):\n\t\t\t\tself.task_queue.put('EXIT')\n\t\telse:\n\t\t\tself._run_worker()\n\n\n"""
algorithms/value_based_actor_learner.py,7,"b'# -*- encoding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport ctypes\r\nimport utils\r\nimport time\r\nimport sys\r\nfrom collections import deque\r\n\r\nfrom utils.decorators import only_on_train\r\nfrom utils.hogupdatemv import copy\r\nfrom networks.q_network import QNetwork\r\nfrom networks.dueling_network import DuelingNetwork\r\nfrom actor_learner import ActorLearner, ONE_LIFE_GAMES\r\n\r\n\r\nlogger = utils.logger.getLogger(\'value_based_actor_learner\')\r\n\r\n\r\nclass ValueBasedLearner(ActorLearner):\r\n\r\n    def __init__(self, args, network_type=QNetwork):\r\n\r\n        super(ValueBasedLearner, self).__init__(args)\r\n\r\n        # Shared mem vars\r\n        self.target_vars = args.target_vars\r\n        self.target_update_flags = args.target_update_flags\r\n        self.q_target_update_steps = args.q_target_update_steps\r\n\r\n        self.scores = list()\r\n\r\n        conf_learning = {\'name\': ""local_learning_{}"".format(self.actor_id),\r\n                         \'input_shape\': self.input_shape,\r\n                         \'num_act\': self.num_actions,\r\n                         \'args\': args}\r\n        conf_target = conf_learning.copy()\r\n        conf_target[\'name\'] = \'local_target_{}\'.format(self.actor_id)\r\n\r\n        self.local_network = network_type(conf_learning)\r\n        self.target_network = network_type(conf_target)\r\n\r\n        if self.is_master():\r\n            var_list = self.local_network.params + self.target_network.params\r\n            self.saver = tf.train.Saver(var_list=var_list, max_to_keep=3,\r\n                                        keep_checkpoint_every_n_hours=2)\r\n\r\n        # Exploration epsilons\r\n        self.initial_epsilon = 1.0\r\n        self.final_epsilon = self.generate_final_epsilon()\r\n        self.epsilon = self.initial_epsilon if self.is_train else args.final_epsilon\r\n        self.epsilon_annealing_steps = args.epsilon_annealing_steps\r\n        self.exploration_strategy = args.exploration_strategy\r\n        self.bolzmann_temperature = args.bolzmann_temperature\r\n\r\n\r\n    def generate_final_epsilon(self):\r\n        values = [.01, .05, .1, .2]\r\n        return values[self.actor_id % 4]\r\n\r\n\r\n    def reduce_thread_epsilon(self):\r\n        """""" Linear annealing """"""\r\n        if self.epsilon > self.final_epsilon:\r\n            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.epsilon_annealing_steps\r\n\r\n\r\n    def _get_summary_vars(self):\r\n        episode_reward = tf.Variable(0., name=\'episode_reward\')\r\n        s1 = tf.summary.scalar(\'Episode_Reward_{}\'.format(self.actor_id), episode_reward)\r\n\r\n        episode_avg_max_q = tf.Variable(0., name=\'episode_avg_max_q\')\r\n        s2 = tf.summary.scalar(\'Max_Q_Value_{}\'.format(self.actor_id), episode_avg_max_q)\r\n\r\n        logged_epsilon = tf.Variable(0., name=\'epsilon_\'.format(self.actor_id))\r\n        s3 = tf.summary.scalar(\'Epsilon_{}\'.format(self.actor_id), logged_epsilon)\r\n\r\n        return [episode_reward, episode_avg_max_q, logged_epsilon]\r\n\r\n\r\n    def epsilon_greedy(self, q_values):\r\n        if np.random.rand() <= self.epsilon:\r\n            return np.random.randint(0, self.num_actions)\r\n        else:\r\n            return np.argmax(q_values)\r\n\r\n\r\n    def boltzmann_exploration(self, q_values):\r\n        exp_minus_max = np.exp(q_values - q_values.max())\r\n        probs = exp_minus_max / exp_minus_max.sum()\r\n\r\n        return np.random.choice(self.num_actions, p=probs)\r\n\r\n\r\n    def choose_next_action(self, state):\r\n        """""" Epsilon greedy """"""\r\n        new_action = np.zeros([self.num_actions])\r\n\r\n        q_values = self.session.run(\r\n            self.local_network.output_layer,\r\n            feed_dict={self.local_network.input_ph: [state]})[0]\r\n\r\n        if self.exploration_strategy == \'epsilon-greedy\':\r\n            action_index = self.epsilon_greedy(q_values)\r\n        else:\r\n            action_index = self.boltzmann_exploration(q_values)\r\n\r\n        new_action[action_index] = 1\r\n        self.reduce_thread_epsilon()\r\n\r\n        return new_action, q_values\r\n\r\n\r\n    def bootstrap_value(self, state, episode_over):\r\n        if episode_over:\r\n            R = 0\r\n        else:\r\n            q_target_values = self.session.run(\r\n                self.target_network.output_layer,\r\n                feed_dict={self.target_network.input_ph: [state]})\r\n            R = np.max(q_target_values)\r\n\r\n        return R\r\n\r\n\r\n    def apply_update(self, states, actions, targets):\r\n        feed_dict={\r\n            self.local_network.input_ph: states,\r\n            self.local_network.target_ph: targets,\r\n            self.local_network.selected_action_ph: actions,\r\n        }\r\n        grads = self.session.run(\r\n            self.local_network.get_gradients,\r\n            feed_dict=feed_dict)\r\n\r\n        self.apply_gradients_to_shared_memory_vars(grads)\r\n\r\n\r\n    def update_target(self):\r\n        copy(np.frombuffer(self.target_vars.vars, ctypes.c_float),\r\n              np.frombuffer(self.learning_vars.vars, ctypes.c_float))\r\n\r\n        # Set shared flags\r\n        for i in xrange(len(self.target_update_flags.updated)):\r\n            self.target_update_flags.updated[i] = 1\r\n\r\n\r\n    def prepare_state(self, state, total_episode_reward, steps_at_last_reward,\r\n                      ep_t, episode_ave_max_q, episode_over):\r\n        # Start a new game on reaching terminal state\r\n        if episode_over:\r\n            T = self.global_step.value()\r\n            t = self.local_step\r\n            e_prog = float(t)/self.epsilon_annealing_steps\r\n            episode_ave_max_q = episode_ave_max_q/float(ep_t)\r\n            s1 = ""Q_MAX {0:.4f}"".format(episode_ave_max_q)\r\n            s2 = ""EPS {0:.4f}"".format(self.epsilon)\r\n\r\n            self.scores.insert(0, total_episode_reward)\r\n            if len(self.scores) > 100:\r\n                self.scores.pop()\r\n\r\n            logger.info(\'T{0} / STEP {1} / REWARD {2} / {3} / {4}\'.format(\r\n                self.actor_id, T, total_episode_reward, s1, s2))\r\n            logger.info(\'ID: {0} -- RUNNING AVG: {1:.0f} \xc2\xb1 {2:.0f} -- BEST: {3:.0f}\'.format(\r\n                self.actor_id,\r\n                np.array(self.scores).mean(),\r\n                2*np.array(self.scores).std(),\r\n                max(self.scores),\r\n            ))\r\n            self.log_summary(\r\n                total_episode_reward,\r\n                episode_ave_max_q,\r\n                self.epsilon)\r\n\r\n            state = self.emulator.get_initial_state()\r\n            total_episode_reward = 0\r\n            episode_ave_max_q = 0\r\n            episode_over = False\r\n            ep_t = 0\r\n\r\n        return state, total_episode_reward, steps_at_last_reward, ep_t, episode_ave_max_q, episode_over\r\n\r\n\r\nclass NStepQLearner(ValueBasedLearner):\r\n\r\n    def train(self):\r\n        """""" Main actor learner loop for n-step Q learning. """"""\r\n        logger.debug(""Actor {} resuming at Step {}, {}"".format(self.actor_id,\r\n            self.global_step.value(), time.ctime()))\r\n\r\n        episode_rewards = deque(maxlen=100)\r\n        episode_max_q = deque(maxlen=100)\r\n\r\n        while (self.global_step.value() < self.max_global_steps):\r\n\r\n            s = self.emulator.get_initial_state()\r\n            total_episode_reward = 0.0\r\n            episode_over = False\r\n            episode_start_step = self.local_step\r\n            self.local_episode += 1\r\n            exec_update_target = False\r\n\r\n            while not episode_over:\r\n\r\n\r\n                # Sync local learning net with shared mem\r\n                self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\r\n                self.save_vars()\r\n\r\n                rewards = list()\r\n                states =  list()\r\n                actions = list()\r\n                max_q = list()\r\n                local_step_start = self.local_step\r\n\r\n                while self.local_step - local_step_start < self.max_local_steps and not episode_over:\r\n\r\n                    # Choose next action and execute it\r\n                    a, readout_t = self.choose_next_action(s)\r\n\r\n                    new_s, reward, episode_over = self.emulator.next(a)\r\n\r\n                    #if reward != 0.0:\r\n                    #    steps_at_last_reward = self.local_step\r\n\r\n                    total_episode_reward += reward\r\n                    # Rescale or clip immediate reward\r\n                    reward = self.rescale_reward(reward)\r\n\r\n                    rewards.append(reward)\r\n                    states.append(s)\r\n                    actions.append(a)\r\n\r\n                    s = new_s\r\n                    self.local_step += 1\r\n                    self.global_step.increment()\r\n                    max_q.append(np.max(readout_t))\r\n\r\n                    global_step, update_target = self.global_step.increment(\r\n                        self.q_target_update_steps)\r\n\r\n                    if update_target:\r\n                        update_target = False\r\n                        exec_update_target = True\r\n\r\n                    self.local_network.global_step = global_step\r\n\r\n                R = self.bootstrap_value(s, episode_over)\r\n                targets = self.compute_targets(rewards, R)\r\n                # Compute gradients on the local Q network\r\n                self.apply_update(states, actions, targets)\r\n\r\n            episode_max_q.append(np.max(max_q))\r\n            episode_rewards.append(total_episode_reward)\r\n            elapsed_time = time.time() - self.start_time\r\n            steps_per_sec = self.global_step.value() / elapsed_time\r\n            perf = ""{:.0f}"".format(steps_per_sec)\r\n\r\n            logger.info(""T{} / EPISODE {} / STEP {}k / MEAN REWARD {:.1f} / MEAN MAX Q {:.1f} / {} STEPS/s"".format(\r\n                self.actor_id,\r\n                self.local_episode,\r\n                self.global_step.value()/1000,\r\n                np.array(episode_rewards).mean(),\r\n                np.array(episode_max_q).mean(),\r\n                perf))\r\n\r\n            self.log_summary(\r\n                total_episode_reward,\r\n                np.array(max_q).mean(),\r\n                self.epsilon)\r\n\r\n            if exec_update_target:\r\n                self.update_target()\r\n                exec_update_target = False\r\n\r\n            # Sync local tensorflow target network params with shared target network params\r\n            if self.target_update_flags.updated[self.actor_id] == 1:\r\n                self.sync_net_with_shared_memory(self.target_network, self.target_vars)\r\n                self.target_update_flags.updated[self.actor_id] = 0\r\n\r\nclass DuelingLearner(NStepQLearner):\r\n    def __init__(self, args):\r\n        super(DuelingLearner, self).__init__(args, network_type=DuelingNetwork)\r\n\r\n\r\nclass OneStepSARSALearner(ValueBasedLearner):\r\n\r\n    def generate_final_epsilon(self):\r\n        return 0.1\r\n\r\n    def train(self):\r\n        """""" Main actor learner loop for 1-step SARSA learning. """"""\r\n        logger.debug(""Actor {} resuming at Step {}, {}"".format(self.actor_id,\r\n            self.global_step.value(), time.ctime()))\r\n\r\n        s = self.emulator.get_initial_state()\r\n\r\n        states =  list()\r\n        actions = list()\r\n        targets = list()\r\n\r\n        steps_at_last_reward = self.local_step\r\n        exec_update_target = False\r\n        total_episode_reward = 0\r\n        episode_ave_max_q = 0\r\n        episode_over = False\r\n        qmax_down = 0\r\n        qmax_up = 0\r\n        prev_qmax = -10*6\r\n        low_qmax = 0\r\n        ep_t = 0\r\n\r\n        # Choose initial action\r\n        a, readout_t = self.choose_next_action(s)\r\n\r\n        while (self.global_step.value() < self.max_global_steps):\r\n            s_prime, reward, episode_over = self.emulator.next(a)\r\n            if reward != 0.0:\r\n                steps_at_last_reward = self.local_step\r\n\r\n            ep_t += 1\r\n            episode_ave_max_q += np.max(readout_t)\r\n\r\n            actions.append(a)\r\n            states.append(s)\r\n            s = s_prime\r\n\r\n            total_episode_reward += reward\r\n            reward = self.rescale_reward(reward)\r\n            if episode_over:\r\n                y = reward\r\n            else:\r\n                # Choose action that we will execute in the next step\r\n                a_prime, readout_t = self.choose_next_action(s_prime)\r\n                q_prime = readout_t[a_prime.argmax()]\r\n                # Q_target in the new state for the next step action\r\n                q_prime = self.session.run(\r\n                    self.target_network.output_layer,\r\n                    feed_dict={self.target_network.input_ph: [s_prime]}\r\n                )[0][a_prime.argmax()]\r\n\r\n                y = reward + self.gamma * q_prime\r\n                a = a_prime\r\n\r\n            targets.append(y)\r\n\r\n            self.local_step += 1\r\n            global_step, update_target = self.global_step.increment(\r\n                self.q_target_update_steps)\r\n\r\n            # Compute grads and asynchronously apply them to shared memory\r\n            if ((self.local_step % self.grads_update_steps == 0)\r\n                or episode_over):\r\n\r\n                # Compute gradients on the local Q network\r\n                self.apply_update(states, actions, targets)\r\n                self.sync_net_with_shared_memory(self.local_network, self.learning_vars)\r\n                self.save_vars()\r\n\r\n                states =  list()\r\n                actions = list()\r\n                targets = list()\r\n\r\n            # Copy shared learning network params to shared target network params\r\n            if update_target:\r\n                self.update_target()\r\n\r\n            # Sync local tensorflow target network params with shared target network params\r\n            if self.target_update_flags.updated[self.actor_id] == 1:\r\n                self.sync_net_with_shared_memory(self.target_network, self.target_vars)\r\n                self.target_update_flags.updated[self.actor_id] = 0\r\n\r\n            s, total_episode_reward, steps_at_last_reward, ep_t, episode_ave_max_q, episode_over = \\\r\n                self.prepare_state(s, total_episode_reward, steps_at_last_reward, ep_t, episode_ave_max_q, episode_over)\r\n'"
environments/__init__.py,0,b''
environments/atari_environment.py,0,"b'""""""\r\nCode based on: https://github.com/coreylynch/async-rl/blob/master/atari_environment.py\r\n""""""\r\n\r\n""""""\r\nThe MIT License (MIT)\r\n\r\nCopyright (c) 2016 Corey Lynch\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the ""Software""), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n""""""\r\n\r\nimport numpy as np\r\nimport gym\r\n\r\nfrom gym.spaces import Box, Discrete\r\nfrom skimage.transform import resize\r\nfrom skimage.color import rgb2gray\r\nfrom collections import deque\r\n\r\n\r\nRESIZE_WIDTH = 84\r\nRESIZE_HEIGHT = 84\r\n\r\n\r\ndef get_actions(game_or_env):\r\n    if isinstance(game_or_env, str):\r\n        env = gym.make(game_or_env)\r\n    else:\r\n        env = game_or_env\r\n\r\n    if isinstance(env.action_space, Discrete):\r\n        num_actions = env.action_space.n\r\n    elif isinstance(env.action_space, Box):\r\n        num_actions = np.prod(env.action_space.shape)\r\n    else:\r\n        raise Exception(\'Unsupported Action Space \\\'{}\\\'\'.format(\r\n            type(env.action_space).__name__))\r\n\r\n    indices = range(num_actions)\r\n    if env.spec.id in [\'Pong-v0\', \'Breakout-v0\']:\r\n        # Gym currently specifies 6 actions for pong and breakout when only 3 are needed\r\n        # TODO: patch the environments instead\r\n        num_actions = 3\r\n        indices = [1 ,2, 3]\r\n\r\n    return num_actions, env.action_space, indices\r\n\r\n\r\ndef get_input_shape(game):\r\n    env = gym.make(game)\r\n    if isinstance(env.observation_space, Discrete):\r\n        return [env.observation_space.n]\r\n    elif len(env.observation_space.shape) == 1:\r\n        return list(env.observation_space.shape)\r\n    else:\r\n        return [RESIZE_WIDTH, RESIZE_HEIGHT]\r\n\r\n\r\nclass AtariEnvironment(object):\r\n    """"""\r\n    Small wrapper for gym atari environments.\r\n    Responsible for preprocessing screens and holding on to a screen buffer \r\n    of size agent_history_length from which environment state\r\n    is constructed.\r\n    """"""\r\n    def __init__(self, game, seed, visualize=False, use_rgb=False, resized_width=RESIZE_WIDTH,\r\n                 resized_height=RESIZE_HEIGHT, agent_history_length=4, frame_skip=4,\r\n                 max_episode_steps=None, single_life_episodes=False):\r\n        self.game = game\r\n        self.env = gym.make(game)\r\n        self.env.seed(seed)\r\n        self.env.frameskip = frame_skip\r\n        if max_episode_steps:\r\n            self.env.spec.max_episode_steps = max_episode_steps\r\n\r\n        self.resized_width = resized_width\r\n        self.resized_height = resized_height\r\n        self.agent_history_length = agent_history_length\r\n        self.single_life_episodes = single_life_episodes\r\n        self.visualize = visualize\r\n        self.use_rgb = use_rgb\r\n\r\n        # Screen buffer of size AGENT_HISTORY_LENGTH to be able\r\n        # to build state arrays of size [1, AGENT_HISTORY_LENGTH, width, height]\r\n        self.state_buffer = deque(maxlen=self.agent_history_length-1)\r\n        self.gym_actions = get_actions(self.env)[2]\r\n\r\n        \r\n    def get_lives(self):\r\n        if hasattr(self.env.env, \'ale\'):\r\n            return self.env.env.ale.lives()\r\n        else:\r\n            return 0\r\n\r\n\r\n    def get_initial_state(self):\r\n        """"""\r\n        Resets the atari game, clears the state buffer\r\n        """"""\r\n        # Clear the state buffer\r\n        self.state_buffer.clear()\r\n\r\n        x_t = self.env.reset()\r\n        x_t = self.get_preprocessed_frame(x_t)\r\n\r\n        if self.use_rgb:\r\n            s_t = x_t\r\n        else:\r\n            s_t = np.stack([x_t]*self.agent_history_length, axis=len(x_t.shape))\r\n        \r\n        self.current_lives = self.get_lives()\r\n        for i in range(self.agent_history_length-1):\r\n            self.state_buffer.append(x_t)\r\n\r\n        return s_t\r\n\r\n    def get_preprocessed_frame(self, observation):\r\n        if isinstance(self.env.observation_space, Discrete):\r\n            expanded_obs = np.zeros(self.env.observation_space.n, dtype=np.float32)\r\n            expanded_obs[observation] = 1\r\n            return expanded_obs\r\n        elif len(observation.shape) > 1:\r\n            if not self.use_rgb:\r\n                observation = rgb2gray(observation)\r\n            return resize(observation, (self.resized_width, self.resized_height))\r\n        else:\r\n            return observation\r\n\r\n    def get_state(self, frame):\r\n        if self.use_rgb:\r\n            state = frame\r\n        else:\r\n            state = np.empty(list(frame.shape)+[self.agent_history_length])\r\n            for i in range(self.agent_history_length-1):\r\n                state[..., i] = self.state_buffer[i] \r\n            state[..., self.agent_history_length-1] = frame\r\n\r\n        return state\r\n\r\n    def next(self, action):\r\n        """"""\r\n        Excecutes an action in the gym environment.\r\n        Builds current state (concatenation of agent_history_length-1 previous frames and current one).\r\n        Pops oldest frame, adds current frame to the state buffer.\r\n        Returns current state.\r\n        """"""\r\n        if self.visualize:\r\n            self.env.render()\r\n        \r\n        if isinstance(self.env.action_space, Discrete):\r\n            action_index = np.argmax(action)\r\n            action = self.gym_actions[action_index]\r\n\r\n        frame, reward, terminal, info = self.env.step(action)\r\n\r\n        frame = self.get_preprocessed_frame(frame)\r\n        state = self.get_state(frame)\r\n\r\n        self.state_buffer.append(frame)\r\n\r\n        if self.single_life_episodes:\r\n            terminal |= self.get_lives() < self.current_lives\r\n        self.current_lives = self.get_lives()\r\n\r\n        return state, reward, terminal\r\n\r\n\r\n'"
environments/emulator.py,0,"b'# -*- coding: utf-8 -*-\r\n# Emulator code inspired by:\r\n# https://github.com/Jabberwockyll/deep_rl_ale/\r\nimport numpy as np\r\nfrom ale_python_interface import ALEInterface\r\nimport cv2\r\n#from skimage.transform import resize\r\n#from skimage.color import rgb2gray\r\nimport sys\r\nimport random\r\nimport utils.logger\r\nlogger = utils.logger.getLogger(\'emulator\')\r\n\r\n#import matplotlib.pyplot as plt\r\n\r\nIMG_SCALE = 255.0\r\nIMG_SIZE_X = 84\r\nIMG_SIZE_Y = 84\r\nNR_IMAGES = 4\r\nACTION_REPEAT = 4\r\nMAX_START_WAIT = 30\r\nFRAMES_IN_POOL = 2\r\nBLEND_METHOD = \'max_pool\'\r\nRANDOM_PLAY_STEPS = 1000000\r\n\r\nclass Emulator:\r\n    def __init__(self, rom_path, rom_name, visualize, actor_id, rseed, single_life_episodes = False):\r\n        \r\n        self.ale = ALEInterface()\r\n\r\n        self.ale.setInt(""random_seed"", rseed * (actor_id +1))\r\n\r\n        # For fuller control on explicit action repeat (>= ALE 0.5.0) \r\n        self.ale.setFloat(""repeat_action_probability"", 0.0)\r\n        \r\n        # Disable frame_skip and color_averaging\r\n        # See: http://is.gd/tYzVpj\r\n        self.ale.setInt(""frame_skip"", 1)\r\n        self.ale.setBool(""color_averaging"", False)\r\n        self.ale.loadROM(rom_path + ""/"" + rom_name + "".bin"")\r\n        self.legal_actions = self.ale.getMinimalActionSet()        \r\n        self.screen_width,self.screen_height = self.ale.getScreenDims()\r\n        #self.ale.setBool(\'display_screen\', True)\r\n        \r\n        # Processed historcal frames that will be fed in to the network \r\n        # (i.e., four 84x84 images)\r\n        self.screen_images_processed = np.zeros((IMG_SIZE_X, IMG_SIZE_Y, \r\n            NR_IMAGES)) \r\n        self.rgb_screen = np.zeros((self.screen_height,self.screen_width, 3), dtype=np.uint8)\r\n        self.gray_screen = np.zeros((self.screen_height,self.screen_width,1), dtype=np.uint8)\r\n\r\n        self.frame_pool = np.empty((2, self.screen_height, self.screen_width))\r\n        self.current = 0\r\n        self.lives = self.ale.lives()\r\n\r\n        self.visualize = visualize\r\n        self.visualize_processed = False\r\n        self.windowname = rom_name + \' \' + str(actor_id)\r\n        if self.visualize:\r\n            logger.debug(""Opening emulator window..."")\r\n            #from skimage import io\r\n            #io.use_plugin(\'qt\')\r\n            cv2.startWindowThread()\r\n            cv2.namedWindow(self.windowname)\r\n            logger.debug(""Emulator window opened"")\r\n            \r\n        if self.visualize_processed:\r\n            logger.debug(""Opening processed frame window..."")\r\n            cv2.startWindowThread()\r\n            logger.debug(""Processed frame window opened"")\r\n            cv2.namedWindow(self.windowname + ""_processed"")\r\n            \r\n        self.single_life_episodes = single_life_episodes\r\n\r\n    def get_screen_image(self):\r\n        """""" Add screen (luminance) to frame pool """"""\r\n        # [screen_image, screen_image_rgb] = [self.ale.getScreenGrayscale(), \r\n        #     self.ale.getScreenRGB()]\r\n        self.ale.getScreenGrayscale(self.gray_screen)\r\n        self.ale.getScreenRGB(self.rgb_screen)\r\n        self.frame_pool[self.current] = np.squeeze(self.gray_screen)\r\n        self.current = (self.current + 1) % FRAMES_IN_POOL\r\n        return self.rgb_screen\r\n\r\n    def new_game(self):\r\n        """""" Restart game """"""\r\n        self.ale.reset_game()\r\n        self.lives = self.ale.lives()\r\n\r\n        if MAX_START_WAIT < 0:\r\n            logger.debug(""Cannot time travel yet."")\r\n            sys.exit()\r\n        elif MAX_START_WAIT > 0:\r\n            wait = random.randint(0, MAX_START_WAIT)\r\n        else:\r\n            wait = 0\r\n        for _ in xrange(wait):\r\n            self.ale.act(self.legal_actions[0])\r\n\r\n    def process_frame_pool(self):\r\n        """""" Preprocess frame pool """"""\r\n        \r\n        img = None\r\n        if BLEND_METHOD == ""max_pool"":\r\n            img = np.amax(self.frame_pool, axis=0)\r\n        \r\n        #img resize(img[:210, :], (84, 84))\r\n        img = cv2.resize(img[:210, :], (84, 84), \r\n            interpolation=cv2.INTER_LINEAR)\r\n        \r\n        img = img.astype(np.float32)\r\n        img *= (1.0/255.0)\r\n        \r\n        return img\r\n        # Reduce height to 210, if not so\r\n        #cropped_img = img[:210, :]\r\n        # Downsample to 110x84\r\n        #down_sampled_img = resize(cropped_img, (84, 84))\r\n        \r\n        # Crop to 84x84 playing area\r\n        #stackable_image = down_sampled_img[:, 26:110]\r\n        #return stackable_image\r\n\r\n    def action_repeat(self, a):\r\n        """""" Repeat action and grab screen into frame pool """"""\r\n        reward = 0\r\n        for i in xrange(ACTION_REPEAT):\r\n            reward += self.ale.act(self.legal_actions[a])\r\n            new_screen_image_rgb = self.get_screen_image()\r\n        return reward, new_screen_image_rgb\r\n\r\n    def get_reshaped_state(self, state):\r\n        return np.reshape(state, \r\n            (1, IMG_SIZE_X, IMG_SIZE_Y, NR_IMAGES))\r\n        #return np.reshape(self.screen_images_processed, \r\n        #    (1, IMG_SIZE_X, IMG_SIZE_Y, NR_IMAGES))\r\n\r\n    def get_initial_state(self):\r\n        """""" Get the initial state """"""\r\n        self.new_game()\r\n        for step in xrange(NR_IMAGES):\r\n            reward, new_screen_image_rgb = self.action_repeat(0)\r\n            self.screen_images_processed[:, :, step] = self.process_frame_pool()\r\n            self.show_screen(new_screen_image_rgb)\r\n        if self.is_terminal():\r\n            MAX_START_WAIT -= 1\r\n            return self.get_initial_state()\r\n        return np.copy(self.screen_images_processed) #get_reshaped_state()      \r\n\r\n    def next(self, action):\r\n        """""" Get the next state, reward, and game over signal """"""\r\n        reward, new_screen_image_rgb = self.action_repeat(np.argmax(action))\r\n        self.screen_images_processed[:, :, 0:3] = \\\r\n            self.screen_images_processed[:, :, 1:4]\r\n        self.screen_images_processed[:, :, 3] = self.process_frame_pool()\r\n        self.show_screen(new_screen_image_rgb)\r\n        terminal = self.is_terminal()\r\n        self.lives = self.ale.lives()\r\n        return np.copy(self.screen_images_processed), reward, terminal #get_reshaped_state(), reward, terminal\r\n    \r\n    def show_screen(self, image):\r\n        """""" Show visuals for raw and processed images """"""\r\n        if self.visualize:\r\n            #io.imshow(image[:210, :], fancy=True)\r\n            cv2.imshow(self.windowname, image[:210, :])\r\n        if self.visualize_processed:\r\n            #io.imshow(self.screen_images_processed[:, :, 3], fancy=True)\r\n            cv2.imshow(self.windowname + ""_processed"", self.screen_images_processed[:, :, 3])\r\n            \r\n    def is_terminal(self):\r\n        if self.single_life_episodes:\r\n            return (self.is_over() or (self.lives > self.ale.lives()))\r\n        else:\r\n            return self.is_over()\r\n\r\n    def is_over(self):\r\n        return self.ale.game_over()\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    emulator = Emulator(""../atari_roms"", ""breakout"", True, 0, 1)\r\n    emulator.get_initial_state()\r\n    n_actions = len(emulator.legal_actions)\r\n    one_hot_action = np.zeros(n_actions)\r\n    for _ in xrange(RANDOM_PLAY_STEPS):\r\n        one_hot_action[random.randint(0, n_actions - 1)] = 1\r\n        _, _, over = emulator.next(one_hot_action, False)\r\n        if over:\r\n            break\r\n        one_hot_action.fill(0)'"
networks/__init__.py,0,b''
networks/continuous_actions.py,16,"b'# -*- encoding: utf-8 -*-\nimport layers\nimport numpy as np\nimport tensorflow as tf\nfrom q_network import QNetwork\nfrom utils.distributions import DiagNormal\nfrom policy_v_network import PolicyValueNetwork\n\n\nclass ContinuousPolicyValueNetwork(PolicyValueNetwork):\n    \'\'\'\n    Shared policy-value network with polciy head parametrizing\n    multivariate normal with diagonal covariance\n    \'\'\'\n    def __init__(self, conf, **kwargs):\n        self.action_space = conf[\'args\'].action_space\n        self.use_state_dependent_std = False\n        super(ContinuousPolicyValueNetwork, self).__init__(conf, **kwargs)\n\n    def _build_policy_head(self, input_state):\n        self.adv_actor_ph = tf.placeholder(""float"", [None], name=\'advantage\')       \n        self.w_mu, self.b_mu, self.mu = layers.fc(\n            \'mean\', input_state, self.num_actions, activation=\'linear\')\n        self.sigma, dist_params = self._build_sigma(input_state)\n\n        self.dist = DiagNormal(dist_params)\n        self.log_output_selected_action = self.dist.log_likelihood(self.selected_action_ph)\n        \n        self.output_layer_entropy = self.dist.entropy()\n        self.entropy = tf.reduce_sum(self.output_layer_entropy)\n\n        self.actor_objective = -tf.reduce_sum(\n            self.log_output_selected_action * self.adv_actor_ph\n            + self.beta * self.output_layer_entropy\n        )\n        self.sample_action = self.dist.sample()\n\n        return self.actor_objective\n\n    def _build_sigma(self, input_state):\n        if self.use_state_dependent_std:\n            self.w_sigma2, self.b_sigma2, self.sigma_hat = layers.fc(\n                \'std2\', input_state, self.num_actions, activation=\'linear\')\n            self.sigma2 = tf.log(1+tf.exp(self.sigma_hat))\n            sigma = tf.sqrt(self.sigma2 + 1e-8)\n            return sigma, tf.concat([self.mu, sigma], 1)\n        else:\n            self.log_sigma = tf.get_variable(\'log_sigma\', self.mu.get_shape().as_list()[1],\n                dtype=tf.float32, initializer=tf.random_uniform_initializer(-2, -1))\n            sigma = tf.expand_dims(tf.exp(self.log_sigma), 0)\n            tiled_sigma = tf.tile(sigma, [tf.shape(self.mu)[0], 1])\n            return sigma, tf.concat([self.mu, tiled_sigma], 1)\n\n    def get_action(self, session, state, lstm_state=None):\n        feed_dict = {self.input_ph: [state]}\n        if lstm_state is not None:\n            feed_dict[self.step_size] = [1]\n            feed_dict[self.initial_lstm_state] = lstm_state\n\n            action, lstm_state, params = session.run([\n                self.sample_action,\n                self.lstm_state,\n                self.dist.params()], feed_dict=feed_dict)\n\n            return action[0], params, lstm_state\n        else:\n            action, params = session.run([\n                self.sample_action,\n                self.dist.params()], feed_dict=feed_dict)\n\n            return action[0], params[0]\n\n    def get_action_and_value(self, session, state, lstm_state=None):\n        feed_dict = {self.input_ph: [state]}\n        if lstm_state is not None:\n            feed_dict[self.step_size] = [1]\n            feed_dict[self.initial_lstm_state] = lstm_state\n\n            action, v, lstm_state, params = session.run([\n                self.sample_action,\n                self.output_layer_v,\n                self.lstm_state,\n                self.dist.params()], feed_dict=feed_dict)\n\n            return action[0], v[0, 0], params[0], lstm_state\n        else:\n            action, v, params = session.run([\n                self.sample_action,\n                self.output_layer_v,\n                self.dist.params()], feed_dict=feed_dict)\n\n            return action[0], v[0, 0], params[0]\n\n\nclass ContinuousPolicyNetwork(ContinuousPolicyValueNetwork):\n    def __init__(self, conf):\n        super(ContinuousPolicyNetwork, self).__init__(conf, use_value_head=False)\n\n\nclass NAFNetwork(QNetwork):\n    \'\'\'\n    Implements Normalized Advantage Functions from ""Continuous Deep Q-Learning\n    with Model-based Acceleration"" (https://arxiv.org/pdf/1603.00748.pdf)\n    \'\'\'\n    def _build_q_head(self, input_state):\n        self.w_value, self.b_value, self.value = layers.fc(\'fc_value\', input_state, 1, activation=\'linear\')\n        self.w_L, self.b_L, self.L_full = layers.fc(\'L_full\', input_state, self.num_actions, activation=\'linear\')\n        self.w_mu, self.b_mu, self.mu = layers.fc(\'mu\', input_state, self.num_actions, activation=\'linear\')\n\n        #elements above the main diagonal in L_full are unused\n        D = tf.matrix_band_part(tf.exp(self.L_full) - L_full, 0, 0)\n        L = tf.matrix_band_part(L_full, -1, 0) + D\n\n        LT_u_minus_mu = tf.einsum(\'ikj,ik\', L, self.selected_action_ph  - self.mu)\n        self.advantage = tf.einsum(\'ijk,ikj->i\', LT_u_minus_mu, LT_u_minus_mu)\n\n        q_selected_action = self.value + self.advantage\n        diff = tf.subtract(self.target_ph, q_selected_action)\n        return self._value_function_loss(diff)\n\n\n\n'"
networks/custom_lstm.py,12,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\n\nfrom tensorflow.contrib.rnn import RNNCell\n\nclass CustomBasicLSTMCell(RNNCell):\n\t\'\'\'\n\tCustom Basic LSTM recurrent network cell.\n\t(Modified to store matrix and bias as member variable.)\n\n\tThe implementation is based on: http://arxiv.org/abs/1409.2329.\n\n\tWe add forget_bias (default: 1) to the biases of the forget gate in order to\n\treduce the scale of forgetting in the beginning of the training.\n\n\tIt does not allow cell clipping, a projection layer, and does not\n\tuse peep-hole connections: it is the basic baseline.\n\n\tFor advanced models, please use the full LSTMCell that follows.\n\t\'\'\'\n\n\tdef __init__(self, num_units, forget_bias=1.0):\n\t\tself._num_units = num_units\n\t\tself._forget_bias = forget_bias\n\n\t@property\n\tdef state_size(self):\n\t\treturn 2 * self._num_units\n\n\t@property\n\tdef output_size(self):\n\t\treturn self._num_units\n\n\tdef __call__(self, inputs, state, scope=None):\n\t\t\'\'\'Long short-term memory cell (LSTM).\'\'\'\n\t\tprint \'Inputs / Cell State:\', inputs.get_shape(), state.get_shape()\n\t\twith tf.variable_scope(scope or type(self).__name__):  # ""BasicLSTMCell""\n\t\t\t# Parameters of gates are concatenated into one multiply for efficiency.\n\t\t\tc, h = tf.split(axis=1, num_or_size_splits=2, value=state)\n\n\t\t\tconcat = self._linear([inputs, h], 4 * self._num_units, True)\n\n\t\t\t# i = input_gate, j = new_input, f = forget_gate, o = output_gate\n\t\t\ti, j, f, o = tf.split(axis=1, num_or_size_splits=4, value=concat)\n\n\t\t\tnew_c = c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * tf.tanh(j)\n\t\t\tnew_h = tf.tanh(new_c) * tf.sigmoid(o)\n\n\t\t\treturn new_h, tf.concat(axis=1, values=[new_c, new_h], name=\'cell_hidden_concat\')\n\n\tdef _linear(self, args, output_size, bias, bias_start=0.0, scope=None):\n\t\t\'\'\'\n\t\tLinear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n  \n\t\tArgs:\n\t\t  args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n\t\t  output_size: int, second dimension of W[i].\n\t\t  bias: boolean, whether to add a bias term or not.\n\t\t  bias_start: starting value to initialize the bias; 0 by default.\n\t\t  scope: VariableScope for the created subgraph; defaults to ""Linear"".\n  \n\t\tReturns:\n\t\t  A 2D Tensor with shape [batch x output_size] equal to\n\t\t  sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n  \n\t\tRaises:\n\t\t  ValueError: if some of the arguments has unspecified or wrong shape.\n\t\t\'\'\'\n\t\tif args is None or (isinstance(args, (list, tuple)) and not args):\n\t\t\traise ValueError(""`args` must be specified"")\n\t\tif not isinstance(args, (list, tuple)):\n\t\t\targs = [args]\n\n  \n\t\t# Calculate the total size of arguments on dimension 1.\n\t\ttotal_arg_size = 0\n\t\tshapes = [a.get_shape().as_list() for a in args]\n\t\tfor shape in shapes:\n\t\t\tif len(shape) != 2:\n\t\t\t\traise ValueError(""Linear is expecting 2D arguments: %s"" % str(shapes))\n\t\t\tif not shape[1]:\n\t\t\t\traise ValueError(""Linear expects shape[1] of arguments: %s"" % str(shapes))\n\t\t\telse:\n\t\t\t\ttotal_arg_size += shape[1]\n  \n\t\t# Now the computation.\n\t\twith tf.variable_scope(scope or ""Linear""):      \n\t\t\tmatrix = tf.get_variable(""Matrix"", [total_arg_size, output_size])\n\n\t\t\tif len(args) == 1:\n\t\t\t\tres = tf.matmul(args[0], matrix)\n\t\t\telse:\n\t\t\t\tres = tf.matmul(tf.concat(axis=1, values=args, name=\'_linear_concat\'), matrix)\n\t\t\tif not bias:\n\t\t\t\treturn res\n\t\t\tbias_term = tf.get_variable(\n\t\t\t\t""Bias"", [output_size],\n\t\t\t\tinitializer=tf.constant_initializer(bias_start))\n\n\t\t\t# Store as a member for copying. (Customized here)\n\t\t\tself.matrix = matrix      \n\t\t\tself.bias = bias_term\n      \n\t\treturn res + bias_term\n'"
networks/dueling_network.py,3,"b""# -*- coding: utf-8 -*-\nimport layers\nimport tensorflow as tf\nfrom q_network import QNetwork\n\n\nclass DuelingNetwork(QNetwork):\n \n    def _build_q_head(self, input_state):\n        self.w_value, self.b_value, self.value = layers.fc('fc_value', input_state, 1, activation='linear')\n        self.w_adv, self.b_adv, self.advantage = layers.fc('fc_advantage', input_state, self.num_actions, activation='linear')\n\n        self.output_layer = (\n            self.value + self.advantage\n            - tf.reduce_mean(\n                self.advantage,\n                axis=1,\n                keep_dims=True\n            )\n        )\n\n        q_selected_action = tf.reduce_sum(self.output_layer * self.selected_action_ph, axis=1)\n        diff = tf.subtract(self.target_ph, q_selected_action)\n        return self._value_function_loss(diff)\n\n"""
networks/feudal_network.py,19,"b""import tensorflow as tf\nfrom tensorflow.contrib.rnn import RNNCell, BasicLSTMCell\nfrom network import Network\n\n\nclass FeudalNetwork(Network):\n\tdef __init__(self, conf):\n\t\tsuper(PolicyValueNetwork, self).__init__(conf)\n\n\t\tself.hidden_state_size = 256\n\t\tself.step_size = tf.placeholder(tf.float32, [None], name='step_size')\n\n\t\twith tf.variable_scope('manager'):\n\t\t\tself.build_manager()\n\t\twith tf.variable_scope('worker'):\n\t\t\tself.build_worker()\n\n\tdef build_manager(self, input_state):\n\t\tself.manager_initial_lstm_state = tf.placeholder(\n\t\t\ttf.float32, [None, 2*self.hidden_state_size], name='initital_state')\n\t\tself.dlstm = DilatedLSTM(\n\t\t\tinput_state,\n\t\t\tself.manager_initial_lstm_state,\n\t\t\tself.hidden_state_size,\n\t\t\tself.step_size)\n\n\n\n\tdef build_worker(self, input_state):\n\t\tself.worker_initial_lstm_state, self.worker_lstm_state, self.worker_out = self._build_lstm(input_state)\n\n\n\tdef _build_lstm(self, input_state):\n\t\tinitial_lstm_state = tf.placeholder(\n\t\t\ttf.float32, [None, 2*self.hidden_state_size], name='initital_state')\n\t\tlstm_cell = BasicLSTMCell(self.hidden_state_size, forget_bias=1.0, state_is_tuple=True)\n\t\t\n\t\tbatch_size = tf.shape(self.step_size)[0]\n\t\tox_reshaped = tf.reshape(input_state,\n\t\t\tbatch_size, -1, input_state.get_shape().as_list()[-1]])\n\n\t\tlstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n\t\t\tlstm_cell,\n\t\t\tox_reshaped,\n\t\t\tinitial_state=initial_lstm_state,\n\t\t\tsequence_length=self.step_size,\n\t\t\ttime_major=False)\n\n\t\tout = tf.reshape(lstm_outputs, [-1,self.hidden_state_size], name='reshaped_lstm_outputs')\n\t\treturn initial_lstm_state, lstm_state, out\n\n\nclass DilatedLSTM(object):\n\tdef __init__(self, inputs, initial_state, hidden_state_size\n\t\t,max_steps, num_cores=10, pool_size=10):\n\n\t\tself.shared_cell = BasicLSTMCell(hidden_state_size)\n\t\tself.initial_state = initial_state\n\t\tself.max_steps = max_steps\n\t\tself.num_cores = num_cores\n\t\tself.pool_size = pool_size\n\t\tself.inputs = inputs\n\t\tself._build_ops()\n\n\n\tdef _build_ops(self):\n\t\ti0 = tf.constant(0, dtype=tf.int32)\n\t\tloop_condition = lambda i, inputs, state: tf.less(i, self.max_steps)\n\n\t\tdef body(i, inputs, full_state):\n\t\t\tidx = i % self.num_cores\n\t\t\tprev_state = full_state[idx]\n\t\t\tinputs, full_state[idx] = self.shared_cell(inputs, prev_state)\n\n\t\t\treturn i+1, inputs, full_state\n\n\t\t_, inputs, full_state = tf.while_loop(\n\t\t\tloop_condition,\n\t\t\tbody,\n\t\t\tloop_vars=[i0,\n\t\t\t\t\t   self.inputs,\n\t\t\t\t\t   self.initial_state])\n\n\t\tlstm_outputs = tf.reshape(tf.concat(full_state, 1), [-1,256])\n\t\tself.outpus = tf.avg_pool(\n\t\t\ttf.expand(lstm_outputs, -1),\n\t\t\t[1, self.pool_size, 1, 1],\n\t\t\tstrides=[1, 1, 1, 1],\n\t\t\tpadding='SAME')\n\n\n\tdef zero_state(self):\n\t\treturn [self.shared_cell.zero_state(\n\t\t\t\t\ttf.shape(self.max_steps)[0],\n\t\t\t\t\ttf.float32) for _ in range(self.stride)]\n\t\t\n\n\n\n"""
networks/layers.py,23,"b""import tensorflow as tf\nimport numpy as np\n\n\ndef flatten(_input):\n    shape = _input.get_shape().as_list()\n    dim = reduce(lambda a, b: a*b, shape[1:])\n    return tf.reshape(_input, [-1, dim], name='_flattened')\n\ndef apply_activation(out, name, activation):\n    if activation == 'relu':\n        return tf.nn.relu(out, name=name+'_relu')\n    elif activation == 'softplus':\n        return tf.nn.softplus(out, name=name+'_softplus')\n    elif activation == 'tanh':\n        return tf.nn.tanh(out, name=name+'_tanh')\n    elif activation == 'selu':\n        return selu(out, name=name+'_selu')\n    elif activation == 'linear':\n        return out\n    else:\n        raise Exception('Invalid activation type \\'{}\\''.format(activation))\n\ndef selu(x, name):\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    return scale*tf.where(x>0.0, x, alpha*tf.exp(x)-alpha)\n\ndef conv2d(name, _input, filters, size, channels, stride, activation='relu', padding='VALID', data_format='NHWC'):\n    if data_format == 'NHWC':\n        strides = [1, stride, stride, 1]\n    else:\n        strides = [1, 1, stride, stride]\n\n    w = conv_weight_variable([size, size, channels, filters], name+'_weights')\n    b = conv_bias_variable([filters], name+'_biases')\n    conv = tf.nn.conv2d(_input, w, strides=strides,\n            padding=padding, data_format=data_format, name=name+'_convs') + b\n\n    out = apply_activation(conv, name, activation)\n    return w, b, out\n\ndef conv_weight_variable(shape, name):\n    # initializer=tf.contrib.layers.xavier_initializer()\n    # initializer = tf.truncated_normal_initializer(0, 0.02)\n    d = 1.0 / np.sqrt(np.prod(shape[:-1]))\n    initializer = tf.random_uniform_initializer(-d, d)\n    return tf.get_variable(name, shape, dtype=tf.float32, initializer=initializer)\n\ndef conv_bias_variable(shape, name):\n    initializer = tf.zeros_initializer()\n    return tf.get_variable(name, shape, dtype=tf.float32, initializer=initializer)\n\ndef fc(name, _input, output_dim, activation='relu'):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim], name+'_weights')\n    b = fc_bias_variable([output_dim], input_dim, name+'_biases')\n    out = tf.matmul(_input, w) + b\n\n    out = apply_activation(out, name, activation)\n    return w, b, out\n\ndef fc_weight_variable(shape, name):\n    # initializer = tf.contrib.layers.xavier_initializer()\n    # initializer = tf.random_normal_initializer(stddev=0.02)\n    d = 1.0 / np.sqrt(shape[0])\n    initializer = tf.random_uniform_initializer(-d, d)\n    return tf.get_variable(name, shape, dtype=tf.float32, initializer=initializer)\n\ndef fc_bias_variable(shape, input_channels, name):\n    initializer = tf.zeros_initializer()\n    return tf.get_variable(name, shape, dtype=tf.float32, initializer=initializer)\n\ndef softmax(name, _input, output_dim):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim], name+'_weights')\n    b = fc_bias_variable([output_dim], input_dim, name+'_biases')\n    out = tf.nn.softmax(tf.add(tf.matmul(_input, w), b), name=name+'_policy')\n \n    return w, b, out\n\ndef softmax_and_log_softmax(name, _input, output_dim):\n    input_dim = _input.get_shape().as_list()[1]\n    w = fc_weight_variable([input_dim, output_dim], name+'_weights')\n    b = fc_bias_variable([output_dim], input_dim, name+'_biases')\n    xformed = tf.matmul(_input, w) + b\n    out = tf.nn.softmax(xformed, name=name+'_policy')\n    log_out = tf.nn.log_softmax(xformed, name=name+'_log_policy')\n\n    return w, b, out, log_out"""
networks/nec_network.py,3,"b'# -*- coding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\n\nfrom q_network import QNetwork\nfrom utils.dnd import DND\n\n\nclass NECNetwork(QNetwork):\n\tdef __init__(self, conf):\n\t\tself.delta = 1e-3\n\t\tself.capacity = 100000\n\t\tself.action_dnds = [DND(self.capacity) for _ in self.num_actions]\n\n\t\tsuper(NECNetwork, self).__init__(conf)\n\n\n\tdef _build_q_head(self, input_state):\n\t\tself.q_values = tf.py_func(self.q_value_lookup, key, tf.float32)\n\t\tself.q_selected_action = tf.reduce_sum(self.q_values * self.selected_action_ph, axis=1)\n\n\t\tdiff = tf.subtract(self.target_ph, self.q_selected_action)\n\t\treturn self._value_function_loss(diff)\n\n\n\tdef q_value_lookup(self, key):\n\t\tvalues = np.zeros((self.num_actions,), np.float32)\n\n\t\tfor i, dnd in enumerate(self.action_dnds):\n\t\t\tvalues, distances = dnd.query(input_state)\n\t\t\tkernel = 1 / (distances**2 + self.delta)\n\t\t\tvalues[i] = kernel * values\n\n\t\treturn values\n'"
networks/network.py,29,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport layers\r\nimport utils.ops\r\nfrom custom_lstm import CustomBasicLSTMCell\r\n\r\n\r\nclass Network(object):\r\n\r\n    def __init__(self, conf):\r\n        \'\'\'\r\n        Initialize hyper-parameters, set up optimizer and network \r\n        layers common across Q and Policy/V nets\r\n        \'\'\'\r\n        self.name = conf[\'name\']\r\n        self.num_actions = conf[\'num_act\']\r\n        self.arch = conf[\'args\'].arch\r\n        self.batch_size = conf[\'args\'].batch_size\r\n        self.optimizer_type = conf[\'args\'].opt_type\r\n        self.optimizer_mode = conf[\'args\'].opt_mode\r\n        self.clip_loss_delta = conf[\'args\'].clip_loss_delta\r\n        self.clip_norm = conf[\'args\'].clip_norm\r\n        self.clip_norm_type = conf[\'args\'].clip_norm_type\r\n        self.input_shape = conf[\'input_shape\']\r\n        self.activation = conf[\'args\'].activation\r\n        self.max_local_steps = conf[\'args\'].max_local_steps\r\n        self.input_channels = 3 if conf[\'args\'].use_rgb else conf[\'args\'].history_length\r\n        self.use_recurrent = \'lstm\' in conf[\'args\'].alg_type\r\n        self.fc_layer_sizes = conf[\'args\'].fc_layer_sizes\r\n        self._init_placeholders()\r\n\r\n\r\n    def _init_placeholders(self):\r\n        with tf.variable_scope(self.name):\r\n            if self.arch == \'FC\':\r\n                self.input_ph = tf.placeholder(\'float32\', [self.batch_size]+self.input_shape+[self.input_channels], name=\'input\')\r\n            else: #assume image input\r\n                self.input_ph = tf.placeholder(\'float32\',[self.batch_size, 84, 84, self.input_channels], name=\'input\')\r\n\r\n            if self.use_recurrent:\r\n                self.hidden_state_size = 256\r\n                self.step_size = tf.placeholder(tf.float32, [None], name=\'step_size\')\r\n                self.initial_lstm_state = tf.placeholder(\r\n                    tf.float32, [None, 2*self.hidden_state_size], name=\'initital_state\')\r\n\r\n            self.selected_action_ph = tf.placeholder(\r\n                \'float32\', [self.batch_size, self.num_actions], name=\'selected_action\')\r\n\r\n\r\n    def _build_encoder(self):\r\n        with tf.variable_scope(self.name):\r\n            if self.arch == \'FC\':\r\n                layer_i = layers.flatten(self.input_ph)\r\n                for i, layer_size in enumerate(self.fc_layer_sizes):\r\n                    layer_i = layers.fc(\'fc{}\'.format(i+1), layer_i, layer_size, activation=self.activation)[-1]\r\n                self.ox = layer_i\r\n            elif self.arch == \'ATARI-TRPO\':\r\n                self.w1, self.b1, self.o1 = layers.conv2d(\'conv1\', self.input_ph, 16, 4, self.input_channels, 2, activation=self.activation)\r\n                self.w2, self.b2, self.o2 = layers.conv2d(\'conv2\', self.o1, 16, 4, 16, 2, activation=self.activation)\r\n                self.w3, self.b3, self.o3 = layers.fc(\'fc3\', layers.flatten(self.o2), 20, activation=self.activation)\r\n                self.ox = self.o3\r\n            elif self.arch == \'NIPS\':\r\n                self.w1, self.b1, self.o1 = layers.conv2d(\'conv1\', self.input_ph, 16, 8, self.input_channels, 4, activation=self.activation)\r\n                self.w2, self.b2, self.o2 = layers.conv2d(\'conv2\', self.o1, 32, 4, 16, 2, activation=self.activation)\r\n                self.w3, self.b3, self.o3 = layers.fc(\'fc3\', layers.flatten(self.o2), 256, activation=self.activation)\r\n                self.ox = self.o3\r\n            elif self.arch == \'NATURE\':\r\n                self.w1, self.b1, self.o1 = layers.conv2d(\'conv1\', self.input_ph, 32, 8, self.input_channels, 4, activation=self.activation)\r\n                self.w2, self.b2, self.o2 = layers.conv2d(\'conv2\', self.o1, 64, 4, 32, 2, activation=self.activation)\r\n                self.w3, self.b3, self.o3 = layers.conv2d(\'conv3\', self.o2, 64, 3, 64, 1, activation=self.activation)\r\n                self.w4, self.b4, self.o4 = layers.fc(\'fc4\', layers.flatten(self.o3), 512, activation=self.activation)\r\n                self.ox = self.o4\r\n            else:\r\n                raise Exception(\'Invalid architecture `{}`\'.format(self.arch))\r\n\r\n            if self.use_recurrent:\r\n                with tf.variable_scope(\'lstm_layer\') as vs:\r\n                    self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(\r\n                        self.hidden_state_size, state_is_tuple=True, forget_bias=1.0)\r\n                    \r\n                    batch_size = tf.shape(self.step_size)[0]\r\n                    self.ox_reshaped = tf.reshape(self.ox,\r\n                        [batch_size, -1, self.ox.get_shape().as_list()[-1]])\r\n                    state_tuple = tf.contrib.rnn.LSTMStateTuple(\r\n                        *tf.split(self.initial_lstm_state, 2, 1))\r\n\r\n                    self.lstm_outputs, self.lstm_state = tf.nn.dynamic_rnn(\r\n                        self.lstm_cell,\r\n                        self.ox_reshaped,\r\n                        initial_state=state_tuple,\r\n                        sequence_length=self.step_size,\r\n                        time_major=False)\r\n\r\n                    self.lstm_state = tf.concat(self.lstm_state, 1)\r\n                    self.ox = tf.reshape(self.lstm_outputs, [-1,self.hidden_state_size], name=\'reshaped_lstm_outputs\')\r\n\r\n                    # Get all LSTM trainable params\r\n                    self.lstm_trainable_variables = [v for v in \r\n                        tf.trainable_variables() if v.name.startswith(vs.name)]\r\n\r\n            return self.ox\r\n\r\n\r\n    def _value_function_loss(self, diff):\r\n        if self.clip_loss_delta > 0:\r\n            # DEFINE HUBER LOSS\r\n            return 0.5 * tf.reduce_sum(tf.where(\r\n                tf.abs(diff) < self.clip_loss_delta,\r\n                tf.square(diff),\r\n                tf.square(self.clip_loss_delta)*tf.abs(diff)))\r\n        else:\r\n            return tf.nn.l2_loss(diff)\r\n\r\n\r\n    def _clip_grads(self, grads):\r\n        if self.clip_norm_type == \'ignore\':\r\n            return grads\r\n        elif self.clip_norm_type == \'global\':\r\n            return tf.clip_by_global_norm(grads, self.clip_norm)[0]\r\n        elif self.clip_norm_type == \'avg\':\r\n            return tf.clip_by_average_norm(grads, self.clip_norm)[0]\r\n        elif self.clip_norm_type == \'local\':\r\n            return [tf.clip_by_norm(g, self.clip_norm)\r\n                    for g in grads]\r\n\r\n\r\n    def _setup_shared_memory_ops(self):\r\n        # Placeholders for shared memory vars\r\n        self.params_ph = []\r\n        for p in self.params:\r\n            self.params_ph.append(tf.placeholder(tf.float32, \r\n                shape=p.get_shape(), \r\n                name=""shared_memory_for_{}"".format(\r\n                    (p.name.split(""/"", 1)[1]).replace("":"", ""_""))))\r\n            \r\n        # Ops to sync net with shared memory vars\r\n        self.sync_with_shared_memory = []\r\n        for i in xrange(len(self.params)):\r\n            self.sync_with_shared_memory.append(\r\n                self.params[i].assign(self.params_ph[i]))\r\n\r\n\r\n    def _build_gradient_ops(self, loss):\r\n        self.params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\r\n        self.flat_vars = utils.ops.flatten_vars(self.params)\r\n\r\n        grads = tf.gradients(loss, self.params)\r\n        self.get_gradients = self._clip_grads(grads)\r\n        self._setup_shared_memory_ops()\r\n\r\n\r\n    def get_input_shape(self):\r\n        return self.input_ph.get_shape().as_list()[1:]\r\n        \r\n'"
networks/policy_v_network.py,43,"b'# -*- encoding: utf-8 -*-\nimport layers\nimport numpy as np\nimport tensorflow as tf\nfrom network import Network\nfrom utils.distributions import Discrete\nfrom custom_lstm import CustomBasicLSTMCell\nfrom sequence_decoder import decoder, loop_gumbel_softmax\n\n\nclass PolicyValueNetwork(Network):\n \n    def __init__(self, conf, use_policy_head=True, use_value_head=True):\n        super(PolicyValueNetwork, self).__init__(conf)\n        self.beta = conf[\'args\'].entropy_regularisation_strength\n        self.share_encoder_weights = conf[\'args\'].share_encoder_weights\n\n        encoded_state = self._build_encoder()\n                \n        with tf.variable_scope(self.name):\n            self.loss = 0.0\n            if use_policy_head:\n                self.loss += self._build_policy_head(encoded_state)\n\n            if use_value_head:\n                if not self.share_encoder_weights:\n                    with tf.variable_scope(\'value_encoder\'):\n                        encoded_state = self._build_encoder()\n\n                self.loss += 0.5 * self._build_value_head(encoded_state)\n            self.loss *= tf.cast(tf.shape(self.input_ph)[0], tf.float32) / self.max_local_steps\n            self._build_gradient_ops(self.loss)\n\n    def _build_policy_head(self, input_state):\n        self.adv_actor_ph = tf.placeholder(""float"", [None], name=\'advantage\')       \n        self.wpi, self.bpi, self.logits = layers.fc(\n            \'logits\', input_state, self.num_actions, activation=\'linear\')\n        self.dist = Discrete(self.logits)\n\n        self.output_layer_entropy = self.dist.entropy()\n        self.entropy = tf.reduce_sum(self.output_layer_entropy)\n        \n        self.output_layer_pi = self.dist.probs\n        self.log_output_layer_pi = self.dist.log_probs\n        self.log_output_selected_action = self.dist.log_likelihood(self.selected_action_ph)\n        self.sample_action = self.dist.sample()\n\n        self.actor_objective = -tf.reduce_sum(\n            self.log_output_selected_action * self.adv_actor_ph\n            + self.beta * self.output_layer_entropy\n        )\n        return self.actor_objective\n\n    def _build_value_head(self, input_state):\n        self.critic_target_ph = tf.placeholder(\'float32\', [None], name=\'target\')\n        self.wv, self.bv, self.output_layer_v = layers.fc(\n            \'fc_value4\', input_state, 1, activation=\'linear\')\n        # Advantage critic\n        self.adv_critic = tf.subtract(self.critic_target_ph, tf.reshape(self.output_layer_v, [-1]))\n        # Critic loss\n        self.critic_loss = self._value_function_loss(self.adv_critic)\n        return self.critic_loss\n\n    def get_action(self, session, state, lstm_state=None):\n        feed_dict = {self.input_ph: [state]}\n        if lstm_state is not None:\n            feed_dict[self.step_size] = [1]\n            feed_dict[self.initial_lstm_state] = lstm_state\n\n            action, logits, lstm_state = session.run([\n                self.sample_action,\n                self.logits,\n                self.lstm_state], feed_dict=feed_dict)\n\n            return action, logits[0], lstm_state\n        else:\n            action, logits = session.run([\n                self.sample_action,\n                self.logits], feed_dict=feed_dict)\n\n            return action, logits[0]\n\n\n    def get_action_and_value(self, session, state, lstm_state=None):\n        feed_dict = {self.input_ph: [state]}\n        if lstm_state is not None:\n            feed_dict[self.step_size] = [1]\n            feed_dict[self.initial_lstm_state] = lstm_state\n\n            action, logits, v, lstm_state = session.run([\n                self.sample_action,\n                self.logits,\n                self.output_layer_v,\n                self.lstm_state], feed_dict=feed_dict)\n\n            return action, v[0, 0], logits[0], lstm_state\n\n        else:\n            action, logits, v = session.run([\n                self.sample_action,\n                self.logits,\n                self.output_layer_v], feed_dict=feed_dict)\n\n            return action, v[0, 0], logits[0]\n\n\nclass PolicyNetwork(PolicyValueNetwork):\n    def __init__(self, conf,):\n        super(PolicyNetwork, self).__init__(conf, use_value_head=False)\n\n\nclass PolicyRepeatNetwork(PolicyValueNetwork):\n \n    def __init__(self, conf):\n        \'\'\'\n        Extends action space to parametrize a discrete distribution over repetion lengths\n        for each original action\n        \'\'\'\n        self.max_repeat = 5\n        super(PolicyRepeatNetwork, self).__init__(conf)\n        \n\n    def _build_policy_head(self, input_state):\n        self.adv_actor_ph = tf.placeholder(""float"", [None], name=\'advantage\')\n\n        self.wpi, self.bpi, self.output_layer_pi, self.log_output_layer_pi = layers.softmax_and_log_softmax(\n            \'action_policy\', input_state, self.num_actions)\n\n        self.w_ar, self.b_ar, self.action_repeat_probs, self.log_action_repeat_probs = layers.softmax_and_log_softmax(\n            \'repeat_policy\', input_state, self.max_repeat)\n\n        self.selected_repeat = tf.placeholder(tf.int32, [None], name=\'selected_repeat_placeholder\')\n        self.selected_repeat_onehot = tf.one_hot(self.selected_repeat, self.max_repeat)\n\n        self.selected_repeat_prob = tf.reduce_sum(self.action_repeat_probs * self.selected_repeat_onehot, 1)\n        self.log_selected_repeat_prob = tf.reduce_sum(self.log_action_repeat_probs * self.selected_repeat_onehot, 1)\n\n        # Entropy: \xe2\x88\x91_a[-p_a ln p_a]\n        self.output_layer_entropy = tf.reduce_sum(\n            - 1.0 * tf.multiply(\n                tf.expand_dims(self.output_layer_pi, 2) * tf.expand_dims(self.action_repeat_probs, 1),\n                tf.expand_dims(self.log_output_layer_pi, 2) + tf.expand_dims(self.log_action_repeat_probs, 1)\n            ), axis=[1, 2])\n\n        self.entropy = tf.reduce_sum(self.output_layer_entropy)\n\n        self.log_output_selected_action = tf.reduce_sum(\n            self.log_output_layer_pi*self.selected_action_ph,\n            axis=1) + self.log_selected_repeat_prob\n\n        self.actor_objective = -tf.reduce_sum(\n            self.log_output_selected_action * self.adv_actor_ph\n            + self.beta * self.output_layer_entropy)\n\n        return self.actor_objective\n\n\n#This is still experimental\nclass SequencePolicyVNetwork(PolicyValueNetwork):\n\n    def __init__(self, conf):\n        \'\'\'Uses lstm decoder to output action sequences\'\'\'\n        self.max_decoder_steps = conf[\'args\'].max_decoder_steps\n        self.max_local_steps = conf[\'args\'].max_local_steps\n        super(SequencePolicyVNetwork, self).__init__(conf)\n\n        \n    def _build_policy_head(self, input_state):\n        self.adv_actor_ph = tf.placeholder(""float"", [self.batch_size], name=\'advantage\')\n\n        with tf.variable_scope(self.name+\'/lstm_decoder\') as vs:\n            self.action_outputs = tf.placeholder(tf.float32, [self.batch_size, None, self.num_actions+1], name=\'action_outputs\')\n            self.action_inputs = tf.placeholder(tf.float32, [self.batch_size, None, self.num_actions+1], name=\'action_inputs\')\n                \n            self.decoder_seq_lengths = tf.placeholder(tf.int32, [self.batch_size], name=\'decoder_seq_lengths\')\n            self.allowed_actions = tf.placeholder(tf.float32, [self.batch_size, None, self.num_actions+1], name=\'allowed_actions\')\n            self.use_fixed_action = tf.placeholder(tf.bool, name=\'use_fixed_action\')\n            self.temperature = tf.placeholder(tf.float32, name=\'temperature\')\n\n            self.decoder_hidden_state_size = input_state.get_shape().as_list()[-1]\n            self.decoder_lstm_cell = CustomBasicLSTMCell(self.decoder_hidden_state_size, forget_bias=1.0)\n            self.decoder_initial_state = tf.placeholder(tf.float32, [self.batch_size, 2*self.decoder_hidden_state_size], name=\'decoder_initial_state\')\n\n            self.network_state = tf.concat(axis=1, values=[\n                tf.zeros_like(input_state), input_state\n                # input_state, tf.zeros_like(input_state)\n            ])\n\n            self.W_actions = tf.get_variable(\'W_actions\', shape=[self.decoder_hidden_state_size, self.num_actions+1], dtype=\'float32\', initializer=tf.contrib.layers.xavier_initializer())\n            self.b_actions = tf.get_variable(\'b_actions\', shape=[self.num_actions+1], dtype=\'float32\', initializer=tf.zeros_initializer())\n\n\n            self.decoder_state, self.logits, self.actions = decoder(\n                self.action_inputs,\n                self.network_state,\n                self.decoder_lstm_cell,\n                self.decoder_seq_lengths,\n                self.W_actions,\n                self.b_actions,\n                self.max_decoder_steps,\n                vs,\n                self.use_fixed_action,\n                self.action_outputs,\n                loop_function=loop_gumbel_softmax(self.temperature),\n            )\n\n            self.decoder_trainable_variables = [\n                v for v in tf.trainable_variables()\n                if v.name.startswith(vs.name)\n            ]\n\n        print \'Decoder out: s,l,a=\', self.decoder_state.get_shape(), self.logits.get_shape(), self.actions.get_shape()\n\n\n        #mask softmax by allowed actions\n        exp_logits = tf.exp(self.logits) * self.allowed_actions\n        Z = tf.expand_dims(tf.reduce_sum(exp_logits, 2), 2)\n        self.action_probs = exp_logits / Z\n        log_action_probs = self.logits - tf.log(Z)\n\n        sequence_probs = tf.reduce_prod(tf.reduce_sum(self.action_probs * self.action_outputs, 2), 1)\n        log_sequence_probs = tf.reduce_sum(tf.reduce_sum(log_action_probs * self.action_outputs, 2), 1)\n\n        # \xe2\x88\x8fa_i * \xe2\x88\x91 log a_i\n        self.output_layer_entropy = - tf.reduce_sum(tf.stop_gradient(1 + log_sequence_probs) * log_sequence_probs)\n        self.entropy = - tf.reduce_sum(log_sequence_probs)\n\n        print \'sp, lsp:\', sequence_probs.get_shape(), log_sequence_probs.get_shape()\n\n\n        self.actor_advantage_term = tf.reduce_sum(log_sequence_probs[:self.max_local_steps] * self.adv_actor_ph)\n        self.actor_entropy_term = self.beta * self.output_layer_entropy\n        self.actor_objective = - (\n            self.actor_advantage_term\n            + self.actor_entropy_term\n        )\n\n        return self.actor_objective\n            \n\n'"
networks/q_network.py,4,"b'# -*- coding: utf-8 -*-\r\nimport layers\r\nimport tensorflow as tf\r\nfrom network import Network\r\n\r\n\r\nclass QNetwork(Network):\r\n\r\n    def __init__(self, conf):\r\n        super(QNetwork, self).__init__(conf)\r\n                \r\n        with tf.variable_scope(self.name):\r\n            self.target_ph = tf.placeholder(\'float32\', [None], name=\'target\')\r\n            encoded_state = self._build_encoder()\r\n\r\n            self.loss = self._build_q_head(encoded_state)\r\n            self._build_gradient_ops(self.loss)\r\n\r\n\r\n    def _build_q_head(self, input_state):\r\n        self.w_out, self.b_out, self.output_layer = layers.fc(\'fc_out\', input_state, self.num_actions, activation=""linear"")\r\n        self.q_selected_action = tf.reduce_sum(self.output_layer * self.selected_action_ph, axis=1)\r\n\r\n        diff = tf.subtract(self.target_ph, self.q_selected_action)\r\n        return self._value_function_loss(diff)\r\n\r\n'"
networks/sequence_decoder.py,32,"b""# -*- encoding: utf-8 -*-\nimport tensorflow as tf\n\n\ndef gumbel_noise(shape, epsilon=1e-30):\n    U = tf.random_uniform(shape)\n    return -tf.log(-tf.log(U + epsilon) + epsilon)\n\n\ndef sample_gumbel_softmax(logits, allow_stop, temperature):\n    y = logits + gumbel_noise(tf.shape(logits))\n\n    mask = tf.concat(axis=1, values=[\n        tf.ones_like(logits[:, :-1]),\n        tf.zeros_like(logits[:, -1:])\n    ])\n    identity = tf.ones_like(logits)\n    mask = tf.cond(allow_stop, lambda: identity, lambda: mask)\n\n    exp_logits = tf.exp(y) * mask\n    Z = tf.expand_dims(tf.reduce_sum(exp_logits, 1), 1)\n    return exp_logits / Z\n\n\n\ndef gumbel_softmax(logits, use_fixed_action, fixed_action, allow_stop, temperature=0.5, hard=True):\n    y = sample_gumbel_softmax(logits, allow_stop, temperature)\n    y.set_shape(logits.get_shape().as_list())\n\n    if hard:\n        y_hard = tf.cast(tf.equal(y,tf.reduce_max(y, 1, keep_dims=True)), y.dtype)\n        y_hard = tf.cond(use_fixed_action, lambda: fixed_action, lambda: y_hard)\n        y = tf.stop_gradient(y_hard - y) + y\n    \n    return y\n\n\ndef loop_gumbel_softmax(temperature=0.5):\n    def loop_function(logits, use_fixed_action, fixed_action, allow_stop):\n        return gumbel_softmax(logits, use_fixed_action, fixed_action, allow_stop, temperature=temperature)\n\n    return loop_function\n\n\ndef decoder(decoder_inputs, initial_state, cell, sequence_lengths, W_actions, b_actions,\n            max_decoder_steps, scope, use_fixed_action, action_outputs,\n            loop_function=None, output_size=None, dtype=tf.float32):\n\n    assert decoder_inputs.get_shape().ndims == 3, 'Decoder inputs must have rank 3'\n\n    if output_size is None:\n        output_size = cell.output_size\n\n    with tf.variable_scope(scope):\n        batch_size = tf.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n        states = [initial_state]\n        outputs = []\n        prev = None\n\n        t0 = tf.constant(0, dtype=tf.int32)\n        max_seq_len = tf.reduce_max(sequence_lengths)\n        loop_condition = lambda t, state, logits, action, s_array, l_array, a_array: tf.less(t, max_seq_len, name='loop_condition')\n\n        state_array = tf.TensorArray(dtype=tf.float32, size=max_seq_len, infer_shape=True, dynamic_size=True, name='state_array')\n        logits_array = tf.TensorArray(dtype=tf.float32, size=max_seq_len, infer_shape=True, dynamic_size=True, name='logits_array')\n        action_array = tf.TensorArray(dtype=tf.float32, size=max_seq_len, infer_shape=True, dynamic_size=True, name='action_array')\n\n        def body(t, hidden_state, logits, action, state_array, logits_array, action_array):\n            o, s = cell(action, hidden_state)\n            l = tf.nn.xw_plus_b(o, W_actions, b_actions)\n            a = loop_function(\n                l,\n                use_fixed_action,\n                action_outputs[:, t, :],\n                tf.greater(t, 0))\n\n            update = tf.less(t, sequence_lengths, name='update_cond')\n\n            s_out = tf.where(update, s, hidden_state)\n            l_out = tf.where(update, l, logits)\n            a_out = tf.where(update, a, action)\n\n            state_array = state_array.write(t, s_out)\n            logits_array = logits_array.write(t, l_out)\n            action_array = action_array.write(t, a_out)\n\n            return t+1, s, l, a, state_array, logits_array, action_array\n\n\n        go_action = decoder_inputs[:, 0, :]\n        final_const, states, logits, actions, state_array, logits_array, action_array = tf.while_loop(\n            loop_condition,\n            body,\n            loop_vars=[t0,\n                       initial_state,\n                       decoder_inputs[:, 0, :],\n                       go_action,\n                       state_array,\n                       logits_array,\n                       action_array])\n\n        return (\n            tf.transpose(state_array.stack(), perm=[1, 0, 2]),\n            tf.transpose(logits_array.stack(), perm=[1, 0, 2]),\n            tf.transpose(action_array.stack(), perm=[1, 0, 2]),\n        )\n\n"""
utils/__init__.py,0,b''
utils/checkpoint_utils.py,1,"b'# -*- coding: utf-8 -*-\r\nimport tensorflow as tf \r\nimport os\r\n\r\n\r\ndef restore_vars(saver, sess, game, alg_type, max_local_steps, restore_checkpoint):\r\n    """"""\r\n    Restore saved net, global step, and epsilons OR \r\n    create checkpoint directory for later storage.\r\n    """"""\r\n    alg = alg_type + ""{}/"".format(""_"" + str(max_local_steps) + ""_steps"" if alg_type == \'q\' else """") \r\n    checkpoint_dir = \'checkpoints/\' + game + \'/\' + alg\r\n    \r\n    check_or_create_checkpoint_dir(checkpoint_dir)\r\n    path = tf.train.latest_checkpoint(checkpoint_dir)\r\n\r\n    if path is None or not restore_checkpoint:\r\n        return 0\r\n    else:\r\n        print \'Restoring checkpoint `{}`\'.format(path)\r\n        saver.restore(sess, path)\r\n        global_step = int(path[path.rfind(""-"") + 1:])\r\n        return global_step \r\n\r\ndef save_vars(saver, sess, game, alg_type, max_local_steps, global_step):\r\n    """""" Checkpoint shared net params, global score and step, and epsilons. """"""\r\n\r\n    alg = alg_type + ""{}/"".format(""_"" + str(max_local_steps) + ""_steps"" if alg_type == \'q\' else """") \r\n    checkpoint_dir = \'checkpoints/\' + game + \'/\' + alg\r\n    \r\n    check_or_create_checkpoint_dir(checkpoint_dir)\r\n    saver.save(sess, checkpoint_dir + ""model"", global_step=global_step)\r\n\r\n\r\ndef check_or_create_checkpoint_dir(checkpoint_dir):\r\n    """""" Create checkpoint directory if it does not exist """"""\r\n    if not os.path.exists(checkpoint_dir):\r\n        try:\r\n            os.makedirs(checkpoint_dir)\r\n        except OSError:\r\n            pass\r\n\r\n'"
utils/cts.py,0,"b'# -*- coding: utf-8 -*-\n""""""An implementation of the Context Tree Switching model.\nThe Context Tree Switching algorithm (Veness et al., 2012) is a variable\norder Markov model with pleasant regret guarantees. It achieves excellent\nperformance over binary, and more generally small alphabet data.\nThis is a fairly vanilla implementation with readability favoured over\nperformance.\n""""""\n\nimport math\nimport random\nimport sys\n\n\n# Parameters of the CTS model. For clarity, we take these as constants.\nPRIOR_STAY_PROB = 0.5\nPRIOR_SPLIT_PROB = 0.5\nLOG_PRIOR_STAY_PROB = math.log(PRIOR_STAY_PROB)\nLOG_PRIOR_SPLIT_PROB = math.log(1.0 - PRIOR_STAY_PROB)\n# Sampling parameter. The maximum number of rejections before we give up and\n# sample from the root estimator.\nMAX_SAMPLE_REJECTIONS = 25\n\n# These define the prior count assigned to each unseen symbol.\nESTIMATOR_PRIOR = {\n    \'laplace\': (lambda unused_alphabet_size: 1.0),\n    \'jeffreys\': (lambda unused_alphabet_size: 0.5),\n    \'perks\': (lambda alphabet_size: 1.0 / alphabet_size),\n}\n\n\ndef log_add(log_x, log_y):\n    """"""Given log x and log y, returns log(x + y).""""""\n    # Swap variables so log_y is larger.\n    if log_x > log_y:\n        log_x, log_y = log_y, log_x\n\n    # Use the log(1 + e^p) trick to compute this efficiently\n    # If the difference is large enough, this is effectively log y.\n    delta = log_y - log_x\n    return math.log1p(math.exp(delta)) + log_x if delta <= 50.0 else log_y\n\n\nclass Error(Exception):\n    """"""Base exception for the `cts` module.""""""\n    pass\n\nclass Estimator(object):\n    """"""The estimator for a CTS node.\n    This implements a Dirichlet-multinomial model with specified prior. This\n    class does not perform alphabet checking, and will return invalid\n    probabilities if it is ever fed more than `model.alphabet_size` distinct\n    symbols.\n    Args:\n        model: Reference to CTS model. We expected model.symbol_prior to be\n            a `float`.\n    """"""\n    def __init__(self, model):\n        self.counts = {}\n        self.count_total = model.alphabet_size * model.symbol_prior\n        self._model = model\n\n    def prob(self, symbol):\n        """"""Returns the probability assigned to this symbol.""""""\n        count = self.counts.get(symbol, None)\n        # Allocate new symbols on the fly.\n        if count is None:\n            count = self.counts[symbol] = self._model.symbol_prior\n\n        return count / self.count_total\n\n    def update(self, symbol):\n        """"""Updates our count for the given symbol.""""""\n        log_prob = math.log(self.prob(symbol))\n        self.counts[symbol] = (\n            self.counts.get(symbol, self._model.symbol_prior) + 1.0)\n        self.count_total += 1.0\n        return log_prob\n\n    def sample(self, rejection_sampling):\n        """"""Samples this estimator\'s PDF in linear time.""""""\n        if rejection_sampling:\n            # Automatically fail if this estimator is empty.\n            if not self.counts:\n                return None\n            else:\n                # TODO(mgbellemare): No need for rejection sampling here --\n                # renormalize instead.\n                symbol = None\n                while symbol is None:\n                    symbol = self._sample_once(use_prior_alphabet=False)\n\n            return symbol\n        else:\n            if len(self._model.alphabet) < self._model.alphabet_size:\n                raise Error(\n                    \'Cannot sample from prior without specifying alphabet\')\n            else:\n                return self._sample_once(use_prior_alphabet=True)\n\n    def _sample_once(self, use_prior_alphabet):\n        """"""Samples once from the PDF.\n        Args:\n            use_prior_alphabet: If True, we will sample the alphabet given\n                by the model to account for symbols not seen by this estimator.\n                Otherwise, we return None.\n        """"""\n        random_index = random.uniform(0, self.count_total)\n\n        for item, count in self.counts.items():\n            if random_index < count:\n                return item\n            else:\n                random_index -= count\n\n        # We reach this point when we sampled a symbol which is not stored in\n        # `self.counts`.\n        if use_prior_alphabet:\n            for symbol in self._model.alphabet:\n                # Ignore symbols already accounted for.\n                if symbol in self.counts:\n                    continue\n                elif random_index < self._model.symbol_prior:\n                    return symbol\n                else:\n                    random_index -= self._model.symbol_prior\n\n            # Account for numerical errors.\n            if random_index < self._model.symbol_prior:\n                sys.stderr.write(\'Warning: sampling issues, random_index={}\'.\n                                 format(random_index))\n                # Return first item by default.\n                return list(self._model.alphabet)[0]\n            else:\n                raise Error(\'Sampling failure, not enough symbols\')\n        else:\n            return None\n\nclass CTSNode(object):\n    """"""A node in the CTS tree.\n    Each node contains a base Dirichlet estimator holding the statistics for\n    this particular context, and pointers to its children.\n    """"""\n\n    def __init__(self, model):\n        self._children = {}\n\n        self._log_stay_prob = LOG_PRIOR_STAY_PROB\n        self._log_split_prob = LOG_PRIOR_SPLIT_PROB\n\n        # Back pointer to the CTS model object.\n        self._model = model\n        self.estimator = Estimator(model)\n\n    def update(self, context, symbol):\n        """"""Updates this node and its children.\n        Recursively updates estimators for all suffixes of context. Each\n        estimator is updated with the given symbol. Also updates the mixing\n        weights.\n        """"""\n        lp_estimator = self.estimator.update(symbol)\n\n        # If not a leaf node, recurse, creating nodes as needed.\n        if len(context) > 0:\n            # We recurse on the last element of the context vector.\n            child = self.get_child(context[-1])\n            lp_child = child.update(context[:-1], symbol)\n\n            # This node predicts according to a mixture between its estimator\n            # and its child.\n            lp_node = self.mix_prediction(lp_estimator, lp_child)\n\n            self.update_switching_weights(lp_estimator, lp_child)\n\n            return lp_node\n        else:\n            # The log probability of staying at a leaf is log(1) = 0. This\n            # isn\'t actually used in the code, tho.\n            self._log_stay_prob = 0.0\n            return lp_estimator\n\n    def log_prob(self, context, symbol):\n        """"""Computes the log probability of the symbol in this subtree.""""""\n        lp_estimator = math.log(self.estimator.prob(symbol))\n\n        if len(context) > 0:\n            # See update() above. More efficient is to avoid creating the\n            # nodes and use a default node, but we omit this for clarity.\n            child = self.get_child(context[-1])\n\n            lp_child = child.log_prob(context[:-1], symbol)\n\n            return self.mix_prediction(lp_estimator, lp_child)\n        else:\n            return lp_estimator\n\n    def sample(self, context, rejection_sampling):\n        """"""Samples a symbol in the given context.""""""\n        if len(context) > 0:\n            # Determine whether we should use this estimator or our child\'s.\n            log_prob_stay = (self._log_stay_prob\n                             - log_add(self._log_stay_prob,\n                                                self._log_split_prob))\n\n            if random.random() < math.exp(log_prob_stay):\n                return self.estimator.sample(rejection_sampling)\n            else:\n                # If using child, recurse.\n                if rejection_sampling:\n                    child = self.get_child(context[-1], allocate=False)\n                    # We\'ll request another sample from the tree.\n                    if child is None:\n                        return None\n                # TODO(mgbellemare): To avoid rampant memory allocation,\n                # it\'s worthwhile to use a default estimator here rather than\n                # recurse when the child is not allocated.\n                else:\n                    child = self.get_child(context[-1])\n\n                symbol = child.sample(context[:-1], rejection_sampling)\n                return symbol\n        else:\n            return self.estimator.sample(rejection_sampling)\n\n    def get_child(self, symbol, allocate=True):\n        """"""Returns the node corresponding to this symbol.\n        New nodes are created as required, unless allocate is set to False.\n        In this case, None is returned.\n        """"""\n        node = self._children.get(symbol, None)\n\n        # If needed and requested, allocated a new node.\n        if node is None and allocate:\n            node = CTSNode(self._model)\n            self._children[symbol] = node\n\n        return node\n\n    def mix_prediction(self, lp_estimator, lp_child):\n        """"""Returns the mixture x = w * p + (1 - w) * q.\n        Here, w is the posterior probability of using the estimator at this\n        node, versus using recursively calling our child node.\n        The mixture is computed in log space, which makes things slightly\n        trickier.\n        Let log_stay_prob_ = p\' = log p, log_split_prob_ = q\' = log q.\n        The mixing coefficient w is\n                w = e^p\' / (e^p\' + e^q\'),\n                v = e^q\' / (e^p\' + e^q\').\n        Then\n                x = (e^{p\' w\'} + e^{q\' v\'}) / (e^w\' + e^v\').\n        """"""\n        numerator = log_add(lp_estimator + self._log_stay_prob,\n                                     lp_child + self._log_split_prob)\n        denominator = log_add(self._log_stay_prob,\n                                       self._log_split_prob)\n        return numerator - denominator\n\n    def update_switching_weights(self, lp_estimator, lp_child):\n        """"""Updates the switching weights according to Veness et al. (2012).""""""\n        log_alpha = self._model.log_alpha\n        log_1_minus_alpha = self._model.log_1_minus_alpha\n\n        # Avoid numerical issues with alpha = 1. This reverts to straight up\n        # weighting.\n        if log_1_minus_alpha == 0:\n            self._log_stay_prob += lp_estimator\n            self._log_split_prob += lp_child\n        # Switching rule. It\'s possible to make this more concise, but we\n        # leave it in full for clarity.\n        else:\n            # Mix in an alpha fraction of the other posterior:\n            #   switchingStayPosterior = ((1 - alpha) * stayPosterior\n            #                            + alpha * splitPosterior)\n            # where here we store the unnormalized posterior.\n            self._log_stay_prob = log_add(log_1_minus_alpha\n                                                   + lp_estimator\n                                                   + self._log_stay_prob,\n                                                   log_alpha\n                                                   + lp_child\n                                                   + self._log_split_prob)\n\n            self._log_split_prob = log_add(log_1_minus_alpha\n                                                    + lp_child\n                                                    + self._log_split_prob,\n                                                    log_alpha\n                                                    + lp_estimator\n                                                    + self._log_stay_prob)\n\n\nclass CTS(object):\n    """"""A class implementing Context Tree Switching.\n    This version works with large alphabets. By default it uses a Dirichlet\n    estimator with a Perks prior (works reasonably well for medium-sized,\n    sparse alphabets) at each node.\n    Methods in this class assume a human-readable context ordering, where the\n    last symbol in the context list is the most recent (in the case of\n    sequential prediction). This is slightly unintuitive from a computer\'s\n    perspective but makes the update more legible.\n    There are also only weak constraints on the alphabet. Basically: don\'t use\n    more than alphabet_size symbols unless you know what you\'re doing. These do\n    symbols can be any integers and need not be contiguous.\n    Alternatively, you may set the full alphabet before using the model.\n    This will allow sampling from the model prior (which is otherwise not\n    possible).\n    """"""\n\n    def __init__(self, context_length, alphabet=None, max_alphabet_size=256,\n                 symbol_prior=\'perks\'):\n        """"""CTS constructor.\n        Args:\n            context_length: The number of variables which CTS conditions on.\n                In general, increasing this term increases prediction accuracy\n                and memory usage.\n            alphabet: The alphabet over which we operate, as a `set`. Set to\n                None to allow CTS to dynamically determine the alphabet.\n            max_alphabet_size: The total number of symbols in the alphabet. For\n                character-level prediction, leave it at 256 (or set alphabet).\n                If alphabet is specified, this field is ignored.\n            symbol_prior: (float or string) The prior used within each node\'s\n                Dirichlet estimator. If a string is given, valid choices are\n                \'dirichlet\', \'jeffreys\', and \'perks\'. This defaults to \'perks\'.\n        """"""\n        # Total number of symbols processed.\n        self._time = 0.0\n        self.context_length = context_length\n        # We store the observed alphabet in a set.\n        if alphabet is None:\n            self.alphabet, self.alphabet_size = set(), max_alphabet_size\n        else:\n            self.alphabet, self.alphabet_size = alphabet, len(alphabet)\n\n        # These are properly set when we call update().\n        self.log_alpha, self.log_1_minus_alpha = 0.0, 0.0\n\n        # If we have an entry for it in our set of default priors, assume it\'s\n        # one of our named priors.\n        if symbol_prior in ESTIMATOR_PRIOR:\n            self.symbol_prior = (\n                float(ESTIMATOR_PRIOR[symbol_prior](self.alphabet_size)))\n        else:\n            self.symbol_prior = float(symbol_prior)  # Otherwise assume numeric.\n\n        # Create root. This must happen after setting alphabet & symbol prior.\n        self._root = CTSNode(self)\n\n    def _check_context(self, context):\n        """"""Verifies that the given context is of the expected length.\n        Args:\n            context: Context to be verified.\n        """"""\n        if self.context_length != len(context):\n            raise Error(\'Invalid context length, {} != {}\'\n                        .format(self.context_length, len(context)))\n\n    def update(self, context, symbol):\n        """"""Updates the CTS model.\n        Args:\n            context: The context list, of size context_length, describing\n                the variables on which CTS should condition. Context elements\n                are assumed to be ranked in increasing order of importance.\n                For example, in sequential prediction the most recent symbol\n                should be context[-1].\n            symbol: The symbol observed in this context.\n        Returns:\n            The log-probability assigned to the symbol before the update.\n        Raises:\n            Error: Provided context is of incorrect length.\n        """"""\n        # Set the switching parameters.\n        self._time += 1.0\n        self.log_alpha = math.log(1.0 / (self._time + 1.0))\n        self.log_1_minus_alpha = math.log(self._time / (self._time + 1.0))\n\n        # Nothing in the code actually prevents invalid contexts, but the\n        # math won\'t work out.\n        self._check_context(context)\n\n        # Add symbol to seen alphabet.\n        self.alphabet.add(symbol)\n        if len(self.alphabet) > self.alphabet_size:\n            raise Error(\'Too many distinct symbols\')\n\n        log_prob = self._root.update(context, symbol)\n\n        return log_prob\n\n    def log_prob(self, context, symbol):\n        """"""Queries the CTS model.\n        Args:\n            context: As per ``update()``.\n            symbol: As per ``update()``.\n        Returns:\n            The log-probability of the symbol in the context.\n        Raises:\n            Error: Provided context is of incorrect length.\n        """"""\n        self._check_context(context)\n        return self._root.log_prob(context, symbol)\n\n    def sample(self, context, rejection_sampling=True):\n        """"""Samples a symbol from the model.\n        Args:\n            context: As per ``update()``.\n            rejection_sampling: Whether to ignore samples from the prior.\n        Returns:\n            A symbol sampled according to the model. The default mode of\n            operation is rejection sampling, which will ignore draws from the\n            prior. This allows us to avoid providing an alphabet in full, and\n            typically produces more pleasing samples (because they are never\n            drawn from data for which we have no prior). If the full alphabet\n            is provided (by setting self.alphabet) then `rejection_sampling`\n            may be set to False. In this case, models may sample symbols in\n            contexts they have never appeared in. This latter mode of operation\n            is the mathematically correct one.\n        """"""\n        if self._time == 0 and rejection_sampling:\n            raise Error(\'Cannot do rejection sampling on prior\')\n\n        self._check_context(context)\n        symbol = self._root.sample(context, rejection_sampling)\n        num_rejections = 0\n        while rejection_sampling and symbol is None:\n            num_rejections += 1\n            if num_rejections >= MAX_SAMPLE_REJECTIONS:\n                symbol = self._root.estimator.sample(rejection_sampling=True)\n                # There should be *some* symbol in the root estimator.\n                assert symbol is not None\n            else:\n                symbol = self._root.sample(context, rejection_sampling=True)\n\n        return symbol\n\nclass ContextualSequenceModel(object):\n    """"""A sequence model.\n    This class maintains a context vector, i.e. a list of the most recent\n    observations. It predicts by querying a contextual model (e.g. CTS) with\n    this context vector.\n    """"""\n\n    def __init__(self, model=None, context_length=None, start_symbol=0):\n        """"""Constructor.\n        Args:\n            model: The model to be used for prediction. If this is none but\n                context_length is not, defaults to CTS(context_length).\n            context_length: If model == None, the length of context for the\n                underlying CTS model.\n            start_symbol: The symbol with which to pad the first context\n                vectors.\n        """"""\n        if model is None:\n            if context_length is None:\n                raise ValueError(\'Must specify model or model parameters\')\n            else:\n                self.model = CTS(context_length)\n        else:\n            self.model = model\n\n        self.context = [start_symbol] * self.model.context_length\n\n    def observe(self, symbol):\n        """"""Updates the current context without updating the model.\n        The new context is generated by discarding the oldest symbol and\n        inserting the new symbol in the rightmost position of the context\n        vector.\n        Args:\n            symbol: Observed symbol.\n        """"""\n        self.context.append(symbol)\n        self.context = self.context[1:]\n\n    def update(self, symbol):\n        """"""Updates the model with the new symbol.\n        The current context is subsequently updated, as per ``observe()``.\n        Args:\n            symbol: Observed symbol.\n        Returns:\n            The log probability of the observed symbol.\n        """"""\n        log_prob = self.model.update(self.context, symbol)\n        self.observe(symbol)\n        return log_prob\n\n    def log_prob(self, symbol):\n        """"""Computes the log probability of the given symbol.\n        Neither model nor context is subsequently updated.\n        Args:\n            symbol: Observed symbol.\n        Returns:\n            The log probability of the observed symbol.\n        """"""\n        return self.model.log_prob(self.context, symbol)\n\n    def sample(self, rejection_sampling=True):\n        """"""Samples a symbol according to the current context.\n        Neither model nor context are updated.\n        This may be used in combination with ``observe()`` to generate sample\n        sequences without updating the model (though a die-hard Bayesian would\n        use ``update()`` instead!).\n        Args:\n            rejection_sampling: If set to True, symbols are not drawn from\n            the prior: only observed symbols are output. Setting to False\n            requires specifying the model\'s alphabet (see ``CTS.__init__``\n            above).\n        Returns:\n            The sampled symbol.\n        """"""\n        return self.model.sample(self.context, rejection_sampling)\n\n__all__ = [""CTS"", ""ContextualSequenceModel""]\n'"
utils/cts_density_model.py,0,"b'# -*- encoding: utf-8 -*-\nimport numpy as np\nfrom cts import CTS\nfrom skimage.transform import resize\n\n\nclass CTSDensityModel(object):\n\t""""""\n\tImplementation of CTS Density Model described in the paper\n\t""Unifying Count-Based Exploration and Intrinsic Motivation"" (https//arxiv.org/abs/1606.01868)\n\n\tNote that the cython version in fast_cts.pyx is significantly faster\n\t""""""\n\tdef __init__(self, height=21, width=21, num_bins=8, beta=0.05):\n\t\tself.beta = beta\n\t\tself.num_bins = num_bins\n\t\tself.factors = np.array([[CTS(4) for _ in range(width)] for _ in range(height)])\n\n\n\tdef update(self, obs):\n\t\tobs = resize(obs, self.factors.shape, preserve_range=True)\n\t\tobs = np.floor((obs*self.num_bins)).astype(np.int32)\n\n\t\tcontext = [0, 0, 0, 0]\n\t\tlog_prob = 0.0\n\t\tlog_recoding_prob = 0.0\n\t\tfor i in range(self.factors.shape[0]):\n\t\t\tfor j in range(self.factors.shape[1]):\n\t\t\t\tcontext[3] = obs[i, j-1] if j > 0 else 0\n\t\t\t\tcontext[2] = obs[i-1, j] if i > 0 else 0\n\t\t\t\tcontext[1] = obs[i-1, j-1] if i > 0 and j > 0 else 0\n\t\t\t\tcontext[0] = obs[i-1, j+1] if i > 0 and j < self.factors.shape[1]-1 else 0\n\n\t\t\t\tlog_prob += self.factors[i, j].update(context, obs[i, j])\n\t\t\t\tlog_recoding_prob += self.factors[i, j].log_prob(context, obs[i, j])\n\n\t\treturn self.exploration_bonus(log_prob, log_recoding_prob)\n\n\n\tdef exploration_bonus(self, log_prob, log_recoding_prob):\n\t\tprob = np.exp(log_prob)\n\t\trecoding_prob = np.exp(log_recoding_prob)\n\n\t\tpseudocount = prob * (1 - recoding_prob) / np.maximum(recoding_prob - prob, 1e-10)\n\t\treturn self.beta / np.sqrt(pseudocount + .01)\n\n'"
utils/decorators.py,0,"b""# -*- coding: utf-8 -*-\nimport logger\n\n\nlogger = logger.getLogger('intrinsic_motivation_actor_learner')\n\ndef only_on_train(return_val=None):\n\tdef _only_on_train(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif args[0].is_train:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn return_val\n\n\t\treturn wrapper\n\treturn _only_on_train\n\n\ndef Experimental(_cls):\n\told_init = getattr(_cls, '__init__')\n\tdef wrapped_init(self, *args, **kwargs):\n\t\tlogger.warning('Using experimental class \\'{0}\\' -- See docstring for more details:\\n{1}'.format(\n\t\t\t_cls.__name__, _cls.__doc__))\n\t\told_init(self, *args, **kwargs)\n\n\tsetattr(_cls, '__init__', wrapped_init)\n\treturn _cls\n\n\n\n\n\n"""
utils/distributions.py,23,"b""import tensorflow as tf\nimport numpy as np\nimport utils.ops\n\n\nclass DiagNormal(object):\n\t'''\n\tModels Gaussian with Diagonal Covariance\n\t'''\n\tdef __init__(self, params):\n\t\tself._params = params\n\t\tself.mu, self.sigma = tf.split(params, 2, 1)\n\t\tself.dim = tf.shape(self.mu)[1]\n\n\tdef params(self):\n\t\treturn self._params\n\n\tdef sample(self):\n\t\taction = self.mu + self.sigma * tf.random_normal([self.dim])\n\t\t# action = tf.Print(action, [self.mu[0,0], self.sigma[0,0], action[0,0]], 'mu/sigma/action: ')\n\t\treturn action\n\n\tdef log_likelihood(self, x):\n\t\treturn -tf.reduce_sum(\n\t\t\t0.5 * tf.square((x - self.mu) / (self.sigma + 1e-8))\n\t\t\t+ tf.log(self.sigma + 1e-8) + 0.5 * tf.log(2.0 * np.pi), axis=1)\n\n\tdef entropy(self):\n\t\treturn tf.reduce_sum(tf.log(self.sigma + 1e-8) + 0.5 * np.log(2 * np.pi * np.e), axis=1)\n\n\tdef kl_divergence(self, params):\n\t\tmu_2, sigma_2 = tf.split(params, 2, 1)\n\t\treturn tf.reduce_sum(\n\t\t\t(tf.square(sigma_2) + tf.square(mu_2 - self.mu)) / (2.0 * tf.square(self.sigma) + 1e-8) \n\t\t\t+ tf.log(self.sigma/(sigma_2 + 1e-8) + 1e-8) - 0.5, axis=1)\n\n\nclass Discrete(object):\n\tdef __init__(self, logits):\n\t\tself.logits = logits\n\t\tself.probs = tf.nn.softmax(self.logits)\n\t\tself.log_probs = tf.nn.log_softmax(self.logits)\n\t\tself.dim = tf.shape(self.logits)[1]\n\n\tdef params(self):\n\t\treturn self.logits\n\n\tdef sample(self):\n\t\tnoisy_logits = self.logits[0] - tf.log(-tf.log(tf.random_uniform([self.dim])))\n\t\taction_idx = tf.argmax(noisy_logits, axis=0)\n\t\treturn tf.one_hot(action_idx, self.dim)\n\n\tdef log_likelihood(self, x):\n\t\taction_idx = tf.argmax(x, axis=1)\n\t\tbatch_idx = tf.range(0, tf.shape(x)[0])\n\t\treturn utils.ops.slice_2d(self.log_probs, batch_idx, action_idx)\n\n\tdef entropy(self):\n\t\treturn -tf.reduce_sum(self.probs * self.log_probs, axis=1)\n\n\tdef kl_divergence(self, logits):\n\t\teps = 1e-8\n\t\tprobs = tf.nn.softmax(logits)\n\t\treturn tf.reduce_sum(probs * tf.log((probs + eps) / (self.probs + eps)), axis=1)\n\n"""
utils/dnd.py,0,"b""import numpy as np\nfrom annoy import AnnoyIndex\nfrom collections import deque, OrderedDict\n\n\nclass LRUCache(object):\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.keys = OrderedDict()\n\n    def update(self, idx):\n        if idx in self.keys:\n            del self.keys[idx]\n            self.keys[idx] = True\n            return None\n        else:\n            self.keys[idx] = True\n            if len(self.keys) > self.capacity-1:\n                return self.keys.popitem(last=False)[0]\n            else:\n                return len(self.keys)\n\n\nclass DND(object):\n    def __init__(self, capacity=100000, key_size=128, cache_size=32, alpha=0.1):\n        self.alpha = alpha\n        self.capacity = capacity\n        self.lru_cache = LRUCache(capacity)\n        self.dup_cache = deque(maxlen=cache_size)\n        self.index = AnnoyIndex(key_size, metric='euclidean')\n        self.keys = np.zeros((capacity, key_size), dtype=np.float32)\n        self.values = np.zeros((capacity,), dtype=np.float32)\n        self.insert_idx = 0\n        self.insertions = 0\n\n    def add(self, key, value):\n        if not self.cache_lookup(key, value):\n            self.keys[self.insert_idx] = key\n            self.values[self.insert_idx] = value\n            self.dup_cache.append(key)\n            self.index.add_item(self.insert_idx, key)\n            #advance insert position to least-recently-used key\n            new_idx = self.lru_cache.update(self.insert_idx)\n            if new_idx:\n                self.insert_idx = new_idx\n\n        self.insertions += 1\n        #rebuilding the index is expensive so we don't want to do it too often\n        if self.insertions % 1000 == 0:\n            self.rebuild_index()\n\n\n    def cache_lookup(self, key, value):\n        for i, e in enumerate(self.dup_cache):\n            if np.allclose(key, e):\n                idx = self.size - len(self.dup_cache) + i\n                self.values[idx] += self.alpha * (value - self.values[idx])\n\n                return True\n\n    def rebuild_index(self):\n        self.index.unbuild()\n        self.index.build(50)\n\n\n    def query(self, key, k_neighbors=40):\n        indices, distances = self.index.get_nns_by_vector(\n            key, k_neighbors, include_distances=True)\n\n        for idx in indices:\n            self.lru_cache.update(idx)\n\n        return self.values[indices], distances\n\n\n"""
utils/forked_debugger.py,0,"b""# -*- coding: utf-8 -*-\nimport sys\nimport pdb\n\n\nclass ForkedPdb(pdb.Pdb):\n    def interaction(self, *args, **kwargs):\n        _stdin = sys.stdin\n        try:\n            sys.stdin = file('/dev/stdin')\n            pdb.Pdb.interaction(self, *args, **kwargs)\n        finally:\n            sys.stdin = _stdin"""
utils/logger.py,0,"b'# -*- coding: utf-8 -*-\r\nimport os\r\nimport logging\r\nimport logging.config\r\n\r\n\r\nLOG_LEVEL = os.getenv(\'LOG_LEVEL\', \'INFO\')\r\nLOGGING = {\r\n    \'version\': 1,\r\n    \'disable_existing_loggers\': True,\r\n    \'formatters\': {\r\n        \'verbose\': {\r\n            \'format\': ""[%(asctime)s] %(levelname)s "" \\\r\n                ""[%(threadName)s:%(lineno)s] %(message)s"",\r\n            \'datefmt\': ""%Y-%m-%d %H:%M:%S""\r\n        },\r\n        \'simple\': {\r\n            \'format\': \'%(levelname)s %(message)s\'\r\n        },\r\n    },\r\n    \'handlers\': {\r\n        \'console\': {\r\n            \'level\': LOG_LEVEL,\r\n            \'class\': \'logging.StreamHandler\',\r\n            \'formatter\': \'verbose\'\r\n        },\r\n        \'file\': {\r\n            \'level\': LOG_LEVEL,\r\n            \'class\': \'logging.handlers.RotatingFileHandler\',\r\n            \'formatter\': \'verbose\',\r\n            \'filename\': \'rl.log\',\r\n            \'maxBytes\': 10*10**6,\r\n            \'backupCount\': 3\r\n            }\r\n    },\r\n    \'loggers\': {\r\n        \'\': {\r\n            \'handlers\': [\'console\', \'file\'],\r\n            \'level\': LOG_LEVEL,\r\n        },\r\n    }\r\n}\r\n\r\n\r\nlogging.config.dictConfig(LOGGING)\r\n\r\ndef getLogger(name):\r\n\r\n    return logging.getLogger(name)\r\n'"
utils/ops.py,7,"b'import tensorflow as tf\nimport numpy as np\n\n\ndef slice_2d(x, inds0, inds1):\n\t#adapted from https://github.com/jjkke88/trpo/blob/master/utils.py\n\tinds0 = tf.cast(inds0, tf.int32)\n\tinds1 = tf.cast(inds1, tf.int32)\n\tshape = tf.cast(tf.shape(x), tf.int32)\n\tncols = shape[1]\n\tx_flat = tf.reshape(x, [-1])\n\treturn tf.gather(x_flat, inds0 * ncols + inds1)\n\n\ndef flatten_vars(var_list):\n\treturn tf.concat([\n\t\ttf.reshape(v, [np.prod(v.get_shape().as_list())])\n\t\tfor v in var_list\n\t], 0)'"
utils/replay_memory.py,0,"b""# -*- coding: utf-8 -*-\nimport os\nimport tempfile\nimport numpy as np\n\n\nclass ReplayMemory(object):\n\n\tdef __init__(self, maxlen, input_shape, action_size):\n\t\tself.maxlen = maxlen\n\t\tdirname = tempfile.mkdtemp()\n\t\t#use memory maps so we won't have to worry about eating up lots of RAM\n\t\tget_path = lambda name: os.path.join(dirname, name)\n\t\tself.screens = np.memmap(get_path('screens'), dtype=np.float32, mode='w+', shape=tuple([self.maxlen]+input_shape))\n\t\tself.actions = np.memmap(get_path('actions'), dtype=np.float32, mode='w+', shape=(self.maxlen, action_size))\n\t\tself.rewards = np.memmap(get_path('rewards'), dtype=np.float32, mode='w+', shape=(self.maxlen,))\n\t\tself.is_terminal = np.memmap(get_path('terminals'), dtype=np.bool, mode='w+', shape=(self.maxlen,))\n\n\t\tself.position = 0\n\t\tself.full = False\n\n\t# def _get_states(batch):\n\t# \ts = list()\n\t# \tfor i in xrange(-3, 2):\n\t# \t\ts.append(self.screens[batch+i])\n\t\t\t\n\t# \treturn np.vstack(s[:-1]), np.vstack(s[1:])\n\n\tdef sample_batch(self, batch_size):\n\t\tbatch = np.zeros((batch_size,), dtype=np.int32)\n\t\tidx = 0\n\t\twhile idx < batch_size:\n\t\t\tmaybe_batch = np.random.choice(len(self)-1, np.minimum(len(self), batch_size-idx))\t\n\t\t\tvalid_idx = np.where(1 - self.is_terminal[maybe_batch])[0]\n\t\t\tbatch[idx:idx+len(valid_idx)] = maybe_batch[valid_idx]\n\t\t\tidx += len(valid_idx)\n\n\t\t# s_i, s_f = self._get_state(batch)\n\t\ts_i = self.screens[batch]\n\t\ts_f = self.screens[batch+1]\n\t\ta = self.actions[batch]\n\t\tr = self.rewards[batch]\n\t\tis_terminal = self.is_terminal[batch+1]\n\n\t\treturn s_i, a, r, s_f, is_terminal\n\n\tdef __len__(self):\n\t\treturn self.maxlen if self.full else self.position\n\n\tdef append(self, s_i, a, r, is_terminal):\n\t\tself.screens[self.position] = s_i\n\t\tself.actions[self.position] = a\n\t\tself.rewards[self.position] = r\n\t\tself.is_terminal[self.position] = is_terminal\n\n\t\tif self.position + 1 == self.maxlen:\t\n\t\t\tself.full = True\n\t\tself.position = (self.position + 1) % self.maxlen\n\n\n\n\t\t\n"""
utils/shared_memory.py,0,"b""# -*- coding: utf-8 -*-\r\nfrom multiprocessing import RawValue, RawArray, Semaphore, Lock\r\nimport ctypes\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass SharedCounter(object):\r\n    def __init__(self, initval=0):\r\n        self.val = RawValue('i', initval)\r\n        self.last_step_update_target = RawValue('i', initval)\r\n        self.lock = Lock()\r\n\r\n    def increment(self, elapsed_steps=None):\r\n        self.val.value += 1\r\n        if ((elapsed_steps is not None) \r\n            and ((self.val.value - self.last_step_update_target.value) \r\n                >= elapsed_steps)):\r\n            self.last_step_update_target.value = self.val.value\r\n            return self.val.value, True\r\n        else:\r\n            return self.val.value, False\r\n\r\n    def set_value(self, value):\r\n        self.lock.acquire()\r\n        self.val.value = value\r\n        self.lock.release()\r\n\r\n    def value(self):\r\n        return self.val.value\r\n\r\nclass Barrier:\r\n    def __init__(self, n):\r\n        self.n = n\r\n        self.counter = SharedCounter(0)\r\n        self.barrier = Semaphore(0)\r\n\r\n    def wait(self):\r\n        with self.counter.lock:\r\n            self.counter.val.value += 1\r\n            if self.counter.val.value == self.n:\r\n                self.barrier.release()\r\n\r\n        self.barrier.acquire()\r\n        self.barrier.release()\r\n\r\nclass SharedVars(object):\r\n    def __init__(self, params, opt_type=None, lr=0, step=0):\r\n        self.var_shapes = [\r\n            var.get_shape().as_list()\r\n            for var in params]\r\n        self.size = sum([np.prod(shape) for shape in self.var_shapes])\r\n        self.step = RawValue(ctypes.c_int, step)\r\n\r\n        if opt_type == 'adam':\r\n            self.ms = self.malloc_contiguous(self.size)\r\n            self.vs = self.malloc_contiguous(self.size)\r\n            self.lr = RawValue(ctypes.c_float, lr)\r\n        elif opt_type == 'adamax':\r\n            self.ms = self.malloc_contiguous(self.size)\r\n            self.vs = self.malloc_contiguous(self.size)\r\n            self.lr = RawValue(ctypes.c_float, lr)\r\n        elif opt_type == 'rmsprop':\r\n            self.vars = self.malloc_contiguous(self.size, np.ones(self.size, dtype=np.float))\r\n        elif opt_type == 'momentum':\r\n            self.vars = self.malloc_contiguous(self.size)\r\n        else:\r\n            self.vars = self.malloc_contiguous(self.size)\r\n\r\n            \r\n    def malloc_contiguous(self, size, initial_val=None):\r\n        if initial_val is None:\r\n            return RawArray(ctypes.c_float, size)\r\n        else:\r\n            return RawArray(ctypes.c_float, initial_val)\r\n\r\n\r\nclass SharedFlags(object):\r\n    def __init__(self, num_actors):\r\n        self.updated = RawArray(ctypes.c_int, num_actors)\r\n            \r\n"""
utils/stats.py,1,"b""# -*- encoding: utf-8 -*-\nimport tensorflow as tf\nimport numpy as np\n\n\ndef mean_kl_divergence_op(P, Q, eps=1e-10):\n\treturn tf.reduce_mean(tf.reduce_sum(P * tf.log((P + eps) / (Q + eps)), axis=1))\n\n\ndef kl_divergence(P, Q, eps = 1e-10):\n\treturn (P * np.log((P + eps) / (Q + eps))).sum()\n\n\ndef jenson_shannon_divergence(P, Q, eps=1e-10):\n\tM = 0.5 * (P + Q)\n\treturn 0.5 * (kl_divergence(P, M, eps=eps) + kl_divergence(Q, M, eps=eps))\n\n\ndef ar1_process(x_previous, mean, theta, sigma):\n\t'''Discrete Ornstein\xe2\x80\x93Uhlenbeck / AR(1) process to produce temporally correlated noise'''\n\treturn theta*(mean - x_previous) + sigma*np.random.normal()"""
