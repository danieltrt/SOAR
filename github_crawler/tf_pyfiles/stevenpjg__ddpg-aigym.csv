file_path,api_count,code
actor_net.py,16,"b'import numpy as np\nimport tensorflow as tf\nimport math\n\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 64\nTAU = 0.001\nclass ActorNet:\n    """""" Actor Network Model of DDPG Algorithm """"""\n    \n    def __init__(self,num_states,num_actions):\n        self.g=tf.Graph()\n        with self.g.as_default():\n            self.sess = tf.InteractiveSession()\n            \n           \n            #actor network model parameters:\n            self.W1_a, self.B1_a, self.W2_a, self.B2_a, self.W3_a, self.B3_a,\\\n            self.actor_state_in, self.actor_model = self.create_actor_net(num_states, num_actions)\n            \n                                   \n            #target actor network model parameters:\n            self.t_W1_a, self.t_B1_a, self.t_W2_a, self.t_B2_a, self.t_W3_a, self.t_B3_a,\\\n            self.t_actor_state_in, self.t_actor_model = self.create_actor_net(num_states, num_actions)\n            \n            #cost of actor network:\n            self.q_gradient_input = tf.placeholder(""float"",[None,num_actions]) #gets input from action_gradient computed in critic network file\n            self.actor_parameters = [self.W1_a, self.B1_a, self.W2_a, self.B2_a, self.W3_a, self.B3_a]\n            self.parameters_gradients = tf.gradients(self.actor_model,self.actor_parameters,-self.q_gradient_input)#/BATCH_SIZE) \n            self.optimizer = tf.train.AdamOptimizer(LEARNING_RATE).apply_gradients(zip(self.parameters_gradients,self.actor_parameters))  \n            #initialize all tensor variable parameters:\n            self.sess.run(tf.initialize_all_variables())    \n            \n            #To make sure actor and target have same intial parmameters copy the parameters:\n            # copy target parameters\n            self.sess.run([\n\t\t\t\tself.t_W1_a.assign(self.W1_a),\n\t\t\t\tself.t_B1_a.assign(self.B1_a),\n\t\t\t\tself.t_W2_a.assign(self.W2_a),\n\t\t\t\tself.t_B2_a.assign(self.B2_a),\n\t\t\t\tself.t_W3_a.assign(self.W3_a),\n\t\t\t\tself.t_B3_a.assign(self.B3_a)])\n\n            self.update_target_actor_op = [\n                self.t_W1_a.assign(TAU*self.W1_a+(1-TAU)*self.t_W1_a),\n                self.t_B1_a.assign(TAU*self.B1_a+(1-TAU)*self.t_B1_a),\n                self.t_W2_a.assign(TAU*self.W2_a+(1-TAU)*self.t_W2_a),\n                self.t_B2_a.assign(TAU*self.B2_a+(1-TAU)*self.t_B2_a),\n                self.t_W3_a.assign(TAU*self.W3_a+(1-TAU)*self.t_W3_a),\n                self.t_B3_a.assign(TAU*self.B3_a+(1-TAU)*self.t_B3_a)]\n        \n\n\n    def create_actor_net(self, num_states=4, num_actions=1):\n        """""" Network that takes states and return action """"""\n        N_HIDDEN_1 = 400\n        N_HIDDEN_2 = 300\n        actor_state_in = tf.placeholder(""float"",[None,num_states])    \n        W1_a=tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n        B1_a=tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n        W2_a=tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n        B2_a=tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n        W3_a=tf.Variable(tf.random_uniform([N_HIDDEN_2,num_actions],-0.003,0.003))\n        B3_a=tf.Variable(tf.random_uniform([num_actions],-0.003,0.003))\n    \n        H1_a=tf.nn.softplus(tf.matmul(actor_state_in,W1_a)+B1_a)\n        H2_a=tf.nn.tanh(tf.matmul(H1_a,W2_a)+B2_a)\n        actor_model=tf.matmul(H2_a,W3_a) + B3_a\n        return W1_a, B1_a, W2_a, B2_a, W3_a, B3_a, actor_state_in, actor_model\n        \n        \n    def evaluate_actor(self,state_t):\n        return self.sess.run(self.actor_model, feed_dict={self.actor_state_in:state_t})        \n        \n        \n    def evaluate_target_actor(self,state_t_1):\n        return self.sess.run(self.t_actor_model, feed_dict={self.t_actor_state_in: state_t_1})\n        \n    def train_actor(self,actor_state_in,q_gradient_input):\n        self.sess.run(self.optimizer, feed_dict={ self.actor_state_in: actor_state_in, self.q_gradient_input: q_gradient_input})\n    \n    def update_target_actor(self):\n        self.sess.run(self.update_target_actor_op)    \n\n        '"
actor_net_bn.py,33,"b'import tensorflow as tf\nimport math\nfrom batch_normalization.batch_norm import batch_norm\nimport numpy as np\nLEARNING_RATE = 0.0001\nTAU = 0.001\nBATCH_SIZE = 64\nN_HIDDEN_1 = 400\nN_HIDDEN_2 = 300\n\nclass ActorNet_bn:\n    """""" Actor Network Model with Batch Normalization of DDPG Algorithm """"""\n    \n    def __init__(self,num_states,num_actions):\n        tf.reset_default_graph()\n        self.g=tf.Graph()\n        with self.g.as_default():\n            self.sess = tf.InteractiveSession()\n            \n            #actor network model parameters:\n            self.actor_state_in = tf.placeholder(""float"",[None,num_states]) \n            self.W1_a = tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.B1_a=tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.W2_a = tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n            self.B2_a=tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n            self.W3_a = tf.Variable(tf.random_uniform([N_HIDDEN_2,num_actions],-0.003,0.003))\n            self.B3_a = tf.Variable(tf.random_uniform([num_actions],-0.003,0.003))\n            \n            self.is_training = tf.placeholder(tf.bool, [])\n            self.H1_t= tf.matmul(self.actor_state_in,self.W1_a)\n            self.H1_a_bn = batch_norm(self.H1_t,N_HIDDEN_1, self.is_training, self.sess)\n            self.H1_a = tf.nn.softplus(self.H1_a_bn.bnorm) + self.B1_a\n            \n            self.H2_t=tf.matmul(self.H1_a,self.W2_a)\n            self.H2_a_bn = batch_norm(self.H2_t,N_HIDDEN_2,self.is_training,self.sess)\n            self.H2_a = tf.nn.tanh(self.H2_a_bn.bnorm) + self.B2_a\n            self.actor_model=tf.matmul(self.H2_a,self.W3_a) + self.B3_a\n            \n                                   \n            #target actor network model parameters:\n            self.t_actor_state_in = tf.placeholder(""float"",[None,num_states]) \n            self.t_W1_a = tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.t_B1_a=tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.t_W2_a = tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n            self.t_B2_a=tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1),1/math.sqrt(N_HIDDEN_1)))\n            self.t_W3_a = tf.Variable(tf.random_uniform([N_HIDDEN_2,num_actions],-0.003,0.003))\n            self.t_B3_a = tf.Variable(tf.random_uniform([num_actions],-0.003,0.003))\n            \n            self.t_is_training = tf.placeholder(tf.bool, [])\n            self.t_H1_t= tf.matmul(self.t_actor_state_in,self.t_W1_a)\n            self.t_H1_a_bn = batch_norm(self.t_H1_t,N_HIDDEN_1, self.t_is_training, self.sess,self.H1_a_bn)\n            self.t_H1_a = tf.nn.softplus(self.t_H1_a_bn.bnorm) + self.t_B1_a\n            \n            self.t_H2_t=tf.matmul(self.t_H1_a,self.t_W2_a)\n            self.t_H2_a_bn = batch_norm(self.t_H2_t,N_HIDDEN_2,self.t_is_training,self.sess,self.H2_a_bn)\n            self.t_H2_a = tf.nn.tanh(self.t_H2_a_bn.bnorm) + self.t_B2_a\n            self.t_actor_model=tf.matmul(self.t_H2_a,self.t_W3_a) + self.t_B3_a\n            \n            #cost of actor network:\n            self.q_gradient_input = tf.placeholder(""float"",[None,num_actions]) #gets input from action_gradient computed in critic network file\n            self.actor_parameters = [self.W1_a, self.B1_a, self.W2_a, self.B2_a,self.W3_a, self.B3_a, self.H1_a_bn.scale,self.H1_a_bn.beta,self.H2_a_bn.scale,self.H2_a_bn.beta]\n            self.parameters_gradients = tf.gradients(self.actor_model,self.actor_parameters,-self.q_gradient_input)#/BATCH_SIZE) changed -self.q_gradient to -\n            \n            self.optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE,epsilon=1e-08).apply_gradients(zip(self.parameters_gradients,self.actor_parameters))  \n            #initialize all tensor variable parameters:\n            self.sess.run(tf.initialize_all_variables())    \n            \n            #To make sure actor and target have same intial parmameters copy the parameters:\n            # copy target parameters\n            self.sess.run([\n\t\t\t\tself.t_W1_a.assign(self.W1_a),\n\t\t\t\tself.t_B1_a.assign(self.B1_a),\n\t\t\t\tself.t_W2_a.assign(self.W2_a),\n\t\t\t\tself.t_B2_a.assign(self.B2_a),\n\t\t\t\tself.t_W3_a.assign(self.W3_a),\n\t\t\t\tself.t_B3_a.assign(self.B3_a)])\n\n            self.update_target_actor_op = [\n                self.t_W1_a.assign(TAU*self.W1_a+(1-TAU)*self.t_W1_a),\n                self.t_B1_a.assign(TAU*self.B1_a+(1-TAU)*self.t_B1_a),  \n                self.t_W2_a.assign(TAU*self.W2_a+(1-TAU)*self.t_W2_a),\n                self.t_B2_a.assign(TAU*self.B2_a+(1-TAU)*self.t_B2_a),  \n                self.t_W3_a.assign(TAU*self.W3_a+(1-TAU)*self.t_W3_a),\n                self.t_B3_a.assign(TAU*self.B3_a+(1-TAU)*self.t_B3_a),\n                self.t_H1_a_bn.updateTarget,\n                self.t_H2_a_bn.updateTarget,\n            ]\n            \n        \n    def evaluate_actor(self,state_t):\n        return self.sess.run(self.actor_model, feed_dict={self.actor_state_in:state_t,self.is_training: False})        \n        \n        \n    def evaluate_target_actor(self,state_t_1):\n        return self.sess.run(self.t_actor_model, feed_dict={self.t_actor_state_in: state_t_1,self.t_is_training: False})\n        \n    def train_actor(self,actor_state_in,q_gradient_input):\n        self.sess.run([self.optimizer,self.H1_a_bn.train_mean,self.H1_a_bn.train_var,self.H2_a_bn.train_mean,self.H2_a_bn.train_var,self.t_H1_a_bn.train_mean,self.t_H1_a_bn.train_var,self.t_H2_a_bn.train_mean, self.t_H2_a_bn.train_var], feed_dict={ self.actor_state_in: actor_state_in,self.t_actor_state_in: actor_state_in, self.q_gradient_input: q_gradient_input,self.is_training: True,self.t_is_training: True})\n        \n    def update_target_actor(self):\n        self.sess.run(self.update_target_actor_op)    \n        \n        '"
critic_net.py,23,"b'import numpy as np\nimport tensorflow as tf\nimport math\n\nTAU = 0.001\nLEARNING_RATE= 0.001\nBATCH_SIZE = 64\nclass CriticNet:\n    """""" Critic Q value model of the DDPG algorithm """"""\n    def __init__(self,num_states,num_actions):\n        \n        self.g=tf.Graph()\n        with self.g.as_default():\n            self.sess = tf.InteractiveSession()\n            \n            #critic_q_model parameters:\n            self.W1_c, self.B1_c, self.W2_c, self.W2_action_c, self.B2_c, self.W3_c, self.B3_c,\\\n            self.critic_q_model, self.critic_state_in, self.critic_action_in = self.create_critic_net(num_states, num_actions)\n                                   \n            #create target_q_model:\n            self.t_W1_c, self.t_B1_c, self.t_W2_c, self.t_W2_action_c, self.t_B2_c, self.t_W3_c, self.t_B3_c,\\\n            self.t_critic_q_model, self.t_critic_state_in, self.t_critic_action_in = self.create_critic_net(num_states, num_actions)\n            \n            self.q_value_in=tf.placeholder(""float"",[None,1]) #supervisor\n            #self.l2_regularizer_loss = tf.nn.l2_loss(self.W1_c)+tf.nn.l2_loss(self.W2_c)+ tf.nn.l2_loss(self.W2_action_c) + tf.nn.l2_loss(self.W3_c)+tf.nn.l2_loss(self.B1_c)+tf.nn.l2_loss(self.B2_c)+tf.nn.l2_loss(self.B3_c) \n            self.l2_regularizer_loss = 0.0001*tf.reduce_sum(tf.pow(self.W2_c,2))+ 0.0001*tf.reduce_sum(tf.pow(self.B2_c,2))             \n            self.cost=tf.pow(self.critic_q_model-self.q_value_in,2)/BATCH_SIZE + self.l2_regularizer_loss#/tf.to_float(tf.shape(self.q_value_in)[0])\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(self.cost)\n            \n            #action gradient to be used in actor network:\n            #self.action_gradients=tf.gradients(self.critic_q_model,self.critic_action_in)\n            #from simple actor net:\n            self.act_grad_v = tf.gradients(self.critic_q_model, self.critic_action_in)\n            self.action_gradients = [self.act_grad_v[0]/tf.to_float(tf.shape(self.act_grad_v[0])[0])] #this is just divided by batch size\n            #from simple actor net:\n            self.check_fl = self.action_gradients             \n                       \n            #initialize all tensor variable parameters:\n            self.sess.run(tf.initialize_all_variables())\n            \n            #To make sure critic and target have same parmameters copy the parameters:\n            # copy target parameters\n            self.sess.run([\n\t\t\t\tself.t_W1_c.assign(self.W1_c),\n\t\t\t\tself.t_B1_c.assign(self.B1_c),\n\t\t\t\tself.t_W2_c.assign(self.W2_c),\n\t\t\t\tself.t_W2_action_c.assign(self.W2_action_c),\n\t\t\t\tself.t_B2_c.assign(self.B2_c),\n\t\t\t\tself.t_W3_c.assign(self.W3_c),\n\t\t\t\tself.t_B3_c.assign(self.B3_c)\n\t\t\t])\n            \n            self.update_target_critic_op = [\n                self.t_W1_c.assign(TAU*self.W1_c+(1-TAU)*self.t_W1_c),\n                self.t_B1_c.assign(TAU*self.B1_c+(1-TAU)*self.t_B1_c),\n                self.t_W2_c.assign(TAU*self.W2_c+(1-TAU)*self.t_W2_c),\n                self.t_W2_action_c.assign(TAU*self.W2_action_c+(1-TAU)*self.t_W2_action_c),\n                self.t_B2_c.assign(TAU*self.B2_c+(1-TAU)*self.t_B2_c),\n                self.t_W3_c.assign(TAU*self.W3_c+(1-TAU)*self.t_W3_c),\n                self.t_B3_c.assign(TAU*self.B3_c+(1-TAU)*self.t_B3_c)\n            ]\n            \n    def create_critic_net(self, num_states=4, num_actions=1):\n        N_HIDDEN_1 = 400\n        N_HIDDEN_2 = 300\n        critic_state_in = tf.placeholder(""float"",[None,num_states])\n        critic_action_in = tf.placeholder(""float"",[None,num_actions])    \n    \n        W1_c = tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n        B1_c = tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n        W2_c = tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))    \n        W2_action_c = tf.Variable(tf.random_uniform([num_actions,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))    \n        B2_c= tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions))) \n        W3_c= tf.Variable(tf.random_uniform([N_HIDDEN_2,1],-0.003,0.003))\n        B3_c= tf.Variable(tf.random_uniform([1],-0.003,0.003))\n    \n        H1_c=tf.nn.softplus(tf.matmul(critic_state_in,W1_c)+B1_c)\n        H2_c=tf.nn.tanh(tf.matmul(H1_c,W2_c)+tf.matmul(critic_action_in,W2_action_c)+B2_c)\n            \n        critic_q_model=tf.matmul(H2_c,W3_c)+B3_c\n            \n       \n        return W1_c, B1_c, W2_c, W2_action_c, B2_c, W3_c, B3_c, critic_q_model, critic_state_in, critic_action_in\n    \n    def train_critic(self, state_t_batch, action_batch, y_i_batch ):\n        self.sess.run(self.optimizer, feed_dict={self.critic_state_in: state_t_batch, self.critic_action_in:action_batch, self.q_value_in: y_i_batch})\n             \n    \n    def evaluate_target_critic(self,state_t_1,action_t_1):\n        return self.sess.run(self.t_critic_q_model, feed_dict={self.t_critic_state_in: state_t_1, self.t_critic_action_in: action_t_1})    \n        \n    def compute_delQ_a(self,state_t,action_t):\n#        print \'\\n\'\n#        print \'check grad number\'        \n#        ch= self.sess.run(self.check_fl, feed_dict={self.critic_state_in: state_t,self.critic_action_in: action_t})\n#        print len(ch)\n#        print len(ch[0])        \n#        raw_input(""Press Enter to continue..."")        \n        return self.sess.run(self.action_gradients, feed_dict={self.critic_state_in: state_t,self.critic_action_in: action_t})\n\n    def update_target_critic(self):\n        self.sess.run(self.update_target_critic_op)\n\n\n'"
critic_net_bn.py,40,"b'import tensorflow as tf\nimport math\nfrom batch_normalization.batch_norm import batch_norm\nimport numpy as np\nLEARNING_RATE= 0.001\nTAU = 0.001\nBATCH_SIZE = 64\nN_HIDDEN_1 = 400\nN_HIDDEN_2 = 300\nclass CriticNet_bn:\n    """""" Critic Q value model with batch normalization of the DDPG algorithm """"""\n    def __init__(self,num_states,num_actions):\n        \n        tf.reset_default_graph()\n        self.g=tf.Graph()\n        with self.g.as_default():\n            self.sess = tf.InteractiveSession()\n            \n            #Critic Q Network:\n            self.critic_state_in =  tf.placeholder(""float"",[None,num_states])\n            self.critic_action_in = tf.placeholder(""float"",[None,num_actions]) \n            self.W1_c = tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.B1_c = tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.W2_c = tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))  \n            self.B2_c= tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))\n            self.W2_action_c = tf.Variable(tf.random_uniform([num_actions,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))\n            self.W3_c = tf.Variable(tf.random_uniform([N_HIDDEN_2,1],-0.003,0.003))\n            self.B3_c = tf.Variable(tf.random_uniform([1],-0.003,0.003))\n            \n            self.is_training = tf.placeholder(tf.bool, [])\n            self.H1_t = tf.matmul(self.critic_state_in,self.W1_c)\n            self.H1_c_bn = batch_norm(self.H1_t,N_HIDDEN_1,self.is_training,self.sess)\n            \n            self.H1_c = tf.nn.softplus(self.H1_c_bn.bnorm) + self.B1_c\n\n        \n            self.H2_t = tf.matmul(self.H1_c,self.W2_c)+tf.matmul(self.critic_action_in,self.W2_action_c)\n            self.H2_c_bn = batch_norm(self.H2_t,N_HIDDEN_2,self.is_training,self.sess)\n            self.H2_c = tf.nn.tanh(self.H2_c_bn.bnorm) + self.B2_c\n            \n            self.critic_q_model = tf.matmul(self.H2_c,self.W3_c)+self.B3_c\n            \n           # Target Critic Q Network:\n            self.t_critic_state_in =  tf.placeholder(""float"",[None,num_states])\n            self.t_critic_action_in = tf.placeholder(""float"",[None,num_actions])\n            self.t_W1_c = tf.Variable(tf.random_uniform([num_states,N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.t_B1_c = tf.Variable(tf.random_uniform([N_HIDDEN_1],-1/math.sqrt(num_states),1/math.sqrt(num_states)))\n            self.t_W2_c = tf.Variable(tf.random_uniform([N_HIDDEN_1,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))  \n            self.t_W2_action_c = tf.Variable(tf.random_uniform([num_actions,N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))\n            self.t_B2_c= tf.Variable(tf.random_uniform([N_HIDDEN_2],-1/math.sqrt(N_HIDDEN_1+num_actions),1/math.sqrt(N_HIDDEN_1+num_actions)))\n            self.t_W3_c = tf.Variable(tf.random_uniform([N_HIDDEN_2,1],-0.003,0.003))\n            self.t_B3_c = tf.Variable(tf.random_uniform([1],-0.003,0.003))\n            \n            self.t_H1_t = tf.matmul(self.t_critic_state_in,self.t_W1_c)\n            self.t_H1_c_bn = batch_norm(self.t_H1_t,N_HIDDEN_1,self.is_training,self.sess,self.H1_c_bn)        \n            self.t_H1_c = tf.nn.softplus(self.t_H1_c_bn.bnorm) + self.t_B1_c\n\n            self.t_H2_t = tf.matmul(self.t_H1_c,self.t_W2_c)+tf.matmul(self.t_critic_action_in,self.t_W2_action_c)\n            self.t_H2_c_bn = batch_norm(self.t_H2_t,N_HIDDEN_2,self.is_training,self.sess,self.H2_c_bn)\n            self.t_H2_c = tf.nn.tanh(self.t_H2_c_bn.bnorm) + self.t_B2_c\n            \n            self.t_critic_q_model = tf.matmul(self.t_H2_c,self.t_W3_c)+self.t_B3_c\n            \n            \n            self.q_value_in=tf.placeholder(""float"",[None,1]) #supervisor\n            #self.l2_regularizer_loss = tf.nn.l2_loss(self.W1_c)+tf.nn.l2_loss(self.W2_c)+ tf.nn.l2_loss(self.W2_action_c) + tf.nn.l2_loss(self.W3_c)+tf.nn.l2_loss(self.B1_c)+tf.nn.l2_loss(self.B2_c)+tf.nn.l2_loss(self.B3_c) \n            self.l2_regularizer_loss = 0.0001*tf.reduce_sum(tf.pow(self.W2_c,2))             \n            self.cost=tf.pow(self.critic_q_model-self.q_value_in,2)/BATCH_SIZE + self.l2_regularizer_loss#/tf.to_float(tf.shape(self.q_value_in)[0])\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(self.cost)\n            self.act_grad_v = tf.gradients(self.critic_q_model, self.critic_action_in)\n            self.action_gradients = [self.act_grad_v[0]/tf.to_float(tf.shape(self.act_grad_v[0])[0])] #this is just divided by batch size\n            #from simple actor net:\n            self.check_fl = self.action_gradients             \n            \n            #initialize all tensor variable parameters:\n            self.sess.run(tf.initialize_all_variables())\n            \n            #To initialize critic and target with the same values:\n            # copy target parameters\n            self.sess.run([\n\t\t\t\tself.t_W1_c.assign(self.W1_c),\n\t\t\t\tself.t_B1_c.assign(self.B1_c),\n\t\t\t\tself.t_W2_c.assign(self.W2_c),\n\t\t\t\tself.t_W2_action_c.assign(self.W2_action_c),\n\t\t\t\tself.t_B2_c.assign(self.B2_c),\n\t\t\t\tself.t_W3_c.assign(self.W3_c),\n\t\t\t\tself.t_B3_c.assign(self.B3_c)\n\t\t\t])\n            \n            self.update_target_critic_op = [\n                self.t_W1_c.assign(TAU*self.W1_c+(1-TAU)*self.t_W1_c),\n                self.t_B1_c.assign(TAU*self.B1_c+(1-TAU)*self.t_B1_c),  \n                self.t_W2_c.assign(TAU*self.W2_c+(1-TAU)*self.t_W2_c),\n                self.t_W2_action_c.assign(TAU*self.W2_action_c+(1-TAU)*self.t_W2_action_c),\n                self.t_B2_c.assign(TAU*self.B2_c+(1-TAU)*self.t_B2_c),\n                self.t_W3_c.assign(TAU*self.W3_c+(1-TAU)*self.t_W3_c),\n                self.t_B3_c.assign(TAU*self.B3_c+(1-TAU)*self.t_B3_c),\n                self.t_H1_c_bn.updateTarget,\n                self.t_H2_c_bn.updateTarget\n            ]\n\n    def train_critic(self, state_t_batch, action_batch, y_i_batch ):\n        self.sess.run([self.optimizer,self.H1_c_bn.train_mean,self.H1_c_bn.train_var,self.H2_c_bn.train_mean,self.H2_c_bn.train_var,self.t_H1_c_bn.train_mean,self.t_H1_c_bn.train_var,self.t_H2_c_bn.train_mean,self.t_H2_c_bn.train_var], feed_dict={self.critic_state_in: state_t_batch,self.t_critic_state_in: state_t_batch, self.critic_action_in:action_batch,self.t_critic_action_in:action_batch, self.q_value_in: y_i_batch,self.is_training: True})\n        \n    def evaluate_target_critic(self,state_t_1,action_t_1):\n        return self.sess.run(self.t_critic_q_model, feed_dict={self.t_critic_state_in: state_t_1, self.t_critic_action_in: action_t_1,self.is_training: False})    \n        \n        \n    def compute_delQ_a(self,state_t,action_t):\n        return self.sess.run(self.action_gradients, feed_dict={self.critic_state_in: state_t,self.critic_action_in: action_t,self.is_training: False})\n\n    def update_target_critic(self):\n        self.sess.run(self.update_target_critic_op)\n         \n\n\n'"
ddpg.py,0,"b'import numpy as np\nfrom actor_net import ActorNet\nfrom critic_net import CriticNet\nfrom actor_net_bn import ActorNet_bn\nfrom critic_net_bn import CriticNet_bn\nfrom collections import deque\nfrom gym.spaces import Box, Discrete\nimport random\nfrom tensorflow_grad_inverter import grad_inverter\n\nREPLAY_MEMORY_SIZE = 10000\nBATCH_SIZE = 64\nGAMMA=0.99\nis_grad_inverter = True\nclass DDPG:\n    \n    """""" Deep Deterministic Policy Gradient Algorithm""""""\n    def __init__(self,env, is_batch_norm):\n        self.env = env \n        self.num_states = env.observation_space.shape[0]\n        self.num_actions = env.action_space.shape[0]\n        \n        \n        if is_batch_norm:\n            self.critic_net = CriticNet_bn(self.num_states, self.num_actions) \n            self.actor_net = ActorNet_bn(self.num_states, self.num_actions)\n            \n        else:\n            self.critic_net = CriticNet(self.num_states, self.num_actions) \n            self.actor_net = ActorNet(self.num_states, self.num_actions)\n        \n        #Initialize Buffer Network:\n        self.replay_memory = deque()\n        \n        #Intialize time step:\n        self.time_step = 0\n        self.counter = 0\n        \n        action_max = np.array(env.action_space.high).tolist()\n        action_min = np.array(env.action_space.low).tolist()        \n        action_bounds = [action_max,action_min] \n        self.grad_inv = grad_inverter(action_bounds)\n        \n        \n    def evaluate_actor(self, state_t):\n        return self.actor_net.evaluate_actor(state_t)\n    \n    def add_experience(self, observation_1, observation_2, action, reward, done):\n        self.observation_1 = observation_1\n        self.observation_2 = observation_2\n        self.action = action\n        self.reward = reward\n        self.done = done\n        self.replay_memory.append((self.observation_1, self.observation_2, self.action, self.reward,self.done))\n        self.time_step = self.time_step + 1\n        if(len(self.replay_memory)>REPLAY_MEMORY_SIZE):\n            self.replay_memory.popleft()\n            \n        \n    def minibatches(self):\n        batch = random.sample(self.replay_memory, BATCH_SIZE)\n        #state t\n        self.state_t_batch = [item[0] for item in batch]\n        self.state_t_batch = np.array(self.state_t_batch)\n        #state t+1        \n        self.state_t_1_batch = [item[1] for item in batch]\n        self.state_t_1_batch = np.array( self.state_t_1_batch)\n        self.action_batch = [item[2] for item in batch]\n        self.action_batch = np.array(self.action_batch)\n        self.action_batch = np.reshape(self.action_batch,[len(self.action_batch),self.num_actions])\n        self.reward_batch = [item[3] for item in batch]\n        self.reward_batch = np.array(self.reward_batch)\n        self.done_batch = [item[4] for item in batch]\n        self.done_batch = np.array(self.done_batch)  \n                  \n                 \n    def train(self):\n        #sample a random minibatch of N transitions from R\n        self.minibatches()\n        self.action_t_1_batch = self.actor_net.evaluate_target_actor(self.state_t_1_batch)\n        #Q\'(s_i+1,a_i+1)        \n        q_t_1 = self.critic_net.evaluate_target_critic(self.state_t_1_batch,self.action_t_1_batch) \n        self.y_i_batch=[]         \n        for i in range(0,BATCH_SIZE):\n                           \n            if self.done_batch[i]:\n                self.y_i_batch.append(self.reward_batch[i])\n            else:\n                \n                self.y_i_batch.append(self.reward_batch[i] + GAMMA*q_t_1[i][0])                 \n        \n        self.y_i_batch=np.array(self.y_i_batch)\n        self.y_i_batch = np.reshape(self.y_i_batch,[len(self.y_i_batch),1])\n        \n        # Update critic by minimizing the loss\n        self.critic_net.train_critic(self.state_t_batch, self.action_batch,self.y_i_batch)\n        \n        # Update actor proportional to the gradients:\n        action_for_delQ = self.evaluate_actor(self.state_t_batch) \n        \n        if is_grad_inverter:        \n            self.del_Q_a = self.critic_net.compute_delQ_a(self.state_t_batch,action_for_delQ)#/BATCH_SIZE            \n            self.del_Q_a = self.grad_inv.invert(self.del_Q_a,action_for_delQ) \n        else:\n            self.del_Q_a = self.critic_net.compute_delQ_a(self.state_t_batch,action_for_delQ)[0]#/BATCH_SIZE\n        \n        # train actor network proportional to delQ/dela and del_Actor_model/del_actor_parameters:\n        self.actor_net.train_actor(self.state_t_batch,self.del_Q_a)\n \n        # Update target Critic and actor network\n        self.critic_net.update_target_critic()\n        self.actor_net.update_target_actor()\n        \n                \n        \n        \n        \n                \n        \n        \n        \n                     \n                 \n        \n\n\n\n'"
main.py,0,"b'#Implementation of Deep Deterministic Gradient with Tensor Flow""\n# Author: Steven Spielberg Pon Kumar (github.com/stevenpjg)\n\nimport gym\nfrom gym.spaces import Box, Discrete\nimport numpy as np\nfrom ddpg import DDPG\nfrom ou_noise import OUNoise\n#specify parameters here:\nepisodes=10000\nis_batch_norm = False #batch normalization switch\n\ndef main():\n    experiment= \'InvertedPendulum-v1\' #specify environments here\n    env= gym.make(experiment)\n    steps= env.spec.timestep_limit #steps per episode    \n    assert isinstance(env.observation_space, Box), ""observation space must be continuous""\n    assert isinstance(env.action_space, Box), ""action space must be continuous""\n    \n    #Randomly initialize critic,actor,target critic, target actor network  and replay buffer   \n    agent = DDPG(env, is_batch_norm)\n    exploration_noise = OUNoise(env.action_space.shape[0])\n    counter=0\n    reward_per_episode = 0    \n    total_reward=0\n    num_states = env.observation_space.shape[0]\n    num_actions = env.action_space.shape[0]    \n    print ""Number of States:"", num_states\n    print ""Number of Actions:"", num_actions\n    print ""Number of Steps per episode:"", steps\n    #saving reward:\n    reward_st = np.array([0])\n      \n    \n    for i in xrange(episodes):\n        print ""==== Starting episode no:"",i,""===="",""\\n""\n        observation = env.reset()\n        reward_per_episode = 0\n        for t in xrange(steps):\n            #rendering environmet (optional)            \n            env.render()\n            x = observation\n            action = agent.evaluate_actor(np.reshape(x,[1,num_states]))\n            noise = exploration_noise.noise()\n            action = action[0] + noise #Select action according to current policy and exploration noise\n            print ""Action at step"", t ,"" :"",action,""\\n""\n            \n            observation,reward,done,info=env.step(action)\n            \n            #add s_t,s_t+1,action,reward to experience memory\n            agent.add_experience(x,observation,action,reward,done)\n            #train critic and actor network\n            if counter > 64: \n                agent.train()\n            reward_per_episode+=reward\n            counter+=1\n            #check if episode ends:\n            if (done or (t == steps-1)):\n                print \'EPISODE: \',i,\' Steps: \',t,\' Total Reward: \',reward_per_episode\n                print ""Printing reward to file""\n                exploration_noise.reset() #reinitializing random noise for action exploration\n                reward_st = np.append(reward_st,reward_per_episode)\n                np.savetxt(\'episode_reward.txt\',reward_st, newline=""\\n"")\n                print \'\\n\\n\'\n                break\n    total_reward+=reward_per_episode            \n    print ""Average reward per episode {}"".format(total_reward / episodes)    \n\n\nif __name__ == \'__main__\':\n    main()    '"
ou_noise.py,0,"b'# --------------------------------------\n# Reference: https://github.com/rllab/rllab/blob/master/rllab/exploration_strategies/ou_strategy.py\n# --------------------------------------\n\nimport numpy as np\nimport numpy.random as nr\n\nclass OUNoise:\n    """""" docstring for OUNoise """"""\n    def __init__(self,action_dimension,mu=0, theta=0.15, sigma=0.3):\n        self.action_dimension = action_dimension\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.state = np.ones(self.action_dimension) * self.mu\n        self.reset()\n\n    def reset(self):\n        self.state = np.ones(self.action_dimension) * self.mu\n\n    def noise(self):\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * nr.randn(len(x))\n        self.state = x + dx\n        return self.state\n\nif __name__ == \'__main__\':\n    ou = OUNoise(3)\n    states = []\n    for i in range(1000):\n        states.append(ou.noise())\n    import matplotlib.pyplot as plt\n\n    plt.plot(states)\n    plt.show()\n'"
result_plot.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Jul  7 13:18:23 2016\n\n@author: steven\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\nlines = np.loadtxt(""episode_reward.txt"", comments=""#"", delimiter=""\\n"", unpack=False)\n\nplt.plot(lines)\nplt.show()'"
tensorflow_grad_inverter.py,10,"b'#Reference:\n#https://github.com/MOCR/\n\nimport tensorflow as tf\n\n\n\nclass grad_inverter:\n    def __init__(self, action_bounds):\n\n        self.sess = tf.InteractiveSession()       \n        \n        self.action_size = len(action_bounds[0])\n        \n        self.action_input = tf.placeholder(tf.float32, [None, self.action_size])\n        self.pmax = tf.constant(action_bounds[0], dtype = tf.float32)\n        self.pmin = tf.constant(action_bounds[1], dtype = tf.float32)\n        self.prange = tf.constant([x - y for x, y in zip(action_bounds[0],action_bounds[1])], dtype = tf.float32)\n        self.pdiff_max = tf.div(-self.action_input+self.pmax, self.prange)\n        self.pdiff_min = tf.div(self.action_input - self.pmin, self.prange)\n        self.zeros_act_grad_filter = tf.zeros([self.action_size])\n        self.act_grad = tf.placeholder(tf.float32, [None, self.action_size])\n        self.grad_inverter = tf.select(tf.greater(self.act_grad, self.zeros_act_grad_filter), tf.mul(self.act_grad, self.pdiff_max), tf.mul(self.act_grad, self.pdiff_min))        \n    \n    def invert(self, grad, action):\n\n        \n        return self.sess.run(self.grad_inverter, feed_dict = {self.action_input: action, self.act_grad: grad[0]})\n'"
tensorflow_session.py,1,b'import tensorflow as tf\n\nclass tfs:\n    session = tf.Session()'
batch_normalization/__init__.py,0,b''
batch_normalization/batch_norm.py,11,"b'import tensorflow as tf\ndecay = 0.95\nTAU = 0.001\n\nclass batch_norm:\n    def __init__(self,inputs,size,is_training,sess,parForTarget=None,bn_param=None):\n        \n        self.sess = sess        \n        self.scale = tf.Variable(tf.random_uniform([size],0.9,1.1))\n        self.beta = tf.Variable(tf.random_uniform([size],-0.03,0.03))\n        self.pop_mean = tf.Variable(tf.random_uniform([size],-0.03,0.03),trainable=False)\n        self.pop_var = tf.Variable(tf.random_uniform([size],0.9,1.1),trainable=False)        \n        self.batch_mean, self.batch_var = tf.nn.moments(inputs,[0])        \n        self.train_mean = tf.assign(self.pop_mean,self.pop_mean * decay + self.batch_mean * (1 - decay))  \n        self.train_var = tf.assign(self.pop_var,self.pop_var * decay + self.batch_var * (1 - decay))\n                \n        def training(): \n            return tf.nn.batch_normalization(inputs,\n                self.batch_mean, self.batch_var, self.beta, self.scale, 0.0000001 )\n    \n        def testing(): \n            return tf.nn.batch_normalization(inputs,\n            self.pop_mean, self.pop_var, self.beta, self.scale, 0.0000001)\n        \n        if parForTarget!=None:\n            self.parForTarget = parForTarget\n            self.updateScale = self.scale.assign(self.scale*(1-TAU)+self.parForTarget.scale*TAU)\n            self.updateBeta = self.beta.assign(self.beta*(1-TAU)+self.parForTarget.beta*TAU)\n            self.updateTarget = tf.group(self.updateScale, self.updateBeta)\n            \n        self.bnorm = tf.cond(is_training,training,testing) \n        '"
tf.gradients_eg/tf.gradients_eg.py,11,"b'\'\'\'\nUnderstanding optimization with tf.gradients using linear regression\n\nAuthor: Steven Spielberg Pon Kumar\n\n\'\'\'\n\nimport tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\nrng = numpy.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 1000\ndisplay_step = 50\n\n# Training Data\ntrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\ntrain_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\nn_samples = train_X.shape[0]\n\n# tf Graph Input\nX = tf.placeholder(""float"")\nY = tf.placeholder(""float"")\n\n# Set model weights\nW = tf.Variable(0.1, name=""weight"")\nb = tf.Variable(0.1, name=""bias"")\n\n# Construct a linear model\npred = tf.add(tf.mul(X, W), b)\n\n# Mean squared error\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n\nparams= [W,b]\n\ngradient = tf.gradients(cost,params)\nopt = tf.train.GradientDescentOptimizer(learning_rate)\nupdate=opt.apply_gradients(zip(gradient,params))\n# Initializing the variables\ninit = tf.initialize_all_variables()\n\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        sess.run(update, feed_dict={X: train_X, Y: train_Y}) #gradient descent\n\n        #Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n            print ""Epoch:"", \'%04d\' % (epoch+1), ""cost="", ""{:.9f}"".format(c), \\\n                ""W="", sess.run(W), ""b="", sess.run(b)\n\n    print ""Optimization Finished!""\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print ""Training cost="", training_cost, ""W="", sess.run(W), ""b="", sess.run(b), \'\\n\'\n\n\n\n'"
