file_path,api_count,code
metrics.py,0,"b'#!/usr/bin/python\n\n\'\'\'\nMartin Kersner, m.kersner@gmail.com\n2015/11/30\n\nEvaluation metrics for image segmentation inspired by\npaper Fully Convolutional Networks for Semantic Segmentation.\n\'\'\'\n\nimport numpy as np\n\ndef pixel_accuracy(eval_segm, gt_segm):\n    \'\'\'\n    sum_i(n_ii) / sum_i(t_i)\n    \'\'\'\n\n    check_size(eval_segm, gt_segm)\n\n    cl, n_cl = extract_classes(gt_segm)\n    eval_mask, gt_mask = extract_both_masks(eval_segm, gt_segm, cl, n_cl)\n\n    sum_n_ii = 0\n    sum_t_i  = 0\n\n    for i, c in enumerate(cl):\n        curr_eval_mask = eval_mask[i, :, :]\n        curr_gt_mask = gt_mask[i, :, :]\n\n        sum_n_ii += np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n        sum_t_i  += np.sum(curr_gt_mask)\n \n    if (sum_t_i == 0):\n        pixel_accuracy_ = 0\n    else:\n        pixel_accuracy_ = sum_n_ii / sum_t_i\n\n    return pixel_accuracy_\n\ndef mean_accuracy(eval_segm, gt_segm):\n    \'\'\'\n    (1/n_cl) sum_i(n_ii/t_i)\n    \'\'\'\n\n    check_size(eval_segm, gt_segm)\n\n    cl, n_cl = extract_classes(gt_segm)\n    eval_mask, gt_mask = extract_both_masks(eval_segm, gt_segm, cl, n_cl)\n\n    accuracy = list([0]) * n_cl\n\n    for i, c in enumerate(cl):\n        curr_eval_mask = eval_mask[i, :, :]\n        curr_gt_mask = gt_mask[i, :, :]\n\n        n_ii = np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n        t_i  = np.sum(curr_gt_mask)\n \n        if (t_i != 0):\n            accuracy[i] = n_ii / t_i\n\n    mean_accuracy_ = np.mean(accuracy)\n    return mean_accuracy_\n\ndef mean_IU(eval_segm, gt_segm):\n    \'\'\'\n    (1/n_cl) * sum_i(n_ii / (t_i + sum_j(n_ji) - n_ii))\n    \'\'\'\n\n    check_size(eval_segm, gt_segm)\n\n    cl, n_cl   = union_classes(eval_segm, gt_segm)\n    _, n_cl_gt = extract_classes(gt_segm)\n    eval_mask, gt_mask = extract_both_masks(eval_segm, gt_segm, cl, n_cl)\n\n    IU = list([0]) * n_cl\n\n    for i, c in enumerate(cl):\n        curr_eval_mask = eval_mask[i, :, :]\n        curr_gt_mask = gt_mask[i, :, :]\n \n        if (np.sum(curr_eval_mask) == 0) or (np.sum(curr_gt_mask) == 0):\n            continue\n\n        n_ii = np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n        t_i  = np.sum(curr_gt_mask)\n        n_ij = np.sum(curr_eval_mask)\n\n        IU[i] = n_ii / (t_i + n_ij - n_ii)\n\n    mean_IU_ = np.sum(IU) / n_cl_gt\n    return mean_IU_\n\ndef frequency_weighted_IU(eval_segm, gt_segm):\n    \'\'\'\n    sum_k(t_k)^(-1) * sum_i((t_i*n_ii)/(t_i + sum_j(n_ji) - n_ii))\n    \'\'\'\n\n    check_size(eval_segm, gt_segm)\n\n    cl, n_cl = union_classes(eval_segm, gt_segm)\n    eval_mask, gt_mask = extract_both_masks(eval_segm, gt_segm, cl, n_cl)\n\n    frequency_weighted_IU_ = list([0]) * n_cl\n\n    for i, c in enumerate(cl):\n        curr_eval_mask = eval_mask[i, :, :]\n        curr_gt_mask = gt_mask[i, :, :]\n \n        if (np.sum(curr_eval_mask) == 0) or (np.sum(curr_gt_mask) == 0):\n            continue\n\n        n_ii = np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n        t_i  = np.sum(curr_gt_mask)\n        n_ij = np.sum(curr_eval_mask)\n\n        frequency_weighted_IU_[i] = (t_i * n_ii) / (t_i + n_ij - n_ii)\n \n    sum_k_t_k = get_pixel_area(eval_segm)\n    \n    frequency_weighted_IU_ = np.sum(frequency_weighted_IU_) / sum_k_t_k\n    return frequency_weighted_IU_\n\n\'\'\'\nAuxiliary functions used during evaluation.\n\'\'\'\ndef get_pixel_area(segm):\n    return segm.shape[0] * segm.shape[1]\n\ndef extract_both_masks(eval_segm, gt_segm, cl, n_cl):\n    eval_mask = extract_masks(eval_segm, cl, n_cl)\n    gt_mask   = extract_masks(gt_segm, cl, n_cl)\n\n    return eval_mask, gt_mask\n\ndef extract_classes(segm):\n    cl = np.unique(segm)\n    n_cl = len(cl)\n\n    return cl, n_cl\n\ndef union_classes(eval_segm, gt_segm):\n    eval_cl, _ = extract_classes(eval_segm)\n    gt_cl, _   = extract_classes(gt_segm)\n\n    cl = np.union1d(eval_cl, gt_cl)\n    n_cl = len(cl)\n\n    return cl, n_cl\n\ndef extract_masks(segm, cl, n_cl):\n    h, w  = segm_size(segm)\n    masks = np.zeros((n_cl, h, w))\n\n    for i, c in enumerate(cl):\n        masks[i, :, :] = segm == c\n\n    return masks\n\ndef segm_size(segm):\n    try:\n        height = segm.shape[0]\n        width  = segm.shape[1]\n    except IndexError:\n        raise\n\n    return height, width\n\ndef check_size(eval_segm, gt_segm):\n    h_e, w_e = segm_size(eval_segm)\n    h_g, w_g = segm_size(gt_segm)\n\n    if (h_e != h_g) or (w_e != w_g):\n        raise EvalSegErr(""DiffDim: Different dimensions of matrices!"")\n\n\'\'\'\nExceptions\n\'\'\'\nclass EvalSegErr(Exception):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)'"
network.py,10,"b'import tensorflow as tf\nslim = tf.contrib.slim\nfrom resnet import resnet_v2, resnet_utils\n\n# ImageNet mean statistics\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n@slim.add_arg_scope\ndef atrous_spatial_pyramid_pooling(net, scope, depth=256, reuse=None):\n    """"""\n    ASPP consists of (a) one 1\xc3\x971 convolution and three 3\xc3\x973 convolutions with rates = (6, 12, 18) when output stride = 16\n    (all with 256 filters and batch normalization), and (b) the image-level features as described in https://arxiv.org/abs/1706.05587\n    :param net: tensor of shape [BATCH_SIZE, WIDTH, HEIGHT, DEPTH]\n    :param scope: scope name of the aspp layer\n    :return: network layer with aspp applyed to it.\n    """"""\n\n    with tf.variable_scope(scope, reuse=reuse):\n        feature_map_size = tf.shape(net)\n\n        # apply global average pooling\n        image_level_features = tf.reduce_mean(net, [1, 2], name=\'image_level_global_pool\', keepdims=True)\n        image_level_features = slim.conv2d(image_level_features, depth, [1, 1], scope=""image_level_conv_1x1"",\n                                           activation_fn=None)\n        image_level_features = tf.image.resize_bilinear(image_level_features, (feature_map_size[1], feature_map_size[2]))\n\n        at_pool1x1 = slim.conv2d(net, depth, [1, 1], scope=""conv_1x1_0"", activation_fn=None)\n\n        at_pool3x3_1 = slim.conv2d(net, depth, [3, 3], scope=""conv_3x3_1"", rate=6, activation_fn=None)\n\n        at_pool3x3_2 = slim.conv2d(net, depth, [3, 3], scope=""conv_3x3_2"", rate=12, activation_fn=None)\n\n        at_pool3x3_3 = slim.conv2d(net, depth, [3, 3], scope=""conv_3x3_3"", rate=18, activation_fn=None)\n\n        net = tf.concat((image_level_features, at_pool1x1, at_pool3x3_1, at_pool3x3_2, at_pool3x3_3), axis=3,\n                        name=""concat"")\n        net = slim.conv2d(net, depth, [1, 1], scope=""conv_1x1_output"", activation_fn=None)\n        return net\n\n\ndef deeplab_v3(inputs, args, is_training, reuse):\n\n    # mean subtraction normalization\n    inputs = inputs - [_R_MEAN, _G_MEAN, _B_MEAN]\n\n    # inputs has shape - Original: [batch, 513, 513, 3]\n    with slim.arg_scope(resnet_utils.resnet_arg_scope(args.l2_regularizer, is_training,\n                                                      args.batch_norm_decay,\n                                                      args.batch_norm_epsilon)):\n        resnet = getattr(resnet_v2, args.resnet_model)\n        _, end_points = resnet(inputs,\n                               args.number_of_classes,\n                               is_training=is_training,\n                               global_pool=False,\n                               spatial_squeeze=False,\n                               output_stride=args.output_stride,\n                               reuse=reuse)\n\n        with tf.variable_scope(""DeepLab_v3"", reuse=reuse):\n\n            # get block 4 feature outputs\n            net = end_points[args.resnet_model + \'/block4\']\n\n            net = atrous_spatial_pyramid_pooling(net, ""ASPP_layer"", depth=256, reuse=reuse)\n\n            net = slim.conv2d(net, args.number_of_classes, [1, 1], activation_fn=None,\n                              normalizer_fn=None, scope=\'logits\')\n\n            size = tf.shape(inputs)[1:3]\n            # resize the output logits to match the labels dimensions\n            #net = tf.image.resize_nearest_neighbor(net, size)\n            net = tf.image.resize_bilinear(net, size)\n            return net\n'"
test.py,14,"b'import tensorflow as tf\nprint(""TF version:"", tf.__version__)\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport network\nslim = tf.contrib.slim\nimport os\nimport argparse\nimport json\nfrom preprocessing.read_data import tf_record_parser, scale_image_with_crop_padding\nfrom preprocessing import training\nfrom metrics import *\n\nplt.interactive(False)\n\nparser = argparse.ArgumentParser()\n\nenvarg = parser.add_argument_group(\'Eval params\')\nenvarg.add_argument(""--model_id"", default=16645, type=int, help=""Model id name to be loaded."")\ninput_args = parser.parse_args()\n\n# best: 16645\nmodel_name = str(input_args.model_id)\n\n# uncomment and set the GPU id if applicable.\n# os.environ[""CUDA_VISIBLE_DEVICES""]=""3""\n\nlog_folder = \'./tboard_logs\'\n\nif not os.path.exists(os.path.join(log_folder, model_name, ""test"")):\n    os.makedirs(os.path.join(log_folder, model_name, ""test""))\n\nwith open(log_folder + \'/\' + model_name + \'/train/data.json\', \'r\') as fp:\n    args = json.load(fp)\n\nclass Dotdict(dict):\n     """"""dot.notation access to dictionary attributes""""""\n     __getattr__ = dict.get\n     __setattr__ = dict.__setitem__\n     __delattr__ = dict.__delitem__\n\nargs = Dotdict(args)\n\n# 0=background\n# 1=aeroplane\n# 2=bicycle\n# 3=bird\n# 4=boat\n# 5=bottle\n# 6=bus\n# 7=car\n# 8=cat\n# 9=chair\n# 10=cow\n# 11=diningtable\n# 12=dog\n# 13=horse\n# 14=motorbike\n# 15=person\n# 16=potted plant\n# 17=sheep\n# 18=sofa\n# 19=train\n# 20=tv/monitor\n# 255=unknown\n\nclass_labels = [v for v in range((args.number_of_classes+1))]\nclass_labels[-1] = 255\n\nLOG_FOLDER = \'./tboard_logs\'\nTEST_DATASET_DIR=""./dataset/tfrecords""\nTEST_FILE = \'test.tfrecords\'\n\ntest_filenames = [os.path.join(TEST_DATASET_DIR,TEST_FILE)]\ntest_dataset = tf.data.TFRecordDataset(test_filenames)\ntest_dataset = test_dataset.map(tf_record_parser)  # Parse the record into tensors.\ntest_dataset = test_dataset.map(lambda image, annotation, image_shape: scale_image_with_crop_padding(image, annotation, image_shape, args.crop_size))\ntest_dataset = test_dataset.shuffle(buffer_size=100)\ntest_dataset = test_dataset.batch(args.batch_size)\n\niterator = test_dataset.make_one_shot_iterator()\nbatch_images_tf, batch_labels_tf, batch_shapes_tf = iterator.get_next()\n\nlogits_tf =  network.deeplab_v3(batch_images_tf, args, is_training=False, reuse=False)\n\nvalid_labels_batch_tf, valid_logits_batch_tf = training.get_valid_logits_and_labels(\n    annotation_batch_tensor=batch_labels_tf,\n    logits_batch_tensor=logits_tf,\n    class_labels=class_labels)\n\ncross_entropies_tf = tf.nn.softmax_cross_entropy_with_logits_v2(logits=valid_logits_batch_tf,\n                                                                labels=valid_labels_batch_tf)\n\ncross_entropy_mean_tf = tf.reduce_mean(cross_entropies_tf)\ntf.summary.scalar(\'cross_entropy\', cross_entropy_mean_tf)\n\npredictions_tf = tf.argmax(logits_tf, axis=3)\nprobabilities_tf = tf.nn.softmax(logits_tf)\n\nmerged_summary_op = tf.summary.merge_all()\nsaver = tf.train.Saver()\n\ntest_folder = os.path.join(log_folder, model_name, ""test"")\ntrain_folder = os.path.join(log_folder, model_name, ""train"")\n\nwith tf.Session() as sess:\n\n    # Create a saver.\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    # Restore variables from disk.\n    saver.restore(sess, os.path.join(train_folder, ""model.ckpt""))\n    print(""Model"", model_name, ""restored."")\n\n    mean_IoU = []\n    mean_pixel_acc = []\n    mean_freq_weighted_IU = []\n    mean_acc = []\n\n    while True:\n        try:\n            batch_images_np, batch_predictions_np, batch_labels_np, batch_shapes_np, summary_string = \\\n                sess.run([batch_images_tf, predictions_tf, batch_labels_tf, batch_shapes_tf, merged_summary_op])\n\n            heights, widths = batch_shapes_np\n\n            # loop through the images in the batch and extract the valid areas from the tensors\n            for i in range(batch_predictions_np.shape[0]):\n\n                label_image = batch_labels_np[i]\n                pred_image = batch_predictions_np[i]\n                input_image = batch_images_np[i]\n\n                indices = np.where(label_image != 255)\n                label_image = label_image[indices]\n                pred_image = pred_image[indices]\n                input_image = input_image[indices]\n\n                if label_image.shape[0] == 263169:\n                    label_image = np.reshape(label_image, (513,513))\n                    pred_image = np.reshape(pred_image, (513,513))\n                    input_image = np.reshape(input_image, (513,513,3))\n                else:\n                    label_image = np.reshape(label_image, (heights[i], widths[i]))\n                    pred_image = np.reshape(pred_image, (heights[i], widths[i]))\n                    input_image = np.reshape(input_image, (heights[i], widths[i], 3))\n\n                pix_acc = pixel_accuracy(pred_image, label_image)\n                m_acc = mean_accuracy(pred_image, label_image)\n                IoU = mean_IU(pred_image, label_image)\n                freq_weighted_IU = frequency_weighted_IU(pred_image, label_image)\n\n                mean_pixel_acc.append(pix_acc)\n                mean_acc.append(m_acc)\n                mean_IoU.append(IoU)\n                mean_freq_weighted_IU.append(freq_weighted_IU)\n\n                f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(8, 8))\n\n                ax1.imshow(input_image.astype(np.uint8))\n                ax2.imshow(label_image)\n                ax3.imshow(pred_image)\n                plt.show()\n\n        except tf.errors.OutOfRangeError:\n            break\n\n    print(""Mean pixel accuracy:"", np.mean(mean_pixel_acc))\n    print(""Mean accuraccy:"", np.mean(mean_acc))\n    print(""Mean IoU:"", np.mean(mean_IoU))\n    print(""Mean frequency weighted IU:"", np.mean(mean_freq_weighted_IU))\n'"
train.py,27,"b'\nimport argparse\nimport tensorflow as tf\nimport numpy as np\nslim = tf.contrib.slim\nimport os\nimport json\nimport network\nfrom preprocessing.read_data import download_resnet_checkpoint_if_necessary, tf_record_parser, \\\n    rescale_image_and_annotation_by_factor, scale_image_with_crop_padding, \\\n    random_flip_image_and_annotation, distort_randomly_image_color\nfrom preprocessing import training\n\n# 0=background\n# 1=aeroplane\n# 2=bicycle\n# 3=bird\n# 4=boat\n# 5=bottle\n# 6=bus\n# 7=car\n# 8=cat\n# 9=chair\n# 10=cow\n# 11=diningtable\n# 12=dog\n# 13=horse\n# 14=motorbike\n# 15=person\n# 16=potted plant\n# 17=sheep\n# 18=sofa\n# 19=train\n# 20=tv/monitor\n# 255=unknown\n\nparser = argparse.ArgumentParser()\n\nenvarg = parser.add_argument_group(\'Training params\')\nenvarg.add_argument(""--batch_norm_epsilon"", type=float, default=1e-5, help=""batch norm epsilon argument for batch normalization"")\nenvarg.add_argument(\'--batch_norm_decay\', type=float, default=0.9997, help=\'batch norm decay argument for batch normalization.\')\nenvarg.add_argument(""--number_of_classes"", type=int, default=21, help=""Number of classes to be predicted."")\nenvarg.add_argument(""--l2_regularizer"", type=float, default=0.0001, help=""l2 regularizer parameter."")\nenvarg.add_argument(\'--starting_learning_rate\', type=float, default=0.00001, help=""initial learning rate."")\nenvarg.add_argument(""--multi_grid"", type=list, default=[1,2,4], help=""Spatial Pyramid Pooling rates"")\nenvarg.add_argument(""--output_stride"", type=int, default=16, help=""Spatial Pyramid Pooling rates"")\nenvarg.add_argument(""--gpu_id"", type=int, default=0, help=""Id of the GPU to be used"")\nenvarg.add_argument(""--crop_size"", type=int, default=513, help=""Image Cropsize."")\nenvarg.add_argument(""--resnet_model"", default=""resnet_v2_50"", choices=[""resnet_v2_50"", ""resnet_v2_101"", ""resnet_v2_152"", ""resnet_v2_200""], help=""Resnet model to use as feature extractor. Choose one of: resnet_v2_50 or resnet_v2_101"")\n\nenvarg.add_argument(""--current_best_val_loss"", type=int, default=99999, help=""Best validation loss value."")\nenvarg.add_argument(""--accumulated_validation_miou"", type=int, default=0, help=""Accumulated validation intersection over union."")\n\ntrainarg = parser.add_argument_group(\'Training\')\ntrainarg.add_argument(""--batch_size"", type=int, default=8, help=""Batch size for network train."")\n\nargs = parser.parse_args()\nos.environ[""CUDA_VISIBLE_DEVICES""]=str(args.gpu_id)\n\nLOG_FOLDER = \'./tboard_logs\'\nTRAIN_DATASET_DIR=""./dataset/tfrecords""\nTRAIN_FILE = \'train.tfrecords\'\nVALIDATION_FILE = \'validation.tfrecords\'\n\ncrop_size = args.crop_size\ntraining_filenames = [os.path.join(TRAIN_DATASET_DIR,TRAIN_FILE)]\ntraining_dataset = tf.data.TFRecordDataset(training_filenames)\ntraining_dataset = training_dataset.map(tf_record_parser)\ntraining_dataset = training_dataset.map(rescale_image_and_annotation_by_factor)\ntraining_dataset = training_dataset.map(distort_randomly_image_color)\ntraining_dataset = training_dataset.map(lambda image, annotation, image_shape: scale_image_with_crop_padding(image, annotation, image_shape, crop_size))\ntraining_dataset = training_dataset.map(random_flip_image_and_annotation)  # Parse the record into tensors.\ntraining_dataset = training_dataset.repeat()  # number of epochs\ntraining_dataset = training_dataset.shuffle(buffer_size=500)\ntraining_dataset = training_dataset.batch(args.batch_size)\n\nvalidation_filenames = [os.path.join(TRAIN_DATASET_DIR,VALIDATION_FILE)]\nvalidation_dataset = tf.data.TFRecordDataset(validation_filenames)\nvalidation_dataset = validation_dataset.map(tf_record_parser)  # Parse the record into tensors.\nvalidation_dataset = validation_dataset.map(lambda image, annotation, image_shape: scale_image_with_crop_padding(image, annotation, image_shape, crop_size))\nvalidation_dataset = validation_dataset.shuffle(buffer_size=100)\nvalidation_dataset = validation_dataset.batch(args.batch_size)\n\nresnet_checkpoints_path = ""./resnet/checkpoints/""\ndownload_resnet_checkpoint_if_necessary(resnet_checkpoints_path, args.resnet_model)\n\n# A feedable iterator is defined by a handle placeholder and its structure. We\n# could use the `output_types` and `output_shapes` properties of either\n# `training_dataset` or `validation_dataset` here, because they have\n# identical structure.\nhandle = tf.placeholder(tf.string, shape=[])\n\niterator = tf.data.Iterator.from_string_handle(\n    handle, training_dataset.output_types, training_dataset.output_shapes)\nbatch_images_tf, batch_labels_tf, _ = iterator.get_next()\n\n# You can use feedable iterators with a variety of different kinds of iterator\n# (such as one-shot and initializable iterators).\ntraining_iterator = training_dataset.make_initializable_iterator()\nvalidation_iterator = validation_dataset.make_initializable_iterator()\n\nclass_labels = [v for v in range((args.number_of_classes+1))]\nclass_labels[-1] = 255\n\nis_training_tf = tf.placeholder(tf.bool, shape=[])\n\nlogits_tf = tf.cond(is_training_tf, true_fn= lambda: network.deeplab_v3(batch_images_tf, args, is_training=True, reuse=False),\n                    false_fn=lambda: network.deeplab_v3(batch_images_tf, args, is_training=False, reuse=True))\n\n# get valid logits and labels (factor the 255 padded mask out for cross entropy)\nvalid_labels_batch_tf, valid_logits_batch_tf = training.get_valid_logits_and_labels(\n    annotation_batch_tensor=batch_labels_tf,\n    logits_batch_tensor=logits_tf,\n    class_labels=class_labels)\n\ncross_entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits=valid_logits_batch_tf,\n                                                             labels=valid_labels_batch_tf)\ncross_entropy_tf = tf.reduce_mean(cross_entropies)\npredictions_tf = tf.argmax(logits_tf, axis=3)\n\ntf.summary.scalar(\'cross_entropy\', cross_entropy_tf)\n#tf.summary.image(""prediction"", tf.expand_dims(tf.cast(pred, tf.float32),3), 1)\n#tf.summary.image(""label"", tf.expand_dims(tf.cast(batch_labels, tf.float32),3), 1)\n\nwith tf.variable_scope(""optimizer_vars""):\n    global_step = tf.Variable(0, trainable=False)\n    optimizer = tf.train.AdamOptimizer(learning_rate=args.starting_learning_rate)\n    train_step = slim.learning.create_train_op(cross_entropy_tf, optimizer, global_step=global_step)\n\n# Put all summary ops into one op. Produces string when you run it.\nprocess_str_id = str(os.getpid())\nmerged_summary_op = tf.summary.merge_all()\nLOG_FOLDER = os.path.join(LOG_FOLDER, process_str_id)\n# Create the tboard_log folder if doesn\'t exist yet\nif not os.path.exists(LOG_FOLDER):\n    print(""Tensoboard folder:"", LOG_FOLDER)\n    os.makedirs(LOG_FOLDER)\n\n#print(end_points)\nvariables_to_restore = slim.get_variables_to_restore(exclude=[args.resnet_model + ""/logits"", ""optimizer_vars"",\n                                                              ""DeepLab_v3/ASPP_layer"", ""DeepLab_v3/logits""])\n\nmiou, update_op = tf.contrib.metrics.streaming_mean_iou(tf.argmax(valid_logits_batch_tf, axis=1),\n                                                        tf.argmax(valid_labels_batch_tf, axis=1),\n                                                        num_classes=args.number_of_classes)\ntf.summary.scalar(\'miou\', miou)\n\n# Add ops to restore all the variables.\nrestorer = tf.train.Saver(variables_to_restore)\n\nsaver = tf.train.Saver()\n\ncurrent_best_val_loss = np.inf\n\nwith tf.Session() as sess:\n    # Create the summary writer -- to write all the tboard_log\n    # into a specified file. This file can be later read by tensorboard.\n    train_writer = tf.summary.FileWriter(LOG_FOLDER + ""/train"", sess.graph)\n    test_writer = tf.summary.FileWriter(LOG_FOLDER + \'/val\')\n\n    # Create a saver.\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n\n    # load resnet checkpoints\n    try:\n        restorer.restore(sess, ""./resnet/checkpoints/"" + args.resnet_model + "".ckpt"")\n        print(""Model checkpoits for "" + args.resnet_model + "" restored!"")\n    except FileNotFoundError:\n        print(""ResNet checkpoints not found. Please download "" + args.resnet_model + "" model checkpoints from: https://github.com/tensorflow/models/tree/master/research/slim"")\n\n    # The `Iterator.string_handle()` method returns a tensor that can be evaluated\n    # and used to feed the `handle` placeholder.\n    training_handle = sess.run(training_iterator.string_handle())\n    validation_handle = sess.run(validation_iterator.string_handle())\n\n    sess.run(training_iterator.initializer)\n\n    validation_running_loss = []\n\n    train_steps_before_eval = 100\n    validation_steps = 20\n    while True:\n        training_average_loss = 0\n        for i in range(train_steps_before_eval): # run this number of batches before validation\n            _, global_step_np, train_loss, summary_string = sess.run([train_step,\n                                                                      global_step, cross_entropy_tf,\n                                                                      merged_summary_op],\n                                                                                feed_dict={is_training_tf:True,\n                                                                                           handle: training_handle})\n            training_average_loss += train_loss\n\n            if i % 10 == 0:\n                train_writer.add_summary(summary_string, global_step_np)\n\n        training_average_loss/=train_steps_before_eval\n\n        # at the end of each train interval, run validation\n        sess.run(validation_iterator.initializer)\n\n        validation_average_loss = 0\n        validation_average_miou = 0\n        for i in range(validation_steps):\n            val_loss, summary_string, _= sess.run([cross_entropy_tf, merged_summary_op, update_op],\n                                                  feed_dict={handle: validation_handle,\n                                                             is_training_tf:False})\n\n\n            validation_average_loss+=val_loss\n            validation_average_miou+=sess.run(miou)\n\n        validation_average_loss/=validation_steps\n        validation_average_miou/=validation_steps\n\n        # keep running average of the miou and validation loss\n        validation_running_loss.append(validation_average_loss)\n\n        validation_global_loss = np.mean(validation_running_loss)\n\n        if validation_global_loss < current_best_val_loss:\n            # Save the variables to disk.\n            save_path = saver.save(sess, LOG_FOLDER + ""/train"" + ""/model.ckpt"")\n            print(""Model checkpoints written! Best average val loss:"", validation_global_loss)\n            current_best_val_loss = validation_global_loss\n\n            # update metadata and save it\n            args.current_best_val_loss = str(current_best_val_loss)\n\n            with open(LOG_FOLDER + ""/train/"" + \'data.json\', \'w\') as fp:\n                json.dump(args.__dict__, fp, sort_keys=True, indent=4)\n\n        print(""Global step:"", global_step_np, ""Average train loss:"",\n              training_average_loss, ""\\tGlobal Validation Avg Loss:"", validation_global_loss,\n              ""MIoU:"", validation_average_miou)\n\n        test_writer.add_summary(summary_string, global_step_np)\n\n    train_writer.close()\n'"
preprocessing/inception_preprocessing.py,65,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None,\n                         add_image_summaries=True):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n    add_image_summaries: Enable image summaries.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    if add_image_summaries:\n      tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    if add_image_summaries:\n      tf.summary.image(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method),\n        num_cases=num_resize_cases)\n\n    if add_image_summaries:\n      tf.summary.image(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    if add_image_summaries:\n      tf.summary.image(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True,\n                     add_image_summaries=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image. If dtype is\n      tf.float32 then the range should be [0, 1], otherwise it would converted\n      to tf.float32 assuming that the range is [0, MAX], where MAX is largest\n      positive representable number for int(8/16/32) data type (see\n      `tf.image.convert_image_dtype` for details).\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n    add_image_summaries: Enable image summaries.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode,\n                                add_image_summaries=add_image_summaries)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
preprocessing/read_data.py,38,"b'import tensorflow as tf\nfrom preprocessing.inception_preprocessing import apply_with_random_selector, distort_color\nimport urllib\nimport tarfile\nimport os\n\ndef random_flip_image_and_annotation(image_tensor, annotation_tensor, image_shape):\n    """"""Accepts image tensor and annotation tensor and returns randomly flipped tensors of both.\n    The function performs random flip of image and annotation tensors with probability of 1/2\n    The flip is performed or not performed for image and annotation consistently, so that\n    annotation matches the image.\n\n    Parameters\n    ----------\n    image_tensor : Tensor of size (width, height, 3)\n        Tensor with image\n    annotation_tensor : Tensor of size (width, height, 1)\n        Tensor with annotation\n\n    Returns\n    -------\n    randomly_flipped_img : Tensor of size (width, height, 3) of type tf.float.\n        Randomly flipped image tensor\n    randomly_flipped_annotation : Tensor of size (width, height, 1)\n        Randomly flipped annotation tensor\n\n    """"""\n    original_shape = tf.shape(annotation_tensor)\n    # ensure the annotation tensor has shape (width, height, 1)\n    annotation_tensor = tf.cond(tf.rank(annotation_tensor) < 3,\n                                lambda: tf.expand_dims(annotation_tensor, axis=2),\n                                lambda: annotation_tensor)\n\n    # Random variable: two possible outcomes (0 or 1)\n    # with a 1 in 2 chance\n    random_var = tf.random_uniform(maxval=2, dtype=tf.int32, shape=[])\n\n\n    randomly_flipped_img = tf.cond(pred=tf.equal(random_var, 0),\n                                                 true_fn=lambda: tf.image.flip_left_right(image_tensor),\n                                                 false_fn=lambda: image_tensor)\n\n    randomly_flipped_annotation = tf.cond(pred=tf.equal(random_var, 0),\n                                                        true_fn=lambda: tf.image.flip_left_right(annotation_tensor),\n                                                        false_fn=lambda: annotation_tensor)\n\n    return randomly_flipped_img, tf.reshape(randomly_flipped_annotation, original_shape, name=""reshape_random_flip_image_and_annotation""), image_shape\n\n\ndef rescale_image_and_annotation_by_factor(image, annotation, image_shape, nin_scale=0.5, max_scale=2):\n    #We apply data augmentation by randomly scaling theinput images(from 0.5 to 2.0)\n    #and randomly left - right flipping during training.\n    input_shape = tf.shape(image)[0:2]\n    input_shape_float = tf.to_float(input_shape)\n\n    scale = tf.random_uniform(shape=[1],\n                                 minval=0.5,\n                                 maxval=2)\n\n    scaled_input_shape = tf.to_int32(tf.round(input_shape_float * scale))\n\n    image = tf.image.resize_images(image, scaled_input_shape,\n                                  method=tf.image.ResizeMethod.BILINEAR)\n\n    # use nearest neighbour for annotations resizing in order to keep proper values\n    annotation = tf.image.resize_images(annotation, scaled_input_shape,\n                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return image, annotation, image_shape\n\n\ndef download_resnet_checkpoint_if_necessary(resnet_checkpoints_path, resnet_model_name):\n    """"""\n    Check if the resnet checkpoints are already downloaded, if not download it\n    :param resnet_checkpoints_path: string: path where the properly resnet checkpoint files should be found\n    :param resnet_model_name: one of resnet_v2_50 or resnet_v2_101\n    :return: None\n    """"""\n    if not os.path.exists(resnet_checkpoints_path):\n        # create the path and download the resnet checkpoints\n        os.mkdir(resnet_checkpoints_path)\n\n        filename = resnet_model_name + ""_2017_04_14.tar.gz""\n\n        url = ""http://download.tensorflow.org/models/"" + filename\n        full_file_path = os.path.join(resnet_checkpoints_path, filename)\n        urllib.request.urlretrieve(url, full_file_path)\n        thetarfile = tarfile.open(full_file_path, ""r:gz"")\n        thetarfile.extractall(path=resnet_checkpoints_path)\n        thetarfile.close()\n        print(""Resnet:"", resnet_model_name, ""successfully downloaded."")\n    else:\n        print(""ResNet checkpoints file successfully found."")\n\n\ndef scale_image_with_crop_padding(image, annotation, image_shape, crop_size):\n\n    image_croped = tf.image.resize_image_with_crop_or_pad(image,crop_size,crop_size)\n\n    # Shift all the classes by one -- to be able to differentiate\n    # between zeros representing padded values and zeros representing\n    # a particular semantic class.\n    annotation_shifted_classes = annotation + 1\n\n    cropped_padded_annotation = tf.image.resize_image_with_crop_or_pad(annotation_shifted_classes,crop_size,crop_size)\n\n    mask_out_number=255\n    annotation_additional_mask_out = tf.to_int32(tf.equal(cropped_padded_annotation, 0)) * (mask_out_number+1)\n    cropped_padded_annotation = cropped_padded_annotation + annotation_additional_mask_out - 1\n\n    return image_croped, tf.squeeze(cropped_padded_annotation), image_shape\n\ndef tf_record_parser(record):\n    keys_to_features = {\n        ""image_raw"": tf.FixedLenFeature((), tf.string, default_value=""""),\n        \'annotation_raw\': tf.FixedLenFeature([], tf.string),\n        ""height"": tf.FixedLenFeature((), tf.int64),\n        ""width"": tf.FixedLenFeature((), tf.int64)\n    }\n\n    features = tf.parse_single_example(record, keys_to_features)\n\n    image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    annotation = tf.decode_raw(features[\'annotation_raw\'], tf.uint8)\n\n    height = tf.cast(features[\'height\'], tf.int32)\n    width = tf.cast(features[\'width\'], tf.int32)\n\n    # reshape input and annotation images\n    image = tf.reshape(image, (height, width, 3), name=""image_reshape"")\n    annotation = tf.reshape(annotation, (height,width,1), name=""annotation_reshape"")\n    annotation = tf.to_int32(annotation)\n\n    return tf.to_float(image), annotation, (height, width)\n\ndef distort_randomly_image_color(image_tensor, annotation_tensor, image_shape):\n    """"""Accepts image tensor of (width, height, 3) and returns color distorted image.\n    The function performs random brightness, saturation, hue, contrast change as it is performed\n    for inception model training in TF-Slim (you can find the link below in comments). All the\n    parameters of random variables were originally preserved. There are two regimes for the function\n    to work: fast and slow. Slow one performs only saturation and brightness random change is performed.\n\n    Parameters\n    ----------\n    image_tensor : Tensor of size (width, height, 3) of tf.int32 or tf.float\n        Tensor with image with range [0,255]\n    fast_mode : boolean\n        Boolean value representing whether to use fast or slow mode\n\n    Returns\n    -------\n    img_float_distorted_original_range : Tensor of size (width, height, 3) of type tf.float.\n        Image Tensor with distorted color in [0,255] intensity range\n    """"""\n    fast_mode=False\n    # Make the range to be in [0,1]\n    img_float_zero_one_range = tf.to_float(image_tensor) / 255\n\n    # Randomly distort the color of image. There are 4 ways to do it.\n    # Credit: TF-Slim\n    # https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py#L224\n    # Most probably the inception models were trainined using this color augmentation:\n    # https://github.com/tensorflow/models/tree/master/slim#pre-trained-models\n    distorted_image = apply_with_random_selector(img_float_zero_one_range,\n                                                 lambda x, ordering: distort_color(x, ordering, fast_mode=fast_mode),\n                                                 num_cases=4)\n\n    img_float_distorted_original_range = distorted_image * 255\n\n    return img_float_distorted_original_range, annotation_tensor, image_shape\n'"
preprocessing/training.py,14,"b'import tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nslim = tf.contrib.slim\n\ndef get_labels_from_annotation(annotation_tensor, class_labels):\n    """"""Returns tensor of size (width, height, num_classes) derived from annotation tensor.\n    The function returns tensor that is of a size (width, height, num_classes) which\n    is derived from annotation tensor with sizes (width, height) where value at\n    each position represents a class. The functions requires a list with class\n    values like [0, 1, 2 ,3] -- they are used to derive labels. Derived values will\n    be ordered in the same way as the class numbers were provided in the list. Last\n    value in the aforementioned list represents a value that indicate that the pixel\n    should be masked out. So, the size of num_classes := len(class_labels) - 1.\n\n    Parameters\n    ----------\n    annotation_tensor : Tensor of size (width, height)\n        Tensor with class labels for each element\n    class_labels : list of ints\n        List that contains the numbers that represent classes. Last\n        value in the list should represent the number that was used\n        for masking out.\n\n    Returns\n    -------\n    labels_2d_stacked : Tensor of size (width, height, num_classes).\n        Tensor with labels for each pixel.\n    """"""\n\n    # Last value in the classes list should show\n    # which number was used in the annotation to mask out\n    # the ambigious regions or regions that should not be\n    # used for training.\n    # TODO: probably replace class_labels list with some custom object\n    valid_entries_class_labels = class_labels[:-1]\n\n    # Stack the binary masks for each class\n    labels_2d = list(map(lambda x: tf.equal(annotation_tensor, x), valid_entries_class_labels))\n\n    # Perform the merging of all of the binary masks into one matrix\n    labels_2d_stacked = tf.stack(labels_2d, axis=2)\n\n    # Convert tf.bool to tf.float\n    # Later on in the labels and logits will be used\n    # in tf.softmax_cross_entropy_with_logits() function\n    # where they have to be of the float type.\n    labels_2d_stacked_float = tf.to_float(labels_2d_stacked)\n\n    return labels_2d_stacked_float\n\ndef get_labels_from_annotation_batch(annotation_batch_tensor, class_labels):\n    """"""Returns tensor of size (batch_size, width, height, num_classes) derived\n    from annotation batch tensor. The function returns tensor that is of a size\n    (batch_size, width, height, num_classes) which is derived from annotation tensor\n    with sizes (batch_size, width, height) where value at each position represents a class.\n    The functions requires a list with class values like [0, 1, 2 ,3] -- they are\n    used to derive labels. Derived values will be ordered in the same way as\n    the class numbers were provided in the list. Last value in the aforementioned\n    list represents a value that indicate that the pixel should be masked out.\n    So, the size of num_classes len(class_labels) - 1.\n\n    Parameters\n    ----------\n    annotation_batch_tensor : Tensor of size (batch_size, width, height)\n        Tensor with class labels for each element\n    class_labels : list of ints\n        List that contains the numbers that represent classes. Last\n        value in the list should represent the number that was used\n        for masking out.\n\n    Returns\n    -------\n    batch_labels : Tensor of size (batch_size, width, height, num_classes).\n        Tensor with labels for each batch.\n    """"""\n\n    batch_labels = tf.map_fn(fn=lambda x: get_labels_from_annotation(annotation_tensor=x, class_labels=class_labels),\n                             elems=annotation_batch_tensor,\n                             dtype=tf.float32)\n\n    return batch_labels\n\ndef get_valid_entries_indices_from_annotation_batch(annotation_batch_tensor, class_labels):\n    """"""Returns tensor of size (num_valid_eintries, 3).\n    Returns tensor that contains the indices of valid entries according\n    to the annotation tensor. This can be used to later on extract only\n    valid entries from logits tensor and labels tensor. This function is\n    supposed to work with a batch input like [b, w, h] -- where b is a\n    batch size, w, h -- are width and height sizes. So the output is\n    a tensor which contains indexes of valid entries. This function can\n    also work with a single annotation like [w, h] -- the output will\n    be (num_valid_eintries, 2).\n\n    Parameters\n    ----------\n    annotation_batch_tensor : Tensor of size (batch_size, width, height)\n        Tensor with class labels for each batch\n    class_labels : list of ints\n        List that contains the numbers that represent classes. Last\n        value in the list should represent the number that was used\n        for masking out.\n\n    Returns\n    -------\n    valid_labels_indices : Tensor of size (num_valid_eintries, 3).\n        Tensor with indices of valid entries\n    """"""\n\n    # Last value in the classes list should show\n    # which number was used in the annotation to mask out\n    # the ambigious regions or regions that should not be\n    # used for training.\n    # TODO: probably replace class_labels list with some custom object\n    mask_out_class_label = class_labels[-1]\n\n    # Get binary mask for the pixels that we want to\n    # use for training. We do this because some pixels\n    # are marked as ambigious and we don\'t want to use\n    # them for trainig to avoid confusing the model\n    valid_labels_mask = tf.not_equal(annotation_batch_tensor,\n                                     mask_out_class_label)\n\n    valid_labels_indices = tf.where(valid_labels_mask)\n\n    return tf.to_int32(valid_labels_indices)\n\n\ndef get_valid_logits_and_labels(annotation_batch_tensor,\n                                logits_batch_tensor,\n                                class_labels):\n    """"""Returns two tensors of size (num_valid_entries, num_classes).\n    The function converts annotation batch tensor input of the size\n    (batch_size, height, width) into label tensor (batch_size, height,\n    width, num_classes) and then selects only valid entries, resulting\n    in tensor of the size (num_valid_entries, num_classes). The function\n    also returns the tensor with corresponding valid entries in the logits\n    tensor. Overall, two tensors of the same sizes are returned and later on\n    can be used as an input into tf.softmax_cross_entropy_with_logits() to\n    get the cross entropy error for each entry.\n\n    Parameters\n    ----------\n    annotation_batch_tensor : Tensor of size (batch_size, width, height)\n        Tensor with class labels for each batch\n    logits_batch_tensor : Tensor of size (batch_size, width, height, num_classes)\n        Tensor with logits. Usually can be achived after inference of fcn network.\n    class_labels : list of ints\n        List that contains the numbers that represent classes. Last\n        value in the list should represent the number that was used\n        for masking out.\n\n    Returns\n    -------\n    (valid_labels_batch_tensor, valid_logits_batch_tensor) : Two Tensors of size (num_valid_eintries, num_classes).\n        Tensors that represent valid labels and logits.\n    """"""\n\n    labels_batch_tensor = get_labels_from_annotation_batch(annotation_batch_tensor=annotation_batch_tensor,\n                                                           class_labels=class_labels)\n\n    valid_batch_indices = get_valid_entries_indices_from_annotation_batch(\n        annotation_batch_tensor=annotation_batch_tensor,\n        class_labels=class_labels)\n\n    valid_labels_batch_tensor = tf.gather_nd(params=labels_batch_tensor, indices=valid_batch_indices)\n\n    valid_logits_batch_tensor = tf.gather_nd(params=logits_batch_tensor, indices=valid_batch_indices)\n\n    return valid_labels_batch_tensor, valid_logits_batch_tensor\n'"
resnet/resnet_utils.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, multi_grid, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            # Only uses atrous convolutions with multi-graid rates in the last (block4) block\n            if block.scope == ""block4"":\n              net = block.unit_fn(net, rate=rate * multi_grid[i], **dict(unit, stride=1))\n            else:\n              net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     is_training=True,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True,\n                     activation_fn=tf.nn.relu,\n                     use_batch_norm=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n    activation_fn: The activation function which is used in ResNet.\n    use_batch_norm: Whether or not to use batch normalization.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': None,\n      \'is_training\': is_training,\n      \'fused\': True,  # Use fused batch norm if possible.\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=activation_fn,\n      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
resnet/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom resnet import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n    """"""Bottleneck residual unit variant with BN before convolutions.\n\n    This is the full preactivation residual unit variant proposed in [2]. See\n    Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n    variant which has an extra bottleneck layer.\n\n    When putting together two consecutive ResNet blocks that use this unit, one\n    should use stride = 2 in the last unit of the first block.\n\n    Args:\n      inputs: A tensor of size [batch, height, width, channels].\n      depth: The depth of the ResNet unit output.\n      depth_bottleneck: The depth of the bottleneck layers.\n      stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n        the units output compared to its input.\n      rate: An integer, rate for atrous convolution.\n      outputs_collections: Collection to add the ResNet unit output.\n      scope: Optional variable_scope.\n\n    Returns:\n      The ResNet unit\'s output.\n    """"""\n    with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n        preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n        if depth == depth_in:\n            shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n        else:\n            shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                                   normalizer_fn=None, activation_fn=None,\n                                   scope=\'shortcut\')\n\n        residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                               scope=\'conv1\')\n        residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                            rate=rate, scope=\'conv2\')\n        residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                               normalizer_fn=None, activation_fn=None,\n                               scope=\'conv3\')\n\n        output = shortcut + residual\n\n        return slim.utils.collect_named_outputs(outputs_collections,\n                                                sc.name,\n                                                output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              multi_grid=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n    """"""Generator for v2 (preactivation) ResNet models.\n\n    This function generates a family of ResNet v2 models. See the resnet_v2_*()\n    methods for specific model instantiations, obtained by selecting different\n    block instantiations that produce ResNets of various depths.\n\n    Training for image classification on Imagenet is usually done with [224, 224]\n    inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n    block for the ResNets defined in [1] that have nominal stride equal to 32.\n    However, for dense prediction tasks we advise that one uses inputs with\n    spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n    this case the feature maps at the ResNet output will have spatial shape\n    [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n    and corners exactly aligned with the input image corners, which greatly\n    facilitates alignment of the features to the image. Using as input [225, 225]\n    images results in [8, 8] feature maps at the output of the last ResNet block.\n\n    For dense prediction tasks, the ResNet needs to run in fully-convolutional\n    (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n    have nominal stride equal to 32 and a good choice in FCN mode is to use\n    output_stride=16 in order to increase the density of the computed features at\n    small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n    Args:\n      inputs: A tensor of size [batch, height_in, width_in, channels].\n      blocks: A list of length equal to the number of ResNet blocks. Each element\n        is a resnet_utils.Block object describing the units in the block.\n      num_classes: Number of predicted classes for classification tasks.\n        If 0 or None, we return the features before the logit layer.\n      is_training: whether batch_norm layers are in training mode.\n      global_pool: If True, we perform global average pooling before computing the\n        logits. Set to True for image classification, False for dense prediction.\n      output_stride: If None, then the output will be computed at the nominal\n        network stride. If output_stride is not None, it specifies the requested\n        ratio of input to output spatial resolution.\n      include_root_block: If True, include the initial convolution followed by\n        max-pooling, if False excludes it. If excluded, `inputs` should be the\n        results of an activation-less convolution.\n      spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n          of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n          To use this parameter, the input images must be smaller than 300x300\n          pixels, in which case the output logit layer does not contain spatial\n          information and can be removed.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n\n\n    Returns:\n      net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n        If global_pool is False, then height_out and width_out are reduced by a\n        factor of output_stride compared to the respective height_in and width_in,\n        else both height_out and width_out equal one. If num_classes is 0 or None,\n        then net is the output of the last ResNet block, potentially after global\n        average pooling. If num_classes is a non-zero integer, net contains the\n        pre-softmax activations.\n      end_points: A dictionary from components of the network to the corresponding\n        activation.\n\n    Raises:\n      ValueError: If the target output_stride is not valid.\n    """"""\n    with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n        end_points_collection = sc.original_name_scope + \'_end_points\'\n        with slim.arg_scope([slim.conv2d, bottleneck,\n                             resnet_utils.stack_blocks_dense],\n                            outputs_collections=end_points_collection):\n\n            net = inputs\n            if include_root_block:\n                if output_stride is not None:\n                    if output_stride % 4 != 0:\n                        raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n                    output_stride /= 4\n                # We do not include batch normalization or activation functions in\n                # conv1 because the first ResNet unit will perform these. Cf.\n                # Appendix of [2].\n                with slim.arg_scope([slim.conv2d],\n                                    activation_fn=None, normalizer_fn=None):\n                    net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n            net = resnet_utils.stack_blocks_dense(net, blocks, multi_grid, output_stride)\n            # This is needed because the pre-activation variant does not have batch\n            # normalization or activation functions in the residual unit output. See\n            # Appendix of [2].\n            net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n            # Convert end_points_collection into a dictionary of end_points.\n            end_points = slim.utils.convert_collection_to_dict(\n                end_points_collection)\n\n            if global_pool:\n                # Global average pooling.\n                net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n                end_points[\'global_pool\'] = net\n            if num_classes is not None:\n                net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                                  normalizer_fn=None, scope=\'logits\')\n                end_points[sc.name + \'/logits\'] = net\n                if spatial_squeeze:\n                    net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n                    end_points[sc.name + \'/spatial_squeeze\'] = net\n                end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n            return net, end_points\n\n\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n    """"""Helper function for creating a resnet_v2 bottleneck block.\n\n    Args:\n      scope: The scope of the block.\n      base_depth: The depth of the bottleneck layer for each unit.\n      num_units: The number of units in the block.\n      stride: The stride of the block, implemented as a stride in the last unit.\n        All other units have stride=1.\n\n    Returns:\n      A resnet_v2 bottleneck block.\n    """"""\n    return resnet_utils.Block(scope, bottleneck, [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': 1\n    }] * (num_units - 1) + [{\n        \'depth\': base_depth * 4,\n        \'depth_bottleneck\': base_depth,\n        \'stride\': stride\n    }])\n\n\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 multi_grid=[1, 2, 4],\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n    """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                     global_pool=global_pool, output_stride=output_stride, multi_grid=multi_grid,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  multi_grid=[1, 2, 4],\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n    """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                     global_pool=global_pool, output_stride=output_stride, multi_grid=multi_grid,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  multi_grid=[1, 2, 4],\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n    """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                     global_pool=global_pool, output_stride=output_stride, multi_grid=multi_grid,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  multi_grid=[1, 2, 4],\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n    """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n    blocks = [\n        resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n        resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n        resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n        resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n    ]\n    return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                     global_pool=global_pool, output_stride=output_stride, multi_grid=multi_grid,\n                     include_root_block=True, spatial_squeeze=spatial_squeeze,\n                     reuse=reuse, scope=scope)\n\n\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
serving/deeplab_saved_model.py,27,"b'import sys\nsys.path.append(\'../\')\n\nimport tensorflow as tf\nimport network\n\nslim = tf.contrib.slim\nimport os\nimport json\n\n# uncomment and set the GPU id if applicable.\n# os.environ[""CUDA_VISIBLE_DEVICES""]=""3""\n\ntf.app.flags.DEFINE_integer(\'model_version\', 1, \'Models version number.\')\ntf.app.flags.DEFINE_string(\'work_dir\', \'./tboard_logs\', \'Working directory.\')\ntf.app.flags.DEFINE_integer(\'model_id\', 16645, \'Model id name to be loaded.\')\ntf.app.flags.DEFINE_string(\'export_model_dir\', ""./versions"", \'Directory where the model exported files should be placed.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n# best: 16645\nmodel_name = str(FLAGS.model_id)\nlog_folder = FLAGS.work_dir\npre_trained_model_dir = os.path.join(log_folder, model_name, ""train"")\n\nif not os.path.exists(os.path.join(log_folder, model_name, ""test"")):\n    os.makedirs(os.path.join(log_folder, model_name, ""test""))\n\nwith open(log_folder + \'/\' + model_name + \'/train/data.json\', \'r\') as fp:\n    args = json.load(fp)\n\nclass Dotdict(dict):\n    """"""dot.notation access to dictionary attributes""""""\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n\nargs = Dotdict(args)\n\ndef main(_):\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        # define placeholders for receiving the input image height and width\n        image_height_tensor = tf.placeholder(tf.int32)\n        image_width_tensor = tf.placeholder(tf.int32)\n\n        # placeholder for receiving the serialized input image\n        serialized_tf_example = tf.placeholder(tf.string, name=\'tf_example\')\n        feature_configs = {\'x\': tf.FixedLenFeature(shape=[], dtype=tf.float32), }\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n\n        # reshape the input image to its original dimension\n        tf_example[\'x\'] = tf.reshape(tf_example[\'x\'], (1, image_height_tensor, image_width_tensor, 3))\n        input_tensor = tf.identity(tf_example[\'x\'], name=\'x\')  # use tf.identity() to assign name\n\n        # perform inference on the input image\n        logits_tf = network.deeplab_v3(input_tensor, args, is_training=False, reuse=False)\n\n        # extract the segmentation mask\n        predictions_tf = tf.argmax(logits_tf, axis=3)\n\n        # specify the directory where the pre-trained model weights are stored\n        pre_trained_model_dir = os.path.join(log_folder, model_name, ""train"")\n\n        saver = tf.train.Saver()\n\n        # Restore variables from disk.\n        saver.restore(sess, os.path.join(pre_trained_model_dir, ""model.ckpt""))\n        print(""Model"", model_name, ""restored."")\n\n        # Create SavedModelBuilder class\n        # defines where the model will be exported\n        export_path_base = FLAGS.export_model_dir\n        export_path = os.path.join(\n            tf.compat.as_bytes(export_path_base),\n            tf.compat.as_bytes(str(FLAGS.model_version)))\n        print(\'Exporting trained model to\', export_path)\n        builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\n        # Creates the TensorInfo protobuf objects that encapsulates the input/output tensors\n        tensor_info_input = tf.saved_model.utils.build_tensor_info(input_tensor)\n        tensor_info_height = tf.saved_model.utils.build_tensor_info(image_height_tensor)\n        tensor_info_width = tf.saved_model.utils.build_tensor_info(image_width_tensor)\n\n        # output tensor info\n        tensor_info_output = tf.saved_model.utils.build_tensor_info(predictions_tf)\n\n        # Defines the DeepLab signatures, uses the TF Predict API\n        # It receives an image and its dimensions and output the segmentation mask\n        prediction_signature = (\n            tf.saved_model.signature_def_utils.build_signature_def(\n                inputs={\'images\': tensor_info_input, \'height\': tensor_info_height, \'width\': tensor_info_width},\n                outputs={\'segmentation_map\': tensor_info_output},\n                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n\n        builder.add_meta_graph_and_variables(\n            sess, [tf.saved_model.tag_constants.SERVING],\n            signature_def_map={\n                \'predict_images\':\n                    prediction_signature,\n            })\n\n        # export the model\n        builder.save(as_text=True)\n        print(\'Done exporting!\')\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
