file_path,api_count,code
main.py,7,"b'import numpy as np\nimport tensorflow as tf\nfrom flask import Flask, jsonify, render_template, request\n\nfrom mnist import model\n\n\nx = tf.placeholder(""float"", [None, 784])\nsess = tf.Session()\n\n# restore trained data\nwith tf.variable_scope(""regression""):\n    y1, variables = model.regression(x)\nsaver = tf.train.Saver(variables)\nsaver.restore(sess, ""mnist/data/regression.ckpt"")\n\n\nwith tf.variable_scope(""convolutional""):\n    keep_prob = tf.placeholder(""float"")\n    y2, variables = model.convolutional(x, keep_prob)\nsaver = tf.train.Saver(variables)\nsaver.restore(sess, ""mnist/data/convolutional.ckpt"")\n\n\ndef regression(input):\n    return sess.run(y1, feed_dict={x: input}).flatten().tolist()\n\n\ndef convolutional(input):\n    return sess.run(y2, feed_dict={x: input, keep_prob: 1.0}).flatten().tolist()\n\n\n# webapp\napp = Flask(__name__)\n\n\n@app.route(\'/api/mnist\', methods=[\'POST\'])\ndef mnist():\n    input = ((255 - np.array(request.json, dtype=np.uint8)) / 255.0).reshape(1, 784)\n    output1 = regression(input)\n    output2 = convolutional(input)\n    return jsonify(results=[output1, output2])\n\n\n@app.route(\'/\')\ndef main():\n    return render_template(\'index.html\')\n\n\nif __name__ == \'__main__\':\n    app.run()\n'"
mnist/__init__.py,0,b''
mnist/convolutional.py,11,"b'import os\nimport model\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# model\nwith tf.variable_scope(""convolutional""):\n    x = tf.placeholder(tf.float32, [None, 784])\n    keep_prob = tf.placeholder(tf.float32)\n    y, variables = model.convolutional(x, keep_prob)\n\n# train\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(variables)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(20000):\n        batch = data.train.next_batch(50)\n        if i % 100 == 0:\n            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n            print(""step %d, training accuracy %g"" % (i, train_accuracy))\n        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n    print(sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels, keep_prob: 1.0}))\n\n    path = saver.save(\n        sess, os.path.join(os.path.dirname(__file__), \'data\', \'convolutional.ckpt\'),\n        write_meta_graph=False, write_state=False)\n    print(""Saved:"", path)\n'"
mnist/model.py,16,"b'import tensorflow as tf\n\n\n# Softmax Regression Model\ndef regression(x):\n    W = tf.Variable(tf.zeros([784, 10]), name=""W"")\n    b = tf.Variable(tf.zeros([10]), name=""b"")\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n    return y, [W, b]\n\n\n# Multilayer Convolutional Network\ndef convolutional(x, keep_prob):\n    def conv2d(x, W):\n        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n    def max_pool_2x2(x):\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\n    def weight_variable(shape):\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    def bias_variable(shape):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    # First Convolutional Layer\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\n    W_conv1 = weight_variable([5, 5, 1, 32])\n    b_conv1 = bias_variable([32])\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n    # Second Convolutional Layer\n    W_conv2 = weight_variable([5, 5, 32, 64])\n    b_conv2 = bias_variable([64])\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n    h_pool2 = max_pool_2x2(h_conv2)\n    # Densely Connected Layer\n    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n    b_fc1 = bias_variable([1024])\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n    # Dropout\n    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n    # Readout Layer\n    W_fc2 = weight_variable([1024, 10])\n    b_fc2 = bias_variable([10])\n    y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n    return y, [W_conv1, b_conv1, W_conv2, b_conv2, W_fc1, b_fc1, W_fc2, b_fc2]\n'"
mnist/regression.py,10,"b'import os\nimport model\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets(""/tmp/data/"", one_hot=True)\n\n# model\nwith tf.variable_scope(""regression""):\n    x = tf.placeholder(tf.float32, [None, 784])\n    y, variables = model.regression(x)\n\n# train\ny_ = tf.placeholder(""float"", [None, 10])\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y))\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(variables)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for _ in range(1000):\n        batch_xs, batch_ys = data.train.next_batch(100)\n        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n    print(sess.run(accuracy, feed_dict={x: data.test.images, y_: data.test.labels}))\n\n    path = saver.save(\n        sess, os.path.join(os.path.dirname(__file__), \'data\', \'regression.ckpt\'),\n        write_meta_graph=False, write_state=False)\n    print(""Saved:"", path)\n'"
