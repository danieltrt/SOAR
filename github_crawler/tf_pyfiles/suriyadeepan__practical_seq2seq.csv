file_path,api_count,code
03-Twitter-chatbot.py,0,"b""\n# In[1]:\n\nimport tensorflow as tf\nimport numpy as np\n\n# preprocessed data\nfrom datasets.twitter import data\nimport data_utils\n\n# load data from pickle and npy files\nmetadata, idx_q, idx_a = data.load_data(PATH='datasets/twitter/')\n(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)\n\n# parameters \nxseq_len = trainX.shape[-1]\nyseq_len = trainY.shape[-1]\nbatch_size = 32\nxvocab_size = len(metadata['idx2w'])  \nyvocab_size = xvocab_size\nemb_dim = 1024\n\nimport seq2seq_wrapper\n\n# In[7]:\n\nmodel = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n                               yseq_len=yseq_len,\n                               xvocab_size=xvocab_size,\n                               yvocab_size=yvocab_size,\n                               ckpt_path='ckpt/twitter/',\n                               emb_dim=emb_dim,\n                               num_layers=3\n                               )\n\n\n# In[8]:\n\nval_batch_gen = data_utils.rand_batch_gen(validX, validY, 32)\ntrain_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n\n\n# In[9]:\nsess = model.restore_last_session()\nsess = model.train(train_batch_gen, val_batch_gen)\n"""
04-Cornell-Movie-Dialog-Bot.py,0,"b""import tensorflow as tf\nimport numpy as np\n\n# preprocessed data\nfrom datasets.cornell_corpus import data\nimport data_utils\n\n# load data from pickle and npy files\nmetadata, idx_q, idx_a = data.load_data(PATH='datasets/cornell_corpus/')\n(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)\n\n# parameters \nxseq_len = trainX.shape[-1]\nyseq_len = trainY.shape[-1]\nbatch_size = 32\nxvocab_size = len(metadata['idx2w'])  \nyvocab_size = xvocab_size\nemb_dim = 1024\n\nimport seq2seq_wrapper\n\n# In[7]:\n\nmodel = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n                               yseq_len=yseq_len,\n                               xvocab_size=xvocab_size,\n                               yvocab_size=yvocab_size,\n                               ckpt_path='ckpt/cornell_corpus/',\n                               emb_dim=emb_dim,\n                               num_layers=3\n                               )\n\n\n# In[8]:\n\nval_batch_gen = data_utils.rand_batch_gen(validX, validY, 32)\ntrain_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n\n\n# In[9]:\n#sess = model.restore_last_session()\nsess = model.train(train_batch_gen, val_batch_gen)\n"""
data_utils.py,0,"b""import numpy as np\nfrom random import sample\n\n'''\n split data into train (70%), test (15%) and valid(15%)\n    return tuple( (trainX, trainY), (testX,testY), (validX,validY) )\n\n'''\ndef split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n    # number of examples\n    data_len = len(x)\n    lens = [ int(data_len*item) for item in ratio ]\n\n    trainX, trainY = x[:lens[0]], y[:lens[0]]\n    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n\n    return (trainX,trainY), (testX,testY), (validX,validY)\n\n\n'''\n generate batches from dataset\n    yield (x_gen, y_gen)\n\n    TODO : fix needed\n\n'''\ndef batch_gen(x, y, batch_size):\n    # infinite while\n    while True:\n        for i in range(0, len(x), batch_size):\n            if (i+1)*batch_size < len(x):\n                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n\n'''\n generate batches, by random sampling a bunch of items\n    yield (x_gen, y_gen)\n\n'''\ndef rand_batch_gen(x, y, batch_size):\n    while True:\n        sample_idx = sample(list(np.arange(len(x))), batch_size)\n        yield x[sample_idx].T, y[sample_idx].T\n\n#'''\n# convert indices of alphabets into a string (word)\n#    return str(word)\n#\n#'''\n#def decode_word(alpha_seq, idx2alpha):\n#    return ''.join([ idx2alpha[alpha] for alpha in alpha_seq if alpha ])\n#\n#\n#'''\n# convert indices of phonemes into list of phonemes (as string)\n#    return str(phoneme_list)\n#\n#'''\n#def decode_phonemes(pho_seq, idx2pho):\n#    return ' '.join( [ idx2pho[pho] for pho in pho_seq if pho ])\n\n\n'''\n a generic decode function \n    inputs : sequence, lookup\n\n'''\ndef decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n    return separator.join([ lookup[element] for element in sequence if element ])\n"""
seq2seq_wrapper.py,22,"b""import tensorflow as tf\nimport numpy as np\nimport sys\n\n\nclass Seq2Seq(object):\n\n    def __init__(self, xseq_len, yseq_len, \n            xvocab_size, yvocab_size,\n            emb_dim, num_layers, ckpt_path,\n            lr=0.0001, \n            epochs=100000, model_name='seq2seq_model'):\n\n        # attach these arguments to self\n        self.xseq_len = xseq_len\n        self.yseq_len = yseq_len\n        self.ckpt_path = ckpt_path\n        self.epochs = epochs\n        self.model_name = model_name\n\n\n        # build thy graph\n        #  attach any part of the graph that needs to be exposed, to the self\n        def __graph__():\n\n            # placeholders\n            tf.reset_default_graph()\n            #  encoder inputs : list of indices of length xseq_len\n            self.enc_ip = [ tf.placeholder(shape=[None,], \n                            dtype=tf.int64, \n                            name='ei_{}'.format(t)) for t in range(xseq_len) ]\n\n            #  labels that represent the real outputs\n            self.labels = [ tf.placeholder(shape=[None,], \n                            dtype=tf.int64, \n                            name='ei_{}'.format(t)) for t in range(yseq_len) ]\n\n            #  decoder inputs : 'GO' + [ y1, y2, ... y_t-1 ]\n            self.dec_ip = [ tf.zeros_like(self.enc_ip[0], dtype=tf.int64, name='GO') ] + self.labels[:-1]\n\n\n            # Basic LSTM cell wrapped in Dropout Wrapper\n            self.keep_prob = tf.placeholder(tf.float32)\n            # define the basic cell\n            basic_cell = tf.contrib.rnn.core_rnn_cell.DropoutWrapper(\n                    tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(emb_dim, state_is_tuple=True),\n                    output_keep_prob=self.keep_prob)\n            # stack cells together : n layered model\n            stacked_lstm = tf.contrib.rnn.core_rnn_cell.MultiRNNCell([basic_cell]*num_layers, state_is_tuple=True)\n\n\n            # for parameter sharing between training model\n            #  and testing model\n            with tf.variable_scope('decoder') as scope:\n                # build the seq2seq model \n                #  inputs : encoder, decoder inputs, LSTM cell type, vocabulary sizes, embedding dimensions\n                self.decode_outputs, self.decode_states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(self.enc_ip,self.dec_ip, stacked_lstm,\n                                                    xvocab_size, yvocab_size, emb_dim)\n                # share parameters\n                scope.reuse_variables()\n                # testing model, where output of previous timestep is fed as input \n                #  to the next timestep\n                self.decode_outputs_test, self.decode_states_test = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n                    self.enc_ip, self.dec_ip, stacked_lstm, xvocab_size, yvocab_size,emb_dim,\n                    feed_previous=True)\n\n            # now, for training,\n            #  build loss function\n\n            # weighted loss\n            #  TODO : add parameter hint\n            loss_weights = [ tf.ones_like(label, dtype=tf.float32) for label in self.labels ]\n            self.loss = tf.contrib.legacy_seq2seq.sequence_loss(self.decode_outputs, self.labels, loss_weights, yvocab_size)\n            # train op to minimize the loss\n            self.train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)\n\n        sys.stdout.write('<log> Building Graph ')\n        # build comput graph\n        __graph__()\n        sys.stdout.write('</log>')\n\n\n\n    '''\n        Training and Evaluation\n\n    '''\n\n    # get the feed dictionary\n    def get_feed(self, X, Y, keep_prob):\n        feed_dict = {self.enc_ip[t]: X[t] for t in range(self.xseq_len)}\n        feed_dict.update({self.labels[t]: Y[t] for t in range(self.yseq_len)})\n        feed_dict[self.keep_prob] = keep_prob # dropout prob\n        return feed_dict\n\n    # run one batch for training\n    def train_batch(self, sess, train_batch_gen):\n        # get batches\n        batchX, batchY = train_batch_gen.__next__()\n        # build feed\n        feed_dict = self.get_feed(batchX, batchY, keep_prob=0.5)\n        _, loss_v = sess.run([self.train_op, self.loss], feed_dict)\n        return loss_v\n\n    def eval_step(self, sess, eval_batch_gen):\n        # get batches\n        batchX, batchY = eval_batch_gen.__next__()\n        # build feed\n        feed_dict = self.get_feed(batchX, batchY, keep_prob=1.)\n        loss_v, dec_op_v = sess.run([self.loss, self.decode_outputs_test], feed_dict)\n        # dec_op_v is a list; also need to transpose 0,1 indices \n        #  (interchange batch_size and timesteps dimensions\n        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n        return loss_v, dec_op_v, batchX, batchY\n\n    # evaluate 'num_batches' batches\n    def eval_batches(self, sess, eval_batch_gen, num_batches):\n        losses = []\n        for i in range(num_batches):\n            loss_v, dec_op_v, batchX, batchY = self.eval_step(sess, eval_batch_gen)\n            losses.append(loss_v)\n        return np.mean(losses)\n\n    # finally the train function that\n    #  runs the train_op in a session\n    #   evaluates on valid set periodically\n    #    prints statistics\n    def train(self, train_set, valid_set, sess=None ):\n        \n        # we need to save the model periodically\n        saver = tf.train.Saver()\n\n        # if no session is given\n        if not sess:\n            # create a session\n            sess = tf.Session()\n            # init all variables\n            sess.run(tf.global_variables_initializer())\n\n        sys.stdout.write('\\n<log> Training started </log>\\n')\n        # run M epochs\n        for i in range(self.epochs):\n            try:\n                self.train_batch(sess, train_set)\n                if i and i% (self.epochs//100) == 0: # TODO : make this tunable by the user\n                    # save model to disk\n                    saver.save(sess, self.ckpt_path + self.model_name + '.ckpt', global_step=i)\n                    # evaluate to get validation loss\n                    val_loss = self.eval_batches(sess, valid_set, 16) # TODO : and this\n                    # print stats\n                    print('\\nModel saved to disk at iteration #{}'.format(i))\n                    print('val   loss : {0:.6f}'.format(val_loss))\n                    sys.stdout.flush()\n            except KeyboardInterrupt: # this will most definitely happen, so handle it\n                print('Interrupted by user at iteration {}'.format(i))\n                self.session = sess\n                return sess\n\n    def restore_last_session(self):\n        saver = tf.train.Saver()\n        # create a session\n        sess = tf.Session()\n        # get checkpoint state\n        ckpt = tf.train.get_checkpoint_state(self.ckpt_path)\n        # restore session\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        # return to user\n        return sess\n\n    # prediction\n    def predict(self, sess, X):\n        feed_dict = {self.enc_ip[t]: X[t] for t in range(self.xseq_len)}\n        feed_dict[self.keep_prob] = 1.\n        dec_op_v = sess.run(self.decode_outputs_test, feed_dict)\n        # dec_op_v is a list; also need to transpose 0,1 indices \n        #  (interchange batch_size and timesteps dimensions\n        dec_op_v = np.array(dec_op_v).transpose([1,0,2])\n        # return the index of item with highest probability\n        return np.argmax(dec_op_v, axis=2)\n\n\n"""
datasets/cmudict/data.py,0,"b""START_LINE = 126\nEND_LINE = 133905\nFILENAME = 'data/cmudict-0.7b'\nALPHA = '_abcdefghijklmnopqrstuvwxyz'\n\nlimit = {\n        'maxw'  : 16,\n        'minw'  : 5,\n        'maxph' : 16,\n        'minph' : 5\n        }\n\nimport random\nimport numpy as np\nimport pickle\n\n\n'''\n read lines from file\n     return [list of lines]\n\n'''\ndef read_line(filename=FILENAME):\n    return open(filename, 'r', encoding='utf-8', errors='ignore').read().split('\\n')[START_LINE:END_LINE]\n\n\n'''\n separate lines into lines of words and pronunciation\n    return tuple( [words], [phoneme lists] )\n\n'''\ndef split_data(lines):\n\n    words, phoneme_lists = [], []\n    for line in lines:\n        word, phonemes = line.split('  ')\n        phoneme_lists.append(phonemes.split(' '))\n        words.append(word.lower())\n    return words, phoneme_lists\n\n\n'''\n read the phoneme_lists, create index to phoneme, \n  phoneme to index dictionaries\n    return tuple( idx2pho, pho2idx )\n\n'''\ndef index_phonemes(phoneme_lists):\n    phoneme_vocab = set([phoneme for phonemes in phoneme_lists \n        for phoneme in phonemes])\n    idx2pho = dict(enumerate(['_'] + sorted(list(phoneme_vocab))))\n    # we add an extra dummy element to make sure \n    #  we dont touch the zero index (zero padding)\n    pho2idx = dict(zip(idx2pho.values(), idx2pho.keys()))\n    return idx2pho, pho2idx\n\n\n'''\n generate index english alphabets\n    return tuple( idx2alpha, alpha2idx )\n\n'''\ndef index_alphabets(alpha = ALPHA):\n    idx2alpha = dict(enumerate(alpha))\n    alpha2idx = dict(zip(idx2alpha.values(), idx2alpha.keys()))\n    return idx2alpha, alpha2idx\n\n\n'''\n filter too long and too short sequences\n    return tuple( filtered_words, filtered_phoneme_lists )\n\n'''\ndef filter_data(words, phoneme_lists):\n    # need a threshold\n    #  say max : 16, 16\n    #      min : 5, 5\n    '''\n        limit = {\n                'maxw'  : 16,\n                'minw'  : 5,\n                'maxph' : 16,\n                'minph' : 5\n                }\n    '''\n\n    # also make sure, every character in words is an alphabet\n    #  if not, skip the row\n    filtered_words, filtered_phoneme_lists = [], []\n    raw_data_len = len(words)\n    for word, phonemes in zip(words, phoneme_lists):\n        if word.isalpha():\n            if len(word) < limit['maxw'] and len(word) > limit['minw']:\n                if len(phonemes) < limit['maxph'] and len(phonemes) > limit['minph']:\n                    filtered_words.append(word)\n                    filtered_phoneme_lists.append(phonemes)\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_words)\n    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n    print(str(filtered) + '% filtered from original data')\n\n    return filtered_words, filtered_phoneme_lists\n\n\n'''\n create the final dataset : \n  - convert list of items to arrays of indices\n  - add zero padding\n      return ( [array_words([indices]), array_phonemes([indices]) )\n \n'''\ndef zero_pad(words, phoneme_lists, alpha2idx, pho2idx):\n    # num of rows\n    data_len = len(words)\n    idx_words = np.zeros([data_len, limit['maxw']], dtype=np.int32)\n    idx_phonemes = np.zeros([data_len, limit['maxph']], dtype=np.int32)\n\n    for i in range(data_len):\n        #print(words[i], phoneme_lists[i], i)\n        word_indices = [ alpha2idx[alpha] for alpha in words[i] ] \\\n                            + [0]*(limit['maxw'] - len(words[i]))\n        pho_indices  = [ pho2idx[phoneme] for phoneme in phoneme_lists[i] ] \\\n                            + [0]*(limit['maxph'] - len(phoneme_lists[i]))\n\n        idx_words[i] = np.array(word_indices)\n        idx_phonemes[i] = np.array(pho_indices)\n\n    return idx_words, idx_phonemes\n\n\ndef process_data():\n\n    print('>> Read from file')\n    lines = read_line()\n    print('\\n:: random.choice(lines)')\n    print(random.choice(lines))\n    print(random.choice(lines))\n\n    print('\\n>> Separate data')\n    # shuffle data\n    random.shuffle(lines)\n    # separate into words and phoneme lists\n    words, phoneme_lists = split_data(lines)\n    print('\\n:: random.choice(words)')\n    print(random.choice(words))\n    print(random.choice(words))\n    print('\\n:: random.choice(phoneme_lists)')\n    print(random.choice(phoneme_lists))\n    print(random.choice(phoneme_lists))\n\n    print('\\n>> Index phonemes')\n    idx2pho, pho2idx = index_phonemes(phoneme_lists)\n    print('\\n:: random.choice(idx2pho)')\n    print(idx2pho[random.choice(list(idx2pho.keys()))])\n    print(idx2pho[random.choice(list(idx2pho.keys()))])\n    print('\\n:: random.choice(pho2idx)')\n    print(pho2idx[random.choice(list(pho2idx.keys()))])\n    print(pho2idx[random.choice(list(pho2idx.keys()))])\n\n    # index alphabets\n    idx2alpha, alpha2idx = index_alphabets()\n\n    print('\\n>> Filter data')\n    words, phoneme_lists = filter_data(words, phoneme_lists)\n\n    # we know the maximum length of both sequences : 15\n    #  we should create zero-padded numpy arrays\n    idx_words, idx_phonemes = zero_pad(words, phoneme_lists, alpha2idx, pho2idx)\n    # save them\n    np.save('idx_words.npy', idx_words)\n    np.save('idx_phonemes.npy', idx_phonemes)\n\n    # let us now save the necessary dictionaries\n    data_ctl = {\n            'idx2alpha' : idx2alpha,\n            'idx2pho' : idx2pho,\n            'pho2idx' : pho2idx,\n            'alpha2idx' : alpha2idx,\n            'limit' : limit\n                }\n    # write to disk : data control dictionaries\n    with open('data_ctl.pkl', 'wb') as f:\n        pickle.dump(data_ctl, f)\n\n\ndef load_data(PATH=''):\n    # read data control dictionaries\n    with open(PATH + 'data_ctl.pkl', 'rb') as f:\n        data_ctl = pickle.load(f)\n    # read numpy arrays\n    idx_words = np.load(PATH + 'idx_words.npy')\n    idx_phonemes = np.load(PATH + 'idx_phonemes.npy')\n    return data_ctl, idx_words, idx_phonemes\n\n\n\nif __name__ == '__main__':\n    process_data()\n"""
datasets/cornell_corpus/data.py,0,"b'EN_WHITELIST = \'0123456789abcdefghijklmnopqrstuvwxyz \' # space is included in whitelist\nEN_BLACKLIST = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\\'\'\n\nlimit = {\n        \'maxq\' : 25,\n        \'minq\' : 2,\n        \'maxa\' : 25,\n        \'mina\' : 2\n        }\n\nUNK = \'unk\'\nVOCAB_SIZE = 8000\n\n\nimport random\n\nimport nltk\nimport itertools\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport pickle\n\n\n\n\'\'\' \n    1. Read from \'movie-lines.txt\'\n    2. Create a dictionary with ( key = line_id, value = text )\n\'\'\'\ndef get_id2line():\n    lines=open(\'raw_data/movie_lines.txt\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')\n    id2line = {}\n    for line in lines:\n        _line = line.split(\' +++$+++ \')\n        if len(_line) == 5:\n            id2line[_line[0]] = _line[4]\n    return id2line\n\n\'\'\'\n    1. Read from \'movie_conversations.txt\'\n    2. Create a list of [list of line_id\'s]\n\'\'\'\ndef get_conversations():\n    conv_lines = open(\'raw_data/movie_conversations.txt\', encoding=\'utf-8\', errors=\'ignore\').read().split(\'\\n\')\n    convs = [ ]\n    for line in conv_lines[:-1]:\n        _line = line.split(\' +++$+++ \')[-1][1:-1].replace(""\'"","""").replace("" "","""")\n        convs.append(_line.split(\',\'))\n    return convs\n\n\'\'\'\n    1. Get each conversation\n    2. Get each line from conversation\n    3. Save each conversation to file\n\'\'\'\ndef extract_conversations(convs,id2line,path=\'\'):\n    idx = 0\n    for conv in convs:\n        f_conv = open(path + str(idx)+\'.txt\', \'w\')\n        for line_id in conv:\n            f_conv.write(id2line[line_id])\n            f_conv.write(\'\\n\')\n        f_conv.close()\n        idx += 1\n\n\'\'\'\n    Get lists of all conversations as Questions and Answers\n    1. [questions]\n    2. [answers]\n\'\'\'\ndef gather_dataset(convs, id2line):\n    questions = []; answers = []\n\n    for conv in convs:\n        if len(conv) %2 != 0:\n            conv = conv[:-1]\n        for i in range(len(conv)):\n            if i%2 == 0:\n                questions.append(id2line[conv[i]])\n            else:\n                answers.append(id2line[conv[i]])\n\n    return questions, answers\n\n\n\'\'\'\n    We need 4 files\n    1. train.enc : Encoder input for training\n    2. train.dec : Decoder input for training\n    3. test.enc  : Encoder input for testing\n    4. test.dec  : Decoder input for testing\n\'\'\'\ndef prepare_seq2seq_files(questions, answers, path=\'\',TESTSET_SIZE = 30000):\n    \n    # open files\n    train_enc = open(path + \'train.enc\',\'w\')\n    train_dec = open(path + \'train.dec\',\'w\')\n    test_enc  = open(path + \'test.enc\', \'w\')\n    test_dec  = open(path + \'test.dec\', \'w\')\n\n    # choose 30,000 (TESTSET_SIZE) items to put into testset\n    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n\n    for i in range(len(questions)):\n        if i in test_ids:\n            test_enc.write(questions[i]+\'\\n\')\n            test_dec.write(answers[i]+ \'\\n\' )\n        else:\n            train_enc.write(questions[i]+\'\\n\')\n            train_dec.write(answers[i]+ \'\\n\' )\n        if i%10000 == 0:\n            print(\'\\n>> written {} lines\'.format(i))\n\n    # close files\n    train_enc.close()\n    train_dec.close()\n    test_enc.close()\n    test_dec.close()\n            \n\n\n\'\'\'\n remove anything that isn\'t in the vocabulary\n    return str(pure en)\n\n\'\'\'\ndef filter_line(line, whitelist):\n    return \'\'.join([ ch for ch in line if ch in whitelist ])\n\n\n\n\'\'\'\n filter too long and too short sequences\n    return tuple( filtered_ta, filtered_en )\n\n\'\'\'\ndef filter_data(qseq, aseq):\n    filtered_q, filtered_a = [], []\n    raw_data_len = len(qseq)\n\n    assert len(qseq) == len(aseq)\n\n    for i in range(raw_data_len):\n        qlen, alen = len(qseq[i].split(\' \')), len(aseq[i].split(\' \'))\n        if qlen >= limit[\'minq\'] and qlen <= limit[\'maxq\']:\n            if alen >= limit[\'mina\'] and alen <= limit[\'maxa\']:\n                filtered_q.append(qseq[i])\n                filtered_a.append(aseq[i])\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\'\'\'\n read list of words, create index to word,\n  word to index dictionaries\n    return tuple( vocab->(word, count), idx2w, w2idx )\n\n\'\'\'\ndef index_(tokenized_sentences, vocab_size):\n    # get frequency distribution\n    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n    # get vocabulary of \'vocab_size\' most used words\n    vocab = freq_dist.most_common(vocab_size)\n    # index2word\n    index2word = [\'_\'] + [UNK] + [ x[0] for x in vocab ]\n    # word2index\n    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n    return index2word, word2index, freq_dist\n\n\'\'\'\n filter based on number of unknowns (words not in vocabulary)\n  filter out the worst sentences\n\n\'\'\'\ndef filter_unk(qtokenized, atokenized, w2idx):\n    data_len = len(qtokenized)\n\n    filtered_q, filtered_a = [], []\n\n    for qline, aline in zip(qtokenized, atokenized):\n        unk_count_q = len([ w for w in qline if w not in w2idx ])\n        unk_count_a = len([ w for w in aline if w not in w2idx ])\n        if unk_count_a <= 2:\n            if unk_count_q > 0:\n                if unk_count_q/len(qline) > 0.2:\n                    pass\n            filtered_q.append(qline)\n            filtered_a.append(aline)\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((data_len - filt_data_len)*100/data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\n\n\'\'\'\n create the final dataset : \n  - convert list of items to arrays of indices\n  - add zero padding\n      return ( [array_en([indices]), array_ta([indices]) )\n \n\'\'\'\ndef zero_pad(qtokenized, atokenized, w2idx):\n    # num of rows\n    data_len = len(qtokenized)\n\n    # numpy arrays to store indices\n    idx_q = np.zeros([data_len, limit[\'maxq\']], dtype=np.int32) \n    idx_a = np.zeros([data_len, limit[\'maxa\']], dtype=np.int32)\n\n    for i in range(data_len):\n        q_indices = pad_seq(qtokenized[i], w2idx, limit[\'maxq\'])\n        a_indices = pad_seq(atokenized[i], w2idx, limit[\'maxa\'])\n\n        #print(len(idx_q[i]), len(q_indices))\n        #print(len(idx_a[i]), len(a_indices))\n        idx_q[i] = np.array(q_indices)\n        idx_a[i] = np.array(a_indices)\n\n    return idx_q, idx_a\n\n\n\'\'\'\n replace words with indices in a sequence\n  replace with unknown if word not in lookup\n    return [list of indices]\n\n\'\'\'\ndef pad_seq(seq, lookup, maxlen):\n    indices = []\n    for word in seq:\n        if word in lookup:\n            indices.append(lookup[word])\n        else:\n            indices.append(lookup[UNK])\n    return indices + [0]*(maxlen - len(seq))\n\n\n\n\n\ndef process_data():\n\n    id2line = get_id2line()\n    print(\'>> gathered id2line dictionary.\\n\')\n    convs = get_conversations()\n    print(convs[121:125])\n    print(\'>> gathered conversations.\\n\')\n    questions, answers = gather_dataset(convs,id2line)\n\n    # change to lower case (just for en)\n    questions = [ line.lower() for line in questions ]\n    answers = [ line.lower() for line in answers ]\n\n    # filter out unnecessary characters\n    print(\'\\n>> Filter lines\')\n    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n\n    # filter out too long or too short sequences\n    print(\'\\n>> 2nd layer of filtering\')\n    qlines, alines = filter_data(questions, answers)\n\n    for q,a in zip(qlines[141:145], alines[141:145]):\n        print(\'q : [{0}]; a : [{1}]\'.format(q,a))\n\n    # convert list of [lines of text] into list of [list of words ]\n    print(\'\\n>> Segment lines into words\')\n    qtokenized = [ [w.strip() for w in wordlist.split(\' \') if w] for wordlist in qlines ]\n    atokenized = [ [w.strip() for w in wordlist.split(\' \') if w] for wordlist in alines ]\n    print(\'\\n:: Sample from segmented list of words\')\n\n    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n        print(\'q : [{0}]; a : [{1}]\'.format(q,a))\n\n    # indexing -> idx2w, w2idx \n    print(\'\\n >> Index words\')\n    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n    \n    # filter out sentences with too many unknowns\n    print(\'\\n >> Filter Unknowns\')\n    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n    print(\'\\n Final dataset len : \' + str(len(qtokenized)))\n\n\n    print(\'\\n >> Zero Padding\')\n    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n\n    print(\'\\n >> Save numpy arrays to disk\')\n    # save them\n    np.save(\'idx_q.npy\', idx_q)\n    np.save(\'idx_a.npy\', idx_a)\n\n    # let us now save the necessary dictionaries\n    metadata = {\n            \'w2idx\' : w2idx,\n            \'idx2w\' : idx2w,\n            \'limit\' : limit,\n            \'freq_dist\' : freq_dist\n                }\n\n    # write to disk : data control dictionaries\n    with open(\'metadata.pkl\', \'wb\') as f:\n        pickle.dump(metadata, f)\n\n    # count of unknowns\n    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n    # count of words\n    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n\n    print(\'% unknown : {0}\'.format(100 * (unk_count/word_count)))\n    print(\'Dataset count : \' + str(idx_q.shape[0]))\n\n\n    #print \'>> gathered questions and answers.\\n\'\n    #prepare_seq2seq_files(questions,answers)\n\n\nif __name__ == \'__main__\':\n    process_data()\n\n\ndef load_data(PATH=\'\'):\n    # read data control dictionaries\n    with open(PATH + \'metadata.pkl\', \'rb\') as f:\n        metadata = pickle.load(f)\n    # read numpy arrays\n    idx_q = np.load(PATH + \'idx_q.npy\')\n    idx_a = np.load(PATH + \'idx_a.npy\')\n    return metadata, idx_q, idx_a\n\n\n\n\n\n\n\n\n'"
datasets/twitter/data.py,0,"b'EN_WHITELIST = \'0123456789abcdefghijklmnopqrstuvwxyz \' # space is included in whitelist\nEN_BLACKLIST = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\\'\'\n\nFILENAME = \'data/chat.txt\'\n\nlimit = {\n        \'maxq\' : 20,\n        \'minq\' : 0,\n        \'maxa\' : 20,\n        \'mina\' : 3\n        }\n\nUNK = \'unk\'\nVOCAB_SIZE = 6000\n\nimport random\nimport sys\n\nimport nltk\nimport itertools\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport pickle\n\n\ndef ddefault():\n    return 1\n\n\'\'\'\n read lines from file\n     return [list of lines]\n\n\'\'\'\ndef read_lines(filename):\n    return open(filename).read().split(\'\\n\')[:-1]\n\n\n\'\'\'\n split sentences in one line\n  into multiple lines\n    return [list of lines]\n\n\'\'\'\ndef split_line(line):\n    return line.split(\'.\')\n\n\n\'\'\'\n remove anything that isn\'t in the vocabulary\n    return str(pure ta/en)\n\n\'\'\'\ndef filter_line(line, whitelist):\n    return \'\'.join([ ch for ch in line if ch in whitelist ])\n\n\n\'\'\'\n read list of words, create index to word,\n  word to index dictionaries\n    return tuple( vocab->(word, count), idx2w, w2idx )\n\n\'\'\'\ndef index_(tokenized_sentences, vocab_size):\n    # get frequency distribution\n    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n    # get vocabulary of \'vocab_size\' most used words\n    vocab = freq_dist.most_common(vocab_size)\n    # index2word\n    index2word = [\'_\'] + [UNK] + [ x[0] for x in vocab ]\n    # word2index\n    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n    return index2word, word2index, freq_dist\n\n\n\'\'\'\n filter too long and too short sequences\n    return tuple( filtered_ta, filtered_en )\n\n\'\'\'\ndef filter_data(sequences):\n    filtered_q, filtered_a = [], []\n    raw_data_len = len(sequences)//2\n\n    for i in range(0, len(sequences), 2):\n        qlen, alen = len(sequences[i].split(\' \')), len(sequences[i+1].split(\' \'))\n        if qlen >= limit[\'minq\'] and qlen <= limit[\'maxq\']:\n            if alen >= limit[\'mina\'] and alen <= limit[\'maxa\']:\n                filtered_q.append(sequences[i])\n                filtered_a.append(sequences[i+1])\n\n    # print the fraction of the original data, filtered\n    filt_data_len = len(filtered_q)\n    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n    print(str(filtered) + \'% filtered from original data\')\n\n    return filtered_q, filtered_a\n\n\n\n\n\n\'\'\'\n create the final dataset : \n  - convert list of items to arrays of indices\n  - add zero padding\n      return ( [array_en([indices]), array_ta([indices]) )\n \n\'\'\'\ndef zero_pad(qtokenized, atokenized, w2idx):\n    # num of rows\n    data_len = len(qtokenized)\n\n    # numpy arrays to store indices\n    idx_q = np.zeros([data_len, limit[\'maxq\']], dtype=np.int32) \n    idx_a = np.zeros([data_len, limit[\'maxa\']], dtype=np.int32)\n\n    for i in range(data_len):\n        q_indices = pad_seq(qtokenized[i], w2idx, limit[\'maxq\'])\n        a_indices = pad_seq(atokenized[i], w2idx, limit[\'maxa\'])\n\n        #print(len(idx_q[i]), len(q_indices))\n        #print(len(idx_a[i]), len(a_indices))\n        idx_q[i] = np.array(q_indices)\n        idx_a[i] = np.array(a_indices)\n\n    return idx_q, idx_a\n\n\n\'\'\'\n replace words with indices in a sequence\n  replace with unknown if word not in lookup\n    return [list of indices]\n\n\'\'\'\ndef pad_seq(seq, lookup, maxlen):\n    indices = []\n    for word in seq:\n        if word in lookup:\n            indices.append(lookup[word])\n        else:\n            indices.append(lookup[UNK])\n    return indices + [0]*(maxlen - len(seq))\n\n\ndef process_data():\n\n    print(\'\\n>> Read lines from file\')\n    lines = read_lines(filename=FILENAME)\n\n    # change to lower case (just for en)\n    lines = [ line.lower() for line in lines ]\n\n    print(\'\\n:: Sample from read(p) lines\')\n    print(lines[121:125])\n\n    # filter out unnecessary characters\n    print(\'\\n>> Filter lines\')\n    lines = [ filter_line(line, EN_WHITELIST) for line in lines ]\n    print(lines[121:125])\n\n    # filter out too long or too short sequences\n    print(\'\\n>> 2nd layer of filtering\')\n    qlines, alines = filter_data(lines)\n    print(\'\\nq : {0} ; a : {1}\'.format(qlines[60], alines[60]))\n    print(\'\\nq : {0} ; a : {1}\'.format(qlines[61], alines[61]))\n\n\n    # convert list of [lines of text] into list of [list of words ]\n    print(\'\\n>> Segment lines into words\')\n    qtokenized = [ wordlist.split(\' \') for wordlist in qlines ]\n    atokenized = [ wordlist.split(\' \') for wordlist in alines ]\n    print(\'\\n:: Sample from segmented list of words\')\n    print(\'\\nq : {0} ; a : {1}\'.format(qtokenized[60], atokenized[60]))\n    print(\'\\nq : {0} ; a : {1}\'.format(qtokenized[61], atokenized[61]))\n\n\n    # indexing -> idx2w, w2idx : en/ta\n    print(\'\\n >> Index words\')\n    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n\n    print(\'\\n >> Zero Padding\')\n    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n\n    print(\'\\n >> Save numpy arrays to disk\')\n    # save them\n    np.save(\'idx_q.npy\', idx_q)\n    np.save(\'idx_a.npy\', idx_a)\n\n    # let us now save the necessary dictionaries\n    metadata = {\n            \'w2idx\' : w2idx,\n            \'idx2w\' : idx2w,\n            \'limit\' : limit,\n            \'freq_dist\' : freq_dist\n                }\n\n    # write to disk : data control dictionaries\n    with open(\'metadata.pkl\', \'wb\') as f:\n        pickle.dump(metadata, f)\n\ndef load_data(PATH=\'\'):\n    # read data control dictionaries\n    with open(PATH + \'metadata.pkl\', \'rb\') as f:\n        metadata = pickle.load(f)\n    # read numpy arrays\n    idx_q = np.load(PATH + \'idx_q.npy\')\n    idx_a = np.load(PATH + \'idx_a.npy\')\n    return metadata, idx_q, idx_a\n\n\nif __name__ == \'__main__\':\n    process_data()\n'"
