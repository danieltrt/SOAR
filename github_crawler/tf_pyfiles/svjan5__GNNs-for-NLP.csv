file_path,api_count,code
pytorch_gcn.py,0,"b'from utils import *\nimport os.path as osp\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv\n\n\nclass KipfGCN(torch.nn.Module):\n\tdef __init__(self, data, num_class, params):\n\t\tsuper(KipfGCN, self).__init__()\n\t\tself.p     = params\n\t\tself.data  = data\n\t\tself.conv1 = GCNConv(self.data.num_features, self.p.gcn_dim, cached=True)\n\t\tself.conv2 = GCNConv(self.p.gcn_dim, num_class,  cached=True)\n\n\tdef forward(self, x, edge_index):\n\t\tx\t\t= F.relu(self.conv1(x, edge_index))\n\t\tx\t\t= F.dropout(x, p=self.p.dropout, training=self.training)\n\t\tx\t\t= self.conv2(x, edge_index)\n\t\treturn F.log_softmax(x, dim=1)\n\n\nclass Main(object):\n\n\tdef load_data(self):\n\t\t""""""\n\t\tReads the data from pickle file\n\n\t\tParameters\n\t\t----------\n\t\tself.p.dataset: The path of the dataset to be loaded\n\n\t\tReturns\n\t\t-------\n\t\tself.X: \tInput Node features\n\t\tself.A: \tAdjacency matrix\n\t\tself.num_nodes: Total nodes in the graph\n\t\tself.input_dim: \n\t\t""""""\n\n\t\tprint(""loading data"")\n\t\tpath\t\t= osp.join(osp.dirname(osp.realpath(__file__)), \'..\', \'data\', self.p.data)\n\t\tdataset\t\t= Planetoid(path, self.p.data, T.NormalizeFeatures())\n\t\tself.num_class  = dataset.num_classes\n\t\tself.data\t= dataset[0]\n\n\n\tdef add_model(self):\n\t\tmodel = KipfGCN(self.data, self.num_class, self.p)\n\t\tmodel.to(self.device)\n\t\treturn model\n\n\n\tdef add_optimizer(self, parameters):\n\t\t""""""\n\t\tAdd optimizer for training variables\n\n\t\tParameters\n\t\t----------\n\t\tparameters:\tModel parameters to be learned\n\n\t\tReturns\n\t\t-------\n\t\ttrain_op:\tTraining optimizer\n\t\t""""""\n\t\tif self.p.opt == \'adam\' : return torch.optim.Adam(parameters, lr=self.p.lr, weight_decay=self.p.l2)\n\t\telse                    : return torch.optim.SGD(parameters,  lr=self.p.lr, weight_decay=self.p.l2)\n\n\tdef __init__(self, params):\n\t\t""""""\n\t\tConstructor for the main function. Loads data and creates computation graph. \n\n\t\tParameters\n\t\t----------\n\t\tparams:\t\tHyperparameters of the model\n\n\t\tReturns\n\t\t-------\n\t\t""""""\n\t\tself.p  = params\n\n\t\tself.p.save_dir = \'{}/{}\'.format(self.p.model_dir, self.p.name)\n\t\tif not os.path.exists(self.p.log_dir): \tos.system(\'mkdir -p {}\'.format(self.p.log_dir))\t\t# Create log directory if doesn\'t exist\n\t\tif not os.path.exists(self.p.save_dir): os.system(\'mkdir -p {}\'.format(self.p.model_dir))\t# Create model directory if doesn\'t exist\n\n\t\t# Get Logger\n\t\tself.logger = get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n\t\tself.logger.info(vars(self.p)); pprint(vars(self.p))\n\n\t\tif self.p.gpu != \'-1\' and torch.cuda.is_available():\n\t\t\tself.device = torch.device(\'cuda\')\n\t\t\ttorch.cuda.set_rng_state(torch.cuda.get_rng_state())\n\t\t\ttorch.backends.cudnn.deterministic = True\n\t\telse:\n\t\t\tself.device = torch.device(\'cpu\')\n\t\n\n\t\tself.load_data()\n\t\tself.data.to(self.device)\n\t\tself.model        = self.add_model()\n\t\tself.optimizer    = self.add_optimizer(self.model.parameters())\n\n\n\tdef get_acc(self, logits, y_actual, mask):\n\t\t""""""\n\t\tCalculates accuracy\n\n\t\tParameters\n\t\t----------\n\t\tlogits:\t\tOutput of the model\n\t\ty_actual: \tGround truth label of nodes\n\t\tmask: \t\tIndicates the nodes to be considered for evaluation\n\n\t\tReturns\n\t\t-------\n\t\taccuracy:\tClassification accuracy for labeled nodes\n\t\t""""""\n\n\t\ty_pred = torch.max(logits, dim=1)[1]\n\t\treturn y_pred.eq(y_actual[mask]).sum().item() / mask.sum().item()\n\n\tdef evaluate(self, sess, split=\'valid\'):\n\t\t""""""\n\t\tEvaluate model on valid/test data\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tSession of tensorflow\n\t\tsplit:\t\tData split to evaluate on\n\n\t\tReturns\n\t\t-------\n\t\tloss:\t\tLoss over the entire data\n\t\tacc:\t\tOverall Accuracy\n\t\t""""""\n\n\t\tfeed_dict \t= self.create_feed_dict(split=split)\n\t\tloss, acc \t= sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n\n\t\treturn loss, acc\n\n\tdef run_epoch(self, epoch, shuffle=True):\n\t\t""""""\n\t\tRuns one epoch of training and evaluation on validation set\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tSession of tensorflow\n\t\tdata:\t\tData to train on\n\t\tepoch:\t\tEpoch number\n\t\tshuffle:\tShuffle data while before creates batches\n\n\t\tReturns\n\t\t-------\n\t\tloss:\t\tLoss over the entire data\n\t\tAccuracy:\tOverall accuracy\n\n\t\t""""""\n\n\t\tt = time.time()\n\n\t\tself.model.train()\n\t\tself.model.train()\n\t\tself.optimizer.zero_grad()\n\n\t\tlogits\t\t= self.model(self.data.x, self.data.edge_index)[self.data.train_mask]\n\t\ttrain_loss\t= F.nll_loss(logits, self.data.y[self.data.train_mask])\n\t\ttrain_loss.backward()\n\t\tself.optimizer.step()\n\n\t\tself.model.eval()\n\t\tlogits\t\t= self.model(self.data.x, self.data.edge_index)\n\t\ttrain_acc\t= self.get_acc(logits[self.data.train_mask], self.data.y, self.data.train_mask)\n\t\tval_acc\t\t= self.get_acc(logits[self.data.val_mask], self.data.y, self.data.val_mask)\n\n\t\tif val_acc > self.best_val: \n\t\t\tself.best_val\t= val_acc\n\t\t\tself.best_test  = self.get_acc(logits[self.data.test_mask], self.data.y, self.data.test_mask)\n\n\t\tprint(\t""Epoch:"", \t\'%04d\' % (epoch + 1), \n\t\t\t""train_loss="", \t""{:.5f}"".format(train_loss),\n\t\t\t""train_acc="",\t""{:.5f}"".format(train_acc), \n\t\t\t""val_acc="", \t""{:.5f}"".format(val_acc), \n\t\t\t""time="", \t""{:.5f}"".format(time.time() - t))\n\n\n\tdef fit(self):\n\t\t""""""\n\t\tTrains the model and finally evaluates on test\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tTensorflow session object\n\n\t\tReturns\n\t\t-------\n\t\t""""""\n\t\tself.save_path = os.path.join(self.p.save_dir, \'best_int_avg\')\n\n\t\tself.best_val, self.best_test = 0.0, 0.0\n\n\t\tif self.p.restore:\n\t\t\tself.saver.restore(self.save_path)\n\n\t\tfor epoch in range(self.p.max_epochs):\n\t\t\ttrain_loss = self.run_epoch(epoch)\n\n\t\tprint(\'Best Valid: {}, Best Test: {}\'.format(self.best_val, self.best_test))\n\n\n\nif __name__== ""__main__"":\n\n\tparser = argparse.ArgumentParser(description=\'GNN for NLP tutorial - Kipf GCN\')\n\n\tparser.add_argument(\'--data\',     \tdest=""data"",    \tdefault=\'cora\', \t\thelp=\'Dataset to use\')\n\tparser.add_argument(\'--gpu\',      \tdest=""gpu"",            \tdefault=\'0\',                \thelp=\'GPU to use\')\n\tparser.add_argument(\'--name\',     \tdest=""name"",           \tdefault=\'test\',             \thelp=\'Name of the run\')\n\n\tparser.add_argument(\'--lr\',       \tdest=""lr"",             \tdefault=0.01,   type=float,     help=\'Learning rate\')\n\tparser.add_argument(\'--epoch\',    \tdest=""max_epochs"",     \tdefault=200,    type=int,       help=\'Max epochs\')\n\tparser.add_argument(\'--l2\',       \tdest=""l2"",             \tdefault=5e-4,   type=float,     help=\'L2 regularization\')\n\tparser.add_argument(\'--seed\',     \tdest=""seed"",           \tdefault=1234,   type=int,       help=\'Seed for randomization\')\n\tparser.add_argument(\'--opt\',      \tdest=""opt"",            \tdefault=\'adam\',             \thelp=\'Optimizer to use for training\')\n\n\t# GCN-related params\n\tparser.add_argument(\'--gcn_dim\',  \tdest=""gcn_dim"",     \tdefault=16,     type=int,       help=\'GCN hidden dimension\')\n\tparser.add_argument(\'--drop\',     \tdest=""dropout"",        \tdefault=0.5,    type=float,     help=\'Dropout for full connected layer\')\n\n\tparser.add_argument(\'--restore\',  \tdest=""restore"",        \taction=\'store_true\',        \thelp=\'Restore from the previous best saved model\')\n\tparser.add_argument(\'--log_dir\',   \tdest=""log_dir"",\t\tdefault=\'./log/\',   \t   \thelp=\'Log directory\')\n\tparser.add_argument(\'--model_dir\',   \tdest=""config_dir"",\tdefault=\'./config/\',        \thelp=\'Config directory\')\n\tparser.add_argument(\'--config_dir\',   \tdest=""model_dir"",\tdefault=\'./models/\',        \thelp=\'Model directory\')\n\n\targs = parser.parse_args()\n\n\tif not args.restore: args.name = args.name + \'_\' + time.strftime(""%d_%m_%Y"") + \'_\' + time.strftime(""%H:%M:%S"")\n\n\t# Set seed\n\tnp.random.seed(args.seed)\n\tnp.random.seed(args.seed)\n\ttorch.manual_seed(args.seed)\n\n\t# Create Model\n\tmodel = Main(args)\n\tmodel.fit()\n\tprint(\'Model Trained Successfully!!\')\n'"
tf_gcn.py,37,"b'from utils import *\nimport tensorflow as tf\n\nclass KipfGCN(object):\n\n\tdef load_data(self):\n\t\t""""""\n\t\tReads the data from pickle file\n\n\t\tParameters\n\t\t----------\n\t\tself.p.dataset: The path of the dataset to be loaded\n\n\t\tReturns\n\t\t-------\n\t\tself.X: \tInput Node features\n\t\tself.A: \tAdjacency matrix\n\t\tself.num_nodes: Total nodes in the graph\n\t\tself.input_dim: \n\t\t""""""\n\n\t\tprint(""loading data"")\n\t\tself.data = {}\n\t\tself.A, self.X, self.data[\'y_train\'], self.data[\'y_valid\'], self.data[\'y_test\'], \\\n\t\tself.data[\'mask_train\'], self.data[\'mask_valid\'], self.data[\'mask_test\'] = load_network(self.p.data)\n\n\t\tself.num_nodes    = self.X.shape[0]\n\t\tself.input_dim    = self.X.shape[1]\n\t\tself.X \t  \t  = preprocess_features(self.X)\n\t\tself.A  \t  = preprocess_adj(self.A)\n\t\tself.num_labels   = self.data[\'y_train\'].shape[1]\n\n\n\tdef add_placehoders(self):\n\t\t""""""\n\t\tDefines the placeholder required for the model\n\t\t""""""\n\t\tself.features \t = tf.sparse_placeholder(tf.float32, \tshape=[self.num_nodes, self.input_dim], name=\'features\')\n\t\tself.adj_mat \t = tf.sparse_placeholder(tf.float32, \tshape=[self.num_nodes, self.num_nodes], name=\'support\')\n\t\tself.labels \t = tf.placeholder(tf.float32, \t  \tshape=[None, self.num_labels], \t\tname=\'labels\')\n\t\tself.labels_mask = tf.placeholder(tf.int32, \t\t\t\t\t\t\tname=\'labels_mask\')\n\t\tself.dropout \t = tf.placeholder_with_default(0.,   \tshape=(), \t\t\t\tname=\'dropout\')\n\t\tself.num_nonzero = tf.placeholder(tf.int32, \t\t\t\t\t\t\tname=\'num_nonzero\')\n\n\tdef create_feed_dict(self, split=\'train\'):\n\t\t""""""\n\t\tCreates the feed_dict for training the given step.\n\t\tA feed_dict takes the form of:\n\t\tfeed_dict = {\n\t\t\t<placeholder>: <tensor of values to be passed for placeholder>,\n\t\t\t....\n\t\t}\n\t\n\t\tIf label_batch is None, then no labels are added to feed_dict.\n\t\tHint: The keys for the feed_dict should be a subset of the placeholder tensors created in add_placeholders.\n\t\t\n\t\tParameters\n\t\t----------\n\t\tinput_batch: \tA batch of input data.\n\t\tlabel_batch: \tA batch of label data.\n\n\t\tReturns\n\t\t-------\n\t\tfeed_dict: \tThe feed dictionary mapping from placeholders to values.\n\t\t""""""\n\t\tfeed = {}\n\t\t\n\t\tfeed[self.features]\t= self.X\n\t\tfeed[self.adj_mat]\t= self.A\n\t\tfeed[self.num_nonzero]\t= self.X[1].shape\n\n\t\tfeed[self.labels]\t= self.data[\'y_{}\'.format(split)]\n\t\tfeed[self.labels_mask]\t= self.data[\'mask_{}\'.format(split)]\n\n\t\treturn feed\n\n\tdef sparse_dropout(self, x, keep_prob, noise_shape):\n\t\t""""""\n\t\tDropout for sparse tensors.\n\t\t""""""\n\t\trandom_tensor  = keep_prob\n\t\trandom_tensor += tf.random_uniform(noise_shape)\n\t\tdropout_mask   = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n\t\tpre_out        = tf.sparse_retain(x, dropout_mask)\n\t\treturn pre_out * (1./keep_prob)\n\n\n\tdef GCNLayer(self, gcn_in, adj_mat, input_dim, output_dim, act, dropout, num_nonzero, input_sparse=False, name=\'GCN\'):\n\t\t""""""\n\t\tGCN Layer Implementation\n\n\t\tParameters\n\t\t----------\n\t\tgcn_in:\t\tInput to GCN Layer\n\t\tadj_mat:\tAdjacency matrix\n\t\tinput_dim:\tDimension of input to GCN Layer \n\t\toutput_dim:\tDimension of output of GCN Layer\n\t\tact:\t\tActivation function used\n\t\tdroptout:\tDropout probability\n\t\tnum_numzero: \tNumber of non-zero elements in input features (used when input_sparse=True)\n\t\tinput_sparse:\tWhether input features are sparse or not\n\t\tname; \t\tName of the Layer\n\n\t\tReturns\n\t\t-------\n\t\tOutput of GCN Layer\n\t\t\n\t\t""""""\n\n\t\twith tf.name_scope(name):\n\t\t\twith tf.variable_scope(\'{}_vars\'.format(name)) as scope:\n\t\t\t\twts  = tf.get_variable(\'weights\', [input_dim, output_dim], \t initializer=tf.initializers.glorot_normal())\n\t\t\t\tbias = tf.get_variable(\'bias\', \t  [output_dim], \t   \t initializer=tf.initializers.glorot_normal())\n\t\t\t\tself.l2_vars.extend([wts, bias])\n\t\t\t\n\t\t\tif input_sparse:\n\t\t\t\tgcn_in  = self.sparse_dropout(gcn_in, 1 - dropout, num_nonzero)\n\t\t\t\tpre_sup = tf.sparse_tensor_dense_matmul(gcn_in,  wts)\n\t\t\telse:\n\t\t\t\tgcn_in = tf.nn.dropout(gcn_in, 1-dropout)\n\t\t\t\tpre_sup = tf.matmul(gcn_in,  wts)\n\n\t\t\tsupport = tf.sparse_tensor_dense_matmul(adj_mat, pre_sup)\n\n\t\treturn act(support)\n\n\tdef add_model(self):\n\n\t\tgcn1_out = self.GCNLayer(\t\t\n\t\t\t\tgcn_in \t\t\t= self.features,\n\t\t\t\tadj_mat \t\t= self.adj_mat,\n\t\t\t\tinput_dim\t\t= self.input_dim,\n\t\t\t\toutput_dim\t\t= self.p.gcn_dim,\n\t\t\t\tact\t\t\t= tf.nn.relu,\n\t\t\t\tdropout\t\t\t= self.dropout,\n\t\t\t\tnum_nonzero \t\t= self.num_nonzero,\n\t\t\t\tinput_sparse \t\t= True,\n\t\t\t\tname \t\t\t= \'GCN_1\'\n\t\t\t)\n\t\t\n\t\tgcn2_out = self.GCNLayer(\t\t\n\t\t\t\tgcn_in \t\t\t= gcn1_out,\n\t\t\t\tadj_mat \t\t= self.adj_mat,\n\t\t\t\tinput_dim\t\t= self.p.gcn_dim,\n\t\t\t\toutput_dim\t\t= self.num_labels,\n\t\t\t\tact\t\t\t= lambda x: x,\n\t\t\t\tdropout\t\t\t= self.dropout,\n\t\t\t\tnum_nonzero \t\t= self.num_nonzero,\n\t\t\t\tinput_sparse \t\t= False,\n\t\t\t\tname \t\t\t= \'GCN_2\'\n\t\t\t)\n\n\t\tnn_out = gcn2_out\n\t\treturn nn_out\n\n\tdef get_accuracy(self, nn_out):\n\t\t""""""\n\t\tCalculates accuracy\n\n\t\tParameters\n\t\t----------\n\t\tnn_out:\t\tLogits for each bag in the batch\n\n\t\tReturns\n\t\t-------\n\t\taccuracy:\tClassification accuracy for labeled nodes\n\t\t""""""\n\n\t\tcorrect_prediction \t = tf.equal(tf.argmax(nn_out, 1), tf.argmax(self.labels, 1))\t# Identity position where prediction matches labels\n\t\taccuracy_all \t\t = tf.cast(correct_prediction, tf.float32)\t\t\t# Cast result to float\n\t\tmask \t\t\t = tf.cast(self.labels_mask, dtype=tf.float32)\t\t\t# Cast mask to float\n\t\tmask \t\t\t/= tf.reduce_mean(mask)\t\t\t\t\t\t# Compute mean of mask\n\t\taccuracy_all \t\t*= mask \t\t\t\t\t\t\t# Apply mask on computed accuracy\n\n\t\treturn tf.reduce_mean(accuracy_all)\n\n\n\tdef add_loss_op(self, nn_out):\n\t\t""""""\n\t\tComputes loss based on logits and actual labels\n\n\t\tParameters\n\t\t----------\n\t\tnn_out:\t\tLogits for each bag in the batch\n\n\t\tReturns\n\t\t-------\n\t\tloss:\t\tComputes loss based on prediction and actual labels of the bags\n\t\t""""""\n\n\t\tloss  = tf.nn.softmax_cross_entropy_with_logits(logits=nn_out, labels=self.labels) \t# Compute cross entropy loss\n\t\tmask  = tf.cast(self.labels_mask, dtype=tf.float32)\t\t\t\t\t# Cast masking from boolean to float\n\t\tmask /= tf.reduce_mean(mask)\t\t\t\t\t\t\t\t# Compute mean for mask\n\t\tloss *= mask \t\t\t\t\t\t\t\t\t\t# Mask the output of cross entropy loss\n\t\tloss  = tf.reduce_mean(loss)\n\n\t\tfor var in self.l2_vars:\n\t\t\tloss += self.p.l2 * tf.nn.l2_loss(var)\n\n\t\treturn loss\n\n\tdef add_optimizer(self, loss, isAdam=True):\n\t\t""""""\n\t\tAdd optimizer for training variables\n\n\t\tParameters\n\t\t----------\n\t\tloss:\t\tComputed loss\n\n\t\tReturns\n\t\t-------\n\t\ttrain_op:\tTraining optimizer\n\t\t""""""\n\t\twith tf.name_scope(\'Optimizer\'):\n\t\t\tif isAdam:  optimizer = tf.train.AdamOptimizer(self.p.lr)\n\t\t\telse:       optimizer = tf.train.GradientDescentOptimizer(self.p.lr)\n\t\t\ttrain_op  = optimizer.minimize(loss)\n\n\t\treturn train_op\n\n\tdef __init__(self, params):\n\t\t""""""\n\t\tConstructor for the main function. Loads data and creates computation graph. \n\n\t\tParameters\n\t\t----------\n\t\tparams:\t\tHyperparameters of the model\n\n\t\tReturns\n\t\t-------\n\t\t""""""\n\t\tself.p  = params\n\n\t\tself.p.save_dir = \'{}/{}\'.format(self.p.model_dir, self.p.name)\n\t\tif not os.path.exists(self.p.log_dir): \tos.system(\'mkdir -p {}\'.format(self.p.log_dir))\t\t# Create log directory if doesn\'t exist\n\t\tif not os.path.exists(self.p.save_dir): os.system(\'mkdir -p {}\'.format(self.p.model_dir))\t# Create model directory if doesn\'t exist\n\n\t\t# Get Logger\n\t\tself.logger = get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n\t\tself.logger.info(vars(self.p)); pprint(vars(self.p))\n\n\t\t# Vairable for storing variables which needs to be regularized\n\t\tself.l2_vars = []\n\n\t\tself.load_data()\t\t\t# Load Dataset\n\t\tself.add_placehoders()\t\t\t# Define Placeholders\n\n\t\tnn_out    \t= self.add_model()\t\t# Construct Computational Graph\n\t\tself.loss \t= self.add_loss_op(nn_out)\n\t\t\n\t\tself.accuracy \t= self.get_accuracy(nn_out)\n\t\tself.train_op \t= self.add_optimizer(self.loss)\n\t\tself.cost_val \t= []\n\n\t\tself.merged_summ = tf.summary.merge_all()\n\n\n\tdef evaluate(self, sess, split=\'valid\'):\n\t\t""""""\n\t\tEvaluate model on valid/test data\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tSession of tensorflow\n\t\tsplit:\t\tData split to evaluate on\n\n\t\tReturns\n\t\t-------\n\t\tloss:\t\tLoss over the entire data\n\t\tacc:\t\tOverall Accuracy\n\t\t""""""\n\n\t\tfeed_dict \t= self.create_feed_dict(split=split)\n\t\tloss, acc \t= sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n\n\t\treturn loss, acc\n\n\tdef run_epoch(self, sess, epoch, shuffle=True):\n\t\t""""""\n\t\tRuns one epoch of training and evaluation on validation set\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tSession of tensorflow\n\t\tdata:\t\tData to train on\n\t\tepoch:\t\tEpoch number\n\t\tshuffle:\tShuffle data while before creates batches\n\n\t\tReturns\n\t\t-------\n\t\tloss:\t\tLoss over the entire data\n\t\tAccuracy:\tOverall accuracy\n\n\t\t""""""\n\n\t\tt = time.time()\n\t\tfeed_dict = self.create_feed_dict(split=\'train\')\n\t\tfeed_dict.update({self.dropout: self.p.dropout})\n\n\t\t# Training step\n\t\t_, train_loss, train_acc = sess.run([self.train_op, self.loss, self.accuracy], feed_dict=feed_dict)\n\n\t\t# Validation\n\t\tval_loss, val_acc = self.evaluate(sess, split=\'valid\')\n\n\t\tif val_acc > self.best_val: \n\t\t\tself.best_val\t\t= val_acc\n\t\t\t_, self.best_test\t= self.evaluate(sess, split=\'test\')\n\n\t\tprint(\t""Epoch:"", \t\'%04d\' % (epoch + 1), \n\t\t\t""train_loss="", \t""{:.5f}"".format(train_loss),\n\t\t\t""train_acc="",\t""{:.5f}"".format(train_acc), \n\t\t\t""val_loss="", \t""{:.5f}"".format(val_loss),\n\t\t\t""val_acc="", \t""{:.5f}"".format(val_acc), \n\t\t\t""time="", \t""{:.5f}"".format(time.time() - t))\n\n\n\tdef fit(self, sess):\n\t\t""""""\n\t\tTrains the model and finally evaluates on test\n\n\t\tParameters\n\t\t----------\n\t\tsess:\t\tTensorflow session object\n\n\t\tReturns\n\t\t-------\n\t\t""""""\n\t\tself.saver\t\t= tf.train.Saver()\n\t\tself.save_path\t\t= os.path.join(self.p.save_dir, \'best_int_avg\')\n\n\t\tself.best_val, self.best_test = 0.0, 0.0\n\n\t\tif self.p.restore:\n\t\t\tself.saver.restore(sess, self.save_path)\n\n\t\tfor epoch in range(self.p.max_epochs):\n\t\t\ttrain_loss = self.run_epoch(sess, epoch)\n\n\t\tprint(\'Best Valid: {}, Best Test: {}\'.format(self.best_val, self.best_test))\n\n\nif __name__== ""__main__"":\n\n\tparser = argparse.ArgumentParser(description=\'GNN for NLP tutorial - Kipf GCN\')\n\n\tparser.add_argument(\'--data\',     \tdest=""data"",    \tdefault=\'cora\', \t\thelp=\'Dataset to use\')\n\tparser.add_argument(\'--gpu\',      \tdest=""gpu"",            \tdefault=\'0\',                \thelp=\'GPU to use\')\n\tparser.add_argument(\'--name\',     \tdest=""name"",           \tdefault=\'test\',             \thelp=\'Name of the run\')\n\n\tparser.add_argument(\'--lr\',       \tdest=""lr"",             \tdefault=0.01,   type=float,     help=\'Learning rate\')\n\tparser.add_argument(\'--epoch\',    \tdest=""max_epochs"",     \tdefault=200,    type=int,       help=\'Max epochs\')\n\tparser.add_argument(\'--l2\',       \tdest=""l2"",             \tdefault=5e-4,   type=float,     help=\'L2 regularization\')\n\tparser.add_argument(\'--seed\',     \tdest=""seed"",           \tdefault=1234,   type=int,       help=\'Seed for randomization\')\n\tparser.add_argument(\'--opt\',      \tdest=""opt"",            \tdefault=\'adam\',             \thelp=\'Optimizer to use for training\')\n\n\t# GCN-related params\n\tparser.add_argument(\'--gcn_dim\',  \tdest=""gcn_dim"",     \tdefault=16,     type=int,       help=\'GCN hidden dimension\')\n\tparser.add_argument(\'--drop\',     \tdest=""dropout"",        \tdefault=0.5,    type=float,     help=\'Dropout for full connected layer\')\n\n\tparser.add_argument(\'--restore\',  \tdest=""restore"",        \taction=\'store_true\',        \thelp=\'Restore from the previous best saved model\')\n\tparser.add_argument(\'--log_dir\',   \tdest=""log_dir"",\t\tdefault=\'./log/\',   \t   \thelp=\'Log directory\')\n\tparser.add_argument(\'--model_dir\',   \tdest=""config_dir"",\tdefault=\'./config/\',        \thelp=\'Config directory\')\n\tparser.add_argument(\'--config_dir\',   \tdest=""model_dir"",\tdefault=\'./models/\',        \thelp=\'Model directory\')\n\n\targs = parser.parse_args()\n\n\tif not args.restore: args.name = args.name + \'_\' + time.strftime(""%d_%m_%Y"") + \'_\' + time.strftime(""%H:%M:%S"")\n\n\t# Set seed\n\ttf.set_random_seed(args.seed)\n\tnp.random.seed(args.seed)\n\n\t# Set GPU to use\n\tset_gpu(args.gpu)\n\n\t# Create Model\n\tmodel = KipfGCN(args)\n\n\t# Start training and evaluation\n\tconfig = tf.ConfigProto()\n\tconfig.gpu_options.allow_growth=True\n\twith tf.Session(config=config) as sess:\n\t\tsess.run(tf.global_variables_initializer())\n\t\tmodel.fit(sess)\n\n\tprint(\'Model Trained Successfully!!\')'"
utils.py,4,"b'import os, sys, time, json, pickle as pkl, argparse\nimport logging, logging.config\nimport networkx as nx\nfrom pprint import pprint\nimport numpy as np, scipy.sparse as sp\nfrom scipy.sparse.linalg.eigen.arpack import eigsh\n\ndef set_gpu(gpus):\n\t""""""\n\tSets the GPU to be used for the run\n\tParameters\n\t----------\n\tgpus:           List of GPUs to be used for the run\n\t\n\tReturns\n\t-------    \n\t""""""\n\tos.environ[""CUDA_DEVICE_ORDER""]    = ""PCI_BUS_ID""\n\tos.environ[""CUDA_VISIBLE_DEVICES""] = gpus\n\ndef debug_nn(res_list, feed_dict):\n\t""""""\n\tFunction for debugging Tensorflow model      \n\tParameters\n\t----------\n\tres_list:       List of tensors/variables to view\n\tfeed_dict:\tFeed dict required for getting values\n\t\n\tReturns\n\t-------\n\tReturns the list of values of given tensors/variables after execution\n\t""""""\n\timport tensorflow as tf\n\t\n\tconfig = tf.ConfigProto()\n\tconfig.gpu_options.allow_growth=True\n\tsess = tf.Session(config=config)\n\tsess.run(tf.global_variables_initializer())\n\tsumm_writer = tf.summary.FileWriter(""tf_board/debug_nn"", sess.graph)\n\tres = sess.run(res_list, feed_dict = feed_dict)\n\treturn res\n\ndef get_logger(name, log_dir, config_dir):\n\t""""""\n\tCreates a logger object\n\tParameters\n\t----------\n\tname:           Name of the logger file\n\tlog_dir:        Directory where logger file needs to be stored\n\tconfig_dir:     Directory from where log_config.json needs to be read\n\t\n\tReturns\n\t-------\n\tA logger object which writes to both file and stdout\n\t\t\n\t""""""\n\tconfig_dict = json.load(open( config_dir + \'log_config.json\'))\n\tconfig_dict[\'handlers\'][\'file_handler\'][\'filename\'] = log_dir + name.replace(\'/\', \'-\')\n\tlogging.config.dictConfig(config_dict)\n\tlogger = logging.getLogger(name)\n\n\tstd_out_format = \'%(asctime)s - [%(levelname)s] - %(message)s\'\n\tconsoleHandler = logging.StreamHandler(sys.stdout)\n\tconsoleHandler.setFormatter(logging.Formatter(std_out_format))\n\tlogger.addHandler(consoleHandler)\n\n\treturn logger\n\n# The following functions are directly taken from https://github.com/tkipf/gcn\n\ndef parse_index_file(filename):\n\t""""""Parse index file.""""""\n\tindex = []\n\tfor line in open(filename):\n\t\tindex.append(int(line.strip()))\n\treturn index\n\n\ndef sample_mask(idx, l):\n\t""""""Create mask.""""""\n\tmask = np.zeros(l)\n\tmask[idx] = 1\n\treturn np.array(mask, dtype=np.bool)\n\n\ndef load_network(dataset_str):\n\t""""""\n\tLoads input data from gcn/data directory\n\tind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n\tind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n\tind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n\t\t(a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n\tind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n\tind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n\tind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n\tind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n\t\tobject;\n\tind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n\tAll objects above must be saved using python pickle module.\n\t:param dataset_str: Dataset name\n\t:return: All data input files loaded (as well the training/test data).\n\t""""""\n\tnames = [\'x\', \'y\', \'tx\', \'ty\', \'allx\', \'ally\', \'graph\']\n\tobjects = []\n\tfor i in range(len(names)):\n\t\twith open(""data/ind.{}.{}"".format(dataset_str, names[i]), \'rb\') as f:\n\t\t\tif sys.version_info > (3, 0):\n\t\t\t\tobjects.append(pkl.load(f, encoding=\'latin1\'))\n\t\t\telse:\n\t\t\t\tobjects.append(pkl.load(f))\n\n\tx, y, tx, ty, allx, ally, graph = tuple(objects)\n\ttest_idx_reorder = parse_index_file(""data/ind.{}.test.index"".format(dataset_str))\n\ttest_idx_range = np.sort(test_idx_reorder)\n\n\tif dataset_str == \'citeseer\':\n\t\t# Fix citeseer dataset (there are some isolated nodes in the graph)\n\t\t# Find isolated nodes, add them as zero-vecs into the right position\n\t\ttest_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n\t\ttx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n\t\ttx_extended[test_idx_range-min(test_idx_range), :] = tx\n\t\ttx = tx_extended\n\t\tty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n\t\tty_extended[test_idx_range-min(test_idx_range), :] = ty\n\t\tty = ty_extended\n\n\tfeatures = sp.vstack((allx, tx)).tolil()\n\tfeatures[test_idx_reorder, :] = features[test_idx_range, :]\n\tadj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n\tlabels = np.vstack((ally, ty))\n\tlabels[test_idx_reorder, :] = labels[test_idx_range, :]\n\n\tidx_test = test_idx_range.tolist()\n\tidx_train = range(len(y))\n\tidx_val = range(len(y), len(y)+500)\n\n\ttrain_mask = sample_mask(idx_train, labels.shape[0])\n\tval_mask = sample_mask(idx_val, labels.shape[0])\n\ttest_mask = sample_mask(idx_test, labels.shape[0])\n\n\ty_train = np.zeros(labels.shape)\n\ty_val = np.zeros(labels.shape)\n\ty_test = np.zeros(labels.shape)\n\ty_train[train_mask, :] = labels[train_mask, :]\n\ty_val[val_mask, :] = labels[val_mask, :]\n\ty_test[test_mask, :] = labels[test_mask, :]\n\n\treturn adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\n\ndef sparse_to_tuple(sparse_mx):\n\t""""""Convert sparse matrix to tuple representation.""""""\n\tdef to_tuple(mx):\n\t\tif not sp.isspmatrix_coo(mx):\n\t\t\tmx = mx.tocoo()\n\t\tcoords = np.vstack((mx.row, mx.col)).transpose()\n\t\tvalues = mx.data\n\t\tshape = mx.shape\n\t\treturn coords, values, shape\n\n\tif isinstance(sparse_mx, list):\n\t\tfor i in range(len(sparse_mx)):\n\t\t\tsparse_mx[i] = to_tuple(sparse_mx[i])\n\telse:\n\t\tsparse_mx = to_tuple(sparse_mx)\n\n\treturn sparse_mx\n\n\ndef preprocess_features(features):\n\t""""""Row-normalize feature matrix and convert to tuple representation""""""\n\trowsum = np.array(features.sum(1))\n\tr_inv = np.power(rowsum, -1).flatten()\n\tr_inv[np.isinf(r_inv)] = 0.\n\tr_mat_inv = sp.diags(r_inv)\n\tfeatures = r_mat_inv.dot(features)\n\treturn sparse_to_tuple(features)\n\n\ndef normalize_adj(adj):\n\t""""""Symmetrically normalize adjacency matrix.""""""\n\tadj = sp.coo_matrix(adj)\n\trowsum = np.array(adj.sum(1))\n\td_inv_sqrt = np.power(rowsum, -0.5).flatten()\n\td_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n\td_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n\treturn adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n\n\ndef preprocess_adj(adj, noTuple=False):\n\t""""""Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.""""""\n\tadj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n\tif noTuple:\n\t\treturn adj_normalized\n\telse:\n\t\treturn sparse_to_tuple(adj_normalized)\n\n\ndef construct_feed_dict(features, support, labels, labels_mask, placeholders):\n\t""""""Construct feed dictionary.""""""\n\tfeed_dict = dict()\n\tfeed_dict.update({placeholders[\'labels\']: labels})\n\tfeed_dict.update({placeholders[\'labels_mask\']: labels_mask})\n\tfeed_dict.update({placeholders[\'features\']: features})\n\tfeed_dict.update({placeholders[\'support\'][i]: support[i] for i in range(len(support))})\n\tfeed_dict.update({placeholders[\'num_features_nonzero\']: features[1].shape})\n\treturn feed_dict\n\n\ndef chebyshev_polynomials(adj, k):\n\t""""""Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).""""""\n\tprint(""Calculating Chebyshev polynomials up to order {}..."".format(k))\n\n\tadj_normalized = normalize_adj(adj)\n\tlaplacian = sp.eye(adj.shape[0]) - adj_normalized\n\tlargest_eigval, _ = eigsh(laplacian, 1, which=\'LM\')\n\tscaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n\n\tt_k = list()\n\tt_k.append(sp.eye(adj.shape[0]))\n\tt_k.append(scaled_laplacian)\n\n\tdef chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n\t\ts_lap = sp.csr_matrix(scaled_lap, copy=True)\n\t\treturn 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n\n\tfor i in range(2, k+1):\n\t\tt_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n\n\treturn sparse_to_tuple(t_k)'"
