file_path,api_count,code
setup.py,0,"b'import numpy\n\nfrom sys import platform\nfrom setuptools import setup\nfrom setuptools.extension import Extension\nfrom Cython.Build import cythonize\n\next = \'tensornets.references.darkflow_utils\'\next_modules = [Extension(""%s.%s"" % (ext, n),\n                         sources=[""%s/%s.pyx"" % (ext.replace(\'.\', \'/\'), n)],\n                         libraries=[] if platform.startswith(""win"") else [\'m\'],\n                         include_dirs=[numpy.get_include()])\n               for n in [\'nms\', \'get_boxes\']]\n\nsetup(name=\'tensornets\',\n      version=\'0.4.6\',\n      description=\'high level network definitions in tensorflow\',\n      author=\'Taehoon Lee\',\n      author_email=\'me@taehoonlee.com\',\n      url=\'https://github.com/taehoonlee/tensornets\',\n      download_url=\'https://github.com/taehoonlee/tensornets/tarball/0.4.6\',\n      license=\'MIT\',\n      packages=[\'tensornets\', \'tensornets.datasets\',\n                \'tensornets.contrib_framework\', \'tensornets.contrib_layers\',\n                \'tensornets.references\', ext],\n      include_package_data=True,\n      ext_modules=cythonize(ext_modules))\n'"
examples/evaluate_imagenet.py,26,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensornets as nets\n\nfrom imagenet_preprocessing import input_fn as _input_fn\n\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 50,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 200, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'steps\', None, \'The number of steps for evaluation.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'ResNet50\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', \'/home/taehoonlee/Data/imagenet/tfrecords\',\n    \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', 224, \'The eval image size\')\n\ntf.app.flags.DEFINE_integer(\n    \'normalize\', 0, \'The normalization type\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\n# Simple trick to suppress the warning\n# ""It seems that global step has not been increased""\nclass hook(tf.train.StepCounterHook):\n    def __init__(self, every_n_steps):\n        self._steps = 0\n        super(hook, self).__init__(every_n_steps)\n\n    def after_run(self, run_context, run_values):\n        self._steps += 1\n        if self._timer.should_trigger_for_step(self._steps):\n            t, steps = self._timer.update_last_triggered_step(self._steps)\n            if t is not None:\n                tf.logging.info(""%g secs per step on average"", t / steps)\n\n\ndef input_fn():\n    return _input_fn(\n        is_training=False,\n        data_dir=FLAGS.dataset_dir,\n        batch_size=FLAGS.batch_size,\n        eval_image_size=FLAGS.eval_image_size,\n        normalize=FLAGS.normalize)\n\n\ndef model_fn(features, labels, mode):\n    models = []\n    logits = []\n    classes = []\n    init_op = [tf.train.get_or_create_global_step().initializer]\n    for (i, model_name) in enumerate(FLAGS.model_name.split(\',\')):\n        with tf.device(""/gpu:%d"" % i):\n            network_fn = getattr(nets, model_name)\n            models.append(network_fn(features, is_training=False))\n            logits.append(models[i].get_outputs()[-2])\n            classes.append(tf.argmax(logits[i], axis=1))\n            if FLAGS.checkpoint_path is None:\n                init_op.extend(models[i].pretrained())\n\n    scaffold = None\n    if FLAGS.checkpoint_path is None:\n        scaffold = tf.train.Scaffold(init_op=init_op)\n\n    loss = []\n    for i in range(len(models)):\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n            logits=logits[i], labels=labels)\n        loss.append(cross_entropy)\n    loss = tf.reduce_sum(loss)\n\n    metrics = None\n    if mode == tf.estimator.ModeKeys.EVAL:\n        metrics = {}\n        for i in range(len(models)):\n            top1 = tf.metrics.accuracy(labels=labels, predictions=classes[i])\n            top5 = tf.contrib.metrics.streaming_sparse_recall_at_k(\n                logits[i], tf.cast(labels, tf.int64), k=5)\n            size = sum([w.shape.num_elements()\n                        for w in models[i].get_weights()])\n            metrics.update({""%dTop1"" % i: top1,\n                            ""%dTop5"" % i: top5,\n                            ""%dSize"" % i: (tf.constant(size), tf.no_op())})\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        scaffold=scaffold,\n        predictions=None,\n        loss=loss,\n        train_op=None,\n        eval_metric_ops=metrics,\n        export_outputs=None)\n\n\ndef main(_):\n    if not FLAGS.dataset_dir:\n        raise ValueError(\'You must supply the dataset directory.\')\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    classifier = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=FLAGS.checkpoint_path)\n\n    if FLAGS.steps is None:\n        FLAGS.steps = 50000 // FLAGS.batch_size\n\n    results = classifier.evaluate(\n        input_fn=input_fn, steps=FLAGS.steps,\n        hooks=[hook(every_n_steps=FLAGS.log_every_n_steps)])\n\n    print(""| {:5d} Samples    | Top-1       | Top-5       | Size   |"".format(\n          FLAGS.batch_size * FLAGS.steps))\n    print(""|------------------|-------------|-------------|--------|"")\n    for (i, model_name) in enumerate(FLAGS.model_name.split(\',\')):\n        print(""| {:16s} | {:6.3f}      | {:6.3f}      | {:5.1f}M |"".format(\n              model_name,\n              100 * (1 - results[""%dTop1"" % i]),\n              100 * (1 - results[""%dTop5"" % i]),\n              results[""%dSize"" % i] / 10e5))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
examples/imagenet_preprocessing.py,56,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\n==============================================================================\nThe codes were largely taken from the TensorFlow Models\n(https://github.com/tensorflow/models). Especially,\neach part was from the following:\n\n1. _decode_crop_and_flip, _central_crop, _mean_image_subtraction,\n    _smallest_size_at_least, _aspect_preserving_resize, _resize_image,\n - copied from ${models}/official/resnet/imagenet_preprocessing.py\n2. preprocess_image\n - copied from ${models}/official/resnet/imagenet_preprocessing.py\n - and slightly modified\n3. process_record_dataset, _parse_example_proto, parse_record, input_fn\n - copied from ${models}/official/resnet/imagenet_main.py\n - and slightly modified\n==============================================================================\n\nTraining images are sampled using the provided bounding boxes, and subsequently\ncropped to the sampled bounding box. Images are additionally flipped randomly,\nthen resized to the target output size (without aspect-ratio preservation).\n\nImages used during evaluation are resized (with aspect-ratio preservation) and\ncentrally cropped.\n\nAll images undergo mean color subtraction.\n\nNote that these steps are colloquially referred to as ""ResNet preprocessing,""\nand they differ from ""VGG preprocessing,"" which does not use bounding boxes\nand instead does an aspect-preserving resize followed by random crop during\ntraining. (These both differ from ""Inception preprocessing,"" which introduces\ncolor distortion steps.)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\n_CHANNEL_MEANS0 = [123.68, 116.78, 103.94]\n_CHANNEL_MEANS3 = [0.485, 0.456, 0.406]\n_CHANNEL_STDS3 = [0.229, 0.224, 0.225]\n_CHANNEL_MEANS4 = [123., 117., 104.]\n_CHANNEL_MEANS5 = [0.491, 0.482, 0.447]\n_CHANNEL_STDS5 = [0.247, 0.244, 0.262]\n\n\ndef _decode_crop_and_flip(image_buffer, bbox, num_channels):\n  """"""Crops the given image to a random part of the image, and randomly flips.\n\n  We use the fused decode_and_crop op, which performs better than the two ops\n  used separately in series, but note that this requires that the image be\n  passed in as an un-decoded string Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    num_channels: Integer depth of the image buffer for decoding.\n\n  Returns:\n    3-D tensor with cropped image.\n\n  """"""\n  # A large fraction of image datasets contain a human-annotated bounding box\n  # delineating the region of the image containing the object of interest.  We\n  # choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an\n  # allowed range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n  sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n      tf.image.extract_jpeg_shape(image_buffer),\n      bounding_boxes=bbox,\n      min_object_covered=0.1,\n      aspect_ratio_range=[0.75, 1.33],\n      area_range=[0.05, 1.0],\n      max_attempts=100,\n      use_image_if_no_bounding_boxes=True)\n  bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n  # Reassemble the bounding box in the format the crop op requires.\n  offset_y, offset_x, _ = tf.unstack(bbox_begin)\n  target_height, target_width, _ = tf.unstack(bbox_size)\n  crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n\n  # Use the fused decode and crop op here, which is faster than each in series.\n  cropped = tf.image.decode_and_crop_jpeg(\n      image_buffer, crop_window, channels=num_channels)\n\n  # Flip to add a little more random distortion in.\n  cropped = tf.image.random_flip_left_right(cropped)\n  return cropped\n\n\ndef _central_crop(image, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image: a 3-D image tensor\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    3-D tensor with cropped image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  amount_to_be_cropped_h = (height - crop_height)\n  crop_top = amount_to_be_cropped_h // 2\n  amount_to_be_cropped_w = (width - crop_width)\n  crop_left = amount_to_be_cropped_w // 2\n  return tf.slice(\n      image, [crop_top, crop_left, 0], [crop_height, crop_width, -1])\n\n\ndef _mean_image_subtraction(image, means, num_channels, stds=None):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  means = tf.reshape(means, (1, 1, 3))\n  images = image - means\n\n  if stds is not None:\n    if len(stds) != num_channels:\n      raise ValueError(\'len(stds) must match the number of channels\')\n\n    # We have a 1-D tensor of means; convert to 3-D.\n    stds = tf.reshape(stds, (1, 1, 3))\n    images = tf.div(images, stds)\n\n  return images\n\n\ndef _smallest_size_at_least(height, width, resize_min):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: an int32 scalar tensor indicating the new width.\n  """"""\n  resize_min = tf.cast(resize_min, tf.float32)\n\n  # Convert to floats to make subsequent calculations go smoothly.\n  height, width = tf.cast(height, tf.float32), tf.cast(width, tf.float32)\n\n  smaller_dim = tf.minimum(height, width)\n  scale_ratio = resize_min / smaller_dim\n\n  # Convert back to ints to make heights and widths that TF ops will accept.\n  new_height = tf.cast(height * scale_ratio, tf.int32)\n  new_width = tf.cast(width * scale_ratio, tf.int32)\n\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, resize_min):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    resize_min: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  shape = tf.shape(image)\n  height, width = shape[0], shape[1]\n\n  new_height, new_width = _smallest_size_at_least(height, width, resize_min)\n\n  return _resize_image(image, new_height, new_width)\n\n\ndef _resize_image(image, height, width):\n  """"""Simple wrapper around tf.resize_images.\n\n  This is primarily to make sure we use the same `ResizeMethod` and other\n  details each time.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    height: The target height for the resized image.\n    width: The target width for the resized image.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image. The first two\n      dimensions have the shape [height, width].\n  """"""\n  return tf.image.resize_images(\n      image, [height, width], method=tf.image.ResizeMethod.BILINEAR,\n      align_corners=False)\n\n\ndef preprocess_image(image_buffer, bbox, output_height, output_width,\n                     num_channels, is_training=False, resize_side_min=256):\n  """"""Preprocesses the given image.\n\n  Preprocessing includes decoding, cropping, and resizing for both training\n  and eval images. Training preprocessing, however, introduces some random\n  distortion of the image to improve accuracy.\n\n  Args:\n    image_buffer: scalar string Tensor representing the raw JPEG image buffer.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    num_channels: Integer depth of the image buffer for decoding.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    # For training, we want to randomize some of the distortions.\n    image = _decode_crop_and_flip(image_buffer, bbox, num_channels)\n    image = _resize_image(image, output_height, output_width)\n  else:\n    # For validation, we want to decode, resize, then just crop the middle.\n    image = tf.image.decode_jpeg(image_buffer, channels=num_channels)\n    image = _aspect_preserving_resize(image, resize_side_min)\n    image = _central_crop(image, output_height, output_width)\n\n  image.set_shape([output_height, output_width, num_channels])\n\n  return image\n\n\ndef process_record_dataset(dataset,\n                           is_training,\n                           batch_size,\n                           eval_image_size,\n                           normalize,\n                           parse_record_fn,\n                           shuffle_buffer=10000,\n                           num_epochs=1,\n                           dtype=tf.float32,\n                           datasets_num_private_threads=None,\n                           num_parallel_batches=1):\n  """"""Given a Dataset with raw records, return an iterator over the records.\n\n  Args:\n    dataset: A Dataset representing raw records\n    is_training: A boolean denoting whether the input is for training.\n    batch_size: The number of samples per batch.\n    shuffle_buffer: The buffer size to use when shuffling records. A larger\n      value results in better randomness, but smaller values reduce startup\n      time and use less memory.\n    parse_record_fn: A function that takes a raw record and returns the\n      corresponding (image, label) pair.\n    num_epochs: The number of epochs to repeat the dataset.\n    dtype: Data type to use for images/features.\n    datasets_num_private_threads: Number of threads for a private\n      threadpool created for all datasets computation.\n    num_parallel_batches: Number of parallel batches for tf.data.\n\n  Returns:\n    Dataset of (image, label) pairs ready for iteration.\n  """"""\n  # Defines a specific size thread pool for tf.data operations.\n  if datasets_num_private_threads:\n    options = tf.data.Options()\n    options.experimental_threading.private_threadpool_size = (\n        datasets_num_private_threads)\n    dataset = dataset.with_options(options)\n    tf.logging.info(\'datasets_num_private_threads: %s\',\n                    datasets_num_private_threads)\n\n  # Prefetches a batch at a time to smooth out the time taken to load input\n  # files for shuffling and processing.\n  dataset = dataset.prefetch(buffer_size=batch_size)\n  if is_training:\n    # Shuffles records before repeating to respect epoch boundaries.\n    dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n\n  # Repeats the dataset for the number of epochs to train.\n  dataset = dataset.repeat(num_epochs)\n\n  # Parses the raw records into images and labels.\n  dataset = dataset.apply(\n      tf.data.experimental.map_and_batch(\n          lambda value: parse_record_fn(value, is_training, dtype,\n                                        eval_image_size, normalize),\n          batch_size=batch_size,\n          num_parallel_batches=num_parallel_batches,\n          drop_remainder=False))\n\n  # Operations between the final prefetch and the get_next call to the iterator\n  # will happen synchronously during run time. We prefetch here again to\n  # background all of the above processing work and keep it out of the\n  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE\n  # allows DistributionStrategies to adjust how many batches to fetch based\n  # on how many devices are present.\n  dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)\n\n  return dataset\n\n\ndef _parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields (values are included as examples):\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n      \'image/class/label\': tf.io.FixedLenFeature([], dtype=tf.int64,\n                                                 default_value=-1),\n      \'image/class/text\': tf.io.FixedLenFeature([], dtype=tf.string,\n                                                default_value=\'\'),\n  }\n  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.io.parse_single_example(serialized=example_serialized,\n                                        features=feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox\n\n\ndef parse_record(raw_record, is_training, dtype, eval_image_size, normalize):\n  """"""Parses a record containing a training example of an image.\n\n  The input record is parsed into a label and image, and the image is passed\n  through preprocessing steps (cropping, flipping, and so on).\n\n  Args:\n    raw_record: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n    is_training: A boolean denoting whether the input is for training.\n    dtype: data type to use for images/features.\n\n  Returns:\n    Tuple with processed image tensor and one-hot-encoded label tensor.\n  """"""\n  image_buffer, label, bbox = _parse_example_proto(raw_record)\n\n  image = preprocess_image(\n      image_buffer=image_buffer,\n      bbox=bbox,\n      output_height=eval_image_size,\n      output_width=eval_image_size,\n      num_channels=3,\n      is_training=is_training,\n      resize_side_min=eval_image_size * 8 // 7)\n\n  if normalize == 0:\n    image = _mean_image_subtraction(image, _CHANNEL_MEANS0, 3)\n  elif normalize == 1:\n    image = _mean_image_subtraction(image, _CHANNEL_MEANS0, 3)\n    image = tf.reverse(image, axis=[-1])\n  elif normalize == 2:\n    image /= 255.\n    image -= 0.5\n    image *= 2.\n  elif normalize == 3:\n    image /= 255.\n    image = _mean_image_subtraction(image, _CHANNEL_MEANS3, 3, _CHANNEL_STDS3)\n  elif normalize == 4:\n    image = _mean_image_subtraction(image, _CHANNEL_MEANS4, 3)\n    image = tf.reverse(image, axis=[-1])\n  elif normalize == 5:\n    image /= 255.\n    image = _mean_image_subtraction(image, _CHANNEL_MEANS5, 3, _CHANNEL_STDS5)\n\n  image = tf.cast(image, dtype)\n  label -= 1\n\n  return image, label\n\n\ndef input_fn(is_training, data_dir, batch_size,\n             eval_image_size, normalize=0, num_epochs=1,\n             dtype=tf.float32, datasets_num_private_threads=None,\n             num_parallel_batches=1, parse_record_fn=parse_record):\n  """"""Input function which provides batches for train or eval.\n\n  Args:\n    is_training: A boolean denoting whether the input is for training.\n    batch_size: The number of samples per batch.\n    num_epochs: The number of epochs to repeat the dataset.\n    dtype: Data type to use for images/features\n    datasets_num_private_threads: Number of private threads for tf.data.\n    num_parallel_batches: Number of parallel batches for tf.data.\n    parse_record_fn: Function to use for parsing the records.\n\n  Returns:\n    A dataset that can be used for iteration.\n  """"""\n  if is_training:\n    filenames = [\n        os.path.join(data_dir, \'train-%05d-of-01024\' % i)\n        for i in range(1024)]\n  else:\n    filenames = [\n        os.path.join(data_dir, \'validation-%05d-of-00128\' % i)\n        for i in range(128)]\n\n  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\n  if is_training:\n    # Shuffle the input files\n    dataset = dataset.shuffle(buffer_size=_NUM_TRAIN_FILES)\n\n  # Convert to individual records.\n  # cycle_length = 10 means 10 files will be read and deserialized in parallel.\n  # This number is low enough to not cause too much contention on small systems\n  # but high enough to provide the benefits of parallelization. You may want\n  # to increase this number if you have a large number of CPU cores.\n  dataset = dataset.apply(tf.data.experimental.parallel_interleave(\n      tf.data.TFRecordDataset, cycle_length=48))\n\n  return process_record_dataset(\n      dataset=dataset,\n      is_training=is_training,\n      batch_size=batch_size,\n      eval_image_size=eval_image_size,\n      normalize=normalize,\n      parse_record_fn=parse_record_fn,\n      num_epochs=num_epochs,\n      dtype=dtype,\n      datasets_num_private_threads=datasets_num_private_threads,\n      num_parallel_batches=num_parallel_batches\n  )\n'"
examples/train_yolov2.py,8,"b'import time\nimport numpy as np\nimport tensorflow as tf\nimport tensornets as nets\n\nfrom tensornets.datasets import voc\n\ndata_dir = ""/home/taehoonlee/Data/VOCdevkit/VOC%d""\ntrains = voc.load_train([data_dir % 2007, data_dir % 2012],\n                        \'trainval\', batch_size=48)\n\n# Define a model\ninputs = tf.placeholder(tf.float32, [None, 416, 416, 3])\nis_training = tf.placeholder(tf.bool)\nmodel = nets.YOLOv2(inputs, nets.Darknet19, is_training=is_training)\n\n# Define an optimizer\nstep = tf.Variable(0, trainable=False)\nlr = tf.train.piecewise_constant(\n    step, [100, 180, 320, 570, 1000, 40000, 60000],\n    [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-4, 1e-5])\ntrain = tf.train.MomentumOptimizer(lr, 0.9).minimize(model.loss,\n                                                     global_step=step)\n\nwith tf.Session() as sess:\n\n    # Load Darknet19\n    sess.run(tf.global_variables_initializer())\n    sess.run(model.stem.pretrained())\n\n    # Note that there are 16551 images (5011 in VOC07 + 11540 in VOC12).\n    # When the mini-batch size is 48, 1 epoch consists of 344(=16551/48) steps.\n    # Thus, 233 epochs will cover 80152 steps.\n    losses = []\n    for i in range(233):\n\n        # Iterate on VOC07+12 trainval once\n        _t = time.time()\n        for (imgs, metas) in trains:\n            # `trains` returns None when it covers the full batch once\n            if imgs is None:\n                break\n            metas.insert(0, model.preprocess(imgs))  # for `inputs`\n            metas.append(True)  # for `is_training`\n            outs = sess.run([train, model.loss],\n                            dict(zip(model.inputs, metas)))\n            losses.append(outs[1])\n\n        # Report step, learning rate, loss, weight decay, runtime\n        print(\'***** %d %.5f %.5f %.5f %.5f *****\' %\n              (sess.run(step), sess.run(lr),\n               losses[-1], sess.run(tf.losses.get_regularization_loss()),\n               time.time() - _t))\n\n        # Report with VOC07 test\n        results = []\n        tests = voc.load(data_dir % 2007, \'test\', total_num=100)\n        for (img, scale) in tests:\n            outs = sess.run(model, {inputs: model.preprocess(img),\n                                    is_training: False})\n            results.append(model.get_boxes(outs, img.shape[1:3]))\n        print(voc.evaluate(results, data_dir % 2007, \'test\'))\n'"
tensornets/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .inceptions import GoogLeNet\nfrom .inceptions import Inception1\nfrom .inceptions import Inception2\nfrom .inceptions import Inception3\nfrom .inceptions import Inception4\nfrom .inceptions import InceptionResNet2\n\nfrom .resnets import ResNet50\nfrom .resnets import ResNet101\nfrom .resnets import ResNet152\nfrom .resnets import ResNet50v2\nfrom .resnets import ResNet101v2\nfrom .resnets import ResNet152v2\nfrom .resnets import ResNet200v2\nfrom .resnets import ResNeXt50\nfrom .resnets import ResNeXt101\nfrom .resnets import ResNeXt50c32\nfrom .resnets import ResNeXt101c32\nfrom .resnets import ResNeXt101c64\nfrom .resnets import WideResNet50\n\nfrom .nasnets import NASNetAlarge\nfrom .nasnets import NASNetAmobile\nfrom .nasnets import PNASNetlarge\n\nfrom .vggs import VGG16\nfrom .vggs import VGG19\n\nfrom .densenets import DenseNet121\nfrom .densenets import DenseNet169\nfrom .densenets import DenseNet201\n\nfrom .mobilenets import MobileNet25\nfrom .mobilenets import MobileNet50\nfrom .mobilenets import MobileNet75\nfrom .mobilenets import MobileNet100\n\nfrom .mobilenets import MobileNet35v2\nfrom .mobilenets import MobileNet50v2\nfrom .mobilenets import MobileNet75v2\nfrom .mobilenets import MobileNet100v2\nfrom .mobilenets import MobileNet130v2\nfrom .mobilenets import MobileNet140v2\n\nfrom .mobilenets import MobileNet75v3\nfrom .mobilenets import MobileNet100v3\nfrom .mobilenets import MobileNet75v3large\nfrom .mobilenets import MobileNet100v3large\nfrom .mobilenets import MobileNet100v3largemini\nfrom .mobilenets import MobileNet75v3small\nfrom .mobilenets import MobileNet100v3small\nfrom .mobilenets import MobileNet100v3smallmini\n\nfrom .efficientnets import EfficientNetB0\nfrom .efficientnets import EfficientNetB1\nfrom .efficientnets import EfficientNetB2\nfrom .efficientnets import EfficientNetB3\nfrom .efficientnets import EfficientNetB4\nfrom .efficientnets import EfficientNetB5\nfrom .efficientnets import EfficientNetB6\nfrom .efficientnets import EfficientNetB7\n\nfrom .squeezenets import SqueezeNet\n\nfrom .capsulenets import CapsuleNet\n\nfrom .wavenets import WaveNet\n\nfrom .references import YOLOv3COCO\nfrom .references import YOLOv3VOC\nfrom .references import YOLOv2COCO\nfrom .references import YOLOv2VOC\nfrom .references import TinyYOLOv2COCO\nfrom .references import TinyYOLOv2VOC\n\nfrom .references import FasterRCNN_ZF_VOC\nfrom .references import FasterRCNN_VGG16_VOC\n\nfrom .darknets import Darknet19\nfrom .darknets import TinyDarknet19\n\nfrom .zf import ZF\n\nfrom .detections import YOLOv2\nfrom .detections import TinyYOLOv2\nfrom .detections import FasterRCNN\n\nfrom .preprocess import preprocess\nfrom .pretrained import assign as pretrained\n\nfrom .utils import *\n\n__version__ = '0.4.6'\n\nremove_utils(__name__, ['init'])\n"""
tensornets/capsulenets.py,11,"b'""""""Collection of CapsuleNet variants\n\nThe reference paper:\n\n - Dynamic Routing Between Capsules\n - Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton\n - https://arxiv.org/abs/1710.09829\n\nThe reference implementations:\n\n1. TensorFlow CapsNet\n - https://github.com/naturomics/CapsNet-Tensorflow\n2. Keras CapsNet\n - https://github.com/XifengGuo/CapsNet-Keras\n""""""\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import convrelu as conv\n\nfrom .ops import *\nfrom .utils import ops_to_outputs\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([batch_norm], {\'scale\': True, \'is_training\': is_training,\n                            \'epsilon\': 1e-5, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'VALID\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'})]\n\n\n@ops_to_outputs\ndef squash(x, epsilon=1e-9, name=None):\n    norm = tf.reduce_sum(tf.square(x), axis=-1, keep_dims=True)\n    scale = norm / (1. + norm) / tf.sqrt(norm + epsilon)\n    return tf.multiply(x, scale, name=name)\n\n\n@var_scope(\'primary\')\ndef primary(x, filters, length, kernel_size, stride, scope=None):\n    x = conv(x, filters * length, kernel_size, stride=stride, scope=\'conv\')\n    pixels = np.prod(x.shape[1:-1].as_list())\n    x = reshape(x, (-1, pixels * filters, length), name=\'out\')\n    return x\n\n\n@var_scope(\'digit\')\ndef digit(x, filters, length, iters=3, scope=None):\n    filters0 = int(x.shape[1]) if tf_later_than(\'2\') else x.shape[1].value\n    length0 = int(x.shape[2]) if tf_later_than(\'2\') else x.shape[2].value\n\n    # fully-connected weights between capsules: [1152, 8, 10 * 16]\n    w = tf.get_variable(\'weights\', shape=(filters0, length0, filters * length),\n                        dtype=tf.float32)\n\n    # coupling logits: [1152, 10]\n    b = tf.zeros((filters0, filters))\n\n    # prediction vectors: [None, 1152, 10, 16]\n    uhat = tf.scan(lambda a, b: tf.matmul(b, w), tf.expand_dims(x, 2),\n                   initializer=tf.zeros([filters0, 1, filters * length]))\n    uhat = reshape(uhat, (-1, filters0, filters, length), name=\'predvec\')\n\n    for r in range(iters):\n        with tf.variable_scope(""iter%d"" % r):\n            # coupling coefficients: [1152, 10]\n            c = softmax(b, name=\'softmax\')\n            # activity vector: [None, 10, 16]\n            v = squash(tf.reduce_sum(uhat * tf.expand_dims(c, -1), axis=1),\n                       name=\'out\')\n            # agreement: [None, 1152, 10]\n            a = reduce_sum(tf.multiply(uhat, tf.expand_dims(v, 1)), axis=-1,\n                           name=\'agreement\')\n            # updates coupling logits\n            b = b + reduce_sum(a, axis=0, name=\'delta\')\n    return v\n\n\n@var_scope(\'capsulenet\')\n@set_args(__args__)\ndef capsulenet_mnist(x, is_training=False, classes=10, scope=None, reuse=None):\n    x = conv(x, 256, 9, stride=1, scope=\'conv1\')\n    x = primary(x, 32, 8, 9, stride=2, scope=\'primary\')\n    x = digit(x, 10, 16, scope=\'digit\')\n    return x\n\n\n# Simple alias.\nCapsuleNet = capsulenet_mnist\n'"
tensornets/darknets.py,0,"b'""""""Darknet19 embedded in YOLO\n\nThe reference paper:\n\n - YOLO9000: Better, Faster, Stronger, CVPR 2017 (Best Paper Honorable Mention)\n - Joseph Redmon, Ali Farhadi\n - https://arxiv.org/abs/1612.08242\n\nThe reference implementation:\n\n1. Darknet\n - https://pjreddie.com/darknet/yolo/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import batch_norm\nfrom .layers import bias_add\nfrom .layers import conv2d\nfrom .layers import darkconv as conv\nfrom .layers import fc\nfrom .layers import max_pool2d as pool\n\nfrom .ops import *\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([batch_norm], {\'is_training\': is_training}),\n            ([bias_add, conv2d], {}),\n            ([pool], {\'padding\': \'SAME\'})]\n\n\n@var_scope(\'stack\')\ndef _stack(x, filters, blocks, scope=None):\n    for i in range(1, blocks+1):\n        if i % 2 > 0:\n            x = conv(x, filters, 3, scope=str(i))\n        else:\n            x = conv(x, filters // 2, 1, scope=str(i))\n    return x\n\n\n@var_scope(\'darknet19\')\n@set_args(__args__)\ndef darknet19(x, is_training=False, classes=1000,\n              stem=False, scope=None, reuse=None):\n    x = _stack(x, 32, 1, scope=\'conv1\')\n    x = pool(x, 2, stride=2, scope=\'pool1\')\n    x = _stack(x, 64, 1, scope=\'conv2\')\n    x = pool(x, 2, stride=2, scope=\'pool2\')\n    x = _stack(x, 128, 3, scope=\'conv3\')\n    x = pool(x, 2, stride=2, scope=\'pool3\')\n    x = _stack(x, 256, 3, scope=\'conv4\')\n    x = pool(x, 2, stride=2, scope=\'pool4\')\n    x = _stack(x, 512, 5, scope=\'conv5\')\n    x = pool(x, 2, stride=2, scope=\'pool5\')\n    x = _stack(x, 1024, 5, scope=\'conv6\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'tinydarknet19\')\n@set_args(__args__)\ndef tinydarknet19(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    x = conv(x, 16, 3, scope=\'conv1\')\n    x = pool(x, 2, stride=2, scope=\'pool1\')\n    x = conv(x, 32, 3, scope=\'conv2\')\n    x = pool(x, 2, stride=2, scope=\'pool2\')\n    x = conv(x, 64, 3, scope=\'conv3\')\n    x = pool(x, 2, stride=2, scope=\'pool3\')\n    x = conv(x, 128, 3, scope=\'conv4\')\n    x = pool(x, 2, stride=2, scope=\'pool4\')\n    x = conv(x, 256, 3, scope=\'conv5\')\n    x = pool(x, 2, stride=2, scope=\'pool5\')\n    x = conv(x, 512, 3, scope=\'conv6\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n# Simple alias.\nDarknet19 = darknet19\nTinyDarknet19 = tinydarknet19\n'"
tensornets/densenets.py,0,"b'""""""Collection of DenseNet variants\n\nThe reference paper:\n\n - Densely Connected Convolutional Networks, CVPR 2017 (Best Paper Award)\n - Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten\n - https://arxiv.org/abs/1608.06993\n\nThe reference implementation:\n\n1. Torch DenseNets\n - https://github.com/liuzhuang13/DenseNet/blob/master/models/densenet.lua\n""""""\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom .layers import avg_pool2d\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import convbnrelu as conv\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([avg_pool2d, max_pool2d], {\'scope\': \'pool\'}),\n            ([batch_norm], {\'scale\': True, \'is_training\': is_training,\n                            \'epsilon\': 1e-5, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'VALID\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'})]\n\n\ndef densenet(x, blocks, is_training, classes, stem, scope=None, reuse=None):\n    x = pad(x, pad_info(7), name=\'conv1/pad\')\n    x = conv(x, 64, 7, stride=2, scope=\'conv1\')\n    x = pad(x, pad_info(3), name=\'pool1/pad\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool1\')\n\n    x = dense(x, blocks[0], scope=\'conv2\')\n    x = transition(x, scope=\'pool2\')\n    x = dense(x, blocks[1], scope=\'conv3\')\n    x = transition(x, scope=\'pool3\')\n    x = dense(x, blocks[2], scope=\'conv4\')\n    x = transition(x, scope=\'pool4\')\n    x = dense(x, blocks[3], scope=\'conv5\')\n\n    x = batch_norm(x)\n    x = relu(x)\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'densenet121\')\n@set_args(__args__)\ndef densenet121(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return densenet(x, [6, 12, 24, 16], is_training, classes,\n                    stem, scope, reuse)\n\n\n@var_scope(\'densenet169\')\n@set_args(__args__)\ndef densenet169(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return densenet(x, [6, 12, 32, 32], is_training, classes,\n                    stem, scope, reuse)\n\n\n@var_scope(\'densenet201\')\n@set_args(__args__)\ndef densenet201(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return densenet(x, [6, 12, 48, 32], is_training, classes,\n                    stem, scope, reuse)\n\n\n@var_scope(\'dense\')\ndef dense(x, blocks, scope=None):\n    for i in range(blocks):\n        x = block(x, scope=""block%d"" % (i + 1))\n    return x\n\n\n@var_scope(\'transition\')\ndef transition(x, reduction=0.5, scope=None):\n    x = batch_norm(x)\n    x = relu(x)\n    infilters = int(x.shape[-1]) if tf_later_than(\'2\') else x.shape[-1].value\n    x = conv2d(x, int(infilters * reduction), 1, stride=1)\n    x = avg_pool2d(x, 2, stride=2, scope=\'pool\')\n    return x\n\n\n@var_scope(\'block\')\ndef block(x, growth_rate=32, scope=None):\n    x1 = batch_norm(x)\n    x1 = relu(x1)\n    x1 = conv(x1, 4 * growth_rate, 1, stride=1, scope=\'1\')\n    x1 = conv2d(x1, growth_rate, 3, stride=1, padding=\'SAME\', scope=\'2/conv\')\n    x = concat([x, x1], axis=3, name=\'out\')\n    return x\n\n\n# Simple alias.\nDenseNet121 = densenet121\nDenseNet169 = densenet169\nDenseNet201 = densenet201\n'"
tensornets/detections.py,5,"b'""""""Collection of generic object detection models\n\nThe reference papers:\n\n1. YOLOv2\n - YOLO9000: Better, Faster, Stronger, CVPR 2017 (Best Paper Honorable Mention)\n - Joseph Redmon, Ali Farhadi\n - https://arxiv.org/abs/1612.08242\n2. Faster R-CNN\n - Faster R-CNN: Towards Real-Time Object Detection\n   with Region Proposal Networks, NIPS 2015\n - Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\n - https://arxiv.org/abs/1506.01497\n\nThe reference implementations:\n\n1. Darknet\n - https://pjreddie.com/darknet/yolo/\n2. darkflow\n - https://github.com/thtrieu/darkflow\n3. Caffe and Python utils\n - https://github.com/rbgirshick/py-faster-rcnn\n4. RoI pooling in TensorFlow\n - https://github.com/deepsense-ai/roi-pooling\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import batch_norm\nfrom .layers import bias_add\nfrom .layers import conv2d\nfrom .layers import darkconv\nfrom .layers import dropout\nfrom .layers import flatten\nfrom .layers import fc\nfrom .layers import max_pool2d\n\nfrom .ops import *\nfrom .utils import remove_head\nfrom .utils import set_args\nfrom .utils import var_scope\n\nfrom .references.yolos import get_v2_boxes as yolo_boxes\nfrom .references.yolos import opts\nfrom .references.yolos import v2_inputs\nfrom .references.yolos import v2_loss\nfrom .references.rcnns import get_boxes as rcnn_boxes\nfrom .references.rcnns import roi_pool2d\nfrom .references.rcnns import rp_net\n\n\ndef __args_yolo__(is_training):\n    return [([batch_norm], {\'is_training\': is_training}),\n            ([bias_add, conv2d], {}),\n            ([max_pool2d], {\'padding\': \'SAME\'})]\n\n\ndef __args_rcnn__(is_training):\n    return [([conv2d], {\'activation_fn\': None, \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'})]\n\n\n@var_scope(\'genYOLOv2\')\n@set_args(__args_yolo__)\ndef yolov2(x, stem_fn, stem_out=None, is_training=False, classes=20,\n           scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'yolov2\' + data_name(classes))\n    stem = x = stem_fn(x, is_training, stem=True, scope=\'stem\')\n    p = x.p\n\n    if stem_out is not None:\n        stem = x = remove_head(x, stem_out)\n\n    x = darkconv(x, 1024, 3, scope=\'conv7\')\n    x = darkconv(x, 1024, 3, scope=\'conv8\')\n\n    p = darkconv(p, 64, 1, scope=\'conv5a\')\n    p = local_flatten(p, 2, name=\'flat5a\')\n\n    x = concat([p, x], axis=3, name=\'concat\')\n    x = darkconv(x, 1024, 3, scope=\'conv9\')\n    x = darkconv(x, (classes + 5) * 5, 1, onlyconv=True, scope=\'linear\')\n    x.aliases = []\n\n    def get_boxes(*args, **kwargs):\n        return yolo_boxes(opt, *args, **kwargs)\n    x.get_boxes = get_boxes\n    x.stem = stem\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\ndef data_name(classes):\n    return \'voc\' if classes == 20 else \'\'\n\n\n@var_scope(\'genTinyYOLOv2\')\n@set_args(__args_yolo__)\ndef tinyyolov2(x, stem_fn, stem_out=None, is_training=False, classes=20,\n               scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'tinyyolov2\' + data_name(classes))\n    stem = x = stem_fn(x, is_training, stem=True, scope=\'stem\')\n\n    if stem_out is not None:\n        stem = x = remove_head(x, stem_out)\n\n    x = max_pool2d(x, 2, stride=1, scope=\'pool6\')\n    x = darkconv(x, 1024, 3, scope=\'conv7\')\n    x = darkconv(x, 1024 if classes == 20 else 512, 3, scope=\'conv8\')\n    x = darkconv(x, (classes + 5) * 5, 1, onlyconv=True, scope=\'linear\')\n    x.aliases = []\n\n    def get_boxes(*args, **kwargs):\n        return yolo_boxes(opt, *args, **kwargs)\n    x.get_boxes = get_boxes\n    x.stem = stem\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\n@var_scope(\'genFasterRCNN\')\n@set_args(__args_rcnn__)\ndef fasterrcnn(x, stem_fn, stem_out=None, is_training=False, classes=21,\n               scope=None, reuse=None):\n    def roi_pool_fn(x, filters, kernel_size):\n        rois = rp_net(x, filters, height, width, scales)\n        x = roi_pool2d(x, kernel_size, rois)\n        return x, rois[0] / scales\n\n    scales = tf.placeholder(tf.float32, [None])\n    height = tf.cast(tf.shape(x)[1], dtype=tf.float32)\n    width = tf.cast(tf.shape(x)[2], dtype=tf.float32)\n\n    stem = x = stem_fn(x, is_training, stem=True, scope=\'stem\')\n\n    if stem_out is not None:\n        stem = x = remove_head(x, stem_out)\n\n    if \'zf\' in stem.model_name:\n        x, rois = roi_pool_fn(x, 256, 6)\n    else:\n        x, rois = roi_pool_fn(x, 512, 7)\n\n    x = flatten(x)\n    x = fc(x, 4096, scope=\'fc6\')\n    x = relu(x, name=\'relu6\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop6\')\n    x = fc(x, 4096, scope=\'fc7\')\n    x = relu(x, name=\'relu7\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop7\')\n    x = concat([softmax(fc(x, classes, scope=\'logits\'), name=\'probs\'),\n                fc(x, 4 * classes, scope=\'boxes\'),\n                rois], axis=1, name=\'out\')\n    x.get_boxes = rcnn_boxes\n    x.scales = scales\n    x.stem = stem\n    return x\n\n\n# Simple alias.\nYOLOv2 = yolov2\nTinyYOLOv2 = tinyyolov2\nFasterRCNN = fasterrcnn\n'"
tensornets/efficientnets.py,3,"b'""""""Collection of EfficientNet variants\n\nThe reference paper:\n\n - EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML 2019\n - Mingxing Tan, Quoc V. Le\n - https://arxiv.org/abs/1905.11946\n\nThe reference implementations:\n\n1. Keras\n - https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py\n2. TF TPU\n - https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport math\nimport tensorflow as tf\n\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import convbn\nfrom .layers import convbnswish as conv\nfrom .layers import dropout\nfrom .layers import fc\nfrom .layers import sconv2d\nfrom .layers import sconvbnswish as sconv\n\nfrom .ops import *\nfrom .ops import _swish\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([batch_norm], {\'decay\': 0.99, \'scale\': True,\n                            \'is_training\': is_training, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training, \'scope\': \'dropout\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([sconv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                         \'biases_initializer\': None, \'scope\': \'sconv\'})]\n\n\ndef blocks_args():\n    return [\n        {\'blocks\': 1, \'filters_in\': 32, \'filters_out\': 16, \'kernel_size\': 3,\n         \'stride\': 1, \'expand_ratio\': 1, \'se_ratio\': 0.25},\n        {\'blocks\': 2, \'filters_in\': 16, \'filters_out\': 24, \'kernel_size\': 3,\n         \'stride\': 2, \'expand_ratio\': 6, \'se_ratio\': 0.25},\n        {\'blocks\': 2, \'filters_in\': 24, \'filters_out\': 40, \'kernel_size\': 5,\n         \'stride\': 2, \'expand_ratio\': 6, \'se_ratio\': 0.25},\n        {\'blocks\': 3, \'filters_in\': 40, \'filters_out\': 80, \'kernel_size\': 3,\n         \'stride\': 2, \'expand_ratio\': 6, \'se_ratio\': 0.25},\n        {\'blocks\': 3, \'filters_in\': 80, \'filters_out\': 112, \'kernel_size\': 5,\n         \'stride\': 1, \'expand_ratio\': 6, \'se_ratio\': 0.25},\n        {\'blocks\': 4, \'filters_in\': 112, \'filters_out\': 192, \'kernel_size\': 5,\n         \'stride\': 2, \'expand_ratio\': 6, \'se_ratio\': 0.25},\n        {\'blocks\': 1, \'filters_in\': 192, \'filters_out\': 320, \'kernel_size\': 3,\n         \'stride\': 1, \'expand_ratio\': 6, \'se_ratio\': 0.25}\n    ]\n\n\ndef efficientnet(x, width_coefficient, depth_coefficient,\n                 default_size, is_training, classes, stem,\n                 keep_prob=0.8, drop_rate=0.2, width_divisor=8,\n                 scope=None, reuse=None):\n    def width(w, coefficient=width_coefficient, divisor=width_divisor):\n        w *= coefficient\n        new_w = max(divisor, int(w + divisor / 2) // divisor * divisor)\n        if new_w < 0.9 * w:\n            new_w += divisor\n        return int(new_w)\n\n    def depth(d, coefficient=depth_coefficient):\n        return int(math.ceil(d * coefficient))\n\n    b = 0\n    x = conv(x, width(32), 3, stride=2, scope=\'stem\')\n    blocks_total = float(sum(args[\'blocks\'] for args in blocks_args()))\n    for args in blocks_args():\n        filters_in = width(args[\'filters_in\'])\n        filters_out = width(args[\'filters_out\'])\n        for j in range(depth(args[\'blocks\'])):\n            x = block(x, filters_in if j == 0 else filters_out, filters_out,\n                      args[\'kernel_size\'], 1 if j > 0 else args[\'stride\'],\n                      args[\'expand_ratio\'], args[\'se_ratio\'],\n                      drop_rate * b / blocks_total,\n                      scope=""block{}"".format(b))\n            b += 1\n    x = conv(x, width(1280), 1, scope=\'head\')\n    if stem: return x\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=keep_prob, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'se\')\ndef se(i, filters_se, filters_out, scope=None):\n    x = reduce_mean(i, [1, 2], keepdims=True, name=\'squeeze\')\n    x = conv2d(x, filters_se, 1, activation_fn=_swish,\n               biases_initializer=tf.zeros_initializer(), scope=\'reduce\')\n    x = conv2d(x, filters_out, 1, activation_fn=tf.sigmoid,\n               biases_initializer=tf.zeros_initializer(), scope=\'expand\')\n    x = multiply(i, x, name=\'excite\')\n    return x\n\n\n@var_scope(\'block\')\ndef block(i, filters_in=32, filters_out=16, kernel_size=3, stride=1,\n          expand_ratio=1, se_ratio=0., drop_rate=0., scope=None):\n    filters = filters_in * expand_ratio\n    x = conv(i, filters, 1, scope=\'econv\') if expand_ratio != 1 else i\n    x = sconv(x, None, kernel_size, 1, stride=stride, scope=\'sconv\')\n    if 0 < se_ratio <= 1:\n        x = se(x, max(1, int(filters_in * se_ratio)), filters, scope=\'se\')\n    x = convbn(x, filters_out, 1, scope=\'pconv\')\n    if (stride == 1) and (filters_in == filters_out):\n        if drop_rate > 0:\n            x = dropout(x, keep_prob=1 - drop_rate, scope=\'dropout\')\n        x = add(i, x, name=\'add\')\n    return x\n\n\n@var_scope(\'efficientnetb0\')\n@set_args(__args__)\ndef efficientnetb0(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.0, 1.0, 224, is_training, classes, stem,\n                        keep_prob=0.8, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb1\')\n@set_args(__args__)\ndef efficientnetb1(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.0, 1.1, 240, is_training, classes, stem,\n                        keep_prob=0.8, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb2\')\n@set_args(__args__)\ndef efficientnetb2(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.1, 1.2, 260, is_training, classes, stem,\n                        keep_prob=0.7, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb3\')\n@set_args(__args__)\ndef efficientnetb3(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.2, 1.4, 300, is_training, classes, stem,\n                        keep_prob=0.7, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb4\')\n@set_args(__args__)\ndef efficientnetb4(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.4, 1.8, 380, is_training, classes, stem,\n                        keep_prob=0.6, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb5\')\n@set_args(__args__)\ndef efficientnetb5(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.6, 2.2, 456, is_training, classes, stem,\n                        keep_prob=0.6, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb6\')\n@set_args(__args__)\ndef efficientnetb6(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 1.8, 2.6, 528, is_training, classes, stem,\n                        keep_prob=0.5, scope=scope, reuse=reuse)\n\n\n@var_scope(\'efficientnetb7\')\n@set_args(__args__)\ndef efficientnetb7(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return efficientnet(x, 2.0, 3.1, 600, is_training, classes, stem,\n                        keep_prob=0.5, scope=scope, reuse=reuse)\n\n\n# Simple alias.\nEfficientNetB0 = efficientnetb0\nEfficientNetB1 = efficientnetb1\nEfficientNetB2 = efficientnetb2\nEfficientNetB3 = efficientnetb3\nEfficientNetB4 = efficientnetb4\nEfficientNetB5 = efficientnetb5\nEfficientNetB6 = efficientnetb6\nEfficientNetB7 = efficientnetb7\n'"
tensornets/inceptions.py,5,"b'""""""Collection of Inception variants\n\nThe reference papers:\n\n1. GoogLeNet, Inception (a.k.a. v1)\n - Going Deeper with Convolutions, CVPR 2015\n - Christian Szegedy et al.\n - https://arxiv.org/abs/1409.4842\n2. BN-Inception (a.k.a. v2)\n - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ICML 2015\n - Sergey Ioffe, Christian Szegedy\n - https://arxiv.org/abs/1502.03167\n3. Inception3\n - Rethinking the Inception Architecture for Computer Vision, CVPR 2016\n - Christian Szegedy et al.\n - https://arxiv.org/abs/1512.00567\n4. Inception4, InceptionResNet1, InceptionResNet2\n - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, AAAI 2017\n - Christian Szegedy et al.\n - https://arxiv.org/abs/1602.07261\n\nThe reference implementations:\n\n1. (initially and mainly for v3) Keras\n - https://github.com/keras-team/keras/blob/master/keras/applications/inception_v3.py\n2. (mainly for v1,2,4,resnetv2) TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_{v1,v2,v3,v4,resnet_v2}.py\n3. (to reproduce the original results) BAIR Caffe Model Zoo\n - https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n""""""\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom .layers import avg_pool2d\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import dropout\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import sconv2d\nfrom .layers import convrelu0 as conv0\nfrom .layers import convbnrelu as conv\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([avg_pool2d, max_pool2d], {\'stride\': 1, \'padding\': \'SAME\',\n                                        \'scope\': \'pool\'}),\n            ([batch_norm], {\'is_training\': is_training, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training, \'scope\': \'dropout\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([sconv2d], {\'padding\': \'SAME\', \'scope\': \'sconv\'})]\n\n\n@var_scope(\'inception1\')\n@set_args(__args__)\ndef inception1(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    x = pad(x, pad_info(7), name=\'pad\')\n    x = conv0(x, 64, 7, stride=2, padding=\'VALID\', scope=\'block1\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool1\')\n    x = lrn(x, depth_radius=2, alpha=0.00002, beta=0.75, name=\'lrn1\')\n\n    x = conv0(x, 64, 1, scope=\'block2/3x3/r\')\n    x = conv0(x, 192, 3, scope=\'block2/3x3/1\')\n    x = lrn(x, depth_radius=2, alpha=0.00002, beta=0.75, name=\'lrn2\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool2\')\n\n    x = inception(x, [64, [96, 128], [16, 32], 32], scope=\'block3a\')\n    x = inception(x, [128, [128, 192], [32, 96], 64], scope=\'block3b\')\n\n    x = max_pool2d(x, 3, stride=2, scope=\'pool3\')\n\n    x = inception(x, [192, [96, 208], [16, 48], 64], scope=\'block4a\')\n    x = inception(x, [160, [112, 224], [24, 64], 64], scope=\'block4b\')\n    x = inception(x, [128, [128, 256], [24, 64], 64], scope=\'block4c\')\n    x = inception(x, [112, [144, 288], [32, 64], 64], scope=\'block4d\')\n    x = inception(x, [256, [160, 320], [32, 128], 128], scope=\'block4e\')\n\n    x = max_pool2d(x, 3, stride=2, scope=\'pool4\')\n\n    x = inception(x, [256, [160, 320], [32, 128], 128], scope=\'block5a\')\n    x = inception(x, [384, [192, 384], [48, 128], 128], scope=\'block5b\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.8, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'inception2\')\n@set_args(__args__)\ndef inception2(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    x = sconv2d(x, 64, 7, stride=2, depth_multiplier=8, scope=\'block1\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool1\')\n\n    x = conv(x, 64, 1, scope=\'block2/1\')\n    x = conv(x, 192, 3, scope=\'block2/2\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool2\')\n\n    x = inceptionA(x, [64, [64, 64], [64, 96], 32], scope=\'block3a\')\n    x = inceptionA(x, [64, [64, 96], [64, 96], 64], scope=\'block3b\')\n\n    x = reductionA(x, [[128, 160], [64, 96, 96]], padding=\'SAME\',\n                   scope=\'block3c\')\n\n    x = inceptionA(x, [224, [64, 96], [96, 128], 128], scope=\'block4a\')\n    x = inceptionA(x, [192, [96, 128], [96, 128], 128], scope=\'block4b\')\n    x = inceptionA(x, [160, [128, 160], [128, 160], 96], scope=\'block4c\')\n    x = inceptionA(x, [96, [128, 192], [160, 192], 96], scope=\'block4d\')\n\n    x = reductionA(x, [[128, 192], [192, 256, 256]], padding=\'SAME\',\n                   scope=\'block4e\')\n\n    x = inceptionA(x, [352, [192, 320], [160, 224], 128], scope=\'block5a\')\n    x = inceptionA(x, [352, [192, 320], [192, 224], 128],\n                   pool_fn=max_pool2d, scope=\'block5b\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.8, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'inception3\')\n@set_args(__args__)\ndef inception3(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    x = conv(x, 32, 3, stride=2, padding=\'VALID\', scope=\'block1a\')\n    x = conv(x, 32, 3, padding=\'VALID\', scope=\'block2a\')\n    x = conv(x, 64, 3, scope=\'block2b\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool3a\')\n\n    x = conv(x, 80, 1, padding=\'VALID\', scope=\'block3b\')\n    x = conv(x, 192, 3, padding=\'VALID\', scope=\'block4a\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool5a\')\n\n    x = inceptionA(x, [64, [48, 64], [64, 96], 32], fs=5, scope=\'block5b\')\n    x = inceptionA(x, [64, [48, 64], [64, 96], 64], fs=5, scope=\'block5c\')\n    x = inceptionA(x, [64, [48, 64], [64, 96], 64], fs=5, scope=\'block5d\')\n\n    x = reductionA(x, [384, [64, 96, 96]], scope=\'block6a\')\n\n    x = inceptionB(x, [192, [128, 128, 192], [128, 128], 192], scope=\'block6b\')\n    x = inceptionB(x, [192, [160, 160, 192], [160, 160], 192], scope=\'block6c\')\n    x = inceptionB(x, [192, [160, 160, 192], [160, 160], 192], scope=\'block6d\')\n    x = inceptionB(x, [192, [192, 192, 192], [192, 192], 192], scope=\'block6e\')\n\n    x = reductionB(x, [[192, 320], [192, 192]], scope=\'block7a\')\n\n    x = inceptionC(x, [320, [384] * 3, [448, 384], 192], scope=\'block7b\')\n    x = inceptionC(x, [320, [384] * 3, [448, 384], 192], scope=\'block7c\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'inception4\')\n@set_args(__args__)\ndef inception4(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    x = stemA(x)\n    for i in range(4):\n        x = inceptionA(x, [96, [64, 96], [64, 96], 96],\n                       scope=""block5%c"" % (98 + i))\n    x = reductionA(x, [384, [192, 224, 256]], scope=\'block6a\')\n\n    for i in range(7):\n        x = inceptionB(x, [384, [192, 224, 256], [192, 224], 128],\n                       scope=""block6%c"" % (98 + i))\n    x = reductionB(x, [[192, 192], [256, 320]], scope=\'block7a\')\n\n    for i in range(3):\n        x = inceptionC(x, [256, [384, 256, 256], [384, 448, 512, 256], 256],\n                       scope=""block7%c"" % (98 + i))\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.8, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\ndef inceptionresnet(x, stem_fn, A, B, C, is_training, classes, stem,\n                    scope=None, reuse=None):\n    x = stem_fn(x)\n    for i in range(A[\'blocks\']):\n        x = inceptionRA(x, A[\'filters\'], scale=0.17,\n                        scope=""block5%c"" % (99 + i))\n    x = reductionA(x, A[\'reduction\'], scope=\'block6a\')\n\n    for i in range(B[\'blocks\']):\n        x = inceptionRB(x, B[\'filters\'], k=7, scale=0.1,\n                        scope=""block6%c"" % (98 + i))\n    x = reductionRB(x, B[\'reduction\'], scope=\'block7a\')\n\n    for i in range(C[\'blocks\']):\n        x = inceptionRB(x, C[\'filters\'], k=3, scale=0.2,\n                        scope=""block7%c"" % (98 + i))\n\n    if C[\'blocks\'] == 9:\n        x = inceptionRB(x, C[\'filters\'], k=3, activation_fn=None,\n                        scope=""block7%c"" % (98 + C[\'blocks\']))\n        x = conv(x, 1536, 1, scope=\'conv\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.8, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'inceptionresnet1\')\n@set_args(__args__)\ndef inceptionresnet1(x, is_training=False, classes=1000,\n                     stem=False, scope=None, reuse=None):\n    return inceptionresnet(\n        x, stemB,\n        {\'blocks\': 5, \'filters\': [32, 32, [32, 32, 32], 256],\n         \'reduction\': [384, [192, 192, 256]]},\n        {\'blocks\': 10, \'filters\': [128, [128, 128, 128], 896],\n         \'reduction\': [256, 384, 256, 256]},\n        {\'blocks\': 5, \'filters\': [192, [192, 192, 192], 1792]},\n        is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'inceptionresnet2\')\n@set_args(__args__)\ndef inceptionresnet2(x, is_training=False, classes=1000,\n                     stem=False, scope=None, reuse=None):\n    return inceptionresnet(\n        x, stemA,\n        {\'blocks\': 10, \'filters\': [32, 32, [32, 48, 64], 384],\n         \'reduction\': [384, [256, 256, 384]]},\n        {\'blocks\': 20, \'filters\': [192, [128, 160, 192], 1154],\n         \'reduction\': [256, 384, 288, 320]},\n        {\'blocks\': 10, \'filters\': [192, [192, 224, 256], 2048]},\n        is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'inceptionresnet2_tfslim\')\n@set_args(__args__)\ndef inceptionresnetS(x, is_training=False, classes=1000,\n                     stem=False, scope=None, reuse=None):\n    return inceptionresnet(\n        x, stemS,\n        {\'blocks\': 10, \'filters\': [32, 32, [32, 48, 64], 320],\n         \'reduction\': [384, [256, 256, 384]]},\n        {\'blocks\': 20, \'filters\': [192, [128, 160, 192], 1088],\n         \'reduction\': [256, 384, 288, 320]},\n        {\'blocks\': 9, \'filters\': [192, [192, 224, 256], 2080]},\n        is_training, classes, stem, scope, reuse)\n\n\ndef stemA(x):\n    """"""Stem for Inception-v4,-ResNet-v2 (Fig. 3 in the v4 paper)""""""\n    x = conv(x, 32, 3, stride=2, padding=\'VALID\', scope=\'block1a\')\n    x = conv(x, 32, 3, padding=\'VALID\', scope=\'block2a\')\n    x = conv(x, 64, 3, scope=\'block2b\')\n\n    with tf.variable_scope(\'block3a\'):\n        x_1 = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool\')\n        x_2 = conv(x, 96, 3, stride=2, padding=\'VALID\', scope=\'conv\')\n        x = concat([x_1, x_2], axis=3, name=\'concat\')\n\n    with tf.variable_scope(\'block4a\'):\n        x_1 = conv(x, 64, 1, scope=\'1a\')\n        x_1 = conv(x_1, 96, 3, padding=\'VALID\', scope=\'1b\')\n        x_2 = conv(x, 64, 1, scope=\'2a\')\n        x_2 = conv(x_2, 64, (1, 7), scope=\'2b\')\n        x_2 = conv(x_2, 64, (7, 1), scope=\'2c\')\n        x_2 = conv(x_2, 96, 3, padding=\'VALID\', scope=\'2d\')\n        x = concat([x_1, x_2], axis=3, name=\'concat\')\n\n    with tf.variable_scope(\'block5a\'):\n        x_1 = conv(x, 192, 3, stride=2, padding=\'VALID\', scope=\'conv\')\n        x_2 = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool\')\n        x = concat([x_1, x_2], axis=3, name=\'concat\')\n\n    return x\n\n\ndef stemB(x):\n    """"""Stem for Inception-ResNet-v1 (Fig. 14 in the v4 paper)""""""\n    x = conv(x, 32, 3, stride=2, padding=\'VALID\', scope=\'block1a\')\n\n    x = conv(x, 32, 3, padding=\'VALID\', scope=\'block2a\')\n    x = conv(x, 64, 3, scope=\'block2b\')\n\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'block3a/pool\')\n    x = conv(x, 80, 1, padding=\'VALID\', scope=\'block3a/conv\')\n\n    x = conv(x, 192, 3, padding=\'VALID\', scope=\'block4a\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'block5a\')\n\n    x = conv(x, 256, 3, stride=2, padding=\'VALID\', scope=\'block5b\')\n    return x\n\n\ndef stemS(x):\n    """"""Stem for Inception-ResNet-v2 in TF Slim which differs from the paper""""""\n    x = conv(x, 32, 3, stride=2, padding=\'VALID\', scope=\'block1a\')\n\n    x = conv(x, 32, 3, padding=\'VALID\', scope=\'block2a\')\n    x = conv(x, 64, 3, scope=\'block2b\')\n\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'block3a/pool\')\n    x = conv(x, 80, 1, padding=\'VALID\', scope=\'block3a/conv\')\n\n    x = conv(x, 192, 3, padding=\'VALID\', scope=\'block4a\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'block5a\')\n\n    x = inceptionA(x, [96, [48, 64], [64, 96], 64], fs=5, scope=\'block5b\')\n    return x\n\n\n@var_scope(\'inception\')\ndef inception(x, f, scope=None):\n    conv1 = conv0(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv0(x, f[1][0], 1, scope=\'3x3/r\')\n    conv2 = conv0(conv2, f[1][1], 3, scope=\'3x3/1\')\n\n    conv3 = conv0(x, f[2][0], 1, scope=\'5x5/r\')\n    conv3 = conv0(conv3, f[2][1], 5, scope=\'5x5/1\')\n\n    pool = max_pool2d(x, 3)\n    pool = conv0(pool, f[3], 1, scope=\'proj\')\n\n    x = concat([conv1, conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'inceptionA\')\ndef inceptionA(x, f, fs=3, pool_fn=avg_pool2d, scale=1.0, scope=None):\n    conv1 = conv(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv(x, f[1][0], 1, scope=\'3x3/r\')\n    conv2 = conv(conv2, f[1][1], fs, scope=\'3x3/1\')\n\n    conv3 = conv(x, f[2][0], 1, scope=\'d3x3/r\')\n    conv3 = conv(conv3, f[2][1], 3, scope=\'d3x3/1\')\n    conv3 = conv(conv3, f[2][1], 3, scope=\'d3x3/2\')\n\n    pool = pool_fn(x, 3)\n    pool = conv(pool, f[3], 1, scope=\'proj\')\n\n    x = concat([conv1, conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'reductionA\')\ndef reductionA(x, f, padding=\'VALID\', scope=None):\n    if padding == \'VALID\':\n        # Reduction-A (Fig. 7 in the v4 paper)\n        conv2 = conv(x, f[0], 3, stride=2, padding=padding, scope=\'3x3\')\n    else:\n        conv2 = conv(x, f[0][0], 1, scope=\'3x3/r\')\n        conv2 = conv(conv2, f[0][1], 3, stride=2, scope=\'3x3/1\')\n\n    conv3 = conv(x, f[1][0], 1, scope=\'d3x3/r\')\n    conv3 = conv(conv3, f[1][1], 3, scope=\'d3x3/1\')\n    conv3 = conv(conv3, f[1][2], 3, stride=2, padding=padding, scope=\'d3x3/2\')\n\n    pool = max_pool2d(x, 3, stride=2, padding=padding)\n\n    x = concat([conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'inceptionB\')\ndef inceptionB(x, f, scale=1.0, scope=None):\n    conv1 = conv(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv(x, f[1][0], 1, scope=\'7x7/r\')\n    conv2 = conv(conv2, f[1][1], (1, 7), scope=\'7x7/1\')\n    conv2 = conv(conv2, f[1][2], (7, 1), scope=\'7x7/2\')\n\n    conv3 = conv(x, f[2][0], 1, scope=\'d7x7/r\')\n    conv3 = conv(conv3, f[2][0], (7, 1), scope=\'d7x7/1\')\n    conv3 = conv(conv3, f[2][1], (1, 7), scope=\'d7x7/2\')\n    conv3 = conv(conv3, f[2][1], (7, 1), scope=\'d7x7/3\')\n    conv3 = conv(conv3, f[1][2], (1, 7), scope=\'d7x7/4\')\n\n    pool = avg_pool2d(x, 3)\n    pool = conv(pool, f[3], 1, scope=\'proj\')\n\n    x = concat([conv1, conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'reductionB\')\ndef reductionB(x, f, scope=None):\n    conv2 = conv(x, f[0][0], 1, scope=\'3x3/r\')\n    conv2 = conv(conv2, f[0][1], 3, stride=2, padding=\'VALID\', scope=\'3x3/1\')\n\n    conv3 = conv(x, f[1][0], 1, scope=\'7x7/r\')\n    conv3 = conv(conv3, f[1][0], (1, 7), scope=\'7x7/1\')\n    conv3 = conv(conv3, f[1][1], (7, 1), scope=\'7x7/2\')\n    conv3 = conv(conv3, f[1][1], 3, stride=2, padding=\'VALID\', scope=\'7x7/3\')\n\n    pool = max_pool2d(x, 3, stride=2, padding=\'VALID\')\n\n    x = concat([conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'inceptionC\')\ndef inceptionC(x, f, scale=1.0, scope=None):\n    conv1 = conv(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv(x, f[1][0], 1, scope=\'3x3/r\')\n    conv2_1 = conv(conv2, f[1][1], (1, 3), scope=\'3x3/1\')\n    conv2_2 = conv(conv2, f[1][2], (3, 1), scope=\'3x3/2\')\n    conv2 = concat([conv2_1, conv2_2], axis=3, name=\'3x3/c\')\n\n    conv3 = conv(x, f[2][0], 1, scope=\'d3x3/r\')\n    if len(f[2]) < 3:\n        conv3 = conv(conv3, f[2][1], (3, 3), scope=\'d3x3/1\')\n    else:\n        conv3 = conv(conv3, f[2][1], (3, 1), scope=\'d3x3/11\')\n        conv3 = conv(conv3, f[2][2], (1, 3), scope=\'d3x3/12\')\n    conv3_1 = conv(conv3, f[2][-1], (1, 3), scope=\'d3x3/21\')\n    conv3_2 = conv(conv3, f[2][-1], (3, 1), scope=\'d3x3/22\')\n    conv3 = concat([conv3_1, conv3_2], axis=3, name=\'d3x3/c\')\n\n    pool = avg_pool2d(x, 3)\n    pool = conv(pool, f[3], 1, scope=\'proj\')\n\n    x = concat([conv1, conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'inceptionRA\')\ndef inceptionRA(x, f, scale=1.0, scope=None):\n    """"""Inception-ResNet-A (Fig. 10 and 16 in the v4 paper)""""""\n    conv1 = conv(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv(x, f[1], 1, scope=\'3x3/r\')\n    conv2 = conv(conv2, f[1], 3, scope=\'3x3/1\')\n\n    conv3 = conv(x, f[2][0], 1, scope=\'d3x3/r\')\n    conv3 = conv(conv3, f[2][1], 3, scope=\'d3x3/1\')\n    conv3 = conv(conv3, f[2][2], 3, scope=\'d3x3/2\')\n\n    convs = concat([conv1, conv2, conv3], axis=3, name=\'concat\')\n    convs = conv2d(convs, f[3], 1, biases_initializer=tf.zeros_initializer(),\n                   scope=\'linear\')\n\n    x = relu(x + scale * convs, name=\'out\')\n    return x\n\n\n@var_scope(\'inceptionRB\')\ndef inceptionRB(x, f, k=7, scale=1.0, activation_fn=relu, scope=None):\n    """"""Inception-ResNet-B (Fig. 11 and 17 in the v4 paper) and\n    Inception-ResNet-C (Fig. 13 and 19 in the v4 paper)\n    """"""\n    conv1 = conv(x, f[0], 1, scope=\'1x1\')\n\n    conv2 = conv(x, f[1][0], 1, scope=""%dx%d/r"" % (k, k))\n    conv2 = conv(conv2, f[1][1], (1, k), scope=""%dx%d/1"" % (k, k))\n    conv2 = conv(conv2, f[1][2], (k, 1), scope=""%dx%d/2"" % (k, k))\n\n    convs = concat([conv1, conv2], axis=3, name=\'concat\')\n    convs = conv2d(convs, f[2], 1, biases_initializer=tf.zeros_initializer(),\n                   scope=\'linear\')\n\n    if activation_fn is not None:\n        x = activation_fn(x + scale * convs, name=\'out\')\n    else:\n        x = add(x, scale * convs, name=\'out\')\n    return x\n\n\n@var_scope(\'reductionRB\')\ndef reductionRB(x, f, scope=None):\n    """"""Reduction-B (Fig. 12 and 18 in the v4 paper)""""""\n    conv1 = conv(x, f[0], 1, scope=\'3x3a/r\')\n    conv1 = conv(conv1, f[1], 3, stride=2, padding=\'VALID\', scope=\'3x3a/1\')\n\n    conv2 = conv(x, f[0], 1, scope=\'3x3b/r\')\n    conv2 = conv(conv2, f[2], 3, stride=2, padding=\'VALID\', scope=\'3x3b/1\')\n\n    conv3 = conv(x, f[0], 1, scope=\'3x3c/r\')\n    conv3 = conv(conv3, f[2], 3, scope=\'3x3c/1\')\n    conv3 = conv(conv3, f[3], 3, stride=2, padding=\'VALID\', scope=\'3x3c/2\')\n\n    pool = max_pool2d(x, 3, stride=2, padding=\'VALID\')\n\n    x = concat([conv1, conv2, conv3, pool], axis=3, name=\'concat\')\n    return x\n\n\n# Simple alias.\nGoogLeNet = Inception1 = inception1\nInception2 = inception2\nInception3 = inception3\nInception4 = inception4\nInceptionResNet2 = inceptionresnetS\n'"
tensornets/layers.py,17,"b""from __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .ops import conv2d_primitive\nfrom .ops import leaky_relu\nfrom .ops import relu\nfrom .ops import relu6\nfrom .ops import reshape\nfrom .ops import swish\nfrom .utils import arg_scope\nfrom .utils import remove_commons\nfrom .version_utils import tf_later_than\n\n\nif tf_later_than('1.14'):\n    tf = tf.compat.v1\n\n\nif tf_later_than('2'):\n    from .contrib_layers import avg_pool2d\n    from .contrib_layers import batch_norm\n    from .contrib_layers import bias_add\n    from .contrib_layers import conv2d\n    from .contrib_layers import dropout\n    from .contrib_layers import flatten\n    from .contrib_layers import fully_connected as fc\n    from .contrib_layers import l2_regularizer as l2\n    from .contrib_layers import max_pool2d\n    from .contrib_layers import separable_conv2d as sconv2d\n    from .contrib_layers import variance_scaling_initializer\nelse:\n    from tensorflow.contrib.layers import avg_pool2d\n    from tensorflow.contrib.layers import batch_norm\n    from tensorflow.contrib.layers import bias_add\n    from tensorflow.contrib.layers import conv2d\n    from tensorflow.contrib.layers import dropout\n    from tensorflow.contrib.layers import flatten\n    from tensorflow.contrib.layers import fully_connected as fc\n    from tensorflow.contrib.layers import l2_regularizer as l2\n    from tensorflow.contrib.layers import max_pool2d\n    from tensorflow.contrib.layers import separable_conv2d as sconv2d\n    from tensorflow.contrib.layers import variance_scaling_initializer\n\n\nconv1d = conv2d\n\n\ndef convbn(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return batch_norm(conv2d(*args, **kwargs))\n\n\ndef convbnact(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    activation_fn = kwargs.pop('activation_fn', None)\n    with tf.variable_scope(scope):\n        return activation_fn(batch_norm(conv2d(*args, **kwargs)))\n\n\ndef convrelu(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return relu(conv2d(*args, **kwargs))\n\n\ndef convrelu0(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    kwargs['biases_initializer'] = tf.zeros_initializer()\n    with tf.variable_scope(scope):\n        return relu(conv2d(*args, **kwargs))\n\n\ndef convbnrelu(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return relu(batch_norm(conv2d(*args, **kwargs)))\n\n\ndef convbnrelu6(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return relu6(batch_norm(conv2d(*args, **kwargs)))\n\n\ndef convbnswish(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return swish(batch_norm(conv2d(*args, **kwargs)))\n\n\ndef gconvbn(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        x = sconv2d(*args, **kwargs)\n        c = args[-1]\n        infilters = int(x.shape[-1]) if tf_later_than('2') else x.shape[-1].value\n        f = infilters // c\n        g = f // c\n        kernel = np.zeros((1, 1, f * c, f), np.float32)\n        for i in range(f):\n            start = (i // c) * c * c + i % c\n            end = start + c * c\n            kernel[:, :, start:end:c, i] = 1.\n        x = conv2d_primitive(x, tf.constant(kernel), strides=[1, 1, 1, 1],\n                             padding='VALID', name='gconv')\n        return batch_norm(x)\n\n\ndef sconvbn(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return batch_norm(sconv2d(*args, **kwargs))\n\n\ndef sconvbnact(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    activation_fn = kwargs.pop('activation_fn', None)\n    with tf.variable_scope(scope):\n        return activation_fn(batch_norm(sconv2d(*args, **kwargs)))\n\n\ndef sconvbnrelu(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return relu(batch_norm(sconv2d(*args, **kwargs)))\n\n\ndef sconvbnrelu6(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return relu6(batch_norm(sconv2d(*args, **kwargs)))\n\n\ndef sconvbnswish(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    with tf.variable_scope(scope):\n        return swish(batch_norm(sconv2d(*args, **kwargs)))\n\n\ndef darkconv(*args, **kwargs):\n    scope = kwargs.pop('scope', None)\n    onlyconv = kwargs.pop('onlyconv', False)\n    with tf.variable_scope(scope):\n        conv_kwargs = {\n            'padding': 'SAME',\n            'activation_fn': None,\n            'weights_initializer': variance_scaling_initializer(1.53846),\n            'weights_regularizer': l2(5e-4),\n            'biases_initializer': None,\n            'scope': 'conv'}\n        if onlyconv:\n            conv_kwargs.pop('biases_initializer')\n        with arg_scope([conv2d], **conv_kwargs):\n            x = conv2d(*args, **kwargs)\n            if onlyconv: return x\n            x = batch_norm(x, decay=0.99, center=False, scale=True,\n                           epsilon=1e-5, scope='bn')\n            x = bias_add(x, scope='bias')\n            x = leaky_relu(x, alpha=0.1, name='lrelu')\n            return x\n\n\nremove_commons(__name__)\n"""
tensornets/middles.py,0,"b'""""""Collection of representative endpoints for each model.""""""\nfrom __future__ import absolute_import\n\nfrom .version_utils import tf_equal_to\nfrom .version_utils import tf_later_than\n\n\nif tf_later_than(\'1.15\'):\n    bn_name = \'FusedBatchNormV3:0\'\nelif tf_later_than(\'1.4\'):\n    bn_name = \'FusedBatchNorm:0\'\nelse:\n    bn_name = \'batchnorm/add_1:0\'\n\n\ndef names_inceptions(k, first_block, omit_first=False,\n                     pool_last=False, resnet=False):\n    names = []\n    postfix = [\'concat\', \'out\' if resnet else \'concat\']\n    for i in range(3):\n        first_char = 98 if omit_first is True and i == 0 else 97\n        names += [""block%d%c/%s:0"" %\n                  (i + first_block, j + first_char, postfix[j > 0])\n                  for j in range(k[i])]\n        if pool_last is True and i < 2:\n            names[-1] = ""pool%d/MaxPool:0"" % (i + first_block)\n    return names\n\n\ndef names_resnets(k, pool_last=False):\n    names = []\n    for i in range(4):\n        names += [""conv%d/block%d/out:0"" % (i + 2, j + 1) for j in range(k[i])]\n        if pool_last is True and i < 3:\n            names += [""pool%d/pool/AvgPool:0"" % (i + 2)]\n    return names\n\n\ndef names_squeezenet():\n    names = [""fire%d/concat:0"" % (i + 2) for i in range(8)]\n    names.insert(2, \'pool3/MaxPool:0\')\n    names.insert(5, \'pool5/MaxPool:0\')\n    return names\n\n\ndef names_nasnets(k):\n    names = []\n    for i in range(3):\n        base = sum(k[:i])\n        names += [""normal%d/concat:0"" % (base + j + 1) for j in range(k[i])]\n        if i < 2:\n            names += [""reduction%d/concat:0"" % (base + k[i])]\n    return names\n\n\ndef names_vggs(k):\n    names = []\n    for i in range(3):\n        names += [""conv%d/%d/Relu:0"" % (i + 3, j + 1) for j in range(k)]\n    return names\n\n\ndef names_darknets(k):\n    names = []\n    for i in range(4):\n        if k[i] > 1:\n            names += [""conv%d/%d/lrelu/Maximum:0"" % (i + 3, j + 1)\n                      for j in range(k[i])]\n        else:\n            names += [""conv%d/lrelu/Maximum:0"" % (i + 3)]\n        if i < 3:\n            names += [""pool%d/MaxPool:0"" % (i + 3)]\n    return names\n\n\ndef tuple_mobilenetv2():\n    def baseidx(b):\n        return [b, b + 3, b + 5]\n    indices = baseidx(2)\n    names = [\'conv1/Relu6:0\', \'sconv1/Relu6:0\', \'pconv1/bn/\' + bn_name]\n    k = 10\n    l = 2\n    for (i, j) in enumerate([2, 3, 4, 3, 3, 1]):\n        indices += baseidx(k)\n        names += [""conv%d/conv/Relu6:0"" % l,\n                  ""conv%d/sconv/Relu6:0"" % l,\n                  ""conv%d/pconv/bn/%s"" % (l, bn_name)]\n        k += 8\n        l += 1\n        for _ in range(j - 1):\n            indices += (baseidx(k) + [k + 6])\n            names += [""conv%d/conv/Relu6:0"" % l,\n                      ""conv%d/sconv/Relu6:0"" % l,\n                      ""conv%d/pconv/bn/%s"" % (l, bn_name),\n                      ""conv%d/out:0"" % l]\n            k += 9\n            l += 1\n    indices += [k]\n    names += [""conv%d/Relu6:0"" % l]\n    return (indices, names, -16)\n\n\ndef tuple_mobilenetv3(is_large, is_mini):\n    if is_mini is True:\n        act_name = \'Relu:0\'\n    else:\n        act_name = \'Mul:0\'\n\n    names = [""conv1/%s"" % act_name]\n    indices = [2]\n    l = 3\n\n    if is_large is True:\n        names += [\'conv2/out:0\']\n        blocks = [2, 3, 4, 2, 3]\n        k = 8\n    else:\n        names += [""conv2/pconv/bn/%s"" % bn_name]\n        blocks = [2, 3, 2, 3]\n        if is_mini is True:\n            k = 7\n        else:\n            k = 13\n\n    indices += [k]\n\n    for (i, j) in enumerate(blocks):\n        names += [""conv%d/pconv/bn/%s"" % (l, bn_name)]\n        l += 1\n        k += 8\n        if is_mini is False:\n            if (is_large is True and i in [1, 3, 4]) or \\\n               (is_large is False and i in [1, 2, 3, 4]):\n                k += 6\n        indices += [k]\n        for _ in range(j - 1):\n            names += [""conv%d/out:0"" % l]\n            l += 1\n            k += 9\n            if is_mini is False:\n                if (is_large is True and i in [1, 3, 4]) or \\\n                   (is_large is False and i in [1, 2, 3, 4]):\n                    k += 6\n            indices += [k]\n\n    indices += [k + 3, k + 4, k + 6]\n    names += [""conv%d/%s"" % (l, act_name),\n              \'pool/AvgPool:0\',\n              ""conv%d/out:0"" % (l + 1)]\n    return (indices, names, -16)\n\n\ndef tuple_efficientnet(index0, index1, blocks, addindices, biases=None):\n    indices = [index0] + list(range(index1, index1 + 12 * blocks, 12))\n    names = [""block%d/pconv/bn/%s"" % (i, bn_name) for i in range(len(indices))]\n    for i in addindices:\n        for j in range(i, len(indices)):\n            indices[j] += 2\n        names.insert(i, ""block%s/add:0"" % names[i - 1].split(\'/\')[0][5:])\n        indices.insert(i, indices[i - 1] + 2)\n    if biases is not None:\n        for b in biases:\n            for j in range(b, len(indices)):\n                indices[j] -= 3\n    return (indices, names, -10)\n\n\ndef direct(model_name):\n    try:\n        return __middles_dict__[model_name]\n    except KeyError:\n        return ([-1], [\'out:0\'])\n\n\n# Dictionary for lists of representative endpoints.\n__middles_dict__ = {\n    \'inception1\': (\n        [24, 38, 39, 53] + list(range(67, 110, 14)) + [110, 124, 138],\n        names_inceptions([3, 6, 2], 3, pool_last=True),\n        -4\n    ),\n    \'inception2\': (\n        [31, 54] + list(range(71, 164, 23)) + list(range(180, 227, 23)),\n        names_inceptions([3, 5, 2], 3),\n        -4\n    ),\n    \'inception3\': (\n        list(range(39, 86, 23)) + list(range(99, 228, 32)) +\n        list(range(247, 310, 31)),\n        names_inceptions([3, 5, 3], 5, omit_first=True),\n        -4\n    ),\n    \'inception4\': (\n        list(range(37, 130, 23)) + list(range(143, 368, 32)) +\n        list(range(387, 490, 34)),\n        names_inceptions([5, 8, 4], 5),\n        -5\n    ),\n    \'inceptionresnet2_tfslim\': (\n        [39] + list(range(60, 250, 21)) + [263] + list(range(278, 564, 15)) +\n        [586] + list(range(601, 737, 15)),\n        names_inceptions([11, 21, 11], 5, omit_first=True, resnet=True),\n        -12\n    ),\n    \'resnet50\': (\n        list(range(16, 35, 9)) + list(range(45, 73, 9)) +\n        list(range(83, 129, 9)) + list(range(139, 158, 9)),\n        names_resnets([3, 4, 6, 3]),\n        -4\n    ),\n    \'resnet101\': (\n        list(range(16, 35, 9)) + list(range(45, 73, 9)) +\n        list(range(83, 282, 9)) + list(range(292, 311, 9)),\n        names_resnets([3, 4, 23, 3]),\n        -4\n    ),\n    \'resnet152\': (\n        list(range(16, 35, 9)) + list(range(45, 109, 9)) +\n        list(range(119, 435, 9)) + list(range(445, 464, 9)),\n        names_resnets([3, 8, 36, 3]),\n        -4\n    ),\n    \'resnet50v2\': (\n        list(range(15, 27, 11)) + [38] + list(range(50, 73, 11)) + [84] +\n        list(range(96, 141, 11)) + [152] + list(range(164, 176, 11)) + [186],\n        names_resnets([3, 4, 6, 3]),\n        -5\n    ),\n    \'resnet101v2\': (\n        list(range(15, 27, 11)) + [38] + list(range(50, 73, 11)) + [84] +\n        list(range(96, 328, 11)) + [339] + list(range(351, 363, 11)) + [373],\n        names_resnets([3, 4, 23, 3]),\n        -5\n    ),\n    \'resnet152v2\': (\n        list(range(15, 27, 11)) + [38] + list(range(50, 117, 11)) + [128] +\n        list(range(140, 515, 11)) + [526] + list(range(538, 550, 11)) + [560],\n        names_resnets([3, 8, 36, 3]),\n        -5\n    ),\n    \'resnet200v2\': (\n        list(range(18, 41, 11)) + list(range(53, 307, 11)) +\n        list(range(319, 705, 11)) + list(range(717, 740, 11)),\n        names_resnets([3, 24, 36, 3]),\n        -4\n    ),\n    \'resnext50\': (\n        list(range(18, 41, 11)) + list(range(53, 87, 11)) +\n        list(range(99, 155, 11)) + list(range(167, 190, 11)),\n        names_resnets([3, 4, 6, 3]),\n        -4\n    ),\n    \'resnext101\': (\n        list(range(18, 41, 11)) + list(range(53, 87, 11)) +\n        list(range(99, 342, 11)) + list(range(354, 377, 11)),\n        names_resnets([3, 4, 23, 3]),\n        -4\n    ),\n    \'resnext50c32\': (\n        list(range(18, 41, 11)) + list(range(53, 87, 11)) +\n        list(range(99, 155, 11)) + list(range(167, 190, 11)),\n        names_resnets([3, 4, 6, 3]),\n        -4\n    ),\n    \'resnext101c32\': (\n        list(range(18, 41, 11)) + list(range(53, 87, 11)) +\n        list(range(99, 342, 11)) + list(range(354, 377, 11)),\n        names_resnets([3, 4, 23, 3]),\n        -4\n    ),\n    \'resnext101c64\': (\n        list(range(18, 41, 11)) + list(range(53, 87, 11)) +\n        list(range(99, 342, 11)) + list(range(354, 377, 11)),\n        names_resnets([3, 4, 23, 3]),\n        -4\n    ),\n    \'wideresnet50\': (\n        list(range(17, 38, 10)) + list(range(49, 80, 10)) +\n        list(range(91, 142, 10)) + list(range(153, 174, 10)),\n        names_resnets([3, 4, 6, 3]),\n        -4\n    ),\n    \'nasnetAlarge\': (\n        list(range(145, 371, 45)) + [416] + list(range(466, 692, 45)) + [748] +\n        list(range(798, 1024, 45)),\n        names_nasnets([6, 6, 6]),\n        -8\n    ),\n    \'nasnetAmobile\': (\n        list(range(145, 281, 45)) + [326] + list(range(376, 512, 45)) + [568] +\n        list(range(618, 754, 45)),\n        names_nasnets([4, 4, 4]),\n        -6\n    ),\n    \'pnasnetlarge\': (\n        list(range(169, 323, 51)) + [376] + list(range(432, 535, 51)) + [588] +\n        list(range(644, 747, 51)),\n        names_nasnets([4, 3, 3]),\n        -5\n    ),\n    \'vgg16\': (\n        list(range(11, 16, 2)) + list(range(18, 23, 2)) +\n        list(range(25, 30, 2)),\n        names_vggs(3),\n        -1\n    ),\n    \'vgg19\': (\n        list(range(11, 18, 2)) + list(range(20, 27, 2)) +\n        list(range(29, 36, 2)),\n        names_vggs(4),\n        -1\n    ),\n    \'densenet121\': (\n        list(range(12, 48, 7)) + [51] + list(range(58, 136, 7)) + [139] +\n        list(range(146, 308, 7)) + [311] + list(range(318, 424, 7)),\n        names_resnets([6, 12, 24, 16], pool_last=True),\n        -18\n    ),\n    \'densenet169\': (\n        list(range(12, 48, 7)) + [51] + list(range(58, 136, 7)) + [139] +\n        list(range(146, 364, 7)) + [367] + list(range(374, 592, 7)),\n        names_resnets([6, 12, 32, 32], pool_last=True),\n        -34\n    ),\n    \'densenet201\': (\n        list(range(12, 48, 7)) + [51] + list(range(58, 136, 7)) + [139] +\n        list(range(146, 476, 7)) + [479] + list(range(486, 704, 7)),\n        names_resnets([6, 12, 48, 32], pool_last=True),\n        -34\n    ),\n    \'mobilenet25\': (\n        list(range(20, 81, 6)),\n        [\'conv%d/conv/Relu6:0\' % (i + 4) for i in range(11)],\n        -3\n    ),\n    \'mobilenet50\': (\n        list(range(20, 81, 6)),\n        [\'conv%d/conv/Relu6:0\' % (i + 4) for i in range(11)],\n        -3\n    ),\n    \'mobilenet75\': (\n        list(range(20, 81, 6)),\n        [\'conv%d/conv/Relu6:0\' % (i + 4) for i in range(11)],\n        -3\n    ),\n    \'mobilenet100\': (\n        list(range(20, 81, 6)),\n        [\'conv%d/conv/Relu6:0\' % (i + 4) for i in range(11)],\n        -3\n    ),\n    \'mobilenet35v2\': tuple_mobilenetv2(),\n    \'mobilenet50v2\': tuple_mobilenetv2(),\n    \'mobilenet75v2\': tuple_mobilenetv2(),\n    \'mobilenet100v2\': tuple_mobilenetv2(),\n    \'mobilenet130v2\': tuple_mobilenetv2(),\n    \'mobilenet140v2\': tuple_mobilenetv2(),\n    \'mobilenet75v3large\': tuple_mobilenetv3(True, False),\n    \'mobilenet100v3large\': tuple_mobilenetv3(True, False),\n    \'mobilenet100v3largemini\': tuple_mobilenetv3(True, True),\n    \'mobilenet75v3small\': tuple_mobilenetv3(False, False),\n    \'mobilenet100v3small\': tuple_mobilenetv3(False, False),\n    \'mobilenet100v3smallmini\': tuple_mobilenetv3(False, True),\n    \'efficientnetb0\': tuple_efficientnet(\n        11, 23, 15,\n        [3, 6, 9, 11, 14, 16, 19, 21, 23]),\n    \'efficientnetb1\': tuple_efficientnet(\n        11, 20, 22,\n        [2, 5, 7, 10, 12, 15, 17, 19, 22, 24, 26, 29, 31, 33, 35, 38]),\n    \'efficientnetb2\': tuple_efficientnet(\n        11, 20, 22,\n        [2, 5, 7, 10, 12, 15, 17, 19, 22, 24, 26, 29, 31, 33, 35, 38]),\n    \'efficientnetb3\': tuple_efficientnet(\n        11, 20, 25,\n        [2, 5, 7, 10, 12, 15, 17, 19, 21, 24, 26, 28, 30, 33, 35, 37,\n         39, 41, 44]),\n    \'efficientnetb4\': tuple_efficientnet(\n        11, 20, 31,\n        [2, 5, 7, 9, 12, 14, 16, 19, 21, 23, 25, 27, 30, 32, 34, 36, 38,\n         41, 43, 45, 47, 49, 51, 53, 56]),\n    \'efficientnetb5\': tuple_efficientnet(\n        11, 20, 38,\n        [2, 4, 7, 9, 11, 13, 16, 18, 20, 22, 25, 27, 29, 31, 33, 35, 38,\n         40, 42, 44, 46, 48, 51, 53, 55, 57, 59, 61, 63, 65, 68, 70],\n        biases=[3]),\n    \'efficientnetb6\': tuple_efficientnet(\n        11, 20, 44,\n        [2, 4, 7, 9, 11, 13, 15, 18, 20, 22, 24, 26, 29, 31, 33, 35, 37,\n         39, 41, 44, 46, 48, 50, 52, 54, 56, 59, 61, 63, 65, 67, 69, 71,\n         73, 75, 77, 80, 82],\n        biases=[3]),\n    \'efficientnetb7\': tuple_efficientnet(\n        11, 20, 54,\n        [2, 4, 6, 9, 11, 13, 15, 17, 19, 22, 24, 26, 28, 30, 32, 35, 37,\n         39, 41, 43, 45, 47, 49, 51, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n         73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 98, 100, 102],\n        biases=[3, 5]),\n    \'squeezenet\': (\n        [9, 16, 17, 24, 31, 32] + list(range(39, 61, 7)),\n        names_squeezenet(),\n        -6\n    ),\n    \'darknet19\': (\n        list(range(13, 22, 4)) + [22] + list(range(26, 35, 4)) + [35] +\n        list(range(39, 56, 4)) + [56] + list(range(60, 77, 4)),\n        names_darknets([3, 3, 5, 5]),\n        -7\n    ),\n    \'tinydarknet19\': (\n        [13, 14, 18, 19, 23, 24, 28],\n        names_darknets([1, 1, 1, 1]),\n        -3\n    ),\n    \'REFyolov2\': ([-1], [\'linear/BiasAdd:0\']),\n    \'REFyolov2voc\': ([-1], [\'linear/BiasAdd:0\']),\n    \'REFtinyyolov2\': ([-1], [\'linear/BiasAdd:0\']),\n    \'REFtinyyolov2voc\': ([-1], [\'linear/BiasAdd:0\']),\n}\n'"
tensornets/mobilenets.py,4,"b'""""""Collection of MobileNet variants\n\nThe reference papers:\n\n1. V1\n - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv 2017\n - Andrew G. Howard et al.\n - https://arxiv.org/abs/1704.04861\n2. V2\n - MobileNetV2: Inverted Residuals and Linear Bottlenecks, CVPR 2018 (arXiv 2018)\n - Mark Sandler et al.\n - https://arxiv.org/abs/1801.04381\n3. V3\n - Searching for MobileNetV3, ICCV 2019\n - Andrew Howard et al.\n - https://arxiv.org/abs/1905.02244\n\nThe reference implementations:\n\n1. (for v1) TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py\n2. (for v2) TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\n3. (for v3) TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py\n""""""\nfrom __future__ import absolute_import\n\nimport functools\nimport tensorflow as tf\n\nfrom .layers import avg_pool2d\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import dropout\nfrom .layers import fc\nfrom .layers import sconv2d\nfrom .layers import convbn\nfrom .layers import convbnact\nfrom .layers import convbnrelu6 as conv\nfrom .layers import sconvbn\nfrom .layers import sconvbnact\nfrom .layers import sconvbnrelu6 as sconv\n\nfrom .ops import *\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __base_args__(is_training, decay):\n    return [([avg_pool2d], {\'padding\': \'VALID\', \'scope\': \'pool\'}),\n            ([batch_norm], {\'decay\': decay, \'scale\': True, \'epsilon\': 0.001,\n                            \'is_training\': is_training, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training, \'scope\': \'dropout\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([sconv2d],\n             {\'activation_fn\': None, \'biases_initializer\': None,\n              \'scope\': \'sconv\'})]\n\n\ndef __args_v1__(is_training):\n    return __base_args__(is_training, 0.9997)\n\n\ndef __args__(is_training):\n    return __base_args__(is_training, 0.999)\n\n\ndef _depth(d, divisor=8):\n    filters = max(divisor, int(d + divisor // 2) // divisor * divisor)\n    if filters < 0.9 * d:\n        filters += divisor\n    return filters\n\n\n@var_scope(\'block\')\ndef block(x, filters, stride=1, scope=None):\n    x = sconv(x, None, 3, 1, stride=stride, scope=\'sconv\')\n    x = conv(x, filters, 1, stride=1, scope=\'conv\')\n    return x\n\n\n@var_scope(\'blockv2\')\ndef block2(x, filters, stride=1, scope=None):\n    shortcut = x\n    infilters = int(x.shape[-1]) if tf_later_than(\'2\') else x.shape[-1].value\n    x = conv(x, 6 * infilters, 1, scope=\'conv\')\n    x = sconv(x, None, 3, 1, stride=stride, scope=\'sconv\')\n    x = convbn(x, filters, 1, stride=1, scope=\'pconv\')\n    if stride == 1 and infilters == filters:\n        return add(shortcut, x, name=\'out\')\n    else:\n        return x\n\n\n@var_scope(\'seblock\')\ndef seblock(i, se, filters, scope=None):\n    x = reduce_mean(i, [1, 2], keepdims=True, name=\'squeeze\')\n    x = conv2d(x, _depth(se * filters), 1, activation_fn=relu,\n               biases_initializer=tf.zeros_initializer(), scope=\'reduce\')\n    x = conv2d(x, filters, 1, activation_fn=hard_sigmoid,\n               biases_initializer=tf.zeros_initializer(), scope=\'expand\')\n    x = multiply(i, x, name=\'excite\')\n    return x\n\n\n@var_scope(\'blockv3\')\ndef block3(x, ex, se, filters, kernel_size, stride=1,\n           activation_fn=relu, scope=None):\n    shortcut = x\n    infilters = int(x.shape[-1]) if tf_later_than(\'2\') else x.shape[-1].value\n    if ex > 1:\n        x = convbnact(x, _depth(ex * infilters), 1,\n                      activation_fn=activation_fn, scope=\'conv\')\n    x = sconvbnact(x, None, kernel_size, 1, stride=stride,\n                   activation_fn=activation_fn, scope=\'sconv\')\n    if 0 < se <= 1:\n        x = seblock(x, se, _depth(ex * infilters), scope=\'se\')\n    x = convbn(x, filters, 1, stride=1, scope=\'pconv\')\n    if stride == 1 and infilters == filters:\n        return add(shortcut, x, name=\'out\')\n    else:\n        return x\n\n\ndef mobilenet(x, depth_multiplier, is_training, classes, stem,\n              scope=None, reuse=None):\n    def depth(d):\n        return max(int(d * depth_multiplier), 8)\n    x = conv(x, depth(32), 3, stride=2, scope=\'conv1\')\n\n    x = block(x, depth(64), scope=\'conv2\')\n    x = block(x, depth(128), stride=2, scope=\'conv3\')\n\n    x = block(x, depth(128), scope=\'conv4\')\n    x = block(x, depth(256), stride=2, scope=\'conv5\')\n\n    x = block(x, depth(256), scope=\'conv6\')\n    x = block(x, depth(512), stride=2, scope=\'conv7\')\n\n    x = block(x, depth(512), scope=\'conv8\')\n    x = block(x, depth(512), scope=\'conv9\')\n    x = block(x, depth(512), scope=\'conv10\')\n    x = block(x, depth(512), scope=\'conv11\')\n    x = block(x, depth(512), scope=\'conv12\')\n    x = block(x, depth(1024), stride=2, scope=\'conv13\')\n\n    x = block(x, depth(1024), scope=\'conv14\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.999, is_training=is_training, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\ndef mobilenetv2(x, depth_multiplier, is_training, classes, stem,\n                scope=None, reuse=None):\n    def depth(d):\n        return _depth(d * depth_multiplier)\n    x = conv(x, depth(32), 3, stride=2, scope=\'conv1\')\n    x = sconv(x, None, 3, 1, scope=\'sconv1\')\n    x = convbn(x, depth(16), 1, scope=\'pconv1\')\n\n    x = block2(x, depth(24), stride=2, scope=\'conv2\')\n    x = block2(x, depth(24), scope=\'conv3\')\n\n    x = block2(x, depth(32), stride=2, scope=\'conv4\')\n    x = block2(x, depth(32), scope=\'conv5\')\n    x = block2(x, depth(32), scope=\'conv6\')\n\n    x = block2(x, depth(64), stride=2, scope=\'conv7\')\n    x = block2(x, depth(64), scope=\'conv8\')\n    x = block2(x, depth(64), scope=\'conv9\')\n    x = block2(x, depth(64), scope=\'conv10\')\n\n    x = block2(x, depth(96), scope=\'conv11\')\n    x = block2(x, depth(96), scope=\'conv12\')\n    x = block2(x, depth(96), scope=\'conv13\')\n\n    x = block2(x, depth(160), stride=2, scope=\'conv14\')\n    x = block2(x, depth(160), scope=\'conv15\')\n    x = block2(x, depth(160), scope=\'conv16\')\n\n    x = block2(x, depth(320), scope=\'conv17\')\n    x = conv(x, 1280 * depth_multiplier if depth_multiplier > 1. else 1280, 1,\n             scope=\'conv18\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\ndef mobilenetv3large(x, depth_multiplier, kernel_size, se, activation_fn,\n                     is_training, classes, stem, scope=None, reuse=None):\n    def depth(d):\n        return _depth(d * depth_multiplier)\n    conva = functools.partial(convbnact, activation_fn=activation_fn)\n    block3a = functools.partial(block3, activation_fn=activation_fn)\n\n    x = conva(x, depth(16), 3, stride=2, scope=\'conv1\')\n    x = block3(x, 1, 0, depth(16), 3, scope=\'conv2\')\n    x = block3(x, 4, 0, depth(24), 3, stride=2, scope=\'conv3\')\n    x = block3(x, 3, 0, depth(24), 3, scope=\'conv4\')\n\n    x = block3(x, 3, se, depth(40), kernel_size, stride=2, scope=\'conv5\')\n    x = block3(x, 3, se, depth(40), kernel_size, scope=\'conv6\')\n    x = block3(x, 3, se, depth(40), kernel_size, scope=\'conv7\')\n\n    x = block3a(x, 6, 0, depth(80), 3, stride=2, scope=\'conv8\')\n    x = block3a(x, 2.5, 0, depth(80), 3, scope=\'conv9\')\n    x = block3a(x, 2.3, 0, depth(80), 3, scope=\'conv10\')\n    x = block3a(x, 2.3, 0, depth(80), 3, scope=\'conv11\')\n\n    x = block3a(x, 6, se, depth(112), 3, scope=\'conv12\')\n    x = block3a(x, 6, se, depth(112), 3, scope=\'conv13\')\n\n    x = block3a(x, 6, se, depth(160), kernel_size, stride=2, scope=\'conv14\')\n    x = block3a(x, 6, se, depth(160), kernel_size, scope=\'conv15\')\n    x = block3a(x, 6, se, depth(160), kernel_size, scope=\'conv16\')\n\n    x = conva(x, depth(960), 1, scope=\'conv17\')\n    x = avg_pool2d(x, 7, scope=\'pool\')\n    x = conv2d(x, depth(1280) if depth_multiplier > 1. else 1280, 1,\n               biases_initializer=tf.zeros_initializer(), scope=\'conv18\')\n    x = activation_fn(x, \'conv18/out\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\ndef mobilenetv3small(x, depth_multiplier, kernel_size, se, activation_fn,\n                     is_training, classes, stem, scope=None, reuse=None):\n    def depth(d):\n        return _depth(d * depth_multiplier)\n    conva = functools.partial(convbnact, activation_fn=activation_fn)\n    block3a = functools.partial(block3, activation_fn=activation_fn)\n\n    x = conva(x, depth(16), 3, stride=2, scope=\'conv1\')\n    x = block3(x, 1, se, depth(16), 3, stride=2, scope=\'conv2\')\n\n    x = block3(x, 72./16, 0, depth(24), 3, stride=2, scope=\'conv3\')\n    x = block3(x, 88./24, 0, depth(24), 3, scope=\'conv4\')\n\n    x = block3a(x, 4, se, depth(40), kernel_size, stride=2, scope=\'conv5\')\n    x = block3a(x, 6, se, depth(40), kernel_size, scope=\'conv6\')\n    x = block3a(x, 6, se, depth(40), kernel_size, scope=\'conv7\')\n\n    x = block3a(x, 3, se, depth(48), kernel_size, scope=\'conv8\')\n    x = block3a(x, 3, se, depth(48), kernel_size, scope=\'conv9\')\n\n    x = block3a(x, 6, se, depth(96), kernel_size, stride=2, scope=\'conv10\')\n    x = block3a(x, 6, se, depth(96), kernel_size, scope=\'conv11\')\n    x = block3a(x, 6, se, depth(96), kernel_size, scope=\'conv12\')\n\n    x = conva(x, depth(576), 1, scope=\'conv13\')\n    x = avg_pool2d(x, 7, scope=\'pool\')\n    x = conv2d(x, depth(1024) if depth_multiplier > 1. else 1024, 1,\n               biases_initializer=tf.zeros_initializer(), scope=\'conv14\')\n    x = activation_fn(x, \'conv14/out\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'mobilenet25\')\n@set_args(__args_v1__)\ndef mobilenet25(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return mobilenet(x, 0.25, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet50\')\n@set_args(__args_v1__)\ndef mobilenet50(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return mobilenet(x, 0.5, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet75\')\n@set_args(__args_v1__)\ndef mobilenet75(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    return mobilenet(x, 0.75, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100\')\n@set_args(__args_v1__)\ndef mobilenet100(x, is_training=False, classes=1000,\n                 stem=False, scope=None, reuse=None):\n    return mobilenet(x, 1.0, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet35v2\')\n@set_args(__args__)\ndef mobilenet35v2(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 0.35, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet50v2\')\n@set_args(__args__)\ndef mobilenet50v2(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 0.50, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet75v2\')\n@set_args(__args__)\ndef mobilenet75v2(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 0.75, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100v2\')\n@set_args(__args__)\ndef mobilenet100v2(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 1.0, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet130v2\')\n@set_args(__args__)\ndef mobilenet130v2(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 1.3, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet140v2\')\n@set_args(__args__)\ndef mobilenet140v2(x, is_training=False, classes=1000,\n                   stem=False, scope=None, reuse=None):\n    return mobilenetv2(x, 1.4, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet75v3large\')\n@set_args(__args__)\ndef mobilenet75v3large(x, is_training=False, classes=1000,\n                       stem=False, scope=None, reuse=None):\n    return mobilenetv3large(x, 0.75, 5, 0.25, hard_swish,\n                            is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100v3large\')\n@set_args(__args__)\ndef mobilenet100v3large(x, is_training=False, classes=1000,\n                        stem=False, scope=None, reuse=None):\n    return mobilenetv3large(x, 1.0, 5, 0.25, hard_swish,\n                            is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100v3largemini\')\n@set_args(__args__)\ndef mobilenet100v3largemini(x, is_training=False, classes=1000,\n                            stem=False, scope=None, reuse=None):\n    return mobilenetv3large(x, 1.0, 3, 0, relu,\n                            is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet75v3small\')\n@set_args(__args__)\ndef mobilenet75v3small(x, is_training=False, classes=1000,\n                       stem=False, scope=None, reuse=None):\n    return mobilenetv3small(x, 0.75, 5, 0.25, hard_swish,\n                            is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100v3small\')\n@set_args(__args__)\ndef mobilenet100v3small(x, is_training=False, classes=1000,\n                        stem=False, scope=None, reuse=None):\n    return mobilenetv3small(x, 1.0, 5, 0.25, hard_swish,\n                            is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'mobilenet100v3smallmini\')\n@set_args(__args__)\ndef mobilenet100v3smallmini(x, is_training=False, classes=1000,\n                            stem=False, scope=None, reuse=None):\n    return mobilenetv3small(x, 1.0, 3, 0, relu,\n                            is_training, classes, stem, scope, reuse)\n\n\n# Simple alias.\nMobileNet25 = mobilenet25\nMobileNet50 = mobilenet50\nMobileNet75 = mobilenet75\nMobileNet100 = mobilenet100\nMobileNet35v2 = mobilenet35v2\nMobileNet50v2 = mobilenet50v2\nMobileNet75v2 = mobilenet75v2\nMobileNet100v2 = mobilenet100v2\nMobileNet130v2 = mobilenet130v2\nMobileNet140v2 = mobilenet140v2\nMobileNet75v3 = MobileNet75v3large = mobilenet75v3large\nMobileNet100v3 = MobileNet100v3large = mobilenet100v3large\nMobileNet100v3largemini = mobilenet100v3largemini\nMobileNet75v3small = mobilenet75v3small\nMobileNet100v3small = mobilenet100v3small\nMobileNet100v3smallmini = mobilenet100v3smallmini\n'"
tensornets/nasnets.py,0,"b'""""""Collection of NASNet variants\n\nThe reference papers:\n\n1. Original (a.k.a. NASNet)\n - Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018 (arXiv 2017)\n - Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le\n - https://arxiv.org/abs/1707.07012\n2. PNASNet\n - Progressive Neural Architecture Search, ECCV 2018 (arXiv 2017)\n - Chenxi Liu et al.\n - https://arxiv.org/abs/1712.00559\n\nThe reference implementation:\n\n1. TF Slim\n - https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/{nasnet,pnasnet}.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import avg_pool2d\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import dropout\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import sconv2d\nfrom .layers import convbn as conv\nfrom .layers import sconvbn\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([avg_pool2d, max_pool2d], {\'padding\': \'SAME\', \'scope\': \'pool\'}),\n            ([batch_norm], {\'decay\': 0.9997, \'scale\': True, \'epsilon\': 0.001,\n                            \'is_training\': is_training, \'fused\': True,\n                            \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'biases_initializer\': None, \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training, \'scope\': \'dropout\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([sconv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                         \'biases_initializer\': None,\n                         \'scope\': \'sconv\'})]\n\n\n@var_scope(\'sconv\')\ndef sconv(x, filters, kernel_size, stride=1, scope=None):\n    x = relu(x)\n    x = sconvbn(x, filters, kernel_size, stride=stride,\n                depth_multiplier=1, scope=\'0\')\n    x = relu(x)\n    x = sconvbn(x, filters, kernel_size,\n                depth_multiplier=1, scope=\'1\')\n    return x\n\n\ndef nasnet(x, stem_filters, normals, filters, skip_reduction, use_aux,\n           scaling, is_training, classes, stem, scope=None, reuse=None):\n    x = conv(x, stem_filters, 3, stride=2, padding=\'VALID\', scope=\'conv0\')\n\n    x, p = reductionA(x, None, filters * scaling ** (-2), scope=\'stem1\')\n    x, p = reductionA(x, p, filters * scaling ** (-1), scope=\'stem2\')\n\n    for i in range(1, normals + 1):\n        x, p = normalA(x, p, filters, scope=""normal%d"" % i)\n\n    x, p0 = reductionA(x, p, filters * scaling,\n                       scope=""reduction%d"" % normals)\n    p = p0 if not skip_reduction else p\n\n    for i in range(normals + 1, normals * 2 + 1):\n        x, p = normalA(x, p, filters * scaling, scope=""normal%d"" % i)\n\n    if use_aux is True:\n        a = aux(x, classes, scope=\'aux\')\n\n    x, p0 = reductionA(x, p, filters * scaling ** 2,\n                       scope=""reduction%d"" % (normals * 2))\n    p = p0 if not skip_reduction else p\n\n    for i in range(normals * 2 + 1, normals * 3 + 1):\n        x, p = normalA(x, p, filters * scaling ** 2, scope=""normal%d"" % i)\n\n    x = relu(x, name=\'relu\')\n    if stem: return x\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.5, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'nasnetAlarge\')\n@set_args(__args__)\ndef nasnetAlarge(x, is_training=False, classes=1000,\n                 stem=False, scope=None, reuse=None):\n    return nasnet(x, 96, 6, 168, True, True, 2,\n                  is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'nasnetAmobile\')\n@set_args(__args__)\ndef nasnetAmobile(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    return nasnet(x, 32, 4, 44, False, True, 2,\n                  is_training, classes, stem, scope, reuse)\n\n\ndef pnasnet(x, stem_filters, blocks, filters,\n            scaling, is_training, classes, stem, scope=None, reuse=None):\n    x = conv(x, stem_filters, 3, stride=2, padding=\'VALID\', scope=\'conv0\')\n\n    x, p = normalP(x, None, filters * scaling ** (-2), stride=2, scope=\'stem1\')\n    x, p = normalP(x, p, filters * scaling ** (-1), stride=2, scope=\'stem2\')\n\n    for i in range(1, blocks + 1):\n        x, p = normalP(x, p, filters, scope=""normal%d"" % i)\n\n    x, p = normalP(x, p, filters * scaling, stride=2,\n                   scope=""reduction%d"" % blocks)\n\n    for i in range(blocks + 1, blocks * 2):\n        x, p = normalP(x, p, filters * scaling, scope=""normal%d"" % i)\n\n    x, p = normalP(x, p, filters * scaling ** 2, stride=2,\n                   scope=""reduction%d"" % (blocks * 2 - 1))\n\n    for i in range(blocks * 2, blocks * 3 - 1):\n        x, p = normalP(x, p, filters * scaling ** 2, scope=""normal%d"" % i)\n\n    x = relu(x, name=\'relu\')\n    if stem: return x\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = dropout(x, keep_prob=0.5, scope=\'dropout\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'pnasnetlarge\')\n@set_args(__args__)\ndef pnasnetlarge(x, is_training=False, classes=1000,\n                 stem=False, scope=None, reuse=None):\n    return pnasnet(x, 96, 4, 216, 2, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'adjust\')\ndef adjust(p, x, filters, scope=None):\n    if p is None:\n        p = x\n    elif int(p.shape[1]) != int(x.shape[1]):\n        p = relu(p, name=\'relu\')\n        p1 = avg_pool2d(p, 1, stride=2, padding=\'VALID\', scope=\'pool1\')\n        p1 = conv2d(p1, int(filters / 2), 1, scope=\'conv1\')\n        p2 = pad(p, [[0, 0], [0, 1], [0, 1], [0, 0]])[:, 1:, 1:, :]\n        p2 = avg_pool2d(p2, 1, stride=2, padding=\'VALID\', scope=\'pool2\')\n        p2 = conv2d(p2, int(filters / 2), 1, scope=\'conv2\')\n        p = concat([p1, p2], axis=3, name=\'concat\')\n        p = batch_norm(p, scope=\'bn\')\n    elif int(p.shape[-1]) != filters:\n        p = relu(p, name=\'relu\')\n        p = conv(p, filters, 1, scope=\'prev_1x1\')\n    return p\n\n\n@var_scope(\'normalA\')\ndef normalA(x, p, filters, scope=None):\n    """"""Normal cell for NASNet-A (Fig. 4 in the paper)""""""\n    p = adjust(p, x, filters)\n\n    h = relu(x)\n    h = conv(h, filters, 1, scope=\'1x1\')\n\n    x1 = add(sconv(h, filters, 3, scope=\'left1\'), h, name=\'add1\')\n    x2 = add(sconv(p, filters, 3, scope=\'left2\'),\n             sconv(h, filters, 5, scope=\'right2\'), name=\'add2\')\n    x3 = add(avg_pool2d(h, 3, stride=1, scope=\'left3\'), p, name=\'add3\')\n    x4 = add(avg_pool2d(p, 3, stride=1, scope=\'left4\'),\n             avg_pool2d(p, 3, stride=1, scope=\'right4\'), name=\'add4\')\n    x5 = add(sconv(p, filters, 5, scope=\'left5\'),\n             sconv(p, filters, 3, scope=\'right5\'), name=\'add5\')\n\n    return concat([p, x2, x5, x3, x4, x1], axis=3, name=\'concat\'), x\n\n\n@var_scope(\'reductionA\')\ndef reductionA(x, p, filters, scope=None):\n    """"""Reduction cell for NASNet-A (Fig. 4 in the paper)""""""\n    filters = int(filters)\n    p = adjust(p, x, filters)\n\n    h = relu(x)\n    h = conv(h, filters, 1, scope=\'1x1\')\n\n    x1 = add(sconv(p, filters, 7, stride=2, scope=\'left1\'),\n             sconv(h, filters, 5, stride=2, scope=\'right1\'), name=\'add1\')\n    x2 = add(max_pool2d(h, 3, scope=\'left2\'),\n             sconv(p, filters, 7, stride=2, scope=\'right2\'), name=\'add2\')\n    x3 = add(avg_pool2d(h, 3, scope=\'left3\'),\n             sconv(p, filters, 5, stride=2, scope=\'right3\'), name=\'add3\')\n    x4 = add(max_pool2d(h, 3, scope=\'left4\'),\n             sconv(x1, filters, 3, scope=\'right4\'), name=\'add4\')\n    x5 = add(avg_pool2d(x1, 3, stride=1, scope=\'left5\'), x2, name=\'add5\')\n\n    return concat([x2, x3, x5, x4], axis=3, name=\'concat\'), x\n\n\n@var_scope(\'pool\')\ndef pool(x, filters, stride, scope=None):\n    y = max_pool2d(x, 3, stride=stride)\n    infilters = int(x.shape[-1]) if tf_later_than(\'2\') else x.shape[-1].value\n    if infilters != filters:\n        y = conv(y, filters, 1, scope=\'1x1\')\n    return y\n\n\n@var_scope(\'normalP\')\ndef normalP(x, p, filters, stride=1, scope=None):\n    filters = int(filters)\n    p = adjust(p, x, filters)\n\n    h = relu(x)\n    h = conv(h, filters, 1, scope=\'1x1\')\n\n    x1 = add(sconv(p, filters, 5, stride, scope=\'left1\'),\n             pool(p, filters, stride, scope=\'right1\'), name=\'add1\')\n    x2 = add(sconv(h, filters, 7, stride, scope=\'left2\'),\n             pool(h, filters, stride, scope=\'right2\'), name=\'add2\')\n    x3 = add(sconv(h, filters, 5, stride, scope=\'left3\'),\n             sconv(h, filters, 3, stride, scope=\'right3\'), name=\'add3\')\n    x4 = add(sconv(x3, filters, 3, scope=\'left4\'),\n             pool(h, filters, stride, scope=\'right4\'), name=\'add4\')\n    x5 = add(\n        sconv(p, filters, 3, stride, scope=\'left5\'),\n        conv(relu(h), filters, 1, stride, scope=\'right5\') if stride > 1 else h,\n        name=\'add5\')\n\n    return concat([x1, x2, x3, x4, x5], axis=3, name=\'concat\'), x\n\n\n@var_scope(\'aux\')\ndef aux(x, classes, scope=None):\n    x = relu(x, name=\'relu1\')\n    x = avg_pool2d(x, 5, stride=3, padding=\'VALID\', scope=\'pool\')\n    x = conv(x, 128, 1, scope=\'proj\')\n    x = relu(x, name=\'relu2\')\n    x = conv(x, 768, int(x.shape[1]), padding=\'VALID\', scope=\'conv\')\n    x = relu(x, name=\'relu3\')\n    x = squeeze(x, [1, 2], name=\'squeeze\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n# Simple alias.\nNASNetAlarge = nasnetAlarge\nNASNetAmobile = nasnetAmobile\nPNASNetlarge = pnasnetlarge\n'"
tensornets/ops.py,51,"b""from __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom .utils import ops_to_outputs\nfrom .version_utils import tf_later_than\n\n\nif tf_later_than('1.14'):\n    tf = tf.compat.v1\n\n\ntry:\n    reduce\nexcept NameError:\n    from functools import reduce\n\n\nif tf_later_than('1.6'):\n    # Note that `tf.nn.leaky_relu` has existed since 1.4.0,\n    # but 1.4.0, 1.4.1, 1.5.0, 1.5.1 do not support float16.\n    _leaky_relu = tf.nn.leaky_relu\nelse:\n    def _leaky_relu(x, alpha=0.2, name=None):\n        return tf.add(tf.nn.relu(x), -alpha * tf.nn.relu(-x), name=name)\n\n\nif tf_later_than('1.5'):\n    # Note that `tf.nn.swish` has existed since 1.5.0.\n    _swish = tf.nn.swish\nelse:\n    def _swish(x, name=None):\n        return tf.multiply(x, tf.sigmoid(x), name=name)\n\n\nif tf_later_than('1.5'):\n    # Note that `tf.reduce_mean` has existed since 1.0,\n    # but the parameter name `keep_dims` has been changed to `keepdims`.\n    _reduce_mean = tf.reduce_mean\nelse:\n    def _reduce_mean(input_tensor, axis=None, keepdims=False, name=None):\n        return tf.reduce_mean(input_tensor, axis=axis, keep_dims=keepdims,\n                              name=name)\n\n\ndef _hard_sigmoid(x, name=None):\n    return tf.divide(tf.nn.relu6(x + 3.), 6., name=name)\n\n\ndef _hard_swish(x, name=None):\n    return tf.multiply(x, tf.nn.relu6(x + 3.) / 6., name=name)\n\n\nargmax = ops_to_outputs(tf.argmax)\nadd = ops_to_outputs(tf.add)\nconcat = ops_to_outputs(tf.concat)\nconv2d_primitive = ops_to_outputs(tf.nn.conv2d)\nexpand_dims = ops_to_outputs(tf.expand_dims)\ngather = ops_to_outputs(tf.gather)\nhard_sigmoid = ops_to_outputs(_hard_sigmoid)\nhard_swish = ops_to_outputs(_hard_swish)\nleaky_relu = ops_to_outputs(_leaky_relu)\nlrn = ops_to_outputs(tf.nn.lrn)\nmaximum = ops_to_outputs(tf.maximum)\nmultiply = ops_to_outputs(tf.multiply)\none_hot = ops_to_outputs(tf.one_hot)\npad = ops_to_outputs(tf.pad)\nreduce_max = ops_to_outputs(tf.reduce_max)\nreduce_mean = ops_to_outputs(_reduce_mean)\nreduce_sum = ops_to_outputs(tf.reduce_sum)\nrelu = ops_to_outputs(tf.nn.relu)\nrelu6 = ops_to_outputs(tf.nn.relu6)\nreshape = ops_to_outputs(tf.reshape)\nsigmoid = ops_to_outputs(tf.sigmoid)\nsoftmax = ops_to_outputs(tf.nn.softmax)\nsqrt = ops_to_outputs(tf.sqrt)\nsquare = ops_to_outputs(tf.square)\nsqueeze = ops_to_outputs(tf.squeeze)\nstack = ops_to_outputs(tf.stack)\nswish = ops_to_outputs(_swish)\ntanh = ops_to_outputs(tf.tanh)\nto_int32 = ops_to_outputs(tf.to_int32)\n\n\n@ops_to_outputs\ndef srn(x, depth_radius, alpha=1.0, beta=0.5, name=None):\n    # Refer to the following code snippet\n    # https://github.com/tensorflow/tensorflow/issues/1246#issuecomment-188588051\n    squared_sum = tf.nn.depthwise_conv2d(\n        tf.square(x),\n        tf.ones([depth_radius] * 2 + [tf.shape(x)[3], 1], dtype=tf.float32),\n        [1, 1, 1, 1],\n        'SAME')\n    alpha = tf.constant(alpha / (depth_radius ** 2), dtype=tf.float32)\n    beta = tf.constant(beta, dtype=tf.float32)\n    return tf.divide(x, (1.0 + alpha * squared_sum) ** beta, name=name)\n\n\n@ops_to_outputs\ndef upsample(x, stride, name=None):\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    assert isinstance(stride, tuple)\n    b = tf.shape(x)[0]\n    h = tf.shape(x)[1] * stride[0]\n    w = tf.shape(x)[2] * stride[1]\n    c = int(x.shape[-1]) if tf_later_than('2') else x.shape[-1].value\n    x = tf.expand_dims(x, 2)\n    x = tf.expand_dims(x, 4)\n    x = tf.tile(x, (1, 1, stride[0], 1, stride[1], 1))\n    return tf.reshape(x, (b, h, w, c), name=name)\n\n\n@ops_to_outputs\ndef local_flatten(x, kernel_size, name=None):\n    if isinstance(kernel_size, int):\n        kernel_size = (kernel_size, kernel_size)\n    assert isinstance(kernel_size, tuple)\n    x = [[tf.strided_slice(x, (0, i, j), tf.shape(x)[:-1], (1,) + kernel_size)\n          for j in range(kernel_size[1])] for i in range(kernel_size[0])]\n    return tf.concat(reduce(lambda x, y: x + y, x), axis=-1, name=name)\n"""
tensornets/preprocess.py,0,"b""import numpy as np\n\ntry:\n    import cv2\nexcept ImportError:\n    cv2 = None\n\n\ndef preprocess(scopes, inputs):\n    import warnings\n    from .utils import parse_scopes\n    if not isinstance(scopes, list):\n        scopes = [scopes]\n    outputs = []\n    for scope in scopes:\n        model_name = parse_scopes(scope)[0]\n        try:\n            outputs.append(__preprocess_dict__[model_name](inputs))\n        except KeyError:\n            found = False\n            for (key, fun) in __preprocess_dict__.items():\n                if key in model_name.lower():\n                    found = True\n                    outputs.append(fun(inputs))\n                    break\n            if not found:\n                warnings.warn('No pre-processing will be performed '\n                              'because the pre-processing for ' +\n                              model_name + ' are not found.')\n                outputs.append(inputs)\n    if len(outputs) == 1:\n        outputs = outputs[0]\n    return outputs\n\n\ndef direct(model_name, target_size):\n    if 'yolo' in model_name.lower():\n        def _direct(inputs):\n            return __preprocess_dict__[model_name](inputs, target_size)\n    else:\n        def _direct(inputs):\n            return __preprocess_dict__[model_name](inputs)\n    return _direct\n\n\ndef bair_preprocess(x):\n    # Refer to the following BAIR Caffe Model Zoo\n    # https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt\n    x = x.copy()\n    x = x[:, :, :, ::-1]\n    x[:, :, :, 0] -= 104.\n    x[:, :, :, 1] -= 117.\n    x[:, :, :, 2] -= 123.\n    return x\n\n\ndef tfslim_preprocess(x):\n    # Copied from keras (equivalent to the same as in TF Slim)\n    x = x.copy()\n    x /= 255.\n    x -= 0.5\n    x *= 2.\n    return x\n\n\ndef keras_resnet_preprocess(x):\n    # Copied from keras\n    x = x.copy()\n    x = x[:, :, :, ::-1]\n    x[:, :, :, 0] -= 103.939\n    x[:, :, :, 1] -= 116.779\n    x[:, :, :, 2] -= 123.68\n    return x\n\n\ndef fb_preprocess(x):\n    # Refer to the following Torch ResNets\n    # https://github.com/facebook/fb.resnet.torch/blob/master/pretrained/classify.lua\n    x = x.copy()\n    x /= 255.\n    x[:, :, :, 0] -= 0.485\n    x[:, :, :, 1] -= 0.456\n    x[:, :, :, 2] -= 0.406\n    x[:, :, :, 0] /= 0.229\n    x[:, :, :, 1] /= 0.224\n    x[:, :, :, 2] /= 0.225\n    return x\n\n\ndef wrn_preprocess(x):\n    # Refer to the following Torch WideResNets\n    # https://github.com/szagoruyko/wide-residual-networks/blob/master/pytorch/main.py\n    x = x.copy()\n    x /= 255.\n    x[:, :, :, 0] -= 0.491\n    x[:, :, :, 1] -= 0.482\n    x[:, :, :, 2] -= 0.447\n    x[:, :, :, 0] /= 0.247\n    x[:, :, :, 1] /= 0.244\n    x[:, :, :, 2] /= 0.262\n    return x\n\n\ndef darknet_preprocess(x, target_size=None):\n    # Refer to the following darkflow\n    # https://github.com/thtrieu/darkflow/blob/master/darkflow/net/yolo/predict.py\n    if target_size is None or target_size[0] is None or target_size[1] is None:\n        y = x.copy()\n    else:\n        h, w = target_size\n        assert cv2 is not None, 'resizing requires `cv2`.'\n        y = np.zeros((len(x), h, w, x.shape[3]))\n        for i in range(len(x)):\n            y[i] = cv2.resize(x[i], (w, h), interpolation=cv2.INTER_CUBIC)\n    y = y[:, :, :, ::-1]\n    y /= 255.\n    return y\n\n\ndef faster_rcnn_preprocess(x):\n    # Refer to the following py-faster-rcnn\n    # https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/fast_rcnn/test.py#L22\n    # https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/fast_rcnn/config.py#L181\n    y = x.copy()\n    y[:, :, :, 0] -= 102.9801\n    y[:, :, :, 1] -= 115.9465\n    y[:, :, :, 2] -= 122.7717\n    return y\n\n\n# Dictionary for pre-processing functions.\n__preprocess_dict__ = {\n    'inception': tfslim_preprocess,\n    'inception1': bair_preprocess,\n    'inception2': tfslim_preprocess,\n    'inception3': tfslim_preprocess,\n    'inception4': tfslim_preprocess,\n    'inceptionresnet2_tfslim': tfslim_preprocess,\n    'resnet': keras_resnet_preprocess,\n    'resnet50': keras_resnet_preprocess,\n    'resnet101': keras_resnet_preprocess,\n    'resnet152': keras_resnet_preprocess,\n    'resnetv2': tfslim_preprocess,\n    'resnet50v2': tfslim_preprocess,\n    'resnet101v2': tfslim_preprocess,\n    'resnet152v2': tfslim_preprocess,\n    'resnet200v2': fb_preprocess,\n    'resnext': fb_preprocess,\n    'resnext50': fb_preprocess,\n    'resnext101': fb_preprocess,\n    'resnext50c32': fb_preprocess,\n    'resnext101c32': fb_preprocess,\n    'resnext101c64': fb_preprocess,\n    'wideresnet50': wrn_preprocess,\n    'nasnetAlarge': tfslim_preprocess,\n    'nasnetAmobile': tfslim_preprocess,\n    'pnasnetlarge': tfslim_preprocess,\n    'vgg16': keras_resnet_preprocess,\n    'vgg19': keras_resnet_preprocess,\n    'densenet': fb_preprocess,\n    'densenet121': fb_preprocess,\n    'densenet169': fb_preprocess,\n    'densenet201': fb_preprocess,\n    'mobilenet': tfslim_preprocess,\n    'mobilenet25': tfslim_preprocess,\n    'mobilenet50': tfslim_preprocess,\n    'mobilenet75': tfslim_preprocess,\n    'mobilenet100': tfslim_preprocess,\n    'mobilenetv2': tfslim_preprocess,\n    'mobilenet35v2': tfslim_preprocess,\n    'mobilenet50v2': tfslim_preprocess,\n    'mobilenet75v2': tfslim_preprocess,\n    'mobilenet100v2': tfslim_preprocess,\n    'mobilenet130v2': tfslim_preprocess,\n    'mobilenet140v2': tfslim_preprocess,\n    'mobilenet75v3large': tfslim_preprocess,\n    'mobilenet100v3large': tfslim_preprocess,\n    'mobilenet100v3largemini': tfslim_preprocess,\n    'mobilenet75v3small': tfslim_preprocess,\n    'mobilenet100v3small': tfslim_preprocess,\n    'mobilenet100v3smallmini': tfslim_preprocess,\n    'efficientnet': fb_preprocess,\n    'efficientnetb0': fb_preprocess,\n    'efficientnetb1': fb_preprocess,\n    'efficientnetb2': fb_preprocess,\n    'efficientnetb3': fb_preprocess,\n    'efficientnetb4': fb_preprocess,\n    'efficientnetb5': fb_preprocess,\n    'efficientnetb6': fb_preprocess,\n    'efficientnetb7': fb_preprocess,\n    'squeezenet': bair_preprocess,\n    'zf': faster_rcnn_preprocess,\n    'darknet19': darknet_preprocess,\n    'tinydarknet19': darknet_preprocess,\n    'REFyolov3coco': darknet_preprocess,\n    'REFyolov3voc': darknet_preprocess,\n    'REFyolov2coco': darknet_preprocess,\n    'REFyolov2voc': darknet_preprocess,\n    'REFtinyyolov2voc': darknet_preprocess,\n    'REFfasterrcnnZFvoc': faster_rcnn_preprocess,\n    'REFfasterrcnnVGG16voc': faster_rcnn_preprocess,\n    'genYOLOv2': darknet_preprocess,\n    'genTinyYOLOv2': darknet_preprocess,\n    'genFasterRCNN': faster_rcnn_preprocess,\n}\n"""
tensornets/pretrained.py,2,"b'""""""Collection of pretrained models.\n\nThis module provides loading functions for pre-trained weights.\nAll the weight files are converted as a Keras-like format that\nserializes every single tensor from the following repositories:\n\n[1]: https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet\n     ""BAIR Caffe Model Zoo""\n[2]: https://github.com/tensorflow/models/tree/master/research/slim\n     ""TF Slim""\n[3]: https://github.com/keras-team/keras/tree/master/keras/applications\n     ""Keras""\n[4]: https://github.com/KaimingHe/deep-residual-networks\n     ""Caffe ResNets""\n[5]: https://github.com/facebook/fb.resnet.torch\n     ""Torch ResNets""\n[6]: https://github.com/facebookresearch/ResNeXt\n     ""Torch ResNeXts""\n[7]: https://github.com/liuzhuang13/DenseNet\n     ""Torch DenseNets""\n[8]: https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1\n     ""Caffe SqueezeNets""\n[9]: https://github.com/szagoruyko/wide-residual-networks\n     ""Torch WideResNets""\n[10]: https://pjreddie.com/darknet/yolo/\n     ""Darknet""\n[11]: https://github.com/thtrieu/darkflow\n     ""TF darkflow""\n[12]: https://github.com/rbgirshick/py-faster-rcnn\n     ""Caffe Faster RCNN""\n[13]: https://github.com/tensorflow/tpu/blob/master/models/official/\n     ""TF TPU""\n""""""\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\nimport warnings\n\nfrom .utils import get_file\nfrom .utils import init\nfrom .utils import parse_scopes\nfrom .utils import parse_weights\nfrom .utils import parse_keras_weights\nfrom .utils import parse_torch_weights\nfrom .utils import pretrained_initializer\n\n\n__keras_url__ = \'https://github.com/fchollet/deep-learning-models/\' \\\n                \'releases/download/v0.2/\'\n__model_url__ = \'https://github.com/taehoonlee/deep-learning-models/\' \\\n                \'releases/download/\'\n\n\ndef assign(scopes):\n    if not isinstance(scopes, list):\n        scopes = [scopes]\n    for scope in scopes:\n        model_name = parse_scopes(scope)[0]\n        try:\n            __load_dict__[model_name](scope)\n        except KeyError:\n            try:\n                tf.get_default_session().run(scope.pretrained())\n            except:\n                found = False\n                for (key, fun) in __load_dict__.items():\n                    if key in model_name.lower():\n                        found = True\n                        fun(scope)\n                        break\n                if not found:\n                    warnings.warn(\'Random initialization will be performed \'\n                                  \'because the pre-trained weights for \' +\n                                  model_name + \' are not found.\')\n                    init(scope)\n\n\ndef direct(model_name, scope):\n    if model_name.startswith(\'gen\'):\n        model_name = model_name[3:].lower()\n        stem_name = scope.stem.model_name\n        try:\n            fun = __gen_load_dict__[model_name][stem_name]\n        except KeyError:\n            fun = load_nothing\n            warnings.warn(\'When `pretrained` is called, random \'\n                          \'initialization will be performed because \'\n                          \'the pre-trained weights for \' + model_name +\n                          \' with \' + stem_name + \' are not found.\')\n    else:\n        try:\n            fun = __load_dict__[model_name]\n        except KeyError:\n            fun = load_nothing\n            warnings.warn(\'When `pretrained` is called, random \'\n                          \'initialization will be performed because \'\n                          \'the pre-trained weights for \' + model_name +\n                          \' are not found.\')\n\n    def _direct():\n        return fun(scope, return_fn=pretrained_initializer)\n    return _direct\n\n\ndef _assign(scopes, values):\n    sess = tf.get_default_session()\n    assert sess is not None, \'The default session should be given.\'\n\n    scopes = parse_scopes(scopes)\n\n    for scope in scopes:\n        sess.run(pretrained_initializer(scope, values))\n\n\ndef load_nothing(scopes, return_fn=_assign):\n    return return_fn(scopes, None)\n\n\ndef load_inception1(scopes, return_fn=_assign):\n    """"""Converted from the [BAIR Caffe Model Zoo][1].""""""\n    filename = \'inception1.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'6a212e3cb60b33f49c372906f18ae4a8\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_inception2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'inception2.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'0476b876a5d35a99e2747f98248d856d\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_inception3(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'inception3.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'546b76313eb0fd8037ae5108c850c9e9\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_inception4(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'inception4.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'8d5a0e8cb451c85112d5c4e363d77a42\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_keras_inception3(scopes, return_fn=_assign):\n    """"""Copied from [keras][3] with modifications on the order of weights.""""""\n    filename = \'inception3.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'7c4556613c348da3b99b633e1c430fff\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_inceptionresnet2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'inception_resnet_v2_2016_08_30.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'inception/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'32d685e68e6be6ba1da64e41f939bc49\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet50(scopes, return_fn=_assign):\n    """"""Converted from the original [Caffe ResNets][4].""""""\n    filename = \'resnet50.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'9df0843bdadb58ed24d360564c45b119\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet101(scopes, return_fn=_assign):\n    """"""Converted from the original [Caffe ResNets][4].""""""\n    filename = \'resnet101.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'e2434bec605870fb4747e1b93f9f0e47\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet152(scopes, return_fn=_assign):\n    """"""Converted from the original [Caffe ResNets][4].""""""\n    filename = \'resnet152.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'e588285d1f919e538515c1f1b1c07b5b\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet50v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'resnet_v2_50.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'fa2ac006361fd5e79792d163c0130667\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet101v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'resnet_v2_101.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'fbc179d55c817e4656992fa582fdc460\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet152v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'resnet_v2_152.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'184c9b439e925762f006d288445997a8\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_resnet200v2(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNets][5].""""""\n    filename = \'resnet_v2_200.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'5bff85adbbc499e200c4bf4dc89cde87\')\n    values = parse_weights(weights_path, move_rules_fb_resnet_torch)\n    return return_fn(scopes, values)\n\n\ndef load_resnext50(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_50_32x4d.npz\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'eead54b31dc282df2c433cc5a0b420e4\')\n    values = parse_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_resnext101(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_101_32x4d.npz\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'f45edc32440c4314563ca57e4e54e26c\')\n    values = parse_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_resnext101c64(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_101_64x4d.npz\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'f226e85525aa2529dc0547c170042082\')\n    values = parse_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_wideresnet50(scopes, return_fn=_assign):\n    """"""Converted from the [Torch WideResNets][9].""""""\n    filename = \'wrn_50_2.npz\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'dd8fcad081890a2685638166d2c2d3f9\')\n    values = parse_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_keras_resnet50(scopes, return_fn=_assign):\n    """"""Copied from [keras][3].""""""\n    filename = \'resnet50_weights_tf_dim_ordering_tf_kernels.h5\'\n    weights_path = get_file(\n        filename, __keras_url__ + filename,\n        cache_subdir=\'models\',\n        file_hash=\'a7b3fe01876f51b976af0dea6bc144eb\')\n    move_rules = []\n    for i in range(2, 6):\n        move_rules.append((""bn%da_branch2c"" % i, -1))\n        move_rules.append((""res%da_branch1"" % i, -6))\n        move_rules.append((""bn%da_branch1"" % i, -6))\n    values = parse_keras_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\nmove_rules_fb_resnet_torch = []\nfor i in range(4, 8):\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.0.weight"" % i, -18))\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.0.bias"" % i, -18))\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.1.weight"" % i, -18))\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.1.bias"" % i, -18))\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.1.running_mean"" % i, -18))\n    move_rules_fb_resnet_torch.append((""%d.0.0.1.1.running_var"" % i, -18))\n\n\ndef load_torch_resnet50(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNets][5].""""""\n    filename = \'resnet_50_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'5b38c39802c94de00b55596145d304aa\')\n    values = parse_torch_weights(weights_path, move_rules_fb_resnet_torch)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnet101(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNets][5].""""""\n    filename = \'resnet_101_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'cb3f0ac4687cb63d5f0861d651da844b\')\n    values = parse_torch_weights(weights_path, move_rules_fb_resnet_torch)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnet152(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNets][5].""""""\n    filename = \'resnet_152_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'3339f6aca7f746f8ae7f6ce577efc0c0\')\n    values = parse_torch_weights(weights_path, move_rules_fb_resnet_torch)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnet200v2(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNets][5].""""""\n    filename = \'resnet_200_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'220df3970701d3e0608eed887fb95d82\')\n    values = parse_torch_weights(weights_path, move_rules_fb_resnet_torch)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnext50(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_50_32x4d_cpu.pth\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'fdfc372bc47f7bf55313c04aebcef8ca\')\n    values = parse_torch_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnext101(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_101_32x4d_cpu.pth\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'5e97757d9f898aa8174fe8bc6e59bce8\')\n    values = parse_torch_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_torch_resnext101c64(scopes, return_fn=_assign):\n    """"""Converted from the [Torch ResNeXts][6].""""""\n    filename = \'resnext_101_64x4d_cpu.pth\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'03c83fe32db97676eace16cc0b577cc2\')\n    values = parse_torch_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_torch_wideresnet50(scopes, return_fn=_assign):\n    """"""Converted from the [Torch WideResNets][9].""""""\n    filename = \'wrn_50_2_cpu.pth\'\n    move_rules = [(r, -15) for (r, i) in move_rules_fb_resnet_torch\n                  if \'1.0.bias\' not in r]\n    weights_path = get_file(\n        filename, __model_url__ + \'resnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'7879cd9f3840f92593a87b6be8192206\')\n    values = parse_torch_weights(weights_path, move_rules)\n    return return_fn(scopes, values)\n\n\ndef load_densenet121(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet121.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'c65564d54d4f7d29da6c84865325d7d4\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_densenet169(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet169.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'f17f3aa42e92489a1e3b981061cd3d96\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_densenet201(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet201.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'d2c503199fc0c5537ca2aa3d723cc493\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_torch_densenet121(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet_121_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'9817430b1d3634645f6b04b8c663c34f\')\n    values = parse_torch_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_torch_densenet169(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet_169_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'98c5cac06124192627391adf17d66493\')\n    values = parse_torch_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_torch_densenet201(scopes, return_fn=_assign):\n    """"""Converted from the [Torch DenseNets][7].""""""\n    filename = \'densenet_201_cpu.pth\'\n    weights_path = get_file(\n        filename, __model_url__ + \'densenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'fa3aa0454be559b81409e92f3bafd155\')\n    values = parse_torch_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet25(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet25.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'aa1f5ccfb8be3d1ef45948a396e04e0a\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet50(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet50.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'0c0b667bc9d707e0e5bd4f383c5dece0\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet75(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet75.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'d4557a46a44eebfeaf08c82ae33765ed\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet100.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'3d14409e3e119c8881baf7dd1d54e714\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet35v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_035_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'cf758f7f8024d39365e553ec924bb395\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet50v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_050_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'218d51cd1b12b03ece24054029e7005b\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet75v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_075_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'25b5f6c93ebec7558a757e7a70b16b1c\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_100_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'ea55ba8d51df1df59b196d2508a3f262\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet130v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_130_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'a2470b36675853bbe107a406b50cd648\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet140v2(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_v2_140_224.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'6988a66bb89a088ce20e2ae97adca88b\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet75v3large(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_75_v3_large.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'be179da8b268a789b8d0ac88f0336a75\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100v3large(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_100_v3_large.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'2c9ae9b64dd9b1f02b9194f2fd7791b9\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100v3largemini(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_100_v3_large_mini.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'591595f3f5f87cc28662e37373593fa5\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet75v3small(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_75_v3_small.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'e3f4bffd40a283b8f1e2bce57f515d1d\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100v3small(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_100_v3_small.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'336c0ceb72acbc9aeac8b91297f84457\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_mobilenet100v3smallmini(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'mobilenet_100_v3_small_mini.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'mobilenetv3/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'d83bf7f022feebfaa924a9f63232363d\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb0(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb0.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'32aaa7fec51d344765bf41244212e519\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb1(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb1.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'1651447ea3c7284351b748aad08184fd\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb2(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb2.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'5e3a7641b6d25ed8a05f55ad34cd6cea\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb3(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb3.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'cdcce2b2124ea4bd0fa9c1e0fbd2a280\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb4(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb4.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'7689e44a5c317489b6c5758c32b29121\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb5(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb5.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'eae93e0ab8e6c627cc3367b80a977eaa\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb6(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb6.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'995d5690e256727e3ec810c4f54b31d0\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_efficientnetb7(scopes, return_fn=_assign):\n    """"""Converted from the [TF TPU][13].""""""\n    filename = \'efficientnetb7.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'efficientnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'a19799fd49953321f514400a1d6ba44a\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_squeezenet(scopes, return_fn=_assign):\n    """"""Converted from the [Caffe SqueezeNets][8].""""""\n    filename = \'squeezenet.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'squeezenet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'1d474f6540f7ec34cb56e6440419b5c5\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_nasnetAlarge(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'nasnet-a_large_04_10_2017.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'nasnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'f14c166457ce43b2c44d4cd3b6325bd6\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_nasnetAmobile(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'nasnet-a_mobile_04_10_2017.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'nasnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'7d75cfc284185c0a6db5cbf9f0492c59\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_pnasnetlarge(scopes, return_fn=_assign):\n    """"""Converted from the [TF Slim][2].""""""\n    filename = \'pnasnet_large.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'nasnet/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'a1afc7679b950b643865aa7ea716c901\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_vgg16(scopes, return_fn=_assign):\n    """"""Copied from [Keras][3].""""""\n    filename = \'vgg16_weights_tf_dim_ordering_tf_kernels.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'vgg/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'64373286793e3c8b2b4e3219cbf3544b\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_vgg19(scopes, return_fn=_assign):\n    """"""Copied from [Keras][3].""""""\n    filename = \'vgg19_weights_tf_dim_ordering_tf_kernels.h5\'\n    weights_path = get_file(\n        filename, __model_url__ + \'vgg/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'cbe5617147190e668d6c5d5026f83318\')\n    values = parse_keras_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_darknet19(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'darknet19.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'645132dcf25a18bf033e829646f90275\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_yolo_v3_coco(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'ref_yolo_v3_coco.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'934a1360a4ff6a65c194a221aa1d60b9\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_yolo_v3_voc(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'ref_yolo_v3_voc.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'f982ecc1393c6c8a23673b59b84be71d\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_yolo_v2_coco(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'ref_yolo_v2.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'bacf9f08bc229d11287a4fa3736a6bad\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_yolo_v2_voc(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'ref_yolo_v2_voc.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'5d7e3c739f8876ee5facbdc5ec6e53d5\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_tiny_yolo_v2_voc(scopes, return_fn=_assign):\n    """"""Converted from the original [Darknet][10] using the [darkflow][11].""""""\n    filename = \'ref_tiny_yolo_v2_voc.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'yolo/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'e1ec6a037a217811e08568b105c22c0f\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_faster_rcnn_zf_voc(scopes, return_fn=_assign):\n    """"""Converted from the [Caffe Faster RCNN][12].""""""\n    filename = \'ref_faster_rcnn_zf_voc.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'rcnn/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'825577525217176903ee9b096bb1cb64\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\ndef load_ref_faster_rcnn_vgg16_voc(scopes, return_fn=_assign):\n    """"""Converted from the [Caffe Faster RCNN][12].""""""\n    filename = \'ref_faster_rcnn_vgg16_voc.npz\'\n    weights_path = get_file(\n        filename, __model_url__ + \'rcnn/\' + filename,\n        cache_subdir=\'models\',\n        file_hash=\'2dfa10d64c169cf105b36c209be91316\')\n    values = parse_weights(weights_path)\n    return return_fn(scopes, values)\n\n\n# Dictionary for loading functions.\n__load_dict__ = {\n    \'inception1\': load_inception1,\n    \'inception2\': load_inception2,\n    \'inception3\': load_inception3,\n    \'inception4\': load_inception4,\n    \'inceptionresnet2_tfslim\': load_inceptionresnet2,\n    \'resnet50\': load_resnet50,\n    \'resnet101\': load_resnet101,\n    \'resnet152\': load_resnet152,\n    \'resnet50v2\': load_resnet50v2,\n    \'resnet101v2\': load_resnet101v2,\n    \'resnet152v2\': load_resnet152v2,\n    \'resnet200v2\': load_resnet200v2,\n    \'resnext50\': load_resnext50,\n    \'resnext101\': load_resnext101,\n    \'resnext50c32\': load_resnext50,\n    \'resnext101c32\': load_resnext101,\n    \'resnext101c64\': load_resnext101c64,\n    \'wideresnet50\': load_wideresnet50,\n    \'nasnetAlarge\': load_nasnetAlarge,\n    \'nasnetAmobile\': load_nasnetAmobile,\n    \'pnasnetlarge\': load_pnasnetlarge,\n    \'vgg16\': load_vgg16,\n    \'vgg19\': load_vgg19,\n    \'densenet121\': load_densenet121,\n    \'densenet169\': load_densenet169,\n    \'densenet201\': load_densenet201,\n    \'mobilenet25\': load_mobilenet25,\n    \'mobilenet50\': load_mobilenet50,\n    \'mobilenet75\': load_mobilenet75,\n    \'mobilenet100\': load_mobilenet100,\n    \'mobilenet35v2\': load_mobilenet35v2,\n    \'mobilenet50v2\': load_mobilenet50v2,\n    \'mobilenet75v2\': load_mobilenet75v2,\n    \'mobilenet100v2\': load_mobilenet100v2,\n    \'mobilenet130v2\': load_mobilenet130v2,\n    \'mobilenet140v2\': load_mobilenet140v2,\n    \'mobilenet75v3large\': load_mobilenet75v3large,\n    \'mobilenet100v3large\': load_mobilenet100v3large,\n    \'mobilenet100v3largemini\': load_mobilenet100v3largemini,\n    \'mobilenet75v3small\': load_mobilenet75v3small,\n    \'mobilenet100v3small\': load_mobilenet100v3small,\n    \'mobilenet100v3smallmini\': load_mobilenet100v3smallmini,\n    \'efficientnetb0\': load_efficientnetb0,\n    \'efficientnetb1\': load_efficientnetb1,\n    \'efficientnetb2\': load_efficientnetb2,\n    \'efficientnetb3\': load_efficientnetb3,\n    \'efficientnetb4\': load_efficientnetb4,\n    \'efficientnetb5\': load_efficientnetb5,\n    \'efficientnetb6\': load_efficientnetb6,\n    \'efficientnetb7\': load_efficientnetb7,\n    \'squeezenet\': load_squeezenet,\n    \'zf\': load_nothing,\n    \'darknet19\': load_darknet19,\n    \'tinydarknet19\': load_nothing,\n    \'REFyolov3coco\': load_ref_yolo_v3_coco,\n    \'REFyolov3voc\': load_ref_yolo_v3_voc,\n    \'REFyolov2coco\': load_ref_yolo_v2_coco,\n    \'REFyolov2voc\': load_ref_yolo_v2_voc,\n    \'REFtinyyolov2voc\': load_ref_tiny_yolo_v2_voc,\n    \'REFfasterrcnnZFvoc\': load_ref_faster_rcnn_zf_voc,\n    \'REFfasterrcnnVGG16voc\': load_ref_faster_rcnn_vgg16_voc,\n}\n\n__gen_load_dict__ = {\n    \'fasterrcnn\': {\n        \'vgg16\': load_ref_faster_rcnn_vgg16_voc,\n        \'zf\': load_ref_faster_rcnn_zf_voc,\n    },\n    \'tinyyolov2\': {\n        \'tinydarknet19\': load_ref_tiny_yolo_v2_voc,\n    },\n    \'yolov2\': {\n        \'darknet19\': load_ref_yolo_v2_voc,\n    },\n}\n'"
tensornets/resnets.py,0,"b'""""""Collection of ResNet variants\n\nThe reference papers:\n\n1. Original (a.k.a. v1)\n - Deep Residual Learning for Image Recognition, CVPR 2016 (Best Paper Award)\n - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n - https://arxiv.org/abs/1512.03385\n2. Pre-activation (a.k.a. v2)\n - Identity Mappings in Deep Residual Networks, ECCV 2016\n - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n - https://arxiv.org/abs/1603.05027\n3. ResNeXt\n - Aggregated Residual Transformations for Deep Neural Networks, CVPR 2017\n - Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, Kaiming He\n - https://arxiv.org/abs/1611.05431\n4. WideResNet\n - Wide Residual Networks, BMVC 2016\n - Sergey Zagoruyko, Nikos Komodakis\n - https://arxiv.org/abs/1605.07146\n\nThe reference implementations:\n\n1. (initially and mainly) Keras\n - https://github.com/keras-team/keras/blob/master/keras/applications/resnet50.py\n2. (to reproduce the original results) Caffe ResNet\n - https://github.com/KaimingHe/deep-residual-networks/tree/master/prototxt\n3. (to factorize over v2) Torch ResNets\n - https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua\n4. (to factorize over v3) Torch ResNeXts\n - https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua\n5. (mainly) Torch WideResNets\n - https://github.com/szagoruyko/wide-residual-networks/blob/master/pretrained\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import batch_norm\nfrom .layers import conv2d\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import sconv2d\nfrom .layers import convbn as conv\nfrom .layers import gconvbn as gconv\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([batch_norm], {\'scale\': True, \'is_training\': is_training,\n                            \'epsilon\': 1e-5, \'scope\': \'bn\'}),\n            ([conv2d], {\'padding\': \'VALID\', \'activation_fn\': None,\n                        \'scope\': \'conv\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([max_pool2d], {\'scope\': \'pool\'}),\n            ([sconv2d], {\'padding\': \'VALID\', \'activation_fn\': None,\n                         \'biases_initializer\': None,\n                         \'scope\': \'sconv\'})]\n\n\ndef resnet(x, preact, stack_fn, is_training, classes, stem,\n           scope=None, reuse=None):\n    x = pad(x, pad_info(7), name=\'conv1/pad\')\n    if preact:\n        x = conv2d(x, 64, 7, stride=2, scope=\'conv1\')\n    else:\n        x = conv(x, 64, 7, stride=2, scope=\'conv1\')\n        x = relu(x, name=\'conv1/relu\')\n    x = pad(x, pad_info(0 if stem else 3, symmetry=not preact),\n            \'SYMMETRIC\', name=\'pool1/pad\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool1\')\n    x = stack_fn(x)\n    if stem: return x\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'resnet50\')\n@set_args(__args__)\ndef resnet50(x, is_training=False, classes=1000,\n             stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stack(x, _block1, 64, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block1, 128, 4, scope=\'conv3\')\n        x = _stack(x, _block1, 256, 6, scope=\'conv4\')\n        x = _stack(x, _block1, 512, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet50v2\')\n@set_args(__args__)\ndef resnet50v2(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stacks(x, 64, 3, scope=\'conv2\')\n        x = _stacks(x, 128, 4, scope=\'conv3\')\n        x = _stacks(x, 256, 6, scope=\'conv4\')\n        x = _stacks(x, 512, 3, stride1=1, scope=\'conv5\')\n        x = batch_norm(x, scope=\'postnorm\')\n        x = relu(x)\n        return x\n    return resnet(x, True, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet101\')\n@set_args(__args__)\ndef resnet101(x, is_training=False, classes=1000,\n              stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stack(x, _block1, 64, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block1, 128, 4, scope=\'conv3\')\n        x = _stack(x, _block1, 256, 23, scope=\'conv4\')\n        x = _stack(x, _block1, 512, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet101v2\')\n@set_args(__args__)\ndef resnet101v2(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stacks(x, 64, 3, scope=\'conv2\')\n        x = _stacks(x, 128, 4, scope=\'conv3\')\n        x = _stacks(x, 256, 23, scope=\'conv4\')\n        x = _stacks(x, 512, 3, stride1=1, scope=\'conv5\')\n        x = batch_norm(x, scope=\'postnorm\')\n        x = relu(x)\n        return x\n    return resnet(x, True, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet152\')\n@set_args(__args__)\ndef resnet152(x, is_training=False, classes=1000,\n              stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stack(x, _block1, 64, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block1, 128, 8, scope=\'conv3\')\n        x = _stack(x, _block1, 256, 36, scope=\'conv4\')\n        x = _stack(x, _block1, 512, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet152v2\')\n@set_args(__args__)\ndef resnet152v2(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stacks(x, 64, 3, scope=\'conv2\')\n        x = _stacks(x, 128, 8, scope=\'conv3\')\n        x = _stacks(x, 256, 36, scope=\'conv4\')\n        x = _stacks(x, 512, 3, stride1=1, scope=\'conv5\')\n        x = batch_norm(x, scope=\'postnorm\')\n        x = relu(x)\n        return x\n    return resnet(x, True, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnet200v2\')\n@set_args(__args__)\ndef resnet200v2(x, is_training=False, classes=1000,\n                stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stack(x, _block2, 64, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block2, 128, 24, scope=\'conv3\')\n        x = _stack(x, _block2, 256, 36, scope=\'conv4\')\n        x = _stack(x, _block2, 512, 3, scope=\'conv5\')\n        x = batch_norm(x)\n        x = relu(x)\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnext50c32\')\n@set_args(__args__, conv_bias=False)\ndef resnext50c32(x, is_training=False, classes=1000,\n                 stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        def _block3c32(*args, **kwargs):\n            kwargs.update({\'groups\': 32})\n            return _block3(*args, **kwargs)\n        x = _stack(x, _block3c32, 128, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block3c32, 256, 4, scope=\'conv3\')\n        x = _stack(x, _block3c32, 512, 6, scope=\'conv4\')\n        x = _stack(x, _block3c32, 1024, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnext101c32\')\n@set_args(__args__, conv_bias=False)\ndef resnext101c32(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        def _block3c32(*args, **kwargs):\n            kwargs.update({\'groups\': 32})\n            return _block3(*args, **kwargs)\n        x = _stack(x, _block3c32, 128, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block3c32, 256, 4, scope=\'conv3\')\n        x = _stack(x, _block3c32, 512, 23, scope=\'conv4\')\n        x = _stack(x, _block3c32, 1024, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'resnext101c64\')\n@set_args(__args__, conv_bias=False)\ndef resnext101c64(x, is_training=False, classes=1000,\n                  stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        def _block3c64(*args, **kwargs):\n            kwargs.update({\'groups\': 64})\n            return _block3(*args, **kwargs)\n        x = _stack(x, _block3c64, 256, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _block3c64, 512, 4, scope=\'conv3\')\n        x = _stack(x, _block3c64, 1024, 23, scope=\'conv4\')\n        x = _stack(x, _block3c64, 2048, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'wideresnet50\')\n@set_args(__args__, conv_bias=False)\ndef wideresnet50(x, is_training=False, classes=1000,\n                 stem=False, scope=None, reuse=None):\n    def stack_fn(x):\n        x = _stack(x, _blockw, 128, 3, stride1=1, scope=\'conv2\')\n        x = _stack(x, _blockw, 256, 4, scope=\'conv3\')\n        x = _stack(x, _blockw, 512, 6, scope=\'conv4\')\n        x = _stack(x, _blockw, 1024, 3, scope=\'conv5\')\n        return x\n    return resnet(x, False, stack_fn, is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'stack\')\ndef _stack(x, block_fn, filters, blocks, stride1=2, scope=None):\n    x = block_fn(x, filters, stride=stride1, scope=\'block1\')\n    for i in range(2, blocks+1):\n        x = block_fn(x, filters, conv_shortcut=False, scope=""block%d"" % i)\n    return x\n\n\n@var_scope(\'stack_tfslim\')\ndef _stacks(x, filters, blocks, stride1=2, scope=None):\n    x = _block2s(x, filters, conv_shortcut=True, scope=\'block1\')\n    for i in range(2, blocks):\n        x = _block2s(x, filters, scope=""block%d"" % i)\n    x = _block2s(x, filters, stride=stride1, scope=""block%d"" % blocks)\n    return x\n\n\n@var_scope(\'block1\')\ndef _block1(x, filters, kernel_size=3, stride=1,\n            conv_shortcut=True, scope=None):\n    if conv_shortcut is True:\n        shortcut = conv(x, 4 * filters, 1, stride=stride, scope=\'0\')\n    else:\n        shortcut = x\n    # Most reference implementations (e.g., TF-slim and Torch-ResNets)\n    # apply a stride of 2 on the 3x3 conv kernel like the below `_block2`,\n    # but here the stride 2 on the 1x1 to follow the original Caffe-ResNets.\n    x = conv(x, filters, 1, stride=stride, scope=\'1\')\n    x = relu(x, name=\'1/relu\')\n    x = conv(x, filters, kernel_size, stride=1, padding=\'SAME\', scope=\'2\')\n    x = relu(x, name=\'2/relu\')\n    x = conv(x, 4 * filters, 1, stride=1, scope=\'3\')\n    x = relu(shortcut + x, name=\'out\')\n    return x\n\n\n@var_scope(\'block2\')\ndef _block2(x, filters, kernel_size=3, stride=1,\n            conv_shortcut=True, scope=None):\n    if conv_shortcut is True:\n        shortcut = conv(x, 4 * filters, 1, stride=stride, scope=\'0\')\n    else:\n        shortcut = x\n    x = batch_norm(x)\n    x = relu(x)\n    x = conv(x, filters, 1, stride=1, scope=\'1\')\n    x = relu(x, name=\'1/relu\')\n    x = pad(x, pad_info(kernel_size), name=\'2/pad\')\n    x = conv(x, filters, kernel_size, stride=stride, scope=\'2\')\n    x = relu(x, name=\'2/relu\')\n    x = conv2d(x, 4 * filters, 1, stride=1, scope=\'3/conv\')\n    x = add(shortcut, x, name=\'out\')\n    return x\n\n\n@var_scope(\'block2_tfslim\')\ndef _block2s(x, filters, kernel_size=3, stride=1,\n             conv_shortcut=False, scope=None):\n    preact = batch_norm(x, scope=\'preact\')\n    preact = relu(preact)\n    if conv_shortcut is True:\n        shortcut = conv2d(preact, 4 * filters, 1, stride=stride, scope=\'0\')\n    else:\n        shortcut = max_pool2d(x, 1, stride, scope=\'0\') if stride > 1 else x\n    x = conv(preact, filters, 1, stride=1, biases_initializer=None, scope=\'1\')\n    x = relu(x, name=\'1/relu\')\n    x = pad(x, pad_info(kernel_size), name=\'2/pad\')\n    x = conv(x, filters, kernel_size, stride=stride, biases_initializer=None,\n             scope=\'2\')\n    x = relu(x, name=\'2/relu\')\n    x = conv2d(x, 4 * filters, 1, stride=1, scope=\'3/conv\')\n    x = add(shortcut, x, name=\'out\')\n    return x\n\n\n@var_scope(\'block3\')\ndef _block3(x, filters, kernel_size=3, stride=1, groups=32,\n            conv_shortcut=True, scope=None):\n    if conv_shortcut is True:\n        shortcut = conv(x, (64 // groups) * filters, 1,\n                        stride=stride, scope=\'0\')\n    else:\n        shortcut = x\n    x = conv(x, filters, 1, stride=1, scope=\'1\')\n    x = relu(x, name=\'1/relu\')\n    x = pad(x, pad_info(kernel_size), name=\'2/pad\')\n    x = gconv(x, None, kernel_size, filters // groups,\n              stride=stride, scope=\'2\')\n    x = relu(x, name=\'2/relu\')\n    x = conv(x, (64 // groups) * filters, 1, stride=1, scope=\'3\')\n    x = relu(shortcut + x, name=\'out\')\n    return x\n\n\n@var_scope(\'blockw\')\ndef _blockw(x, filters, kernel_size=3, stride=1,\n            conv_shortcut=True, scope=None):\n    if conv_shortcut is True:\n        shortcut = conv(x, 2 * filters, 1, stride=stride, scope=\'0\')\n    else:\n        shortcut = x\n    x = conv(x, filters, 1, stride=1, scope=\'1\')\n    x = relu(x, name=\'1/relu\')\n    x = pad(x, pad_info(kernel_size), name=\'2/pad\')\n    x = conv(x, filters, kernel_size, stride=stride, scope=\'2\')\n    x = relu(x, name=\'2/relu\')\n    x = conv(x, 2 * filters, 1, stride=1, scope=\'3\')\n    x = relu(shortcut + x, name=\'out\')\n    return x\n\n\n# Simple alias.\nResNet50 = resnet50\nResNet101 = resnet101\nResNet152 = resnet152\nResNet50v2 = resnet50v2\nResNet101v2 = resnet101v2\nResNet152v2 = resnet152v2\nResNet200v2 = resnet200v2\nResNeXt50 = ResNeXt50c32 = resnext50c32\nResNeXt101 = ResNeXt101c32 = resnext101c32\nResNeXt101c64 = resnext101c64\nWideResNet50 = wideresnet50\n'"
tensornets/squeezenets.py,0,"b'""""""Collection of SqueezeNet variants\n\nThe reference paper:\n\n - SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size, arXiv 2016\n - Forrest N. Iandola et al.\n - https://arxiv.org/abs/1602.07360\n\nThe reference implementation:\n\n1. Caffe SqueezeNets\n - https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1\n""""""\nfrom __future__ import absolute_import\n\nimport tensorflow as tf\n\nfrom .layers import conv2d\nfrom .layers import dropout\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import convrelu as conv\n\nfrom .ops import *\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training, \'scope\': \'dropout\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([max_pool2d], {\'scope\': \'pool\'})]\n\n\n@var_scope(\'fire\')\ndef fire(x, squeeze, expand, scope=None):\n    x = conv(x, squeeze, 1, scope=\'squeeze1x1\')\n    x1 = conv(x, expand, 1, scope=\'expand1x1\')\n    x2 = conv(x, expand, 3, scope=\'expand3x3\')\n    x = concat([x1, x2], axis=3, name=\'concat\')\n    return x\n\n\n@var_scope(\'squeezenet\')\n@set_args(__args__)\ndef squeezenet(x, is_training=False, classes=1000,\n               stem=False, scope=None, reuse=None):\n    x = conv(x, 64, 3, stride=2, padding=\'VALID\', scope=\'conv1\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool1\')\n\n    x = fire(x, 16, 64, scope=\'fire2\')\n    x = fire(x, 16, 64, scope=\'fire3\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool3\')\n\n    x = fire(x, 32, 128, scope=\'fire4\')\n    x = fire(x, 32, 128, scope=\'fire5\')\n    x = max_pool2d(x, 3, stride=2, scope=\'pool5\')\n\n    x = fire(x, 48, 192, scope=\'fire6\')\n    x = fire(x, 48, 192, scope=\'fire7\')\n    x = fire(x, 64, 256, scope=\'fire8\')\n    x = fire(x, 64, 256, scope=\'fire9\')\n    if stem: return x\n    x = dropout(x, keep_prob=0.5, scope=\'drop9\')\n\n    x = reduce_mean(x, [1, 2], name=\'pool10\')\n    x = fc(x, classes, scope=\'logits\')  # the original name is `conv10`\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n# Simple alias.\nSqueezeNet = squeezenet\n'"
tensornets/utils.py,21,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport warnings\n\nfrom contextlib import contextmanager\n\nfrom .version_utils import tf_later_than\n\n\ntry:\n    import cv2\nexcept ImportError:\n    cv2 = None\n\n\n__middles__ = \'middles\'\n__outputs__ = \'outputs\'\n\n\nif tf_later_than(\'1.14\'):\n    tf = tf.compat.v1\n\n\nif tf_later_than(\'2\'):\n    from .contrib_framework import arg_scope\n    from .contrib_layers.utils import collect_named_outputs\nelse:\n    from tensorflow.contrib.framework import arg_scope\n    from tensorflow.contrib.layers.python.layers.utils import collect_named_outputs\n\n\nif tf_later_than(\'2.1\'):\n    from tensorflow.python.keras.applications.imagenet_utils \\\n        import decode_predictions\n    from tensorflow.python.keras.utils.data_utils import get_file\nelif tf_later_than(\'1.9\'):\n    from tensorflow.python.keras.applications.imagenet_utils \\\n        import decode_predictions\n    from tensorflow.python.keras.utils import get_file\nelif tf_later_than(\'1.4\'):\n    from tensorflow.python.keras._impl.keras.applications.imagenet_utils \\\n        import decode_predictions\n    from tensorflow.python.keras.utils import get_file\nelse:\n    from tensorflow.contrib.keras.python.keras.applications.imagenet_utils \\\n        import decode_predictions\n    from tensorflow.contrib.keras.python.keras.utils.data_utils \\\n        import get_file\n\n\ndef print_collection(collection, scope):\n    if scope is not None:\n        print(""Scope: %s"" % scope)\n    for x in tf.get_collection(collection, scope=scope + \'/\'):\n        name = x.name\n        if scope is not None:\n            name = name[len(scope)+1:]\n        print(""%s %s"" % (name, x.shape))\n\n\ndef parse_scopes(inputs):\n    if not isinstance(inputs, list):\n        inputs = [inputs]\n    outputs = []\n    for scope_or_tensor in inputs:\n        if isinstance(scope_or_tensor, tf.Tensor):\n            outputs.append(scope_or_tensor.aliases[0])\n        elif isinstance(scope_or_tensor, str):\n            outputs.append(scope_or_tensor)\n        else:\n            outputs.append(None)\n    return outputs\n\n\ndef print_middles(scopes=None):\n    scopes = parse_scopes(scopes)\n    for scope in scopes:\n        print_collection(__middles__, scope)\n\n\ndef print_outputs(scopes=None):\n    scopes = parse_scopes(scopes)\n    for scope in scopes:\n        print_collection(__outputs__, scope)\n\n\ndef print_weights(scopes=None):\n    scopes = parse_scopes(scopes)\n    for scope in scopes:\n        print_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope)\n\n\ndef print_summary(scopes=None):\n    scopes = parse_scopes(scopes)\n    for scope in scopes:\n        if scope is not None:\n            print(""Scope: %s"" % scope)\n        weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                    scope=scope + \'/\')\n        names = [w.name for w in weights]\n        starts = [n.rfind(\'/\') + 1 for n in names]\n        ends = [n.rfind(\':\') for n in names]\n\n        layers = sum([n[s:e] == \'weights\'\n                      for (n, s, e) in zip(names, starts, ends)])\n        parameters = sum([w.shape.num_elements() for w in weights])\n        print(""Total layers: %d"" % layers)\n        print(""Total weights: %d"" % len(weights))\n        print(""Total parameters: {:,}"".format(parameters))\n\n\ndef get_collection(collection_name, scope=None, names=None):\n    scope = parse_scopes(scope)[0]\n    collection = tf.get_collection(collection_name, scope=scope + \'/\')\n    if names is None:\n        return collection\n    else:\n        if not isinstance(names, list):\n            names = [names]\n        _collection = []\n        for x in collection:\n            if any([name in x.name for name in names]):\n                _collection.append(x)\n        return _collection\n\n\ndef get_bottleneck(scope=None):\n    return get_collection(__middles__, scope, names=None)[-1]\n\n\ndef get_middles(scope=None, names=None):\n    return get_collection(__middles__, scope, names)\n\n\ndef get_outputs(scope=None, names=None):\n    return get_collection(__outputs__, scope, names)\n\n\ndef get_weights(scope=None, names=None):\n    return get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope, names)\n\n\ndef load(model, weights_path, sess):\n    if sess is None:\n        sess = tf.get_default_session()\n        assert sess is not None, \'The default session should be given.\'\n\n    values = parse_weights(weights_path)\n    sess.run(pretrained_initializer(model, values))\n\n\ndef save(model, weights_path, sess):\n    if sess is None:\n        sess = tf.get_default_session()\n        assert sess is not None, \'The default session should be given.\'\n\n    weights = get_weights(model)\n    names = [w.name for w in weights]\n    values = sess.run(weights)\n    np.savez(weights_path, names=names, values=values)\n\n\ndef pad_info(s, symmetry=True):\n    pads = [[0, 0], [s // 2, s // 2], [s // 2, s // 2], [0, 0]]\n    if not symmetry:\n        pads[1][0] -= 1\n        pads[2][0] -= 1\n    return pads\n\n\ndef crop_idx(total_size, crop_size, crop_loc, crop_grid):\n    if isinstance(total_size, int):\n        total_size = (total_size, total_size)\n    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    if crop_loc > -1:\n        row_loc = crop_loc // crop_grid[0]\n        col_loc = crop_loc % crop_grid[1]\n        row_start = row_loc * (total_size[0] - crop_size[0]) // 2\n        col_start = col_loc * (total_size[1] - crop_size[1]) // 2\n    else:\n        row_start = np.random.randint(0, total_size[0] - crop_size[0], 1)[0]\n        col_start = np.random.randint(0, total_size[1] - crop_size[1], 1)[0]\n    return row_start, col_start\n\n\ndef crop(img, crop_size, crop_loc=4, crop_grid=(3, 3)):\n    if isinstance(crop_loc, list):\n        imgs = np.zeros((img.shape[0], len(crop_loc), crop_size, crop_size, 3),\n                        np.float32)\n        for (i, loc) in enumerate(crop_loc):\n            r, c = crop_idx(img.shape[1:3], crop_size, loc, crop_grid)\n            imgs[:, i] = img[:, r:r+crop_size, c:c+crop_size, :]\n        return imgs\n    elif crop_loc == np.prod(crop_grid) + 1:\n        imgs = np.zeros((img.shape[0], crop_loc, crop_size, crop_size, 3),\n                        np.float32)\n        r, c = crop_idx(img.shape[1:3], crop_size, 4, crop_grid)\n        imgs[:, 0] = img[:, r:r+crop_size, c:c+crop_size, :]\n        imgs[:, 1] = img[:, 0:crop_size, 0:crop_size, :]\n        imgs[:, 2] = img[:, 0:crop_size, -crop_size:, :]\n        imgs[:, 3] = img[:, -crop_size:, 0:crop_size, :]\n        imgs[:, 4] = img[:, -crop_size:, -crop_size:, :]\n        imgs[:, 5:] = np.flip(imgs[:, :5], axis=3)\n        return imgs\n    else:\n        r, c = crop_idx(img.shape[1:3], crop_size, crop_loc, crop_grid)\n        return img[:, r:r+crop_size, c:c+crop_size, :]\n\n\ndef load_img(paths, grayscale=False, target_size=None, crop_size=None,\n             interp=None):\n    assert cv2 is not None, \'`load_img` requires `cv2`.\'\n    if interp is None:\n        interp = cv2.INTER_CUBIC\n    if not isinstance(paths, list):\n        paths = [paths]\n    if len(paths) > 1 and (target_size is None or\n                           isinstance(target_size, int)):\n        raise ValueError(\'A tuple `target_size` should be provided \'\n                         \'when loading multiple images.\')\n\n    def _load_img(path):\n        img = cv2.imread(path)\n        if target_size:\n            if isinstance(target_size, int):\n                hw_tuple = tuple([x * target_size // min(img.shape[:2])\n                                  for x in img.shape[1::-1]])\n            else:\n                hw_tuple = (target_size[1], target_size[0])\n            if img.shape[1::-1] != hw_tuple:\n                img = cv2.resize(img, hw_tuple, interpolation=interp)\n        img = img[:, :, ::-1]\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, -1)\n        return img\n\n    if len(paths) > 1:\n        imgs = np.zeros((len(paths),) + target_size + (3,), dtype=np.float32)\n        for (i, path) in enumerate(paths):\n            imgs[i] = _load_img(path)\n    else:\n        imgs = np.array([_load_img(paths[0])], dtype=np.float32)\n\n    if crop_size is not None:\n        imgs = crop(imgs, crop_size)\n\n    return imgs\n\n\ndef init(scopes, sess):\n    if sess is None:\n        sess = tf.get_default_session()\n        assert sess is not None, \'The default session should be given.\'\n\n    if not isinstance(scopes, list):\n        scopes = [scopes]\n\n    for scope in scopes:\n        sess.run(tf.variables_initializer(get_weights(scope)))\n\n\ndef var_scope(name):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            stem = kwargs.get(\'stem\', False)\n            scope = kwargs.get(\'scope\', name)\n            reuse = kwargs.get(\'reuse\', None)\n            with tf.variable_scope(scope, reuse=reuse):\n                x = func(*args, **kwargs)\n                if func.__name__ == \'wrapper\':\n                    from .middles import direct as p0\n                    from .preprocess import direct as p1\n                    from .pretrained import direct as p2\n                    _scope = tf.get_variable_scope().name\n                    if tf_later_than(\'1.2\'):\n                        _name = tf.get_default_graph().get_name_scope()\n                    else:\n                        # Note that `get_middles` and `get_outputs`\n                        # may NOT work well for TensorFlow == 1.1.0.\n                        _name = _scope\n                    if tf_later_than(\'2\'):\n                        _input_shape = tuple(args[0].shape[1:3])\n                    else:\n                        _input_shape = tuple([i.value for i in args[0].shape[1:3]])\n                    _outs = get_outputs(_name)\n                    for i in p0(name)[0]:\n                        collect_named_outputs(__middles__, _scope, _outs[i])\n                    if stem:\n                        x.aliases.insert(0, _scope)\n                        x.p = get_middles(_name)[p0(name)[2]]\n                    else:\n                        x.logits = get_outputs(_name)[-2]\n                    setattr(x, \'preprocess\', p1(name, _input_shape))\n                    setattr(x, \'pretrained\', p2(name, x))\n                    setattr(x, \'get_bottleneck\',\n                            lambda: get_bottleneck(_scope))\n                    setattr(x, \'get_middles\', lambda names=None: get_middles(_name, names))\n                    setattr(x, \'get_outputs\', lambda names=None: get_outputs(_name, names))\n                    setattr(x, \'get_weights\', lambda names=None: get_weights(_scope, names))\n                    setattr(x, \'middles\', lambda names=None: get_middles(_name, names))\n                    setattr(x, \'outputs\', lambda names=None: get_outputs(_name, names))\n                    setattr(x, \'weights\', lambda names=None: get_weights(_scope, names))\n                    setattr(x, \'summary\', lambda: print_summary(_scope))\n                    setattr(x, \'print_middles\', lambda: print_middles(_name))\n                    setattr(x, \'print_outputs\', lambda: print_outputs(_name))\n                    setattr(x, \'print_weights\', lambda: print_weights(_scope))\n                    setattr(x, \'print_summary\', lambda: print_summary(_scope))\n                    setattr(x, \'init\', lambda sess=None: init(_scope, sess))\n                    setattr(x, \'load\',\n                            lambda weights_path, sess=None: load(x, weights_path, sess))\n                    setattr(x, \'save\',\n                            lambda weights_path, sess=None: save(x, weights_path, sess))\n                return x\n        return wrapper\n    return decorator\n\n\ndef ops_to_outputs(func):\n    def wrapper(*args, **kwargs):\n        x = func(*args, **kwargs)\n        x = collect_named_outputs(__outputs__, tf.get_variable_scope().name, x)\n        return x\n    return wrapper\n\n\n@contextmanager\ndef arg_scopes(l):\n    for x in l:\n        x.__enter__()\n    yield\n\n\ndef set_args(largs, conv_bias=True, weights_regularizer=None):\n    from .layers import conv2d\n    from .layers import fc\n    from .layers import sconv2d\n\n    def real_set_args(func):\n        def wrapper(*args, **kwargs):\n            is_training = kwargs.get(\'is_training\', False)\n            layers = sum([x for (x, y) in largs(is_training)], [])\n            layers_args = [arg_scope(x, **y) for (x, y) in largs(is_training)]\n            if not conv_bias:\n                layers_args += [arg_scope([conv2d], biases_initializer=None)]\n            if weights_regularizer is not None:\n                layers_args += [arg_scope(\n                    [conv2d, fc, sconv2d],\n                    weights_regularizer=weights_regularizer)]\n            with arg_scope(layers, outputs_collections=__outputs__):\n                with arg_scopes(layers_args):\n                    x = func(*args, **kwargs)\n                    x.model_name = func.__name__\n                    return x\n        return wrapper\n    return real_set_args\n\n\ndef pretrained_initializer(scope, values):\n    weights = get_weights(scope)\n\n    if values is None:\n        return tf.variables_initializer(weights)\n\n    if len(weights) > len(values):  # excluding weights in Optimizer\n        weights = weights[:len(values)]\n\n    if len(weights) != len(values):\n        values = values[:len(weights)]\n        warnings.warn(\'The sizes of symbolic and actual weights do not match. \'\n                      \'Never mind if you are trying to load stem layers only.\')\n\n    if scope.dtype == tf.float16:\n        ops = [weights[0].assign(np.asarray(values[0], dtype=np.float16))]\n        for (w, v) in zip(weights[1:-2], values[1:-2]):\n            w.load(np.asarray(v, dtype=np.float16))\n        if weights[-1].shape != values[-1].shape:\n            ops += [w.initializer for w in weights[-2:]]\n        else:\n            for (w, v) in zip(weights[-2:], values[-2:]):\n                w.load(np.asarray(v, dtype=np.float16))\n        return ops\n\n    ops = [w.assign(v) for (w, v) in zip(weights[:-2], values[:-2])]\n    if weights[-1].shape != values[-1].shape:  # for transfer learning\n        ops += [w.initializer for w in weights[-2:]]\n    else:\n        # The logits layer can be either 1x1 conv or fc. In other words,\n        # the weight shape is (1, 1, features, classes) for the former,\n        # or (features, classes) the latter.\n        if weights[-2].shape != values[-2].shape:\n            values[-2] = values[-2].reshape(weights[-2].shape)\n            warnings.warn(\'The weight has been reshaped because 1x1 conv and \'\n                          \'fc layers are interchangeable for a logits layer. \'\n                          \'But, the conversion may affect the precision.\')\n        ops += [w.assign(v) for (w, v) in zip(weights[-2:], values[-2:])]\n\n    return ops\n\n\ndef parse_weights(weights_path, move_rules=None):\n    data = np.load(weights_path, encoding=\'bytes\', allow_pickle=True)\n\n    if isinstance(data, np.lib.npyio.NpzFile) or isinstance(data, dict):\n        values = data[\'values\']\n\n        if tf_later_than(\'1.4\'):\n            for (i, name) in enumerate(data[\'names\']):\n                if \'/beta\' in str(data[\'names\'][i-1]) and \'/gamma\' in str(name):\n                    values[i], values[i-1] = values[i-1], values[i]\n    else:\n        values = data\n\n    return values\n\n\ndef parse_keras_weights(weights_path, move_rules=None):\n    try:\n        import h5py\n    except ImportError:\n        h5py = None\n    assert h5py is not None, \'`get_values_from_keras_file` requires `h5py`.\'\n\n    values = []\n    with h5py.File(weights_path, mode=\'r\') as f:\n        names = [n.decode(\'utf8\')\n                 for n in f.attrs[\'layer_names\']\n                 if len(f[n.decode(\'utf8\')].attrs[\'weight_names\']) > 0]\n        if move_rules is not None:\n            if isinstance(move_rules, list):\n                for (name, loc) in move_rules:\n                    idx = names.index(name)\n                    names.insert(idx + loc, names.pop(idx))\n            elif move_rules == \'ordered\':\n                bn_names, conv_names, other_names = [], [], []\n                for n in names:\n                    if \'batch\' in n:\n                        bn_names.append(n)\n                    elif \'conv\' in n:\n                        conv_names.append(n)\n                    else:\n                        other_names.append(n)\n                names = []\n                for n in range(1, len(conv_names) + 1):\n                    names.append(""conv2d_%d"" % n)\n                    names.append(""batch_normalization_%d"" % n)\n                names += other_names\n\n        for name in names:\n            g = f[name]\n            w = [n.decode(\'utf8\') for n in g.attrs[\'weight_names\']]\n            v = [np.asarray(g[n]) for n in w]\n            if not tf_later_than(\'1.4\'):\n                if len(v) == 4:\n                    w[0], w[1] = w[1], w[0]\n                    v[0], v[1] = v[1], v[0]\n            values += v\n\n    return values\n\n\ndef parse_torch_weights(weights_path, move_rules=None):\n    try:\n        import torch\n        import torch.nn as nn\n        import torch.nn.functional as F\n    except ImportError:\n        torch = None\n    assert torch is not None, \'`get_values_from_torch_file` requires `torch`.\'\n\n    model = torch.load(weights_path)\n    names = list(model.keys())\n    if move_rules is not None:\n        if isinstance(move_rules, list):\n            for (name, loc) in move_rules:\n                idx = names.index(name)\n                names.insert(idx + loc, names.pop(idx))\n\n    if not tf_later_than(\'1.4\'):\n        for (i, name) in enumerate(names):\n            if \'running_mean\' in str(name):\n                names[i-1], names[i-2] = names[i-2], names[i-1]\n\n    values = []\n    for name in names:\n        val = model[name].numpy()\n        if val.ndim == 4:\n            val = np.transpose(val, [2, 3, 1, 0])\n        if val.ndim == 2:\n            val = np.transpose(val, [1, 0])\n        if val.ndim == 4:\n            groups = val.shape[3] // val.shape[2]\n            if (groups == 32) or (groups == 64):\n                values += np.split(val, groups, axis=3)\n            else:\n                values.append(val)\n        else:\n            values.append(val)\n\n    return values\n\n\ndef remove_head(original_stem, name):\n    _scope = ""%s/stem"" % tf.get_variable_scope().name\n    g = tf.get_default_graph()\n    for x in g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                              scope=_scope + \'/\')[::-1]:\n        if name in x.name:\n            break\n        g.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES).pop()\n\n    for x in g.get_collection(__outputs__, scope=_scope + \'/\')[::-1]:\n        if name in x.name:\n            break\n        g.get_collection_ref(__outputs__).pop()\n    x.model_name = original_stem.model_name\n    return x\n\n\ndef remove_utils(module_name, exceptions):\n    import sys\n    from . import utils\n    module = sys.modules[module_name]\n    for util in dir(utils):\n        if not ((util.startswith(\'_\')) or (util in exceptions)):\n            try:\n                delattr(module, util)\n            except:\n                None\n\n\ndef remove_commons(module_name, exceptions=[]):\n    import sys\n    _commons = [\n        \'absolute_import\',\n        \'division\'\n        \'print_function\',\n        \'remove_commons\',\n    ]\n    module = sys.modules[module_name]\n    for _common in _commons:\n        if _common not in exceptions:\n            try:\n                delattr(module, _common)\n            except:\n                None\n\n\nremove_commons(__name__, [\'remove_commons\'])\n'"
tensornets/version_utils.py,2,b'import tensorflow as tf\n\nfrom distutils.version import LooseVersion\n\n\ndef tf_later_than(v):\n    return LooseVersion(tf.__version__) > LooseVersion(v)\n\n\ndef tf_equal_to(v):\n   return tf.__version__ == v\n'
tensornets/vggs.py,0,"b'""""""Collection of VGG variants\n\nThe reference paper:\n\n - Very Deep Convolutional Networks for Large-Scale Image Recognition, ICLR 2015\n - Karen Simonyan, Andrew Zisserman\n - https://arxiv.org/abs/1409.1556\n\nThe reference implementation:\n\n1. Keras\n - https://github.com/keras-team/keras/blob/master/keras/applications/vgg{16,19}.py\n2. Caffe VGG\n - http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import conv2d\nfrom .layers import dropout\nfrom .layers import flatten\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import convrelu as conv\n\nfrom .ops import *\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training}),\n            ([flatten], {\'scope\': \'flatten\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([max_pool2d], {\'scope\': \'pool\'})]\n\n\n@var_scope(\'stack\')\ndef _stack(x, filters, blocks, scope=None):\n    for i in range(1, blocks+1):\n        x = conv(x, filters, 3, scope=str(i))\n    x = max_pool2d(x, 2, stride=2)\n    return x\n\n\ndef vgg(x, blocks, is_training, classes, stem, scope=None, reuse=None):\n    x = _stack(x, 64, blocks[0], scope=\'conv1\')\n    x = _stack(x, 128, blocks[1], scope=\'conv2\')\n    x = _stack(x, 256, blocks[2], scope=\'conv3\')\n    x = _stack(x, 512, blocks[3], scope=\'conv4\')\n    x = _stack(x, 512, blocks[4], scope=\'conv5\')\n    if stem: return x\n    x = flatten(x)\n    x = fc(x, 4096, scope=\'fc6\')\n    x = relu(x, name=\'relu6\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop6\')\n    x = fc(x, 4096, scope=\'fc7\')\n    x = relu(x, name=\'relu7\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop7\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n@var_scope(\'vgg16\')\n@set_args(__args__)\ndef vgg16(x, is_training=False, classes=1000,\n          stem=False, scope=None, reuse=None):\n    return vgg(x, [2, 2, 3, 3, 3], is_training, classes, stem, scope, reuse)\n\n\n@var_scope(\'vgg19\')\n@set_args(__args__)\ndef vgg19(x, is_training=False, classes=1000,\n          stem=False, scope=None, reuse=None):\n    return vgg(x, [2, 2, 4, 4, 4], is_training, classes, stem, scope, reuse)\n\n\n# Simple alias.\nVGG16 = vgg16\nVGG19 = vgg19\n'"
tensornets/wavenets.py,2,"b'""""""Collection of WaveNet variants\n\nThe reference paper:\n\n - WaveNet: A Generative Model for Raw Audio, arXiv 2016\n - Aaron van den Oord et al.\n - https://arxiv.org/abs/1609.03499\n\nThe reference implementations:\n\n1. (initially and mainly) @ibab\'s repository\n - https://github.com/ibab/tensorflow-wavenet/blob/master/wavenet/model.py\n2. (to improve readability) @basveeling\'s repository\n - https://github.com/basveeling/wavenet/blob/master/wavenet.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import conv1d\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([conv1d], {\'padding\': \'VALID\', \'activation_fn\': None})]\n\n\n@var_scope(\'block\')\ndef block(x, filters, skipfilters, dilation, scope=None):\n    x = tf.pad(x, [[0, 0], [dilation, 0], [0, 0]])\n    f = conv1d(x, filters, 2, rate=dilation, scope=\'filter\')\n    g = conv1d(x, filters, 2, rate=dilation, scope=\'gate\')\n    o = tanh(f, name=\'filter/tanh\') * sigmoid(g, name=\'gate/sigmoid\')\n    d = conv1d(o, filters, 1, scope=\'dense\')\n    s = conv1d(o, skipfilters, 1, scope=\'skip\')\n    return x[:, dilation:] + d, s\n\n\n@var_scope(\'wavenet\')\n@set_args(__args__)\ndef wavenet(x, filters=32, skipfilters=512,\n            quantization=256, blocks=10, repeats=5,\n            is_training=False, scope=None, reuse=None):\n    x = one_hot(x, quantization, name=\'one_hot\')\n    x = tf.pad(x, [[0, 0], [1, 0], [0, 0]])\n    x = conv1d(x, filters, 2, biases_initializer=None, scope=\'embedding\')\n\n    skips = []\n    for i in range(blocks * repeats):\n        x, s = block(x, filters, skipfilters, 2 ** (i % blocks), scope=str(i))\n        skips.append(s)\n\n    x = relu(sum(skips), name=\'skips\')\n    x = conv1d(x, skipfilters, 1, scope=\'fc\')\n    x = relu(x, name=\'fc/relu\')\n    x = conv1d(x, quantization, 1, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n# Simple alias.\nWaveNet = wavenet\n'"
tensornets/zf.py,0,"b'""""""ZF net embedded in Faster RCNN\n\nThe reference paper:\n\n - Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, NIPS 2015\n - Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\n - https://arxiv.org/abs/1506.01497\n\nThe reference implementation:\n\n1. Caffe and Python utils\n - https://github.com/rbgirshick/py-faster-rcnn\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom .layers import conv2d\nfrom .layers import fc\nfrom .layers import max_pool2d\nfrom .layers import convrelu as conv\n\nfrom .ops import *\nfrom .utils import pad_info\nfrom .utils import set_args\nfrom .utils import var_scope\n\n\ndef __args__(is_training):\n    return [([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'scope\': \'conv\'}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([max_pool2d], {\'scope\': \'pool\'})]\n\n\n@var_scope(\'zf\')\n@set_args(__args__)\ndef zf(x, is_training=False, classes=1000, stem=False, scope=None, reuse=None):\n    x = pad(x, pad_info(7), name=\'pad1\')\n    x = conv(x, 96, 7, stride=2, padding=\'VALID\', scope=\'conv1\')\n    x = srn(x, depth_radius=3, alpha=0.00005, beta=0.75, name=\'srn1\')\n    x = pad(x, pad_info(3, symmetry=False), name=\'pad2\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    x = pad(x, pad_info(5), name=\'pad3\')\n    x = conv(x, 256, 5, stride=2, padding=\'VALID\', scope=\'conv2\')\n    x = srn(x, depth_radius=3, alpha=0.00005, beta=0.75, name=\'srn2\')\n    x = pad(x, pad_info(3, symmetry=False), name=\'pad4\')\n    x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool2\')\n\n    x = conv(x, 384, 3, scope=\'conv3\')\n    x = conv(x, 384, 3, scope=\'conv4\')\n    x = conv(x, 256, 3, scope=\'conv5\')\n    if stem: return x\n\n    x = reduce_mean(x, [1, 2], name=\'avgpool\')\n    x = fc(x, classes, scope=\'logits\')\n    x = softmax(x, name=\'probs\')\n    return x\n\n\n# Simple alias.\nZF = zf\n'"
tests/all_imagenet_models.py,4,"b'import numpy as np\nimport tensorflow as tf\nimport tensornets as nets\n\nfrom tensornets.datasets import imagenet\ndata_dir = \'/home/taehoonlee/Data/imagenet/inputs\'\n\n\ndef imagenet_load(data_dir, resize_wh, crop_wh, crops):\n    return imagenet.load(\n        data_dir, \'val\', batch_size=10 if crops == 10 else 100,\n        resize_wh=resize_wh,\n        crop_locs=10 if crops == 10 else 4,\n        crop_wh=crop_wh)\n\n\ndef test(models_list, crops=1, verbose=False):\n    batches1 = imagenet_load(data_dir, 256, 224, crops)\n    batches2 = imagenet_load(data_dir, 341, 299, crops)\n    batches3 = imagenet_load(data_dir, 378, 331, crops)\n    inputs, models, shapes, params = [], [], [], []\n    labels, preds_list = [], []\n    if verbose:\n        print("""")\n\n    with tf.Graph().as_default():\n        for (_net, _shape, _gpu) in models_list:\n            with tf.device(""gpu:%d"" % _gpu):\n                _input = tf.placeholder(tf.float32, [None] + list(_shape))\n                _model = _net(_input, is_training=False)\n                _weights = _model.get_weights()\n                inputs.append(_input)\n                models.append(_model)\n                shapes.append(_shape)\n                params.append(sum([w.shape.num_elements() for w in _weights]))\n\n        with tf.Session() as sess:\n            nets.pretrained(models)\n            while True:\n                try:\n                    batch1, label1 = batches1.next()\n                    batch2, label2 = batches2.next()\n                    batch3, label3 = batches3.next()\n                except:\n                    break\n                feed_dict = dict((i, m.preprocess(batch1 if s[0] == 224 else\n                                                  batch2 if s[0] == 299 else\n                                                  batch3))\n                                 for (i, m, s) in zip(inputs, models, shapes))\n                preds = sess.run(models, feed_dict)\n                if crops > 1:\n                    preds = [np.mean(pred.reshape(-1, crops, 1000), axis=1)\n                             for pred in preds]\n                labels.append(label1)\n                preds_list.append(preds)\n                if verbose:\n                    print(\'.\'),\n        labels = np.concatenate(labels)\n\n    if verbose:\n        print("""")\n\n    def err(x):\n        return 100 * (1 - sum(x) / float(len(x)))\n\n    print(""Crops: %d"" % crops)\n    print(""Samples: %d"" % len(labels))\n    print(""|                  | Top-1 | Top-5 | Top-1  | Top-5  | Size  |"")\n    print(""|------------------|-------|-------|--------|--------|-------|"")\n\n    for i in range(len(models)):\n        preds = np.concatenate([np.argsort(pred[i], axis=1)[:, -5:]\n                                for pred in preds_list], axis=0)\n        actuals = labels[:preds.shape[0]]\n        top1 = (actuals == preds[:, -1])\n        top5 = [1 if actual in pred else 0\n                for (actual, pred) in zip(actuals, preds)]\n        print(""| %16s | %5d | %5d | %2.3f | %2.3f | %.1fM |"" %\n              (models[i].aliases[0][:16],\n               sum(top1), sum(top5),\n               err(top1), err(top5),\n               params[i] / 10e5))\n\n\ntest([(nets.ResNet50, (224, 224, 3), 0),\n      (nets.ResNet101, (224, 224, 3), 0),\n      (nets.ResNet152, (224, 224, 3), 0),\n      (nets.ResNeXt50, (224, 224, 3), 0),\n      (nets.ResNeXt101, (224, 224, 3), 1),\n      (nets.ResNeXt101c64, (224, 224, 3), 1),\n      (nets.WideResNet50, (224, 224, 3), 1)])\n\ntest([(nets.ResNet50v2, (299, 299, 3), 0),\n      (nets.ResNet101v2, (299, 299, 3), 1),\n      (nets.ResNet152v2, (299, 299, 3), 1),\n      (nets.ResNet200v2, (224, 224, 3), 0)])\n\ntest([(nets.Inception1, (224, 224, 3), 0),\n      (nets.Inception2, (224, 224, 3), 1),\n      (nets.Inception3, (299, 299, 3), 0),\n      (nets.Inception4, (299, 299, 3), 0),\n      (nets.InceptionResNet2, (299, 299, 3), 1)])\n\ntest([(nets.NASNetAlarge, (331, 331, 3), 0)])\n\ntest([(nets.NASNetAmobile, (224, 224, 3), 0),\n      (nets.VGG16, (224, 224, 3), 0),\n      (nets.VGG19, (224, 224, 3), 1),\n      (nets.SqueezeNet, (224, 224, 3), 1)])\n\ntest([(nets.DenseNet121, (224, 224, 3), 0),\n      (nets.DenseNet169, (224, 224, 3), 0),\n      (nets.DenseNet201, (224, 224, 3), 1),\n      (nets.MobileNet25, (224, 224, 3), 0),\n      (nets.MobileNet50, (224, 224, 3), 1),\n      (nets.MobileNet75, (224, 224, 3), 1),\n      (nets.MobileNet100, (224, 224, 3), 0)])\n\n\ntest([(nets.ResNet50, (224, 224, 3), 0),\n      (nets.ResNet101, (224, 224, 3), 0),\n      (nets.ResNet152, (224, 224, 3), 0),\n      (nets.ResNeXt50, (224, 224, 3), 0),\n      (nets.ResNeXt101, (224, 224, 3), 1),\n      (nets.ResNeXt101c64, (224, 224, 3), 1),\n      (nets.WideResNet50, (224, 224, 3), 1)], 10)\n\ntest([(nets.ResNet50v2, (299, 299, 3), 0),\n      (nets.ResNet101v2, (299, 299, 3), 1),\n      (nets.ResNet152v2, (299, 299, 3), 1),\n      (nets.ResNet200v2, (224, 224, 3), 0)], 10)\n\ntest([(nets.Inception1, (224, 224, 3), 0),\n      (nets.Inception2, (224, 224, 3), 1),\n      (nets.Inception3, (299, 299, 3), 0),\n      (nets.Inception4, (299, 299, 3), 0),\n      (nets.InceptionResNet2, (299, 299, 3), 1)], 10)\n\ntest([(nets.NASNetAlarge, (331, 331, 3), 0)], 10)\n\ntest([(nets.NASNetAmobile, (224, 224, 3), 0),\n      (nets.VGG16, (224, 224, 3), 0),\n      (nets.VGG19, (224, 224, 3), 1),\n      (nets.SqueezeNet, (224, 224, 3), 1)], 10)\n\ntest([(nets.DenseNet121, (224, 224, 3), 0),\n      (nets.DenseNet169, (224, 224, 3), 0),\n      (nets.DenseNet201, (224, 224, 3), 1),\n      (nets.MobileNet25, (224, 224, 3), 0),\n      (nets.MobileNet50, (224, 224, 3), 1),\n      (nets.MobileNet75, (224, 224, 3), 1),\n      (nets.MobileNet100, (224, 224, 3), 0)], 10)\n'"
tests/basics_test.py,28,"b""from __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\nimport tensornets as nets\nimport os\nimport pytest\nimport random\n\nfrom tensornets.middles import direct as middle_names\n\nfrom distutils.version import LooseVersion\n\n\npytestmark = pytest.mark.skipif(\n    os.environ.get('CORE_CHANGED', 'True') == 'False',\n    reason='Runs only when the relevant files have been modified.')\n\n\nif LooseVersion(tf.__version__) > LooseVersion('1.14'):\n    tf = tf.compat.v1\n\n\n@pytest.mark.parametrize('net,shape,weights,outputs,middles', [\n    random.choice([\n        (nets.ResNet50, (224, 224, 3), 320, 161, 16),\n        (nets.ResNet101, (224, 224, 3), 626, 314, 33),\n        (nets.ResNet152, (224, 224, 3), 932, 467, 50),\n    ]),\n    random.choice([\n        (nets.ResNet50v2, (299, 299, 3), 272, 192, 16),\n        (nets.ResNet101v2, (299, 299, 3), 544, 379, 33),\n        (nets.ResNet152v2, (299, 299, 3), 816, 566, 50),\n    ]),\n    (nets.ResNet200v2, (224, 224, 3), 1224, 745, 66),\n    random.choice([\n        (nets.ResNeXt50, (224, 224, 3), 267, 193, 16),\n        (nets.ResNeXt101, (224, 224, 3), 522, 380, 33),\n        # (nets.ResNeXt101c64, (224, 224, 3), 522, 380, 33),  # too heavy on Travis\n    ]),\n    (nets.WideResNet50, (224, 224, 3), 267, 177, 16),\n    (nets.Inception1, (224, 224, 3), 116, 143, 11),\n    (nets.Inception2, (224, 224, 3), 277, 231, 10),\n    (nets.Inception3, (299, 299, 3), 378, 313, 11),\n    (nets.Inception4, (299, 299, 3), 598, 494, 17),\n    (nets.InceptionResNet2, (299, 299, 3), 898, 744, 43),\n    pytest.param(\n        nets.NASNetAlarge, (331, 331, 3), 1558, 1029, 20,\n        marks=pytest.mark.xfail(\n            LooseVersion(tf.__version__) < LooseVersion('1.3.0'),\n            reason='NASNetAlarge requires TensorFlow >= 1.3.0')),\n    pytest.param(\n        nets.NASNetAmobile, (224, 224, 3), 1138, 759, 14,\n        marks=pytest.mark.xfail(\n            LooseVersion(tf.__version__) < LooseVersion('1.3.0'),\n            reason='NASNetAmobile requires TensorFlow >= 1.3.0')),\n    pytest.param(\n        nets.PNASNetlarge, (331, 331, 3), 1179, 752, 12,\n        marks=pytest.mark.xfail(\n            LooseVersion(tf.__version__) < LooseVersion('1.3.0'),\n            reason='PNASNetlarge requires TensorFlow >= 1.3.0')),\n    pytest.param(\n        *random.choice([\n            (nets.VGG16, (224, 224, 3), 32, 40, 9),\n            (nets.VGG19, (224, 224, 3), 38, 46, 12),\n        ]),\n        marks=pytest.mark.skipif(\n            LooseVersion(tf.__version__) == LooseVersion('1.2.0'),\n            reason='Deployments of VGGs on local are OK. But there is '\n                   'something wrong in those tests on Travis with TF 1.2.0.')),\n    random.choice([\n        (nets.DenseNet121, (224, 224, 3), 606, 429, 61),\n        (nets.DenseNet169, (224, 224, 3), 846, 597, 85),\n        (nets.DenseNet201, (224, 224, 3), 1006, 709, 101),\n    ]),\n    random.choice([\n        (nets.MobileNet25, (224, 224, 3), 137, 85, 11),\n        (nets.MobileNet50, (224, 224, 3), 137, 85, 11),\n        (nets.MobileNet75, (224, 224, 3), 137, 85, 11),\n        (nets.MobileNet100, (224, 224, 3), 137, 85, 11),\n    ]),\n    random.choice([\n        (nets.MobileNet35v2, (224, 224, 3), 262, 152, 62),\n        (nets.MobileNet50v2, (224, 224, 3), 262, 152, 62),\n        (nets.MobileNet75v2, (224, 224, 3), 262, 152, 62),\n        (nets.MobileNet100v2, (224, 224, 3), 262, 152, 62),\n        (nets.MobileNet130v2, (224, 224, 3), 262, 152, 62),\n        (nets.MobileNet140v2, (224, 224, 3), 262, 152, 62),\n    ]),\n    random.choice([\n        (nets.MobileNet75v3, (224, 224, 3), 266, 187, 19),\n        (nets.MobileNet75v3small, (224, 224, 3), 210, 157, 15),\n        (nets.MobileNet100v3, (224, 224, 3), 266, 187, 19),\n        (nets.MobileNet100v3small, (224, 224, 3), 210, 157, 15),\n        (nets.MobileNet100v3largemini, (224, 224, 3), 234, 139, 19),\n        (nets.MobileNet100v3smallmini, (224, 224, 3), 174, 103, 15),\n    ]),\n    random.choice([\n        (nets.EfficientNetB0, (224, 224, 3), 311, 217, 25),\n        (nets.EfficientNetB1, (240, 240, 3), 439, 312, 39),\n        (nets.EfficientNetB2, (260, 260, 3), 439, 312, 39),\n        (nets.EfficientNetB3, (300, 300, 3), 496, 354, 45),\n        # (nets.EfficientNetB4, (380, 380, 3), 610, 438, 57),  # too heavy on Travis\n        # (nets.EfficientNetB5, (456, 456, 3), 738, 533, 71),  # too heavy on Travis\n        # (nets.EfficientNetB6, (528, 528, 3), 852, 617, 83),  # too heavy on Travis\n        # (nets.EfficientNetB7, (600, 600, 3), 1037, 754, 103),  # too heavy on Travis\n    ]),\n    (nets.SqueezeNet, (224, 224, 3), 52, 65, 10),\n], ids=[\n    'ResNet',\n    'ResNetv2',\n    'ResNet200v2',\n    'ResNeXt',\n    'WideResNet50',\n    'Inception1',\n    'Inception2',\n    'Inception3',\n    'Inception4',\n    'InceptionResNet2',\n    'NASNetAlarge',\n    'NASNetAmobile',\n    'PNASNetlarge',\n    'VGG',\n    'DenseNet',\n    'MobileNet',\n    'MobileNetv2',\n    'MobileNetv3',\n    'EfficientNet',\n    'SqueezeNet',\n])\ndef test_classification_basics(net, shape, weights, outputs, middles):\n    with tf.Graph().as_default():\n        inputs = tf.placeholder(tf.float32, [None] + list(shape))\n        model = net(inputs, is_training=False)\n        assert isinstance(model, tf.Tensor)\n\n        x = np.random.random((1,) + shape).astype(np.float32) * 255\n\n        with tf.Session() as sess:\n            model.init()\n            y = model.eval({inputs: model.preprocess(x)})\n\n        assert y.shape == (1, 1000)\n\n        # Check whether the tensor names match the desired ones\n        assert 'probs' in model.name  # for `model`\n        assert 'logits' in model.logits.name  # for `model.logits`\n        model_name = model.aliases[0]\n        for (a, b) in zip(model.get_middles(), middle_names(model_name)[1]):\n            assert a.name.endswith(b)  # for `model.get_middles()`\n\n        # Disable the following tests for TF==1.1.0\n        if LooseVersion(tf.__version__) == LooseVersion('1.1.0'):\n            return\n\n        # Check whether the desired list is returned\n        assert len(model.get_weights()) == weights\n        assert len(model.get_outputs()) == outputs\n        assert len(model.get_middles()) == middles\n\n    # Clear GraphDef to avoid `GraphDef cannot be larger than 2GB`\n    with tf.Graph().as_default():\n        inputs = tf.placeholder(tf.float32, [None] + list(shape))\n\n        # Check whether the desired list is returned under scope functions\n        with tf.name_scope('a'):\n            with tf.variable_scope('b'):\n                with tf.name_scope('c'):\n                    model = net(inputs, is_training=False)\n                    assert len(model.get_weights()) == weights\n                    assert len(model.get_outputs()) == outputs\n                    assert len(model.get_middles()) == middles\n\n        with tf.variable_scope('d'):\n            with tf.name_scope('e'):\n                with tf.variable_scope('f'):\n                    model = net(inputs, is_training=False)\n                    assert len(model.get_weights()) == weights\n                    assert len(model.get_outputs()) == outputs\n                    assert len(model.get_middles()) == middles\n\n\n@pytest.mark.parametrize('net,shape,stem', [\n    (nets.YOLOv2, (416, 416, 3), nets.Darknet19),\n    (nets.TinyYOLOv2, (416, 416, 3), nets.TinyDarknet19),\n], ids=[\n    'YOLOv2',\n    'TinyYOLOv2',\n])\ndef test_detection_basics(net, shape, stem):\n    # TODO: Once the roi-pooling dependency is removed,\n    # FasterRCNN-related tests should be added.\n    with tf.Graph().as_default():\n        inputs = tf.placeholder(tf.float32, [None] + list(shape))\n        model = net(inputs, stem, is_training=False)\n        assert isinstance(model, tf.Tensor)\n\n        x = np.random.random((1, 733, 490, 3)).astype(np.float32) * 255\n\n        with tf.Session() as sess:\n            model.init()\n            y = model.eval({inputs: model.preprocess(x)})\n\n        # TODO: Once the get_boxes's are translated from cython,\n        # get_boxes tests should be enabled.\n        # boxes = model.get_boxes(y, x.shape[1:3])\n\n        # assert len(boxes) == 20\n\n\n@pytest.mark.parametrize('net,shape', [\n    (nets.MobileNet25, (224, 224, 3)),\n    (nets.SqueezeNet, (224, 224, 3)),\n], ids=[\n    'MobileNet',\n    'SqueezeNet',\n])\ndef test_load_save(net, shape):\n    with tf.Graph().as_default():\n        inputs = tf.placeholder(tf.float32, [None] + list(shape))\n        model = net(inputs, is_training=False)\n\n        # usages with the default session\n\n        with tf.Session() as sess:\n            model.init()\n            model.save('test.npz')\n            values0 = sess.run(model.weights())\n\n            sess.run(model.pretrained())\n            values1 = sess.run(model.weights())\n\n            for (v0, v1) in zip(values0, values1):\n                assert not np.allclose(v0, v1)\n\n        with tf.Session() as sess:\n            model.load('test.npz')\n            values2 = sess.run(model.weights())\n\n            for (v0, v2) in zip(values0, values2):\n                assert np.allclose(v0, v2)\n\n        # usages without the default session\n\n        sess = tf.Session()\n\n        model.init(sess)\n        model.save('test2.npz', sess)\n        values0 = sess.run(model.weights())\n\n        sess.run(model.pretrained())\n        values1 = sess.run(model.weights())\n\n        for (v0, v1) in zip(values0, values1):\n            assert not np.allclose(v0, v1)\n\n        model.load('test2.npz', sess)\n        values2 = sess.run(model.weights())\n\n        for (v0, v2) in zip(values0, values2):\n            assert np.allclose(v0, v2)\n\n        with pytest.raises(AssertionError):\n            model.init()\n\n        with pytest.raises(AssertionError):\n            model.save('test2.npz')\n\n        with pytest.raises(AssertionError):\n            model.load('test2.npz')\n\n        sess.close()\n\n        os.remove('test.npz')\n        os.remove('test2.npz')\n"""
tests/utils_test.py,0,"b""from __future__ import absolute_import\n\nimport numpy as np\nimport tensornets as nets\nfrom tensornets.utils import load_img\nimport os\nimport pytest\n\n\npytestmark = pytest.mark.skipif(\n    os.environ.get('CORE_CHANGED', 'True') == 'False',\n    reason='Runs only when the relevant files have been modified.')\n\n\ndef test_load_img():\n    x = load_img('cat.png')\n    assert x.shape == (1, 733, 490, 3)\n\n    x = load_img(['cat.png', 'cat.png'], target_size=(100, 200))\n    assert x.shape == (2, 100, 200, 3)\n\n    x = load_img(['cat.png'] * 3, target_size=(100, 200), crop_size=50)\n    assert x.shape == (3, 50, 50, 3)\n\n    with pytest.raises(ValueError):\n        x = load_img(['cat.png', 'cat.png'])\n\n    with pytest.raises(ValueError):\n        x = load_img(['cat.png'] * 3, target_size=100)\n"""
translations/mobilenetv3_tfslim.py,8,"b'""""""Weight translation of MobileNetv3 variants\n(tested with tensornets: 0.4.3 and tensorflow: 1.15.0)\n\nThe codes are executable on the path ""research/slim/""\nin the ""tensorflow/models"" repository.\n\nFor the 0.75 variants, the following modifications are necessary.\n\nIn the line 116 of ""research/slim/nets/mobilenet/mobilenet.py"",\n def op(opfunc, multiplier_func=depth_multiplier, **params):\n   multiplier = params.pop(\'multiplier_transform\', multiplier_func)\n-  return _Op(opfunc, params=params, multiplier_func=multiplier)\n+  if params.get(\'normalizer_fn\', True) is not None:\n+    return _Op(opfunc, params=params, multiplier_func=multiplier)\n+  else:\n+    return _Op(opfunc, params=params, multiplier_func=lambda x, y: x)\n""""""\nimport numpy as np\nimport tensorflow as tf\nimport tensornets as nets\n\nfrom datasets import imagenet\nfrom nets.mobilenet import mobilenet_v3\n\nmodels_list = [\n    (nets.MobileNet75v3large, (224, 224, 3), \'mobilenet_75_v3_large\',\n     mobilenet_v3.large, 0.75, \'v3-large_224_0.75_float/ema/model-220000\'),\n    (nets.MobileNet75v3small, (224, 224, 3), \'mobilenet_75_v3_small\',\n     mobilenet_v3.small, 0.75, \'v3-small_224_0.75_float/ema/model-497500\'),\n    (nets.MobileNet100v3large, (224, 224, 3), \'mobilenet_100_v3_large\',\n     mobilenet_v3.large, 1.0, \'v3-large_224_1.0_float/ema/model-540000\'),\n    (nets.MobileNet100v3small, (224, 224, 3), \'mobilenet_100_v3_small\',\n     mobilenet_v3.small, 1.0, \'v3-small_224_1.0_float/ema/model-388500\'),\n    (nets.MobileNet100v3largemini, (224, 224, 3),\n     \'mobilenet_100_v3_large_mini\',\n     mobilenet_v3.large_minimalistic, 1.0,\n     \'v3-large-minimalistic_224_1.0_float/ema/model-342500\'),\n    (nets.MobileNet100v3smallmini, (224, 224, 3),\n     \'mobilenet_100_v3_small_mini\',\n     mobilenet_v3.small_minimalistic, 1.0,\n     \'v3-small-minimalistic_224_1.0_float/ema/model-498000\'),\n]\n\n\nfor (net, shape, model_name, net_slim, alpha, checkpoint) in models_list:\n\n    with tf.Graph().as_default():\n\n        inputs = tf.compat.v1.placeholder(tf.float32, [None] + list(shape))\n\n        with tf.contrib.slim.arg_scope(mobilenet_v3.training_scope(is_training=False)):\n            logits, endpoints = net_slim(inputs, depth_multiplier=alpha)\n\n        saver = tf.compat.v1.train.Saver()\n\n        weights_tfslim = tf.compat.v1.get_collection(\n            tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n\n        model = net(inputs, scope=\'a\')\n\n        img = nets.utils.load_img(\'/home/taehoonlee/tensornets/cat.png\',\n                                  target_size=int(shape[0] * 8 / 7),\n                                  crop_size=shape[0])\n\n        with tf.compat.v1.Session() as sess:\n\n            # Retrieve values\n            sess.run(tf.compat.v1.global_variables_initializer())\n            saver.restore(sess, checkpoint)\n            names = [w.name[2:] for w in model.weights()]\n            values = sess.run(weights_tfslim)\n\n            # Trim the background class (1001 -> 1000)\n            for i in range(-2, 0):\n                values[i] = np.delete(np.squeeze(values[i]), 0, axis=-1)\n\n            # Save the values as the TensorNets format\n            np.savez(model_name, names=names, values=values)\n\n            # Load and set the values\n            weights = model.weights()\n            values = nets.utils.parse_weights(model_name + \'.npz\')\n            sess.run([w.assign(v) for (w, v) in zip(weights, values)])\n\n            # Run equivalence tests\n            preds, preds_tfslim = sess.run([model, endpoints[\'Predictions\']],\n                                           {inputs: model.preprocess(img)})\n            preds_tfslim = preds_tfslim[:, 1:]\n            np.testing.assert_allclose(preds, preds_tfslim, atol=2e-4)\n            print(model_name, \'ok\')\n'"
translations/tfslim.py,7,"b'import numpy as np\nimport tensorflow as tf\nimport tensornets as nets\nimport tensorflow_hub as hub\n\nfrom tensornets.utils import tf_later_than\n\nif tf_later_than(\'1.14\'):\n    tf = tf.compat.v1\n\nmodels_list = [\n    (nets.Inception2, (224, 224, 3), \'inception_v2\'),\n    (nets.Inception3, (299, 299, 3), \'inception_v3\'),\n    (nets.MobileNet35v2, (224, 224, 3), \'mobilenet_v2_035_224\'),\n    (nets.MobileNet50v2, (224, 224, 3), \'mobilenet_v2_050_224\'),\n    (nets.MobileNet75v2, (224, 224, 3), \'mobilenet_v2_075_224\'),\n    (nets.MobileNet100v2, (224, 224, 3), \'mobilenet_v2_100_224\'),\n    (nets.MobileNet130v2, (224, 224, 3), \'mobilenet_v2_130_224\'),\n    (nets.MobileNet140v2, (224, 224, 3), \'mobilenet_v2_140_224\'),\n    (nets.PNASNetlarge, (331, 331, 3), \'pnasnet_large\'),\n    (nets.EfficientNetB0, (224, 224, 3), \'efficientnet/b0\'),\n    (nets.EfficientNetB1, (240, 240, 3), \'efficientnet/b1\'),\n    (nets.EfficientNetB2, (260, 260, 3), \'efficientnet/b2\'),\n    (nets.EfficientNetB3, (300, 300, 3), \'efficientnet/b3\'),\n    (nets.EfficientNetB4, (380, 380, 3), \'efficientnet/b4\'),\n    (nets.EfficientNetB5, (456, 456, 3), \'efficientnet/b5\'),\n    (nets.EfficientNetB6, (528, 528, 3), \'efficientnet/b6\'),\n    (nets.EfficientNetB7, (600, 600, 3), \'efficientnet/b7\'),\n]\n\nurl = \'https://tfhub.dev/google\'\n\n\nfor (net, shape, model_name) in models_list:\n\n    with tf.Graph().as_default():\n\n        inputs = tf.placeholder(tf.float32, [None] + list(shape))\n        model = net(inputs, scope=\'a\')\n\n        if model_name[:12] == \'efficientnet\':\n            tfhub = hub.Module(""%s/%s/classification/1"" % (url, model_name))\n        else:\n            tfhub = hub.Module(""%s/imagenet/%s/classification/1"" % (url, model_name))\n        features = tfhub(inputs, signature=""image_classification"",\n                         as_dict=True)\n        model_tfhub = tf.nn.softmax(features[\'default\'])\n\n        img = nets.utils.load_img(\'cat.png\',\n                                  target_size=int(shape[0] * 8 / 7),\n                                  crop_size=shape[0])\n\n        with tf.Session() as sess:\n\n            # Retrieve values\n            sess.run(tf.global_variables_initializer())\n            weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                        scope=\'module\')\n            values = sess.run(weights)\n\n            # Trim the background class (1001 -> 1000)\n            if not model_name[:12] == \'efficientnet\':\n                for i in range(-2, 0):\n                    values[i] = np.delete(np.squeeze(values[i]), 0, axis=-1)\n\n            names = [w.name[2:] for w in model.get_weights()]\n            if not nets.utils.tf_later_than(\'1.4.0\'):\n                # Adjust the order of the values to cover TF < 1.4.0\n                for i in range(len(names) - 1):\n                    if \'gamma:0\' in names[i] and \'beta:0\' in names[i + 1]:\n                        names[i], names[i + 1] = names[i + 1], names[i]\n                        values[i], values[i + 1] = values[i + 1], values[i]\n\n            # Save the values as the TensorNets format\n            np.savez(model_name, names=names, values=values)\n\n            # Load and set the values\n            weights = model.get_weights()\n            values = nets.utils.parse_weights(model_name + \'.npz\')\n            sess.run([w.assign(v) for (w, v) in zip(weights, values)])\n\n            # Run equivalence tests\n            preds = sess.run(model, {inputs: model.preprocess(img)})\n            preds_tfhub = sess.run(model_tfhub, {inputs: img / 255.})\n            if not model_name[:12] == \'efficientnet\':\n                preds_tfhub = preds_tfhub[:, 1:]\n            np.testing.assert_allclose(preds, preds_tfhub, atol=1e-4)\n'"
tensornets/contrib_framework/__init__.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A module containing TensorFlow ops whose API may change in the future.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# TODO(ptucker): Add these to tf.contrib.variables?\n# pylint: disable=wildcard-import\nfrom .arg_scope import *\n#from .checkpoint_ops import *\n#from .ops import *\n#from .prettyprint_ops import *\n#from .script_ops import *\n#from .sort_ops import *\nfrom .variables import *\n# pylint: enable=wildcard-import\n'"
tensornets/contrib_framework/arg_scope.py,4,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the arg_scope used for scoping layers arguments.\n\n  Allows one to define models much more compactly by eliminating boilerplate\n  code. This is accomplished through the use of argument scoping (arg_scope).\n\n  Example of how to use tf.contrib.framework.arg_scope:\n\n  ```\n  from third_party.tensorflow.contrib.layers.python import layers\n\n  arg_scope = tf.contrib.framework.arg_scope\n\n  with arg_scope([layers.conv2d], padding=\'SAME\',\n                 initializer=layers.variance_scaling_initializer(),\n                 regularizer=layers.l2_regularizer(0.05)):\n    net = layers.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\', scope=\'conv1\')\n    net = layers.conv2d(net, 256, [5, 5], scope=\'conv2\')\n  ```\n  The first call to conv2d will behave as follows:\n    layers.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                  initializer=layers.variance_scaling_initializer(),\n                  regularizer=layers.l2_regularizer(0.05), scope=\'conv1\')\n\n  The second call to conv2d will also use the arg_scope\'s default for padding:\n    layers.conv2d(inputs, 256, [5, 5], padding=\'SAME\',\n                  initializer=layers.variance_scaling_initializer(),\n                  regularizer=layers.l2_regularizer(0.05), scope=\'conv2\')\n\n  Example of how to reuse an arg_scope:\n\n  ```\n  with arg_scope([layers.conv2d], padding=\'SAME\',\n                 initializer=layers.variance_scaling_initializer(),\n                 regularizer=layers.l2_regularizer(0.05)) as sc:\n    net = layers.conv2d(net, 256, [5, 5], scope=\'conv1\')\n    ....\n\n  with arg_scope(sc):\n    net = layers.conv2d(net, 256, [5, 5], scope=\'conv2\')\n  ```\n\n  Example of how to use tf.contrib.framework.add_arg_scope to enable your\n  function to be called within an arg_scope later:\n\n  @tf.contrib.framework.add_arg_scope\n  def conv2d(*args, **kwargs)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.util import tf_contextlib\nfrom tensorflow.python.util import tf_decorator\n\n__all__ = [\n    \'arg_scope\', \'add_arg_scope\', \'current_arg_scope\', \'has_arg_scope\',\n    \'arg_scoped_arguments\', \'arg_scope_func_key\'\n]\n\n_ARGSTACK = [{}]\n\n_DECORATED_OPS = {}\n\n\ndef _get_arg_stack():\n  if _ARGSTACK:\n    return _ARGSTACK\n  else:\n    _ARGSTACK.append({})\n    return _ARGSTACK\n\n\ndef current_arg_scope():\n  stack = _get_arg_stack()\n  return stack[-1]\n\n\ndef arg_scope_func_key(op):\n  return getattr(op, \'_key_op\', str(op))\n\n\ndef _name_op(op):\n  return (op.__module__, op.__name__)\n\n\ndef _kwarg_names(func):\n  kwargs_length = len(func.__defaults__) if func.__defaults__ else 0\n  return func.__code__.co_varnames[-kwargs_length:func.__code__.co_argcount]\n\n\ndef _add_op(op):\n  key_op = arg_scope_func_key(op)\n  _DECORATED_OPS[key_op] = _kwarg_names(op)\n\n\n@tf_contextlib.contextmanager\ndef arg_scope(list_ops_or_scope, **kwargs):\n  """"""Stores the default arguments for the given set of list_ops.\n\n  For usage, please see examples at top of the file.\n\n  Args:\n    list_ops_or_scope: List or tuple of operations to set argument scope for or\n      a dictionary containing the current scope. When list_ops_or_scope is a\n      dict, kwargs must be empty. When list_ops_or_scope is a list or tuple,\n      then every op in it need to be decorated with @add_arg_scope to work.\n    **kwargs: keyword=value that will define the defaults for each op in\n              list_ops. All the ops need to accept the given set of arguments.\n\n  Yields:\n    the current_scope, which is a dictionary of {op: {arg: value}}\n  Raises:\n    TypeError: if list_ops is not a list or a tuple.\n    ValueError: if any op in list_ops has not be decorated with @add_arg_scope.\n  """"""\n  if isinstance(list_ops_or_scope, dict):\n    # Assumes that list_ops_or_scope is a scope that is being reused.\n    if kwargs:\n      raise ValueError(\'When attempting to re-use a scope by suppling a\'\n                       \'dictionary, kwargs must be empty.\')\n    current_scope = list_ops_or_scope.copy()\n    try:\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()\n  else:\n    # Assumes that list_ops_or_scope is a list/tuple of ops with kwargs.\n    if not isinstance(list_ops_or_scope, (list, tuple)):\n      raise TypeError(\'list_ops_or_scope must either be a list/tuple or reused \'\n                      \'scope (i.e. dict)\')\n    try:\n      current_scope = current_arg_scope().copy()\n      for op in list_ops_or_scope:\n        key = arg_scope_func_key(op)\n        if not has_arg_scope(op):\n          raise ValueError(\'%s is not decorated with @add_arg_scope\',\n                           _name_op(op))\n        if key in current_scope:\n          current_kwargs = current_scope[key].copy()\n          current_kwargs.update(kwargs)\n          current_scope[key] = current_kwargs\n        else:\n          current_scope[key] = kwargs.copy()\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()\n\n\ndef add_arg_scope(func):\n  """"""Decorates a function with args so it can be used within an arg_scope.\n\n  Args:\n    func: function to decorate.\n\n  Returns:\n    A tuple with the decorated function func_with_args().\n  """"""\n\n  def func_with_args(*args, **kwargs):\n    current_scope = current_arg_scope()\n    current_args = kwargs\n    key_func = arg_scope_func_key(func)\n    if key_func in current_scope:\n      current_args = current_scope[key_func].copy()\n      current_args.update(kwargs)\n    return func(*args, **current_args)\n\n  _add_op(func)\n  setattr(func_with_args, \'_key_op\', arg_scope_func_key(func))\n  return tf_decorator.make_decorator(func, func_with_args)\n\n\ndef has_arg_scope(func):\n  """"""Checks whether a func has been decorated with @add_arg_scope or not.\n\n  Args:\n    func: function to check.\n\n  Returns:\n    a boolean.\n  """"""\n  return arg_scope_func_key(func) in _DECORATED_OPS\n\n\ndef arg_scoped_arguments(func):\n  """"""Returns the list kwargs that arg_scope can set for a func.\n\n  Args:\n    func: function which has been decorated with @add_arg_scope.\n\n  Returns:\n    a list of kwargs names.\n  """"""\n  assert has_arg_scope(func)\n  return _DECORATED_OPS[arg_scope_func_key(func)]\n'"
tensornets/contrib_framework/variables.py,18,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Variable functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport re\n\nfrom .arg_scope import add_arg_scope as contrib_add_arg_scope\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.framework import device as tf_device\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.training import saver as tf_saver\nfrom tensorflow.python.training import training_util\nfrom tensorflow.python.util.deprecation import deprecated\n\n\n__all__ = [\'add_model_variable\',\n           \'assert_global_step\',\n           \'assert_or_get_global_step\',\n           \'assign_from_checkpoint\',\n           \'assign_from_checkpoint_fn\',\n           \'assign_from_values\',\n           \'assign_from_values_fn\',\n           \'create_global_step\',\n           \'filter_variables\',\n           \'get_global_step\',\n           \'get_or_create_global_step\',\n           \'get_local_variables\',\n           \'get_model_variables\',\n           \'get_trainable_variables\',\n           \'get_unique_variable\',\n           \'get_variables_by_name\',\n           \'get_variables_by_suffix\',\n           \'get_variable_full_name\',\n           \'get_variables_to_restore\',\n           \'get_variables\',\n           \'global_variable\',\n           \'local_variable\',\n           \'model_variable\',\n           \'variable\',\n           \'VariableDeviceChooser\']\n\n\n@deprecated(None, \'Please switch to tf.train.assert_global_step\')\ndef assert_global_step(global_step_tensor):\n  training_util.assert_global_step(global_step_tensor)\n\n\ndef assert_or_get_global_step(graph=None, global_step_tensor=None):\n  """"""Verifies that a global step tensor is valid or gets one if None is given.\n\n  If `global_step_tensor` is not None, check that it is a valid global step\n  tensor (using `assert_global_step`). Otherwise find a global step tensor using\n  `get_global_step` and return it.\n\n  Args:\n    graph: The graph to find the global step tensor for.\n    global_step_tensor: The tensor to check for suitability as a global step. If\n      None is given (the default), find a global step tensor.\n\n  Returns:\n    A tensor suitable as a global step, or `None` if none was provided and none\n    was found.\n  """"""\n  if global_step_tensor is None:\n    # Get the global step tensor the same way the supervisor would.\n    global_step_tensor = get_global_step(graph)\n  else:\n    assert_global_step(global_step_tensor)\n  return global_step_tensor\n\n\n@deprecated(None, \'Please switch to tf.train.get_global_step\')\ndef get_global_step(graph=None):\n  return training_util.get_global_step(graph)\n\n\n@deprecated(None, \'Please switch to tf.train.create_global_step\')\ndef create_global_step(graph=None):\n  """"""Create global step tensor in graph.\n\n  This API is deprecated. Use core framework training version instead.\n\n  Args:\n    graph: The graph in which to create the global step tensor. If missing, use\n      default graph.\n\n  Returns:\n    Global step tensor.\n\n  Raises:\n    ValueError: if global step tensor is already defined.\n  """"""\n  return training_util.create_global_step(graph)\n\n\n@deprecated(None, \'Please switch to tf.train.get_or_create_global_step\')\ndef get_or_create_global_step(graph=None):\n  """"""Returns and create (if necessary) the global step tensor.\n\n  Args:\n    graph: The graph in which to create the global step tensor. If missing, use\n      default graph.\n\n  Returns:\n    The global step tensor.\n  """"""\n  return training_util.get_or_create_global_step(graph)\n\n\ndef local_variable(initial_value,\n                   validate_shape=True,\n                   name=None,\n                   use_resource=None):\n  """"""Create a variable with a value and add it to `GraphKeys.LOCAL_VARIABLES`.\n\n  Args:\n    initial_value: See variables.Variable.__init__.\n    validate_shape: See variables.Variable.__init__.\n    name: See variables.Variable.__init__.\n    use_resource: If `True` use a ResourceVariable instead of a Variable.\n\n  Returns:\n    New variable.\n  """"""\n  return variable_scope.variable(\n      initial_value,\n      trainable=False,\n      collections=[ops.GraphKeys.LOCAL_VARIABLES],\n      validate_shape=validate_shape,\n      use_resource=use_resource,\n      name=name)\n\n\ndef global_variable(initial_value,\n                    validate_shape=True,\n                    name=None,\n                    use_resource=None):\n  """"""Create a variable with a value and add it to `GraphKeys.GLOBAL_VARIABLES`.\n\n  Args:\n    initial_value: See variables.Variable.__init__.\n    validate_shape: See variables.Variable.__init__.\n    name: See variables.Variable.__init__.\n    use_resource: If `True` use a ResourceVariable instead of a Variable.\n\n  Returns:\n    New variable.\n  """"""\n  return variable_scope.variable(\n      initial_value,\n      trainable=False,\n      collections=[ops.GraphKeys.GLOBAL_VARIABLES],\n      validate_shape=validate_shape,\n      use_resource=use_resource,\n      name=name)\n\n\n@contrib_add_arg_scope\ndef variable(name,\n             shape=None,\n             dtype=None,\n             initializer=None,\n             regularizer=None,\n             trainable=True,\n             collections=None,\n             caching_device=None,\n             device=None,\n             partitioner=None,\n             custom_getter=None,\n             use_resource=None,\n             synchronization=variables.VariableSynchronization.AUTO,\n             aggregation=variables.VariableAggregation.NONE):\n  """"""Gets an existing variable with these parameters or creates a new one.\n\n  Args:\n    name: the name of the new or existing variable.\n    shape: shape of the new or existing variable.\n    dtype: type of the new or existing variable (defaults to `DT_FLOAT`).\n    initializer: initializer for the variable if one is created.\n    regularizer: a (Tensor -> Tensor or None) function; the result of applying\n      it on a newly created variable will be added to the collection\n      GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n    trainable: If `True` also add the variable to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    collections: A list of collection names to which the Variable will be added.\n      If None it would default to `tf.GraphKeys.GLOBAL_VARIABLES`.\n    caching_device: Optional device string or function describing where the\n      Variable should be cached for reading.  Defaults to the Variable\'s device.\n    device: Optional device to place the variable. It can be an string or a\n      function that is called to get the device for the variable.\n    partitioner: Optional callable that accepts a fully defined `TensorShape`\n      and dtype of the `Variable` to be created, and returns a list of\n      partitions for each axis (currently only one axis can be partitioned).\n    custom_getter: Callable that allows overwriting the internal get_variable\n      method and has to have the same signature.\n    use_resource: If `True` use a ResourceVariable instead of a Variable.\n    synchronization: Indicates when a distributed a variable will be aggregated.\n      Accepted values are constants defined in the class\n      `tf.VariableSynchronization`. By default the synchronization is set to\n      `AUTO` and the current `DistributionStrategy` chooses when to synchronize.\n    aggregation: Indicates how a distributed variable will be aggregated.\n      Accepted values are constants defined in the class\n      `tf.VariableAggregation`.\n\n  Returns:\n    The created or existing variable.\n  """"""\n  collections = list(collections if collections is not None else\n                     [ops.GraphKeys.GLOBAL_VARIABLES])\n\n  # Remove duplicates\n  collections = list(set(collections))\n  getter = variable_scope.get_variable\n  if custom_getter is not None:\n    getter = functools.partial(\n        custom_getter, reuse=variable_scope.get_variable_scope().reuse)\n  with ops.device(device or \'\'):\n    return getter(\n        name,\n        shape=shape,\n        dtype=dtype,\n        initializer=initializer,\n        regularizer=regularizer,\n        trainable=trainable,\n        collections=collections,\n        caching_device=caching_device,\n        partitioner=partitioner,\n        use_resource=use_resource,\n        synchronization=synchronization,\n        aggregation=aggregation)\n\n\n@contrib_add_arg_scope\ndef model_variable(name,\n                   shape=None,\n                   dtype=dtypes.float32,\n                   initializer=None,\n                   regularizer=None,\n                   trainable=True,\n                   collections=None,\n                   caching_device=None,\n                   device=None,\n                   partitioner=None,\n                   custom_getter=None,\n                   use_resource=None,\n                   synchronization=variables.VariableSynchronization.AUTO,\n                   aggregation=variables.VariableAggregation.NONE):\n  """"""Gets an existing model variable with these parameters or creates a new one.\n\n  Args:\n    name: the name of the new or existing variable.\n    shape: shape of the new or existing variable.\n    dtype: type of the new or existing variable (defaults to `DT_FLOAT`).\n    initializer: initializer for the variable if one is created.\n    regularizer: a (Tensor -> Tensor or None) function; the result of applying\n      it on a newly created variable will be added to the collection\n      GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n    trainable: If `True` also add the variable to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    collections: A list of collection names to which the Variable will be added.\n      Note that the variable is always also added to the\n      `GraphKeys.GLOBAL_VARIABLES` and `GraphKeys.MODEL_VARIABLES` collections.\n    caching_device: Optional device string or function describing where the\n      Variable should be cached for reading.  Defaults to the Variable\'s device.\n    device: Optional device to place the variable. It can be an string or a\n      function that is called to get the device for the variable.\n    partitioner: Optional callable that accepts a fully defined `TensorShape`\n      and dtype of the `Variable` to be created, and returns a list of\n      partitions for each axis (currently only one axis can be partitioned).\n    custom_getter: Callable that allows overwriting the internal get_variable\n      method and has to have the same signature.\n    use_resource: If `True` use a ResourceVariable instead of a Variable.\n    synchronization: Indicates when a distributed a variable will be aggregated.\n      Accepted values are constants defined in the class\n      `tf.VariableSynchronization`. By default the synchronization is set to\n      `AUTO` and the current `DistributionStrategy` chooses when to synchronize.\n    aggregation: Indicates how a distributed variable will be aggregated.\n      Accepted values are constants defined in the class\n      `tf.VariableAggregation`.\n\n  Returns:\n    The created or existing variable.\n  """"""\n  collections = list(collections or [])\n  collections += [ops.GraphKeys.GLOBAL_VARIABLES, ops.GraphKeys.MODEL_VARIABLES]\n  var = variable(\n      name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      regularizer=regularizer,\n      trainable=trainable,\n      collections=collections,\n      caching_device=caching_device,\n      device=device,\n      partitioner=partitioner,\n      custom_getter=custom_getter,\n      use_resource=use_resource,\n      synchronization=synchronization,\n      aggregation=aggregation)\n  return var\n\n\ndef add_model_variable(var):\n  """"""Adds a variable to the `GraphKeys.MODEL_VARIABLES` collection.\n\n  Args:\n    var: a variable.\n  """"""\n  if var not in ops.get_collection(ops.GraphKeys.MODEL_VARIABLES):\n    ops.add_to_collection(ops.GraphKeys.MODEL_VARIABLES, var)\n\n\ndef get_variables(scope=None,\n                  suffix=None,\n                  collection=ops.GraphKeys.GLOBAL_VARIABLES):\n  """"""Gets the list of variables, filtered by scope and/or suffix.\n\n  Args:\n    scope: an optional scope for filtering the variables to return. Can be a\n      variable scope or a string.\n    suffix: an optional suffix for filtering the variables to return.\n    collection: in which collection search for. Defaults to\n      `GraphKeys.GLOBAL_VARIABLES`.\n\n  Returns:\n    a list of variables in collection with scope and suffix.\n  """"""\n  if isinstance(scope, variable_scope.VariableScope):\n    scope = scope.name\n  if suffix is not None:\n    if \':\' not in suffix:\n      suffix += \':\'\n    scope = (scope or \'\') + \'.*\' + suffix\n  return ops.get_collection(collection, scope)\n\n\ndef get_model_variables(scope=None, suffix=None):\n  """"""Gets the list of model variables, filtered by scope and/or suffix.\n\n  Args:\n    scope: an optional scope for filtering the variables to return.\n    suffix: an optional suffix for filtering the variables to return.\n\n  Returns:\n    a list of variables in collection with scope and suffix.\n  """"""\n  return get_variables(scope, suffix, ops.GraphKeys.MODEL_VARIABLES)\n\n\ndef get_local_variables(scope=None, suffix=None):\n  """"""Gets the list of local variables, filtered by scope and/or suffix.\n\n  Args:\n    scope: an optional scope for filtering the variables to return.\n    suffix: an optional suffix for filtering the variables to return.\n\n  Returns:\n    a list of variables in collection with scope and suffix.\n  """"""\n  return get_variables(scope, suffix, ops.GraphKeys.LOCAL_VARIABLES)\n\n\ndef get_trainable_variables(scope=None, suffix=None):\n  """"""Gets the list of trainable variables, filtered by scope and/or suffix.\n\n  Args:\n    scope: an optional scope for filtering the variables to return.\n    suffix: an optional suffix for filtering the variables to return.\n\n  Returns:\n    a list of variables in the trainable collection with scope and suffix.\n  """"""\n  return get_variables(scope, suffix, ops.GraphKeys.TRAINABLE_VARIABLES)\n\n\ndef get_variables_to_restore(include=None, exclude=None):\n  """"""Gets the list of the variables to restore.\n\n  Args:\n    include: an optional list/tuple of scope strings for filtering which\n      variables from the VARIABLES collection to include. None would include all\n      the variables.\n    exclude: an optional list/tuple of scope strings for filtering which\n      variables from the VARIABLES collection to exclude. None it would not\n      exclude any.\n\n  Returns:\n    a list of variables to restore.\n\n  Raises:\n    TypeError: include or exclude is provided but is not a list or a tuple.\n  """"""\n  if include is None:\n    # Include all variables.\n    vars_to_include = get_variables()\n  else:\n    if not isinstance(include, (list, tuple)):\n      raise TypeError(\'include is provided but is not a list or a tuple.\')\n    vars_to_include = []\n    for scope in include:\n      vars_to_include += get_variables(scope)\n  vars_to_exclude = set()\n  if exclude is not None:\n    if not isinstance(exclude, (list, tuple)):\n      raise TypeError(\'exclude is provided but is not a list or a tuple.\')\n    for scope in exclude:\n      vars_to_exclude |= set(get_variables(scope))\n  # Exclude the variables in vars_to_exclude\n  return [v for v in vars_to_include if v not in vars_to_exclude]\n\n\ndef get_variables_by_suffix(suffix, scope=None):\n  """"""Gets the list of variables that end with the given suffix.\n\n  Args:\n    suffix: suffix for filtering the variables to return.\n    scope: an optional scope for filtering the variables to return.\n\n  Returns:\n    a copied list of variables with the given name and prefix.\n  """"""\n  return get_variables(scope=scope, suffix=suffix)\n\n\ndef get_variables_by_name(given_name, scope=None):\n  """"""Gets the list of variables that were given that name.\n\n  Args:\n    given_name: name given to the variable without any scope.\n    scope: an optional scope for filtering the variables to return.\n\n  Returns:\n    a copied list of variables with the given name and scope.\n  """"""\n  suffix = \'/\' + given_name + \':|^\' + given_name + \':\'\n  return get_variables(scope=scope, suffix=suffix)\n\n\ndef get_unique_variable(var_op_name):\n  """"""Gets the variable uniquely identified by that var_op_name.\n\n  Args:\n    var_op_name: the full name of the variable op, including the scope.\n\n  Returns:\n    a tensorflow variable.\n\n  Raises:\n    ValueError: if no variable uniquely identified by the name exists.\n  """"""\n  candidates = get_variables(scope=var_op_name)\n  if not candidates:\n    raise ValueError(\'Couldn\\\'t find variable %s\' % var_op_name)\n\n  for candidate in candidates:\n    if candidate.op.name == var_op_name:\n      return candidate\n  raise ValueError(\'Variable %s does not uniquely identify a variable\' %\n                   var_op_name)\n\n\ndef assign_from_values(var_names_to_values):\n  """"""Creates an assignment operation from a given mapping.\n\n  This function provides a mechanism for performing assignment of variables\n  to values in a way that does not fill the graph with large assignment values.\n\n  Args:\n    var_names_to_values: A map from variable names to values.\n\n  Returns:\n    assign_op: An `Operation` that assigns each of the given variables to the\n      requested values.\n    feed_dict: The feed dictionary to use when evaluating `assign_op`.\n\n  Raises:\n    ValueError: if any of the given variable names were not found.\n  """"""\n  feed_dict = {}\n  assign_ops = []\n\n  for var_name in var_names_to_values:\n    var_value = var_names_to_values[var_name]\n    var = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES, var_name)\n    if not var:\n      raise ValueError(\'Variable %s wasn\\\'t found\' % var_name)\n    elif len(var) > 1:\n      # tf.compat.v1.get_collection is just a filter on the prefix: find the exact match:\n      found = False\n      for v in var:\n        if v.op.name == var_name:\n          var = v\n          found = True\n          break\n\n      if not found:\n        raise ValueError(\'Variable %s doesn\\\'t uniquely identify a variable\' %\n                         var_name)\n    else:\n      var = var[0]\n\n    # TODO(nsilberman): ensure placeholder and assign are on the same device.\n    # Assign a placeholder to the value that will be filled later.\n    placeholder_name = \'placeholder/\' + var.op.name\n    placeholder_value = array_ops.placeholder(\n        dtype=var.dtype.base_dtype,\n        shape=var.get_shape(),\n        name=placeholder_name)\n    assign_ops.append(var.assign(placeholder_value))\n\n    feed_dict[placeholder_value] = var_value.reshape(var.get_shape())\n\n  assign_op = control_flow_ops.group(*assign_ops)\n  return assign_op, feed_dict\n\n\ndef assign_from_values_fn(var_names_to_values):\n  """"""Returns a function that assigns specific variables from the given values.\n\n  This function provides a mechanism for performing assignment of variables\n  to values in a way that does not fill the graph with large assignment values.\n\n  Args:\n    var_names_to_values: A map from variable names to values.\n\n  Returns:\n    A function that takes a single argument, a `tf.compat.v1.Session`, that\n    applies the\n    assignment operation.\n\n  Raises:\n    ValueError: if any of the given variable names were not found.\n  """"""\n  assign_op, feed_dict = assign_from_values(var_names_to_values)\n\n  def callback(session):\n    return session.run(assign_op, feed_dict)\n\n  return callback\n\n\n# pylint: disable=protected-access\n# Currently variable_scope doesn\'t provide very good APIs to access\n# all variables under scope and retrieve and check existing scopes.\ndef get_variable_full_name(var):\n  """"""Returns the full name of a variable.\n\n  For normal Variables, this is the same as the var.op.name.  For\n  sliced or PartitionedVariables, this name is the same for all the\n  slices/partitions. In both cases, this is normally the name used in\n  a checkpoint file.\n\n  Args:\n    var: A `Variable` object.\n\n  Returns:\n    A string that is the full name.\n  """"""\n  if var._save_slice_info:\n    return var._save_slice_info.full_name\n  else:\n    return var.op.name\n\n\n# TODO(nsilberman): add flag to load exponential moving averages instead\n#\n# TODO(sguada): Update docs in slim/g3doc/index.md to describe\n# the new feature where the var_list dictionary can have values that\n# are each a list of Variables.\ndef assign_from_checkpoint(model_path, var_list, ignore_missing_vars=False):\n  """"""Creates an operation to assign specific variables from a checkpoint.\n\n  Args:\n    model_path: The full path to the model checkpoint. To get latest checkpoint\n      use `model_path = tf.train.latest_checkpoint(checkpoint_dir)`\n    var_list: A list of (possibly partitioned) `Variable` objects or a\n      dictionary mapping names in the checkpoint to the corresponding variables\n      or list of variables to initialize from that checkpoint value. For\n      partitioned Variables, the name in the checkpoint must be the full\n      variable, not the name of the partitioned variable, eg. ""my_var"" rather\n      than ""my_var/part_4"". If empty, returns no_op(), {}.\n    ignore_missing_vars: Boolean, if True ignore variables missing in the\n      checkpoint with a warning instead of failing.\n\n  Returns:\n    the restore_op and the feed_dict that need to be run to restore var_list.\n\n  Raises:\n    ValueError: If `ignore_missing_vars` is False and the checkpoint specified\n        at `model_path` is missing one of the variables in `var_list`.\n  """"""\n  # Normalize var_list into a dictionary mapping names in the\n  # checkpoint to the list of variables to initialize from that\n  # checkpoint variable. Sliced (including partitioned) variables will\n  # end up under the same key.\n  grouped_vars = {}\n  if isinstance(var_list, (tuple, list)):\n    for var in var_list:\n      ckpt_name = get_variable_full_name(var)\n      if ckpt_name not in grouped_vars:\n        grouped_vars[ckpt_name] = []\n      grouped_vars[ckpt_name].append(var)\n\n  else:\n    for ckpt_name, value in var_list.items():\n      if isinstance(value, (tuple, list)):\n        grouped_vars[ckpt_name] = value\n      else:\n        grouped_vars[ckpt_name] = [value]\n\n  # Read each checkpoint entry. Create a placeholder variable and\n  # add the (possibly sliced) data from the checkpoint to the feed_dict.\n  reader = pywrap_tensorflow.NewCheckpointReader(model_path)\n  feed_dict = {}\n  assign_ops = []\n  for ckpt_name in grouped_vars:\n    if not reader.has_tensor(ckpt_name):\n      log_str = \'Checkpoint is missing variable [%s]\' % ckpt_name\n      if ignore_missing_vars:\n        logging.warning(log_str)\n        continue\n      else:\n        raise ValueError(log_str)\n    ckpt_value = reader.get_tensor(ckpt_name)\n\n    for var in grouped_vars[ckpt_name]:\n      placeholder_tensor = array_ops.placeholder(\n          dtype=var.dtype.base_dtype,\n          shape=var.get_shape(),\n          name=\'placeholder/\' + var.op.name)\n      assign_ops.append(var.assign(placeholder_tensor))\n\n      if not var._save_slice_info:\n        if var.get_shape() != ckpt_value.shape:\n          raise ValueError(\n              \'Total size of new array must be unchanged for %s \'\n              \'lh_shape: [%s], rh_shape: [%s]\' %\n              (ckpt_name, str(ckpt_value.shape), str(var.get_shape())))\n\n        feed_dict[placeholder_tensor] = ckpt_value.reshape(ckpt_value.shape)\n      else:\n        slice_dims = zip(var._save_slice_info.var_offset,\n                         var._save_slice_info.var_shape)\n        slice_dims = [(start, start + size) for (start, size) in slice_dims]\n        slice_dims = [slice(*x) for x in slice_dims]\n        slice_value = ckpt_value[slice_dims]\n        slice_value = slice_value.reshape(var._save_slice_info.var_shape)\n        feed_dict[placeholder_tensor] = slice_value\n\n  assign_op = control_flow_ops.group(*assign_ops)\n  return assign_op, feed_dict\n\n\n# pylint: enable=protected-access\n\n\ndef assign_from_checkpoint_fn(model_path,\n                              var_list,\n                              ignore_missing_vars=False,\n                              reshape_variables=False):\n  """"""Returns a function that assigns specific variables from a checkpoint.\n\n  If ignore_missing_vars is True and no variables are found in the checkpoint\n  it returns None.\n\n  Args:\n    model_path: The full path to the model checkpoint. To get latest checkpoint\n      use `model_path = tf.train.latest_checkpoint(checkpoint_dir)`\n    var_list: A list of `Variable` objects or a dictionary mapping names in the\n      checkpoint to the corresponding variables to initialize. If empty or\n      `None`, it would return `no_op(), None`.\n    ignore_missing_vars: Boolean, if True it would ignore variables missing in\n      the checkpoint with a warning instead of failing.\n    reshape_variables: Boolean, if True it would automatically reshape variables\n      which are of different shape then the ones stored in the checkpoint but\n      which have the same number of elements.\n\n  Returns:\n    A function that takes a single argument, a `tf.compat.v1.Session`, that\n    applies the\n    assignment operation. If no matching variables were found in the checkpoint\n    then `None` is returned.\n\n  Raises:\n    ValueError: If var_list is empty.\n  """"""\n  if not var_list:\n    raise ValueError(\'var_list cannot be empty\')\n  if ignore_missing_vars:\n    reader = pywrap_tensorflow.NewCheckpointReader(model_path)\n    if isinstance(var_list, dict):\n      var_dict = var_list\n    else:\n      var_dict = {var.op.name: var for var in var_list}\n    available_vars = {}\n    for var in var_dict:\n      if reader.has_tensor(var):\n        available_vars[var] = var_dict[var]\n      else:\n        logging.warning(\'Variable %s missing in checkpoint %s\', var, model_path)\n    var_list = available_vars\n  if var_list:\n    saver = tf_saver.Saver(\n        var_list,\n        reshape=reshape_variables,\n        write_version=saver_pb2.SaverDef.V1)\n\n    def callback(session):\n      saver.restore(session, model_path)\n\n    return callback\n  else:\n    logging.warning(\'No Variables to restore\')\n    return None\n\n\nclass VariableDeviceChooser(object):\n  """"""Device chooser for variables.\n\n  When using a parameter server it will assign them in a round-robin fashion.\n  When not using a parameter server it allows GPU or CPU placement.\n  """"""\n\n  def __init__(self,\n               num_tasks=0,\n               job_name=\'ps\',\n               device_type=\'CPU\',\n               device_index=0,\n               replica=None):\n    """"""Initialize VariableDeviceChooser.\n\n    Usage:\n      To use with 2 parameter servers:\n        VariableDeviceChooser(2)\n\n      To use without parameter servers:\n        VariableDeviceChooser()\n        VariableDeviceChooser(device_type=\'GPU\') # For GPU placement\n\n    Args:\n      num_tasks: number of tasks.\n      job_name: String, a name for the parameter server job.\n      device_type: Optional device type string (e.g. ""CPU"" or ""GPU"")\n      device_index: int.  Optional device index.  If left unspecified, device\n        represents \'any\' device_index.\n    """"""\n    self._job_name = job_name\n    self._device_type = device_type\n    self._device_index = device_index\n    self._replica = replica\n    self._num_tasks = num_tasks\n    self._next_task_id = 0\n\n  def __call__(self, op):\n    device_spec = tf_device.DeviceSpec(\n        replica=self._replica,\n        device_type=self._device_type,\n        device_index=self._device_index)\n    if self._num_tasks > 0:\n      task_id = self._next_task_id\n      self._next_task_id = (self._next_task_id + 1) % self._num_tasks\n      device_spec.job = self._job_name\n      device_spec.task = task_id\n    return device_spec.to_string()\n\n\ndef filter_variables(var_list,\n                     include_patterns=None,\n                     exclude_patterns=None,\n                     reg_search=True):\n  """"""Filter a list of variables using regular expressions.\n\n  First includes variables according to the list of include_patterns.\n  Afterwards, eliminates variables according to the list of exclude_patterns.\n\n  For example, one can obtain a list of variables with the weights of all\n  convolutional layers (depending on the network definition) by:\n\n  ```python\n  variables = tf.contrib.framework.get_model_variables()\n  conv_weight_variables = tf.contrib.framework.filter_variables(\n      variables,\n      include_patterns=[\'Conv\'],\n      exclude_patterns=[\'biases\', \'Logits\'])\n  ```\n\n  Args:\n    var_list: list of variables.\n    include_patterns: list of regular expressions to include. Defaults to None,\n      which means all variables are selected according to the include rules. A\n      variable is included if it matches any of the include_patterns.\n    exclude_patterns: list of regular expressions to exclude. Defaults to None,\n      which means all variables are selected according to the exclude rules. A\n      variable is excluded if it matches any of the exclude_patterns.\n    reg_search: boolean. If True (default), performs re.search to find matches\n      (i.e. pattern can match any substring of the variable name). If False,\n      performs re.match (i.e. regexp should match from the beginning of the\n      variable name).\n\n  Returns:\n    filtered list of variables.\n  """"""\n  if reg_search:\n    reg_exp_func = re.search\n  else:\n    reg_exp_func = re.match\n\n  # First include variables.\n  if include_patterns is None:\n    included_variables = list(var_list)\n  else:\n    included_variables = []\n    for var in var_list:\n      if any(reg_exp_func(ptrn, var.name) for ptrn in include_patterns):\n        included_variables.append(var)\n\n  # Afterwards, exclude variables.\n  if exclude_patterns is None:\n    filtered_variables = included_variables\n  else:\n    filtered_variables = []\n    for var in included_variables:\n      if not any(reg_exp_func(ptrn, var.name) for ptrn in exclude_patterns):\n        filtered_variables.append(var)\n\n  return filtered_variables\n'"
tensornets/contrib_layers/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""layers module with higher level NN primitives.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=wildcard-import\n#from .embedding_ops import *\n#from .encoders import *\n#from .feature_column import *\n#from .feature_column_ops import *\nfrom .initializers import *\nfrom .layers import *\nfrom .normalization import *\nfrom .optimizers import *\nfrom .regularizers import *\nfrom .rev_block_lib import *\nfrom .summaries import *\n#from .target_column import *\n#from tensorflow.contrib.layers.python.ops.bucketization_op import *\n#from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op import *\n# pylint: enable=wildcard-import\n'"
tensornets/contrib_layers/initializers.py,2,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Weight initializers for use with layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import random_ops\n\n\n__all__ = [\'xavier_initializer\', \'xavier_initializer_conv2d\',\n           \'variance_scaling_initializer\']\n\n\ndef xavier_initializer(uniform=True, seed=None, dtype=dtypes.float32):\n  """"""Returns an initializer performing ""Xavier"" initialization for weights.\n\n  This function implements the weight initialization from:\n\n  Xavier Glorot and Yoshua Bengio (2010):\n           [Understanding the difficulty of training deep feedforward neural\n           networks. International conference on artificial intelligence and\n           statistics.](\n           http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n\n  This initializer is designed to keep the scale of the gradients roughly the\n  same in all layers. In uniform distribution this ends up being the range:\n  `x = sqrt(6. / (in + out)); [-x, x]` and for normal distribution a standard\n  deviation of `sqrt(2. / (in + out))` is used.\n\n  Args:\n    uniform: Whether to use uniform or normal distributed random initialization.\n    seed: A Python integer. Used to create random seeds. See\n          `tf.compat.v1.set_random_seed` for behavior.\n    dtype: The data type. Only floating point types are supported.\n\n  Returns:\n    An initializer for a weight matrix.\n  """"""\n  return variance_scaling_initializer(factor=1.0, mode=\'FAN_AVG\',\n                                      uniform=uniform, seed=seed, dtype=dtype)\n\nxavier_initializer_conv2d = xavier_initializer\n\n\ndef variance_scaling_initializer(factor=2.0, mode=\'FAN_IN\', uniform=False,\n                                 seed=None, dtype=dtypes.float32):\n  """"""Returns an initializer that generates tensors without scaling variance.\n\n  When initializing a deep network, it is in principle advantageous to keep\n  the scale of the input variance constant, so it does not explode or diminish\n  by reaching the final layer. This initializer use the following formula:\n\n  ```python\n    if mode=\'FAN_IN\': # Count only number of input connections.\n      n = fan_in\n    elif mode=\'FAN_OUT\': # Count only number of output connections.\n      n = fan_out\n    elif mode=\'FAN_AVG\': # Average number of inputs and output connections.\n      n = (fan_in + fan_out)/2.0\n\n      truncated_normal(shape, 0.0, stddev=sqrt(factor / n))\n  ```\n\n  * To get [Delving Deep into Rectifiers](\n     http://arxiv.org/pdf/1502.01852v1.pdf) (also know as the ""MSRA \n     initialization""), use (Default):<br/>\n    `factor=2.0 mode=\'FAN_IN\' uniform=False`\n  * To get [Convolutional Architecture for Fast Feature Embedding](\n     http://arxiv.org/abs/1408.5093), use:<br/>\n    `factor=1.0 mode=\'FAN_IN\' uniform=True`\n  * To get [Understanding the difficulty of training deep feedforward neural\n    networks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf),\n    use:<br/>\n    `factor=1.0 mode=\'FAN_AVG\' uniform=True.`\n  * To get `xavier_initializer` use either:<br/>\n    `factor=1.0 mode=\'FAN_AVG\' uniform=True`, or<br/>\n    `factor=1.0 mode=\'FAN_AVG\' uniform=False`.\n\n  Args:\n    factor: Float.  A multiplicative factor.\n    mode: String.  \'FAN_IN\', \'FAN_OUT\', \'FAN_AVG\'.\n    uniform: Whether to use uniform or normal distributed random initialization.\n    seed: A Python integer. Used to create random seeds. See\n          `tf.compat.v1.set_random_seed` for behavior.\n    dtype: The data type. Only floating point types are supported.\n\n  Returns:\n    An initializer that generates tensors with unit variance.\n\n  Raises:\n    ValueError: if `dtype` is not a floating point type.\n    TypeError: if `mode` is not in [\'FAN_IN\', \'FAN_OUT\', \'FAN_AVG\'].\n  """"""\n  if not dtype.is_floating:\n    raise TypeError(\'Cannot create initializer for non-floating point type.\')\n  if mode not in [\'FAN_IN\', \'FAN_OUT\', \'FAN_AVG\']:\n    raise TypeError(\'Unknown mode %s [FAN_IN, FAN_OUT, FAN_AVG]\', mode)\n\n  # pylint: disable=unused-argument\n  def _initializer(shape, dtype=dtype, partition_info=None):\n    """"""Initializer function.""""""\n    if not dtype.is_floating:\n      raise TypeError(\'Cannot create initializer for non-floating point type.\')\n    # Estimating fan_in and fan_out is not possible to do perfectly, but we try.\n    # This is the right thing for matrix multiply and convolutions.\n    if shape:\n      fan_in = float(shape[-2]) if len(shape) > 1 else float(shape[-1])\n      fan_out = float(shape[-1])\n    else:\n      fan_in = 1.0\n      fan_out = 1.0\n    for dim in shape[:-2]:\n      fan_in *= float(dim)\n      fan_out *= float(dim)\n    if mode == \'FAN_IN\':\n      # Count only number of input connections.\n      n = fan_in\n    elif mode == \'FAN_OUT\':\n      # Count only number of output connections.\n      n = fan_out\n    elif mode == \'FAN_AVG\':\n      # Average number of inputs and output connections.\n      n = (fan_in + fan_out) / 2.0\n    if uniform:\n      # To get stddev = math.sqrt(factor / n) need to adjust for uniform.\n      limit = math.sqrt(3.0 * factor / n)\n      return random_ops.random_uniform(shape, -limit, limit,\n                                       dtype, seed=seed)\n    else:\n      # To get stddev = math.sqrt(factor / n) need to adjust for truncated.\n      trunc_stddev = math.sqrt(1.3 * factor / n)\n      return random_ops.truncated_normal(shape, 0.0, trunc_stddev, dtype,\n                                         seed=seed)\n  # pylint: enable=unused-argument\n\n  return _initializer\n'"
tensornets/contrib_layers/layers.py,29,"b'# -*- coding: utf-8 -*-\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=g-short-docstring-punctuation\n""""""Higher level ops for building layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport six\n\nfrom ..contrib_framework import add_arg_scope\nfrom ..contrib_framework import variables\nfrom . import initializers\nfrom . import utils\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.keras.engine import input_spec\nfrom tensorflow.python.layers import base\nfrom tensorflow.python.layers import convolutional as convolutional_layers\nfrom tensorflow.python.layers import core as core_layers\nfrom tensorflow.python.layers import normalization as normalization_layers\nfrom tensorflow.python.layers import pooling as pooling_layers\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import linalg_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import standard_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables as tf_variables\nfrom tensorflow.python.training import moving_averages\n\n# TODO(b/28426988): Replace legacy_* fns migrated from slim.\n# TODO(b/28426988): Remove legacy_* when all uses have migrated to new API.\n__all__ = [\n    \'avg_pool2d\', \'avg_pool3d\', \'batch_norm\', \'bias_add\', \'conv1d\', \'conv2d\',\n    \'conv3d\', \'conv2d_in_plane\', \'conv2d_transpose\', \'conv3d_transpose\',\n    \'convolution\', \'convolution1d\', \'convolution2d\', \'convolution2d_in_plane\',\n    \'convolution2d_transpose\', \'convolution3d\', \'convolution3d_transpose\',\n    \'dense_to_sparse\', \'dropout\', \'elu\', \'flatten\', \'fully_connected\', \'GDN\',\n    \'gdn\', \'images_to_sequence\', \'layer_norm\', \'linear\', \'pool\', \'max_pool2d\',\n    \'max_pool3d\', \'one_hot_encoding\', \'relu\', \'relu6\', \'repeat\',\n    \'scale_gradient\', \'separable_conv2d\', \'separable_convolution2d\',\n    \'sequence_to_images\', \'softmax\', \'spatial_softmax\', \'stack\', \'unit_norm\',\n    \'legacy_fully_connected\', \'legacy_linear\', \'legacy_relu\', \'maxout\'\n]\n\nDATA_FORMAT_NCHW = \'NCHW\'\nDATA_FORMAT_NHWC = \'NHWC\'\nDATA_FORMAT_NCDHW = \'NCDHW\'\nDATA_FORMAT_NDHWC = \'NDHWC\'\n\n\n@add_arg_scope\ndef avg_pool2d(inputs,\n               kernel_size,\n               stride=2,\n               padding=\'VALID\',\n               data_format=DATA_FORMAT_NHWC,\n               outputs_collections=None,\n               scope=None):\n  """"""Adds a 2D average pooling op.\n\n  It is assumed that the pooling is done per image but not in batch or channels.\n\n  Args:\n    inputs: A 4-D tensor of shape `[batch_size, height, width, channels]` if\n      `data_format` is `NHWC`, and `[batch_size, channels, height, width]` if\n      `data_format` is `NCHW`.\n    kernel_size: A list of length 2: [kernel_height, kernel_width] of the\n      pooling kernel over which the op is computed. Can be an int if both values\n      are the same.\n    stride: A list of length 2: [stride_height, stride_width]. Can be an int if\n      both strides are the same. Note that presently both strides must have the\n      same value.\n    padding: The padding method, either \'VALID\' or \'SAME\'.\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  with ops.name_scope(scope, \'AvgPool2D\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = pooling_layers.AveragePooling2D(\n        pool_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        _scope=sc)\n    outputs = layer.apply(inputs)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\n@add_arg_scope\ndef avg_pool3d(inputs,\n               kernel_size,\n               stride=2,\n               padding=\'VALID\',\n               data_format=DATA_FORMAT_NDHWC,\n               outputs_collections=None,\n               scope=None):\n  """"""Adds a 3D average pooling op.\n\n  It is assumed that the pooling is done per image but not in batch or channels.\n\n  Args:\n    inputs: A 5-D tensor of shape `[batch_size, depth, height, width, channels]`\n      if `data_format` is `NDHWC`, and `[batch_size, channels, depth, height,\n      width]` if `data_format` is `NCDHW`.\n    kernel_size: A list of length 3: [kernel_depth, kernel_height, kernel_width]\n      of the pooling kernel over which the op is computed. Can be an int if both\n      values are the same.\n    stride: A list of length 3: [stride_depth, stride_height, stride_width]. Can\n      be an int if both strides are the same. Note that presently both strides\n      must have the same value.\n    padding: The padding method, either \'VALID\' or \'SAME\'.\n    data_format: A string. `NDHWC` (default) and `NCDHW` are supported.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NDHWC` nor `NCDHW`.\n  """"""\n  if data_format not in (DATA_FORMAT_NCDHW, DATA_FORMAT_NDHWC):\n    raise ValueError(\'data_format has to be either NCDHW or NDHWC.\')\n  with ops.name_scope(scope, \'AvgPool3D\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = pooling_layers.AveragePooling3D(\n        pool_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        _scope=sc)\n    outputs = layer.apply(inputs)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\ndef _fused_batch_norm(inputs,\n                      decay=0.999,\n                      center=True,\n                      scale=False,\n                      epsilon=0.001,\n                      activation_fn=None,\n                      param_initializers=None,\n                      param_regularizers=None,\n                      updates_collections=ops.GraphKeys.UPDATE_OPS,\n                      is_training=True,\n                      reuse=None,\n                      variables_collections=None,\n                      outputs_collections=None,\n                      trainable=True,\n                      data_format=DATA_FORMAT_NHWC,\n                      zero_debias_moving_mean=False,\n                      scope=None):\n  """"""Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n\n    ""Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift""\n\n    Sergey Ioffe, Christian Szegedy\n\n  Can be used as a normalizer function for conv2d and fully_connected.\n\n  Note: when training, the moving_mean and moving_variance need to be updated.\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n  need to be added as a dependency to the `train_op`. For example:\n\n  ```python\n    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss)\n  ```\n\n  One can set updates_collections=None to force the updates in place, but that\n  can have a speed penalty, especially in distributed settings.\n\n  Args:\n    inputs: A tensor with 2 or more dimensions, where the first dimension has\n      `batch_size`. The normalization is over all but the last dimension if\n      `data_format` is `NHWC` and the second dimension if `data_format` is\n      `NCHW`.\n    decay: Decay for the moving average. Reasonable values for `decay` are close\n      to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n        reasonably good training performance but poor validation and/or test\n        performance.\n    center: If True, add offset of `beta` to normalized tensor.  If False,\n      `beta` is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n      scaling can be done by the next layer.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    param_initializers: Optional initializers for beta, gamma, moving mean and\n      moving variance.\n    param_regularizers: Optional regularizer for beta and gamma.\n    updates_collections: Collections to collect the update ops for computation.\n      The updates_ops need to be executed with the train_op. If None, a control\n      dependency would be added to make sure the updates are computed in place.\n    is_training: Whether or not the layer is in training mode. In training mode\n      it would accumulate the statistics of the moments into `moving_mean` and\n      `moving_variance` using an exponential moving average with the given\n      `decay`. When it is not in training mode then it would use the values of\n      the `moving_mean` and the `moving_variance`.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    zero_debias_moving_mean: Use zero_debias for moving_mean.\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If the rank of `inputs` is neither 2 or 4.\n    ValueError: If rank or `C` dimension of `inputs` is undefined.\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  with variable_scope.variable_scope(\n      scope, \'BatchNorm\', [inputs], reuse=reuse) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    original_shape = inputs.get_shape()\n    original_inputs = inputs\n    original_rank = original_shape.ndims\n    if original_rank is None:\n      raise ValueError(\'Inputs %s has undefined rank\' % inputs.name)\n    elif original_rank not in [2, 4]:\n      raise ValueError(\'Inputs %s has unsupported rank.\'\n                       \' Expected 2 or 4 but got %d\' %\n                       (inputs.name, original_rank))\n    if original_rank == 2:\n      channels = inputs.get_shape().dims[-1].value\n      if channels is None:\n        raise ValueError(\'`C` dimension must be known but is None\')\n      new_shape = [-1, 1, 1, channels]\n      if data_format == DATA_FORMAT_NCHW:\n        new_shape = [-1, channels, 1, 1]\n      inputs = array_ops.reshape(inputs, new_shape)\n    inputs_shape = inputs.get_shape()\n    if data_format == DATA_FORMAT_NHWC:\n      params_shape = inputs_shape[-1:]\n    else:\n      params_shape = inputs_shape[1:2]\n    if not params_shape.is_fully_defined():\n      raise ValueError(\'Inputs %s has undefined `C` dimension %s.\' %\n                       (inputs.name, params_shape))\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta_collections = utils.get_variable_collections(variables_collections,\n                                                      \'beta\')\n    # Float32 required to avoid precision-loss when using fp16 input/output\n    variable_dtype = dtypes.float32\n    if not param_initializers:\n      param_initializers = {}\n    if not param_regularizers:\n      param_regularizers = {}\n    beta_regularizer = param_regularizers.get(\'beta\')\n    gamma_regularizer = param_regularizers.get(\'gamma\')\n\n    if center:\n      beta_initializer = param_initializers.get(\'beta\',\n                                                init_ops.zeros_initializer())\n      beta = variables.model_variable(\n          \'beta\',\n          shape=params_shape,\n          dtype=variable_dtype,\n          initializer=beta_initializer,\n          regularizer=beta_regularizer,\n          collections=beta_collections,\n          trainable=trainable)\n    else:\n      beta = array_ops.constant(0.0, dtype=variable_dtype, shape=params_shape)\n\n    if scale:\n      gamma_collections = utils.get_variable_collections(\n          variables_collections, \'gamma\')\n      gamma_initializer = param_initializers.get(\'gamma\',\n                                                 init_ops.ones_initializer())\n      gamma = variables.model_variable(\n          \'gamma\',\n          shape=params_shape,\n          dtype=variable_dtype,\n          initializer=gamma_initializer,\n          regularizer=gamma_regularizer,\n          collections=gamma_collections,\n          trainable=trainable)\n    else:\n      gamma = array_ops.constant(1.0, dtype=variable_dtype, shape=params_shape)\n\n    # Create moving_mean and moving_variance variables and add them to the\n    # appropriate collections. We disable variable partitioning while creating\n    # them, because assign_moving_average is not yet supported for partitioned\n    # variables (this needs to be handled carefully, as it may break\n    # the checkpoint backward compatibility).\n    with variable_scope.variable_scope(\n        variable_scope.get_variable_scope()) as local_scope:\n      local_scope.set_partitioner(None)\n      moving_mean_collections = utils.get_variable_collections(\n          variables_collections, \'moving_mean\')\n      moving_mean_initializer = param_initializers.get(\n          \'moving_mean\', init_ops.zeros_initializer())\n      moving_mean = variables.model_variable(\n          \'moving_mean\',\n          shape=params_shape,\n          dtype=variable_dtype,\n          initializer=moving_mean_initializer,\n          trainable=False,\n          collections=moving_mean_collections)\n      moving_variance_collections = utils.get_variable_collections(\n          variables_collections, \'moving_variance\')\n      moving_variance_initializer = param_initializers.get(\n          \'moving_variance\', init_ops.ones_initializer())\n      moving_variance = variables.model_variable(\n          \'moving_variance\',\n          shape=params_shape,\n          dtype=variable_dtype,\n          initializer=moving_variance_initializer,\n          trainable=False,\n          collections=moving_variance_collections)\n\n    def _fused_batch_norm_training():\n      return nn.fused_batch_norm(\n          inputs, gamma, beta, epsilon=epsilon, data_format=data_format)\n\n    def _fused_batch_norm_inference():\n      return nn.fused_batch_norm(\n          inputs,\n          gamma,\n          beta,\n          mean=moving_mean,\n          variance=moving_variance,\n          epsilon=epsilon,\n          is_training=False,\n          data_format=data_format)\n\n    outputs, mean, variance = utils.smart_cond(is_training,\n                                               _fused_batch_norm_training,\n                                               _fused_batch_norm_inference)\n\n    # If `is_training` doesn\'t have a constant value, because it is a `Tensor`,\n    # a `Variable` or `Placeholder` then is_training_value will be None and\n    # `need_updates` will be true.\n    is_training_value = utils.constant_value(is_training)\n    need_updates = is_training_value is None or is_training_value\n    if need_updates:\n      if updates_collections is None:\n        no_updates = lambda: outputs\n\n        def _force_updates():\n          """"""Internal function forces updates moving_vars if is_training.""""""\n          update_moving_mean = moving_averages.assign_moving_average(\n              moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\n          update_moving_variance = moving_averages.assign_moving_average(\n              moving_variance, variance, decay, zero_debias=False)\n          with ops.control_dependencies(\n              [update_moving_mean, update_moving_variance]):\n            return array_ops.identity(outputs)\n\n        outputs = utils.smart_cond(is_training, _force_updates, no_updates)\n      else:\n        moving_vars_fn = lambda: (moving_mean, moving_variance)\n\n        def _delay_updates():\n          """"""Internal function that delay updates moving_vars if is_training.""""""\n          update_moving_mean = moving_averages.assign_moving_average(\n              moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\n          update_moving_variance = moving_averages.assign_moving_average(\n              moving_variance, variance, decay, zero_debias=False)\n          return update_moving_mean, update_moving_variance\n\n        update_mean, update_variance = utils.smart_cond(is_training,\n                                                        _delay_updates,\n                                                        moving_vars_fn)\n        ops.add_to_collections(updates_collections, update_mean)\n        ops.add_to_collections(updates_collections, update_variance)\n\n    outputs.set_shape(inputs_shape)\n    if original_shape.ndims == 2:\n      outputs = array_ops.reshape(outputs, array_ops.shape(original_inputs))\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               activation_fn=None,\n               param_initializers=None,\n               param_regularizers=None,\n               updates_collections=ops.GraphKeys.UPDATE_OPS,\n               is_training=True,\n               reuse=None,\n               variables_collections=None,\n               outputs_collections=None,\n               trainable=True,\n               batch_weights=None,\n               fused=None,\n               data_format=DATA_FORMAT_NHWC,\n               zero_debias_moving_mean=False,\n               scope=None,\n               renorm=False,\n               renorm_clipping=None,\n               renorm_decay=0.99,\n               adjustment=None):\n  """"""Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n\n    ""Batch Normalization: Accelerating Deep Network Training by Reducing\n    Internal Covariate Shift""\n\n    Sergey Ioffe, Christian Szegedy\n\n  Can be used as a normalizer function for conv2d and fully_connected. The\n  normalization is over all but the last dimension if `data_format` is `NHWC`\n  and all but the second dimension if `data_format` is `NCHW`.  In case of a 2D\n  tensor this corresponds to the batch dimension, while in case of a 4D tensor\n  this\n  corresponds to the batch and space dimensions.\n\n  Note: when training, the moving_mean and moving_variance need to be updated.\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n  need to be added as a dependency to the `train_op`. For example:\n\n  ```python\n    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss)\n  ```\n\n  One can set updates_collections=None to force the updates in place, but that\n  can have a speed penalty, especially in distributed settings.\n\n  Args:\n    inputs: A tensor with 2 or more dimensions, where the first dimension has\n      `batch_size`. The normalization is over all but the last dimension if\n      `data_format` is `NHWC` and the second dimension if `data_format` is\n      `NCHW`.\n    decay: Decay for the moving average. Reasonable values for `decay` are close\n      to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n        reasonably good training performance but poor validation and/or test\n        performance. Try zero_debias_moving_mean=True for improved stability.\n    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n      is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n      scaling can be done by the next layer.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    param_initializers: Optional initializers for beta, gamma, moving mean and\n      moving variance.\n    param_regularizers: Optional regularizer for beta and gamma.\n    updates_collections: Collections to collect the update ops for computation.\n      The updates_ops need to be executed with the train_op. If None, a control\n      dependency would be added to make sure the updates are computed in place.\n    is_training: Whether or not the layer is in training mode. In training mode\n      it would accumulate the statistics of the moments into `moving_mean` and\n      `moving_variance` using an exponential moving average with the given\n      `decay`. When it is not in training mode then it would use the values of\n      the `moving_mean` and the `moving_variance`.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    batch_weights: An optional tensor of shape `[batch_size]`, containing a\n      frequency weight for each batch item. If present, then the batch\n      normalization uses weighted mean and variance. (This can be used to\n      correct for bias in training example selection.)\n    fused: if `None` or `True`, use a faster, fused implementation if possible.\n      If `False`, use the system recommended implementation.\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    zero_debias_moving_mean: Use zero_debias for moving_mean. It creates a new\n      pair of variables \'moving_mean/biased\' and \'moving_mean/local_step\'.\n    scope: Optional scope for `variable_scope`.\n    renorm: Whether to use Batch Renormalization\n      (https://arxiv.org/abs/1702.03275). This adds extra variables during\n        training. The inference is the same for either value of this parameter.\n    renorm_clipping: A dictionary that may map keys \'rmax\', \'rmin\', \'dmax\' to\n      scalar `Tensors` used to clip the renorm correction. The correction `(r,\n      d)` is used as `corrected_value = normalized_value * r + d`, with `r`\n      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n      dmax are set to inf, 0, inf, respectively.\n    renorm_decay: Momentum used to update the moving means and standard\n      deviations with renorm. Unlike `momentum`, this affects training and\n      should be neither too small (which would add noise) nor too large (which\n      would give stale estimates). Note that `decay` is still applied to get the\n      means and variances for inference.\n    adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n      the input tensor and returning a pair (scale, bias) to apply to the\n      normalized values (before gamma and beta), only during training. For\n      example,\n        `adjustment = lambda shape: (\n          tf.random.uniform(shape[-1:], 0.93, 1.07),\n          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized\n            value by up to 7% up or down, then shift the result by up to 0.1\n            (with independent scaling and bias for each feature but shared\n            across all examples), and finally apply gamma and/or beta. If\n            `None`, no adjustment is applied.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If rank or channels dimension of `inputs` is undefined.\n  """"""\n  if fused is None:\n    fused = True\n\n  # Only use _fused_batch_norm if all of the following three\n  # conditions are true:\n  # (1) fused is set True;\n  # (2) it is possible to use (currently it doesn\'t support batch weights,\n  #   renorm, and the case when rank is neither 2 nor 4);\n  # (3) it is used with zero_debias_moving_mean, or an input shape of rank 2,\n  #   or non-default updates_collections (not implemented in\n  #   normalization_layers.BatchNormalization yet); otherwise use the fused\n  #   implementation in normalization_layers.BatchNormalization.\n  inputs = ops.convert_to_tensor(inputs)\n  rank = inputs.get_shape().ndims\n  possible_to_fuse = (\n      batch_weights is None and not renorm and rank in [2, 4] and\n      adjustment is None)\n  if fused and possible_to_fuse and (\n      zero_debias_moving_mean or rank == 2 or\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\n    return _fused_batch_norm(\n        inputs,\n        decay=decay,\n        center=center,\n        scale=scale,\n        epsilon=epsilon,\n        activation_fn=activation_fn,\n        param_initializers=param_initializers,\n        param_regularizers=param_regularizers,\n        updates_collections=updates_collections,\n        is_training=is_training,\n        reuse=reuse,\n        variables_collections=variables_collections,\n        outputs_collections=outputs_collections,\n        trainable=trainable,\n        data_format=data_format,\n        zero_debias_moving_mean=zero_debias_moving_mean,\n        scope=scope)\n\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n\n  layer_variable_getter = _build_variable_getter()\n  with variable_scope.variable_scope(\n      scope,\n      \'BatchNorm\', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n\n    # Determine whether we can use the core layer class.\n    if (batch_weights is None and\n        updates_collections is ops.GraphKeys.UPDATE_OPS and\n        not zero_debias_moving_mean):\n      # Use the core layer class.\n      axis = 1 if data_format == DATA_FORMAT_NCHW else -1\n      if not param_initializers:\n        param_initializers = {}\n      beta_initializer = param_initializers.get(\'beta\',\n                                                init_ops.zeros_initializer())\n      gamma_initializer = param_initializers.get(\'gamma\',\n                                                 init_ops.ones_initializer())\n      moving_mean_initializer = param_initializers.get(\n          \'moving_mean\', init_ops.zeros_initializer())\n      moving_variance_initializer = param_initializers.get(\n          \'moving_variance\', init_ops.ones_initializer())\n      if not param_regularizers:\n        param_regularizers = {}\n      beta_regularizer = param_regularizers.get(\'beta\')\n      gamma_regularizer = param_regularizers.get(\'gamma\')\n      layer = normalization_layers.BatchNormalization(\n          axis=axis,\n          momentum=decay,\n          epsilon=epsilon,\n          center=center,\n          scale=scale,\n          beta_initializer=beta_initializer,\n          gamma_initializer=gamma_initializer,\n          moving_mean_initializer=moving_mean_initializer,\n          moving_variance_initializer=moving_variance_initializer,\n          beta_regularizer=beta_regularizer,\n          gamma_regularizer=gamma_regularizer,\n          trainable=trainable,\n          renorm=renorm,\n          renorm_clipping=renorm_clipping,\n          renorm_momentum=renorm_decay,\n          adjustment=adjustment,\n          name=sc.name,\n          _scope=sc,\n          _reuse=reuse,\n          fused=fused)\n      outputs = layer.apply(inputs, training=is_training)\n\n      # Add variables to collections.\n      _add_variable_to_collections(layer.moving_mean, variables_collections,\n                                   \'moving_mean\')\n      _add_variable_to_collections(layer.moving_variance, variables_collections,\n                                   \'moving_variance\')\n      if layer.beta is not None:\n        _add_variable_to_collections(layer.beta, variables_collections, \'beta\')\n      if layer.gamma is not None:\n        _add_variable_to_collections(layer.gamma, variables_collections,\n                                     \'gamma\')\n\n      if activation_fn is not None:\n        outputs = activation_fn(outputs)\n      return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n    # Not supported by layer class: batch_weights argument,\n    # and custom updates_collections. In that case, use the legacy BN\n    # implementation.\n    # Custom updates collections are not supported because the update logic\n    # is different in this case, in particular w.r.t. ""forced updates"" and\n    # update op reuse.\n    if renorm:\n      raise ValueError(\'renorm is not supported with batch_weights, \'\n                       \'updates_collections or zero_debias_moving_mean\')\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    if batch_weights is not None:\n      batch_weights = ops.convert_to_tensor(batch_weights)\n      inputs_shape[0:1].assert_is_compatible_with(batch_weights.get_shape())\n      # Reshape batch weight values so they broadcast across inputs.\n      nshape = [-1] + [1 for _ in range(inputs_rank - 1)]\n      batch_weights = array_ops.reshape(batch_weights, nshape)\n\n    if data_format == DATA_FORMAT_NCHW:\n      moments_axes = [0] + list(range(2, inputs_rank))\n      params_shape = inputs_shape[1:2]\n      # For NCHW format, rather than relying on implicit broadcasting, we\n      # explicitly reshape the params to params_shape_broadcast when computing\n      # the moments and the batch normalization.\n      params_shape_broadcast = list([1, inputs_shape.dims[1].value] +\n                                    [1 for _ in range(2, inputs_rank)])\n    else:\n      moments_axes = list(range(inputs_rank - 1))\n      params_shape = inputs_shape[-1:]\n      params_shape_broadcast = None\n    if not params_shape.is_fully_defined():\n      raise ValueError(\'Inputs %s has undefined channels dimension %s.\' %\n                       (inputs.name, params_shape))\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if not param_initializers:\n      param_initializers = {}\n    if center:\n      beta_collections = utils.get_variable_collections(variables_collections,\n                                                        \'beta\')\n      beta_initializer = param_initializers.get(\'beta\',\n                                                init_ops.zeros_initializer())\n      beta = variables.model_variable(\n          \'beta\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=beta_initializer,\n          collections=beta_collections,\n          trainable=trainable)\n    if scale:\n      gamma_collections = utils.get_variable_collections(\n          variables_collections, \'gamma\')\n      gamma_initializer = param_initializers.get(\'gamma\',\n                                                 init_ops.ones_initializer())\n      gamma = variables.model_variable(\n          \'gamma\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=gamma_initializer,\n          collections=gamma_collections,\n          trainable=trainable)\n\n    # Create moving_mean and moving_variance variables and add them to the\n    # appropriate collections. We disable variable partitioning while creating\n    # them, because assign_moving_average is not yet supported for partitioned\n    # variables (this needs to be handled carefully, as it may break\n    # the checkpoint backward compatibility).\n    with variable_scope.variable_scope(\n        variable_scope.get_variable_scope()) as local_scope:\n      local_scope.set_partitioner(None)\n      moving_mean_collections = utils.get_variable_collections(\n          variables_collections, \'moving_mean\')\n      moving_mean_initializer = param_initializers.get(\n          \'moving_mean\', init_ops.zeros_initializer())\n      moving_mean = variables.model_variable(\n          \'moving_mean\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=moving_mean_initializer,\n          trainable=False,\n          collections=moving_mean_collections)\n      moving_variance_collections = utils.get_variable_collections(\n          variables_collections, \'moving_variance\')\n      moving_variance_initializer = param_initializers.get(\n          \'moving_variance\', init_ops.ones_initializer())\n      moving_variance = variables.model_variable(\n          \'moving_variance\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=moving_variance_initializer,\n          trainable=False,\n          collections=moving_variance_collections)\n\n    # If `is_training` doesn\'t have a constant value, because it is a `Tensor`,\n    # a `Variable` or `Placeholder` then is_training_value will be None and\n    # `needs_moments` will be true.\n    is_training_value = utils.constant_value(is_training)\n    need_moments = is_training_value is None or is_training_value\n    if need_moments:\n      # Calculate the moments based on the individual batch.\n      if batch_weights is None:\n        if data_format == DATA_FORMAT_NCHW:\n          mean, variance = nn.moments(inputs, moments_axes, keep_dims=True)\n          mean = array_ops.reshape(mean, [-1])\n          variance = array_ops.reshape(variance, [-1])\n        else:\n          mean, variance = nn.moments(inputs, moments_axes)\n      else:\n        if data_format == DATA_FORMAT_NCHW:\n          mean, variance = nn.weighted_moments(\n              inputs, moments_axes, batch_weights, keepdims=True)\n          mean = array_ops.reshape(mean, [-1])\n          variance = array_ops.reshape(variance, [-1])\n        else:\n          mean, variance = nn.weighted_moments(inputs, moments_axes,\n                                               batch_weights)\n\n      moving_vars_fn = lambda: (moving_mean, moving_variance)\n      if updates_collections is None:\n\n        def _force_updates():\n          """"""Internal function forces updates moving_vars if is_training.""""""\n          update_moving_mean = moving_averages.assign_moving_average(\n              moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\n          update_moving_variance = moving_averages.assign_moving_average(\n              moving_variance, variance, decay, zero_debias=False)\n          with ops.control_dependencies(\n              [update_moving_mean, update_moving_variance]):\n            return array_ops.identity(mean), array_ops.identity(variance)\n\n        mean, variance = utils.smart_cond(is_training, _force_updates,\n                                          moving_vars_fn)\n      else:\n\n        def _delay_updates():\n          """"""Internal function that delay updates moving_vars if is_training.""""""\n          update_moving_mean = moving_averages.assign_moving_average(\n              moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)\n          update_moving_variance = moving_averages.assign_moving_average(\n              moving_variance, variance, decay, zero_debias=False)\n          return update_moving_mean, update_moving_variance\n\n        update_mean, update_variance = utils.smart_cond(is_training,\n                                                        _delay_updates,\n                                                        moving_vars_fn)\n        ops.add_to_collections(updates_collections, update_mean)\n        ops.add_to_collections(updates_collections, update_variance)\n        # Use computed moments during training and moving_vars otherwise.\n        vars_fn = lambda: (mean, variance)\n        mean, variance = utils.smart_cond(is_training, vars_fn, moving_vars_fn)\n    else:\n      mean, variance = moving_mean, moving_variance\n    if data_format == DATA_FORMAT_NCHW:\n      mean = array_ops.reshape(mean, params_shape_broadcast)\n      variance = array_ops.reshape(variance, params_shape_broadcast)\n      if beta is not None:\n        beta = array_ops.reshape(beta, params_shape_broadcast)\n      if gamma is not None:\n        gamma = array_ops.reshape(gamma, params_shape_broadcast)\n\n    # Compute batch_normalization.\n    outputs = nn.batch_normalization(inputs, mean, variance, beta, gamma,\n                                     epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef bias_add(inputs,\n             activation_fn=None,\n             initializer=init_ops.zeros_initializer(),\n             regularizer=None,\n             reuse=None,\n             variables_collections=None,\n             outputs_collections=None,\n             trainable=True,\n             data_format=DATA_FORMAT_NHWC,\n             scope=None):\n  """"""Adds a bias to the inputs.\n\n  Can be used as a normalizer function for conv2d and fully_connected.\n\n  Args:\n    inputs: A tensor of with at least rank 2 and value for the last dimension,\n      e.g. `[batch_size, depth]`, `[None, None, None, depth]`.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    initializer: An initializer for the bias, defaults to 0.\n    regularizer: A regularizer like the result of `l1_regularizer` or\n      `l2_regularizer`.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    data_format: A string. \'NHWC\' and \'NCHW\' are supported.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A tensor representing the result of adding biases to the inputs.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If `data_format` is `NCHW` and rank of `inputs` is not 4.\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If rank or `C` dimension of `inputs` is undefined.\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  with variable_scope.variable_scope(\n      scope, \'BiasAdd\', [inputs], reuse=reuse) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    dtype = inputs.dtype.base_dtype\n    inputs_shape = inputs.get_shape()\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError(\'Dims of shape must be known but is None\')\n    elif inputs_rank != 4 and data_format == DATA_FORMAT_NCHW:\n      raise ValueError(\'Data format NCHW only supports 4D Tensor\')\n    axis = 1 if data_format == DATA_FORMAT_NCHW else -1\n    num_features = inputs_shape.dims[axis].value\n    if num_features is None:\n      raise ValueError(\'`C` dimension must be known but is None\')\n    biases_collections = utils.get_variable_collections(variables_collections,\n                                                        \'biases\')\n    biases = variables.model_variable(\n        \'biases\',\n        shape=[\n            num_features,\n        ],\n        dtype=dtype,\n        initializer=initializer,\n        regularizer=regularizer,\n        collections=biases_collections,\n        trainable=trainable)\n    outputs = nn.bias_add(inputs, biases, data_format=data_format)\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n# TODO(jbms): change `rate` parameter to `dilation_rate` for consistency with\n# underlying op.\n@add_arg_scope\ndef convolution(inputs,\n                num_outputs,\n                kernel_size,\n                stride=1,\n                padding=\'SAME\',\n                data_format=None,\n                rate=1,\n                activation_fn=nn.relu,\n                normalizer_fn=None,\n                normalizer_params=None,\n                weights_initializer=initializers.xavier_initializer(),\n                weights_regularizer=None,\n                biases_initializer=init_ops.zeros_initializer(),\n                biases_regularizer=None,\n                reuse=None,\n                variables_collections=None,\n                outputs_collections=None,\n                trainable=True,\n                scope=None,\n                conv_dims=None):\n  """"""Adds an N-D convolution followed by an optional batch_norm layer.\n\n  It is required that 1 <= N <= 3.\n\n  `convolution` creates a variable called `weights`, representing the\n  convolutional kernel, that is convolved (actually cross-correlated) with the\n  `inputs` to produce a `Tensor` of activations. If a `normalizer_fn` is\n  provided (such as `batch_norm`), it is then applied. Otherwise, if\n  `normalizer_fn` is None and a `biases_initializer` is provided then a `biases`\n  variable would be created and added the activations. Finally, if\n  `activation_fn` is not `None`, it is applied to the activations as well.\n\n  Performs atrous convolution with input stride/dilation rate equal to `rate`\n  if a value > 1 for any dimension of `rate` is specified.  In this case\n  `stride` values != 1 are not supported.\n\n  Args:\n    inputs: A Tensor of rank N+2 of shape `[batch_size] + input_spatial_shape +\n      [in_channels]` if data_format does not start with ""NC"" (default), or\n      `[batch_size, in_channels] + input_spatial_shape` if data_format starts\n      with ""NC"".\n    num_outputs: Integer, the number of output filters.\n    kernel_size: A sequence of N positive integers specifying the spatial\n      dimensions of the filters.  Can be a single integer to specify the same\n      value for all spatial dimensions.\n    stride: A sequence of N positive integers specifying the stride at which to\n      compute output.  Can be a single integer to specify the same value for all\n      spatial dimensions.  Specifying any `stride` value != 1 is incompatible\n      with specifying any `rate` value != 1.\n    padding: One of `""VALID""` or `""SAME""`.\n    data_format: A string or None.  Specifies whether the channel dimension of\n      the `input` and output is the last dimension (default, or if `data_format`\n      does not start with ""NC""), or the second dimension (if `data_format`\n      starts with ""NC"").  For N=1, the valid values are ""NWC"" (default) and\n      ""NCW"".  For N=2, the valid values are ""NHWC"" (default) and ""NCHW"". For\n      N=3, the valid values are ""NDHWC"" (default) and ""NCDHW"".\n    rate: A sequence of N positive integers specifying the dilation rate to use\n      for atrous convolution.  Can be a single integer to specify the same value\n      for all spatial dimensions.  Specifying any `rate` value != 1 is\n      incompatible with specifying any `stride` value != 1.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collection per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n    conv_dims: Optional convolution dimensionality, when set it would use the\n      corresponding convolution (e.g. 2 for Conv 2D, 3 for Conv 3D, ..). When\n      leaved to None it would select the convolution dimensionality based on the\n      input rank (i.e. Conv ND, with N = input_rank - 2).\n\n  Returns:\n    A tensor representing the output of the operation.\n\n  Raises:\n    ValueError: If `data_format` is invalid.\n    ValueError: Both \'rate\' and `stride` are not uniformly 1.\n  """"""\n  if data_format not in [None, \'NWC\', \'NCW\', \'NHWC\', \'NCHW\', \'NDHWC\', \'NCDHW\']:\n    raise ValueError(\'Invalid data_format: %r\' % (data_format,))\n\n  layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'kernel\': \'weights\'\n  })\n\n  with variable_scope.variable_scope(\n      scope, \'Conv\', [inputs], reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    input_rank = inputs.get_shape().ndims\n\n    if conv_dims is not None and conv_dims + 2 != input_rank:\n      raise ValueError(\'Convolution expects input with rank %d, got %d\' %\n                       (conv_dims + 2, input_rank))\n    if input_rank == 3:\n      layer_class = convolutional_layers.Convolution1D\n    elif input_rank == 4:\n      layer_class = convolutional_layers.Convolution2D\n    elif input_rank == 5:\n      layer_class = convolutional_layers.Convolution3D\n    else:\n      raise ValueError(\'Convolution not supported for input with rank\',\n                       input_rank)\n\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = layer_class(\n        filters=num_outputs,\n        kernel_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        dilation_rate=rate,\n        activation=None,\n        use_bias=not normalizer_fn and biases_initializer,\n        kernel_initializer=weights_initializer,\n        bias_initializer=biases_initializer,\n        kernel_regularizer=weights_regularizer,\n        bias_regularizer=biases_regularizer,\n        activity_regularizer=None,\n        trainable=trainable,\n        name=sc.name,\n        dtype=inputs.dtype.base_dtype,\n        _scope=sc,\n        _reuse=reuse)\n    outputs = layer.apply(inputs)\n\n    # Add variables to collections.\n    _add_variable_to_collections(layer.kernel, variables_collections, \'weights\')\n    if layer.use_bias:\n      _add_variable_to_collections(layer.bias, variables_collections, \'biases\')\n\n    if normalizer_fn is not None:\n      normalizer_params = normalizer_params or {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef convolution1d(inputs,\n                  num_outputs,\n                  kernel_size,\n                  stride=1,\n                  padding=\'SAME\',\n                  data_format=None,\n                  rate=1,\n                  activation_fn=nn.relu,\n                  normalizer_fn=None,\n                  normalizer_params=None,\n                  weights_initializer=initializers.xavier_initializer(),\n                  weights_regularizer=None,\n                  biases_initializer=init_ops.zeros_initializer(),\n                  biases_regularizer=None,\n                  reuse=None,\n                  variables_collections=None,\n                  outputs_collections=None,\n                  trainable=True,\n                  scope=None):\n  return convolution(\n      inputs,\n      num_outputs,\n      kernel_size,\n      stride,\n      padding,\n      data_format,\n      rate,\n      activation_fn,\n      normalizer_fn,\n      normalizer_params,\n      weights_initializer,\n      weights_regularizer,\n      biases_initializer,\n      biases_regularizer,\n      reuse,\n      variables_collections,\n      outputs_collections,\n      trainable,\n      scope,\n      conv_dims=1)\n\n\nconvolution1d.__doc__ = convolution.__doc__\n\n\n@add_arg_scope\ndef convolution2d(inputs,\n                  num_outputs,\n                  kernel_size,\n                  stride=1,\n                  padding=\'SAME\',\n                  data_format=None,\n                  rate=1,\n                  activation_fn=nn.relu,\n                  normalizer_fn=None,\n                  normalizer_params=None,\n                  weights_initializer=initializers.xavier_initializer(),\n                  weights_regularizer=None,\n                  biases_initializer=init_ops.zeros_initializer(),\n                  biases_regularizer=None,\n                  reuse=None,\n                  variables_collections=None,\n                  outputs_collections=None,\n                  trainable=True,\n                  scope=None):\n  return convolution(\n      inputs,\n      num_outputs,\n      kernel_size,\n      stride,\n      padding,\n      data_format,\n      rate,\n      activation_fn,\n      normalizer_fn,\n      normalizer_params,\n      weights_initializer,\n      weights_regularizer,\n      biases_initializer,\n      biases_regularizer,\n      reuse,\n      variables_collections,\n      outputs_collections,\n      trainable,\n      scope,\n      conv_dims=2)\n\n\nconvolution2d.__doc__ = convolution.__doc__\n\n\n@add_arg_scope\ndef convolution3d(inputs,\n                  num_outputs,\n                  kernel_size,\n                  stride=1,\n                  padding=\'SAME\',\n                  data_format=None,\n                  rate=1,\n                  activation_fn=nn.relu,\n                  normalizer_fn=None,\n                  normalizer_params=None,\n                  weights_initializer=initializers.xavier_initializer(),\n                  weights_regularizer=None,\n                  biases_initializer=init_ops.zeros_initializer(),\n                  biases_regularizer=None,\n                  reuse=None,\n                  variables_collections=None,\n                  outputs_collections=None,\n                  trainable=True,\n                  scope=None):\n  return convolution(\n      inputs,\n      num_outputs,\n      kernel_size,\n      stride,\n      padding,\n      data_format,\n      rate,\n      activation_fn,\n      normalizer_fn,\n      normalizer_params,\n      weights_initializer,\n      weights_regularizer,\n      biases_initializer,\n      biases_regularizer,\n      reuse,\n      variables_collections,\n      outputs_collections,\n      trainable,\n      scope,\n      conv_dims=3)\n\n\nconvolution3d.__doc__ = convolution.__doc__\n\n\n@add_arg_scope\ndef convolution2d_in_plane(\n    inputs,\n    kernel_size,\n    stride=1,\n    padding=\'SAME\',\n    activation_fn=nn.relu,\n    normalizer_fn=None,\n    normalizer_params=None,\n    weights_initializer=initializers.xavier_initializer(),\n    weights_regularizer=None,\n    biases_initializer=init_ops.zeros_initializer(),\n    biases_regularizer=None,\n    reuse=None,\n    variables_collections=None,\n    outputs_collections=None,\n    trainable=True,\n    scope=None):\n  """"""Performs the same in-plane convolution to each channel independently.\n\n  This is useful for performing various simple channel-independent convolution\n  operations such as image gradients:\n\n    image = tf.constant(..., shape=(16, 240, 320, 3))\n    vert_gradients = layers.conv2d_in_plane(image,\n                                            kernel=[1, -1],\n                                            kernel_size=[2, 1])\n    horz_gradients = layers.conv2d_in_plane(image,\n                                            kernel=[1, -1],\n                                            kernel_size=[1, 2])\n\n  Args:\n    inputs: A 4-D tensor with dimensions [batch_size, height, width, channels].\n    kernel_size: A list of length 2 holding the [kernel_height, kernel_width] of\n      of the pooling. Can be an int if both values are the same.\n    stride: A list of length 2 `[stride_height, stride_width]`. Can be an int if\n      both strides are the same. Note that presently both strides must have the\n      same value.\n    padding: The padding type to use, either \'SAME\' or \'VALID\'.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collection per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n  """"""\n  with variable_scope.variable_scope(\n      scope, \'ConvInPlane\', [inputs], reuse=reuse) as sc:\n    dtype = inputs.dtype.base_dtype\n    kernel_h, kernel_w = utils.two_element_tuple(kernel_size)\n    stride_h, stride_w = utils.two_element_tuple(stride)\n    num_filters_in = utils.last_dimension(inputs.get_shape(), min_rank=4)\n    weights_shape = [kernel_h, kernel_w, 1, 1]\n    weights_collections = utils.get_variable_collections(\n        variables_collections, \'weights\')\n    weights = variables.model_variable(\n        \'weights\',\n        shape=weights_shape,\n        dtype=dtype,\n        initializer=weights_initializer,\n        regularizer=weights_regularizer,\n        collections=weights_collections,\n        trainable=trainable)\n    depthwise_weights = array_ops.tile(weights, [1, 1, num_filters_in, 1])\n    outputs = nn.depthwise_conv2d(inputs, depthwise_weights,\n                                  [1, stride_h, stride_w, 1], padding)\n    if normalizer_fn is not None:\n      normalizer_params = normalizer_params or {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n    else:\n      if biases_initializer is not None:\n        biases_collections = utils.get_variable_collections(\n            variables_collections, \'biases\')\n        biases = variables.model_variable(\n            \'biases\',\n            shape=[\n                num_filters_in,\n            ],\n            dtype=dtype,\n            initializer=biases_initializer,\n            regularizer=biases_regularizer,\n            collections=biases_collections,\n            trainable=trainable)\n        outputs = nn.bias_add(outputs, biases)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef convolution2d_transpose(\n    inputs,\n    num_outputs,\n    kernel_size,\n    stride=1,\n    padding=\'SAME\',\n    data_format=DATA_FORMAT_NHWC,\n    activation_fn=nn.relu,\n    normalizer_fn=None,\n    normalizer_params=None,\n    weights_initializer=initializers.xavier_initializer(),\n    weights_regularizer=None,\n    biases_initializer=init_ops.zeros_initializer(),\n    biases_regularizer=None,\n    reuse=None,\n    variables_collections=None,\n    outputs_collections=None,\n    trainable=True,\n    scope=None):\n  """"""Adds a convolution2d_transpose with an optional batch normalization layer.\n\n  The function creates a variable called `weights`, representing the\n  kernel, that is convolved with the input. If `normalizer_fn` is `None`, a\n  second variable called \'biases\' is added to the result of the operation.\n\n  Args:\n    inputs: A 4-D `Tensor` of type `float` and shape `[batch, height, width,\n      in_channels]` for `NHWC` data format or `[batch, in_channels, height,\n      width]` for `NCHW` data format.\n    num_outputs: Integer, the number of output filters.\n    kernel_size: A list of length 2 holding the [kernel_height, kernel_width] of\n      of the filters. Can be an int if both values are the same.\n    stride: A list of length 2: [stride_height, stride_width]. Can be an int if\n      both strides are the same.  Note that presently both strides must have the\n      same value.\n    padding: One of \'VALID\' or \'SAME\'.\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collection per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: Whether or not the variables should be trainable or not.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A tensor representing the output of the operation.\n\n  Raises:\n    ValueError: If \'kernel_size\' is not a list of length 2.\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If `C` dimension of `inputs` is None.\n  """"""\n  layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'kernel\': \'weights\'\n  })\n\n  with variable_scope.variable_scope(\n      scope,\n      \'Conv2d_transpose\', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n      raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n\n    inputs = ops.convert_to_tensor(inputs)\n\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = convolutional_layers.Convolution2DTranspose(\n        filters=num_outputs,\n        kernel_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        activation=None,\n        use_bias=not normalizer_fn and biases_initializer,\n        kernel_initializer=weights_initializer,\n        bias_initializer=biases_initializer,\n        kernel_regularizer=weights_regularizer,\n        bias_regularizer=biases_regularizer,\n        activity_regularizer=None,\n        trainable=trainable,\n        name=sc.name,\n        dtype=inputs.dtype.base_dtype,\n        _scope=sc,\n        _reuse=reuse)\n    outputs = layer.apply(inputs)\n\n    # Add variables to collections.\n    _add_variable_to_collections(layer.kernel, variables_collections, \'weights\')\n    if layer.bias is not None:\n      _add_variable_to_collections(layer.bias, variables_collections, \'biases\')\n\n    if normalizer_fn is not None:\n      normalizer_params = normalizer_params or {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef convolution3d_transpose(\n    inputs,\n    num_outputs,\n    kernel_size,\n    stride=1,\n    padding=\'SAME\',\n    data_format=DATA_FORMAT_NDHWC,\n    activation_fn=nn.relu,\n    normalizer_fn=None,\n    normalizer_params=None,\n    weights_initializer=initializers.xavier_initializer(),\n    weights_regularizer=None,\n    biases_initializer=init_ops.zeros_initializer(),\n    biases_regularizer=None,\n    reuse=None,\n    variables_collections=None,\n    outputs_collections=None,\n    trainable=True,\n    scope=None):\n  """"""Adds a convolution3d_transpose with an optional batch normalization layer.\n\n  The function creates a variable called `weights`, representing the\n  kernel, that is convolved with the input. If `batch_norm_params` is `None`, a\n  second variable called \'biases\' is added to the result of the operation.\n  Args:\n    inputs: A 5-D `Tensor` of type `float` and shape `[batch, depth, height,\n      width, in_channels]` for `NDHWC` data format or `[batch, in_channels,\n      depth, height, width]` for `NCDHW` data format.\n    num_outputs: Integer, the number of output filters.\n    kernel_size: A list of length 3 holding the [kernel_depth, kernel_height,\n      kernel_width] of the filters. Can be an int if both values are the same.\n    stride: A list of length 3: [stride_depth, stride_height, stride_width]. Can\n      be an int if both strides are the same.  Note that presently both strides\n      must have the same value.\n    padding: One of \'VALID\' or \'SAME\'.\n    data_format: A string. `NDHWC` (default) and `NCDHW` are supported.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collection per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: Whether or not the variables should be trainable or not.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A tensor representing the output of the operation.\n  Raises:\n    ValueError: If \'kernel_size\' is not a list of length 3.\n    ValueError: If `data_format` is neither `NDHWC` nor `NCDHW`.\n    ValueError: If `C` dimension of `inputs` is None.\n  """"""\n  layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'kernel\': \'weights\'\n  })\n\n  with variable_scope.variable_scope(\n      scope,\n      \'Conv3d_transpose\', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    if data_format not in (DATA_FORMAT_NCDHW, DATA_FORMAT_NDHWC):\n      raise ValueError(\'data_format has to be either NCDHW or NDHWC.\')\n\n    inputs = ops.convert_to_tensor(inputs)\n\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = convolutional_layers.Convolution3DTranspose(\n        filters=num_outputs,\n        kernel_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        activation=None,\n        use_bias=not normalizer_fn and biases_initializer,\n        kernel_initializer=weights_initializer,\n        bias_initializer=biases_initializer,\n        kernel_regularizer=weights_regularizer,\n        bias_regularizer=biases_regularizer,\n        activity_regularizer=None,\n        trainable=trainable,\n        name=sc.name,\n        dtype=inputs.dtype.base_dtype,\n        _scope=sc,\n        _reuse=reuse)\n    outputs = layer.apply(inputs)\n\n    # Add variables to collections.\n    _add_variable_to_collections(layer.kernel, variables_collections, \'weights\')\n    if layer.bias is not None:\n      _add_variable_to_collections(layer.bias, variables_collections, \'biases\')\n\n    if normalizer_fn is not None:\n      normalizer_params = normalizer_params or {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef dense_to_sparse(tensor, eos_token=0, outputs_collections=None, scope=None):\n  """"""Converts a dense tensor into a sparse tensor.\n\n  An example use would be to convert dense labels to sparse ones\n  so that they can be fed to the ctc_loss.\n\n  Args:\n     tensor: An `int` `Tensor` to be converted to a `Sparse`.\n     eos_token: An integer. It is part of the target label that signifies the\n       end of a sentence.\n     outputs_collections: Collection to add the outputs.\n     scope: Optional scope for name_scope.\n  """"""\n  with variable_scope.variable_scope(scope, \'dense_to_sparse\', [tensor]) as sc:\n    tensor = ops.convert_to_tensor(tensor)\n    indices = array_ops.where(\n        math_ops.not_equal(tensor, constant_op.constant(eos_token,\n                                                        tensor.dtype)))\n    values = array_ops.gather_nd(tensor, indices)\n    shape = array_ops.shape(tensor, out_type=dtypes.int64)\n    outputs = sparse_tensor.SparseTensor(indices, values, shape)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef dropout(inputs,\n            keep_prob=0.5,\n            noise_shape=None,\n            is_training=True,\n            outputs_collections=None,\n            scope=None,\n            seed=None):\n  """"""Returns a dropout op applied to the input.\n\n  With probability `keep_prob`, outputs the input element scaled up by\n  `1 / keep_prob`, otherwise outputs `0`.  The scaling is so that the expected\n  sum is unchanged.\n\n  Args:\n    inputs: The tensor to pass to the nn.dropout op.\n    keep_prob: A scalar `Tensor` with the same type as x. The probability that\n      each element is kept.\n    noise_shape: A 1-D `Tensor` of type `int32`, representing the shape for\n      randomly generated keep/drop flags.\n    is_training: A bool `Tensor` indicating whether or not the model is in\n      training mode. If so, dropout is applied and values scaled. Otherwise,\n      inputs is returned.\n    outputs_collections: Collection to add the outputs.\n    scope: Optional scope for name_scope.\n    seed: A Python integer. Used to create random seeds. See\n      `tf.compat.v1.set_random_seed` for behavior.\n\n  Returns:\n    A tensor representing the output of the operation.\n  """"""\n  with variable_scope.variable_scope(\n      scope, \'Dropout\', [inputs], custom_getter=_model_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    layer = core_layers.Dropout(\n        rate=1 - keep_prob,\n        noise_shape=noise_shape,\n        seed=seed,\n        name=sc.name,\n        _scope=sc)\n    outputs = layer.apply(inputs, training=is_training)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef flatten(inputs, outputs_collections=None, scope=None):\n  """"""Flattens the input while maintaining the batch_size.\n\n    Assumes that the first dimension represents the batch.\n\n  Args:\n    inputs: A tensor of size [batch_size, ...].\n    outputs_collections: Collection to add the outputs.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A flattened tensor with shape [batch_size, k].\n  Raises:\n    ValueError: If inputs rank is unknown or less than 2.\n  """"""\n  with ops.name_scope(scope, \'Flatten\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    outputs = core_layers.flatten(inputs)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\ndef _sparse_inner_flatten(inputs, new_rank):\n  """"""Helper function for `inner_flatten`.""""""\n  inputs_rank = inputs.dense_shape.get_shape().as_list()[0]\n  if inputs_rank < new_rank:\n    raise ValueError(\n        \'Inputs has rank less than new_rank. {} must have rank at least\'\n        \' {}. Received rank {}, shape {}\'.format(inputs, new_rank, inputs_rank,\n                                                 inputs.get_shape()))\n\n  outer_dimensions = inputs.dense_shape[:new_rank - 1]\n  inner_dimensions = inputs.dense_shape[new_rank - 1:]\n  new_shape = array_ops.concat(\n      (outer_dimensions, [math_ops.reduce_prod(inner_dimensions)]), 0)\n  flattened = sparse_ops.sparse_reshape(inputs, new_shape)\n  return flattened\n\n\ndef _dense_inner_flatten(inputs, new_rank):\n  """"""Helper function for `inner_flatten`.""""""\n  rank_assertion = check_ops.assert_rank_at_least(\n      inputs, new_rank, message=\'inputs has rank less than new_rank\')\n  with ops.control_dependencies([rank_assertion]):\n    outer_dimensions = array_ops.strided_slice(\n        array_ops.shape(inputs), [0], [new_rank - 1])\n    new_shape = array_ops.concat((outer_dimensions, [-1]), 0)\n    reshaped = array_ops.reshape(inputs, new_shape)\n\n  # if `new_rank` is an integer, try to calculate new shape.\n  if isinstance(new_rank, six.integer_types):\n    static_shape = inputs.get_shape()\n    if static_shape is not None and static_shape.dims is not None:\n      static_shape = static_shape.as_list()\n      static_outer_dims = static_shape[:new_rank - 1]\n      static_inner_dims = static_shape[new_rank - 1:]\n      flattened_dimension = 1\n      for inner_dim in static_inner_dims:\n        if inner_dim is None:\n          flattened_dimension = None\n          break\n        flattened_dimension *= inner_dim\n      reshaped.set_shape(static_outer_dims + [flattened_dimension])\n  return reshaped\n\n\n@add_arg_scope\ndef _inner_flatten(inputs, new_rank, output_collections=None, scope=None):\n  """"""Flattens inner dimensions of `inputs`, returns a Tensor with `new_rank`.\n\n  For example:\n  \'\'\'\n      x = tf.random.uniform(shape=[1, 2, 3, 4, 5, 6])\n      y = _inner_flatten(x, 4)\n      assert y.get_shape().as_list() == [1, 2, 3, (4 * 5 * 6)]\n  \'\'\'\n  This layer will fail at run time if `new_rank` is greater than the current\n  rank of `inputs`.\n\n  Args:\n    inputs: A `Tensor` or `SparseTensor`.\n    new_rank: The desired rank of the returned `Tensor` or `SparseTensor`.\n    output_collections: Collection to which the outputs will be added.\n    scope: Optional scope for `name_scope`.\n\n  Returns:\n    A `Tensor` or `SparseTensor` containing the same values as `inputs`, but\n    with innermost dimensions flattened to obtain rank `new_rank`.\n\n  Raises:\n    TypeError: `inputs` is not a `Tensor` or `SparseTensor`.\n  """"""\n  with ops.name_scope(scope, \'InnerFlatten\', [inputs, new_rank]) as sc:\n    if isinstance(inputs, sparse_tensor.SparseTensor):\n      flattened = _sparse_inner_flatten(inputs, new_rank)\n    else:\n      inputs = ops.convert_to_tensor(inputs)\n      flattened = _dense_inner_flatten(inputs, new_rank)\n  return utils.collect_named_outputs(output_collections, sc, flattened)\n\n\ndef _model_variable_getter(\n    getter,\n    name,\n    shape=None,\n    dtype=None,\n    initializer=None,\n    regularizer=None,\n    trainable=True,\n    collections=None,\n    caching_device=None,\n    partitioner=None,\n    rename=None,\n    use_resource=None,\n    synchronization=tf_variables.VariableSynchronization.AUTO,\n    aggregation=tf_variables.VariableAggregation.NONE,\n    **_):\n  """"""Getter that uses model_variable for compatibility with core layers.""""""\n  short_name = name.split(\'/\')[-1]\n  if rename and short_name in rename:\n    name_components = name.split(\'/\')\n    name_components[-1] = rename[short_name]\n    name = \'/\'.join(name_components)\n  return variables.model_variable(\n      name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      regularizer=regularizer,\n      collections=collections,\n      trainable=trainable,\n      caching_device=caching_device,\n      partitioner=partitioner,\n      custom_getter=getter,\n      use_resource=use_resource,\n      synchronization=synchronization,\n      aggregation=aggregation)\n\n\ndef _build_variable_getter(rename=None):\n  """"""Build a model variable getter that respects scope getter and renames.""""""\n\n  # VariableScope will nest the getters\n  def layer_variable_getter(getter, *args, **kwargs):\n    kwargs[\'rename\'] = rename\n    return _model_variable_getter(getter, *args, **kwargs)\n\n  return layer_variable_getter\n\n\ndef _add_variable_to_collections(variable, collections_set, collections_name):\n  """"""Adds variable (or all its parts) to all collections with that name.""""""\n  collections = utils.get_variable_collections(collections_set,\n                                               collections_name) or []\n  variables_list = [variable]\n  if isinstance(variable, tf_variables.PartitionedVariable):\n    variables_list = [v for v in variable]\n  for collection in collections:\n    for var in variables_list:\n      if var not in ops.get_collection(collection):\n        ops.add_to_collection(collection, var)\n\n\n@add_arg_scope\ndef fully_connected(inputs,\n                    num_outputs,\n                    activation_fn=nn.relu,\n                    normalizer_fn=None,\n                    normalizer_params=None,\n                    weights_initializer=initializers.xavier_initializer(),\n                    weights_regularizer=None,\n                    biases_initializer=init_ops.zeros_initializer(),\n                    biases_regularizer=None,\n                    reuse=None,\n                    variables_collections=None,\n                    outputs_collections=None,\n                    trainable=True,\n                    scope=None):\n  """"""Adds a fully connected layer.\n\n  `fully_connected` creates a variable called `weights`, representing a fully\n  connected weight matrix, which is multiplied by the `inputs` to produce a\n  `Tensor` of hidden units. If a `normalizer_fn` is provided (such as\n  `batch_norm`), it is then applied. Otherwise, if `normalizer_fn` is\n  None and a `biases_initializer` is provided then a `biases` variable would be\n  created and added the hidden units. Finally, if `activation_fn` is not `None`,\n  it is applied to the hidden units as well.\n\n  Note: that if `inputs` have a rank greater than 2, then `inputs` is flattened\n  prior to the initial matrix multiply by `weights`.\n\n  Args:\n    inputs: A tensor of at least rank 2 and static value for the last dimension;\n      i.e. `[batch_size, depth]`, `[None, None, None, channels]`.\n    num_outputs: Integer or long, the number of output units in the layer.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collections per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for variable_scope.\n\n  Returns:\n     The tensor variable representing the result of the series of operations.\n\n  Raises:\n    ValueError: If x has rank less than 2 or if its last dimension is not set.\n  """"""\n  if not isinstance(num_outputs, six.integer_types):\n    raise ValueError(\'num_outputs type should be one of %s, got %s.\' %\n                     (list(six.integer_types), type(num_outputs)))\n\n  layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'kernel\': \'weights\'\n  })\n\n  with variable_scope.variable_scope(\n      scope,\n      \'fully_connected\', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    layer = core_layers.Dense(\n        units=num_outputs,\n        activation=None,\n        use_bias=not normalizer_fn and biases_initializer,\n        kernel_initializer=weights_initializer,\n        bias_initializer=biases_initializer,\n        kernel_regularizer=weights_regularizer,\n        bias_regularizer=biases_regularizer,\n        activity_regularizer=None,\n        trainable=trainable,\n        name=sc.name,\n        dtype=inputs.dtype.base_dtype,\n        _scope=sc,\n        _reuse=reuse)\n    outputs = layer.apply(inputs)\n\n    # Add variables to collections.\n    _add_variable_to_collections(layer.kernel, variables_collections, \'weights\')\n    if layer.bias is not None:\n      _add_variable_to_collections(layer.bias, variables_collections, \'biases\')\n\n    # Apply normalizer function / layer.\n    if normalizer_fn is not None:\n      if not normalizer_params:\n        normalizer_params = {}\n      outputs = normalizer_fn(outputs, **normalizer_params)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\nclass GDN(base.Layer):\n  """"""Generalized divisive normalization layer.\n\n  Based on the papers:\n\n    ""Density Modeling of Images using a Generalized Normalization\n    Transformation""\n\n    Johannes Ball\xc3\xa9, Valero Laparra, Eero P. Simoncelli\n\n    https://arxiv.org/abs/1511.06281\n\n    ""End-to-end Optimized Image Compression""\n\n    Johannes Ball\xc3\xa9, Valero Laparra, Eero P. Simoncelli\n\n    https://arxiv.org/abs/1611.01704\n\n  Implements an activation function that is essentially a multivariate\n  generalization of a particular sigmoid-type function:\n\n  ```\n  y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]))\n  ```\n\n  where `i` and `j` run over channels. This implementation never sums across\n  spatial dimensions. It is similar to local response normalization, but much\n  more flexible, as `beta` and `gamma` are trainable parameters.\n\n  Arguments:\n    inverse: If `False` (default), compute GDN response. If `True`, compute IGDN\n      response (one step of fixed point iteration to invert GDN; the division is\n      replaced by multiplication).\n    beta_min: Lower bound for beta, to prevent numerical error from causing\n      square root of zero or negative values.\n    gamma_init: The gamma matrix will be initialized as the identity matrix\n      multiplied with this value. If set to zero, the layer is effectively\n      initialized to the identity operation, since beta is initialized as one. A\n      good default setting is somewhere between 0 and 0.5.\n    reparam_offset: Offset added to the reparameterization of beta and gamma.\n      The reparameterization of beta and gamma as their square roots lets the\n      training slow down when their values are close to zero, which is desirable\n      as small values in the denominator can lead to a situation where gradient\n      noise on beta/gamma leads to extreme amounts of noise in the GDN\n      activations. However, without the offset, we would get zero gradients if\n      any elements of beta or gamma were exactly zero, and thus the training\n      could get stuck. To prevent this, we add this small constant. The default\n      value was empirically determined as a good starting point. Making it\n      bigger potentially leads to more gradient noise on the activations, making\n      it too small may lead to numerical precision issues.\n    data_format: Format of input tensor. Currently supports `\'channels_first\'`\n      and `\'channels_last\'`.\n    activity_regularizer: Regularizer function for the output.\n    trainable: Boolean, if `True`, also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer. Layers with the same name will share\n      weights, but to avoid mistakes we require `reuse=True` in such cases.\n  Properties:\n    inverse: Boolean, whether GDN is computed (`True`) or IGDN (`False`).\n    data_format: Format of input tensor. Currently supports `\'channels_first\'`\n      and `\'channels_last\'`.\n    beta: The beta parameter as defined above (1D `Tensor`).\n    gamma: The gamma parameter as defined above (2D `Tensor`).\n  """"""\n\n  def __init__(self,\n               inverse=False,\n               beta_min=1e-6,\n               gamma_init=.1,\n               reparam_offset=2**-18,\n               data_format=\'channels_last\',\n               activity_regularizer=None,\n               trainable=True,\n               name=None,\n               **kwargs):\n    super(GDN, self).__init__(\n        trainable=trainable,\n        name=name,\n        activity_regularizer=activity_regularizer,\n        **kwargs)\n    self.inverse = inverse\n    self._beta_min = beta_min\n    self._gamma_init = gamma_init\n    self._reparam_offset = reparam_offset\n    self.data_format = data_format\n    self._channel_axis()  # trigger ValueError early\n    self.input_spec = input_spec.InputSpec(min_ndim=3, max_ndim=5)\n\n  def _channel_axis(self):\n    try:\n      return {\'channels_first\': 1, \'channels_last\': -1}[self.data_format]\n    except KeyError:\n      raise ValueError(\'Unsupported `data_format` for GDN layer: {}.\'.format(\n          self.data_format))\n\n  @staticmethod\n  def _lower_bound(inputs, bound, name=None):\n    """"""Same as tf.maximum, but with helpful gradient for inputs < bound.\n\n    The gradient is overwritten so that it is passed through if the input is not\n    hitting the bound. If it is, only gradients that push `inputs` higher than\n    the bound are passed through. No gradients are passed through to the bound.\n\n    Args:\n      inputs: input tensor\n      bound: lower bound for the input tensor\n      name: name for this op\n\n    Returns:\n      tf.maximum(inputs, bound)\n    """"""\n    with ops.name_scope(name, \'GDNLowerBound\', [inputs, bound]) as scope:\n      inputs = ops.convert_to_tensor(inputs, name=\'inputs\')\n      bound = ops.convert_to_tensor(bound, name=\'bound\')\n      with ops.get_default_graph().gradient_override_map(\n          {\'Maximum\': \'GDNLowerBound\'}):\n        return math_ops.maximum(inputs, bound, name=scope)\n\n  @staticmethod\n  def _lower_bound_grad(op, grad):\n    """"""Gradient for `_lower_bound`.\n\n    Args:\n      op: the tensorflow op for which to calculate a gradient\n      grad: gradient with respect to the output of the op\n\n    Returns:\n      gradients with respect to the inputs of the op\n    """"""\n    inputs = op.inputs[0]\n    bound = op.inputs[1]\n    pass_through_if = math_ops.logical_or(inputs >= bound, grad < 0)\n    return [math_ops.cast(pass_through_if, grad.dtype) * grad, None]\n\n  def build(self, input_shape):\n    channel_axis = self._channel_axis()\n    input_shape = tensor_shape.TensorShape(input_shape)\n    num_channels = input_shape.dims[channel_axis].value\n    if num_channels is None:\n      raise ValueError(\'The channel dimension of the inputs to `GDN` \'\n                       \'must be defined.\')\n    self._input_rank = input_shape.ndims\n    self.input_spec = input_spec.InputSpec(\n        ndim=input_shape.ndims, axes={channel_axis: num_channels})\n\n    pedestal = array_ops.constant(self._reparam_offset**2, dtype=self.dtype)\n    beta_bound = array_ops.constant(\n        (self._beta_min + self._reparam_offset**2)**.5, dtype=self.dtype)\n    gamma_bound = array_ops.constant(self._reparam_offset, dtype=self.dtype)\n\n    def beta_initializer(shape, dtype=None, partition_info=None):\n      del partition_info  # unused\n      pedestal = array_ops.constant(self._reparam_offset**2, dtype=self.dtype)\n      return math_ops.sqrt(array_ops.ones(shape, dtype=dtype) + pedestal)\n\n    def gamma_initializer(shape, dtype=None, partition_info=None):\n      del partition_info  # unused\n      assert len(shape) == 2\n      assert shape[0] == shape[1]\n      eye = linalg_ops.eye(shape[0], dtype=dtype)\n      pedestal = array_ops.constant(self._reparam_offset**2, dtype=self.dtype)\n      return math_ops.sqrt(self._gamma_init * eye + pedestal)\n\n    beta = self.add_variable(\n        \'reparam_beta\',\n        shape=[num_channels],\n        initializer=beta_initializer,\n        dtype=self.dtype,\n        trainable=True)\n    beta = self._lower_bound(beta, beta_bound)\n    self.beta = math_ops.square(beta) - pedestal\n\n    gamma = self.add_variable(\n        \'reparam_gamma\',\n        shape=[num_channels, num_channels],\n        initializer=gamma_initializer,\n        dtype=self.dtype,\n        trainable=True)\n    gamma = self._lower_bound(gamma, gamma_bound)\n    self.gamma = math_ops.square(gamma) - pedestal\n\n    self.built = True\n\n  def call(self, inputs):\n    inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n    ndim = self._input_rank\n\n    shape = self.gamma.get_shape().as_list()\n    gamma = array_ops.reshape(self.gamma, (ndim - 2) * [1] + shape)\n\n    # Compute normalization pool.\n    if self.data_format == \'channels_first\':\n      norm_pool = nn.convolution(\n          math_ops.square(inputs),\n          gamma,\n          \'VALID\',\n          data_format=\'NC\' + \'DHW\' [-(ndim - 2):])\n      if ndim == 3:\n        norm_pool = array_ops.expand_dims(norm_pool, 2)\n        norm_pool = nn.bias_add(norm_pool, self.beta, data_format=\'NCHW\')\n        norm_pool = array_ops.squeeze(norm_pool, [2])\n      elif ndim == 5:\n        shape = array_ops.shape(norm_pool)\n        norm_pool = array_ops.reshape(norm_pool, shape[:3] + [-1])\n        norm_pool = nn.bias_add(norm_pool, self.beta, data_format=\'NCHW\')\n        norm_pool = array_ops.reshape(norm_pool, shape)\n      else:  # ndim == 4\n        norm_pool = nn.bias_add(norm_pool, self.beta, data_format=\'NCHW\')\n    else:  # channels_last\n      norm_pool = nn.convolution(math_ops.square(inputs), gamma, \'VALID\')\n      norm_pool = nn.bias_add(norm_pool, self.beta, data_format=\'NHWC\')\n    norm_pool = math_ops.sqrt(norm_pool)\n\n    if self.inverse:\n      outputs = inputs * norm_pool\n    else:\n      outputs = inputs / norm_pool\n    outputs.set_shape(inputs.get_shape())\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    channel_axis = self._channel_axis()\n    input_shape = tensor_shape.TensorShape(input_shape)\n    if not 3 <= input_shape.ndim <= 5:\n      raise ValueError(\'`input_shape` must be of rank 3 to 5, inclusive.\')\n    if input_shape.dims[channel_axis].value is None:\n      raise ValueError(\n          \'The channel dimension of `input_shape` must be defined.\')\n    return input_shape\n\n\nops.RegisterGradient(\'GDNLowerBound\')(GDN._lower_bound_grad)  # pylint:disable=protected-access\n\n\ndef gdn(inputs,\n        inverse=False,\n        beta_min=1e-6,\n        gamma_init=.1,\n        reparam_offset=2**-18,\n        data_format=\'channels_last\',\n        activity_regularizer=None,\n        trainable=True,\n        name=None,\n        reuse=None):\n  """"""Functional interface for GDN layer.\n\n  Based on the papers:\n\n    ""Density Modeling of Images using a Generalized Normalization\n    Transformation""\n    Johannes Ball\xc3\xa9, Valero Laparra, Eero P. Simoncelli\n    https://arxiv.org/abs/1511.06281\n\n    ""End-to-end Optimized Image Compression""\n    Johannes Ball\xc3\xa9, Valero Laparra, Eero P. Simoncelli\n    https://arxiv.org/abs/1611.01704\n\n  Implements an activation function that is essentially a multivariate\n  generalization of a particular sigmoid-type function:\n\n  ```\n  y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]))\n  ```\n\n  where `i` and `j` run over channels. This implementation never sums across\n  spatial dimensions. It is similar to local response normalization, but much\n  more flexible, as `beta` and `gamma` are trainable parameters.\n\n  Args:\n    inputs: Tensor input.\n    inverse: If `False` (default), compute GDN response. If `True`, compute IGDN\n      response (one step of fixed point iteration to invert GDN; the division is\n      replaced by multiplication).\n    beta_min: Lower bound for beta, to prevent numerical error from causing\n      square root of zero or negative values.\n    gamma_init: The gamma matrix will be initialized as the identity matrix\n      multiplied with this value. If set to zero, the layer is effectively\n      initialized to the identity operation, since beta is initialized as one. A\n      good default setting is somewhere between 0 and 0.5.\n    reparam_offset: Offset added to the reparameterization of beta and gamma.\n      The reparameterization of beta and gamma as their square roots lets the\n      training slow down when their values are close to zero, which is desirable\n      as small values in the denominator can lead to a situation where gradient\n      noise on beta/gamma leads to extreme amounts of noise in the GDN\n      activations. However, without the offset, we would get zero gradients if\n      any elements of beta or gamma were exactly zero, and thus the training\n      could get stuck. To prevent this, we add this small constant. The default\n      value was empirically determined as a good starting point. Making it\n      bigger potentially leads to more gradient noise on the activations, making\n      it too small may lead to numerical precision issues.\n    data_format: Format of input tensor. Currently supports `\'channels_first\'`\n      and `\'channels_last\'`.\n    activity_regularizer: Regularizer function for the output.\n    trainable: Boolean, if `True`, also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    name: String, the name of the layer. Layers with the same name will share\n      weights, but to avoid mistakes we require `reuse=True` in such cases.\n    reuse: Boolean, whether to reuse the weights of a previous layer by the same\n      name.\n\n  Returns:\n    Output tensor.\n  """"""\n  layer = GDN(\n      inverse=inverse,\n      beta_min=beta_min,\n      gamma_init=gamma_init,\n      reparam_offset=reparam_offset,\n      data_format=data_format,\n      activity_regularizer=activity_regularizer,\n      trainable=trainable,\n      name=name,\n      dtype=inputs.dtype.base_dtype,\n      _scope=name,\n      _reuse=reuse)\n  return layer.apply(inputs)\n\n\n@add_arg_scope\ndef layer_norm(inputs,\n               center=True,\n               scale=True,\n               activation_fn=None,\n               reuse=None,\n               variables_collections=None,\n               outputs_collections=None,\n               trainable=True,\n               begin_norm_axis=1,\n               begin_params_axis=-1,\n               scope=None):\n  """"""Adds a Layer Normalization layer.\n\n  Based on the paper:\n\n    ""Layer Normalization""\n\n    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n\n    https://arxiv.org/abs/1607.06450.\n\n  Can be used as a normalizer function for conv2d and fully_connected.\n\n  Given a tensor `inputs` of rank `R`, moments are calculated and normalization\n  is performed over axes `begin_norm_axis ... R - 1`.  Scaling and centering,\n  if requested, is performed over axes `begin_params_axis .. R - 1`.\n\n  By default, `begin_norm_axis = 1` and `begin_params_axis = -1`,\n  meaning that normalization is performed over all but the first axis\n  (the `HWC` if `inputs` is `NHWC`), while the `beta` and `gamma` trainable\n  parameters are calculated for the rightmost axis (the `C` if `inputs` is\n  `NHWC`).  Scaling and recentering is performed via broadcast of the\n  `beta` and `gamma` parameters with the normalized tensor.\n\n  The shapes of `beta` and `gamma` are `inputs.shape[begin_params_axis:]`,\n  and this part of the inputs\' shape must be fully defined.\n\n  Args:\n    inputs: A tensor having rank `R`. The normalization is performed over axes\n      `begin_norm_axis ... R - 1` and centering and scaling parameters are\n      calculated over `begin_params_axis ... R - 1`.\n    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n      is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n      scaling can be done by the next layer.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    begin_norm_axis: The first normalization dimension: normalization will be\n      performed along dimensions `begin_norm_axis : rank(inputs)`\n    begin_params_axis: The first parameter (beta, gamma) dimension: scale and\n      centering parameters will have dimensions\n      `begin_params_axis : rank(inputs)` and will be broadcast with the\n        normalized inputs accordingly.\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation, having the same\n    shape and dtype as `inputs`.\n\n  Raises:\n    ValueError: If the rank of `inputs` is not known at graph build time,\n      or if `inputs.shape[begin_params_axis:]` is not fully defined at\n      graph build time.\n  """"""\n  with variable_scope.variable_scope(\n      scope, \'LayerNorm\', [inputs], reuse=reuse) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    inputs_shape = inputs.shape\n    inputs_rank = inputs_shape.ndims\n    if inputs_rank is None:\n      raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n    dtype = inputs.dtype.base_dtype\n    if begin_norm_axis < 0:\n      begin_norm_axis = inputs_rank + begin_norm_axis\n    if begin_params_axis >= inputs_rank or begin_norm_axis >= inputs_rank:\n      raise ValueError(\'begin_params_axis (%d) and begin_norm_axis (%d) \'\n                       \'must be < rank(inputs) (%d)\' %\n                       (begin_params_axis, begin_norm_axis, inputs_rank))\n    params_shape = inputs_shape[begin_params_axis:]\n    if not params_shape.is_fully_defined():\n      raise ValueError(\n          \'Inputs %s: shape(inputs)[%s:] is not fully defined: %s\' %\n          (inputs.name, begin_params_axis, inputs_shape))\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta_collections = utils.get_variable_collections(variables_collections,\n                                                        \'beta\')\n      beta = variables.model_variable(\n          \'beta\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=init_ops.zeros_initializer(),\n          collections=beta_collections,\n          trainable=trainable)\n    if scale:\n      gamma_collections = utils.get_variable_collections(\n          variables_collections, \'gamma\')\n      gamma = variables.model_variable(\n          \'gamma\',\n          shape=params_shape,\n          dtype=dtype,\n          initializer=init_ops.ones_initializer(),\n          collections=gamma_collections,\n          trainable=trainable)\n    # By default, compute the moments across all the dimensions except the one with index 0.\n    norm_axes = list(range(begin_norm_axis, inputs_rank))\n    mean, variance = nn.moments(inputs, norm_axes, keep_dims=True)\n    # Compute layer normalization using the batch_normalization function.\n    # Note that epsilon must be increased for float16 due to the limited\n    # representable range.\n    variance_epsilon = 1e-12 if dtype != dtypes.float16 else 1e-3\n    outputs = nn.batch_normalization(\n        inputs,\n        mean,\n        variance,\n        offset=beta,\n        scale=gamma,\n        variance_epsilon=variance_epsilon)\n    outputs.set_shape(inputs_shape)\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef images_to_sequence(inputs,\n                       data_format=DATA_FORMAT_NHWC,\n                       outputs_collections=None,\n                       scope=None):\n  """"""Convert a batch of images into a batch of sequences.\n\n  Args:\n    inputs: a (num_images, height, width, depth) tensor\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Raises:\n     ValueError: If `data_format` is not either NCHW or NHWC.\n\n  Returns:\n    (width, num_images*height, depth) sequence tensor\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  with ops.name_scope(scope, \'ImagesToSequence\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    if df == \'channels_first\':\n      inputs = array_ops.transpose(inputs, [0, 2, 3, 1])\n    _, _, width, depth = inputs.get_shape().as_list()\n    s = array_ops.shape(inputs)\n    batch_size, height = s[0], s[1]\n    transposed = array_ops.transpose(inputs, [2, 0, 1, 3])\n    outputs = array_ops.reshape(transposed, [width, batch_size * height, depth])\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\n@add_arg_scope\ndef max_pool2d(inputs,\n               kernel_size,\n               stride=2,\n               padding=\'VALID\',\n               data_format=DATA_FORMAT_NHWC,\n               outputs_collections=None,\n               scope=None):\n  """"""Adds a 2D Max Pooling op.\n\n  It is assumed that the pooling is done per image but not in batch or channels.\n\n  Args:\n    inputs: A 4-D tensor of shape `[batch_size, height, width, channels]` if\n      `data_format` is `NHWC`, and `[batch_size, channels, height, width]` if\n      `data_format` is `NCHW`.\n    kernel_size: A list of length 2: [kernel_height, kernel_width] of the\n      pooling kernel over which the op is computed. Can be an int if both values\n      are the same.\n    stride: A list of length 2: [stride_height, stride_width]. Can be an int if\n      both strides are the same. Note that presently both strides must have the\n      same value.\n    padding: The padding method, either \'VALID\' or \'SAME\'.\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If \'kernel_size\' is not a 2-D list\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  with ops.name_scope(scope, \'MaxPool2D\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = pooling_layers.MaxPooling2D(\n        pool_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        _scope=sc)\n    outputs = layer.apply(inputs)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\n@add_arg_scope\ndef max_pool3d(inputs,\n               kernel_size,\n               stride=2,\n               padding=\'VALID\',\n               data_format=DATA_FORMAT_NDHWC,\n               outputs_collections=None,\n               scope=None):\n  """"""Adds a 3D Max Pooling op.\n\n  It is assumed that the pooling is done per image but not in batch or channels.\n\n  Args:\n    inputs: A 5-D tensor of shape `[batch_size, depth, height, width, channels]`\n      if `data_format` is `NDHWC`, and `[batch_size, channels, depth, height,\n      width]` if `data_format` is `NCDHW`.\n    kernel_size: A list of length 3: [kernel_depth, kernel_height, kernel_width]\n      of the pooling kernel over which the op is computed. Can be an int if both\n      values are the same.\n    stride: A list of length 3: [stride_depth, stride_height, stride_width]. Can\n      be an int if both strides are the same. Note that presently both strides\n      must have the same value.\n    padding: The padding method, either \'VALID\' or \'SAME\'.\n    data_format: A string. `NDHWC` (default) and `NCDHW` are supported.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NDHWC` nor `NCDHW`.\n    ValueError: If \'kernel_size\' is not a 3-D list\n  """"""\n  if data_format not in (DATA_FORMAT_NCDHW, DATA_FORMAT_NDHWC):\n    raise ValueError(\'data_format has to be either NCDHW or NDHWC.\')\n  with ops.name_scope(scope, \'MaxPool3D\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    layer = pooling_layers.MaxPooling3D(\n        pool_size=kernel_size,\n        strides=stride,\n        padding=padding,\n        data_format=df,\n        _scope=sc)\n    outputs = layer.apply(inputs)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\n@add_arg_scope\ndef pool(inputs,\n         kernel_size,\n         pooling_type,\n         padding=\'VALID\',\n         data_format=None,\n         dilation_rate=1,\n         stride=1,\n         outputs_collections=None,\n         scope=None):\n  # pylint: disable=line-too-long\n  """"""Adds a pooling op.\n\n\n  Args:\n    inputs: Tensor of rank N+2, of shape `[batch_size] + input_spatial_shape +\n      [num_channels]` if data_format does not start with ""NC"" (default), or\n      `[batch_size, num_channels] + input_spatial_shape` if data_format starts\n      with ""NC"".  Pooling happens over the spatial dimensions only.\n    kernel_size: Sequence of N ints >= 1.  Can also be a single integer to\n      specify the same value for all spatial dimensions.\n    pooling_type: Specifies pooling operation, must be ""AVG"" or ""MAX"".\n    padding: The padding algorithm, must be ""SAME"" or ""VALID"".\n    data_format: A string or None.  Specifies whether the channel dimension of\n      the `input` and output is the last dimension (default, or if `data_format`\n      does not start with ""NC""), or the second dimension (if `data_format`\n      starts with ""NC"").  For N=1, the valid values are ""NWC"" (default) and\n      ""NCW"".  For N=2, the valid values are ""NHWC"" (default) and ""NCHW"". For\n      N=3, the valid values are ""NDHWC"" (default) and ""NCDHW"".\n    dilation_rate: Optional.  Dilation rate.  Sequence of N ints >= 1.  Defaults\n      to [1]*N.  Can also be a single integer to specify the same value for all\n      spatial dimensions.  If any value of dilation_rate is > 1, then all values\n      of stride must be 1.\n    stride: Optional.  Sequence of N ints >= 1.  Defaults to [1]*N.  Can also be\n      a single integer to specify the same value for all spatial dimensions.  If\n      any value of stride is > 1, then all values of dilation_rate must be 1.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: If arguments are invalid.\n\n  """"""\n  # pylint: enable=line-too-long\n  with ops.name_scope(scope, \'%s_pool\' % (pooling_type.lower()),\n                      [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    input_rank = inputs.get_shape().ndims\n    if input_rank is None:\n      raise ValueError(\'Rank of inputs must be known\')\n    if input_rank < 3:\n      raise ValueError(\'Rank of inputs must be >= 3\')\n    num_spatial_dims = input_rank - 2\n    output = nn.pool(\n        input=inputs,\n        window_shape=utils.n_positive_integers(num_spatial_dims, kernel_size),\n        pooling_type=pooling_type,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=utils.n_positive_integers(num_spatial_dims,\n                                                dilation_rate),\n        strides=utils.n_positive_integers(num_spatial_dims, stride),\n        name=sc)\n    return utils.collect_named_outputs(outputs_collections, sc, output)\n\n\n@add_arg_scope\ndef one_hot_encoding(labels,\n                     num_classes,\n                     on_value=1.0,\n                     off_value=0.0,\n                     outputs_collections=None,\n                     scope=None):\n  """"""Transform numeric labels into onehot_labels using `tf.one_hot`.\n\n  Args:\n    labels: [batch_size] target labels.\n    num_classes: Total number of classes.\n    on_value: A scalar defining the on-value.\n    off_value: A scalar defining the off-value.\n    outputs_collections: Collection to add the outputs.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    One-hot encoding of the labels.\n  """"""\n  with ops.name_scope(scope, \'OneHotEncoding\', [labels, num_classes]) as sc:\n    labels = ops.convert_to_tensor(labels)\n    if labels.dtype == dtypes.int32:\n      labels = standard_ops.to_int64(labels)\n    outputs = standard_ops.one_hot(\n        labels, num_classes, on_value=on_value, off_value=off_value)\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\ndef _apply_activation(y, activation_fn, output_collections):\n  if activation_fn is not None:\n    y = activation_fn(y)\n  ops.add_to_collections(\n      list(output_collections or []) + [ops.GraphKeys.ACTIVATIONS], y)\n  return y\n\n\ndef repeat(inputs, repetitions, layer, *args, **kwargs):\n  """"""Applies the same layer with the same arguments repeatedly.\n\n  ```python\n    y = repeat(x, 3, conv2d, 64, [3, 3], scope=\'conv1\')\n    # It is equivalent to:\n\n    x = conv2d(x, 64, [3, 3], scope=\'conv1/conv1_1\')\n    x = conv2d(x, 64, [3, 3], scope=\'conv1/conv1_2\')\n    y = conv2d(x, 64, [3, 3], scope=\'conv1/conv1_3\')\n  ```\n\n  If the `scope` argument is not given in `kwargs`, it is set to\n  `layer.__name__`, or `layer.func.__name__` (for `functools.partial`\n  objects). If neither `__name__` nor `func.__name__` is available, the\n  layers are called with `scope=\'stack\'`.\n\n  Args:\n    inputs: A `Tensor` suitable for layer.\n    repetitions: Int, number of repetitions.\n    layer: A layer with arguments `(inputs, *args, **kwargs)`\n    *args: Extra args for the layer.\n    **kwargs: Extra kwargs for the layer.\n\n  Returns:\n    A tensor result of applying the layer, repetitions times.\n  Raises:\n    ValueError: If the op is unknown or wrong.\n  """"""\n  scope = kwargs.pop(\'scope\', None)\n  with variable_scope.variable_scope(scope, \'Repeat\', [inputs]):\n    inputs = ops.convert_to_tensor(inputs)\n    if scope is None:\n      if hasattr(layer, \'__name__\'):\n        scope = layer.__name__\n      elif hasattr(layer, \'func\') and hasattr(layer.func, \'__name__\'):\n        scope = layer.func.__name__  # In case layer is a functools.partial.\n      else:\n        scope = \'repeat\'\n    outputs = inputs\n    for i in range(repetitions):\n      kwargs[\'scope\'] = scope + \'_\' + str(i + 1)\n      outputs = layer(outputs, *args, **kwargs)\n    return outputs\n\n\ndef _scale_gradient_shape(op):\n  """"""Shape helper function for scale_gradient function below.""""""\n  return [op.inputs[0].shape]\n\n\ndef _scale_gradient_grad(op, grad):\n  """"""Python gradient helper function for scale_gradient function below.""""""\n  return [grad * op.inputs[1], None]\n\n\n@function.Defun(\n    python_grad_func=_scale_gradient_grad, shape_func=_scale_gradient_shape)\ndef scale_gradient(inputs, gradient_multiplier):\n  """"""Identity operation, but with the gradient multiplied by a tensor.\n\n  The TensorFlow gradient system will compute the gradient with respect to\n  `inputs` as the product of the gradient with respect to the `output`\n  multiplied by a specified `gradient_multiplier` tensor.  If\n  `gradient_multiplier` is equal to 1, then this results in the true gradient.\n  Otherwise, it results in a scaled gradient.\n\n  This can be useful for adjusting the relative learning rate of different\n  parameter tensors when performing gradient descent, and because this rescaling\n  can be inserted at arbitrary locations within a graph, is often more\n  convenient to apply than simply rescaling the final computed gradients.\n\n  Args:\n    inputs: Tensor to be output.\n    gradient_multiplier: Tensor by which to multiply the gradient with respect\n      to `output` to compute the gradient with respect to `inputs`.  Its shape\n      must be broadcastable to the shape of `inputs`.\n\n  Returns:\n    output Tensor, equal to `inputs`.\n  """"""\n  # gradient_multiplier is implicitly saved by decorator, and only used for\n  # gradient computation.\n  del gradient_multiplier\n\n  return inputs\n\n\n@add_arg_scope\ndef separable_convolution2d(\n    inputs,\n    num_outputs,\n    kernel_size,\n    depth_multiplier=1,\n    stride=1,\n    padding=\'SAME\',\n    data_format=DATA_FORMAT_NHWC,\n    rate=1,\n    activation_fn=nn.relu,\n    normalizer_fn=None,\n    normalizer_params=None,\n    weights_initializer=initializers.xavier_initializer(),\n    pointwise_initializer=None,\n    weights_regularizer=None,\n    biases_initializer=init_ops.zeros_initializer(),\n    biases_regularizer=None,\n    reuse=None,\n    variables_collections=None,\n    outputs_collections=None,\n    trainable=True,\n    scope=None):\n  """"""Adds a depth-separable 2D convolution with optional batch_norm layer.\n\n  This op first performs a depthwise convolution that acts separately on\n  channels, creating a variable called `depthwise_weights`. If `num_outputs`\n  is not None, it adds a pointwise convolution that mixes channels, creating a\n  variable called `pointwise_weights`. Then, if `normalizer_fn` is None,\n  it adds bias to the result, creating a variable called \'biases\', otherwise,\n  the `normalizer_fn` is applied. It finally applies an activation function\n  to produce the end result.\n\n  Args:\n    inputs: A tensor of size [batch_size, height, width, channels].\n    num_outputs: The number of pointwise convolution output filters. If is None,\n      then we skip the pointwise convolution stage.\n    kernel_size: A list of length 2: [kernel_height, kernel_width] of of the\n      filters. Can be an int if both values are the same.\n    depth_multiplier: The number of depthwise convolution output channels for\n      each input channel. The total number of depthwise convolution output\n      channels will be equal to `num_filters_in * depth_multiplier`.\n    stride: A list of length 2: [stride_height, stride_width], specifying the\n      depthwise convolution stride. Can be an int if both strides are the same.\n    padding: One of \'VALID\' or \'SAME\'.\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    rate: A list of length 2: [rate_height, rate_width], specifying the dilation\n      rates for atrous convolution. Can be an int if both rates are the same. If\n      any value is larger than one, then both stride values need to be one.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the depthwise weights.\n    pointwise_initializer: An initializer for the pointwise weights. default set\n      to None, means use weights_initializer.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collection per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: Whether or not the variables should be trainable or not.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n  Raises:\n    ValueError: If `data_format` is invalid.\n  """"""\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n  layer_variable_getter = _build_variable_getter({\n      \'bias\': \'biases\',\n      \'depthwise_kernel\': \'depthwise_weights\',\n      \'pointwise_kernel\': \'pointwise_weights\'\n  })\n\n  with variable_scope.variable_scope(\n      scope,\n      \'SeparableConv2d\', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n\n    if pointwise_initializer is None:\n      pointwise_initializer = weights_initializer\n\n    df = (\'channels_first\'\n          if data_format and data_format.startswith(\'NC\') else \'channels_last\')\n    if num_outputs is not None:\n      # Apply separable conv using the SeparableConvolution2D layer.\n      layer = convolutional_layers.SeparableConvolution2D(\n          filters=num_outputs,\n          kernel_size=kernel_size,\n          strides=stride,\n          padding=padding,\n          data_format=df,\n          dilation_rate=utils.two_element_tuple(rate),\n          activation=None,\n          depth_multiplier=depth_multiplier,\n          use_bias=not normalizer_fn and biases_initializer,\n          depthwise_initializer=weights_initializer,\n          pointwise_initializer=pointwise_initializer,\n          bias_initializer=biases_initializer,\n          depthwise_regularizer=weights_regularizer,\n          pointwise_regularizer=weights_regularizer,\n          bias_regularizer=biases_regularizer,\n          activity_regularizer=None,\n          trainable=trainable,\n          name=sc.name,\n          dtype=inputs.dtype.base_dtype,\n          _scope=sc,\n          _reuse=reuse)\n      outputs = layer.apply(inputs)\n\n      # Add variables to collections.\n      _add_variable_to_collections(layer.depthwise_kernel,\n                                   variables_collections, \'weights\')\n      _add_variable_to_collections(layer.pointwise_kernel,\n                                   variables_collections, \'weights\')\n      if layer.bias is not None:\n        _add_variable_to_collections(layer.bias, variables_collections,\n                                     \'biases\')\n\n      if normalizer_fn is not None:\n        normalizer_params = normalizer_params or {}\n        outputs = normalizer_fn(outputs, **normalizer_params)\n    else:\n      # Actually apply depthwise conv instead of separable conv.\n      dtype = inputs.dtype.base_dtype\n      kernel_h, kernel_w = utils.two_element_tuple(kernel_size)\n      stride_h, stride_w = utils.two_element_tuple(stride)\n      num_filters_in = utils.channel_dimension(\n          inputs.get_shape(), df, min_rank=4)\n      weights_collections = utils.get_variable_collections(\n          variables_collections, \'weights\')\n\n      depthwise_shape = [kernel_h, kernel_w, num_filters_in, depth_multiplier]\n      depthwise_weights = variables.model_variable(\n          \'depthwise_weights\',\n          shape=depthwise_shape,\n          dtype=dtype,\n          initializer=weights_initializer,\n          regularizer=weights_regularizer,\n          trainable=trainable,\n          collections=weights_collections)\n      strides = [\n          1, 1, stride_h, stride_w\n      ] if data_format.startswith(\'NC\') else [1, stride_h, stride_w, 1]\n\n      outputs = nn.depthwise_conv2d(\n          inputs,\n          depthwise_weights,\n          strides,\n          padding,\n          rate=utils.two_element_tuple(rate),\n          data_format=data_format)\n      num_outputs = depth_multiplier * num_filters_in\n\n      if normalizer_fn is not None:\n        normalizer_params = normalizer_params or {}\n        outputs = normalizer_fn(outputs, **normalizer_params)\n      else:\n        if biases_initializer is not None:\n          biases_collections = utils.get_variable_collections(\n              variables_collections, \'biases\')\n          biases = variables.model_variable(\n              \'biases\',\n              shape=[\n                  num_outputs,\n              ],\n              dtype=dtype,\n              initializer=biases_initializer,\n              regularizer=biases_regularizer,\n              trainable=trainable,\n              collections=biases_collections)\n          outputs = nn.bias_add(outputs, biases, data_format=data_format)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef sequence_to_images(inputs,\n                       height,\n                       output_data_format=\'channels_last\',\n                       outputs_collections=None,\n                       scope=None):\n  """"""Convert a batch of sequences into a batch of images.\n\n  Args:\n    inputs: (num_steps, num_batches, depth) sequence tensor\n    height: the height of the images\n    output_data_format: Format of output tensor. Currently supports\n      `\'channels_first\'` and `\'channels_last\'`.\n    outputs_collections: The collections to which the outputs are added.\n    scope: Optional scope for name_scope.\n\n  Returns:\n    A tensor representing the output of the operation.\n  """"""\n  with ops.name_scope(scope, \'SequenceToImages\', [inputs]) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    width, num_batches, depth = inputs.get_shape().as_list()\n    if num_batches is None:\n      num_batches = -1\n    else:\n      num_batches //= height\n    reshaped = array_ops.reshape(inputs, [width, num_batches, height, depth])\n    if output_data_format == \'channels_first\':\n      outputs = array_ops.transpose(reshaped, [1, 3, 2, 0])\n    else:\n      outputs = array_ops.transpose(reshaped, [1, 2, 0, 3])\n    return utils.collect_named_outputs(outputs_collections, sc, outputs)\n\n\n@add_arg_scope\ndef softmax(logits, scope=None):\n  """"""Performs softmax on Nth dimension of N-dimensional logit tensor.\n\n  For two-dimensional logits this reduces to tf.nn.softmax. The N-th dimension\n  needs to have a specified number of elements (number of classes).\n\n  Args:\n    logits: N-dimensional `Tensor` with logits, where N > 1.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A `Tensor` with same shape and type as logits.\n  """"""\n  # TODO(jrru): Add axis argument which defaults to last dimension.\n  with variable_scope.variable_scope(scope, \'softmax\', [logits]):\n    num_logits = utils.last_dimension(logits.get_shape(), min_rank=2)\n    logits_2d = array_ops.reshape(logits, [-1, num_logits])\n    predictions = nn.softmax(logits_2d)\n    predictions = array_ops.reshape(predictions, array_ops.shape(logits))\n    if not context.executing_eagerly():\n      predictions.set_shape(logits.get_shape())\n    return predictions\n\n\n@add_arg_scope\ndef spatial_softmax(features,\n                    temperature=None,\n                    name=None,\n                    variables_collections=None,\n                    trainable=True,\n                    data_format=\'NHWC\'):\n  """"""Computes the spatial softmax of a convolutional feature map.\n\n  First computes the softmax over the spatial extent of each channel of a\n  convolutional feature map. Then computes the expected 2D position of the\n  points of maximal activation for each channel, resulting in a set of\n  feature keypoints [i1, j1, ... iN, jN] for all N channels.\n\n  Read more here:\n  ""Learning visual feature spaces for robotic manipulation with\n  deep spatial autoencoders."" Finn et al., http://arxiv.org/abs/1509.06113.\n\n  Args:\n    features: A `Tensor` of size [batch_size, W, H, num_channels]; the\n      convolutional feature map.\n    temperature: Softmax temperature (optional). If None, a learnable\n      temperature is created.\n    name: A name for this operation (optional).\n    variables_collections: Collections for the temperature variable.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n\n  Returns:\n    feature_keypoints: A `Tensor` with size [batch_size, num_channels * 2];\n      the expected 2D locations of each channel\'s feature keypoint (normalized\n      to the range (-1,1)). The inner dimension is arranged as\n      [i1, j1, ... iN, jN].\n  Raises:\n    ValueError: If unexpected data_format specified.\n    ValueError: If num_channels dimension is unspecified.\n  """"""\n  with variable_scope.variable_scope(name, \'spatial_softmax\'):\n    shape = array_ops.shape(features)\n    static_shape = features.shape\n    if data_format == DATA_FORMAT_NHWC:\n      height, width, num_channels = shape[1], shape[2], static_shape[3]\n    elif data_format == DATA_FORMAT_NCHW:\n      num_channels, height, width = static_shape[1], shape[2], shape[3]\n    else:\n      raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n    if tensor_shape.dimension_value(num_channels) is None:\n      raise ValueError(\'The num_channels dimension of the inputs to \'\n                       \'`spatial_softmax` should be defined. Found `None`.\')\n\n    with ops.name_scope(\'spatial_softmax_op\', \'spatial_softmax_op\', [features]):\n      # Create tensors for x and y coordinate values, scaled to range [-1, 1].\n      pos_x, pos_y = array_ops.meshgrid(\n          math_ops.lin_space(-1., 1., num=height),\n          math_ops.lin_space(-1., 1., num=width),\n          indexing=\'ij\')\n      pos_x = array_ops.reshape(pos_x, [height * width])\n      pos_y = array_ops.reshape(pos_y, [height * width])\n\n      if temperature is None:\n        temp_initializer = init_ops.ones_initializer()\n      else:\n        temp_initializer = init_ops.constant_initializer(temperature)\n\n      if not trainable:\n        temp_collections = None\n      else:\n        temp_collections = utils.get_variable_collections(\n            variables_collections, \'temperature\')\n\n      temperature = variables.model_variable(\n          \'temperature\',\n          shape=(),\n          dtype=dtypes.float32,\n          initializer=temp_initializer,\n          collections=temp_collections,\n          trainable=trainable)\n      if data_format == \'NCHW\':\n        features = array_ops.reshape(features, [-1, height * width])\n      else:\n        features = array_ops.reshape(\n            array_ops.transpose(features, [0, 3, 1, 2]), [-1, height * width])\n\n      softmax_attention = nn.softmax(features / temperature)\n      expected_x = math_ops.reduce_sum(\n          pos_x * softmax_attention, [1], keepdims=True)\n      expected_y = math_ops.reduce_sum(\n          pos_y * softmax_attention, [1], keepdims=True)\n      expected_xy = array_ops.concat([expected_x, expected_y], 1)\n      feature_keypoints = array_ops.reshape(\n          expected_xy, [-1, tensor_shape.dimension_value(num_channels) * 2])\n      feature_keypoints.set_shape(\n          [None, tensor_shape.dimension_value(num_channels) * 2])\n  return feature_keypoints\n\n\ndef stack(inputs, layer, stack_args, **kwargs):\n  """"""Builds a stack of layers by applying layer repeatedly using stack_args.\n\n  `stack` allows you to repeatedly apply the same operation with different\n  arguments `stack_args[i]`. For each application of the layer, `stack` creates\n  a new scope appended with an increasing number. For example:\n\n  ```python\n    y = stack(x, fully_connected, [32, 64, 128], scope=\'fc\')\n    # It is equivalent to:\n\n    x = fully_connected(x, 32, scope=\'fc/fc_1\')\n    x = fully_connected(x, 64, scope=\'fc/fc_2\')\n    y = fully_connected(x, 128, scope=\'fc/fc_3\')\n  ```\n\n  If the `scope` argument is not given in `kwargs`, it is set to\n  `layer.__name__`, or `layer.func.__name__` (for `functools.partial`\n  objects). If neither `__name__` nor `func.__name__` is available, the\n  layers are called with `scope=\'stack\'`.\n\n  Args:\n    inputs: A `Tensor` suitable for layer.\n    layer: A layer with arguments `(inputs, *args, **kwargs)`\n    stack_args: A list/tuple of parameters for each call of layer.\n    **kwargs: Extra kwargs for the layer.\n\n  Returns:\n    A `Tensor` result of applying the stacked layers.\n\n  Raises:\n    ValueError: If the op is unknown or wrong.\n  """"""\n  scope = kwargs.pop(\'scope\', None)\n  if not isinstance(stack_args, (list, tuple)):\n    raise ValueError(\'stack_args need to be a list or tuple\')\n  with variable_scope.variable_scope(scope, \'Stack\', [inputs]):\n    inputs = ops.convert_to_tensor(inputs)\n    if scope is None:\n      if hasattr(layer, \'__name__\'):\n        scope = layer.__name__\n      elif hasattr(layer, \'func\') and hasattr(layer.func, \'__name__\'):\n        scope = layer.func.__name__  # In case layer is a functools.partial.\n      else:\n        scope = \'stack\'\n    outputs = inputs\n    for i in range(len(stack_args)):\n      kwargs[\'scope\'] = scope + \'_\' + str(i + 1)\n      layer_args = stack_args[i]\n      if not isinstance(layer_args, (list, tuple)):\n        layer_args = [layer_args]\n      outputs = layer(outputs, *layer_args, **kwargs)\n    return outputs\n\n\n@add_arg_scope\ndef unit_norm(inputs, dim, epsilon=1e-7, scope=None):\n  """"""Normalizes the given input across the specified dimension to unit length.\n\n  Note that the rank of `input` must be known.\n\n  Args:\n    inputs: A `Tensor` of arbitrary size.\n    dim: The dimension along which the input is normalized.\n    epsilon: A small value to add to the inputs to avoid dividing by zero.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    The normalized `Tensor`.\n\n  Raises:\n    ValueError: If dim is smaller than the number of dimensions in \'inputs\'.\n  """"""\n  with variable_scope.variable_scope(scope, \'UnitNorm\', [inputs]):\n    if not inputs.get_shape():\n      raise ValueError(\'The input rank must be known.\')\n    input_rank = len(inputs.get_shape().as_list())\n    if dim < 0 or dim >= input_rank:\n      raise ValueError(\'dim must be positive but smaller than the input rank.\')\n\n    lengths = math_ops.sqrt(\n        epsilon + math_ops.reduce_sum(math_ops.square(inputs), dim, True))\n    multiples = []\n    if dim > 0:\n      multiples.append(array_ops.ones([dim], dtypes.int32))\n    multiples.append(\n        array_ops.strided_slice(array_ops.shape(inputs), [dim], [dim + 1]))\n    if dim < (input_rank - 1):\n      multiples.append(array_ops.ones([input_rank - 1 - dim], dtypes.int32))\n    multiples = array_ops.concat(multiples, 0)\n    return math_ops.div(inputs, array_ops.tile(lengths, multiples))\n\n\n@add_arg_scope\ndef maxout(inputs, num_units, axis=-1, scope=None):\n  """"""Adds a maxout op from https://arxiv.org/abs/1302.4389\n\n  ""Maxout Networks"" Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron\n  Courville,\n   Yoshua Bengio\n\n  Usually the operation is performed in the filter/channel dimension. This can\n  also be\n  used after fully-connected layers to reduce number of features.\n\n  Arguments:\n    inputs: Tensor input\n    num_units: Specifies how many features will remain after maxout in the\n      `axis` dimension (usually channel). This must be a factor of number of\n      features.\n    axis: The dimension where max pooling will be performed. Default is the last\n      dimension.\n    scope: Optional scope for variable_scope.\n\n  Returns:\n    A `Tensor` representing the results of the pooling operation.\n\n  Raises:\n    ValueError: if num_units is not multiple of number of features.\n  """"""\n  with variable_scope.variable_scope(scope, \'MaxOut\', [inputs]):\n    inputs = ops.convert_to_tensor(inputs)\n    shape = inputs.get_shape().as_list()\n    num_channels = shape[axis]\n    if num_channels % num_units:\n      raise ValueError(\'number of features({}) is not \'\n                       \'a multiple of num_units({})\'.format(\n                           num_channels, num_units))\n    shape[axis] = num_units\n    shape += [num_channels // num_units]\n\n    # Dealing with batches with arbitrary sizes\n    for i in range(len(shape)):\n      if shape[i] is None:\n        shape[i] = array_ops.shape(inputs)[i]\n    outputs = math_ops.reduce_max(\n        array_ops.reshape(inputs, shape), -1, keepdims=False)\n    return outputs\n\n\ndef poincare_normalize(x, axis=1, epsilon=1e-5, name=None):\n  """"""Project into the Poincare ball with norm <= 1.0 - epsilon.\n\n  https://en.wikipedia.org/wiki/Poincare_ball_model\n\n  Used in\n  Poincare Embeddings for Learning Hierarchical Representations\n  Maximilian Nickel, Douwe Kiela\n  https://arxiv.org/pdf/1705.08039.pdf\n\n  For a 1-D tensor with `axis = 0`, computes\n\n                (x * (1 - epsilon)) / ||x||     if ||x|| > 1 - epsilon\n      output =\n                 x                              otherwise\n\n  For `x` with more dimensions, independently normalizes each 1-D slice along\n  dimension `axis`.\n\n  Args:\n    x: A `Tensor`.\n    axis: Axis along which to normalize.  A scalar or a vector of integers.\n    epsilon: A small deviation from the edge of the unit sphere for numerical\n      stability.\n    name: A name for this operation (optional).\n\n  Returns:\n    A `Tensor` with the same shape as `x`.\n  """"""\n  with ops.name_scope(name, \'poincare_normalize\', [x]) as name:\n    x = ops.convert_to_tensor(x, name=\'x\')\n    square_sum = math_ops.reduce_sum(math_ops.square(x), axis, keepdims=True)\n    x_inv_norm = math_ops.rsqrt(square_sum)\n    x_inv_norm = math_ops.minimum((1. - epsilon) * x_inv_norm, 1.)\n    return math_ops.multiply(x, x_inv_norm, name=name)\n\n\ndef legacy_fully_connected(x,\n                           num_output_units,\n                           activation_fn=None,\n                           weight_init=initializers.xavier_initializer(),\n                           bias_init=init_ops.zeros_initializer(),\n                           name=None,\n                           weight_collections=(ops.GraphKeys.WEIGHTS,),\n                           bias_collections=(ops.GraphKeys.BIASES,),\n                           output_collections=(ops.GraphKeys.ACTIVATIONS,),\n                           trainable=True,\n                           weight_regularizer=None,\n                           bias_regularizer=None):\n  # pylint: disable=anomalous-backslash-in-string\n  r""""""Adds the parameters for a fully connected layer and returns the output.\n\n  A fully connected layer is generally defined as a matrix multiply:\n  `y = f(w * x + b)` where `f` is given by `activation_fn`. If\n  `activation_fn` is `None`, the result of `y = w * x + b` is\n  returned.\n\n  If `x` has shape [\\\\(\\text{dim}_0, \\text{dim}_1, ..., \\text{dim}_n\\\\)]\n  with more than 2 dimensions (\\\\(n > 1\\\\)), then we repeat the matrix\n  multiply along the first dimensions. The result r is a tensor of shape\n  [\\\\(\\text{dim}_0, ..., \\text{dim}_{n-1},\\\\) `num_output_units`],\n  where \\\\( r_{i_0, ..., i_{n-1}, k} =\n  \\sum_{0 \\leq j < \\text{dim}_n} x_{i_0, ... i_{n-1}, j} \\cdot w_{j, k}\\\\).\n  This is accomplished by reshaping `x` to 2-D\n  [\\\\(\\text{dim}_0 \\cdot ... \\cdot \\text{dim}_{n-1}, \\text{dim}_n\\\\)]\n  before the matrix multiply and afterwards reshaping it to\n  [\\\\(\\text{dim}_0, ..., \\text{dim}_{n-1},\\\\) `num_output_units`].\n\n  This op creates `w` and optionally `b`. Bias (`b`) can be disabled by setting\n  `bias_init` to `None`.\n\n  The variable creation is compatible with `tf.compat.v1.variable_scope` and so\n  can be\n  reused with `tf.compat.v1.variable_scope` or `tf.compat.v1.make_template`.\n\n  Most of the details of variable creation can be controlled by specifying the\n  initializers (`weight_init` and `bias_init`) and in which collections to place\n  the created variables (`weight_collections` and `bias_collections`; note that\n  the variables are always added to the `VARIABLES` collection). The output of\n  the layer can be placed in custom collections using `output_collections`.\n  The collections arguments default to `WEIGHTS`, `BIASES` and `ACTIVATIONS`,\n  respectively.\n\n  A per layer regularization can be specified by setting `weight_regularizer`\n  and `bias_regularizer`, which are applied to the weights and biases\n  respectively, and whose output is added to the `REGULARIZATION_LOSSES`\n  collection.\n\n  Args:\n    x: The input `Tensor`.\n    num_output_units: The size of the output.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    weight_init: An optional weight initialization, defaults to\n      `xavier_initializer`.\n    bias_init: An initializer for the bias, defaults to 0. Set to `None` in\n      order to disable bias.\n    name: The name for this operation is used to name operations and to find\n      variables. If specified it must be unique for this scope, otherwise a\n      unique name starting with ""fully_connected"" will be created.  See\n      `tf.compat.v1.variable_scope` for details.\n    weight_collections: List of graph collections to which weights are added.\n    bias_collections: List of graph collections to which biases are added.\n    output_collections: List of graph collections to which outputs are added.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    weight_regularizer: A regularizer like the result of `l1_regularizer` or\n      `l2_regularizer`. Used for weights.\n    bias_regularizer: A regularizer like the result of `l1_regularizer` or\n      `l2_regularizer`. Used for biases.\n\n  Returns:\n    The output of the fully connected layer.\n\n  Raises:\n    ValueError: If x has rank less than 2 or if its last dimension is not set.\n  """"""\n  with variable_scope.variable_scope(name, \'fully_connected\', [x]):\n    x = ops.convert_to_tensor(x)\n    dims = x.get_shape().dims\n    if dims is None:\n      raise ValueError(\'dims of x must be known but is None\')\n    if len(dims) < 2:\n      raise ValueError(\'rank of x must be at least 2 not: %d\' % len(dims))\n    num_input_units = dims[-1].value\n    if num_input_units is None:\n      raise ValueError(\'last dimension of x must be known but is None\')\n    dtype = x.dtype.base_dtype\n\n    weight_collections = set(\n        list(weight_collections or []) + [ops.GraphKeys.GLOBAL_VARIABLES])\n    w = variable_scope.get_variable(\n        \'weights\',\n        shape=[num_input_units, num_output_units],\n        dtype=dtype,\n        initializer=weight_init,\n        collections=weight_collections,\n        regularizer=weight_regularizer,\n        trainable=trainable)\n    x_2_dim = x if len(dims) <= 2 else array_ops.reshape(\n        x, [-1, num_input_units])\n    y = standard_ops.matmul(x_2_dim, w)\n\n    if bias_init is not None:\n      bias_collections = set(\n          list(bias_collections or []) + [ops.GraphKeys.GLOBAL_VARIABLES])\n      b = variable_scope.get_variable(\n          \'bias\',\n          shape=[num_output_units],\n          dtype=dtype,\n          initializer=bias_init,\n          collections=bias_collections,\n          regularizer=bias_regularizer,\n          trainable=trainable)\n\n      y = nn.bias_add(y, b)\n\n    if len(dims) > 2:\n      out_shape = array_ops.unstack(array_ops.shape(x))\n      out_shape[-1] = num_output_units\n\n      y = array_ops.reshape(y, array_ops.stack(out_shape))\n\n      static_shape = x.get_shape().as_list()\n      static_shape[-1] = num_output_units\n      y.set_shape(static_shape)\n\n    return _apply_activation(y, activation_fn, output_collections)\n\n\n# TODO(eiderm): Verify and fix autocomplete in colab (also relu6).\n# Simple aliases which remove the activation_fn parameter.\nelu = functools.partial(fully_connected, activation_fn=nn.elu)\nlegacy_relu = functools.partial(legacy_fully_connected, activation_fn=nn.relu)\nlegacy_linear = functools.partial(legacy_fully_connected, activation_fn=None)\nrelu = functools.partial(fully_connected, activation_fn=nn.relu)\nrelu6 = functools.partial(fully_connected, activation_fn=nn.relu6)\nlinear = functools.partial(fully_connected, activation_fn=None)\n\n# Simple alias.\nconv1d = convolution1d\nconv2d = convolution2d\nconv3d = convolution3d\nconv2d_transpose = convolution2d_transpose\nconv3d_transpose = convolution3d_transpose\nconv2d_in_plane = convolution2d_in_plane\nseparable_conv2d = separable_convolution2d\n'"
tensornets/contrib_layers/normalization.py,2,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Contains the normalization layer classes and their functional aliases.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom ..contrib_framework import add_arg_scope\nfrom ..contrib_framework import variables\nfrom . import utils\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.framework import dtypes\n\n__all__ = [\n    \'group_norm\',\n    \'instance_norm\',\n]\n\nDATA_FORMAT_NCHW = \'NCHW\'\nDATA_FORMAT_NHWC = \'NHWC\'\n\n\n@add_arg_scope\ndef instance_norm(inputs,\n                  center=True,\n                  scale=True,\n                  epsilon=1e-6,\n                  activation_fn=None,\n                  param_initializers=None,\n                  reuse=None,\n                  variables_collections=None,\n                  outputs_collections=None,\n                  trainable=True,\n                  data_format=DATA_FORMAT_NHWC,\n                  scope=None):\n  """"""Functional interface for the instance normalization layer.\n\n  Reference: https://arxiv.org/abs/1607.08022.\n\n    ""Instance Normalization: The Missing Ingredient for Fast Stylization""\n    Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky\n\n  Args:\n    inputs: A tensor with 2 or more dimensions, where the first dimension has\n      `batch_size`. The normalization is over all but the last dimension if\n      `data_format` is `NHWC` and the second dimension if `data_format` is\n      `NCHW`.\n    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n      is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    param_initializers: Optional initializers for beta, gamma, moving mean and\n      moving variance.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    data_format: A string. `NHWC` (default) and `NCHW` are supported.\n    scope: Optional scope for `variable_scope`.\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If rank or channels dimension of `inputs` is undefined.\n  """"""\n  inputs = ops.convert_to_tensor(inputs)\n  inputs_shape = inputs.shape\n  inputs_rank = inputs.shape.ndims\n  # For big endian, precision difference in last decimal values getting in\n  # float32 Vs float64 data type is causing normalization_test failure.\n  # The cast to float64 will calculate mean and variance correctly while\n  # normalization of `inputs` tensor.\n  if sys.byteorder == ""big"" and inputs.dtype.base_dtype == dtypes.float32:\n    inputs = math_ops.cast(inputs, dtypes.float64)\n\n  if inputs_rank is None:\n    raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n  if data_format not in (DATA_FORMAT_NCHW, DATA_FORMAT_NHWC):\n    raise ValueError(\'data_format has to be either NCHW or NHWC.\')\n\n  with variable_scope.variable_scope(\n      scope, \'InstanceNorm\', [inputs], reuse=reuse) as sc:\n    if data_format == DATA_FORMAT_NCHW:\n      reduction_axis = 1\n      # For NCHW format, rather than relying on implicit broadcasting, we\n      # explicitly reshape the params to params_shape_broadcast when computing\n      # the moments and the batch normalization.\n      params_shape_broadcast = list(\n          [1, inputs_shape[1].value] + [1 for _ in range(2, inputs_rank)])\n    else:\n      reduction_axis = inputs_rank - 1\n      params_shape_broadcast = None\n    moments_axes = list(range(inputs_rank))\n    del moments_axes[reduction_axis]\n    del moments_axes[0]\n    params_shape = inputs_shape[reduction_axis:reduction_axis + 1]\n    if not params_shape.is_fully_defined():\n      raise ValueError(\'Inputs %s has undefined channels dimension %s.\' % (\n          inputs.name, params_shape))\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    dtype = inputs.dtype.base_dtype\n    if param_initializers is None:\n      param_initializers = {}\n    if center:\n      beta_collections = utils.get_variable_collections(\n          variables_collections, \'beta\')\n      beta_initializer = param_initializers.get(\n          \'beta\', init_ops.zeros_initializer())\n      beta = variables.model_variable(\'beta\',\n                                      shape=params_shape,\n                                      dtype=dtype,\n                                      initializer=beta_initializer,\n                                      collections=beta_collections,\n                                      trainable=trainable)\n      if params_shape_broadcast:\n        beta = array_ops.reshape(beta, params_shape_broadcast)\n    if scale:\n      gamma_collections = utils.get_variable_collections(\n          variables_collections, \'gamma\')\n      gamma_initializer = param_initializers.get(\n          \'gamma\', init_ops.ones_initializer())\n      gamma = variables.model_variable(\'gamma\',\n                                       shape=params_shape,\n                                       dtype=dtype,\n                                       initializer=gamma_initializer,\n                                       collections=gamma_collections,\n                                       trainable=trainable)\n      if params_shape_broadcast:\n        gamma = array_ops.reshape(gamma, params_shape_broadcast)\n\n    # Calculate the moments (instance activations).\n    mean, variance = nn.moments(inputs, moments_axes, keep_dims=True)\n\n    # Compute instance normalization.\n    outputs = nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, epsilon, name=\'instancenorm\')\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n\n\n@add_arg_scope\ndef group_norm(inputs,\n               groups=32,\n               channels_axis=-1,\n               reduction_axes=(-3, -2),\n               center=True,\n               scale=True,\n               epsilon=1e-6,\n               activation_fn=None,\n               param_initializers=None,\n               reuse=None,\n               variables_collections=None,\n               outputs_collections=None,\n               trainable=True,\n               scope=None,\n               mean_close_to_zero=False):\n  """"""Functional interface for the group normalization layer.\n\n  Reference: https://arxiv.org/abs/1803.08494.\n\n    ""Group Normalization"", Yuxin Wu, Kaiming He\n\n  Args:\n    inputs: A Tensor with at least 2 dimensions one which is channels. All\n     shape dimensions except for batch must be fully defined.\n    groups: Integer. Divide the channels into this number of groups over which\n      normalization statistics are computed. This number must be commensurate\n      with the number of channels in `inputs`.\n    channels_axis: An integer. Specifies index of channels axis which will be\n      broken into `groups`, each of which whose statistics will be computed\n      across. Must be mutually exclusive with `reduction_axes`. Preferred usage\n      is to specify negative integers to be agnostic as to whether a batch\n      dimension is included.\n    reduction_axes: Tuple of integers. Specifies dimensions over which\n       statistics will be accumulated. Must be mutually exclusive with\n       `channels_axis`. Statistics will not be accumulated across axes not\n       specified in `reduction_axes` nor `channel_axis`. Preferred usage is to\n       specify negative integers to be agnostic to whether a batch dimension is\n       included.\n\n      Some sample usage cases:\n        NHWC format: channels_axis=-1, reduction_axes=[-3, -2]\n        NCHW format: channels_axis=-3, reduction_axes=[-2, -1]\n\n    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n      is ignored.\n    scale: If True, multiply by `gamma`. If False, `gamma` is\n      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n      disabled since the scaling can be done by the next layer.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    activation_fn: Activation function, default set to None to skip it and\n      maintain a linear activation.\n    param_initializers: Optional initializers for beta, gamma, moving mean and\n      moving variance.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional collections for the variables.\n    outputs_collections: Collections to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n    scope: Optional scope for `variable_scope`.\n    mean_close_to_zero: The mean of `input` before ReLU will be close to zero\n      when batch size >= 4k for Resnet-50 on TPU. If `True`, use\n      `nn.sufficient_statistics` and `nn.normalize_moments` to calculate the\n      variance. This is the same behavior as `fused` equals `True` in batch\n      normalization. If `False`, use `nn.moments` to calculate the variance.\n      When `mean` is close to zero, like 1e-4, use `mean` to calculate the\n      variance may have poor result due to repeated roundoff error and\n      denormalization in `mean`.  When `mean` is large, like 1e2,\n      sum(`input`^2) is so large that only the high-order digits of the elements\n      are being accumulated. Thus, use sum(`input` - `mean`)^2/n to calculate\n      the variance has better accuracy compared to (sum(`input`^2)/n - `mean`^2)\n      when `mean` is large.\n\n\n  Returns:\n    A `Tensor` representing the output of the operation.\n\n  Raises:\n    ValueError: If the rank of `inputs` is undefined.\n    ValueError: If rank or channels dimension of `inputs` is undefined.\n    ValueError: If number of groups is not commensurate with number of channels.\n    ValueError: If reduction_axes or channels_axis are out of bounds.\n    ValueError: If reduction_axes are not mutually exclusive with channels_axis.\n  """"""\n  # TODO(shlens): Support partially defined shapes for the inputs.\n  inputs = ops.convert_to_tensor(inputs)\n\n  if inputs.shape.ndims is None:\n    raise ValueError(\'Inputs %s has undefined rank.\' % inputs.name)\n  if channels_axis > (inputs.shape.ndims - 1):\n    raise ValueError(\'Axis is out of bounds.\')\n\n  # Use dynamic shape for not fully defined dimensions in the inputs.\n  dyanmic_shape = array_ops.shape(inputs)\n  input_shape_list = []\n  for i, dim in enumerate(inputs.shape):\n    if dim.value is None:\n      input_shape_list.append(dyanmic_shape[i])\n    else:\n      input_shape_list.append(dim)\n\n  # Standardize the channels_axis to be positive and identify # of channels.\n  if channels_axis < 0:\n    channels_axis = inputs.shape.ndims + channels_axis\n  channels = inputs.shape[channels_axis].value\n\n  if channels is None:\n    raise ValueError(\'Inputs %s has undefined channel dimension: %d.\' % (\n        inputs.name, channels_axis))\n\n  # Standardize the reduction_axes to be positive.\n  reduction_axes = list(reduction_axes)\n  for i in range(len(reduction_axes)):\n    if reduction_axes[i] < 0:\n      reduction_axes[i] += inputs.shape.ndims\n\n  for a in reduction_axes:\n    if a > inputs.shape.ndims:\n      raise ValueError(\'Axis is out of bounds.\')\n    if inputs.shape[a].value is None:\n      raise ValueError(\'Inputs %s has undefined dimensions %d.\' % (\n          inputs.name, a))\n    if channels_axis == a:\n      raise ValueError(\'reduction_axis must be mutually exclusive \'\n                       \'with channels_axis\')\n  if groups > channels:\n    raise ValueError(\'Invalid groups %d for %d channels.\' % (groups, channels))\n  if channels % groups != 0:\n    raise ValueError(\'%d channels is not commensurate with %d groups.\' %\n                     (channels, groups))\n\n  # Determine axes before channels. Some examples of common image formats:\n  #  \'NCHW\': before = [N], after = [HW]\n  #  \'NHWC\': before = [NHW], after = []\n  axes_before_channels = input_shape_list[:channels_axis]\n  axes_after_channels = input_shape_list[channels_axis+1:]\n\n  # Manually broadcast the parameters to conform to the number of groups.\n  params_shape_broadcast = ([1] * len(axes_before_channels) +\n                            [groups, channels // groups] +\n                            [1] * len(axes_after_channels))\n\n  # Reshape the input by the group within the channel dimension.\n  inputs_shape = (axes_before_channels + [groups, channels // groups] +\n                  axes_after_channels)\n  inputs = array_ops.reshape(inputs, inputs_shape)\n\n  # Determine the dimensions across which moments are calculated.\n  moments_axes = [channels_axis + 1]\n  for a in reduction_axes:\n    if a > channels_axis:\n      moments_axes.append(a + 1)\n    else:\n      moments_axes.append(a)\n\n  with variable_scope.variable_scope(\n      scope, \'GroupNorm\', [inputs], reuse=reuse) as sc:\n    # Note that the params_shape is the number of channels always.\n    params_shape = [channels]\n\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    dtype = inputs.dtype.base_dtype\n    if param_initializers is None:\n      param_initializers = {}\n    if center:\n      beta_collections = utils.get_variable_collections(\n          variables_collections, \'beta\')\n      beta_initializer = param_initializers.get(\n          \'beta\', init_ops.zeros_initializer())\n      beta = variables.model_variable(\'beta\',\n                                      shape=params_shape,\n                                      dtype=dtype,\n                                      initializer=beta_initializer,\n                                      collections=beta_collections,\n                                      trainable=trainable)\n      beta = array_ops.reshape(beta, params_shape_broadcast)\n\n    if scale:\n      gamma_collections = utils.get_variable_collections(\n          variables_collections, \'gamma\')\n      gamma_initializer = param_initializers.get(\n          \'gamma\', init_ops.ones_initializer())\n      gamma = variables.model_variable(\'gamma\',\n                                       shape=params_shape,\n                                       dtype=dtype,\n                                       initializer=gamma_initializer,\n                                       collections=gamma_collections,\n                                       trainable=trainable)\n      gamma = array_ops.reshape(gamma, params_shape_broadcast)\n\n    # Calculate the moments.\n    if mean_close_to_zero:\n      # One pass algorithm returns better result when mean is close to zero.\n      counts, means_ss, variance_ss, _ = nn.sufficient_statistics(\n          inputs, moments_axes, keep_dims=True)\n      mean, variance = nn.normalize_moments(\n          counts, means_ss, variance_ss, shift=None)\n    else:\n      mean, variance = nn.moments(inputs, moments_axes, keep_dims=True)\n\n    # Compute normalization.\n    # TODO(shlens): Fix nn.batch_normalization to handle the 5-D Tensor\n    # appropriately so that this operation may be faster.\n    gain = math_ops.rsqrt(variance + epsilon)\n    offset = -mean * gain\n    if gamma is not None:\n      gain *= gamma\n      offset *= gamma\n    if beta is not None:\n      offset += beta\n    outputs = inputs * gain + offset\n\n    # Collapse the groups into the channel dimension.\n    outputs = array_ops.reshape(outputs, input_shape_list)\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n'"
tensornets/contrib_layers/optimizers.py,9,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Optimizer ops for use in layers and tf.learn.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\n\nfrom .. import contrib_framework\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import variables as vars_\nfrom tensorflow.python.summary import summary\nfrom tensorflow.python.training import moving_averages\nfrom tensorflow.python.training import optimizer as optimizer_\nfrom tensorflow.python.training import training as train\n\nOPTIMIZER_CLS_NAMES = {\n    ""Adagrad"": train.AdagradOptimizer,\n    ""Adam"": train.AdamOptimizer,\n    ""Ftrl"": train.FtrlOptimizer,\n    ""Momentum"": lambda learning_rate: train.MomentumOptimizer(learning_rate, momentum=0.9),  # pylint: disable=line-too-long\n    ""RMSProp"": train.RMSPropOptimizer,\n    ""SGD"": train.GradientDescentOptimizer,\n}\n\nOPTIMIZER_SUMMARIES = [\n    ""learning_rate"",\n    ""loss"",\n    ""gradients"",\n    ""gradient_norm"",\n    ""global_gradient_norm"",\n]\n\n\ndef optimize_loss(loss,\n                  global_step,\n                  learning_rate,\n                  optimizer,\n                  gradient_noise_scale=None,\n                  gradient_multipliers=None,\n                  clip_gradients=None,\n                  learning_rate_decay_fn=None,\n                  update_ops=None,\n                  variables=None,\n                  name=None,\n                  summaries=None,\n                  colocate_gradients_with_ops=False,\n                  increment_global_step=True):\n  """"""Given loss and parameters for optimizer, returns a training op.\n\n  Various ways of passing optimizers include:\n\n  - by string specifying the name of the optimizer. See OPTIMIZER_CLS_NAMES\n      for full list. E.g. `optimize_loss(..., optimizer=\'Adam\')`.\n  - by function taking learning rate `Tensor` as argument and returning an\n      `Optimizer` instance. E.g. `optimize_loss(...,\n      optimizer=lambda lr: tf.compat.v1.train.MomentumOptimizer(lr,\n      momentum=0.5))`.\n    Alternatively, if `learning_rate` is `None`, the function takes no\n    arguments. E.g. `optimize_loss(..., learning_rate=None,\n      optimizer=lambda: tf.compat.v1.train.MomentumOptimizer(0.5,\n      momentum=0.5))`.\n  - by a subclass of `Optimizer` having a single-argument constructor\n      (the argument is the learning rate), such as AdamOptimizer or\n      AdagradOptimizer. E.g. `optimize_loss(...,\n      optimizer=tf.compat.v1.train.AdagradOptimizer)`.\n  - by an instance of a subclass of `Optimizer`.\n      E.g., `optimize_loss(...,\n      optimizer=tf.compat.v1.train.AdagradOptimizer(0.5))`.\n\n  Args:\n    loss: Scalar `Tensor`.\n    global_step: Scalar int `Tensor`, step counter to update on each step unless\n      `increment_global_step` is `False`. If not supplied, it will be fetched\n      from the default graph (see `tf.compat.v1.train.get_global_step` for\n      details). If it has not been created, no step will be incremented with\n      each weight update. `learning_rate_decay_fn` requires `global_step`.\n    learning_rate: float or `Tensor`, magnitude of update per each training\n      step. Can be `None`.\n    optimizer: string, class or optimizer instance, used as trainer. string\n      should be name of optimizer, like \'SGD\', \'Adam\', \'Adagrad\'. Full list in\n      OPTIMIZER_CLS_NAMES constant. class should be sub-class of `tf.Optimizer`\n      that implements `compute_gradients` and `apply_gradients` functions.\n      optimizer instance should be instantiation of `tf.Optimizer` sub-class and\n      have `compute_gradients` and `apply_gradients` functions.\n    gradient_noise_scale: float or None, adds 0-mean normal noise scaled by this\n      value.\n    gradient_multipliers: dict of variables or variable names to floats. If\n      present, gradients for specified variables will be multiplied by given\n      constant.\n    clip_gradients: float, callable or `None`. If a float is provided, a global\n      clipping is applied to prevent the norm of the gradient from exceeding\n      this value. Alternatively, a callable can be provided, e.g.,\n      `adaptive_clipping_fn()`.  This callable takes a list of `(gradients,\n      variables)` tuples and returns the same thing with the gradients modified.\n    learning_rate_decay_fn: function, takes `learning_rate` and `global_step`\n      `Tensor`s, returns `Tensor`. Can be used to implement any learning rate\n      decay functions.\n                            For example: `tf.compat.v1.train.exponential_decay`.\n                              Ignored if `learning_rate` is not supplied.\n    update_ops: list of update `Operation`s to execute at each step. If `None`,\n      uses elements of UPDATE_OPS collection. The order of execution between\n      `update_ops` and `loss` is non-deterministic.\n    variables: list of variables to optimize or `None` to use all trainable\n      variables.\n    name: The name for this operation is used to scope operations and summaries.\n    summaries: List of internal quantities to visualize on tensorboard. If not\n      set, the loss, the learning rate, and the global norm of the gradients\n      will be reported. The complete list of possible values is in\n      OPTIMIZER_SUMMARIES.\n    colocate_gradients_with_ops: If True, try colocating gradients with the\n      corresponding op.\n    increment_global_step: Whether to increment `global_step`. If your model\n      calls `optimize_loss` multiple times per training step (e.g. to optimize\n      different parts of the model), use this arg to avoid incrementing\n      `global_step` more times than necessary.\n\n  Returns:\n    Training op.\n\n  Raises:\n    ValueError: if:\n        * `loss` is an invalid type or shape.\n        * `global_step` is an invalid type or shape.\n        * `learning_rate` is an invalid type or value.\n        * `optimizer` has the wrong type.\n        * `clip_gradients` is neither float nor callable.\n        * `learning_rate` and `learning_rate_decay_fn` are supplied, but no\n          `global_step` is available.\n        * `gradients` is empty.\n  """"""\n  loss = ops.convert_to_tensor(loss)\n  contrib_framework.assert_scalar(loss)\n  if global_step is None:\n    global_step = train.get_global_step()\n  else:\n    train.assert_global_step(global_step)\n  with vs.variable_scope(name, ""OptimizeLoss"", [loss, global_step]):\n    # Update ops take UPDATE_OPS collection if not provided.\n    if update_ops is None:\n      update_ops = set(ops.get_collection(ops.GraphKeys.UPDATE_OPS))\n    # Make sure update ops are ran before computing loss.\n    if update_ops:\n      loss = control_flow_ops.with_dependencies(list(update_ops), loss)\n\n    # Learning rate variable, with possible decay.\n    lr = None\n    if learning_rate is not None:\n      if (isinstance(learning_rate, ops.Tensor) and\n          learning_rate.get_shape().ndims == 0):\n        lr = learning_rate\n      elif isinstance(learning_rate, float):\n        if learning_rate < 0.0:\n          raise ValueError(""Invalid learning_rate %s."", learning_rate)\n        lr = vs.get_variable(\n            ""learning_rate"", [],\n            trainable=False,\n            initializer=init_ops.constant_initializer(learning_rate))\n      else:\n        raise ValueError(""Learning rate should be 0d Tensor or float. ""\n                         ""Got %s of type %s"" %\n                         (str(learning_rate), str(type(learning_rate))))\n    if summaries is None:\n      summaries = [""loss"", ""learning_rate"", ""global_gradient_norm""]\n    else:\n      for summ in summaries:\n        if summ not in OPTIMIZER_SUMMARIES:\n          raise ValueError(""Summaries should be one of [%s], you provided %s."" %\n                           ("", "".join(OPTIMIZER_SUMMARIES), summ))\n    if learning_rate is not None and learning_rate_decay_fn is not None:\n      if global_step is None:\n        raise ValueError(""global_step is required for learning_rate_decay_fn."")\n      lr = learning_rate_decay_fn(lr, global_step)\n      if ""learning_rate"" in summaries:\n        summary.scalar(""learning_rate"", lr)\n\n    # Create optimizer, given specified parameters.\n    if isinstance(optimizer, six.string_types):\n      if lr is None:\n        raise ValueError(""Learning rate is None, but should be specified if ""\n                         ""optimizer is string (%s)."" % optimizer)\n      if optimizer not in OPTIMIZER_CLS_NAMES:\n        raise ValueError(\n            ""Optimizer name should be one of [%s], you provided %s."" %\n            ("", "".join(OPTIMIZER_CLS_NAMES), optimizer))\n      opt = OPTIMIZER_CLS_NAMES[optimizer](learning_rate=lr)\n    elif (isinstance(optimizer, type) and\n          issubclass(optimizer, optimizer_.Optimizer)):\n      if lr is None:\n        raise ValueError(""Learning rate is None, but should be specified if ""\n                         ""optimizer is class (%s)."" % optimizer)\n      opt = optimizer(learning_rate=lr)\n    elif isinstance(optimizer, optimizer_.Optimizer):\n      opt = optimizer\n    elif callable(optimizer):\n      if learning_rate is not None:\n        opt = optimizer(lr)\n      else:\n        opt = optimizer()\n      if not isinstance(opt, optimizer_.Optimizer):\n        raise ValueError(""Unrecognized optimizer: function should return ""\n                         ""subclass of Optimizer. Got %s."" % str(opt))\n    else:\n      raise ValueError(""Unrecognized optimizer: should be string, ""\n                       ""subclass of Optimizer, instance of ""\n                       ""subclass of Optimizer or function with one argument. ""\n                       ""Got %s."" % str(optimizer))\n\n    # All trainable variables, if specific variables are not specified.\n    if variables is None:\n      variables = vars_.trainable_variables()\n\n    # Compute gradients.\n    gradients = opt.compute_gradients(\n        loss,\n        variables,\n        colocate_gradients_with_ops=colocate_gradients_with_ops)\n\n    # Optionally add gradient noise.\n    if gradient_noise_scale is not None:\n      gradients = _add_scaled_noise_to_gradients(gradients,\n                                                 gradient_noise_scale)\n\n    # Multiply some gradients.\n    if gradient_multipliers is not None:\n      gradients = _multiply_gradients(gradients, gradient_multipliers)\n      if not gradients:\n        raise ValueError(\n            ""Empty list of (gradient, var) pairs encountered. This is most ""\n            ""likely to be caused by an improper value of gradient_multipliers."")\n\n    if ""global_gradient_norm"" in summaries or ""gradient_norm"" in summaries:\n      summary.scalar(""global_norm/gradient_norm"",\n                     clip_ops.global_norm(list(zip(*gradients))[0]))\n\n    # Optionally clip gradients by global norm.\n    if isinstance(clip_gradients, float):\n      gradients = _clip_gradients_by_norm(gradients, clip_gradients)\n    elif callable(clip_gradients):\n      gradients = clip_gradients(gradients)\n    elif clip_gradients is not None:\n      raise ValueError(""Unknown type %s for clip_gradients"" %\n                       type(clip_gradients))\n\n    # Add scalar summary for loss.\n    if ""loss"" in summaries:\n      summary.scalar(""loss"", loss)\n\n    # Add histograms for variables, gradients and gradient norms.\n    for gradient, variable in gradients:\n      if isinstance(gradient, ops.IndexedSlices):\n        grad_values = gradient.values\n      else:\n        grad_values = gradient\n\n      if grad_values is not None:\n        var_name = variable.name.replace("":"", ""_"")\n        if ""gradients"" in summaries:\n          summary.histogram(""gradients/%s"" % var_name, grad_values)\n        if ""gradient_norm"" in summaries:\n          summary.scalar(""gradient_norm/%s"" % var_name,\n                         clip_ops.global_norm([grad_values]))\n\n    if clip_gradients is not None and (""global_gradient_norm"" in summaries or\n                                       ""gradient_norm"" in summaries):\n      summary.scalar(""global_norm/clipped_gradient_norm"",\n                     clip_ops.global_norm(list(zip(*gradients))[0]))\n\n    # Create gradient updates.\n    grad_updates = opt.apply_gradients(\n        gradients,\n        global_step=global_step if increment_global_step else None,\n        name=""train"")\n\n    # Ensure the train_tensor computes grad_updates.\n    train_tensor = control_flow_ops.with_dependencies([grad_updates], loss)\n\n    return train_tensor\n\n\ndef _clip_gradients_by_norm(grads_and_vars, clip_gradients):\n  """"""Clips gradients by global norm.""""""\n  gradients, variables = zip(*grads_and_vars)\n  clipped_gradients, _ = clip_ops.clip_by_global_norm(gradients, clip_gradients)\n  return list(zip(clipped_gradients, variables))\n\n\ndef _adaptive_max_norm(norm, std_factor, decay, global_step, epsilon, name):\n  """"""Find max_norm given norm and previous average.""""""\n  with vs.variable_scope(name, ""AdaptiveMaxNorm"", [norm]):\n    log_norm = math_ops.log(norm + epsilon)\n\n    def moving_average(name, value, decay):\n      moving_average_variable = vs.get_variable(\n          name,\n          shape=value.get_shape(),\n          dtype=value.dtype,\n          initializer=init_ops.zeros_initializer(),\n          trainable=False)\n      return moving_averages.assign_moving_average(\n          moving_average_variable, value, decay, zero_debias=False)\n\n    # quicker adaptation at the beginning\n    if global_step is not None:\n      n = math_ops.cast(global_step, dtypes.float32)\n      decay = math_ops.minimum(decay, n / (n + 1.))\n\n    # update averages\n    mean = moving_average(""mean"", log_norm, decay)\n    sq_mean = moving_average(""sq_mean"", math_ops.square(log_norm), decay)\n\n    variance = sq_mean - math_ops.square(mean)\n    std = math_ops.sqrt(math_ops.maximum(epsilon, variance))\n    max_norms = math_ops.exp(mean + std_factor * std)\n    return max_norms, mean\n\n\ndef adaptive_clipping_fn(std_factor=2.,\n                         decay=0.95,\n                         static_max_norm=None,\n                         global_step=None,\n                         report_summary=False,\n                         epsilon=1e-8,\n                         name=None):\n  """"""Adapt the clipping value using statistics on the norms.\n\n  Implement adaptive gradient as presented in section 3.2.1 of\n  https://arxiv.org/abs/1412.1602.\n\n  Keeps a moving average of the mean and std of the log(norm) of the gradient.\n  If the norm exceeds `exp(mean + std_factor*std)` then all gradients will be\n  rescaled such that the global norm becomes `exp(mean)`.\n\n  Args:\n    std_factor: Python scaler (or tensor). `max_norm = exp(mean +\n      std_factor*std)`\n    decay: The smoothing factor of the moving averages.\n    static_max_norm: If provided, will threshold the norm to this value as an\n      extra safety.\n    global_step: Optional global_step. If provided, `decay = decay*n/(n+1)`.\n      This provides a quicker adaptation of the mean for the first steps.\n    report_summary: If `True`, will add histogram summaries of the `max_norm`.\n    epsilon: Small value chosen to avoid zero variance.\n    name: The name for this operation is used to scope operations and summaries.\n\n  Returns:\n    A function for applying gradient clipping.\n  """"""\n\n  def gradient_clipping(grads_and_vars):\n    """"""Internal function for adaptive clipping.""""""\n    grads, variables = zip(*grads_and_vars)\n\n    norm = clip_ops.global_norm(grads)\n\n    max_norm, log_mean = _adaptive_max_norm(norm, std_factor, decay,\n                                            global_step, epsilon, name)\n\n    # reports the max gradient norm for debugging\n    if report_summary:\n      summary.scalar(""global_norm/adaptive_max_gradient_norm"", max_norm)\n\n    # factor will be 1. if norm is smaller than max_norm\n    factor = array_ops.where(norm < max_norm, array_ops.ones_like(norm),\n                             math_ops.exp(log_mean) / norm)\n\n    if static_max_norm is not None:\n      factor = math_ops.minimum(static_max_norm / norm, factor)\n\n    # apply factor\n    clipped_grads = []\n    for grad in grads:\n      if grad is None:\n        clipped_grads.append(None)\n      elif isinstance(grad, ops.IndexedSlices):\n        clipped_grads.append(\n            ops.IndexedSlices(grad.values * factor, grad.indices,\n                              grad.dense_shape))\n      else:\n        clipped_grads.append(grad * factor)\n\n    return list(zip(clipped_grads, variables))\n\n  return gradient_clipping\n\n\ndef _add_scaled_noise_to_gradients(grads_and_vars, gradient_noise_scale):\n  """"""Adds scaled noise from a 0-mean normal distribution to gradients.""""""\n  gradients, variables = zip(*grads_and_vars)\n  noisy_gradients = []\n  for gradient in gradients:\n    if gradient is None:\n      noisy_gradients.append(None)\n      continue\n    if isinstance(gradient, ops.IndexedSlices):\n      gradient_shape = gradient.dense_shape\n    else:\n      gradient_shape = gradient.get_shape()\n    noise = random_ops.truncated_normal(gradient_shape) * gradient_noise_scale\n    noisy_gradients.append(gradient + noise)\n  return list(zip(noisy_gradients, variables))\n\n\ndef _multiply_gradients(grads_and_vars, gradient_multipliers):\n  """"""Multiply specified gradients.""""""\n  multiplied_grads_and_vars = []\n  for grad, var in grads_and_vars:\n    if (grad is not None and\n        (var in gradient_multipliers or var.name in gradient_multipliers)):\n      key = var if var in gradient_multipliers else var.name\n      multiplier = gradient_multipliers[key]\n      if isinstance(grad, ops.IndexedSlices):\n        grad_values = grad.values * multiplier\n        grad = ops.IndexedSlices(grad_values, grad.indices, grad.dense_shape)\n      else:\n        grad *= math_ops.cast(multiplier, grad.dtype)\n    multiplied_grads_and_vars.append((grad, var))\n  return multiplied_grads_and_vars\n'"
tensornets/contrib_layers/regularizers.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Regularizers for use with layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numbers\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn\nfrom tensorflow.python.ops import standard_ops\nfrom tensorflow.python.platform import tf_logging as logging\n\n__all__ = [\'l1_regularizer\',\n           \'l2_regularizer\',\n           \'l1_l2_regularizer\',\n           \'sum_regularizer\',\n           \'apply_regularization\']\n\n\ndef l1_regularizer(scale, scope=None):\n  """"""Returns a function that can be used to apply L1 regularization to weights.\n\n  L1 regularization encourages sparsity.\n\n  Args:\n    scale: A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: An optional scope name.\n\n  Returns:\n    A function with signature `l1(weights)` that apply L1 regularization.\n\n  Raises:\n    ValueError: If scale is negative or if scale is not a float.\n  """"""\n  if isinstance(scale, numbers.Integral):\n    raise ValueError(\'scale cannot be an integer: %s\' % scale)\n  if isinstance(scale, numbers.Real):\n    if scale < 0.:\n      raise ValueError(\'Setting a scale less than 0 on a regularizer: %g\' %\n                       scale)\n    if scale == 0.:\n      logging.info(\'Scale of 0 disables regularizer.\')\n      return lambda _: None\n\n  def l1(weights, name=None):\n    """"""Applies L1 regularization to weights.""""""\n    with ops.name_scope(scope, \'l1_regularizer\', [weights]) as name:\n      my_scale = ops.convert_to_tensor(scale,\n                                       dtype=weights.dtype.base_dtype,\n                                       name=\'scale\')\n      return standard_ops.multiply(\n          my_scale,\n          standard_ops.reduce_sum(standard_ops.abs(weights)),\n          name=name)\n\n  return l1\n\n\ndef l2_regularizer(scale, scope=None):\n  """"""Returns a function that can be used to apply L2 regularization to weights.\n\n  Small values of L2 can help prevent overfitting the training data.\n\n  Args:\n    scale: A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: An optional scope name.\n\n  Returns:\n    A function with signature `l2(weights)` that applies L2 regularization.\n\n  Raises:\n    ValueError: If scale is negative or if scale is not a float.\n  """"""\n  if isinstance(scale, numbers.Integral):\n    raise ValueError(\'scale cannot be an integer: %s\' % (scale,))\n  if isinstance(scale, numbers.Real):\n    if scale < 0.:\n      raise ValueError(\'Setting a scale less than 0 on a regularizer: %g.\' %\n                       scale)\n    if scale == 0.:\n      logging.info(\'Scale of 0 disables regularizer.\')\n      return lambda _: None\n\n  def l2(weights):\n    """"""Applies l2 regularization to weights.""""""\n    with ops.name_scope(scope, \'l2_regularizer\', [weights]) as name:\n      my_scale = ops.convert_to_tensor(scale,\n                                       dtype=weights.dtype.base_dtype,\n                                       name=\'scale\')\n      return standard_ops.multiply(my_scale, nn.l2_loss(weights), name=name)\n\n  return l2\n\n\ndef l1_l2_regularizer(scale_l1=1.0, scale_l2=1.0, scope=None):\n  """"""Returns a function that can be used to apply L1 L2 regularizations.\n\n  Args:\n    scale_l1: A scalar multiplier `Tensor` for L1 regularization.\n    scale_l2: A scalar multiplier `Tensor` for L2 regularization.\n    scope: An optional scope name.\n\n  Returns:\n    A function with signature `l1_l2(weights)` that applies a weighted sum of\n    L1 L2 regularization.\n\n  Raises:\n    ValueError: If scale is negative or if scale is not a float.\n  """"""\n  if isinstance(scale_l1, numbers.Integral):\n    raise ValueError(\'scale_l1 cannot be an integer: %s\' % (scale_l1,))\n  if isinstance(scale_l2, numbers.Integral):\n    raise ValueError(\'scale_l2 cannot be an integer: %s\' % (scale_l2,))\n  scope = scope or \'l1_l2_regularizer\'\n  if scale_l1 == 0.:\n    return l2_regularizer(scale_l2, scope)\n  if scale_l2 == 0.:\n    return l1_regularizer(scale_l1, scope)\n  return sum_regularizer([l1_regularizer(scale_l1),\n                          l2_regularizer(scale_l2)],\n                         scope=scope)\n\n\ndef sum_regularizer(regularizer_list, scope=None):\n  """"""Returns a function that applies the sum of multiple regularizers.\n\n  Args:\n    regularizer_list: A list of regularizers to apply.\n    scope: An optional scope name\n\n  Returns:\n    A function with signature `sum_reg(weights)` that applies the\n    sum of all the input regularizers.\n  """"""\n  regularizer_list = [reg for reg in regularizer_list if reg is not None]\n  if not regularizer_list:\n    return None\n\n  def sum_reg(weights):\n    """"""Applies the sum of all the input regularizers.""""""\n    with ops.name_scope(scope, \'sum_regularizer\', [weights]) as name:\n      regularizer_tensors = []\n      for reg in regularizer_list:\n        tensor = reg(weights)\n        if tensor is not None:\n          regularizer_tensors.append(tensor)\n      return math_ops.add_n(\n          regularizer_tensors, name=name) if regularizer_tensors else None\n\n  return sum_reg\n\n\ndef apply_regularization(regularizer, weights_list=None):\n  """"""Returns the summed penalty by applying `regularizer` to the `weights_list`.\n\n  Adding a regularization penalty over the layer weights and embedding weights\n  can help prevent overfitting the training data. Regularization over layer\n  biases is less common/useful, but assuming proper data preprocessing/mean\n  subtraction, it usually shouldn\'t hurt much either.\n\n  Args:\n    regularizer: A function that takes a single `Tensor` argument and returns\n      a scalar `Tensor` output.\n    weights_list: List of weights `Tensors` or `Variables` to apply\n      `regularizer` over. Defaults to the `GraphKeys.WEIGHTS` collection if\n      `None`.\n\n  Returns:\n    A scalar representing the overall regularization penalty.\n\n  Raises:\n    ValueError: If `regularizer` does not return a scalar output, or if we find\n        no weights.\n  """"""\n  if not weights_list:\n    weights_list = ops.get_collection(ops.GraphKeys.WEIGHTS)\n  if not weights_list:\n    raise ValueError(\'No weights to regularize.\')\n  with ops.name_scope(\'get_regularization_penalty\',\n                      values=weights_list) as scope:\n    penalties = [regularizer(w) for w in weights_list]\n    penalties = [\n        p if p is not None else constant_op.constant(0.0) for p in penalties\n    ]\n    for p in penalties:\n      if p.get_shape().ndims != 0:\n        raise ValueError(\'regularizer must return a scalar Tensor instead of a \'\n                         \'Tensor with rank %d.\' % p.get_shape().ndims)\n\n    summed_penalty = math_ops.add_n(penalties, name=scope)\n    ops.add_to_collection(ops.GraphKeys.REGULARIZATION_LOSSES, summed_penalty)\n    return summed_penalty\n'"
tensornets/contrib_layers/rev_block_lib.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Reversible Residual Block.\n\nFrom\n[The Reversible Residual Network: Backpropagation Without Storing\nActivations](https://arxiv.org/abs/1707.04585).\n\nAlso contains the @recompute_grad decorator, which recomputes the forward\nfunction on the backwards pass.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport re\n\nimport numpy as np\nimport six\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom .. import contrib_framework\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops as framework_ops\nfrom tensorflow.python.layers import base\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import custom_gradient\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util import tf_inspect\n\n__all__ = [""rev_block"", ""RevBlock"", ""recompute_grad""]\n\nLAYER_RE = re.compile("".*revlayer_([0-9]*)/([fg])/.*"")\n_USE_DEFAULT = ""__rev_block_lib_default""\n_WRONG_VARS_ERR = """"""\\\nThe variables used on recompute were different than the variables originally\nused. The function wrapped with @recompute_grad likley creates its own variable\nscope with a default name and has been called twice in the same enclosing scope.\nTo fix, ensure each call to the function happens in its own unique variable\nscope.\n""""""\n\n\ndef _acc_grads(*lists_of_grads):\n  """"""Accumulates lists of gradients.""""""\n  acc_grads = []\n  for grads in zip(*lists_of_grads):\n    grads = [g for g in grads if g is not None]\n    if grads:\n      acc_grads.append(math_ops.add_n(grads))\n    else:\n      acc_grads.append(None)\n  return acc_grads\n\n\ndef _rev_layer_forward(xs, f, g, f_side_input, g_side_input,\n                       gate_outputs=False):\n  """"""Forward for 1 reversible layer.""""""\n  x1, x2 = xs\n  y1 = x1 + (f(x2, f_side_input) if f_side_input else f(x2))\n  y2 = x2 + (g(y1, g_side_input) if g_side_input else g(y1))\n  if gate_outputs:\n    return control_flow_ops.tuple([y1, y2])\n  else:\n    return (y1, y2)\n\n\ndef _rev_layer_backward(ys, grad_ys, f, g, f_vars, f_side_input, g_vars,\n                        g_side_input):\n  """"""Backprop for 1 layer.""""""\n  y1, y2 = ys\n  grad_y1, grad_y2 = grad_ys\n\n  # Reconstruct intermediates and inputs (x1, x2)\n  # stop_gradients required on fn inputs to prevent infinite recursion into this\n  # grad function on the calls to gradients.\n  y1_stop = array_ops.stop_gradient(y1)\n  g_side_input = [array_ops.stop_gradient(t) for t in g_side_input]\n  gy1 = g(y1_stop, g_side_input) if g_side_input else g(y1_stop)\n\n  x2 = y2 - gy1\n  x2_stop = array_ops.stop_gradient(x2)\n  f_side_input = [array_ops.stop_gradient(t) for t in f_side_input]\n  fx2 = f(x2_stop, f_side_input) if f_side_input else f(x2_stop)\n\n  x1 = y1 - fx2\n\n  # Compute gradients wrt to inputs\n  # dL/dy2 * dG(y1)/y1\n  grad_gy1_y2 = gradients_impl.gradients(gy1, y1_stop, grad_y2)[0]\n  grad_x1 = grad_y1 + grad_gy1_y2\n  grad_x2 = (\n      gradients_impl.gradients(fx2, x2_stop, grad_y1)[0] + grad_y2 +\n      gradients_impl.gradients(fx2, x2_stop, grad_gy1_y2)[0])\n\n  # Compute gradients wrt to vars and side inputs in f and g\n  grads1 = gradients_impl.gradients(gy1, g_vars + g_side_input, grad_y2)\n  grad_g_vars, grad_g_side = grads1[:len(g_vars)], grads1[len(g_vars):]\n  grads2 = gradients_impl.gradients(fx2, f_vars + f_side_input, grad_y1)\n  grad_f_y1, grad_f_side1 = grads2[:len(f_vars)], grads2[len(f_vars):]\n  grads3 = gradients_impl.gradients(fx2, f_vars + f_side_input, grad_gy1_y2)\n  grad_f_y2, grad_f_side2 = grads3[:len(f_vars)], grads3[len(f_vars):]\n  grad_f_vars = _acc_grads(grad_f_y1, grad_f_y2)\n\n  grad_f_side = _acc_grads(grad_f_side1, grad_f_side2)\n\n  # Put returns in a tuple to ensure a constant memory budget (i.e. don\'t want\n  # the subsequent layer to start computing and consuming memory based on a\n  # subset of these values).\n  outputs = ((x1, x2), (grad_x1, grad_x2), (grad_f_vars, grad_f_side),\n             (grad_g_vars, grad_g_side))\n  tupled = control_flow_ops.tuple(nest.flatten(outputs))\n  return nest.pack_sequence_as(outputs, tupled)\n\n\ndef _rev_block_forward(x1,\n                       x2,\n                       f,\n                       g,\n                       num_layers=1,\n                       f_side_input=None,\n                       g_side_input=None,\n                       gate_outputs=False):\n  """"""Forward for a series of reversible layers.""""""\n  out = (x1, x2)\n  for i in xrange(num_layers):\n    out = _rev_layer_forward(\n        out, f[i], g[i], f_side_input, g_side_input, gate_outputs=gate_outputs)\n\n  y1, y2 = out\n  return y1, y2\n\n\ndef _safe_wraps(fn):\n  if isinstance(fn, functools.partial):\n    # functools.partial objects cannot be wrapped as they are missing the\n    # necessary properties (__name__, __module__, __doc__).\n    def passthrough(f):\n      return f\n    return passthrough\n  return functools.wraps(fn)\n\n\ndef _scope_wrap(fn, scope):\n\n  @_safe_wraps(fn)\n  def wrap(*args, **kwargs):\n    with variable_scope.variable_scope(scope, use_resource=True):\n      return fn(*args, **kwargs)\n\n  return wrap\n\n\nclass RevBlock(base.Layer):\n  """"""Block of reversible layers. See rev_block.""""""\n\n  def __init__(self,\n               f,\n               g,\n               num_layers=1,\n               f_side_input=None,\n               g_side_input=None,\n               use_efficient_backprop=True,\n               name=""revblock"",\n               **kwargs):\n    super(RevBlock, self).__init__(name=name, **kwargs)\n\n    if isinstance(f, list):\n      assert len(f) == num_layers\n    else:\n      f = [f] * num_layers\n\n    if isinstance(g, list):\n      assert len(g) == num_layers\n    else:\n      g = [g] * num_layers\n\n    f = [_scope_wrap(fn, ""revlayer_%d/f"" % i) for i, fn in enumerate(f)]\n    g = [_scope_wrap(fn, ""revlayer_%d/g"" % i) for i, fn in enumerate(g)]\n\n    self.f = f\n    self.g = g\n\n    self.num_layers = num_layers\n    self.f_side_input = f_side_input or []\n    self.g_side_input = g_side_input or []\n\n    self._use_efficient_backprop = use_efficient_backprop\n\n  def call(self, inputs, forward=True):\n    vs = variable_scope.get_variable_scope()\n    vars_before = vs.global_variables()\n\n    if forward:\n      x1, x2 = inputs\n      out = self._forward(x1, x2)\n    else:\n      y1, y2 = inputs\n      out = self._backward(y1, y2)\n\n    # Add any created variables to the Layer\'s variable stores\n    new_vars = vs.global_variables()[len(vars_before):]\n    train_vars = vs.trainable_variables()\n    for new_var in new_vars:\n      if new_var in train_vars:\n        self._trainable_weights.append(new_var)\n      else:\n        self._non_trainable_weights.append(new_var)\n\n    return out\n\n  def forward(self, x1, x2):\n    return self.apply([x1, x2])\n\n  def backward(self, y1, y2):\n    return self.apply([y1, y2], forward=False)\n\n  def build(self, _):\n    logging.warn(""RevBlock constructs its variables on first call, not on ""\n                 ""build."")\n    self.built = True\n\n  def _make_efficient_grad_fn(self, inputs_, ys_):\n    def _efficient_grad_fn(*grad_ys, **kwargs):\n      """"""Custom gradient fn for a block of reversible residual layers.""""""\n      inputs = inputs_\n      ys = ys_\n      variables = kwargs[""variables""]\n      side_inputs = inputs[2:]\n\n      f_side_idxs = [None] * len(self.f_side_input)\n      g_side_idxs = [None] * len(self.g_side_input)\n      assert len(side_inputs) == len(self.f_side_input) + len(self.g_side_input)\n\n      for i, t in enumerate(side_inputs):\n        if t in self.f_side_input:\n          f_side_idxs[self.f_side_input.index(t)] = i\n        elif t in self.g_side_input:\n          g_side_idxs[self.g_side_input.index(t)] = i\n        else:\n          assert False\n\n      f_vars = [[] for _ in range(self.num_layers)]\n      g_vars = [[] for _ in range(self.num_layers)]\n      f_vars_idxs = [[] for _ in range(self.num_layers)]\n      g_vars_idxs = [[] for _ in range(self.num_layers)]\n\n      for i, ref in enumerate(variables):\n        # Use the name to identify the layer number and function (f or g)\n        regex = LAYER_RE.match(ref.name)\n        layer_no = int(regex.group(1))\n        fn_name = regex.group(2)\n        if fn_name == ""f"":\n          f_vars[layer_no].append(ref)\n          f_vars_idxs[layer_no].append(i)\n        else:\n          assert fn_name == ""g""\n          g_vars[layer_no].append(ref)\n          g_vars_idxs[layer_no].append(i)\n\n      f_var_grads = []\n      g_var_grads = []\n      f_side_grads = []\n      g_side_grads = []\n\n      # Reverse variable containers to go backward\n      f_vars.reverse()\n      g_vars.reverse()\n      f = list(self.f)\n      g = list(self.g)\n      f.reverse()\n      g.reverse()\n\n      with variable_scope.variable_scope(self.scope_name, reuse=True):\n        for i in xrange(self.num_layers):\n          ys, grad_ys, f_ret, g_ret = _rev_layer_backward(\n              ys, grad_ys, f[i], g[i], f_vars[i], self.f_side_input, g_vars[i],\n              self.g_side_input)\n\n          grad_f_vars, grad_f_side = f_ret\n          grad_g_vars, grad_g_side = g_ret\n          f_var_grads.append(grad_f_vars)\n          g_var_grads.append(grad_g_vars)\n          f_side_grads.append(grad_f_side)\n          g_side_grads.append(grad_g_side)\n\n      # Accumulate layer gradients for f_side_input and g_side_input\n      acc_f_side_grads = _acc_grads(*f_side_grads)\n      acc_g_side_grads = _acc_grads(*g_side_grads)\n\n      # Use the stored idxs to put gradients in the passed-in order.\n      side_input_grads = [None] * len(side_inputs)\n      variable_grads = [None] * len(variables)\n\n      # Variable gradients were collected in reverse layer order. Reverse to\n      # match idxs.\n      f_var_grads.reverse()\n      g_var_grads.reverse()\n      for idxs, grads in list(zip(f_vars_idxs, f_var_grads)) + list(\n          zip(g_vars_idxs, g_var_grads)):\n        for i, grad in zip(idxs, grads):\n          variable_grads[i] = grad\n\n      for i, grad in zip(f_side_idxs, acc_f_side_grads):\n        side_input_grads[i] = grad\n      for i, grad in zip(g_side_idxs, acc_g_side_grads):\n        side_input_grads[i] = grad\n\n      grad_x1, grad_x2 = grad_ys\n      return [grad_x1, grad_x2] + side_input_grads, variable_grads\n    return _efficient_grad_fn\n\n  def _forward(self, x1, x2):\n    """"""Run forward through the reversible layers.""""""\n\n    side_inputs = [self.f_side_input, self.g_side_input]\n    flat_side_inputs = nest.flatten(side_inputs)\n\n    def _forward_wrap(x1_, x2_, *flat_side_inputs):\n      f_side, g_side = nest.pack_sequence_as(side_inputs, flat_side_inputs)\n      return _rev_block_forward(\n          x1_,\n          x2_,\n          self.f,\n          self.g,\n          num_layers=self.num_layers,\n          f_side_input=f_side,\n          g_side_input=g_side,\n          gate_outputs=self._use_efficient_backprop)\n\n    @custom_gradient.custom_gradient\n    def _forward_with_custom_grad(*args):\n      out = _forward_wrap(*args)  # pylint: disable=no-value-for-parameter\n      grad_fn = self._make_efficient_grad_fn(args, out)\n      return out, grad_fn\n\n    if self._use_efficient_backprop:\n      return _forward_with_custom_grad(x1, x2, *flat_side_inputs)\n    else:\n      return _forward_wrap(x1, x2, *flat_side_inputs)\n\n  def _backward(self, y1, y2):\n    """"""Run backward through the reversible layers.""""""\n\n    f = list(self.f)\n    g = list(self.g)\n    f.reverse()\n    g.reverse()\n\n    for i in xrange(self.num_layers):\n      gy1 = g[i](y1, self.g_side_input) if self.g_side_input else g[i](y1)\n      x2 = y2 - gy1\n      fx2 = f[i](x2, self.f_side_input) if self.f_side_input else f[i](x2)\n      x1 = y1 - fx2\n\n      y1, y2 = x1, x2\n\n    return x1, x2\n\n\ndef rev_block(x1,\n              x2,\n              f,\n              g,\n              num_layers=1,\n              f_side_input=None,\n              g_side_input=None,\n              is_training=True):\n  """"""A block of reversible residual layers.\n\n  A reversible residual layer is defined as:\n\n  ```\n  y1 = x1 + f(x2, f_side_input)\n  y2 = x2 + g(y1, g_side_input)\n  ```\n\n  A reversible residual block, defined here, is a series of reversible residual\n  layers.\n\n  Limitations:\n  * f and g must not close over any Tensors; all side inputs to f and g should\n    be passed in with f_side_input and g_side_input which will be forwarded to\n    f and g.\n  * f and g must not change the dimensionality of their inputs in order for the\n    addition in the equations above to work.\n\n  Args:\n    x1: a float Tensor.\n    x2: a float Tensor.\n    f: a function, (Tensor) -> (Tensor) (or list of such of length num_layers).\n      Should not change the shape of the Tensor. Can make calls to get_variable.\n      See f_side_input if there are side inputs.\n    g: a function, (Tensor) -> (Tensor) (or list of such of length num_layers).\n      Should not change the shape of the Tensor. Can make calls to get_variable.\n      See g_side_input if there are side inputs.\n    num_layers: int, number of reversible residual layers. Each layer will\n      apply f and g according to the equations above, with new variables in each\n      layer.\n    f_side_input: list of Tensors, side input to f. If not None, signature of f\n      should be (Tensor, list<Tensor>) -> (Tensor).\n    g_side_input: list of Tensors, side input to g. If not None, signature of g\n      should be (Tensor, list<Tensor>) -> (Tensor).\n    is_training: bool, whether to actually use the efficient backprop codepath.\n\n  Returns:\n    y1, y2: tuple of float Tensors.\n  """"""\n  block = RevBlock(\n      f=f,\n      g=g,\n      num_layers=num_layers,\n      f_side_input=f_side_input,\n      g_side_input=g_side_input,\n      use_efficient_backprop=is_training,\n      _reuse=variable_scope.get_variable_scope().reuse)\n  return block.forward(x1, x2)\n\n\ndef enable_with_args(dec):\n  """"""A decorator for decorators to enable their usage with or without args.""""""\n\n  @_safe_wraps(dec)\n  def new_dec(*args, **kwargs):\n    if len(args) == 1 and not kwargs and callable(args[0]):\n      # Used as decorator without args\n      fn = args[0]\n      return dec(fn)\n    else:\n      return lambda fn: dec(fn, *args, **kwargs)\n\n  return new_dec\n\n\n@enable_with_args\ndef recompute_grad(fn, use_data_dep=_USE_DEFAULT, tupleize_grads=False):\n  """"""Decorator that recomputes the function on the backwards pass.\n\n  To use this function, you must use `ResourceVariable`s (i.e.\n  `variable_scope(name, use_resource=True), which are the default in Eager mode\n  and when running on TPU.\n\n  Warning: Because the function will be called again on the backwards pass, the\n  user should be careful to not use ops in their function that mutate state or\n  have randomness (for example, batch normalization or dropout). If the function\n  does have such operations, it is recommended that the function take the\n  `is_recomputing` keyword argument which will be `False` on the forward pass\n  and `True` on the backwards pass so that it can disable state changes when\n  `is_recomputing=True` (for example, not updating the moving averages in batch\n  normalization).\n\n  Args:\n    fn: a function that takes Tensors (all as positional arguments) and returns\n      a tuple of Tensors. Note that `fn` should not close over any other\n      Tensors or Variables.\n    use_data_dep: `bool`, if `True` will use a dummy data dependency to force\n      the recompute to happen. If `False` will use a control dependency. By\n      default will be `True` if in an XLA context and `False` otherwise. XLA\n      ignores control dependencies and so this data dependency is necessary.\n    tupleize_grads: `bool`, if `True` will use control dependencies to ensure\n      that all gradients are produced before any are consumed by downstream ops.\n      If `use_data_dep` is also `True`, will use a data dependency instead of\n      a control dependency.\n\n  Returns:\n    A wrapped fn that is identical to fn when called, but its activations will\n    be discarded and recomputed on the backwards pass (i.e. on a call to\n    tf.gradients).\n\n  Raises:\n    ValueError: if `fn` closes over any Tensors or Variables.\n  """"""\n  # Check for closed-over Tensors/Variables\n  if fn.__code__.co_freevars:\n    closed_over_vars = dict(zip(fn.__code__.co_freevars,\n                                [c.cell_contents for c in fn.__closure__]))\n    for var_name, value in six.iteritems(closed_over_vars):\n      if isinstance(value, (framework_ops.Tensor, variables_lib.Variable)):\n        raise ValueError(\n            ""fn decorated with @recompute_grad closes over Tensor %s ""\n            ""(local variable name: %s). The decorated fn must not close over ""\n            ""Tensors or Variables because gradients will NOT be computed for ""\n            ""them through fn. To ensure correct gradients, make the ""\n            ""Tensor an input to fn."" % (value.name, var_name))\n\n  @_safe_wraps(fn)\n  def wrapped(*args):\n    return _recompute_grad(\n        fn, args, use_data_dep=use_data_dep, tupleize_grads=tupleize_grads)\n\n  return wrapped\n\n\ndef _is_on_tpu():\n  ctxt = framework_ops.get_default_graph()._get_control_flow_context()  # pylint: disable=protected-access\n  return control_flow_util.GetContainingXLAContext(ctxt) is not None\n\n\ndef _recomputing_grad_fn(compute_fn,\n                         original_args,\n                         original_vars,\n                         output_grads,\n                         grad_fn_variables,\n                         use_data_dep,\n                         tupleize_grads,\n                         arg_scope,\n                         var_scope,\n                         has_is_recompute_kwarg):\n  """"""Grad fn for recompute_grad.""""""\n  variables = grad_fn_variables or []\n\n  # Identity ops around the inputs ensures correct gradient graph-walking.\n  inputs = [array_ops.identity(x) for x in list(original_args)]\n\n  # Recompute outputs\n  # Use a control dependency to ensure that the recompute is not eliminated by\n  # CSE and that it happens on the backwards pass.\n  ctrl_dep_grads = [g for g in output_grads if g is not None]\n  with framework_ops.control_dependencies(ctrl_dep_grads):\n    if use_data_dep:\n      inputs = _force_data_dependency(output_grads, inputs)\n    # Re-enter scopes\n    with contrib_framework_ops.arg_scope(arg_scope):\n      with variable_scope.variable_scope(var_scope, reuse=True):\n        # Re-call the function and ensure that the touched variables are the\n        # same as in the first call.\n        with backprop.GradientTape() as tape:\n          fn_kwargs = {}\n          if has_is_recompute_kwarg:\n            fn_kwargs[""is_recomputing""] = True\n          outputs = compute_fn(*inputs, **fn_kwargs)\n        recompute_vars = set(tape.watched_variables())\n        if original_vars != recompute_vars:\n          raise ValueError(_WRONG_VARS_ERR)\n\n  if not isinstance(outputs, (list, tuple)):\n    outputs = [outputs]\n  outputs = list(outputs)\n\n  # Compute gradients\n  grads = gradients_impl.gradients(outputs, inputs + variables,\n                                   output_grads)\n\n  if tupleize_grads:\n    if use_data_dep:\n      grads = _tuple_with_data_dep(grads)\n    else:\n      grads = control_flow_ops.tuple(grads)\n\n  grad_inputs = grads[:len(inputs)]\n  grad_vars = grads[len(inputs):]\n  return grad_inputs, grad_vars\n\n\ndef _recompute_grad(fn, args, use_data_dep=_USE_DEFAULT, tupleize_grads=False):\n  """"""See recompute_grad.""""""\n  has_is_recompute_kwarg = ""is_recomputing"" in tf_inspect.getargspec(fn).args\n  for arg in args:\n    if not isinstance(arg, framework_ops.Tensor):\n      raise ValueError(""All inputs to function must be Tensors"")\n  use_data_dep_ = use_data_dep\n  if use_data_dep_ == _USE_DEFAULT:\n    use_data_dep_ = _is_on_tpu()\n\n  # Use custom_gradient and return a grad_fn that recomputes on the backwards\n  # pass.\n  @custom_gradient.custom_gradient\n  def fn_with_recompute(*args):\n    """"""Wrapper for fn.""""""\n    # Capture the variable and arg scopes so we can re-enter them when\n    # recomputing.\n    vs = variable_scope.get_variable_scope()\n    arg_scope = contrib_framework_ops.current_arg_scope()\n    # Track all variables touched in the function.\n    with backprop.GradientTape() as tape:\n      fn_kwargs = {}\n      if has_is_recompute_kwarg:\n        fn_kwargs[""is_recomputing""] = False\n      outputs = fn(*args, **fn_kwargs)\n    original_vars = set(tape.watched_variables())\n\n    def _grad_fn(output_grads, variables=None):\n      # Validate that custom_gradient passes the right variables into grad_fn.\n      if original_vars:\n        assert variables, (""Fn created variables but the variables were not ""\n                           ""passed to the gradient fn."")\n        if set(variables) != original_vars:\n          raise ValueError(_WRONG_VARS_ERR)\n\n      return _recomputing_grad_fn(\n          compute_fn=fn,\n          original_args=args,\n          original_vars=original_vars,\n          output_grads=output_grads,\n          grad_fn_variables=variables,\n          use_data_dep=use_data_dep_,\n          tupleize_grads=tupleize_grads,\n          arg_scope=arg_scope,\n          var_scope=vs,\n          has_is_recompute_kwarg=has_is_recompute_kwarg)\n\n    # custom_gradient inspects the signature of the function to determine\n    # whether the user expects variables passed in the grad_fn. If the function\n    # created variables, the grad_fn should accept the ""variables"" kwarg.\n    if original_vars:\n      def grad_fn(*output_grads, **kwargs):\n        return _grad_fn(output_grads, kwargs[""variables""])\n    else:\n      def grad_fn(*output_grads):\n        return _grad_fn(output_grads)\n\n    return outputs, grad_fn\n\n  return fn_with_recompute(*args)\n\n\ndef _underlying_variable_ref(t):\n  """"""Find the underlying variable ref.\n\n  Traverses through Identity, ReadVariableOp, and Enter ops.\n  Stops when op type has Variable or VarHandle in name.\n\n  Args:\n    t: a Tensor\n\n  Returns:\n    a Tensor that is a variable ref, or None on error.\n  """"""\n  while t.op.type in [""Identity"", ""ReadVariableOp"", ""Enter""]:\n    t = t.op.inputs[0]\n\n  op_type = t.op.type\n  if ""Variable"" in op_type or ""VarHandle"" in op_type:\n    return t\n  else:\n    return None\n\n\ndef _force_data_dependency(first_compute, then_compute):\n  """"""Force all of `then_compute` to depend on all of `first_compute`.\n\n  Uses a dummy data dependency, which is useful when running on TPUs because\n  XLA ignores control dependencies. Only supports float arguments.\n\n  Args:\n    first_compute: `list<Tensor>`. These will be made to run before the\n      `Tensor`s `then_compute`.\n    then_compute: `list<Tensor>`. These will run after all the `Tensor`s in\n      `first_compute`.\n\n  Returns:\n    `list<Tensor>`, same length as `then_compute`.\n\n  Raises:\n    ValueError: if ranks are unknown or types are not floating.\n  """"""\n\n  def _first_element(x):\n    if x.get_shape().ndims is None:\n      raise ValueError(""Rank of Tensor %s must be known"" % x)\n    ndims = x.get_shape().ndims\n    begin = framework_ops.convert_to_tensor([0] * ndims, dtype=dtypes.int32)\n    size = framework_ops.convert_to_tensor([1] * ndims, dtype=dtypes.int32)\n    return array_ops.reshape(array_ops.slice(x, begin, size), [])\n\n  first_compute_sum = math_ops.add_n(\n      [_first_element(x) for x in first_compute if x is not None])\n  dtype = first_compute_sum.dtype\n  if not dtype.is_floating:\n    raise ValueError(""_force_data_dependency only supports floating dtypes."")\n  epsilon = np.finfo(dtype.as_numpy_dtype).tiny\n  zero = array_ops.stop_gradient(epsilon * first_compute_sum)\n\n  return [\n      array_ops.identity(x) + zero if x is not None else None\n      for x in then_compute\n  ]\n\n\ndef _tuple_with_data_dep(tensors):\n  return _force_data_dependency(tensors, tensors)\n'"
tensornets/contrib_layers/summaries.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utility functions for summary creation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport re\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import standard_ops\nfrom tensorflow.python.summary import summary\n\n__all__ = [\n    \'summarize_tensor\',\n    \'summarize_activation\',\n    \'summarize_tensors\',\n    \'summarize_collection\',\n    \'summarize_variables\',\n    \'summarize_weights\',\n    \'summarize_biases\',\n    \'summarize_activations\',\n]\n\n# TODO(wicke): add more unit tests for summarization functions.\n\n\ndef _add_scalar_summary(tensor, tag=None):\n  """"""Add a scalar summary operation for the tensor.\n\n  Args:\n    tensor: The tensor to summarize.\n    tag: The tag to use, if None then use tensor\'s op\'s name.\n\n  Returns:\n    The created histogram summary.\n\n  Raises:\n    ValueError: If the tag is already in use or the rank is not 0.\n  """"""\n  tensor.get_shape().assert_has_rank(0)\n  tag = tag or \'%s_summary\' % tensor.op.name\n  return summary.scalar(tag, tensor)\n\n\ndef _add_histogram_summary(tensor, tag=None):\n  """"""Add a summary operation for the histogram of a tensor.\n\n  Args:\n    tensor: The tensor to summarize.\n    tag: The tag to use, if None then use tensor\'s op\'s name.\n\n  Returns:\n    The created histogram summary.\n\n  Raises:\n    ValueError: If the tag is already in use.\n  """"""\n  tag = tag or \'%s_summary\' % tensor.op.name\n  return summary.histogram(tag, tensor)\n\n\ndef summarize_activation(op):\n  """"""Summarize an activation.\n\n  This applies the given activation and adds useful summaries specific to the\n  activation.\n\n  Args:\n    op: The tensor to summarize (assumed to be a layer activation).\n  Returns:\n    The summary op created to summarize `op`.\n  """"""\n  if op.op.type in (\'Relu\', \'Softplus\', \'Relu6\'):\n    # Using inputs to avoid floating point equality and/or epsilons.\n    _add_scalar_summary(\n        standard_ops.reduce_mean(\n            standard_ops.to_float(\n                standard_ops.less(op.op.inputs[\n                    0], standard_ops.cast(0.0, op.op.inputs[0].dtype)))),\n        \'%s/zeros\' % op.op.name)\n  if op.op.type == \'Relu6\':\n    _add_scalar_summary(\n        standard_ops.reduce_mean(\n            standard_ops.to_float(\n                standard_ops.greater(op.op.inputs[\n                    0], standard_ops.cast(6.0, op.op.inputs[0].dtype)))),\n        \'%s/sixes\' % op.op.name)\n  return _add_histogram_summary(op, \'%s/activation\' % op.op.name)\n\n\ndef summarize_tensor(tensor, tag=None):\n  """"""Summarize a tensor using a suitable summary type.\n\n  This function adds a summary op for `tensor`. The type of summary depends on\n  the shape of `tensor`. For scalars, a `scalar_summary` is created, for all\n  other tensors, `histogram_summary` is used.\n\n  Args:\n    tensor: The tensor to summarize\n    tag: The tag to use, if None then use tensor\'s op\'s name.\n\n  Returns:\n    The summary op created or None for string tensors.\n  """"""\n  # Skips string tensors and boolean tensors (not handled by the summaries).\n  if (tensor.dtype.is_compatible_with(dtypes.string) or\n      tensor.dtype.base_dtype == dtypes.bool):\n    return None\n\n  if tensor.get_shape().ndims == 0:\n    # For scalars, use a scalar summary.\n    return _add_scalar_summary(tensor, tag)\n  else:\n    # We may land in here if the rank is still unknown. The histogram won\'t\n    # hurt if this ends up being a scalar.\n    return _add_histogram_summary(tensor, tag)\n\n\ndef summarize_tensors(tensors, summarizer=summarize_tensor):\n  """"""Summarize a set of tensors.""""""\n  return [summarizer(tensor) for tensor in tensors]\n\n\ndef summarize_collection(collection,\n                         name_filter=None,\n                         summarizer=summarize_tensor):\n  """"""Summarize a graph collection of tensors, possibly filtered by name.""""""\n  tensors = []\n  for op in ops.get_collection(collection):\n    if name_filter is None or re.match(name_filter, op.op.name):\n      tensors.append(op)\n  return summarize_tensors(tensors, summarizer)\n\n\n# Utility functions for commonly used collections\nsummarize_variables = functools.partial(summarize_collection,\n                                        ops.GraphKeys.GLOBAL_VARIABLES)\n\nsummarize_weights = functools.partial(summarize_collection,\n                                      ops.GraphKeys.WEIGHTS)\n\nsummarize_biases = functools.partial(summarize_collection, ops.GraphKeys.BIASES)\n\n\ndef summarize_activations(name_filter=None, summarizer=summarize_activation):\n  """"""Summarize activations, using `summarize_activation` to summarize.""""""\n  return summarize_collection(ops.GraphKeys.ACTIVATIONS, name_filter,\n                              summarizer)\n'"
tensornets/contrib_layers/utils.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common util functions used by layers.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nfrom collections import OrderedDict\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import variables\n\n__all__ = [\'collect_named_outputs\',\n           \'constant_value\',\n           \'static_cond\',\n           \'smart_cond\',\n           \'get_variable_collections\',\n           \'two_element_tuple\',\n           \'n_positive_integers\',\n           \'channel_dimension\',\n           \'last_dimension\']\n\nNamedOutputs = namedtuple(\'NamedOutputs\', [\'name\', \'outputs\'])\n\n\ndef collect_named_outputs(collections, alias, outputs):\n  """"""Add `Tensor` outputs tagged with alias to collections.\n\n  It is useful to collect end-points or tags for summaries. Example of usage:\n\n  logits = collect_named_outputs(\'end_points\', \'inception_v3/logits\', logits)\n  assert \'inception_v3/logits\' in logits.aliases\n\n  Args:\n    collections: A collection or list of collections. If None skip collection.\n    alias: String to append to the list of aliases of outputs, for example,\n           \'inception_v3/conv1\'.\n    outputs: Tensor, an output tensor to collect\n\n  Returns:\n    The outputs Tensor to allow inline call.\n  """"""\n  if collections:\n    append_tensor_alias(outputs, alias)\n    ops.add_to_collections(collections, outputs)\n  return outputs\n\n\ndef append_tensor_alias(tensor, alias):\n  """"""Append an alias to the list of aliases of the tensor.\n\n  Args:\n    tensor: A `Tensor`.\n    alias: String, to add to the list of aliases of the tensor.\n\n  Returns:\n    The tensor with a new alias appended to its list of aliases.\n  """"""\n  # Remove ending \'/\' if present.\n  if alias[-1] == \'/\':\n    alias = alias[:-1]\n  if hasattr(tensor, \'aliases\'):\n    tensor.aliases.append(alias)\n  else:\n    tensor.aliases = [alias]\n  return tensor\n\n\ndef gather_tensors_aliases(tensors):\n  """"""Given a list of tensors, gather their aliases.\n\n  Args:\n    tensors: A list of `Tensors`.\n\n  Returns:\n    A list of strings with the aliases of all tensors.\n  """"""\n  aliases = []\n  for tensor in tensors:\n    aliases += get_tensor_aliases(tensor)\n  return aliases\n\n\ndef get_tensor_aliases(tensor):\n  """"""Get a list with the aliases of the input tensor.\n\n  If the tensor does not have any alias, it would default to its its op.name or\n  its name.\n\n  Args:\n    tensor: A `Tensor`.\n\n  Returns:\n    A list of strings with the aliases of the tensor.\n  """"""\n  if hasattr(tensor, \'aliases\'):\n    aliases = tensor.aliases\n  else:\n    if tensor.name[-2:] == \':0\':\n      # Use op.name for tensor ending in :0\n      aliases = [tensor.op.name]\n    else:\n      aliases = [tensor.name]\n  return aliases\n\n\ndef convert_collection_to_dict(collection, clear_collection=False):\n  """"""Returns an OrderedDict of Tensors with their aliases as keys.\n\n  Args:\n    collection: A collection.\n    clear_collection: When True, it clears the collection after converting to\n      OrderedDict.\n\n  Returns:\n    An OrderedDict of {alias: tensor}\n  """"""\n  output = OrderedDict((alias, tensor)\n                       for tensor in ops.get_collection(collection)\n                       for alias in get_tensor_aliases(tensor))\n  if clear_collection:\n    ops.get_default_graph().clear_collection(collection)\n  return output\n\n\ndef constant_value(value_or_tensor_or_var, dtype=None):\n  """"""Returns value if value_or_tensor_or_var has a constant value.\n\n  Args:\n    value_or_tensor_or_var: A value, a `Tensor` or a `Variable`.\n    dtype: Optional `tf.dtype`, if set it would check it has the right\n      dtype.\n\n  Returns:\n    The constant value or None if it not constant.\n\n  Raises:\n    ValueError: if value_or_tensor_or_var is None or the tensor_variable has the\n    wrong dtype.\n  """"""\n  if value_or_tensor_or_var is None:\n    raise ValueError(\'value_or_tensor_or_var cannot be None\')\n  value = value_or_tensor_or_var\n  if isinstance(value_or_tensor_or_var, (ops.Tensor, variables.Variable)):\n    if dtype and value_or_tensor_or_var.dtype != dtype:\n      raise ValueError(\'It has the wrong type %s instead of %s\' % (\n          value_or_tensor_or_var.dtype, dtype))\n    if isinstance(value_or_tensor_or_var, variables.Variable):\n      value = None\n    else:\n      value = tensor_util.constant_value(value_or_tensor_or_var)\n  return value\n\n\ndef static_cond(pred, fn1, fn2):\n  """"""Return either fn1() or fn2() based on the boolean value of `pred`.\n\n  Same signature as `control_flow_ops.cond()` but requires pred to be a bool.\n\n  Args:\n    pred: A value determining whether to return the result of `fn1` or `fn2`.\n    fn1: The callable to be performed if pred is true.\n    fn2: The callable to be performed if pred is false.\n\n  Returns:\n    Tensors returned by the call to either `fn1` or `fn2`.\n\n  Raises:\n    TypeError: if `fn1` or `fn2` is not callable.\n  """"""\n  if not callable(fn1):\n    raise TypeError(\'fn1 must be callable.\')\n  if not callable(fn2):\n    raise TypeError(\'fn2 must be callable.\')\n  if pred:\n    return fn1()\n  else:\n    return fn2()\n\n\ndef smart_cond(pred, fn1, fn2, name=None):\n  """"""Return either fn1() or fn2() based on the boolean predicate/value `pred`.\n\n  If `pred` is bool or has a constant value it would use `static_cond`,\n  otherwise it would use `tf.cond`.\n\n  Args:\n    pred: A scalar determining whether to return the result of `fn1` or `fn2`.\n    fn1: The callable to be performed if pred is true.\n    fn2: The callable to be performed if pred is false.\n    name: Optional name prefix when using tf.cond\n  Returns:\n    Tensors returned by the call to either `fn1` or `fn2`.\n  """"""\n  pred_value = constant_value(pred)\n  if pred_value is not None:\n    # Use static_cond if pred has a constant value.\n    return static_cond(pred_value, fn1, fn2)\n  else:\n    # Use dynamic cond otherwise.\n    return control_flow_ops.cond(pred, fn1, fn2, name)\n\n\ndef get_variable_collections(variables_collections, name):\n  if isinstance(variables_collections, dict):\n    variable_collections = variables_collections.get(name, None)\n  else:\n    variable_collections = variables_collections\n  return variable_collections\n\n\ndef _get_dimension(shape, dim, min_rank=1):\n  """"""Returns the `dim` dimension of `shape`, while checking it has `min_rank`.\n\n  Args:\n    shape: A `TensorShape`.\n    dim: Integer, which dimension to return.\n    min_rank: Integer, minimum rank of shape.\n\n  Returns:\n    The value of the `dim` dimension.\n\n  Raises:\n    ValueError: if inputs don\'t have at least min_rank dimensions, or if the\n      first dimension value is not defined.\n  """"""\n  dims = shape.dims\n  if dims is None:\n    raise ValueError(\'dims of shape must be known but is None\')\n  if len(dims) < min_rank:\n    raise ValueError(\'rank of shape must be at least %d not: %d\' % (min_rank,\n                                                                    len(dims)))\n  value = dims[dim].value\n  if value is None:\n    raise ValueError(\n        \'dimension %d of shape must be known but is None: %s\' % (dim, shape))\n  return value\n\n\ndef channel_dimension(shape, data_format, min_rank=1):\n  """"""Returns the channel dimension of shape, while checking it has min_rank.\n\n  Args:\n    shape: A `TensorShape`.\n    data_format: `channels_first` or `channels_last`.\n    min_rank: Integer, minimum rank of shape.\n\n  Returns:\n    The value of the first dimension.\n\n  Raises:\n    ValueError: if inputs don\'t have at least min_rank dimensions, or if the\n      first dimension value is not defined.\n  """"""\n  return _get_dimension(shape, 1 if data_format == \'channels_first\' else -1,\n                        min_rank=min_rank)\n\n\ndef last_dimension(shape, min_rank=1):\n  """"""Returns the last dimension of shape while checking it has min_rank.\n\n  Args:\n    shape: A `TensorShape`.\n    min_rank: Integer, minimum rank of shape.\n\n  Returns:\n    The value of the last dimension.\n\n  Raises:\n    ValueError: if inputs don\'t have at least min_rank dimensions, or if the\n      last dimension value is not defined.\n  """"""\n  return _get_dimension(shape, -1, min_rank=min_rank)\n\n\ndef two_element_tuple(int_or_tuple):\n  """"""Converts `int_or_tuple` to height, width.\n\n  Several of the functions that follow accept arguments as either\n  a tuple of 2 integers or a single integer.  A single integer\n  indicates that the 2 values of the tuple are the same.\n\n  This functions normalizes the input value by always returning a tuple.\n\n  Args:\n    int_or_tuple: A list of 2 ints, a single int or a `TensorShape`.\n\n  Returns:\n    A tuple with 2 values.\n\n  Raises:\n    ValueError: If `int_or_tuple` it not well formed.\n  """"""\n  if isinstance(int_or_tuple, (list, tuple)):\n    if len(int_or_tuple) != 2:\n      raise ValueError(\'Must be a list with 2 elements: %s\' % int_or_tuple)\n    return int(int_or_tuple[0]), int(int_or_tuple[1])\n  if isinstance(int_or_tuple, int):\n    return int(int_or_tuple), int(int_or_tuple)\n  if isinstance(int_or_tuple, tensor_shape.TensorShape):\n    if len(int_or_tuple) == 2:\n      return int_or_tuple[0], int_or_tuple[1]\n  raise ValueError(\'Must be an int, a list with 2 elements or a TensorShape of \'\n                   \'length 2\')\n\n\ndef n_positive_integers(n, value):\n  """"""Converts `value` to a sequence of `n` positive integers.\n\n  `value` may be either be a sequence of values convertible to `int`, or a\n  single value convertible to `int`, in which case the resulting integer is\n  duplicated `n` times.  It may also be a TensorShape of rank `n`.\n\n  Args:\n    n: Length of sequence to return.\n    value: Either a single value convertible to a positive `int` or an\n      `n`-element sequence of values convertible to a positive `int`.\n\n  Returns:\n    A tuple of `n` positive integers.\n\n  Raises:\n    TypeError: If `n` is not convertible to an integer.\n    ValueError: If `n` or `value` are invalid.\n  """"""\n\n  n_orig = n\n  n = int(n)\n  if n < 1 or n != n_orig:\n    raise ValueError(\'n must be a positive integer\')\n\n  try:\n    value = int(value)\n  except (TypeError, ValueError):\n    sequence_len = len(value)\n    if sequence_len != n:\n      raise ValueError(\n          \'Expected sequence of %d positive integers, but received %r\' %\n          (n, value))\n    try:\n      values = tuple(int(x) for x in value)\n    except:\n      raise ValueError(\n          \'Expected sequence of %d positive integers, but received %r\' %\n          (n, value))\n    for x in values:\n      if x < 1:\n        raise ValueError(\'expected positive integer, but received %d\' % x)\n    return values\n\n  if value < 1:\n    raise ValueError(\'expected positive integer, but received %d\' % value)\n  return (value,) * n\n'"
tensornets/datasets/__init__.py,0,b'from __future__ import absolute_import\n\nfrom . import coco\nfrom . import imagenet\nfrom . import voc\n'
tensornets/datasets/coco.py,0,"b'""""""Collection of MS COCO utils\n\nThe codes were adapted from [py-faster-rcnn](https://github.com/\nrbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py).\n""""""\nfrom __future__ import division\n\nimport os\nimport json\nimport numpy as np\n\ntry:\n    import cv2\nexcept ImportError:\n    cv2 = None\n\ntry:\n    from pycocotools.coco import COCO\nexcept ImportError:\n    COCO = None\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\nmetas = {}\n\nwith open(os.path.join(os.path.dirname(__file__), \'coco.names\'), \'r\') as f:\n    classnames = [line.rstrip() for line in f.readlines()]\n\n\ndef classidx(classname):\n    return dict((k, i) for (i, k) in enumerate(classnames))[classname]\n\n\ndef area(box):\n    if box.ndim == 1:\n        return (box[2] - box[0] + 1.) * (box[3] - box[1] + 1.)\n    else:\n        return (box[:, 2] - box[:, 0] + 1.) * (box[:, 3] - box[:, 1] + 1.)\n\n\ndef get_files(data_dir, data_name, total_num=None):\n    assert COCO is not None, \'`datasets.coco` requires `pycocotools`.\'\n    if data_name not in metas:\n        metas[data_name] = COCO(""%s/annotations/instances_%s.json"" %\n                                (data_dir, data_name))\n    images = metas[data_name].imgs\n    fileids = images.keys()\n    if total_num is not None:\n        fileids = fileids[:total_num]\n    files = [images[i][\'file_name\'] for i in fileids]\n    return fileids, files\n\n\ndef get_annotations(data_dir, data_name, ids):\n    assert COCO is not None, \'`datasets.coco` requires `pycocotools`.\'\n    if data_name not in metas:\n        metas[data_name] = COCO(""%s/annotations/instances_%s.json"" %\n                                (data_dir, data_name))\n    cmap = dict([(b, a) for (a, b) in enumerate(metas[data_name].getCatIds())])\n    annotations = {}\n    for i in ids:\n        annids = metas[data_name].getAnnIds(imgIds=i, iscrowd=None)\n        objs = metas[data_name].loadAnns(annids)\n        annotations[i] = [[] for _ in range(80)]\n        width = metas[data_name].imgs[i][\'width\']\n        height = metas[data_name].imgs[i][\'height\']\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj_struct = {\'bbox\': [x1, y1, x2, y2]}\n                cidx = cmap[obj[\'category_id\']]\n                annotations[i][cidx].append(obj_struct)\n    return annotations\n\n\ndef load(data_dir, data_name, min_shorter_side=None, max_longer_side=1000,\n         batch_size=1, total_num=None):\n    assert cv2 is not None, \'`load` requires `cv2`.\'\n    _, files = get_files(data_dir, data_name, total_num)\n    total_num = len(files)\n\n    for batch_start in range(0, total_num, batch_size):\n        x = cv2.imread(""%s/%s/%s"" % (data_dir, data_name, files[batch_start]))\n        if min_shorter_side is not None:\n            scale = float(min_shorter_side) / np.min(x.shape[:2])\n        else:\n            scale = 1.0\n        if round(scale * np.max(x.shape[:2])) > max_longer_side:\n            scale = float(max_longer_side) / np.max(x.shape[:2])\n        x = cv2.resize(x, None, None, fx=scale, fy=scale,\n                       interpolation=cv2.INTER_LINEAR)\n        x = np.array([x], dtype=np.float32)\n        scale = np.array([scale], dtype=np.float32)\n        yield x, scale\n        del x\n\n\ndef evaluate_class(ids, scores, boxes, annotations, files, ovthresh):\n    if scores.shape[0] == 0:\n        return 0.0, np.zeros(len(ids)), np.zeros(len(ids))\n\n    # extract gt objects for this class\n    diff = [np.array([0 for obj in annotations[filename]])\n            for filename in files]\n    total = sum([sum(x == 0) for x in diff])\n    detected = dict(zip(files, [[False] * len(x) for x in diff]))\n\n    # sort by confidence\n    sorted_ind = np.argsort(-scores)\n    ids = ids[sorted_ind]\n    boxes = boxes[sorted_ind, :]\n\n    # go down dets and mark TPs and FPs\n    tp_list = []\n    fp_list = []\n    for d in range(len(ids)):\n        actual = np.array([x[\'bbox\'] for x in annotations[ids[d]]])\n        difficult = np.array([0 for x in annotations[ids[d]]])\n\n        if actual.size > 0:\n            iw = np.maximum(np.minimum(actual[:, 2], boxes[d, 2]) -\n                            np.maximum(actual[:, 0], boxes[d, 0]) + 1, 0)\n            ih = np.maximum(np.minimum(actual[:, 3], boxes[d, 3]) -\n                            np.maximum(actual[:, 1], boxes[d, 1]) + 1, 0)\n            inters = iw * ih\n            overlaps = inters / (area(actual) + area(boxes[d, :]) - inters)\n            jmax = np.argmax(overlaps)\n            ovmax = overlaps[jmax]\n        else:\n            ovmax = -np.inf\n\n        tp = 0.\n        fp = 0.\n        if ovmax > ovthresh:\n            if difficult[jmax] == 0:\n                if not detected[ids[d]][jmax]:\n                    tp = 1.\n                    detected[ids[d]][jmax] = True\n                else:\n                    fp = 1.\n        else:\n            fp = 1.\n        tp_list.append(tp)\n        fp_list.append(fp)\n\n    tp = np.cumsum(tp_list)\n    fp = np.cumsum(fp_list)\n    recall = tp / float(total)\n    precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = np.mean([0 if np.sum(recall >= t) == 0\n                  else np.max(precision[recall >= t])\n                  for t in np.linspace(0, 1, 11)])\n\n    return ap, precision, recall\n\n\ndef evaluate(results, data_dir, data_name, ovthresh=0.5, verbose=True):\n    fileids, _ = get_files(data_dir, data_name)\n    fileids = fileids[:len(results)]\n    annotations = get_annotations(data_dir, data_name, fileids)\n    aps = []\n\n    for c in range(80):\n        ids = []\n        scores = []\n        boxes = []\n        for (i, fileid) in enumerate(fileids):\n            pred = results[i][c]\n            if pred.shape[0] > 0:\n                for k in xrange(pred.shape[0]):\n                    ids.append(fileid)\n                    scores.append(pred[k, -1])\n                    boxes.append(pred[k, :4] + 1)\n        ids = np.array(ids)\n        scores = np.array(scores)\n        boxes = np.array(boxes)\n        _annotations = dict((k, v[c]) for (k, v) in annotations.items())\n        ap, _, _ = evaluate_class(ids, scores, boxes, _annotations,\n                                  fileids, ovthresh)\n        aps += [ap]\n\n    strs = \'\'\n    for c in range(80):\n        strs += ""| %6s "" % classnames[c][:6]\n    strs += \'|\\n\'\n\n    for ap in aps:\n        strs += \'|--------\'\n    strs += \'|\\n\'\n\n    for ap in aps:\n        strs += ""| %.4f "" % ap\n    strs += \'|\\n\'\n\n    strs += ""Mean = %.4f"" % np.mean(aps)\n    return strs\n'"
tensornets/datasets/imagenet.py,0,"b'""""""Collection of ImageNet utils\n""""""\nfrom __future__ import absolute_import\n\nimport os\nimport numpy as np\nimport concurrent.futures as cf\n\nfrom os.path import isfile, join\nfrom ..utils import crop, load_img\n\n\ndef get_files(data_dir, data_name, max_rows=None):\n    """"""Reads a \\`data_name.txt\\` (e.g., \\`val.txt\\`) from\n    http://www.image-net.org/challenges/LSVRC/2012/\n    """"""\n    files, labels = np.split(\n        np.genfromtxt(""%s/%s.txt"" % (data_dir, data_name),\n                      dtype=np.str, max_rows=max_rows),\n        [1], axis=1)\n    files = files.flatten()\n    labels = np.asarray(labels.flatten(), dtype=np.int)\n    return files, labels\n\n\ndef get_labels(data_dir, data_name, max_rows=None):\n    _, labels = get_files(data_dir, data_name, max_rows)\n    return labels\n\n\ndef load_single(filename, resize_wh, crop_wh, crop_locs):\n    img = load_img(filename, target_size=resize_wh)\n    return crop(img, crop_wh, crop_locs)\n\n\ndef load(data_dir, data_name, batch_size, resize_wh,\n         crop_locs, crop_wh, total_num=None):\n\n    files, labels = get_files(data_dir, data_name, total_num)\n    total_num = len(labels)\n\n    for batch_start in range(0, total_num, batch_size):\n\n        data_spec = [batch_size, 1, crop_wh, crop_wh, 3]\n        if isinstance(crop_locs, list):\n            data_spec[1] = len(crop_locs)\n        elif crop_locs == 10:\n            data_spec[1] = 10\n        X = np.zeros(data_spec, np.float32)\n\n        jobs = []\n        with cf.ThreadPoolExecutor(max_workers=48) as executor:\n            for (k, f) in enumerate(files[batch_start:batch_start+batch_size]):\n                filename = os.path.join(""%s/ILSVRC2012_img_val"" % data_dir, f)\n                if os.path.isfile(filename):\n                    jobs.append(executor.submit(\n                        load_single, (*(filename, resize_wh, crop_wh, crop_locs))))\n\n        cf.wait(jobs)\n\n        for (k, out) in enumerate(jobs):\n            X[k] = out.result()\n\n        yield X.reshape((-1, crop_wh, crop_wh, 3)), \\\n            labels[batch_start:batch_start+batch_size]\n\n        del X\n'"
tensornets/datasets/voc.py,0,"b'""""""Collection of PASCAL VOC utils\n\nThe codes were refactored from [py-faster-rcnn](https://github.com/\nrbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py) and\n[darkflow](https://github.com/thtrieu/darkflow/blob/master/darkflow/\nnet/yolov2/data.py). Especially, each part was from the following:\n\n1. get_annotations: parse_rec in py-faster-rcnn\n2. evaluate_class: voc_ap in py-faster-rcnn\n3. evaluate: voc_eval in py-faster-rcnn\n4. load_train: _batch in darkflow\n""""""\nfrom __future__ import division\n\nimport os\nimport numpy as np\nimport xml.etree.ElementTree as ET\n\ntry:\n    import cv2\nexcept ImportError:\n    cv2 = None\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\ntry:\n    reduce\nexcept NameError:\n    from functools import reduce\n\n\nwith open(os.path.join(os.path.dirname(__file__), \'voc.names\'), \'r\') as f:\n    classnames = [line.rstrip() for line in f.readlines()]\n\n\ndef classidx(classname):\n    return dict((k, i) for (i, k) in enumerate(classnames))[classname]\n\n\ndef area(box):\n    if box.ndim == 1:\n        return (box[2] - box[0] + 1.) * (box[3] - box[1] + 1.)\n    else:\n        return (box[:, 2] - box[:, 0] + 1.) * (box[:, 3] - box[:, 1] + 1.)\n\n\ndef get_files(data_dir, data_name, total_num=None):\n    with open(""%s/ImageSets/Main/%s.txt"" % (data_dir, data_name)) as f:\n        files = [x.strip() for x in f.readlines()]\n    if total_num is not None:\n        files = files[:total_num]\n    return files\n\n\ndef get_annotations(data_dir, files):\n    annotations = {}\n    for filename in files:\n        tree = ET.parse(""%s/Annotations/%s.xml"" % (data_dir, filename))\n        annotations[filename] = [[] for _ in range(20)]\n        for obj in tree.findall(\'object\'):\n            obj_struct = {}\n            obj_struct[\'name\'] = obj.find(\'name\').text\n            obj_struct[\'pose\'] = obj.find(\'pose\').text\n            obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n            obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n            bbox = obj.find(\'bndbox\')\n            obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                                  int(bbox.find(\'ymin\').text),\n                                  int(bbox.find(\'xmax\').text),\n                                  int(bbox.find(\'ymax\').text)]\n            cidx = classidx(obj_struct[\'name\'])\n            annotations[filename][cidx].append(obj_struct)\n    return annotations\n\n\ndef load(data_dir, data_name, min_shorter_side=None, max_longer_side=1000,\n         batch_size=1, total_num=None):\n    assert cv2 is not None, \'`load` requires `cv2`.\'\n    files = get_files(data_dir, data_name, total_num)\n    total_num = len(files)\n\n    for batch_start in range(0, total_num, batch_size):\n        x = cv2.imread(""%s/JPEGImages/%s.jpg"" % (data_dir, files[batch_start]))\n        if min_shorter_side is not None:\n            scale = float(min_shorter_side) / np.min(x.shape[:2])\n        else:\n            scale = 1.0\n        if round(scale * np.max(x.shape[:2])) > max_longer_side:\n            scale = float(max_longer_side) / np.max(x.shape[:2])\n        x = cv2.resize(x, None, None, fx=scale, fy=scale,\n                       interpolation=cv2.INTER_LINEAR)\n        x = np.array([x], dtype=np.float32)\n        scale = np.array([scale], dtype=np.float32)\n        yield x, scale\n        del x\n\n\ndef evaluate_class(ids, scores, boxes, annotations, files, ovthresh):\n    if scores.shape[0] == 0:\n        return 0.0, np.zeros(len(ids)), np.zeros(len(ids))\n\n    # extract gt objects for this class\n    diff = [np.array([obj[\'difficult\'] for obj in annotations[filename]])\n            for filename in files]\n    total = sum([sum(x == 0) for x in diff])\n    detected = dict(zip(files, [[False] * len(x) for x in diff]))\n\n    # sort by confidence\n    sorted_ind = np.argsort(-scores)\n    ids = ids[sorted_ind]\n    boxes = boxes[sorted_ind, :]\n\n    # go down dets and mark TPs and FPs\n    tp_list = []\n    fp_list = []\n    for d in range(len(ids)):\n        actual = np.array([x[\'bbox\'] for x in annotations[ids[d]]])\n        difficult = np.array([x[\'difficult\'] for x in annotations[ids[d]]])\n\n        if actual.size > 0:\n            iw = np.maximum(np.minimum(actual[:, 2], boxes[d, 2]) -\n                            np.maximum(actual[:, 0], boxes[d, 0]) + 1, 0)\n            ih = np.maximum(np.minimum(actual[:, 3], boxes[d, 3]) -\n                            np.maximum(actual[:, 1], boxes[d, 1]) + 1, 0)\n            inters = iw * ih\n            overlaps = inters / (area(actual) + area(boxes[d, :]) - inters)\n            jmax = np.argmax(overlaps)\n            ovmax = overlaps[jmax]\n        else:\n            ovmax = -np.inf\n\n        tp = 0.\n        fp = 0.\n        if ovmax > ovthresh:\n            if difficult[jmax] == 0:\n                if not detected[ids[d]][jmax]:\n                    tp = 1.\n                    detected[ids[d]][jmax] = True\n                else:\n                    fp = 1.\n        else:\n            fp = 1.\n        tp_list.append(tp)\n        fp_list.append(fp)\n\n    tp = np.cumsum(tp_list)\n    fp = np.cumsum(fp_list)\n    recall = tp / float(total)\n    precision = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = np.mean([0 if np.sum(recall >= t) == 0\n                  else np.max(precision[recall >= t])\n                  for t in np.linspace(0, 1, 11)])\n\n    return ap, precision, recall\n\n\ndef evaluate(results, data_dir, data_name, ovthresh=0.5, verbose=True):\n    files = get_files(data_dir, data_name)\n    files = files[:len(results)]\n    annotations = get_annotations(data_dir, files)\n    aps = []\n\n    for c in range(20):\n        ids = []\n        scores = []\n        boxes = []\n        for (i, filename) in enumerate(files):\n            pred = results[i][c]\n            if pred.shape[0] > 0:\n                for k in xrange(pred.shape[0]):\n                    ids.append(filename)\n                    scores.append(pred[k, -1])\n                    boxes.append(pred[k, :4] + 1)\n        ids = np.array(ids)\n        scores = np.array(scores)\n        boxes = np.array(boxes)\n        _annotations = dict((k, v[c]) for (k, v) in annotations.items())\n        ap, _, _ = evaluate_class(ids, scores, boxes, _annotations,\n                                  files, ovthresh)\n        aps += [ap]\n\n    strs = \'\'\n    for c in range(20):\n        strs += ""| %6s "" % classnames[c][:6]\n    strs += \'|\\n\'\n\n    for ap in aps:\n        strs += \'|--------\'\n    strs += \'|\\n\'\n\n    for ap in aps:\n        strs += ""| %.4f "" % ap\n    strs += \'|\\n\'\n\n    strs += ""Mean = %.4f"" % np.mean(aps)\n    return strs\n\n\ndef load_train(data_dir, data_name,\n               batch_size=64, shuffle=True,\n               target_size=416, anchors=5, classes=20,\n               total_num=None, dtype=np.float32):\n    assert cv2 is not None, \'`load_train` requires `cv2`.\'\n    if isinstance(data_dir, list):\n        files = []\n        annotations = {}\n        for d in data_dir:\n            files.append(get_files(d, data_name, total_num))\n            annotations.update(get_annotations(d, files[-1]))\n        dirs = np.concatenate([i * np.ones(len(f), dtype=np.int)\n                               for (i, f) in enumerate(files)])\n        files = reduce(lambda x, y: x + y, files)\n    else:\n        files = get_files(data_dir, data_name, total_num)\n        annotations = get_annotations(data_dir, files)\n        dirs = np.zeros(len(files), dtype=np.int)\n        data_dir = [data_dir]  # put in list for consistent further processing\n\n    total_num = len(files)\n    for f in files:\n        annotations[f] = reduce(lambda x, y: x + y, annotations[f])\n\n    if isinstance(target_size, int):\n        target_size = (target_size, target_size)\n    feature_size = [x // 32 for x in target_size]\n    cells = feature_size[0] * feature_size[1]\n\n    b = 0\n    while True:\n        if b == 0:\n            if shuffle is True:\n                idx = np.random.permutation(total_num)\n            else:\n                idx = np.arange(total_num)\n        if b + batch_size > total_num:\n            b = 0\n            yield None, None\n        else:\n            batch_num = batch_size\n\n        imgs = np.zeros((batch_num,) + target_size + (3,), dtype=dtype)\n        probs = np.zeros((batch_num, cells, anchors, classes), dtype=dtype)\n        confs = np.zeros((batch_num, cells, anchors), dtype=dtype)\n        coord = np.zeros((batch_num, cells, anchors, 4), dtype=dtype)\n        proid = np.zeros((batch_num, cells, anchors, classes), dtype=dtype)\n        prear = np.zeros((batch_num, cells, 4), dtype=dtype)\n        areas = np.zeros((batch_num, cells, anchors), dtype=dtype)\n        upleft = np.zeros((batch_num, cells, anchors, 2), dtype=dtype)\n        botright = np.zeros((batch_num, cells, anchors, 2), dtype=dtype)\n\n        for i in range(batch_num):\n            d = data_dir[dirs[idx[b + i]]]\n            f = files[idx[b + i]]\n            x = cv2.imread(""%s/JPEGImages/%s.jpg"" % (d, f))\n            h, w = x.shape[:2]\n            cellx = 1. * w / feature_size[1]\n            celly = 1. * h / feature_size[0]\n\n            processed_objs = []\n            for obj in annotations[f]:\n                bbox = obj[\'bbox\']\n                centerx = .5 * (bbox[0] + bbox[2])  # xmin, xmax\n                centery = .5 * (bbox[1] + bbox[3])  # ymin, ymax\n                cx = centerx / cellx\n                cy = centery / celly\n                if cx >= feature_size[1] or cy >= feature_size[0]:\n                    continue\n                processed_objs += [[\n                    classidx(obj[\'name\']),\n                    cx - np.floor(cx),  # centerx\n                    cy - np.floor(cy),  # centery\n                    np.sqrt(float(bbox[2] - bbox[0]) / w),\n                    np.sqrt(float(bbox[3] - bbox[1]) / h),\n                    int(np.floor(cy) * feature_size[1] + np.floor(cx))\n                ]]\n\n            # Calculate placeholders\' values\n            for obj in processed_objs:\n                probs[i, obj[5], :, :] = [[0.] * classes] * anchors\n                probs[i, obj[5], :, obj[0]] = 1.\n                proid[i, obj[5], :, :] = [[1.] * classes] * anchors\n                coord[i, obj[5], :, :] = [obj[1:5]] * anchors\n                prear[i, obj[5], 0] = obj[1] - obj[3]**2 * .5 * feature_size[1]\n                prear[i, obj[5], 1] = obj[2] - obj[4]**2 * .5 * feature_size[0]\n                prear[i, obj[5], 2] = obj[1] + obj[3]**2 * .5 * feature_size[1]\n                prear[i, obj[5], 3] = obj[2] + obj[4]**2 * .5 * feature_size[0]\n                confs[i, obj[5], :] = [1.] * anchors\n\n            # Finalise the placeholders\' values\n            ul = np.expand_dims(prear[i, :, 0:2], 1)\n            br = np.expand_dims(prear[i, :, 2:4], 1)\n            wh = br - ul\n            area = wh[:, :, 0] * wh[:, :, 1]\n            upleft[i, :, :, :] = np.concatenate([ul] * anchors, 1)\n            botright[i, :, :, :] = np.concatenate([br] * anchors, 1)\n            areas[i, :, :] = np.concatenate([area] * anchors, 1)\n\n            imgs[i] = cv2.resize(x, target_size,\n                                 interpolation=cv2.INTER_LINEAR)\n        yield imgs, [probs, confs, coord, proid, areas, upleft, botright]\n        b += batch_size\n'"
tensornets/references/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .yolos import YOLOv3COCO\nfrom .yolos import YOLOv3VOC\nfrom .yolos import YOLOv2COCO\nfrom .yolos import YOLOv2VOC\nfrom .yolos import TinyYOLOv2COCO\nfrom .yolos import TinyYOLOv2VOC\n\nfrom .rcnns import FasterRCNN_ZF_VOC\nfrom .rcnns import FasterRCNN_VGG16_VOC\n'
tensornets/references/rcnns.py,19,"b'""""""Collection of RCNN variants\n\nThe reference paper:\n\n - Faster R-CNN: Towards Real-Time Object Detection\n   with Region Proposal Networks, NIPS 2015\n - Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun\n - https://arxiv.org/abs/1506.01497\n\nThe reference implementation:\n\n1. Caffe and Python utils\n - https://github.com/rbgirshick/py-faster-rcnn\n2. RoI pooling in TensorFlow\n - https://github.com/deepsense-ai/roi-pooling\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom ..layers import conv2d\nfrom ..layers import dropout\nfrom ..layers import flatten\nfrom ..layers import fc\nfrom ..layers import max_pool2d\nfrom ..layers import convrelu as conv\n\nfrom ..ops import *\nfrom ..utils import pad_info\nfrom ..utils import set_args\nfrom ..utils import var_scope\n\nfrom .rpn_utils import filter_boxes\nfrom .rpn_utils import get_anchors\nfrom .rpn_utils import get_boxes\nfrom .rpn_utils import get_shifts\nfrom .rpn_utils import inv_boxes\nfrom .rpn_utils import nms\nfrom .rpn_utils import roi_pooling\n\n\ndef __args__(is_training):\n    return [([conv2d], {\'padding\': \'SAME\', \'activation_fn\': None,\n                        \'scope\': \'conv\'}),\n            ([dropout], {\'is_training\': is_training}),\n            ([fc], {\'activation_fn\': None, \'scope\': \'fc\'}),\n            ([max_pool2d], {\'scope\': \'pool\'})]\n\n\n@var_scope(\'stack\')\ndef _stack(x, filters, blocks, pool_fn=max_pool2d, scope=None):\n    for i in range(1, blocks+1):\n        x = conv(x, filters, 3, scope=str(i))\n    if pool_fn is not None:\n        x = pool_fn(x, 2, stride=2)\n    return x\n\n\n@var_scope(\'rp_net\')\ndef rp_net(x, filters, original_height, original_width, scales,\n           anchors=9, feat_stride=16,\n           nms_thresh=0.7,  # NMS threshold used on RPN proposals\n           pre_nms_topN=6000,  # Number of top scoring boxes to keep before NMS\n           post_nms_topN=300,  # Number of top scoring boxes to keep after NMS\n           min_size=16,  # Minimum of box sizes at original scale\n           scope=None):\n    x = conv(x, filters, 3, padding=\'SAME\', scope=\'0\')\n\n    height = tf.shape(x)[1]\n    width = tf.shape(x)[2]\n\n    x1 = conv2d(x, 2 * anchors, 1, scope=\'logits\')\n    x1 = tf.reshape(x1, (-1, height, width, 2, anchors))\n    x1 = tf.nn.softmax(x1, dim=3)\n    x1 = reshape(x1, (-1, height, width, 2 * anchors), name=\'probs\')\n\n    x2 = conv2d(x, 4 * anchors, 1, scope=\'boxes\')\n\n    # Force the following operations to use CPU\n    # Note that inference time may increase up to 10x without this designation\n    with tf.device(\'cpu:0\'):\n        # Enumerate all shifts\n        shifts = get_shifts(width, height, feat_stride)\n\n        # Enumerate all shifted anchors\n        shifted_anchors = tf.expand_dims(get_anchors(), 0) + \\\n            tf.expand_dims(shifts, 1)\n        shifted_anchors = tf.reshape(shifted_anchors, (-1, 4))\n\n        # Same story for the scores\n        scores = tf.reshape(x1[:, :, :, anchors:],\n                            (-1, height * width * anchors))\n        bbox_deltas = tf.reshape(x2, (-1, height * width * anchors, 4))\n\n        # Convert anchors into proposals via bbox transformations\n        # 2. clip predicted boxes to image\n        proposals = inv_boxes(shifted_anchors, bbox_deltas,\n                              original_height, original_width)\n\n        # 3. remove predicted boxes with either height or width < threshold\n        # (NOTE: convert min_size to input image scale stored in im_info[2])\n        keep = filter_boxes(proposals, min_size * scales[0])\n        scores = gather(scores, keep, axis=1, name=\'filtered/probs\')\n        proposals = gather(proposals, keep, axis=1, name=\'filtered/boxes\')\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take top pre_nms_topN (e.g. 6000)\n        _, order = tf.nn.top_k(scores[0], k=tf.shape(scores)[1])\n        order = order[:pre_nms_topN]\n        scores = gather(scores, order, axis=1, name=\'topk/probs\')\n        proposals = gather(proposals, order, axis=1, name=\'topk/boxes\')\n\n        # 6. apply nms (e.g. threshold = 0.7)\n        # 7. take after_nms_topN (e.g. 300)\n        # 8. return the top proposals (-> RoIs top)\n        keep = nms(proposals[0], scores[0], nms_thresh)\n        keep = keep[:post_nms_topN]\n        scores = gather(scores, keep, axis=1, name=\'nms/probs\')\n        proposals = gather(proposals, keep, axis=1, name=\'nms/boxes\')\n\n    return proposals\n\n\n@var_scope(\'roi_pool\')\ndef roi_pool2d(x, kernel_size, rois, spatial_scale=0.0625, scope=None):\n    rois = tf.cast(tf.round(rois * spatial_scale), dtype=tf.int32)\n    rois = tf.pad(rois[0], [[0, 0], [1, 0]])\n    return roi_pooling(x, rois, kernel_size, kernel_size)\n\n\ndef rcnn(x, stem_fn, roi_pool_fn, is_training, classes,\n         scope=None, reuse=None):\n    x = stem_fn(x)\n    x, rois = roi_pool_fn(x)\n    x = flatten(x)\n    x = fc(x, 4096, scope=\'fc6\')\n    x = relu(x, name=\'relu6\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop6\')\n    x = fc(x, 4096, scope=\'fc7\')\n    x = relu(x, name=\'relu7\')\n    x = dropout(x, keep_prob=0.5, scope=\'drop7\')\n    x = concat([softmax(fc(x, classes, scope=\'logits\'), name=\'probs\'),\n                fc(x, 4 * classes, scope=\'boxes\'),\n                rois], axis=1, name=\'out\')\n    x.get_boxes = get_boxes\n    return x\n\n\n@var_scope(\'REFfasterrcnnZFvoc\')\n@set_args(__args__)\ndef faster_rcnn_zf_voc(x, is_training=False, classes=21,\n                       scope=None, reuse=None):\n    scales = tf.placeholder(tf.float32, [None])\n    height = tf.cast(tf.shape(x)[1], dtype=tf.float32)\n    width = tf.cast(tf.shape(x)[2], dtype=tf.float32)\n\n    def stem_fn(x):\n        x = pad(x, pad_info(7), name=\'pad1\')\n        x = conv(x, 96, 7, stride=2, padding=\'VALID\', scope=\'conv1\')\n        x = srn(x, depth_radius=3, alpha=0.00005, beta=0.75, name=\'srn1\')\n        x = pad(x, pad_info(3, symmetry=False), name=\'pad2\')\n        x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool1\')\n\n        x = pad(x, pad_info(5), name=\'pad3\')\n        x = conv(x, 256, 5, stride=2, padding=\'VALID\', scope=\'conv2\')\n        x = srn(x, depth_radius=3, alpha=0.00005, beta=0.75, name=\'srn2\')\n        x = pad(x, pad_info(3, symmetry=False), name=\'pad4\')\n        x = max_pool2d(x, 3, stride=2, padding=\'VALID\', scope=\'pool2\')\n\n        x = conv(x, 384, 3, scope=\'conv3\')\n        x = conv(x, 384, 3, scope=\'conv4\')\n        x = conv(x, 256, 3, scope=\'conv5\')\n        return x\n\n    def roi_pool_fn(x):\n        rois = rp_net(x, 256, height, width, scales)\n        x = roi_pool2d(x, 6, rois)\n        return x, rois[0] / scales\n\n    x = rcnn(x, stem_fn, roi_pool_fn, is_training, classes, scope, reuse)\n    x.scales = scales\n    return x\n\n\n@var_scope(\'REFfasterrcnnVGG16voc\')\n@set_args(__args__)\ndef faster_rcnn_vgg16_voc(x, is_training=False, classes=21,\n                          scope=None, reuse=None):\n    scales = tf.placeholder(tf.float32, [None])\n    height = tf.cast(tf.shape(x)[1], dtype=tf.float32)\n    width = tf.cast(tf.shape(x)[2], dtype=tf.float32)\n\n    def stem_fn(x):\n        x = _stack(x, 64, 2, scope=\'conv1\')\n        x = _stack(x, 128, 2, scope=\'conv2\')\n        x = _stack(x, 256, 3, scope=\'conv3\')\n        x = _stack(x, 512, 3, scope=\'conv4\')\n        x = _stack(x, 512, 3, pool_fn=None, scope=\'conv5\')\n        return x\n\n    def roi_pool_fn(x):\n        rois = rp_net(x, 512, height, width, scales)\n        x = roi_pool2d(x, 7, rois)\n        return x, rois[0] / scales\n\n    x = rcnn(x, stem_fn, roi_pool_fn, is_training, classes, scope, reuse)\n    x.scales = scales\n    return x\n\n\n# Simple alias.\nFasterRCNN_ZF_VOC = faster_rcnn_zf_voc\nFasterRCNN_VGG16_VOC = faster_rcnn_vgg16_voc\n'"
tensornets/references/rpn_utils.py,40,"b'""""""Collection of region proposal related utils\n\nThe codes were largely taken from the original py-faster-rcnn\n(https://github.com/rbgirshick/py-faster-rcnn), and translated\ninto TensorFlow. Especially, each part was from the following:\n\n1. _whctrs, _mkanchors, _ratio_enum, _scale_enum, get_anchors\n - ${py-faster-rcnn}/lib/rpn/generate_anchors.py\n2. inv_boxes, inv_boxes_np\n - ${py-faster-rcnn}/lib/fast_rcnn/bbox_transform.py\n3. get_shifts, filter_boxes\n - ${py-faster-rcnn}/lib/rpn/proposal_layer.py\n4. nms, nms_np\n - ${py-faster-rcnn}/lib/nms/py_cpu_nms.py\n5. get_boxes\n - ${py-faster-rcnn}/lib/fast_rcnn/test.py\n""""""\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\n\ntry:\n    # installation guide:\n    # $ git clone git@github.com:deepsense-io/roi-pooling.git\n    # $ cd roi-pooling\n    # $ vi roi_pooling/Makefile\n    # (edit according to https://github.com/tensorflow/tensorflow/\n    #                    issues/13607#issuecomment-335530430)\n    # $ python setup.py install\n    from roi_pooling.roi_pooling_ops import roi_pooling\nexcept:\n    def roi_pooling(x, rois, w, h):\n        raise AssertionError(\'`roi_pooling` requires deepsense-ai\\\'s package.\')\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + (w - 1) / 2\n    y_ctr = anchor[1] + (h - 1) / 2\n    return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = (ws - 1) / 2\n    hs = (hs - 1) / 2\n    return tf.stack([\n        x_ctr - ws,\n        y_ctr - hs,\n        x_ctr + ws,\n        y_ctr + hs],\n        axis=-1)\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = tf.round(tf.sqrt(size_ratios))\n    hs = tf.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef get_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = tf.constant(\n        [0, 0, base_size - 1, base_size - 1], dtype=tf.float32)\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = tf.concat(\n        [_scale_enum(ratio_anchors[i, :], scales)\n         for i in xrange(ratio_anchors.shape[0])],\n        axis=0)\n    return anchors\n\n\ndef get_shifts(width, height, feat_stride):\n    shift_x = tf.range(width) * feat_stride\n    shift_y = tf.range(height) * feat_stride\n    shift_x, shift_y = tf.meshgrid(shift_x, shift_y)\n    shift_x = tf.reshape(shift_x, (-1,))\n    shift_y = tf.reshape(shift_y, (-1,))\n    shifts = tf.stack([shift_x, shift_y, shift_x, shift_y], axis=0)\n    shifts = tf.transpose(shifts)\n    return tf.cast(shifts, dtype=tf.float32)\n\n\ndef inv_boxes(boxes, deltas, height, width):\n    w = boxes[:, 2] - boxes[:, 0] + 1.0\n    h = boxes[:, 3] - boxes[:, 1] + 1.0\n    x = boxes[:, 0] + 0.5 * w\n    y = boxes[:, 1] + 0.5 * h\n\n    pred_x = deltas[:, :, 0] * w + x\n    pred_y = deltas[:, :, 1] * h + y\n    pred_w = tf.exp(deltas[:, :, 2]) * w\n    pred_h = tf.exp(deltas[:, :, 3]) * h\n\n    x1 = tf.maximum(tf.minimum(pred_x - 0.5 * pred_w, width - 1), 0)\n    y1 = tf.maximum(tf.minimum(pred_y - 0.5 * pred_h, height - 1), 0)\n    x2 = tf.maximum(tf.minimum(pred_x + 0.5 * pred_w, width - 1), 0)\n    y2 = tf.maximum(tf.minimum(pred_y + 0.5 * pred_h, height - 1), 0)\n\n    return tf.stack([x1, y1, x2, y2], axis=-1)\n\n\ndef inv_boxes_np(boxes, deltas, im_shape):\n    w = boxes[:, 2] - boxes[:, 0] + 1\n    h = boxes[:, 3] - boxes[:, 1] + 1\n    x = boxes[:, 0] + 0.5 * w\n    y = boxes[:, 1] + 0.5 * h\n\n    pred_x = deltas[:, 0::4] * w[:, np.newaxis] + x[:, np.newaxis]\n    pred_y = deltas[:, 1::4] * h[:, np.newaxis] + y[:, np.newaxis]\n    pred_w = np.exp(deltas[:, 2::4]) * w[:, np.newaxis]\n    pred_h = np.exp(deltas[:, 3::4]) * h[:, np.newaxis]\n\n    x1 = np.maximum(np.minimum(pred_x - 0.5 * pred_w, im_shape[1] - 1), 0)\n    y1 = np.maximum(np.minimum(pred_y - 0.5 * pred_h, im_shape[0] - 1), 0)\n    x2 = np.maximum(np.minimum(pred_x + 0.5 * pred_w, im_shape[1] - 1), 0)\n    y2 = np.maximum(np.minimum(pred_y + 0.5 * pred_h, im_shape[0] - 1), 0)\n\n    return np.stack([x1, y1, x2, y2], axis=-1)\n\n\ndef filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[0, :, 2] - boxes[0, :, 0] + 1\n    hs = boxes[0, :, 3] - boxes[0, :, 1] + 1\n    keep = tf.where((ws >= min_size) & (hs >= min_size))[:, 0]\n    return keep\n\n\ndef nms(proposals, scores, thresh):\n    x1 = proposals[:, 0]\n    y1 = proposals[:, 1]\n    x2 = proposals[:, 2]\n    y2 = proposals[:, 3]\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    num = tf.range(tf.shape(scores)[0])\n\n    def body(i, keep, screen):\n        xx1 = tf.maximum(x1[i], x1)\n        yy1 = tf.maximum(y1[i], y1)\n        xx2 = tf.minimum(x2[i], x2)\n        yy2 = tf.minimum(y2[i], y2)\n\n        w = tf.maximum(0.0, xx2 - xx1 + 1)\n        h = tf.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas - inter)\n\n        bools = (ovr <= thresh) & (num >= i) & (screen)\n        i = tf.cond(tf.count_nonzero(bools) > 0,\n                    lambda: tf.cast(tf.where(bools)[0, 0], tf.int32),\n                    lambda: tf.shape(scores)[0])\n\n        return [i, tf.concat([keep, tf.stack([i])], axis=0), bools]\n\n    def condition(i, keep, screen):\n        return i < tf.shape(scores)[0]\n\n    i = tf.constant(0)\n    i, keep, screen = tf.while_loop(\n        condition, body, [i, tf.stack([i]), num >= 0],\n        shape_invariants=[tf.TensorShape([]),\n                          tf.TensorShape([None, ]),\n                          tf.TensorShape([None, ])],\n        back_prop=False)\n\n    return keep[:-1]\n\n\ndef nms_np(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef get_boxes(outs, im_shape, max_per_image=100, thresh=0.05, nmsth=0.3):\n    classes = (outs.shape[1] - 4) // 5 - 1\n    scores, boxes, rois = np.split(outs, [classes + 1, -4], axis=1)\n    pred_boxes = inv_boxes_np(rois, boxes, im_shape)\n    objs = []\n    total_boxes = 0\n    for j in xrange(1, classes + 1):\n        inds = np.where(scores[:, j] > thresh)[0]\n        cls_scores = scores[inds, j]\n        cls_boxes = pred_boxes[inds, j]\n        cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis]))\n        keep = nms_np(cls_dets, nmsth)\n        cls_dets = cls_dets[keep, :]\n        objs.append(cls_dets)\n        total_boxes += cls_dets.shape[0]\n\n    if max_per_image > 0 and total_boxes > max_per_image:\n        image_scores = np.hstack([objs[j][:, -1] for j in xrange(classes)])\n        image_thresh = np.sort(image_scores)[-max_per_image]\n        for j in xrange(classes):\n            keep = np.where(objs[j][:, -1] >= image_thresh)[0]\n            objs[j] = objs[j][keep, :]\n\n    return objs\n'"
tensornets/references/yolo_utils.py,35,"b""from __future__ import absolute_import\nfrom __future__ import division\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..version_utils import tf_later_than\n\ntry:\n    from .darkflow_utils.get_boxes import yolov3_box\n    from .darkflow_utils.get_boxes import yolov2_box\nexcept ImportError:\n    yolov3_box = None\n    yolov2_box = None\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\n\nif tf_later_than('1.14'):\n    tf = tf.compat.v1\n\n\nwith open(os.path.join(os.path.dirname(__file__), 'coco.names'), 'r') as f:\n    labels_coco = [line.rstrip() for line in f.readlines()]\n\nwith open(os.path.join(os.path.dirname(__file__), 'voc.names'), 'r') as f:\n    labels_voc = [line.rstrip() for line in f.readlines()]\n\nbases = dict()\nbases['yolov3'] = {'anchors': [10., 13., 16., 30., 33., 23., 30., 61.,\n                               62., 45., 59., 119., 116., 90., 156., 198.,\n                               373., 326.]}\nbases['yolov3coco'] = bases['yolov3']\nbases['yolov3voc'] = bases['yolov3']\nbases['yolov2'] = {'anchors': [0.57273, 0.677385, 1.87446, 2.06253, 3.33843,\n                               5.47434, 7.88282, 3.52778, 9.77052, 9.16828]}\nbases['yolov2voc'] = {'anchors': [1.3221, 1.73145, 3.19275, 4.00944, 5.05587,\n                                  8.09892, 9.47112, 4.84053, 11.2364, 10.0071]}\nbases['tinyyolov2voc'] = {'anchors': [1.08, 1.19, 3.42, 4.41, 6.63,\n                                      11.38, 9.42, 5.11, 16.62, 10.52]}\n\n\ndef opts(model_name):\n    opt = bases[model_name].copy()\n    opt.update({'num': len(opt['anchors']) // 2})\n    if 'voc' in model_name:\n        opt.update({'classes': len(labels_voc), 'labels': labels_voc})\n    else:\n        opt.update({'classes': len(labels_coco), 'labels': labels_coco})\n    return opt\n\n\ndef parse_box(b, t, w, h):\n    idx = np.argmax(b.probs)\n    score = b.probs[idx]\n    if score > t:\n        try:\n            x1 = int((b.x - b.w / 2) * w)\n            y1 = int((b.y - b.h / 2) * h)\n            x2 = int((b.x + b.w / 2) * w)\n            y2 = int((b.y + b.h / 2) * h)\n            if x1 < 0:\n                x1 = 0\n            if x2 > w - 1:\n                x2 = w - 1\n            if y1 < 0:\n                y1 = 0\n            if y2 > h - 1:\n                y2 = h - 1\n            return idx, (x1, y1, x2, y2, score)\n        except:\n            return None, None\n    else:\n        return None, None\n\n\ndef get_v3_boxes(opts, outs, source_size, threshold=0.1):\n    h, w = source_size\n    boxes = [[] for _ in xrange(opts['classes'])]\n    opts['thresh'] = threshold\n    results = yolov3_box(opts,\n                         np.array(outs[0][0], dtype=np.float32),\n                         np.array(outs[1][0], dtype=np.float32),\n                         np.array(outs[2][0], dtype=np.float32))\n    for b in results:\n        idx, box = parse_box(b, threshold, w, h)\n        if idx is not None:\n            boxes[idx].append(box)\n    for i in xrange(opts['classes']):\n        boxes[i] = np.asarray(boxes[i], dtype=np.float32)\n    return boxes\n\n\ndef get_v2_boxes(opts, outs, source_size, threshold=0.1):\n    h, w = source_size\n    boxes = [[] for _ in xrange(opts['classes'])]\n    opts['thresh'] = threshold\n    results = yolov2_box(opts, np.array(outs[0], dtype=np.float32))\n    for b in results:\n        idx, box = parse_box(b, threshold, w, h)\n        if idx is not None:\n            boxes[idx].append(box)\n    for i in xrange(opts['classes']):\n        boxes[i] = np.asarray(boxes[i], dtype=np.float32)\n    return boxes\n\n\ndef v2_inputs(out_shape, anchors, classes, dtype):\n    sizes = [None, np.prod(out_shape), anchors]\n    return [tf.placeholder(dtype, sizes + [classes], name='probs'),\n            tf.placeholder(dtype, sizes, name='confs'),\n            tf.placeholder(dtype, sizes + [4], name='coord'),\n            tf.placeholder(dtype, sizes + [classes], name='proid'),\n            tf.placeholder(dtype, sizes, name='areas'),\n            tf.placeholder(dtype, sizes + [2], name='upleft'),\n            tf.placeholder(dtype, sizes + [2], name='botright')]\n\n\ndef v2_loss(outs, anchorcoords, classes):\n    # Refer to the following darkflow loss\n    # https://github.com/thtrieu/darkflow/blob/master/darkflow/net/yolov2/train.py\n    sprob = 1.\n    sconf = 5.\n    snoob = 1.\n    scoor = 1.\n    H = int(outs.shape[1]) if tf_later_than('2') else outs.shape[1].value\n    W = int(outs.shape[2]) if tf_later_than('2') else outs.shape[2].value\n    cells = H * W\n    sizes = np.array([[[[W, H]]]], dtype=np.float32)\n    anchors = len(anchorcoords) // 2\n    anchorcoords = np.reshape(anchorcoords, [1, 1, anchors, 2])\n    _, _probs, _confs, _coord, _proid, _areas, _ul, _br = outs.inputs[:8]\n\n    # Extract the coordinate prediction from net.out\n    outs = tf.reshape(outs, [-1, H, W, anchors, (5 + classes)])\n    coords = tf.reshape(outs[:, :, :, :, :4], [-1, cells, anchors, 4])\n    adj_xy = 1. / (1. + tf.exp(-coords[:, :, :, 0:2]))\n    adj_wh = tf.sqrt(tf.exp(coords[:, :, :, 2:4]) * anchorcoords / sizes)\n    adj_c = 1. / (1. + tf.exp(-outs[:, :, :, :, 4]))\n    adj_c = tf.reshape(adj_c, [-1, cells, anchors, 1])\n    adj_prob = tf.reshape(tf.nn.softmax(outs[:, :, :, :, 5:]),\n                          [-1, cells, anchors, classes])\n    adj_outs = tf.concat([adj_xy, adj_wh, adj_c, adj_prob], 3)\n\n    coords = tf.concat([adj_xy, adj_wh], 3)\n    wh = tf.pow(coords[:, :, :, 2:4], 2) * sizes\n    area_pred = wh[:, :, :, 0] * wh[:, :, :, 1]\n    centers = coords[:, :, :, 0:2]\n    floor = centers - (wh * .5)\n    ceil = centers + (wh * .5)\n\n    # calculate the intersection areas\n    intersect_upleft = tf.maximum(floor, _ul)\n    intersect_botright = tf.minimum(ceil, _br)\n    intersect_wh = intersect_botright - intersect_upleft\n    intersect_wh = tf.maximum(intersect_wh, 0.0)\n    intersect = tf.multiply(intersect_wh[:, :, :, 0], intersect_wh[:, :, :, 1])\n\n    # calculate the best IOU, set 0.0 confidence for worse boxes\n    iou = tf.truediv(intersect, _areas + area_pred - intersect)\n    best_box = tf.equal(iou, tf.reduce_max(iou, [2], True))\n    best_box = tf.to_float(best_box)\n    confs = tf.multiply(best_box, _confs)\n\n    # take care of the weight terms\n    conid = snoob * (1. - confs) + sconf * confs\n    weight_coo = tf.concat(4 * [tf.expand_dims(confs, -1)], 3)\n    cooid = scoor * weight_coo\n    weight_pro = tf.concat(classes * [tf.expand_dims(confs, -1)], 3)\n    proid = sprob * weight_pro\n\n    true = tf.concat([_coord, tf.expand_dims(confs, 3), _probs], 3)\n    wght = tf.concat([cooid, tf.expand_dims(conid, 3), proid], 3)\n\n    loss = tf.pow(adj_outs - true, 2)\n    loss = tf.multiply(loss, wght)\n    loss = tf.reshape(loss, [-1, cells * anchors * (5 + classes)])\n    loss = tf.reduce_sum(loss, 1)\n    return .5 * tf.reduce_mean(loss) + tf.losses.get_regularization_loss()\n"""
tensornets/references/yolos.py,4,"b'""""""Collection of YOLO variants\n\nThe reference papers:\n\n1. YOLO9000\n - YOLO9000: Better, Faster, Stronger, CVPR 2017 (Best Paper Honorable Mention)\n - Joseph Redmon, Ali Farhadi\n - https://arxiv.org/abs/1612.08242\n2. YOLOv3\n - YOLOv3: An Incremental Improvement\n - Joseph Redmon, Ali Farhadi\n - https://pjreddie.com/media/files/papers/YOLOv3.pdf\n\nThe reference implementations:\n\n1. Darknet\n - https://pjreddie.com/darknet/yolo/\n2. darkflow\n - https://github.com/thtrieu/darkflow\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom ..layers import batch_norm\nfrom ..layers import bias_add\nfrom ..layers import conv2d\nfrom ..layers import darkconv as conv\nfrom ..layers import max_pool2d\n\nfrom ..ops import *\nfrom ..utils import pad_info\nfrom ..utils import set_args\nfrom ..utils import var_scope\n\nfrom .yolo_utils import opts\nfrom .yolo_utils import get_v3_boxes\nfrom .yolo_utils import get_v2_boxes\nfrom .yolo_utils import v2_inputs\nfrom .yolo_utils import v2_loss\n\n\ndef __args__(is_training):\n    return [([batch_norm], {\'is_training\': is_training}),\n            ([bias_add, conv2d], {}),\n            ([max_pool2d], {\'padding\': \'SAME\'})]\n\n\n@var_scope(\'stack\')\ndef _stack(x, filters, blocks, scope=None):\n    for i in range(1, blocks+1):\n        if i % 2 > 0:\n            x = conv(x, filters, 3, scope=str(i))\n        else:\n            x = conv(x, filters // 2, 1, scope=str(i))\n    return x\n\n\n@var_scope(\'stackv3\')\ndef stackv3(x, filters, blocks, kernel_size=3,\n            conv_shortcut=True, scope=None):\n    for i in range(1, blocks+1):\n        shortcut = x\n        p = conv(x, filters // 2, 1, scope=""%d/1"" % i)\n        x = conv(p, filters, kernel_size, scope=""%d/2"" % i)\n        if conv_shortcut is True:\n            x = add(shortcut, x, name=""%d/out"" % i)\n    if conv_shortcut is True:\n        return x\n    else:\n        return x, p\n\n\n@var_scope(\'down\')\ndef down(x, filters, kernel_size=3, scope=None):\n    x = pad(x, pad_info(kernel_size), name=\'pad\')\n    x = conv(x, filters, kernel_size, stride=2,\n             padding=\'VALID\', scope=\'conv\')\n    return x\n\n\n@var_scope(\'up\')\ndef up(x, filters, kernel_size=2, scope=None):\n    x = conv(x, filters, 1, scope=\'conv\')\n    x = upsample(x, kernel_size, name=\'upsample\')\n    return x\n\n\ndef yolov3(x, blocks, is_training, classes, scope=None, reuse=None):\n    x = conv(x, 32, 3, scope=\'conv1\')\n    x = down(x, 64, scope=\'down1\')\n    x = stackv3(x, 64, blocks[0], scope=\'conv2\')\n    x = down(x, 128, scope=\'down2\')\n    x = stackv3(x, 128, blocks[1], scope=\'conv3\')\n    x = down(x, 256, scope=\'down3\')\n    x = p0 = stackv3(x, 256, blocks[2], scope=\'conv4\')\n    x = down(x, 512, scope=\'down4\')\n    x = p1 = stackv3(x, 512, blocks[3], scope=\'conv5\')\n    x = down(x, 1024, scope=\'down5\')\n    x = stackv3(x, 1024, blocks[4], scope=\'conv6\')\n\n    x, p = stackv3(x, 1024, blocks[5], conv_shortcut=False, scope=\'conv7\')\n    out0 = conv(x, (classes + 5) * 3, 1, onlyconv=True, scope=\'linear7\')\n    p = up(p, 256, 2, scope=\'up7\')\n    x = concat([p, p1], axis=3, name=\'concat7\')\n\n    x, p = stackv3(x, 512, blocks[5], conv_shortcut=False, scope=\'conv8\')\n    out1 = conv(x, (classes + 5) * 3, 1, onlyconv=True, scope=\'linear8\')\n    p = up(p, 128, 2, scope=\'up8\')\n    x = concat([p, p0], axis=3, name=\'concat8\')\n\n    x, _ = stackv3(x, 256, blocks[5], conv_shortcut=False, scope=\'conv9\')\n    out2 = conv(x, (classes + 5) * 3, 1, onlyconv=True, scope=\'linear9\')\n    out2.aliases = []\n    out2.preds = [out0, out1, out2]\n    return out2\n\n\ndef yolo(x, blocks, is_training, classes, scope=None, reuse=None):\n    x = _stack(x, 32, blocks[0], scope=\'conv1\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool1\')\n    x = _stack(x, 64, blocks[1], scope=\'conv2\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool2\')\n    x = _stack(x, 128, blocks[2], scope=\'conv3\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool3\')\n    x = _stack(x, 256, blocks[3], scope=\'conv4\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool4\')\n    x = p = _stack(x, 512, blocks[4], scope=\'conv5\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool5\')\n    x = _stack(x, 1024, blocks[5], scope=\'conv6\')\n\n    x = conv(x, 1024, 3, scope=\'conv7\')\n    x = conv(x, 1024, 3, scope=\'conv8\')\n\n    p = conv(p, 64, 1, scope=\'conv5a\')\n    p = local_flatten(p, 2, name=\'flat5a\')\n\n    x = concat([p, x], axis=3, name=\'concat\')\n    x = conv(x, 1024, 3, scope=\'conv9\')\n    x = conv(x, (classes + 5) * 5, 1, onlyconv=True, scope=\'linear\')\n    x.aliases = []\n    return x\n\n\ndef tinyyolo(x, is_training, classes, scope=None, reuse=None):\n    x = conv(x, 16, 3, scope=\'conv1\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool1\')\n    x = conv(x, 32, 3, scope=\'conv2\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool2\')\n    x = conv(x, 64, 3, scope=\'conv3\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool3\')\n    x = conv(x, 128, 3, scope=\'conv4\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool4\')\n    x = conv(x, 256, 3, scope=\'conv5\')\n    x = max_pool2d(x, 2, stride=2, scope=\'pool5\')\n    x = conv(x, 512, 3, scope=\'conv6\')\n\n    x = max_pool2d(x, 2, stride=1, scope=\'pool6\')\n    x = conv(x, 1024, 3, scope=\'conv7\')\n    x = conv(x, 1024 if classes == 20 else 512, 3, scope=\'conv8\')\n    x = conv(x, (classes + 5) * 5, 1, onlyconv=True, scope=\'linear\')\n    x.aliases = []\n    return x\n\n\n@var_scope(\'REFyolov3coco\')\n@set_args(__args__)\ndef yolov3coco(x, is_training=False, classes=80, scope=None, reuse=None):\n    def _get_boxes(*args, **kwargs):\n        return get_v3_boxes(opts(\'yolov3\'), *args, **kwargs)\n    x = yolov3(x, [1, 2, 8, 8, 4, 3], is_training, classes, scope, reuse)\n    x.get_boxes = _get_boxes\n    return x\n\n\n@var_scope(\'REFyolov3voc\')\n@set_args(__args__)\ndef yolov3voc(x, is_training=False, classes=20, scope=None, reuse=None):\n    def _get_boxes(*args, **kwargs):\n        return get_v3_boxes(opts(\'yolov3voc\'), *args, **kwargs)\n    x = yolov3(x, [1, 2, 8, 8, 4, 3], is_training, classes, scope, reuse)\n    x.get_boxes = _get_boxes\n    return x\n\n\n@var_scope(\'REFyolov2coco\')\n@set_args(__args__)\ndef yolov2coco(x, is_training=False, classes=80, scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'yolov2\')\n    x = yolo(x, [1, 1, 3, 3, 5, 5], is_training, classes, scope, reuse)\n\n    def _get_boxes(*args, **kwargs):\n        return get_v2_boxes(opt, *args, **kwargs)\n    x.get_boxes = _get_boxes\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\n@var_scope(\'REFyolov2voc\')\n@set_args(__args__)\ndef yolov2voc(x, is_training=False, classes=20, scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'yolov2voc\')\n    x = yolo(x, [1, 1, 3, 3, 5, 5], is_training, classes, scope, reuse)\n\n    def _get_boxes(*args, **kwargs):\n        return get_v2_boxes(opt, *args, **kwargs)\n    x.get_boxes = _get_boxes\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\n@var_scope(\'REFtinyyolov2coco\')\n@set_args(__args__)\ndef tinyyolov2coco(x, is_training=False, classes=80, scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'tinyyolov2\')\n    x = tinyyolo(x, is_training, classes, scope, reuse)\n\n    def _get_boxes(*args, **kwargs):\n        return get_v2_boxes(opt, *args, **kwargs)\n    x.get_boxes = _get_boxes\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\n@var_scope(\'REFtinyyolov2voc\')\n@set_args(__args__)\ndef tinyyolov2voc(x, is_training=False, classes=20, scope=None, reuse=None):\n    inputs = x\n    opt = opts(\'tinyyolov2voc\')\n    x = tinyyolo(x, is_training, classes, scope, reuse)\n\n    def _get_boxes(*args, **kwargs):\n        return get_v2_boxes(opt, *args, **kwargs)\n    x.get_boxes = _get_boxes\n    x.inputs = [inputs]\n    x.inputs += v2_inputs(x.shape[1:3], opt[\'num\'], classes, x.dtype)\n    if isinstance(is_training, tf.Tensor):\n        x.inputs.append(is_training)\n    x.loss = v2_loss(x, opt[\'anchors\'], classes)\n    return x\n\n\n# Simple alias.\nYOLOv3COCO = yolov3coco\nYOLOv3VOC = yolov3voc\nYOLOv2COCO = yolov2coco\nYOLOv2VOC = yolov2voc\nTinyYOLOv2COCO = tinyyolov2coco\nTinyYOLOv2VOC = tinyyolov2voc\n'"
tensornets/references/darkflow_utils/__init__.py,0,"b'""""""Collection of darkflow utils\n\nThe codes were copied without modification from the original darkflow\n(https://github.com/thtrieu/darkflow), and each module was from the following:\n\n1. nms\n - ${darkflow}/darkflow/cython_utils/nms.pyx\n2. get_boxes\n - ${darkflow}/darkflow/cython_utils/cy_yolo2_findboxes.pyx\n\nAdditionally, `yolov3_box` was adapted from `yolov2_box` by taehoonlee.\n""""""\nfrom __future__ import absolute_import\n\ntry:\n    from . import get_boxes\nexcept ImportError:\n    class emptyboxes:\n        yolov3_box = None\n        yolov2_box = None\n    get_boxes = emptyboxes()\n'"
tensornets/references/darkflow_utils/box.py,0,"b'import numpy as np\n\n\nclass BoundBox:\n    def __init__(self, classes):\n        self.x, self.y = float(), float()\n        self.w, self.h = float(), float()\n        self.c = float()\n        self.class_num = classes\n        self.probs = np.zeros((classes,))\n\n\ndef overlap(x1, w1, x2, w2):\n    l1 = x1 - w1 / 2.\n    l2 = x2 - w2 / 2.\n    left = max(l1, l2)\n    r1 = x1 + w1 / 2.\n    r2 = x2 + w2 / 2.\n    right = min(r1, r2)\n    return right - left\n\n\ndef box_intersection(a, b):\n    w = overlap(a.x, a.w, b.x, b.w)\n    h = overlap(a.y, a.h, b.y, b.h)\n    if w < 0 or h < 0: return 0\n    area = w * h\n    return area\n\n\ndef box_union(a, b):\n    i = box_intersection(a, b)\n    u = a.w * a.h + b.w * b.h - i\n    return u\n\n\ndef box_iou(a, b):\n    return box_intersection(a, b) / box_union(a, b)\n\n\ndef prob_compare(box):\n    return box.probs[box.class_num]\n\n\ndef prob_compare2(boxa, boxb):\n    if (boxa.pi < boxb.pi):\n        return 1\n    elif(boxa.pi == boxb.pi):\n        return 0\n    else:\n        return -1\n'"
