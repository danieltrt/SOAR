file_path,api_count,code
Cifar10/Densenet_Cifar10.py,34,"b'import tensorflow as tf\nfrom tflearn.layers.conv import global_avg_pool\nfrom tensorflow.contrib.layers import batch_norm, flatten\nfrom tensorflow.contrib.layers import xavier_initializer\nfrom tensorflow.contrib.framework import arg_scope\nfrom cifar10 import *\n\n# Hyperparameter\ngrowth_k = 24\nnb_block = 2 # how many (dense block + Transition Layer) ?\ninit_learning_rate = 1e-4\nepsilon = 1e-4 # AdamOptimizer epsilon\ndropout_rate = 0.2\n\n# Momentum Optimizer will use\nnesterov_momentum = 0.9\nweight_decay = 1e-4\n\n# Label & batch_size\nbatch_size = 64\n\niteration = 782\n# batch_size * iteration = data_set_number\n\ntest_iteration = 10\n\ntotal_epochs = 300\n\ndef conv_layer(input, filter, kernel, stride=1, layer_name=""conv""):\n    with tf.name_scope(layer_name):\n        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=\'SAME\')\n        return network\n\ndef Global_Average_Pooling(x, stride=1):\n    """"""\n    width = np.shape(x)[1]\n    height = np.shape(x)[2]\n    pool_size = [width, height]\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter\n    It is global average pooling without tflearn\n    """"""\n\n    return global_avg_pool(x, name=\'Global_avg_pooling\')\n    # But maybe you need to install h5py and curses or not\n\n\ndef Batch_Normalization(x, training, scope):\n    with arg_scope([batch_norm],\n                   scope=scope,\n                   updates_collections=None,\n                   decay=0.9,\n                   center=True,\n                   scale=True,\n                   zero_debias_moving_mean=True) :\n        return tf.cond(training,\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n\ndef Drop_out(x, rate, training) :\n    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n\ndef Relu(x):\n    return tf.nn.relu(x)\n\ndef Average_pooling(x, pool_size=[2,2], stride=2, padding=\'VALID\'):\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\n\ndef Max_Pooling(x, pool_size=[3,3], stride=2, padding=\'VALID\'):\n    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Concatenation(layers) :\n    return tf.concat(layers, axis=3)\n\ndef Linear(x) :\n    return tf.layers.dense(inputs=x, units=class_num, name=\'linear\')\n\ndef Evaluate(sess):\n    test_acc = 0.0\n    test_loss = 0.0\n    test_pre_index = 0\n    add = 1000\n\n    for it in range(test_iteration):\n        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n        test_pre_index = test_pre_index + add\n\n        test_feed_dict = {\n            x: test_batch_x,\n            label: test_batch_y,\n            learning_rate: epoch_learning_rate,\n            training_flag: False\n        }\n\n        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n\n        test_loss += loss_ / 10.0\n        test_acc += acc_ / 10.0\n\n    summary = tf.Summary(value=[tf.Summary.Value(tag=\'test_loss\', simple_value=test_loss),\n                                tf.Summary.Value(tag=\'test_accuracy\', simple_value=test_acc)])\n\n    return test_acc, test_loss, summary\n\nclass DenseNet():\n    def __init__(self, x, nb_blocks, filters, training):\n        self.nb_blocks = nb_blocks\n        self.filters = filters\n        self.training = training\n        self.model = self.Dense_net(x)\n\n\n    def bottleneck_layer(self, x, scope):\n        # print(x)\n        with tf.name_scope(scope):\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n            x = conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+\'_conv1\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch2\')\n            x = Relu(x)\n            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+\'_conv2\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n\n            # print(x)\n\n            return x\n\n    def transition_layer(self, x, scope):\n        with tf.name_scope(scope):\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n            # x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+\'_conv1\')\n            \n            # https://github.com/taki0112/Densenet-Tensorflow/issues/10\n            \n            in_channel = x.shape[-1]\n            x = conv_layer(x, filter=in_channel*0.5, kernel=[1,1], layer_name=scope+\'_conv1\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n            x = Average_pooling(x, pool_size=[2,2], stride=2)\n\n            return x\n\n    def dense_block(self, input_x, nb_layers, layer_name):\n        with tf.name_scope(layer_name):\n            layers_concat = list()\n            layers_concat.append(input_x)\n\n            x = self.bottleneck_layer(input_x, scope=layer_name + \'_bottleN_\' + str(0))\n\n            layers_concat.append(x)\n\n            for i in range(nb_layers - 1):\n                x = Concatenation(layers_concat)\n                x = self.bottleneck_layer(x, scope=layer_name + \'_bottleN_\' + str(i + 1))\n                layers_concat.append(x)\n\n            x = Concatenation(layers_concat)\n\n            return x\n\n    def Dense_net(self, input_x):\n        x = conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name=\'conv0\')\n        # x = Max_Pooling(x, pool_size=[3,3], stride=2)\n\n\n        """"""\n        for i in range(self.nb_blocks) :\n            # 6 -> 12 -> 48\n            x = self.dense_block(input_x=x, nb_layers=4, layer_name=\'dense_\'+str(i))\n            x = self.transition_layer(x, scope=\'trans_\'+str(i))\n        """"""\n\n\n\n\n        x = self.dense_block(input_x=x, nb_layers=6, layer_name=\'dense_1\')\n        x = self.transition_layer(x, scope=\'trans_1\')\n\n        x = self.dense_block(input_x=x, nb_layers=12, layer_name=\'dense_2\')\n        x = self.transition_layer(x, scope=\'trans_2\')\n\n        x = self.dense_block(input_x=x, nb_layers=48, layer_name=\'dense_3\')\n        x = self.transition_layer(x, scope=\'trans_3\')\n\n        x = self.dense_block(input_x=x, nb_layers=32, layer_name=\'dense_final\')\n\n\n\n        # 100 Layer\n        x = Batch_Normalization(x, training=self.training, scope=\'linear_batch\')\n        x = Relu(x)\n        x = Global_Average_Pooling(x)\n        x = flatten(x)\n        x = Linear(x)\n\n\n        # x = tf.reshape(x, [-1, 10])\n        return x\n\n\n\ntrain_x, train_y, test_x, test_y = prepare_data()\ntrain_x, test_x = color_preprocessing(train_x, test_x)\n\n# image_size = 32, img_channels = 3, class_num = 10 in cifar10\nx = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\nlabel = tf.placeholder(tf.float32, shape=[None, class_num])\n\ntraining_flag = tf.placeholder(tf.bool)\n\n\nlearning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n\nlogits = DenseNet(x=x, nb_blocks=nb_block, filters=growth_k, training=training_flag).model\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n\n""""""\nl2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=nesterov_momentum, use_nesterov=True)\ntrain = optimizer.minimize(cost + l2_loss * weight_decay)\n\nIn paper, use MomentumOptimizer\ninit_learning_rate = 0.1\n\nbut, I\'ll use AdamOptimizer\n""""""\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\ntrain = optimizer.minimize(cost)\n\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(\'./model\')\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n    summary_writer = tf.summary.FileWriter(\'./logs\', sess.graph)\n\n    epoch_learning_rate = init_learning_rate\n    for epoch in range(1, total_epochs + 1):\n        if epoch == (total_epochs * 0.5) or epoch == (total_epochs * 0.75):\n            epoch_learning_rate = epoch_learning_rate / 10\n\n        pre_index = 0\n        train_acc = 0.0\n        train_loss = 0.0\n\n\n        for step in range(1, iteration + 1):\n            if pre_index+batch_size < 50000 :\n                batch_x = train_x[pre_index : pre_index+batch_size]\n                batch_y = train_y[pre_index : pre_index+batch_size]\n            else :\n                batch_x = train_x[pre_index : ]\n                batch_y = train_y[pre_index : ]\n\n            batch_x = data_augmentation(batch_x)\n\n            train_feed_dict = {\n                x: batch_x,\n                label: batch_y,\n                learning_rate: epoch_learning_rate,\n                training_flag : True\n            }\n\n            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n\n            train_loss += batch_loss\n            train_acc += batch_acc\n            pre_index += batch_size\n\n            if step == iteration :\n                train_loss /= iteration # average loss\n                train_acc /= iteration # average accuracy\n\n                train_summary = tf.Summary(value=[tf.Summary.Value(tag=\'train_loss\', simple_value=train_loss),\n                                                  tf.Summary.Value(tag=\'train_accuracy\', simple_value=train_acc)])\n\n                test_acc, test_loss, test_summary = Evaluate(sess)\n\n                summary_writer.add_summary(summary=train_summary, global_step=epoch)\n                summary_writer.add_summary(summary=test_summary, global_step=epoch)\n                summary_writer.flush()\n\n                line = ""epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n"" % (\n                    epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n                print(line)\n\n                with open(\'logs.txt\', \'a\') as f :\n                    f.write(line)\n\n\n\n        saver.save(sess=sess, save_path=\'./model/dense.ckpt\')\n'"
Cifar10/cifar10.py,0,"b'# -*- coding:utf-8 -*-\n\nimport os\nimport sys\nimport time\nimport pickle\nimport random\nimport numpy as np\n\nclass_num = 10\nimage_size = 32\nimg_channels = 3\n\n\n# ========================================================== #\n# \xe2\x94\x9c\xe2\x94\x80 prepare_data()\n#  \xe2\x94\x9c\xe2\x94\x80 download training data if not exist by download_data()\n#  \xe2\x94\x9c\xe2\x94\x80 load data by load_data()\n#  \xe2\x94\x94\xe2\x94\x80 shuffe and return data\n# ========================================================== #\n\n\n\ndef download_data():\n    dirname = \'cifar-10-batches-py\'\n    origin = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    fname = \'cifar-10-python.tar.gz\'\n    fpath = \'./\' + dirname\n\n    download = False\n    if os.path.exists(fpath) or os.path.isfile(fname):\n        download = False\n        print(""DataSet aready exist!"")\n    else:\n        download = True\n    if download:\n        print(\'Downloading data from\', origin)\n        import urllib.request\n        import tarfile\n\n        def reporthook(count, block_size, total_size):\n            global start_time\n            if count == 0:\n                start_time = time.time()\n                return\n            duration = time.time() - start_time\n            progress_size = int(count * block_size)\n            speed = int(progress_size / (1024 * duration))\n            percent = min(int(count * block_size * 100 / total_size), 100)\n            sys.stdout.write(""\\r...%d%%, %d MB, %d KB/s, %d seconds passed"" %\n                             (percent, progress_size / (1024 * 1024), speed, duration))\n            sys.stdout.flush()\n\n        urllib.request.urlretrieve(origin, fname, reporthook)\n        print(\'Download finished. Start extract!\', origin)\n        if (fname.endswith(""tar.gz"")):\n            tar = tarfile.open(fname, ""r:gz"")\n            tar.extractall()\n            tar.close()\n        elif (fname.endswith(""tar"")):\n            tar = tarfile.open(fname, ""r:"")\n            tar.extractall()\n            tar.close()\n\n\ndef unpickle(file):\n    with open(file, \'rb\') as fo:\n        dict = pickle.load(fo, encoding=\'bytes\')\n    return dict\n\n\ndef load_data_one(file):\n    batch = unpickle(file)\n    data = batch[b\'data\']\n    labels = batch[b\'labels\']\n    print(""Loading %s : %d."" % (file, len(data)))\n    return data, labels\n\n\ndef load_data(files, data_dir, label_count):\n    global image_size, img_channels\n    data, labels = load_data_one(data_dir + \'/\' + files[0])\n    for f in files[1:]:\n        data_n, labels_n = load_data_one(data_dir + \'/\' + f)\n        data = np.append(data, data_n, axis=0)\n        labels = np.append(labels, labels_n, axis=0)\n    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n    data = data.reshape([-1, img_channels, image_size, image_size])\n    data = data.transpose([0, 2, 3, 1])\n    return data, labels\n\n\ndef prepare_data():\n    print(""======Loading data======"")\n    download_data()\n    data_dir = \'./cifar-10-batches-py\'\n    image_dim = image_size * image_size * img_channels\n    meta = unpickle(data_dir + \'/batches.meta\')\n\n    label_names = meta[b\'label_names\']\n    label_count = len(label_names)\n    train_files = [\'data_batch_%d\' % d for d in range(1, 6)]\n    train_data, train_labels = load_data(train_files, data_dir, label_count)\n    test_data, test_labels = load_data([\'test_batch\'], data_dir, label_count)\n\n    print(""Train data:"", np.shape(train_data), np.shape(train_labels))\n    print(""Test data :"", np.shape(test_data), np.shape(test_labels))\n    print(""======Load finished======"")\n\n    print(""======Shuffling data======"")\n    indices = np.random.permutation(len(train_data))\n    train_data = train_data[indices]\n    train_labels = train_labels[indices]\n    print(""======Prepare Finished======"")\n\n    return train_data, train_labels, test_data, test_labels\n\n\n# ========================================================== #\n# \xe2\x94\x9c\xe2\x94\x80 _random_crop()\n# \xe2\x94\x9c\xe2\x94\x80 _random_flip_leftright()\n# \xe2\x94\x9c\xe2\x94\x80 data_augmentation()\n# \xe2\x94\x94\xe2\x94\x80 color_preprocessing()\n# ========================================================== #\n\ndef _random_crop(batch, crop_shape, padding=None):\n    oshape = np.shape(batch[0])\n\n    if padding:\n        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n    new_batch = []\n    npad = ((padding, padding), (padding, padding), (0, 0))\n    for i in range(len(batch)):\n        new_batch.append(batch[i])\n        if padding:\n            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n                                      mode=\'constant\', constant_values=0)\n        nh = random.randint(0, oshape[0] - crop_shape[0])\n        nw = random.randint(0, oshape[1] - crop_shape[1])\n        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n                       nw:nw + crop_shape[1]]\n    return new_batch\n\n\ndef _random_flip_leftright(batch):\n    for i in range(len(batch)):\n        if bool(random.getrandbits(1)):\n            batch[i] = np.fliplr(batch[i])\n    return batch\n\n\ndef color_preprocessing(x_train, x_test):\n    x_train = x_train.astype(\'float32\')\n    x_test = x_test.astype(\'float32\')\n    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n\n    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n\n    return x_train, x_test\n\n\ndef data_augmentation(batch):\n    batch = _random_flip_leftright(batch)\n    batch = _random_crop(batch, [32, 32], 4)\n    return batch'"
MNIST/Densenet_MNIST.py,34,"b'import tensorflow as tf\nfrom tflearn.layers.conv import global_avg_pool\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib.layers import batch_norm, flatten\nfrom tensorflow.contrib.framework import arg_scope\nimport numpy as np\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\n\n# Hyperparameter\ngrowth_k = 12\nnb_block = 2 # how many (dense block + Transition Layer) ?\ninit_learning_rate = 1e-4\nepsilon = 1e-8 # AdamOptimizer epsilon\ndropout_rate = 0.2\n\n# Momentum Optimizer will use\nnesterov_momentum = 0.9\nweight_decay = 1e-4\n\n# Label & batch_size\nclass_num = 10\nbatch_size = 100\n\ntotal_epochs = 50\n\n\ndef conv_layer(input, filter, kernel, stride=1, layer_name=""conv""):\n    with tf.name_scope(layer_name):\n        network = tf.layers.conv2d(inputs=input, filters=filter, kernel_size=kernel, strides=stride, padding=\'SAME\')\n        return network\n\ndef Global_Average_Pooling(x, stride=1):\n    """"""\n    width = np.shape(x)[1]\n    height = np.shape(x)[2]\n    pool_size = [width, height]\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter\n    It is global average pooling without tflearn\n    """"""\n\n    return global_avg_pool(x, name=\'Global_avg_pooling\')\n    # But maybe you need to install h5py and curses or not\n\n\ndef Batch_Normalization(x, training, scope):\n    with arg_scope([batch_norm],\n                   scope=scope,\n                   updates_collections=None,\n                   decay=0.9,\n                   center=True,\n                   scale=True,\n                   zero_debias_moving_mean=True) :\n        return tf.cond(training,\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n\ndef Drop_out(x, rate, training) :\n    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n\ndef Relu(x):\n    return tf.nn.relu(x)\n\ndef Average_pooling(x, pool_size=[2,2], stride=2, padding=\'VALID\'):\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\n\ndef Max_Pooling(x, pool_size=[3,3], stride=2, padding=\'VALID\'):\n    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Concatenation(layers) :\n    return tf.concat(layers, axis=3)\n\ndef Linear(x) :\n    return tf.layers.dense(inputs=x, units=class_num, name=\'linear\')\n\n\n\nclass DenseNet():\n    def __init__(self, x, nb_blocks, filters, training):\n        self.nb_blocks = nb_blocks\n        self.filters = filters\n        self.training = training\n        self.model = self.Dense_net(x)\n\n\n    def bottleneck_layer(self, x, scope):\n        # print(x)\n        with tf.name_scope(scope):\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n            x = conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+\'_conv1\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch2\')\n            x = Relu(x)\n            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+\'_conv2\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n\n            # print(x)\n\n            return x\n\n    def transition_layer(self, x, scope):\n        with tf.name_scope(scope):\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n            # x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+\'_conv1\')\n            \n            # https://github.com/taki0112/Densenet-Tensorflow/issues/10\n            \n            in_channel = x.shape[-1]\n            x = conv_layer(x, filter=in_channel*0.5, kernel=[1,1], layer_name=scope+\'_conv1\')\n            x = Drop_out(x, rate=dropout_rate, training=self.training)\n            x = Average_pooling(x, pool_size=[2,2], stride=2)\n\n            return x\n\n    def dense_block(self, input_x, nb_layers, layer_name):\n        with tf.name_scope(layer_name):\n            layers_concat = list()\n            layers_concat.append(input_x)\n\n            x = self.bottleneck_layer(input_x, scope=layer_name + \'_bottleN_\' + str(0))\n\n            layers_concat.append(x)\n\n            for i in range(nb_layers - 1):\n                x = Concatenation(layers_concat)\n                x = self.bottleneck_layer(x, scope=layer_name + \'_bottleN_\' + str(i + 1))\n                layers_concat.append(x)\n\n            x = Concatenation(layers_concat)\n\n            return x\n\n    def Dense_net(self, input_x):\n        x = conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name=\'conv0\')\n        x = Max_Pooling(x, pool_size=[3,3], stride=2)\n\n\n\n        for i in range(self.nb_blocks) :\n            # 6 -> 12 -> 48\n            x = self.dense_block(input_x=x, nb_layers=4, layer_name=\'dense_\'+str(i))\n            x = self.transition_layer(x, scope=\'trans_\'+str(i))\n\n\n        """"""\n        x = self.dense_block(input_x=x, nb_layers=6, layer_name=\'dense_1\')\n        x = self.transition_layer(x, scope=\'trans_1\')\n\n        x = self.dense_block(input_x=x, nb_layers=12, layer_name=\'dense_2\')\n        x = self.transition_layer(x, scope=\'trans_2\')\n\n        x = self.dense_block(input_x=x, nb_layers=48, layer_name=\'dense_3\')\n        x = self.transition_layer(x, scope=\'trans_3\')\n        """"""\n\n        x = self.dense_block(input_x=x, nb_layers=32, layer_name=\'dense_final\')\n\n        # 100 Layer\n        x = Batch_Normalization(x, training=self.training, scope=\'linear_batch\')\n        x = Relu(x)\n        x = Global_Average_Pooling(x)\n        x = flatten(x)\n        x = Linear(x)\n\n\n        # x = tf.reshape(x, [-1, 10])\n        return x\n\n\nx = tf.placeholder(tf.float32, shape=[None, 784])\nbatch_images = tf.reshape(x, [-1, 28, 28, 1])\n\nlabel = tf.placeholder(tf.float32, shape=[None, 10])\n\ntraining_flag = tf.placeholder(tf.bool)\n\n\nlearning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n\nlogits = DenseNet(x=batch_images, nb_blocks=nb_block, filters=growth_k, training=training_flag).model\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n\n""""""\nl2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=nesterov_momentum, use_nesterov=True)\ntrain = optimizer.minimize(cost + l2_loss * weight_decay)\nIn paper, use MomentumOptimizer\ninit_learning_rate = 0.1\nbut, I\'ll use AdamOptimizer\n""""""\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\ntrain = optimizer.minimize(cost)\n\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\ntf.summary.scalar(\'loss\', cost)\ntf.summary.scalar(\'accuracy\', accuracy)\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(\'./model\')\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n    merged = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(\'./logs\', sess.graph)\n\n    global_step = 0\n    epoch_learning_rate = init_learning_rate\n    for epoch in range(total_epochs):\n        if epoch == (total_epochs * 0.5) or epoch == (total_epochs * 0.75):\n            epoch_learning_rate = epoch_learning_rate / 10\n\n        total_batch = int(mnist.train.num_examples / batch_size)\n\n        for step in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n\n            train_feed_dict = {\n                x: batch_x,\n                label: batch_y,\n                learning_rate: epoch_learning_rate,\n                training_flag : True\n            }\n\n            _, loss = sess.run([train, cost], feed_dict=train_feed_dict)\n\n            if step % 100 == 0:\n                global_step += 100\n                train_summary, train_accuracy = sess.run([merged, accuracy], feed_dict=train_feed_dict)\n                # accuracy.eval(feed_dict=feed_dict)\n                print(""Step:"", step, ""Loss:"", loss, ""Training accuracy:"", train_accuracy)\n                writer.add_summary(train_summary, global_step=epoch)\n\n            test_feed_dict = {\n                x: mnist.test.images,\n                label: mnist.test.labels,\n                learning_rate: epoch_learning_rate,\n                training_flag : False\n            }\n\n        accuracy_rates = sess.run(accuracy, feed_dict=test_feed_dict)\n        print(\'Epoch:\', \'%04d\' % (epoch + 1), \'/ Accuracy =\', accuracy_rates)\n        # writer.add_summary(test_summary, global_step=epoch)\n\n    saver.save(sess=sess, save_path=\'./model/dense.ckpt\')\n'"
