file_path,api_count,code
MUNIT.py,35,"b'from ops import *\nfrom utils import *\nfrom glob import glob\nimport time\nfrom tensorflow.contrib.data import batch_and_drop_remainder\n\nclass MUNIT(object) :\n    def __init__(self, sess, args):\n        self.model_name = \'MUNIT\'\n        self.sess = sess\n        self.checkpoint_dir = args.checkpoint_dir\n        self.result_dir = args.result_dir\n        self.log_dir = args.log_dir\n        self.sample_dir = args.sample_dir\n        self.dataset_name = args.dataset\n        self.augment_flag = args.augment_flag\n\n        self.epoch = args.epoch\n        self.iteration = args.iteration\n\n        self.gan_type = args.gan_type\n\n        self.batch_size = args.batch_size\n        self.print_freq = args.print_freq\n        self.save_freq = args.save_freq\n        self.num_style = args.num_style # for test\n        self.guide_img = args.guide_img\n        self.direction = args.direction\n\n        self.img_h = args.img_h\n        self.img_w = args.img_w\n        self.img_ch = args.img_ch\n\n        self.init_lr = args.lr\n        self.ch = args.ch\n\n        """""" Weight """"""\n        self.gan_w = args.gan_w\n        self.recon_x_w = args.recon_x_w\n        self.recon_s_w = args.recon_s_w\n        self.recon_c_w = args.recon_c_w\n        self.recon_x_cyc_w = args.recon_x_cyc_w\n\n        """""" Generator """"""\n        self.n_res = args.n_res\n        self.mlp_dim = pow(2, args.n_sample) * args.ch # default : 256\n\n        self.n_downsample = args.n_sample\n        self.n_upsample = args.n_sample\n        self.style_dim = args.style_dim\n\n        """""" Discriminator """"""\n        self.n_dis = args.n_dis\n        self.n_scale = args.n_scale\n\n        self.sample_dir = os.path.join(args.sample_dir, self.model_dir)\n        check_folder(self.sample_dir)\n\n        self.trainA_dataset = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/trainA\'))\n        self.trainB_dataset = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/trainB\'))\n        self.dataset_num = max(len(self.trainA_dataset), len(self.trainB_dataset))\n\n        print(""##### Information #####"")\n        print(""# gan type : "", self.gan_type)\n        print(""# dataset : "", self.dataset_name)\n        print(""# max dataset number : "", self.dataset_num)\n        print(""# batch_size : "", self.batch_size)\n        print(""# epoch : "", self.epoch)\n        print(""# iteration per epoch : "", self.iteration)\n        print(""# style in test phase : "", self.num_style)\n\n        print()\n\n        print(""##### Generator #####"")\n        print(""# residual blocks : "", self.n_res)\n        print(""# Style dimension : "", self.style_dim)\n        print(""# MLP dimension : "", self.mlp_dim)\n        print(""# Down sample : "", self.n_downsample)\n        print(""# Up sample : "", self.n_upsample)\n\n        print()\n\n        print(""##### Discriminator #####"")\n        print(""# Discriminator layer : "", self.n_dis)\n        print(""# Multi-scale Dis : "", self.n_scale)\n\n    ##################################################################################\n    # Encoder and Decoders\n    ##################################################################################\n\n    def Style_Encoder(self, x, reuse=False, scope=\'style_encoder\'):\n        # IN removes the original feature mean and variance that represent important style information\n        channel = self.ch\n        with tf.variable_scope(scope, reuse=reuse) :\n            x = conv(x, channel, kernel=7, stride=1, pad=3, pad_type=\'reflect\', scope=\'conv_0\')\n            x = relu(x)\n\n            for i in range(2) :\n                x = conv(x, channel*2, kernel=4, stride=2, pad=1, pad_type=\'reflect\', scope=\'conv_\'+str(i+1))\n                x = relu(x)\n\n                channel = channel * 2\n\n            for i in range(2) :\n                x = conv(x, channel, kernel=4, stride=2, pad=1, pad_type=\'reflect\', scope=\'down_conv_\'+str(i))\n                x = relu(x)\n\n            x = adaptive_avg_pooling(x) # global average pooling\n            x = conv(x, self.style_dim, kernel=1, stride=1, scope=\'SE_logit\')\n\n            return x\n\n    def Content_Encoder(self, x, reuse=False, scope=\'content_encoder\'):\n        channel = self.ch\n        with tf.variable_scope(scope, reuse=reuse) :\n            x = conv(x, channel, kernel=7, stride=1, pad=3, pad_type=\'reflect\', scope=\'conv_0\')\n            x = instance_norm(x, scope=\'ins_0\')\n            x = relu(x)\n\n            for i in range(self.n_downsample) :\n                x = conv(x, channel*2, kernel=4, stride=2, pad=1, pad_type=\'reflect\', scope=\'conv_\'+str(i+1))\n                x = instance_norm(x, scope=\'ins_\'+str(i+1))\n                x = relu(x)\n\n                channel = channel * 2\n\n            for i in range(self.n_res) :\n                x = resblock(x, channel, scope=\'resblock_\'+str(i))\n\n            return x\n\n    def generator(self, contents, style, reuse=False, scope=""decoder""):\n        channel = self.mlp_dim\n        with tf.variable_scope(scope, reuse=reuse) :\n            mu, var = self.MLP(style)\n            x = contents\n\n            for i in range(self.n_res) :\n                idx = 2 * i\n                x = adaptive_resblock(x, channel, mu[idx], var[idx], mu[idx + 1], var[idx + 1], scope=\'adaptive_resblock_\'+str(i))\n\n            for i in range(self.n_upsample) :\n                # # IN removes the original feature mean and variance that represent important style information\n                x = up_sample(x, scale_factor=2)\n                x = conv(x, channel//2, kernel=5, stride=1, pad=2, pad_type=\'reflect\', scope=\'conv_\'+str(i))\n                x = layer_norm(x, scope=\'layer_norm_\'+str(i))\n                x = relu(x)\n\n                channel = channel // 2\n\n            x = conv(x, channels=self.img_ch, kernel=7, stride=1, pad=3, pad_type=\'reflect\', scope=\'G_logit\')\n            x = tanh(x)\n\n            return x\n\n    def MLP(self, style, scope=\'MLP\'):\n        channel = self.mlp_dim\n        with tf.variable_scope(scope) :\n            x = style\n\n            for i in range(2):\n                x = fully_connected(x, channel, scope=\'FC_\' + str(i))\n                x = relu(x)\n\n            mu_list = []\n            var_list = []\n\n            for i in range(self.n_res * 2):\n                mu = fully_connected(x, channel, scope=\'FC_mu_\' + str(i))\n                var = fully_connected(x, channel, scope=\'FC_var_\' + str(i))\n\n                mu = tf.reshape(mu, shape=[-1, 1, 1, channel])\n                var = tf.reshape(var, shape=[-1, 1, 1, channel])\n\n                mu_list.append(mu)\n                var_list.append(var)\n\n            return mu_list, var_list\n\n    ##################################################################################\n    # Discriminator\n    ##################################################################################\n\n    def discriminator(self, x_init, reuse=False, scope=""discriminator""):\n        D_logit = []\n        with tf.variable_scope(scope, reuse=reuse) :\n            for scale in range(self.n_scale) :\n                channel = self.ch\n                x = conv(x_init, channel, kernel=4, stride=2, pad=1, pad_type=\'reflect\', scope=\'ms_\' + str(scale) + \'conv_0\')\n                x = lrelu(x, 0.2)\n\n                for i in range(1, self.n_dis):\n                    x = conv(x, channel * 2, kernel=4, stride=2, pad=1, pad_type=\'reflect\', scope=\'ms_\' + str(scale) +\'conv_\' + str(i))\n                    x = lrelu(x, 0.2)\n\n                    channel = channel * 2\n\n                x = conv(x, channels=1, kernel=1, stride=1, scope=\'ms_\' + str(scale) + \'D_logit\')\n                D_logit.append(x)\n\n                x_init = down_sample(x_init)\n\n            return D_logit\n\n    ##################################################################################\n    # Model\n    ##################################################################################\n\n    def Encoder_A(self, x_A, reuse=False):\n        style_A = self.Style_Encoder(x_A, reuse=reuse, scope=\'style_encoder_A\')\n        content_A = self.Content_Encoder(x_A, reuse=reuse, scope=\'content_encoder_A\')\n\n        return content_A, style_A\n\n    def Encoder_B(self, x_B, reuse=False):\n        style_B = self.Style_Encoder(x_B, reuse=reuse, scope=\'style_encoder_B\')\n        content_B = self.Content_Encoder(x_B, reuse=reuse, scope=\'content_encoder_B\')\n\n        return content_B, style_B\n\n    def Decoder_A(self, content_B, style_A, reuse=False):\n        x_ba = self.generator(contents=content_B, style=style_A, reuse=reuse, scope=\'decoder_A\')\n\n        return x_ba\n\n    def Decoder_B(self, content_A, style_B, reuse=False):\n        x_ab = self.generator(contents=content_A, style=style_B, reuse=reuse, scope=\'decoder_B\')\n\n        return x_ab\n\n    def discriminate_real(self, x_A, x_B):\n        real_A_logit = self.discriminator(x_A, scope=""discriminator_A"")\n        real_B_logit = self.discriminator(x_B, scope=""discriminator_B"")\n\n        return real_A_logit, real_B_logit\n\n    def discriminate_fake(self, x_ba, x_ab):\n        fake_A_logit = self.discriminator(x_ba, reuse=True, scope=""discriminator_A"")\n        fake_B_logit = self.discriminator(x_ab, reuse=True, scope=""discriminator_B"")\n\n        return fake_A_logit, fake_B_logit\n\n    def build_model(self):\n        self.lr = tf.placeholder(tf.float32, name=\'learning_rate\')\n\n        """""" Input Image""""""\n        Image_Data_Class = ImageData(self.img_h, self.img_w, self.img_ch, self.augment_flag)\n\n        trainA = tf.data.Dataset.from_tensor_slices(self.trainA_dataset)\n        trainB = tf.data.Dataset.from_tensor_slices(self.trainB_dataset)\n\n        trainA = trainA.prefetch(self.batch_size).shuffle(self.dataset_num).map(Image_Data_Class.image_processing, num_parallel_calls=8).apply(batch_and_drop_remainder(self.batch_size)).repeat()\n        trainB = trainB.prefetch(self.batch_size).shuffle(self.dataset_num).map(Image_Data_Class.image_processing, num_parallel_calls=8).apply(batch_and_drop_remainder(self.batch_size)).repeat()\n\n        trainA_iterator = trainA.make_one_shot_iterator()\n        trainB_iterator = trainB.make_one_shot_iterator()\n\n\n        self.domain_A = trainA_iterator.get_next()\n        self.domain_B = trainB_iterator.get_next()\n\n\n        """""" Define Encoder, Generator, Discriminator """"""\n        self.style_a = tf.placeholder(tf.float32, shape=[self.batch_size, 1, 1, self.style_dim], name=\'style_a\')\n        self.style_b = tf.placeholder(tf.float32, shape=[self.batch_size, 1, 1, self.style_dim], name=\'style_b\')\n\n        # encode\n        content_a, style_a_prime = self.Encoder_A(self.domain_A)\n        content_b, style_b_prime = self.Encoder_B(self.domain_B)\n\n        # decode (within domain)\n        x_aa = self.Decoder_A(content_B=content_a, style_A=style_a_prime)\n        x_bb = self.Decoder_B(content_A=content_b, style_B=style_b_prime)\n\n        # decode (cross domain)\n        x_ba = self.Decoder_A(content_B=content_b, style_A=self.style_a, reuse=True)\n        x_ab = self.Decoder_B(content_A=content_a, style_B=self.style_b, reuse=True)\n\n        # encode again\n        content_b_, style_a_ = self.Encoder_A(x_ba, reuse=True)\n        content_a_, style_b_ = self.Encoder_B(x_ab, reuse=True)\n\n        # decode again (if needed)\n        if self.recon_x_cyc_w > 0 :\n            x_aba = self.Decoder_A(content_B=content_a_, style_A=style_a_prime, reuse=True)\n            x_bab = self.Decoder_B(content_A=content_b_, style_B=style_b_prime, reuse=True)\n\n            cyc_recon_A = L1_loss(x_aba, self.domain_A)\n            cyc_recon_B = L1_loss(x_bab, self.domain_B)\n\n        else :\n            cyc_recon_A = 0.0\n            cyc_recon_B = 0.0\n\n        real_A_logit, real_B_logit = self.discriminate_real(self.domain_A, self.domain_B)\n        fake_A_logit, fake_B_logit = self.discriminate_fake(x_ba, x_ab)\n\n        """""" Define Loss """"""\n        G_ad_loss_a = generator_loss(self.gan_type, fake_A_logit)\n        G_ad_loss_b = generator_loss(self.gan_type, fake_B_logit)\n\n        D_ad_loss_a = discriminator_loss(self.gan_type, real_A_logit, fake_A_logit)\n        D_ad_loss_b = discriminator_loss(self.gan_type, real_B_logit, fake_B_logit)\n\n        recon_A = L1_loss(x_aa, self.domain_A) # reconstruction\n        recon_B = L1_loss(x_bb, self.domain_B) # reconstruction\n\n        # The style reconstruction loss encourages\n        # diverse outputs given different style codes\n        recon_style_A = L1_loss(style_a_, self.style_a)\n        recon_style_B = L1_loss(style_b_, self.style_b)\n\n        # The content reconstruction loss encourages\n        # the translated image to preserve semantic content of the input image\n        recon_content_A = L1_loss(content_a_, content_a)\n        recon_content_B = L1_loss(content_b_, content_b)\n\n\n        Generator_A_loss = self.gan_w * G_ad_loss_a + \\\n                           self.recon_x_w * recon_A + \\\n                           self.recon_s_w * recon_style_A + \\\n                           self.recon_c_w * recon_content_A + \\\n                           self.recon_x_cyc_w * cyc_recon_A\n\n        Generator_B_loss = self.gan_w * G_ad_loss_b + \\\n                           self.recon_x_w * recon_B + \\\n                           self.recon_s_w * recon_style_B + \\\n                           self.recon_c_w * recon_content_B + \\\n                           self.recon_x_cyc_w * cyc_recon_B\n\n        Discriminator_A_loss = self.gan_w * D_ad_loss_a\n        Discriminator_B_loss = self.gan_w * D_ad_loss_b\n\n        self.Generator_loss = Generator_A_loss + Generator_B_loss + regularization_loss(\'encoder\') + regularization_loss(\'decoder\')\n        self.Discriminator_loss = Discriminator_A_loss + Discriminator_B_loss + regularization_loss(\'discriminator\')\n\n        """""" Training """"""\n        t_vars = tf.trainable_variables()\n        G_vars = [var for var in t_vars if \'decoder\' in var.name or \'encoder\' in var.name]\n        D_vars = [var for var in t_vars if \'discriminator\' in var.name]\n\n\n        self.G_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n        self.D_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n\n        """""""" Summary """"""\n        self.all_G_loss = tf.summary.scalar(""Generator_loss"", self.Generator_loss)\n        self.all_D_loss = tf.summary.scalar(""Discriminator_loss"", self.Discriminator_loss)\n        self.G_A_loss = tf.summary.scalar(""G_A_loss"", Generator_A_loss)\n        self.G_B_loss = tf.summary.scalar(""G_B_loss"", Generator_B_loss)\n        self.D_A_loss = tf.summary.scalar(""D_A_loss"", Discriminator_A_loss)\n        self.D_B_loss = tf.summary.scalar(""D_B_loss"", Discriminator_B_loss)\n\n        self.G_loss = tf.summary.merge([self.G_A_loss, self.G_B_loss, self.all_G_loss])\n        self.D_loss = tf.summary.merge([self.D_A_loss, self.D_B_loss, self.all_D_loss])\n\n        """""" Image """"""\n        self.fake_A = x_ba\n        self.fake_B = x_ab\n\n        self.real_A = self.domain_A\n        self.real_B = self.domain_B\n\n        """""" Test """"""\n        self.test_image = tf.placeholder(tf.float32, [1, self.img_h, self.img_w, self.img_ch], name=\'test_image\')\n        self.test_style = tf.placeholder(tf.float32, [1, 1, 1, self.style_dim], name=\'test_style\')\n\n        test_content_a, _ = self.Encoder_A(self.test_image, reuse=True)\n        test_content_b, _ = self.Encoder_B(self.test_image, reuse=True)\n\n        self.test_fake_A = self.Decoder_A(content_B=test_content_b, style_A=self.test_style, reuse=True)\n        self.test_fake_B = self.Decoder_B(content_A=test_content_a, style_B=self.test_style, reuse=True)\n\n        """""" Guided Image Translation """"""\n        self.content_image = tf.placeholder(tf.float32, [1, self.img_h, self.img_w, self.img_ch], name=\'content_image\')\n        self.style_image = tf.placeholder(tf.float32, [1, self.img_h, self.img_w, self.img_ch], name=\'guide_style_image\')\n\n        if self.direction == \'a2b\' :\n            guide_content_A, guide_style_A = self.Encoder_A(self.content_image, reuse=True)\n            guide_content_B, guide_style_B = self.Encoder_B(self.style_image, reuse=True)\n\n        else :\n            guide_content_B, guide_style_B = self.Encoder_B(self.content_image, reuse=True)\n            guide_content_A, guide_style_A = self.Encoder_A(self.style_image, reuse=True)\n\n        self.guide_fake_A = self.Decoder_A(content_B=guide_content_B, style_A=guide_style_A, reuse=True)\n        self.guide_fake_B = self.Decoder_B(content_A=guide_content_A, style_B=guide_style_B, reuse=True)\n\n    def train(self):\n        # initialize all variables\n        tf.global_variables_initializer().run()\n\n        # saver to save model\n        self.saver = tf.train.Saver()\n\n        # summary writer\n        self.writer = tf.summary.FileWriter(self.log_dir + \'/\' + self.model_dir, self.sess.graph)\n\n        # restore check-point if it exits\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n        if could_load:\n            start_epoch = (int)(checkpoint_counter / self.iteration)\n            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n            counter = checkpoint_counter\n            print("" [*] Load SUCCESS"")\n        else:\n            start_epoch = 0\n            start_batch_id = 0\n            counter = 1\n            print("" [!] Load failed..."")\n\n        # loop for epoch\n        start_time = time.time()\n        for epoch in range(start_epoch, self.epoch):\n\n            lr = self.init_lr * pow(0.5, epoch)\n\n            for idx in range(start_batch_id, self.iteration):\n                style_a = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, 1, self.style_dim])\n                style_b = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, 1, self.style_dim])\n\n                train_feed_dict = {\n                    self.style_a : style_a,\n                    self.style_b : style_b,\n                    self.lr : lr\n                }\n\n                # Update D\n                _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss], feed_dict = train_feed_dict)\n                self.writer.add_summary(summary_str, counter)\n\n                # Update G\n                batch_A_images, batch_B_images, fake_A, fake_B, _, g_loss, summary_str = self.sess.run([self.real_A, self.real_B, self.fake_A, self.fake_B, self.G_optim, self.Generator_loss, self.G_loss], feed_dict = train_feed_dict)\n                self.writer.add_summary(summary_str, counter)\n\n                # display training status\n                counter += 1\n                print(""Epoch: [%2d] [%6d/%6d] time: %4.4f d_loss: %.8f, g_loss: %.8f"" \\\n                      % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss))\n\n                if np.mod(idx+1, self.print_freq) == 0 :\n                    save_images(batch_A_images, [self.batch_size, 1],\n                                \'./{}/real_A_{:02d}_{:06d}.jpg\'.format(self.sample_dir, epoch, idx+1))\n                    # save_images(batch_B_images, [self.batch_size, 1],\n                    #             \'./{}/real_B_{}_{:02d}_{:06d}.jpg\'.format(self.sample_dir, gpu_id, epoch, idx+1))\n\n                    # save_images(fake_A, [self.batch_size, 1],\n                    #             \'./{}/fake_A_{}_{:02d}_{:06d}.jpg\'.format(self.sample_dir, gpu_id, epoch, idx+1))\n                    save_images(fake_B, [self.batch_size, 1],\n                                \'./{}/fake_B_{:02d}_{:06d}.jpg\'.format(self.sample_dir, epoch, idx+1))\n\n                if np.mod(idx+1, self.save_freq) == 0 :\n                    self.save(self.checkpoint_dir, counter)\n\n            # After an epoch, start_batch_id is set to zero\n            # non-zero value is only for the first epoch after loading pre-trained model\n            start_batch_id = 0\n\n            # save model for final step\n            self.save(self.checkpoint_dir, counter)\n\n    @property\n    def model_dir(self):\n        return ""{}_{}_{}"".format(self.model_name, self.dataset_name, self.gan_type)\n\n    def save(self, checkpoint_dir, step):\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + \'.model\'), global_step=step)\n\n    def load(self, checkpoint_dir):\n        import re\n        print("" [*] Reading checkpoints..."")\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            counter = int(next(re.finditer(""(\\d+)(?!.*\\d)"", ckpt_name)).group(0))\n            print("" [*] Success to read {}"".format(ckpt_name))\n            return True, counter\n        else:\n            print("" [*] Failed to find a checkpoint"")\n            return False, 0\n\n    def test(self):\n        tf.global_variables_initializer().run()\n        test_A_files = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/testA\'))\n        test_B_files = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/testB\'))\n\n        self.saver = tf.train.Saver()\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n        self.result_dir = os.path.join(self.result_dir, self.model_dir)\n        check_folder(self.result_dir)\n\n        if could_load :\n            print("" [*] Load SUCCESS"")\n        else :\n            print("" [!] Load failed..."")\n\n        # write html for visual comparison\n        index_path = os.path.join(self.result_dir, \'index.html\')\n        index = open(index_path, \'w\')\n        index.write(""<html><body><table><tr>"")\n        index.write(""<th>name</th><th>input</th><th>output</th></tr>"")\n\n        for sample_file  in test_A_files : # A -> B\n            print(\'Processing A image: \' + sample_file)\n            sample_image = np.asarray(load_test_data(sample_file, size_h=self.img_h, size_w=self.img_w))\n            file_name = os.path.basename(sample_file).split(""."")[0]\n            file_extension = os.path.basename(sample_file).split(""."")[1]\n\n            for i in range(self.num_style) :\n                test_style = np.random.normal(loc=0.0, scale=1.0, size=[1, 1, 1, self.style_dim])\n                image_path = os.path.join(self.result_dir, \'{}_style{}.{}\'.format(file_name, i, file_extension))\n\n                fake_img = self.sess.run(self.test_fake_B, feed_dict = {self.test_image : sample_image, self.test_style : test_style})\n                save_images(fake_img, [1, 1], image_path)\n\n                index.write(""<td>%s</td>"" % os.path.basename(image_path))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (sample_file if os.path.isabs(sample_file) else (\n                    \'../..\' + os.path.sep + sample_file), self.img_w, self.img_h))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (image_path if os.path.isabs(image_path) else (\n                    \'../..\' + os.path.sep + image_path), self.img_w, self.img_h))\n                index.write(""</tr>"")\n\n        for sample_file  in test_B_files : # B -> A\n            print(\'Processing B image: \' + sample_file)\n            sample_image = np.asarray(load_test_data(sample_file, size_h=self.img_h, size_w=self.img_w))\n            file_name = os.path.basename(sample_file).split(""."")[0]\n            file_extension = os.path.basename(sample_file).split(""."")[1]\n\n            for i in range(self.num_style):\n                test_style = np.random.normal(loc=0.0, scale=1.0, size=[1, 1, 1, self.style_dim])\n                image_path = os.path.join(self.result_dir, \'{}_style{}.{}\'.format(file_name, i, file_extension))\n\n                fake_img = self.sess.run(self.test_fake_A, feed_dict={self.test_image: sample_image, self.test_style: test_style})\n                save_images(fake_img, [1, 1], image_path)\n\n                index.write(""<td>%s</td>"" % os.path.basename(image_path))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (sample_file if os.path.isabs(sample_file) else (\n                        \'../..\' + os.path.sep + sample_file), self.img_w, self.img_h))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (image_path if os.path.isabs(image_path) else (\n                        \'../..\' + os.path.sep + image_path), self.img_w, self.img_h))\n                index.write(""</tr>"")\n        index.close()\n\n    def style_guide_test(self):\n        tf.global_variables_initializer().run()\n        test_A_files = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/testA\'))\n        test_B_files = glob(\'./dataset/{}/*.*\'.format(self.dataset_name + \'/testB\'))\n\n        style_file = np.asarray(load_test_data(self.guide_img, size_h=self.img_h, size_w=self.img_w))\n\n        self.saver = tf.train.Saver()\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n        self.result_dir = os.path.join(self.result_dir, self.model_dir, \'guide\')\n        check_folder(self.result_dir)\n\n        if could_load:\n            print("" [*] Load SUCCESS"")\n        else:\n            print("" [!] Load failed..."")\n\n        # write html for visual comparison\n        index_path = os.path.join(self.result_dir, \'index.html\')\n        index = open(index_path, \'w\')\n        index.write(""<html><body><table><tr>"")\n        index.write(""<th>name</th><th>input</th><th>output</th></tr>"")\n\n        if self.direction == \'a2b\' :\n            for sample_file in test_A_files:  # A -> B\n                print(\'Processing A image: \' + sample_file)\n                sample_image = np.asarray(load_test_data(sample_file, size_h=self.img_h, size_w=self.img_w))\n                image_path = os.path.join(self.result_dir, \'{}\'.format(os.path.basename(sample_file)))\n\n                fake_img = self.sess.run(self.guide_fake_B, feed_dict={self.content_image: sample_image, self.style_image : style_file})\n                save_images(fake_img, [1, 1], image_path)\n\n                index.write(""<td>%s</td>"" % os.path.basename(image_path))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (sample_file if os.path.isabs(sample_file) else (\n                        \'../../..\' + os.path.sep + sample_file), self.img_w, self.img_h))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (image_path if os.path.isabs(image_path) else (\n                        \'../../..\' + os.path.sep + image_path), self.img_w, self.img_h))\n                index.write(""</tr>"")\n\n        else :\n            for sample_file in test_B_files:  # B -> A\n                print(\'Processing B image: \' + sample_file)\n                sample_image = np.asarray(load_test_data(sample_file, size_h=self.img_h, size_w=self.img_w))\n                image_path = os.path.join(self.result_dir, \'{}\'.format(os.path.basename(sample_file)))\n\n                fake_img = self.sess.run(self.guide_fake_A, feed_dict={self.content_image: sample_image, self.style_image : style_file})\n                save_images(fake_img, [1, 1], image_path)\n\n                index.write(""<td>%s</td>"" % os.path.basename(image_path))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (sample_file if os.path.isabs(sample_file) else (\n                        \'../../..\' + os.path.sep + sample_file), self.img_w, self.img_h))\n                index.write(""<td><img src=\'%s\' width=\'%d\' height=\'%d\'></td>"" % (image_path if os.path.isabs(image_path) else (\n                        \'../../..\' + os.path.sep + image_path), self.img_w, self.img_h))\n                index.write(""</tr>"")\n        index.close()\n'"
main.py,1,"b'from MUNIT import MUNIT\nimport argparse\nfrom utils import *\n\n""""""parsing and configuration""""""\ndef parse_args():\n    desc = ""Tensorflow implementation of MUNIT""\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument(\'--phase\', type=str, default=\'train\', help=\'train or test or guide\')\n    parser.add_argument(\'--dataset\', type=str, default=\'summer2winter\', help=\'dataset_name\')\n    parser.add_argument(\'--augment_flag\', type=bool, default=False, help=\'Image augmentation use or not\')\n\n    parser.add_argument(\'--epoch\', type=int, default=10, help=\'The number of epochs to run\')\n    parser.add_argument(\'--iteration\', type=int, default=100000, help=\'The number of training iterations\')\n    parser.add_argument(\'--batch_size\', type=int, default=1, help=\'The batch size\')\n    parser.add_argument(\'--print_freq\', type=int, default=1000, help=\'The number of image_print_freq\')\n    parser.add_argument(\'--save_freq\', type=int, default=1000, help=\'The number of ckpt_save_freq\')\n    parser.add_argument(\'--num_style\', type=int, default=3, help=\'number of styles to sample\')\n    parser.add_argument(\'--direction\', type=str, default=\'a2b\', help=\'direction of style guided image translation\')\n    parser.add_argument(\'--guide_img\', type=str, default=\'guide.jpg\', help=\'Style guided image translation\')\n\n    parser.add_argument(\'--gan_type\', type=str, default=\'lsgan\', help=\'GAN loss type [gan / lsgan]\')\n\n    parser.add_argument(\'--lr\', type=float, default=0.0001, help=\'The learning rate\')\n    parser.add_argument(\'--gan_w\', type=float, default=1.0, help=\'weight of adversarial loss\')\n    parser.add_argument(\'--recon_x_w\', type=float, default=10.0, help=\'weight of image reconstruction loss\')\n    parser.add_argument(\'--recon_s_w\', type=float, default=1.0, help=\'weight of style reconstruction loss\')\n    parser.add_argument(\'--recon_c_w\', type=float, default=1.0, help=\'weight of content reconstruction loss\')\n    parser.add_argument(\'--recon_x_cyc_w\', type=float, default=0.0, help=\'weight of explicit style augmented cycle consistency loss\')\n\n    parser.add_argument(\'--ch\', type=int, default=64, help=\'base channel number per layer\')\n    parser.add_argument(\'--style_dim\', type=int, default=8, help=\'length of style code\')\n    parser.add_argument(\'--n_sample\', type=int, default=2, help=\'number of sampling layers in content encoder\')\n    parser.add_argument(\'--n_res\', type=int, default=4, help=\'number of residual blocks in content encoder/decoder\')\n\n    parser.add_argument(\'--n_dis\', type=int, default=4, help=\'number of discriminator layer\')\n    parser.add_argument(\'--n_scale\', type=int, default=3, help=\'number of scales\')\n\n    parser.add_argument(\'--img_h\', type=int, default=256, help=\'The size of image hegiht\')\n    parser.add_argument(\'--img_w\', type=int, default=256, help=\'The size of image width\')\n    parser.add_argument(\'--img_ch\', type=int, default=3, help=\'The size of image channel\')\n\n    parser.add_argument(\'--checkpoint_dir\', type=str, default=\'checkpoint\',\n                        help=\'Directory name to save the checkpoints\')\n    parser.add_argument(\'--result_dir\', type=str, default=\'results\',\n                        help=\'Directory name to save the generated images\')\n    parser.add_argument(\'--log_dir\', type=str, default=\'logs\',\n                        help=\'Directory name to save training logs\')\n    parser.add_argument(\'--sample_dir\', type=str, default=\'samples\',\n                        help=\'Directory name to save the samples on training\')\n\n    return check_args(parser.parse_args())\n\n""""""checking arguments""""""\ndef check_args(args):\n    # --checkpoint_dir\n    check_folder(args.checkpoint_dir)\n\n    # --result_dir\n    check_folder(args.result_dir)\n\n    # --result_dir\n    check_folder(args.log_dir)\n\n    # --sample_dir\n    check_folder(args.sample_dir)\n\n    # --epoch\n    try:\n        assert args.epoch >= 1\n    except:\n        print(\'number of epochs must be larger than or equal to one\')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print(\'batch size must be larger than or equal to one\')\n    return args\n\n""""""main""""""\ndef main():\n    # parse arguments\n    args = parse_args()\n    if args is None:\n      exit()\n\n    # open session\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        gan = MUNIT(sess, args)\n\n        # build graph\n        gan.build_model()\n\n        # show network architecture\n        show_all_variables()\n\n        if args.phase == \'train\' :\n            # launch the graph in a session\n            gan.train()\n            print("" [*] Training finished!"")\n\n        if args.phase == \'test\' :\n            gan.test()\n            print("" [*] Test finished!"")\n\n        if args.phase == \'guide\' :\n            gan.style_guide_test()\n            print("" [*] Guide finished!"")\n\nif __name__ == \'__main__\':\n    main()'"
ops.py,31,"b'import tensorflow as tf\nimport tensorflow.contrib as tf_contrib\nfrom utils import pytorch_kaiming_weight_factor\n\nfactor, mode, uniform = pytorch_kaiming_weight_factor(a=0.0, uniform=False)\nweight_init = tf_contrib.layers.variance_scaling_initializer(factor=factor, mode=mode, uniform=uniform)\nweight_regularizer = tf_contrib.layers.l2_regularizer(scale=0.0001)\n\n##################################################################################\n# Layer\n##################################################################################\n\ndef conv(x, channels, kernel=4, stride=2, pad=0, pad_type=\'zero\', use_bias=True, scope=\'conv\'):\n    with tf.variable_scope(scope):\n        if scope.__contains__(""discriminator"") :\n            weight_init = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n        else :\n            weight_init = tf_contrib.layers.variance_scaling_initializer()\n\n        if pad > 0:\n            h = x.get_shape().as_list()[1]\n            if h % stride == 0:\n                pad = pad * 2\n            else:\n                pad = max(kernel - (h % stride), 0)\n\n            pad_top = pad // 2\n            pad_bottom = pad - pad_top\n            pad_left = pad // 2\n            pad_right = pad - pad_left\n\n            if pad_type == \'zero\':\n                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])\n            if pad_type == \'reflect\':\n                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], mode=\'REFLECT\')\n\n        x = tf.layers.conv2d(inputs=x, filters=channels,\n                             kernel_size=kernel, kernel_initializer=weight_init,\n                             kernel_regularizer=weight_regularizer,\n                             strides=stride, use_bias=use_bias)\n\n        return x\n\ndef fully_connected(x, units, use_bias=True, scope=\'fully_connected\'):\n    with tf.variable_scope(scope):\n        x = flatten(x)\n        x = tf.layers.dense(x, units=units, kernel_initializer=weight_init,\n                            kernel_regularizer=weight_regularizer,\n                            use_bias=use_bias)\n\n        return x\n\ndef flatten(x) :\n    return tf.layers.flatten(x)\n\n##################################################################################\n# Residual-block\n##################################################################################\n\ndef resblock(x_init, channels, use_bias=True, scope=\'resblock\'):\n    with tf.variable_scope(scope):\n        with tf.variable_scope(\'res1\'):\n            x = conv(x_init, channels, kernel=3, stride=1, pad=1, pad_type=\'reflect\', use_bias=use_bias)\n            x = instance_norm(x)\n            x = relu(x)\n\n        with tf.variable_scope(\'res2\'):\n            x = conv(x, channels, kernel=3, stride=1, pad=1, pad_type=\'reflect\', use_bias=use_bias)\n            x = instance_norm(x)\n\n        return x + x_init\n\ndef adaptive_resblock(x_init, channels, gamma1, beta1, gamma2, beta2, use_bias=True, scope=\'adaptive_resblock\') :\n    with tf.variable_scope(scope):\n        with tf.variable_scope(\'res1\'):\n            x = conv(x_init, channels, kernel=3, stride=1, pad=1, pad_type=\'reflect\', use_bias=use_bias)\n            x = adaptive_instance_norm(x, gamma1, beta1)\n            x = relu(x)\n\n        with tf.variable_scope(\'res2\'):\n            x = conv(x, channels, kernel=3, stride=1, pad=1, pad_type=\'reflect\', use_bias=use_bias)\n            x = adaptive_instance_norm(x, gamma2, beta2)\n\n        return x + x_init\n\n##################################################################################\n# Sampling\n##################################################################################\n\ndef down_sample(x) :\n    return tf.layers.average_pooling2d(x, pool_size=3, strides=2, padding=\'SAME\')\n\ndef up_sample(x, scale_factor=2):\n    _, h, w, _ = x.get_shape().as_list()\n    new_size = [h * scale_factor, w * scale_factor]\n    return tf.image.resize_nearest_neighbor(x, size=new_size)\n\ndef adaptive_avg_pooling(x):\n    # global average pooling\n    gap = tf.reduce_mean(x, axis=[1, 2], keep_dims=True)\n\n    return gap\n\n##################################################################################\n# Activation function\n##################################################################################\n\ndef lrelu(x, alpha=0.01):\n    # pytorch alpha is 0.01\n    return tf.nn.leaky_relu(x, alpha)\n\n\ndef relu(x):\n    return tf.nn.relu(x)\n\n\ndef tanh(x):\n    return tf.tanh(x)\n\n##################################################################################\n# Normalization function\n##################################################################################\n\ndef adaptive_instance_norm(content, gamma, beta, epsilon=1e-5):\n    # gamma, beta = style_mean, style_std from MLP\n\n    c_mean, c_var = tf.nn.moments(content, axes=[1, 2], keep_dims=True)\n    c_std = tf.sqrt(c_var + epsilon)\n\n    return gamma * ((content - c_mean) / c_std) + beta\n\n\ndef instance_norm(x, scope=\'instance_norm\'):\n    return tf_contrib.layers.instance_norm(x,\n                                           epsilon=1e-05,\n                                           center=True, scale=True,\n                                           scope=scope)\n\ndef layer_norm(x, scope=\'layer_norm\') :\n    return tf_contrib.layers.layer_norm(x,\n                                        center=True, scale=True,\n                                        scope=scope)\n\n##################################################################################\n# Loss function\n##################################################################################\n\n""""""\n\nAuthor use LSGAN\nFor LSGAN, multiply each of G and D by 0.5.\nHowever, MUNIT authors did not do this.\n\n""""""\n\ndef discriminator_loss(type, real, fake):\n    n_scale = len(real)\n    loss = []\n\n    real_loss = 0\n    fake_loss = 0\n\n    for i in range(n_scale) :\n        if type == \'lsgan\' :\n            real_loss = tf.reduce_mean(tf.squared_difference(real[i], 1.0))\n            fake_loss = tf.reduce_mean(tf.square(fake[i]))\n\n        if type == \'gan\' :\n            real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real[i]), logits=real[i]))\n            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake[i]), logits=fake[i]))\n\n        loss.append(real_loss + fake_loss)\n\n    return sum(loss)\n\n\ndef generator_loss(type, fake):\n    n_scale = len(fake)\n    loss = []\n\n    fake_loss = 0\n\n    for i in range(n_scale) :\n        if type == \'lsgan\' :\n            fake_loss = tf.reduce_mean(tf.squared_difference(fake[i], 1.0))\n\n        if type == \'gan\' :\n            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake[i]), logits=fake[i]))\n\n        loss.append(fake_loss)\n\n\n    return sum(loss)\n\n\ndef L1_loss(x, y):\n    loss = tf.reduce_mean(tf.abs(x - y))\n\n    return loss\n\ndef regularization_loss(scope_name) :\n    """"""\n    If you want to use ""Regularization""\n    g_loss += regularization_loss(\'generator\')\n    d_loss += regularization_loss(\'discriminator\')\n    """"""\n    collection_regularization = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n    loss = []\n    for item in collection_regularization :\n        if scope_name in item.name :\n            loss.append(item)\n\n    return tf.reduce_sum(loss)'"
utils.py,9,"b""import tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom scipy import misc\nimport os, random\nimport numpy as np\n\n# https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n# https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/\n\nclass ImageData:\n\n    def __init__(self, img_h, img_w, channels, augment_flag=False):\n        self.img_h = img_h\n        self.img_w = img_w\n        self.channels = channels\n        self.augment_flag = augment_flag\n\n    def image_processing(self, filename):\n        x = tf.read_file(filename)\n        x_decode = tf.image.decode_jpeg(x, channels=self.channels)\n        img = tf.image.resize_images(x_decode, [self.img_h, self.img_w])\n        img = tf.cast(img, tf.float32) / 127.5 - 1\n\n        if self.augment_flag :\n            augment_size_h = self.img_h + (30 if self.img_h == 256 else 15)\n            augment_size_w = self.img_w + (30 if self.img_w == 256 else 15)\n            p = random.random()\n            if p > 0.5:\n                img = augmentation(img, augment_size_h, augment_size_w)\n\n        return img\n\n\ndef load_test_data(image_path, size_h=256, size_w=256):\n    img = misc.imread(image_path, mode='RGB')\n    img = misc.imresize(img, [size_h, size_w])\n    img = np.expand_dims(img, axis=0)\n    img = preprocessing(img)\n\n    return img\n\ndef preprocessing(x):\n    x = x/127.5 - 1 # -1 ~ 1\n    return x\n\ndef augmentation(image, aug_img_h, aug_img_w):\n    seed = random.randint(0, 2 ** 31 - 1)\n    ori_image_shape = tf.shape(image)\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.resize_images(image, [aug_img_h, aug_img_w])\n    image = tf.random_crop(image, ori_image_shape, seed=seed)\n    return image\n\ndef save_images(images, size, image_path):\n    return imsave(inverse_transform(images), size, image_path)\n\ndef inverse_transform(images):\n    return (images+1.) / 2\n\ndef imsave(images, size, path):\n    return misc.imsave(path, merge(images, size))\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[h*j:h*(j+1), w*i:w*(i+1), :] = image\n\n    return img\n\ndef show_all_variables():\n    model_vars = tf.trainable_variables()\n    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef check_folder(log_dir):\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n\ndef pytorch_xavier_weight_factor(gain=0.02, uniform=False) :\n\n    if uniform :\n        factor = gain * gain\n        mode = 'FAN_AVG'\n    else :\n        factor = (gain * gain) / 1.3\n        mode = 'FAN_AVG'\n\n    return factor, mode, uniform\n\ndef pytorch_kaiming_weight_factor(a=0.0, activation_function='relu', uniform=False) :\n\n    if activation_function == 'relu' :\n        gain = np.sqrt(2.0)\n    elif activation_function == 'leaky_relu' :\n        gain = np.sqrt(2.0 / (1 + a ** 2))\n    elif activation_function =='tanh' :\n        gain = 5.0 / 3\n    else :\n        gain = 1.0\n\n    if uniform :\n        factor = gain * gain\n        mode = 'FAN_IN'\n    else :\n        factor = (gain * gain) / 1.3\n        mode = 'FAN_IN'\n\n    return factor, mode, uniform"""
