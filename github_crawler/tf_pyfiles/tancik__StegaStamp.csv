file_path,api_count,code
decode_image.py,4,"b'import bchlib\nimport glob\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.image\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.saved_model import signature_constants\n\nBCH_POLYNOMIAL = 137\nBCH_BITS = 5\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model\', type=str)\n    parser.add_argument(\'--image\', type=str, default=None)\n    parser.add_argument(\'--images_dir\', type=str, default=None)\n    parser.add_argument(\'--secret_size\', type=int, default=100)\n    args = parser.parse_args()\n\n    if args.image is not None:\n        files_list = [args.image]\n    elif args.images_dir is not None:\n        files_list = glob.glob(args.images_dir + \'/*\')\n    else:\n        print(\'Missing input image\')\n        return\n\n    sess = tf.InteractiveSession(graph=tf.Graph())\n\n    model = tf.saved_model.loader.load(sess, [tag_constants.SERVING], args.model)\n\n    input_image_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[\'image\'].name\n    input_image = tf.get_default_graph().get_tensor_by_name(input_image_name)\n\n    output_secret_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs[\'decoded\'].name\n    output_secret = tf.get_default_graph().get_tensor_by_name(output_secret_name)\n\n    bch = bchlib.BCH(BCH_POLYNOMIAL, BCH_BITS)\n\n    for filename in files_list:\n        image = Image.open(filename).convert(""RGB"")\n        image = np.array(ImageOps.fit(image,(400, 400)),dtype=np.float32)\n        image /= 255.\n\n        feed_dict = {input_image:[image]}\n\n        secret = sess.run([output_secret],feed_dict=feed_dict)[0][0]\n\n        packet_binary = """".join([str(int(bit)) for bit in secret[:96]])\n        packet = bytes(int(packet_binary[i : i + 8], 2) for i in range(0, len(packet_binary), 8))\n        packet = bytearray(packet)\n\n        data, ecc = packet[:-bch.ecc_bytes], packet[-bch.ecc_bytes:]\n\n        bitflips = bch.decode_inplace(data, ecc)\n\n        if bitflips != -1:\n            try:\n                code = data.decode(""utf-8"")\n                print(filename, code)\n                continue\n            except:\n                continue\n        print(filename, \'Failed to decode\')\n\n\nif __name__ == ""__main__"":\n    main()\n'"
detector.py,7,"b'import os,time,cv2, sys, math\nimport bchlib\nimport tensorflow as tf\nimport argparse\nimport numpy as np\nimport tensorflow.contrib.image\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.saved_model import signature_constants\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--detector_model\', type=str, required=True)\nparser.add_argument(\'--decoder_model\', type=str, required=True)\nparser.add_argument(\'--video\', type=str, required=True)\nparser.add_argument(\'--secret_size\', type=int, default=100)\nparser.add_argument(\'--save_video\', type=str, default=None)\nparser.add_argument(\'--visualize_detector\', action=\'store_true\', help=\'Visualize detector mask output\')\nargs = parser.parse_args()\n\nBCH_POLYNOMIAL = 137\nBCH_BITS = 5\n\ndef get_intersect(p1, p2, p3, p4):\n    s = np.vstack([p1,p2,p3,p4])\n    h = np.hstack((s, np.ones((4, 1))))\n    l1 = np.cross(h[0], h[1])\n    l2 = np.cross(h[2], h[3])\n    x, y, z = np.cross(l1, l2)\n    if z == 0:\n        print(\'invalid\')\n        return (0,0)\n    return (x/z, y/z)\n\ndef poly_area(poly):\n    return 0.5*np.abs(np.dot(poly[:,0],np.roll(poly[:,1],1))-np.dot(poly[:,1],np.roll(poly[:,0],1)))\n\ndef order_points(pts):\n    rect = np.zeros((4, 2), dtype=np.float32)\n    s = pts.sum(axis=1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n\n    diff = np.diff(pts, axis = 1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect\n\ndef main():\n    # Initializing network\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    detector_graph = tf.Graph()\n    decoder_graph = tf.Graph()\n\n    with detector_graph.as_default():\n        detector_sess = tf.Session()\n        detector_model = tf.saved_model.loader.load(detector_sess, [tag_constants.SERVING], args.detector_model)\n\n        detector_input_name = detector_model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[\'image\'].name\n        detector_input = detector_graph.get_tensor_by_name(detector_input_name)\n\n        detector_output_name = detector_model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs[\'detections\'].name\n        detector_output = detector_graph.get_tensor_by_name(detector_output_name)\n\n    with decoder_graph.as_default():\n        decoder_sess = tf.Session()\n        decoder_model = tf.saved_model.loader.load(decoder_sess, [tag_constants.SERVING], args.decoder_model)\n\n        decoder_input_name = decoder_model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[\'image\'].name\n        decoder_input = decoder_graph.get_tensor_by_name(decoder_input_name)\n\n        decoder_output_name = decoder_model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs[\'decoded\'].name\n        decoder_output = decoder_graph.get_tensor_by_name(decoder_output_name)\n\n    cap = cv2.VideoCapture(args.video)\n    bch = bchlib.BCH(BCH_POLYNOMIAL, BCH_BITS)\n\n    ret, frame = cap.read()\n    f_height, f_width = frame.shape[0:2]\n\n    if args.save_video is not None:\n        fourcc1 = cv2.VideoWriter_fourcc(*\'XVID\')\n        out = cv2.VideoWriter(args.save_video, fourcc1, 30.0, (f_width, f_height))\n\n    while(True):\n        ret, frame = cap.read()\n        if frame is None:\n            break\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        detector_image_input = cv2.resize(frame_rgb, (1024,1024))\n        detector_image_input = np.expand_dims(np.float32(detector_image_input),axis=0)/255.0\n\n        output_image = detector_sess.run(detector_output,feed_dict={detector_input:detector_image_input})\n        output_image = np.array(output_image[0,:,:,:])\n        output_image = x = np.argmax(output_image, axis = -1)\n\n        color_codes = np.array([[255,255,255],[0,0,0]])\n        out_vis_image = color_codes[output_image.astype(int)]\n\n        mask_im = cv2.resize(np.float32(out_vis_image), (f_width,f_height))\n        if args.visualize_detector:\n            mask_vis = mask_im.astype(np.uint8)\n\n        contours, _ = cv2.findContours(cv2.cvtColor(mask_im, cv2.COLOR_BGR2GRAY).astype(np.uint8),1,2)\n        extrema = np.zeros((8,2))\n        corners = np.zeros((4,2))\n        for cnt in contours:\n            area = cv2.contourArea(cnt)\n            if area < 1000:\n                continue\n\n            hull = cv2.convexHull(cnt)\n            if len(hull) < 4:\n                continue\n\n            if args.visualize_detector:\n                cv2.polylines(mask_vis, np.int32([corners]), thickness=6, color=(100,100,250), isClosed=True)\n\n            extrema[0,:] = hull[np.argmax(hull[:,0,0]),0,:]\n            extrema[1,:] = hull[np.argmax(hull[:,0,0]+hull[:,0,1]),0,:]\n            extrema[2,:] = hull[np.argmax(hull[:,0,1]),0,:]\n            extrema[3,:] = hull[np.argmax(-hull[:,0,0]+hull[:,0,1]),0,:]\n            extrema[4,:] = hull[np.argmax(-hull[:,0,0]),0,:]\n            extrema[5,:] = hull[np.argmax(-hull[:,0,0]-hull[:,0,1]),0,:]\n            extrema[6,:] = hull[np.argmax(-hull[:,0,1]),0,:]\n            extrema[7,:] = hull[np.argmax(hull[:,0,0]-hull[:,0,1]),0,:]\n\n            extrema_lines = extrema - np.roll(extrema, shift=1, axis=0)\n            extrema_len = extrema_lines[:,0]**2 + extrema_lines[:,1]**2\n            line_idx = np.sort(extrema_len.argsort()[-4:])\n            for c in range(4):\n                p1 = extrema[line_idx[(c-1)%4],:]\n                p2 = extrema[(line_idx[(c-1)%4]-1)%8,:]\n                p3 = extrema[line_idx[c],:]\n                p4 = extrema[(line_idx[c]-1)%8,:]\n                corners[c,:] = get_intersect(p1, p2, p3, p4)\n\n            new_area = poly_area(corners)\n            if new_area / area > 1.5:\n                continue\n\n            corners = order_points(corners)\n            corners_full_res = corners\n\n            pts_dst = np.array([[0,0],[399,0],[399,399],[0,399]])\n            h, status = cv2.findHomography(corners_full_res, pts_dst)\n            try:\n                warped_im = cv2.warpPerspective(frame_rgb, h, (400,400))\n                w_im = warped_im.astype(np.float32)\n                w_im /= 255.\n            except:\n                continue\n\n            for im_rotation in range(4):\n                w_rotated = np.rot90(w_im, im_rotation)\n                recovered_secret = decoder_sess.run([decoder_output],feed_dict={decoder_input:[w_rotated]})[0][0]\n                recovered_secret = list(recovered_secret)\n                recovered_secret = [int(i) for i in recovered_secret]\n\n                packet_binary = """".join([str(bit) for bit in recovered_secret[:96]])\n                footer = recovered_secret[96:]\n                if np.sum(footer) > 0:\n                    continue\n                packet = bytes(int(packet_binary[i : i + 8], 2) for i in range(0, len(packet_binary), 8))\n                packet = bytearray(packet)\n\n                data, ecc = packet[:-bch.ecc_bytes], packet[-bch.ecc_bytes:]\n\n                bitflips = bch.decode_inplace(data, ecc)\n\n                if bitflips != -1:\n                    print(\'Num bits corrected: \', bitflips)\n                    try:\n                        code = data.decode(""utf-8"")\n                    except:\n                        continue\n                    color = (100,250,100)\n                    cv2.polylines(frame, np.int32([corners]), thickness=6, color=color, isClosed=True)\n                    font = cv2.FONT_HERSHEY_SIMPLEX\n                    im = cv2.putText(frame, code, tuple((corners[0,:]+np.array([0,-15])).astype(np.int)), font, 1,(0,0,0), 2, cv2.LINE_AA)\n\n        if args.save_video is not None:\n            out.write(frame)\n        else:\n            cv2.imshow(\'frame\',frame)\n            if args.visualize_detector:\n                cv2.imshow(\'detector_mask\', mask_vis)\n            cv2.waitKey(1)\n\n    cap.release()\n    if args.save_video:\n        out.release()\n\nif __name__ == ""__main__"":\n    main()\n'"
encode_image.py,6,"b'import bchlib\nimport glob\nimport os\nfrom PIL import Image,ImageOps\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.image\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.saved_model import signature_constants\n\nBCH_POLYNOMIAL = 137\nBCH_BITS = 5\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model\', type=str)\n    parser.add_argument(\'--image\', type=str, default=None)\n    parser.add_argument(\'--images_dir\', type=str, default=None)\n    parser.add_argument(\'--save_dir\', type=str, default=None)\n    parser.add_argument(\'--secret\', type=str, default=\'Stega!!\')\n    parser.add_argument(\'--secret_size\', type=int, default=100)\n    args = parser.parse_args()\n\n    if args.image is not None:\n        files_list = [args.image]\n    elif args.images_dir is not None:\n        files_list = glob.glob(args.images_dir + \'/*\')\n    else:\n        print(\'Missing input image\')\n        return\n\n    sess = tf.InteractiveSession(graph=tf.Graph())\n\n    model = tf.saved_model.loader.load(sess, [tag_constants.SERVING], args.model)\n\n    input_secret_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[\'secret\'].name\n    input_image_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[\'image\'].name\n    input_secret = tf.get_default_graph().get_tensor_by_name(input_secret_name)\n    input_image = tf.get_default_graph().get_tensor_by_name(input_image_name)\n\n    output_stegastamp_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs[\'stegastamp\'].name\n    output_residual_name = model.signature_def[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs[\'residual\'].name\n    output_stegastamp = tf.get_default_graph().get_tensor_by_name(output_stegastamp_name)\n    output_residual = tf.get_default_graph().get_tensor_by_name(output_residual_name)\n\n    width = 400\n    height = 400\n\n    bch = bchlib.BCH(BCH_POLYNOMIAL, BCH_BITS)\n\n    if len(args.secret) > 7:\n        print(\'Error: Can only encode 56bits (7 characters) with ECC\')\n        return\n\n    data = bytearray(args.secret + \' \'*(7-len(args.secret)), \'utf-8\')\n    ecc = bch.encode(data)\n    packet = data + ecc\n\n    packet_binary = \'\'.join(format(x, \'08b\') for x in packet)\n    secret = [int(x) for x in packet_binary]\n    secret.extend([0,0,0,0])\n\n    if args.save_dir is not None:\n        if not os.path.exists(args.save_dir):\n            os.makedirs(args.save_dir)\n        size = (width, height)\n        for filename in files_list:\n            image = Image.open(filename).convert(""RGB"")\n            image = np.array(ImageOps.fit(image,size),dtype=np.float32)\n            image /= 255.\n\n            feed_dict = {input_secret:[secret],\n                         input_image:[image]}\n\n            hidden_img, residual = sess.run([output_stegastamp, output_residual],feed_dict=feed_dict)\n\n            rescaled = (hidden_img[0] * 255).astype(np.uint8)\n            raw_img = (image * 255).astype(np.uint8)\n            residual = residual[0]+.5\n\n            residual = (residual * 255).astype(np.uint8)\n\n            save_name = filename.split(\'/\')[-1].split(\'.\')[0]\n\n            im = Image.fromarray(np.array(rescaled))\n            im.save(args.save_dir + \'/\'+save_name+\'_hidden.png\')\n\n            im = Image.fromarray(np.squeeze(np.array(residual)))\n            im.save(args.save_dir + \'/\'+save_name+\'_residual.png\')\n\nif __name__ == ""__main__"":\n    main()\n'"
models.py,72,"b'import numpy as np\nimport os\nimport lpips.lpips_tf as lpips_tf\nimport tensorflow as tf\nimport utils\nfrom tensorflow import keras\nfrom tensorflow.python.keras.models import *\nfrom tensorflow.python.keras.layers import *\nfrom stn import spatial_transformer_network as stn_transformer\n\nclass StegaStampEncoder(Layer):\n    def __init__(self, height, width):\n        super(StegaStampEncoder, self).__init__()\n        self.secret_dense = Dense(7500, activation=\'relu\', kernel_initializer=\'he_normal\')\n\n        self.conv1 = Conv2D(32, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv2 = Conv2D(32, 3, activation=\'relu\', strides=2, padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv3 = Conv2D(64, 3, activation=\'relu\', strides=2, padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv4 = Conv2D(128, 3, activation=\'relu\', strides=2, padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv5 = Conv2D(256, 3, activation=\'relu\', strides=2, padding=\'same\', kernel_initializer=\'he_normal\')\n        self.up6 = Conv2D(128, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv6 = Conv2D(128, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.up7 = Conv2D(64, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv7 = Conv2D(64, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.up8 = Conv2D(32, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv8 = Conv2D(32, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.up9 = Conv2D(32, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv9 = Conv2D(32, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.conv10 = Conv2D(32, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'he_normal\')\n        self.residual = Conv2D(3, 1, activation=None, padding=\'same\', kernel_initializer=\'he_normal\')\n\n    def call(self, inputs):\n        secret, image = inputs\n        secret = secret - .5\n        image = image - .5\n\n        secret = self.secret_dense(secret)\n        secret = Reshape((50, 50, 3))(secret)\n        secret_enlarged = UpSampling2D(size=(8,8))(secret)\n\n        inputs = concatenate([secret_enlarged, image], axis=-1)\n        conv1 = self.conv1(inputs)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n        up6 = self.up6(UpSampling2D(size=(2,2))(conv5))\n        merge6 = concatenate([conv4,up6], axis=3)\n        conv6 = self.conv6(merge6)\n        up7 = self.up7(UpSampling2D(size=(2,2))(conv6))\n        merge7 = concatenate([conv3,up7], axis=3)\n        conv7 = self.conv7(merge7)\n        up8 = self.up8(UpSampling2D(size=(2,2))(conv7))\n        merge8 = concatenate([conv2,up8], axis=3)\n        conv8 = self.conv8(merge8)\n        up9 = self.up9(UpSampling2D(size=(2,2))(conv8))\n        merge9 = concatenate([conv1,up9,inputs], axis=3)\n        conv9 = self.conv9(merge9)\n        conva = self.conv9(merge9)\n        conv10 = self.conv10(conv9)\n        residual = self.residual(conv9)\n        return residual\n\nclass StegaStampDecoder(Layer):\n    def __init__(self, secret_size, height, width):\n        super(StegaStampDecoder, self).__init__()\n        self.height = height\n        self.width = width\n        self.stn_params = Sequential([\n            Conv2D(32, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(64, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(128, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Flatten(),\n            Dense(128, activation=\'relu\')\n        ])\n        initial = np.array([[1., 0, 0], [0, 1., 0]])\n        initial = initial.astype(\'float32\').flatten()\n\n        self.W_fc1 = tf.Variable(tf.zeros([128, 6]), name=\'W_fc1\')\n        self.b_fc1 = tf.Variable(initial_value=initial, name=\'b_fc1\')\n\n        self.decoder = Sequential([\n            Conv2D(32, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\'),\n            Conv2D(64, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(64, (3, 3), activation=\'relu\', padding=\'same\'),\n            Conv2D(64, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(128, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(128, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Flatten(),\n            Dense(512, activation=\'relu\'),\n            Dense(secret_size)\n        ])\n\n    def call(self, image):\n        image = image - .5\n        stn_params = self.stn_params(image)\n        x = tf.matmul(stn_params, self.W_fc1) + self.b_fc1\n        transformed_image = stn_transformer(image, x, [self.height, self.width, 3])\n        return self.decoder(transformed_image)\n\nclass Discriminator(Layer):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = Sequential([\n            Conv2D(8, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(16, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(32, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(64, (3, 3), strides=2, activation=\'relu\', padding=\'same\'),\n            Conv2D(1, (3, 3), activation=None, padding=\'same\')\n        ])\n\n    def call(self, image):\n            x = image - .5\n            x = self.model(x)\n            output = tf.reduce_mean(x)\n            return output, x\n\ndef transform_net(encoded_image, args, global_step):\n    sh = tf.shape(encoded_image)\n\n    ramp_fn = lambda ramp : tf.minimum(tf.to_float(global_step) / ramp, 1.)\n\n    rnd_bri = ramp_fn(args.rnd_bri_ramp) * args.rnd_bri\n    rnd_hue = ramp_fn(args.rnd_hue_ramp) * args.rnd_hue\n    rnd_brightness = utils.get_rnd_brightness_tf(rnd_bri, rnd_hue, args.batch_size)\n\n    jpeg_quality = 100. - tf.random.uniform([]) * ramp_fn(args.jpeg_quality_ramp) * (100.-args.jpeg_quality)\n    jpeg_factor = tf.cond(tf.less(jpeg_quality, 50), lambda: 5000. / jpeg_quality, lambda: 200. - jpeg_quality * 2) / 100. + .0001\n\n    rnd_noise = tf.random.uniform([]) * ramp_fn(args.rnd_noise_ramp) * args.rnd_noise\n\n    contrast_low = 1. - (1. - args.contrast_low) * ramp_fn(args.contrast_ramp)\n    contrast_high = 1. + (args.contrast_high - 1.) * ramp_fn(args.contrast_ramp)\n    contrast_params = [contrast_low, contrast_high]\n\n    rnd_sat = tf.random.uniform([]) * ramp_fn(args.rnd_sat_ramp) * args.rnd_sat\n\n    # blur\n    f = utils.random_blur_kernel(probs=[.25,.25], N_blur=7,\n                           sigrange_gauss=[1.,3.], sigrange_line=[.25,1.], wmin_line=3)\n    encoded_image = tf.nn.conv2d(encoded_image, f, [1,1,1,1], padding=\'SAME\')\n\n    noise = tf.random_normal(shape=tf.shape(encoded_image), mean=0.0, stddev=rnd_noise, dtype=tf.float32)\n    encoded_image = encoded_image + noise\n    encoded_image = tf.clip_by_value(encoded_image, 0, 1)\n\n    contrast_scale = tf.random_uniform(shape=[tf.shape(encoded_image)[0]], minval=contrast_params[0], maxval=contrast_params[1])\n    contrast_scale = tf.reshape(contrast_scale, shape=[tf.shape(encoded_image)[0],1,1,1])\n\n    encoded_image = encoded_image * contrast_scale\n    encoded_image = encoded_image + rnd_brightness\n    encoded_image = tf.clip_by_value(encoded_image, 0, 1)\n\n\n    encoded_image_lum = tf.expand_dims(tf.reduce_sum(encoded_image * tf.constant([.3,.6,.1]), axis=3), 3)\n    encoded_image = (1 - rnd_sat) * encoded_image + rnd_sat * encoded_image_lum\n\n    encoded_image = tf.reshape(encoded_image, [-1,400,400,3])\n    if not args.no_jpeg:\n        encoded_image = utils.jpeg_compress_decompress(encoded_image, rounding=utils.round_only_at_0, factor=jpeg_factor, downsample_c=True)\n\n    summaries = [tf.summary.scalar(\'transformer/rnd_bri\', rnd_bri),\n                 tf.summary.scalar(\'transformer/rnd_sat\', rnd_sat),\n                 tf.summary.scalar(\'transformer/rnd_hue\', rnd_hue),\n                 tf.summary.scalar(\'transformer/rnd_noise\', rnd_noise),\n                 tf.summary.scalar(\'transformer/contrast_low\', contrast_low),\n                 tf.summary.scalar(\'transformer/contrast_high\', contrast_high),\n                 tf.summary.scalar(\'transformer/jpeg_quality\', jpeg_quality)]\n\n    return encoded_image, summaries\n\n\ndef get_secret_acc(secret_true,secret_pred):\n    with tf.variable_scope(""acc""):\n        secret_pred = tf.round(tf.sigmoid(secret_pred))\n        correct_pred = tf.to_int64(tf.shape(secret_pred)[1]) - tf.count_nonzero(secret_pred - secret_true, axis=1)\n\n        str_acc = 1.0 - tf.count_nonzero(correct_pred - tf.to_int64(tf.shape(secret_pred)[1])) / tf.size(correct_pred, out_type=tf.int64)\n\n        bit_acc = tf.reduce_sum(correct_pred) / tf.size(secret_pred, out_type=tf.int64)\n        return bit_acc, str_acc\n\ndef build_model(encoder,\n                decoder,\n                discriminator,\n                secret_input,\n                image_input,\n                l2_edge_gain,\n                borders,\n                secret_size,\n                M,\n                loss_scales,\n                yuv_scales,\n                args,\n                global_step):\n\n    input_warped = tf.contrib.image.transform(image_input, M[:,1,:], interpolation=\'BILINEAR\')\n    mask_warped = tf.contrib.image.transform(tf.ones_like(input_warped), M[:,1,:], interpolation=\'BILINEAR\')\n    input_warped += (1-mask_warped) * image_input\n\n    residual_warped = encoder((secret_input, input_warped))\n    encoded_warped = residual_warped + input_warped\n    residual = tf.contrib.image.transform(residual_warped, M[:,0,:], interpolation=\'BILINEAR\')\n\n    if borders == \'no_edge\':\n        encoded_image = image_input + residual\n    elif borders == \'black\':\n        encoded_image = residual_warped + input_warped\n        encoded_image = tf.contrib.image.transform(encoded_image, M[:,0,:], interpolation=\'BILINEAR\')\n        input_unwarped = tf.contrib.image.transform(input_warped, M[:,0,:], interpolation=\'BILINEAR\')\n    elif borders.startswith(\'random\'):\n        mask = tf.contrib.image.transform(tf.ones_like(residual), M[:,0,:], interpolation=\'BILINEAR\')\n        encoded_image = residual_warped + input_warped\n        encoded_image = tf.contrib.image.transform(encoded_image, M[:,0,:], interpolation=\'BILINEAR\')\n        input_unwarped = tf.contrib.image.transform(input_warped, M[:,0,:], interpolation=\'BILINEAR\')\n        ch = 3 if borders.endswith(\'rgb\') else 1\n        encoded_image += (1-mask) * tf.ones_like(residual) * tf.random.uniform([ch])\n    elif borders == \'white\':\n        mask = tf.contrib.image.transform(tf.ones_like(residual), M[:,0,:], interpolation=\'BILINEAR\')\n        encoded_image = residual_warped + input_warped\n        encoded_image = tf.contrib.image.transform(encoded_image, M[:,0,:], interpolation=\'BILINEAR\')\n        input_unwarped = tf.contrib.image.transform(input_warped, M[:,0,:], interpolation=\'BILINEAR\')\n        encoded_image += (1-mask) * tf.ones_like(residual)\n    elif borders == \'image\':\n        mask = tf.contrib.image.transform(tf.ones_like(residual), M[:,0,:], interpolation=\'BILINEAR\')\n        encoded_image = residual_warped + input_warped\n        encoded_image = tf.contrib.image.transform(encoded_image, M[:,0,:], interpolation=\'BILINEAR\')\n        encoded_image += (1-mask) * tf.manip.roll(image_input, shift=1, axis=0)\n\n    if borders == \'no_edge\':\n        D_output_real, _ = discriminator(image_input)\n        D_output_fake, D_heatmap = discriminator(encoded_image)\n    else:\n        D_output_real, _ = discriminator(input_warped)\n        D_output_fake, D_heatmap = discriminator(encoded_warped)\n\n    transformed_image, transform_summaries = transform_net(encoded_image, args, global_step)\n\n    decoded_secret = decoder(transformed_image)\n\n    bit_acc, str_acc = get_secret_acc(secret_input, decoded_secret)\n\n    lpips_loss_op = tf.reduce_mean(lpips_tf.lpips(image_input, encoded_image))\n    secret_loss_op = tf.losses.sigmoid_cross_entropy(secret_input, decoded_secret)\n\n    size = (int(image_input.shape[1]),int(image_input.shape[2]))\n    gain = 10\n    falloff_speed = 4 # Cos dropoff that reaches 0 at distance 1/x into image\n    falloff_im = np.ones(size)\n    for i in range(int(falloff_im.shape[0]/falloff_speed)):\n        falloff_im[-i,:] *= (np.cos(4*np.pi*i/size[0]+np.pi)+1)/2\n        falloff_im[i,:] *= (np.cos(4*np.pi*i/size[0]+np.pi)+1)/2\n    for j in range(int(falloff_im.shape[1]/falloff_speed)):\n        falloff_im[:,-j] *= (np.cos(4*np.pi*j/size[0]+np.pi)+1)/2\n        falloff_im[:,j] *= (np.cos(4*np.pi*j/size[0]+np.pi)+1)/2\n    falloff_im = 1-falloff_im\n    falloff_im = tf.convert_to_tensor(falloff_im, dtype=tf.float32)\n    falloff_im *= l2_edge_gain\n\n    encoded_image_yuv = tf.image.rgb_to_yuv(encoded_image)\n    image_input_yuv = tf.image.rgb_to_yuv(image_input)\n    im_diff = encoded_image_yuv-image_input_yuv\n    im_diff += im_diff * tf.expand_dims(falloff_im, axis=[-1])\n    yuv_loss_op = tf.reduce_mean(tf.square(im_diff), axis=[0,1,2])\n    image_loss_op = tf.tensordot(yuv_loss_op, yuv_scales, axes=1)\n\n    D_loss = D_output_real - D_output_fake\n    G_loss = D_output_fake\n\n    loss_op = loss_scales[0]*image_loss_op + loss_scales[1]*lpips_loss_op + loss_scales[2]*secret_loss_op\n    if not args.no_gan:\n        loss_op += loss_scales[3]*G_loss\n\n    summary_op = tf.summary.merge([\n        tf.summary.scalar(\'bit_acc\', bit_acc, family=\'train\'),\n        tf.summary.scalar(\'str_acc\', str_acc, family=\'train\'),\n        tf.summary.scalar(\'loss\', loss_op, family=\'train\'),\n        tf.summary.scalar(\'image_loss\', image_loss_op, family=\'train\'),\n        tf.summary.scalar(\'lpip_loss\', lpips_loss_op, family=\'train\'),\n        tf.summary.scalar(\'G_loss\', G_loss, family=\'train\'),\n        tf.summary.scalar(\'secret_loss\', secret_loss_op, family=\'train\'),\n        tf.summary.scalar(\'dis_loss\', D_loss, family=\'train\'),\n        tf.summary.scalar(\'Y_loss\', yuv_loss_op[0], family=\'color_loss\'),\n        tf.summary.scalar(\'U_loss\', yuv_loss_op[1], family=\'color_loss\'),\n        tf.summary.scalar(\'V_loss\', yuv_loss_op[2], family=\'color_loss\'),\n    ] + transform_summaries)\n\n    image_summary_op = tf.summary.merge([\n        image_to_summary(image_input, \'image_input\', family=\'input\'),\n        image_to_summary(input_warped, \'image_warped\', family=\'input\'),\n        image_to_summary(encoded_warped, \'encoded_warped\', family=\'encoded\'),\n        image_to_summary(residual_warped+.5, \'residual\', family=\'encoded\'),\n        image_to_summary(encoded_image, \'encoded_image\', family=\'encoded\'),\n        image_to_summary(transformed_image, \'transformed_image\', family=\'transformed\'),\n        image_to_summary(D_heatmap, \'discriminator\', family=\'losses\'),\n    ])\n\n    return loss_op, secret_loss_op, D_loss, summary_op, image_summary_op, bit_acc\n\ndef image_to_summary(image, name, family=\'train\'):\n    image = tf.clip_by_value(image, 0, 1)\n    image = tf.cast(image * 255, dtype=tf.uint8)\n    summary = tf.summary.image(name,image,max_outputs=1,family=family)\n    return summary\n\ndef prepare_deployment_hiding_graph(encoder, secret_input, image_input):\n\n    residual = encoder((secret_input, image_input))\n    encoded_image = residual + image_input\n    encoded_image = tf.clip_by_value(encoded_image, 0, 1)\n\n    return encoded_image, residual\n\ndef prepare_deployment_reveal_graph(decoder, image_input):\n    decoded_secret = decoded_secret = decoder(image_input)\n\n    return tf.round(tf.sigmoid(decoded_secret))\n'"
train.py,32,"b'import glob\nimport os\nfrom PIL import Image,ImageOps\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport utils\nimport models\nfrom os.path import join\n\nTRAIN_PATH = \'./data/mirflickr/images1/images/\'\nLOGS_Path = ""./logs/""\nCHECKPOINTS_PATH = \'./checkpoints/\'\nSAVED_MODELS = \'./saved_models\'\n\nif not os.path.exists(CHECKPOINTS_PATH):\n    os.makedirs(CHECKPOINTS_PATH)\n\ndef get_img_batch(files_list,\n                  secret_size,\n                  batch_size=4,\n                  size=(400,400)):\n\n    batch_cover = []\n    batch_secret = []\n\n    for i in range(batch_size):\n        img_cover_path = random.choice(files_list)\n        try:\n            img_cover = Image.open(img_cover_path).convert(""RGB"")\n            img_cover = ImageOps.fit(img_cover, size)\n            img_cover = np.array(img_cover, dtype=np.float32) / 255.\n        except:\n            img_cover = np.zeros((size[0],size[1],3), dtype=np.float32)\n        batch_cover.append(img_cover)\n\n        secret = np.random.binomial(1, .5, secret_size)\n        batch_secret.append(secret)\n\n    batch_cover, batch_secret = np.array(batch_cover), np.array(batch_secret)\n    return batch_cover, batch_secret\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'exp_name\', type=str)\n    parser.add_argument(\'--secret_size\', type=int, default=20)\n    parser.add_argument(\'--num_steps\', type=int, default=140000)\n    parser.add_argument(\'--batch_size\', type=int, default=4)\n    parser.add_argument(\'--lr\', type=float, default=.0001)\n    parser.add_argument(\'--l2_loss_scale\', type=float, default=1.5)\n    parser.add_argument(\'--l2_loss_ramp\', type=int, default=20000)\n    parser.add_argument(\'--l2_edge_gain\', type=float, default=10.0)\n    parser.add_argument(\'--l2_edge_ramp\', type=int, default=20000)\n    parser.add_argument(\'--l2_edge_delay\', type=int, default=60000)\n    parser.add_argument(\'--lpips_loss_scale\', type=float, default=1)\n    parser.add_argument(\'--lpips_loss_ramp\', type=int, default=20000)\n    parser.add_argument(\'--secret_loss_scale\', type=float, default=1)\n    parser.add_argument(\'--secret_loss_ramp\', type=int, default=1)\n    parser.add_argument(\'--G_loss_scale\', type=float, default=1)\n    parser.add_argument(\'--G_loss_ramp\', type=int, default=20000)\n    parser.add_argument(\'--borders\', type=str, choices=[\'no_edge\',\'black\',\'random\',\'randomrgb\',\'image\',\'white\'], default=\'black\')\n    parser.add_argument(\'--y_scale\', type=float, default=1.0)\n    parser.add_argument(\'--u_scale\', type=float, default=1.0)\n    parser.add_argument(\'--v_scale\', type=float, default=1.0)\n    parser.add_argument(\'--no_gan\', action=\'store_true\')\n    parser.add_argument(\'--rnd_trans\', type=float, default=.1)\n    parser.add_argument(\'--rnd_bri\', type=float, default=.3)\n    parser.add_argument(\'--rnd_noise\', type=float, default=.02)\n    parser.add_argument(\'--rnd_sat\', type=float, default=1.0)\n    parser.add_argument(\'--rnd_hue\', type=float, default=.1)\n    parser.add_argument(\'--contrast_low\', type=float, default=.5)\n    parser.add_argument(\'--contrast_high\', type=float, default=1.5)\n    parser.add_argument(\'--jpeg_quality\', type=float, default=25)\n    parser.add_argument(\'--no_jpeg\', action=\'store_true\')\n    parser.add_argument(\'--rnd_trans_ramp\', type=int, default=10000)\n    parser.add_argument(\'--rnd_bri_ramp\', type=int, default=1000)\n    parser.add_argument(\'--rnd_sat_ramp\', type=int, default=1000)\n    parser.add_argument(\'--rnd_hue_ramp\', type=int, default=1000)\n    parser.add_argument(\'--rnd_noise_ramp\', type=int, default=1000)\n    parser.add_argument(\'--contrast_ramp\', type=int, default=1000)\n    parser.add_argument(\'--jpeg_quality_ramp\', type=float, default=1000)\n    parser.add_argument(\'--no_im_loss_steps\', help=""Train without image loss for first x steps"", type=int, default=500)\n    parser.add_argument(\'--pretrained\', type=str, default=None)\n    args = parser.parse_args()\n\n    EXP_NAME = args.exp_name\n\n    files_list = glob.glob(join(TRAIN_PATH,""**/*""))\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n\n    height = 400\n    width = 400\n\n    secret_pl = tf.placeholder(shape=[None,args.secret_size],dtype=tf.float32,name=""input_prep"")\n    image_pl = tf.placeholder(shape=[None,height,width,3],dtype=tf.float32,name=""input_hide"")\n    M_pl = tf.placeholder(shape=[None,2,8],dtype=tf.float32,name=""input_transform"")\n    global_step_tensor = tf.Variable(0, trainable=False, name=\'global_step\')\n    loss_scales_pl = tf.placeholder(shape=[4],dtype=tf.float32,name=""input_loss_scales"")\n    l2_edge_gain_pl = tf.placeholder(shape=[1],dtype=tf.float32,name=""input_edge_gain"")\n    yuv_scales_pl = tf.placeholder(shape=[3],dtype=tf.float32,name=""input_yuv_scales"")\n\n    log_decode_mod_pl = tf.placeholder(shape=[],dtype=tf.float32,name=""input_log_decode_mod"")\n\n    encoder = models.StegaStampEncoder(height=height, width=width)\n    decoder = models.StegaStampDecoder(secret_size=args.secret_size, height=height, width=width)\n    discriminator = models.Discriminator()\n\n    loss_op, secret_loss_op, D_loss_op, summary_op, image_summary_op, _ = models.build_model(\n            encoder=encoder,\n            decoder=decoder,\n            discriminator=discriminator,\n            secret_input=secret_pl,\n            image_input=image_pl,\n            l2_edge_gain=l2_edge_gain_pl,\n            borders=args.borders,\n            secret_size=args.secret_size,\n            M=M_pl,\n            loss_scales=loss_scales_pl,\n            yuv_scales=yuv_scales_pl,\n            args=args,\n            global_step=global_step_tensor)\n\n    tvars=tf.trainable_variables()  #returns all variables created(the two variable scopes) and makes trainable true\n\n\n    d_vars=[var for var in tvars if \'discriminator\' in var.name]\n    g_vars=[var for var in tvars if \'stega_stamp\' in var.name]\n\n    clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in d_vars]\n\n    train_op = tf.train.AdamOptimizer(args.lr).minimize(loss_op, var_list=g_vars, global_step=global_step_tensor)\n    train_secret_op = tf.train.AdamOptimizer(args.lr).minimize(secret_loss_op, var_list=g_vars, global_step=global_step_tensor)\n    optimizer = tf.train.RMSPropOptimizer(.00001)\n    gvs = optimizer.compute_gradients(D_loss_op, var_list=d_vars)\n    capped_gvs = [(tf.clip_by_value(grad, -.25, .25), var) for grad, var in gvs]\n    train_dis_op = optimizer.apply_gradients(capped_gvs)\n\n    deploy_hide_image_op, residual_op = models.prepare_deployment_hiding_graph(encoder, secret_pl, image_pl)\n    deploy_decoder_op =  models.prepare_deployment_reveal_graph(decoder, image_pl)\n\n    saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=100, keep_checkpoint_every_n_hours=4)\n    sess.run(tf.global_variables_initializer())\n\n    if args.pretrained is not None:\n        saver.restore(sess, args.pretrained)\n\n    writer = tf.summary.FileWriter(join(LOGS_Path,EXP_NAME),sess.graph)\n\n    total_steps = len(files_list)//args.batch_size + 1\n    global_step = 0\n\n    while global_step < args.num_steps:\n        for _ in range(min(total_steps,args.num_steps-global_step)):\n            no_im_loss = global_step < args.no_im_loss_steps\n            images, secrets = get_img_batch(files_list=files_list,\n                                                     secret_size=args.secret_size,\n                                                     batch_size=args.batch_size,\n                                                     size=(height,width))\n            l2_loss_scale = min(args.l2_loss_scale * global_step / args.l2_loss_ramp, args.l2_loss_scale)\n            lpips_loss_scale = min(args.lpips_loss_scale * global_step / args.lpips_loss_ramp, args.lpips_loss_scale)\n            secret_loss_scale = min(args.secret_loss_scale * global_step / args.secret_loss_ramp, args.secret_loss_scale)\n            G_loss_scale = min(args.G_loss_scale * global_step / args.G_loss_ramp, args.G_loss_scale)\n            l2_edge_gain = 0\n            if global_step > args.l2_edge_delay:\n                l2_edge_gain = min(args.l2_edge_gain * (global_step-args.l2_edge_delay) / args.l2_edge_ramp, args.l2_edge_gain)\n\n            rnd_tran = min(args.rnd_trans * global_step / args.rnd_trans_ramp, args.rnd_trans)\n            rnd_tran = np.random.uniform() * rnd_tran\n            M = utils.get_rand_transform_matrix(width, np.floor(width * rnd_tran), args.batch_size)\n\n            feed_dict = {secret_pl:secrets,\n                         image_pl:images,\n                         M_pl:M,\n                         l2_edge_gain_pl:[l2_edge_gain],\n                         loss_scales_pl:[l2_loss_scale, lpips_loss_scale, secret_loss_scale, G_loss_scale],\n                         yuv_scales_pl:[args.y_scale, args.u_scale, args.v_scale],}\n\n            if no_im_loss:\n                _, _, global_step = sess.run([train_secret_op,loss_op,global_step_tensor],feed_dict)\n            else:\n                _, _, global_step = sess.run([train_op,loss_op,global_step_tensor],feed_dict)\n                if not args.no_gan:\n                    sess.run([train_dis_op, clip_D],feed_dict)\n\n            if global_step % 100 ==0 :\n                summary, global_step = sess.run([summary_op,global_step_tensor], feed_dict)\n                writer.add_summary(summary, global_step)\n                summary = tf.Summary(value=[tf.Summary.Value(tag=\'transformer/rnd_tran\', simple_value=rnd_tran),\n                                            tf.Summary.Value(tag=\'loss_scales/l2_loss_scale\', simple_value=l2_loss_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/lpips_loss_scale\', simple_value=lpips_loss_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/secret_loss_scale\', simple_value=secret_loss_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/y_scale\', simple_value=args.y_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/u_scale\', simple_value=args.u_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/v_scale\', simple_value=args.v_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/G_loss_scale\', simple_value=G_loss_scale),\n                                            tf.Summary.Value(tag=\'loss_scales/L2_edge_gain\', simple_value=l2_edge_gain),])\n                writer.add_summary(summary, global_step)\n\n            if global_step % 100 ==0 :\n                summary, global_step = sess.run([image_summary_op,global_step_tensor], feed_dict)\n                writer.add_summary(summary, global_step)\n\n            if global_step % 10000 ==0:\n                save_path = saver.save(sess, join(CHECKPOINTS_PATH,EXP_NAME,EXP_NAME+"".chkp""), global_step=global_step)\n\n    constant_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            sess.graph.as_graph_def(),\n            [deploy_hide_image_op.name[:-2], residual_op.name[:-2], deploy_decoder_op.name[:-2]])\n    with tf.Session(graph=tf.Graph()) as session:\n        tf.import_graph_def(constant_graph_def, name=\'\')\n        tf.saved_model.simple_save(session,\n                                   SAVED_MODELS + \'/\' + EXP_NAME,\n                                   inputs={\'secret\':secret_pl, \'image\':image_pl},\n                                   outputs={\'stegastamp\':deploy_hide_image_op, \'residual\':residual_op, \'decoded\':deploy_decoder_op})\n\n    writer.close()\n\nif __name__ == ""__main__"":\n    main()\n'"
utils.py,53,"b'import cv2\nimport itertools\nimport numpy as np\nimport random\nimport tensorflow as tf\n\n\ndef random_blur_kernel(probs, N_blur, sigrange_gauss, sigrange_line, wmin_line):\n    N = N_blur\n    coords = tf.to_float(tf.stack(tf.meshgrid(tf.range(N_blur), tf.range(N_blur), indexing=\'ij\'), -1)) - (.5 * (N-1))\n    # coords = tf.to_float(coords)\n    manhat = tf.reduce_sum(tf.abs(coords), -1)\n\n    # nothing, default\n\n    vals_nothing = tf.to_float(manhat < .5)\n\n    # gauss\n\n    sig_gauss = tf.random.uniform([], sigrange_gauss[0], sigrange_gauss[1])\n    vals_gauss = tf.exp(-tf.reduce_sum(coords**2, -1)/2./sig_gauss**2)\n\n    # line\n\n    theta = tf.random_uniform([], 0, 2.*np.pi)\n    v = tf.convert_to_tensor([tf.cos(theta), tf.sin(theta)])\n    dists = tf.reduce_sum(coords * v, -1)\n\n    sig_line = tf.random.uniform([], sigrange_line[0], sigrange_line[1])\n    w_line = tf.random.uniform([], wmin_line, .5 * (N-1) + .1)\n\n    vals_line = tf.exp(-dists**2/2./sig_line**2) * tf.to_float(manhat < w_line)\n\n    t = tf.random_uniform([])\n    vals = vals_nothing\n    vals = tf.cond(t < probs[0]+probs[1], lambda : vals_line, lambda : vals)\n    vals = tf.cond(t < probs[0], lambda : vals_gauss, lambda : vals)\n\n    v = vals / tf.reduce_sum(vals)\n    z = tf.zeros_like(v)\n    f = tf.reshape(tf.stack([v,z,z, z,v,z, z,z,v],-1), [N,N,3,3])\n\n    return f\n\n\ndef get_rand_transform_matrix(image_size, d, batch_size):\n    Ms = np.zeros((batch_size, 2, 8))\n\n    for i in range(batch_size):\n        tl_x = random.uniform(-d, d)     # Top left corner, top\n        tl_y = random.uniform(-d, d)    # Top left corner, left\n        bl_x = random.uniform(-d, d)  # Bot left corner, bot\n        bl_y = random.uniform(-d, d)    # Bot left corner, left\n        tr_x = random.uniform(-d, d)     # Top right corner, top\n        tr_y = random.uniform(-d, d)   # Top right corner, right\n        br_x = random.uniform(-d, d)  # Bot right corner, bot\n        br_y = random.uniform(-d, d)   # Bot right corner, right\n\n        rect = np.array([\n            [tl_x, tl_y],\n            [tr_x + image_size, tr_y],\n            [br_x + image_size, br_y + image_size],\n            [bl_x, bl_y +  image_size]], dtype = ""float32"")\n\n        dst = np.array([\n            [0, 0],\n            [image_size, 0],\n            [image_size, image_size],\n            [0, image_size]], dtype = ""float32"")\n\n        M = cv2.getPerspectiveTransform(rect, dst)\n        M_inv = np.linalg.inv(M)\n        Ms[i,0,:] = M_inv.flatten()[:8]\n        Ms[i,1,:] = M.flatten()[:8]\n    return Ms\n\ndef get_rnd_brightness_tf(rnd_bri, rnd_hue, batch_size):\n    rnd_hue = tf.random.uniform((batch_size,1,1,3), -rnd_hue, rnd_hue)\n    rnd_brightness = tf.random.uniform((batch_size,1,1,1), -rnd_bri, rnd_bri)\n    return rnd_hue + rnd_brightness\n\n\n## Differentiable JPEG, Source - https://github.com/rshin/differentiable-jpeg/blob/master/jpeg-tensorflow.ipynb\n\n# 1. RGB -> YCbCr\n# https://en.wikipedia.org/wiki/YCbCr\ndef rgb_to_ycbcr(image):\n  matrix = np.array(\n      [[65.481, 128.553, 24.966], [-37.797, -74.203, 112.],\n       [112., -93.786, -18.214]],\n      dtype=np.float32).T / 255\n  shift = [16., 128., 128.]\n\n  result = tf.tensordot(image, matrix, axes=1) + shift\n  result.set_shape(image.shape.as_list())\n  return result\n\n\ndef rgb_to_ycbcr_jpeg(image):\n  matrix = np.array(\n      [[0.299, 0.587, 0.114], [-0.168736, -0.331264, 0.5],\n       [0.5, -0.418688, -0.081312]],\n      dtype=np.float32).T\n  shift = [0., 128., 128.]\n\n  result = tf.tensordot(image, matrix, axes=1) + shift\n  result.set_shape(image.shape.as_list())\n  return result\n\n\n# 2. Chroma subsampling\ndef downsampling_420(image):\n  # input: batch x height x width x 3\n  # output: tuple of length 3\n  #   y:  batch x height x width\n  #   cb: batch x height/2 x width/2\n  #   cr: batch x height/2 x width/2\n  y, cb, cr = tf.split(image, 3, axis=3)\n  cb = tf.nn.avg_pool(\n      cb, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n  cr = tf.nn.avg_pool(\n      cr, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n  return (tf.squeeze(\n      y, axis=-1), tf.squeeze(\n          cb, axis=-1), tf.squeeze(\n              cr, axis=-1))\n\n\n# 3. Block splitting\n# From https://stackoverflow.com/questions/41564321/split-image-tensor-into-small-patches\ndef image_to_patches(image):\n  # input: batch x h x w\n  # output: batch x h*w/64 x h x w\n  k = 8\n  height, width = image.shape.as_list()[1:3]\n  batch_size = tf.shape(image)[0]\n  image_reshaped = tf.reshape(image, [batch_size, height // k, k, -1, k])\n  image_transposed = tf.transpose(image_reshaped, [0, 1, 3, 2, 4])\n  return tf.reshape(image_transposed, [batch_size, -1, k, k])\n\n\n# 4. DCT\ndef dct_8x8_ref(image):\n  image = image - 128\n  result = np.zeros((8, 8), dtype=np.float32)\n  for u, v in itertools.product(range(8), range(8)):\n    value = 0\n    for x, y in itertools.product(range(8), range(8)):\n      value += image[x, y] * np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n          (2 * y + 1) * v * np.pi / 16)\n    result[u, v] = value\n  alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n  scale = np.outer(alpha, alpha) * 0.25\n  return result * scale\n\n\ndef dct_8x8(image):\n  image = image - 128\n  tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n  for x, y, u, v in itertools.product(range(8), repeat=4):\n    tensor[x, y, u, v] = np.cos((2 * x + 1) * u * np.pi / 16) * np.cos(\n        (2 * y + 1) * v * np.pi / 16)\n  alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n  scale = np.outer(alpha, alpha) * 0.25\n  result = scale * tf.tensordot(image, tensor, axes=2)\n  result.set_shape(image.shape.as_list())\n  return result\n\n\n# 5. Quantizaztion\ny_table = np.array(\n    [[16, 11, 10, 16, 24, 40, 51, 61], [12, 12, 14, 19, 26, 58, 60,\n                                        55], [14, 13, 16, 24, 40, 57, 69, 56],\n     [14, 17, 22, 29, 51, 87, 80, 62], [18, 22, 37, 56, 68, 109, 103,\n                                        77], [24, 35, 55, 64, 81, 104, 113, 92],\n     [49, 64, 78, 87, 103, 121, 120, 101], [72, 92, 95, 98, 112, 100, 103, 99]],\n    dtype=np.float32).T\nc_table = np.empty((8, 8), dtype=np.float32)\nc_table.fill(99)\nc_table[:4, :4] = np.array([[17, 18, 24, 47], [18, 21, 26, 66],\n                            [24, 26, 56, 99], [47, 66, 99, 99]]).T\n\n\ndef y_quantize(image, rounding, factor=1):\n  image = image / (y_table * factor)\n  image = rounding(image)\n  return image\n\n\ndef c_quantize(image, rounding, factor=1):\n  image = image / (c_table * factor)\n  image = rounding(image)\n  return image\n\n\n# -5. Dequantization\ndef y_dequantize(image, factor=1):\n  return image * (y_table * factor)\n\n\ndef c_dequantize(image, factor=1):\n  return image * (c_table * factor)\n\n\n# -4. Inverse DCT\ndef idct_8x8_ref(image):\n  alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n  alpha = np.outer(alpha, alpha)\n  image = image * alpha\n\n  result = np.zeros((8, 8), dtype=np.float32)\n  for u, v in itertools.product(range(8), range(8)):\n    value = 0\n    for x, y in itertools.product(range(8), range(8)):\n      value += image[x, y] * np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n          (2 * v + 1) * y * np.pi / 16)\n    result[u, v] = value\n  return result * 0.25 + 128\n\n\ndef idct_8x8(image):\n  alpha = np.array([1. / np.sqrt(2)] + [1] * 7)\n  alpha = np.outer(alpha, alpha)\n  image = image * alpha\n\n  tensor = np.zeros((8, 8, 8, 8), dtype=np.float32)\n  for x, y, u, v in itertools.product(range(8), repeat=4):\n    tensor[x, y, u, v] = np.cos((2 * u + 1) * x * np.pi / 16) * np.cos(\n        (2 * v + 1) * y * np.pi / 16)\n  result = 0.25 * tf.tensordot(image, tensor, axes=2) + 128\n  result.set_shape(image.shape.as_list())\n  return result\n\n\n# -3. Block joining\ndef patches_to_image(patches, height, width):\n  # input: batch x h*w/64 x h x w\n  # output: batch x h x w\n  k = 8\n  batch_size = tf.shape(patches)[0]\n  image_reshaped = tf.reshape(patches,\n                              [batch_size, height // k, width // k, k, k])\n  image_transposed = tf.transpose(image_reshaped, [0, 1, 3, 2, 4])\n  return tf.reshape(image_transposed, [batch_size, height, width])\n\n\n# -2. Chroma upsampling\ndef upsampling_420(y, cb, cr):\n  # input:\n  #   y:  batch x height x width\n  #   cb: batch x height/2 x width/2\n  #   cr: batch x height/2 x width/2\n  # output:\n  #   image: batch x height x width x 3\n  def repeat(x, k=2):\n    height, width = x.shape.as_list()[1:3]\n    x = tf.expand_dims(x, -1)\n    x = tf.tile(x, [1, 1, k, k])\n    x = tf.reshape(x, [-1, height * k, width * k])\n    return x\n\n  cb = repeat(cb)\n  cr = repeat(cr)\n  return tf.stack((y, cb, cr), axis=-1)\n\n\n# -1. YCbCr -> RGB\ndef ycbcr_to_rgb(image):\n  matrix = np.array(\n      [[298.082, 0, 408.583], [298.082, -100.291, -208.120],\n       [298.082, 516.412, 0]],\n      dtype=np.float32).T / 256\n  shift = [-222.921, 135.576, -276.836]\n\n  result = tf.tensordot(image, matrix, axes=1) + shift\n  result.set_shape(image.shape.as_list())\n  return result\n\n\ndef ycbcr_to_rgb_jpeg(image):\n  matrix = np.array(\n      [[1., 0., 1.402], [1, -0.344136, -0.714136], [1, 1.772, 0]],\n      dtype=np.float32).T\n  shift = [0, -128, -128]\n\n  result = tf.tensordot(image + shift, matrix, axes=1)\n  result.set_shape(image.shape.as_list())\n  return result\n\ndef diff_round(x):\n  return tf.round(x) + (x - tf.round(x))**3\n\ndef round_only_at_0(x):\n  cond = tf.cast(tf.abs(x) < 0.5, tf.float32)\n  return cond * (x ** 3) + (1 - cond) * x\n\ndef jpeg_compress_decompress(image,\n                             downsample_c=True,\n                             rounding=diff_round,\n                             factor=1):\n  image *= 255\n  height, width = image.shape.as_list()[1:3]\n  orig_height, orig_width = height, width\n  if height % 16 != 0 or width % 16 != 0:\n    # Round up to next multiple of 16\n    height = ((height - 1) // 16 + 1) * 16\n    width = ((width - 1) // 16 + 1) * 16\n\n    vpad = height - orig_height\n    wpad = width - orig_width\n    top = vpad // 2\n    bottom = vpad - top\n    left = wpad // 2\n    right = wpad - left\n\n    #image = tf.pad(image, [[0, 0], [top, bottom], [left, right], [0, 0]], \'SYMMETRIC\')\n    image = tf.pad(image, [[0, 0], [0, vpad], [0, wpad], [0, 0]], \'SYMMETRIC\')\n\n  # ""Compression""\n  image = rgb_to_ycbcr_jpeg(image)\n  if downsample_c:\n    y, cb, cr = downsampling_420(image)\n  else:\n    y, cb, cr = tf.split(image, 3, axis=3)\n  components = {\'y\': y, \'cb\': cb, \'cr\': cr}\n  for k in components.keys():\n    comp = components[k]\n    comp = image_to_patches(comp)\n    comp = dct_8x8(comp)\n    comp = c_quantize(comp, rounding,\n                      factor) if k in (\'cb\', \'cr\') else y_quantize(\n                          comp, rounding, factor)\n    components[k] = comp\n\n  # ""Decompression""\n  for k in components.keys():\n    comp = components[k]\n    comp = c_dequantize(comp, factor) if k in (\'cb\', \'cr\') else y_dequantize(\n        comp, factor)\n    comp = idct_8x8(comp)\n    if k in (\'cb\', \'cr\'):\n      if downsample_c:\n        comp = patches_to_image(comp, int(height/2), int(width/2))\n      else:\n        comp = patches_to_image(comp, height, width)\n    else:\n      comp = patches_to_image(comp, height, width)\n    components[k] = comp\n\n  y, cb, cr = components[\'y\'], components[\'cb\'], components[\'cr\']\n  if downsample_c:\n    image = upsampling_420(y, cb, cr)\n  else:\n    image = tf.stack((y, cb, cr), axis=-1)\n  image = ycbcr_to_rgb_jpeg(image)\n\n  # Crop to original size\n  if orig_height != height or orig_width != width:\n    #image = image[:, top:-bottom, left:-right]\n    image = image[:, :-vpad, :-wpad]\n\n  # Hack: RGB -> YUV -> RGB sometimes results in incorrect values\n  #    min_value = tf.minimum(tf.reduce_min(image), 0.)\n  #    max_value = tf.maximum(tf.reduce_max(image), 255.)\n  #    value_range = max_value - min_value\n  #    image = 255 * (image - min_value) / value_range\n  image = tf.minimum(255., tf.maximum(0., image))\n  image /= 255\n\n  return image\n\ndef quality_to_factor(quality):\n    if quality < 50:\n        quality = 5000. / quality\n    else:\n        quality = 200. - quality*2\n    return quality / 100.\n'"
