file_path,api_count,code
adanet/__init__.py,0,"b'# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""AdaNet: Fast and flexible AutoML with learning guarantees.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet import distributed\nfrom adanet import ensemble\nfrom adanet import replay\nfrom adanet import subnetwork\nfrom adanet.autoensemble import AutoEnsembleEstimator\nfrom adanet.autoensemble import AutoEnsembleSubestimator\nfrom adanet.autoensemble import AutoEnsembleTPUEstimator\nfrom adanet.core import Estimator\nfrom adanet.core import Evaluator\nfrom adanet.core import ReportMaterializer\nfrom adanet.core import Summary\nfrom adanet.core import TPUEstimator\n# For backwards compatibility. Previously all Ensemblers were complexity\n# regularized using the AdaNet objective.\nfrom adanet.ensemble import ComplexityRegularized as Ensemble\nfrom adanet.ensemble import MixtureWeightType\nfrom adanet.ensemble import WeightedSubnetwork\nfrom adanet.subnetwork import Subnetwork\n\nfrom adanet.version import __version__\n\n__all__ = [\n    ""AutoEnsembleEstimator"",\n    ""AutoEnsembleSubestimator"",\n    ""AutoEnsembleTPUEstimator"",\n    ""distributed"",\n    ""ensemble"",\n    ""Ensemble"",\n    ""Estimator"",\n    ""Evaluator"",\n    ""replay"",\n    ""ReportMaterializer"",\n    ""subnetwork"",\n    ""Summary"",\n    ""TPUEstimator"",\n    ""MixtureWeightType"",\n    ""WeightedSubnetwork"",\n    ""Subnetwork"",\n]\n'"
adanet/adanet_test.py,2,"b'""""""Test AdaNet package.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport adanet\nfrom adanet.examples import simple_dnn\nimport tensorflow.compat.v1 as tf\n\n\nclass AdaNetTest(tf.test.TestCase):\n\n  def test_public(self):\n    self.assertIsNotNone(adanet.__version__)\n    self.assertIsNotNone(adanet.AutoEnsembleEstimator)\n    self.assertIsNotNone(adanet.AutoEnsembleSubestimator)\n    self.assertIsNotNone(adanet.AutoEnsembleTPUEstimator)\n    self.assertIsNotNone(adanet.distributed.PlacementStrategy)\n    self.assertIsNotNone(adanet.distributed.ReplicationStrategy)\n    self.assertIsNotNone(adanet.distributed.RoundRobinStrategy)\n    self.assertIsNotNone(adanet.ensemble.Ensemble)\n    self.assertIsNotNone(adanet.ensemble.Ensembler)\n    self.assertIsNotNone(adanet.ensemble.TrainOpSpec)\n    self.assertIsNotNone(adanet.ensemble.AllStrategy)\n    self.assertIsNotNone(adanet.ensemble.Candidate)\n    self.assertIsNotNone(adanet.ensemble.GrowStrategy)\n    self.assertIsNotNone(adanet.ensemble.Strategy)\n    self.assertIsNotNone(adanet.ensemble.ComplexityRegularized)\n    self.assertIsNotNone(adanet.ensemble.ComplexityRegularizedEnsembler)\n    self.assertIsNotNone(adanet.ensemble.MeanEnsemble)\n    self.assertIsNotNone(adanet.ensemble.MeanEnsembler)\n    self.assertIsNotNone(adanet.ensemble.MixtureWeightType)\n    self.assertIsNotNone(adanet.ensemble.WeightedSubnetwork)\n    self.assertIsNotNone(adanet.Ensemble)\n    self.assertIsNotNone(adanet.Estimator)\n    self.assertIsNotNone(adanet.Evaluator)\n    self.assertIsNotNone(adanet.MixtureWeightType)\n    self.assertIsNotNone(adanet.replay.Config)\n    self.assertIsNotNone(adanet.ReportMaterializer)\n    self.assertIsNotNone(adanet.Subnetwork)\n    self.assertIsNotNone(adanet.subnetwork.Builder)\n    self.assertIsNotNone(adanet.subnetwork.Generator)\n    self.assertIsNotNone(adanet.subnetwork.Subnetwork)\n    self.assertIsNotNone(adanet.subnetwork.TrainOpSpec)\n    self.assertIsNotNone(adanet.Summary)\n    self.assertIsNotNone(adanet.TPUEstimator)\n    self.assertIsNotNone(adanet.WeightedSubnetwork)\n    self.assertIsNotNone(simple_dnn.Generator)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/modelflow_test.py,2,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Test ModelFlow imports.""""""\n\nimport adanet.experimental as adanet\nimport tensorflow.compat.v2 as tf\n\n\nclass ModelFlowTest(tf.test.TestCase):\n\n  def test_public(self):\n    self.assertIsNotNone(adanet.controllers.SequentialController)\n    self.assertIsNotNone(adanet.keras.EnsembleModel)\n    self.assertIsNotNone(adanet.keras.MeanEnsemble)\n    self.assertIsNotNone(adanet.keras.WeightedEnsemble)\n    self.assertIsNotNone(adanet.keras.ModelSearch)\n    self.assertIsNotNone(adanet.phases.AutoEnsemblePhase)\n    self.assertIsNotNone(adanet.phases.InputPhase)\n    self.assertIsNotNone(adanet.phases.KerasTrainerPhase)\n    self.assertIsNotNone(adanet.phases.KerasTunerPhase)\n    self.assertIsNotNone(adanet.phases.RepeatPhase)\n    self.assertIsNotNone(adanet.schedulers.InProcessScheduler)\n    self.assertIsNotNone(adanet.storages.InMemoryStorage)\n    self.assertIsNotNone(adanet.work_units.KerasTrainerWorkUnit)\n    self.assertIsNotNone(adanet.work_units.KerasTunerWorkUnit)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/version.py,0,"b'# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the version string.""""""\n\n__version__ = u""0.9.0.dev""\n'"
adanet/autoensemble/__init__.py,0,"b'""""""The TensorFlow AdaNet autoensemble module.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.autoensemble.common import AutoEnsembleSubestimator\nfrom adanet.autoensemble.estimator import AutoEnsembleEstimator\nfrom adanet.autoensemble.estimator import AutoEnsembleTPUEstimator\n\n__all__ = [\n    ""AutoEnsembleEstimator"",\n    ""AutoEnsembleSubestimator"",\n    ""AutoEnsembleTPUEstimator"",\n]\n'"
adanet/autoensemble/common.py,10,"b'""""""Common utilities for AutoEnsemblers.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport inspect\n\nfrom adanet import subnetwork as subnetwork_lib\nfrom adanet import tf_compat\n\nimport tensorflow.compat.v2 as tf\n\n\ndef _default_logits(estimator_spec):\n  from tensorflow.python.estimator.canned import prediction_keys  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n  if isinstance(estimator_spec.predictions, dict):\n    pred_keys = prediction_keys.PredictionKeys\n    if pred_keys.LOGITS in estimator_spec.predictions:\n      return estimator_spec.predictions[pred_keys.LOGITS]\n    if pred_keys.PREDICTIONS in estimator_spec.predictions:\n      return estimator_spec.predictions[pred_keys.PREDICTIONS]\n  return estimator_spec.predictions\n\n\nclass _SecondaryTrainOpRunnerHook(tf_compat.SessionRunHook):\n  """"""A hook for running a train op separate from the main session run call.""""""\n\n  def __init__(self, train_op):\n    """"""Initializes a `_SecondaryTrainOpRunnerHook`.\n\n    Args:\n      train_op: The secondary train op to execute before runs.\n    """"""\n\n    self._train_op = train_op\n\n  def before_run(self, run_context):\n    run_context.session.run(self._train_op)\n\n\nclass AutoEnsembleSubestimator(  # pylint: disable=g-classes-have-attributes\n    collections.namedtuple(""AutoEnsembleSubestimator"",\n                           [""estimator"", ""train_input_fn"", ""prediction_only""])):\n  """"""A subestimator to train and consider for ensembling.\n\n  Args:\n    estimator: A `tf.estimator.Estimator` or `tf.estimator.tpu.TPUEstimator`\n      instance to consider for ensembling.\n    train_input_fn: A function that provides input data for training as\n      minibatches. It can be used to implement ensemble methods like bootstrap\n      aggregating (a.k.a bagging) where each subnetwork trains on different\n      slices of the training data. The function should construct and return one\n      of the following:\n       * A `tf.data.Dataset` object: Outputs of `Dataset` object must be a tuple\n         `(features, labels)` with same constraints as below. NOTE: A Dataset\n           must return *at least* two batches before hitting the end-of-input,\n           otherwise all of training terminates.\n         TODO: Figure out how to handle single-batch datasets.\n       * A tuple `(features, labels)`: Where `features` is a `tf.Tensor` or a\n         dictionary of string feature name to `Tensor` and `labels` is a\n         `Tensor` or a dictionary of string label name to `Tensor`. Both\n         `features` and `labels` are consumed by `estimator#model_fn`. They\n         should satisfy the expectation of `estimator#model_fn` from inputs.\n     prediction_only: If set to True, only runs the subestimator in prediction\n       mode.\n\n  Returns:\n    An `AutoEnsembleSubestimator` instance to be auto-ensembled.\n  """"""\n\n  # pylint: enable=g-classes-have-attributes\n\n  def __new__(cls, estimator, train_input_fn=None, prediction_only=False):\n    return super(AutoEnsembleSubestimator,\n                 cls).__new__(cls, estimator, train_input_fn, prediction_only)\n\n\nclass _BuilderFromSubestimator(subnetwork_lib.Builder):\n  """"""An `adanet.Builder` from a :class:`tf.estimator.Estimator`.""""""\n\n  def __init__(self, name, subestimator, logits_fn, last_layer_fn, config):\n    self._name = name\n    self._subestimator = subestimator\n    self._logits_fn = logits_fn\n    self._last_layer_fn = last_layer_fn\n    self._config = config\n\n  @property\n  def name(self):\n    return self._name\n\n  def _call_model_fn(self, subestimator, features, labels, mode, summary):\n    with summary.current_scope():\n      model_fn = subestimator.estimator.model_fn\n      estimator_spec = model_fn(\n          features=features, labels=labels, mode=mode, config=self._config)\n      logits = self._logits_fn(estimator_spec=estimator_spec)\n      last_layer = logits\n      if self._last_layer_fn:\n        last_layer = self._last_layer_fn(estimator_spec=estimator_spec)\n\n      if estimator_spec.scaffold and estimator_spec.scaffold.local_init_op:\n        local_init_op = estimator_spec.scaffold.local_init_op\n      else:\n        local_init_op = None\n\n      train_op = subnetwork_lib.TrainOpSpec(\n          estimator_spec.train_op,\n          chief_hooks=estimator_spec.training_chief_hooks,\n          hooks=estimator_spec.training_hooks)\n    return logits, last_layer, train_op, local_init_op\n\n  def build_subnetwork(self,\n                       features,\n                       labels,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble,\n                       config=None):\n    # We don\'t need an EVAL mode since AdaNet takes care of evaluation for us.\n    subestimator = self._subestimator(config)\n    mode = tf.estimator.ModeKeys.PREDICT\n    if training and not subestimator.prediction_only:\n      mode = tf.estimator.ModeKeys.TRAIN\n\n    # Call in template to ensure that variables are created once and reused.\n    call_model_fn_template = tf.compat.v1.make_template(""model_fn"",\n                                                        self._call_model_fn)\n    subestimator_features, subestimator_labels = features, labels\n    local_init_ops = []\n    if training and subestimator.train_input_fn:\n      # TODO: Consider tensorflow_estimator/python/estimator/util.py.\n      inputs = subestimator.train_input_fn()\n      if isinstance(inputs, (tf_compat.DatasetV1, tf_compat.DatasetV2)):\n        subestimator_features, subestimator_labels = (\n            tf_compat.make_one_shot_iterator(inputs).get_next())\n      else:\n        subestimator_features, subestimator_labels = inputs\n\n      # Construct subnetwork graph first because of dependencies on scope.\n      _, _, bagging_train_op_spec, sub_local_init_op = call_model_fn_template(\n          subestimator, subestimator_features, subestimator_labels, mode,\n          summary)\n      # Graph for ensemble learning gets model_fn_1 for scope.\n      logits, last_layer, _, ensemble_local_init_op = call_model_fn_template(\n          subestimator, features, labels, mode, summary)\n\n      if sub_local_init_op:\n        local_init_ops.append(sub_local_init_op)\n      if ensemble_local_init_op:\n        local_init_ops.append(ensemble_local_init_op)\n\n      # Run train op in a hook so that exceptions can be intercepted by the\n      # AdaNet framework instead of the Estimator\'s monitored training session.\n      hooks = bagging_train_op_spec.hooks + (_SecondaryTrainOpRunnerHook(\n          bagging_train_op_spec.train_op),)\n      train_op_spec = subnetwork_lib.TrainOpSpec(\n          train_op=tf.no_op(),\n          chief_hooks=bagging_train_op_spec.chief_hooks,\n          hooks=hooks)\n    else:\n      logits, last_layer, train_op_spec, local_init_op = call_model_fn_template(\n          subestimator, features, labels, mode, summary)\n      if local_init_op:\n        local_init_ops.append(local_init_op)\n\n    # TODO: Replace with variance complexity measure.\n    complexity = tf.constant(0.)\n    return subnetwork_lib.Subnetwork(\n        logits=logits,\n        last_layer=last_layer,\n        shared={""train_op"": train_op_spec},\n        complexity=complexity,\n        local_init_ops=local_init_ops)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    return subnetwork.shared[""train_op""]\n\n\ndef _convert_to_subestimator(candidate):\n  """"""Converts a candidate to an AutoEnsembleSubestimator.""""""\n\n  if callable(candidate):\n    return candidate\n  if isinstance(candidate, AutoEnsembleSubestimator):\n    return lambda config: candidate\n\n  from tensorflow_estimator.python.estimator import estimator as estimator_lib  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n  if isinstance(candidate,\n                (estimator_lib.Estimator, estimator_lib.EstimatorV2)):\n    return lambda config: AutoEnsembleSubestimator(candidate)\n  raise ValueError(\n      ""subestimator in candidate_pool must have type tf.estimator.Estimator or ""\n      ""adanet.AutoEnsembleSubestimator but got {}"".format(candidate.__class__))\n\n\nclass _GeneratorFromCandidatePool(subnetwork_lib.Generator):\n  """"""An `adanet.Generator` from a pool of `Estimator` and `Model` instances.""""""\n\n  def __init__(self, candidate_pool, logits_fn, last_layer_fn):\n    self._candidate_pool = candidate_pool\n    if logits_fn is None:\n      logits_fn = _default_logits\n    self._logits_fn = logits_fn\n    self._last_layer_fn = last_layer_fn\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports, config):\n    assert config\n    builders = []\n    candidate_pool = self._maybe_call_candidate_pool(config, iteration_number)\n\n    if isinstance(candidate_pool, dict):\n      for name in sorted(candidate_pool):\n        builders.append(\n            _BuilderFromSubestimator(\n                name,\n                _convert_to_subestimator(candidate_pool[name]),\n                logits_fn=self._logits_fn,\n                last_layer_fn=self._last_layer_fn,\n                config=config))\n      return builders\n\n    for i, estimator in enumerate(candidate_pool):\n      name = ""{class_name}{index}"".format(\n          class_name=estimator.__class__.__name__, index=i)\n      builders.append(\n          _BuilderFromSubestimator(\n              name,\n              _convert_to_subestimator(estimator),\n              logits_fn=self._logits_fn,\n              last_layer_fn=self._last_layer_fn,\n              config=config))\n    return builders\n\n  def _maybe_call_candidate_pool(self, config, iteration_number):\n    if callable(self._candidate_pool):\n      # candidate_pool can be a function.\n      candidate_pool_args = inspect.getargs(self._candidate_pool.__code__).args\n      if ""iteration_number"" in candidate_pool_args:\n        # TODO: Make the ""config"" argument optional using introspection.\n        return self._candidate_pool(\n            config=config, iteration_number=iteration_number)\n      else:\n        return self._candidate_pool(config=config)\n\n    return self._candidate_pool\n'"
adanet/autoensemble/estimator.py,50,"b'""""""An estimator that learns to ensemble.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet import core\nfrom adanet.autoensemble.common import _GeneratorFromCandidatePool\n\nimport tensorflow.compat.v2 as tf\n\n\nclass AutoEnsembleEstimator(core.Estimator):  # pylint: disable=g-classes-have-attributes\n  # pyformat: disable\n  """"""A :class:`tf.estimator.Estimator` that learns to ensemble models.\n\n  Specifically, it learns to ensemble models from a candidate pool using the\n  Adanet algorithm.\n\n  .. code-block:: python\n\n      # A simple example of learning to ensemble linear and neural network\n      # models.\n\n      import adanet\n      import tensorflow as tf\n\n      feature_columns = ...\n\n      head = MultiClassHead(n_classes=10)\n\n      # Learn to ensemble linear and DNN models.\n      estimator = adanet.AutoEnsembleEstimator(\n          head=head,\n          candidate_pool=lambda config: {\n              ""linear"":\n                  tf.estimator.LinearEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      config=config,\n                      optimizer=...),\n              ""dnn"":\n                  tf.estimator.DNNEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      config=config,\n                      optimizer=...,\n                      hidden_units=[1000, 500, 100])},\n          max_iteration_steps=50)\n\n      # Input builders\n      def input_fn_train:\n        # Returns tf.data.Dataset of (x, y) tuple where y represents label\'s\n        # class index.\n        pass\n      def input_fn_eval:\n        # Returns tf.data.Dataset of (x, y) tuple where y represents label\'s\n        # class index.\n        pass\n      def input_fn_predict:\n        # Returns tf.data.Dataset of (x, None) tuple.\n        pass\n      estimator.train(input_fn=input_fn_train, steps=100)\n      metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n      predictions = estimator.predict(input_fn=input_fn_predict)\n\n  Or to train candidate subestimators on different training data subsets:\n\n  .. code-block:: python\n\n      train_data_files = [...]\n\n      # Learn to ensemble linear and DNN models.\n      estimator = adanet.AutoEnsembleEstimator(\n          head=head,\n          candidate_pool=lambda config: {\n              ""linear"":\n                  adanet.AutoEnsembleSubestimator(\n                      tf.estimator.LinearEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          config=config,\n                          optimizer=...),\n                      make_train_input_fn(train_data_files[:-1])),\n              ""dnn"":\n                  adanet.AutoEnsembleSubestimator(\n                      tf.estimator.DNNEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          config=config,\n                          optimizer=...,\n                          hidden_units=[1000, 500, 100]),\n                      make_train_input_fn(train_data_files[0:]))},\n          max_iteration_steps=50)\n\n      estimator.train(input_fn=make_train_input_fn(train_data_files), steps=100)\n\n\n  Args:\n    head: A :class:`tf.contrib.estimator.Head` instance for computing loss and\n      evaluation metrics for every candidate.\n    candidate_pool: List of :class:`tf.estimator.Estimator` and\n      :class:`AutoEnsembleSubestimator` objects, or dict of string name to\n      :class:`tf.estimator.Estimator` and :class:`AutoEnsembleSubestimator`\n      objects that are candidate subestimators to ensemble at each iteration.\n      The order does not directly affect which candidates will be included in\n      the final ensemble, but will affect the name of the candidate. When using\n      a dict, the string key becomes the candidate subestimator\'s name.\n      Alternatively, this argument can be a function that takes a `config`\n      argument and returns the aforementioned values in case the\n      objects need to be re-instantiated at each adanet iteration.\n    max_iteration_steps: Total number of steps for which to train candidates per\n      iteration. If `OutOfRange` or `StopIteration` occurs in the middle,\n      training stops before `max_iteration_steps` steps.\n    logits_fn: A function for fetching the subnetwork logits from a\n      :class:`tf.estimator.EstimatorSpec`, which should obey the following\n      signature:\n        - `Args`: Can only have following argument:\n          - estimator_spec: The candidate\'s :class:`tf.estimator.EstimatorSpec`.\n        - `Returns`: Logits :class:`tf.Tensor` or dict of string to logits\n          :class:`tf.Tensor` (for multi-head) for the candidate subnetwork\n          extracted from the given `estimator_spec`. When `None`, it will\n          default to returning `estimator_spec.predictions` when they are a\n          :class:`tf.Tensor` or the :class:`tf.Tensor` for the key \'logits\' when\n          they are a dict of string to :class:`tf.Tensor`.\n    last_layer_fn: An optional function for fetching the subnetwork last_layer\n      from a :class:`tf.estimator.EstimatorSpec`, which should obey the\n      following signature:\n        - `Args`: Can only have following argument:\n          - estimator_spec: The candidate\'s :class:`tf.estimator.EstimatorSpec`.\n        - `Returns`: Last layer :class:`tf.Tensor` or dict of string to last\n          layer :class:`tf.Tensor` (for multi-head) for the candidate subnetwork\n          extracted from the given `estimator_spec`. The last_layer can be used\n          for learning ensembles or exporting them as embeddings.\n      When `None`, it will default to using the logits as the last_layer.\n    ensemblers: See :class:`adanet.Estimator`.\n    ensemble_strategies: See :class:`adanet.Estimator`.\n    evaluator:  See :class:`adanet.Estimator`.\n    metric_fn:  See :class:`adanet.Estimator`.\n    force_grow:  See :class:`adanet.Estimator`.\n    adanet_loss_decay: See :class:`adanet.Estimator`.\n    worker_wait_timeout_secs: See :class:`adanet.Estimator`.\n    model_dir: See :class:`adanet.Estimator`.\n    config: See :class:`adanet.Estimator`.\n    debug: See :class:`adanet.Estimator`.\n    enable_ensemble_summaries: See :class:`adanet.Estimator`.\n    enable_subnetwork_summaries: See :class:`adanet.Estimator`.\n    global_step_combiner_fn: See :class:`adanet.Estimator`.\n    max_iterations: See :class:`adanet.Estimator`.\n    replay_config: See :class:`adanet.Estimator`.\n    **kwargs: Extra keyword args passed to the parent.\n\n  Returns:\n    An :class:`adanet.AutoEnsembleEstimator` instance.\n\n  Raises:\n    ValueError: If any of the candidates in `candidate_pool` are not\n      :class:`tf.estimator.Estimator` instances.\n  """"""\n  # pyformat: enable\n\n  def __init__(self,\n               head,\n               candidate_pool,\n               max_iteration_steps,\n               ensemblers=None,\n               ensemble_strategies=None,\n               logits_fn=None,\n               last_layer_fn=None,\n               evaluator=None,\n               metric_fn=None,\n               force_grow=False,\n               adanet_loss_decay=.9,\n               worker_wait_timeout_secs=7200,\n               model_dir=None,\n               config=None,\n               debug=False,\n               enable_ensemble_summaries=True,\n               enable_subnetwork_summaries=True,\n               global_step_combiner_fn=tf.math.reduce_mean,\n               max_iterations=None,\n               replay_config=None,\n               **kwargs):\n    subnetwork_generator = _GeneratorFromCandidatePool(candidate_pool,\n                                                       logits_fn, last_layer_fn)\n    super(AutoEnsembleEstimator, self).__init__(\n        head=head,\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=max_iteration_steps,\n        ensemblers=ensemblers,\n        ensemble_strategies=ensemble_strategies,\n        evaluator=evaluator,\n        metric_fn=metric_fn,\n        force_grow=force_grow,\n        adanet_loss_decay=adanet_loss_decay,\n        worker_wait_timeout_secs=worker_wait_timeout_secs,\n        model_dir=model_dir,\n        config=config,\n        debug=debug,\n        enable_ensemble_summaries=enable_ensemble_summaries,\n        enable_subnetwork_summaries=enable_subnetwork_summaries,\n        global_step_combiner_fn=global_step_combiner_fn,\n        max_iterations=max_iterations,\n        replay_config=replay_config,\n        **kwargs)\n\n\nclass AutoEnsembleTPUEstimator(core.TPUEstimator):  # pylint: disable=g-classes-have-attributes\n  # pyformat: disable\n  """"""A :class:`tf.estimator.tpu.TPUEstimator` that learns to ensemble models.\n\n  Specifically, it learns to ensemble models from a candidate pool using the\n  Adanet algorithm.\n\n  This estimator is capable of training and evaluating on TPU. It can ensemble\n  both :class:`tf.estimator.tpu.TPUEstimator` candidates as well as regular\n  :class:`tf.estimator.Estimator` candidates, as long as these candidates are\n  TPU compatible.\n\n  Note the following restrictions compared to AutoEnsembleEstimator:\n    * All candidates must wrap their optimizers with a\n      :class:`tf.tpu.CrossShardOptimizer`.\n    * The `input_fn` must expose a `params` argument.\n    * The `model_fn` of :class:`tf.estimator.tpu.TPUEstimator` candidates must\n      also expose a `params` argument.\n\n  WARNING: This Estimator is a work in progress and the API could change at any\n  moment. May not support all AutoEnsembleEstimator features.\n\n    .. code-block:: python\n\n      # A simple example of learning to ensemble linear and neural network\n      # models on TPU.\n\n      import adanet\n      import tensorflow as tf\n\n      feature_columns = ...\n\n      head = MultiClassHead(n_classes=10)\n\n      # Learn to ensemble linear and DNN models.\n      estimator = adanet.AutoEnsembleTPUEstimator(\n          head=head,\n          candidate_pool=lambda config: {\n              ""linear"":\n                  tf.estimator.LinearEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      config=config,\n                      optimizer=tf.tpu.CrossShardOptimizer(...)),\n              ""dnn"":\n                  tf.estimator.DNNEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      config=config,\n                      optimizer=tf.tpu.CrossShardOptimzier(...),\n                      hidden_units=[1000, 500, 100])},\n          max_iteration_steps=50)\n\n      # Input builders\n      def input_fn_train(params):\n        # Returns tf.data.Dataset of (x, y) tuple where y represents label\'s\n        # class index.\n        pass\n      def input_fn_eval(params):\n        # Returns tf.data.Dataset of (x, y) tuple where y represents label\'s\n        # class index.\n        pass\n      def input_fn_predict():\n        # Returns tf.data.Dataset of (x, None) tuple.\n        pass\n      estimator.train(input_fn=input_fn_train, steps=100)\n      metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n      predictions = estimator.predict(input_fn=input_fn_predict)\n\n  Args:\n    head: A :class:`tf.contrib.estimator.Head` instance for computing loss and\n      evaluation metrics for every candidate.\n    candidate_pool: List of :class:`tf.estimator.tpu.TPUEstimator` and\n      :class:`AutoEnsembleSubestimator` objects, or dict of string name to\n      :class:`tf.estimator.tpu.TPUEstimator` and\n      :class:`AutoEnsembleSubestimator` objects that are candidate subestimators\n      to ensemble at each iteration. The order does not directly affect which\n      candidates will be included in the final ensemble, but will affect the\n      name of the candidate. When using a dict, the string key becomes the\n      candidate subestimator\'s name. Alternatively, this argument can be a\n      function that takes a `config` argument and returns the aforementioned\n      values in case the objects need to be re-instantiated at each adanet\n      iteration.\n    max_iteration_steps: See :class:`adanet.Estimator`.\n    logits_fn: A function for fetching the subnetwork logits from a\n      :class:`tf.estimator.EstimatorSpec`, which should obey the following\n      signature:\n        - `Args`: Can only have following argument:\n          - estimator_spec: The candidate\'s :class:`tf.estimator.EstimatorSpec`.\n        - `Returns`: Logits :class:`tf.Tensor` or dict of string to logits\n          :class:`tf.Tensor` (for multi-head) for the candidate subnetwork\n          extracted from the given `estimator_spec`. When `None`, it will\n          default to returning `estimator_spec.predictions` when they are a\n          :class:`tf.Tensor` or the :class:`tf.Tensor` for the key \'logits\' when\n          they are a dict of string to :class:`tf.Tensor`.\n    last_layer_fn: An optional function for fetching the subnetwork last_layer\n      from a :class:`tf.estimator.EstimatorSpec`, which should obey the\n      following signature:\n        - `Args`: Can only have following argument:\n          - estimator_spec: The candidate\'s :class:`tf.estimator.EstimatorSpec`.\n        - `Returns`: Last layer :class:`tf.Tensor` or dict of string to last\n          layer :class:`tf.Tensor` (for multi-head) for the candidate subnetwork\n          extracted from the given `estimator_spec`. The last_layer can be used\n          for learning ensembles or exporting them as embeddings.\n      When `None`, it will default to using the logits as the last_layer.\n    ensemblers: See :class:`adanet.Estimator`.\n    ensemble_strategies: See :class:`adanet.Estimator`.\n    evaluator:  See :class:`adanet.Estimator`.\n    metric_fn:  See :class:`adanet.Estimator`.\n    force_grow:  See :class:`adanet.Estimator`.\n    adanet_loss_decay: See :class:`adanet.Estimator`.\n    model_dir: See :class:`adanet.Estimator`.\n    config: See :class:`adanet.Estimator`.\n    use_tpu: See :class:`adanet.Estimator`.\n    eval_on_tpu: See :class:`adanet.Estimator`.\n    export_to_tpu: See :class:`adanet.Estimator`.\n    train_batch_size: See :class:`adanet.Estimator`.\n    eval_batch_size: See :class:`adanet.Estimator`.\n    embedding_config_spec: See :class:`adanet.Estimator`.\n    debug: See :class:`adanet.Estimator`.\n    enable_ensemble_summaries: See :class:`adanet.Estimator`.\n    enable_subnetwork_summaries: See :class:`adanet.Estimator`.\n    global_step_combiner_fn: See :class:`adanet.Estimator`.\n    max_iterations: See :class:`adanet.Estimator`.\n    replay_config: See :class:`adanet.Estimator`.\n    **kwargs: Extra keyword args passed to the parent.\n\n  Returns:\n    An :class:`adanet.AutoEnsembleTPUEstimator` instance.\n\n  Raises:\n    ValueError: If any of the candidates in `candidate_pool` are not\n      :class:`tf.estimator.Estimator` instances.\n  """"""\n  # pyformat: enable\n\n  def __init__(self,\n               head,\n               candidate_pool,\n               max_iteration_steps,\n               ensemblers=None,\n               ensemble_strategies=None,\n               logits_fn=None,\n               last_layer_fn=None,\n               evaluator=None,\n               metric_fn=None,\n               force_grow=False,\n               adanet_loss_decay=.9,\n               model_dir=None,\n               config=None,\n               use_tpu=True,\n               eval_on_tpu=True,\n               export_to_tpu=True,\n               train_batch_size=None,\n               eval_batch_size=None,\n               predict_batch_size=None,\n               embedding_config_spec=None,\n               debug=False,\n               enable_ensemble_summaries=True,\n               enable_subnetwork_summaries=True,\n               global_step_combiner_fn=tf.math.reduce_mean,\n               max_iterations=None,\n               replay_config=None,\n               **kwargs):\n    subnetwork_generator = _GeneratorFromCandidatePool(candidate_pool,\n                                                       logits_fn, last_layer_fn)\n    super(AutoEnsembleTPUEstimator, self).__init__(\n        head=head,\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=max_iteration_steps,\n        ensemblers=ensemblers,\n        ensemble_strategies=ensemble_strategies,\n        evaluator=evaluator,\n        metric_fn=metric_fn,\n        force_grow=force_grow,\n        adanet_loss_decay=adanet_loss_decay,\n        model_dir=model_dir,\n        config=config,\n        use_tpu=use_tpu,\n        eval_on_tpu=eval_on_tpu,\n        export_to_tpu=export_to_tpu,\n        train_batch_size=train_batch_size,\n        eval_batch_size=eval_batch_size,\n        predict_batch_size=predict_batch_size,\n        embedding_config_spec=embedding_config_spec,\n        debug=debug,\n        enable_ensemble_summaries=enable_ensemble_summaries,\n        enable_subnetwork_summaries=enable_subnetwork_summaries,\n        global_step_combiner_fn=global_step_combiner_fn,\n        max_iterations=max_iterations,\n        replay_config=replay_config,\n        **kwargs)\n'"
adanet/autoensemble/estimator_test.py,78,"b'""""""Tests for AdaNet AutoEnsembleEstimator in TF 1.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport os\nimport shutil\nimport sys\n\nfrom absl import flags\nfrom absl import logging\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.autoensemble.common import AutoEnsembleSubestimator\nfrom adanet.autoensemble.estimator import _GeneratorFromCandidatePool\nfrom adanet.autoensemble.estimator import AutoEnsembleEstimator\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.estimator.export import export\n# pylint: enable=g-direct-tensorflow-import\n\nlogging.set_verbosity(logging.INFO)\n\n\n# Ensures ""local_init_op"" is called.\nclass CheckLocalInitOpEstimator(tf.estimator.Estimator):\n\n  def __init__(self):\n    super(CheckLocalInitOpEstimator,\n          self).__init__(model_fn=self._get_model_fn())\n\n  def _get_model_fn(self):\n\n    def _model_fn(features, labels, mode, params):\n\n      del labels\n      del params\n\n      flag = tf.Variable(initial_value=False, collections=[])\n      set_flag = tf.assign(flag, True)\n      test_flag = tf.debugging.Assert(tf.equal(flag, True), [flag])\n\n      scaffold = tf.train.Scaffold(\n          local_init_op=tf.group(tf.train.Scaffold.default_local_init_op(),\n                                 set_flag))\n\n      # Note: Not consuming the feature stales the test input_fn.\n      feature = next(iter(features.values()))\n      with tf.control_dependencies([feature, test_flag]):\n        batch_size = tf.shape(feature)[0]\n        predictions = tf.zeros([batch_size, 1])\n\n      return tf.estimator.EstimatorSpec(\n          mode,\n          loss=tf.constant(-1.0),\n          train_op=test_flag,\n          scaffold=scaffold,\n          predictions=predictions)\n\n    return _model_fn\n\n\n# Creates an a file in the model directory during training, and check the\n# presence of this file during eval and inference.\nclass CheckAssetEstimator(tf.estimator.Estimator):\n\n  def __init__(self, config):\n    super(CheckAssetEstimator, self).__init__(\n        config=config, model_fn=self._get_model_fn())\n\n  def _get_model_fn(self):\n\n    def _model_fn(features, labels, mode, params):\n\n      del labels\n      del params\n\n      scaffold = None\n      training_hooks = None\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n\n        class CheckpointSaverListener(tf.train.CheckpointSaverListener):\n\n          def after_save(subself, session, global_step_value):  # pylint: disable=no-self-argument\n            tf.logging.info(""Creating data in %s"", self._model_dir)\n            with tf.gfile.Open(os.path.join(self._model_dir, ""data""), ""w"") as f:\n              f.write(""Hello"")\n            assert tf.io.gfile.exists(os.path.join(self._model_dir, ""data""))\n\n        listener = CheckpointSaverListener()\n        saver_hook = tf.estimator.CheckpointSaverHook(\n            self._model_dir, listeners=[listener], save_steps=1000000)\n        training_hooks = [saver_hook]\n\n      else:\n\n        def check_asset():\n          tf.logging.info(""Checking data %s"", self._model_dir)\n          assert tf.io.gfile.exists(os.path.join(self._model_dir, ""data""))\n          return 1\n\n        check_asset_op = tf.compat.v1.py_func(check_asset, [], tf.int64)\n        scaffold = tf.train.Scaffold(\n            local_init_op=tf.group(tf.train.Scaffold.default_local_init_op(),\n                                   check_asset_op))\n\n      feature = next(iter(features.values()))\n      with tf.control_dependencies([feature]):\n        batch_size = tf.shape(feature)[0]\n        predictions = tf.zeros([batch_size, 1])\n        train_op = tf.no_op()\n        loss = tf.constant(-1.0)\n\n      return tf.estimator.EstimatorSpec(\n          mode,\n          loss=loss,\n          train_op=train_op,\n          scaffold=scaffold,\n          predictions=predictions,\n          training_hooks=training_hooks)\n\n    return _model_fn\n\n\nclass AutoEnsembleEstimatorTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(AutoEnsembleEstimatorTest, self).setUp()\n    # Setup and cleanup test directory.\n    # Flags are not automatically parsed at this point.\n    flags.FLAGS(sys.argv)\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(AutoEnsembleEstimatorTest, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""dict_candidate_pool"",\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: {\n                  ""dnn"":\n                      tf.estimator.DNNEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer,\n                          hidden_units=[3]),\n                  ""linear"":\n                      tf.estimator.LinearEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer),\n              },\n          ""want_loss"":\n              .209,\n      },\n      {\n          ""testcase_name"":\n              ""list_candidate_pool"",\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: [\n                  tf.estimator.DNNEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      optimizer=optimizer,\n                      hidden_units=[3]),\n                  tf.estimator.LinearEstimator(\n                      head=head,\n                      feature_columns=feature_columns,\n                      optimizer=optimizer),\n              ],\n          ""want_loss"":\n              .209,\n      },\n      {\n          ""testcase_name"":\n              ""candidate_pool_lambda"",\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: lambda config: {\n                  ""dnn"":\n                      tf.estimator.DNNEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer,\n                          hidden_units=[3],\n                          config=config),\n                  ""linear"":\n                      tf.estimator.LinearEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer,\n                          config=config),\n              },\n          ""want_loss"":\n              .209,\n      },\n      {\n          ""testcase_name"":\n              ""bagging"",\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: {\n                  ""same_train_data"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.LinearEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer)),\n                  ""different_train_data"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.DNNEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer,\n                              hidden_units=[3]),\n                          train_input_fn=lambda: ({\n                              ""input_1"": tf.constant([[0., 1.]])\n                          }, tf.constant([[1.]]))),\n              },\n          # TODO: Figure out why this test\'s loss changes with every\n          # change to the TensorFlow graph.\n          ""want_loss"":\n              0.2,\n      },\n      {\n          ""testcase_name"":\n              ""bagging_out_of_range_error"",\n          ""max_train_steps"":\n              15,\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: {\n                  ""same_train_data"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.LinearEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer)),\n                  ""different_train_data"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.DNNEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer,\n                              hidden_units=[3]),\n                          # TODO: Dataset must have at least 2 batches,\n                          # otherwise all of training terminates.\n                          train_input_fn=lambda: tf.data.Dataset.\n                          from_tensor_slices(({\n                              ""input_1"": [[0., 1.], [0., 1.]]\n                          }, [[1.], [1.]])).batch(1),\n                      ),\n              },\n          # TODO: Figure out why this test\'s loss changes with every\n          # change to the TensorFlow graph.\n          ""want_loss"":\n              0.3,\n      },\n      {\n          ""testcase_name"":\n              ""check_local_init_op"",\n          ""max_train_steps"":\n              10,\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: {\n                  ""expect_local_init_op"":\n                      AutoEnsembleSubestimator(CheckLocalInitOpEstimator()),\n              },\n          ""want_loss"":\n              1.0,\n      },\n      {\n          ""testcase_name"":\n              ""check_iteration_count"",\n          ""max_train_steps"":\n              10,\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer:\n              (lambda config, iteration_number: {\n                  ""dnn"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.DNNEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer,\n                              hidden_units=[3])),\n                  ""linear"":\n                      AutoEnsembleSubestimator(\n                          tf.estimator.LinearEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer)),\n              }),\n          ""want_loss"":\n              .209,\n      },\n      {\n          ""testcase_name"":\n              ""check_has_asset"",\n          ""max_train_steps"":\n              10,\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: {\n                  ""linear"":\n                      lambda subconfig: AutoEnsembleSubestimator(\n                          tf.estimator.LinearEstimator(\n                              head=head,\n                              feature_columns=feature_columns,\n                              optimizer=optimizer,\n                              config=subconfig)),\n                  ""check_asset_1"":\n                      lambda subconfig: AutoEnsembleSubestimator(\n                          CheckAssetEstimator(config=subconfig)),\n                  ""check_asset_2"":\n                      lambda subconfig: AutoEnsembleSubestimator(\n                          CheckAssetEstimator(config=subconfig)),\n              },\n          ""want_loss"":\n              .209,\n      })\n  # pylint: enable=g-long-lambda\n  @tf_compat.skip_for_tf2\n  def test_auto_ensemble_estimator_lifecycle(self,\n                                             candidate_pool,\n                                             want_loss,\n                                             max_train_steps=30):\n    features = {""input_1"": [[1., 0.]]}\n    labels = [[1.]]\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    head = tf.contrib.estimator.regression_head(\n        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.01)\n    feature_columns = [tf.feature_column.numeric_column(""input_1"", shape=[2])]\n\n    def train_input_fn():\n      input_features = {}\n      for key, feature in features.items():\n        input_features[key] = tf.constant(feature, name=key)\n      input_labels = tf.constant(labels, name=""labels"")\n      return input_features, input_labels\n\n    def test_input_fn():\n      input_features = tf.data.Dataset.from_tensors([\n          tf.constant(features[""input_1""])\n      ]).make_one_shot_iterator().get_next()\n      return {""input_1"": input_features}, None\n\n    estimator = AutoEnsembleEstimator(\n        head=head,\n        candidate_pool=candidate_pool(head, feature_columns, optimizer),\n        max_iteration_steps=10,\n        force_grow=True,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    # Train for three iterations.\n    estimator.train(input_fn=train_input_fn, max_steps=max_train_steps)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n\n    self.assertAllClose(max_train_steps, eval_results[""global_step""])\n    self.assertAllClose(want_loss, eval_results[""loss""], atol=.3)\n\n    # Predict.\n    predictions = estimator.predict(input_fn=test_input_fn)\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[""predictions""])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      for key, value in features.items():\n        features[key] = tf.constant(value)\n      return export.SupervisedInputReceiver(\n          features=features,\n          labels=tf.constant(labels),\n          receiver_tensors=serialized_example)\n\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    export_saved_model_fn = getattr(estimator, ""export_saved_model"", None)\n    if not callable(export_saved_model_fn):\n      export_saved_model_fn = estimator.export_savedmodel\n    export_saved_model_fn(\n        export_dir_base=export_dir_base,\n        serving_input_receiver_fn=serving_input_fn)\n\n  @tf_compat.skip_for_tf2\n  def test_last_layer_fn(self):\n    head = tf.contrib.estimator.regression_head(\n        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.01)\n    feature_columns = [tf.feature_column.numeric_column(""input_1"", shape=[2])]\n    cand_pool = [\n        tf.estimator.DNNEstimator(\n            head=head,\n            feature_columns=feature_columns,\n            optimizer=optimizer,\n            hidden_units=[3])\n    ]\n    input_features = {}\n    features = {""input_1"": [[1., 0.]]}\n    labels = [[1.]]\n    for key, feature in features.items():\n      input_features[key] = tf.constant(feature, name=key)\n    input_labels = tf.constant(labels, name=""labels"")\n\n    class _FakeSummary(object):\n      """"""A fake adanet.Summary.""""""\n\n      def scalar(self, name, tensor, family=None):\n        del name, tensor, family\n        return ""fake_scalar""\n\n      @contextlib.contextmanager\n      def current_scope(self):\n        yield\n\n    def _adanet_last_layer_fn(estimator_spec):\n      del estimator_spec\n      return input_labels\n\n    # Call with custom last_layer_fn which simply returns the labels tensor.\n    generator = _GeneratorFromCandidatePool(\n        cand_pool, logits_fn=None, last_layer_fn=_adanet_last_layer_fn)\n    candidates = generator.generate_candidates(\n        previous_ensemble=None,\n        iteration_number=None,\n        previous_ensemble_reports=None,\n        all_reports=None,\n        config=tf.estimator.RunConfig())\n    subnetwork = candidates[0].build_subnetwork(input_features,\n                                                input_labels, None, False, 1,\n                                                _FakeSummary(), None)\n\n    self.assertEqual(input_labels, subnetwork.last_layer)\n\n  @tf_compat.skip_for_tf2\n  def test_extra_checkpoint_saver_hook(self):\n    """"""Tests b/122795064.""""""\n\n    features = {""input_1"": [[1., 0.]]}\n    labels = [[1.]]\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    head = tf.contrib.estimator.binary_classification_head(\n        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=.01)\n    feature_columns = [tf.feature_column.numeric_column(""input_1"", shape=[2])]\n\n    estimator = AutoEnsembleEstimator(\n        head=head,\n        candidate_pool=[\n            tf.estimator.LinearClassifier(\n                n_classes=2,\n                feature_columns=feature_columns,\n                optimizer=optimizer),\n            tf.estimator.DNNClassifier(\n                n_classes=2,\n                feature_columns=feature_columns,\n                optimizer=optimizer,\n                hidden_units=[3]),\n        ],\n        max_iteration_steps=3,\n        force_grow=True,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    ckpt_dir = os.path.join(self.test_subdirectory)\n    hooks = [tf.train.CheckpointSaverHook(ckpt_dir, save_steps=1)]\n\n    def train_input_fn():\n      input_features = {}\n      for key, feature in features.items():\n        input_features[key] = tf.constant(feature, name=key)\n      input_labels = tf.constant(labels, name=""labels"")\n      return input_features, input_labels\n\n    estimator.train(input_fn=train_input_fn, max_steps=6, hooks=hooks)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/autoensemble/estimator_v2_test.py,16,"b'""""""Tests for AdaNet AutoEnsembleEstimator in TF 2.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport sys\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.autoensemble.estimator import AutoEnsembleEstimator\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.estimator.export import export\nfrom tensorflow_estimator.python.estimator.head import regression_head\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass AutoEnsembleEstimatorV2Test(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(AutoEnsembleEstimatorV2Test, self).setUp()\n    # Setup and cleanup test directory.\n    # Flags are not automatically parsed at this point.\n    flags.FLAGS(sys.argv)\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(AutoEnsembleEstimatorV2Test, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""candidate_pool_lambda"",\n          ""candidate_pool"":\n              lambda head, feature_columns, optimizer: lambda config: {\n                  ""dnn"":\n                      tf.compat.v2.estimator.DNNEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer,\n                          hidden_units=[3],\n                          config=config),\n                  ""linear"":\n                      tf.compat.v2.estimator.LinearEstimator(\n                          head=head,\n                          feature_columns=feature_columns,\n                          optimizer=optimizer,\n                          config=config),\n              },\n          ""want_loss"":\n              .209,\n      },)\n  # pylint: enable=g-long-lambda\n  @tf_compat.skip_for_tf1\n  def test_auto_ensemble_estimator_lifecycle(self,\n                                             candidate_pool,\n                                             want_loss,\n                                             max_train_steps=30):\n    features = {""input_1"": [[1., 0.]]}\n    labels = [[1.]]\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    head = regression_head.RegressionHead()\n\n    # Always create optimizers in a lambda to prevent error like:\n    # `RuntimeError: Cannot set `iterations` to a new Variable after the\n    # Optimizer weights have been created`\n    optimizer = lambda: tf.keras.optimizers.SGD(lr=.01)\n    feature_columns = [tf.feature_column.numeric_column(""input_1"", shape=[2])]\n\n    def train_input_fn():\n      input_features = {}\n      for key, feature in features.items():\n        input_features[key] = tf.constant(feature, name=key)\n      input_labels = tf.constant(labels, name=""labels"")\n      return input_features, input_labels\n\n    def test_input_fn():\n      dataset = tf.data.Dataset.from_tensors([tf.constant(features[""input_1""])])\n      input_features = tf.compat.v1.data.make_one_shot_iterator(\n          dataset).get_next()\n      return {""input_1"": input_features}, None\n\n    estimator = AutoEnsembleEstimator(\n        head=head,\n        candidate_pool=candidate_pool(head, feature_columns, optimizer),\n        max_iteration_steps=10,\n        force_grow=True,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    # Train for three iterations.\n    estimator.train(input_fn=train_input_fn, max_steps=max_train_steps)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n\n    self.assertAllClose(max_train_steps, eval_results[""global_step""])\n    self.assertAllClose(want_loss, eval_results[""loss""], atol=.3)\n\n    # Predict.\n    predictions = estimator.predict(input_fn=test_input_fn)\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[""predictions""])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf.compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      for key, value in features.items():\n        features[key] = tf.constant(value)\n      return export.SupervisedInputReceiver(\n          features=features,\n          labels=tf.constant(labels),\n          receiver_tensors=serialized_example)\n\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    estimator.export_saved_model(\n        export_dir_base=export_dir_base,\n        serving_input_receiver_fn=serving_input_fn)\n\n\nif __name__ == ""__main__"":\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/autoensemble/tpu_estimator_test.py,19,"b'# Lint as: python3\n""""""Tests for AutoEnsembleTPUEstimator.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport os\nimport shutil\nimport sys\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.autoensemble.estimator import AutoEnsembleTPUEstimator\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-import-not-at-top\n# pylint: disable=g-direct-tensorflow-import\ntry:\n  from tensorflow_estimator.contrib.estimator.python.estimator import head as head_lib\nexcept (AttributeError, ImportError):\n  head_lib = None\nfrom tensorflow_estimator.python.estimator.canned import dnn\n# pylint: enable=g-direct-tensorflow-import\n# pylint: enable=g-import-not-at-top\n\n\nclass _DNNTPUEstimator(tf.compat.v1.estimator.tpu.TPUEstimator):\n\n  def __init__(self,\n               head,\n               hidden_units,\n               feature_columns,\n               optimizer,\n               use_tpu,\n               embedding_config_spec=None):\n    config = tf.compat.v1.estimator.tpu.RunConfig(\n        tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n            per_host_input_for_training=tf.compat.v1.estimator.tpu\n            .InputPipelineConfig.PER_HOST_V2))\n\n    def model_fn(features, labels, mode=None, params=None, config=None):\n      del params  # Unused.\n\n      return dnn._dnn_model_fn(\n          features=features,\n          labels=labels,\n          mode=mode,\n          head=head,\n          hidden_units=hidden_units,\n          feature_columns=tuple(feature_columns or []),\n          optimizer=optimizer,\n          config=config,\n          use_tpu=use_tpu)\n\n    super(_DNNTPUEstimator, self).__init__(\n        model_fn=model_fn,\n        config=config,\n        train_batch_size=64,\n        use_tpu=use_tpu,\n        embedding_config_spec=embedding_config_spec)\n\n\nclass AutoEnsembleTPUEstimatorTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(AutoEnsembleTPUEstimatorTest, self).setUp()\n    # Setup and cleanup test directory.\n    # Flags are not automatically parsed at this point.\n    flags.FLAGS(sys.argv)\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(AutoEnsembleTPUEstimatorTest, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""not_use_tpu"",\n          ""use_tpu"": False,\n      },\n  )\n  # TODO: Support V2 head and optimizer in AdaNet TPU.\n  @tf_compat.skip_for_tf2\n  def test_auto_ensemble_estimator_lifecycle(self, use_tpu):\n    head = head_lib.regression_head()\n    feature_columns = [tf.feature_column.numeric_column(""xor"", shape=2)]\n\n    def optimizer_fn():\n      optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=.01)\n      if use_tpu:\n        optimizer = tf.compat.v1.tpu.CrossShardOptimizer(optimizer)\n      return optimizer\n\n    candidate_pool = {\n        ""tpu_estimator_dnn"":\n            _DNNTPUEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=optimizer_fn,\n                hidden_units=[3],\n                use_tpu=True),\n        ""tpu_estimator_wider_dnn"":\n            _DNNTPUEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=optimizer_fn,\n                hidden_units=[6],\n                use_tpu=True),\n        ""estimator_dnn"":\n            tf.compat.v1.estimator.DNNEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=optimizer_fn,\n                hidden_units=[3]),\n        ""estimator_linear"":\n            tf.compat.v1.estimator.LinearEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=optimizer_fn),\n    }\n\n    run_config = tf.compat.v1.estimator.tpu.RunConfig(\n        master="""", tf_random_seed=42)\n    estimator = AutoEnsembleTPUEstimator(\n        head=head,\n        candidate_pool=candidate_pool,\n        max_iteration_steps=10,\n        model_dir=self.test_subdirectory,\n        config=run_config,\n        use_tpu=use_tpu,\n        train_batch_size=4,\n        force_grow=True)\n\n    features = {""xor"": [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]}\n    labels = [[0.], [1.], [1.], [0.]]\n\n    def train_input_fn(params):\n      del params  # Unused.\n\n      input_features = {}\n      for key, feature in features.items():\n        input_features[key] = tf.constant(feature, name=key)\n      input_labels = tf.constant(labels, name=""labels"")\n      return input_features, input_labels\n\n    def test_input_fn(params):\n      del params  # Unused.\n      return tf.compat.v1.data.Dataset.from_tensor_slices(\n          [features[""xor""]]).map(lambda f: {""xor"": f})\n\n    # Train for three iterations.\n    estimator.train(input_fn=train_input_fn, max_steps=30)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertAllClose(30, eval_results[""global_step""])\n    self.assertAllClose(0.315863, eval_results[""loss""], atol=.3)\n\n    # Predict.\n    predictions = estimator.predict(input_fn=test_input_fn)\n    # We need to iterate over all the predictions before moving on, otherwise\n    # the TPU will not be shut down.\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[""predictions""])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf.compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      for key in features:\n        features[key] = tf.constant([[0., 0.], [0., 0.]])\n      return tf.estimator.export.ServingInputReceiver(\n          features=features, receiver_tensors=serialized_example)\n\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    export_saved_model_fn = getattr(estimator, ""export_saved_model"", None)\n    if not callable(export_saved_model_fn):\n      export_saved_model_fn = estimator.export_savedmodel\n    export_saved_model_fn(\n        export_dir_base=export_dir_base,\n        serving_input_receiver_fn=serving_input_fn)\n\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/__init__.py,0,"b'""""""TensorFLow AdaNet core logic.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.core.estimator import Estimator\nfrom adanet.core.evaluator import Evaluator\nfrom adanet.core.report_materializer import ReportMaterializer\nfrom adanet.core.summary import Summary\nfrom adanet.core.tpu_estimator import TPUEstimator\n\n__all__ = [\n    ""Estimator"",\n    ""Evaluator"",\n    ""ReportMaterializer"",\n    ""Summary"",\n    ""TPUEstimator"",\n]\n'"
adanet/core/architecture.py,0,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An internal AdaNet model architecture definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport json\n\n\nclass _Architecture(object):\n  """"""An AdaNet model architecture.\n\n  This data structure is the blueprint for reconstructing an AdaNet model. It\n  contains not only information about the underlying Ensemble, but also the\n  `adanet.subnetwork.Builder` instances that compose the ensemble, the\n  `adanet.ensemble.Ensembler` that constructed it, as well as the sequence\n  of states in the search space that led to the construction of this model.\n  In addition, it stores `replay_indices` A list of indices (an index per\n  boosting iteration); Holding the index of the ensemble in the candidate list\n  throughout the run.\n\n  It is serializable and deserializable for persistent storage.\n  """"""\n\n  def __init__(self, ensemble_candidate_name, ensembler_name, global_step=None,\n               replay_indices=None):\n    self._ensemble_candidate_name = ensemble_candidate_name\n    self._ensembler_name = ensembler_name\n    self._global_step = global_step\n    self._subnets = []\n    self._replay_indices = replay_indices or []\n\n  @property\n  def ensemble_candidate_name(self):\n    """"""The ensemble candidate\'s name.\n\n    Returns:\n      String name of the ensemble candidate with this architecture.\n    """"""\n    return self._ensemble_candidate_name\n\n  @property\n  def ensembler_name(self):\n    """"""The ensembler\'s name.\n\n    Returns:\n      String name of the ensembler that constructed the ensemble.\n    """"""\n    return self._ensembler_name\n\n  @property\n  def global_step(self):\n    """"""The global step when this architecture was serialized.\n\n    Returns:\n      Integer global step.\n    """"""\n\n    return self._global_step\n\n  @property\n  def subnetworks(self):\n    """"""The component subnetworks.\n\n    Returns:\n      An Iterable of (iteration_number, builder_name) tuples.\n    """"""\n\n    return tuple(self._subnets)\n\n  @property\n  def replay_indices(self):\n    """"""The list of replay indices.\n\n    Returns:\n      A list of integers (an integer per boosting iteration); Holding the index\n      of the ensemble in the candidate list throughout the run\n    """"""\n\n    return self._replay_indices\n\n  @property\n  def subnetworks_grouped_by_iteration(self):\n    """"""The component subnetworks grouped by iteration number.\n\n    Returns:\n      An Iterable of (iteration_number, builder_names) tuples where the builder\n        names are grouped by iteration number.\n    """"""\n\n    subnet_by_iteration = {}\n    for iteration_number, builder_name in self._subnets:\n      if iteration_number not in subnet_by_iteration:\n        subnet_by_iteration[iteration_number] = []\n      subnet_by_iteration[iteration_number].append(builder_name)\n    return tuple([\n        (i, tuple(subnet_by_iteration[i])) for i in sorted(subnet_by_iteration)\n    ])\n\n  def add_subnetwork(self, iteration_number, builder_name):\n    """"""Adds the given subnetwork metadata.\n\n    Args:\n      iteration_number: Integer iteration number when this Subnetwork was\n        created.\n      builder_name: String name of the `adanet.subnetwork.Builder` that produced\n        this Subnetwork.\n    """"""\n    self._subnets.append((iteration_number, builder_name))\n\n  # TODO: Remove setters and getters.\n  def add_replay_index(self, index):\n    self._replay_indices.append(index)\n\n  def set_replay_indices(self, indices):\n    self._replay_indices = copy.copy(indices)\n\n  def serialize(self, iteration_number, global_step):\n    """"""Returns a string serialization of this object.""""""\n\n    # TODO: Confirm that it makes sense to have global step of 0.\n    assert global_step is not None\n    ensemble_arch = {\n        ""ensemble_candidate_name"": self.ensemble_candidate_name,\n        ""iteration_number"": int(iteration_number),\n        ""global_step"": int(global_step),\n        ""ensembler_name"": self.ensembler_name,\n        ""subnetworks"": [],\n        ""replay_indices"": self._replay_indices\n    }\n    for iteration_number, builder_name in self._subnets:\n      subnetwork_arch = {\n          ""iteration_number"": int(iteration_number),\n          ""builder_name"": builder_name,\n      }\n      ensemble_arch[""subnetworks""].append(subnetwork_arch)\n    return json.dumps(ensemble_arch, sort_keys=True)\n\n  @staticmethod\n  def deserialize(serialized_architecture):\n    """"""Deserializes a serialized architecture.\n\n    Args:\n      serialized_architecture: String representation of an `_Architecture`\n        obtained by calling `serialize`.\n\n    Returns:\n      A deserialized `_Architecture` instance.\n    """"""\n\n    ensemble_arch = json.loads(serialized_architecture)\n    architecture = _Architecture(ensemble_arch[""ensemble_candidate_name""],\n                                 ensemble_arch[""ensembler_name""],\n                                 ensemble_arch[""global_step""],\n                                 ensemble_arch[""replay_indices""])\n    for subnet in ensemble_arch[""subnetworks""]:\n      architecture.add_subnetwork(subnet[""iteration_number""],\n                                  subnet[""builder_name""])\n    return architecture\n'"
adanet/core/architecture_test.py,2,"b'""""""Test for the AdaNet architecture.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet.core.architecture import _Architecture\nimport tensorflow.compat.v1 as tf\n\n\nclass ArchitectureTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""empty"",\n      ""subnetworks"": [],\n      ""want"": (),\n  }, {\n      ""testcase_name"": ""single"",\n      ""subnetworks"": [(0, ""linear"")],\n      ""want"": ((0, ""linear""),),\n  }, {\n      ""testcase_name"": ""different_iterations"",\n      ""subnetworks"": [(0, ""linear""), (1, ""dnn"")],\n      ""want"": ((0, ""linear""), (1, ""dnn"")),\n  }, {\n      ""testcase_name"": ""same_iterations"",\n      ""subnetworks"": [(0, ""linear""), (0, ""dnn""), (1, ""dnn"")],\n      ""want"": ((0, ""linear""), (0, ""dnn""), (1, ""dnn"")),\n  })\n  def test_subnetworks(self, subnetworks, want):\n    arch = _Architecture(""foo"", ""dummy_ensembler_name"")\n    for subnetwork in subnetworks:\n      arch.add_subnetwork(*subnetwork)\n    self.assertEqual(want, arch.subnetworks)\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""empty"",\n      ""subnetworks"": [],\n      ""want"": (),\n  }, {\n      ""testcase_name"": ""single"",\n      ""subnetworks"": [(0, ""linear"")],\n      ""want"": ((0, (""linear"",)),),\n  }, {\n      ""testcase_name"": ""different_iterations"",\n      ""subnetworks"": [(0, ""linear""), (1, ""dnn"")],\n      ""want"": ((0, (""linear"",)), (1, (""dnn"",))),\n  }, {\n      ""testcase_name"": ""same_iterations"",\n      ""subnetworks"": [(0, ""linear""), (0, ""dnn""), (1, ""dnn"")],\n      ""want"": ((0, (""linear"", ""dnn"")), (1, (""dnn"",))),\n  })\n  def test_subnetworks_grouped_by_iteration(self, subnetworks, want):\n    arch = _Architecture(""foo"", ""dummy_ensembler_name"")\n    for subnetwork in subnetworks:\n      arch.add_subnetwork(*subnetwork)\n    self.assertEqual(want, arch.subnetworks_grouped_by_iteration)\n\n  def test_set_and_add_replay_index(self):\n    arch = _Architecture(""foo"", ""dummy_ensembler_name"")\n    arch.set_replay_indices([1, 2, 3])\n    self.assertAllEqual([1, 2, 3], arch.replay_indices)\n    arch.add_replay_index(4)\n    self.assertAllEqual([1, 2, 3, 4], arch.replay_indices)\n\n  def test_serialization_lifecycle(self):\n    arch = _Architecture(""foo"", ""dummy_ensembler_name"", replay_indices=[1, 2])\n    arch.add_subnetwork(0, ""linear"")\n    arch.add_subnetwork(0, ""dnn"")\n    arch.add_subnetwork(1, ""dnn"")\n    self.assertEqual(""foo"", arch.ensemble_candidate_name)\n    self.assertEqual(""dummy_ensembler_name"", arch.ensembler_name)\n    self.assertEqual(((0, (""linear"", ""dnn"")), (1, (""dnn"",))),\n                     arch.subnetworks_grouped_by_iteration)\n    iteration_number = 2\n    global_step = 100\n    serialized = arch.serialize(iteration_number, global_step)\n    self.assertEqual(\n        \'{""ensemble_candidate_name"": ""foo"", ""ensembler_name"": \'\n        \'""dummy_ensembler_name"", ""global_step"": 100, ""iteration_number"": 2, \'\n        \'""replay_indices"": [1, 2], \'\n        \'""subnetworks"": [{""builder_name"": ""linear"", ""iteration_number"": 0}, \'\n        \'{""builder_name"": ""dnn"", ""iteration_number"": 0},\'\n        \' {""builder_name"": ""dnn"", ""iteration_number"": 1}]}\', serialized)\n    deserialized_arch = _Architecture.deserialize(serialized)\n    self.assertEqual(arch.ensemble_candidate_name,\n                     deserialized_arch.ensemble_candidate_name)\n    self.assertEqual(arch.ensembler_name,\n                     deserialized_arch.ensembler_name)\n    self.assertEqual(arch.subnetworks_grouped_by_iteration,\n                     deserialized_arch.subnetworks_grouped_by_iteration)\n    self.assertEqual(global_step, deserialized_arch.global_step)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/candidate.py,3,"b'""""""The AdaNet candidate implementation in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom adanet import tf_compat\nimport tensorflow.compat.v2 as tf\n\n\nclass _Candidate(\n    collections.namedtuple(""_Candidate"",\n                           [""ensemble_spec"", ""adanet_loss"", ""variables""])):\n  """"""An AdaNet candidate.\n\n  A `_Candidate` tracks the progress of a candidate subnetwork\'s training\n  within an ensemble, as well as their AdaNet loss over time.\n  """"""\n\n  def __new__(cls, ensemble_spec, adanet_loss, variables):\n    """"""Creates a validated `_Candidate` instance.\n\n    Args:\n      ensemble_spec: The `_EnsembleSpec` instance to track.\n      adanet_loss: float `Tensor` representing the ensemble\'s AdaNet loss on the\n        training set as defined in Equation (4) of the paper.\n      variables: List of `tf.Variable` instances associated with the ensemble.\n\n    Returns:\n      A validated `_Candidate` object.\n\n    Raises:\n      ValueError: If validation fails.\n    """"""\n\n    if ensemble_spec is None:\n      raise ValueError(""ensemble_spec is required"")\n    if adanet_loss is None:\n      raise ValueError(""adanet_loss is required"")\n    return super(_Candidate, cls).__new__(\n        cls,\n        ensemble_spec=ensemble_spec,\n        adanet_loss=adanet_loss,\n        variables=variables)\n\n\nclass _CandidateBuilder(object):\n  """"""Builds AdaNet candidates.""""""\n\n  def __init__(self, adanet_loss_decay=.999):\n    """"""Creates a `_CandidateBuilder` instance.\n\n    Args:\n      adanet_loss_decay: Float. The adanet loss is tracked as an exponential\n        moving average, so this is the decay rate to use.\n\n    Returns:\n      A `_CandidateBuilder` object.\n    """"""\n\n    self._adanet_loss_decay = adanet_loss_decay\n    super(_CandidateBuilder, self).__init__()\n\n  def build_candidate(self,\n                      ensemble_spec,\n                      training,\n                      summary,\n                      rebuilding=False,\n                      track_moving_average=True):\n    """"""Builds and returns an AdaNet candidate.\n\n    Args:\n      ensemble_spec: `_EnsembleSpec` instance to track.\n      training: A python boolean indicating whether the graph is in training\n        mode or prediction mode.\n      summary: A `Summary` for recording summaries for TensorBoard.\n      rebuilding: Boolean whether the iteration is being rebuilt only to restore\n        the previous best subnetworks and ensembles.\n      track_moving_average: Bool whether to track the moving average of the\n        ensemble\'s adanet loss.\n\n    Returns:\n      A _Candidate instance.\n    """"""\n\n    from tensorflow.python.training import moving_averages  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    candidate_scope = ""candidate_{}"".format(ensemble_spec.name)\n\n    with tf_compat.v1.variable_scope(candidate_scope):\n      adanet_loss = ensemble_spec.adanet_loss\n      variables = []\n      if track_moving_average:\n        loss_to_track = ensemble_spec.adanet_loss\n        if loss_to_track is None:\n          # Dummy loss so that we always create moving averages variables.\n          # If we pass a None loss assign_moving_average raises an exception.\n          loss_to_track = tf.constant(99999., name=""dummy_adanet_loss"")\n\n        adanet_loss_var = tf_compat.v1.get_variable(\n            ""adanet_loss"", initializer=0., trainable=False)\n        update_adanet_loss_op = moving_averages.assign_moving_average(\n            adanet_loss_var, loss_to_track, decay=self._adanet_loss_decay)\n        # Get the two moving average variables created by assign_moving_average.\n        # Since these two variables are the most recently created ones, we can\n        # slice them both from the end of the global variables collection.\n        variables = [adanet_loss_var] + tf_compat.v1.global_variables()[-2:]\n        if training and not rebuilding:\n          with tf.control_dependencies([update_adanet_loss_op]):\n            adanet_loss = adanet_loss_var.read_value()\n        else:\n          adanet_loss = adanet_loss_var.read_value()\n\n      with summary.current_scope():\n        summary.scalar(""adanet_loss/adanet/adanet_weighted_ensemble"",\n                       adanet_loss)\n\n      return _Candidate(\n          ensemble_spec=ensemble_spec,\n          adanet_loss=adanet_loss,\n          variables=variables)\n'"
adanet/core/candidate_test.py,4,"b'""""""Test AdaNet single graph candidate implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.candidate import _Candidate\nfrom adanet.core.candidate import _CandidateBuilder\nimport adanet.core.testing_utils as tu\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass CandidateTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""valid"",\n      ""ensemble_spec"": tu.dummy_ensemble_spec(""foo""),\n      ""adanet_loss"": [.1],\n  })\n  @test_util.run_in_graph_and_eager_modes\n  def test_new(self, ensemble_spec, adanet_loss, variables=None):\n    with self.test_session():\n      got = _Candidate(ensemble_spec, adanet_loss, variables)\n      self.assertEqual(got.ensemble_spec, ensemble_spec)\n      self.assertEqual(got.adanet_loss, adanet_loss)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""none_ensemble_spec"",\n          ""ensemble_spec"": None,\n          ""adanet_loss"": [.1],\n      }, {\n          ""testcase_name"": ""none_adanet_loss"",\n          ""ensemble_spec"": tu.dummy_ensemble_spec(""foo""),\n          ""adanet_loss"": None,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_new_errors(self, ensemble_spec, adanet_loss, variables=None):\n    with self.test_session():\n      with self.assertRaises(ValueError):\n        _Candidate(ensemble_spec, adanet_loss, variables)\n\n\nclass _FakeSummary(object):\n  """"""A fake adanet.Summary.""""""\n\n  def scalar(self, name, tensor, family=None):\n    del name\n    del tensor\n    del family\n    return ""fake_scalar""\n\n  @contextlib.contextmanager\n  def current_scope(self):\n    yield\n\n\nclass CandidateBuilderTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""evaluate"",\n          ""training"": False,\n          ""want_adanet_losses"": [0., 0., 0.],\n      }, {\n          ""testcase_name"": ""train_exactly_max_steps"",\n          ""training"": True,\n          ""want_adanet_losses"": [1., .750, .583],\n      }, {\n          ""testcase_name"": ""train_one_step_max_one_step"",\n          ""training"": True,\n          ""want_adanet_losses"": [1.],\n      }, {\n          ""testcase_name"": ""train_two_steps_max_two_steps"",\n          ""training"": True,\n          ""want_adanet_losses"": [1., .750],\n      }, {\n          ""testcase_name"": ""train_three_steps_max_four_steps"",\n          ""training"": True,\n          ""want_adanet_losses"": [1., .750, .583],\n      }, {\n          ""testcase_name"": ""eval_one_step"",\n          ""training"": False,\n          ""want_adanet_losses"": [0.],\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_candidate(self, training, want_adanet_losses):\n    # `Cadidate#build_candidate` will only ever be called in graph mode.\n    with context.graph_mode():\n      # A fake adanet_loss that halves at each train step: 1.0, 0.5, 0.25, ...\n      fake_adanet_loss = tf.Variable(1.)\n      fake_train_op = fake_adanet_loss.assign(fake_adanet_loss / 2)\n      fake_ensemble_spec = tu.dummy_ensemble_spec(\n          ""new"", adanet_loss=fake_adanet_loss, train_op=fake_train_op)\n\n      builder = _CandidateBuilder()\n      candidate = builder.build_candidate(\n          ensemble_spec=fake_ensemble_spec,\n          training=training,\n          summary=_FakeSummary())\n      self.evaluate(tf_compat.v1.global_variables_initializer())\n      adanet_losses = []\n      for _ in range(len(want_adanet_losses)):\n        adanet_loss = self.evaluate(candidate.adanet_loss)\n        adanet_losses.append(adanet_loss)\n        self.evaluate(fake_train_op)\n\n      # Verify that adanet_loss moving average works.\n      self.assertAllClose(want_adanet_losses, adanet_losses, atol=1e-3)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/ensemble_builder.py,36,"b'""""""An AdaNet ensemble definition in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\n\nfrom absl import logging\nfrom adanet import ensemble as ensemble_lib\nfrom adanet import subnetwork as subnetwork_lib\nfrom adanet import tf_compat\nfrom adanet.core.architecture import _Architecture\nfrom adanet.core.eval_metrics import _EnsembleMetrics\nfrom adanet.core.eval_metrics import _SubnetworkMetrics\nfrom adanet.core.summary import monkey_patched_summaries\nimport tensorflow.compat.v1 as tf_v1\nimport tensorflow.compat.v2 as tf\n\n_VALID_METRIC_FN_ARGS = {""features"", ""labels"", ""predictions""}\n\n\nclass _EnsembleSpec(\n    collections.namedtuple(""_EnsembleSpec"", [\n        ""name"",\n        ""ensemble"",\n        ""architecture"",\n        ""subnetwork_builders"",\n        ""subnetwork_specs"",\n        ""predictions"",\n        ""step"",\n        ""variables"",\n        ""loss"",\n        ""adanet_loss"",\n        ""train_op"",\n        ""eval_metrics"",\n        ""export_outputs"",\n    ])):\n  """"""Ensemble training and evaluation `Tensors` and `Ops`.\n\n  Args:\n    name: String name of this ensemble. Should be unique in the graph.\n    ensemble: The `adanet.ensemble.Ensemble` of interest.\n    architecture: The `_Architecture` that represents this ensemble.\n    subnetwork_builders: The Iterable of candidate subnetworks for the current\n      iteration.\n    predictions: Predictions `Tensor` or dict of `Tensor`.\n    step: `tf.Variable` step counter representing the number of steps this\n      ensemble trained for. Resets at every AdaNet iteration.\n    variables: List of `tf.Variable` instances associated with the ensemble.\n    loss: Loss `Tensor` as defined by the surrogate loss function Phi in\n      Equations (4), (5), and (6). Must be either scalar, or with shape `[1]`.\n    adanet_loss: Loss `Tensor` as defined by F(w) in Equation (4). Must be\n      either scalar, or with shape `[1]`. The AdaNet algorithm aims to minimize\n      this objective which balances training loss with the total complexity of\n      the subnetworks in the ensemble.\n    train_op: Candidate ensemble\'s mixture weights `TrainOpSpec`.\n    eval_metrics: `_EnsembleMetrics` object.\n    export_outputs: Describes the output signatures to be exported to\n      `SavedModel` and used during serving. See `tf.estimator.EstimatorSpec`.\n    subnetwork_specs: Iterable of `_SubnetworkSpecs` for this iteration.\n\n  Returns:\n    An `EnsembleSpec` object.\n  """"""\n\n  def __new__(cls,\n              name,\n              ensemble,\n              architecture,\n              subnetwork_builders,\n              predictions,\n              step,\n              variables,\n              loss=None,\n              adanet_loss=None,\n              train_op=None,\n              eval_metrics=None,\n              export_outputs=None,\n              subnetwork_specs=None):\n    if subnetwork_specs is None:\n      subnetwork_specs = []\n    return super(_EnsembleSpec, cls).__new__(\n        cls,\n        name=name,\n        ensemble=ensemble,\n        architecture=architecture,\n        subnetwork_builders=subnetwork_builders,\n        subnetwork_specs=subnetwork_specs,\n        predictions=predictions,\n        step=step,\n        variables=variables,\n        loss=loss,\n        adanet_loss=adanet_loss,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        export_outputs=export_outputs)\n\n\ndef _verify_metric_fn_args(metric_fn):\n  if not metric_fn:\n    return\n  # Calling low level getargs for py_2_and_3 compatibility.\n  args = set(inspect.getargs(metric_fn.__code__).args)\n  invalid_args = list(args - _VALID_METRIC_FN_ARGS)\n  if invalid_args:\n    raise ValueError(""metric_fn (%s) has following not expected args: %s"" %\n                     (metric_fn, invalid_args))\n\n\ndef _get_value(target, key):\n  if isinstance(target, dict):\n    return target[key]\n  return target\n\n\ndef _to_train_op_spec(train_op):\n  if isinstance(train_op, subnetwork_lib.TrainOpSpec):\n    return train_op\n  return subnetwork_lib.TrainOpSpec(train_op)\n\n\n@contextlib.contextmanager\ndef _monkey_patch_context(iteration_step_scope, scoped_summary, trainable_vars):\n  """"""Monkey-patches global attributes with subnetwork-specifics ones.""""""\n\n  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n  from tensorflow.python.training import training as train\n  from tensorflow.python.training import training_util\n  # pylint: enable=g-direct-tensorflow-import,g-import-not-at-top\n\n  old_get_global_step_fn = tf_compat.v1.train.get_global_step\n  old_get_or_create_global_step_fn = tf_compat.v1.train.get_or_create_global_step\n  old_trainable_vars = tf_compat.v1.trainable_variables()\n\n  def iteration_step(graph=None):\n    graph = graph or tf_compat.v1.get_default_graph()\n    with graph.as_default() as g, g.name_scope(None):\n      with tf_compat.v1.variable_scope(\n          iteration_step_scope, reuse=tf_compat.v1.AUTO_REUSE):\n        return tf_compat.v1.get_variable(\n            ""iteration_step"",\n            shape=[],\n            initializer=tf_compat.v1.zeros_initializer(),\n            trainable=False,\n            dtype=tf.int64)\n\n  # monkey-patch global attributes.\n  setattr(tf_compat.v1.train, ""get_global_step"", iteration_step)\n  setattr(tf_compat.v1.train, ""get_or_create_global_step"", iteration_step)\n  setattr(tf_v1.train, ""get_global_step"", iteration_step)\n  setattr(tf_v1.train, ""get_or_create_global_step"", iteration_step)\n  setattr(tf.train, ""get_global_step"", iteration_step)\n  setattr(tf.train, ""get_or_create_global_step"", iteration_step)\n  setattr(train, ""get_global_step"", iteration_step)\n  setattr(training_util, ""get_global_step"", iteration_step)\n  setattr(train, ""get_or_create_global_step"", iteration_step)\n  setattr(training_util, ""get_or_create_global_step"", iteration_step)\n  # The TPUEmbedding uses dummy variables to coordinate sending and receiving\n  # gradients. If no gradients are computed on these dummy variables, the\n  # TPUEmbedding will throw an error.\n  embedding_variables = tf_compat.v1.get_collection(\n      ""tpu_embedding_dummy_table_variables"")\n  _set_trainable_variables(trainable_vars + embedding_variables)\n\n  try:\n    with monkey_patched_summaries(scoped_summary):\n      yield\n  finally:\n    # Revert monkey-patches.\n    new_trainable_vars = _get_current_vars(\n        diffbase={""trainable"": trainable_vars})[""trainable""]\n    _set_trainable_variables(old_trainable_vars + new_trainable_vars)\n    setattr(training_util, ""get_or_create_global_step"",\n            old_get_or_create_global_step_fn)\n    setattr(train, ""get_or_create_global_step"",\n            old_get_or_create_global_step_fn)\n    setattr(training_util, ""get_global_step"", old_get_global_step_fn)\n    setattr(train, ""get_global_step"", old_get_global_step_fn)\n    setattr(tf.train, ""get_or_create_global_step"",\n            old_get_or_create_global_step_fn)\n    setattr(tf.train, ""get_global_step"", old_get_global_step_fn)\n    setattr(tf_v1.train, ""get_or_create_global_step"",\n            old_get_or_create_global_step_fn)\n    setattr(tf_v1.train, ""get_global_step"", old_get_global_step_fn)\n    setattr(tf_compat.v1.train, ""get_or_create_global_step"",\n            old_get_or_create_global_step_fn)\n    setattr(tf_compat.v1.train, ""get_global_step"", old_get_global_step_fn)\n\n\ndef _clear_trainable_variables():\n  del tf_compat.v1.get_collection_ref(\n      tf_compat.v1.GraphKeys.TRAINABLE_VARIABLES)[:]\n\n\ndef _set_trainable_variables(var_list):\n  _clear_trainable_variables()\n  for var in var_list:\n    assert isinstance(var, tf.Variable)\n    tf_compat.v1.add_to_collections(tf_compat.v1.GraphKeys.TRAINABLE_VARIABLES,\n                                    var)\n\n\ndef _get_current_vars(diffbase=None):\n  """"""Returns all current trainable, global, and savable variables.\n\n  Args:\n    diffbase: A dictionary of lists variables to diffbase. The allowed keys are:\n    ""trainable"", ""global"" and ""savable"".\n\n  Returns:\n    A dictionary containing the current trainable, global and savable variables.\n    The expected keys are: ""trainable"", ""global"" and ""savable"".\n  """"""\n\n  trainable_vars = tf_compat.v1.trainable_variables()\n  global_vars = tf_compat.v1.global_variables()\n  savable_vars = tf_compat.v1.get_collection(\n      tf_compat.v1.GraphKeys.SAVEABLE_OBJECTS)\n\n  # Since newly created variables are appended to the global collections, we can\n  # obtain the newer variables by taking slices of the ends of the collections.\n  if diffbase:\n    if ""trainable"" in diffbase:\n      trainable_vars = trainable_vars[len(diffbase[""trainable""]):]\n    if ""global"" in diffbase:\n      global_vars = global_vars[len(diffbase[""global""]):]\n    if ""savable"" in diffbase:\n      savable_vars = savable_vars[len(diffbase[""savable""]):]\n\n  return {\n      ""trainable"": trainable_vars,\n      ""global"": global_vars,\n      ""savable"": savable_vars,\n  }\n\n\nclass _EnsembleBuilder(object):\n  """"""Builds `_EnsembleSpec` instances.\n\n  Args:\n    head: A `tf.contrib.estimator.Head` instance.\n    metric_fn: A function which should obey the following signature:\n      - Args: can only have following three arguments in any order:\n        * predictions: Predictions `Tensor` or dict of `Tensor` created by given\n          `Head`.\n        * features: Input `dict` of `Tensor` objects created by `input_fn` which\n          is given to `estimator.evaluate` as an argument.\n        * labels:  Labels `Tensor` or dict of `Tensor` (for multi-head) created\n          by `input_fn` which is given to `estimator.evaluate` as an argument.\n      - Returns: Dict of metric results keyed by name. Final metrics are a union\n        of this and `Head\'s` existing metrics. If there is a name conflict\n        between this and `estimator`s existing metrics, this will override the\n        existing one. The values of the dict are the results of calling a metric\n        function, namely a `(metric_tensor, update_op)` tuple.\n    use_tpu: Whether AdaNet is running on TPU.\n    export_subnetwork_logits: Include subnetwork logits in exports.\n    export_subnetwork_last_layer: Include subnetwork last layer in exports.\n\n  Returns:\n    An `_EnsembleBuilder` instance.\n  """"""\n\n  _SUBNETWORK_LOGITS_EXPORT_SIGNATURE = ""subnetwork_logits""\n  _SUBNETWORK_LAST_LAYER_EXPORT_SIGNATURE = ""subnetwork_last_layer""\n\n  def __init__(self,\n               head,\n               metric_fn=None,\n               use_tpu=False,\n               export_subnetwork_logits=False,\n               export_subnetwork_last_layer=False):\n    _verify_metric_fn_args(metric_fn)\n\n    self._head = head\n    self._metric_fn = metric_fn\n    self._use_tpu = use_tpu\n    self._export_subnetwork_logits = export_subnetwork_logits\n    self._export_subnetwork_last_layer = export_subnetwork_last_layer\n\n  def build_ensemble_spec(self,\n                          name,\n                          candidate,\n                          ensembler,\n                          subnetwork_specs,\n                          summary,\n                          features,\n                          mode,\n                          iteration_number,\n                          labels,\n                          my_ensemble_index,\n                          previous_ensemble_spec,\n                          previous_iteration_checkpoint):\n    """"""Builds an `_EnsembleSpec` with the given `adanet.ensemble.Candidate`.\n\n    Args:\n      name: The string name of the ensemble. Typically the name of the builder\n        that returned the given `Subnetwork`.\n      candidate: The `adanet.ensemble.Candidate` for this spec.\n      ensembler: The :class:`adanet.ensemble.Ensembler` to use to ensemble a\n        group of subnetworks.\n      subnetwork_specs: Iterable of `_SubnetworkSpecs` for this iteration.\n      summary: A `_ScopedSummary` instance for recording ensemble summaries.\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator `ModeKeys` indicating training, evaluation, or inference.\n      iteration_number: Integer current iteration number.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head).\n      my_ensemble_index: An integer holding the index of the ensemble in the\n        candidates list of AdaNet.\n      previous_ensemble_spec: Link the rest of the `_EnsembleSpec` from\n        iteration t-1. Used for creating the subnetwork train_op.\n      previous_iteration_checkpoint: `tf.train.Checkpoint` for iteration t-1.\n\n    Returns:\n      An `_EnsembleSpec` instance.\n    """"""\n\n    with tf_compat.v1.variable_scope(""ensemble_{}"".format(name)):\n      step = tf_compat.v1.get_variable(\n          ""step"",\n          shape=[],\n          initializer=tf_compat.v1.zeros_initializer(),\n          trainable=False,\n          dtype=tf.int64)\n      # Convert to tensor so that users cannot mutate it.\n      step_tensor = tf.convert_to_tensor(value=step)\n      with summary.current_scope():\n        summary.scalar(""iteration_step/adanet/iteration_step"", step_tensor)\n      replay_indices = []\n      if previous_ensemble_spec:\n        replay_indices = copy.copy(\n            previous_ensemble_spec.architecture.replay_indices)\n      if my_ensemble_index is not None:\n        replay_indices.append(my_ensemble_index)\n\n      architecture = _Architecture(\n          candidate.name, ensembler.name, replay_indices=replay_indices)\n      previous_subnetworks = []\n      previous_subnetwork_specs = []\n      subnetwork_builders = []\n      previous_ensemble = None\n      if previous_ensemble_spec:\n        previous_ensemble = previous_ensemble_spec.ensemble\n        previous_architecture = previous_ensemble_spec.architecture\n        keep_indices = range(len(previous_ensemble.subnetworks))\n        if len(candidate.subnetwork_builders) == 1 and previous_ensemble:\n          # Prune previous ensemble according to the subnetwork.Builder for\n          # backwards compatibility.\n          subnetwork_builder = candidate.subnetwork_builders[0]\n          prune_previous_ensemble = getattr(subnetwork_builder,\n                                            ""prune_previous_ensemble"", None)\n          if callable(prune_previous_ensemble):\n            logging.warn(\n                ""Using an `adanet.subnetwork.Builder#prune_previous_ensemble` ""\n                ""is deprecated. Please use a custom `adanet.ensemble.Strategy` ""\n                ""instead."")\n            keep_indices = prune_previous_ensemble(previous_ensemble)\n        for i, builder in enumerate(previous_ensemble_spec.subnetwork_builders):\n          if i not in keep_indices:\n            continue\n          if builder not in candidate.previous_ensemble_subnetwork_builders:\n            continue\n          previous_subnetworks.append(previous_ensemble.subnetworks[i])\n          previous_subnetwork_specs.append(\n              previous_ensemble_spec.subnetwork_specs[i])\n          subnetwork_builders.append(builder)\n          architecture.add_subnetwork(*previous_architecture.subnetworks[i])\n      for builder in candidate.subnetwork_builders:\n        architecture.add_subnetwork(iteration_number, builder.name)\n        subnetwork_builders.append(builder)\n      subnetwork_spec_map = {s.builder.name: s for s in subnetwork_specs}\n      relevant_subnetwork_specs = [\n          subnetwork_spec_map[s.name] for s in candidate.subnetwork_builders\n      ]\n      ensemble_scope = tf_compat.v1.get_variable_scope()\n\n      old_vars = _get_current_vars()\n\n      with summary.current_scope(), _monkey_patch_context(\n          iteration_step_scope=ensemble_scope,\n          scoped_summary=summary,\n          trainable_vars=[]):\n        ensemble = ensembler.build_ensemble(\n            subnetworks=[s.subnetwork for s in relevant_subnetwork_specs],\n            previous_ensemble_subnetworks=previous_subnetworks,\n            features=features,\n            labels=labels,\n            logits_dimension=self._head.logits_dimension,\n            training=mode == tf.estimator.ModeKeys.TRAIN,\n            iteration_step=step_tensor,\n            summary=summary,\n            previous_ensemble=previous_ensemble,\n            previous_iteration_checkpoint=previous_iteration_checkpoint)\n\n      estimator_spec = _create_estimator_spec(self._head, features, labels,\n                                              mode, ensemble.logits,\n                                              self._use_tpu)\n\n      ensemble_loss = estimator_spec.loss\n      adanet_loss = None\n      if mode != tf.estimator.ModeKeys.PREDICT:\n        adanet_loss = estimator_spec.loss\n        # Add ensembler specific loss\n        if isinstance(ensemble, ensemble_lib.ComplexityRegularized):\n          adanet_loss += ensemble.complexity_regularization\n\n      predictions = estimator_spec.predictions\n      export_outputs = estimator_spec.export_outputs\n\n      if (self._export_subnetwork_logits and\n          export_outputs and subnetwork_spec_map):\n        first_subnetwork_logits = list(\n            subnetwork_spec_map.values())[0].subnetwork.logits\n        if isinstance(first_subnetwork_logits, dict):\n          for head_name in first_subnetwork_logits.keys():\n            subnetwork_logits = {\n                subnetwork_name: subnetwork_spec.subnetwork.logits[head_name]\n                for subnetwork_name, subnetwork_spec in\n                subnetwork_spec_map.items()\n            }\n            export_outputs.update({\n                ""{}_{}"".format(\n                    _EnsembleBuilder._SUBNETWORK_LOGITS_EXPORT_SIGNATURE,\n                    head_name):\n                    tf.estimator.export.PredictOutput(subnetwork_logits)\n            })\n        else:\n          subnetwork_logits = {\n              subnetwork_name: subnetwork_spec.subnetwork.logits for\n              subnetwork_name, subnetwork_spec in subnetwork_spec_map.items()\n          }\n          export_outputs.update({\n              _EnsembleBuilder._SUBNETWORK_LOGITS_EXPORT_SIGNATURE:\n                  tf.estimator.export.PredictOutput(subnetwork_logits)\n          })\n\n      if (self._export_subnetwork_last_layer and export_outputs and\n          subnetwork_spec_map and\n          list(subnetwork_spec_map.values())[0].subnetwork.last_layer is\n          not None):\n        first_subnetwork_last_layer = list(\n            subnetwork_spec_map.values())[0].subnetwork.last_layer\n        if isinstance(first_subnetwork_last_layer, dict):\n          for head_name in first_subnetwork_last_layer.keys():\n            subnetwork_last_layer = {\n                subnetwork_name:\n                subnetwork_spec.subnetwork.last_layer[head_name] for\n                subnetwork_name, subnetwork_spec in subnetwork_spec_map.items()\n            }\n            export_outputs.update({\n                ""{}_{}"".format(\n                    _EnsembleBuilder._SUBNETWORK_LAST_LAYER_EXPORT_SIGNATURE,\n                    head_name):\n                    tf.estimator.export.PredictOutput(subnetwork_last_layer)\n            })\n        else:\n          subnetwork_last_layer = {\n              subnetwork_name: subnetwork_spec.subnetwork.last_layer for\n              subnetwork_name, subnetwork_spec in subnetwork_spec_map.items()\n          }\n          export_outputs.update({\n              _EnsembleBuilder._SUBNETWORK_LAST_LAYER_EXPORT_SIGNATURE:\n                  tf.estimator.export.PredictOutput(subnetwork_last_layer)\n          })\n\n      if ensemble.predictions and predictions:\n        predictions.update(ensemble.predictions)\n      if ensemble.predictions and export_outputs:\n        export_outputs.update({\n            k: tf.estimator.export.PredictOutput(v)\n            for k, v in ensemble.predictions.items()\n        })\n\n      ensemble_metrics = _EnsembleMetrics(use_tpu=self._use_tpu)\n      if mode == tf.estimator.ModeKeys.EVAL:\n        ensemble_metrics.create_eval_metrics(\n            features=features,\n            labels=labels,\n            estimator_spec=estimator_spec,\n            metric_fn=self._metric_fn,\n            architecture=architecture)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        with summary.current_scope():\n          summary.scalar(""loss"", estimator_spec.loss)\n\n      ensemble_trainable_vars = _get_current_vars(\n          diffbase=old_vars)[""trainable""]\n      # Create train ops for training subnetworks and ensembles.\n      train_op = None\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        # Note that these mixture weights are on top of the last_layer of the\n        # subnetwork constructed in TRAIN mode, which means that dropout is\n        # still applied when the mixture weights are being trained.\n        ensemble_scope = tf_compat.v1.get_variable_scope()\n        with tf_compat.v1.variable_scope(""train_mixture_weights""):\n          with summary.current_scope(), _monkey_patch_context(\n              iteration_step_scope=ensemble_scope,\n              scoped_summary=summary,\n              trainable_vars=ensemble_trainable_vars):\n            # For backwards compatibility.\n            subnetwork_builder = candidate.subnetwork_builders[0]\n            old_train_op_fn = getattr(subnetwork_builder,\n                                      ""build_mixture_weights_train_op"", None)\n            if callable(old_train_op_fn):\n              logging.warn(\n                  ""The `build_mixture_weights_train_op` method is deprecated. ""\n                  ""Please use the `Ensembler#build_train_op` instead."")\n              train_op = _to_train_op_spec(\n                  subnetwork_builder.build_mixture_weights_train_op(\n                      loss=adanet_loss,\n                      var_list=ensemble_trainable_vars,\n                      logits=ensemble.logits,\n                      labels=labels,\n                      iteration_step=step_tensor,\n                      summary=summary))\n            else:\n              train_op = _to_train_op_spec(\n                  ensembler.build_train_op(\n                      ensemble=ensemble,\n                      loss=adanet_loss,\n                      var_list=ensemble_trainable_vars,\n                      labels=labels,\n                      iteration_step=step_tensor,\n                      summary=summary,\n                      previous_ensemble=previous_ensemble))\n\n      new_vars = _get_current_vars(diffbase=old_vars)\n      # Sort our dictionary by key to remove non-determinism of variable order.\n      new_vars = collections.OrderedDict(sorted(new_vars.items()))\n      # Combine all trainable, global and savable variables into a single list.\n      ensemble_variables = sum(new_vars.values(), []) + [step]\n\n    return _EnsembleSpec(\n        name=name,\n        architecture=architecture,\n        subnetwork_builders=subnetwork_builders,\n        subnetwork_specs=previous_subnetwork_specs + relevant_subnetwork_specs,\n        ensemble=ensemble,\n        predictions=predictions,\n        step=step,\n        variables=ensemble_variables,\n        loss=ensemble_loss,\n        adanet_loss=adanet_loss,\n        train_op=train_op,\n        eval_metrics=ensemble_metrics,\n        export_outputs=export_outputs)\n\n\ndef _create_estimator_spec(head, features, labels, mode, logits, use_tpu):\n  """"""Creates the head\'s EstimatorSpec or TPUEstimatorSpec on TPU.""""""\n\n  if use_tpu:\n    create_spec_fn = head._create_tpu_estimator_spec  # pylint: disable=protected-access\n  else:\n    create_spec_fn = head.create_estimator_spec\n  return create_spec_fn(\n      features=features,\n      labels=labels,\n      mode=mode,\n      logits=logits,\n      train_op_fn=lambda _: tf.no_op())\n\n\nclass _SubnetworkSpec(\n    collections.namedtuple(""_SubnetworkSpec"", [\n        ""name"",\n        ""subnetwork"",\n        ""builder"",\n        ""predictions"",\n        ""step"",\n        ""variables"",\n        ""loss"",\n        ""train_op"",\n        ""eval_metrics"",\n        ""asset_dir"",\n    ])):\n  """"""Subnetwork training and evaluation `Tensors` and `Ops`.\n\n  Args:\n    name: String name of this subnetwork. Should be unique in the graph.\n    subnetwork: The `adanet.subnetwork.Subnetwork` for this spec.\n    builder: The `adanet.subnetwork.Builder` that produced `subnetwork`.\n    predictions: Predictions `Tensor` or dict of `Tensor`.\n    step: `tf.Variable` step counter representing the number of steps this\n      subnetwork trained for. Resets at every AdaNet iteration.\n    loss: Loss `Tensor` as computed by the `Head`. Must be either scalar, or\n      with shape `[1]`.\n    train_op: Candidate subnetwork\'s `TrainOpSpec`.\n    eval_metrics: `_SubnetworkMetrics` object.\n    asset_dir: Checkpoint directory for the sub-estimators.\n\n  Returns:\n    A `_SubnetworkSpec` object.\n  """"""\n\n  def __new__(cls,\n              name,\n              subnetwork,\n              builder,\n              predictions,\n              step,\n              variables,\n              loss=None,\n              train_op=None,\n              eval_metrics=None,\n              asset_dir=None):\n    return super(_SubnetworkSpec, cls).__new__(\n        cls,\n        name=name,\n        subnetwork=subnetwork,\n        builder=builder,\n        predictions=predictions,\n        step=step,\n        variables=variables,\n        loss=loss,\n        train_op=train_op,\n        eval_metrics=eval_metrics,\n        asset_dir=asset_dir)\n\n\nclass _SubnetworkManager(object):\n  """"""Builds `_SubnetworkSpec` instances.\n\n  This class manages an `adanet.subnetwork.Builder`, creates its subnetwork and\n  train ops, and returns a `_SubnetworkSpec` that holds them.\n\n  Args:\n    head: A `tf.contrib.estimator.Head` instance.\n    metric_fn: A function which should obey the following signature:\n      - Args: can only have following three arguments in any order:\n        * predictions: Predictions `Tensor` or dict of `Tensor` created by given\n          `Head`.\n        * features: Input `dict` of `Tensor` objects created by `input_fn` which\n          is given to `estimator.evaluate` as an argument.\n        * labels:  Labels `Tensor` or dict of `Tensor` (for multi-head) created\n          by `input_fn` which is given to `estimator.evaluate` as an argument.\n      - Returns: Dict of metric results keyed by name. Final metrics are a union\n        of this and `Head\'s` existing metrics. If there is a name conflict\n        between this and `estimator`s existing metrics, this will override the\n        existing one. The values of the dict are the results of calling a metric\n        function, namely a `(metric_tensor, update_op)` tuple.\n    use_tpu: Whether AdaNet is running on TPU.\n\n  Returns:\n    An `_SubnetworkManager` instance.\n\n  Raises:\n    ValueError: If `max_steps` is <= 0.\n  """"""\n\n  def __init__(self, head, metric_fn=None, use_tpu=False):\n    _verify_metric_fn_args(metric_fn)\n    self._head = head\n    self._metric_fn = metric_fn\n    self._use_tpu = use_tpu\n\n  def build_subnetwork_spec(self,\n                            name,\n                            subnetwork_builder,\n                            summary,\n                            features,\n                            mode,\n                            labels=None,\n                            previous_ensemble=None,\n                            config=None):\n    """"""Builds a `_SubnetworkSpec` from the given `adanet.subnetwork.Builder`.\n\n    Args:\n      name: String name of the subnetwork.\n      subnetwork_builder: A `adanet.Builder` instance which defines how to train\n        the subnetwork and ensemble mixture weights.\n      summary: A `_ScopedSummary` instance for recording ensemble summaries.\n      features: Input `dict` of `Tensor` objects.\n      mode: Estimator\'s `ModeKeys`.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head). Can be `None`.\n      previous_ensemble: The previous `Ensemble` from iteration t-1. Used for\n        creating the subnetwork train_op.\n      config: The `tf.estimator.RunConfig` to use this iteration.\n\n    Returns:\n      An new `EnsembleSpec` instance with the `Subnetwork` appended.\n    """"""\n\n    old_vars = _get_current_vars()\n\n    with tf_compat.v1.variable_scope(""subnetwork_{}"".format(name)):\n      step = tf_compat.v1.get_variable(\n          ""step"",\n          shape=[],\n          initializer=tf_compat.v1.zeros_initializer(),\n          trainable=False,\n          dtype=tf.int64)\n\n      # Convert to tensor so that users cannot mutate it.\n      step_tensor = tf.convert_to_tensor(value=step)\n      with summary.current_scope():\n        summary.scalar(""iteration_step/adanet/iteration_step"", step_tensor)\n      if config:\n        subnetwork_config = config.replace(\n            model_dir=os.path.join(config.model_dir, ""assets"", name))\n      else:\n        subnetwork_config = tf.estimator.RunConfig(\n            session_config=tf.compat.v1.ConfigProto(\n                gpu_options=tf.compat.v1.GPUOptions(allow_growth=True)))\n\n      build_subnetwork = functools.partial(\n          subnetwork_builder.build_subnetwork,\n          features=features,\n          logits_dimension=self._head.logits_dimension,\n          training=mode == tf.estimator.ModeKeys.TRAIN,\n          iteration_step=step_tensor,\n          summary=summary,\n          previous_ensemble=previous_ensemble)\n      # Check which args are in the implemented build_subnetwork method\n      # signature for backwards compatibility.\n      # Calling low level getargs for py_2_and_3 compatibility.\n      defined_args = inspect.getargs(\n          subnetwork_builder.build_subnetwork.__code__).args\n      if ""labels"" in defined_args:\n        build_subnetwork = functools.partial(build_subnetwork, labels=labels)\n      if ""config"" in defined_args:\n        build_subnetwork = functools.partial(\n            build_subnetwork, config=subnetwork_config)\n      subnetwork_scope = tf_compat.v1.get_variable_scope()\n      with summary.current_scope(), _monkey_patch_context(\n          iteration_step_scope=subnetwork_scope,\n          scoped_summary=summary,\n          trainable_vars=[]):\n        subnetwork = build_subnetwork()\n\n      subnetwork_var_list = _get_current_vars(diffbase=old_vars)[""trainable""]\n\n      estimator_spec = _create_estimator_spec(self._head, features, labels,\n                                              mode, subnetwork.logits,\n                                              self._use_tpu)\n\n      subnetwork_metrics = _SubnetworkMetrics(self._use_tpu)\n      if mode == tf.estimator.ModeKeys.EVAL:\n        subnetwork_metrics.create_eval_metrics(\n            features=features,\n            labels=labels,\n            estimator_spec=estimator_spec,\n            metric_fn=self._metric_fn)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        with summary.current_scope():\n          summary.scalar(""loss"", estimator_spec.loss)\n\n      # Create train ops for training subnetworks and ensembles.\n      train_op = None\n      if mode == tf.estimator.ModeKeys.TRAIN and subnetwork_builder:\n        with summary.current_scope(), _monkey_patch_context(\n            iteration_step_scope=subnetwork_scope,\n            scoped_summary=summary,\n            trainable_vars=subnetwork_var_list):\n          train_op = _to_train_op_spec(\n              subnetwork_builder.build_subnetwork_train_op(\n                  subnetwork=subnetwork,\n                  loss=estimator_spec.loss,\n                  var_list=subnetwork_var_list,\n                  labels=labels,\n                  iteration_step=step_tensor,\n                  summary=summary,\n                  previous_ensemble=previous_ensemble))\n\n      new_vars = _get_current_vars(diffbase=old_vars)\n      # Sort our dictionary by key to remove non-determinism of variable order.\n      new_vars = collections.OrderedDict(sorted(new_vars.items()))\n      # Combine all trainable, global and savable variables into a single list.\n      subnetwork_variables = sum(new_vars.values(), []) + [step]\n\n    return _SubnetworkSpec(\n        name=name,\n        subnetwork=subnetwork,\n        builder=subnetwork_builder,\n        predictions=estimator_spec.predictions,\n        variables=subnetwork_variables,\n        loss=estimator_spec.loss,\n        step=step,\n        train_op=train_op,\n        eval_metrics=subnetwork_metrics,\n        asset_dir=subnetwork_config.model_dir)\n'"
adanet/core/ensemble_builder_test.py,32,"b'""""""Test AdaNet ensemble single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.ensemble_builder import _EnsembleBuilder\nfrom adanet.core.ensemble_builder import _SubnetworkManager\nfrom adanet.core.summary import Summary\nimport adanet.core.testing_utils as tu\nfrom adanet.ensemble import Candidate as EnsembleCandidate\nfrom adanet.ensemble import ComplexityRegularizedEnsembler\nfrom adanet.ensemble import MeanEnsemble\nfrom adanet.ensemble import MeanEnsembler\nfrom adanet.ensemble import MixtureWeightType\nfrom adanet.subnetwork import Builder\nfrom adanet.subnetwork import Subnetwork\nimport tensorflow.compat.v1 as tf_v1\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.training import training as train\nfrom tensorflow.python.training import training_util\n# pylint: enable=g-direct-tensorflow-import\nfrom tensorflow_estimator.python.estimator.head import binary_class_head\nfrom tensorflow_estimator.python.estimator.head import multi_head as multi_head_lib\n\n\nclass _Builder(Builder):\n\n  def __init__(self,\n               subnetwork_train_op_fn,\n               mixture_weights_train_op_fn,\n               use_logits_last_layer,\n               seed=42,\n               multi_head=False):\n    self._subnetwork_train_op_fn = subnetwork_train_op_fn\n    self._mixture_weights_train_op_fn = mixture_weights_train_op_fn\n    self._use_logits_last_layer = use_logits_last_layer\n    self._seed = seed\n    self._multi_head = multi_head\n\n  @property\n  def name(self):\n    return ""test""\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    assert features is not None\n    assert training is not None\n    assert iteration_step is not None\n    assert summary is not None\n\n    # Trainable variables collection should always be empty when\n    # build_subnetwork is called.\n    assert not tf_compat.v1.get_collection(\n        tf_compat.v1.GraphKeys.TRAINABLE_VARIABLES)\n\n    # Subnetworks get iteration steps instead of global steps.\n    step_name = ""subnetwork_test/iteration_step""\n    assert step_name == tf_compat.tensor_name(\n        tf_compat.v1.train.get_global_step())\n    assert step_name == tf_compat.tensor_name(train.get_global_step())\n    assert step_name == tf_compat.tensor_name(training_util.get_global_step())\n    assert step_name == tf_compat.tensor_name(tf_v1.train.get_global_step())\n    assert step_name == tf_compat.tensor_name(\n        tf_compat.v1.train.get_or_create_global_step())\n    assert step_name == tf_compat.tensor_name(train.get_or_create_global_step())\n    assert step_name == tf_compat.tensor_name(\n        training_util.get_or_create_global_step())\n    assert step_name == tf_compat.tensor_name(\n        tf_v1.train.get_or_create_global_step())\n\n    # Subnetworks get scoped summaries.\n    assert ""fake_scalar"" == tf_compat.v1.summary.scalar(""scalar"", 1.)\n    assert ""fake_image"" == tf_compat.v1.summary.image(""image"", 1.)\n    assert ""fake_histogram"" == tf_compat.v1.summary.histogram(""histogram"", 1.)\n    assert ""fake_audio"" == tf_compat.v1.summary.audio(""audio"", 1., 1.)\n    last_layer = tu.dummy_tensor(shape=(2, 3))\n\n    def logits_fn(logits_dim):\n      return tf_compat.v1.layers.dense(\n          last_layer,\n          units=logits_dim,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer(\n              seed=self._seed))\n\n    if self._multi_head:\n      logits = {\n          ""head1"": logits_fn(logits_dimension / 2),\n          ""head2"": logits_fn(logits_dimension / 2)\n      }\n      last_layer = {""head1"": last_layer, ""head2"": last_layer}\n    else:\n      logits = logits_fn(logits_dimension)\n\n    return Subnetwork(\n        last_layer=logits if self._use_logits_last_layer else last_layer,\n        logits=logits,\n        complexity=2,\n        persisted_tensors={})\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    assert iteration_step is not None\n    assert summary is not None\n    return self._subnetwork_train_op_fn(loss, var_list)\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    assert iteration_step is not None\n    assert summary is not None\n    return self._mixture_weights_train_op_fn(loss, var_list)\n\n\nclass _BuilderPrunerAll(_Builder):\n  """"""Removed previous ensemble completely.""""""\n\n  def prune_previous_ensemble(self, previous_ensemble):\n    return []\n\n\nclass _BuilderPrunerLeaveOne(_Builder):\n  """"""Removed previous ensemble completely.""""""\n\n  def prune_previous_ensemble(self, previous_ensemble):\n    if previous_ensemble:\n      return [0]\n    return []\n\n\nclass _FakeSummary(Summary):\n  """"""A fake adanet.Summary.""""""\n\n  def scalar(self, name, tensor, family=None):\n    return ""fake_scalar""\n\n  def image(self, name, tensor, max_outputs=3, family=None):\n    return ""fake_image""\n\n  def histogram(self, name, values, family=None):\n    return ""fake_histogram""\n\n  def audio(self, name, tensor, sample_rate, max_outputs=3, family=None):\n    return ""fake_audio""\n\n  @contextlib.contextmanager\n  def current_scope(self):\n    yield\n\n\nclass EnsembleBuilderTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""no_previous_ensemble"",\n          ""want_logits"": [[.016], [.117]],\n          ""want_loss"": 1.338,\n          ""want_adanet_loss"": 1.338,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""mean_ensembler"",\n          ""want_logits"": [[.621], [.979]],\n          ""want_loss"": 1.3702,\n          ""want_adanet_loss"": 1.3702,\n          ""want_ensemble_trainable_vars"": 0,\n          ""ensembler_class"": MeanEnsembler,\n          ""want_predictions"": {\n              MeanEnsemble.MEAN_LAST_LAYER: [[-0.2807, -0.1377, -0.6763],\n                                             [0.0245, -0.8935, -0.8284]],\n          }\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_prune_all"",\n          ""want_logits"": [[.016], [.117]],\n          ""want_loss"": 1.338,\n          ""want_adanet_loss"": 1.338,\n          ""want_ensemble_trainable_vars"": 1,\n          ""subnetwork_builder_class"": _BuilderPrunerAll\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_prune_leave_one"",\n          ""want_logits"": [[.016], [.117]],\n          ""want_loss"": 1.338,\n          ""want_adanet_loss"": 1.338,\n          ""want_ensemble_trainable_vars"": 1,\n          ""subnetwork_builder_class"": _BuilderPrunerLeaveOne\n      }, {\n          ""testcase_name"": ""default_mixture_weight_initializer_scalar"",\n          ""mixture_weight_initializer"": None,\n          ""mixture_weight_type"": MixtureWeightType.SCALAR,\n          ""use_logits_last_layer"": True,\n          ""want_logits"": [[.580], [.914]],\n          ""want_loss"": 1.362,\n          ""want_adanet_loss"": 1.362,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""default_mixture_weight_initializer_vector"",\n          ""mixture_weight_initializer"": None,\n          ""mixture_weight_type"": MixtureWeightType.VECTOR,\n          ""use_logits_last_layer"": True,\n          ""want_logits"": [[.580], [.914]],\n          ""want_loss"": 1.362,\n          ""want_adanet_loss"": 1.362,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""default_mixture_weight_initializer_matrix"",\n          ""mixture_weight_initializer"": None,\n          ""mixture_weight_type"": MixtureWeightType.MATRIX,\n          ""want_logits"": [[.016], [.117]],\n          ""want_loss"": 1.338,\n          ""want_adanet_loss"": 1.338,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"":\n              ""default_mixture_weight_initializer_matrix_on_logits"",\n          ""mixture_weight_initializer"":\n              None,\n          ""mixture_weight_type"":\n              MixtureWeightType.MATRIX,\n          ""use_logits_last_layer"":\n              True,\n          ""want_logits"": [[.030], [.047]],\n          ""want_loss"":\n              1.378,\n          ""want_adanet_loss"":\n              1.378,\n          ""want_ensemble_trainable_vars"":\n              1,\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_use_bias"",\n          ""use_bias"": True,\n          ""want_logits"": [[0.013], [0.113]],\n          ""want_loss"": 1.338,\n          ""want_adanet_loss"": 1.338,\n          ""want_ensemble_trainable_vars"": 2,\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_predict_mode"",\n          ""mode"": tf.estimator.ModeKeys.PREDICT,\n          ""want_logits"": [[0.], [0.]],\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_lambda"",\n          ""adanet_lambda"": .01,\n          ""want_logits"": [[.014], [.110]],\n          ""want_loss"": 1.340,\n          ""want_adanet_loss"": 1.343,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_beta"",\n          ""adanet_beta"": .1,\n          ""want_logits"": [[.006], [.082]],\n          ""want_loss"": 1.349,\n          ""want_adanet_loss"": 1.360,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""no_previous_ensemble_lambda_and_beta"",\n          ""adanet_lambda"": .01,\n          ""adanet_beta"": .1,\n          ""want_logits"": [[.004], [.076]],\n          ""want_loss"": 1.351,\n          ""want_adanet_loss"": 1.364,\n          ""want_ensemble_trainable_vars"": 1,\n      }, {\n          ""testcase_name"": ""multi_head"",\n          ""want_logits"": {\n              ""head1"": [[.016], [.117]],\n              ""head2"": [[.016], [.117]],\n          },\n          ""want_loss"": 2.675,\n          ""want_adanet_loss"": 2.675,\n          ""multi_head"": True,\n          ""want_ensemble_trainable_vars"": 2,\n          ""want_subnetwork_trainable_vars"": 4,\n      }, {\n          ""testcase_name"": ""expect_subnetwork_exports"",\n          ""mode"": tf.estimator.ModeKeys.PREDICT,\n          ""want_logits"": [[0.], [0.]],\n          ""want_ensemble_trainable_vars"": 1,\n          ""export_subnetworks"": True,\n      }, {\n          ""testcase_name"": ""multi_head_expect_subnetwork_exports"",\n          ""mode"": tf.estimator.ModeKeys.PREDICT,\n          ""multi_head"": True,\n          ""want_logits"": {\n              ""head1"": [[0.], [0.]],\n              ""head2"": [[0.], [0.]],\n          },\n          ""want_ensemble_trainable_vars"": 2,\n          ""want_subnetwork_trainable_vars"": 4,\n          ""export_subnetworks"": True,\n      }, {\n          ""testcase_name"": ""replay_no_prev"",\n          ""adanet_beta"": .1,\n          ""want_logits"": [[.006], [.082]],\n          ""want_loss"": 1.349,\n          ""want_adanet_loss"": 1.360,\n          ""want_ensemble_trainable_vars"": 1,\n          ""my_ensemble_index"": 2,\n          ""want_replay_indices"": [2],\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_ensemble_spec(\n      self,\n      want_logits,\n      want_loss=None,\n      want_adanet_loss=None,\n      want_ensemble_trainable_vars=None,\n      adanet_lambda=0.,\n      adanet_beta=0.,\n      ensemble_spec_fn=lambda: None,\n      use_bias=False,\n      use_logits_last_layer=False,\n      mixture_weight_type=MixtureWeightType.MATRIX,\n      mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n      warm_start_mixture_weights=True,\n      subnetwork_builder_class=_Builder,\n      mode=tf.estimator.ModeKeys.TRAIN,\n      multi_head=False,\n      want_subnetwork_trainable_vars=2,\n      ensembler_class=ComplexityRegularizedEnsembler,\n      my_ensemble_index=None,\n      want_replay_indices=None,\n      want_predictions=None,\n      export_subnetworks=False,\n      previous_ensemble_spec=None,\n      previous_iteration_checkpoint=None):\n    seed = 64\n\n    if multi_head:\n      head = multi_head_lib.MultiHead(heads=[\n          binary_class_head.BinaryClassHead(\n              name=""head1"", loss_reduction=tf_compat.SUM),\n          binary_class_head.BinaryClassHead(\n              name=""head2"", loss_reduction=tf_compat.SUM)\n      ])\n    else:\n      head = binary_class_head.BinaryClassHead(loss_reduction=tf_compat.SUM)\n    builder = _EnsembleBuilder(\n        head=head,\n        export_subnetwork_logits=export_subnetworks,\n        export_subnetwork_last_layer=export_subnetworks)\n\n    def _subnetwork_train_op_fn(loss, var_list):\n      self.assertLen(var_list, want_subnetwork_trainable_vars)\n      self.assertEqual(\n          var_list,\n          tf_compat.v1.get_collection(\n              tf_compat.v1.GraphKeys.TRAINABLE_VARIABLES))\n      # Subnetworks get iteration steps instead of global steps.\n      self.assertEqual(""subnetwork_test/iteration_step"",\n                       tf_compat.v1.train.get_global_step().op.name)\n\n      # Subnetworks get scoped summaries.\n      self.assertEqual(""fake_scalar"", tf_compat.v1.summary.scalar(""scalar"", 1.))\n      self.assertEqual(""fake_image"", tf_compat.v1.summary.image(""image"", 1.))\n      self.assertEqual(""fake_histogram"",\n                       tf_compat.v1.summary.histogram(""histogram"", 1.))\n      self.assertEqual(""fake_audio"",\n                       tf_compat.v1.summary.audio(""audio"", 1., 1.))\n      optimizer = tf_compat.v1.train.GradientDescentOptimizer(learning_rate=.1)\n      return optimizer.minimize(loss, var_list=var_list)\n\n    def _mixture_weights_train_op_fn(loss, var_list):\n      self.assertLen(var_list, want_ensemble_trainable_vars)\n      self.assertEqual(\n          var_list,\n          tf_compat.v1.get_collection(\n              tf_compat.v1.GraphKeys.TRAINABLE_VARIABLES))\n      # Subnetworks get iteration steps instead of global steps.\n      self.assertEqual(""ensemble_test/iteration_step"",\n                       tf_compat.v1.train.get_global_step().op.name)\n\n      # Subnetworks get scoped summaries.\n      self.assertEqual(""fake_scalar"", tf_compat.v1.summary.scalar(""scalar"", 1.))\n      self.assertEqual(""fake_image"", tf_compat.v1.summary.image(""image"", 1.))\n      self.assertEqual(""fake_histogram"",\n                       tf_compat.v1.summary.histogram(""histogram"", 1.))\n      self.assertEqual(""fake_audio"",\n                       tf_compat.v1.summary.audio(""audio"", 1., 1.))\n      if not var_list:\n        return tf.no_op()\n      optimizer = tf_compat.v1.train.GradientDescentOptimizer(learning_rate=.1)\n      return optimizer.minimize(loss, var_list=var_list)\n\n    previous_ensemble = None\n    previous_ensemble_spec = ensemble_spec_fn()\n    if previous_ensemble_spec:\n      previous_ensemble = previous_ensemble_spec.ensemble\n\n    subnetwork_manager = _SubnetworkManager(head)\n    subnetwork_builder = subnetwork_builder_class(\n        _subnetwork_train_op_fn,\n        _mixture_weights_train_op_fn,\n        use_logits_last_layer,\n        seed,\n        multi_head=multi_head)\n\n    with tf.Graph().as_default() as g:\n      tf_compat.v1.train.get_or_create_global_step()\n      # A trainable variable to later verify that creating models does not\n      # affect the global variables collection.\n      _ = tf_compat.v1.get_variable(""some_var"", shape=0, trainable=True)\n\n      features = {""x"": tf.constant([[1.], [2.]])}\n      if multi_head:\n        labels = {""head1"": tf.constant([0, 1]), ""head2"": tf.constant([0, 1])}\n      else:\n        labels = tf.constant([0, 1])\n\n      session_config = tf.compat.v1.ConfigProto(\n          gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n\n      subnetwork_spec = subnetwork_manager.build_subnetwork_spec(\n          name=""test"",\n          subnetwork_builder=subnetwork_builder,\n          summary=_FakeSummary(),\n          features=features,\n          mode=mode,\n          labels=labels,\n          previous_ensemble=previous_ensemble)\n      ensembler_kwargs = {}\n      if ensembler_class is ComplexityRegularizedEnsembler:\n        ensembler_kwargs.update({\n            ""mixture_weight_type"": mixture_weight_type,\n            ""mixture_weight_initializer"": mixture_weight_initializer,\n            ""warm_start_mixture_weights"": warm_start_mixture_weights,\n            ""model_dir"": self.test_subdirectory,\n            ""adanet_lambda"": adanet_lambda,\n            ""adanet_beta"": adanet_beta,\n            ""use_bias"": use_bias\n        })\n      if ensembler_class is MeanEnsembler:\n        ensembler_kwargs.update({""add_mean_last_layer_predictions"": True})\n      ensemble_spec = builder.build_ensemble_spec(\n          # Note: when ensemble_spec is not None and warm_start_mixture_weights\n          # is True, we need to make sure that the bias and mixture weights are\n          # already saved to the checkpoint_dir.\n          name=""test"",\n          previous_ensemble_spec=previous_ensemble_spec,\n          candidate=EnsembleCandidate(""foo"", [subnetwork_builder], None),\n          ensembler=ensembler_class(**ensembler_kwargs),\n          subnetwork_specs=[subnetwork_spec],\n          summary=_FakeSummary(),\n          features=features,\n          iteration_number=1,\n          labels=labels,\n          my_ensemble_index=my_ensemble_index,\n          mode=mode,\n          previous_iteration_checkpoint=previous_iteration_checkpoint)\n\n      if want_replay_indices:\n        self.assertAllEqual(want_replay_indices,\n                            ensemble_spec.architecture.replay_indices)\n\n      with tf_compat.v1.Session(\n          graph=g, config=session_config).as_default() as sess:\n        sess.run(tf_compat.v1.global_variables_initializer())\n\n        # Equals the number of subnetwork and ensemble trainable variables,\n        # plus the one \'some_var\' created earlier.\n        self.assertLen(\n            tf_compat.v1.trainable_variables(),\n            want_subnetwork_trainable_vars + want_ensemble_trainable_vars + 1)\n\n        # Get the real global step outside a subnetwork\'s context.\n        self.assertEqual(""global_step"",\n                         tf_compat.v1.train.get_global_step().op.name)\n        self.assertEqual(""global_step"", train.get_global_step().op.name)\n        self.assertEqual(""global_step"", tf_v1.train.get_global_step().op.name)\n        self.assertEqual(""global_step"", training_util.get_global_step().op.name)\n        self.assertEqual(""global_step"",\n                         tf_compat.v1.train.get_or_create_global_step().op.name)\n        self.assertEqual(""global_step"",\n                         train.get_or_create_global_step().op.name)\n        self.assertEqual(""global_step"",\n                         tf_v1.train.get_or_create_global_step().op.name)\n        self.assertEqual(""global_step"",\n                         training_util.get_or_create_global_step().op.name)\n\n        # Get global tf.summary outside a subnetwork\'s context.\n        self.assertNotEqual(""fake_scalar"",\n                            tf_compat.v1.summary.scalar(""scalar"", 1.))\n        self.assertNotEqual(""fake_image"",\n                            tf_compat.v1.summary.image(""image"", 1.))\n        self.assertNotEqual(""fake_histogram"",\n                            tf_compat.v1.summary.histogram(""histogram"", 1.))\n        self.assertNotEqual(""fake_audio"",\n                            tf_compat.v1.summary.audio(""audio"", 1., 1.))\n\n        if mode == tf.estimator.ModeKeys.PREDICT:\n          self.assertAllClose(\n              want_logits, sess.run(ensemble_spec.ensemble.logits), atol=1e-3)\n          self.assertIsNone(ensemble_spec.loss)\n          self.assertIsNone(ensemble_spec.adanet_loss)\n          self.assertIsNone(ensemble_spec.train_op)\n          self.assertIsNotNone(ensemble_spec.export_outputs)\n          if not export_subnetworks:\n            return\n          if not multi_head:\n            subnetwork_logits = sess.run(ensemble_spec.export_outputs[\n                _EnsembleBuilder._SUBNETWORK_LOGITS_EXPORT_SIGNATURE].outputs)\n            self.assertAllClose(subnetwork_logits[""test""],\n                                sess.run(subnetwork_spec.subnetwork.logits))\n            subnetwork_last_layer = sess.run(ensemble_spec.export_outputs[\n                _EnsembleBuilder._SUBNETWORK_LAST_LAYER_EXPORT_SIGNATURE]\n                                             .outputs)\n            self.assertAllClose(subnetwork_last_layer[""test""],\n                                sess.run(subnetwork_spec.subnetwork.last_layer))\n          else:\n            self.assertIn(""subnetwork_logits_head2"",\n                          ensemble_spec.export_outputs)\n            subnetwork_logits_head1 = sess.run(\n                ensemble_spec.export_outputs[""subnetwork_logits_head1""].outputs)\n            self.assertAllClose(\n                subnetwork_logits_head1[""test""],\n                sess.run(subnetwork_spec.subnetwork.logits[""head1""]))\n            self.assertIn(""subnetwork_logits_head2"",\n                          ensemble_spec.export_outputs)\n            subnetwork_last_layer_head1 = sess.run(\n                ensemble_spec.export_outputs[""subnetwork_last_layer_head1""]\n                .outputs)\n            self.assertAllClose(\n                subnetwork_last_layer_head1[""test""],\n                sess.run(subnetwork_spec.subnetwork.last_layer[""head1""]))\n          return\n\n        # Verify that train_op works, previous loss should be greater than loss\n        # after a train op.\n        loss = sess.run(ensemble_spec.loss)\n        train_op = tf.group(subnetwork_spec.train_op.train_op,\n                            ensemble_spec.train_op.train_op)\n        for _ in range(3):\n          sess.run(train_op)\n        self.assertGreater(loss, sess.run(ensemble_spec.loss))\n\n        self.assertAllClose(\n            want_logits, sess.run(ensemble_spec.ensemble.logits), atol=1e-3)\n\n        if ensembler_class is ComplexityRegularizedEnsembler:\n          # Bias should learn a non-zero value when used.\n          bias = sess.run(ensemble_spec.ensemble.bias)\n          if isinstance(bias, dict):\n            bias = sum(abs(b) for b in bias.values())\n          if use_bias:\n            self.assertNotEqual(0., bias)\n          else:\n            self.assertAlmostEqual(0., bias)\n\n        self.assertAlmostEqual(\n            want_loss, sess.run(ensemble_spec.loss), places=3)\n        self.assertAlmostEqual(\n            want_adanet_loss, sess.run(ensemble_spec.adanet_loss), places=3)\n\n        if want_predictions:\n          self.assertAllClose(\n              want_predictions,\n              sess.run(ensemble_spec.ensemble.predictions),\n              atol=1e-3)\n\n\nclass EnsembleBuilderMetricFnTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _make_metrics(self,\n                    metric_fn,\n                    mode=tf.estimator.ModeKeys.EVAL,\n                    multi_head=False,\n                    sess=None):\n\n    with context.graph_mode():\n      if multi_head:\n        head = multi_head_lib.MultiHead(heads=[\n            binary_class_head.BinaryClassHead(\n                name=""head1"", loss_reduction=tf_compat.SUM),\n            binary_class_head.BinaryClassHead(\n                name=""head2"", loss_reduction=tf_compat.SUM)\n        ])\n        labels = {""head1"": tf.constant([0, 1]), ""head2"": tf.constant([0, 1])}\n      else:\n        head = binary_class_head.BinaryClassHead(loss_reduction=tf_compat.SUM)\n        labels = tf.constant([0, 1])\n      features = {""x"": tf.constant([[1.], [2.]])}\n      builder = _EnsembleBuilder(head, metric_fn=metric_fn)\n      subnetwork_manager = _SubnetworkManager(head, metric_fn=metric_fn)\n      subnetwork_builder = _Builder(\n          lambda unused0, unused1: tf.no_op(),\n          lambda unused0, unused1: tf.no_op(),\n          use_logits_last_layer=True)\n\n      subnetwork_spec = subnetwork_manager.build_subnetwork_spec(\n          name=""test"",\n          subnetwork_builder=subnetwork_builder,\n          summary=_FakeSummary(),\n          features=features,\n          mode=mode,\n          labels=labels)\n      ensemble_spec = builder.build_ensemble_spec(\n          name=""test"",\n          candidate=EnsembleCandidate(""foo"", [subnetwork_builder], None),\n          ensembler=ComplexityRegularizedEnsembler(\n              mixture_weight_type=MixtureWeightType.SCALAR),\n          subnetwork_specs=[subnetwork_spec],\n          summary=_FakeSummary(),\n          features=features,\n          iteration_number=0,\n          labels=labels,\n          mode=mode,\n          my_ensemble_index=0,\n          previous_ensemble_spec=None,\n          previous_iteration_checkpoint=None)\n      subnetwork_metric_ops = subnetwork_spec.eval_metrics.eval_metrics_ops()\n      ensemble_metric_ops = ensemble_spec.eval_metrics.eval_metrics_ops()\n      evaluate = self.evaluate\n      if sess is not None:\n        evaluate = sess.run\n      evaluate((tf_compat.v1.global_variables_initializer(),\n                tf_compat.v1.local_variables_initializer()))\n      evaluate((subnetwork_metric_ops, ensemble_metric_ops))\n      # Return the idempotent tensor part of the (tensor, op) metrics tuple.\n      return {\n          k: evaluate(subnetwork_metric_ops[k][0])\n          for k in subnetwork_metric_ops\n      }, {k: evaluate(ensemble_metric_ops[k][0]) for k in ensemble_metric_ops}\n\n  def setUp(self):\n    super(EnsembleBuilderMetricFnTest, self).setUp()\n    tf_compat.v1.train.create_global_step()\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""mode_train"",\n          ""mode"": tf.estimator.ModeKeys.TRAIN,\n      }, {\n          ""testcase_name"": ""mode_predict"",\n          ""mode"": tf.estimator.ModeKeys.PREDICT,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_only_adds_metrics_when_evaluating(self, mode):\n    """"""Ensures that metrics are only added during evaluation.\n\n    Adding metrics during training will break when running on TPU.\n\n    Args:\n      mode: The mode with which to run the test.\n    """"""\n\n    def metric_fn(features):\n      return {""mean_x"": tf_compat.v1.metrics.mean(features[""x""])}\n\n    subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn, mode)\n\n    self.assertEmpty(subnetwork_metrics)\n    self.assertEmpty(ensemble_metrics)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_should_add_metrics(self):\n\n    def _test_metric_fn(metric_fn):\n      subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn)\n      self.assertIn(""mean_x"", subnetwork_metrics)\n      self.assertIn(""mean_x"", ensemble_metrics)\n      self.assertEqual(1.5, subnetwork_metrics[""mean_x""])\n      self.assertEqual(1.5, ensemble_metrics[""mean_x""])\n      # assert that it keeps original head metrics\n      self.assertIn(""average_loss"", subnetwork_metrics)\n      self.assertIn(""average_loss"", ensemble_metrics)\n\n    def metric_fn_1(features):\n      return {""mean_x"": tf_compat.v1.metrics.mean(features[""x""])}\n\n    # TODO: Add support for tf.keras.metrics.Mean like `add_metrics`.\n    _test_metric_fn(metric_fn_1)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_should_error_out_for_not_recognized_args(self):\n    head = binary_class_head.BinaryClassHead(loss_reduction=tf_compat.SUM)\n\n    def metric_fn(features, not_recognized):\n      _, _ = features, not_recognized\n      return {}\n\n    with self.assertRaisesRegexp(ValueError, ""not_recognized""):\n      _EnsembleBuilder(head, metric_fn=metric_fn)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_all_supported_args(self):\n\n    def metric_fn(features, predictions, labels):\n      self.assertIn(""x"", features)\n      self.assertIsNotNone(labels)\n      self.assertIn(""logistic"", predictions)\n      return {}\n\n    self._make_metrics(metric_fn)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_all_supported_args_in_different_order(self):\n\n    def metric_fn(labels, features, predictions):\n      self.assertIn(""x"", features)\n      self.assertIsNotNone(labels)\n      self.assertIn(""logistic"", predictions)\n      return {}\n\n    self._make_metrics(metric_fn)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_all_args_are_optional(self):\n\n    def _test_metric_fn(metric_fn):\n      subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn)\n      self.assertEqual(2., subnetwork_metrics[""two""])\n      self.assertEqual(2., ensemble_metrics[""two""])\n\n    def metric_fn_1():\n      return {""two"": tf_compat.v1.metrics.mean(tf.constant([2.]))}\n\n    # TODO: Add support for tf.keras.metrics.Mean like `add_metrics`.\n    _test_metric_fn(metric_fn_1)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_overrides_existing_metrics(self):\n\n    def _test_metric_fn(metric_fn):\n      subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn=None)\n      self.assertNotEqual(2., subnetwork_metrics[""average_loss""])\n      self.assertNotEqual(2., ensemble_metrics[""average_loss""])\n\n      with tf.Graph().as_default() as g, self.test_session(g) as sess:\n        subnetwork_metrics, ensemble_metrics = self._make_metrics(\n            metric_fn=metric_fn, sess=sess)\n      self.assertEqual(2., subnetwork_metrics[""average_loss""])\n      self.assertEqual(2., ensemble_metrics[""average_loss""])\n\n    def metric_fn_1():\n      return {""average_loss"": tf_compat.v1.metrics.mean(tf.constant([2.]))}\n\n    # TODO: Add support for tf.keras.metrics.Mean like `add_metrics`.\n    _test_metric_fn(metric_fn_1)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_multi_head(self):\n    """"""Tests b/123084079.""""""\n\n    def metric_fn(predictions):\n      self.assertIn((""head1"", ""logits""), predictions)\n      self.assertIn((""head2"", ""logits""), predictions)\n      return {}\n\n    self._make_metrics(metric_fn, multi_head=True)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_operation_metrics(self):\n\n    def metric_fn():\n      var = tf_compat.v1.get_variable(\n          ""metric_var"",\n          shape=[],\n          trainable=False,\n          initializer=tf_compat.v1.zeros_initializer(),\n          collections=[tf_compat.v1.GraphKeys.LOCAL_VARIABLES])\n      # A metric with an op that doesn\'t return a Tensor.\n      op = tf.group(tf_compat.v1.assign_add(var, 1))\n      return {""operation_metric"": (var, op)}\n\n    subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn)\n    self.assertEqual(1., subnetwork_metrics[""operation_metric""])\n    self.assertEqual(1., ensemble_metrics[""operation_metric""])\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_eval_metric_different_shape_op(self):\n\n    def metric_fn():\n      var = tf_compat.v1.get_variable(\n          ""metric_var"",\n          shape=[2],\n          trainable=False,\n          initializer=tf_compat.v1.zeros_initializer(),\n          collections=[tf_compat.v1.GraphKeys.LOCAL_VARIABLES])\n      # Shape of metric different from shape of op\n      op = tf_compat.v1.assign_add(var, [1, 2])\n      metric = tf.reshape(var[0] + var[1], [])\n      return {""different_shape_metric"": (metric, op)}\n\n    subnetwork_metrics, ensemble_metrics = self._make_metrics(metric_fn)\n    self.assertEqual(3., subnetwork_metrics[""different_shape_metric""])\n    self.assertEqual(3., ensemble_metrics[""different_shape_metric""])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/estimator.py,89,"b'""""""An AdaNet estimator implementation in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport errno\nimport inspect\nimport os\nimport time\n\nfrom absl import logging\nfrom adanet import distributed as distributed_lib\nfrom adanet import ensemble as ensemble_lib\nfrom adanet import tf_compat\nfrom adanet.core.architecture import _Architecture\nfrom adanet.core.candidate import _CandidateBuilder\nfrom adanet.core.ensemble_builder import _EnsembleBuilder\nfrom adanet.core.ensemble_builder import _SubnetworkManager\nfrom adanet.core.iteration import _Iteration\nfrom adanet.core.iteration import _IterationBuilder\nfrom adanet.core.report_accessor import _ReportAccessor\nfrom adanet.core.summary import _ScopedSummary\nfrom adanet.core.summary import _ScopedSummaryV2\nfrom adanet.core.summary import _TPUScopedSummary\nfrom adanet.core.timer import _CountDownTimer\nfrom adanet.distributed.devices import monkey_patch_default_variable_placement_strategy\nimport numpy as np\nimport six\nimport tensorflow.compat.v2 as tf\nfrom typing import Any, Callable, Dict, Optional, Sequence, Text  # (b/144172555) pylint:disable=unused-import\n\n\nclass _StopAfterTrainingHook(tf_compat.SessionRunHook):\n  """"""Hook that requests stop once iteration is over.""""""\n\n  def __init__(self, iteration, after_fn):\n    # type: (_Iteration, Callable[[], None]) -> None\n    """"""Initializes a `_StopAfterTrainingHook`.\n\n    Args:\n      iteration: An `_Iteration` instance.\n      after_fn: A function to call after training stopped.\n\n    Returns:\n      A `_StopAfterTrainingHook` instance.\n    """"""\n\n    self._iteration = iteration\n    self._after_fn = after_fn\n\n  def before_run(self, run_context):\n    """"""See `SessionRunHook`.""""""\n\n    self._stop_if_is_over(run_context)\n\n  def after_run(self, run_context, run_values):\n    """"""See `SessionRunHook`.""""""\n\n    self._stop_if_is_over(run_context)\n\n  def _stop_if_is_over(self, run_context):\n    """"""Signals the monitored session to step when the iteration is over.""""""\n\n    if not self._iteration.train_manager.is_over():\n      return\n    logging.info(""Now stopping iteration %d training"", self._iteration.number)\n    run_context.request_stop()\n    self._after_fn()\n\n\nclass _SummaryV2SaverHook(tf_compat.SessionRunHook):\n  """"""A hook that writes summaries to the appropriate log directory on disk.""""""\n\n  def __init__(self, summaries, save_steps=None, save_secs=None):\n    """"""Initializes a `SummaryV2SaverHook` for writing TF 2 summaries.\n\n    Args:\n      summaries: List of `_ScopedSummaryV2` instances.\n      save_steps: `int`, save summaries every N steps. Exactly one of\n        `save_secs` and `save_steps` should be set.\n      save_secs: `int`, save summaries every N seconds.\n    """"""\n\n    self._summaries = summaries\n    self._summary_ops = []\n    self._writer_init_ops = []\n    self._timer = tf_compat.v1.train.SecondOrStepTimer(\n        every_secs=save_secs, every_steps=save_steps)\n\n  def begin(self):\n    self._next_step = None\n    self._global_step_tensor = tf_compat.v1.train.get_global_step()\n\n    for summary in self._summaries:\n      assert isinstance(summary, _ScopedSummaryV2)\n      writer = tf_compat.v2.summary.create_file_writer(summary.logdir)\n      with writer.as_default():\n        for summary_fn, tensor in summary.summary_tuples():\n          self._summary_ops.append(\n              summary_fn(tensor, step=tf.compat.v1.train.get_global_step()))\n      self._writer_init_ops.append(writer.init())\n\n  def after_create_session(self, session, coord):\n    session.run(self._writer_init_ops)\n\n  def before_run(self, run_context):\n    requests = {""global_step"": self._global_step_tensor}\n    self._request_summary = (\n        self._next_step is None or\n        self._timer.should_trigger_for_step(self._next_step))\n    if self._request_summary:\n      requests[""summary""] = self._summary_ops\n\n    return tf_compat.SessionRunArgs(requests)\n\n  def after_run(self, run_context, run_values):\n    stale_global_step = run_values.results[""global_step""]\n    global_step = stale_global_step + 1\n    if self._next_step is None or self._request_summary:\n      global_step = run_context.session.run(self._global_step_tensor)\n\n    if self._request_summary:\n      self._timer.update_last_triggered_step(global_step)\n\n    self._next_step = global_step + 1\n\n  def end(self, session):\n    # TODO: Run writer.flush() at Session end.\n    # Currently disabled because the flush op crashes between iterations.\n    return\n\n\nclass _EvalMetricSaverHook(tf_compat.SessionRunHook):\n  """"""A hook for writing candidate evaluation metrics as summaries to disk.""""""\n\n  def __init__(self, name, kind, eval_metrics, output_dir):\n    # type: (Text, Text, Any, Text) -> None\n    """"""Initializes a `_EvalMetricSaverHook` instance.\n\n    Args:\n      name: String name of candidate owner of these metrics.\n      kind: The kind of candidate that the metrics belong to (e.g. subnetwork).\n      eval_metrics: Tuple of (metric_fn, tensors) which returns a dict of metric\n        results keyed by name. The values of the dict are the results of calling\n        a metric function, namely a `(metric_tensor, update_op)` tuple.\n        `metric_tensor` should be evaluated without any impact on state\n        (typically is a pure computation based on variables.). For example, it\n        should not trigger the `update_op` or require any input fetching.\n      output_dir: Directory for writing evaluation summaries.\n\n    Returns:\n      An `_EvalMetricSaverHook` instance.\n    """"""\n\n    self._name = name\n    self._kind = kind\n    self._eval_metrics = eval_metrics\n    self._output_dir = output_dir\n\n  def begin(self):\n    """"""See `SessionRunHook`.""""""\n\n    # The metric_fn is called with tf.placeholders to simply read the value of\n    # the metric variables. The metrics themselves are computed as a result of\n    # being returned in the EstimatorSpec by _adanet_model_fn.\n    metric_fn, tensors = self._eval_metrics.eval_metrics_tuple()\n    tensors = [tf_compat.v1.placeholder(t.dtype, t.shape) for t in tensors]\n    eval_metric_ops = metric_fn(*tensors)\n    self._eval_metric_tensors = {}\n    for key in sorted(eval_metric_ops):\n      value = tf_compat.metric_op(eval_metric_ops[key])\n      self._eval_metric_tensors[key] = value[0]\n\n  def _dict_to_str(self, dictionary):\n    """"""Get a `str` representation of a `dict`.\n\n    Args:\n      dictionary: The `dict` to be represented as `str`.\n\n    Returns:\n      A `str` representing the `dictionary`.\n    """"""\n    return "", "".join(\n        ""{} = {}"".format(k, v) for k, v in sorted(dictionary.items()))\n\n  def end(self, session):\n    """"""See `SessionRunHook`.""""""\n\n    # Forked from tensorflow/python/estimator/estimator.py function called\n    # _write_dict_to_summary.\n    current_global_step = tf_compat.v1.train.get_global_step()\n    eval_dict, current_global_step = session.run(\n        (self._eval_metric_tensors, current_global_step))\n\n    logging.info(""Saving %s \'%s\' dict for global step %d: %s"", self._kind,\n                 self._name, current_global_step, self._dict_to_str(eval_dict))\n    summary_writer = tf_compat.v1.summary.FileWriterCache.get(self._output_dir)\n    summary_proto = tf_compat.v1.summary.Summary()\n    for key in eval_dict:\n      value = eval_dict[key]\n      if isinstance(value, (np.float32, float)):\n        summary_proto.value.add(tag=key, simple_value=float(value))\n      elif isinstance(value, six.binary_type):\n        summ = tf_compat.v1.summary.Summary.FromString(value)\n        for i, _ in enumerate(summ.value):\n          summ.value[i].tag = ""{}/{}"".format(key, i)\n        summary_proto.value.extend(summ.value)\n      else:\n        logging.warn(\n            ""Skipping summary for %s, must be a float, np.float32, ""\n            ""or a serialized string of Summary."", key)\n    summary_writer.add_summary(summary_proto, current_global_step)\n    summary_writer.flush()\n    # Note(b/137672676): Do not explicitly call summary_writer.close() here.\n    # This will cause eval summaries to not be written out after the first time\n    # in continuous evals.\n\n\nclass _OverwriteCheckpointHook(tf_compat.SessionRunHook):\n  """"""Hook to overwrite the latest checkpoint with next iteration variables.""""""\n\n  def __init__(self, current_iteration, iteration_number_tensor,\n               previous_iteration_vars, config, enable_v2_checkpoint):\n    """"""Initializes an _OverwriteCheckpointHook instance.\n\n    Args:\n      current_iteration: Current `_Iteration` object.\n      iteration_number_tensor: Int variable `Tensor` storing the current\n        iteration number.\n      previous_iteration_vars: Variables to restore from the previous iteration\n        before overwriting the checkpoint.\n      config: The Estimator\'s RunConfig object.\n      enable_v2_checkpoint: Whether `tf.train.Checkpoint` is used for\n        checkpointing.\n    """"""\n\n    self._current_iteration = current_iteration\n    self._iteration_number = current_iteration.number\n    self._iteration_number_tensor = iteration_number_tensor\n    self._previous_iteration_vars = previous_iteration_vars\n    self._model_dir = config.model_dir\n    self._checkpoint_state = tf.train.get_checkpoint_state(self._model_dir)\n    self._keep_checkpoint_max = config.keep_checkpoint_max\n    self._enable_v2_checkpoint = enable_v2_checkpoint\n\n    self._update_op = None\n    self._overwrite_saver = None\n    self._checkpoint_overwritten = False\n\n  def begin(self):\n    """"""Creates the savers and adds ops needed for overwriting the checkpoint.\n\n    Two savers are created, a restore saver which is passed the variables from\n    the previous iteration to restore, and an overwrite saver which will\n    actually overwrite the checkpoint.\n    """"""\n\n    from tensorflow.python.training.tracking import graph_view  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    if self._enable_v2_checkpoint:\n      prev_checkpoint = self._current_iteration.previous_iteration.checkpoint\n      self._status = prev_checkpoint.restore(\n          self._checkpoint_state.model_checkpoint_path)\n      # Because we prune the previous iteration\'s candidates, only a subset of\n      # the variables present in the checkpoint will be used. Assert they are\n      # restored.\n      self._status.expect_partial().assert_existing_objects_matched()\n\n      self._overwrite_saver = tf_compat.v1.train.Saver(\n          var_list=graph_view.ObjectGraphView(\n              self._current_iteration.checkpoint).frozen_saveable_objects(),\n          sharded=True,\n          max_to_keep=self._keep_checkpoint_max)\n    else:\n      self._restore_saver = tf_compat.v1.train.Saver(\n          sharded=True, var_list=self._previous_iteration_vars)\n      # Note: self._iteration_number already contains the value of the next\n      # iteration since _OverwriteCheckpointHook should only execute during the\n      # graph growing phase.\n      self._update_op = self._iteration_number_tensor.assign(\n          self._iteration_number)\n      self._overwrite_saver = tf_compat.v1.train.Saver(\n          sharded=True, max_to_keep=self._keep_checkpoint_max)\n    self._overwrite_saver.recover_last_checkpoints(\n        self._checkpoint_state.all_model_checkpoint_paths)\n\n  def before_run(self, run_context):\n    """"""Overwrites checkpoint before any calls to session.run().\n\n    This is to ensure that the values of the variables in the overwritten\n    checkpoint match those in the pevious iteration checkpoint.\n\n    Args:\n      run_context: The tf.train.SessionRunContext passed to the hook.\n    """"""\n\n    if not self._checkpoint_overwritten:\n      session = run_context.session\n      if self._enable_v2_checkpoint:\n        self._status.initialize_or_restore(session)\n      else:\n        self._restore_saver.restore(\n            session, self._checkpoint_state.model_checkpoint_path)\n        session.run(self._update_op)\n      checkpoint_path = os.path.join(self._model_dir, ""increment.ckpt"")\n      logging.info(\n          ""Overwriting checkpoint with new graph for iteration %d to %s-%d"",\n          self._current_iteration.number, checkpoint_path,\n          self._current_iteration.number)\n      # Specify global_step=self._iteration_number to append the iteration\n      # number to the checkpoint name, e.g. <model_dir>/increment.ckpt-1.\n      self._overwrite_saver.save(\n          session, checkpoint_path, global_step=self._current_iteration.number)\n      self._checkpoint_overwritten = True\n\n\ndef _copy_recursively(source, destination):\n  """"""Copies a directory and its content.\n\n  Args:\n    source: Source directory.\n    destination: Destination directory.\n  """"""\n\n  for src_dir, _, src_files in tf.io.gfile.walk(source):\n    dst_dir = os.path.join(destination, os.path.relpath(src_dir, source))\n    if not tf.io.gfile.exists(dst_dir):\n      tf.io.gfile.makedirs(dst_dir)\n    for src_file in src_files:\n      tf.io.gfile.copy(\n          os.path.join(src_dir, src_file),\n          os.path.join(dst_dir, src_file),\n          overwrite=True)\n\n\nclass _GraphGrowingHookDecorator(tf_compat.SessionRunHook):\n  """"""Decorates a SessionRunHook to only run begin() and end() methods.""""""\n\n  def __init__(self, hook):\n    # type: (tf_compat.SessionRunHook) -> None\n    """"""Initializes a _GraphGrowingHookDecorator instance.\n\n    Args:\n      hook: The SessionRunHook to decorate.\n    """"""\n    self._hook = hook\n\n  def begin(self):\n    self._hook.begin()\n\n  def end(self, session):\n    self._hook.end(session)\n\n\ndef _delete_directory(directory):\n  # type: (Text) -> None\n  """"""Removes directory and handles any folder or file exceptions.""""""\n  if not tf.io.gfile.exists(directory):\n    return\n  try:\n    tf.io.gfile.rmtree(directory)\n  except (tf.errors.PermissionDeniedError,\n          tf.errors.FailedPreconditionError) as e:\n    logging.info(""Ignoring folder or file issues: %s \'%s\'"", e.error_code,\n                 e.message)\n\n\n@contextlib.contextmanager\ndef _disable_asserts_for_confusion_matrix_at_thresholds():\n  """"""Disables asserts in metrics_impl._confusion_matrix_at_thresholds.\n\n  AdaNet sometimes have a few NaN and non-NaN subnetworks at a given iteration.\n  This doesn\'t crash during training, since AdaNet simply chooses the best\n  subnetwork among the non-NaN candidates. However, during estimator.evaluate(),\n  AdaNet evaluates all subnetworks and ensembles. This triggers an assertion\n  failure in V1 binary classifier _Head, since it expects the predictions to be\n  between 0 and 1, and NaN is not between 0 and 1. This causes AdaNet and\n  to raise an exception during estimator.evaluate(), even though the final model\n  is servable. Hence, we disable these assertions during evaluate(), and allow\n  the NaNs to be written to disk.\n\n  Yields:\n    Nothing. Simply returns control back to the caller.\n  """"""\n\n  from tensorflow.python.ops import metrics_impl  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n  def _no_op_assert(x, y, data=None, summarize=None, message=None, name=None):\n    """"""Dummy assert that never fails.""""""\n\n    del x, y, data, summarize, message, name  # unused\n    return tf.no_op()\n\n  old_confusion_matrix_at_thresholds = (\n      metrics_impl._confusion_matrix_at_thresholds)  # pylint:disable=protected-access\n\n  def _confusion_matrix_at_thresholds_without_asserts(labels,\n                                                      predictions,\n                                                      thresholds,\n                                                      weights=None,\n                                                      includes=None):\n    """"""Calls _confusion_matrix_at_thresholds without asserts; returns output.""""""\n\n    from tensorflow.python.ops import check_ops  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    old_assert_greater_equal = check_ops.assert_greater_equal\n    old_assert_less_equal = check_ops.assert_less_equal\n    setattr(check_ops, ""assert_greater_equal"", _no_op_assert)\n    setattr(check_ops, ""assert_less_equal"", _no_op_assert)\n    conf_matrix = old_confusion_matrix_at_thresholds(labels, predictions,\n                                                     thresholds, weights,\n                                                     includes)\n    setattr(check_ops, ""assert_greater_equal"", old_assert_greater_equal)\n    setattr(check_ops, ""assert_less_equal"", old_assert_less_equal)\n    return conf_matrix\n\n  setattr(metrics_impl, ""_confusion_matrix_at_thresholds"",\n          _confusion_matrix_at_thresholds_without_asserts)\n  try:\n    yield\n  finally:\n    setattr(metrics_impl, ""_confusion_matrix_at_thresholds"",\n            old_confusion_matrix_at_thresholds)\n\n\nclass Estimator(tf.estimator.Estimator):\n  # pyformat: disable\n  r""""""A :class:`tf.estimator.Estimator` for training, evaluation, and serving.\n\n  This implementation uses an :class:`adanet.subnetwork.Generator` as its weak\n  learning algorithm for generating candidate subnetworks. These are trained in\n  parallel using a single graph per iteration. At the end of each iteration, the\n  estimator saves the sub-graph of the best subnetwork ensemble and its weights\n  as a separate checkpoint. At the beginning of the next iteration, the\n  estimator imports the previous iteration\'s frozen graph and adds ops for the\n  next candidates as part of a new graph and session. This allows the estimator\n  have the performance of Tensorflow\'s static graph constraint (minus the\n  performance hit of reconstructing a graph between iterations), while having\n  the flexibility of having a dynamic graph.\n\n  NOTE: Subclassing :class:`tf.estimator.Estimator` is only necessary to work\n  with :meth:`tf.estimator.train_and_evaluate` which asserts that the estimator\n  argument is a :class:`tf.estimator.Estimator` subclass. However, all training\n  is delegated to a separate :class:`tf.estimator.Estimator` instance. It is\n  responsible for supporting both local and distributed training. As such, the\n  :class:`adanet.Estimator` is only responsible for bookkeeping across\n  iterations.\n\n  Args:\n    head: A :class:`tf.contrib.estimator.Head` instance for computing loss and\n      evaluation metrics for every candidate.\n    subnetwork_generator: The :class:`adanet.subnetwork.Generator` which defines\n      the candidate subnetworks to train and evaluate at every AdaNet iteration.\n    max_iteration_steps: Total number of steps for which to train candidates per\n      iteration. If :class:`OutOfRange` or :class:`StopIteration` occurs in the\n      middle, training stops before `max_iteration_steps` steps. When\n      :code:`None`, it will train the current iteration forever.\n    ensemblers: An iterable of :class:`adanet.ensemble.Ensembler` objects that\n      define how to ensemble a group of subnetworks. If there are multiple,\n      each should have a different `name` property.\n    ensemble_strategies: An iterable of :class:`adanet.ensemble.Strategy`\n      objects that define the candidate ensembles of subnetworks to explore at\n      each iteration.\n    evaluator: An :class:`adanet.Evaluator` for candidate selection after all\n      subnetworks are done training. When :code:`None`, candidate selection uses\n      a moving average of their :class:`adanet.Ensemble` AdaNet loss during\n      training instead. In order to use the *AdaNet algorithm* as described in\n      [Cortes et al., \'17], the given :class:`adanet.Evaluator` must be created\n      with the same dataset partition used during training. Otherwise, this\n      framework will perform *AdaNet.HoldOut* which uses a holdout set for\n      candidate selection, but does not benefit from learning guarantees.\n    report_materializer: An :class:`adanet.ReportMaterializer`. Its reports are\n      made available to the `subnetwork_generator` at the next iteration, so\n      that it can adapt its search space. When `None`, the\n      `subnetwork_generator` :meth:`generate_candidates` method will receive\n      empty Lists for their `previous_ensemble_reports` and `all_reports`\n      arguments.\n    metric_fn: A function for adding custom evaluation metrics, which should\n      obey the following signature:\n        - `Args`:\n          Can only have the following three arguments in any order:\n          - :code:`predictions`: Predictions `Tensor` or dict of `Tensor`\n            created by given :code:`head`.\n          - :code:`features`: Input `dict` of `Tensor` objects created by\n            :code:`input_fn` which is given to :meth:`estimator.evaluate` as an\n            argument.\n          - :code:`labels`: Labels `Tensor` or dict of `Tensor` (for multi-head)\n            created by :code:`input_fn` which is given to\n            :meth:`estimator.evaluate` as an argument.\n        - `Returns`: Dict of metric results keyed by name. Final metrics are a\n          union of this and :code:`head`\'s existing metrics. If there is a name\n          conflict between this and :code:`head`s existing metrics, this will\n          override the existing one. The values of the dict are the results of\n          calling a metric function, namely a :code:`(metric_tensor, update_op)`\n          tuple.\n    force_grow: Boolean override that forces the ensemble to grow by one\n      subnetwork at the end of each iteration. Normally at the end of each\n      iteration, AdaNet selects the best candidate ensemble according to its\n      performance on the AdaNet objective. In some cases, the best ensemble is\n      the `previous_ensemble` as opposed to one that includes a newly trained\n      subnetwork. When `True`, the algorithm will not select the\n      `previous_ensemble` as the best candidate, and will ensure that after n\n      iterations the final ensemble is composed of n subnetworks.\n    replicate_ensemble_in_training: Whether to rebuild the frozen subnetworks of\n      the ensemble in training mode, which can change the outputs of the frozen\n      subnetworks in the ensemble. When `False` and during candidate training,\n      the frozen subnetworks in the ensemble are in prediction mode, so\n      training-only ops like dropout are not applied to them. When `True` and\n      training the candidates, the frozen subnetworks will be in training mode\n      as well, so they will apply training-only ops like dropout.  This argument\n      is useful for regularizing learning mixture weights, or for making\n      training-only side inputs available in subsequent iterations. For most\n      use-cases, this should be `False`.\n    adanet_loss_decay: Float decay for the exponential-moving-average of the\n      AdaNet objective throughout training. This moving average is a data-\n      driven way tracking the best candidate with only the training set.\n    delay_secs_per_worker: Float number of seconds to delay starting the\n      i-th worker. Staggering worker start-up during distributed asynchronous\n      SGD can improve training stability and speed up convergence. Each worker\n      will wait (i+1) * delay_secs_per_worker seconds before beginning training.\n    max_worker_delay_secs: Float max number of seconds to delay starting the\n      i-th worker. Staggering worker start-up during distributed asynchronous\n      SGD can improve training stability and speed up convergence. Each worker\n      will wait up to max_worker_delay_secs before beginning training.\n    worker_wait_secs: Float number of seconds for workers to wait before\n      checking if the chief prepared the next iteration.\n    worker_wait_timeout_secs: Float number of seconds for workers to wait for\n      chief to prepare the next iteration during distributed training. This is\n      needed to prevent workers waiting indefinitely for a chief that may have\n      crashed or been turned down. When the timeout is exceeded, the worker\n      exits the train loop. In situations where the chief job is much slower\n      than the worker jobs, this timeout should be increased.\n    model_dir: Directory to save model parameters, graph and etc. This can also\n      be used to load checkpoints from the directory into a estimator to\n      continue training a previously saved model.\n    report_dir: Directory where the\n      :class:`adanet.subnetwork.MaterializedReport`s materialized by\n      :code:`report_materializer` would be saved. If :code:`report_materializer`\n      is :code:`None`, this will not save anything. If :code:`None` or\n      empty string, defaults to :code:`<model_dir>/report`.\n    config: :class:`RunConfig` object to configure the runtime settings.\n    debug: Boolean to enable debug mode which will check features and labels\n      for Infs and NaNs.\n    enable_ensemble_summaries: Whether to record summaries to display in\n      TensorBoard for each ensemble candidate. Disable to reduce memory and disk\n      usage per run.\n    enable_subnetwork_summaries: Whether to record summaries to display in\n      TensorBoard for each subnetwork. Disable to reduce memory and disk usage\n      per run.\n    global_step_combiner_fn: Function for combining each subnetwork\'s\n      iteration step into the global step. By default it is the average of all\n      subnetwork iteration steps, which may affect the global_steps/sec as\n      subnetworks early stop and no longer increase their iteration step.\n    max_iterations: Integer maximum number of AdaNet iterations (a.k.a. rounds)\n      of generating new subnetworks and ensembles, training them, and evaluating\n      them against the current best ensemble. When :code:`None`, AdaNet will\n      keep iterating until `Estimator#train` terminates. Otherwise, if\n      :code:`max_iteratios` is supplied and is met or exceeded during training,\n      training will terminate even before `steps` or `max_steps`.\n    export_subnetwork_logits: Whether to include subnetwork logits in exports.\n    export_subnetwork_last_layer: Whether to include subnetwork last layer in\n      exports.\n    replay_config: Optional :class:`adanet.replay.Config` to specify a previous\n      AdaNet run to replay. Given the exact same search space but potentially\n      different training data, the `replay_config` causes the estimator to\n      reconstruct the previously trained model without performing a search.\n      NOTE: The previous run must have executed with identical hyperparameters\n      as the new run in order to be replayable. The only supported difference is\n      that the underlying data can change.\n    **kwargs: Extra keyword args passed to the parent.\n\n  Returns:\n    An :class:`adanet.Estimator` instance.\n\n  Raises:\n    :code:`ValueError`: If :code:`subnetwork_generator` is :code:`None`.\n    :code:`ValueError`: If :code:`max_iteration_steps` is <= 0.\n    :code:`ValueError`: If :code:`model_dir` is not specified during distributed\n      training.\n    :code:`ValueError`: If :code:`max_iterations` is <= 0.\n  """"""\n  # pyformat: enable\n\n  class _Keys(object):\n    CURRENT_ITERATION = ""current_iteration""\n    SUBNETWORK_GENERATOR = ""subnetwork_generator""\n\n  def __init__(self,\n               head,\n               subnetwork_generator,\n               max_iteration_steps,\n               ensemblers=None,\n               ensemble_strategies=None,\n               evaluator=None,\n               report_materializer=None,\n               metric_fn=None,\n               force_grow=False,\n               replicate_ensemble_in_training=False,\n               adanet_loss_decay=.9,\n               delay_secs_per_worker=5,\n               max_worker_delay_secs=60,\n               worker_wait_secs=5,\n               worker_wait_timeout_secs=7200,\n               model_dir=None,\n               report_dir=None,\n               config=None,\n               debug=False,\n               enable_ensemble_summaries=True,\n               enable_subnetwork_summaries=True,\n               global_step_combiner_fn=tf.math.reduce_mean,\n               max_iterations=None,\n               export_subnetwork_logits=False,\n               export_subnetwork_last_layer=True,\n               replay_config=None,\n               **kwargs):\n    if subnetwork_generator is None:\n      raise ValueError(""subnetwork_generator can\'t be None."")\n    if max_iteration_steps is not None and max_iteration_steps <= 0.:\n      raise ValueError(""max_iteration_steps must be > 0 or None."")\n    if max_iterations is not None and max_iterations <= 0.:\n      raise ValueError(""max_iterations must be > 0 or None."")\n    is_distributed_training = config and config.num_worker_replicas > 1\n    is_model_dir_specified = model_dir or (config and config.model_dir)\n    if is_distributed_training and not is_model_dir_specified:\n      # A common model dir for the chief and workers is required for\n      # coordination during distributed training.\n      raise ValueError(\n          ""For distributed training, a model_dir must be specified."")\n\n    self._subnetwork_generator = subnetwork_generator\n\n    # Overwrite superclass\'s assert that members are not overwritten in order\n    # to overwrite public methods. Note that we are doing something that is not\n    # explicitly supported by the Estimator API and may break in the future.\n    tf.estimator.Estimator._assert_members_are_not_overridden = staticmethod(  # pylint: disable=protected-access\n        lambda _: None)\n\n    self._enable_v2_checkpoint = kwargs.pop(""enable_v2_checkpoint"", False)\n    self._evaluator = evaluator\n    self._report_materializer = report_materializer\n\n    self._force_grow = force_grow\n    self._delay_secs_per_worker = delay_secs_per_worker\n    self._max_worker_delay_secs = max_worker_delay_secs\n    self._worker_wait_secs = worker_wait_secs\n    self._worker_wait_timeout_secs = worker_wait_timeout_secs\n    self._max_iterations = max_iterations\n    self._replay_config = replay_config\n\n    # Added for backwards compatibility.\n    default_ensembler_args = [\n        ""mixture_weight_type"", ""mixture_weight_initializer"",\n        ""warm_start_mixture_weights"", ""adanet_lambda"", ""adanet_beta"", ""use_bias""\n    ]\n    default_ensembler_kwargs = {\n        k: v for k, v in kwargs.items() if k in default_ensembler_args\n    }\n    if default_ensembler_kwargs:\n      logging.warning(\n          ""The following arguments have been moved to ""\n          ""`adanet.ensemble.ComplexityRegularizedEnsembler` which can be ""\n          ""specified in the `ensemblers` argument: %s"",\n          sorted(default_ensembler_kwargs.keys()))\n    for key in default_ensembler_kwargs:\n      del kwargs[key]\n\n    # Experimental feature.\n    placement_strategy_arg = ""experimental_placement_strategy""\n    placement_strategy = kwargs.pop(placement_strategy_arg, None)\n    if placement_strategy:\n      logging.warning(\n          ""%s is an experimental feature. Its behavior is not guaranteed ""\n          ""to be backwards compatible."", placement_strategy_arg)\n\n    # Monkey patch the default variable placement strategy that Estimator uses\n    # since it does not support workers having different graphs from the chief.\n    # TODO: Consider using `RunConfig.replace` with the new device_fn,\n    # but this can cause issues since RunConfig automatically parses TF_CONFIG\n    # environment variable.\n    with monkey_patch_default_variable_placement_strategy():\n      # This `Estimator` is responsible for bookkeeping across iterations, and\n      # for training the subnetworks in both a local and distributed setting.\n      # Subclassing improves future-proofing against new private methods being\n      # added to `tf.estimator.Estimator` that are expected to be callable by\n      # external functions, such as in b/110435640.\n      super(Estimator, self).__init__(\n          model_fn=self._create_model_fn(),\n          params={},\n          config=config,\n          model_dir=model_dir,\n          **kwargs)\n\n    if default_ensembler_kwargs and ensemblers:\n      raise ValueError(""When specifying the `ensemblers` argument, ""\n                       ""the following arguments must not be given: {}"".format(\n                           default_ensembler_kwargs.keys()))\n    if not ensemblers:\n      default_ensembler_kwargs[""model_dir""] = self.model_dir\n      ensemblers = [\n          ensemble_lib.ComplexityRegularizedEnsembler(\n              **default_ensembler_kwargs)\n      ]\n\n    # These are defined after base Estimator\'s init so that they can\n    # use the same temporary model_dir as the underlying Estimator even if\n    # model_dir is not provided.\n    self._use_tpu = kwargs.get(""use_tpu"", False)\n    ensemble_builder = _EnsembleBuilder(\n        head=head,\n        metric_fn=metric_fn,\n        use_tpu=self._use_tpu,\n        export_subnetwork_logits=export_subnetwork_logits,\n        export_subnetwork_last_layer=export_subnetwork_last_layer)\n\n    # TODO: Merge CandidateBuilder into SubnetworkManager.\n    candidate_builder = _CandidateBuilder(adanet_loss_decay=adanet_loss_decay)\n    subnetwork_manager = _SubnetworkManager(\n        head=head, metric_fn=metric_fn, use_tpu=self._use_tpu)\n    if not placement_strategy:\n      placement_strategy = distributed_lib.ReplicationStrategy()\n    self._iteration_builder = _IterationBuilder(\n        candidate_builder,\n        subnetwork_manager,\n        ensemble_builder,\n        ensemblers,\n        max_iteration_steps,\n        self._summary_maker,\n        global_step_combiner_fn,\n        placement_strategy,\n        replicate_ensemble_in_training,\n        use_tpu=self._use_tpu,\n        debug=debug,\n        enable_ensemble_summaries=enable_ensemble_summaries,\n        enable_subnetwork_summaries=enable_subnetwork_summaries,\n        enable_subnetwork_reports=self._report_materializer is not None)\n    self._ensemble_strategies = ensemble_strategies or [\n        ensemble_lib.GrowStrategy()\n    ]\n\n    report_dir = report_dir or os.path.join(self._model_dir, ""report"")\n    self._report_accessor = _ReportAccessor(report_dir)\n\n  def _summary_maker(self, scope=None, skip_summary=False, namespace=None):\n    """"""Constructs a `_ScopedSummary`.""""""\n    if tf_compat.is_v2_behavior_enabled():\n      # Here we assume TF 2 behavior is enabled.\n      return _ScopedSummaryV2(\n          logdir=self._model_dir,\n          scope=scope,\n          skip_summary=skip_summary,\n          namespace=namespace)\n    if self._use_tpu:\n      return _TPUScopedSummary(\n          logdir=self._model_dir,\n          scope=scope,\n          skip_summary=skip_summary,\n          namespace=namespace)\n    else:\n      return _ScopedSummary(\n          scope=scope, skip_summary=skip_summary, namespace=namespace)\n\n  def _checkpoint_iteration_number(self, checkpoint_path):\n    # type: (Text) -> int\n    """"""Returns the iteration number from the latest checkpoint.""""""\n\n    if checkpoint_path is None:\n      return 0\n\n    if self._enable_v2_checkpoint:\n      return tf_compat.load_variable(\n          checkpoint_path, ""iteration_number"", shape=[], dtype=tf.int64)\n    return tf.train.load_variable(checkpoint_path,\n                                  self._Keys.CURRENT_ITERATION).item()\n\n  def _checkpoint_global_step(self, checkpoint_path):\n    # type: (Text) -> int\n    """"""Returns the global step from the given checkpoint.""""""\n\n    if checkpoint_path is None:\n      return 0\n\n    if self._enable_v2_checkpoint:\n      return tf_compat.load_variable(\n          checkpoint_path,\n          tf_compat.v1.GraphKeys.GLOBAL_STEP,\n          shape=[],\n          dtype=tf.int64)\n    return tf.train.load_variable(checkpoint_path,\n                                  tf_compat.v1.GraphKeys.GLOBAL_STEP).item()\n\n  def train(self,\n            input_fn,\n            hooks=None,\n            steps=None,\n            max_steps=None,\n            saving_listeners=None):\n    # pyformat: disable\n    """"""Trains a model given training data :code:`input_fn`.\n\n    NOTE: If a given input_fn raises an :code:`OutOfRangeError`, then *all* of\n    training will exit. The best practice is to make the training dataset repeat\n    forever, in order to perform model search for more than one iteration.\n\n    Args:\n      input_fn: A function that provides input data for training as minibatches.\n        See [Premade Estimators](\n        https://tensorflow.org/guide/premade_estimators#create_input_functions)\n        for more information. The function should construct and return one of\n        the following:\n          * A :code:`tf.data.Dataset` object: Outputs of `Dataset` object must\n            be a tuple `(features, labels)` with same constraints as below.\n          * A tuple `(features, labels)`: Where `features` is a\n            :code:`tf.Tensor` or a dictionary of string feature name to\n            `Tensor` and `labels` is a :code:`Tensor` or a dictionary of string\n            label name to `Tensor`. Both `features` and `labels` are consumed by\n            `model_fn`. They should satisfy the expectation of `model_fn` from\n            inputs.\n      hooks: List of :code:`tf.train.SessionRunHook` subclass instances. Used\n        for callbacks inside the training loop.\n      steps: Number of steps for which to train the model. If :code:`None`,\n        train forever or train until `input_fn` generates the\n        :code:`tf.errors.OutOfRange` error or :code:`StopIteration` exception.\n        `steps` works incrementally. If you call two times `train(steps=10)`\n        then training occurs in total 20 steps. If :code:`OutOfRange` or\n        :code:`StopIteration` occurs in the middle, training stops before 20\n        steps. If you don\'t want to have incremental behavior please set\n        `max_steps` instead. If set, `max_steps` must be :code:`None`.\n      max_steps: Number of total steps for which to train model. If\n        :code:`None`, train forever or train until `input_fn` generates the\n        :code:`tf.errors.OutOfRange` error or :code:`StopIteration` exception.\n        If set, `steps` must be `None`. If :code:`OutOfRange` or\n        :code:`StopIteration` occurs in the middle, training stops before\n        `max_steps` steps. Two calls to `train(steps=100)` means 200 training\n        iterations. On the other hand, two calls to `train(max_steps=100)`\n        means that the second call will not do any iteration since first call\n        did all 100 steps.\n      saving_listeners: list of :code:`CheckpointSaverListener` objects. Used\n        for callbacks that run immediately before or after checkpoint savings.\n\n    Returns:\n      `self`, for chaining.\n\n    Raises:\n      ValueError: If both `steps` and `max_steps` are not `None`.\n      ValueError: If either `steps` or `max_steps <= 0`.\n    """"""\n    # pyformat: enable\n\n    if (steps is not None) and (max_steps is not None):\n      raise ValueError(""Can not provide both steps and max_steps."")\n    if steps is not None and steps <= 0:\n      raise ValueError(""Must specify steps > 0, given: {}"".format(steps))\n\n    latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n    latest_global_steps = self._checkpoint_global_step(latest_checkpoint)\n    if steps is not None:\n      max_steps = latest_global_steps + steps\n\n    # Each iteration of this AdaNet loop represents an `_Iteration`. The\n    # current iteration number is stored as a variable in the checkpoint so\n    # that training can be stopped and started at anytime.\n    with monkey_patch_default_variable_placement_strategy():\n      while True:\n        latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n        latest_global_steps = self._checkpoint_global_step(latest_checkpoint)\n        current_iteration = self._checkpoint_iteration_number(latest_checkpoint)\n        logging.info(""Beginning training AdaNet iteration %s"",\n                     current_iteration)\n        self._iteration_ended = False\n\n        # Delegate training to a temporary estimator instead of super to make\n        # passing arguments more functional (via params).\n        temp_estimator = self._create_temp_estimator(\n            config=self.config,\n            is_inside_training_loop=True,\n            checkpoint_path=latest_checkpoint,\n            hooks=hooks)\n        result = temp_estimator.train(\n            input_fn=input_fn,\n            hooks=hooks,\n            max_steps=max_steps,\n            saving_listeners=saving_listeners)\n        # In TensorFlow v2.0.0.rc1 and below, saving listeners are attached to\n        # the first CheckpointSaverHook each time train is called. Instead, we\n        # pass in the saving_listeners in the first AdaNet iteration only.\n        if not tf_compat.version_greater_or_equal(""2.0.0.rc1""):\n          saving_listeners = None\n        logging.info(""Finished training Adanet iteration %s"", current_iteration)\n\n        # If training ended because the maximum number of training steps\n        # occurred, exit training.\n        latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n        global_steps = self._checkpoint_global_step(latest_checkpoint)\n        if max_steps is not None and global_steps >= max_steps:\n          logging.info(""Training ended after %s global steps"", global_steps)\n          return result\n\n        # If training ended for any reason other than the iteration ending,\n        # exit training.\n        if not self._iteration_ended:\n          logging.info(""Training stop requested"")\n          return result\n\n        max_iterations = self._max_iterations\n        if max_iterations and current_iteration + 1 >= max_iterations:\n          logging.info(\n              ""Training ended after exceeding maximum AdaNet iterations"")\n          if steps is not None and global_steps - latest_global_steps < steps:\n            logging.warning(\n                ""Both `max_iterations` and `steps` were specified, but ""\n                ""`max_iterations` takes precedence over `steps`"")\n          return result\n\n        logging.info(""Beginning bookkeeping phase for iteration %s"",\n                     current_iteration)\n\n        # The chief prepares the next AdaNet iteration, and increments the\n        # iteration number by 1.\n        if self.config.is_chief:\n          with self._force_replication_strategy():\n            self._execute_bookkeeping_phase(\n                input_fn,\n                current_iteration,\n                train_hooks=hooks or [],\n                checkpoint_path=latest_checkpoint)\n\n        # This inner loop serves mainly for synchronizing the workers with the\n        # chief during distributed training. Workers that finish training early\n        # wait for the chief to prepare the next iteration and increment the\n        # iteration number. Workers that are slow to finish training quickly\n        # move onto the next iteration. And workers that go offline and return\n        # online after training ended terminate gracefully.\n        wait_for_chief = not self.config.is_chief\n        timer = _CountDownTimer(self._worker_wait_timeout_secs)\n        while wait_for_chief:\n          # Fetch the latest checkpoint.\n          latest_checkpoint = tf.train.latest_checkpoint(self.model_dir)\n\n          # If the chief hits max_steps, it will stop training itself and not\n          # increment the iteration number, so this is how the worker knows to\n          # exit if it wakes up and the chief is gone.\n          # TODO: Support steps parameter.\n          if self._checkpoint_global_step(latest_checkpoint) >= max_steps:\n            return result\n\n          # In distributed training, a worker may end training before the chief\n          # overwrites the checkpoint with the incremented iteration number. If\n          # that is the case, it should wait for the chief to do so. Otherwise\n          # the worker will get stuck waiting for its weights to be initialized.\n          next_iteration = self._checkpoint_iteration_number(latest_checkpoint)\n          if next_iteration > current_iteration:\n            break\n          logging.info(""Iteration number in latest checkpoint: %d"",\n                       next_iteration)\n\n          # Check timeout when waiting for potentially downed chief.\n          if timer.secs_remaining() == 0:\n            logging.error(\n                ""Chief job did not prepare iteration %d after %s secs. It ""\n                ""may have been preempted, been turned down, or crashed. This ""\n                ""worker is now exiting training."", current_iteration + 1,\n                self._worker_wait_timeout_secs)\n            return result\n          logging.info(""Waiting for chief to prepare iteration %d"",\n                       current_iteration + 1)\n          time.sleep(self._worker_wait_secs)\n\n        # Stagger starting workers to prevent training instability.\n        # Mimics behavior of tf.estimator.train_and_evaluate.\n        if not self.config.is_chief and self.config.task_type == ""worker"":\n          task_id = self.config.task_id or 0\n          # Stagger each worker up to 60 secs.\n          delay_secs = min(self._max_worker_delay_secs,\n                           (task_id + 1.) * self._delay_secs_per_worker)\n          if delay_secs > 0.:\n            logging.info(""Waiting %d secs before continuing training."",\n                         delay_secs)\n            time.sleep(delay_secs)\n\n        logging.info(""Finished bookkeeping phase for iteration %s"",\n                     current_iteration)\n\n  def evaluate(self,\n               input_fn,\n               steps=None,\n               hooks=None,\n               checkpoint_path=None,\n               name=None):\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Evaluating AdaNet model at checkpoint: %s"", checkpoint_path)\n\n    # Delegate evaluation to a temporary estimator instead of super to make\n    # passing arguments more functional (via params).\n    temp_estimator = self._create_temp_estimator(\n        config=self.config,\n        checkpoint_path=checkpoint_path,\n        evaluation_name=name,\n        # Ensure that the read to get the iteration number and read to restore\n        # variable values come from the same checkpoint during evaluation.\n        best_ensemble_index=self._compute_best_ensemble_index(\n            checkpoint_path, hooks),\n        hooks=hooks)\n    with _disable_asserts_for_confusion_matrix_at_thresholds():\n      result = temp_estimator.evaluate(\n          input_fn,\n          steps=steps,\n          hooks=hooks,\n          checkpoint_path=checkpoint_path,\n          name=name)\n    return result\n\n  def predict(self,\n              input_fn,\n              predict_keys=None,\n              hooks=None,\n              checkpoint_path=None,\n              yield_single_examples=True):\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Computing predictions for AdaNet model at checkpoint: %s"",\n                 checkpoint_path)\n    # Delegate predicting to a temporary estimator instead of super to make\n    # passing arguments more functional (via params).\n    temp_estimator = self._create_temp_estimator(\n        config=self.config,\n        best_ensemble_index=self._compute_best_ensemble_index(\n            checkpoint_path, hooks=hooks),\n        checkpoint_path=checkpoint_path,\n        hooks=hooks)\n    return temp_estimator.predict(\n        input_fn=input_fn,\n        predict_keys=predict_keys,\n        checkpoint_path=checkpoint_path,\n        yield_single_examples=yield_single_examples,\n        hooks=hooks)\n\n  from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n  @deprecation.deprecated(\n      None, ""This function has been renamed, use `export_saved_model` instead."")\n  def export_savedmodel(self,\n                        export_dir_base,\n                        serving_input_receiver_fn,\n                        hooks=None,\n                        assets_extra=None,\n                        as_text=False,\n                        checkpoint_path=None,\n                        strip_default_attrs=False):\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Exporting SavedModel for AdaNet model at checkpoint: %s"",\n                 checkpoint_path)\n    # Delegate exporting to a temporary estimator instead of super to make\n    # passing arguments more functional (via params).\n    temp_estimator = self._create_temp_estimator(\n        config=self.config,\n        hooks=hooks,\n        best_ensemble_index=self._compute_best_ensemble_index(\n            checkpoint_path, hooks=hooks),\n        checkpoint_path=checkpoint_path,\n        is_export=True)\n    with self._force_replication_strategy():\n      return temp_estimator.export_savedmodel(\n          export_dir_base=export_dir_base,\n          serving_input_receiver_fn=serving_input_receiver_fn,\n          assets_extra=assets_extra,\n          as_text=as_text,\n          checkpoint_path=checkpoint_path,\n          strip_default_attrs=strip_default_attrs)\n\n  def export_saved_model(self,\n                         export_dir_base,\n                         serving_input_receiver_fn,\n                         hooks=None,\n                         assets_extra=None,\n                         as_text=False,\n                         checkpoint_path=None,\n                         experimental_mode=tf.estimator.ModeKeys.PREDICT):\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Exporting SavedModel for AdaNet model at checkpoint: %s"",\n                 checkpoint_path)\n    # Delegate exporting to a temporary estimator instead of super to make\n    # passing arguments more functional (via params).\n    temp_estimator = self._create_temp_estimator(\n        config=self.config,\n        best_ensemble_index=self._compute_best_ensemble_index(\n            checkpoint_path, hooks=hooks),\n        checkpoint_path=checkpoint_path,\n        hooks=hooks,\n        is_export=True)\n    with self._force_replication_strategy():\n      return temp_estimator.export_saved_model(\n          export_dir_base=export_dir_base,\n          serving_input_receiver_fn=serving_input_receiver_fn,\n          assets_extra=assets_extra,\n          as_text=as_text,\n          checkpoint_path=checkpoint_path,\n          experimental_mode=experimental_mode)\n\n  def experimental_export_all_saved_models(self,\n                                           export_dir_base,\n                                           input_receiver_fn_map,\n                                           hooks=None,\n                                           assets_extra=None,\n                                           as_text=False,\n                                           checkpoint_path=None):\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Exporting SavedModel for AdaNet model at checkpoint: %s"",\n                 checkpoint_path)\n    # Delegate exporting to a temporary estimator instead of super to make\n    # passing arguments more functional (via params).\n    temp_estimator = self._create_temp_estimator(\n        config=self.config,\n        best_ensemble_index=self._compute_best_ensemble_index(\n            checkpoint_path, hooks=hooks),\n        checkpoint_path=checkpoint_path,\n        hooks=hooks,\n        is_export=True)\n    with self._force_replication_strategy():\n      return temp_estimator.experimental_export_all_saved_models(\n          export_dir_base=export_dir_base,\n          input_receiver_fn_map=input_receiver_fn_map,\n          assets_extra=assets_extra,\n          as_text=as_text,\n          checkpoint_path=checkpoint_path)\n\n  def _compute_best_ensemble_index(self, checkpoint_path, hooks):\n    # type: (Text, Sequence[tf_compat.SessionRunHook]) -> Optional[int]\n    """"""Runs the Evaluator to obtain the best ensemble index among candidates.""""""\n\n    # AdaNet Replay.\n    if self._replay_config:\n      iteration_number = self._checkpoint_iteration_number(checkpoint_path)\n      best_index = self._replay_config.get_best_ensemble_index(iteration_number)\n      if best_index is not None:\n        return best_index\n\n    if self._evaluator:\n      return self._execute_candidate_evaluation_phase(\n          self._evaluator.input_fn,\n          export_best_architecture=False,\n          checkpoint_path=checkpoint_path,\n          hooks=hooks)\n    return None\n\n  @contextlib.contextmanager\n  def _force_replication_strategy(self):\n    """"""Sets placement_strategy to always be ReplicationStrategy.\n\n    This is useful during the bookkeeping phase and when Estimator\'s export\n    saved model functions are called. In both of these cases, local tf.Sessions\n    are created which do not have access to the cluster. Therefore,\n    RoundRobinReplicationStrategy will fail when it tries to place ops on\n    cluster devices which the local tf.Sessions cannot access.\n\n    Yields:\n      Nothing. Simply returns control back to the caller.\n    """"""\n\n    temp_placement_strategy = self._iteration_builder.placement_strategy\n    try:\n      placement_strategy = distributed_lib.ReplicationStrategy()\n      self._iteration_builder.placement_strategy = placement_strategy\n      yield\n    finally:\n      self._iteration_builder.placement_strategy = temp_placement_strategy\n\n  @contextlib.contextmanager\n  def _call_input_fn_in_new_graph(self, input_fn, mode, config):\n    """"""Calls the given input_fn and yields results within a new graph context.\n\n    Yields features, labels, and hooks from the result of an Estimator input_fn.\n\n    Args:\n      input_fn: a function that takes no arguments and returns one of:\n        * A \'tf.data.Dataset\' object: Outputs of `Dataset` object must be a\n          tuple (features, labels) with same constraints as below.\n        * A tuple (features, labels): Where `features` is a `Tensor` or a\n          dictionary of string feature name to `Tensor` and `labels` is a\n          `Tensor` or a dictionary of string label name to `Tensor`. Both\n          `features` and `labels` are consumed by `model_fn`. They should\n          satisfy the expectation of `model_fn` from inputs.\n      mode: Defines whether this is training, evaluation or prediction. See\n        `ModeKeys`.\n      config: The current `tf.estimator.RunConfig`.\n\n    Yields:\n      Tuple of features, labels, and input_hooks, where features are as\n      described above, labels are as described above or None, and input_hooks\n      are a list of SessionRunHooks to be included when running.\n\n    Raises:\n      ValueError: if the result is a list or tuple of length != 2.\n    """"""\n\n    from tensorflow_estimator.python.estimator import util  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    with tf.Graph().as_default() as g:\n      tf_compat.v1.set_random_seed(config.tf_random_seed)\n      # Create global step before calling model_fn as does superclass.\n      self._create_and_assert_global_step(g)\n      with tf.device(""/cpu:0""):\n        input_fn_outs = input_fn()\n      yield util.parse_input_fn_result(input_fn_outs)\n\n  def _create_temp_run_config(self, temp_model_dir):\n    # type: (Text) -> tf.estimator.RunConfig\n    """"""Creates a temp `RunConfig` for the bookkeeping phase.""""""\n\n    config = self.config\n    return tf.estimator.RunConfig(\n        model_dir=temp_model_dir,\n        tf_random_seed=config.tf_random_seed,\n        session_config=config.session_config,\n        protocol=config.protocol)\n\n  def _create_temp_estimator(self, config, **create_model_fn_args):\n    # type: (tf.estimator.RunConfig, Any[...]) -> tf.estimator.Estimator  # pylint:disable=line-too-long\n    """"""Creates a temp `Estimator` to grow the graph for the next iteration.""""""\n\n    return tf.estimator.Estimator(\n        model_fn=self._create_model_fn(**create_model_fn_args), config=config)\n\n  def _execute_bookkeeping_phase(self, train_input_fn, iteration_number,\n                                 train_hooks, checkpoint_path):\n    """"""Run the AdaNet bookkeeping phase to prepare the next iteration.\n\n    This method creates a TensorFlow graph up to three times:\n      1. To evaluate all candidate ensembles to find the best one.\n      2. To materialize reports and store them to disk (if report_materializer\n         exists).\n      3. To grow the TensorFlow graph and overwrite the model directory\'s\n         checkpoint with the next iteration\'s ops.\n\n    Args:\n      train_input_fn: The input_fn used during training.\n      iteration_number: Integer current iteration number.\n      train_hooks: List of `SessionRunHook` passed for training.\n      checkpoint_path: Path to the checkpoint to restore from.\n    """"""\n\n    next_iteration_number = iteration_number + 1\n    logging.info(""Preparing iteration %s:"", next_iteration_number)\n\n    if self._evaluator:\n      evaluator_input_fn = self._evaluator.input_fn\n    else:\n      evaluator_input_fn = train_input_fn\n\n    best_ensemble_index = self._execute_candidate_evaluation_phase(\n        evaluator_input_fn,\n        export_best_architecture=True,\n        checkpoint_path=checkpoint_path,\n        hooks=train_hooks)\n    self._execute_report_materialization_phase(\n        best_ensemble_index, checkpoint_path=checkpoint_path, hooks=train_hooks)\n    self._execute_graph_growing_phase(train_input_fn, train_hooks,\n                                      checkpoint_path)\n\n    logging.info(""Finished preparing iteration %s."", next_iteration_number)\n\n  def _execute_candidate_evaluation_phase(self, evaluator_input_fn,\n                                          export_best_architecture,\n                                          checkpoint_path, hooks):\n    """"""Evaluates and chooses the best ensemble for this iteration.\n\n    Args:\n      evaluator_input_fn: The input_fn for evaluation.\n      export_best_architecture: Boolean whether to persist the best ensemble\'s\n        architecture to the model_dir.\n      checkpoint_path: Path to the checkpoint to restore from.\n      hooks: A list of `tf.estimator.SessionRunHook`s.\n\n    Returns:\n      Integer index of the best ensemble withing the list of candidate ensembles\n      for the current iteration.\n    """"""\n\n    logging.info(""Evaluating candidates..."")\n    config = self.config\n    mode = tf.estimator.ModeKeys.EVAL\n    with self._call_input_fn_in_new_graph(evaluator_input_fn, mode,\n                                          config) as (features, labels,\n                                                      input_hooks):\n      current_iteration, _ = self._create_iteration(\n          features,\n          labels,\n          mode,\n          config,\n          is_growing_phase=False,\n          checkpoint_path=checkpoint_path,\n          hooks=hooks)\n      best_ensemble_index = self._get_best_ensemble_index(\n          current_iteration, input_hooks, checkpoint_path)\n      architecture = current_iteration.candidates[\n          best_ensemble_index].ensemble_spec.architecture\n    if export_best_architecture:\n      iteration_number = self._checkpoint_iteration_number(checkpoint_path)\n      new_architecture_filename = self._architecture_filename(iteration_number)\n      logging.info(""Exporting best ensemble architecture to %s"",\n                   new_architecture_filename)\n      self._save_architecture(new_architecture_filename, architecture,\n                              checkpoint_path)\n    logging.info(""Done evaluating candidates."")\n\n    return best_ensemble_index\n\n  def _execute_report_materialization_phase(self, best_ensemble_index,\n                                            checkpoint_path, hooks):\n    """"""Materializes and store subnetwork reports.""""""\n\n    if not self._report_materializer:\n      return\n\n    logging.info(""Materializing reports..."")\n    input_fn = self._report_materializer.input_fn\n    mode = tf.estimator.ModeKeys.EVAL\n    config = self.config\n    with self._call_input_fn_in_new_graph(input_fn, mode,\n                                          config) as (features, labels,\n                                                      input_hooks):\n      current_iteration, _ = self._create_iteration(\n          features,\n          labels,\n          mode,\n          config,\n          is_growing_phase=False,\n          checkpoint_path=checkpoint_path,\n          hooks=hooks)\n      self._materialize_report(current_iteration, input_hooks,\n                               best_ensemble_index, checkpoint_path)\n    logging.info(""Done materializing reports."")\n\n  def _execute_graph_growing_phase(self, train_input_fn, train_hooks,\n                                   checkpoint_path):\n    """"""Grows the tensorflow graph for the next iteration.\n\n    Normally the MonitoredTrainingSession does not allow one to add new ops to\n    the TensorFlow graph once training starts. To get around this limitation,\n    create the graph for the next iteration and overwrite the model directory\n    checkpoint with the expanded graph.\n\n    Args:\n      train_input_fn: The input_fn used during training.\n      train_hooks: List of `SessionRunHook` passed for training.\n      checkpoint_path: Path of the checkpoint to use for restoring variables.\n    """"""\n\n    logging.info(""Adapting graph and incrementing iteration number..."")\n    config = self.config\n    temp_model_dir = os.path.join(self.model_dir, ""temp_model_dir"")\n    if not tf.io.gfile.exists(temp_model_dir):\n      tf.io.gfile.makedirs(temp_model_dir)\n    # Since deleting a model_dir can fail, we need each temporary directory to\n    # be unique. So we use the UTC time when creating it.\n    time_in_millis = int(time.time() * 1000)\n    temp_model_sub_dir = os.path.join(temp_model_dir, str(time_in_millis))\n    temp_run_config = config.replace(model_dir=temp_model_sub_dir)\n    temp_estimator = self._create_temp_estimator(\n        config=temp_run_config,\n        is_growing_phase=True,\n        is_inside_training_loop=True,\n        checkpoint_path=checkpoint_path,\n        hooks=train_hooks)\n\n    _copy_recursively(\n        os.path.join(self._model_dir, ""assets""),\n        os.path.join(temp_model_sub_dir, ""assets""))\n\n    # Do not train with any saving_listeners since this is just a temporary\n    # estimator.\n    temp_estimator.train(\n        input_fn=train_input_fn,\n        max_steps=1,\n        hooks=self._process_hooks_for_growing_phase(train_hooks),\n        saving_listeners=None)\n\n    _copy_recursively(\n        os.path.join(temp_model_sub_dir, ""assets""),\n        os.path.join(self._model_dir, ""assets""))\n\n    _delete_directory(temp_model_dir)\n    logging.info(""Done adapting graph and incrementing iteration number."")\n\n  def _architecture_filename(self, iteration_number):\n    # type: (int) -> Text\n    """"""Returns the filename of the given iteration\'s frozen graph.""""""\n\n    frozen_checkpoint = os.path.join(self.model_dir, ""architecture"")\n    return ""{}-{}.json"".format(frozen_checkpoint, iteration_number)\n\n  def _get_best_ensemble_index(self,\n                               current_iteration,\n                               input_hooks,\n                               checkpoint_path=None):\n    # type: (_Iteration, Sequence[tf_compat.SessionRunHook], Text) -> int\n    """"""Returns the best candidate ensemble\'s index in this iteration.\n\n    Evaluates the ensembles using an `Evaluator` when provided. Otherwise,\n    it returns the index of the best candidate as defined by the `_Iteration`.\n\n    Args:\n      current_iteration: Current `_Iteration`.\n      input_hooks: List of SessionRunHooks to be included when running.\n      checkpoint_path: Checkpoint to use when determining the best index.\n\n    Returns:\n      Index of the best ensemble in the iteration\'s list of `_Candidates`.\n    """"""\n    # AdaNet Replay.\n    if self._replay_config:\n      best_index = self._replay_config.get_best_ensemble_index(\n          current_iteration.number)\n      if best_index is not None:\n        return best_index\n\n    # Skip the evaluation phase when there is only one candidate subnetwork.\n    if len(current_iteration.candidates) == 1:\n      logging.info(""\'%s\' is the only ensemble"",\n                   current_iteration.candidates[0].ensemble_spec.name)\n      return 0\n\n    # The zero-th index candidate at iteration t>0 is always the\n    # previous_ensemble.\n    if current_iteration.number > 0 and self._force_grow and (len(\n        current_iteration.candidates) == 2):\n      logging.info(""With `force_grow` enabled, \'%s\' is the only ensemble"",\n                   current_iteration.candidates[1].ensemble_spec.name)\n      return 1\n\n    logging.info(""Starting ensemble evaluation for iteration %s"",\n                 current_iteration.number)\n    for hook in input_hooks:\n      hook.begin()\n    with tf_compat.v1.Session(config=self.config.session_config) as sess:\n      init = tf.group(\n          tf_compat.v1.global_variables_initializer(),\n          tf_compat.v1.local_variables_initializer(),\n          tf_compat.v1.tables_initializer(),\n          current_iteration.estimator_spec.scaffold.local_init_op if isinstance(\n              current_iteration.estimator_spec,\n              tf.estimator.EstimatorSpec) else tf.no_op())\n      sess.run(init)\n\n      if self._enable_v2_checkpoint:\n        status = current_iteration.checkpoint.restore(checkpoint_path)\n        status.expect_partial()  # Optional sanity checks.\n        status.initialize_or_restore(sess)\n      else:\n        saver = tf_compat.v1.train.Saver(sharded=True)\n        saver.restore(sess, checkpoint_path)\n\n      coord = tf.train.Coordinator()\n      for hook in input_hooks:\n        hook.after_create_session(sess, coord)\n\n      tf_compat.v1.train.start_queue_runners(sess=sess, coord=coord)\n      ensemble_metrics = []\n      for candidate in current_iteration.candidates:\n        metrics = candidate.ensemble_spec.eval_metrics.eval_metrics_ops()\n        metrics[""adanet_loss""] = tf_compat.v1.metrics.mean(\n            candidate.ensemble_spec.adanet_loss)\n        ensemble_metrics.append(metrics)\n      if self._evaluator:\n        metric_name = self._evaluator.metric_name\n        metrics = self._evaluator.evaluate(sess, ensemble_metrics)\n        objective_fn = self._evaluator.objective_fn\n      else:\n        metric_name = ""adanet_loss""\n        metrics = sess.run(\n            [c.adanet_loss for c in current_iteration.candidates])\n        objective_fn = np.nanargmin\n\n      values = []\n      for i in range(len(current_iteration.candidates)):\n        ensemble_name = current_iteration.candidates[i].ensemble_spec.name\n        values.append(""{}/{} = {:.6f}"".format(metric_name, ensemble_name,\n                                              metrics[i]))\n      logging.info(""Computed ensemble metrics: %s"", "", "".join(values))\n      if self._force_grow and current_iteration.number > 0:\n        logging.info(\n            ""The `force_grow` override is enabled, so the ""\n            ""the performance of the previous ensemble will be ignored."")\n        # NOTE: The zero-th index candidate at iteration t>0 is always the\n        # previous_ensemble.\n        metrics = metrics[1:]\n        index = objective_fn(metrics) + 1\n      else:\n        index = objective_fn(metrics)\n    logging.info(""Finished ensemble evaluation for iteration %s"",\n                 current_iteration.number)\n    logging.info(""\'%s\' at index %s is the best ensemble"",\n                 current_iteration.candidates[index].ensemble_spec.name, index)\n    return index\n\n  def _materialize_report(self, current_iteration, input_hooks,\n                          best_ensemble_index, checkpoint_path):\n    """"""Generates reports as defined by `Builder`s.\n\n    Materializes the Tensors and metrics defined in the `Builder`s\'\n    `build_subnetwork_report` method using `ReportMaterializer`, and stores\n    them to disk using `_ReportAccessor`.\n\n    Args:\n      current_iteration: Current `_Iteration`.\n      input_hooks: List of SessionRunHooks to be included when running.\n      best_ensemble_index: Integer index of the best candidate ensemble.\n      checkpoint_path: Path of the checkpoint to use.\n    """"""\n\n    logging.info(""Starting metric logging for iteration %s"",\n                 current_iteration.number)\n\n    best_candidate = current_iteration.candidates[best_ensemble_index]\n    best_architecture = best_candidate.ensemble_spec.architecture\n    included_subnetwork_names = [\n        name for i, name in best_architecture.subnetworks\n        if i == current_iteration.number\n    ]\n    for hook in input_hooks:\n      hook.begin()\n    if self._enable_v2_checkpoint:\n      status = current_iteration.checkpoint.restore(checkpoint_path)\n      # Verify that restoring subset of ops from previous iteration works.\n      status.expect_partial()  # Optional sanity checks.\n    with tf_compat.v1.Session(config=self.config.session_config) as sess:\n      init = tf.group(\n          tf_compat.v1.global_variables_initializer(),\n          tf_compat.v1.local_variables_initializer(),\n          tf_compat.v1.tables_initializer(),\n          current_iteration.estimator_spec.scaffold.local_init_op if isinstance(\n              current_iteration.estimator_spec,\n              tf.estimator.EstimatorSpec) else tf.no_op())\n      sess.run(init)\n\n      if self._enable_v2_checkpoint:\n        status.initialize_or_restore(sess)\n      else:\n        saver = tf_compat.v1.train.Saver(sharded=True)\n        saver.restore(sess, checkpoint_path)\n\n      coord = tf.train.Coordinator()\n      for hook in input_hooks:\n        hook.after_create_session(sess, coord)\n\n      tf_compat.v1.train.start_queue_runners(sess=sess, coord=coord)\n      materialized_reports = (\n          self._report_materializer.materialize_subnetwork_reports(\n              sess, current_iteration.number,\n              current_iteration.subnetwork_reports, included_subnetwork_names))\n      self._report_accessor.write_iteration_report(current_iteration.number,\n                                                   materialized_reports)\n\n    logging.info(""Finished saving subnetwork reports for iteration %s"",\n                 current_iteration.number)\n\n  def _process_hooks_for_growing_phase(self, hooks):\n    """"""Processes hooks which will run during the graph growing phase.\n\n    In particular the following things are done:\n      - CheckpointSaverHooks are filtered out since they are not intended to\n        run between training runs and will cause errors. We also reset the\n        CheckpointSaverHooks\' Saver between iterations, see b/122795064 for more\n        details.\n      - Decorate the remaining hooks with _GraphGrowingHookDecorator to only run\n        the begin() and end() methods during the graph growing phase.\n\n    Args:\n      hooks: The list of `SessionRunHooks` to process.\n\n    Returns:\n      The processed hooks which should run during the growing phase.\n    """"""\n\n    processed_hooks = []\n    for hook in hooks:\n      # Reset CheckpointSaverHooks\' Saver and filter out.\n      if isinstance(hook, tf_compat.CheckpointSaverHook):\n        hook._saver = None  # pylint: disable=protected-access\n        continue\n      # Do not decorate the _OverwriteCheckpointHook since it should always\n      # run during the graph growing phase.\n      if not isinstance(hook, _OverwriteCheckpointHook):\n        hook = _GraphGrowingHookDecorator(hook)\n      processed_hooks.append(hook)\n    return processed_hooks\n\n  def _training_chief_hooks(self, current_iteration, training):\n    """"""Returns chief-only training hooks to be run this iteration.\n\n    Args:\n      current_iteration: Current `_Iteration`.\n      training: Whether in training mode.\n\n    Returns:\n      A list of `SessionRunHook` instances.\n    """"""\n\n    if not training:\n      return []\n\n    training_hooks = []\n    if tf_compat.is_v2_behavior_enabled():\n      # Use V2 summaries and hook when user is using TF 2 behavior.\n      training_hooks.append(\n          _SummaryV2SaverHook(\n              current_iteration.summaries,\n              save_steps=self.config.save_summary_steps))\n    else:\n      # Fallback to V1 summaries.\n      for summary in current_iteration.summaries:\n        output_dir = self.model_dir\n        if summary.scope:\n          output_dir = os.path.join(output_dir, summary.namespace,\n                                    summary.scope)\n        summary_saver_hook = tf_compat.SummarySaverHook(\n            save_steps=self.config.save_summary_steps,\n            output_dir=output_dir,\n            summary_op=summary.merge_all())\n        training_hooks.append(summary_saver_hook)\n    training_hooks += list(\n        current_iteration.estimator_spec.training_chief_hooks)\n    return training_hooks\n\n  def _training_hooks(self, current_iteration, training,\n                      iteration_number_tensor, previous_iteration_vars,\n                      is_growing_phase):\n    """"""Returns training hooks to be run on all workers and chief this iteration.\n\n    Args:\n      current_iteration: Current `_Iteration`.\n      training: Whether in training mode.\n      iteration_number_tensor: An int tensor of the current AdaNet iteraiton.\n      previous_iteration_vars: The variables of the previous iteration to be\n        restored by the _OverwriteCheckpointHook. If empty, no\n        _OverwriteCheckpointHook will be created.\n      is_growing_phase: Whether we are in the AdaNet graph growing phase.\n\n    Returns:\n      A list of `SessionRunHook` instances.\n    """"""\n\n    if not training:\n      return []\n\n    def after_fn():\n      self._iteration_ended = True\n\n    training_hooks = list(current_iteration.estimator_spec.training_hooks) + [\n        _StopAfterTrainingHook(current_iteration, after_fn=after_fn)\n    ]\n\n    if is_growing_phase:\n      training_hooks.append(\n          _OverwriteCheckpointHook(current_iteration, iteration_number_tensor,\n                                   previous_iteration_vars, self.config,\n                                   self._enable_v2_checkpoint))\n    return training_hooks\n\n  def _evaluation_hooks(self, current_iteration, training, evaluation_name):\n    """"""Returns evaluation hooks for this iteration.\n\n    Args:\n      current_iteration: Current `_Iteration`.\n      training: Whether in training mode.\n      evaluation_name: String name to append to the eval directory.\n\n    Returns:\n      A list of `SessionRunHook` instances.\n    """"""\n\n    if training:\n      return []\n    evaluation_hooks = []\n    for subnetwork_spec in current_iteration.subnetwork_specs:\n      evaluation_hooks.append(\n          self._create_eval_metric_saver_hook(\n              subnetwork_spec.eval_metrics,\n              subnetwork_spec.name,\n              kind=""subnetwork"",\n              evaluation_name=evaluation_name))\n    for candidate in current_iteration.candidates:\n      evaluation_hooks.append(\n          self._create_eval_metric_saver_hook(\n              candidate.ensemble_spec.eval_metrics,\n              candidate.ensemble_spec.name,\n              kind=""ensemble"",\n              evaluation_name=evaluation_name))\n    return evaluation_hooks\n\n  def _create_eval_metric_saver_hook(self, eval_metrics, name, kind,\n                                     evaluation_name):\n    eval_subdir = ""eval""\n    if evaluation_name:\n      eval_subdir = ""eval_{}"".format(evaluation_name)\n    return _EvalMetricSaverHook(\n        name=name,\n        kind=kind,\n        eval_metrics=eval_metrics,\n        output_dir=os.path.join(self.model_dir, kind, name, eval_subdir))\n\n  def _save_architecture(self, filename, architecture, checkpoint_path):\n    # type: (Text, _Architecture, Text) -> None\n    """"""Persists the ensemble\'s architecture in a serialized format.\n\n    Writes to a text file with one subnetwork\'s iteration number and name\n    per line.\n\n    Args:\n      filename: String filename to persist the ensemble architecture.\n      architecture: Target `_Architecture` instance.\n      checkpoint_path: Path of the checkpoint to use.\n    """"""\n\n    # Make directories since model_dir may not have been created yet.\n    tf.io.gfile.makedirs(os.path.dirname(filename))\n    iteration_number = self._checkpoint_iteration_number(checkpoint_path)\n    global_step = self._checkpoint_global_step(checkpoint_path)\n    serialized_architecture = architecture.serialize(iteration_number,\n                                                     global_step)\n    logging.info(""Saving architecture to %s:\\n%s"", filename,\n                 serialized_architecture)\n    with tf.io.gfile.GFile(filename, ""w"") as record_file:\n      record_file.write(serialized_architecture)\n\n  def _read_architecture(self, filename):\n    # type: (Text) -> _Architecture\n    """"""Reads an ensemble architecture from disk.\n\n    Assumes the file was written with `_save_architecture`.\n\n    Args:\n      filename: String filename where features were recorded.\n\n    Returns:\n      An `_Architecture` instance.\n\n    Raises:\n      OSError: When file not found at `filename`.\n    """"""\n\n    if not tf.io.gfile.exists(filename):\n      raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), filename)\n\n    with tf.io.gfile.GFile(filename, ""rb"") as gfile:\n      return _Architecture.deserialize(gfile.read().decode())\n\n  def _find_ensemble_candidate(self, ensemble_candidate_name,\n                               ensemble_candidates):\n    # type: (Text, Sequence[ensemble_lib.Candidate]) -> ensemble_lib.Candidate\n    """"""Returns the ensemble candidate with the given name.""""""\n\n    for ensemble_candidate in ensemble_candidates:\n      if ensemble_candidate.name == ensemble_candidate_name:\n        return ensemble_candidate\n    raise ValueError(\n        ""Could not find a matching ensemble candidate with name \'{}\'. ""\n        ""Are you sure the `adanet.ensemble.Strategy` is deterministic?"".format(\n            ensemble_candidate_name))\n\n  # TODO: Refactor architecture building logic to its own module.\n  def _architecture_ensemble_spec(self, architecture, iteration_number,\n                                  features, mode, labels,\n                                  previous_ensemble_spec, config,\n                                  previous_iteration, hooks):\n    """"""Returns an `_EnsembleSpec` with the given architecture.\n\n    Creates the ensemble architecture by calling `generate_subnetworks` on\n    `self._subnetwork_generator` and only calling `build_subnetwork` on\n    `Builders` included in the architecture. Once their ops are created, their\n    variables are restored from the checkpoint.\n\n    Args:\n      architecture: An `_Architecture` instance.\n      iteration_number: Integer current iteration number.\n      features: Dictionary of `Tensor` objects keyed by feature name.\n      mode: Defines whether this is training, evaluation or prediction. See\n        `ModeKeys`.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head). Can be `None`.\n      previous_ensemble_spec: The `_EnsembleSpec` for the previous iteration.\n        Will be `None` for the first iteration.\n      config: The current `tf.estimator.RunConfig`.\n      previous_iteration: The previous `_Iteration`.\n      hooks: A list of `tf.estimator.SessionRunHook`s.\n\n    Returns:\n      An `EnsembleSpec` instance for the given architecture.\n\n    Raises:\n      ValueError: If a subnetwork from `architecture` is not found in the\n        generated candidate `Builders` of the specified iteration.\n    """"""\n\n    previous_ensemble = None\n    if previous_ensemble_spec:\n      previous_ensemble = previous_ensemble_spec.ensemble\n    current_iteration = previous_iteration\n    for t, names in architecture.subnetworks_grouped_by_iteration:\n      if t != iteration_number:\n        continue\n      previous_ensemble_reports, all_reports = [], []\n      if self._report_materializer:\n        previous_ensemble_reports, all_reports = (\n            self._collate_subnetwork_reports(iteration_number))\n      generated_subnetwork_builders = (\n          self._call_generate_candidates(\n              previous_ensemble=previous_ensemble,\n              iteration_number=iteration_number,\n              previous_ensemble_reports=previous_ensemble_reports,\n              all_reports=all_reports,\n              config=config))\n      subnetwork_builder_names = {\n          b.name: b for b in generated_subnetwork_builders\n      }\n      rebuild_subnetwork_builders = []\n      for name in names:\n        if name not in subnetwork_builder_names:\n          raise ValueError(\n              ""Required subnetwork builder is missing for iteration {}: {}""\n              .format(iteration_number, name))\n        rebuild_subnetwork_builders.append(subnetwork_builder_names[name])\n      previous_ensemble_summary = None\n      previous_ensemble_subnetwork_builders = None\n      if previous_ensemble_spec:\n        # Always skip summaries when rebuilding previous architecture,\n        # since they are not useful.\n        previous_ensemble_summary = self._summary_maker(\n            namespace=""ensemble"",\n            scope=previous_ensemble_spec.name,\n            skip_summary=True)\n        previous_ensemble_subnetwork_builders = (\n            previous_ensemble_spec.subnetwork_builders)\n      ensemble_candidates = []\n      for ensemble_strategy in self._ensemble_strategies:\n        ensemble_candidates += ensemble_strategy.generate_ensemble_candidates(\n            rebuild_subnetwork_builders, previous_ensemble_subnetwork_builders)\n      ensemble_candidate = self._find_ensemble_candidate(\n          architecture.ensemble_candidate_name, ensemble_candidates)\n      current_iteration = self._iteration_builder.build_iteration(\n          base_global_step=architecture.global_step,\n          iteration_number=iteration_number,\n          ensemble_candidates=[ensemble_candidate],\n          subnetwork_builders=rebuild_subnetwork_builders,\n          features=features,\n          labels=labels,\n          mode=mode,\n          config=config,\n          previous_ensemble_summary=previous_ensemble_summary,\n          rebuilding=True,\n          rebuilding_ensembler_name=architecture.ensembler_name,\n          previous_iteration=current_iteration)\n      max_candidates = 2 if previous_ensemble_spec else 1\n      assert len(current_iteration.candidates) == max_candidates\n      previous_ensemble_spec = current_iteration.candidates[-1].ensemble_spec\n      previous_ensemble = previous_ensemble_spec.ensemble\n    previous_ensemble_spec.architecture.set_replay_indices(\n        architecture.replay_indices)\n    return current_iteration\n\n  def _collate_subnetwork_reports(self, iteration_number):\n    """"""Prepares subnetwork.Reports to be passed to Generator.\n\n    Reads subnetwork.MaterializedReports from past iterations,\n    collates those that were included in previous_ensemble into\n    previous_ensemble_reports as a List of subnetwork.MaterializedReports,\n    and collates all reports from previous iterations into all_reports as\n    another List of subnetwork.MaterializedReports.\n\n    Args:\n      iteration_number: Python integer AdaNet iteration number, starting from 0.\n\n    Returns:\n      (previous_ensemble_reports: List<subnetwork.MaterializedReport>,\n       materialized_reports: List<MaterializedReport>)\n    """"""\n\n    materialized_reports_all = (self._report_accessor.read_iteration_reports())\n    previous_ensemble_reports = []\n    all_reports = []\n\n    # Since the number of iteration reports changes after the\n    # MATERIALIZE_REPORT phase, we need to make sure that we always pass the\n    # same reports to the Generator in the same iteration,\n    # otherwise the graph that is built in the FREEZE_ENSEMBLE phase would be\n    # different from the graph built in the training phase.\n\n    # Iteration 0 should have 0 iteration reports passed to the\n    #   Generator, since there are no previous iterations.\n    # Iteration 1 should have 1 list of reports for Builders\n    #   generated in iteration 0.\n    # Iteration 2 should have 2 lists of reports -- one for iteration 0,\n    #   one for iteration 1. Note that the list of reports for iteration >= 1\n    #   should contain ""previous_ensemble"", in addition to the\n    #   Builders at the start of that iteration.\n    # Iteration t should have t lists of reports.\n\n    for i, iteration_reports in enumerate(materialized_reports_all):\n\n      # This ensures that the FREEZE_ENSEMBLE phase does not pass the reports\n      # generated in the previous phase of the same iteration to the\n      # Generator when building the graph.\n      if i >= iteration_number:\n        break\n\n      chosen_subnetworks_in_this_iteration = [\n          subnetwork_report for subnetwork_report in iteration_reports\n          if subnetwork_report.included_in_final_ensemble\n      ]\n      previous_ensemble_reports += chosen_subnetworks_in_this_iteration\n      all_reports.extend(iteration_reports)\n\n    return previous_ensemble_reports, all_reports\n\n  def _train_op(self, iteration_estimator_spec, is_growing_phase):\n    """"""Returns the iteration train op or tf.no_op if growing the graph.""""""\n\n    train_op = iteration_estimator_spec.train_op\n    if is_growing_phase:\n      train_op = tf_compat.v1.train.get_global_step().assign_add(1)\n      # NOTE: some version of TensorFlow check that train_op is an Op or Tensor\n      # and crash if train_op is a Variable.\n      train_op = tf.identity(train_op)\n    return train_op\n\n  def _create_estimator_spec(self, current_iteration, mode,\n                             iteration_number_tensor, previous_iteration_vars,\n                             is_growing_phase, evaluation_name):\n    """"""Creates the EstimatorSpec which will be returned by _adanet_model_fn.""""""\n\n    from tensorflow.python.training.tracking import graph_view  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    training = mode == tf.estimator.ModeKeys.TRAIN\n    iteration_estimator_spec = current_iteration.estimator_spec\n    training_chief_hooks = self._training_chief_hooks(current_iteration,\n                                                      training)\n    training_hooks = self._training_hooks(current_iteration, training,\n                                          iteration_number_tensor,\n                                          previous_iteration_vars,\n                                          is_growing_phase)\n    if is_growing_phase:\n      training_chief_hooks = self._process_hooks_for_growing_phase(\n          training_chief_hooks)\n      training_hooks = self._process_hooks_for_growing_phase(training_hooks)\n\n    saver = None\n    if self._enable_v2_checkpoint:\n      saver = tf_compat.v1.train.Saver(\n          var_list=graph_view.ObjectGraphView(\n              current_iteration.checkpoint).frozen_saveable_objects(),\n          sharded=True,\n          max_to_keep=self.config.keep_checkpoint_max)\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=iteration_estimator_spec.predictions,\n        loss=iteration_estimator_spec.loss,\n        train_op=self._train_op(iteration_estimator_spec, is_growing_phase),\n        eval_metric_ops=iteration_estimator_spec.eval_metric_ops,\n        training_chief_hooks=training_chief_hooks,\n        training_hooks=training_hooks,\n        evaluation_hooks=self._evaluation_hooks(current_iteration, training,\n                                                evaluation_name),\n        scaffold=tf_compat.v1.train.Scaffold(\n            summary_op=tf.constant(""""),\n            saver=saver,\n            local_init_op=current_iteration.estimator_spec.scaffold\n            .local_init_op if isinstance(current_iteration.estimator_spec,\n                                         tf.estimator.EstimatorSpec) else None),\n        export_outputs=iteration_estimator_spec.export_outputs)\n\n  def _call_generate_candidates(self, previous_ensemble, iteration_number,\n                                previous_ensemble_reports, all_reports, config):\n    # Calling low level getargs for py_2_and_3 compatibility.\n    defined_args = inspect.getargs(\n        self._subnetwork_generator.generate_candidates.__code__).args\n    generate_args = dict(\n        previous_ensemble=previous_ensemble,\n        iteration_number=iteration_number,\n        previous_ensemble_reports=previous_ensemble_reports,\n        all_reports=all_reports)\n    if ""config"" in defined_args:\n      generate_args[""config""] = config\n    return self._subnetwork_generator.generate_candidates(**generate_args)\n\n  def _create_iteration(self,\n                        features,\n                        labels,\n                        mode,\n                        config,\n                        is_growing_phase,\n                        checkpoint_path,\n                        hooks,\n                        best_ensemble_index_override=None):\n    """"""Constructs the TF ops and variables for the current iteration.\n\n    Args:\n      features: Dictionary of `Tensor` objects keyed by feature name.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head). Can be `None`.\n      mode: Defines whether this is training, evaluation or prediction. See\n        `ModeKeys`.\n      config: The current `tf.estimator.RunConfig`.\n      is_growing_phase: Whether we are in the AdaNet graph growing phase.\n      checkpoint_path: Path of the checkpoint to use. When `None`, this method\n        uses the latest checkpoint instead.\n      hooks: A list of `tf.estimator.SessionRunHooks`.\n      best_ensemble_index_override: Integer index to identify the latest\n        iteration\'s best ensemble candidate instead of computing the best\n        ensemble index dynamically conditional on the ensemble AdaNet losses.\n\n    Returns:\n      A two-tuple of the current `_Iteration`, and list of variables from\n        the previous iteration for restoring during the graph growing phase.\n    """"""\n\n    # Use the evaluation checkpoint path to get both the iteration number and\n    # variable values to avoid any race conditions between the first and second\n    # checkpoint reads.\n    iteration_number = self._checkpoint_iteration_number(checkpoint_path)\n\n    if mode == tf.estimator.ModeKeys.EVAL and checkpoint_path is None:\n      # This should only happen during some tests, so we log instead of\n      # asserting here.\n      logging.warning(""There are no checkpoints available during evaluation. ""\n                      ""Variables will be initialized to their defaults."")\n\n    if is_growing_phase:\n      assert mode == tf.estimator.ModeKeys.TRAIN\n      assert config.is_chief\n      iteration_number += 1\n\n    # Only record summaries when training.\n    skip_summaries = (mode != tf.estimator.ModeKeys.TRAIN or is_growing_phase)\n    base_global_step = 0\n    with tf_compat.v1.variable_scope(""adanet""):\n      previous_iteration = None\n      previous_ensemble_spec = None\n      previous_ensemble = None\n      previous_ensemble_summary = None\n      previous_ensemble_subnetwork_builders = None\n      architecture = None\n      for i in range(iteration_number):\n        architecture_filename = self._architecture_filename(i)\n        if not tf.io.gfile.exists(architecture_filename):\n          continue\n        architecture = self._read_architecture(architecture_filename)\n        logging.info(\n            ""Importing architecture from %s: [%s]."", architecture_filename,\n            "", "".join(\n                sorted([\n                    ""\'{}:{}\'"".format(t, n)\n                    for t, n in architecture.subnetworks_grouped_by_iteration\n                ])))\n        base_global_step = architecture.global_step\n        previous_iteration = self._architecture_ensemble_spec(\n            architecture, i, features, mode, labels, previous_ensemble_spec,\n            config, previous_iteration, hooks)\n        previous_ensemble_spec = previous_iteration.candidates[-1].ensemble_spec\n        previous_ensemble = previous_ensemble_spec.ensemble\n        previous_ensemble_summary = self._summary_maker(\n            namespace=""ensemble"",\n            scope=previous_ensemble_spec.name,\n            skip_summary=skip_summaries)\n        previous_ensemble_subnetwork_builders = (\n            previous_ensemble_spec.subnetwork_builders)\n      previous_iteration_vars = None\n      if is_growing_phase:\n        # Keep track of the previous iteration variables so we can restore them\n        # from the previous checkpoint after growing the graph. After this line,\n        # any variables created will not have a matching one in the checkpoint\n        # until it gets overwritten.\n        # Note: It\'s not possible to just create a tf.train.Saver here since\n        # this code is also run on TPU, which does not support creating Savers\n        # inside model_fn.\n        previous_iteration_vars = (\n            tf_compat.v1.get_collection(tf_compat.v1.GraphKeys.GLOBAL_VARIABLES)\n            + tf_compat.v1.get_collection(\n                tf_compat.v1.GraphKeys.SAVEABLE_OBJECTS))\n      previous_ensemble_reports, all_reports = [], []\n      if self._report_materializer:\n        previous_ensemble_reports, all_reports = (\n            self._collate_subnetwork_reports(iteration_number))\n\n      subnetwork_builders = self._call_generate_candidates(\n          previous_ensemble=previous_ensemble,\n          iteration_number=iteration_number,\n          previous_ensemble_reports=previous_ensemble_reports,\n          all_reports=all_reports,\n          config=config)\n      ensemble_candidates = []\n      for ensemble_strategy in self._ensemble_strategies:\n        ensemble_candidates += ensemble_strategy.generate_ensemble_candidates(\n            subnetwork_builders, previous_ensemble_subnetwork_builders)\n      current_iteration = self._iteration_builder.build_iteration(\n          base_global_step=base_global_step,\n          iteration_number=iteration_number,\n          ensemble_candidates=ensemble_candidates,\n          subnetwork_builders=subnetwork_builders,\n          features=features,\n          labels=labels,\n          mode=mode,\n          config=config,\n          previous_ensemble_summary=previous_ensemble_summary,\n          best_ensemble_index_override=best_ensemble_index_override,\n          previous_iteration=previous_iteration)\n    return current_iteration, previous_iteration_vars\n\n  def _create_model_fn(self,\n                       is_growing_phase=False,\n                       is_inside_training_loop=False,\n                       is_export=False,\n                       evaluation_name=None,\n                       best_ensemble_index=None,\n                       checkpoint_path=None,\n                       hooks=None):\n    """"""Creates the AdaNet model_fn.\n\n    Args:\n      is_growing_phase: Whether the model_fn will be called in the growing\n        phase.\n      is_inside_training_loop: Whether the model_fn will be called inside the\n        AdaNet training loop.\n      is_export: Whether the model_fn will be called from functions which export\n        a SavedModel.\n      evaluation_name: String name to append to the eval directory.\n      best_ensemble_index: The index of the best performing ensemble in the\n        latest AdaNet iteration.\n      checkpoint_path: The checkpoint path from which to restore variables.\n      hooks: Extra hooks to use when creating the graph.\n\n    Returns:\n      The adanet_model_fn which will create the computation graph when called.\n    """"""\n\n    del is_export  # Unused.\n\n    def _adanet_model_fn(features, labels, mode, params, config):\n      """"""AdaNet model_fn.\n\n      Args:\n        features: Dictionary of `Tensor` objects keyed by feature name.\n        labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n          (for multi-head). Can be `None`.\n        mode: Defines whether this is training, evaluation or prediction. See\n          `ModeKeys`.\n        params: A dict of parameters.\n        config: The current `tf.estimator.RunConfig`.\n\n      Returns:\n        A `EstimatorSpec` instance.\n\n      Raises:\n        UserWarning: When calling model_fn directly in TRAIN mode.\n      """"""\n\n      del params  # Unused.\n\n      path = checkpoint_path or tf.train.latest_checkpoint(self.model_dir)\n\n      training = mode == tf.estimator.ModeKeys.TRAIN\n      if training and not is_inside_training_loop:\n        raise UserWarning(\n            ""The adanet.Estimator\'s model_fn should not be called directly in ""\n            ""TRAIN mode, because its behavior is undefined outside the context ""\n            ""of its `train` method. If you are trying to add custom metrics ""\n            ""with `tf.contrib.estimator.add_metrics`, pass the `metric_fn` to ""\n            ""this `Estimator\'s` constructor instead."")\n\n      current_iteration, previous_iteration_vars = self._create_iteration(\n          features,\n          labels,\n          mode,\n          config,\n          is_growing_phase,\n          checkpoint_path=path,\n          hooks=hooks,\n          best_ensemble_index_override=best_ensemble_index)\n\n      # Variable which allows us to read the current iteration from a\n      # checkpoint. This must be created here so it is available when calling\n      # _execute_bookkeeping_phase after the first iteration.\n      iteration_number_tensor = None\n      if not self._enable_v2_checkpoint:\n        iteration_number_tensor = tf_compat.v1.get_variable(\n            self._Keys.CURRENT_ITERATION,\n            shape=[],\n            dtype=tf.int64,\n            initializer=tf_compat.v1.zeros_initializer(),\n            trainable=False)\n\n      return self._create_estimator_spec(\n          current_iteration,\n          mode,\n          iteration_number_tensor,\n          previous_iteration_vars,\n          is_growing_phase,\n          evaluation_name=evaluation_name)\n\n    return _adanet_model_fn\n'"
adanet/core/estimator_distributed_test.py,3,"b'""""""Test AdaNet estimator cluster training support.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport itertools\nimport json\nimport os\nimport shutil\nimport socket\nimport subprocess\nimport sys\nimport time\n\nfrom absl import flags\nfrom absl import logging\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.timer import _CountDownTimer\nimport tensorflow.compat.v2 as tf\n\n# A process. name is a string identifying the process in logs. stderr is a file\n# object of the process\'s stderr.\n_ProcessInfo = collections.namedtuple(""_ProcessInfo"",\n                                      [""name"", ""popen"", ""stderr""])\n\n\ndef _create_task_process(task_type, task_index, estimator_type,\n                         placement_strategy, tf_config, model_dir):\n  """"""Creates a process for a single estimator task.\n\n  Args:\n    task_type: \'chief\', \'worker\' or \'ps\'.\n    task_index: The index of the task within the cluster.\n    estimator_type: The estimator type to train. \'estimator\' or \'autoensemble\'.\n    placement_strategy: The distributed placement strategy.\n    tf_config: Dictionary representation of the TF_CONFIG environment variable.\n      This method creates a copy as to not mutate the input dict.\n    model_dir: The Estimator\'s model directory.\n\n  Returns:\n    A _ProcessInfo namedtuple of the running process. The stderr field of this\n      tuple must be closed by the caller once the process ends.\n  """"""\n\n  process_name = ""%s_%s"" % (task_type, task_index)\n  args = [""python"", ""adanet/core/estimator_distributed_test_runner.py""]\n  args.append(""--estimator_type={}"".format(estimator_type))\n  args.append(""--placement_strategy={}"".format(placement_strategy))\n  # Log everything to stderr.\n  args.append(""--stderrthreshold=info"")\n  args.append(""--model_dir={}"".format(model_dir))\n  logging.info(""Spawning %s process: %s"", process_name, "" "".join(args))\n  stderr_filename = os.path.join(model_dir, ""%s_stderr.txt"" % process_name)\n  logging.info(""Logging to %s"", model_dir)\n  stderr_file = open(stderr_filename, ""w+"")\n  tf_config = copy.deepcopy(tf_config)\n  tf_config[""task""][""type""] = task_type\n  tf_config[""task""][""index""] = task_index\n  json_tf_config = json.dumps(tf_config)\n  env = os.environ.copy()\n  # Allow stderr to be viewed before the process ends.\n  env[""PYTHONUNBUFFERED""] = ""1""\n  env[""TF_CPP_MIN_LOG_LEVEL""] = ""0""\n  env[""TF_CONFIG""] = json_tf_config\n  # Change gRPC polling strategy to prevent blocking forever.\n  # See https://github.com/tensorflow/tensorflow/issues/17852.\n  env[""GRPC_POLL_STRATEGY""] = ""poll""\n  popen = subprocess.Popen(args, stderr=stderr_file, env=env)\n  return _ProcessInfo(process_name, popen, stderr_file)\n\n\ndef _pick_unused_port():\n  """"""Returns a free port on localhost.""""""\n\n  for family in (socket.AF_INET6, socket.AF_INET):\n    try:\n      sock = socket.socket(family, socket.SOCK_STREAM)\n      sock.bind(("""", 0))  # Passing port \'0\' binds to a free port on localhost.\n      port = sock.getsockname()[1]\n      sock.close()\n      return port\n    except socket.error:\n      continue\n  raise socket.error\n\n\ndef log_all(process, status):\n  """"""Logs full text to INFO without truncating.""""""\n\n  logging.info(""Logging STDERR for %s process %s"", status, process.name)\n  logging.info(""===================== BEGIN %s LOG ====================="",\n               process.name)\n  process.stderr.seek(0)\n  for line in process.stderr:\n    logging.info(""FROM %s: %s"", process.name, line)\n  logging.info(""====================== END %s LOG ======================"",\n               process.name)\n\n\nclass EstimatorDistributedTrainingTest(parameterized.TestCase,\n                                       tf.test.TestCase):\n  """"""Tests distributed training.""""""\n\n  def setUp(self):\n    super(EstimatorDistributedTrainingTest, self).setUp()\n    flags.FLAGS(sys.argv)\n    # Setup and cleanup test directory.\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def _wait_for_processes(self, wait_processes, kill_processes, timeout_secs):\n    """"""Waits until all `wait_processes` finish, then kills `kill_processes`.\n\n    Fails an assert if a process in `wait_processes` finishes unsuccessfully.\n    The processes in `kill_processes` are assumed to never finish so they are\n    killed.\n\n    Args:\n      wait_processes: A list of _ProcessInfo tuples. This function will wait for\n        each to finish.\n      kill_processes: A list of _ProcessInfo tuples. Each will be killed once\n        every process in `wait_processes` is finished.\n      timeout_secs: Seconds to wait before timing out and terminating processes.\n\n    Returns:\n      A list of strings, each which is a string of the stderr of a wait process.\n\n    Raises:\n      Exception: When waiting for tasks to finish times out.\n    """"""\n\n    timer = _CountDownTimer(timeout_secs)\n    finished_wait_processes = set()\n    poll_count = {wait_process: 0.0 for wait_process in wait_processes}\n\n    while len(finished_wait_processes) < len(wait_processes):\n      if timer.secs_remaining() == 0:\n        logging.error(""Timed out! Outputting logs of unfinished processes:"")\n        for i, wait_process in enumerate(wait_processes):\n          if i in finished_wait_processes:\n            continue\n          log_all(wait_process, ""incompleted"")\n        raise Exception(""Timed out waiting for tasks to complete."")\n      for i, wait_process in enumerate(wait_processes):\n        if i in finished_wait_processes:\n          continue\n        ret_code = wait_process.popen.poll()\n        if ret_code is None:\n          poll_count[wait_process] += 0.25\n          if ((poll_count[wait_process] / 10.) -\n              int(poll_count[wait_process] / 10.)) == 0:\n            logging.info(""%d secs has elapsed for %s"", poll_count[wait_process],\n                         wait_process.name)\n          continue\n        logging.info(""%s finished"", wait_process.name)\n        log_all(wait_process, ""completed"")\n        self.assertEqual(0, ret_code)\n        finished_wait_processes.add(i)\n      for kill_process in kill_processes:\n        ret_code = kill_process.popen.poll()\n        # Kill processes should not end until we kill them.\n        # If it returns early, note the return code.\n        if ret_code is not None:\n          logging.error(""kill process %s ended with ret_code %d"",\n                        kill_process.name, ret_code)\n          log_all(kill_process, ""ended with code {}"".format(ret_code))\n          self.assertIsNone(ret_code)\n      # Delay between polling loops.\n      time.sleep(0.25)\n    logging.info(""All wait processes finished"")\n    for i, kill_process in enumerate(kill_processes):\n      # Kill each kill process.\n      kill_process.popen.kill()\n      kill_process.popen.wait()\n      log_all(kill_process, ""killed"")\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.named_parameters(\n      itertools.chain(*[\n          [\n              {\n                  ""testcase_name"": ""{}_one_worker"".format(placement),\n                  ""placement_strategy"": placement,\n                  ""num_workers"": 1,\n                  ""num_ps"": 0,\n              },\n              {\n                  ""testcase_name"": ""{}_one_worker_one_ps"".format(placement),\n                  ""placement_strategy"": placement,\n                  ""num_workers"": 1,\n                  ""num_ps"": 1,\n              },\n              {\n                  ""testcase_name"": ""{}_two_workers_one_ps"".format(placement),\n                  ""placement_strategy"": placement,\n                  ""num_workers"": 2,\n                  ""num_ps"": 1,\n              },\n              {\n                  ""testcase_name"":\n                      ""{}_three_workers_three_ps"".format(placement),\n                  ""placement_strategy"":\n                      placement,\n                  ""num_workers"":\n                      3,\n                  ""num_ps"":\n                      3,\n              },\n              {\n                  ""testcase_name"": ""{}_five_workers_three_ps"".format(placement),\n                  ""placement_strategy"": placement,\n                  ""num_workers"": 5,\n                  ""num_ps"": 3,\n              },\n              {\n                  ""testcase_name"":\n                      ""autoensemble_{}_five_workers_three_ps"".format(placement),\n                  ""estimator"":\n                      ""autoensemble"",\n                  ""placement_strategy"":\n                      placement,\n                  ""num_workers"":\n                      5,\n                  ""num_ps"":\n                      3,\n              },\n              # TODO: Need to restore boosted trees support.\n              # {\n              #     ""testcase_name"":\n              #         ""autoensemble_trees_multiclass_{}_five_workers_three_ps""\n              #         .format(placement),\n              #     ""estimator"":\n              #         ""autoensemble_trees_multiclass"",\n              #     ""placement_strategy"":\n              #         placement,\n              #     ""num_workers"":\n              #         5,\n              #     ""num_ps"":\n              #         3,\n              # },\n              {\n                  ""testcase_name"":\n                      ""estimator_with_experimental_multiworker_{}_five_workers""\n                      .format(placement),\n                  ""estimator"":\n                      ""estimator_with_experimental_multiworker_strategy"",\n                  ""placement_strategy"":\n                      placement,\n                  ""num_workers"":\n                      5,\n                  # Multiworker strategy means that all workers hold a copy of\n                  # the variables, and there are no parameter servers.\n                  ""num_ps"":\n                      0,\n              },\n          ] for placement in [""replication"", ""round_robin""]\n      ]))\n  # pylint: enable=g-complex-comprehension\n  # TODO: Test distributed training in TF 2.\n  @tf_compat.skip_for_tf2\n  def test_distributed_training(self,\n                                num_workers,\n                                num_ps,\n                                placement_strategy,\n                                estimator=""estimator""):\n    """"""Uses multiprocessing to simulate a distributed training environment.""""""\n\n    # Inspired by `tf.test.create_local_cluster`.\n    worker_ports = [_pick_unused_port() for _ in range(num_workers)]\n    ps_ports = [_pick_unused_port() for _ in range(num_ps)]\n    ws_targets = [""localhost:%s"" % port for port in worker_ports]\n    ps_targets = [""localhost:%s"" % port for port in ps_ports]\n\n    # For details see:\n    # https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\n    tf_config = {\n        ""cluster"": {\n            # The chief is always worker 0.\n            ""chief"": [ws_targets[0]],\n        },\n        ""task"": {\n            ""type"": ""chief"",\n            ""index"": 0\n        },\n    }\n\n    # The chief is already worker 0.\n    if len(ws_targets) > 1:\n      tf_config[""cluster""][""worker""] = ws_targets[1:]\n    if ps_targets:\n      tf_config[""cluster""][""ps""] = ps_targets\n\n    worker_processes = []\n    ps_processes = []\n    evaluator_processes = []\n\n    model_dir = self.test_subdirectory\n\n    # Chief\n    worker_processes.append(\n        _create_task_process(""chief"", 0, estimator, placement_strategy,\n                             tf_config, model_dir))\n    # Workers\n    for i in range(len(ws_targets[1:])):\n      worker_processes.append(\n          _create_task_process(""worker"", i, estimator, placement_strategy,\n                               tf_config, model_dir))\n    # Parameter Servers (PS)\n    for i in range(len(ps_targets)):\n      ps_processes.append(\n          _create_task_process(""ps"", i, estimator, placement_strategy,\n                               tf_config, model_dir))\n    # Evaluator\n    evaluator_processes.append(\n        _create_task_process(""evaluator"", 0, estimator, placement_strategy,\n                             tf_config, model_dir))\n\n    # Run processes.\n    try:\n      # NOTE: Parameter servers do not shut down on their own.\n      self._wait_for_processes(\n          worker_processes + evaluator_processes,\n          kill_processes=ps_processes,\n          timeout_secs=500)\n    finally:\n      for process in worker_processes + ps_processes + evaluator_processes:\n        try:\n          process.popen.kill()\n        except OSError:\n          pass  # It\'s OK (and expected) if the process already exited.\n        process.stderr.close()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/estimator_distributed_test_runner.py,40,"b'# List as: python2, python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Used to run estimators for distributed tests.\n\nIn distributed tests, we spawn processes to run estimator tasks like chief,\nworkers, parameter servers. The role of each task is determined by the TF_CONFIG\nenvironment variable.\n\nFor more information on how tf.estimator.RunConfig uses TF_CONFIG, see\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport json\nimport os\nimport sys\n\n# Allow this file to import adanet.\nsys.path.insert(\n    0, os.path.join(os.path.dirname(os.path.realpath(__file__)), ""../..""))\n\n# pylint: disable=g-import-not-at-top\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom adanet import tf_compat\nfrom adanet.autoensemble.estimator import AutoEnsembleEstimator\nfrom adanet.core.estimator import Estimator\nfrom adanet.core.evaluator import Evaluator\nfrom adanet.distributed.placement import RoundRobinStrategy\nfrom adanet.subnetwork import Builder\nfrom adanet.subnetwork import SimpleGenerator\nfrom adanet.subnetwork import Subnetwork\n# TODO: Switch back to TF 2.0 once the distribution bug is fixed.\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-direct-tensorflow-import\n\n# Contrib\ntry:\n  from tensorflow.contrib.boosted_trees.python.utils import losses as bt_losses\nexcept ImportError:\n  # Not much we can do here except skip the test.\n  bt_losses = None\n\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.training import session_manager as session_manager_lib\nfrom tensorflow_estimator.python.estimator import training as training_lib\nfrom tensorflow_estimator.python.estimator.canned import head as head_lib\n# pylint: enable=g-import-not-at-top\n# pylint: enable=g-direct-tensorflow-import\n\nflags.DEFINE_enum(""estimator_type"", ""estimator"", [\n    ""estimator"", ""autoensemble"", ""autoensemble_trees_multiclass"",\n    ""estimator_with_experimental_multiworker_strategy""\n], ""The estimator type to train."")\n\nflags.DEFINE_enum(""placement_strategy"", ""replication"", [\n    ""replication"",\n    ""round_robin"",\n], ""The distributed placement strategy."")\n\nflags.DEFINE_string(""model_dir"", """", ""The model directory."")\n\nFLAGS = flags.FLAGS\n\n\nclass SessionManager(session_manager_lib.SessionManager):\n  """"""A session manager with a shorter recovery time.""""""\n\n  def __init__(self, *args, **kwargs):\n    # Reduced wait time.\n    kwargs[""recovery_wait_secs""] = .5\n    super(SessionManager, self).__init__(*args, **kwargs)\n\n\n@contextlib.contextmanager\ndef _monkey_patch_distributed_training_times():\n  """"""Monkey-patches global attributes with subnetwork-specifics ones.""""""\n\n  old_delay_secs_per_worker = training_lib._DELAY_SECS_PER_WORKER  # pylint: disable=protected-access\n  old_session_manager = session_manager_lib.SessionManager\n  old_min_max_variable_partitioner = (\n      partitioned_variables.min_max_variable_partitioner)\n\n  # monkey-patch global attributes.\n  session_manager_lib.SessionManager = SessionManager\n  # Override default delay per worker to speed up tests.\n  training_lib._DELAY_SECS_PER_WORKER = .2  # pylint: disable=protected-access\n\n  # NOTE: DNNEstimator uses min-max partitioner under the hood which will not\n  # partition layers unless they are above a certain size. In order to test that\n  # we handle partitioned variables correctly in distributed training we patch\n  # the min size to be significantly lower. For more context, see b/133435012\n  # and b/136958627. For some reason, creating a custom DNN using a fixed\n  # partitioner does not cause the issues described in the bugs so we must test\n  # DNNEstimator.\n  def patched_min_max_variable_partitioner(max_partitions=1,\n                                           axis=0,\n                                           min_slice_size=64,\n                                           bytes_per_string_element=16):\n    del min_slice_size  # Unused, min_slice_size is patched to be constant.\n    return old_min_max_variable_partitioner(\n        max_partitions=max_partitions,\n        axis=axis,\n        min_slice_size=64,\n        bytes_per_string_element=bytes_per_string_element)\n\n  partitioned_variables.min_max_variable_partitioner = (\n      patched_min_max_variable_partitioner)\n\n  try:\n    yield\n  finally:\n    # Revert monkey-patches.\n    session_manager_lib.SessionManager = old_session_manager\n    training_lib._DELAY_SECS_PER_WORKER = old_delay_secs_per_worker  # pylint: disable=protected-access\n    partitioned_variables.min_max_variable_partitioner = (\n        old_min_max_variable_partitioner)\n\n\nclass _DNNBuilder(Builder):\n  """"""A simple DNN subnetwork builder.""""""\n\n  def __init__(self, name, config, layer_size=3, seed=13):\n    self._name = name\n    self._layer_size = layer_size\n    self._config = config\n    self._seed = seed\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    seed = self._seed\n    if previous_ensemble:\n      # Increment seed so different iterations don\'t learn the exact same thing.\n      seed += 1\n    num_ps_replicas = self._config.num_ps_replicas if self._config else 0\n    partitioner = tf_compat.v1.min_max_variable_partitioner(\n        max_partitions=num_ps_replicas)\n    with tf_compat.v1.variable_scope(""dnn"", partitioner=partitioner):\n      shared = {}\n      with tf_compat.v1.variable_scope(""hidden_layer""):\n        w = tf_compat.v1.get_variable(\n            shape=[2, self._layer_size],\n            initializer=tf_compat.v1.glorot_uniform_initializer(seed=seed),\n            name=""weight"")\n        hidden_layer = tf.matmul(features[""x""], w)\n\n      if previous_ensemble:\n        other_hidden_layer = previous_ensemble.weighted_subnetworks[\n            -1].subnetwork.shared[""hidden_layer""]\n        hidden_layer = tf.concat([hidden_layer, other_hidden_layer], axis=1)\n\n      # Use a leaky-relu activation so that gradients can flow even when\n      # outputs are negative. Leaky relu has a non-zero slope when x < 0.\n      # Otherwise success at learning is completely dependent on random seed.\n      hidden_layer = tf.nn.leaky_relu(hidden_layer, alpha=.2)\n      shared[""hidden_layer""] = hidden_layer\n\n      with tf_compat.v1.variable_scope(""logits""):\n        logits = tf_compat.v1.layers.dense(\n            hidden_layer,\n            logits_dimension,\n            kernel_initializer=tf_compat.v1.glorot_uniform_initializer(\n                seed=seed))\n\n      summary.scalar(""scalar"", 3)\n\n      return Subnetwork(\n          last_layer=logits, logits=logits, complexity=3, shared=shared)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    optimizer = tf_compat.v1.train.AdamOptimizer(learning_rate=.001)\n    return optimizer.minimize(loss, var_list=var_list)\n\n\ndef train_and_evaluate_estimator():\n  """"""Runs Estimator distributed training.""""""\n\n  # The tf.estimator.RunConfig automatically parses the TF_CONFIG environment\n  # variables during construction.\n  # For more information on how tf.estimator.RunConfig uses TF_CONFIG, see\n  # https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig.\n  config = tf.estimator.RunConfig(\n      tf_random_seed=42,\n      save_checkpoints_steps=10,\n      save_checkpoints_secs=None,\n      # Keep all checkpoints to avoid checkpoint GC causing failures during\n      # evaluation.\n      # TODO: Prevent checkpoints that are currently being\n      # evaluated by another process from being garbage collected.\n      keep_checkpoint_max=None,\n      model_dir=FLAGS.model_dir,\n      session_config=tf_compat.v1.ConfigProto(\n          log_device_placement=False,\n          # Ignore other workers; only talk to parameter servers.\n          # Otherwise, when a chief/worker terminates, the others will hang.\n          device_filters=[""/job:ps""]))\n\n  def input_fn():\n    input_features = {""x"": tf.constant(features, name=""x"")}\n    input_labels = tf.constant(labels, name=""y"")\n    return tf.data.Dataset.from_tensors((input_features, input_labels)).repeat()\n\n  kwargs = {\n      ""max_iteration_steps"": 100,\n      ""force_grow"": True,\n      ""delay_secs_per_worker"": .2,\n      ""max_worker_delay_secs"": 1,\n      ""worker_wait_secs"": 1,\n      # Set low timeout to reduce wait time for failures.\n      ""worker_wait_timeout_secs"": 180,\n      ""evaluator"": Evaluator(input_fn, steps=10),\n      ""config"": config\n  }\n\n  head = head_lib._regression_head(  # pylint: disable=protected-access\n      loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n  features = [[1., 0.], [0., 0], [0., 1.], [1., 1.]]\n  labels = [[1.], [0.], [1.], [0.]]\n\n  estimator_type = FLAGS.estimator_type\n  if FLAGS.placement_strategy == ""round_robin"":\n    kwargs[""experimental_placement_strategy""] = RoundRobinStrategy()\n  if estimator_type == ""autoensemble"":\n    feature_columns = [tf.feature_column.numeric_column(""x"", shape=[2])]\n    # pylint: disable=g-long-lambda\n    # TODO: Switch optimizers to tf.keras.optimizers.Adam once the\n    # distribution bug is fixed.\n    candidate_pool = {\n        ""linear"":\n            tf.estimator.LinearEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=lambda: tf_compat.v1.train.AdamOptimizer(\n                    learning_rate=.001)),\n        ""dnn"":\n            tf.estimator.DNNEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=lambda: tf_compat.v1.train.AdamOptimizer(\n                    learning_rate=.001),\n                hidden_units=[3]),\n        ""dnn2"":\n            tf.estimator.DNNEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=lambda: tf_compat.v1.train.AdamOptimizer(\n                    learning_rate=.001),\n                hidden_units=[10, 10]),\n    }\n    # pylint: enable=g-long-lambda\n\n    estimator = AutoEnsembleEstimator(\n        head=head, candidate_pool=candidate_pool, **kwargs)\n  elif estimator_type == ""estimator"":\n    subnetwork_generator = SimpleGenerator([\n        _DNNBuilder(""dnn1"", config, layer_size=3),\n        _DNNBuilder(""dnn2"", config, layer_size=4),\n        _DNNBuilder(""dnn3"", config, layer_size=5),\n    ])\n\n    estimator = Estimator(\n        head=head, subnetwork_generator=subnetwork_generator, **kwargs)\n  elif FLAGS.estimator_type == ""autoensemble_trees_multiclass"":\n    if not bt_losses:\n      logging.warning(\n          ""Skipped autoensemble_trees_multiclass test since contrib is missing.""\n      )\n      return\n    n_classes = 3\n    head = head_lib._multi_class_head_with_softmax_cross_entropy_loss(  # pylint: disable=protected-access\n        n_classes=n_classes,\n        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n    def tree_loss_fn(labels, logits):\n      result = bt_losses.per_example_maxent_loss(\n          labels=labels, logits=logits, num_classes=n_classes, weights=None)\n      return result[0]\n\n    tree_head = head_lib._multi_class_head_with_softmax_cross_entropy_loss(  # pylint: disable=protected-access\n        loss_fn=tree_loss_fn,\n        n_classes=n_classes,\n        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n    labels = [[1], [0], [1], [2]]\n    feature_columns = [tf.feature_column.numeric_column(""x"", shape=[2])]\n    # TODO: Switch optimizers to tf.keras.optimizers.Adam once the\n    # distribution bug is fixed.\n    candidate_pool = lambda config: {  # pylint: disable=g-long-lambda\n        ""linear"":\n            tf.estimator.LinearEstimator(\n                head=head,\n                feature_columns=feature_columns,\n                optimizer=tf_compat.v1.train.AdamOptimizer(\n                    learning_rate=.001),\n                config=config),\n        ""gbdt"":\n            tf.estimator.BoostedTreesEstimator(\n                head=tree_head,\n                feature_columns=feature_columns,\n                n_trees=10,\n                n_batches_per_layer=1,\n                center_bias=False,\n                config=config),\n    }\n\n    estimator = AutoEnsembleEstimator(\n        head=head, candidate_pool=candidate_pool, **kwargs)\n\n  elif estimator_type == ""estimator_with_experimental_multiworker_strategy"":\n\n    def _model_fn(features, labels, mode):\n      """"""Test model_fn.""""""\n      layer = tf.keras.layers.Dense(1)\n      logits = layer(features[""x""])\n\n      if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions = {""logits"": logits}\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n      loss = tf.losses.mean_squared_error(\n          labels=labels,\n          predictions=logits,\n          reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n      if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(0.2)\n        train_op = optimizer.minimize(\n            loss, global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    if json.loads(os.environ[""TF_CONFIG""])[""task""][""type""] == ""evaluator"":\n      # The evaluator job would crash if MultiWorkerMirroredStrategy is called.\n      distribution = None\n    else:\n      distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n    multiworker_config = tf.estimator.RunConfig(\n        tf_random_seed=42,\n        model_dir=FLAGS.model_dir,\n        train_distribute=distribution,\n        session_config=tf_compat.v1.ConfigProto(log_device_placement=False))\n    # TODO: Replace with adanet.Estimator. Currently this just verifies\n    # that the distributed testing framework supports distribute strategies.\n    estimator = tf.estimator.Estimator(\n        model_fn=_model_fn, config=multiworker_config)\n\n  train_hooks = [\n      tf.estimator.ProfilerHook(save_steps=50, output_dir=FLAGS.model_dir)\n  ]\n  # Train for three iterations.\n  train_spec = tf.estimator.TrainSpec(\n      input_fn=input_fn, max_steps=300, hooks=train_hooks)\n  eval_spec = tf.estimator.EvalSpec(\n      input_fn=input_fn, steps=1, start_delay_secs=.5, throttle_secs=.05)\n\n  # Calling train_and_evaluate is the official way to perform distributed\n  # training with an Estimator. Calling Estimator#train directly results\n  # in an error when the TF_CONFIG is setup for a cluster.\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n\ndef main(argv):\n  del argv  # Unused.\n\n  # Reduce hard-coded waits, delays, and timeouts for quicker tests.\n  with _monkey_patch_distributed_training_times():\n    train_and_evaluate_estimator()\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
adanet/core/estimator_test.py,116,"b'""""""Test AdaNet estimator single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport time\n\nfrom absl import logging\nfrom absl.testing import parameterized\nfrom adanet import replay\nfrom adanet import tf_compat\nfrom adanet.core import testing_utils as tu\nfrom adanet.core.estimator import Estimator\nfrom adanet.core.evaluator import Evaluator\nfrom adanet.core.report_materializer import ReportMaterializer\nfrom adanet.distributed.placement import RoundRobinStrategy\nfrom adanet.ensemble import AllStrategy\nfrom adanet.ensemble import ComplexityRegularizedEnsembler\nfrom adanet.ensemble import GrowStrategy\nfrom adanet.ensemble import MixtureWeightType\nfrom adanet.ensemble import SoloStrategy\nfrom adanet.subnetwork import Builder\nfrom adanet.subnetwork import Generator\nfrom adanet.subnetwork import MaterializedReport\nfrom adanet.subnetwork import Report\nfrom adanet.subnetwork import SimpleGenerator\nfrom adanet.subnetwork import Subnetwork\nfrom adanet.subnetwork import TrainOpSpec\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.tools import saved_model_utils\n# pylint: enable=g-direct-tensorflow-import\nfrom tensorflow_estimator.python.estimator.canned.head import _binary_logistic_head_with_sigmoid_cross_entropy_loss as binary_class_head_v1\nfrom tensorflow_estimator.python.estimator.export import export\nfrom tensorflow_estimator.python.estimator.head import binary_class_head\nfrom tensorflow_estimator.python.estimator.head import multi_head as multi_head_lib\nfrom tensorflow_estimator.python.estimator.head import regression_head\n\nlogging.set_verbosity(logging.INFO)\n\nXOR_FEATURES = [[1., 0.], [0., 0], [0., 1.], [1., 1.]]\nXOR_LABELS = [[1.], [0.], [1.], [0.]]\n\n\nclass _DNNBuilder(Builder):\n  """"""A simple DNN subnetwork builder.""""""\n\n  def __init__(self,\n               name,\n               learning_rate=.001,\n               mixture_weight_learning_rate=.001,\n               return_penultimate_layer=True,\n               layer_size=1,\n               subnetwork_chief_hooks=None,\n               subnetwork_hooks=None,\n               mixture_weight_chief_hooks=None,\n               mixture_weight_hooks=None,\n               seed=13):\n    self._name = name\n    self._learning_rate = learning_rate\n    self._mixture_weight_learning_rate = mixture_weight_learning_rate\n    self._return_penultimate_layer = return_penultimate_layer\n    self._layer_size = layer_size\n    self._subnetwork_chief_hooks = subnetwork_chief_hooks\n    self._subnetwork_hooks = subnetwork_hooks\n    self._mixture_weight_chief_hooks = mixture_weight_chief_hooks\n    self._mixture_weight_hooks = mixture_weight_hooks\n    self._seed = seed\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    seed = self._seed\n    if previous_ensemble:\n      # Increment seed so different iterations don\'t learn the exact same thing.\n      seed += 1\n    with tf_compat.v1.variable_scope(""dnn""):\n      persisted_tensors = {}\n      with tf_compat.v1.variable_scope(""hidden_layer""):\n        w = tf_compat.v1.get_variable(\n            shape=[2, self._layer_size],\n            initializer=tf_compat.v1.glorot_uniform_initializer(seed=seed),\n            name=""weight"")\n        disjoint_op = tf.constant([1], name=""disjoint_op"")\n        with tf_compat.v1.colocate_with(disjoint_op):  # tests b/118865235\n          hidden_layer = tf.matmul(features[""x""], w)\n\n      if previous_ensemble:\n        other_hidden_layer = previous_ensemble.weighted_subnetworks[\n            -1].subnetwork.persisted_tensors[""hidden_layer""]\n        hidden_layer = tf.concat([hidden_layer, other_hidden_layer], axis=1)\n\n      # Use a leaky-relu activation so that gradients can flow even when\n      # outputs are negative. Leaky relu has a non-zero slope when x < 0.\n      # Otherwise success at learning is completely dependent on random seed.\n      hidden_layer = tf.nn.leaky_relu(hidden_layer, alpha=.2)\n      persisted_tensors[""hidden_layer""] = hidden_layer\n      if training:\n        # This change will only be in the next iteration if\n        # `freeze_training_graph` is `True`.\n        persisted_tensors[""hidden_layer""] = 2 * hidden_layer\n\n    last_layer = hidden_layer\n\n    with tf_compat.v1.variable_scope(""logits""):\n      logits = tf_compat.v1.layers.dense(\n          hidden_layer,\n          logits_dimension,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer(seed=seed))\n\n    summary.scalar(""scalar"", 3)\n    batch_size = features[""x""].get_shape().as_list()[0]\n    summary.image(""image"", tf.ones([batch_size, 3, 3, 1]))\n    with tf_compat.v1.variable_scope(""nested""):\n      summary.scalar(""scalar"", 5)\n\n    return Subnetwork(\n        last_layer=last_layer if self._return_penultimate_layer else logits,\n        logits=logits,\n        complexity=3,\n        persisted_tensors=persisted_tensors,\n        shared=persisted_tensors)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(\n        learning_rate=self._learning_rate)\n    train_op = optimizer.minimize(loss, var_list=var_list)\n    if not self._subnetwork_hooks:\n      return train_op\n    return TrainOpSpec(train_op, self._subnetwork_chief_hooks,\n                       self._subnetwork_hooks)\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(\n        learning_rate=self._mixture_weight_learning_rate)\n    train_op = optimizer.minimize(loss, var_list=var_list)\n    if not self._mixture_weight_hooks:\n      return train_op\n    return TrainOpSpec(train_op, self._mixture_weight_chief_hooks,\n                       self._mixture_weight_hooks)\n\n  def build_subnetwork_report(self):\n    return Report(\n        hparams={""layer_size"": self._layer_size},\n        attributes={""complexity"": tf.constant(3, dtype=tf.int32)},\n        metrics={\n            ""moo"": (tf.constant(3,\n                                dtype=tf.int32), tf.constant(3, dtype=tf.int32))\n        })\n\n\nclass _SimpleBuilder(Builder):\n  """"""A simple subnetwork builder that takes feature_columns.""""""\n\n  def __init__(self, name, feature_columns, seed=42):\n    self._name = name\n    self._feature_columns = feature_columns\n    self._seed = seed\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    seed = self._seed\n    if previous_ensemble:\n      # Increment seed so different iterations don\'t learn the exact same thing.\n      seed += 1\n\n    with tf_compat.v1.variable_scope(""simple""):\n      input_layer = tf_compat.v1.feature_column.input_layer(\n          features=features, feature_columns=self._feature_columns)\n      last_layer = input_layer\n\n    with tf_compat.v1.variable_scope(""logits""):\n      logits = tf_compat.v1.layers.dense(\n          last_layer,\n          logits_dimension,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer(seed=seed))\n\n    return Subnetwork(\n        last_layer=last_layer,\n        logits=logits,\n        complexity=1,\n        persisted_tensors={},\n    )\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(learning_rate=.001)\n    return optimizer.minimize(loss, var_list=var_list)\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(learning_rate=.001)\n    return optimizer.minimize(loss, var_list=var_list)\n\n\nclass _NanLossBuilder(Builder):\n  """"""A subnetwork builder always produces a NaN loss.""""""\n\n  @property\n  def name(self):\n    return ""nan""\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    logits = tf_compat.v1.layers.dense(\n        features[""x""],\n        logits_dimension,\n        kernel_initializer=tf_compat.v1.glorot_uniform_initializer(\n            seed=42)) * np.nan\n    return Subnetwork(last_layer=logits, logits=logits, complexity=0)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    return tf.no_op()\n\n\nclass _FrozenLinearBuilder(Builder):\n  """"""A simple linear subnetwork builder that doesn\'t train.""""""\n\n  def __init__(self, name, seed=42):\n    self._name = name\n    self._seed = seed\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n\n    logits = tf_compat.v1.layers.dense(\n        features[""x""],\n        logits_dimension,\n        kernel_initializer=tf_compat.v1.glorot_uniform_initializer(\n            seed=self._seed))\n\n    return Subnetwork(\n        last_layer=features[""x""],\n        logits=logits,\n        complexity=1,\n        persisted_tensors={},\n    )\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    return tf.no_op()\n\n\nclass _FakeGenerator(Generator):\n  """"""Generator that exposed generate_candidates\' arguments.""""""\n\n  def __init__(self, spy_fn, subnetwork_builders):\n    """"""Checks the arguments passed to generate_candidates.\n\n    Args:\n      spy_fn: (iteration_number, previous_ensemble_reports, all_reports) -> ().\n        Spies on the arguments passed to generate_candidates whenever it is\n        called.\n      subnetwork_builders: List of `Builder`s to return in every call to\n        generate_candidates.\n    """"""\n\n    self._spy_fn = spy_fn\n    self._subnetwork_builders = subnetwork_builders\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports):\n    """"""Spys on arguments passed in, then returns a fixed list of candidates.""""""\n\n    del previous_ensemble  # unused\n    self._spy_fn(iteration_number, previous_ensemble_reports, all_reports)\n    return self._subnetwork_builders\n\n\nclass _WidthLimitingDNNBuilder(_DNNBuilder):\n  """"""Limits the width of the previous_ensemble.""""""\n\n  def __init__(self,\n               name,\n               learning_rate=.001,\n               mixture_weight_learning_rate=.001,\n               return_penultimate_layer=True,\n               layer_size=1,\n               width_limit=None,\n               seed=13):\n    if width_limit is not None and width_limit == 0:\n      raise ValueError(""width_limit must be at least 1 or None."")\n\n    super(_WidthLimitingDNNBuilder,\n          self).__init__(name, learning_rate, mixture_weight_learning_rate,\n                         return_penultimate_layer, layer_size, seed)\n    self._width_limit = width_limit\n\n  def prune_previous_ensemble(self, previous_ensemble):\n    indices = list(range(len(previous_ensemble.weighted_subnetworks)))\n    if self._width_limit is None:\n      return indices\n    if self._width_limit == 1:\n      return []\n    return indices[-self._width_limit + 1:]  # pylint: disable=invalid-unary-operand-type\n\n\nclass _FakeEvaluator(object):\n  """"""Fakes an `adanet.Evaluator`.""""""\n\n  def __init__(self, input_fn):\n    self._input_fn = input_fn\n\n  @property\n  def input_fn(self):\n    """"""Return the input_fn.""""""\n    return self._input_fn\n\n  @property\n  def steps(self):\n    """"""Return the number of evaluation steps.""""""\n    return 1\n\n  @property\n  def metric_name(self):\n    """"""Returns the name of the metric being optimized.""""""\n    return ""adanet_loss""\n\n  @property\n  def objective_fn(self):\n    """"""Always returns the minimize objective.""""""\n    return np.nanargmin\n\n  def evaluate(self, sess, ensemble_metrics):\n    """"""Abstract method to be overridden in subclasses.""""""\n\n    del sess, ensemble_metrics  # Unused.\n    raise NotImplementedError\n\n\nclass _AlwaysLastEvaluator(_FakeEvaluator):\n\n  def evaluate(self, sess, ensemble_metrics):\n    """"""Always makes the last loss the smallest.""""""\n\n    del sess  # Unused.\n\n    losses = [np.inf] * len(ensemble_metrics)\n    losses[-1] = 0.\n    return losses\n\n\nclass _AlwaysSecondToLastEvaluator(_FakeEvaluator):\n\n  def evaluate(self, sess, ensemble_metrics):\n    """"""Always makes the second to last loss the smallest.""""""\n\n    del sess  # Unused.\n\n    losses = [np.inf] * len(ensemble_metrics)\n    losses[-2] = 0.\n    return losses\n\n\nclass _EarlyStoppingHook(tf_compat.SessionRunHook):\n  """"""Hook that immediately requests training to stop.""""""\n\n  def after_run(self, run_context, run_values):\n    run_context.request_stop()\n\n\nclass EstimatorTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""one_step"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 1,\n          ""steps"": 1,\n          ""max_steps"": None,\n          ""want_loss"": 0.49899703,\n          ""want_iteration"": 0,\n          ""want_global_step"": 1,\n      },\n      {\n          ""testcase_name"": ""enable_v2_checkpoint"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 100,\n          ""steps"": 300,\n          ""max_steps"": None,\n          ""want_loss"": 0.3221922,\n          ""want_iteration"": 2,\n          ""want_global_step"": 300,\n          ""enable_v2_checkpoint"": True,\n      },\n      {\n          ""testcase_name"": ""none_max_iteration_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": None,\n          ""steps"": 300,\n          ""max_steps"": None,\n          ""want_loss"": 0.32487726,\n          ""want_iteration"": 0,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""single_builder_max_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 200,\n          ""max_steps"": 300,\n          ""want_loss"": 0.32420248,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""single_builder_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 200,\n          ""steps"": 300,\n          ""max_steps"": None,\n          ""want_loss"": 0.32420248,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""single_builder_two_max_iteration_fewer_max_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 200,\n          ""max_iterations"": 2,\n          ""max_steps"": 300,\n          ""want_loss"": 0.32420248,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""single_builder_no_bias"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 200,\n          ""use_bias"": False,\n          ""want_loss"": 0.496736,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"":\n              ""single_builder_subnetwork_hooks"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(\n                      ""dnn"",\n                      subnetwork_chief_hooks=[\n                          tu.ModifierSessionRunHook(""chief_hook_var"")\n                      ],\n                      subnetwork_hooks=[tu.ModifierSessionRunHook(""hook_var"")])\n              ]),\n          ""max_iteration_steps"":\n              200,\n          ""use_bias"":\n              False,\n          ""want_loss"":\n              0.496736,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""single_builder_mixture_weight_hooks"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(\n                      ""dnn"",\n                      mixture_weight_chief_hooks=[\n                          tu.ModifierSessionRunHook(""chief_hook_var"")\n                      ],\n                      mixture_weight_hooks=[\n                          tu.ModifierSessionRunHook(""hook_var"")\n                      ])\n              ]),\n          ""max_iteration_steps"":\n              200,\n          ""use_bias"":\n              False,\n          ""want_loss"":\n              0.496736,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""single_builder_scalar_mixture_weight"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn"", return_penultimate_layer=False)]),\n          ""max_iteration_steps"":\n              200,\n          ""mixture_weight_type"":\n              MixtureWeightType.SCALAR,\n          ""want_loss"":\n              0.32317898,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""single_builder_vector_mixture_weight"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn"", return_penultimate_layer=False)]),\n          ""max_iteration_steps"":\n              200,\n          ""mixture_weight_type"":\n              MixtureWeightType.VECTOR,\n          ""want_loss"":\n              0.32317898,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"": ""single_builder_replicate_ensemble_in_training"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""replicate_ensemble_in_training"": True,\n          ""max_iteration_steps"": 200,\n          ""max_steps"": 300,\n          ""want_loss"": 0.32420215,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""single_builder_with_hook"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 200,\n          ""hooks"": [tu.ModifierSessionRunHook()],\n          ""want_loss"": 0.32420248,\n          ""want_iteration"": 1,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"": ""high_max_iteration_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 500,\n          ""want_loss"": 0.32487726,\n          ""want_iteration"": 0,\n          ""want_global_step"": 300,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", seed=99)]),\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.27713922,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders_different_layer_sizes"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.29696745,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders_one_max_iteration_none_steps_and_none_max_steps"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              200,\n          ""max_iterations"":\n              1,\n          ""steps"":\n              None,\n          ""max_steps"":\n              None,\n          ""want_loss"":\n              0.35249719,\n          ""want_iteration"":\n              0,\n          ""want_global_step"":\n              200,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders_one_max_iteration_two_hundred_steps"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              200,\n          ""max_iterations"":\n              1,\n          ""steps"":\n              300,\n          ""max_steps"":\n              None,\n          ""want_loss"":\n              0.35249719,\n          ""want_iteration"":\n              0,\n          ""want_global_step"":\n              200,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders_two_max_iteration_none_steps_and_none_max_steps"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              200,\n          ""max_iterations"":\n              2,\n          ""steps"":\n              None,\n          ""max_steps"":\n              None,\n          ""want_loss"":\n              0.26503286,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              400,\n      },\n      {\n          ""testcase_name"":\n              ""two_builders_different_layer_sizes_three_iterations"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              100,\n          ""want_loss"":\n              0.26433355,\n          ""want_iteration"":\n              2,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""two_dnn_export_subnetworks"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""max_iteration_steps"":\n              100,\n          ""want_loss"":\n              0.26433355,\n          ""want_iteration"":\n              2,\n          ""want_global_step"":\n              300,\n          ""export_subnetworks"":\n              True,\n      },\n      {\n          ""testcase_name"":\n              ""width_limiting_builder_no_pruning"",\n          ""subnetwork_generator"":\n              SimpleGenerator([_WidthLimitingDNNBuilder(""no_pruning"")]),\n          ""max_iteration_steps"":\n              75,\n          ""want_loss"":\n              0.32001898,\n          ""want_iteration"":\n              3,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""width_limiting_builder_some_pruning"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_WidthLimitingDNNBuilder(""some_pruning"", width_limit=2)]),\n          ""max_iteration_steps"":\n              75,\n          ""want_loss"":\n              0.38592532,\n          ""want_iteration"":\n              3,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""width_limiting_builder_prune_all"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_WidthLimitingDNNBuilder(""prune_all"", width_limit=1)]),\n          ""max_iteration_steps"":\n              75,\n          ""want_loss"":\n              0.43161362,\n          ""want_iteration"":\n              3,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""width_limiting_builder_mixed"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _WidthLimitingDNNBuilder(""no_pruning""),\n                  _WidthLimitingDNNBuilder(""some_pruning"", width_limit=2),\n                  _WidthLimitingDNNBuilder(""prune_all"", width_limit=1)\n              ]),\n          ""max_iteration_steps"":\n              75,\n          ""want_loss"":\n              0.32001898,\n          ""want_iteration"":\n              3,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""evaluator_good_input"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""evaluator"":\n              Evaluator(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=3),\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.36189985,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""evaluator_bad_input"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""evaluator"":\n              Evaluator(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[1.]]), steps=3),\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.29696745,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""evaluator_always_last"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(""dnn""),\n                  _DNNBuilder(""dnn2"", layer_size=3),\n              ]),\n          ""evaluator"":\n              _AlwaysLastEvaluator(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]])),\n          ""max_iteration_steps"":\n              None,\n          ""want_loss"":\n              0.31389591,\n          ""want_iteration"":\n              0,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""evaluator_always_second_to_last"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(""dnn""),\n                  _DNNBuilder(""dnn2"", layer_size=3),\n              ]),\n          ""evaluator"":\n              _AlwaysSecondToLastEvaluator(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]])),\n          ""max_iteration_steps"":\n              None,\n          ""want_loss"":\n              0.32487726,\n          ""want_iteration"":\n              0,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""report_materializer"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""report_materializer"":\n              ReportMaterializer(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1),\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.29696745,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""all_strategy"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""ensemble_strategies"": [AllStrategy()],\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.29196805,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""all_strategy_multiple_ensemblers"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""ensemble_strategies"": [AllStrategy()],\n          ""ensemblers"": [\n              ComplexityRegularizedEnsembler(),\n              ComplexityRegularizedEnsembler(use_bias=True, name=""with_bias"")\n          ],\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.23053232,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""solo_strategy"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""ensemble_strategies"": [SoloStrategy()],\n          ""max_iteration_steps"":\n              200,\n          ""want_loss"":\n              0.35249719,\n          ""want_iteration"":\n              1,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""solo_strategy_three_iterations"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""ensemble_strategies"": [SoloStrategy()],\n          ""max_iteration_steps"":\n              100,\n          ""want_loss"":\n              0.36163166,\n          ""want_iteration"":\n              2,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""multi_ensemble_strategy"",\n          ""subnetwork_generator"":\n              SimpleGenerator(\n                  [_DNNBuilder(""dnn""),\n                   _DNNBuilder(""dnn2"", layer_size=3)]),\n          ""ensemble_strategies"":\n              [AllStrategy(), GrowStrategy(),\n               SoloStrategy()],\n          ""max_iteration_steps"":\n              100,\n          ""want_loss"":\n              0.24838975,\n          ""want_iteration"":\n              2,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""dataset_train_input_fn"",\n          ""subnetwork_generator"":\n              SimpleGenerator([_DNNBuilder(""dnn"")]),\n          # pylint: disable=g-long-lambda\n          ""train_input_fn"":\n              lambda: tf.data.Dataset.from_tensors(({\n                  ""x"": XOR_FEATURES\n              }, XOR_LABELS)).repeat(),\n          # pylint: enable=g-long-lambda\n          ""max_iteration_steps"":\n              100,\n          ""want_loss"":\n              0.32219219,\n          ""want_iteration"":\n              2,\n          ""want_global_step"":\n              300,\n      },\n      {\n          ""testcase_name"":\n              ""early_stopping_subnetwork"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(""dnn""),\n                  _DNNBuilder(""dnn2"", subnetwork_hooks=[_EarlyStoppingHook()])\n              ]),\n          ""max_iteration_steps"":\n              100,\n          ""max_steps"":\n              200,\n          ""want_loss"":\n              0.2958503,\n          # Since one subnetwork stops after 1 step and global step is the\n          # mean of iteration steps, global step will be incremented at half\n          # the rate.\n          ""want_iteration"":\n              3,\n          ""want_global_step"":\n              200,\n      })\n  def test_lifecycle(self,\n                     subnetwork_generator,\n                     want_loss,\n                     want_iteration,\n                     want_global_step,\n                     max_iteration_steps,\n                     mixture_weight_type=MixtureWeightType.MATRIX,\n                     evaluator=None,\n                     use_bias=True,\n                     replicate_ensemble_in_training=False,\n                     hooks=None,\n                     ensemblers=None,\n                     ensemble_strategies=None,\n                     max_steps=300,\n                     steps=None,\n                     report_materializer=None,\n                     train_input_fn=None,\n                     max_iterations=None,\n                     export_subnetworks=False,\n                     enable_v2_checkpoint=False):\n    """"""Train entire estimator lifecycle using XOR dataset.""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n\n    def _metric_fn(predictions):\n      mean = tf.keras.metrics.Mean()\n      mean.update_state(predictions[""predictions""])\n      return {""keras_mean"": mean}\n\n    default_ensembler_kwargs = {\n        ""mixture_weight_type"": mixture_weight_type,\n        ""mixture_weight_initializer"": tf_compat.v1.zeros_initializer(),\n        ""warm_start_mixture_weights"": True,\n        ""use_bias"": use_bias,\n        ""enable_v2_checkpoint"": enable_v2_checkpoint,\n    }\n    if ensemblers:\n      default_ensembler_kwargs = {}\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=max_iteration_steps,\n        evaluator=evaluator,\n        ensemblers=ensemblers,\n        ensemble_strategies=ensemble_strategies,\n        report_materializer=report_materializer,\n        replicate_ensemble_in_training=replicate_ensemble_in_training,\n        metric_fn=_metric_fn,\n        model_dir=self.test_subdirectory,\n        config=run_config,\n        max_iterations=max_iterations,\n        export_subnetwork_logits=export_subnetworks,\n        export_subnetwork_last_layer=export_subnetworks,\n        **default_ensembler_kwargs)\n\n    if not train_input_fn:\n      train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    # Train.\n    estimator.train(\n        input_fn=train_input_fn, steps=steps, max_steps=max_steps, hooks=hooks)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(\n        input_fn=train_input_fn, steps=10, hooks=hooks)\n    logging.info(""%s"", eval_results)\n    self.assertAlmostEqual(want_loss, eval_results[""loss""], places=3)\n    self.assertEqual(want_global_step, eval_results[""global_step""])\n    self.assertEqual(want_iteration, eval_results[""iteration""])\n\n    # Predict.\n    predictions = estimator.predict(\n        input_fn=tu.dataset_input_fn(features=[0., 0.], labels=None))\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[""predictions""])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      return tf.estimator.export.ServingInputReceiver(\n          features={""x"": tf.constant([[0., 0.]], name=""serving_x"")},\n          receiver_tensors=serialized_example)\n\n    export_saved_model_fn = getattr(estimator, ""export_saved_model"", None)\n    if not callable(export_saved_model_fn):\n      export_saved_model_fn = estimator.export_savedmodel\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    export_saved_model_fn(\n        export_dir_base=export_dir_base,\n        serving_input_receiver_fn=serving_input_fn)\n    if export_subnetworks:\n      saved_model = saved_model_utils.read_saved_model(\n          os.path.join(export_dir_base,\n                       tf.io.gfile.listdir(export_dir_base)[0]))\n      export_signature_def = saved_model.meta_graphs[0].signature_def\n      self.assertIn(""subnetwork_logits"", export_signature_def.keys())\n      self.assertIn(""subnetwork_last_layer"", export_signature_def.keys())\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""hash_bucket_with_one_hot"",\n          ""feature_column"": (tf.feature_column.indicator_column(\n              categorical_column=(\n                  tf.feature_column.categorical_column_with_hash_bucket(\n                      key=""human_names"", hash_bucket_size=4, dtype=tf.string)))\n                            ),\n      }, {\n          ""testcase_name"":\n              ""vocab_list_with_one_hot"",\n          ""feature_column"": (tf.feature_column.indicator_column(\n              categorical_column=(\n                  tf.feature_column.categorical_column_with_vocabulary_list(\n                      key=""human_names"",\n                      vocabulary_list=[""alice"", ""bob""],\n                      dtype=tf.string)))),\n      }, {\n          ""testcase_name"":\n              ""hash_bucket_with_embedding"",\n          ""feature_column"": (tf.feature_column.embedding_column(\n              categorical_column=(\n                  tf.feature_column.categorical_column_with_hash_bucket(\n                      key=""human_names"", hash_bucket_size=4, dtype=tf.string)),\n              dimension=2)),\n      }, {\n          ""testcase_name"":\n              ""vocab_list_with_embedding"",\n          ""feature_column"": (tf.feature_column.embedding_column(\n              categorical_column=(\n                  tf.feature_column.categorical_column_with_vocabulary_list(\n                      key=""human_names"",\n                      vocabulary_list=[""alice"", ""bob""],\n                      dtype=tf.string)),\n              dimension=2)),\n      })\n  def test_categorical_columns(self, feature_column):\n\n    def train_input_fn():\n      input_features = {\n          ""human_names"": tf.constant([[""alice""], [""bob""]], name=""human_names"")\n      }\n      input_labels = tf.constant([[1.], [0.]], name=""starts_with_a"")\n      return input_features, input_labels\n\n    report_materializer = ReportMaterializer(input_fn=train_input_fn, steps=1)\n    estimator = Estimator(\n        head=regression_head.RegressionHead(),\n        subnetwork_generator=SimpleGenerator(\n            [_SimpleBuilder(name=""simple"", feature_columns=[feature_column])]),\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=1,\n        use_bias=True,\n        model_dir=self.test_subdirectory)\n\n    estimator.train(input_fn=train_input_fn, max_steps=3)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""no_subnetwork_generator"",\n          ""subnetwork_generator"": None,\n          ""max_iteration_steps"": 100,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""negative_max_iteration_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": -1,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""zero_max_iteration_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 0,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""negative_max_iterations"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 1,\n          ""max_iterations"": -1,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""zero_max_iterations"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 1,\n          ""max_iterations"": 0,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""steps_and_max_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 1,\n          ""steps"": 1,\n          ""max_steps"": 1,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""zero_steps"",\n          ""subnetwork_generator"": SimpleGenerator([_DNNBuilder(""dnn"")]),\n          ""max_iteration_steps"": 1,\n          ""steps"": 0,\n          ""max_steps"": None,\n          ""want_error"": ValueError,\n      },\n      {\n          ""testcase_name"": ""nan_loss_builder"",\n          ""subnetwork_generator"": SimpleGenerator([_NanLossBuilder()]),\n          ""max_iteration_steps"": 1,\n          ""max_steps"": None,\n          ""want_error"": tf_compat.v1.estimator.NanLossDuringTrainingError,\n      },\n      {\n          ""testcase_name"":\n              ""nan_loss_builder_first"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _NanLossBuilder(),\n                  _DNNBuilder(""dnn""),\n              ]),\n          ""max_iteration_steps"":\n              1,\n          ""max_steps"":\n              None,\n          ""want_error"":\n              tf_compat.v1.estimator.NanLossDuringTrainingError,\n      },\n      {\n          ""testcase_name"":\n              ""nan_loss_builder_last"",\n          ""subnetwork_generator"":\n              SimpleGenerator([\n                  _DNNBuilder(""dnn""),\n                  _NanLossBuilder(),\n              ]),\n          ""max_iteration_steps"":\n              1,\n          ""max_steps"":\n              None,\n          ""want_error"":\n              tf_compat.v1.estimator.NanLossDuringTrainingError,\n      },\n  )\n  def test_train_error(self,\n                       subnetwork_generator,\n                       max_iteration_steps,\n                       want_error,\n                       steps=None,\n                       max_steps=10,\n                       max_iterations=None):\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    with self.assertRaises(want_error):\n      estimator = Estimator(\n          head=tu.head(),\n          subnetwork_generator=subnetwork_generator,\n          report_materializer=report_materializer,\n          mixture_weight_type=MixtureWeightType.MATRIX,\n          mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n          warm_start_mixture_weights=True,\n          max_iteration_steps=max_iteration_steps,\n          use_bias=True,\n          max_iterations=max_iterations,\n          model_dir=self.test_subdirectory)\n      train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n      estimator.train(input_fn=train_input_fn, steps=steps, max_steps=max_steps)\n\n  def test_binary_head_asserts_are_disabled(self):\n    """"""Tests b/140267630.""""""\n\n    subnetwork_generator = SimpleGenerator([\n        _DNNBuilder(""dnn""),\n        _NanLossBuilder(),\n    ])\n    estimator = Estimator(\n        head=binary_class_head_v1(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=10,\n        model_dir=self.test_subdirectory)\n    eval_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator.evaluate(input_fn=eval_input_fn, steps=1)\n\n\nclass KerasCNNBuilder(Builder):\n  """"""Builds a CNN subnetwork for AdaNet.""""""\n\n  def __init__(self, name, learning_rate, num_dense, units=3, seed=42):\n    """"""Initializes a `SimpleCNNBuilder`.\n\n    Args:\n      name: String name.\n      learning_rate: The float learning rate to use.\n      num_dense: Number of layers.\n      units: Units per layer.\n      seed: The random seed.\n\n    Returns:\n      An instance of `SimpleCNNBuilder`.\n    """"""\n\n    self._name = name\n    self._learning_rate = learning_rate\n    self._num_dense = num_dense\n    self._units = units\n    self._seed = seed\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    seed = self._seed\n    if previous_ensemble:\n      seed += len(previous_ensemble.weighted_subnetworks)\n    images = list(features.values())[0]\n    images = tf.reshape(images, [-1, 2, 2, 1])\n    kernel_initializer = tf_compat.v1.keras.initializers.he_normal(seed=seed)\n    x = images\n    x = tf.keras.layers.Conv2D(\n        filters=3,\n        kernel_size=1,\n        padding=""same"",\n        activation=""relu"",\n        kernel_initializer=kernel_initializer)(\n            x)\n    x = tf.keras.layers.MaxPool2D(pool_size=1, strides=1)(x)\n    x = tf.keras.layers.Flatten()(x)\n    for _ in range(self._num_dense):\n      x = tf_compat.v1.layers.Dense(\n          units=self._units,\n          activation=""relu"",\n          kernel_initializer=kernel_initializer)(\n              x)\n    logits = tf.keras.layers.Dense(\n        units=1, activation=None, kernel_initializer=kernel_initializer)(\n            x)\n    complexity = tf.constant(1)\n    return Subnetwork(\n        last_layer=x, logits=logits, complexity=complexity, shared={})\n\n  def build_subnetwork_train_op(self,\n                                subnetwork,\n                                loss,\n                                var_list,\n                                labels,\n                                iteration_step,\n                                summary,\n                                previous_ensemble=None):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(self._learning_rate)\n    return optimizer.minimize(loss=loss, var_list=var_list)\n\n  @property\n  def name(self):\n    return self._name\n\n\n# TODO: Test should be enabled when we support Keras layers.\n# class EstimatorKerasLayersTest(tu.AdanetTestCase):\n#\n#   def test_lifecycle(self):\n#     """"""Train entire estimator lifecycle using XOR dataset.""""""\n#\n#     run_config = tf.estimator.RunConfig(tf_random_seed=42)\n#     estimator = Estimator(\n#         head=tu.head(),\n#         subnetwork_generator=SimpleGenerator([\n#             KerasCNNBuilder(""cnn0"", learning_rate=.001, num_dense=1, units=3),\n#         ]),\n#         max_iteration_steps=100,\n#         evaluator=Evaluator(\n#             input_fn=tu.dummy_input_fn([[1., 1., .1, .1]], [[0.]]), steps=3),\n#         model_dir=self.test_subdirectory,\n#         force_grow=True,\n#         config=run_config)\n#\n#     xor_features = [[1., 0., 1., 0.], [0., 0., 0., 0.], [0., 1., 0., 1.],\n#                     [1., 1., 1., 1.]]\n#     xor_labels = [[1.], [0.], [1.], [0.]]\n#     train_input_fn = tu.dummy_input_fn(xor_features, xor_labels)\n#\n#     # Train.\n#     estimator.train(input_fn=train_input_fn, max_steps=300)\n#\n#     # Restore from checkpoint to check that variables match up.\n#     estimator.train(input_fn=train_input_fn, max_steps=1)\n#\n#     # Evaluate.\n#     eval_results = estimator.evaluate(input_fn=train_input_fn, steps=3)\n#     logging.info(""%s"", eval_results)\n#     want_loss = 0.164\n#     self.assertAlmostEqual(want_loss, eval_results[""loss""], places=3)\n#\n#     # Predict.\n#     predictions = estimator.predict(\n#         input_fn=tu.dataset_input_fn(features=[0., 0., 0., 0.], labels=None))\n#     for prediction in predictions:\n#       self.assertIsNotNone(prediction[""predictions""])\n#\n#     # Export SavedModel.\n#     def serving_input_fn():\n#       """"""Input fn for serving export, starting from serialized example.""""""\n#       serialized_example = tf_compat.v1.placeholder(\n#           dtype=tf.string, shape=(None), name=""serialized_example"")\n#       return tf.estimator.export.ServingInputReceiver(\n#           features={""x"": tf.constant([[0., 0., 0., 0.]], name=""serving_x"")},\n#           receiver_tensors=serialized_example)\n#\n#     estimator.export_saved_model(\n#         export_dir_base=self.test_subdirectory,\n#         serving_input_receiver_fn=serving_input_fn)\n\n\nclass MultiHeadBuilder(Builder):\n  """"""Builds a subnetwork for AdaNet that uses dict labels.""""""\n\n  def __init__(self, learning_rate=.001, split_logits=False, seed=42):\n    """"""Initializes a `LabelsDictBuilder`.\n\n    Args:\n      learning_rate: The float learning rate to use.\n      split_logits: Whether to return a dict of logits or a single concatenated\n        logits `Tensor`.\n      seed: The random seed.\n\n    Returns:\n      An instance of `MultiHeadBuilder`.\n    """"""\n    self._learning_rate = learning_rate\n    self._split_logits = split_logits\n    self._seed = seed\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    seed = self._seed\n    if previous_ensemble:\n      seed += len(previous_ensemble.weighted_subnetworks)\n    kernel_initializer = tf_compat.v1.keras.initializers.he_normal(seed=seed)\n    x = features[""x""]\n    logits = tf_compat.v1.layers.dense(\n        x,\n        units=logits_dimension,\n        activation=None,\n        kernel_initializer=kernel_initializer)\n    if self._split_logits:\n      # Return different logits, one for each head.\n      logits1, logits2 = tf.split(logits, [1, 1], 1)\n      logits = {\n          ""head1"": logits1,\n          ""head2"": logits2,\n      }\n\n    complexity = tf.constant(1)\n    return Subnetwork(\n        last_layer=logits,\n        logits=logits,\n        complexity=complexity,\n        persisted_tensors={})\n\n  def build_subnetwork_train_op(self,\n                                subnetwork,\n                                loss,\n                                var_list,\n                                labels,\n                                iteration_step,\n                                summary,\n                                previous_ensemble=None):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(self._learning_rate)\n    return optimizer.minimize(loss=loss, var_list=var_list)\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(self._learning_rate)\n    return optimizer.minimize(loss=loss, var_list=var_list)\n\n  @property\n  def name(self):\n    return ""multi_head""\n\n\nclass EstimatorMultiHeadTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""concatenated_logits"",\n          ""builders"": [MultiHeadBuilder()],\n          ""want_loss"": 3.218,\n      }, {\n          ""testcase_name"": ""split_logits_with_export_subnetworks"",\n          ""builders"": [MultiHeadBuilder(split_logits=True)],\n          ""want_loss"": 3.224,\n          ""export_subnetworks"": True,\n      }, {\n          ""testcase_name"": ""split_logits"",\n          ""builders"": [MultiHeadBuilder(split_logits=True)],\n          ""want_loss"": 3.224,\n      })\n  def test_lifecycle(self, builders, want_loss, export_subnetworks=False):\n    """"""Train entire estimator lifecycle using XOR dataset.""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n\n    xor_features = [[1., 0., 1., 0.], [0., 0., 0., 0.], [0., 1., 0., 1.],\n                    [1., 1., 1., 1.]]\n    xor_labels = [[1.], [0.], [1.], [0.]]\n\n    def train_input_fn():\n      return {\n          ""x"": tf.constant(xor_features)\n      }, {\n          ""head1"": tf.constant(xor_labels),\n          ""head2"": tf.constant(xor_labels)\n      }\n\n    estimator = Estimator(\n        head=multi_head_lib.MultiHead(heads=[\n            regression_head.RegressionHead(\n                name=""head1"", loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n            regression_head.RegressionHead(\n                name=""head2"", loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n        ]),\n        subnetwork_generator=SimpleGenerator(builders),\n        max_iteration_steps=3,\n        evaluator=Evaluator(input_fn=train_input_fn, steps=1),\n        model_dir=self.test_subdirectory,\n        config=run_config,\n        export_subnetwork_logits=export_subnetworks,\n        export_subnetwork_last_layer=export_subnetworks)\n\n    # Train.\n    estimator.train(input_fn=train_input_fn, max_steps=9)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=3)\n    self.assertAlmostEqual(want_loss, eval_results[""loss""], places=3)\n\n    # Predict.\n    predictions = estimator.predict(\n        input_fn=tu.dataset_input_fn(features=[0., 0., 0., 0.], labels=None))\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[(""head1"", ""predictions"")])\n      self.assertIsNotNone(prediction[(""head2"", ""predictions"")])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      return tf.estimator.export.ServingInputReceiver(\n          features={""x"": tf.constant([[0., 0., 0., 0.]], name=""serving_x"")},\n          receiver_tensors=serialized_example)\n\n    export_saved_model_fn = getattr(estimator, ""export_saved_model"", None)\n    if not callable(export_saved_model_fn):\n      export_saved_model_fn = estimator.export_savedmodel\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    export_saved_model_fn(\n        export_dir_base=export_dir_base,\n        serving_input_receiver_fn=serving_input_fn)\n    if export_subnetworks:\n      saved_model = saved_model_utils.read_saved_model(\n          os.path.join(export_dir_base,\n                       tf.io.gfile.listdir(export_dir_base)[0]))\n      export_signature_def = saved_model.meta_graphs[0].signature_def\n      self.assertIn(""subnetwork_logits_head1"", export_signature_def.keys())\n      self.assertIn(""subnetwork_logits_head2"", export_signature_def.keys())\n      self.assertIn(""subnetwork_last_layer_head1"", export_signature_def.keys())\n      self.assertIn(""subnetwork_last_layer_head2"", export_signature_def.keys())\n\n\nclass EstimatorCallingModelFnDirectlyTest(tu.AdanetTestCase):\n  """"""Tests b/112108745. Warn users not to call model_fn directly.""""""\n\n  def test_calling_model_fn_directly(self):\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        max_iteration_steps=3,\n        use_bias=True,\n        model_dir=self.test_subdirectory)\n    model_fn = estimator.model_fn\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    tf_compat.v1.train.create_global_step()\n    features, labels = train_input_fn()\n    with self.assertRaises(UserWarning):\n      model_fn(\n          features=features,\n          mode=tf.estimator.ModeKeys.TRAIN,\n          labels=labels,\n          config={})\n\n  def test_calling_model_fn_directly_for_predict(self):\n    with context.graph_mode():\n      subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n      report_materializer = ReportMaterializer(\n          input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n      estimator = Estimator(\n          head=tu.head(),\n          subnetwork_generator=subnetwork_generator,\n          report_materializer=report_materializer,\n          max_iteration_steps=3,\n          use_bias=True,\n          model_dir=self.test_subdirectory)\n      model_fn = estimator.model_fn\n      train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n      tf_compat.v1.train.create_global_step()\n      features, labels = train_input_fn()\n      model_fn(\n          features=features,\n          mode=tf.estimator.ModeKeys.PREDICT,\n          labels=labels,\n          config=tf.estimator.RunConfig(\n              save_checkpoints_steps=1,\n              keep_checkpoint_max=3,\n              model_dir=self.test_subdirectory,\n          ))\n\n\nclass EstimatorCheckpointTest(tu.AdanetTestCase):\n  """"""Tests estimator checkpoints.""""""\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""single_iteration"",\n          ""max_iteration_steps"": 3,\n          ""keep_checkpoint_max"": 3,\n          ""want_num_checkpoints"": 3,\n      }, {\n          ""testcase_name"": ""single_iteration_keep_one"",\n          ""max_iteration_steps"": 3,\n          ""keep_checkpoint_max"": 1,\n          ""want_num_checkpoints"": 1,\n      }, {\n          ""testcase_name"": ""three_iterations"",\n          ""max_iteration_steps"": 1,\n          ""keep_checkpoint_max"": 3,\n          ""want_num_checkpoints"": 3,\n      }, {\n          ""testcase_name"": ""three_iterations_keep_one"",\n          ""max_iteration_steps"": 1,\n          ""keep_checkpoint_max"": 1,\n          ""want_num_checkpoints"": 1,\n      })\n  def test_checkpoints(self,\n                       max_iteration_steps,\n                       keep_checkpoint_max,\n                       want_num_checkpoints,\n                       max_steps=3):\n    config = tf.estimator.RunConfig(\n        save_checkpoints_steps=1,\n        keep_checkpoint_max=keep_checkpoint_max,\n    )\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=max_iteration_steps,\n        use_bias=True,\n        config=config,\n        model_dir=self.test_subdirectory)\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator.train(input_fn=train_input_fn, max_steps=max_steps)\n\n    checkpoints = tf.io.gfile.glob(\n        os.path.join(self.test_subdirectory, ""*.meta""))\n    self.assertEqual(want_num_checkpoints, len(checkpoints))\n\n\ndef _check_eventfile_for_keyword(keyword, dir_):\n  """"""Checks event files for the keyword.""""""\n\n  tf_compat.v1.summary.FileWriterCache.clear()\n\n  if not tf.io.gfile.exists(dir_):\n    raise ValueError(""Directory \'{}\' not found."".format(dir_))\n\n  # Get last `Event` written.\n  filenames = os.path.join(dir_, ""events*"")\n  event_paths = tf.io.gfile.glob(filenames)\n  if not event_paths:\n    raise ValueError(""Path \'{}\' not found."".format(filenames))\n\n  for last_event in tf_compat.v1.train.summary_iterator(event_paths[-1]):\n    if last_event.summary is not None:\n      for value in last_event.summary.value:\n        if keyword == value.tag:\n          if value.HasField(""simple_value""):\n            return value.simple_value\n          if value.HasField(""image""):\n            return (value.image.height, value.image.width,\n                    value.image.colorspace)\n          if value.HasField(""tensor""):\n            return value.tensor.string_val\n\n  raise ValueError(""Keyword \'{}\' not found in path \'{}\'."".format(\n      keyword, filenames))\n\n\nclass _FakeMetric(object):\n  """"""A fake metric.""""""\n\n  def __init__(self, value, dtype):\n    self._value = value\n    self._dtype = dtype\n\n  def to_metric(self):\n    tensor = tf.convert_to_tensor(value=self._value, dtype=self._dtype)\n    return (tensor, tensor)\n\n\nclass _EvalMetricsHead(object):\n  """"""A fake head with the given evaluation metrics.""""""\n\n  def __init__(self, fake_metrics):\n    self._fake_metrics = fake_metrics\n\n  @property\n  def logits_dimension(self):\n    return 1\n\n  def create_estimator_spec(self,\n                            features,\n                            mode,\n                            logits,\n                            labels=None,\n                            train_op_fn=None):\n    del features  # Unused\n\n    metric_ops = None\n    if self._fake_metrics:\n      metric_ops = {}\n      for k, fake_metric in self._fake_metrics.items():\n        metric_ops[k] = fake_metric.to_metric()\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=logits,\n        loss=tf.reduce_mean(input_tensor=labels - logits),\n        eval_metric_ops=metric_ops,\n        train_op=train_op_fn(1))\n\n\ndef _mean_keras_metric(value):\n  """"""Returns the mean of given value as a Keras metric.""""""\n\n  mean = tf.keras.metrics.Mean()\n  mean.update_state(value)\n  return mean\n\n\nclass EstimatorSummaryWriterTest(tu.AdanetTestCase):\n  """"""Test that Tensorboard summaries get written correctly.""""""\n\n  @tf_compat.skip_for_tf2\n  def test_summaries(self):\n    """"""Tests that summaries are written to candidate directory.""""""\n\n    run_config = tf.estimator.RunConfig(\n        tf_random_seed=42, log_step_count_steps=2, save_summary_steps=2)\n    subnetwork_generator = SimpleGenerator(\n        [_DNNBuilder(""dnn"", mixture_weight_learning_rate=.001)])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=10,\n        use_bias=True,\n        config=run_config,\n        model_dir=self.test_subdirectory)\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator.train(input_fn=train_input_fn, max_steps=3)\n\n    ensemble_loss = 1.\n    self.assertAlmostEqual(\n        ensemble_loss,\n        _check_eventfile_for_keyword(""loss"", self.test_subdirectory),\n        places=3)\n    self.assertIsNotNone(\n        _check_eventfile_for_keyword(""global_step/sec"", self.test_subdirectory))\n    self.assertEqual(\n        0.,\n        _check_eventfile_for_keyword(""iteration/adanet/iteration"",\n                                     self.test_subdirectory))\n\n    subnetwork_subdir = os.path.join(self.test_subdirectory,\n                                     ""subnetwork/t0_dnn"")\n    self.assertAlmostEqual(\n        3., _check_eventfile_for_keyword(""scalar"", subnetwork_subdir), places=3)\n    self.assertEqual((3, 3, 1),\n                     _check_eventfile_for_keyword(""image/image/0"",\n                                                  subnetwork_subdir))\n    self.assertAlmostEqual(\n        5.,\n        _check_eventfile_for_keyword(""nested/scalar"", subnetwork_subdir),\n        places=3)\n\n    ensemble_subdir = os.path.join(\n        self.test_subdirectory, ""ensemble/t0_dnn_grow_complexity_regularized"")\n    self.assertAlmostEqual(\n        ensemble_loss,\n        _check_eventfile_for_keyword(\n            ""adanet_loss/adanet/adanet_weighted_ensemble"", ensemble_subdir),\n        places=3)\n    self.assertAlmostEqual(\n        0.,\n        _check_eventfile_for_keyword(\n            ""complexity_regularization/adanet/adanet_weighted_ensemble"",\n            ensemble_subdir),\n        places=3)\n    self.assertAlmostEqual(\n        0.,\n        _check_eventfile_for_keyword(\n            ""mixture_weight_norms/adanet/""\n            ""adanet_weighted_ensemble/subnetwork_0"", ensemble_subdir),\n        places=3)\n\n  @tf_compat.skip_for_tf2\n  def test_disable_summaries(self):\n    """"""Tests that summaries can be disabled for ensembles and subnetworks.""""""\n\n    run_config = tf.estimator.RunConfig(\n        tf_random_seed=42, log_step_count_steps=2, save_summary_steps=2)\n    subnetwork_generator = SimpleGenerator(\n        [_DNNBuilder(""dnn"", mixture_weight_learning_rate=.001)])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=10,\n        use_bias=True,\n        config=run_config,\n        model_dir=self.test_subdirectory,\n        enable_ensemble_summaries=False,\n        enable_subnetwork_summaries=False,\n    )\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator.train(input_fn=train_input_fn, max_steps=3)\n\n    ensemble_loss = 1.\n    self.assertAlmostEqual(\n        ensemble_loss,\n        _check_eventfile_for_keyword(""loss"", self.test_subdirectory),\n        places=3)\n    self.assertIsNotNone(\n        _check_eventfile_for_keyword(""global_step/sec"", self.test_subdirectory))\n    self.assertEqual(\n        0.,\n        _check_eventfile_for_keyword(""iteration/adanet/iteration"",\n                                     self.test_subdirectory))\n\n    subnetwork_subdir = os.path.join(self.test_subdirectory,\n                                     ""subnetwork/t0_dnn"")\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(""scalar"", subnetwork_subdir)\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(""image/image/0"", subnetwork_subdir)\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(""nested/scalar"", subnetwork_subdir)\n\n    ensemble_subdir = os.path.join(\n        self.test_subdirectory, ""ensemble/t0_dnn_grow_complexity_regularized"")\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(\n          ""adanet_loss/adanet/adanet_weighted_ensemble"", ensemble_subdir)\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(\n          ""complexity_regularization/adanet/adanet_weighted_ensemble"",\n          ensemble_subdir)\n    with self.assertRaises(ValueError):\n      _check_eventfile_for_keyword(\n          ""mixture_weight_norms/adanet/""\n          ""adanet_weighted_ensemble/subnetwork_0"", ensemble_subdir)\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""none_metrics"",\n          ""head"": _EvalMetricsHead(None),\n          ""want_summaries"": [],\n          ""want_loss"": -1.791,\n      }, {\n          ""testcase_name"":\n              ""metrics_fn"",\n          ""head"":\n              _EvalMetricsHead(None),\n          ""metric_fn"":\n              lambda predictions: {\n                  ""avg"": tf_compat.v1.metrics.mean(predictions)\n              },\n          ""want_summaries"": [""avg""],\n          ""want_loss"":\n              -1.791,\n      }, {\n          ""testcase_name"":\n              ""keras_metrics_fn"",\n          ""head"":\n              _EvalMetricsHead(None),\n          ""metric_fn"":\n              lambda predictions: {\n                  ""avg"": _mean_keras_metric(predictions)\n              },\n          ""want_summaries"": [""avg""],\n          ""want_loss"":\n              -1.791,\n      }, {\n          ""testcase_name"": ""empty_metrics"",\n          ""head"": _EvalMetricsHead({}),\n          ""want_summaries"": [],\n          ""want_loss"": -1.791,\n      }, {\n          ""testcase_name"":\n              ""evaluation_name"",\n          ""head"":\n              _EvalMetricsHead({}),\n          ""evaluation_name"":\n              ""continuous"",\n          ""want_summaries"": [],\n          ""want_loss"":\n              -1.791,\n          ""global_subdir"":\n              ""eval_continuous"",\n          ""subnetwork_subdir"":\n              ""subnetwork/t0_dnn/eval_continuous"",\n          ""ensemble_subdir"":\n              ""ensemble/t0_dnn_grow_complexity_regularized/eval_continuous"",\n      }, {\n          ""testcase_name"":\n              ""regression_head"",\n          ""head"":\n              regression_head.RegressionHead(\n                  loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n          ""want_summaries"": [""average_loss""],\n          ""want_loss"":\n              .256,\n      }, {\n          ""testcase_name"":\n              ""binary_classification_head"",\n          ""head"":\n              binary_class_head.BinaryClassHead(\n                  loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n          ""learning_rate"":\n              .6,\n          ""want_summaries"": [""average_loss"", ""accuracy"", ""recall""],\n          ""want_loss"":\n              0.122,\n      }, {\n          ""testcase_name"":\n              ""all_metrics"",\n          ""head"":\n              _EvalMetricsHead({\n                  ""float32"":\n                      _FakeMetric(1., tf.float32),\n                  ""float64"":\n                      _FakeMetric(1., tf.float64),\n                  ""serialized_summary"":\n                      _FakeMetric(\n                          tf_compat.v1.Summary(value=[\n                              tf_compat.v1.Summary.Value(\n                                  tag=""summary_tag"", simple_value=1.)\n                          ]).SerializeToString(), tf.string),\n              }),\n          ""want_summaries"": [\n              ""float32"",\n              ""float64"",\n              ""serialized_summary/0"",\n          ],\n          ""want_loss"":\n              -1.791,\n      })\n  # pylint: enable=g-long-lambda\n  def test_eval_metrics(\n      self,\n      head,\n      want_loss,\n      want_summaries,\n      evaluation_name=None,\n      metric_fn=None,\n      learning_rate=.01,\n      global_subdir=""eval"",\n      subnetwork_subdir=""subnetwork/t0_dnn/eval"",\n      ensemble_subdir=""ensemble/t0_dnn_grow_complexity_regularized/eval""):\n    """"""Test that AdaNet evaluation metrics get persisted correctly.""""""\n\n    seed = 42\n    run_config = tf.estimator.RunConfig(tf_random_seed=seed)\n    subnetwork_generator = SimpleGenerator([\n        _DNNBuilder(\n            ""dnn"",\n            learning_rate=learning_rate,\n            mixture_weight_learning_rate=0.,\n            layer_size=8,\n            seed=seed)\n    ])\n    estimator = Estimator(\n        head=head,\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=100,\n        metric_fn=metric_fn,\n        config=run_config,\n        model_dir=self.test_subdirectory)\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n    estimator.train(input_fn=train_input_fn, max_steps=100)\n\n    metrics = estimator.evaluate(\n        input_fn=train_input_fn, steps=1, name=evaluation_name)\n    self.assertAlmostEqual(want_loss, metrics[""loss""], places=3)\n\n    global_subdir = os.path.join(self.test_subdirectory, global_subdir)\n    subnetwork_subdir = os.path.join(self.test_subdirectory, subnetwork_subdir)\n    ensemble_subdir = os.path.join(self.test_subdirectory, ensemble_subdir)\n    self.assertAlmostEqual(\n        want_loss,\n        _check_eventfile_for_keyword(""loss"", subnetwork_subdir),\n        places=3)\n    for metric in want_summaries:\n      self.assertIsNotNone(\n          _check_eventfile_for_keyword(metric, subnetwork_subdir),\n          msg=""{} should be under \'eval\'."".format(metric))\n    for dir_ in [global_subdir, ensemble_subdir]:\n      self.assertAlmostEqual(metrics[""loss""],\n                             _check_eventfile_for_keyword(""loss"", dir_))\n      self.assertEqual([b""| dnn |""],\n                       _check_eventfile_for_keyword(\n                           ""architecture/adanet/ensembles/0"", dir_))\n      for metric in want_summaries:\n        self.assertTrue(\n            _check_eventfile_for_keyword(metric, dir_) > 0.,\n            msg=""{} should be under \'eval\'."".format(metric))\n\n\nclass EstimatorMembersOverrideTest(tu.AdanetTestCase):\n  """"""Tests b/77494544 fix.""""""\n\n  def test_assert_members_are_not_overridden(self):\n    """"""Assert that AdaNet estimator does not break other estimators.""""""\n\n    config = tf.estimator.RunConfig()\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    adanet = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=10,\n        use_bias=True,\n        config=config)\n    self.assertIsNotNone(adanet)\n    if hasattr(tf.estimator, ""LinearEstimator""):\n      estimator_fn = tf.estimator.LinearEstimator\n    else:\n      estimator_fn = tf.contrib.estimator.LinearEstimator\n    linear = estimator_fn(\n        head=tu.head(), feature_columns=[tf.feature_column.numeric_column(""x"")])\n    self.assertIsNotNone(linear)\n\n\ndef _dummy_feature_dict_input_fn(features, labels):\n  """"""Returns an input_fn that returns feature and labels `Tensors`.""""""\n\n  def _input_fn():\n    input_features = {}\n    for key, feature in features.items():\n      input_features[key] = tf.constant(feature, name=key)\n    input_labels = tf.constant(labels, name=""labels"")\n    return input_features, input_labels\n\n  return _input_fn\n\n\nclass EstimatorDifferentFeaturesPerModeTest(tu.AdanetTestCase):\n  """"""Tests b/109751254.""""""\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""extra_train_features"",\n          ""train_features"": {\n              ""x"": [[1., 0.]],\n              ""extra"": [[1., 0.]],\n          },\n          ""eval_features"": {\n              ""x"": [[1., 0.]],\n          },\n          ""predict_features"": {\n              ""x"": [[1., 0.]],\n          },\n      }, {\n          ""testcase_name"": ""extra_eval_features"",\n          ""train_features"": {\n              ""x"": [[1., 0.]],\n          },\n          ""eval_features"": {\n              ""x"": [[1., 0.]],\n              ""extra"": [[1., 0.]],\n          },\n          ""predict_features"": {\n              ""x"": [[1., 0.]],\n          },\n      }, {\n          ""testcase_name"": ""extra_predict_features"",\n          ""train_features"": {\n              ""x"": [[1., 0.]],\n          },\n          ""eval_features"": {\n              ""x"": [[1., 0.]],\n          },\n          ""predict_features"": {\n              ""x"": [[1., 0.]],\n              ""extra"": [[1., 0.]],\n          },\n      })\n  def test_different_features_per_mode(self, train_features, eval_features,\n                                       predict_features):\n    """"""Tests tests different numbers of features per mode.""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=1,\n        use_bias=True,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    labels = [[1.]]\n    train_input_fn = _dummy_feature_dict_input_fn(train_features, labels)\n\n    # Train.\n    estimator.train(input_fn=train_input_fn, max_steps=2)\n\n    # Evaluate.\n    eval_input_fn = _dummy_feature_dict_input_fn(eval_features, labels)\n    estimator.evaluate(input_fn=eval_input_fn, steps=1)\n\n    # Predict.\n    predict_input_fn = _dummy_feature_dict_input_fn(predict_features, None)\n    estimator.predict(input_fn=predict_input_fn)\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      features = {}\n      for key, value in predict_features.items():\n        features[key] = tf.constant(value)\n      return tf.estimator.export.ServingInputReceiver(\n          features=features, receiver_tensors=serialized_example)\n\n    export_saved_model_fn = getattr(estimator, ""export_saved_model"", None)\n    if not callable(export_saved_model_fn):\n      export_saved_model_fn = estimator.export_savedmodel\n    export_saved_model_fn(\n        export_dir_base=self.test_subdirectory,\n        serving_input_receiver_fn=serving_input_fn)\n\n\nclass EstimatorExportSavedModelTest(tu.AdanetTestCase):\n\n  def test_export_saved_model_for_predict(self):\n    """"""Tests SavedModel exporting functionality for predict (b/110435640).""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=1,\n        use_bias=True,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    features = {""x"": [[1., 0.]]}\n    labels = [[1.]]\n    train_input_fn = _dummy_feature_dict_input_fn(features, labels)\n\n    # Train.\n    estimator.train(input_fn=train_input_fn, max_steps=2)\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      for key, value in features.items():\n        features[key] = tf.constant(value)\n      return tf.estimator.export.ServingInputReceiver(\n          features=features, receiver_tensors=serialized_example)\n\n    estimator.export_saved_model(\n        export_dir_base=self.test_subdirectory,\n        serving_input_receiver_fn=serving_input_fn,\n        experimental_mode=tf.estimator.ModeKeys.PREDICT)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_export_saved_model_for_eval(self):\n    """"""Tests SavedModel exporting functionality for eval (b/110991908).""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator(\n        [_DNNBuilder(""dnn"", layer_size=8, learning_rate=1.)])\n    estimator = Estimator(\n        head=binary_class_head.BinaryClassHead(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=100,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    # Train.\n    estimator.train(input_fn=train_input_fn, max_steps=300)\n\n    metrics = estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertAlmostEqual(.067, metrics[""average_loss""], places=3)\n    self.assertAlmostEqual(1., metrics[""recall""], places=3)\n    self.assertAlmostEqual(1., metrics[""accuracy""], places=3)\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      return export.SupervisedInputReceiver(\n          features={""x"": tf.constant(XOR_FEATURES)},\n          labels=tf.constant(XOR_LABELS),\n          receiver_tensors=serialized_example)\n\n    export_dir_base = os.path.join(self.test_subdirectory, ""export"")\n    try:\n      estimator.export_saved_model(\n          export_dir_base=export_dir_base,\n          serving_input_receiver_fn=serving_input_fn,\n          experimental_mode=tf.estimator.ModeKeys.EVAL)\n    except AttributeError:\n      pass\n\n    try:\n      tf.contrib.estimator.export_saved_model_for_mode(\n          estimator,\n          export_dir_base=export_dir_base,\n          input_receiver_fn=serving_input_fn,\n          mode=tf.estimator.ModeKeys.EVAL)\n    except AttributeError:\n      pass\n\n    subdir = tf.io.gfile.listdir(export_dir_base)[0]\n\n    with context.graph_mode(), self.test_session() as sess:\n      meta_graph_def = tf_compat.v1.saved_model.loader.load(\n          sess, [""eval""], os.path.join(export_dir_base, subdir))\n      signature_def = meta_graph_def.signature_def.get(""eval"")\n\n      # Read zero metric.\n      self.assertAlmostEqual(\n          0.,\n          sess.run(\n              tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                  signature_def.outputs[""metrics/average_loss/value""])),\n          places=3)\n\n      # Run metric update op.\n      sess.run((tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n          signature_def.outputs[""metrics/average_loss/update_op""]),\n                tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                    signature_def.outputs[""metrics/accuracy/update_op""]),\n                tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                    signature_def.outputs[""metrics/recall/update_op""])))\n\n      # Read metric again; it should no longer be zero.\n      self.assertAlmostEqual(\n          0.067,\n          sess.run(\n              tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                  signature_def.outputs[""metrics/average_loss/value""])),\n          places=3)\n      self.assertAlmostEqual(\n          1.,\n          sess.run(\n              tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                  signature_def.outputs[""metrics/recall/value""])),\n          places=3)\n\n      self.assertAlmostEqual(\n          1.,\n          sess.run(\n              tf_compat.v1.saved_model.utils.get_tensor_from_tensor_info(\n                  signature_def.outputs[""metrics/accuracy/value""])),\n          places=3)\n\n  def test_export_saved_model_always_uses_replication_placement(self):\n    """"""Tests b/137675014.""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator(\n        [_DNNBuilder(""dnn1""), _DNNBuilder(""dnn2"")])\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=1,\n        model_dir=self.test_subdirectory,\n        config=run_config,\n        experimental_placement_strategy=RoundRobinStrategy())\n\n    features = {""x"": [[1., 0.]]}\n    labels = [[1.]]\n    train_input_fn = _dummy_feature_dict_input_fn(features, labels)\n\n    # Train.\n    estimator.train(input_fn=train_input_fn, max_steps=2)\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf_compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      tensor_features = {}\n      for key, value in features.items():\n        tensor_features[key] = tf.constant(value)\n      return tf.estimator.export.ServingInputReceiver(\n          features=tensor_features, receiver_tensors=serialized_example)\n\n    # Fake the number of PS replicas so RoundRobinStrategy will be used.\n    estimator._config._num_ps_replicas = 2\n    # If we\'re still using RoundRobinStrategy, this call will fail by trying\n    # to place ops on non-existent devices.\n    # Check all three export methods.\n    estimator.export_saved_model(\n        export_dir_base=self.test_subdirectory,\n        serving_input_receiver_fn=serving_input_fn,\n        experimental_mode=tf.estimator.ModeKeys.PREDICT)\n    try:\n      estimator.export_savedmodel(\n          export_dir_base=self.test_subdirectory,\n          serving_input_receiver_fn=serving_input_fn)\n    except AttributeError as error:\n      # Log deprecation errors.\n      logging.warning(""Testing estimator#export_savedmodel: %s"", error)\n    estimator.experimental_export_all_saved_models(\n        export_dir_base=self.test_subdirectory,\n        input_receiver_fn_map={\n            tf.estimator.ModeKeys.PREDICT: serving_input_fn,\n        })\n\n\nclass EstimatorReportTest(tu.AdanetTestCase):\n  """"""Tests report generation and usage.""""""\n\n  def compare_report_lists(self, report_list1, report_list2):\n    # Essentially assertEqual(report_list1, report_list2), but ignoring\n    # the ""metrics"" attribute.\n\n    def make_qualified_name(iteration_number, name):\n      return ""iteration_{}/{}"".format(iteration_number, name)\n\n    report_dict_1 = {\n        make_qualified_name(report.iteration_number, report.name): report\n        for report in report_list1\n    }\n    report_dict_2 = {\n        make_qualified_name(report.iteration_number, report.name): report\n        for report in report_list2\n    }\n\n    self.assertEqual(len(report_list1), len(report_list2))\n\n    for qualified_name in report_dict_1.keys():\n      report_1 = report_dict_1[qualified_name]\n      report_2 = report_dict_2[qualified_name]\n      self.assertEqual(\n          report_1.hparams,\n          report_2.hparams,\n          msg=""{} vs. {}"".format(report_1, report_2))\n      self.assertEqual(\n          report_1.attributes,\n          report_2.attributes,\n          msg=""{} vs. {}"".format(report_1, report_2))\n      self.assertEqual(\n          report_1.included_in_final_ensemble,\n          report_2.included_in_final_ensemble,\n          msg=""{} vs. {}"".format(report_1, report_2))\n      for metric_key, metric_value in report_1.metrics.items():\n        self.assertEqual(\n            metric_value,\n            report_2.metrics[metric_key],\n            msg=""{} vs. {}"".format(report_1, report_2))\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""one_iteration_one_subnetwork"",\n          ""subnetwork_builders"": [_DNNBuilder(""dnn"", layer_size=1),],\n          ""num_iterations"": 1,\n          ""want_materialized_iteration_reports"": [[\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ]],\n          ""want_previous_ensemble_reports"": [],\n          ""want_all_reports"": [],\n      },\n      {\n          ""testcase_name"": ""one_iteration_three_subnetworks"",\n          ""subnetwork_builders"": [\n              # learning_rate is set to 0 for all but one Builder\n              # to make sure that only one of them can learn.\n              _DNNBuilder(\n                  ""dnn_1"",\n                  layer_size=1,\n                  learning_rate=0.,\n                  mixture_weight_learning_rate=0.),\n              _DNNBuilder(\n                  ""dnn_2"",\n                  layer_size=2,\n                  learning_rate=0.,\n                  mixture_weight_learning_rate=0.),\n              # fixing the match for dnn_3 to win.\n              _DNNBuilder(""dnn_3"", layer_size=3),\n          ],\n          ""num_iterations"": 1,\n          ""want_materialized_iteration_reports"": [[\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_1"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_2"",\n                  hparams={""layer_size"": 2},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_3"",\n                  hparams={""layer_size"": 3},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ]],\n          ""want_previous_ensemble_reports"": [],\n          ""want_all_reports"": [],\n      },\n      {\n          ""testcase_name"":\n              ""three_iterations_one_subnetwork"",\n          ""subnetwork_builders"": [_DNNBuilder(""dnn"", layer_size=1),],\n          ""num_iterations"":\n              3,\n          ""want_materialized_iteration_reports"": [\n              [\n                  MaterializedReport(\n                      iteration_number=0,\n                      name=""dnn"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  )\n              ],\n              [\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""previous_ensemble"",\n                      hparams={},\n                      attributes={},\n                      metrics={},\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""dnn"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  ),\n              ],\n              [\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""previous_ensemble"",\n                      hparams={},\n                      attributes={},\n                      metrics={},\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""dnn"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  ),\n              ],\n          ],\n          ""want_previous_ensemble_reports"": [\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ],\n          ""want_all_reports"": [\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""previous_ensemble"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""three_iterations_three_subnetworks"",\n          ""subnetwork_builders"": [\n              # learning_rate is set to 0 for all but one Builder\n              # to make sure that only one of them can learn.\n              _DNNBuilder(\n                  ""dnn_1"",\n                  layer_size=1,\n                  learning_rate=0.,\n                  mixture_weight_learning_rate=0.),\n              _DNNBuilder(\n                  ""dnn_2"",\n                  layer_size=2,\n                  learning_rate=0.,\n                  mixture_weight_learning_rate=0.),\n              # fixing the match for dnn_3 to win in every iteration.\n              _DNNBuilder(""dnn_3"", layer_size=3),\n          ],\n          ""num_iterations"":\n              3,\n          ""want_materialized_iteration_reports"": [\n              [\n                  MaterializedReport(\n                      iteration_number=0,\n                      name=""dnn_1"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=0,\n                      name=""dnn_2"",\n                      hparams={""layer_size"": 2},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=0,\n                      name=""dnn_3"",\n                      hparams={""layer_size"": 3},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  ),\n              ],\n              [\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""previous_ensemble"",\n                      hparams={},\n                      attributes={},\n                      metrics={},\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""dnn_1"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""dnn_2"",\n                      hparams={""layer_size"": 2},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=1,\n                      name=""dnn_3"",\n                      hparams={""layer_size"": 3},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  ),\n              ],\n              [\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""previous_ensemble"",\n                      hparams={},\n                      attributes={},\n                      metrics={},\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""dnn_1"",\n                      hparams={""layer_size"": 1},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""dnn_2"",\n                      hparams={""layer_size"": 2},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=False,\n                  ),\n                  MaterializedReport(\n                      iteration_number=2,\n                      name=""dnn_3"",\n                      hparams={""layer_size"": 3},\n                      attributes={\n                          ""complexity"": 3,\n                      },\n                      metrics={\n                          ""moo"": 3,\n                      },\n                      included_in_final_ensemble=True,\n                  ),\n              ],\n          ],\n          ""want_previous_ensemble_reports"": [\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_3"",\n                  hparams={""layer_size"": 3},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn_3"",\n                  hparams={""layer_size"": 3},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ],\n          ""want_all_reports"": [\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_1"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_2"",\n                  hparams={""layer_size"": 2},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=0,\n                  name=""dnn_3"",\n                  hparams={""layer_size"": 3},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""previous_ensemble"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn_1"",\n                  hparams={""layer_size"": 1},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn_2"",\n                  hparams={""layer_size"": 2},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=False,\n              ),\n              MaterializedReport(\n                  iteration_number=1,\n                  name=""dnn_3"",\n                  hparams={""layer_size"": 3},\n                  attributes={\n                      ""complexity"": 3,\n                  },\n                  metrics={\n                      ""moo"": 3,\n                  },\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      },\n  )\n  def test_report_generation_and_usage(self, subnetwork_builders,\n                                       num_iterations,\n                                       want_materialized_iteration_reports,\n                                       want_previous_ensemble_reports,\n                                       want_all_reports):\n    # Stores the iteration_number, previous_ensemble_reports and all_reports\n    # arguments in the self._iteration_reports dictionary, overwriting what\n    # was seen in previous iterations.\n    spied_iteration_reports = {}\n\n    def _spy_fn(iteration_number, previous_ensemble_reports, all_reports):\n      spied_iteration_reports[iteration_number] = {\n          ""previous_ensemble_reports"": previous_ensemble_reports,\n          ""all_reports"": all_reports,\n      }\n\n    subnetwork_generator = _FakeGenerator(\n        spy_fn=_spy_fn, subnetwork_builders=subnetwork_builders)\n\n    max_iteration_steps = 5\n    max_steps = max_iteration_steps * num_iterations + 1\n\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        mixture_weight_type=MixtureWeightType.MATRIX,\n        mixture_weight_initializer=tf_compat.v1.zeros_initializer(),\n        warm_start_mixture_weights=True,\n        max_iteration_steps=max_iteration_steps,\n        use_bias=True,\n        report_materializer=ReportMaterializer(\n            input_fn=train_input_fn, steps=1),\n        model_dir=self.test_subdirectory)\n\n    report_accessor = estimator._report_accessor\n\n    estimator.train(input_fn=train_input_fn, max_steps=max_steps)\n\n    materialized_iteration_reports = list(\n        report_accessor.read_iteration_reports())\n    self.assertEqual(num_iterations, len(materialized_iteration_reports))\n    for i in range(num_iterations):\n      want_materialized_reports = (want_materialized_iteration_reports[i])\n      materialized_reports = materialized_iteration_reports[i]\n      self.compare_report_lists(want_materialized_reports, materialized_reports)\n\n      # Compute argmin adanet loss.\n      argmin_adanet_loss = 0\n      smallest_known_adanet_loss = float(""inf"")\n      for j, materialized_subnetwork_report in enumerate(materialized_reports):\n        if (smallest_known_adanet_loss >\n            materialized_subnetwork_report.metrics[""adanet_loss""]):\n          smallest_known_adanet_loss = (\n              materialized_subnetwork_report.metrics[""adanet_loss""])\n          argmin_adanet_loss = j\n\n      # Check that the subnetwork with the lowest adanet loss is the one\n      # that is included in the final ensemble.\n      for j, materialized_reports in enumerate(materialized_reports):\n        self.assertEqual(j == argmin_adanet_loss,\n                         materialized_reports.included_in_final_ensemble)\n\n    # Check the arguments passed into the generate_candidates method of the\n    # Generator.\n    iteration_report = spied_iteration_reports[num_iterations - 1]\n    self.compare_report_lists(want_previous_ensemble_reports,\n                              iteration_report[""previous_ensemble_reports""])\n    self.compare_report_lists(want_all_reports, iteration_report[""all_reports""])\n\n\nclass EstimatorForceGrowTest(tu.AdanetTestCase):\n  """"""Tests the force_grow override.\n\n  Uses linear subnetworks with the same seed. They will produce identical\n  outputs, so unless the `force_grow` override is set, none of the new\n  subnetworks will improve the AdaNet objective, and AdaNet will not add them to\n  the ensemble.\n  """"""\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""one_builder_no_force_grow"",\n          ""builders"": [_FrozenLinearBuilder(""linear"")],\n          ""force_grow"": False,\n          ""want_subnetworks"": 1,\n      }, {\n          ""testcase_name"": ""two_builders_no_force_grow"",\n          ""builders"": [\n              _FrozenLinearBuilder(""linear""),\n              _FrozenLinearBuilder(""linear2""),\n          ],\n          ""force_grow"": False,\n          ""want_subnetworks"": 1,\n      }, {\n          ""testcase_name"": ""one_builder"",\n          ""builders"": [_FrozenLinearBuilder(""linear"")],\n          ""force_grow"": True,\n          ""want_subnetworks"": 2,\n      }, {\n          ""testcase_name"": ""two_builders"",\n          ""builders"":\n              [_FrozenLinearBuilder(""linear""),\n               _FrozenLinearBuilder(""linear2"")],\n          ""force_grow"": True,\n          ""want_subnetworks"": 2,\n      }, {\n          ""testcase_name"":\n              ""two_builders_with_evaluator"",\n          ""builders"":\n              [_FrozenLinearBuilder(""linear""),\n               _FrozenLinearBuilder(""linear2"")],\n          ""force_grow"":\n              True,\n          ""evaluator"":\n              Evaluator(\n                  input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1),\n          ""want_subnetworks"":\n              3,\n      })\n  def test_force_grow(self,\n                      builders,\n                      force_grow,\n                      want_subnetworks,\n                      evaluator=None):\n    """"""Test force grow with identical frozen subnetworks.""""""\n\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator(builders)\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=1,\n        evaluator=evaluator,\n        force_grow=force_grow,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    # Train for four iterations.\n    estimator.train(input_fn=train_input_fn, max_steps=3)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertEqual(\n        want_subnetworks,\n        str(eval_results[""architecture/adanet/ensembles""]).count("" linear ""))\n\n\nclass EstimatorDebugTest(tu.AdanetTestCase):\n  """"""Tests b/125483534. Detect NaNs in input_fns.""""""\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""nan_features"",\n          ""head"":\n              regression_head.RegressionHead(\n                  name=""y"", loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n          ""input_fn"":\n              lambda: ({\n                  ""x"": tf.math.log([[1., 0.]])\n              }, tf.zeros([1, 1]))\n      }, {\n          ""testcase_name"":\n              ""nan_label"",\n          ""head"":\n              regression_head.RegressionHead(\n                  name=""y"", loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n          ""input_fn"":\n              lambda: ({\n                  ""x"": tf.ones([1, 2])\n              }, tf.math.log([[0.]]))\n      }, {\n          ""testcase_name"":\n              ""nan_labels_dict"",\n          ""head"":\n              multi_head_lib.MultiHead(heads=[\n                  regression_head.RegressionHead(\n                      name=""y"", loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n              ]),\n          ""input_fn"":\n              lambda: ({\n                  ""x"": tf.ones([1, 2])\n              }, {\n                  ""y"": tf.math.log([[0.]])\n              })\n      })\n  # pylint: enable=g-long-lambda\n  def test_nans_from_input_fn(self, head, input_fn):\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    estimator = Estimator(\n        head=head,\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=3,\n        model_dir=self.test_subdirectory,\n        debug=True)\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      estimator.train(input_fn=input_fn, max_steps=3)\n\n\nclass EstimatorEvaluateDuringTrainHookTest(tu.AdanetTestCase):\n  """"""Tests b/129000842 with a hook that calls estimator.evaluate().""""""\n\n  def test_train(self):\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=1,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    class EvalTrainHook(tf.estimator.SessionRunHook):\n\n      def end(self, session):\n        estimator.evaluate(input_fn=train_input_fn, steps=1)\n\n    # This should not infinite loop.\n    estimator.train(\n        input_fn=train_input_fn, max_steps=3, hooks=[EvalTrainHook()])\n\n\nclass CheckpointSaverHookDuringTrainingTest(tu.AdanetTestCase):\n  """"""Tests b/139057887.""""""\n\n  def test_checkpoint_saver_hooks_not_decorated_during_training(self):\n    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n    subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=1,\n        model_dir=self.test_subdirectory,\n        config=run_config)\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    saver_hook = tf_compat.v1.train.CheckpointSaverHook(\n        checkpoint_dir=self.test_subdirectory, save_steps=10)\n    listener = tf_compat.v1.train.CheckpointSaverListener()\n    estimator.train(\n        input_fn=train_input_fn,\n        max_steps=3,\n        hooks=[saver_hook],\n        saving_listeners=[listener])\n\n    # If CheckpointSaverHook was not recognized during training then all\n    # saving_listeners would be attached to a default CheckpointSaverHook that\n    # Estimator creates.\n    self.assertLen(saver_hook._listeners, 1)\n    self.assertIs(saver_hook._listeners[0], listener)\n\n\nclass EstimatorTFLearnRunConfigTest(tu.AdanetTestCase):\n  """"""Tests b/129483642 for tf.contrib.learn.RunConfig.\n\n  Checks that TF_CONFIG is overwritten correctly when no cluster is specified\n  in the RunConfig and the only task is of type chief.\n  """"""\n\n  def test_train(self):\n    try:\n      run_config = tf.contrib.learn.RunConfig(tf_random_seed=42)\n      # Removed in TF 1.15 (nightly). See\n      # https://travis-ci.org/tensorflow/adanet/jobs/583471908\n      _ = run_config._session_creation_timeout_secs\n    except AttributeError:\n      self.skipTest(""There is no tf.contrib in TF 2.0."")\n\n    try:\n      tf_config = {\n          ""task"": {\n              ""type"": ""chief"",\n              ""index"": 0\n          },\n      }\n      os.environ[""TF_CONFIG""] = json.dumps(tf_config)\n      run_config = tf.contrib.learn.RunConfig(tf_random_seed=42)\n      run_config._is_chief = True  # pylint: disable=protected-access\n\n      subnetwork_generator = SimpleGenerator([_DNNBuilder(""dnn"")])\n      estimator = Estimator(\n          head=tu.head(),\n          subnetwork_generator=subnetwork_generator,\n          max_iteration_steps=1,\n          model_dir=self.test_subdirectory,\n          config=run_config)\n      train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n      # Will fail if TF_CONFIG is not overwritten correctly in\n      # Estimator#prepare_next_iteration.\n      estimator.train(input_fn=train_input_fn, max_steps=3)\n    finally:\n      # Revert TF_CONFIG environment variable in order to not break other tests.\n      del os.environ[""TF_CONFIG""]\n\n\n\n\nclass EstimatorReplayTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""no_evaluator"",\n          ""evaluator"": None,\n          ""replay_evaluator"": None,\n          ""want_architecture"": "" dnn3 | dnn3 | dnn "",\n      }, {\n          ""testcase_name"":\n              ""evaluator"",\n          ""evaluator"":\n              Evaluator(\n                  input_fn=tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS),\n                  steps=1),\n          ""replay_evaluator"":\n              Evaluator(\n                  input_fn=tu.dummy_input_fn([[0., 0.], [0., 0], [0., 0.],\n                                              [0., 0.]], [[0], [0], [0], [0]]),\n                  steps=1),\n          ""want_architecture"":\n              "" dnn3 | dnn3 | dnn "",\n      })\n  def test_replay(self, evaluator, replay_evaluator, want_architecture):\n    """"""Train entire estimator lifecycle using Replay.""""""\n\n    original_model_dir = os.path.join(self.test_subdirectory, ""original"")\n    run_config = tf.estimator.RunConfig(\n        tf_random_seed=42, model_dir=original_model_dir)\n    subnetwork_generator = SimpleGenerator([\n        _DNNBuilder(""dnn""),\n        _DNNBuilder(""dnn2"", layer_size=3),\n        _DNNBuilder(""dnn3"", layer_size=5),\n    ])\n    estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=10,\n        evaluator=evaluator,\n        config=run_config)\n\n    train_input_fn = tu.dummy_input_fn(XOR_FEATURES, XOR_LABELS)\n\n    # Train for three iterations.\n    estimator.train(input_fn=train_input_fn, max_steps=30)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertIn(want_architecture,\n                  str(eval_results[""architecture/adanet/ensembles""]))\n\n    replay_run_config = tf.estimator.RunConfig(\n        tf_random_seed=42,\n        model_dir=os.path.join(self.test_subdirectory, ""replayed""))\n\n    # Use different features and labels to represent a shift in the data\n    # distribution.\n    different_features = [[0., 0.], [0., 0], [0., 0.], [0., 0.]]\n    different_labels = [[0], [0], [0], [0]]\n\n    replay_estimator = Estimator(\n        head=tu.head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=10,\n        evaluator=replay_evaluator,\n        config=replay_run_config,\n        replay_config=replay.Config(best_ensemble_indices=[2, 3, 1]))\n\n    train_input_fn = tu.dummy_input_fn(different_features, different_labels)\n\n    # Train for three iterations.\n    replay_estimator.train(input_fn=train_input_fn, max_steps=30)\n\n    # Evaluate.\n    eval_results = replay_estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertIn(want_architecture,\n                  str(eval_results[""architecture/adanet/ensembles""]))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/estimator_v2_test.py,5,"b'""""""Test AdaNet estimator single graph implementation for TF 2.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import logging\nfrom adanet import tf_compat\nfrom adanet.core import testing_utils as tu\nfrom adanet.core.estimator import Estimator\nfrom adanet.core.report_materializer import ReportMaterializer\nfrom adanet.subnetwork import Builder\nfrom adanet.subnetwork import SimpleGenerator\nfrom adanet.subnetwork import Subnetwork\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_estimator.python.estimator.head import regression_head\n\nlogging.set_verbosity(logging.INFO)\n\nXOR_FEATURES = [[1., 0.], [0., 0], [0., 1.], [1., 1.]]\nXOR_LABELS = [[1.], [0.], [1.], [0.]]\n\n\nclass _SimpleBuilder(Builder):\n  """"""A simple subnetwork builder that takes feature_columns.""""""\n\n  def __init__(self, name, seed=42):\n    self._name = name\n    self._seed = seed\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    seed = self._seed\n    if previous_ensemble:\n      # Increment seed so different iterations don\'t learn the exact same thing.\n      seed += 1\n\n    with tf_compat.v1.variable_scope(""simple""):\n      input_layer = tf_compat.v1.feature_column.input_layer(\n          features=features,\n          feature_columns=tf.feature_column.numeric_column(""x"", 2))\n      last_layer = input_layer\n\n    with tf_compat.v1.variable_scope(""logits""):\n      logits = tf_compat.v1.layers.dense(\n          last_layer,\n          logits_dimension,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer(seed=seed))\n\n    summary.scalar(""scalar"", 3)\n    batch_size = features[""x""].get_shape().as_list()[0]\n    summary.image(""image"", tf.ones([batch_size, 3, 3, 1]))\n    with tf_compat.v1.variable_scope(""nested""):\n      summary.scalar(""scalar"", 5)\n\n    return Subnetwork(\n        last_layer=last_layer,\n        logits=logits,\n        complexity=1,\n        persisted_tensors={},\n    )\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    optimizer = tf_compat.v1.train.GradientDescentOptimizer(learning_rate=.001)\n    return optimizer.minimize(loss, var_list=var_list)\n\n\nclass EstimatorSummaryWriterTest(tu.AdanetTestCase):\n  """"""Test that Tensorboard summaries get written correctly.""""""\n\n  @tf_compat.skip_for_tf1\n  def test_summaries(self):\n    """"""Tests that summaries are written to candidate directory.""""""\n\n    run_config = tf.estimator.RunConfig(\n        tf_random_seed=42,\n        log_step_count_steps=2,\n        save_summary_steps=2,\n        model_dir=self.test_subdirectory)\n    subnetwork_generator = SimpleGenerator([_SimpleBuilder(""dnn"")])\n    report_materializer = ReportMaterializer(\n        input_fn=tu.dummy_input_fn([[1., 1.]], [[0.]]), steps=1)\n    estimator = Estimator(\n        head=regression_head.RegressionHead(\n            loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE),\n        subnetwork_generator=subnetwork_generator,\n        report_materializer=report_materializer,\n        max_iteration_steps=10,\n        config=run_config)\n    train_input_fn = tu.dummy_input_fn([[1., 0.]], [[1.]])\n    estimator.train(input_fn=train_input_fn, max_steps=3)\n\n    ensemble_loss = 1.52950\n    self.assertAlmostEqual(\n        ensemble_loss,\n        tu.check_eventfile_for_keyword(""loss"", self.test_subdirectory),\n        places=3)\n    self.assertIsNotNone(\n        tu.check_eventfile_for_keyword(""global_step/sec"",\n                                       self.test_subdirectory))\n    self.assertEqual(\n        0.,\n        tu.check_eventfile_for_keyword(""iteration/adanet/iteration"",\n                                       self.test_subdirectory))\n\n    subnetwork_subdir = os.path.join(self.test_subdirectory,\n                                     ""subnetwork/t0_dnn"")\n    self.assertAlmostEqual(\n        3.,\n        tu.check_eventfile_for_keyword(""scalar"", subnetwork_subdir),\n        places=3)\n    self.assertEqual((3, 3, 1),\n                     tu.check_eventfile_for_keyword(""image"", subnetwork_subdir))\n    self.assertAlmostEqual(\n        5.,\n        tu.check_eventfile_for_keyword(""nested/scalar"", subnetwork_subdir),\n        places=3)\n\n    ensemble_subdir = os.path.join(\n        self.test_subdirectory, ""ensemble/t0_dnn_grow_complexity_regularized"")\n    self.assertAlmostEqual(\n        ensemble_loss,\n        tu.check_eventfile_for_keyword(\n            ""adanet_loss/adanet/adanet_weighted_ensemble"", ensemble_subdir),\n        places=1)\n    self.assertAlmostEqual(\n        0.,\n        tu.check_eventfile_for_keyword(\n            ""complexity_regularization/adanet/adanet_weighted_ensemble"",\n            ensemble_subdir),\n        places=3)\n    self.assertAlmostEqual(\n        1.,\n        tu.check_eventfile_for_keyword(\n            ""mixture_weight_norms/adanet/""\n            ""adanet_weighted_ensemble/subnetwork_0"", ensemble_subdir),\n        places=3)\n\n\nif __name__ == ""__main__"":\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/core/eval_metrics.py,18,"b'""""""AdaNet metrics objects and functions.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport inspect\n\nfrom absl import logging\nfrom adanet import tf_compat\nimport six\nimport tensorflow.compat.v2 as tf\n\n\ndef _call_eval_metrics(eval_metrics):\n  if not eval_metrics:\n    return {}\n  fn, args = eval_metrics\n  if isinstance(args, dict):\n    return fn(**args)\n  else:\n    return fn(*args)\n\n\nclass _EvalMetricsStore(object):\n  """"""Stores and manipulate eval_metric tuples.""""""\n\n  def __init__(self):\n    self._metric_fns = []\n    self._args = []\n\n  def add_eval_metrics(self, metric_fn, args):\n    """"""Adds an eval_metrics tuple to the internal store.""""""\n\n    self._metric_fns.append(metric_fn)\n    self._args.append(args)\n\n  @property\n  def metric_fns(self):\n    return self._metric_fns\n\n  def flatten_args(self):\n    """"""Flattens the eval_metrics arguments to a list.""""""\n\n    from tensorflow.python.util import nest  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    return nest.flatten(self._args)\n\n  def pack_args(self, args):\n    """"""Packs the given list of arguments into the internal args structure.""""""\n\n    from tensorflow.python.util import nest  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    return nest.pack_sequence_as(self._args, args)\n\n\nclass _SubnetworkMetrics(object):\n  """"""A object which creates evaluation metrics for Subnetworks.""""""\n\n  def __init__(self, use_tpu=False):\n    """"""Creates a _SubnetworkMetrics.\n\n    Args:\n      use_tpu: Whether to use TPU-specific variable sharing logic. This ensures\n        that eval metrics created on TPU can be written to disk on the host CPU.\n\n    Returns:\n      A `_SubnetworkMetrics` instance.\n    """"""\n\n    self._use_tpu = use_tpu\n    self._eval_metrics_store = _EvalMetricsStore()\n\n  def create_eval_metrics(self, features, labels, estimator_spec, metric_fn):\n    """"""Creates evaluation metrics from the given arguments.\n\n    Args:\n      features: Input `dict` of `Tensor` objects.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head).\n      estimator_spec: The `EstimatorSpec` created by a `Head` instance.\n      metric_fn: A function which should obey the following signature:\n      - Args: can only have following three arguments in any order:\n        * predictions: Predictions `Tensor` or dict of `Tensor` created by given\n          `Head`.\n        * features: Input `dict` of `Tensor` objects created by `input_fn` which\n          is given to `estimator.evaluate` as an argument.\n        * labels:  Labels `Tensor` or dict of `Tensor` (for multi-head) created\n          by `input_fn` which is given to `estimator.evaluate` as an argument.\n      - Returns: Dict of metric results keyed by name. Final metrics are a union\n        of this and `estimator`s existing metrics. If there is a name conflict\n        between this and `estimator`s existing metrics, this will override the\n        existing one. The values of the dict are the results of calling a metric\n        function, namely a `(metric_tensor, update_op)` tuple.\n    """"""\n\n    # If estimator_spec is not a TPUEstimatorSpec we create dummy metric_fn\n    # and args.\n    if isinstance(estimator_spec, tf.estimator.EstimatorSpec):\n      spec_fn, spec_args = lambda: estimator_spec.eval_metric_ops, []\n    else:\n      spec_fn, spec_args = estimator_spec.eval_metrics\n    self._eval_metrics_store.add_eval_metrics(\n        self._templatize_metric_fn(spec_fn), spec_args)\n\n    loss_fn = lambda loss: {""loss"": tf_compat.v1.metrics.mean(loss)}\n    loss_fn_args = [tf.reshape(estimator_spec.loss, [1])]\n\n    if not self._use_tpu:\n      loss_ops = _call_eval_metrics((loss_fn, loss_fn_args))\n      loss_fn, loss_fn_args = lambda: loss_ops, []\n    self._eval_metrics_store.add_eval_metrics(\n        self._templatize_metric_fn(loss_fn), loss_fn_args)\n\n    # NOTE: the user supplied metrics_fn must be added last. This is because we\n    # want user metrics to override AdaNet\'s metrics.\n    if metric_fn:\n      metric_fn_args = {}\n      # Calling low level getargs for py_2_and_3 compatibility.\n      argspec = inspect.getargs(metric_fn.__code__).args\n      if ""features"" in argspec:\n        metric_fn_args[""features""] = features\n      if ""labels"" in argspec:\n        metric_fn_args[""labels""] = labels\n      if ""predictions"" in argspec:\n        metric_fn_args[""predictions""] = estimator_spec.predictions\n\n      if not self._use_tpu:\n        metric_fn_ops = _call_eval_metrics((metric_fn, metric_fn_args))\n        metric_fn, metric_fn_args = lambda: metric_fn_ops, []\n      self._eval_metrics_store.add_eval_metrics(\n          self._templatize_metric_fn(metric_fn), metric_fn_args)\n\n  def _templatize_metric_fn(self, metric_fn):\n    """"""Wraps the given metric_fn with a template so it\'s Variables are shared.\n\n    Hooks on TPU cannot depend on any graph Tensors. Instead the eval metrics\n    returned by metric_fn are stored in Variables. These variables are later\n    read from the evaluation hooks which run on the host CPU.\n\n    Args:\n      metric_fn: The function to wrap with a template.\n\n    Returns:\n      The original metric_fn wrapped with a template function.\n    """"""\n\n    def _metric_fn(*args, **kwargs):\n      """"""The wrapping function to be returned.""""""\n\n      # We can only be passed in either a dict or a list of tensors.\n      args = args if args else kwargs\n      metrics = _call_eval_metrics((metric_fn, args))\n      if not self._use_tpu:\n        return metrics\n\n      logging.log_first_n(logging.INFO,\n                          ""Writing eval metrics to variables for TPU"", 1)\n      wrapped_metrics = {}\n      for i, key in enumerate(sorted(metrics)):\n        tensor, op = tf_compat.metric_op(metrics[key])\n        # key cannot be in var name since it may contain illegal chars.\n        var = tf_compat.v1.get_variable(\n            ""metric_{}"".format(i),\n            shape=tensor.shape,\n            dtype=tensor.dtype,\n            trainable=False,\n            initializer=tf_compat.v1.zeros_initializer(),\n            collections=[tf_compat.v1.GraphKeys.LOCAL_VARIABLES])\n        if isinstance(op, tf.Operation) or op.shape != tensor.shape:\n          with tf.control_dependencies([op]):\n            op = var.assign(tensor)\n        metric = (var, var.assign(op))\n        wrapped_metrics[key] = metric\n      return wrapped_metrics\n\n    return tf_compat.v1.make_template(""metric_fn_template"", _metric_fn)\n\n  def eval_metrics_tuple(self):\n    """"""Returns tuple of (metric_fn, tensors) which can be executed on TPU.""""""\n\n    if not self._eval_metrics_store.metric_fns:\n      return None\n\n    def _metric_fn(*args):\n      metric_fns = self._eval_metrics_store.metric_fns\n      metric_fn_args = self._eval_metrics_store.pack_args(args)\n      eval_metric_ops = {}\n      for metric_fn, args in zip(metric_fns, metric_fn_args):\n        eval_metric_ops.update(_call_eval_metrics((metric_fn, args)))\n      return eval_metric_ops\n\n    return _metric_fn, self._eval_metrics_store.flatten_args()\n\n  def eval_metrics_ops(self):\n    """"""Returns the eval_metrics_ops.""""""\n\n    return _call_eval_metrics(self.eval_metrics_tuple())\n\n\nclass _EnsembleMetrics(_SubnetworkMetrics):\n  """"""A object which creates evaluation metrics for Ensembles.""""""\n\n  def create_eval_metrics(self, features, labels, estimator_spec, metric_fn,\n                          architecture):\n    """"""Overrides parent\'s method to also add the ensemble\'s architecture.""""""\n\n    super(_EnsembleMetrics, self).create_eval_metrics(features, labels,\n                                                      estimator_spec, metric_fn)\n    self._eval_metrics_store.add_eval_metrics(\n        self._architecture_as_metric(architecture), [])\n\n  def _architecture_as_metric(self, architecture):\n    """"""Returns a representation of an ensemble\'s architecture as a tf.metric.""""""\n\n    def _architecture_metric_fn():\n      """"""Manually creates the tf.metric with a serialized tf.Summary proto.""""""\n\n      # TODO: Should architecture.subnetworks be sorted by iteration\n      # number first? Or perhaps, to make this more general, to have one line\n      # for each iteration, with ""|"" as a delimiter if there are multiple\n      # subnetworks in one iteration? Something like:\n      # 0 linear\n      # 1 dnn_width_32_depth_1 | dnn_width_64_depth_1\n      # 2\n      # 3 dnn_with_32_depth_2\n      # Also consider adding ensemble candidate\'s name, though that is already\n      # included in the ensemble name.\n      architecture_ = "" | "".join([name for _, name in architecture.subnetworks])\n      architecture_ = ""| {} |"".format(architecture_)\n      summary_metadata = tf_compat.v1.SummaryMetadata(\n          plugin_data=tf_compat.v1.SummaryMetadata.PluginData(\n              plugin_name=""text""))\n      summary_proto = tf_compat.v1.summary.Summary()\n      summary_proto.value.add(\n          metadata=summary_metadata,\n          tag=""architecture/adanet"",\n          tensor=tf_compat.v1.make_tensor_proto(architecture_, dtype=tf.string))\n      architecture_summary = tf.convert_to_tensor(\n          value=summary_proto.SerializeToString(), name=""architecture"")\n\n      return {\n          ""architecture/adanet/ensembles"": (architecture_summary, tf.no_op())\n      }\n\n    if not self._use_tpu:\n      ops = _architecture_metric_fn()\n      return lambda: ops\n    else:\n      return _architecture_metric_fn\n\n\nclass _IterationMetrics(object):\n  """"""A object which creates evaluation metrics for an Iteration.""""""\n\n  def __init__(self,\n               iteration_number,\n               candidates,\n               subnetwork_specs,\n               use_tpu=False,\n               replay_indices_for_all=None):\n    self._iteration_number = iteration_number\n    self._candidates = candidates\n    self._subnetwork_specs = subnetwork_specs\n    self._use_tpu = use_tpu\n    self._replay_indices_for_all = replay_indices_for_all\n\n    self._candidates_eval_metrics_store = self._build_eval_metrics_store(\n        [candidate.ensemble_spec for candidate in self._candidates])\n    self._subnetworks_eval_metrics_store = self._build_eval_metrics_store(\n        self._subnetwork_specs)\n\n    self._best_eval_metrics_tuple = None\n\n  def _build_eval_metrics_store(self, specs):\n    """"""Creates an _EvalMetricsStore from Subnetwork or Ensemble specs.""""""\n\n    store = _EvalMetricsStore()\n    for spec in specs:\n      if not spec.eval_metrics or not spec.eval_metrics.eval_metrics_tuple():\n        continue\n      metric_fn, args = spec.eval_metrics.eval_metrics_tuple()\n      store.add_eval_metrics(metric_fn, args)\n    return store\n\n  def best_eval_metric_ops(self, best_candidate_index, mode):\n    """"""Returns best ensemble\'s metrics.""""""\n\n    return _call_eval_metrics(\n        self.best_eval_metrics_tuple(best_candidate_index, mode))\n\n  def best_eval_metrics_tuple(self, best_candidate_index, mode):\n    """"""Returns (metric_fn, tensors) which computes the best ensemble\'s metrics.\n\n    Specifically, when metric_fn(tensors) is called, it separates the metric ops\n    by metric name. All candidates are not required to have the same metrics.\n    When they all share a given metric, an additional metric is added which\n    represents that of the best candidate.\n\n    Args:\n      best_candidate_index: `Tensor` index of the best candidate in the list.\n      mode: Defines whether this is training, evaluation or inference. Eval\n        metrics are only defined during evaluation. See `ModeKeys`.\n\n    Returns:\n      Dict of metric results keyed by name. The values of the dict are the\n      results of calling a metric function.\n    """"""\n\n    if mode != tf.estimator.ModeKeys.EVAL:\n      return None\n\n    candidate_args = self._candidates_eval_metrics_store.flatten_args()\n    subnetwork_args = self._subnetworks_eval_metrics_store.flatten_args()\n    args = candidate_args + subnetwork_args\n    args.append(tf.reshape(best_candidate_index, [1]))\n\n    def _replay_eval_metrics(best_candidate_idx, eval_metric_ops):\n      """"""Saves replay indices as eval metrics.""""""\n      # _replay_indices_for_all is a dict: {candidate: [list of replay_indices]}\n      # We are finding the max length replay list.\n      pad_value = max([len(v) for _, v in self._replay_indices_for_all.items()])\n\n      # Creating a matrix of (#candidate) times (max length replay indices).\n      # Entry i,j is the jth replay index of the ith candidate (ensemble).\n      replay_indices_as_tensor = tf.constant([\n          value + [-1] * (pad_value - len(value))\n          for _, value in self._replay_indices_for_all.items()\n      ])\n\n      # Passing the right entries (entries of the best candidate). Note: we use\n      # TensorShape.as_list here so the code works on both TF 1.0 and 2.0.\n      for iteration in range(replay_indices_as_tensor.get_shape().as_list()[1]):\n        index_t = replay_indices_as_tensor[best_candidate_idx, iteration]\n        eval_metric_ops[""best_ensemble_index_{}"".format(iteration)] = (index_t,\n                                                                       index_t)\n\n    def _best_eval_metrics_fn(*args):\n      """"""Returns the best eval metrics.""""""\n\n      with tf_compat.v1.variable_scope(""best_eval_metrics""):\n        args = list(args)\n        idx, idx_update_op = tf_compat.v1.metrics.mean(args.pop())\n        idx = tf.cast(idx, tf.int32)\n        metric_fns = self._candidates_eval_metrics_store.metric_fns\n        metric_fn_args = self._candidates_eval_metrics_store.pack_args(\n            args[:len(candidate_args)])\n        candidate_grouped_metrics = self._group_metric_ops(\n            metric_fns, metric_fn_args)\n\n        metric_fns = self._subnetworks_eval_metrics_store.metric_fns\n        metric_fn_args = self._subnetworks_eval_metrics_store.pack_args(\n            args[(len(args) - len(subnetwork_args)):])\n        subnetwork_grouped_metrics = self._group_metric_ops(\n            metric_fns, metric_fn_args)\n\n        eval_metric_ops = {}\n        for metric_name in sorted(candidate_grouped_metrics):\n          metric_ops = candidate_grouped_metrics[metric_name]\n          if len(metric_ops) != len(self._candidates):\n            continue\n          if metric_name == ""loss"":\n            continue\n          values, ops = list(six.moves.zip(*metric_ops))\n          best_value = tf.stack(values)[idx]\n          # All tensors in this function have been outfed from the TPU, so we\n          # must update them manually, otherwise the TPU will hang indefinitely\n          # for the value of idx to update.\n          ops = list(ops)\n          ops.append(idx_update_op)\n          # Bundle subnetwork eval metric ops and ensemble ""loss"""" ops (which\n          # is a restricted Estimator keyword) into other metric ops so that\n          # they are computed.\n          ensemble_loss_ops = candidate_grouped_metrics.get(""loss"", tf.no_op())\n          all_ops = tf.group(ops, ensemble_loss_ops, subnetwork_grouped_metrics)\n          eval_metric_ops[metric_name] = (best_value, all_ops)\n        iteration_number = tf.constant(self._iteration_number)\n        eval_metric_ops[""iteration""] = (iteration_number, iteration_number)\n\n        if self._replay_indices_for_all:\n          _replay_eval_metrics(idx, eval_metric_ops)\n\n        # tf.estimator.Estimator does not allow a ""loss"" key to be present in\n        # its eval_metrics.\n        assert ""loss"" not in eval_metric_ops\n        return eval_metric_ops\n\n    if not self._use_tpu:\n      if not self._best_eval_metrics_tuple:\n        best_ops = _call_eval_metrics((_best_eval_metrics_fn, args))\n        self._best_eval_metrics_tuple = lambda: best_ops, []\n      return self._best_eval_metrics_tuple\n\n    return _best_eval_metrics_fn, args\n\n  def _group_metric_ops(self, metric_fns, metric_fn_args):\n    """"""Runs the metric_fns and groups the returned metric ops by name.\n\n    Args:\n      metric_fns: The eval_metrics functions to run.\n      metric_fn_args: The eval_metrics function arguments.\n\n    Returns:\n      The metric ops grouped by name.\n    """"""\n\n    grouped_metrics = collections.defaultdict(list)\n    for metric_fn, args in zip(metric_fns, metric_fn_args):\n      eval_metric_ops = _call_eval_metrics((metric_fn, args))\n      for metric_name in sorted(eval_metric_ops):\n        metric_op = tf_compat.metric_op(eval_metric_ops[metric_name])\n        grouped_metrics[metric_name].append(metric_op)\n    return grouped_metrics\n'"
adanet/core/eval_metrics_test.py,19,"b'""""""Tests for AdaNet eval metrics.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.architecture import _Architecture\nfrom adanet.core.eval_metrics import _call_eval_metrics\nimport adanet.core.testing_utils as tu\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass MetricsTest(tu.AdanetTestCase):\n\n  def setup_graph(self):\n    # We only test the multi head since this is the general case.\n    self._features = {""x"": tf.constant([[1.], [2.]])}\n    heads = (""head_1"", ""head_2"")\n    labels = tf.constant([0, 1])\n    self._labels = {head: labels for head in heads}\n    predictions = {(head, ""predictions""): labels for head in heads}\n    loss = tf.constant(2.)\n    self._estimator_spec = tf_compat.v1.estimator.tpu.TPUEstimatorSpec(\n        mode=tf.estimator.ModeKeys.EVAL,\n        loss=loss,\n        predictions=predictions,\n        eval_metrics=(self._spec_metric_fn, {\n            ""features"": self._features,\n            ""labels"": self._labels,\n            ""predictions"": predictions,\n            ""loss"": loss\n        }))\n\n  def _run_metrics(self, metrics):\n    metric_ops = metrics\n    if isinstance(metric_ops, tuple):\n      metric_ops = _call_eval_metrics(metric_ops)\n    self.evaluate((tf_compat.v1.global_variables_initializer(),\n                   tf_compat.v1.local_variables_initializer()))\n    self.evaluate(metric_ops)\n    return {k: self.evaluate(metric_ops[k][0]) for k in metric_ops}\n\n  def _assert_tensors_equal(self, actual, expected):\n    actual, expected = self.evaluate((actual, expected))\n    self.assertEqual(actual, expected)\n\n  def _spec_metric_fn(self, features, labels, predictions, loss):\n    actual = [features, labels, predictions, loss]\n    expected = [\n        self._features, self._labels, self._estimator_spec.predictions,\n        self._estimator_spec.loss\n    ]\n    self._assert_tensors_equal(actual, expected)\n    return {""metric_1"": tf_compat.v1.metrics.mean(tf.constant(1.))}\n\n  def _metric_fn(self, features, predictions):\n    actual = [features, predictions]\n    expected = [self._features, self._estimator_spec.predictions]\n    self._assert_tensors_equal(actual, expected)\n    return {""metric_2"": tf_compat.v1.metrics.mean(tf.constant(2.))}\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""use_tpu"",\n          ""use_tpu"": True,\n      },\n      {\n          # TODO: Figure out why this gives error in TF 2.0:\n          # ValueError: Please call update_state(...) on the ""mean_1"" metric.\n          ""testcase_name"": ""not_use_tpu"",\n          ""use_tpu"": False,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_subnetwork_metrics(self, use_tpu):\n    with context.graph_mode():\n      self.setup_graph()\n      spec = self._estimator_spec\n      if not use_tpu:\n        spec = spec.as_estimator_spec()\n      metrics = tu.create_subnetwork_metrics(\n          self._metric_fn,\n          use_tpu=use_tpu,\n          features=self._features,\n          labels=self._labels,\n          estimator_spec=spec)\n      actual = self._run_metrics(metrics.eval_metrics_tuple())\n\n      expected = {""loss"": 2., ""metric_1"": 1., ""metric_2"": 2.}\n      self.assertEqual(actual, expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_subnetwork_metrics_user_metric_fn_overrides_metrics(self):\n    with context.graph_mode():\n      self.setup_graph()\n      overridden_value = 100.\n\n      def _overriding_metric_fn():\n        value = tf.constant(overridden_value)\n        return {""metric_1"": tf_compat.v1.metrics.mean(value)}\n\n      metrics = tu.create_subnetwork_metrics(\n          _overriding_metric_fn,\n          features=self._features,\n          labels=self._labels,\n          estimator_spec=self._estimator_spec)\n\n      actual = self._run_metrics(metrics.eval_metrics_tuple())\n\n      expected = {""loss"": 2., ""metric_1"": overridden_value}\n      self.assertEqual(actual, expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_ensemble_metrics(self):\n    with context.graph_mode():\n      self.setup_graph()\n      architecture = _Architecture(""test_ensemble_candidate"", ""test_ensembler"")\n      architecture.add_subnetwork(iteration_number=0, builder_name=""b_0_0"")\n      architecture.add_subnetwork(iteration_number=0, builder_name=""b_0_1"")\n      architecture.add_subnetwork(iteration_number=1, builder_name=""b_1_0"")\n      architecture.add_subnetwork(iteration_number=2, builder_name=""b_2_0"")\n\n      metrics = tu.create_ensemble_metrics(\n          self._metric_fn,\n          features=self._features,\n          labels=self._labels,\n          estimator_spec=self._estimator_spec,\n          architecture=architecture)\n\n      actual = self._run_metrics(metrics.eval_metrics_tuple())\n\n      serialized_arch_proto = actual[""architecture/adanet/ensembles""]\n      expected_arch_string = b""| b_0_0 | b_0_1 | b_1_0 | b_2_0 |""\n      self.assertIn(expected_arch_string, serialized_arch_proto)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""use_tpu_evaluating"",\n          ""use_tpu"": True,\n          ""mode"": tf.estimator.ModeKeys.EVAL,\n      }, {\n          ""testcase_name"": ""use_tpu_not_evaluating"",\n          ""use_tpu"": True,\n          ""mode"": tf.estimator.ModeKeys.TRAIN,\n      }, {\n          ""testcase_name"": ""not_use_tpu_evaluating"",\n          ""use_tpu"": False,\n          ""mode"": tf.estimator.ModeKeys.EVAL,\n      }, {\n          ""testcase_name"": ""not_use_tpu_not_evaluating"",\n          ""use_tpu"": False,\n          ""mode"": tf.estimator.ModeKeys.TRAIN,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_iteration_metrics(self, use_tpu, mode):\n    with context.graph_mode():\n      self.setup_graph()\n      best_candidate_index = 3\n      ensemble_metrics = []\n      for i in range(10):\n\n        def metric_fn(val=i):\n          metric = tf.keras.metrics.Mean()\n          metric.update_state(tf.constant(val))\n          return {\n              ""ensemble_v1_metric"": tf_compat.v1.metrics.mean(tf.constant(val)),\n              ""ensemble_keras_metric"": metric\n          }\n\n        ensemble_metrics.append(tu.create_ensemble_metrics(metric_fn))\n      metrics = tu.create_iteration_metrics(ensemble_metrics=ensemble_metrics)\n\n      metrics_fn = (\n          metrics.best_eval_metrics_tuple\n          if use_tpu else metrics.best_eval_metric_ops)\n      actual = self._run_metrics(\n          metrics_fn(tf.constant(best_candidate_index), mode) or {})\n\n      if mode == tf.estimator.ModeKeys.EVAL:\n        expected = {\n            ""ensemble_v1_metric"": best_candidate_index,\n            ""ensemble_keras_metric"": best_candidate_index,\n            ""iteration"": 1\n        }\n        # We don\'t actually provide an architecture, so the default will be\n        # inside.\n        del actual[""architecture/adanet/ensembles""]\n      else:\n        expected = {}\n      self.assertEqual(actual, expected)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_metric_ops_not_duplicated_on_cpu(self):\n\n    with context.graph_mode():\n      self.setup_graph()\n      metric_fn = lambda: {""metric"": (tf.constant(5), tf.constant(5))}\n      best_candidate_index = 3\n      mode = tf.estimator.ModeKeys.EVAL\n      ensemble_metrics = tu.create_ensemble_metrics(metric_fn)\n      subnetwork_metrics = tu.create_subnetwork_metrics(metric_fn)\n      iteration_metrics = tu.create_iteration_metrics(\n          ensemble_metrics=[ensemble_metrics],\n          subnetwork_metrics=[subnetwork_metrics])\n\n      ensemble_ops1 = ensemble_metrics.eval_metrics_ops()\n      ensemble_ops2 = ensemble_metrics.eval_metrics_ops()\n      subnetwork_ops1 = subnetwork_metrics.eval_metrics_ops()\n      subnetwork_ops2 = subnetwork_metrics.eval_metrics_ops()\n      iteration_ops1 = iteration_metrics.best_eval_metric_ops(\n          best_candidate_index, mode)\n      iteration_ops2 = iteration_metrics.best_eval_metric_ops(\n          best_candidate_index, mode)\n\n      self.assertEqual(subnetwork_ops1, subnetwork_ops2)\n      self.assertEqual(ensemble_ops1, ensemble_ops2)\n      self.assertEqual(iteration_ops1, iteration_ops2)\n      for ops in [ensemble_ops1, subnetwork_ops1, iteration_ops1]:\n        self.assertIsNotNone(ops)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/evaluator.py,2,"b'""""""An AdaNet evaluator implementation in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl import logging\nfrom adanet import tf_compat\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\n# TODO: Remove uses of Evaluator once AdaNet Ranker is implemented.\nclass Evaluator(object):\n  """"""Evaluates candidate ensemble performance.""""""\n\n  class Objective(object):\n    """"""The Evaluator objective for the metric being optimized.\n\n    Two objectives are currently supported:\n      - MINIMIZE: Lower is better for the metric being optimized.\n      - MAXIMIZE: Higher is better for the metric being optimized.\n    """"""\n\n    MINIMIZE = ""minimize""\n    MAXIMIZE = ""maximize""\n\n  def __init__(self,\n               input_fn,\n               metric_name=""adanet_loss"",\n               objective=Objective.MINIMIZE,\n               steps=None):\n    """"""Initializes a new Evaluator instance.\n\n    Args:\n      input_fn: Input function returning a tuple of: features - Dictionary of\n        string feature name to `Tensor`. labels - `Tensor` of labels.\n      metric_name: The name of the evaluation metrics to use when choosing the\n        best ensemble. Must refer to a valid evaluation metric.\n      objective: Either `Objective.MINIMIZE` or `Objective.MAXIMIZE`.\n      steps: Number of steps for which to evaluate the ensembles. If an\n        `OutOfRangeError` occurs, evaluation stops. If set to None, will iterate\n        the dataset until all inputs are exhausted.\n\n    Returns:\n      An :class:`adanet.Evaluator` instance.\n    """"""\n    self._input_fn = input_fn\n    self._steps = steps\n    self._metric_name = metric_name\n    self._objective = objective\n    if objective == self.Objective.MINIMIZE:\n      self._objective_fn = np.nanargmin\n    elif objective == self.Objective.MAXIMIZE:\n      self._objective_fn = np.nanargmax\n    else:\n      raise ValueError(\n          ""Evaluator objective must be one of MINIMIZE or MAXIMIZE."")\n\n  @property\n  def input_fn(self):\n    """"""Return the input_fn.""""""\n    return self._input_fn\n\n  @property\n  def steps(self):\n    """"""Return the number of evaluation steps.""""""\n    return self._steps\n\n  @property\n  def metric_name(self):\n    """"""Returns the name of the metric being optimized.""""""\n    return self._metric_name\n\n  @property\n  def objective_fn(self):\n    """"""Returns a fn which selects the best metric based on the objective.""""""\n    return self._objective_fn\n\n  def evaluate(self, sess, ensemble_metrics):\n    """"""Evaluates the given AdaNet objectives on the data from `input_fn`.\n\n    The candidates are fed the same batches of features and labels as\n    provided by `input_fn`, and their losses are computed and summed over\n    `steps` batches.\n\n    Args:\n      sess: `Session` instance with most recent variable values loaded.\n      ensemble_metrics: A list dictionaries of `tf.metrics` for each candidate\n        ensemble.\n\n    Returns:\n      List of evaluated metrics.\n    """"""\n\n    evals_completed = 0\n    if self.steps is None:\n      logging_frequency = 1000\n    elif self.steps < 10:\n      logging_frequency = 1\n    else:\n      logging_frequency = math.floor(self.steps / 10.)\n\n    objective_metrics = [em[self._metric_name] for em in ensemble_metrics]\n\n    sess.run(tf_compat.v1.local_variables_initializer())\n    while True:\n      if self.steps is not None and evals_completed == self.steps:\n        break\n      try:\n        evals_completed += 1\n        if (evals_completed % logging_frequency == 0 or\n            self.steps == evals_completed):\n          logging.info(""Ensemble evaluation [%d/%s]"", evals_completed,\n                       self.steps or ""??"")\n        sess.run(objective_metrics)\n      except tf.errors.OutOfRangeError:\n        logging.info(""Encountered end of input after %d evaluations"",\n                     evals_completed)\n        break\n\n    # Evaluating the first element is idempotent for metric tuples.\n    return sess.run([metric[0] for metric in objective_metrics])\n'"
adanet/core/evaluator_test.py,10,"b'""""""Test AdaNet evaluator single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.evaluator import Evaluator\nimport adanet.core.testing_utils as tu\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef _fake_adanet_losses_0(input_fn):\n  _, labels = input_fn()\n  return [\n      tf.reduce_sum(labels),\n      tf.reduce_sum(labels * 2),\n  ]\n\n\ndef _fake_adanet_losses_1(input_fn):\n  _, labels = input_fn()\n  return [\n      tf.reduce_sum(labels * 2),\n      tf.reduce_sum(labels),\n  ]\n\n\nclass EvaluatorTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""choose_index_0"",\n      ""input_fn"": tu.dummy_input_fn([[1., 2]], [[3.]]),\n      ""steps"": 3,\n      ""adanet_losses"": _fake_adanet_losses_0,\n      ""want_adanet_losses"": [3, 6],\n  }, {\n      ""testcase_name"": ""choose_index_1"",\n      ""input_fn"": tu.dummy_input_fn([[1., 2]], [[3.]]),\n      ""steps"": 3,\n      ""adanet_losses"": _fake_adanet_losses_1,\n      ""want_adanet_losses"": [6, 3],\n  }, {\n      ""testcase_name"": ""none_steps"",\n      ""input_fn"": tu.dataset_input_fn(),\n      ""steps"": None,\n      ""adanet_losses"": _fake_adanet_losses_1,\n      ""want_adanet_losses"": [18, 9],\n  }, {\n      ""testcase_name"": ""input_fn_out_of_range"",\n      ""input_fn"": tu.dataset_input_fn(),\n      ""steps"": 3,\n      ""adanet_losses"": _fake_adanet_losses_1,\n      ""want_adanet_losses"": [18, 9],\n  })\n  @test_util.run_in_graph_and_eager_modes\n  def test_evaluate_no_metric_fn_falls_back_to_adanet_losses(\n      self, input_fn, steps, adanet_losses, want_adanet_losses):\n    with context.graph_mode():\n      adanet_losses = adanet_losses(input_fn)\n      metrics = [{\n          ""adanet_loss"": tf_compat.v1.metrics.mean(loss)\n      } for loss in adanet_losses]\n      with self.test_session() as sess:\n        evaluator = Evaluator(input_fn=input_fn, steps=steps)\n        adanet_losses = evaluator.evaluate(sess, ensemble_metrics=metrics)\n        self.assertEqual(want_adanet_losses, adanet_losses)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""minimize_returns_nanargmin"",\n          ""objective"": Evaluator.Objective.MAXIMIZE,\n          ""expected_objective_fn"": np.nanargmax,\n          ""metric_fn"": lambda x, y: None\n      }, {\n          ""testcase_name"": ""maximize_returns_nanargmax"",\n          ""objective"": Evaluator.Objective.MINIMIZE,\n          ""expected_objective_fn"": np.nanargmin,\n          ""metric_fn"": lambda x, y: None\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_objective(self, objective, expected_objective_fn, metric_fn=None):\n    evaluator = Evaluator(input_fn=None, objective=objective)\n    self.assertEqual(expected_objective_fn, evaluator.objective_fn)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_objective_unsupported_objective(self):\n    with self.assertRaises(ValueError):\n      Evaluator(input_fn=None, objective=""non_existent_objective"")\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_evaluate(self):\n    with context.graph_mode():\n      input_fn = tu.dummy_input_fn([[1., 2]], [[3.]])\n      _, labels = input_fn()\n      predictions = [labels * 2, labels * 3]\n      metrics = []\n      for preds in predictions:\n        metrics.append({\n            ""mse"": tf_compat.v1.metrics.mean_squared_error(labels, preds),\n            ""other_metric_1"": (tf.constant(1), tf.constant(1)),\n            ""other_metric_2"": (tf.constant(2), tf.constant(2))\n        })\n\n      with self.test_session() as sess:\n        evaluator = Evaluator(input_fn=input_fn, metric_name=""mse"", steps=3)\n        metrics = evaluator.evaluate(sess, ensemble_metrics=metrics)\n        self.assertEqual([9, 36], metrics)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_evaluate_invalid_metric(self):\n    with context.graph_mode():\n      input_fn = tu.dummy_input_fn([[1., 2]], [[3.]])\n      _, labels = input_fn()\n      predictions = [labels * 2, labels * 3]\n      metrics = []\n      for preds in predictions:\n        metrics.append({\n            ""mse"": tf_compat.v1.metrics.mean_squared_error(labels, preds),\n            ""other_metric_1"": (tf.constant(1), tf.constant(1)),\n            ""other_metric_2"": (tf.constant(2), tf.constant(2))\n        })\n\n      with self.test_session() as sess:\n        evaluator = Evaluator(input_fn=input_fn, metric_name=""dne"", steps=3)\n        with self.assertRaises(KeyError):\n          metrics = evaluator.evaluate(sess, ensemble_metrics=metrics)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/iteration.py,61,"b'""""""An AdaNet iteration implementation in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport copy\nimport json\nimport os\n\nfrom absl import logging\nfrom adanet import distributed\nfrom adanet import subnetwork\nfrom adanet import tf_compat\nfrom adanet.core.ensemble_builder import _EnsembleSpec\nfrom adanet.core.eval_metrics import _IterationMetrics\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nfrom typing import Any\n\n\nclass _TrainManager(object):\n  """"""Manages the training of SubnetworkSpecs and EnsembleSpecs.\n\n  This object maintains a dictionary of states for each SubnetworkSpec and\n  EnsembleSpec to coordinate and manage training. Users can check the\n  training status of a spec, or request that it stops training.\n\n  It also persists metadata about specs to disk in order to be consistent across\n  runs and robust to preemptions.\n  """"""\n\n  def __init__(self, subnetwork_specs, ensemble_specs, train_manager_dir,\n               is_chief):\n    """"""Initializes a _TrainManager instance.\n\n    Args:\n      subnetwork_specs: List of `_SubnetworkSpec` instances to monitor.\n      ensemble_specs: List of `EstimatorSpec` instances to monitor.\n      train_manager_dir: Directory for storing metadata about training. When a\n        spec should no longer be trained, a JSON file with its name and metadata\n        is written to this directory, to persist across runs and preemptions.\n      is_chief: Boolean whether the current worker is a chief.\n    """"""\n\n    if not tf.io.gfile.exists(train_manager_dir):\n      tf.io.gfile.makedirs(train_manager_dir)\n    self._train_manager_dir = train_manager_dir\n\n    self._is_training = {\n        spec.name: not self._is_done_training(spec)\n        for spec in subnetwork_specs + ensemble_specs\n    }\n    self._ensemble_specs = set([e.name for e in ensemble_specs])\n\n    self._is_chief = is_chief\n\n  def should_train(self, spec):\n    """"""Whether the given spec should keep training.""""""\n\n    return self._is_training[spec.name]\n\n  def _is_done_training(self, spec):\n    """"""If the file exists, then the candidate is done training.""""""\n\n    return tf.io.gfile.exists(self._filename_for(spec))\n\n  def _filename_for(self, spec):\n    """"""Returns the filename to identify the spec.""""""\n\n    return os.path.join(self._train_manager_dir, ""{}.json"".format(spec.name))\n\n  def request_stop(self, spec, message):\n    """"""Registers that given spec should no longer train.""""""\n\n    self._is_training[spec.name] = False\n\n    # Only write to disk if chief worker, otherwise there is a risk of conflicts\n    # and race conditions during writes.\n    if self._is_chief and not self._is_done_training(spec):\n      with tf.io.gfile.GFile(self._filename_for(spec), ""w"") as record_file:\n        # TODO: Consider making these messages be some kind of Enum.\n        # There # might be a case where we want to parse these files. For\n        # example, in iteration n+1, maybe we no longer even want to build\n        # NaN candidates.\n        message = {""message"": message}\n        record_file.write(json.dumps(message))\n\n  def is_over(self):\n    """"""Whether all specs are done training and the iteration is over.""""""\n\n    for k in sorted(self._is_training):\n      if k in self._ensemble_specs:\n        # In case the sub-estimator is done training (e.g. dataset ran out of\n        # data without repeat) but the ""max_iteration_steps"" is not reached.\n        continue\n      if self._is_training[k]:\n        # Still needs to train.\n        return False\n    return True\n\n\nclass _NanLossHook(tf_compat.SessionRunHook):\n  """"""Monitors a spec\'s loss tensor and stops its training if loss is NaN.""""""\n\n  def __init__(self, train_manager, spec):\n    """"""Initializes a `NanTensorHook`.\n\n    Args:\n      train_manager: The current iteration\'s `_TrainManager`.\n      spec: Either a `SubnetworkSpec` or `EnsembleSpec` to monitor.\n    """"""\n\n    self._train_manager = train_manager\n    self._spec = spec\n\n  def before_run(self, run_context):\n    del run_context  # Unused\n    if self._train_manager.should_train(self._spec):\n      return tf_compat.SessionRunArgs(self._spec.loss)\n\n  def after_run(self, run_context, run_values):\n    loss = run_values.results\n    if loss is None or not np.isnan(loss):\n      return\n    logging.warning(""\'%s\' diverged with loss = NaN."", self._spec.name)\n    # TODO: Re-enable once we know that evaluation won\'t\n    # fail from NaNs.\n    # self._train_manager.request_stop(self._spec, ""NaN loss during training."")\n\n\nclass _TrainingLimitHook(tf_compat.SessionRunHook):\n  """"""Limits a given spec\'s training to a maximum number of steps.\n\n  Is also responsible for incrementing the spec\'s step.\n  """"""\n\n  def __init__(self, train_manager, spec, max_steps, increment_step_op):\n    """"""Initializes a _TrainingLimitHook instance.\n\n    Args:\n      train_manager: The current iteration\'s `_TrainManager`.\n      spec: Either a `SubnetworkSpec` or `EnsembleSpec` to monitor.\n      max_steps: Maximum number steps to train the given spec.\n      increment_step_op: That increments the current step and executes one train\n        op run.\n    """"""\n\n    self._train_manager = train_manager\n    self._spec = spec\n    self._max_steps = max_steps\n    self._increment_step_op = increment_step_op\n\n  def after_create_session(self, session, coord):\n    if not self._train_manager.should_train(self._spec):\n      return\n    if self._spec.step is None:\n      # None for dummy candidates used during round-robin placement.\n      self._train_manager.request_stop(self._spec, ""Dummy candidate to ignore."")\n      return\n    step_value = session.run(self._spec.step)\n    if self._should_stop(step_value):\n      logging.info(""Skipping \'%s\' training which already trained %d steps"",\n                   self._spec.name, step_value)\n      self._train_manager.request_stop(self._spec, ""Training already complete."")\n\n  def before_run(self, run_context):\n    del run_context  # Unused\n    if not self._train_manager.should_train(self._spec):\n      return None\n    if self._increment_step_op is None:\n      # None on TPU.\n      return tf_compat.SessionRunArgs(self._spec.step)\n    return tf_compat.SessionRunArgs(self._increment_step_op)\n\n  def after_run(self, run_context, run_values):\n    step_value = run_values.results\n    if step_value is None:\n      return\n    if self._should_stop(step_value):\n      logging.info(""Now stopping \'%s\' training after %d steps"", self._spec.name,\n                   step_value)\n      self._train_manager.request_stop(\n          self._spec, ""Training complete after {} steps."".format(step_value))\n\n  def _should_stop(self, step):\n    return self._max_steps is not None and step >= self._max_steps\n\n\nclass _GlobalStepSetterHook(tf_compat.SessionRunHook):\n  """"""A hook for setting the global step variable.\n\n  Should only be run on CPU and GPU, but not TPU. TPUs run many training steps\n  per hook run, so the global step should be incremented in an op along with the\n  candidates\' train ops.\n  """"""\n\n  def __init__(self, train_manager, subnetwork_specs, base_global_step,\n               global_step_combiner_fn):\n    """"""Initializes a _GlobalStepSetterHook instance.\n\n    Args:\n      train_manager: The current iteration\'s `_TrainManager`.\n      subnetwork_specs: List of `_SubnetworkSpec` instances for this iteration.\n      base_global_step: Integer global step at the beginning of this iteration.\n      global_step_combiner_fn: Function for combining each subnetwork\'s\n        iteration step into the global step.\n    """"""\n\n    self._train_manager = train_manager\n    self._subnetwork_specs = subnetwork_specs\n    self._base_global_step = base_global_step\n    self._global_step_combiner_fn = global_step_combiner_fn\n\n  def begin(self):\n    logging.info(""Starting iteration at global step %s"", self._base_global_step)\n    steps = [\n        self._base_global_step + s.step.read_value()\n        for s in self._subnetwork_specs\n    ]\n    updated_global_step = self._global_step_combiner_fn(steps)\n    global_step = tf_compat.v1.train.get_global_step()\n    self._assign_global_step_op = global_step.assign(updated_global_step)\n\n  def after_run(self, run_context, run_values):\n    # Global step cannot be retrieved via SessionRunArgs and before_run due to\n    # race condition in hook execution.\n    run_context.session.run(self._assign_global_step_op)\n\n\nclass _TrainingHookRunnerHook(tf_compat.SessionRunHook):\n  """"""Hook wrapper for executing a spec\'s training hook.\n\n  Will only run the hook according to the current TrainManager.\n  """"""\n\n  def __init__(self, train_manager, spec, hook):\n    """"""Initializes a _TrainingHookRunnerHook instance.\n\n    Only accepts a single hook, since merging hooks is complex and should be\n    handled by the MonitoredTrainingSession instead.\n\n    Args:\n      train_manager: The current iteration\'s `_TrainManager`.\n      spec: Either a `SubnetworkSpec` or `EnsembleSpec` to train.\n      hook: The spec\'s training hook to execute.\n    """"""\n\n    self._train_manager = train_manager\n    self._spec = spec\n    self._hook = hook\n\n  def begin(self):\n    self._hook.begin()\n\n  @contextlib.contextmanager\n  def _session_run_context(self):\n    """"""Intercepts input out of range errors to gracefully stop spec training.""""""\n\n    try:\n      yield\n    except (tf.errors.OutOfRangeError, StopIteration) as e:\n      logging.info(""Now stopping \'%s\' training after hitting end of input"",\n                   self._spec.name)\n      self._train_manager.request_stop(self._spec,\n                                       ""OutOfRangeError: {}"".format(e))\n\n  def after_create_session(self, session, coord):\n    with self._session_run_context():\n      self._hook.after_create_session(session, coord)\n\n  def before_run(self, run_context):\n    if self._train_manager.should_train(self._spec):\n      # Use a tmp run context to intercept if the hook requests stop.\n      tmp_run_context = tf_compat.v1.train.SessionRunContext(\n          run_context.original_args, run_context.session)\n      with self._session_run_context():\n        return self._hook.before_run(tmp_run_context)\n      if tmp_run_context.stop_requested:\n        self._train_manager.request_stop(self._spec, ""Stop requested."")\n\n  def after_run(self, run_context, run_values):\n    if self._train_manager.should_train(self._spec):\n      # Use a tmp run context to intercept if the hook requests stop.\n      tmp_run_context = tf_compat.v1.train.SessionRunContext(\n          run_context.original_args, run_context.session)\n      with self._session_run_context():\n        self._hook.after_run(tmp_run_context, run_values)\n      if tmp_run_context.stop_requested:\n        self._train_manager.request_stop(self._spec, ""Stop requested."")\n\n  def end(self, session):\n    with self._session_run_context():\n      self._hook.end(session)\n\n\n# TODO: Replace candidates with ensemble_specs.\nclass _Iteration(\n    collections.namedtuple(""_Iteration"", [\n        ""number"", ""candidates"", ""subnetwork_specs"", ""estimator_spec"",\n        ""best_candidate_index"", ""summaries"", ""train_manager"",\n        ""subnetwork_reports"", ""checkpoint"", ""previous_iteration""\n    ])):\n  """"""An AdaNet iteration.\n\n  An AdaNet iteration represents the simultaneous training of multiple\n  candidates for one iteration of the AdaNet loop, and tracks the best\n  candidate\'s loss, predictions, and evaluation metrics.\n\n  There must be maximum one _Iteration per graph.\n  """"""\n\n  def __new__(cls, number, candidates, subnetwork_specs, estimator_spec,\n              best_candidate_index, summaries, train_manager,\n              subnetwork_reports, checkpoint, previous_iteration):\n    """"""Creates a validated `_Iteration` instance.\n\n    Args:\n      number: The iteration number.\n      candidates: List of `_Candidate` instances to track.\n      subnetwork_specs: List of `_SubnetworkSpec` instances.\n      estimator_spec: `EstimatorSpec` instance.\n      best_candidate_index: Int `Tensor` indicating the best candidate\'s index.\n      summaries: List of `adanet.Summary` instances for each candidate.\n      train_manager: The current `_TrainManager` for monitoring candidate per\n        training.\n      subnetwork_reports: Dict mapping string names to `subnetwork.Report`s, one\n        per candidate.\n      checkpoint: The `tf.train.Checkpoint` object associated with this\n        iteration.\n      previous_iteration: The iteration occuring before this one or None if this\n        is the first iteration.\n\n    Returns:\n      A validated `_Iteration` object.\n\n    Raises:\n      ValueError: If validation fails.\n    """"""\n\n    if not isinstance(number, (int, np.integer)):\n      raise ValueError(""number must be an integer"")\n    if number < 0:\n      raise ValueError(""number must be greater than 0 got %d"" % (number))\n    if not isinstance(candidates, list) or not candidates:\n      raise ValueError(""candidates must be a non-empty list"")\n    if estimator_spec is None:\n      raise ValueError(""estimator_spec is required"")\n    if best_candidate_index is None:\n      raise ValueError(""best_candidate_index is required"")\n    if not isinstance(subnetwork_reports, dict):\n      raise ValueError(""subnetwork_reports must be a dict"")\n    return super(_Iteration, cls).__new__(\n        cls,\n        number=number,\n        candidates=candidates,\n        subnetwork_specs=subnetwork_specs,\n        estimator_spec=estimator_spec,\n        best_candidate_index=best_candidate_index,\n        summaries=summaries,\n        train_manager=train_manager,\n        subnetwork_reports=subnetwork_reports,\n        checkpoint=checkpoint,\n        previous_iteration=previous_iteration)\n\n\ndef _is_numeric(tensor):\n  """"""Determines if given tensor is a float numeric.""""""\n\n  if not isinstance(tensor, tf.Tensor):\n    return False\n  return tensor.dtype in [tf.bfloat16, tf.float16, tf.float32, tf.float64]\n\n\nclass _IterationBuilder(object):\n  """"""Builds AdaNet iterations.""""""\n\n  def __init__(self,\n               candidate_builder,\n               subnetwork_manager,\n               ensemble_builder,\n               ensemblers,\n               max_steps,\n               summary_maker,\n               global_step_combiner_fn=tf.math.reduce_mean,\n               placement_strategy=distributed.ReplicationStrategy(),\n               replicate_ensemble_in_training=False,\n               use_tpu=False,\n               debug=False,\n               enable_ensemble_summaries=True,\n               enable_subnetwork_summaries=True,\n               enable_subnetwork_reports=True):\n    """"""Creates an `_IterationBuilder` instance.\n\n    Args:\n      candidate_builder: A `_CandidateBuilder` instance.\n      subnetwork_manager: A `_SubnetworkManager` instance.\n      ensemble_builder: An `_EnsembleBuilder` instance.\n      ensemblers: An iterable of :class:`adanet.ensemble.Ensembler` objects that\n        define how to ensemble a group of subnetworks.\n      max_steps: Maximum number of steps to train candidate subnetworks.\n      summary_maker: A function that constructs an `adanet.Summary` instance\n        from (namespace, scope, and skip_summary).\n      global_step_combiner_fn: Function for combining each subnetwork\'s\n        iteration step into the global step.\n      placement_strategy: A `PlacementStrategy` for assigning subnetworks and\n        ensembles to specific workers.\n      replicate_ensemble_in_training: Whether to build the frozen subnetworks in\n        `training` mode during training.\n      use_tpu: Whether AdaNet is running on TPU.\n      debug: Boolean to enable debug mode which will check features and labels\n        for Infs and NaNs.\n      enable_ensemble_summaries: Whether to record summaries to display in\n        TensorBoard for each ensemble candidate. Disable to reduce memory and\n        disk usage per run.\n      enable_subnetwork_summaries: Whether to record summaries to display in\n        TensorBoard for each subnetwork. Disable to reduce memory and disk usage\n        per run.\n      enable_subnetwork_reports: Whether to enable generating subnetwork\n        reports.\n\n    Returns:\n      An `_IterationBuilder` object.\n    """"""\n\n    if max_steps is not None and max_steps <= 0:\n      raise ValueError(""max_steps must be > 0 or None"")\n    self._candidate_builder = candidate_builder\n    self._subnetwork_manager = subnetwork_manager\n    self._ensemble_builder = ensemble_builder\n    self._ensemblers = ensemblers\n    self._max_steps = max_steps\n    self._summary_maker = summary_maker\n    self._global_step_combiner_fn = global_step_combiner_fn\n    self._placement_strategy = placement_strategy\n    self._replicate_ensemble_in_training = replicate_ensemble_in_training\n    self._use_tpu = use_tpu\n    self._debug = debug\n    self._enable_ensemble_summaries = enable_ensemble_summaries\n    self._enable_subnetwork_summaries = enable_subnetwork_summaries\n    self._enable_subnetwork_reports = enable_subnetwork_reports\n    super(_IterationBuilder, self).__init__()\n\n  @property\n  def placement_strategy(self):\n    return self._placement_strategy\n\n  @placement_strategy.setter\n  def placement_strategy(self, new_placement_strategy):\n    self._placement_strategy = new_placement_strategy\n\n  def _check_numerics(self, features, labels):\n    """"""Checks for NaNs and Infs in input features and labels.\n\n    Args:\n      features: Dictionary of `Tensor` objects keyed by feature name.\n      labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n        (for multi-head). Can be `None`.\n\n    Returns:\n      A features and labels tuple with same types and respective inputs, but\n      with numeric check ops wrapping them.\n    """"""\n\n    if not self._debug:\n      return features, labels\n\n    checked_features, checked_labels = {}, {}\n    logging.info(""DEBUG: Checking numerics of float features."")\n    for name in sorted(features):\n      if not _is_numeric(features[name]):\n        continue\n      logging.info(""DEBUG: Checking numerics of float feature \'%s\'."", name)\n      checked_features[name] = tf.debugging.check_numerics(\n          features[name], ""features \'{}\'"".format(name))\n    if isinstance(labels, dict):\n      for name in sorted(labels):\n        if not _is_numeric(labels[name]):\n          continue\n        logging.info(""DEBUG: Checking numerics of float label \'%s\'."", name)\n        checked_labels[name] = tf.debugging.check_numerics(\n            labels[name], ""labels \'{}\'"".format(name))\n    elif labels is not None and _is_numeric(labels):\n      logging.info(""DEBUG: Checking numerics of labels."")\n      checked_labels = tf.debugging.check_numerics(labels, ""\'labels\'"")\n    return checked_features, checked_labels\n\n  def build_iteration(self,\n                      base_global_step,\n                      iteration_number,\n                      ensemble_candidates,\n                      subnetwork_builders,\n                      features,\n                      mode,\n                      config,\n                      labels=None,\n                      previous_ensemble_summary=None,\n                      rebuilding=False,\n                      rebuilding_ensembler_name=None,\n                      best_ensemble_index_override=None,\n                      previous_iteration=None):\n    """"""Builds and returns AdaNet iteration t.\n\n    This method uses the generated the candidate subnetworks given the ensemble\n    at iteration t-1 and creates graph operations to train them. The returned\n    `_Iteration` tracks the training of all candidates to know when the\n    iteration is over, and tracks the best candidate\'s predictions and loss, as\n    defined by lowest complexity-regularized loss on the train set.\n\n    Args:\n      base_global_step: Integer global step at the beginning of this iteration.\n      iteration_number: Integer iteration number.\n      ensemble_candidates: Iterable of `adanet.ensemble.Candidate` instances.\n      subnetwork_builders: A list of `Builders` for adding ` Subnetworks` to the\n        graph. Each subnetwork is then wrapped in a `_Candidate` to train.\n      features: Dictionary of `Tensor` objects keyed by feature name.\n      mode: Defines whether this is training, evaluation or prediction. See\n        `ModeKeys`.\n      config: The `tf.estimator.RunConfig` to use this iteration.\n      labels: `Tensor` of labels. Can be `None`.\n      previous_ensemble_summary: The `adanet.Summary` for the previous ensemble.\n      rebuilding: Boolean whether the iteration is being rebuilt only to restore\n        the previous best subnetworks and ensembles.\n      rebuilding_ensembler_name: Optional ensembler to restrict to, only\n        relevant when rebuilding is set as True.\n      best_ensemble_index_override: Integer index to identify the best ensemble\n        candidate instead of computing the best ensemble index dynamically\n        conditional on the ensemble AdaNet losses.\n      previous_iteration: The iteration occuring before this one or None if this\n        is the first iteration.\n\n    Returns:\n      An _Iteration instance.\n\n    Raises:\n      ValueError: If subnetwork_builders is empty.\n      ValueError: If two subnetworks share the same name.\n      ValueError: If two ensembles share the same name.\n    """"""\n\n    self._placement_strategy.config = config\n\n    logging.info(""%s iteration %s"", ""Rebuilding"" if rebuilding else ""Building"",\n                 iteration_number)\n\n    if not subnetwork_builders:\n      raise ValueError(""Each iteration must have at least one Builder."")\n\n    # TODO: Consider moving builder mode logic to ensemble_builder.py.\n    builder_mode = mode\n    if rebuilding:\n      # Build the subnetworks and ensembles in EVAL mode by default. This way\n      # their outputs aren\'t affected by dropout etc.\n      builder_mode = tf.estimator.ModeKeys.EVAL\n      if mode == tf.estimator.ModeKeys.PREDICT:\n        builder_mode = mode\n\n      # Only replicate in training mode when the user requests it.\n      if self._replicate_ensemble_in_training and (\n          mode == tf.estimator.ModeKeys.TRAIN):\n        builder_mode = mode\n\n    features, labels = self._check_numerics(features, labels)\n    replay_indices_for_all = {}\n    training = mode == tf.estimator.ModeKeys.TRAIN\n    skip_summaries = mode == tf.estimator.ModeKeys.PREDICT or rebuilding\n    with tf_compat.v1.variable_scope(""iteration_{}"".format(iteration_number)):\n      seen_builder_names = {}\n      candidates = []\n      summaries = []\n      subnetwork_reports = {}\n      previous_ensemble = None\n      previous_ensemble_spec = None\n      previous_iteration_checkpoint = None\n      if previous_iteration:\n        previous_iteration_checkpoint = previous_iteration.checkpoint\n        previous_best_candidate = previous_iteration.candidates[-1]\n        previous_ensemble_spec = previous_best_candidate.ensemble_spec\n        previous_ensemble = previous_ensemble_spec.ensemble\n        replay_indices_for_all[len(candidates)] = copy.copy(\n            previous_ensemble_spec.architecture.replay_indices)\n        # Include previous best subnetwork as a candidate so that its\n        # predictions are returned until a new candidate outperforms.\n        seen_builder_names = {previous_ensemble_spec.name: True}\n        candidates.append(previous_best_candidate)\n        if self._enable_ensemble_summaries:\n          summaries.append(previous_ensemble_summary)\n\n        # Generate subnetwork reports.\n        if (self._enable_subnetwork_reports and\n            mode == tf.estimator.ModeKeys.EVAL):\n          metrics = previous_ensemble_spec.eval_metrics.eval_metrics_ops()\n          subnetwork_report = subnetwork.Report(\n              hparams={},\n              attributes={},\n              metrics=metrics,\n          )\n          subnetwork_report.metrics[""adanet_loss""] = tf_compat.v1.metrics.mean(\n              previous_ensemble_spec.adanet_loss)\n          subnetwork_reports[""previous_ensemble""] = subnetwork_report\n\n      for subnetwork_builder in subnetwork_builders:\n        if subnetwork_builder.name in seen_builder_names:\n          raise ValueError(""Two subnetworks have the same name \'{}\'"".format(\n              subnetwork_builder.name))\n        seen_builder_names[subnetwork_builder.name] = True\n      subnetwork_specs = []\n      num_subnetworks = len(subnetwork_builders)\n      skip_summary = skip_summaries or not self._enable_subnetwork_summaries\n      for i, subnetwork_builder in enumerate(subnetwork_builders):\n        if not self._placement_strategy.should_build_subnetwork(\n            num_subnetworks, i) and not rebuilding:\n          continue\n        with self._placement_strategy.subnetwork_devices(num_subnetworks, i):\n          subnetwork_name = ""t{}_{}"".format(iteration_number,\n                                            subnetwork_builder.name)\n          subnetwork_summary = self._summary_maker(\n              namespace=""subnetwork"",\n              scope=subnetwork_name,\n              skip_summary=skip_summary)\n          if not skip_summary:\n            summaries.append(subnetwork_summary)\n          logging.info(""%s subnetwork \'%s\'"",\n                       ""Rebuilding"" if rebuilding else ""Building"",\n                       subnetwork_builder.name)\n          subnetwork_spec = self._subnetwork_manager.build_subnetwork_spec(\n              name=subnetwork_name,\n              subnetwork_builder=subnetwork_builder,\n              summary=subnetwork_summary,\n              features=features,\n              mode=builder_mode,\n              labels=labels,\n              previous_ensemble=previous_ensemble,\n              config=config)\n          subnetwork_specs.append(subnetwork_spec)\n          # Workers that don\'t build ensembles need a dummy candidate in order\n          # to train the subnetwork.\n          # Because only ensembles can be considered candidates, we need to\n          # convert the subnetwork into a dummy ensemble and subsequently a\n          # dummy candidate. However, this dummy candidate is never considered a\n          # true candidate during candidate evaluation and selection.\n          # TODO: Eliminate need for candidates.\n          if not self._placement_strategy.should_build_ensemble(\n              num_subnetworks) and not rebuilding:\n            candidates.append(\n                self._create_dummy_candidate(subnetwork_spec,\n                                             subnetwork_builders,\n                                             subnetwork_summary, training))\n        # Generate subnetwork reports.\n        if (self._enable_subnetwork_reports and\n            mode != tf.estimator.ModeKeys.PREDICT):\n          subnetwork_report = subnetwork_builder.build_subnetwork_report()\n          if not subnetwork_report:\n            subnetwork_report = subnetwork.Report(\n                hparams={}, attributes={}, metrics={})\n          metrics = subnetwork_spec.eval_metrics.eval_metrics_ops()\n          for metric_name in sorted(metrics):\n            metric = metrics[metric_name]\n            subnetwork_report.metrics[metric_name] = metric\n          subnetwork_reports[subnetwork_builder.name] = subnetwork_report\n\n      # Create (ensemble_candidate*ensembler) ensembles.\n      skip_summary = skip_summaries or not self._enable_ensemble_summaries\n      seen_ensemble_names = {}\n      for ensembler in self._ensemblers:\n        if rebuilding and rebuilding_ensembler_name and (\n            ensembler.name != rebuilding_ensembler_name):\n          continue\n        for ensemble_candidate in ensemble_candidates:\n          if not self._placement_strategy.should_build_ensemble(\n              num_subnetworks) and not rebuilding:\n            continue\n          ensemble_name = ""t{}_{}_{}"".format(iteration_number,\n                                             ensemble_candidate.name,\n                                             ensembler.name)\n          if ensemble_name in seen_ensemble_names:\n            raise ValueError(\n                ""Two ensembles have the same name \'{}\'"".format(ensemble_name))\n          seen_ensemble_names[ensemble_name] = True\n          summary = self._summary_maker(\n              namespace=""ensemble"",\n              scope=ensemble_name,\n              skip_summary=skip_summary)\n          if not skip_summary:\n            summaries.append(summary)\n          ensemble_spec = self._ensemble_builder.build_ensemble_spec(\n              name=ensemble_name,\n              candidate=ensemble_candidate,\n              ensembler=ensembler,\n              subnetwork_specs=subnetwork_specs,\n              summary=summary,\n              features=features,\n              mode=builder_mode,\n              iteration_number=iteration_number,\n              labels=labels,\n              my_ensemble_index=len(candidates),\n              previous_ensemble_spec=previous_ensemble_spec,\n              previous_iteration_checkpoint=previous_iteration_checkpoint)\n          # TODO: Eliminate need for candidates.\n          candidate = self._candidate_builder.build_candidate(\n              ensemble_spec=ensemble_spec,\n              training=training,\n              summary=summary,\n              rebuilding=rebuilding)\n          replay_indices_for_all[len(candidates)] = copy.copy(\n              ensemble_spec.architecture.replay_indices)\n          candidates.append(candidate)\n          # TODO: Move adanet_loss from subnetwork report to a new\n          # ensemble report, since the adanet_loss is associated with an\n          # ensemble, and only when using a ComplexityRegularizedEnsemblers.\n          # Keep adanet_loss in subnetwork report for backwards compatibility.\n          if len(ensemble_candidates) != len(subnetwork_builders):\n            continue\n          if len(ensemble_candidate.subnetwork_builders) > 1:\n            continue\n          if mode == tf.estimator.ModeKeys.PREDICT:\n            continue\n          builder_name = ensemble_candidate.subnetwork_builders[0].name\n          if self._enable_subnetwork_reports:\n            subnetwork_reports[builder_name].metrics[\n                ""adanet_loss""] = tf_compat.v1.metrics.mean(\n                    ensemble_spec.adanet_loss)\n\n      # Dynamically select the outputs of best candidate.\n      best_candidate_index = self._best_candidate_index(\n          candidates, best_ensemble_index_override)\n      best_predictions = self._best_predictions(candidates,\n                                                best_candidate_index)\n      best_loss = self._best_loss(candidates, best_candidate_index, mode)\n      best_export_outputs = self._best_export_outputs(candidates,\n                                                      best_candidate_index,\n                                                      mode, best_predictions)\n      train_manager_dir = os.path.join(config.model_dir, ""train_manager"",\n                                       ""t{}"".format(iteration_number))\n      train_manager, training_chief_hooks, training_hooks = self._create_hooks(\n          base_global_step, subnetwork_specs, candidates, num_subnetworks,\n          rebuilding, train_manager_dir, config.is_chief)\n\n      local_init_ops = []\n      if previous_ensemble_spec:\n        for s in previous_ensemble_spec.ensemble.subnetworks:\n          if s.local_init_ops:\n            local_init_ops.extend(s.local_init_ops)\n      for subnetwork_spec in subnetwork_specs:\n        if (subnetwork_spec and subnetwork_spec.subnetwork and\n            subnetwork_spec.subnetwork.local_init_ops):\n          local_init_ops.extend(subnetwork_spec.subnetwork.local_init_ops)\n\n      summary = self._summary_maker(\n          namespace=None, scope=None, skip_summary=skip_summaries)\n      summaries.append(summary)\n      with summary.current_scope():\n        summary.scalar(""iteration/adanet/iteration"", iteration_number)\n        if best_loss is not None:\n          summary.scalar(""loss"", best_loss)\n      iteration_metrics = _IterationMetrics(iteration_number, candidates,\n                                            subnetwork_specs, self._use_tpu,\n                                            replay_indices_for_all)\n      checkpoint = self._make_checkpoint(candidates, subnetwork_specs,\n                                         iteration_number, previous_iteration)\n      if self._use_tpu:\n        estimator_spec = tf_compat.v1.estimator.tpu.TPUEstimatorSpec(\n            mode=mode,\n            predictions=best_predictions,\n            loss=best_loss,\n            train_op=self._create_tpu_train_op(base_global_step,\n                                               subnetwork_specs, candidates,\n                                               mode, num_subnetworks, config),\n            eval_metrics=iteration_metrics.best_eval_metrics_tuple(\n                best_candidate_index, mode),\n            export_outputs=best_export_outputs,\n            training_hooks=training_hooks,\n            scaffold_fn=self._get_scaffold_fn(local_init_ops))\n      else:\n        estimator_spec = tf.estimator.EstimatorSpec(\n            mode=mode,\n            predictions=best_predictions,\n            loss=best_loss,\n            # All training happens in hooks so we don\'t need a train op.\n            train_op=tf.no_op() if training else None,\n            eval_metric_ops=iteration_metrics.best_eval_metric_ops(\n                best_candidate_index, mode),\n            export_outputs=best_export_outputs,\n            training_chief_hooks=training_chief_hooks,\n            training_hooks=training_hooks,\n            scaffold=self._get_scaffold_fn(local_init_ops)())\n\n      return _Iteration(\n          number=iteration_number,\n          candidates=candidates,\n          subnetwork_specs=subnetwork_specs,\n          estimator_spec=estimator_spec,\n          best_candidate_index=best_candidate_index,\n          summaries=summaries,\n          train_manager=train_manager,\n          subnetwork_reports=subnetwork_reports,\n          checkpoint=checkpoint,\n          previous_iteration=previous_iteration)\n\n  def _get_scaffold_fn(self, local_init_ops):\n    """"""Creates a method generating a scaffold.\n\n    TODO: Make this code compatible with TPU estimators.\n\n    Args:\n      local_init_ops: List of tf.Operations to call during initialization.\n\n    Returns:\n      Method returning a `tf.train.Scaffold`.\n    """"""\n\n    def get_scaffold():\n      return tf_compat.v1.train.Scaffold(\n          local_init_op=tf.group(\n              local_init_ops +\n              [tf_compat.v1.train.Scaffold.default_local_init_op()]))\n\n    return get_scaffold\n\n  def _create_dummy_candidate(self, subnetwork_spec, subnetwork_builders,\n                              subnetwork_summary, training):\n    """"""Returns a dummy candidate for the given SubnetworkSpec.\n\n    AdaNet only considers ensembles as candidate models, and ensembles\n    are represented as `_Candidates`. When training only subnetworks, such as\n    on a subnetwork-worker in the RoundRobinStrategy, then we still need a\n    candidate to manage the training of the subnetwork, even if it gets\n    discarded, hence the dummy candidate.\n\n    Args:\n      subnetwork_spec: The subnetwork spec for the dummy candidate to wrap.\n      subnetwork_builders: List of all subnetwork builders generated this\n        iteration.\n      subnetwork_summary: `_Summary` object to use for TensorBoard.\n      training: Whether or not we are currently training.\n    """"""\n\n    dummy_ensemble_spec = _EnsembleSpec(\n        name=""dummy_{}"".format(subnetwork_spec.name),\n        ensemble=None,\n        architecture=None,\n        subnetwork_builders=subnetwork_builders,\n        predictions=subnetwork_spec.predictions,\n        loss=subnetwork_spec.loss,\n        step=None,\n        adanet_loss=0.,\n        variables=[])\n    return self._candidate_builder.build_candidate(\n        ensemble_spec=dummy_ensemble_spec,\n        training=training,\n        summary=subnetwork_summary,\n        track_moving_average=False)\n\n  def _create_tpu_train_op(self, base_global_step, subnetwork_specs, candidates,\n                           mode, num_subnetworks, config):\n    """"""Returns the train op for this set of candidates.\n\n    This train op combines the train ops from all the candidates into a single\n    train op. Additionally, it is responsible for incrementing the global step.\n\n    The train op is only non-None during the `TRAIN` mode.\n\n    Args:\n      base_global_step: Integer global step at the beginning of this iteration.\n      subnetwork_specs: List of `_SubnetworkSpec` instances for this iteration.\n      candidates: List of `_Candidate` instances to train.\n      mode: Defines whether this is training, evaluation or inference. The train\n        op is only non-None during `TRAIN`. See `ModeKeys`.\n      num_subnetworks: Integer number of subnetwork builders generated for the\n        current iteration.\n      config: The `tf.estimator.RunConfig` to use this iteration.\n\n    Returns:\n      A `Tensor` train op.\n    """"""\n\n    if mode != tf.estimator.ModeKeys.TRAIN:\n      return None\n    ensemble_specs = [c.ensemble_spec for c in candidates]\n    with tf_compat.v1.variable_scope(""train_op""):\n      train_ops = []\n      if self._placement_strategy.should_train_subnetworks(num_subnetworks):\n        for subnetwork_spec in subnetwork_specs:\n          if subnetwork_spec.train_op is not None:\n            train_ops.append(subnetwork_spec.train_op.train_op)\n      for ensemble_spec in ensemble_specs:\n        if ensemble_spec.train_op is not None:\n          # The train op of a previous ensemble is None even during `TRAIN`.\n          train_ops.append(ensemble_spec.train_op.train_op)\n\n      with tf.control_dependencies(train_ops):\n        # Increment steps after train ops complete to avoid non-determinism.\n        increment_ops = [s.step.assign_add(1) for s in subnetwork_specs]\n        increment_ops += [e.step.assign_add(1) for e in ensemble_specs]\n\n        if not config.is_chief:\n          return tf.group(*increment_ops)\n        # AdaNet\'s chief worker is responsible for setting the global step, not\n        # the candidates it trains. Assigning the global step is the final\n        # action performed in the train op.\n        with tf.control_dependencies(increment_ops):\n          steps = [s.step.read_value() for s in subnetwork_specs]\n          global_step = tf_compat.v1.train.get_global_step()\n          return global_step.assign(\n              tf.cast(\n                  base_global_step + self._global_step_combiner_fn(steps),\n                  dtype=tf.int64))\n\n  def _create_hooks(self, base_global_step, subnetwork_specs, candidates,\n                    num_subnetworks, rebuilding, train_manager_dir, is_chief):\n    """"""Returns the hooks to monitor and train this iteration.\n\n    Args:\n      base_global_step: Integer global step at the beginning of this iteration.\n      subnetwork_specs: List of `_SubnetworkSpec` instances.\n      candidates: List of `_Candidate` instances to compare.\n      num_subnetworks: Integer number of subnetwork builders generated for the\n        current iteration.\n      rebuilding: Boolean whether the iteration is being rebuilt only to restore\n        the previous best subnetworks and ensembles.\n      train_manager_dir: Directory for the TrainManager to store spec metadata.\n      is_chief: Whether the current worker is chief.\n\n    Returns:\n      A 3-tuple of a _TrainManager for monitoring training, a list of\n      `SessionRunHooks` to run on chief, and a list of `SessionRunHooks` to run\n      on all workers.\n    """"""\n\n    training_chief_hooks, training_hooks = [], []\n    ensemble_specs = [c.ensemble_spec for c in candidates]\n    train_manager = _TrainManager(subnetwork_specs, ensemble_specs,\n                                  train_manager_dir, is_chief)\n    if not self._use_tpu:\n      # On TPU, the global step gets incremented in an op since it doesn\'t have\n      # hook run granularity of CPU and GPU training.\n      training_chief_hooks.append(\n          _GlobalStepSetterHook(train_manager, subnetwork_specs,\n                                base_global_step,\n                                self._global_step_combiner_fn))\n    should_train_subnetworks = (\n        self._placement_strategy.should_train_subnetworks(num_subnetworks))\n    for spec in subnetwork_specs:\n      if not self._use_tpu:\n        training_hooks.append(_NanLossHook(train_manager, spec))\n      # We increment the step along with the global step as part of the train\n      # op on TPU, whereas on CPU and GPU we use hooks for fine grained control.\n      if self._use_tpu or not should_train_subnetworks or spec.train_op is None:\n        increment_step_op = None\n      else:\n        with tf.control_dependencies([spec.train_op.train_op]):\n          increment_step_op = spec.step.assign_add(1)\n      # TPU also supports uneven training, but up to num_iterations_per_loop.\n      training_hooks.append(\n          _TrainingLimitHook(\n              train_manager,\n              spec,\n              self._max_steps,\n              increment_step_op=increment_step_op))\n      if not should_train_subnetworks and not rebuilding:\n        continue\n      self._add_hooks(spec, train_manager, training_chief_hooks, training_hooks)\n    for spec in ensemble_specs:\n      if not self._use_tpu:\n        training_hooks.append(_NanLossHook(train_manager, spec))\n      # See above comment about incrementing the step on CPU vs. TPU.\n      if self._use_tpu or spec.train_op is None:\n        increment_step_op = None\n      else:\n        with tf.control_dependencies([spec.train_op.train_op]):\n          increment_step_op = spec.step.assign_add(1)\n      training_hooks.append(\n          _TrainingLimitHook(\n              train_manager,\n              spec,\n              self._max_steps,\n              increment_step_op=increment_step_op))\n      self._add_hooks(spec, train_manager, training_chief_hooks, training_hooks)\n    return train_manager, training_chief_hooks, training_hooks\n\n  def _add_hooks(self, spec, train_manager, training_chief_hooks,\n                 training_hooks):\n    """"""Appends spec train hooks to the given hook lists.""""""\n\n    if not spec.train_op:\n      return\n    for hook in spec.train_op.chief_hooks:\n      training_chief_hooks.append(\n          _TrainingHookRunnerHook(train_manager, spec, hook))\n    for hook in spec.train_op.hooks:\n      training_hooks.append(_TrainingHookRunnerHook(train_manager, spec, hook))\n\n  def _best_candidate_index(self, candidates, best_ensemble_index_override):\n    """"""Returns the index of the best candidate in the list.\n\n    The best candidate is the one with the smallest AdaNet loss, unless\n    `best_ensemble_index_override` is given.\n\n    TODO: Best ensemble index should always be static during EVAL\n    and PREDICT modes.\n\n    In case a candidate has a NaN loss, their loss is immediately set to\n    infinite, so that they are not selected. As long as one candidate ensemble\n    has a non-NaN loss during training, the dreaded `NanLossDuringTrainingError`\n    should not be raised.\n\n    Args:\n      candidates: List of `_Candidate` instances to choose from.\n      best_ensemble_index_override: Integer index to return instead of computing\n        the best ensemble index dynamically.\n\n    Returns:\n      An integer `Tensor` representing the index of the best candidate.\n    """"""\n\n    with tf_compat.v1.variable_scope(""best_candidate_index""):\n      if best_ensemble_index_override is not None:\n        return tf.constant(best_ensemble_index_override)\n\n      if len(candidates) == 1:\n        return tf.constant(0)\n      adanet_losses = [candidate.adanet_loss for candidate in candidates]\n      # Replace NaNs with -Infs so that NaN loss candidates are always chosen,\n      # causing tf.estimator.Estimator to raise a NanLossDuringTrainingError.\n      adanet_losses = tf.where(\n          tf_compat.v1.is_nan(adanet_losses),\n          tf.ones_like(adanet_losses) * -np.inf, adanet_losses)\n      return tf.argmin(input=adanet_losses, axis=0)\n\n  def _best_predictions(self, candidates, best_candidate_index):\n    """"""Returns the best predictions from a set of candidates.\n\n    Args:\n      candidates: List of `_Candidate` instances to compare.\n      best_candidate_index: `Tensor` index of the best candidate in the list.\n\n    Returns:\n      A `Tensor` or dictionary of `Tensor`s representing the best candidate\'s\n      predictions (depending on what the subnetworks return).\n    """"""\n\n    if len(candidates) == 1:\n      return candidates[0].ensemble_spec.predictions\n\n    with tf_compat.v1.variable_scope(""best_predictions""):\n      if isinstance(candidates[0].ensemble_spec.predictions, dict):\n        predictions = {}\n        for candidate in candidates:\n          ensemble_spec = candidate.ensemble_spec\n          for key in sorted(ensemble_spec.predictions):\n            tensor = ensemble_spec.predictions[key]\n            if key in predictions:\n              predictions[key].append(tensor)\n            else:\n              predictions[key] = [tensor]\n      else:\n        predictions = []\n        for candidate in candidates:\n          ensemble_spec = candidate.ensemble_spec\n          predictions.append(ensemble_spec.predictions)\n\n      if isinstance(predictions, dict):\n        best_predictions = {}\n        for key in sorted(predictions):\n          tensor_list = predictions[key]\n          best_predictions[key] = tf.stack(tensor_list)[best_candidate_index]\n      else:\n        best_predictions = tf.stack(predictions)[best_candidate_index]\n      return best_predictions\n\n  def _best_loss(self, candidates, best_candidate_index, mode):\n    """"""Returns the best loss from a set of candidates.\n\n    Args:\n      candidates: List of `_Candidate` instances to compare.\n      best_candidate_index: `Tensor` index of the best candidate in the list.\n      mode: Defines whether this is training, evaluation or inference. Loss is\n        always None during inference. See `ModeKeys`.\n\n    Returns:\n      Float `Tensor` of the best candidate\'s loss.\n    """"""\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return None\n    if len(candidates) == 1:\n      return candidates[0].ensemble_spec.loss\n    with tf_compat.v1.variable_scope(""best_loss""):\n      losses = [c.ensemble_spec.loss for c in candidates]\n      loss = tf.slice(tf.stack(losses), [best_candidate_index], [1])\n      return tf.reshape(loss, [])\n\n  def _best_export_outputs(self, candidates, best_candidate_index, mode,\n                           best_predictions):\n    """"""Returns the best `SavedModel` export outputs from a set of candidates.\n\n    Assumes that all candidate ensembles have identical export output keys and\n    `ExportOutput` types.\n\n    Args:\n      candidates: List of `_Candidate` instances to compare.\n      best_candidate_index: `Tensor` index of the best candidate in the list.\n      mode: Defines whether this is training, evaluation or inference. Export\n        outputs are always None during training and evaluation. See `ModeKeys`.\n      best_predictions: A `Tensor` or dictionary of `Tensor`s representing the\n        best candidate\'s predictions (depending on what the subnetworks return).\n\n    Returns:\n      A `Tensor` dictionary representing the best candidate\'s export outputs.\n\n    Raises:\n      TypeError: If the `ExportOutput` type is not supported.\n    """"""\n\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      return None\n    if len(candidates) == 1:\n      return candidates[0].ensemble_spec.export_outputs\n    with tf_compat.v1.variable_scope(""best_export_outputs""):\n      # Group tensors by export output key and ExportOutput type.\n      export_outputs = {}  # type: Any\n      for candidate in candidates:\n        ensemble_spec = candidate.ensemble_spec\n        for key in sorted(ensemble_spec.export_outputs):\n          export_output = ensemble_spec.export_outputs[key]\n          if isinstance(export_output,\n                        tf.estimator.export.ClassificationOutput):\n            if key not in export_outputs:\n              export_outputs[key] = ([], [])\n            if export_output.scores is not None:\n              export_outputs[key][0].append(export_output.scores)\n            if export_output.classes is not None:\n              export_outputs[key][1].append(export_output.classes)\n          elif isinstance(export_output, tf.estimator.export.RegressionOutput):\n            if key not in export_outputs:\n              export_outputs[key] = []\n            export_outputs[key].append(export_output.value)\n          elif isinstance(export_output, tf.estimator.export.PredictOutput):\n            # Use self._best_predictions() below to get prediction output.\n            continue\n          else:\n            raise TypeError(\n                ""Values in export_outputs must be ClassificationOutput, ""\n                ""RegressionOutput, or PredictOutput objects. Given: {}"".format(\n                    export_output))\n\n      # Stack tensor lists into correct ExportOutput type, outputting the\n      # correct values based on the best candidate index.\n      best_export_outputs = {}\n      for key in sorted(candidates[0].ensemble_spec.export_outputs):\n        export_output = candidates[0].ensemble_spec.export_outputs[key]\n        if isinstance(export_output, tf.estimator.export.ClassificationOutput):\n          scores, classes = None, None\n          if export_outputs[key][0]:\n            scores = tf.stack(export_outputs[key][0])[best_candidate_index]\n          if export_outputs[key][1]:\n            classes = tf.stack(export_outputs[key][1])[best_candidate_index]\n          output = tf.estimator.export.ClassificationOutput(\n              scores=scores, classes=classes)\n        elif isinstance(export_output, tf.estimator.export.RegressionOutput):\n          value = tf.stack(export_outputs[key])[best_candidate_index]\n          output = tf.estimator.export.RegressionOutput(value)\n        else:\n          predictions = copy.copy(export_output.outputs)\n          predictions.update(best_predictions)\n          output = tf.estimator.export.PredictOutput(predictions)\n        best_export_outputs[key] = output\n      return best_export_outputs\n\n  def _make_checkpoint(self, candidates, subnetwork_specs, iteration_number,\n                       previous_iteration):\n    """"""Returns a `tf.train.Checkpoint` for the iteration.""""""\n\n    # TODO: Handle hook created variables.\n    # TODO: Handle TPU embedding variables.\n    trackable = {}\n\n    for candidate in candidates:\n      for ensemble_var in candidate.ensemble_spec.variables:\n        trackable[""{}_{}"".format(candidate.ensemble_spec.name,\n                                 ensemble_var.name)] = ensemble_var\n      for candidate_var in candidate.variables:\n        trackable[""candidate_{}_{}"".format(candidate.ensemble_spec.name,\n                                           candidate_var.name)] = candidate_var\n\n    for subnetwork_spec in subnetwork_specs:\n      for subnetwork_var in subnetwork_spec.variables:\n        trackable[""{}_{}"".format(subnetwork_spec.name,\n                                 subnetwork_var.name)] = subnetwork_var\n\n    global_step = tf_compat.v1.train.get_global_step()\n    # TODO: Currently, TPUEstimator has no global_step set when\n    # exporting the saved model.\n    if global_step is not None:\n      trackable[tf_compat.v1.GraphKeys.GLOBAL_STEP] = global_step\n\n    trackable[""iteration_number""] = tf_compat.v1.get_variable(\n        ""iteration_number"",\n        dtype=tf.int64,\n        # Lambda initializer required for TPU.\n        initializer=lambda: tf.constant(iteration_number, dtype=tf.int64),\n        trainable=False)\n    if previous_iteration:\n      trackable[""previous_iteration""] = previous_iteration.checkpoint\n\n    logging.info(""TRACKABLE: %s"", trackable)\n\n    checkpoint = tf_compat.v2.train.Checkpoint(**trackable)\n    # Make the save counter to satisfy the assert_consumed() assertion later.\n    # This property creates variables the first time it is called.\n    checkpoint.save_counter  # pylint: disable=pointless-statement\n    return checkpoint\n'"
adanet/core/iteration_test.py,46,"b'""""""Test AdaNet iteration single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core.architecture import _Architecture\nfrom adanet.core.candidate import _Candidate\nfrom adanet.core.ensemble_builder import _EnsembleSpec\nfrom adanet.core.ensemble_builder import _SubnetworkSpec\nfrom adanet.core.iteration import _Iteration\nfrom adanet.core.iteration import _IterationBuilder\nfrom adanet.core.iteration import _TrainManager\nfrom adanet.core.summary import _ScopedSummary\nfrom adanet.core.summary import _TPUScopedSummary\nimport adanet.core.testing_utils as tu\nfrom adanet.ensemble import Candidate as EnsembleCandidate\nfrom adanet.subnetwork import Builder as SubnetworkBuilder\nfrom adanet.subnetwork import Report as SubnetworkReport\nfrom adanet.subnetwork import Subnetwork\nfrom adanet.subnetwork import TrainOpSpec\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\nfrom tensorflow_estimator.python.estimator.head import binary_class_head\nfrom tensorflow_estimator.python.estimator.head import regression_head\n\n\ndef _dummy_candidate():\n  """"""Returns a dummy `_Candidate` instance.""""""\n\n  return _Candidate(\n      ensemble_spec=tu.dummy_ensemble_spec(""foo""),\n      adanet_loss=1.,\n      variables=[tf.Variable(1.)])\n\n\nclass IterationTest(tu.AdanetTestCase):\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""single_candidate"",\n          ""number"": 0,\n          ""candidates"": [_dummy_candidate()],\n          ""estimator_spec"": tu.dummy_estimator_spec(),\n          ""best_candidate_index"": 0,\n      }, {\n          ""testcase_name"": ""two_candidates"",\n          ""number"": 0,\n          ""candidates"": [_dummy_candidate(),\n                         _dummy_candidate()],\n          ""estimator_spec"": tu.dummy_estimator_spec(),\n          ""best_candidate_index"": 0,\n      }, {\n          ""testcase_name"": ""positive_number"",\n          ""number"": 1,\n          ""candidates"": [_dummy_candidate()],\n          ""estimator_spec"": tu.dummy_estimator_spec(),\n          ""best_candidate_index"": 0,\n      }, {\n          ""testcase_name"": ""zero_best_predictions"",\n          ""number"": 1,\n          ""candidates"": [_dummy_candidate()],\n          ""estimator_spec"": tu.dummy_estimator_spec(),\n          ""best_candidate_index"": 0,\n      }, {\n          ""testcase_name"": ""zero_best_loss"",\n          ""number"": 1,\n          ""candidates"": [_dummy_candidate()],\n          ""estimator_spec"": tu.dummy_estimator_spec(),\n          ""best_candidate_index"": 0,\n      }, {\n          ""testcase_name"":\n              ""pass_subnetwork_report"",\n          ""number"":\n              1,\n          ""candidates"": [_dummy_candidate()],\n          ""estimator_spec"":\n              tu.dummy_estimator_spec(),\n          ""best_candidate_index"":\n              0,\n          ""subnetwork_reports_fn"":\n              lambda: {\n                  ""foo"":\n                      SubnetworkReport(\n                          hparams={""dropout"": 1.0},\n                          attributes={""aoo"": tf.constant(""aoo"")},\n                          metrics={\n                              ""moo"": (tf.constant(""moo1""), tf.constant(""moo2""))\n                          })\n              },\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_new(self,\n               number,\n               candidates,\n               estimator_spec,\n               best_candidate_index,\n               subnetwork_reports_fn=None):\n    if subnetwork_reports_fn is None:\n      subnetwork_reports = {}\n    else:\n      subnetwork_reports = subnetwork_reports_fn()\n    iteration = _Iteration(\n        number=number,\n        candidates=candidates,\n        subnetwork_specs=None,\n        estimator_spec=estimator_spec,\n        best_candidate_index=best_candidate_index,\n        summaries=[],\n        subnetwork_reports=subnetwork_reports,\n        train_manager=_TrainManager([], [],\n                                    self.test_subdirectory,\n                                    is_chief=True),\n        previous_iteration=None,\n        checkpoint=None)\n    self.assertEqual(iteration.number, number)\n    self.assertEqual(iteration.candidates, candidates)\n    self.assertEqual(iteration.estimator_spec, estimator_spec)\n    self.assertEqual(iteration.best_candidate_index, best_candidate_index)\n    self.assertEqual(iteration.subnetwork_reports, subnetwork_reports)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""negative_number"",\n          ""number"": -1,\n      }, {\n          ""testcase_name"": ""float_number"",\n          ""number"": 1.213,\n      }, {\n          ""testcase_name"": ""none_number"",\n          ""number"": None,\n      }, {\n          ""testcase_name"": ""empty_candidates"",\n          ""candidates"": lambda: [],\n      }, {\n          ""testcase_name"": ""none_candidates"",\n          ""candidates"": lambda: None,\n      }, {\n          ""testcase_name"": ""non_list_candidates"",\n          ""candidates"": lambda: {\n              ""foo"": _dummy_candidate()\n          },\n      }, {\n          ""testcase_name"": ""none_estimator_spec"",\n          ""estimator_spec"": None,\n      }, {\n          ""testcase_name"": ""none_best_candidate_index"",\n          ""best_candidate_index"": None,\n      }, {\n          ""testcase_name"": ""none_subnetwork_reports"",\n          ""subnetwork_reports"": lambda: None,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_new_errors(self,\n                      number=0,\n                      candidates=lambda: [_dummy_candidate()],\n                      estimator_spec=tu.dummy_estimator_spec(),\n                      best_candidate_index=0,\n                      subnetwork_reports=lambda: []):\n    with self.assertRaises(ValueError):\n      _Iteration(\n          number=number,\n          candidates=candidates(),\n          subnetwork_specs=None,\n          estimator_spec=estimator_spec,\n          best_candidate_index=best_candidate_index,\n          summaries=[],\n          subnetwork_reports=subnetwork_reports(),\n          train_manager=_TrainManager([], [],\n                                      self.test_subdirectory,\n                                      is_chief=True),\n          previous_iteration=None,\n          checkpoint=None)\n\n\nclass _FakeBuilder(SubnetworkBuilder):\n\n  def __init__(self, name, random_seed=11, chief_hook=None):\n    self._name = name\n    self._random_seed = random_seed\n    self._chief_hook = chief_hook\n\n  @property\n  def name(self):\n    return self._name\n\n  @property\n  def seed(self):\n    return self._random_seed\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    return Subnetwork(\n        last_layer=tu.dummy_tensor(),\n        logits=tu.dummy_tensor([2, logits_dimension]),\n        complexity=tu.dummy_tensor(),\n        persisted_tensors={""random_seed"": self._random_seed})\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    if self._chief_hook:\n      return TrainOpSpec(\n          train_op=tf.no_op(), chief_hooks=[self._chief_hook], hooks=None)\n    return None\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    return None\n\n\nclass _FakeEnsembleBuilder(object):\n\n  def __init__(self,\n               dict_predictions=False,\n               eval_metric_ops_fn=None,\n               export_output_key=None):\n    self._dict_predictions = dict_predictions\n    if not eval_metric_ops_fn:\n      eval_metric_ops_fn = lambda: {""a"": (tf.constant(1), tf.constant(1))}\n    self._eval_metric_ops_fn = eval_metric_ops_fn\n    self._export_output_key = export_output_key\n\n  def build_ensemble_spec(self,\n                          name,\n                          candidate,\n                          ensembler,\n                          subnetwork_specs,\n                          summary,\n                          features,\n                          mode,\n                          iteration_number,\n                          labels=None,\n                          previous_ensemble_spec=None,\n                          my_ensemble_index=None,\n                          params=None,\n                          previous_iteration_checkpoint=None):\n    del ensembler\n    del subnetwork_specs\n    del summary\n    del features\n    del mode\n    del labels\n    del iteration_number\n    del params\n    del my_ensemble_index\n    del previous_iteration_checkpoint\n\n    num_subnetworks = 0\n    if previous_ensemble_spec:\n      num_subnetworks += 1\n\n    return tu.dummy_ensemble_spec(\n        name=name,\n        num_subnetworks=num_subnetworks,\n        random_seed=candidate.subnetwork_builders[0].seed,\n        subnetwork_builders=candidate.subnetwork_builders,\n        dict_predictions=self._dict_predictions,\n        eval_metrics=tu.create_ensemble_metrics(\n            metric_fn=self._eval_metric_ops_fn),\n        export_output_key=self._export_output_key,\n        variables=[tf.Variable(1.)])\n\n\nclass _FakeSubnetworkManager(object):\n\n  def build_subnetwork_spec(self,\n                            name,\n                            subnetwork_builder,\n                            summary,\n                            features,\n                            mode,\n                            labels=None,\n                            previous_ensemble=None,\n                            config=None,\n                            params=None):\n    del summary\n    del features\n    del mode\n    del labels\n    del previous_ensemble\n    del params\n    del config\n\n    return _SubnetworkSpec(\n        name=name,\n        subnetwork=None,\n        builder=subnetwork_builder,\n        step=tf.Variable(0, dtype=tf.int64),\n        variables=[tf.Variable(1.)],\n        predictions=None,\n        loss=None,\n        train_op=subnetwork_builder.build_subnetwork_train_op(\n            *[None for _ in range(7)]),\n        eval_metrics=tu.create_subnetwork_metrics(\n            metric_fn=lambda: {""a"": (tf.constant(1), tf.constant(1))}))\n\n\nclass _FakeCandidateBuilder(object):\n\n  def build_candidate(self,\n                      ensemble_spec,\n                      training,\n                      summary,\n                      rebuilding):\n    del training  # Unused\n    del summary  # Unused\n    del rebuilding  # Unused\n\n    return _Candidate(\n        ensemble_spec=ensemble_spec,\n        adanet_loss=ensemble_spec.adanet_loss,\n        variables=[tf.Variable(1.)])\n\n\ndef _export_output_tensors(export_outputs):\n  """"""Returns a dict of `Tensor`, tuple of `Tensor`, or dict of `Tensor`.""""""\n\n  outputs = {}\n  for key, export_output in export_outputs.items():\n    if isinstance(export_output, tf.estimator.export.ClassificationOutput):\n      result = ()\n      if export_output.classes is not None:\n        result += (tf.strings.to_number(export_output.classes),)\n      if export_output.scores is not None:\n        result += (export_output.scores,)\n      outputs[key] = result\n    elif isinstance(export_output, tf.estimator.export.RegressionOutput):\n      outputs[key] = export_output.value\n    elif isinstance(export_output, tf.estimator.export.PredictOutput):\n      outputs[key] = export_output.outputs\n  return outputs\n\n\nclass _FakeEnsembler(object):\n\n  @property\n  def name(self):\n    return ""fake_ensembler""\n\n\nclass _FakeIteration(object):\n\n  def __init__(self, fake_ensemble_spec):\n    self.number = 0\n    self.checkpoint = tf.train.Checkpoint()\n    self.candidates = [\n        _FakeCandidateBuilder().build_candidate(fake_ensemble_spec, None, None,\n                                                None)\n    ]\n\n\nclass IterationBuilderTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""negative_max_steps"",\n          ""max_steps"": -1,\n      }, {\n          ""testcase_name"": ""zero_max_steps"",\n          ""max_steps"": 0,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_init_errors(self, max_steps):\n    with self.assertRaises(ValueError):\n      _IterationBuilder(\n          _FakeCandidateBuilder(),\n          _FakeSubnetworkManager(),\n          _FakeEnsembleBuilder(),\n          summary_maker=_ScopedSummary,\n          ensemblers=[_FakeEnsembler()],\n          max_steps=max_steps)\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""single_subnetwork_fn"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"":\n              ""single_subnetwork_fn_mock_summary"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""summary_maker"":\n              functools.partial(_TPUScopedSummary, logdir=""/tmp/fakedir""),\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_loss"":\n              1.403943,\n          ""want_predictions"":\n              2.129,\n          ""want_best_candidate_index"":\n              0,\n      },\n      {\n          ""testcase_name"":\n              ""single_subnetwork_with_eval_metrics"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(eval_metric_ops_fn=lambda:\n                                   {""a"": (tf.constant(1), tf.constant(2))}),\n          ""subnetwork_builders"": [_FakeBuilder(""training"",),],\n          ""mode"":\n              tf.estimator.ModeKeys.EVAL,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_loss"":\n              1.403943,\n          ""want_predictions"":\n              2.129,\n          ""want_eval_metric_ops"": [""a"", ""iteration""],\n          ""want_best_candidate_index"":\n              0,\n      },\n      {\n          ""testcase_name"":\n              ""single_subnetwork_with_non_tensor_eval_metric_op"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(eval_metric_ops_fn=lambda:\n                                   {""a"": (tf.constant(1), tf.no_op())}),\n          ""subnetwork_builders"": [_FakeBuilder(""training"",),],\n          ""mode"":\n              tf.estimator.ModeKeys.EVAL,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_loss"":\n              1.403943,\n          ""want_predictions"":\n              2.129,\n          ""want_eval_metric_ops"": [""a"", ""iteration""],\n          ""want_best_candidate_index"":\n              0,\n      },\n      {\n          ""testcase_name"": ""single_subnetwork_done_training_fn"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [_FakeBuilder(""done"")],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"": ""single_dict_predictions_subnetwork_fn"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(dict_predictions=True),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"": ""previous_ensemble"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""previous_iteration"":\n              lambda: _FakeIteration(\n                  tu.dummy_ensemble_spec(""old"", variables=[tf.Variable(1.)])),\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 1,\n      },\n      {\n          ""testcase_name"":\n              ""previous_ensemble_is_best"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""previous_iteration"":\n              lambda: _FakeIteration(\n                  tu.dummy_ensemble_spec(\n                      ""old"", random_seed=12, variables=[tf.Variable(1.)])),\n          ""want_loss"":\n              -.437,\n          ""want_predictions"":\n              .688,\n          ""want_best_candidate_index"":\n              0,\n      },\n      {\n          ""testcase_name"":\n              ""previous_ensemble_spec_and_eval_metrics"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(eval_metric_ops_fn=lambda:\n                                   {""a"": (tf.constant(1), tf.constant(2))}),\n          ""subnetwork_builders"": [_FakeBuilder(""training"")],\n          ""mode"":\n              tf.estimator.ModeKeys.EVAL,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""previous_iteration"":\n              lambda: _FakeIteration(\n                  tu.dummy_ensemble_spec(\n                      ""old"",\n                      eval_metrics=tu.create_ensemble_metrics(\n                          metric_fn=lambda:\n                          {""a"": (tf.constant(1), tf.constant(2))}),\n                      variables=[tf.Variable(1.)])),\n          ""want_loss"":\n              1.403943,\n          ""want_predictions"":\n              2.129,\n          ""want_eval_metric_ops"": [""a"", ""iteration""],\n          ""want_best_candidate_index"":\n              1,\n      },\n      {\n          ""testcase_name"": ""two_subnetwork_fns"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.40394,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"": ""two_subnetwork_fns_other_best"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=12)\n          ],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": -.437,\n          ""want_predictions"": .688,\n          ""want_best_candidate_index"": 1,\n      },\n      {\n          ""testcase_name"": ""two_subnetwork_one_training_fns"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"":\n              [_FakeBuilder(""training""),\n               _FakeBuilder(""done"", random_seed=7)],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"": ""two_subnetwork_done_training_fns"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"":\n              [_FakeBuilder(""done""),\n               _FakeBuilder(""done1"", random_seed=7)],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"": ""two_dict_predictions_subnetwork_fns"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(dict_predictions=True),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.404,\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"": 0,\n      },\n      {\n          ""testcase_name"":\n              ""two_dict_predictions_subnetwork_fns_predict_classes"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(\n                  dict_predictions=True,\n                  export_output_key=tu.ExportOutputKeys.CLASSIFICATION_CLASSES),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""mode"":\n              tf.estimator.ModeKeys.PREDICT,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_loss"":\n              1.404,\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"":\n              0,\n          ""want_export_outputs"": {\n              tu.ExportOutputKeys.CLASSIFICATION_CLASSES: [2.129],\n              ""serving_default"": [2.129],\n          },\n      },\n      {\n          ""testcase_name"":\n              ""two_dict_predictions_subnetwork_fns_predict_scores"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(\n                  dict_predictions=True,\n                  export_output_key=tu.ExportOutputKeys.CLASSIFICATION_SCORES),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""mode"":\n              tf.estimator.ModeKeys.PREDICT,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_loss"":\n              1.404,\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"":\n              0,\n          ""want_export_outputs"": {\n              tu.ExportOutputKeys.CLASSIFICATION_SCORES: [2.129],\n              ""serving_default"": [2.129],\n          },\n      },\n      {\n          ""testcase_name"":\n              ""two_dict_predictions_subnetwork_fns_predict_regression"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(\n                  dict_predictions=True,\n                  export_output_key=tu.ExportOutputKeys.REGRESSION),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""mode"":\n              tf.estimator.ModeKeys.PREDICT,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"":\n              0,\n          ""want_export_outputs"": {\n              tu.ExportOutputKeys.REGRESSION: 2.129,\n              ""serving_default"": 2.129,\n          },\n      },\n      {\n          ""testcase_name"":\n              ""two_dict_predictions_subnetwork_fns_predict_prediction"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(\n                  dict_predictions=True,\n                  export_output_key=tu.ExportOutputKeys.PREDICTION),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""mode"":\n              tf.estimator.ModeKeys.PREDICT,\n          ""features"":\n              lambda: [[1., -1., 0.]],\n          ""labels"":\n              lambda: [1],\n          ""want_predictions"": {\n              ""classes"": 2,\n              ""logits"": 2.129\n          },\n          ""want_best_candidate_index"":\n              0,\n          ""want_export_outputs"": {\n              tu.ExportOutputKeys.PREDICTION: {\n                  ""classes"": 2,\n                  ""logits"": 2.129\n              },\n              ""serving_default"": {\n                  ""classes"": 2,\n                  ""logits"": 2.129\n              },\n          },\n      },\n      {\n          ""testcase_name"": ""chief_session_run_hook"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training"", chief_hook=tu.ModifierSessionRunHook())\n          ],\n          ""features"": lambda: [[1., -1., 0.]],\n          ""labels"": lambda: [1],\n          ""want_loss"": 1.403943,\n          ""want_predictions"": 2.129,\n          ""want_best_candidate_index"": 0,\n          ""want_chief_hooks"": True,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_iteration(self,\n                           ensemble_builder,\n                           subnetwork_builders,\n                           features,\n                           labels,\n                           want_predictions,\n                           want_best_candidate_index,\n                           want_eval_metric_ops=(),\n                           previous_iteration=None,\n                           want_loss=None,\n                           want_export_outputs=None,\n                           mode=tf.estimator.ModeKeys.TRAIN,\n                           summary_maker=_ScopedSummary,\n                           want_chief_hooks=False):\n    with context.graph_mode():\n      tf_compat.v1.train.create_global_step()\n      builder = _IterationBuilder(\n          _FakeCandidateBuilder(),\n          _FakeSubnetworkManager(),\n          ensemble_builder,\n          summary_maker=summary_maker,\n          ensemblers=[_FakeEnsembler()],\n          max_steps=1)\n      iteration = builder.build_iteration(\n          base_global_step=0,\n          iteration_number=0,\n          ensemble_candidates=[\n              EnsembleCandidate(b.name, [b], None) for b in subnetwork_builders\n          ],\n          previous_iteration=previous_iteration()\n          if previous_iteration else None,\n          subnetwork_builders=subnetwork_builders,\n          features=features(),\n          labels=labels(),\n          mode=mode,\n          config=tf.estimator.RunConfig(model_dir=self.test_subdirectory))\n      init = tf.group(tf_compat.v1.global_variables_initializer(),\n                      tf_compat.v1.local_variables_initializer())\n      self.evaluate(init)\n      estimator_spec = iteration.estimator_spec\n      if want_chief_hooks:\n        self.assertNotEmpty(iteration.estimator_spec.training_chief_hooks)\n      self.assertAllClose(\n          want_predictions,\n          self.evaluate(estimator_spec.predictions),\n          atol=1e-3)\n      # A default architecture metric is always included, even if we don\'t\n      # specify one.\n      eval_metric_ops = estimator_spec.eval_metric_ops\n      if ""architecture/adanet/ensembles"" in eval_metric_ops:\n        del eval_metric_ops[""architecture/adanet/ensembles""]\n      self.assertEqual(set(want_eval_metric_ops), set(eval_metric_ops.keys()))\n\n      self.assertEqual(want_best_candidate_index,\n                       self.evaluate(iteration.best_candidate_index))\n      if mode == tf.estimator.ModeKeys.PREDICT:\n        self.assertIsNotNone(estimator_spec.export_outputs)\n        self.assertAllClose(\n            want_export_outputs,\n            self.evaluate(\n                _export_output_tensors(estimator_spec.export_outputs)),\n            atol=1e-3)\n        self.assertIsNone(iteration.estimator_spec.train_op)\n        self.assertIsNone(iteration.estimator_spec.loss)\n        self.assertIsNotNone(want_export_outputs)\n        return\n\n      self.assertAlmostEqual(\n          want_loss, self.evaluate(iteration.estimator_spec.loss), places=3)\n      self.assertIsNone(iteration.estimator_spec.export_outputs)\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        self.evaluate(iteration.estimator_spec.train_op)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""empty_subnetwork_builders"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"": [],\n          ""want_raises"": ValueError,\n      }, {\n          ""testcase_name"": ""same_subnetwork_builder_names"",\n          ""ensemble_builder"": _FakeEnsembleBuilder(),\n          ""subnetwork_builders"":\n              [_FakeBuilder(""same_name""),\n               _FakeBuilder(""same_name"")],\n          ""want_raises"": ValueError,\n      }, {\n          ""testcase_name"":\n              ""same_ensembler_names"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(),\n          ""multiple_candidates"": True,\n          ""subnetwork_builders"": [_FakeBuilder(""fake_builder_name"")],\n          ""want_raises"":\n              ValueError,\n      }, {\n          ""testcase_name"":\n              ""predict_invalid"",\n          ""ensemble_builder"":\n              _FakeEnsembleBuilder(\n                  dict_predictions=True,\n                  export_output_key=tu.ExportOutputKeys.INVALID),\n          ""subnetwork_builders"": [\n              _FakeBuilder(""training""),\n              _FakeBuilder(""training2"", random_seed=7)\n          ],\n          ""mode"":\n              tf.estimator.ModeKeys.PREDICT,\n          ""want_raises"":\n              TypeError,\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_iteration_error(self,\n                                 ensemble_builder,\n                                 subnetwork_builders,\n                                 want_raises,\n                                 multiple_candidates=False,\n                                 mode=tf.estimator.ModeKeys.TRAIN,\n                                 summary_maker=_ScopedSummary):\n    with context.graph_mode():\n      tf_compat.v1.train.create_global_step()\n      builder = _IterationBuilder(\n          _FakeCandidateBuilder(),\n          _FakeSubnetworkManager(),\n          ensemble_builder,\n          summary_maker=summary_maker,\n          ensemblers=[_FakeEnsembler()],\n          max_steps=100)\n      features = [[1., -1., 0.]]\n      labels = [1]\n      ensemble_candidates = [\n          EnsembleCandidate(""test"", subnetwork_builders, None)\n      ]\n      if multiple_candidates:\n        ensemble_candidates += [\n            EnsembleCandidate(""test"", subnetwork_builders, None)\n        ]\n      with self.assertRaises(want_raises):\n        builder.build_iteration(\n            base_global_step=0,\n            iteration_number=0,\n            ensemble_candidates=ensemble_candidates,\n            subnetwork_builders=subnetwork_builders,\n            features=features,\n            labels=labels,\n            mode=mode,\n            config=tf.estimator.RunConfig(model_dir=self.test_subdirectory))\n\n\nclass _HeadEnsembleBuilder(object):\n\n  def __init__(self, head):\n    self._head = head\n\n  def build_ensemble_spec(self,\n                          name,\n                          candidate,\n                          ensembler,\n                          subnetwork_specs,\n                          summary,\n                          features,\n                          mode,\n                          iteration_number,\n                          labels=None,\n                          previous_ensemble_spec=None,\n                          my_ensemble_index=None,\n                          params=None,\n                          previous_iteration_checkpoint=None):\n    del ensembler\n    del subnetwork_specs\n    del summary\n    del iteration_number\n    del previous_ensemble_spec\n    del my_ensemble_index\n    del params\n    del previous_iteration_checkpoint\n\n    logits = [[.5]]\n\n    estimator_spec = self._head.create_estimator_spec(\n        features=features, mode=mode, labels=labels, logits=logits)\n    return _EnsembleSpec(\n        name=name,\n        ensemble=None,\n        architecture=_Architecture(""foo"", ""bar""),\n        subnetwork_builders=candidate.subnetwork_builders,\n        predictions=estimator_spec.predictions,\n        step=tf.Variable(0, dtype=tf.int64),\n        variables=[tf.Variable(1.)],\n        loss=None,\n        adanet_loss=.1,\n        train_op=None,\n        eval_metrics=None,\n        export_outputs=estimator_spec.export_outputs)\n\n\nclass IterationExportOutputsTest(tu.AdanetTestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""regression_head"",\n          ""head"": regression_head.RegressionHead(),\n      }, {\n          ""testcase_name"": ""binary_classification_head"",\n          ""head"": binary_class_head.BinaryClassHead(),\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_head_export_outputs(self, head):\n    with context.graph_mode():\n      tf_compat.v1.train.create_global_step()\n      ensemble_builder = _HeadEnsembleBuilder(head)\n      builder = _IterationBuilder(\n          _FakeCandidateBuilder(),\n          _FakeSubnetworkManager(),\n          ensemble_builder,\n          summary_maker=_ScopedSummary,\n          ensemblers=[_FakeEnsembler()],\n          max_steps=10)\n      features = [[1., -1., 0.]]\n      labels = [1]\n      mode = tf.estimator.ModeKeys.PREDICT\n      subnetwork_builders = [_FakeBuilder(""test"")]\n      iteration = builder.build_iteration(\n          base_global_step=0,\n          iteration_number=0,\n          ensemble_candidates=[\n              EnsembleCandidate(""test"", subnetwork_builders, [tf.Variable(1.)])\n          ],\n          subnetwork_builders=subnetwork_builders,\n          features=features,\n          labels=labels,\n          config=tf.estimator.RunConfig(model_dir=self.test_subdirectory),\n          mode=mode)\n\n      # Compare iteration outputs with default head outputs.\n      spec = head.create_estimator_spec(\n          features=features, labels=labels, mode=mode, logits=[[.5]])\n      self.assertEqual(\n          len(spec.export_outputs),\n          len(iteration.estimator_spec.export_outputs))\n      for key in spec.export_outputs:\n        if isinstance(spec.export_outputs[key],\n                      tf.estimator.export.RegressionOutput):\n          self.assertAlmostEqual(\n              self.evaluate(spec.export_outputs[key].value),\n              self.evaluate(iteration.estimator_spec.export_outputs[key].value))\n          continue\n        if isinstance(spec.export_outputs[key],\n                      tf.estimator.export.ClassificationOutput):\n          self.assertAllClose(\n              self.evaluate(spec.export_outputs[key].scores),\n              self.evaluate(\n                  iteration.estimator_spec.export_outputs[key].scores))\n          self.assertAllEqual(\n              self.evaluate(spec.export_outputs[key].classes),\n              self.evaluate(\n                  iteration.estimator_spec.export_outputs[key].classes))\n          continue\n        if isinstance(spec.export_outputs[key],\n                      tf.estimator.export.PredictOutput):\n          if ""classes"" in spec.export_outputs[key].outputs:\n            # Verify string Tensor outputs separately.\n            self.assertAllEqual(\n                self.evaluate(spec.export_outputs[key].outputs[""classes""]),\n                self.evaluate(iteration.estimator_spec.export_outputs[key]\n                              .outputs[""classes""]))\n            del spec.export_outputs[key].outputs[""classes""]\n            del iteration.estimator_spec.export_outputs[key].outputs[""classes""]\n          if ""all_classes"" in spec.export_outputs[key].outputs:\n            # Verify string Tensor outputs separately.\n            self.assertAllEqual(\n                self.evaluate(spec.export_outputs[key].outputs[""all_classes""]),\n                self.evaluate(iteration.estimator_spec.export_outputs[key]\n                              .outputs[""all_classes""]))\n            del spec.export_outputs[key].outputs[""all_classes""]\n            del iteration.estimator_spec.export_outputs[key].outputs[\n                ""all_classes""]\n          self.assertAllClose(\n              self.evaluate(spec.export_outputs[key].outputs),\n              self.evaluate(\n                  iteration.estimator_spec.export_outputs[key].outputs))\n          continue\n        self.fail(""Invalid export_output for {}."".format(key))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/report_accessor.py,1,"b'""""""Store and retrieve adanet.IterationReport protos.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nfrom absl import logging\nfrom adanet import subnetwork\n\nimport numpy as np\nimport six\nimport tensorflow.compat.v2 as tf\n\n\n# TODO: Encapsulate conversion and serialization of a\n# MaterializedReport dict within MaterializedReport.\ndef _json_report_to_materialized_report(iteration_report_json):\n  """"""Converts a JSON loaded iteration report to a `MaterializedReport` list.""""""\n\n  subnetwork_reports = []\n  for subnetwork_report_json in iteration_report_json[""subnetwork_reports""]:\n    subnetwork_reports.append(\n        subnetwork.MaterializedReport(\n            iteration_number=int(iteration_report_json[""iteration_number""]),\n            name=subnetwork_report_json[""name""],\n            hparams=subnetwork_report_json[""hparams""],\n            attributes=subnetwork_report_json[""attributes""],\n            metrics=subnetwork_report_json[""metrics""],\n            included_in_final_ensemble=subnetwork_report_json[\n                ""included_in_final_ensemble""]))\n  return subnetwork_reports\n\n\ndef _validate_report_dict(dictionary):\n  """"""Validates that entries of a MaterializedReport dictionary field.""""""\n\n  for key, value in dictionary.items():\n    if isinstance(value, np.integer):\n      dictionary[key] = int(value)\n    if isinstance(value, np.float):\n      dictionary[key] = float(value)\n    if isinstance(value, (six.string_types, six.binary_type)):\n      if six.PY2:\n        if not isinstance(value, six.text_type):\n          dictionary[key] = six.u(value).encode(""utf-8"")\n      if six.PY3:\n        dictionary[key] = str(dictionary[key])\n    elif not isinstance(value, (bool, six.text_type, int, float)):\n      raise ValueError(""Values must be a binary type ""\n                       ""(str in python 2; bytes in python 3), ""\n                       ""a text type (unicode in python 2; str in python 3), ""\n                       ""int, bool, or float, but its type is {}."".format(\n                           type(value)))\n  return dictionary\n\n\ndef _subnetwork_report_to_dict(subnetwork_report):\n  """"""Converts a Subnetwork report to a JSON serializable dict.""""""\n\n  return {\n      ""name"": subnetwork_report.name,\n      ""hparams"": _validate_report_dict(subnetwork_report.hparams),\n      ""attributes"": _validate_report_dict(subnetwork_report.attributes),\n      ""metrics"": _validate_report_dict(subnetwork_report.metrics),\n      ""included_in_final_ensemble"": subnetwork_report.included_in_final_ensemble\n  }\n\n\nclass _ReportAccessor(object):\n  """"""Store and retrieve report JSON files.""""""\n\n  def __init__(self, report_dir, filename=""iteration_reports.json""):\n    """"""Creates a `_ReportAccessor` instance.\n\n    Args:\n      report_dir: Directory to store the report.\n      filename: Name of the file.\n\n    Returns:\n      A `_ReportAccessor` instance.\n    """"""\n\n    tf.io.gfile.makedirs(report_dir)\n    self._full_filepath = os.path.join(report_dir, filename)\n\n  def write_iteration_report(self, iteration_number, materialized_reports):\n    """"""Writes an iteration\'s `MaterializedReports` to a JSON file.\n\n    TODO: Remove iteration_number from the argument of this method.\n\n    Note that even materialized_reports also contain iteration\n    number, those are ignored -- only the iteration_number that is passed into\n    this method would be written to the proto.\n\n    Args:\n      iteration_number: Int for the iteration number.\n      materialized_reports: A list of `adanet.subnetwork.MaterializedReport`\n        objects.\n    """"""\n\n    iteration_report = {\n        ""iteration_number"":\n            int(iteration_number),\n        ""subnetwork_reports"":\n            list(map(_subnetwork_report_to_dict, materialized_reports))\n    }\n\n    self._append_iteration_report_json(iteration_report)\n    logging.info(""Wrote IterationReport for iteration %s to %s"",\n                 iteration_number, self._full_filepath)\n\n  def _append_iteration_report_json(self, iteration_report):\n    """"""Appends an iteration report dictionary object to the output file.""""""\n    iteration_reports = []\n\n    if os.path.exists(self._full_filepath):\n      with open(self._full_filepath, ""r"") as f:\n        iteration_reports = json.load(f)\n\n    iteration_reports.append(iteration_report)\n    with open(self._full_filepath, ""w"") as f:\n      json.dump(iteration_reports, f)\n\n  def read_iteration_reports(self):\n    """"""Reads all iterations of the Report.\n\n    Each `adanet.subnetwork.MaterializedReport` list is one AdaNet iteration.\n    The first list in the sequence is iteration 0, followed by iteration 1, and\n    so on.\n\n    Returns:\n      Iterable of lists of `adanet.subnetwork.MaterializedReport`s.\n    """"""\n    if os.path.exists(self._full_filepath):\n      with open(self._full_filepath, ""r"") as f:\n        iteration_reports = json.load(f)\n      return [\n          _json_report_to_materialized_report(ir) for ir in iteration_reports\n      ]\n\n    return []\n'"
adanet/core/report_accessor_test.py,2,"b'""""""Tests for run_report_accessor.py.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import subnetwork\nfrom adanet.core.report_accessor import _ReportAccessor\nimport tensorflow.compat.v2 as tf\n\n\nclass ReportAccessorTest(parameterized.TestCase, tf.test.TestCase):\n\n  def test_read_from_empty_file(self):\n    report_accessor = _ReportAccessor(self.get_temp_dir())\n    self.assertEqual([], list(report_accessor.read_iteration_reports()))\n\n  def test_add_to_empty_file(self):\n    report_accessor = _ReportAccessor(self.get_temp_dir())\n    materialized_reports = [\n        subnetwork.MaterializedReport(\n            iteration_number=0,\n            name=""foo"",\n            hparams={\n                ""p1"": 1,\n                ""p2"": ""default_hparam"",\n                ""p3"": b""binary_hparam"",\n                ""p4"": u""unicode_hparam"",\n                ""p5"": True,\n            },\n            attributes={\n                ""a1"": 1,\n                ""a2"": ""default_attribute"",\n                ""a3"": b""binary_attribute"",\n                ""a4"": u""unicode_attribute"",\n                ""a5"": True,\n            },\n            metrics={\n                ""m1"": 1,\n                ""m2"": ""default_metric"",\n                ""m3"": b""binary_metric"",\n                ""m4"": u""unicode_metric"",\n                ""m5"": True,\n            },\n            included_in_final_ensemble=True,\n        ),\n    ]\n\n    report_accessor.write_iteration_report(\n        iteration_number=0,\n        materialized_reports=materialized_reports,\n    )\n    actual_iteration_reports = list(report_accessor.read_iteration_reports())\n\n    self.assertLen(actual_iteration_reports, 1)\n    self.assertEqual(materialized_reports, actual_iteration_reports[0])\n\n  def test_add_to_existing_file(self):\n    materialized_reports = [\n        [\n            subnetwork.MaterializedReport(\n                iteration_number=0,\n                name=""foo1"",\n                hparams={\n                    ""p1"": 11,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 11,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 11,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=False,\n            ),\n            subnetwork.MaterializedReport(\n                iteration_number=0,\n                name=""foo2"",\n                hparams={\n                    ""p1"": 12,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 12,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 12,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=True,\n            ),\n        ],\n        [\n            subnetwork.MaterializedReport(\n                iteration_number=1,\n                name=""foo1"",\n                hparams={\n                    ""p1"": 21,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 21,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 21,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=True,\n            ),\n            subnetwork.MaterializedReport(\n                iteration_number=1,\n                name=""foo2"",\n                hparams={\n                    ""p1"": 22,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 22,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 22,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=False,\n            ),\n        ],\n        [\n            subnetwork.MaterializedReport(\n                iteration_number=2,\n                name=""foo1"",\n                hparams={\n                    ""p1"": 31,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 31,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 31,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=False,\n            ),\n            subnetwork.MaterializedReport(\n                iteration_number=2,\n                name=""foo2"",\n                hparams={\n                    ""p1"": 32,\n                    ""p2"": ""default_hparam"",\n                    ""p3"": b""binary_hparam"",\n                    ""p4"": u""unicode_hparam"",\n                    ""p5"": True,\n                },\n                attributes={\n                    ""a1"": 32,\n                    ""a2"": ""default_attribute"",\n                    ""a3"": b""binary_attribute"",\n                    ""a4"": u""unicode_attribute"",\n                    ""a5"": True,\n                },\n                metrics={\n                    ""m1"": 32,\n                    ""m2"": ""default_metric"",\n                    ""m3"": b""binary_metric"",\n                    ""m4"": u""unicode_metric"",\n                    ""m5"": True,\n                },\n                included_in_final_ensemble=True,\n            ),\n        ],\n    ]\n\n    report_accessor = _ReportAccessor(self.get_temp_dir())\n\n    report_accessor.write_iteration_report(0, materialized_reports[0])\n    report_accessor.write_iteration_report(1, materialized_reports[1])\n    report_accessor.write_iteration_report(2, materialized_reports[2])\n    actual_reports = list(report_accessor.read_iteration_reports())\n    self.assertEqual(materialized_reports, actual_reports)\n\n  def test_write_iteration_report_encoding(self):\n    """"""Tests GitHub issue #4.""""""\n\n    report_accessor = _ReportAccessor(self.get_temp_dir())\n    binary_type_value = b""\\n\\x83\\x01\\n;adanet/iteration_2/ensemble_2_layer_dnn/""\n    text_type_value = u""\\U0001f937""\n    materialized_reports = [\n        subnetwork.MaterializedReport(\n            iteration_number=0,\n            name=""foo"",\n            hparams={\n                ""p1"": binary_type_value,\n                ""p2"": text_type_value,\n            },\n            attributes={\n                ""a1"": binary_type_value,\n                ""a2"": text_type_value,\n            },\n            metrics={\n                ""m1"": binary_type_value,\n                ""m2"": text_type_value,\n            },\n            included_in_final_ensemble=True,\n        ),\n    ]\n\n    report_accessor.write_iteration_report(\n        iteration_number=0,\n        materialized_reports=materialized_reports,\n    )\n    actual_iteration_reports = list(report_accessor.read_iteration_reports())\n    self.assertLen(actual_iteration_reports, 1)\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""hparams_invalid_type"",\n      ""hparams"": {\n          ""h1"": None\n      },\n  }, {\n      ""testcase_name"": ""attributes_invalid_type"",\n      ""attributes"": {\n          ""a1"": None\n      },\n  }, {\n      ""testcase_name"": ""metrics_invalid_type"",\n      ""metrics"": {\n          ""m1"": None\n      },\n  })\n  def test_value_error(self, hparams=None, attributes=None, metrics=None):\n    if hparams is None:\n      hparams = {}\n    if attributes is None:\n      attributes = {}\n    if metrics is None:\n      metrics = {}\n    report_accessor = _ReportAccessor(self.get_temp_dir())\n    materialized_reports = [\n        subnetwork.MaterializedReport(\n            iteration_number=0,\n            name=""foo"",\n            hparams=hparams,\n            attributes=attributes,\n            metrics=metrics,\n            included_in_final_ensemble=True,\n        ),\n    ]\n    with self.assertRaises(ValueError):\n      report_accessor.write_iteration_report(\n          iteration_number=0,\n          materialized_reports=materialized_reports,\n      )\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/report_materializer.py,2,"b'""""""Materializes the subnetwork.Reports.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl import logging\nfrom adanet import subnetwork\nfrom adanet import tf_compat\nimport tensorflow.compat.v2 as tf\n\n\nclass ReportMaterializer(object):\n  """"""Materializes reports.\n\n  Specifically it materializes a subnetwork\'s :class:`adanet.subnetwork.Report`\n  instances into :class:`adanet.subnetwork.MaterializedReport` instances.\n\n  Requires an input function `input_fn` that returns a tuple of:\n\n  * features: Dictionary of string feature name to `Tensor`.\n  * labels: `Tensor` of labels.\n\n  Args:\n    input_fn: The input function.\n    steps: Number of steps for which to materialize the ensembles. If an\n      `OutOfRangeError` occurs, materialization stops. If set to None, will\n      iterate the dataset until all inputs are exhausted.\n\n  Returns:\n    A `ReportMaterializer` instance.\n  """"""\n\n  def __init__(self, input_fn, steps=None):\n    self._input_fn = input_fn\n    self._steps = steps\n    super(ReportMaterializer, self).__init__()\n\n  @property\n  def input_fn(self):\n    """"""Returns the input_fn that materialize_subnetwork_reports would run on.\n\n    Even though this property appears to be unused, it would be used to build\n    the AdaNet model graph inside AdaNet estimator.train(). After the graph is\n    built, the queue_runners are started and the initializers are run,\n    AdaNet estimator.train() passes its tf.Session as an argument to\n    materialize_subnetwork_reports(), thus indirectly making input_fn\n    available to materialize_subnetwork_reports.\n    """"""\n    return self._input_fn\n\n  @property\n  def steps(self):\n    """"""Return the number of steps.""""""\n    return self._steps\n\n  def materialize_subnetwork_reports(self, sess, iteration_number,\n                                     subnetwork_reports,\n                                     included_subnetwork_names):\n    """"""Materializes the Tensor objects in subnetwork_reports using sess.\n\n    This converts the Tensors in subnetwork_reports to ndarrays, logs the\n    progress, converts the ndarrays to python primitives, then packages them\n    into `adanet.subnetwork.MaterializedReports`.\n\n    Args:\n      sess: `Session` instance with most recent variable values loaded.\n      iteration_number: Integer iteration number.\n      subnetwork_reports: Dict mapping string names to `subnetwork.Report`\n        objects to be materialized.\n      included_subnetwork_names: List of string names of the\n        `subnetwork.Report`s that are included in the final ensemble.\n\n    Returns:\n      List of `adanet.subnetwork.MaterializedReport` objects.\n    """"""\n\n    # A metric is a tuple where the first element is a Tensor and\n    # the second element is an update op. We collate the update ops here.\n    metric_update_ops = []\n    for subnetwork_report in subnetwork_reports.values():\n      for metric_tuple in subnetwork_report.metrics.values():\n        metric_update_ops.append(tf_compat.metric_op(metric_tuple)[1])\n\n    # Extract the Tensors to be materialized.\n    tensors_to_materialize = {}\n    for name, subnetwork_report in subnetwork_reports.items():\n      metrics = {\n          metric_key: tf_compat.metric_op(metric_tuple)[0]\n          for metric_key, metric_tuple in subnetwork_report.metrics.items()\n      }\n      tensors_to_materialize[name] = {\n          ""attributes"": subnetwork_report.attributes,\n          ""metrics"": metrics\n      }\n\n    if self.steps is None:\n      logging_frequency = 1000\n    elif self.steps < 10:\n      logging_frequency = 1\n    else:\n      logging_frequency = math.floor(self.steps / 10.)\n\n    steps_completed = 0\n    while True:\n      if self.steps is not None and steps_completed == self.steps:\n        break\n      try:\n        steps_completed += 1\n        if (steps_completed % logging_frequency == 0 or\n            self.steps == steps_completed):\n          logging.info(""Report materialization [%d/%s]"", steps_completed,\n                       self.steps or ""??"")\n\n        sess.run(metric_update_ops)\n      except tf.errors.OutOfRangeError:\n        logging.info(""Encountered end of input during report materialization"")\n        break\n\n    materialized_tensors_dict = sess.run(tensors_to_materialize)\n    logging.info(""Materialized subnetwork_reports."")\n\n    # Convert scalar ndarrays into python primitives, then place them into\n    # subnetwork.MaterializedReports.\n    materialized_reports = []\n    for name, materialized_tensors in materialized_tensors_dict.items():\n      attributes = {\n          key: value.item() if hasattr(value, ""item"") else value\n          for key, value in materialized_tensors[""attributes""].items()\n      }\n      metrics = {\n          key: value.item() if hasattr(value, ""item"") else value\n          for key, value in materialized_tensors[""metrics""].items()\n      }\n      materialized_reports.append(\n          subnetwork.MaterializedReport(\n              iteration_number=iteration_number,\n              name=name,\n              hparams=subnetwork_reports[name].hparams,\n              attributes=attributes,\n              metrics=metrics,\n              included_in_final_ensemble=(name in included_subnetwork_names)))\n    return materialized_reports\n'"
adanet/core/report_materializer_test.py,20,"b'""""""Test AdaNet materializer single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import subnetwork\nfrom adanet import tf_compat\nfrom adanet.core.report_materializer import ReportMaterializer\nimport adanet.core.testing_utils as tu\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef decode(param):\n  """"""Decodes the given param when it is bytes.""""""\n\n  if isinstance(param, (float, int)):\n    return param\n  return param.decode(""utf-8"")\n\n\nclass ReportMaterializerTest(parameterized.TestCase, tf.test.TestCase):\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""one_empty_subnetwork"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(hparams={}, attributes={}, metrics={}),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""one_subnetwork"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(\n                          hparams={\n                              ""learning_rate"": 1.e-5,\n                              ""optimizer"": ""sgd"",\n                              ""num_layers"": 0,\n                              ""use_side_inputs"": True,\n                          },\n                          attributes={\n                              ""weight_norms"": tf.constant(3.14),\n                              ""foo"": tf.constant(""bar""),\n                              ""parameters"": tf.constant(7777),\n                              ""boo"": tf.constant(True),\n                          },\n                          metrics={},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo"",\n                  hparams={\n                      ""learning_rate"": 1.e-5,\n                      ""optimizer"": ""sgd"",\n                      ""num_layers"": 0,\n                      ""use_side_inputs"": True,\n                  },\n                  attributes={\n                      ""weight_norms"": 3.14,\n                      ""foo"": ""bar"",\n                      ""parameters"": 7777,\n                      ""boo"": True,\n                  },\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""one_subnetwork_iteration_2"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(\n                          hparams={\n                              ""learning_rate"": 1.e-5,\n                              ""optimizer"": ""sgd"",\n                              ""num_layers"": 0,\n                              ""use_side_inputs"": True,\n                          },\n                          attributes={\n                              ""weight_norms"": tf.constant(3.14),\n                              ""foo"": tf.constant(""bar""),\n                              ""parameters"": tf.constant(7777),\n                              ""boo"": tf.constant(True),\n                          },\n                          metrics={},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""iteration_number"":\n              2,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=2,\n                  name=""foo"",\n                  hparams={\n                      ""learning_rate"": 1.e-5,\n                      ""optimizer"": ""sgd"",\n                      ""num_layers"": 0,\n                      ""use_side_inputs"": True,\n                  },\n                  attributes={\n                      ""weight_norms"": 3.14,\n                      ""foo"": ""bar"",\n                      ""parameters"": 7777,\n                      ""boo"": True,\n                  },\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""two_subnetworks"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo1"":\n                      subnetwork.Report(\n                          hparams={\n                              ""learning_rate"": 1.e-5,\n                              ""optimizer"": ""sgd"",\n                              ""num_layers"": 0,\n                              ""use_side_inputs"": True,\n                          },\n                          attributes={\n                              ""weight_norms"": tf.constant(3.14),\n                              ""foo"": tf.constant(""bar""),\n                              ""parameters"": tf.constant(7777),\n                              ""boo"": tf.constant(True),\n                          },\n                          metrics={},\n                      ),\n                  ""foo2"":\n                      subnetwork.Report(\n                          hparams={\n                              ""learning_rate"": 1.e-6,\n                              ""optimizer"": ""sgd"",\n                              ""num_layers"": 1,\n                              ""use_side_inputs"": True,\n                          },\n                          attributes={\n                              ""weight_norms"": tf.constant(3.1445),\n                              ""foo"": tf.constant(""baz""),\n                              ""parameters"": tf.constant(7788),\n                              ""boo"": tf.constant(True),\n                          },\n                          metrics={},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo2""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo1"",\n                  hparams={\n                      ""learning_rate"": 1.e-5,\n                      ""optimizer"": ""sgd"",\n                      ""num_layers"": 0,\n                      ""use_side_inputs"": True,\n                  },\n                  attributes={\n                      ""weight_norms"": 3.14,\n                      ""foo"": ""bar"",\n                      ""parameters"": 7777,\n                      ""boo"": True,\n                  },\n                  metrics={},\n                  included_in_final_ensemble=False,\n              ),\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo2"",\n                  hparams={\n                      ""learning_rate"": 1.e-6,\n                      ""optimizer"": ""sgd"",\n                      ""num_layers"": 1,\n                      ""use_side_inputs"": True,\n                  },\n                  attributes={\n                      ""weight_norms"": 3.1445,\n                      ""foo"": ""baz"",\n                      ""parameters"": 7788,\n                      ""boo"": True,\n                  },\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""two_subnetworks_zero_included"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo1"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={},\n                      ),\n                  ""foo2"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo1"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=False,\n              ),\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo2"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=False,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""two_subnetworks_both_included"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo1"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={},\n                      ),\n                  ""foo2"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo1"", ""foo2""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo1"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo2"",\n                  hparams={},\n                  attributes={},\n                  metrics={},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""materialize_metrics"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 1.], [1., 1.], [1., 1.]],\n                                [[1.], [2.], [3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={""moo"": tf_compat.v1.metrics.mean(labels)},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo"",\n                  hparams={},\n                  attributes={},\n                  metrics={""moo"": 2.},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""materialize_metrics_none_steps"",\n          ""input_fn"":\n              tu.dataset_input_fn([[1., 1.], [1., 1.], [1., 1.]],\n                                  [[1.], [2.], [3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={""moo"": tf_compat.v1.metrics.mean(labels)},\n                      ),\n              },\n          ""steps"":\n              None,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo"",\n                  hparams={},\n                  attributes={},\n                  metrics={""moo"": 2.},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      }, {\n          ""testcase_name"":\n              ""materialize_metrics_non_tensor_op"",\n          ""input_fn"":\n              tu.dummy_input_fn([[1., 2]], [[3.]]),\n          ""subnetwork_reports_fn"":\n              lambda features, labels: {\n                  ""foo"":\n                      subnetwork.Report(\n                          hparams={},\n                          attributes={},\n                          metrics={""moo"": (tf.constant(42), tf.no_op())},\n                      ),\n              },\n          ""steps"":\n              3,\n          ""included_subnetwork_names"": [""foo""],\n          ""want_materialized_reports"": [\n              subnetwork.MaterializedReport(\n                  iteration_number=0,\n                  name=""foo"",\n                  hparams={},\n                  attributes={},\n                  metrics={""moo"": 42},\n                  included_in_final_ensemble=True,\n              ),\n          ],\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_materialize_subnetwork_reports(self,\n                                          input_fn,\n                                          subnetwork_reports_fn,\n                                          steps,\n                                          iteration_number=0,\n                                          included_subnetwork_names=None,\n                                          want_materialized_reports=None):\n    with context.graph_mode():\n      tf.constant(0.)  # dummy op so that the session graph is never empty.\n      features, labels = input_fn()\n      subnetwork_reports = subnetwork_reports_fn(features, labels)\n      with self.test_session() as sess:\n        sess.run(tf_compat.v1.initializers.local_variables())\n        report_materializer = ReportMaterializer(input_fn=input_fn, steps=steps)\n        materialized_reports = (\n            report_materializer.materialize_subnetwork_reports(\n                sess, iteration_number, subnetwork_reports,\n                included_subnetwork_names))\n        self.assertEqual(\n            len(want_materialized_reports), len(materialized_reports))\n        materialized_reports_dict = {\n            blrm.name: blrm for blrm in materialized_reports\n        }\n        for want_materialized_report in want_materialized_reports:\n          materialized_report = (\n              materialized_reports_dict[want_materialized_report.name])\n          self.assertEqual(iteration_number,\n                           materialized_report.iteration_number)\n          self.assertEqual(\n              set(want_materialized_report.hparams.keys()),\n              set(materialized_report.hparams.keys()))\n          for hparam_key, want_hparam in (\n              want_materialized_report.hparams.items()):\n            if isinstance(want_hparam, float):\n              self.assertAllClose(want_hparam,\n                                  materialized_report.hparams[hparam_key])\n            else:\n              self.assertEqual(want_hparam,\n                               materialized_report.hparams[hparam_key])\n\n          self.assertSetEqual(\n              set(want_materialized_report.attributes.keys()),\n              set(materialized_report.attributes.keys()))\n          for attribute_key, want_attribute in (\n              want_materialized_report.attributes.items()):\n            if isinstance(want_attribute, float):\n              self.assertAllClose(\n                  want_attribute,\n                  decode(materialized_report.attributes[attribute_key]))\n            else:\n              self.assertEqual(\n                  want_attribute,\n                  decode(materialized_report.attributes[attribute_key]))\n\n          self.assertSetEqual(\n              set(want_materialized_report.metrics.keys()),\n              set(materialized_report.metrics.keys()))\n          for metric_key, want_metric in (\n              want_materialized_report.metrics.items()):\n            if isinstance(want_metric, float):\n              self.assertAllClose(\n                  want_metric, decode(materialized_report.metrics[metric_key]))\n            else:\n              self.assertEqual(want_metric,\n                               decode(materialized_report.metrics[metric_key]))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/summary.py,48,"b'""""""Tensorboard summaries for the single graph AdaNet implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport contextlib\nimport os\n\nfrom absl import logging\nfrom adanet import tf_compat\nimport six\nimport tensorflow.compat.v1 as tf_v1\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorboard import compat\nfrom tensorflow.python.ops import summary_op_util\nfrom tensorflow.python.summary import summary as summary_lib\n# pylint: enable=g-direct-tensorflow-import\n\n_DEFAULT_SCOPE = ""default""\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Summary(object):\n  """"""Interface for writing summaries to Tensorboard.""""""\n\n  @abc.abstractmethod\n  def scalar(self, name, tensor, family=None, description=None):\n    """"""Outputs a `tf.Summary` protocol buffer containing a single scalar value.\n\n    The generated tf.Summary has a Tensor.proto containing the input Tensor.\n\n    Args:\n      name: A name for this summary. The summary tag used for TensorBoard will\n        be this name prefixed by any active name scopes.\n      tensor: A real numeric scalar value, convertible to a float32 Tensor.\n      family: Optional; if provided, used as the prefix of the summary tag name,\n        which controls the tab name used for display on Tensorboard. DEPRECATED\n        in TF 2.\n      description: Optional long-form description for this summary, as a\n        constant str. Markdown is supported. Defaults to empty.\n\n    Returns:\n      A scalar `Tensor` of type `string`. Which contains a `tf.Summary`\n      protobuf.\n\n    Raises:\n      ValueError: If tensor has the wrong shape or type.\n    """"""\n\n  @abc.abstractmethod\n  def image(self, name, tensor, max_outputs=3, family=None, description=None):\n    """"""Outputs a `tf.Summary` protocol buffer with images.\n\n    The summary has up to `max_outputs` summary values containing images. The\n    images are built from `tensor` which must be 4-D with shape `[batch_size,\n    height, width, channels]` and where `channels` can be:\n\n    *  1: `tensor` is interpreted as Grayscale.\n    *  3: `tensor` is interpreted as RGB.\n    *  4: `tensor` is interpreted as RGBA.\n\n    The images have the same number of channels as the input tensor. For float\n    input, the values are normalized one image at a time to fit in the range\n    `[0, 255]`.  `uint8` values are unchanged.  The op uses two different\n    normalization algorithms:\n\n    *  If the input values are all positive, they are rescaled so the largest\n    one is 255.\n    *  If any input value is negative, the values are shifted so input value 0.0\n      is at 127.  They are then rescaled so that either the smallest value is 0,\n      or the largest one is 255.\n\n    The `tag` in the outputted tf.Summary.Value protobufs is generated based on\n    the\n    name, with a suffix depending on the max_outputs setting:\n\n    *  If `max_outputs` is 1, the summary value tag is \'*name*/image\'.\n    *  If `max_outputs` is greater than 1, the summary value tags are\n      generated sequentially as \'*name*/image/0\', \'*name*/image/1\', etc.\n\n    Args:\n      name: A name for this summary. The summary tag used for TensorBoard will\n        be this name prefixed by any active name scopes.\n      tensor: A Tensor representing pixel data with shape [k, h, w, c], where k\n        is the number of images, h and w are the height and width of the images,\n        and c is the number of channels, which should be 1, 2, 3, or 4\n        (grayscale, grayscale with alpha, RGB, RGBA). Any of the dimensions may\n        be statically unknown (i.e., None). Floating point data will be clipped\n        to the range [0,1).\n      max_outputs: Optional int or rank-0 integer Tensor. At most this many\n        images will be emitted at each step. When more than max_outputs many\n        images are provided, the first max_outputs many images will be used and\n        the rest silently discarded.\n      family: Optional; if provided, used as the prefix of the summary tag name,\n        which controls the tab name used for display on Tensorboard. DEPRECATED\n        in TF 2.\n      description: Optional long-form description for this summary, as a\n        constant str. Markdown is supported. Defaults to empty.\n\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `tf.Summary` protocol\n      buffer.\n    """"""\n\n  @abc.abstractmethod\n  def histogram(self,\n                name,\n                values,\n                family=None,\n                buckets=None,\n                description=None):\n    """"""Outputs a `tf.Summary` protocol buffer with a histogram.\n\n    Adding a histogram summary makes it possible to visualize your data\'s\n    distribution in TensorBoard. You can see a detailed explanation of the\n    TensorBoard histogram dashboard\n    [here](https://www.tensorflow.org/get_started/tensorboard_histograms).\n\n    The generated [`tf.Summary`](\n    tensorflow/core/framework/summary.proto)\n    has one summary value containing a histogram for `values`.\n\n    This op reports an `InvalidArgument` error if any value is not finite.\n\n    Args:\n      name: A name for this summary. The summary tag used for TensorBoard will\n        be this name prefixed by any active name scopes.\n      values: A Tensor of any shape. Must be castable to float64.\n      family: Optional; if provided, used as the prefix of the summary tag name,\n        which controls the tab name used for display on Tensorboard. DEPRECATED\n        in TF 2.\n      buckets: Optional positive int. The output will have this many buckets,\n        except in two edge cases. If there is no data, then there are no\n        buckets. If there is data but all points have the same value, then there\n        is one bucket whose left and right endpoints are the same.\n      description: Optional long-form description for this summary, as a\n        constant str. Markdown is supported. Defaults to empty.\n\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `tf.Summary` protocol\n      buffer.\n    """"""\n\n  @abc.abstractmethod\n  def audio(self,\n            name,\n            tensor,\n            sample_rate,\n            max_outputs=3,\n            family=None,\n            encoding=None,\n            description=None):\n    """"""Writes an audio summary.\n\n    Args:\n      name: A name for this summary. The summary tag used for TensorBoard will\n        be this name prefixed by any active name scopes.\n      tensor: A Tensor representing audio data with shape [k, t, c], where k is\n        the number of audio clips, t is the number of frames, and c is the\n        number of channels. Elements should be floating-point values in [-1.0,\n        1.0]. Any of the dimensions may be statically unknown (i.e., None).\n      sample_rate: An int or rank-0 int32 Tensor that represents the sample\n        rate, in Hz. Must be positive.\n      max_outputs: Optional int or rank-0 integer Tensor. At most this many\n        audio clips will be emitted at each step. When more than max_outputs\n        many clips are provided, the first max_outputs many clips will be used\n        and the rest silently discarded.\n      family: Optional; if provided, used as the prefix of the summary tag name,\n        which controls the tab name used for display on Tensorboard. DEPRECATED\n        in TF 2.\n      encoding: Optional constant str for the desired encoding. Only ""wav"" is\n        currently supported, but this is not guaranteed to remain the default,\n        so if you want ""wav"" in particular, set this explicitly.\n      description: Optional long-form description for this summary, as a\n        constant str. Markdown is supported. Defaults to empty.\n\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `tf.Summary` protocol\n      buffer.\n    """"""\n\n\ndef _strip_scope(name, scope, additional_scope):\n  """"""Returns the name with scope stripped from it.""""""\n\n  if additional_scope:\n    name = name.replace(""{}/"".format(additional_scope), """")\n  if not scope:\n    scope = _DEFAULT_SCOPE\n  name = name.replace(""{}/"".format(scope), """", 1)\n  return name\n\n\nclass _ScopedSummary(Summary):\n  """"""Records summaries in a given scope.\n\n  Each scope gets assigned a different collection where summary ops gets added.\n\n  This allows Tensorboard to display summaries with different scopes but the\n  same name in the same charts.\n  """"""\n\n  def __init__(self, scope=None, skip_summary=False, namespace=None):\n    """"""Initializes a `_ScopedSummary`.\n\n    Args:\n      scope: String scope name.\n      skip_summary: Whether to record summary ops.\n      namespace: Optional string namespace for the summary.\n\n    Returns:\n      A `_ScopedSummary` instance.\n    """"""\n\n    if tf_compat.tpu_function.get_tpu_context().number_of_shards:\n      logging.log_first_n(\n          logging.WARN,\n          ""Scoped summaries will be skipped since they do not support TPU"", 1)\n      skip_summary = True\n\n    self._scope = scope\n    self._namespace = namespace\n    self._additional_scope = None\n    self._skip_summary = skip_summary\n    self._summary_ops = []\n    self._actual_summary_scalar_fn = summary_lib.scalar\n    self._actual_summary_image_fn = summary_lib.image\n    self._actual_summary_histogram_fn = summary_lib.histogram\n    self._actual_summary_audio_fn = summary_lib.audio\n\n  @property\n  def scope(self):\n    """"""Returns scope string.""""""\n\n    return self._scope\n\n  @property\n  def namespace(self):\n    """"""Returns namespace string.""""""\n\n    return self._namespace\n\n  @contextlib.contextmanager\n  def current_scope(self):\n    """"""Registers the current context\'s scope to strip it from summary tags.""""""\n\n    self._additional_scope = tf_compat.v1.get_default_graph().get_name_scope()\n    yield\n    self._additional_scope = None\n\n  @contextlib.contextmanager\n  def _strip_tag_scope(self):\n    """"""Monkey patches `summary_op_util.summary_scope` to strip tag scopes.""""""\n\n    original_summary_scope = summary_op_util.summary_scope\n\n    @contextlib.contextmanager\n    def strip_tag_scope_fn(name, family=None, default_name=None, values=None):\n      tag, scope = (None, None)\n      with original_summary_scope(name, family, default_name, values) as (t, s):\n        tag = _strip_scope(t, self.scope, self._additional_scope)\n        scope = s\n      yield tag, scope\n\n    summary_op_util.summary_scope = strip_tag_scope_fn\n    yield\n    summary_op_util.summary_scope = original_summary_scope\n\n  def _prefix_scope(self, name):\n    """"""Prefixes summary name with scope.""""""\n\n    if self._scope:\n      if name[0] == ""/"":\n        name = name[1:]\n      return ""{scope}/{name}"".format(scope=self._scope, name=name)\n    return name\n\n  def scalar(self, name, tensor, family=None):\n    """"""See `Summary`.""""""\n\n    if self._skip_summary:\n      return tf.constant("""")\n\n    with self._strip_tag_scope():\n      summary = self._actual_summary_scalar_fn(\n          name=self._prefix_scope(name),\n          tensor=tensor,\n          family=family,\n          collections=[])\n    self._summary_ops.append(summary)\n    return summary\n\n  def image(self, name, tensor, max_outputs=3, family=None):\n    """"""See `Summary`.""""""\n\n    if self._skip_summary:\n      return tf.constant("""")\n\n    with self._strip_tag_scope():\n      summary = self._actual_summary_image_fn(\n          name=self._prefix_scope(name),\n          tensor=tensor,\n          max_outputs=max_outputs,\n          family=family,\n          collections=[])\n    self._summary_ops.append(summary)\n    return summary\n\n  def histogram(self, name, values, family=None):\n    """"""See `Summary`.""""""\n\n    if self._skip_summary:\n      return tf.constant("""")\n\n    with self._strip_tag_scope():\n      summary = self._actual_summary_histogram_fn(\n          name=self._prefix_scope(name),\n          values=values,\n          family=family,\n          collections=[])\n    self._summary_ops.append(summary)\n    return summary\n\n  def audio(self, name, tensor, sample_rate, max_outputs=3, family=None):\n    """"""See `Summary`.""""""\n\n    if self._skip_summary:\n      return tf.constant("""")\n\n    with self._strip_tag_scope():\n      summary = self._actual_summary_audio_fn(\n          name=self._prefix_scope(name),\n          tensor=tensor,\n          sample_rate=sample_rate,\n          max_outputs=max_outputs,\n          family=family,\n          collections=[])\n    self._summary_ops.append(summary)\n    return summary\n\n  def merge_all(self):\n    """"""Returns the list of this graph\'s scoped summary ops.\n\n    Note: this is an abuse of the tf.summary.merge_all API since it is expected\n    to return a summary op with all summaries merged. However, ScopedSummary is\n    only used in the internal implementation, so this should be OK.\n    """"""\n\n    current_graph = tf_compat.v1.get_default_graph()\n    return [op for op in self._summary_ops if op.graph == current_graph]\n\n\n# TODO: _ScopedSummary and _ScopedSummaryV2 share a lot of the same\n# methods. Extract a base class for the two, or move shared methods into\n# Summary.\nclass _ScopedSummaryV2(Summary):\n  """"""Records summaries in a given scope.\n\n  Only for TPUEstimator.\n\n  Each scope gets assigned a different collection where summary ops gets added.\n\n  This allows Tensorboard to display summaries with different scopes but the\n  same name in the same charts.\n  """"""\n\n  def __init__(self, logdir, namespace=None, scope=None, skip_summary=False):\n    """"""Initializes a `_TPUScopedSummary`.\n\n    Args:\n      logdir: String directory path for logging summaries.\n      namespace: String namespace to append to the logdir. Can be shared with\n        other `_ScopedSummary` objects.\n      scope: String scope name.\n      skip_summary: Whether to record summary ops.\n\n    Returns:\n      A `_ScopedSummary` instance.\n    """"""\n\n    # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    from tensorboard.plugins.audio import summary_v2 as audio_v2_lib\n    from tensorboard.plugins.histogram import summary_v2 as histogram_v2_lib\n    from tensorboard.plugins.image import summary_v2 as image_v2_lib\n    from tensorboard.plugins.scalar import summary_v2 as scalar_v2_lib\n    # pylint: enable=g-direct-tensorflow-import,g-import-not-at-top\n\n    assert logdir\n\n    if scope == _DEFAULT_SCOPE:\n      raise ValueError(""scope cannot be \'default\'."")\n\n    if namespace:\n      logdir = os.path.join(logdir, namespace)\n    if scope:\n      logdir = os.path.join(logdir, scope)\n    self._logdir = logdir\n    self._namespace = namespace\n    self._scope = scope\n    self._additional_scope = None\n    self._skip_summary = skip_summary\n    self._actual_summary_scalar_fn = scalar_v2_lib.scalar\n    self._actual_summary_image_fn = image_v2_lib.image\n    self._actual_summary_histogram_fn = histogram_v2_lib.histogram\n    self._actual_summary_audio_fn = audio_v2_lib.audio\n    self._summary_tuples = []\n\n  @property\n  def namespace(self):\n    """"""Returns namespace string.""""""\n\n    return self._namespace\n\n  @property\n  def scope(self):\n    """"""Returns scope string.""""""\n\n    return self._scope\n\n  @property\n  def logdir(self):\n    """"""Returns the logdir.""""""\n\n    return self._logdir\n\n  @property\n  def writer(self):\n    """"""Returns the file writer.""""""\n\n    return self._writer\n\n  @contextlib.contextmanager\n  def current_scope(self):\n    """"""Registers the current context\'s scope to strip it from summary tags.""""""\n\n    self._additional_scope = tf_compat.v1.get_default_graph().get_name_scope()\n    try:\n      yield\n    finally:\n      self._additional_scope = None\n\n  @contextlib.contextmanager\n  def _strip_tag_scope(self, additional_scope):\n    """"""Monkey patches `summary_op_util.summary_scope` to strip tag scopes.""""""\n\n    # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    from tensorflow.python.ops import summary_ops_v2 as summary_v2_lib\n    from tensorflow.python.ops.summary_ops_v2 import _INVALID_SCOPE_CHARACTERS\n    # pylint: enable=g-direct-tensorflow-import,g-import-not-at-top\n\n    original_summary_scope = summary_op_util.summary_scope\n    original_summary_scope_v2 = getattr(summary_v2_lib, ""summary_scope"")\n\n    # TF 1.\n    @contextlib.contextmanager\n    def strip_tag_scope_fn(name, family=None, default_name=None, values=None):\n      tag, scope = (None, None)\n      with original_summary_scope(name, family, default_name, values) as (t, s):\n        tag = _strip_scope(t, self.scope, additional_scope)\n        scope = s\n      yield tag, scope\n\n    # TF 2.\n    @contextlib.contextmanager\n    def monkey_patched_summary_scope_fn(name,\n                                        default_name=""summary"",\n                                        values=None):\n      """"""Rescopes the summary tag with the ScopedSummary\'s scope.""""""\n\n      name = name or default_name\n      current_scope = tf_compat.v1.get_default_graph().get_name_scope()\n      tag = current_scope + ""/"" + name if current_scope else name\n      # Strip illegal characters from the scope name, and if that leaves\n      # nothing, use None instead so we pick up the default name.\n      name = _INVALID_SCOPE_CHARACTERS.sub("""", name) or None\n      with tf.compat.v1.name_scope(name, default_name, values) as scope:\n        tag = _strip_scope(tag, self.scope, additional_scope)\n        yield tag, scope\n\n    setattr(summary_op_util, ""summary_scope"", strip_tag_scope_fn)\n    setattr(summary_v2_lib, ""summary_scope"", monkey_patched_summary_scope_fn)\n    setattr(compat.tf2.summary.experimental, ""summary_scope"",\n            monkey_patched_summary_scope_fn)\n    setattr(compat.tf2.summary, ""summary_scope"",\n            monkey_patched_summary_scope_fn)\n    try:\n      yield\n    finally:\n      setattr(summary_op_util, ""summary_scope"", original_summary_scope)\n      setattr(summary_v2_lib, ""summary_scope"", original_summary_scope_v2)\n      setattr(compat.tf2.summary.experimental, ""summary_scope"",\n              original_summary_scope_v2)\n      setattr(compat.tf2.summary, ""summary_scope"", original_summary_scope_v2)\n\n  def _prefix_scope(self, name):\n    scope = self._scope\n    if name[0] == ""/"":\n      name = name[1:]\n    if not scope:\n      scope = _DEFAULT_SCOPE\n    return ""{scope}/{name}"".format(scope=scope, name=name)\n\n  def _create_summary(self, summary_fn, name, tensor):\n    """"""Creates a summary op.\n\n    This will create a function that takes a `Tensor` and adds it to a list with\n    its matching `tensor`.\n\n    Args:\n      summary_fn: A function that takes a name string and `Tensor` and returns a\n        summary op.\n      name: String name of the summary.\n      tensor: `Tensor` to pass to the summary.\n    """"""\n    if self._skip_summary:\n      return\n\n    # additional_scope is set with the context from `current_scope`.\n    # e.g. ""foo/bar"".\n    additional_scope = self._additional_scope\n    # name_scope is from whichever scope the summary actually gets called in.\n    # e.g. ""foo/bar/baz""\n    name_scope = tf_compat.v1.get_default_graph().get_name_scope()\n    # Reuse name_scope if it exists by appending ""/"" to it.\n    name_scope = name_scope + ""/"" if name_scope else name_scope\n\n    def _summary_fn(tensor, step):\n      """"""Creates a summary with the given `Tensor`.""""""\n\n      summary_name = self._prefix_scope(name)\n      # Recover the current name scope when this fn is be called, because the\n      # scope may be different when fns are called.\n      # e.g. ""foo/bar/baz/scalar"" will become ""baz/scalar"" when\n      # additional_scope is ""foo/bar"".\n      # TODO: Figure out a cleaner way to handle this.\n      assert not tf_compat.v1.get_default_graph().get_name_scope()\n      with tf_compat.v1.name_scope(name_scope):\n        with self._strip_tag_scope(additional_scope):\n          # TODO: Do summaries need to be reduced before writing?\n          # Presumably each tensor core creates its own summary so we may be\n          # writing out num_tensor_cores copies of the same value.\n          return summary_fn(summary_name, tensor, step)\n\n    self._summary_tuples.append((_summary_fn, tensor))\n\n  def scalar(self, name, tensor, family=None, description=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_scalar_fn(\n          name=name, data=tensor, description=description, step=step)\n\n    self._create_summary(_summary_fn, name,\n                         tf.reshape(tf.convert_to_tensor(value=tensor), []))\n\n  def image(self, name, tensor, max_outputs=3, family=None, description=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_image_fn(\n          name=name,\n          data=tensor,\n          max_outputs=max_outputs,\n          description=description,\n          step=step)\n\n    self._create_summary(_summary_fn, name, tf.cast(tensor, tf.float32))\n\n  def histogram(self,\n                name,\n                values,\n                family=None,\n                buckets=None,\n                description=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_histogram_fn(\n          name=name,\n          data=tensor,\n          buckets=buckets,\n          description=description,\n          step=step)\n\n    self._create_summary(_summary_fn, name, tf.convert_to_tensor(value=values))\n\n  def audio(self,\n            name,\n            tensor,\n            sample_rate,\n            max_outputs=3,\n            family=None,\n            encoding=None,\n            description=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_audio_fn(\n          name=name,\n          data=tensor,\n          sample_rate=sample_rate,\n          encoding=encoding,\n          description=description,\n          step=step)\n\n    self._create_summary(_summary_fn, name, tf.cast(tensor, tf.float32))\n\n  def summary_tuples(self):\n    """"""Returns an iterable of functions that convert a Tensor to a summary.\n\n    Used for TPU host calls.\n\n    Returns:\n      Iterable of functions that take a single `Tensor` argument.\n    """"""\n    return tuple(self._summary_tuples)\n\n  def clear_summary_tuples(self):\n    """"""Clears the list of current summary tuples.""""""\n\n    self._summary_tuples = []\n\n\nclass _TPUScopedSummary(_ScopedSummaryV2):\n  """"""Records summaries in a given scope.\n\n  Only for TPUEstimator.\n\n  Each scope gets assigned a different collection where summary ops gets added.\n\n  This allows Tensorboard to display summaries with different scopes but the\n  same name in the same charts.\n  """"""\n\n  def __init__(self, logdir, namespace=None, scope=None, skip_summary=False):\n    super(_TPUScopedSummary, self).__init__(logdir, namespace, scope,\n                                            skip_summary)\n    from tensorflow.python.ops import summary_ops_v2 as summary_v2_lib  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    self._actual_summary_scalar_fn = summary_v2_lib.scalar\n    self._actual_summary_image_fn = summary_v2_lib.image\n    self._actual_summary_histogram_fn = summary_v2_lib.histogram\n    self._actual_summary_audio_fn = summary_v2_lib.audio\n\n  def scalar(self, name, tensor, family=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_scalar_fn(\n          name=name, tensor=tensor, family=family, step=step)\n\n    self._create_summary(_summary_fn, name,\n                         tf.reshape(tf.convert_to_tensor(value=tensor), [1]))\n\n  def image(self, name, tensor, max_outputs=3, family=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_image_fn(\n          name=name,\n          tensor=tensor,\n          max_images=max_outputs,\n          family=family,\n          step=step)\n\n    self._create_summary(_summary_fn, name, tf.cast(tensor, tf.float32))\n\n  def histogram(self, name, values, family=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_histogram_fn(\n          name=name, tensor=tensor, family=family, step=step)\n\n    self._create_summary(_summary_fn, name, tf.convert_to_tensor(value=values))\n\n  def audio(self, name, tensor, sample_rate, max_outputs=3, family=None):\n\n    def _summary_fn(name, tensor, step):\n      return self._actual_summary_audio_fn(\n          name=name,\n          tensor=tensor,\n          sample_rate=sample_rate,\n          max_outputs=max_outputs,\n          family=family,\n          step=step)\n\n    self._create_summary(_summary_fn, name, tf.cast(tensor, tf.float32))\n\n\nclass _SummaryWrapper(object):\n  """"""Wraps an `adanet.Summary` to provide summary-like APIs.""""""\n\n  def __init__(self, summary):\n    self._summary = summary\n\n  def scalar(self, name, tensor, collections=None, family=None):\n    """"""See `tf.summary.scalar`.""""""\n\n    if collections is not None:\n      logging.warning(\n          ""The `collections` argument will be ""\n          ""ignored for scalar summary: %s, %s"", name, tensor)\n    return self._summary.scalar(name=name, tensor=tensor, family=family)\n\n  def image(self, name, tensor, max_outputs=3, collections=None, family=None):\n    """"""See `tf.summary.image`.""""""\n\n    if collections is not None:\n      logging.warning(\n          ""The `collections` argument will be ""\n          ""ignored for image summary: %s, %s"", name, tensor)\n    return self._summary.image(\n        name=name, tensor=tensor, max_outputs=max_outputs, family=family)\n\n  def histogram(self, name, values, collections=None, family=None):\n    """"""See `tf.summary.histogram`.""""""\n\n    if collections is not None:\n      logging.warning(\n          ""The `collections` argument will be ""\n          ""ignored for histogram summary: %s, %s"", name, values)\n    return self._summary.histogram(name=name, values=values, family=family)\n\n  def audio(self,\n            name,\n            tensor,\n            sample_rate,\n            max_outputs=3,\n            collections=None,\n            family=None):\n    """"""See `tf.summary.audio`.""""""\n\n    if collections is not None:\n      logging.warning(\n          ""The `collections` argument will be ""\n          ""ignored for audio summary: %s, %s"", name, tensor)\n    return self._summary.audio(\n        name=name,\n        tensor=tensor,\n        sample_rate=sample_rate,\n        max_outputs=max_outputs,\n        family=family)\n\n  def scalar_v2(self, name, tensor, family=None, step=None):\n    """"""See `tf.contrib.summary.scalar`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""scalar summary: %s, %s"", name, tensor)\n    return self._summary.scalar(name=name, tensor=tensor, family=family)\n\n  def image_v2(self,\n               name,\n               tensor,\n               bad_color=None,\n               max_images=3,\n               family=None,\n               step=None):\n    """"""See `tf.contrib.summary.image`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""image summary: %s, %s"", name, tensor)\n    # TODO: Add support for `bad_color` arg.\n    if bad_color is not None:\n      logging.warning(\n          ""The `bad_color` arg is not supported for image summary: %s, %s"",\n          name, tensor)\n    return self._summary.image(\n        name=name, tensor=tensor, max_outputs=max_images, family=family)\n\n  def histogram_v2(self, name, tensor, family=None, step=None):\n    """"""See `tf.contrib.summary.histogram`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""histogram summary: %s, %s"", name, tensor)\n    return self._summary.histogram(name=name, values=tensor, family=family)\n\n  def audio_v2(self,\n               name,\n               tensor,\n               sample_rate,\n               max_outputs,\n               family=None,\n               step=None):\n    """"""See `tf.contrib.summary.audio`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""audio summary: %s, %s"", name, tensor)\n    return self._summary.audio(\n        name=name,\n        tensor=tensor,\n        sample_rate=sample_rate,\n        max_outputs=max_outputs,\n        family=family)\n\n  def scalar_v3(self, name, data, step=None, description=None):\n    """"""See `tf.compat.v2.summary.scalar`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the iteration step for ""\n          ""scalar summary: %s"", name)\n    return self._summary.scalar(name=name, tensor=data, description=description)\n\n  def image_v3(self, name, data, step=None, max_outputs=3, description=None):\n    """"""See `tf.compat.v2.summary.image`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the iteration step for ""\n          ""image summary: %s"", name)\n    return self._summary.image(\n        name=name,\n        tensor=data,\n        max_outputs=max_outputs,\n        description=description)\n\n  def histogram_v3(self, name, data, step=None, buckets=None, description=None):\n    """"""See `tf.compat.v2.summary.histogram`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""histogram summary: %s"", name)\n    return self._summary.histogram(\n        name=name, tensor=data, buckets=buckets, description=description)\n\n  def audio_v3(self,\n               name,\n               data,\n               sample_rate,\n               step=None,\n               max_outputs=3,\n               encoding=None,\n               description=None):\n    """"""See `tf.compat.v2.summary.audio`.""""""\n\n    if step is not None:\n      logging.warning(\n          ""The `step` argument will be ignored to use the global step for ""\n          ""audio summary: %s"", name)\n    return self._summary.audio(\n        name=name,\n        tensor=data,\n        sample_rate=sample_rate,\n        max_outputs=max_outputs,\n        encoding=encoding,\n        description=description)\n\n\n@contextlib.contextmanager\ndef monkey_patched_summaries(summary):\n  """"""A context where global summary functions point to the given summary.\n\n  Restores original summary functions upon exit.\n\n  NOTE: This function is not thread-safe.\n\n  Args:\n    summary: An `adanet.Summary` instance.\n\n  Yields:\n    A context where summary functions are routed to the given `adanet.Summary`.\n  """"""\n\n  from tensorflow.python.ops import summary_ops_v2 as summary_v2_lib  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n  old_summary_scalar = summary_lib.scalar\n  old_summary_image = summary_lib.image\n  old_summary_histogram = summary_lib.histogram\n  old_summary_audio = summary_lib.audio\n  old_summary_v2_scalar = summary_v2_lib.scalar\n  old_summary_v2_image = summary_v2_lib.image\n  old_summary_v2_histogram = summary_v2_lib.histogram\n  old_summary_v2_audio = summary_v2_lib.audio\n  old_summary_compat_v2_scalar = tf_compat.v2.summary.scalar\n  old_summary_compat_v2_image = tf_compat.v2.summary.image\n  old_summary_compat_v2_histogram = tf_compat.v2.summary.histogram\n  old_summary_compat_v2_audio = tf_compat.v2.summary.audio\n\n  # Monkey-patch global attributes.\n  wrapped_summary = _SummaryWrapper(summary)\n  setattr(tf_v1.summary, ""scalar"", wrapped_summary.scalar)\n  setattr(tf_v1.summary, ""image"", wrapped_summary.image)\n  setattr(tf_v1.summary, ""histogram"", wrapped_summary.histogram)\n  setattr(tf_v1.summary, ""audio"", wrapped_summary.audio)\n  setattr(tf_compat.v1.summary, ""scalar"", wrapped_summary.scalar)\n  setattr(tf_compat.v1.summary, ""image"", wrapped_summary.image)\n  setattr(tf_compat.v1.summary, ""histogram"", wrapped_summary.histogram)\n  setattr(tf_compat.v1.summary, ""audio"", wrapped_summary.audio)\n  setattr(summary_lib, ""scalar"", wrapped_summary.scalar)\n  setattr(summary_lib, ""image"", wrapped_summary.image)\n  setattr(summary_lib, ""histogram"", wrapped_summary.histogram)\n  setattr(summary_lib, ""audio"", wrapped_summary.audio)\n  setattr(tf_compat.v2.summary, ""scalar"", wrapped_summary.scalar_v3)\n  setattr(tf_compat.v2.summary, ""image"", wrapped_summary.image_v3)\n  setattr(tf_compat.v2.summary, ""histogram"", wrapped_summary.histogram_v3)\n  setattr(tf_compat.v2.summary, ""audio"", wrapped_summary.audio_v3)\n  setattr(summary_v2_lib, ""scalar"", wrapped_summary.scalar_v2)\n  setattr(summary_v2_lib, ""image"", wrapped_summary.image_v2)\n  setattr(summary_v2_lib, ""histogram"", wrapped_summary.histogram_v2)\n  setattr(summary_v2_lib, ""audio"", wrapped_summary.audio_v2)\n  try:\n    # TF 2.0 eliminates tf.contrib.\n    setattr(tf_v1.contrib.summary, ""scalar"", wrapped_summary.scalar_v2)\n    setattr(tf_v1.contrib.summary, ""image"", wrapped_summary.image_v2)\n    setattr(tf_v1.contrib.summary, ""histogram"", wrapped_summary.histogram_v2)\n    setattr(tf_v1.contrib.summary, ""audio"", wrapped_summary.audio_v2)\n  except (AttributeError, ImportError):\n    # TF 2.0 eliminates tf.contrib.\n    # Also set the new tf.summary to be use the new summaries in TF 2.\n    if tf_compat.version_greater_or_equal(""2.0.0""):\n      setattr(tf.summary, ""scalar"", wrapped_summary.scalar_v3)\n      setattr(tf.summary, ""image"", wrapped_summary.image_v3)\n      setattr(tf.summary, ""histogram"", wrapped_summary.histogram_v3)\n      setattr(tf.summary, ""audio"", wrapped_summary.audio_v3)\n\n  try:\n    yield\n  finally:\n    # Revert monkey-patches.\n    try:\n      setattr(tf_v1.contrib.summary, ""audio"", old_summary_v2_audio)\n      setattr(tf_v1.contrib.summary, ""histogram"", old_summary_v2_histogram)\n      setattr(tf_v1.contrib.summary, ""image"", old_summary_v2_image)\n      setattr(tf_v1.contrib.summary, ""scalar"", old_summary_v2_scalar)\n    except (AttributeError, ImportError):\n      # TF 2.0 eliminates tf.contrib.\n      pass\n    setattr(summary_v2_lib, ""audio"", old_summary_v2_audio)\n    setattr(summary_v2_lib, ""histogram"", old_summary_v2_histogram)\n    setattr(summary_v2_lib, ""image"", old_summary_v2_image)\n    setattr(summary_v2_lib, ""scalar"", old_summary_v2_scalar)\n    setattr(tf.summary, ""audio"", old_summary_compat_v2_audio)\n    setattr(tf.summary, ""histogram"", old_summary_compat_v2_histogram)\n    setattr(tf.summary, ""image"", old_summary_compat_v2_image)\n    setattr(tf.summary, ""scalar"", old_summary_compat_v2_scalar)\n    setattr(tf_compat.v2.summary, ""audio"", old_summary_compat_v2_audio)\n    setattr(tf_compat.v2.summary, ""histogram"", old_summary_compat_v2_histogram)\n    setattr(tf_compat.v2.summary, ""image"", old_summary_compat_v2_image)\n    setattr(tf_compat.v2.summary, ""scalar"", old_summary_compat_v2_scalar)\n    setattr(summary_lib, ""audio"", old_summary_audio)\n    setattr(summary_lib, ""histogram"", old_summary_histogram)\n    setattr(summary_lib, ""image"", old_summary_image)\n    setattr(summary_lib, ""scalar"", old_summary_scalar)\n    setattr(tf_compat.v1.summary, ""audio"", old_summary_audio)\n    setattr(tf_compat.v1.summary, ""histogram"", old_summary_histogram)\n    setattr(tf_compat.v1.summary, ""image"", old_summary_image)\n    setattr(tf_compat.v1.summary, ""scalar"", old_summary_scalar)\n    setattr(tf_v1.summary, ""audio"", old_summary_audio)\n    setattr(tf_v1.summary, ""histogram"", old_summary_histogram)\n    setattr(tf_v1.summary, ""image"", old_summary_image)\n    setattr(tf_v1.summary, ""scalar"", old_summary_scalar)\n'"
adanet/core/summary_test.py,95,"b'""""""Test AdaNet summary single graph implementation for TF 1.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core import testing_utils as tu\nfrom adanet.core.summary import _ScopedSummary\nfrom adanet.core.summary import _TPUScopedSummary\nfrom adanet.core.summary import monkey_patched_summaries\nfrom six.moves import range\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.ops import summary_ops_v2\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef decode(proto_str):\n  """"""Decodes a proto string.""""""\n\n  return proto_str.decode(""utf-8"")\n\n\nclass ScopedSummaryTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_scope(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    self.assertEqual(scope, scoped_summary.scope)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_scalar_summary(self, scope, skip_summary=False):\n    scoped_summary = _ScopedSummary(scope, skip_summary)\n    with self.test_session() as s:\n      i = tf.constant(3)\n      with tf.name_scope(""outer""):\n        im = scoped_summary.scalar(""inner"", i)\n      summary_str = s.run(im)\n    if skip_summary:\n      self.assertEqual("""", decode(summary_str))\n      return\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    values = summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""outer/inner"")\n    self.assertEqual(values[0].simple_value, 3.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_scalar_summary_with_family(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    with self.test_session() as s:\n      i = tf.constant(7)\n      with tf.name_scope(""outer""):\n        im1 = scoped_summary.scalar(""inner"", i, family=""family"")\n        im2 = scoped_summary.scalar(""inner"", i, family=""family"")\n      sm1, sm2 = s.run([im1, im2])\n    summary = tf.Summary()\n\n    summary.ParseFromString(sm1)\n    values = summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""family/outer/family/inner"")\n    self.assertEqual(values[0].simple_value, 7.0)\n\n    summary.ParseFromString(sm2)\n    values = summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""family/outer/family/inner_1"")\n    self.assertEqual(values[0].simple_value, 7.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_summarizing_variable(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    with self.test_session() as s:\n      c = tf.constant(42.0)\n      v = tf.Variable(c)\n      ss = scoped_summary.scalar(""summary"", v)\n      init = tf.global_variables_initializer()\n      s.run(init)\n      summ_str = s.run(ss)\n    summary = tf.Summary()\n    summary.ParseFromString(summ_str)\n    self.assertLen(summary.value, 1)\n    value = summary.value[0]\n    self.assertEqual(value.tag, ""summary"")\n    self.assertEqual(value.simple_value, 42.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_image_summary(self, scope, skip_summary=False):\n    scoped_summary = _ScopedSummary(scope, skip_summary)\n    with self.test_session() as s:\n      i = tf.ones((5, 4, 4, 3))\n      with tf.name_scope(""outer""):\n        im = scoped_summary.image(""inner"", i, max_outputs=3)\n      summary_str = s.run(im)\n    if skip_summary:\n      self.assertEqual("""", decode(summary_str))\n      return\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    values = summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(""outer/inner/image/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_image_summary_with_family(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    with self.test_session() as s:\n      i = tf.ones((5, 2, 3, 1))\n      with tf.name_scope(""outer""):\n        im = scoped_summary.image(""inner"", i, max_outputs=3, family=""family"")\n      summary_str = s.run(im)\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    values = summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(\n        ""family/outer/family/inner/image/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_histogram_summary(self, scope, skip_summary=False):\n    scoped_summary = _ScopedSummary(scope, skip_summary)\n    with self.test_session() as s:\n      i = tf.ones((5, 4, 4, 3))\n      with tf.name_scope(""outer""):\n        summ_op = scoped_summary.histogram(""inner"", i)\n      summary_str = s.run(summ_op)\n    if skip_summary:\n      self.assertEqual("""", decode(summary_str))\n      return\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    self.assertLen(summary.value, 1)\n    self.assertEqual(summary.value[0].tag, ""outer/inner"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_histogram_summary_with_family(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    with self.test_session() as s:\n      i = tf.ones((5, 4, 4, 3))\n      with tf.name_scope(""outer""):\n        summ_op = scoped_summary.histogram(""inner"", i, family=""family"")\n      summary_str = s.run(summ_op)\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    self.assertLen(summary.value, 1)\n    self.assertEqual(summary.value[0].tag, ""family/outer/family/inner"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_audio_summary(self, scope, skip_summary=False):\n    scoped_summary = _ScopedSummary(scope, skip_summary)\n    with self.test_session() as s:\n      i = tf.ones((5, 3, 4))\n      with tf.name_scope(""outer""):\n        aud = scoped_summary.audio(""inner"", i, 0.2, max_outputs=3)\n      summary_str = s.run(aud)\n    if skip_summary:\n      self.assertEqual("""", decode(summary_str))\n      return\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    values = summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(""outer/inner/audio/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_audio_summary_with_family(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    with self.test_session() as s:\n      i = tf.ones((5, 3, 4))\n      with tf.name_scope(""outer""):\n        aud = scoped_summary.audio(\n            ""inner"", i, 0.2, max_outputs=3, family=""family"")\n      summary_str = s.run(aud)\n    summary = tf.Summary()\n    summary.ParseFromString(summary_str)\n    values = summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(\n        ""family/outer/family/inner/audio/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_summary_name_conversion(self, scope):\n    scoped_summary = _ScopedSummary(scope)\n    c = tf.constant(3)\n    summary = tf.Summary()\n\n    with self.test_session() as sess:\n      s = scoped_summary.scalar(""name with spaces"", c)\n      summary.ParseFromString(sess.run(s))\n      self.assertEqual(summary.value[0].tag, ""name_with_spaces"")\n\n      s2 = scoped_summary.scalar(""name with many $#illegal^: characters!"", c)\n      summary.ParseFromString(sess.run(s2))\n      self.assertEqual(summary.value[0].tag,\n                       ""name_with_many___illegal___characters_"")\n\n      s3 = scoped_summary.scalar(""/name/with/leading/slash"", c)\n      summary.ParseFromString(sess.run(s3))\n      self.assertEqual(summary.value[0].tag, ""name/with/leading/slash"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""single_graph"",\n          ""nest_graph"": False,\n      }, {\n          ""testcase_name"": ""nested_graph"",\n          ""nest_graph"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_merge_all(self, nest_graph):\n    c0 = tf.constant(0)\n    c1 = tf.constant(1)\n\n    scoped_summary0 = _ScopedSummary()\n    scoped_summary0.scalar(""c0"", c0)\n    scoped_summary0.scalar(""c1"", c1)\n\n    scoped_summary1 = _ScopedSummary(""scope1"")\n    scoped_summary1.scalar(""c0"", c0)\n    scoped_summary1.scalar(""c1"", c1)\n\n    scoped_summary2 = _ScopedSummary(""scope2"")\n    scoped_summary2.scalar(""c0"", c0)\n    scoped_summary2.scalar(""c1"", c1)\n\n    config = tf.compat.v1.ConfigProto(\n        gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n\n    if nest_graph:\n      with tf.Graph().as_default():\n        scoped_summary2.scalar(""c2"", tf.constant(2))\n        with tf.Session(config=config) as sess:\n          summaries = scoped_summary2.merge_all()\n          tf.logging.warn(""summaries %s"", summaries)\n          summary = tf.Summary()\n          summary.ParseFromString(sess.run(tf.summary.merge(summaries)))\n          self.assertEqual([""c2""], [s.tag for s in summary.value])\n          self.assertEqual([2], [s.simple_value for s in summary.value])\n\n    with tf.Session(config=config) as sess:\n      for scoped_summary in [scoped_summary0, scoped_summary1, scoped_summary2]:\n        summaries = scoped_summary.merge_all()\n        summary = tf.Summary()\n        summary.ParseFromString(sess.run(tf.summary.merge(summaries)))\n        self.assertEqual([""c0"", ""c1""], [s.tag for s in summary.value])\n        self.assertEqual([0, 1], [s.simple_value for s in summary.value])\n\n  @tf_compat.skip_for_tf2\n  def test_summary_args(self):\n    summary = _ScopedSummary()\n    summary.scalar(""scalar"", 1, ""family"")\n    summary.image(""image"", 1, 3, ""family"")\n    summary.histogram(""histogram"", 1, ""family"")\n    summary.audio(""audio"", 1, 3, 3, ""family"")\n    self.assertLen(summary.merge_all(), 4)\n\n  @tf_compat.skip_for_tf2\n  def test_summary_kwargs(self):\n    summary = _ScopedSummary()\n    summary.scalar(name=""scalar"", tensor=1, family=""family"")\n    summary.image(name=""image"", tensor=1, max_outputs=3, family=""family"")\n    summary.histogram(name=""histogram"", values=1, family=""family"")\n    summary.audio(\n        name=""audio"", tensor=1, sample_rate=3, max_outputs=3, family=""family"")\n    self.assertLen(summary.merge_all(), 4)\n\n\nclass TPUScopedSummaryTest(tu.AdanetTestCase):\n\n  def read_single_event_from_eventfile(self, summary):\n    dir_ = self.test_subdirectory\n    if summary.namespace:\n      dir_ = os.path.join(dir_, summary.namespace)\n    if summary.scope:\n      dir_ = os.path.join(dir_, summary.scope)\n    event_files = sorted(tf.gfile.Glob(os.path.join(dir_, ""*.v2"")))\n    events = list(tf.train.summary_iterator(event_files[-1]))\n    # Expect a boilerplate event for the file_version, then the summary one.\n    self.assertGreaterEqual(len(events), 2)\n    return events[1:]\n\n  def write_summaries(self, summary):\n    summary_ops = []\n    writer = summary_ops_v2.create_file_writer(summary.logdir)\n    with writer.as_default(), summary_ops_v2.always_record_summaries():\n      for summary_fn, tensor in summary.summary_tuples():\n        summary_ops.append(summary_fn(tensor, step=10))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(summary_ops_v2.summary_writer_initializer_op())\n      sess.run(summary_ops)\n      sess.run(writer.flush())\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_scope(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    self.assertEqual(scope, scoped_summary.scope)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_scalar_summary(self, scope, skip_summary=False):\n    scoped_summary = _TPUScopedSummary(\n        self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n    i = tf.constant(3)\n    with tf.name_scope(""outer""):\n      scoped_summary.scalar(""inner"", i)\n    self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""outer/inner"")\n    self.assertEqual(values[0].simple_value, 3.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_scalar_summary_with_family(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    i = tf.constant(7)\n    with tf.name_scope(""outer""):\n      scoped_summary.scalar(""inner"", i, family=""family"")\n      scoped_summary.scalar(""inner"", i, family=""family"")\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    self.assertLen(events[0].summary.value, 1)\n    self.assertLen(events[1].summary.value, 1)\n\n    self.assertEqual(\n        {\n            ""family/outer/family/inner"": 7.0,\n            ""family/outer/family/inner_1"": 7.0\n        }, {\n            event.summary.value[0].tag: event.summary.value[0].simple_value\n            for event in events\n        })\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_summarizing_variable(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    c = tf.constant(42.0)\n    v = tf.Variable(c)\n    scoped_summary.scalar(""summary"", v)\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    value = values[0]\n    self.assertEqual(value.tag, ""summary"")\n    self.assertEqual(value.simple_value, 42.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_image_summary(self, scope, skip_summary=False):\n    scoped_summary = _TPUScopedSummary(\n        self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n    i = tf.ones((5, 4, 4, 3))\n    with tf.name_scope(""outer""):\n      scoped_summary.image(""inner"", i, max_outputs=3)\n    self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(""outer/inner/image/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_image_summary_with_family(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    i = tf.ones((5, 2, 3, 1))\n    with tf.name_scope(""outer""):\n      scoped_summary.image(""inner"", i, max_outputs=3, family=""family"")\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(\n        ""family/outer/family/inner/image/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_histogram_summary(self, scope, skip_summary=False):\n    scoped_summary = _TPUScopedSummary(\n        self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n    i = tf.ones((5, 4, 4, 3))\n    with tf.name_scope(""outer""):\n      scoped_summary.histogram(""inner"", i)\n    self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""outer/inner"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_histogram_summary_with_family(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    i = tf.ones((5, 4, 4, 3))\n    with tf.name_scope(""outer""):\n      scoped_summary.histogram(""inner"", i, family=""family"")\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""family/outer/family/inner"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf2\n  def test_audio_summary(self, scope, skip_summary=False):\n    scoped_summary = _TPUScopedSummary(\n        self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n    i = tf.ones((5, 3, 4))\n    with tf.name_scope(""outer""):\n      scoped_summary.audio(""inner"", i, 0.2, max_outputs=3)\n    self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(""outer/inner/audio/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_audio_summary_with_family(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    i = tf.ones((5, 3, 4))\n    with tf.name_scope(""outer""):\n      scoped_summary.audio(""inner"", i, 0.2, max_outputs=3, family=""family"")\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 3)\n    tags = sorted(v.tag for v in values)\n    expected = sorted(\n        ""family/outer/family/inner/audio/{}"".format(i) for i in range(3))\n    self.assertEqual(tags, expected)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_summary_name_conversion(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    c = tf.constant(3)\n    scoped_summary.scalar(""name with spaces"", c)\n    scoped_summary.scalar(""name with many $#illegal^: characters!"", c)\n    scoped_summary.scalar(""/name/with/leading/slash"", c)\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    self.assertLen(events, 3)\n    tags = [event.summary.value[0].tag for event in events]\n    self.assertIn(""name_with_spaces"", tags)\n    self.assertIn(""name_with_many___illegal___characters_"", tags)\n    self.assertIn(""name/with/leading/slash"", tags)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf2\n  def test_current_scope(self, scope):\n    scoped_summary = _TPUScopedSummary(self.test_subdirectory, scope=scope)\n    i = tf.constant(3)\n    with tf.variable_scope(""outer1""):\n      with tf.variable_scope(""outer2""):\n        with scoped_summary.current_scope():\n          with tf.variable_scope(""inner1""):\n            scoped_summary.scalar(""inner2/a/b/c"", i)\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""inner1/inner2/a/b/c"")\n    self.assertEqual(values[0].simple_value, 3.0)\n\n  @tf_compat.skip_for_tf2\n  def test_summary_args(self):\n    summary = _TPUScopedSummary(self.test_subdirectory)\n    summary.scalar(""scalar"", 1, ""family"")\n    summary.image(""image"", 1, 3, ""family"")\n    summary.histogram(""histogram"", 1, ""family"")\n    summary.audio(""audio"", 1, 3, 3, ""family"")\n    self.assertLen(summary.summary_tuples(), 4)\n\n  @tf_compat.skip_for_tf2\n  def test_summary_kwargs(self):\n    summary = _TPUScopedSummary(self.test_subdirectory)\n    summary.scalar(name=""scalar"", tensor=1, family=""family"")\n    summary.image(name=""image"", tensor=1, max_outputs=3, family=""family"")\n    summary.histogram(name=""histogram"", values=1, family=""family"")\n    summary.audio(\n        name=""audio"", tensor=1, sample_rate=3, max_outputs=3, family=""family"")\n    self.assertLen(summary.summary_tuples(), 4)\n\n\ndef _summaries():\n  """"""Returns all summary functions.""""""\n\n  fns = [\n      tf.summary.scalar, tf.summary.audio, tf.summary.histogram,\n      tf.summary.image, tf_compat.v1.summary.scalar, tf_compat.v1.summary.audio,\n      tf_compat.v1.summary.histogram, tf_compat.v1.summary.image,\n      tf_compat.v2.summary.scalar, tf_compat.v2.summary.audio,\n      tf_compat.v2.summary.histogram, tf_compat.v2.summary.image\n  ]\n  try:\n    fns += [\n        tf.contrib.summary.scalar, tf.contrib.summary.audio,\n        tf.contrib.summary.histogram, tf.contrib.summary.image\n    ]\n  except (AttributeError, ImportError):\n    # TF 2.0 eliminates tf.contrib.\n    pass\n  return fns\n\n\nclass MonkeyPatchTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _get_summary_ops(self, summary):\n    if isinstance(summary, _TPUScopedSummary):\n      return [fn(arg, step=10) for fn, arg in summary.summary_tuples()]\n    return summary.merge_all()\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""with_scoped_summary"",\n          ""summary_maker"": _ScopedSummary,\n      }, {\n          ""testcase_name"":\n              ""with_tpu_scoped_summary"",\n          ""summary_maker"":\n              functools.partial(_TPUScopedSummary, logdir=""/tmp/fakedir"")\n      })\n  @tf_compat.skip_for_tf2\n  def test_monkey_patched_summaries_args(self, summary_maker):\n    summary = summary_maker()\n    before = _summaries()\n    with monkey_patched_summaries(summary):\n      for want, got in zip(before, _summaries()):\n        self.assertNotEqual(want, got)\n      tf.summary.scalar(""scalar"", 1, [""collection""], ""family"")\n      tf.summary.image(""image"", 1, 3, [""collection""], ""family"")\n      tf.summary.histogram(""histogram"", 1, [""collection""], ""family"")\n      tf.summary.audio(""audio"", 1, 3, 3, [""collection""], ""family"")\n\n      want_summary_fn_count = 4\n      try:\n        tf.contrib.summary.scalar(""scalar_v2"", 1, ""family"", 10)\n        tf.contrib.summary.image(""image_v2"", 1, True, 3, ""family"", 10)\n        tf.contrib.summary.histogram(""histogram_v2"", 1, ""family"", 10)\n        tf.contrib.summary.audio(""audio_v2"", 1, 3, 3, ""family"", 10)\n        want_summary_fn_count += 4\n      except (AttributeError, ImportError):\n        # TF 2.0 eliminates tf.contrib.\n        pass\n    self.assertEqual(before, _summaries())\n    self.assertLen(self._get_summary_ops(summary), want_summary_fn_count)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""with_scoped_summary"",\n          ""summary_maker"": _ScopedSummary,\n      }, {\n          ""testcase_name"":\n              ""with_tpu_scoped_summary"",\n          ""summary_maker"":\n              functools.partial(_TPUScopedSummary, logdir=""/tmp/fakedir""),\n      })\n  @tf_compat.skip_for_tf2\n  def test_monkey_patched_summaries_kwargs(self, summary_maker):\n    summary = summary_maker()\n    before = _summaries()\n    with monkey_patched_summaries(summary):\n      for want, got in zip(before, _summaries()):\n        self.assertNotEqual(want, got)\n      tf.summary.scalar(\n          name=""scalar"", tensor=1, collections=[""collection""], family=""family"")\n      tf.summary.image(\n          name=""image"",\n          tensor=1,\n          max_outputs=3,\n          collections=[""collection""],\n          family=""family"")\n      tf.summary.histogram(\n          name=""histogram"",\n          values=1,\n          collections=[""collection""],\n          family=""family"")\n      tf.summary.audio(\n          name=""audio"",\n          tensor=1,\n          sample_rate=3,\n          max_outputs=3,\n          collections=[""collection""],\n          family=""family"")\n\n      want_summary_fn_count = 4\n      try:\n        tf.contrib.summary.scalar(\n            name=""scalar_v2"", tensor=1, family=""family"", step=10)\n        tf.contrib.summary.image(\n            name=""image_v2"",\n            tensor=1,\n            bad_color=True,\n            max_images=3,\n            family=""family"",\n            step=10)\n        tf.contrib.summary.histogram(\n            name=""histogram_v2"", tensor=1, family=""family"", step=10)\n        tf.contrib.summary.audio(\n            name=""audio_v2"",\n            tensor=1,\n            sample_rate=3,\n            max_outputs=3,\n            family=""family"",\n            step=10)\n        want_summary_fn_count += 4\n      except (AttributeError, ImportError):\n        # TF 2.0 eliminates tf.contrib.\n        pass\n    self.assertEqual(before, _summaries())\n    self.assertLen(self._get_summary_ops(summary), want_summary_fn_count)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/summary_v2_test.py,21,"b'""""""Test AdaNet summary single graph implementation for TF 2.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport struct\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core import testing_utils as tu\nfrom adanet.core.summary import _ScopedSummaryV2\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\ndef simple_value(summary_value):\n  """"""Returns the scalar parsed from the summary proto tensor_value bytes.""""""\n\n  return struct.unpack(""<f"", summary_value.tensor.tensor_content)[0]\n\n\nclass ScopedSummaryV2Test(tu.AdanetTestCase):\n\n  def read_single_event_from_eventfile(self, summary):\n    dir_ = self.test_subdirectory\n    if summary.namespace:\n      dir_ = os.path.join(dir_, summary.namespace)\n    if summary.scope:\n      dir_ = os.path.join(dir_, summary.scope)\n    event_files = sorted(tf.io.gfile.glob(os.path.join(dir_, ""*.v2"")))\n    events = list(tf.compat.v1.train.summary_iterator(event_files[-1]))\n    # Expect a boilerplate event for the file_version, then the summary one.\n    self.assertGreaterEqual(len(events), 2)\n    return events[1:]\n\n  def write_summaries(self, summary):\n    summary_ops = []\n    writer = tf.summary.create_file_writer(summary.logdir)\n    with writer.as_default():\n      for summary_fn, tensor in summary.summary_tuples():\n        summary_ops.append(summary_fn(tensor, step=10))\n\n    writer_flush = writer.flush()\n    self.evaluate([tf.compat.v1.global_variables_initializer(), writer.init()])\n    self.evaluate(summary_ops)\n    self.evaluate(writer_flush)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_scope(self, scope):\n    scoped_summary = _ScopedSummaryV2(self.test_subdirectory, scope=scope)\n    self.assertEqual(scope, scoped_summary.scope)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_scalar_summary(self, scope, skip_summary=False):\n    with context.graph_mode():\n      scoped_summary = _ScopedSummaryV2(\n          self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n      i = tf.constant(3)\n      with tf.name_scope(""outer""):\n        scoped_summary.scalar(""inner"", i)\n      self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""outer/inner"")\n    self.assertEqual(simple_value(values[0]), 3.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_summarizing_variable(self, scope):\n    scoped_summary = _ScopedSummaryV2(self.test_subdirectory, scope=scope)\n    c = tf.constant(42.0)\n    v = tf.Variable(c)\n    scoped_summary.scalar(""summary"", v)\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    value = values[0]\n    self.assertEqual(value.tag, ""summary"")\n    self.assertEqual(simple_value(value), 42.0)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_image_summary(self, scope, skip_summary=False):\n    with context.graph_mode():\n      scoped_summary = _ScopedSummaryV2(\n          self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n      i = tf.ones((5, 4, 4, 3))\n      with tf.name_scope(""outer""):\n        scoped_summary.image(""inner"", i, max_outputs=3)\n      self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(""outer/inner"", values[0].tag)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_histogram_summary(self, scope, skip_summary=False):\n    with context.graph_mode():\n      scoped_summary = _ScopedSummaryV2(\n          self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n      i = tf.ones((5, 4, 4, 3))\n      with tf.name_scope(""outer""):\n        scoped_summary.histogram(""inner"", i)\n      self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(""outer/inner"", values[0].tag)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      }, {\n          ""testcase_name"": ""skip_summary"",\n          ""scope"": None,\n          ""skip_summary"": True,\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_audio_summary(self, scope, skip_summary=False):\n    with context.graph_mode():\n      scoped_summary = _ScopedSummaryV2(\n          self.test_subdirectory, scope=scope, skip_summary=skip_summary)\n      i = tf.ones((5, 3, 4))\n      with tf.name_scope(""outer""):\n        scoped_summary.audio(""inner"", i, sample_rate=2, max_outputs=3)\n      self.write_summaries(scoped_summary)\n    if skip_summary:\n      return\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""outer/inner"")\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_summary_name_conversion(self, scope):\n    scoped_summary = _ScopedSummaryV2(self.test_subdirectory, scope=scope)\n    c = tf.constant(3)\n    scoped_summary.scalar(""name with spaces"", c)\n    scoped_summary.scalar(""name with many $#illegal^: characters!"", c)\n    scoped_summary.scalar(""/name/with/leading/slash"", c)\n    self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    self.assertLen(events, 3)\n    tags = [event.summary.value[0].tag for event in events]\n    # Characters that were illegal in TF 1 are now valid in TF 2.\n    self.assertIn(""name with spaces"", tags)\n    self.assertIn(""name with many $#illegal^: characters!"", tags)\n    self.assertIn(""name/with/leading/slash"", tags)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""without_scope"",\n          ""scope"": None,\n      }, {\n          ""testcase_name"": ""with_scope"",\n          ""scope"": ""with_scope"",\n      })\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_current_scope(self, scope):\n    with context.graph_mode():\n      scoped_summary = _ScopedSummaryV2(self.test_subdirectory, scope=scope)\n      i = tf.constant(3)\n      with tf.compat.v1.variable_scope(""outer1""):\n        with tf.compat.v1.variable_scope(""outer2""):\n          with scoped_summary.current_scope():\n            with tf.compat.v1.variable_scope(""inner1""):\n              scoped_summary.scalar(""inner2/a/b/c"", i)\n      self.write_summaries(scoped_summary)\n    events = self.read_single_event_from_eventfile(scoped_summary)\n    values = events[0].summary.value\n    self.assertLen(values, 1)\n    self.assertEqual(values[0].tag, ""inner1/inner2/a/b/c"")\n    self.assertEqual(simple_value(values[0]), 3.0)\n\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_summary_args(self):\n    summary = _ScopedSummaryV2(self.test_subdirectory)\n    summary.scalar(""scalar"", 1, ""family"")\n    summary.image(""image"", 1, 3, ""family"")\n    summary.histogram(""histogram"", 1, ""family"")\n    summary.audio(""audio"", 1, 3, 3, ""family"")\n    self.assertLen(summary.summary_tuples(), 4)\n\n  @tf_compat.skip_for_tf1\n  @test_util.run_in_graph_and_eager_modes\n  def test_summary_kwargs(self):\n    summary = _ScopedSummaryV2(self.test_subdirectory)\n    summary.scalar(name=""scalar"", tensor=1, family=""family"")\n    summary.image(name=""image"", tensor=1, max_outputs=3, family=""family"")\n    summary.histogram(name=""histogram"", values=1, family=""family"")\n    summary.audio(\n        name=""audio"", tensor=1, sample_rate=3, max_outputs=3, family=""family"")\n    self.assertLen(summary.summary_tuples(), 4)\n\n\nif __name__ == ""__main__"":\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/core/testing_utils.py,31,"b'""""""Test utilities for AdaNet single graph implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport struct\nimport sys\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom adanet import ensemble as ensemble_lib\nfrom adanet import subnetwork as subnetwork_lib\nfrom adanet import tf_compat\nfrom adanet.core.architecture import _Architecture\nfrom adanet.core.candidate import _Candidate\nfrom adanet.core.ensemble_builder import _EnsembleSpec\nfrom adanet.core.ensemble_builder import _SubnetworkSpec\nfrom adanet.core.eval_metrics import _EnsembleMetrics\nfrom adanet.core.eval_metrics import _IterationMetrics\nfrom adanet.core.eval_metrics import _SubnetworkMetrics\nimport tensorflow.compat.v2 as tf\n\n\ndef dummy_tensor(shape=(), random_seed=42):\n  """"""Returns a randomly initialized tensor.""""""\n\n  return tf.Variable(\n      tf_compat.random_normal(shape=shape, seed=random_seed),\n      trainable=False).read_value()\n\n\nclass ExportOutputKeys(object):\n  """"""Different export output keys for the dummy ensemble builder.""""""\n\n  CLASSIFICATION_CLASSES = ""classification_classes""\n  CLASSIFICATION_SCORES = ""classification_scores""\n  REGRESSION = ""regression""\n  PREDICTION = ""prediction""\n  INVALID = ""invalid""\n\n\ndef dummy_ensemble_spec(name,\n                        random_seed=42,\n                        num_subnetworks=1,\n                        bias=0.,\n                        loss=None,\n                        adanet_loss=None,\n                        eval_metrics=None,\n                        variables=None,\n                        dict_predictions=False,\n                        export_output_key=None,\n                        subnetwork_builders=None,\n                        train_op=None):\n  """"""Creates a dummy `_EnsembleSpec` instance.\n\n  Args:\n    name: _EnsembleSpec\'s name.\n    random_seed: A scalar random seed.\n    num_subnetworks: The number of fake subnetworks in this ensemble.\n    bias: Bias value.\n    loss: Float loss to return. When None, it\'s picked from a random\n      distribution.\n    adanet_loss: Float AdaNet loss to return. When None, it\'s picked from a\n      random distribution.\n    eval_metrics: Optional eval metrics tuple of (metric_fn, tensor args).\n    variables: List of `tf.Variable` instances associated with the ensemble.\n    dict_predictions: Boolean whether to return predictions as a dictionary of\n      `Tensor` or just a single float `Tensor`.\n    export_output_key: An `ExportOutputKeys` for faking export outputs.\n    subnetwork_builders: List of `adanet.subnetwork.Builder` objects.\n    train_op: A train op.\n\n  Returns:\n    A dummy `_EnsembleSpec` instance.\n  """"""\n\n  if loss is None:\n    loss = dummy_tensor([], random_seed)\n\n  if adanet_loss is None:\n    adanet_loss = dummy_tensor([], random_seed * 2)\n  else:\n    adanet_loss = tf.convert_to_tensor(value=adanet_loss)\n\n  logits = dummy_tensor([], random_seed * 3)\n  if dict_predictions:\n    predictions = {\n        ""logits"": logits,\n        ""classes"": tf.cast(tf.abs(logits), dtype=tf.int64)\n    }\n  else:\n    predictions = logits\n  weighted_subnetworks = [\n      ensemble_lib.WeightedSubnetwork(\n          name=name,\n          iteration_number=1,\n          logits=dummy_tensor([2, 1], random_seed * 4),\n          weight=dummy_tensor([2, 1], random_seed * 4),\n          subnetwork=subnetwork_lib.Subnetwork(\n              last_layer=dummy_tensor([1, 2], random_seed * 4),\n              logits=dummy_tensor([2, 1], random_seed * 4),\n              complexity=1.,\n              persisted_tensors={}))\n  ]\n\n  export_outputs = _dummy_export_outputs(export_output_key, logits, predictions)\n  bias = tf.constant(bias)\n  return _EnsembleSpec(\n      name=name,\n      ensemble=ensemble_lib.ComplexityRegularized(\n          weighted_subnetworks=weighted_subnetworks * num_subnetworks,\n          bias=bias,\n          logits=logits,\n      ),\n      architecture=_Architecture(""dummy_ensemble_candidate"", ""dummy_ensembler""),\n      subnetwork_builders=subnetwork_builders,\n      predictions=predictions,\n      step=tf.Variable(0),\n      variables=variables,\n      loss=loss,\n      adanet_loss=adanet_loss,\n      train_op=train_op,\n      eval_metrics=eval_metrics,\n      export_outputs=export_outputs)\n\n\ndef _dummy_export_outputs(export_output_key, logits, predictions):\n  """"""Returns a dummy export output dictionary for the given key.""""""\n\n  export_outputs = None\n  if export_output_key == ExportOutputKeys.CLASSIFICATION_CLASSES:\n    export_outputs = {\n        export_output_key:\n            tf.estimator.export.ClassificationOutput(\n                classes=tf.as_string(logits))\n    }\n  elif export_output_key == ExportOutputKeys.CLASSIFICATION_SCORES:\n    export_outputs = {\n        export_output_key:\n            tf.estimator.export.ClassificationOutput(scores=logits)\n    }\n  elif export_output_key == ExportOutputKeys.REGRESSION:\n    export_outputs = {\n        export_output_key: tf.estimator.export.RegressionOutput(value=logits)\n    }\n  elif export_output_key == ExportOutputKeys.PREDICTION:\n    export_outputs = {\n        export_output_key:\n            tf.estimator.export.PredictOutput(outputs=predictions)\n    }\n  elif export_output_key == ExportOutputKeys.INVALID:\n    export_outputs = {export_output_key: predictions}\n  return export_outputs\n\n\ndef dummy_estimator_spec(loss=None, random_seed=42, eval_metric_ops=None):\n  """"""Creates a dummy `EstimatorSpec` instance.\n\n  Args:\n    loss: Float loss to return. When None, it\'s picked from a random\n      distribution.\n    random_seed: Scalar seed for random number generators.\n    eval_metric_ops: Optional dictionary of metric ops.\n\n  Returns:\n    A `EstimatorSpec` instance.\n  """"""\n\n  if loss is None:\n    loss = dummy_tensor([], random_seed)\n  predictions = dummy_tensor([], random_seed * 2)\n  return tf.estimator.EstimatorSpec(\n      mode=tf.estimator.ModeKeys.TRAIN,\n      predictions=predictions,\n      loss=loss,\n      # Train_op cannot be tf.no_op() for Estimator, because in eager mode\n      # tf.no_op() returns None.\n      train_op=tf.constant(0.),\n      eval_metric_ops=eval_metric_ops)\n\n\ndef dummy_input_fn(features, labels):\n  """"""Returns an input_fn that returns feature and labels `Tensors`.""""""\n\n  def _input_fn(params=None):\n    del params  # Unused.\n\n    input_features = {""x"": tf.constant(features, name=""x"")}\n    input_labels = tf.constant(labels, name=""y"")\n    return input_features, input_labels\n\n  return _input_fn\n\n\ndef dataset_input_fn(features=8., labels=9., return_dataset=False):\n  """"""Returns feature and label `Tensors` via a `Dataset`.""""""\n\n  labels = labels or 0.\n\n  def _input_fn(params=None):\n    """"""The `Dataset` input_fn which will be returned.""""""\n\n    del params  # Unused.\n\n    def _map(f, l):\n      return {""x"": f}, l\n\n    input_features = tf.data.Dataset.from_tensors([features])\n    input_labels = tf.data.Dataset.from_tensors([labels])\n    dataset = tf.data.Dataset.zip((input_features, input_labels)).map(_map)\n    if return_dataset:\n      return dataset\n    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n\n  return _input_fn\n\n\ndef head():\n  from tensorflow_estimator.python.estimator.head import regression_head  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n  return regression_head.RegressionHead(\n      loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE)\n\n\nclass ModifierSessionRunHook(tf_compat.SessionRunHook):\n  """"""Modifies the graph by adding a variable.""""""\n\n  def __init__(self, var_name=""hook_created_variable""):\n    self._var_name = var_name\n    self._begun = False\n\n  def begin(self):\n    """"""Adds a variable to the graph.\n\n    Raises:\n      ValueError: If we\'ve already begun a run.\n    """"""\n\n    if self._begun:\n      raise ValueError(""begin called twice without end."")\n    self._begun = True\n    _ = tf_compat.v1.get_variable(name=self._var_name, initializer="""")\n\n  def end(self, session):\n    """"""Adds a variable to the graph.\n\n    Args:\n      session: A `tf.Session` object that can be used to run ops.\n\n    Raises:\n      ValueError: If we\'ve not begun a run.\n    """"""\n\n    _ = session\n    if not self._begun:\n      raise ValueError(""end called without begin."")\n    self._begun = False\n\n\nclass AdanetTestCase(parameterized.TestCase, tf.test.TestCase):\n  """"""A parameterized `TestCase` that manages a test subdirectory.""""""\n\n  def setUp(self):\n    super(AdanetTestCase, self).setUp()\n    # Setup and cleanup test directory.\n    # Flags are not automatically parsed at this point.\n    flags.FLAGS(sys.argv)\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(AdanetTestCase, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n\ndef summary_simple_value(summary_value):\n  """"""Returns the scalar parsed from the summary proto tensor_value bytes.""""""\n\n  return struct.unpack(""<f"", summary_value.tensor.tensor_content)[0]\n\n\ndef check_eventfile_for_keyword(keyword, dir_):\n  """"""Checks event files for the keyword.""""""\n\n  tf_compat.v1.summary.FileWriterCache.clear()\n\n  if not tf.io.gfile.exists(dir_):\n    raise ValueError(""Directory \'{}\' not found."".format(dir_))\n\n  # Get last `Event` written.\n  filenames = os.path.join(dir_, ""events*"")\n  event_paths = tf.io.gfile.glob(filenames)\n  if not event_paths:\n    raise ValueError(""Path \'{}\' not found."".format(filenames))\n\n  for event_path in event_paths:\n    for last_event in tf_compat.v1.train.summary_iterator(event_path):\n      if last_event.summary is not None:\n        for value in last_event.summary.value:\n          if keyword == value.tag:\n            if value.HasField(""simple_value""):\n              return value.simple_value\n            if value.HasField(""image""):\n              return (value.image.height, value.image.width,\n                      value.image.colorspace)\n            if value.HasField(""tensor""):\n              if value.metadata.plugin_data.plugin_name == ""scalars"":\n                return summary_simple_value(value)\n              if value.metadata.plugin_data.plugin_name == ""images"":\n                return (int(value.tensor.string_val[0]),\n                        int(value.tensor.string_val[1]), 1)\n              if value.tensor.string_val is not None:\n                return value.tensor.string_val\n\n  raise ValueError(""Keyword \'{}\' not found in path \'{}\'."".format(\n      keyword, filenames))\n\n\ndef create_ensemble_metrics(metric_fn,\n                            use_tpu=False,\n                            features=None,\n                            labels=None,\n                            estimator_spec=None,\n                            architecture=None):\n  """"""Creates an instance of the _EnsembleMetrics class.\n\n  Args:\n    metric_fn: A function which should obey the following signature:\n    - Args: can only have following three arguments in any order:\n        * predictions: Predictions `Tensor` or dict of `Tensor` created by given\n          `Head`.\n        * features: Input `dict` of `Tensor` objects created by `input_fn` which\n          is given to `estimator.evaluate` as an argument.\n        * labels:  Labels `Tensor` or dict of `Tensor` (for multi-head) created\n          by `input_fn` which is given to `estimator.evaluate` as an argument.\n      - Returns: Dict of metric results keyed by name. Final metrics are a union\n        of this and `estimator`s existing metrics. If there is a name conflict\n        between this and `estimator`s existing metrics, this will override the\n        existing one. The values of the dict are the results of calling a metric\n        function, namely a `(metric_tensor, update_op)` tuple.\n    use_tpu: Whether to use TPU-specific variable sharing logic.\n    features: Input `dict` of `Tensor` objects.\n    labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n      (for multi-head).\n    estimator_spec: The `EstimatorSpec` created by a `Head` instance.\n    architecture: `_Architecture` object.\n\n  Returns:\n    An instance of _EnsembleMetrics.\n  """"""\n\n  if not estimator_spec:\n    estimator_spec = tf_compat.v1.estimator.tpu.TPUEstimatorSpec(\n        mode=tf.estimator.ModeKeys.EVAL,\n        loss=tf.constant(2.),\n        predictions=None,\n        eval_metrics=None)\n    if not use_tpu:\n      estimator_spec = estimator_spec.as_estimator_spec()\n\n  if not architecture:\n    architecture = _Architecture(None, None)\n\n  metrics = _EnsembleMetrics(use_tpu=use_tpu)\n  metrics.create_eval_metrics(features, labels, estimator_spec, metric_fn,\n                              architecture)\n\n  return metrics\n\n\ndef create_subnetwork_metrics(metric_fn,\n                              use_tpu=False,\n                              features=None,\n                              labels=None,\n                              estimator_spec=None):\n  """"""Creates an instance of the _SubnetworkMetrics class.\n\n  Args:\n    metric_fn: A function which should obey the following signature:\n    - Args: can only have following three arguments in any order:\n        * predictions: Predictions `Tensor` or dict of `Tensor` created by given\n          `Head`.\n        * features: Input `dict` of `Tensor` objects created by `input_fn` which\n          is given to `estimator.evaluate` as an argument.\n        * labels:  Labels `Tensor` or dict of `Tensor` (for multi-head) created\n          by `input_fn` which is given to `estimator.evaluate` as an argument.\n      - Returns: Dict of metric results keyed by name. Final metrics are a union\n        of this and `estimator`s existing metrics. If there is a name conflict\n        between this and `estimator`s existing metrics, this will override the\n        existing one. The values of the dict are the results of calling a metric\n        function, namely a `(metric_tensor, update_op)` tuple.\n    use_tpu: Whether to use TPU-specific variable sharing logic.\n    features: Input `dict` of `Tensor` objects.\n    labels: Labels `Tensor` or a dictionary of string label name to `Tensor`\n      (for multi-head).\n    estimator_spec: The `EstimatorSpec` created by a `Head` instance.\n\n  Returns:\n    An instance of _SubnetworkMetrics.\n  """"""\n\n  if not estimator_spec:\n    estimator_spec = tf_compat.v1.estimator.tpu.TPUEstimatorSpec(\n        mode=tf.estimator.ModeKeys.EVAL,\n        loss=tf.constant(2.),\n        predictions=None,\n        eval_metrics=None)\n    if not use_tpu:\n      estimator_spec = estimator_spec.as_estimator_spec()\n\n  metrics = _SubnetworkMetrics(use_tpu=use_tpu)\n  metrics.create_eval_metrics(features, labels, estimator_spec, metric_fn)\n\n  return metrics\n\n\ndef create_iteration_metrics(subnetwork_metrics=None,\n                             ensemble_metrics=None,\n                             use_tpu=False,\n                             iteration_number=1):\n  """"""Creates an instance of the _IterationMetrics class.\n\n  Args:\n    subnetwork_metrics: List of _SubnetworkMetrics objects.\n    ensemble_metrics: List of _EnsembleMetrics objects.\n    use_tpu: Whether to use TPU-specific variable sharing logic.\n    iteration_number: What number iteration these metrics are for.\n\n  Returns:\n    An instance of _IterationMetrics that has been populated with the\n    input metrics.\n  """"""\n  subnetwork_metrics = subnetwork_metrics or []\n  ensemble_metrics = ensemble_metrics or []\n\n  candidates = []\n  for i, metric in enumerate(ensemble_metrics):\n    spec = _EnsembleSpec(\n        name=""ensemble_{}"".format(i),\n        ensemble=None,\n        architecture=None,\n        subnetwork_builders=None,\n        predictions=None,\n        step=None,\n        variables=None,\n        eval_metrics=metric)\n\n    candidate = _Candidate(\n        ensemble_spec=spec, adanet_loss=tf.constant(i), variables=None)\n    candidates.append(candidate)\n\n  subnetwork_specs = []\n  for i, metric in enumerate(subnetwork_metrics):\n    spec = _SubnetworkSpec(\n        name=""subnetwork_{}"".format(i),\n        subnetwork=None,\n        builder=None,\n        predictions=None,\n        step=None,\n        loss=None,\n        train_op=None,\n        asset_dir=None,\n        eval_metrics=metric,\n        variables=None)\n    subnetwork_specs.append(spec)\n\n  return _IterationMetrics(\n      iteration_number,\n      candidates,\n      subnetwork_specs=subnetwork_specs,\n      use_tpu=use_tpu)\n'"
adanet/core/timer.py,0,"b'""""""A simple timer implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\n\nclass _CountDownTimer(object):\n  """"""A simple count down timer implementation.""""""\n\n  def __init__(self, duration_secs):\n    """"""Initializes a `_CountDownTimer`.\n\n    Args:\n      duration_secs: Float seconds for countdown.\n\n    Returns:\n      A `_CountDownTimer` instance.\n    """"""\n\n    self._start_time_secs = time.time()\n    self._duration_secs = duration_secs\n\n  def secs_remaining(self):\n    """"""Returns the remaining countdown seconds.""""""\n\n    diff = self._duration_secs - (time.time() - self._start_time_secs)\n    return max(0., diff)\n'"
adanet/core/timer_test.py,2,"b'""""""Tests for timer.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nfrom adanet.core.timer import _CountDownTimer\nimport tensorflow.compat.v1 as tf\n\n\nclass CountDownTimerTest(tf.test.TestCase):\n\n  def test_secs_remaining_long(self):\n    timer = _CountDownTimer(60)\n    time.sleep(.1)\n    secs_remaining = timer.secs_remaining()\n    self.assertLess(0., secs_remaining)\n    self.assertGreater(60., secs_remaining)\n\n  def test_secs_remaining_short(self):\n    timer = _CountDownTimer(.001)\n    time.sleep(.1)\n    secs_remaining = timer.secs_remaining()\n    self.assertEqual(0., secs_remaining)\n\n  def test_secs_remaining_zero(self):\n    timer = _CountDownTimer(0.)\n    time.sleep(.01)\n    secs_remaining = timer.secs_remaining()\n    self.assertEqual(0., secs_remaining)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/core/tpu_estimator.py,17,"b'""""""An AdaNet estimator implementation which can run on TPU.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport functools\n\nfrom absl import logging\nfrom adanet import tf_compat\nfrom adanet.core.estimator import Estimator\nimport tensorflow.compat.v2 as tf\n\n\n# pylint: disable=g-classes-have-attributes\nclass TPUEstimator(Estimator, tf.compat.v1.estimator.tpu.TPUEstimator):\n  """"""An :class:`adanet.Estimator` capable of training and evaluating on TPU.\n\n  Unless :code:`use_tpu=False`, training will run on TPU. However, certain parts\n  of the AdaNet training loop, such as report materialization and best candidate\n  selection, will still occurr on CPU. Furthermore, if using TPUEmbedding (i.e.\n  :code:`embedding_config_spec` is supplied), inference will also occurr on CPU.\n\n  TODO: Provide the missing functionality detailed below.\n  N.B: Embeddings using the TPUEmbedding (i.e. :code:`embedding_config_spec`\n  is provided) only support :code:`shared_embedding_columns` when running for\n  multiple AdaNet iterations. Using regular :code:`embedding_columns` will cause\n  iterations 2..n to fail because of mismatched embedding scopes.\n\n  Args:\n    head: See :class:`adanet.Estimator`.\n    subnetwork_generator: See :class:`adanet.Estimator`.\n    max_iteration_steps: See :class:`adanet.Estimator`.\n    ensemblers: See :class:`adanet.Estimator`.\n    ensemble_strategies: See :class:`adanet.Estimator`.\n    evaluator: See :class:`adanet.Estimator`.\n    report_materializer: See :class:`adanet.Estimator`.\n    metric_fn: See :class:`adanet.Estimator`.\n    force_grow: See :class:`adanet.Estimator`.\n    replicate_ensemble_in_training: See :class:`adanet.Estimator`.\n    adanet_loss_decay: See :class:`adanet.Estimator`.\n    report_dir: See :class:`adanet.Estimator`.\n    config: See :class:`adanet.Estimator`.\n    use_tpu: Boolean to enable training on TPU. Defaults to :code:`True` and is\n      only provided to allow debugging models on CPU/GPU. Use\n      :class:`adanet.Estimator` instead if you do not plan to run on TPU.\n    eval_on_tpu: Boolean to enable evaluating on TPU. Defaults to :code:`True`.\n      Ignored if :code:`use_tpu=False`.\n    export_to_tpu: See :class:`tf.compat.v1.estimator.tpu.TPUEstimator`.\n    train_batch_size: See :class:`tf.compat.v1.estimator.tpu.TPUEstimator`.\n      Defaults to 0 if `None`.\n    eval_batch_size: See :class:`tf.compat.v1.estimator.tpu.TPUEstimator`.\n      Defaults to train_batch_size if `None`.\n    predict_batch_size: See :class:`tf.compat.v1.estimator.tpu.TPUEstimator`.\n      Defaults to eval_batch_size if `None`.\n    embedding_config_spec: See :class:`tf.compat.v1.estimator.tpu.TPUEstimator`.\n      If supplied, :code:`predict` will be called on CPU and no TPU compatible\n        :code:`SavedModel` will be exported.\n    debug: See :class:`adanet.Estimator`.\n    enable_ensemble_summaries: See :class:`adanet.Estimator`.\n    enable_subnetwork_summaries: See :class:`adanet.Estimator`.\n    export_subnetwork_logits: Whether to include subnetwork logits in exports.\n    export_subnetwork_last_layer: Whether to include subnetwork last layer in\n      exports.\n    global_step_combiner_fn: See :class:`adanet.Estimator`.\n    max_iterations: See :class:`adanet.Estimator`.\n    replay_config: See :class:`adanet.Estimator`.\n    **kwargs: Extra keyword args passed to the parent.\n  """"""\n\n  def __init__(self,\n               head,\n               subnetwork_generator,\n               max_iteration_steps,\n               ensemblers=None,\n               ensemble_strategies=None,\n               evaluator=None,\n               report_materializer=None,\n               metric_fn=None,\n               force_grow=False,\n               replicate_ensemble_in_training=False,\n               adanet_loss_decay=.9,\n               model_dir=None,\n               report_dir=None,\n               config=None,\n               use_tpu=True,\n               eval_on_tpu=True,\n               export_to_tpu=True,\n               train_batch_size=None,\n               eval_batch_size=None,\n               predict_batch_size=None,\n               embedding_config_spec=None,\n               debug=False,\n               enable_ensemble_summaries=True,\n               enable_subnetwork_summaries=True,\n               export_subnetwork_logits=False,\n               export_subnetwork_last_layer=True,\n               global_step_combiner_fn=tf.math.reduce_mean,\n               max_iterations=None,\n               replay_config=None,\n               **kwargs):\n    self._use_tpu = use_tpu\n    if not self._use_tpu:\n      logging.warning(\n          ""This adanet.TPUEstimator is meant to be used for running on TPU. ""\n          ""If you want to run on CPU/GPU, use adanet.Estimator instead."")\n    # TPUEstimator modifies config under the hood. We keep track of it here so\n    # we can use it from _create_temp_run_config.\n    self._original_config = config or tf_compat.v1.estimator.tpu.RunConfig()\n    self._eval_on_tpu = eval_on_tpu if self._use_tpu else False\n    self._export_to_tpu = export_to_tpu\n    self._train_batch_size = train_batch_size or 0\n    self._eval_batch_size = eval_batch_size or train_batch_size or 0\n    self._predict_batch_size = (\n        predict_batch_size or eval_batch_size or train_batch_size or 0)\n    self._embedding_config_spec = embedding_config_spec\n    if self._embedding_config_spec:\n      logging.warning(\n          ""TPU does not support inference with TPUEmbedding. Force setting ""\n          ""`export_to_tpu=False` so no TPU SavedModel will be exported."")\n      self._export_to_tpu = False\n\n    from tensorflow_estimator.python.estimator.tpu import tpu_estimator  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    super(TPUEstimator, self).__init__(\n        head=head,\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=max_iteration_steps,\n        ensemblers=ensemblers,\n        ensemble_strategies=ensemble_strategies,\n        evaluator=evaluator,\n        report_materializer=report_materializer,\n        metric_fn=metric_fn,\n        force_grow=force_grow,\n        replicate_ensemble_in_training=replicate_ensemble_in_training,\n        adanet_loss_decay=adanet_loss_decay,\n        model_dir=model_dir,\n        report_dir=report_dir,\n        config=self._original_config,\n        use_tpu=self._use_tpu,\n        eval_on_tpu=self._eval_on_tpu,\n        export_to_tpu=self._export_to_tpu,\n        export_saved_model_api_version=(\n            tpu_estimator.ExportSavedModelApiVersion.V2),\n        train_batch_size=self._train_batch_size,\n        eval_batch_size=self._eval_batch_size,\n        predict_batch_size=self._predict_batch_size,\n        embedding_config_spec=self._embedding_config_spec,\n        debug=debug,\n        enable_ensemble_summaries=enable_ensemble_summaries,\n        enable_subnetwork_summaries=enable_subnetwork_summaries,\n        export_subnetwork_logits=export_subnetwork_logits,\n        export_subnetwork_last_layer=export_subnetwork_last_layer,\n        global_step_combiner_fn=global_step_combiner_fn,\n        max_iterations=max_iterations,\n        replay_config=replay_config,\n        **kwargs)\n\n  def predict(self,\n              input_fn,\n              predict_keys=None,\n              hooks=None,\n              checkpoint_path=None,\n              yield_single_examples=True):\n\n    use_tpu = self._use_tpu\n    eval_on_tpu = self._eval_on_tpu\n    if self._embedding_config_spec:\n      logging.warning(""TPU does not support inference with TPUEmbedding. ""\n                      ""Falling back to CPU."")\n      use_tpu = False\n      eval_on_tpu = False\n    if not checkpoint_path:\n      checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n    logging.info(""Computing predictions for AdaNet model at checkpoint: %s"",\n                 checkpoint_path)\n    params = self.params\n    params.update({\n        ""best_ensemble_index"":\n            self._compute_best_ensemble_index(checkpoint_path, hooks),\n        ""checkpoint_path"":\n            checkpoint_path,\n    })\n    from tensorflow_estimator.python.estimator.tpu import tpu_estimator  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    # TODO: Consider extracting a common function to use here and in\n    # _create_temp_estimator().\n    estimator = tf_compat.v1.estimator.tpu.TPUEstimator(\n        model_fn=self._create_model_fn(hooks=hooks, is_export=False),\n        params=params,\n        config=self._original_config,\n        model_dir=self.model_dir,\n        use_tpu=use_tpu,\n        eval_on_tpu=eval_on_tpu,\n        export_to_tpu=self._export_to_tpu,\n        export_saved_model_api_version=(\n            tpu_estimator.ExportSavedModelApiVersion.V2),\n        train_batch_size=self._train_batch_size,\n        eval_batch_size=self._eval_batch_size,\n        predict_batch_size=self._predict_batch_size,\n        embedding_config_spec=self._embedding_config_spec)\n    return estimator.predict(\n        input_fn,\n        predict_keys=predict_keys,\n        hooks=hooks,\n        checkpoint_path=checkpoint_path,\n        yield_single_examples=yield_single_examples)\n\n  def _create_temp_run_config(self, temp_model_dir):\n    """"""See the `Estimator` base class for details.""""""\n\n    return tf_compat.v1.estimator.tpu.RunConfig(\n        model_dir=temp_model_dir,\n        tpu_config=self._original_config.tpu_config,\n        evaluation_master=self._original_config.evaluation_master,\n        master=self._original_config.master,\n        cluster=self._original_config.cluster,\n        tf_random_seed=self._original_config.tf_random_seed,\n        session_config=self._original_config.session_config,\n        protocol=self._original_config.protocol)\n\n  def _create_temp_estimator(self, config, **create_model_fn_args):\n    """"""See the `Estimator` base class for details.""""""\n\n    from tensorflow_estimator.python.estimator.tpu import tpu_estimator  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    temp_model_dir = config.model_dir\n    return tf_compat.v1.estimator.tpu.TPUEstimator(\n        model_fn=self._create_model_fn(**create_model_fn_args),\n        config=config,\n        model_dir=temp_model_dir,\n        use_tpu=self._use_tpu,\n        eval_on_tpu=self._eval_on_tpu,\n        export_to_tpu=self._export_to_tpu,\n        export_saved_model_api_version=(\n            tpu_estimator.ExportSavedModelApiVersion.V2),\n        train_batch_size=self._train_batch_size,\n        eval_batch_size=self._eval_batch_size,\n        predict_batch_size=self._predict_batch_size,\n        embedding_config_spec=self._embedding_config_spec)\n\n  @contextlib.contextmanager\n  def _call_input_fn_in_new_graph(self, input_fn, mode, config):\n    """"""See the `Estimator` base class for details.""""""\n\n    # Bind parameters to input_fn since the parent\'s input_fn is not expected to\n    # have any arguments.\n    from tensorflow.python.util import function_utils  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n    input_fn_args = function_utils.fn_args(input_fn)\n    kwargs = {}\n    if ""mode"" in input_fn_args:\n      kwargs[""mode""] = mode\n    if ""params"" in input_fn_args:\n      kwargs[""params""] = self.params\n    if ""config"" in input_fn_args:\n      kwargs[""config""] = config\n    input_fn = functools.partial(input_fn, **kwargs)\n    with super(TPUEstimator,\n               self)._call_input_fn_in_new_graph(input_fn, mode, config) as res:\n      yield res\n\n  def _create_estimator_spec(self, current_iteration, mode,\n                             iteration_number_tensor, previous_iteration_vars,\n                             is_growing_phase, evaluation_name):\n    """"""See the `Estimator` base class for details.""""""\n\n    if not self._use_tpu:\n      return super(TPUEstimator, self)._create_estimator_spec(\n          current_iteration, mode, iteration_number_tensor,\n          previous_iteration_vars, is_growing_phase, evaluation_name)\n\n    training = mode == tf.estimator.ModeKeys.TRAIN\n    iteration_estimator_spec = current_iteration.estimator_spec\n    training_hooks = self._training_hooks(current_iteration, training,\n                                          iteration_number_tensor,\n                                          previous_iteration_vars,\n                                          is_growing_phase)\n    if is_growing_phase:\n      training_hooks = self._process_hooks_for_growing_phase(training_hooks)\n    evaluation_hooks = self._evaluation_hooks(current_iteration, training,\n                                              evaluation_name)\n    return tf_compat.v1.estimator.tpu.TPUEstimatorSpec(\n        mode=mode,\n        predictions=iteration_estimator_spec.predictions,\n        loss=iteration_estimator_spec.loss,\n        train_op=self._train_op(iteration_estimator_spec, is_growing_phase),\n        host_call=self._create_host_call(current_iteration, training),\n        eval_metrics=iteration_estimator_spec.eval_metrics,\n        export_outputs=iteration_estimator_spec.export_outputs,\n        # Return a constant summary_op, otherwise `Estimator` creates summary\n        # ops that do not work on TPU.\n        scaffold_fn=lambda: tf.compat.v1.train.Scaffold(  # pylint: disable=g-long-lambda\n            summary_op=tf.constant("""")),\n        training_hooks=training_hooks,\n        evaluation_hooks=evaluation_hooks)\n\n  def _training_hooks(self, current_iteration, training,\n                      iteration_number_tensor, previous_iteration_vars,\n                      is_growing_phase):\n    """"""See the `Estimator` base class for details.""""""\n\n    training_hooks = super(TPUEstimator,\n                           self)._training_hooks(current_iteration, training,\n                                                 iteration_number_tensor,\n                                                 previous_iteration_vars,\n                                                 is_growing_phase)\n    if self._use_tpu:\n      # Remove summary hooks on TPU since summaries are saved via host_call.\n      training_hooks = [\n          hook for hook in training_hooks\n          if not isinstance(hook, tf.compat.v1.train.SummarySaverHook)\n      ]\n\n    return training_hooks\n\n  def _create_host_call(self, current_iteration, training):\n    """"""Construct a host_call writing scalar summaries.\n\n    Args:\n      current_iteration: The current `_Iteration`.\n      training: Boolean indicating whether in training mode.\n\n    Returns:\n      (fn, args) Pair to be called by TPUEstimator as the host_call.\n    """"""\n\n    if not training:\n      return lambda **kwargs: [tf.no_op()], {}\n\n    # Collect and flatten summary functions and arguments.\n    summary_kwargs = collections.OrderedDict()\n    gs_t = tf.reshape(tf.cast(tf.train.get_global_step(), dtype=tf.int32), [1])\n    summary_kwargs[""global_step""] = gs_t\n\n    summary_fns = collections.defaultdict(list)\n    for i, summary in enumerate(current_iteration.summaries):\n      for j, (summary_fn, tensor) in enumerate(summary.summary_tuples()):\n        summary_fns[i].append(summary_fn)\n        summary_kwargs[""summary_{}_{}"".format(i, j)] = tensor\n\n    def _host_call_fn(**kwargs):\n      """"""Training host call.\n\n      Creates summaries for training metrics.\n\n      Args:\n        **kwargs: Dict of {str: Tensor} , with `Tensor` of shape `[batch]`. Must\n          contain key ""global_step"" with value of current global_step Tensor.\n\n      Returns:\n        List of summary ops to run on the CPU host.\n      """"""\n\n      from tensorflow.python.ops import summary_ops_v2  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n      gs = tf.cast(kwargs.pop(""global_step"")[0], dtype=tf.int64)\n      for i, summary in enumerate(current_iteration.summaries):\n        with summary_ops_v2.create_file_writer(summary.logdir).as_default():\n          with summary_ops_v2.record_summaries_every_n_global_steps(\n              n=self.config.save_summary_steps, global_step=gs):\n            for j, summary_fn in enumerate(summary_fns[i]):\n              tensor = kwargs[""summary_{}_{}"".format(i, j)]\n              summary_fn(tensor, step=gs)\n        summary.clear_summary_tuples()\n      return tf.compat.v1.summary.all_v2_summary_ops()\n\n    return _host_call_fn, summary_kwargs\n\n  def _create_model_fn(self,\n                       is_growing_phase=False,\n                       is_inside_training_loop=False,\n                       is_export=False,\n                       evaluation_name=None,\n                       best_ensemble_index=None,\n                       checkpoint_path=None,\n                       hooks=None):\n    """"""See the `Estimator` base class for details.""""""\n\n    from tensorflow_estimator.python.estimator.tpu import tpu_estimator  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n    adanet_model_fn = super(TPUEstimator, self)._create_model_fn(\n        is_growing_phase, is_inside_training_loop, is_export, evaluation_name,\n        best_ensemble_index, checkpoint_path, hooks)\n\n    def _model_fn(features, labels, mode, params, config):\n      """"""The model_fn to return which supports exporting on TPU.""""""\n\n      if (is_export and params[""use_tpu""] and\n          mode == tf.estimator.ModeKeys.PREDICT):\n        batch_config = tpu_estimator.BatchConfig(\n            # Set num_batch_threads to the number of TPU cores on Servomatic.\n            num_batch_threads=2,\n            max_batch_size=self._predict_batch_size,\n            # TODO: Magic number. Investigate whether there is a better\n            # way to set this, or have the user pass it in.\n            batch_timeout_micros=60 * 1000,\n            allowed_batch_sizes=[self._predict_batch_size])\n        return tpu_estimator.model_fn_inference_on_tpu(\n            adanet_model_fn,\n            features=features,\n            labels=labels,\n            config=config,\n            params=params,\n            batch_config=batch_config)\n\n      return adanet_model_fn(features, labels, mode, params, config)\n\n    return _model_fn\n'"
adanet/core/tpu_estimator_test.py,27,"b'""""""Tests AdaNet TPU estimator.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport json\nimport os\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.core import testing_utils as tu\nfrom adanet.core.tpu_estimator import TPUEstimator\nfrom adanet.subnetwork import Builder\nfrom adanet.subnetwork import Report\nfrom adanet.subnetwork import SimpleGenerator\nfrom adanet.subnetwork import Subnetwork\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\n# pylint: disable=g-import-not-at-top\nfrom tensorflow_estimator.python.estimator.head import regression_head\ntry:\n  from tensorflow_estimator.contrib.estimator.python.estimator import head as head_lib\nexcept (AttributeError, ImportError):\n  head_lib = None\n# pylint: enable=g-direct-tensorflow-import\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass _DNNBuilder(Builder):\n  """"""A simple DNN subnetwork builder.""""""\n\n  def __init__(self,\n               name,\n               feature_columns=None,\n               learning_rate=.01,\n               layer_size=16,\n               seed=13,\n               use_tpu=False):\n    self._name = name\n    self._feature_columns = feature_columns\n    self._learning_rate = learning_rate\n    self._layer_size = layer_size\n    self._seed = seed\n    self._use_tpu = use_tpu\n\n  @property\n  def name(self):\n    return self._name\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    seed = self._seed\n    if previous_ensemble:\n      # Increment seed so different iterations don\'t learn the exact same thing.\n      seed += 1\n    with tf.compat.v1.variable_scope(""dnn""):\n      persisted_tensors = {}\n      with tf.compat.v1.variable_scope(""hidden_layer""):\n        if self._feature_columns:\n          input_layer = tf.compat.v1.feature_column.input_layer(\n              features=features, feature_columns=self._feature_columns)\n        else:\n          input_layer = features[""x""]\n        w = tf.compat.v1.get_variable(\n            shape=[input_layer.shape[1], self._layer_size],\n            initializer=tf.compat.v1.glorot_uniform_initializer(seed=seed),\n            name=""weight"")\n        hidden_layer = tf.matmul(input_layer, w)\n\n      if previous_ensemble:\n        other_hidden_layer = previous_ensemble.weighted_subnetworks[\n            -1].subnetwork.persisted_tensors[""hidden_layer""]\n        hidden_layer = tf.concat([hidden_layer, other_hidden_layer], axis=1)\n\n      # Use a leaky-relu activation so that gradients can flow even when\n      # outputs are negative. Leaky relu has a non-zero slope when x < 0.\n      # Otherwise success at learning is completely dependent on random seed.\n      hidden_layer = tf.nn.leaky_relu(hidden_layer, alpha=.2)\n      persisted_tensors[""hidden_layer""] = hidden_layer\n      if training:\n        # This change will only be in the next iteration if\n        # `freeze_training_graph` is `True`.\n        persisted_tensors[""hidden_layer""] = 2 * hidden_layer\n\n    with tf.compat.v1.variable_scope(""logits""):\n      logits = tf.compat.v1.layers.dense(\n          hidden_layer,\n          logits_dimension,\n          kernel_initializer=tf.compat.v1.glorot_uniform_initializer(seed=seed))\n\n    summary.scalar(""scalar"", 3)\n    summary.image(""image"", tf.ones([1, 3, 3, 1]))\n    with tf.compat.v1.variable_scope(""nested""):\n      summary.scalar(""scalar"", 5)\n\n    return Subnetwork(\n        last_layer=logits,\n        logits=logits,\n        complexity=3,\n        persisted_tensors=persisted_tensors)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(\n        learning_rate=self._learning_rate)\n    if self._use_tpu:\n      optimizer = tf.compat.v1.tpu.CrossShardOptimizer(optimizer)\n    return optimizer.minimize(loss, var_list=var_list)\n\n  def build_subnetwork_report(self):\n    return Report(\n        hparams={""layer_size"": self._layer_size},\n        attributes={""complexity"": tf.constant(3, dtype=tf.int32)},\n        metrics={\n            ""moo"": (tf.constant(3,\n                                dtype=tf.int32), tf.constant(3, dtype=tf.int32))\n        })\n\n\nclass _NanLossBuilder(Builder):\n  """"""A subnetwork builder always produces a NaN loss.""""""\n\n  @property\n  def name(self):\n    return ""nan""\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    logits = tf_compat.v1.layers.dense(\n        features[""x""],\n        logits_dimension,\n        kernel_initializer=tf_compat.v1.glorot_uniform_initializer(\n            seed=42)) * np.nan\n    return Subnetwork(last_layer=logits, logits=logits, complexity=0)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    return tf.no_op()\n\n\ndef make_regression_head(use_tpu):\n  if use_tpu and head_lib:\n    # AdaNet TPU currently requires the old head.\n    return head_lib.regression_head(\n        loss_reduction=tf_compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE)\n  # TF 2.0 eliminates tf.contrib.\n  return regression_head.RegressionHead(\n      loss_reduction=tf_compat.SUM_OVER_BATCH_SIZE)\n\n\nclass TPUEstimatorTest(tu.AdanetTestCase):\n\n  def setUp(self):\n    super(TPUEstimatorTest, self).setUp()\n\n    if not tf_compat.version_greater_or_equal(""1.14.0""):\n      self.skipTest(""TPUEmbedding not supported in version 1.13.0 and below."")\n\n    # TPUConfig initializes model_dir from TF_CONFIG and checks that the user\n    # provided model_dir matches the TF_CONFIG one.\n    tf_config = {""model_dir"": self.test_subdirectory}\n    os.environ[""TF_CONFIG""] = json.dumps(tf_config)\n\n  def tearDown(self):\n    super(TPUEstimatorTest, self).tearDown()\n    del os.environ[""TF_CONFIG""]\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""not_use_tpu"",\n          ""use_tpu"":\n              False,\n          ""subnetwork_generator"":\n              SimpleGenerator([_DNNBuilder(""dnn"", use_tpu=False)]),\n          ""want_loss"":\n              0.41315794,\n      },\n  )\n  def test_tpu_estimator_simple_lifecycle(self, use_tpu, subnetwork_generator,\n                                          want_loss):\n    config = tf.compat.v1.estimator.tpu.RunConfig(master="""", tf_random_seed=42)\n    estimator = TPUEstimator(\n        # TODO: Add test with estimator Head v2.\n        head=make_regression_head(use_tpu),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=10,\n        model_dir=self.test_subdirectory,\n        config=config,\n        use_tpu=use_tpu,\n        train_batch_size=64 if use_tpu else 0)\n    max_steps = 30\n\n    xor_features = [[1., 0.], [0., 0], [0., 1.], [1., 1.]]\n    xor_labels = [[1.], [0.], [1.], [0.]]\n    train_input_fn = tu.dummy_input_fn(xor_features, xor_labels)\n\n    # Train.\n    estimator.train(\n        input_fn=train_input_fn, steps=None, max_steps=max_steps, hooks=None)\n\n    # Evaluate.\n    eval_results = estimator.evaluate(\n        input_fn=train_input_fn, steps=1, hooks=None)\n\n    # Predict.\n    predictions = estimator.predict(\n        input_fn=tu.dataset_input_fn(features=[0., 0.], return_dataset=True))\n    # We need to iterate over all the predictions before moving on, otherwise\n    # the TPU will not be shut down.\n    for prediction in predictions:\n      self.assertIsNotNone(prediction[""predictions""])\n\n    # Export SavedModel.\n    def serving_input_fn():\n      """"""Input fn for serving export, starting from serialized example.""""""\n      serialized_example = tf.compat.v1.placeholder(\n          dtype=tf.string, shape=(None), name=""serialized_example"")\n      return tf.estimator.export.ServingInputReceiver(\n          features={""x"": tf.constant([[0., 0.]], name=""serving_x"")},\n          receiver_tensors=serialized_example)\n\n    estimator.export_saved_model(\n        export_dir_base=estimator.model_dir,\n        serving_input_receiver_fn=serving_input_fn)\n\n    self.assertAlmostEqual(want_loss, eval_results[""loss""], places=2)\n    self.assertEqual(max_steps, eval_results[""global_step""])\n    self.assertEqual(2, eval_results[""iteration""])\n\n\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""not_use_tpu"",\n          ""use_tpu"": False,\n          ""want_loss"": 0.55584925,\n          ""want_adanet_loss"": .64416,\n          ""want_eval_summary_loss"": 0.555849,\n          ""want_predictions"": 0.46818,\n      },\n  )\n  def test_tpu_estimator_summaries(self, use_tpu, want_loss, want_adanet_loss,\n                                   want_eval_summary_loss, want_predictions):\n    max_steps = 10\n    config = tf.compat.v1.estimator.tpu.RunConfig(\n        tf_random_seed=42, save_summary_steps=2, log_step_count_steps=max_steps)\n    assert config.log_step_count_steps\n\n    def metric_fn(predictions):\n      return {\n          ""predictions"": tf_compat.v1.metrics.mean(predictions[""predictions""])\n      }\n\n    estimator = TPUEstimator(\n        head=make_regression_head(use_tpu),\n        subnetwork_generator=SimpleGenerator(\n            [_DNNBuilder(""dnn"", use_tpu=use_tpu)]),\n        max_iteration_steps=max_steps,\n        model_dir=self.test_subdirectory,\n        metric_fn=metric_fn,\n        config=config,\n        use_tpu=use_tpu,\n        train_batch_size=64 if use_tpu else 0)\n    xor_features = [[1., 0.], [0., 0], [0., 1.], [1., 1.]]\n    xor_labels = [[1.], [0.], [1.], [0.]]\n    train_input_fn = tu.dummy_input_fn(xor_features, xor_labels)\n\n    estimator.train(input_fn=train_input_fn, max_steps=max_steps)\n    eval_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n    self.assertAlmostEqual(want_loss, eval_results[""loss""], places=2)\n    self.assertEqual(max_steps, eval_results[""global_step""])\n    self.assertEqual(0, eval_results[""iteration""])\n\n    subnetwork_subdir = os.path.join(self.test_subdirectory,\n                                     ""subnetwork/t0_dnn"")\n\n    ensemble_subdir = os.path.join(\n        self.test_subdirectory, ""ensemble/t0_dnn_grow_complexity_regularized"")\n\n    # TODO: Why is the adanet_loss written to \'loss\'?\n    self.assertAlmostEqual(\n        want_adanet_loss,\n        tu.check_eventfile_for_keyword(""loss"", self.test_subdirectory),\n        places=1)\n    self.assertEqual(\n        0.,\n        tu.check_eventfile_for_keyword(""iteration/adanet/iteration"",\n                                       self.test_subdirectory))\n    self.assertAlmostEqual(\n        3.,\n        tu.check_eventfile_for_keyword(""scalar"", subnetwork_subdir),\n        places=3)\n    self.assertEqual(\n        (3, 3, 1),\n        tu.check_eventfile_for_keyword(\n            # When TF 2 behavior is enabled AdaNet uses V2 summaries.\n            ""image"" if tf_compat.is_v2_behavior_enabled() else ""image/image/0"",\n            subnetwork_subdir))\n    self.assertAlmostEqual(\n        5.,\n        tu.check_eventfile_for_keyword(""nested/scalar"", subnetwork_subdir),\n        places=3)\n    self.assertAlmostEqual(\n        want_adanet_loss,\n        tu.check_eventfile_for_keyword(\n            ""adanet_loss/adanet/adanet_weighted_ensemble"", ensemble_subdir),\n        places=1)\n    self.assertAlmostEqual(\n        0.,\n        tu.check_eventfile_for_keyword(\n            ""complexity_regularization/adanet/adanet_weighted_ensemble"",\n            ensemble_subdir),\n        places=1)\n    self.assertAlmostEqual(\n        1.,\n        tu.check_eventfile_for_keyword(\n            ""mixture_weight_norms/adanet/""\n            ""adanet_weighted_ensemble/subnetwork_0"", ensemble_subdir),\n        places=1)\n\n    # Eval metric summaries are always written out during eval.\n    subnetwork_eval_subdir = os.path.join(subnetwork_subdir, ""eval"")\n    self.assertAlmostEqual(\n        want_eval_summary_loss,\n        tu.check_eventfile_for_keyword(""loss"", subnetwork_eval_subdir),\n        places=1)\n    # TODO: Check why some eval metrics are zero on TPU.\n    self.assertAlmostEqual(\n        0.0 if use_tpu else want_eval_summary_loss,\n        tu.check_eventfile_for_keyword(""average_loss"", subnetwork_eval_subdir),\n        places=1)\n    self.assertAlmostEqual(\n        want_predictions,\n        tu.check_eventfile_for_keyword(""predictions"", subnetwork_eval_subdir),\n        places=3)\n\n    eval_subdir = os.path.join(self.test_subdirectory, ""eval"")\n    ensemble_eval_subdir = os.path.join(ensemble_subdir, ""eval"")\n    for subdir in [ensemble_eval_subdir, eval_subdir]:\n      self.assertEqual([b""| dnn |""],\n                       tu.check_eventfile_for_keyword(\n                           ""architecture/adanet/ensembles/0"", subdir))\n      if subdir == eval_subdir:\n        self.assertAlmostEqual(\n            want_loss, tu.check_eventfile_for_keyword(""loss"", subdir), places=1)\n      # TODO: Check why some eval metrics are zero on TPU.\n      self.assertAlmostEqual(\n          0.0 if use_tpu else want_eval_summary_loss,\n          tu.check_eventfile_for_keyword(""average_loss"", subdir),\n          places=1)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/distributed/__init__.py,0,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The `adanet.distributed` package.\n\nThis package methods for distributing computation using the TensorFlow\ncomputation graph.\n""""""\n\n# TODO: Add more details documentation.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.distributed.placement import PlacementStrategy\nfrom adanet.distributed.placement import ReplicationStrategy\nfrom adanet.distributed.placement import RoundRobinStrategy\n\n__all__ = [\n    ""PlacementStrategy"",\n    ""ReplicationStrategy"",\n    ""RoundRobinStrategy"",\n]\n'"
adanet/distributed/devices.py,1,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Device placement functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport hashlib\n\n\nclass _OpNameHashStrategy(object):\n  """"""Returns the ps task index for placement using a hash of the op name.""""""\n\n  def __init__(self, num_tasks):\n    """"""Create a new `_OpNameHashStrategy`.\n\n    Args:\n      num_tasks: Number of ps tasks to cycle among.\n    """"""\n\n    self._num_tasks = num_tasks\n\n  def __call__(self, op):\n    """"""Choose a ps task index for the given `Operation`.\n\n    Hashes the op name and assigns it to a ps task modulo the number of tasks.\n    This ensures that variables with the same name are always placed on the same\n    parameter server.\n\n    Args:\n      op: An `Operation` to be placed on ps.\n\n    Returns:\n      The ps task index to use for the `Operation`.\n    """"""\n\n    hashed = int(hashlib.sha256(op.name.encode(""utf-8"")).hexdigest(), 16)\n    return hashed % self._num_tasks\n\n\n@contextlib.contextmanager\ndef monkey_patch_default_variable_placement_strategy():\n  """"""Monkey patches the default variable placement strategy.\n\n  This strategy is used by tf.train.replica_device_setter. The new strategy\n  allows workers to having different graphs from the chief.\n\n  Yields:\n    A context with the monkey-patched default variable placement strategy.\n  """"""\n\n  # Import here to avoid strict BUILD deps check.\n  from tensorflow.python.training import device_setter  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n  old_round_robin_strategy = device_setter._RoundRobinStrategy  # pylint: disable=protected-access\n  setattr(device_setter, ""_RoundRobinStrategy"", _OpNameHashStrategy)\n  try:\n    yield\n  finally:\n    setattr(device_setter, ""_RoundRobinStrategy"", old_round_robin_strategy)\n'"
adanet/distributed/devices_test.py,7,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Device placement function tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet.distributed.devices import monkey_patch_default_variable_placement_strategy\n\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass DevicesTest(parameterized.TestCase, tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_monkey_patch_default_variable_placement_strategy_no_ps(self):\n    with context.graph_mode():\n      with monkey_patch_default_variable_placement_strategy():\n        device_fn = tf.compat.v1.train.replica_device_setter(ps_tasks=0)\n    self.assertIsNone(device_fn)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""one_ps"",\n          ""num_tasks"":\n              1,\n          ""op_names"": [""foo"", ""bar"", ""baz""],\n          ""before_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0""],\n          ""after_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0""],\n      }, {\n          ""testcase_name"":\n              ""three_ps"",\n          ""num_tasks"":\n              3,\n          ""op_names"": [""foo"", ""bar"", ""baz""],\n          ""before_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:1"", ""/job:ps/task:2""],\n          ""after_want_ps"":\n              [""/job:ps/task:2"", ""/job:ps/task:0"", ""/job:ps/task:1""],\n      }, {\n          ""testcase_name"":\n              ""reverse_three_ps"",\n          ""num_tasks"":\n              3,\n          ""op_names"": [""baz"", ""bar"", ""foo""],\n          ""before_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:1"", ""/job:ps/task:2""],\n          ""after_want_ps"":\n              [""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:2""],\n      }, {\n          ""testcase_name"":\n              ""six_ps"",\n          ""num_tasks"":\n              6,\n          ""op_names"": [""foo"", ""bar"", ""baz""],\n          ""before_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:1"", ""/job:ps/task:2""],\n          ""after_want_ps"":\n              [""/job:ps/task:2"", ""/job:ps/task:3"", ""/job:ps/task:4""],\n      }, {\n          ""testcase_name"":\n              ""reverse_six_ps"",\n          ""num_tasks"":\n              6,\n          ""op_names"": [""baz"", ""bar"", ""foo""],\n          ""before_want_ps"":\n              [""/job:ps/task:0"", ""/job:ps/task:1"", ""/job:ps/task:2""],\n          ""after_want_ps"":\n              [""/job:ps/task:4"", ""/job:ps/task:3"", ""/job:ps/task:2""],\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_monkey_patch_default_variable_placement_strategy(\n      self, num_tasks, op_names, before_want_ps, after_want_ps):\n    """"""Checks that ps placement is based on var name.""""""\n\n    with context.graph_mode():\n      var_ops = [tf.Variable(0., name=op_name).op for op_name in op_names]\n      before_device_fn = tf.compat.v1.train.replica_device_setter(\n          ps_tasks=num_tasks)\n      self.assertEqual(before_want_ps, [before_device_fn(op) for op in var_ops])\n\n      with monkey_patch_default_variable_placement_strategy():\n        after_device_fn = tf.compat.v1.train.replica_device_setter(\n            ps_tasks=num_tasks)\n      self.assertEqual(after_want_ps, [after_device_fn(op) for op in var_ops])\n\n      # Check that monkey-patch is only for the context.\n      before_device_fn = tf.compat.v1.train.replica_device_setter(\n          ps_tasks=num_tasks)\n      self.assertEqual(before_want_ps, [before_device_fn(op) for op in var_ops])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/distributed/placement.py,2,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Distributed placement strategies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport contextlib\n\nfrom absl import logging\nfrom adanet import tf_compat\nfrom adanet.distributed.devices import _OpNameHashStrategy\nimport numpy as np\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass PlacementStrategy(object):  # pytype: disable=ignored-metaclass\n  """"""Abstract placement strategy for distributed training.\n\n  Given a cluster of workers, the placement strategy determines which subgraph\n  each worker constructs.\n  """"""\n\n  @property\n  def config(self):\n    """"""Returns this strategy\'s configuration.\n\n    Returns:\n      The :class:`tf.estimator.RunConfig` instance that defines the cluster.\n    """"""\n\n    return self._config\n\n  @config.setter\n  def config(self, config):\n    """"""Configures the placement strategy with the given cluster description.\n\n    Args:\n      config: A :class:`tf.estimator.RunConfig` instance that defines the\n        cluster.\n    """"""\n\n    self._config = config\n\n  @abc.abstractmethod\n  def should_build_ensemble(self, num_subnetworks):\n    """"""Whether to build the ensemble on the current worker.\n\n    Args:\n      num_subnetworks: Integer number of subnetworks to train in the current\n        iteration.\n\n    Returns:\n      Boolean whether to build the ensemble on the current worker.\n    """"""\n\n  @abc.abstractmethod\n  def should_build_subnetwork(self, num_subnetworks, subnetwork_index):\n    """"""Whether to build the given subnetwork on the current worker.\n\n    Args:\n      num_subnetworks: Integer number of subnetworks to train in the current\n        iteration.\n      subnetwork_index: Integer index of the subnetwork in the list of the\n        current iteration\'s subnetworks.\n\n    Returns:\n      Boolean whether to build the given subnetwork on the current worker.\n    """"""\n\n  @abc.abstractmethod\n  def should_train_subnetworks(self, num_subnetworks):\n    """"""Whether to train subnetworks on the current worker.\n\n    Args:\n      num_subnetworks: Integer number of subnetworks to train in the current\n        iteration.\n\n    Returns:\n      Boolean whether to train subnetworks on the current worker.\n    """"""\n\n  @abc.abstractmethod\n  @contextlib.contextmanager\n  def subnetwork_devices(self, num_subnetworks, subnetwork_index):\n    """"""A context for assigning subnetwork ops to devices.""""""\n\n\nclass ReplicationStrategy(PlacementStrategy):\n  # pyformat: disable\n  """"""A simple strategy that replicates the same graph on every worker.\n\n  This strategy does not scale well as the number of subnetworks and workers\n  increases. For :math:`m` workers, :math:`n` parameter servers, and :math:`k`\n  subnetworks, this strategy will scale with :math:`O(m)` training speedup,\n  :math:`O(m*n*k)` variable fetches from parameter servers, and :math:`O(k)`\n  memory required per worker. Additionally there will be :math:`O(m)` stale\n  gradients per subnetwork when training with asynchronous SGD.\n\n  Returns:\n    A :class:`ReplicationStrategy` instance for the current cluster.\n  """"""\n  # pyformat: enable\n\n  def should_build_ensemble(self, num_subnetworks):\n    return True\n\n  def should_build_subnetwork(self, num_subnetworks, subnetwork_index):\n    return True\n\n  def should_train_subnetworks(self, num_subnetworks):\n    return True\n\n  @contextlib.contextmanager\n  def subnetwork_devices(self, num_subnetworks, subnetwork_index):\n    # Use default devices.\n    yield\n\n\nclass RoundRobinStrategy(PlacementStrategy):\n  # pyformat: disable\n  """"""A strategy that round-robin assigns subgraphs to specific workers.\n\n  Specifically, it selects dedicated workers to only train ensemble variables,\n  and round-robin assigns subnetworks to dedicated subnetwork-training workers.\n\n  Unlike :class:`ReplicationStrategy`, this strategy scales better with the\n  number of subnetworks, workers, and parameter servers. For :math:`m` workers,\n  :math:`n` parameter servers, and :math:`k` subnetworks, this strategy will\n  scale with :math:`O(m/k)` training speedup, :math:`O(m*n/k)` variable fetches\n  from parameter servers, and :math:`O(1)` memory required per worker.\n  Additionally, there will only be :math:`O(m/k)` stale gradients per subnetwork\n  when training with asynchronous SGD, which reduces training instability versus\n  :class:`ReplicationStrategy`.\n\n  When there are more workers than subnetworks, this strategy assigns\n  subnetworks to workers modulo the number of subnetworks.\n\n  Conversely, when there are more subnetworks than workers, this round robin\n  assigns subnetworks modulo the number of workers. So certain workers may end\n  up training more than one subnetwork.\n\n  This strategy gracefully handles scenarios when the number of subnetworks\n  does not perfectly divide the number of workers and vice-versa. It also\n  supports different numbers of subnetworks at different iterations, and\n  reloading training with a resized cluster.\n\n  Args:\n    drop_remainder: Bool whether to drop remaining subnetworks that haven\'t been\n      assigned to a worker in the remainder after perfect division of workers by\n      the current iteration\'s num_subnetworks + 1. When :code:`True`, each subnetwork\n      worker will only train a single subnetwork, and subnetworks that have not\n      been assigned to assigned to a worker are dropped. NOTE: This can result\n      in subnetworks not being assigned to any worker when\n      num_workers < num_subnetworks + 1. When :code:`False`, remaining subnetworks\n      during the round-robin assignment will be placed on workers that already\n      have a subnetwork.\n\n  Returns:\n    A :class:`RoundRobinStrategy` instance for the current cluster.\n  """"""\n  # pyformat: enable\n\n  # TODO: Allow user to disable ensemble workers. For example, when there\n  # are no ensemble variables to train, such as in a uniform average ensemble,\n  # there is no need for a non-chief to create the full ensemble during\n  # training, except for the chief to initialize the ensemble\'s non-trainable\n  # variables.\n\n  # TODO: Optional code organization suggestion:\n  # Explicitly define what a ""task"" is, to make the below code clearer. One way\n  # of doing this:\n  #\n  # def _worker_tasks(self, num_subnetworks):\n  #   """"""Returns the set of tasks that this worker can work on.\n  #\n  #   Each task is represented by an integer between 0 and num_subnetworks\n  #   (inclusive). 0 corresponds to the task of training the ensemble(s), 1\n  #   corresponds to the task of training subnetwork 0, 2 corresponds to the\n  #   task of training subnetwork 1, and so on.\n  #\n  #   Examples:\n  #     - 1 worker, 3 subnetworks. This would return {0, 1, 2, 3} for the only\n  #       worker, since the only worker would have to train the ensemble(s) and\n  #       all 3 subnetworks.\n  #     - 2 workers, 3 subnetworks. This would return {0} for worker 0, and\n  #       {1, 2, 3} for worker 1. This means that the first worker trains the\n  #       ensemble(s), while the second worker trains all three subnetworks.\n  #     - 4 workers, 3 subnetworks. This would return {0} for worker 0, {1} for\n  #       worker 1, {2} for worker 2, and {3} for worker 3. This means that\n  #       worker 0 trains the ensemble(s) while the rest of the workers train\n  #       one subnetwork each.\n  #     - 5 workers, 3 subnetworks. This would return {0} for worker 0, {1} for\n  #       worker 1, {2} for worker 2, {3} for worker 3, and {1} for worker 4.\n  #       This is like the previous case, except that worker 4 also helps to\n  #       train subnetwork 0.\n  #   """"""\n  #\n  # That way, should_build_ensemble can just be:\n  #\n  #   return 0 in self._worker_tasks(...)\n  #\n  # then should_build_subnetwork can just be:\n  #\n  #   if (subnetwork_index in self._worker_tasks(...) or 0 in\n  #       subnetwork_index in self._worker_tasks(...)):\n  #     return True\n  #   return False\n  #\n  # and should_train_subnetwork can just be:\n  #\n  #   return subnetwork_index in self._worker_tasks(...)\n\n  def __init__(self, drop_remainder=False, dedicate_parameter_servers=True):\n    self._drop_remainder = drop_remainder\n    self._dedicate_parameter_servers = dedicate_parameter_servers\n\n  @property\n  def _num_workers(self):\n    return self.config.num_worker_replicas\n\n  @property\n  def _worker_index(self):\n    return self.config.global_id_in_cluster or 0\n\n  def _worker_task(self, num_subnetworks):\n    """"""Returns the worker index modulo the number of subnetworks.""""""\n\n    if self._drop_remainder and self._num_workers > 1 and (num_subnetworks >\n                                                           self._num_workers):\n      logging.log_first_n(\n          logging.WARNING,\n          ""With drop_remainer=True, %s workers and %s subnetworks, the last %s ""\n          ""subnetworks will be dropped and will not be trained"", 1,\n          self._num_workers, num_subnetworks,\n          num_subnetworks - self._num_workers - 1)\n    # The first worker will always build the ensemble so we add 1.\n    return self._worker_index % (num_subnetworks + 1)\n\n  def should_build_ensemble(self, num_subnetworks):\n    if num_subnetworks == 1:\n      return True\n    worker_task = self._worker_task(num_subnetworks)\n    # The ensemble builder is always the first worker task.\n    return worker_task == 0\n\n  def should_build_subnetwork(self, num_subnetworks, subnetwork_index):\n    if num_subnetworks == 1:\n      return True\n    worker_task = self._worker_task(num_subnetworks)\n    if worker_task == 0:\n      # The zeroth index worker is an ensemble worker.\n      return True\n\n    subnetwork_worker_index = worker_task - 1\n    if self._drop_remainder:\n      return subnetwork_worker_index == subnetwork_index\n\n    workers_per_subnetwork = self._num_workers // (num_subnetworks + 1)\n    if self._num_workers % (num_subnetworks + 1) == 0:\n      num_subnetwork_workers = num_subnetworks\n    elif self._worker_index >= workers_per_subnetwork * (num_subnetworks + 1):\n      num_subnetwork_workers = self._num_workers % (num_subnetworks + 1) - 1\n    else:\n      num_subnetwork_workers = num_subnetworks\n    return subnetwork_worker_index == subnetwork_index % num_subnetwork_workers\n\n  def should_train_subnetworks(self, num_subnetworks):\n    if num_subnetworks == 1 or self._num_workers == 1:\n      return True\n    return not self.should_build_ensemble(num_subnetworks)\n\n  @contextlib.contextmanager\n  def subnetwork_devices(self, num_subnetworks, subnetwork_index):\n    if not self._dedicate_parameter_servers:\n      # Use default device placement.\n      yield\n      return\n\n    # Each subnetwork gets its own dedicated parameter servers\n    num_ps_replicas = self.config.num_ps_replicas\n    ps_numbers = np.array(range(num_ps_replicas))\n    subnetwork_group = subnetwork_index\n    if num_ps_replicas > 0 and num_subnetworks > num_ps_replicas:\n      subnetwork_group = subnetwork_index % num_ps_replicas\n    ps_group = np.array_split(ps_numbers, num_subnetworks)[subnetwork_group]\n\n    # Assign ops to parameter servers based on hashed op names.\n    ps_strategy = _OpNameHashStrategy(len(ps_group))\n\n    def device_fn(op):\n      """"""Assigns variables to a subnetwork\'s dedicated parameter servers.""""""\n\n      # Import here to avoid strict BUILD deps check.\n      from tensorflow.core.framework import node_def_pb2  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n      node_def = op if isinstance(op, node_def_pb2.NodeDef) else op.node_def\n      from tensorflow.python.training import device_setter  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n      if num_ps_replicas > 0 and node_def.op in device_setter.STANDARD_PS_OPS:\n        # ps_group lists the task ids in the group. Adding the first task id in\n        # the group to the task number determined by the PS strategy gives the\n        # correct parameter server assignment.\n        return ""/job:ps/task:{}"".format(ps_group[0] + ps_strategy(op))\n      return op.device\n\n    with tf_compat.v1.device(device_fn):\n      yield\n'"
adanet/distributed/placement_test.py,7,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Distributed placement strategy tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nfrom adanet.distributed.placement import ReplicationStrategy\nfrom adanet.distributed.placement import RoundRobinStrategy\n\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass ReplicationStrategyTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_strategy(self):\n    strategy = ReplicationStrategy()\n    num_subnetworks = 3\n    subnetwork_index = 1\n    self.assertTrue(strategy.should_build_ensemble(num_subnetworks))\n    self.assertTrue(\n        strategy.should_build_subnetwork(num_subnetworks, subnetwork_index))\n    self.assertTrue(strategy.should_train_subnetworks(num_subnetworks))\n\n\nclass WorkerConfig(object):\n\n  def __init__(self, num_worker_replicas, global_id_in_cluster):\n    self.num_worker_replicas = num_worker_replicas\n    self.global_id_in_cluster = global_id_in_cluster\n\n\nclass ParameterServerConfig(object):\n\n  def __init__(self, num_ps_replicas):\n    self.num_ps_replicas = num_ps_replicas\n\n\ndef _testcase_name(name, drop_remainder):\n  return ""{}{}"".format(name, ""_drop_remainder"" if drop_remainder else """")\n\n\nclass RoundRobinStrategyTest(parameterized.TestCase, tf.test.TestCase):\n\n  # pylint: disable=g-complex-comprehension\n  @parameterized.named_parameters(\n      itertools.chain(*[[\n          {\n              ""testcase_name"":\n                  _testcase_name(""one_worker_one_subnetwork"", drop_remainder),\n              ""num_workers"":\n                  1,\n              ""num_subnetworks"":\n                  1,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True],\n              ""want_should_build_subnetwork"": [[True]],\n              ""want_should_train_subnetworks"": [True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""three_workers_one_subnetworks"", drop_remainder\n                                ),\n              ""num_workers"":\n                  3,\n              ""num_subnetworks"":\n                  1,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, True, True],\n              ""want_should_build_subnetwork"": [[True], [True], [True]],\n              ""want_should_train_subnetworks"": [True, True, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""two_workers_one_subnetworks"", drop_remainder),\n              ""num_workers"":\n                  2,\n              ""num_subnetworks"":\n                  5,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, False],\n              ""want_should_build_subnetwork"": [[True, True, True, True, True],\n                                               [\n                                                   True,\n                                                   not drop_remainder,\n                                                   not drop_remainder,\n                                                   not drop_remainder,\n                                                   not drop_remainder,\n                                               ]],\n              ""want_should_train_subnetworks"": [False, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""one_worker_three_subnetworks"", drop_remainder\n                                ),\n              ""num_workers"":\n                  1,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True],\n              ""want_should_build_subnetwork"": [[True, True, True]],\n              ""want_should_train_subnetworks"": [True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""two_workers_three_subnetworks"", drop_remainder\n                                ),\n              ""num_workers"":\n                  2,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, not drop_remainder, not drop_remainder],\n              ],\n              ""want_should_train_subnetworks"": [False, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""three_workers_three_subnetworks"",\n                                 drop_remainder),\n              ""num_workers"":\n                  3,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, False, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, not drop_remainder],\n                  [False, True, False],\n              ],\n              ""want_should_train_subnetworks"": [False, True, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""four_workers_three_subnetworks"",\n                                 drop_remainder),\n              ""num_workers"":\n                  4,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, False, False, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n              ],\n              ""want_should_train_subnetworks"": [False, True, True, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""five_workers_three_subnetworks"",\n                                 drop_remainder),\n              ""num_workers"":\n                  5,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"": [True, False, False, False, True],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n                  [True, True, True],\n              ],\n              ""want_should_train_subnetworks"": [False, True, True, True, False],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""six_workers_three_subnetworks"", drop_remainder\n                                ),\n              ""num_workers"":\n                  6,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"":\n                  [True, False, False, False, True, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n                  [True, True, True],\n                  [True, not drop_remainder, not drop_remainder],\n              ],\n              ""want_should_train_subnetworks"":\n                  [False, True, True, True, False, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""seven_workers_three_subnetworks"",\n                                 drop_remainder),\n              ""num_workers"":\n                  7,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"":\n                  [True, False, False, False, True, False, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n                  [True, True, True],\n                  [True, False, not drop_remainder],\n                  [False, True, False],\n              ],\n              ""want_should_train_subnetworks"":\n                  [False, True, True, True, False, True, True],\n          },\n          {\n              ""testcase_name"":\n                  _testcase_name(""eight_workers_three_subnetworks"",\n                                 drop_remainder),\n              ""num_workers"":\n                  8,\n              ""num_subnetworks"":\n                  3,\n              ""drop_remainder"":\n                  drop_remainder,\n              ""want_should_build_ensemble"":\n                  [True, False, False, False, True, False, False, False],\n              ""want_should_build_subnetwork"": [\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n                  [True, True, True],\n                  [True, False, False],\n                  [False, True, False],\n                  [False, False, True],\n              ],\n              ""want_should_train_subnetworks"":\n                  [False, True, True, True, False, True, True, True],\n          },\n      ] for drop_remainder in [False, True]]))\n  # pylint: enable=g-complex-comprehension\n  @test_util.run_in_graph_and_eager_modes\n  def test_worker_methods(self, num_workers, num_subnetworks, drop_remainder,\n                          want_should_build_ensemble,\n                          want_should_build_subnetwork,\n                          want_should_train_subnetworks):\n    should_build_ensemble = []\n    should_build_subnetwork = []\n    should_train_subnetworks = []\n    for worker_index in range(num_workers):\n      strategy = RoundRobinStrategy(drop_remainder)\n      strategy.config = WorkerConfig(num_workers, worker_index)\n      should_build_ensemble.append(\n          strategy.should_build_ensemble(num_subnetworks))\n      should_build_subnetwork.append([])\n      should_train_subnetworks.append(\n          strategy.should_train_subnetworks(num_subnetworks))\n      for subnetwork_index in range(num_subnetworks):\n        should_build_subnetwork[-1].append(\n            strategy.should_build_subnetwork(num_subnetworks, subnetwork_index))\n    self.assertEqual(want_should_build_ensemble, should_build_ensemble)\n    self.assertEqual(want_should_build_subnetwork, should_build_subnetwork)\n    self.assertEqual(want_should_train_subnetworks, should_train_subnetworks)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"":\n              ""one_ps_one_subnetwork"",\n          ""num_ps"":\n              1,\n          ""num_subnetworks"":\n              1,\n          ""want_variable_devices"": [[\n              ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n              ""/job:ps/task:0""\n          ],],\n      },\n      {\n          ""testcase_name"":\n              ""three_ps_one_subnetwork"",\n          ""num_ps"":\n              3,\n          ""num_subnetworks"":\n              1,\n          ""want_variable_devices"": [[\n              ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:2"",\n              ""/job:ps/task:0""\n          ],],\n      },\n      {\n          ""testcase_name"":\n              ""two_ps_five_subnetworks"",\n          ""num_ps"":\n              2,\n          ""num_subnetworks"":\n              5,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:1"", ""/job:ps/task:1"",\n                  ""/job:ps/task:1""\n              ],\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:1"", ""/job:ps/task:1"",\n                  ""/job:ps/task:1""\n              ],\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""one_ps_three_subnetworks"",\n          ""num_ps"":\n              1,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""two_ps_three_subnetworks"",\n          ""num_ps"":\n              2,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:1"", ""/job:ps/task:1"",\n                  ""/job:ps/task:1""\n              ],\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""three_ps_three_subnetworks"",\n          ""num_ps"":\n              3,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:0"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:1"", ""/job:ps/task:1"",\n                  ""/job:ps/task:1""\n              ],\n              [\n                  ""/job:ps/task:2"", ""/job:ps/task:2"", ""/job:ps/task:2"",\n                  ""/job:ps/task:2""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""three_ps_three_subnetworks_no_dedicated_parameter_servers"",\n          ""num_ps"":\n              3,\n          ""num_subnetworks"":\n              3,\n          ""dedicate_parameter_servers"":\n              False,\n          ""want_variable_devices"": [\n              ["""", """", """", """"],\n              ["""", """", """", """"],\n              ["""", """", """", """"],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""four_ps_three_subnetworks"",\n          ""num_ps"":\n              4,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:2"", ""/job:ps/task:2"", ""/job:ps/task:2"",\n                  ""/job:ps/task:2""\n              ],\n              [\n                  ""/job:ps/task:3"", ""/job:ps/task:3"", ""/job:ps/task:3"",\n                  ""/job:ps/task:3""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""five_ps_three_subnetworks"",\n          ""num_ps"":\n              5,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:2"", ""/job:ps/task:3"", ""/job:ps/task:3"",\n                  ""/job:ps/task:2""\n              ],\n              [\n                  ""/job:ps/task:4"", ""/job:ps/task:4"", ""/job:ps/task:4"",\n                  ""/job:ps/task:4""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""six_ps_three_subnetworks"",\n          ""num_ps"":\n              6,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:0"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:2"", ""/job:ps/task:3"", ""/job:ps/task:3"",\n                  ""/job:ps/task:2""\n              ],\n              [\n                  ""/job:ps/task:5"", ""/job:ps/task:4"", ""/job:ps/task:4"",\n                  ""/job:ps/task:5""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""seven_ps_three_subnetworks"",\n          ""num_ps"":\n              7,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:2"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:3"", ""/job:ps/task:4"", ""/job:ps/task:4"",\n                  ""/job:ps/task:3""\n              ],\n              [\n                  ""/job:ps/task:6"", ""/job:ps/task:5"", ""/job:ps/task:5"",\n                  ""/job:ps/task:6""\n              ],\n          ],\n      },\n      {\n          ""testcase_name"":\n              ""eight_ps_three_subnetworks"",\n          ""num_ps"":\n              8,\n          ""num_subnetworks"":\n              3,\n          ""want_variable_devices"": [\n              [\n                  ""/job:ps/task:1"", ""/job:ps/task:0"", ""/job:ps/task:2"",\n                  ""/job:ps/task:0""\n              ],\n              [\n                  ""/job:ps/task:4"", ""/job:ps/task:5"", ""/job:ps/task:5"",\n                  ""/job:ps/task:4""\n              ],\n              [\n                  ""/job:ps/task:7"", ""/job:ps/task:6"", ""/job:ps/task:6"",\n                  ""/job:ps/task:7""\n              ],\n          ],\n      },\n  )\n  @test_util.run_in_graph_and_eager_modes\n  def test_device_methods(self,\n                          num_ps,\n                          num_subnetworks,\n                          want_variable_devices,\n                          dedicate_parameter_servers=True):\n    with context.graph_mode():\n      x = tf.constant([[1., 0.]])\n      strategy = RoundRobinStrategy(\n          dedicate_parameter_servers=dedicate_parameter_servers)\n      strategy.config = ParameterServerConfig(num_ps)\n      variable_devices = []\n      for i in range(num_subnetworks):\n        with strategy.subnetwork_devices(num_subnetworks, i):\n          subnetwork = tf.keras.Sequential()\n          subnetwork.add(tf.keras.layers.Dense(4))\n          subnetwork.add(tf.keras.layers.Dense(3))\n          subnetwork(x)\n        variable_devices.append([w.op.device for w in subnetwork.weights])\n    self.assertEqual(want_variable_devices, variable_devices)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/ensemble/__init__.py,0,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Defines built-in ensemble methods and interfaces for custom ensembles.""""""\n\n# TODO: Add more detailed documentation.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.ensemble.ensembler import Ensemble\nfrom adanet.ensemble.ensembler import Ensembler\nfrom adanet.ensemble.ensembler import TrainOpSpec\nfrom adanet.ensemble.mean import MeanEnsemble\nfrom adanet.ensemble.mean import MeanEnsembler\nfrom adanet.ensemble.strategy import AllStrategy\nfrom adanet.ensemble.strategy import Candidate\nfrom adanet.ensemble.strategy import GrowStrategy\nfrom adanet.ensemble.strategy import SoloStrategy\nfrom adanet.ensemble.strategy import Strategy\nfrom adanet.ensemble.weighted import ComplexityRegularized\nfrom adanet.ensemble.weighted import ComplexityRegularizedEnsembler\nfrom adanet.ensemble.weighted import MixtureWeightType\nfrom adanet.ensemble.weighted import WeightedSubnetwork\n\n__all__ = [\n    ""Ensemble"",\n    ""Ensembler"",\n    ""TrainOpSpec"",\n    ""AllStrategy"",\n    ""Candidate"",\n    ""GrowStrategy"",\n    ""SoloStrategy"",\n    ""Strategy"",\n    ""ComplexityRegularized"",\n    ""ComplexityRegularizedEnsembler"",\n    ""MeanEnsemble"",\n    ""MeanEnsembler"",\n    ""MixtureWeightType"",\n    ""WeightedSubnetwork"",\n]\n'"
adanet/ensemble/ensembler.py,20,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Ensembler definitions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\n\nimport six\n\n\nclass TrainOpSpec(\n    collections.namedtuple(""TrainOpSpec"",\n                           [""train_op"", ""chief_hooks"", ""hooks""])):\n  """"""A data structure for specifying ensembler training operations.\n\n  Args:\n    train_op: Op for the training step.\n    chief_hooks: Iterable of :class:`tf.train.SessionRunHook` objects to run on\n      the chief worker during training.\n    hooks: Iterable of :class:`tf.train.SessionRunHook` objects to run on all\n      workers during training.\n\n  Returns:\n    An :class:`adanet.ensemble.TrainOpSpec` object.\n  """"""\n\n  def __new__(cls, train_op, chief_hooks=None, hooks=None):\n    # Make hooks immutable.\n    chief_hooks = tuple(chief_hooks) if chief_hooks else ()\n    hooks = tuple(hooks) if hooks else ()\n    return super(TrainOpSpec, cls).__new__(cls, train_op, chief_hooks, hooks)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Ensemble(object):\n  """"""An abstract ensemble of subnetworks.""""""\n\n  @abc.abstractproperty\n  def logits(self):\n    """"""Ensemble logits :class:`tf.Tensor`.""""""\n\n  @abc.abstractproperty\n  def subnetworks(self):\n    """"""Returns an ordered :class:`Iterable` of the ensemble\'s subnetworks.""""""\n\n  @property\n  def predictions(self):\n    """"""Optional dict of Ensemble predictions to be merged in EstimatorSpec.\n\n    These will be additional (over the default included by the head) predictions\n    which will be included in the EstimatorSpec in `predictions` and\n    `export_outputs` (wrapped as PredictOutput).\n    """"""\n    return None\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Ensembler(object):\n  """"""An abstract ensembler.""""""\n\n  @abc.abstractproperty\n  def name(self):\n    """"""This ensembler\'s unique string name.""""""\n\n  @abc.abstractmethod\n  def build_ensemble(self, subnetworks, previous_ensemble_subnetworks, features,\n                     labels, logits_dimension, training, iteration_step,\n                     summary, previous_ensemble, previous_iteration_checkpoint):\n    # pyformat: disable\n    """"""Builds an ensemble of subnetworks.\n\n    Accessing the global step via :meth:`tf.train.get_or_create_global_step()`\n    or :meth:`tf.train.get_global_step()` within this scope will return an\n    incrementable iteration step since the beginning of the iteration.\n\n    Args:\n      subnetworks: Ordered iterable of :class:`adanet.subnetwork.Subnetwork`\n        instances to ensemble. Must have at least one element.\n      previous_ensemble_subnetworks: Ordered iterable of\n        :class:`adanet.subnetwork.Subnetwork` instances present in previous\n        ensemble to be used. The subnetworks from previous_ensemble not\n        included in this list should be pruned. Can be set to None or empty.\n      features: Input :code:`dict` of :class:`tf.Tensor` objects.\n      labels: Labels :class:`tf.Tensor` or a dictionary of string label name to\n        :class:`tf.Tensor` (for multi-head). Can be :code:`None`.\n      logits_dimension: Size of the last dimension of the logits\n        :class:`tf.Tensor`. Typically, logits have for shape `[batch_size,\n        logits_dimension]`.\n      training: A python boolean indicating whether the graph is in training\n        mode or prediction mode.\n      iteration_step: Integer :class:`tf.Tensor` representing the step since the\n        beginning of the current iteration, as opposed to the global step.\n      summary: An :class:`adanet.Summary` for scoping summaries to individual\n        ensembles in Tensorboard. Using :meth:`tf.summary` within this scope\n        will use this :class:`adanet.Summary` under the hood.\n      previous_ensemble: The best :class:`adanet.Ensemble` from iteration *t-1*.\n        The created subnetwork will extend the previous ensemble to form the\n        :class:`adanet.Ensemble` at iteration *t*.\n      previous_iteration_checkpoint: The `tf.train.Checkpoint` object associated\n        with the previous iteration.\n\n    Returns:\n      An :class:`adanet.ensemble.Ensemble` subclass instance.\n    """"""\n    # pyformat: enable\n\n  @abc.abstractmethod\n  def build_train_op(self, ensemble, loss, var_list, labels, iteration_step,\n                     summary, previous_ensemble):\n    # pyformat: disable\n    """"""Returns an op for training an ensemble.\n\n    Accessing the global step via :meth:`tf.train.get_or_create_global_step`\n    or :meth:`tf.train.get_global_step` within this scope will return an\n    incrementable iteration step since the beginning of the iteration.\n\n    Args:\n      ensemble: The :class:`adanet.ensemble.Ensemble` subclass instance returned\n        by this instance\'s :meth:`build_ensemble`.\n      loss: A :class:`tf.Tensor` containing the ensemble\'s loss to minimize.\n      var_list: List of ensemble :class:`tf.Variable` parameters to update as\n        part of the training operation.\n      labels: Labels :class:`tf.Tensor` or a dictionary of string label name to\n        :class:`tf.Tensor` (for multi-head).\n      iteration_step: Integer :class:`tf.Tensor` representing the step since the\n        beginning of the current iteration, as opposed to the global step.\n      summary: An :class:`adanet.Summary` for scoping summaries to individual\n        ensembles in Tensorboard. Using :code:`tf.summary` within this scope\n        will use this :class:`adanet.Summary` under the hood.\n      previous_ensemble: The best :class:`adanet.ensemble.Ensemble` from the\n        previous iteration.\n    Returns:\n      Either a train op or an :class:`adanet.ensemble.TrainOpSpec`.\n    """"""\n    # pyformat: enable\n'"
adanet/ensemble/mean.py,10,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Adanet implementation for an ensembler for the mean of subnetwork logits.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom adanet.ensemble.ensembler import Ensemble\nfrom adanet.ensemble.ensembler import Ensembler\nimport tensorflow.compat.v2 as tf\n\n\nclass MeanEnsemble(\n    collections.namedtuple(\'MeanEnsemble\',\n                           [\'logits\', \'subnetworks\', \'predictions\']),\n    Ensemble):\n  r""""""Mean ensemble.\n\n  Attributes:\n    logits: Logits :class:`tf.Tensor` or dict of string to logits\n      :class:`tf.Tensor` (for multi-head).\n    subnetworks: List of :class:`adanet.subnetwork.Subnetwork` instances that\n      form this ensemble.\n    predictions: Optional dict mapping prediction keys to Tensors. MeanEnsembler\n      can export mean_last_layer if the subnetworks have the last_layer of the\n      same dimension.\n  """"""\n  # Key in predictions and export_outputs for mean of last_layer.\n  MEAN_LAST_LAYER = \'mean_last_layer\'\n\n  def __new__(cls,\n              logits,\n              subnetworks=None,\n              predictions=None):\n    return super(MeanEnsemble, cls).__new__(\n        cls,\n        logits=logits,\n        subnetworks=list(subnetworks or []),\n        predictions=predictions)\n\n\nclass MeanEnsembler(Ensembler):\n  # pyformat: disable\n  r""""""Ensembler that takes the mean of logits returned by its subnetworks.\n\n  Attributes:\n    name: Optional name for the ensembler. Defaults to \'complexity_regularized\'.\n    add_mean_last_layer_predictions: Set to True to add mean of last_layer in\n      subnetworks in estimator\'s predictions and export outputs.\n  """"""\n  # pyformat: enable\n\n  def __init__(self,\n               name=None, add_mean_last_layer_predictions=False):\n    self._name = name\n    self._add_mean_last_layer_predictions = add_mean_last_layer_predictions\n\n  @property\n  def name(self):\n    if self._name:\n      return self._name\n    return \'mean\'\n\n  def _assert_last_layer_compatible_shapes(self, tensors):\n    if not tensors:\n      return True\n    first_shape = tensors[0].shape\n    for tensor in tensors:\n      try:\n        first_shape.assert_is_compatible_with(tensor.shape)\n      except ValueError:\n        raise ValueError(\n            \'Shape of `last_layer` tensors must be same if setting \'\n            \'`add_mean_last_layer_predictions` to True. Found %s vs %s.\'\n            % (first_shape, tensor.shape))\n    return True\n\n  def build_ensemble(self, subnetworks, previous_ensemble_subnetworks, features,\n                     labels, logits_dimension, training, iteration_step,\n                     summary, previous_ensemble, previous_iteration_checkpoint):\n    del features, labels, logits_dimension, training, iteration_step  # unused\n    del previous_ensemble_subnetworks, previous_iteration_checkpoint  # unused\n\n    if isinstance(subnetworks[0].logits, dict):\n      mean_logits = {\n          key: tf.math.reduce_mean(\n              tf.stack([s.logits[key] for s in subnetworks]), axis=0)\n          for key in subnetworks[0].logits\n      }\n    else:\n      mean_logits = tf.math.reduce_mean(\n          tf.stack([s.logits for s in subnetworks]), axis=0)\n\n    mean_last_layer = None\n    if self._add_mean_last_layer_predictions:\n      mean_last_layer = {}\n      if isinstance(subnetworks[0].last_layer, dict):\n        for key in subnetworks[0].logits:\n          last_layers = [s.last_layer[key] for s in subnetworks]\n          self._assert_last_layer_compatible_shapes(last_layers)\n          mean_last_layer[\'{}_{}\'.format(MeanEnsemble.MEAN_LAST_LAYER,\n                                         key)] = tf.math.reduce_mean(\n                                             tf.stack(last_layers), axis=0)\n      else:\n        last_layers = [subnetwork.last_layer for subnetwork in subnetworks]\n        self._assert_last_layer_compatible_shapes(last_layers)\n        mean_last_layer = {\n            MeanEnsemble.MEAN_LAST_LAYER:\n                tf.math.reduce_mean(tf.stack(last_layers), axis=0)\n        }\n\n    return MeanEnsemble(\n        subnetworks=subnetworks,\n        logits=mean_logits,\n        predictions=mean_last_layer)\n\n  def build_train_op(self, ensemble, loss, var_list, labels, iteration_step,\n                     summary, previous_ensemble):\n    del ensemble, loss, var_list, labels, iteration_step, summary  # unused\n    del previous_ensemble  # unused\n    return tf.no_op()\n'"
adanet/ensemble/mean_test.py,3,"b'""""""Test AdaNet mean ensemble and ensembler implementation.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import ensemble\nfrom adanet import subnetwork\nfrom adanet import tf_compat\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass MeanTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _build_subnetwork(self, multi_head=False, last_layer_dim=3):\n\n    last_layer = tf.Variable(\n        tf_compat.random_normal(shape=(2, last_layer_dim)),\n        trainable=False).read_value()\n\n    def new_logits():\n      return tf_compat.v1.layers.dense(\n          last_layer,\n          units=1,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer())\n\n    if multi_head:\n      logits = {k: new_logits() for k in multi_head}\n      last_layer = {k: last_layer for k in multi_head}\n    else:\n      logits = new_logits()\n\n    return subnetwork.Subnetwork(\n        last_layer=last_layer, logits=logits, complexity=2)\n\n  @parameterized.named_parameters({\n      \'testcase_name\': \'base\',\n  }, {\n      \'testcase_name\': \'base_with_last_layer_predictions\',\n      \'add_mean_last_layer_predictions\': True\n  }, {\n      \'testcase_name\': \'base_with_last_layer_predictions_diff_shapes\',\n      \'add_mean_last_layer_predictions\': True,\n      \'diff_last_layer_shapes\': True\n  }, {\n      \'testcase_name\': \'multi_head\',\n      \'multi_head\': [\'first_head\', \'second_head\'],\n  }, {\n      \'testcase_name\': \'multi_head_with_last_layer_predictions\',\n      \'multi_head\': [\'first_head\', \'second_head\'],\n      \'add_mean_last_layer_predictions\': True\n  }, {\n      \'testcase_name\': \'multi_head_with_last_layer_predictions_diff_shapes\',\n      \'multi_head\': [\'first_head\', \'second_head\'],\n      \'add_mean_last_layer_predictions\': True,\n      \'diff_last_layer_shapes\': True\n  })\n  @test_util.run_in_graph_and_eager_modes\n  def test_mean_ensembler(self,\n                          multi_head=False,\n                          add_mean_last_layer_predictions=False,\n                          diff_last_layer_shapes=False):\n    with context.graph_mode():\n      ensembler = ensemble.MeanEnsembler(\n          add_mean_last_layer_predictions=add_mean_last_layer_predictions)\n      last_layer_dims = [3, 3]\n      if diff_last_layer_shapes:\n        last_layer_dims = [3, 5]\n      if multi_head:\n        subnetworks = [\n            self._build_subnetwork(\n                multi_head=multi_head, last_layer_dim=last_layer_dim)\n            for last_layer_dim in last_layer_dims\n        ]\n      else:\n        subnetworks = [\n            self._build_subnetwork(last_layer_dim=last_layer_dim)\n            for last_layer_dim in last_layer_dims\n        ]\n\n      if diff_last_layer_shapes:\n        with self.assertRaisesRegexp(\n            ValueError, r\'Shape of \\`last_layer\\` tensors must be same\'):\n          built_ensemble = ensembler.build_ensemble(\n              subnetworks=subnetworks,\n              previous_ensemble_subnetworks=None,\n              features=None,\n              labels=None,\n              logits_dimension=None,\n              training=None,\n              iteration_step=None,\n              summary=None,\n              previous_ensemble=None,\n              previous_iteration_checkpoint=None)\n        return\n      built_ensemble = ensembler.build_ensemble(\n          subnetworks=subnetworks,\n          previous_ensemble_subnetworks=None,\n          features=None,\n          labels=None,\n          logits_dimension=None,\n          training=None,\n          iteration_step=None,\n          summary=None,\n          previous_ensemble=None,\n          previous_iteration_checkpoint=None)\n\n      with self.test_session() as sess:\n        sess.run(tf_compat.v1.global_variables_initializer())\n        got_logits = sess.run(built_ensemble.logits)\n\n        if add_mean_last_layer_predictions:\n          got_predictions = sess.run(built_ensemble.predictions)\n\n        logits = sess.run([s.logits for s in subnetworks])\n        last_layer = sess.run([s.last_layer for s in subnetworks])\n        if not multi_head:\n          expected_logits = np.mean(logits, axis=0)\n          expected_predictions = {\n              ensemble.MeanEnsemble.MEAN_LAST_LAYER: np.mean(\n                  last_layer, axis=0)\n          }\n        else:\n          expected_logits = {\n              head_name: np.mean([s[head_name] for s in logits\n                                 ], axis=0) for head_name in multi_head\n          }\n          expected_predictions = {\n              \'{}_{}\'.format(ensemble.MeanEnsemble.MEAN_LAST_LAYER, head_name):\n              np.mean([s[head_name] for s in last_layer], axis=0)\n              for head_name in multi_head\n          }\n\n        self.assertAllClose(expected_logits, got_logits)\n        if add_mean_last_layer_predictions:\n          self.assertAllClose(expected_predictions, got_predictions)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
adanet/ensemble/strategy.py,0,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Search strategy algorithms.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\n\nimport six\n\n\nclass Candidate(\n    collections.namedtuple(""Candidate"", [\n        ""name"", ""subnetwork_builders"", ""previous_ensemble_subnetwork_builders""\n    ])):\n  """"""An ensemble candidate found during the search phase.\n\n  Args:\n    name: String name of this ensemble candidate.\n    subnetwork_builders: Candidate :class:`adanet.subnetwork.Builder` instances\n      to include in the ensemble.\n    previous_ensemble_subnetwork_builders: :class:`adanet.subnetwork.Builder`\n      instances to include from the previous ensemble.\n  """"""\n\n  def __new__(cls, name, subnetwork_builders,\n              previous_ensemble_subnetwork_builders):\n    return super(Candidate, cls).__new__(\n        cls,\n        name=name,\n        subnetwork_builders=tuple(subnetwork_builders),\n        previous_ensemble_subnetwork_builders=tuple(\n            previous_ensemble_subnetwork_builders or []))\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Strategy(object):  # pytype: disable=ignored-metaclass\n  """"""An abstract ensemble strategy.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def generate_ensemble_candidates(self, subnetwork_builders,\n                                   previous_ensemble_subnetwork_builders):\n    """"""Generates ensemble candidates to search over this iteration.\n\n    Args:\n      subnetwork_builders: Candidate :class:`adanet.subnetwork.Builder`\n        instances for this iteration.\n      previous_ensemble_subnetwork_builders: :class:`adanet.subnetwork.Builder`\n        instances from the previous ensemble. Including only a subset of these\n        in a returned :class:`adanet.ensemble.Candidate` is equivalent to\n        pruning the previous ensemble.\n\n    Returns:\n      An iterable of :class:`adanet.ensemble.Candidate` instances to train and\n      consider this iteration.\n    """"""\n\n    # TODO: Pruning the previous subnetwork may require more metadata\n    # such as `subnetwork.Reports` and `ensemble.Reports` to make smart\n    # decisions.\n\n\nclass SoloStrategy(Strategy):\n  """"""Produces a model composed of a single subnetwork.\n\n  *An ensemble of one.*\n\n  This is effectively the same as pruning all previous ensemble subnetworks,\n  and only adding one subnetwork candidate to the ensemble.\n  """"""\n\n  def generate_ensemble_candidates(self, subnetwork_builders,\n                                   previous_ensemble_subnetwork_builders):\n    return [\n        Candidate(""{}_solo"".format(subnetwork_builder.name),\n                  [subnetwork_builder], None)\n        for subnetwork_builder in subnetwork_builders\n    ]\n\n\nclass GrowStrategy(Strategy):\n  """"""Greedily grows an ensemble, one subnetwork at a time.""""""\n\n  def generate_ensemble_candidates(self, subnetwork_builders,\n                                   previous_ensemble_subnetwork_builders):\n    return [\n        Candidate(""{}_grow"".format(subnetwork_builder.name),\n                  [subnetwork_builder], previous_ensemble_subnetwork_builders)\n        for subnetwork_builder in subnetwork_builders\n    ]\n\n\nclass AllStrategy(Strategy):\n  """"""Ensembles all subnetworks from the current iteration.""""""\n\n  def generate_ensemble_candidates(self, subnetwork_builders,\n                                   previous_ensemble_subnetwork_builders):\n    return [\n        Candidate(""all"", subnetwork_builders,\n                  previous_ensemble_subnetwork_builders)\n    ]\n'"
adanet/ensemble/strategy_test.py,2,"b'""""""Test AdaNet single graph subnetwork implementation.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet import ensemble\nfrom adanet import subnetwork\nimport mock\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass StrategyTest(tf.test.TestCase):\n\n  def setUp(self):\n    self.fake_builder_1 = mock.create_autospec(spec=subnetwork.Builder)\n    self.fake_builder_2 = mock.create_autospec(spec=subnetwork.Builder)\n    self.fake_builder_3 = mock.create_autospec(spec=subnetwork.Builder)\n    self.fake_builder_4 = mock.create_autospec(spec=subnetwork.Builder)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_solo_strategy(self):\n    want = [\n        ensemble.Candidate(""{}_solo"".format(self.fake_builder_1.name),\n                           [self.fake_builder_1], []),\n        ensemble.Candidate(""{}_solo"".format(self.fake_builder_2.name),\n                           [self.fake_builder_2], [])\n    ]\n    got = ensemble.SoloStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2], None)\n\n    self.assertEqual(want, got)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_solo_strategy_with_previous_ensemble_subnetwork_builders(self):\n    want = [\n        ensemble.Candidate(""{}_solo"".format(self.fake_builder_1.name),\n                           [self.fake_builder_1], []),\n        ensemble.Candidate(""{}_solo"".format(self.fake_builder_2.name),\n                           [self.fake_builder_2], [])\n    ]\n    got = ensemble.SoloStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2],\n        [self.fake_builder_3, self.fake_builder_4])\n\n    self.assertEqual(want, got)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_grow_strategy(self):\n    want = [\n        ensemble.Candidate(""{}_grow"".format(self.fake_builder_1.name),\n                           [self.fake_builder_1], []),\n        ensemble.Candidate(""{}_grow"".format(self.fake_builder_2.name),\n                           [self.fake_builder_2], [])\n    ]\n    got = ensemble.GrowStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2], None)\n    self.assertEqual(want, got)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_grow_strategy_with_previous_ensemble_subnetwork_builders(self):\n    want = [\n        ensemble.Candidate(""{}_grow"".format(self.fake_builder_1.name),\n                           [self.fake_builder_1],\n                           [self.fake_builder_3, self.fake_builder_4]),\n        ensemble.Candidate(""{}_grow"".format(self.fake_builder_2.name),\n                           [self.fake_builder_2],\n                           [self.fake_builder_3, self.fake_builder_4])\n    ]\n    got = ensemble.GrowStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2],\n        [self.fake_builder_3, self.fake_builder_4])\n    self.assertEqual(want, got)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_all_strategy(self):\n    want = [\n        ensemble.Candidate(""all"", [self.fake_builder_1, self.fake_builder_2],\n                           [])\n    ]\n    got = ensemble.AllStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2], None)\n    self.assertEqual(want, got)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_all_strategy_with_previous_ensemble_subnetwork_builders(self):\n    want = [\n        ensemble.Candidate(""all"", [self.fake_builder_1, self.fake_builder_2],\n                           [self.fake_builder_3, self.fake_builder_4])\n    ]\n    got = ensemble.AllStrategy().generate_ensemble_candidates(\n        [self.fake_builder_1, self.fake_builder_2],\n        [self.fake_builder_3, self.fake_builder_4])\n    self.assertEqual(want, got)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/ensemble/weighted.py,27,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Adanet implementation for weighted Subnetwork and Ensemblers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl import logging\nfrom adanet import tf_compat\nfrom adanet.ensemble.ensembler import Ensemble\nfrom adanet.ensemble.ensembler import Ensembler\nimport tensorflow.compat.v2 as tf\n\n\ndef _stringify(key):\n  """"""Flattens tuple and list keys into strings.""""""\n\n  if isinstance(key, (tuple, list)):\n    return ""_"".join([str(el) for el in key])\n  return key\n\n\ndef _lookup_if_dict(target, key):\n  if isinstance(target, dict):\n    return target[key]\n  return target\n\n\nclass WeightedSubnetwork(\n    collections.namedtuple(\n        ""WeightedSubnetwork"",\n        [""name"", ""iteration_number"", ""weight"", ""logits"", ""subnetwork""])):\n  # pyformat: disable\n  """"""An AdaNet weighted subnetwork.\n\n  A weighted subnetwork is a weight applied to a subnetwork\'s last layer\n  or logits (depending on the mixture weights type).\n\n  Args:\n    name: String name of :code:`subnetwork` as defined by its\n      :class:`adanet.subnetwork.Builder`.\n    iteration_number: Integer iteration when the subnetwork was created.\n    weight: The weight :class:`tf.Tensor` or dict of string to weight\n      :class:`tf.Tensor` (for multi-head) to apply to this subnetwork. The\n      AdaNet paper refers to this weight as :math:`w` in Equations (4), (5),\n      and (6).\n    logits: The output :class:`tf.Tensor` or dict of string to weight\n      :class:`tf.Tensor` (for multi-head) after the matrix multiplication of\n      :code:`weight` and the subnetwork\'s :code:`last_layer`. The output\'s shape\n      is [batch_size, logits_dimension]. It is equivalent to a linear logits\n      layer in a neural network.\n    subnetwork: The :class:`adanet.subnetwork.Subnetwork` to weight.\n\n  Returns:\n    An :class:`adanet.ensemble.WeightedSubnetwork` object.\n  """"""\n  # pyformat: enable\n\n  def __new__(cls,\n              name="""",\n              iteration_number=0,\n              weight=None,\n              logits=None,\n              subnetwork=None):\n    return super(WeightedSubnetwork, cls).__new__(\n        cls,\n        name=name,\n        iteration_number=iteration_number,\n        weight=weight,\n        logits=logits,\n        subnetwork=subnetwork)\n\n\nclass ComplexityRegularized(\n    collections.namedtuple(""ComplexityRegularized"", [\n        ""weighted_subnetworks"", ""bias"", ""logits"", ""subnetworks"",\n        ""complexity_regularization""\n    ]), Ensemble):\n  r""""""An AdaNet ensemble where subnetworks are regularized by model complexity.\n\n  Hence an ensemble is a collection of subnetworks which forms a neural network\n  through the weighted sum of their outputs:\n\n  .. math::\n\n      F(x) = \\sum_{i=1}^{N}w_ih_i(x) + b\n\n  Args:\n    weighted_subnetworks: List of :class:`adanet.ensemble.WeightedSubnetwork`\n      instances that form this ensemble. Ordered from first to most recent.\n    bias: Bias term :class:`tf.Tensor` or dict of string to bias term\n      :class:`tf.Tensor` (for multi-head) for the ensemble\'s logits.\n    logits: Logits :class:`tf.Tensor` or dict of string to logits\n      :class:`tf.Tensor` (for multi-head). The result of the function *f* as\n      defined in Section 5.1 which is the sum of the logits of all\n      :class:`adanet.WeightedSubnetwork` instances in ensemble.\n    subnetworks: List of :class:`adanet.subnetwork.Subnetwork` instances that\n      form this ensemble. This is kept together with weighted_subnetworks for\n      legacy reasons.\n    complexity_regularization: Regularization to be added in the Adanet loss.\n\n  Returns:\n    An :class:`adanet.ensemble.Weighted` instance.\n  """"""\n\n  def __new__(cls,\n              weighted_subnetworks,\n              bias,\n              logits,\n              subnetworks=None,\n              complexity_regularization=None):\n    return super(ComplexityRegularized, cls).__new__(\n        cls,\n        weighted_subnetworks=list(weighted_subnetworks),\n        bias=bias,\n        logits=logits,\n        subnetworks=list(subnetworks or []),\n        complexity_regularization=complexity_regularization)\n\n\nclass MixtureWeightType(object):\n  """"""Mixture weight types available for learning subnetwork contributions.\n\n  The following mixture weight types are defined:\n\n  * `SCALAR`: Produces a rank 0 `Tensor` mixture weight.\n  * `VECTOR`: Produces a rank 1 `Tensor` mixture weight.\n  * `MATRIX`: Produces a rank 2 `Tensor` mixture weight.\n  """"""\n\n  SCALAR = ""scalar""\n  VECTOR = ""vector""\n  MATRIX = ""matrix""\n\n\nclass ComplexityRegularizedEnsembler(Ensembler):\n  # pyformat: disable\n  r""""""The AdaNet algorithm implemented as an :class:`adanet.ensemble.Ensembler`.\n\n  The AdaNet algorithm was introduced in the [Cortes et al. ICML 2017] paper:\n  https://arxiv.org/abs/1607.01097.\n\n  The AdaNet algorithm uses a weak learning algorithm to iteratively generate a\n  set of candidate subnetworks that attempt to minimize the loss function\n  defined in Equation (4) as part of an ensemble. At the end of each iteration,\n  the best candidate is chosen based on its ensemble\'s complexity-regularized\n  train loss. New subnetworks are allowed to use any subnetwork weights within\n  the previous iteration\'s ensemble in order to improve upon them. If the\n  complexity-regularized loss of the new ensemble, as defined in Equation (4),\n  is less than that of the previous iteration\'s ensemble, the AdaNet algorithm\n  continues onto the next iteration.\n\n  AdaNet attempts to minimize the following loss function to learn the mixture\n  weights :math:`w` of each subnetwork :math:`h` in the ensemble with\n  differentiable convex non-increasing surrogate loss function :math:`\\Phi`:\n\n  Equation (4):\n\n  .. math::\n\n      F(w) = \\frac{1}{m} \\sum_{i=1}^{m} \\Phi \\left(\\sum_{j=1}^{N}w_jh_j(x_i),\n      y_i \\right) + \\sum_{j=1}^{N} \\left(\\lambda r(h_j) + \\beta \\right) |w_j|\n\n  with :math:`\\lambda >= 0` and :math:`\\beta >= 0`.\n\n  Args:\n    optimizer: String, :class:`tf.train.Optimizer` object, or callable that\n      creates the optimizer to use for training the ensemble weights. If left\n      as :code:`None`, :meth:`tf.no_op()` is used instead.\n    mixture_weight_type: The :class:`adanet.ensemble.MixtureWeightType` defining\n      which mixture weight type to learn on top of the subnetworks\' logits.\n    mixture_weight_initializer: The initializer for mixture_weights. When\n      :code:`None`, the default is different according to\n      :code:`mixture_weight_type`:\n\n        - :code:`SCALAR` initializes to :math:`1/N` where :math:`N` is the\n          number of subnetworks in the ensemble giving a uniform average.\n        - :code:`VECTOR` initializes each entry to :math:`1/N` where :math:`N`\n          is the number of subnetworks in the ensemble giving a uniform average.\n        - :code:`MATRIX` uses :meth:`tf.zeros_initializer`.\n    warm_start_mixture_weights: Whether, at the beginning of an iteration, to\n      initialize the mixture weights of the subnetworks from the previous\n      ensemble to their learned value at the previous iteration, as opposed to\n      retraining them from scratch. Takes precedence over the value for\n      :code:`mixture_weight_initializer` for subnetworks from previous\n      iterations.\n    model_dir: The model dir to use for warm-starting mixture weights and bias\n      at the logit layer. Ignored if :code:`warm_start_mixture_weights` is\n      :code:`False`.\n    adanet_lambda: Float multiplier :math:`\\lambda` for applying :math:`L1`\n      regularization to subnetworks\' mixture weights :math:`w` in the ensemble\n      proportional to their complexity. See Equation (4) in the AdaNet paper.\n    adanet_beta: Float :math:`L1` regularization multiplier :math:`\\beta` to apply\n      equally to all subnetworks\' weights :math:`w` in the ensemble regardless of\n      their complexity. See Equation (4) in the AdaNet paper.\n    use_bias: Whether to add a bias term to the ensemble\'s logits.\n    name: Optional name for the ensembler. Defaults to \'complexity_regularized\'.\n\n  Returns:\n    An `adanet.ensemble.ComplexityRegularizedEnsembler` instance.\n\n  Raises:\n    ValueError: if :code:`warm_start_mixture_weights` is :code:`True` but\n    :code:`model_dir` is :code:`None`.\n  """"""\n  # pyformat: enable\n\n  def __init__(self,\n               optimizer=None,\n               mixture_weight_type=MixtureWeightType.SCALAR,\n               mixture_weight_initializer=None,\n               warm_start_mixture_weights=False,\n               model_dir=None,\n               adanet_lambda=0.,\n               adanet_beta=0.,\n               use_bias=False,\n               name=None):\n    if warm_start_mixture_weights:\n      if model_dir is None:\n        raise ValueError(""model_dir cannot be None when ""\n                         ""warm_start_mixture_weights is True."")\n\n    self._optimizer = optimizer\n    self._mixture_weight_type = mixture_weight_type\n    self._mixture_weight_initializer = mixture_weight_initializer\n    self._warm_start_mixture_weights = warm_start_mixture_weights\n    self._model_dir = model_dir\n    self._adanet_lambda = adanet_lambda\n    self._adanet_beta = adanet_beta\n    self._use_bias = use_bias\n    self._name = name\n\n  @property\n  def name(self):\n    if self._name:\n      return self._name\n    return ""complexity_regularized""\n\n  def build_ensemble(self,\n                     subnetworks,\n                     previous_ensemble_subnetworks,\n                     features,\n                     labels,\n                     logits_dimension,\n                     training,\n                     iteration_step,\n                     summary,\n                     previous_ensemble,\n                     previous_iteration_checkpoint=None):\n    del features, labels, logits_dimension, training, iteration_step  # unused\n    weighted_subnetworks = []\n    subnetwork_index = 0\n    num_subnetworks = len(subnetworks)\n\n    if previous_ensemble_subnetworks and previous_ensemble:\n      num_subnetworks += len(previous_ensemble_subnetworks)\n      for weighted_subnetwork in previous_ensemble.weighted_subnetworks:\n        if weighted_subnetwork.subnetwork not in previous_ensemble_subnetworks:\n          # Pruned.\n          continue\n        weight_initializer = None\n        if self._warm_start_mixture_weights:\n          if isinstance(weighted_subnetwork.subnetwork.last_layer, dict):\n            weight_initializer = {\n                key: self._load_variable(weighted_subnetwork.weight[key],\n                                         previous_iteration_checkpoint)\n                for key in sorted(weighted_subnetwork.subnetwork.last_layer)\n            }\n          else:\n            weight_initializer = self._load_variable(\n                weighted_subnetwork.weight, previous_iteration_checkpoint)\n        with tf_compat.v1.variable_scope(\n            ""weighted_subnetwork_{}"".format(subnetwork_index)):\n          weighted_subnetworks.append(\n              self._build_weighted_subnetwork(\n                  weighted_subnetwork.subnetwork,\n                  num_subnetworks,\n                  weight_initializer=weight_initializer))\n        subnetwork_index += 1\n\n    for subnetwork in subnetworks:\n      with tf_compat.v1.variable_scope(\n          ""weighted_subnetwork_{}"".format(subnetwork_index)):\n        weighted_subnetworks.append(\n            self._build_weighted_subnetwork(subnetwork, num_subnetworks))\n      subnetwork_index += 1\n\n    if previous_ensemble:\n      if len(\n          previous_ensemble.subnetworks) == len(previous_ensemble_subnetworks):\n        bias = self._create_bias_term(\n            weighted_subnetworks,\n            prior=previous_ensemble.bias,\n            previous_iteration_checkpoint=previous_iteration_checkpoint)\n      else:\n        bias = self._create_bias_term(\n            weighted_subnetworks,\n            prior=None,\n            previous_iteration_checkpoint=previous_iteration_checkpoint)\n        logging.info(""Builders using a pruned set of the subnetworks ""\n                     ""from the previous ensemble, so its ensemble\'s bias ""\n                     ""term will not be warm started with the previous ""\n                     ""ensemble\'s bias."")\n    else:\n      bias = self._create_bias_term(weighted_subnetworks)\n\n    logits = self._create_ensemble_logits(weighted_subnetworks, bias, summary)\n    complexity_regularization = 0\n    if isinstance(logits, dict):\n      for key in sorted(logits):\n        complexity_regularization += self._compute_complexity_regularization(\n            weighted_subnetworks, summary, key)\n    else:\n      complexity_regularization = self._compute_complexity_regularization(\n          weighted_subnetworks, summary)\n\n    return ComplexityRegularized(\n        weighted_subnetworks=weighted_subnetworks,\n        bias=bias,\n        subnetworks=[ws.subnetwork for ws in weighted_subnetworks],\n        logits=logits,\n        complexity_regularization=complexity_regularization)\n\n  def _load_variable(self, var, previous_iteration_checkpoint):\n    latest_checkpoint = tf.train.latest_checkpoint(self._model_dir)\n    status = previous_iteration_checkpoint.restore(latest_checkpoint)\n    try:\n      status.expect_partial().assert_nontrivial_match()\n    except AssertionError:\n      # Fall back to v1 checkpoint when not using v2 checkpoint.\n      return tf.train.load_variable(self._model_dir, tf_compat.tensor_name(var))\n    else:\n      with tf_compat.v1.Session() as sess:\n        status.initialize_or_restore(sess)\n        return sess.run(var)\n\n  def _compute_adanet_gamma(self, complexity):\n    """"""For a subnetwork, computes: lambda * r(h) + beta.""""""\n\n    if self._adanet_lambda == 0.:\n      return self._adanet_beta\n    return tf.scalar_mul(self._adanet_lambda,\n                         tf.cast(complexity,\n                                 dtype=tf.float32)) + self._adanet_beta\n\n  def _select_mixture_weight_initializer(self, num_subnetworks):\n    if self._mixture_weight_initializer:\n      return self._mixture_weight_initializer\n    if (self._mixture_weight_type == MixtureWeightType.SCALAR or\n        self._mixture_weight_type == MixtureWeightType.VECTOR):\n      return tf_compat.v1.constant_initializer(1. / num_subnetworks)\n    return tf_compat.v1.zeros_initializer()\n\n  def _build_weighted_subnetwork(self,\n                                 subnetwork,\n                                 num_subnetworks,\n                                 weight_initializer=None):\n    """"""Builds an `adanet.ensemble.WeightedSubnetwork`.\n\n    Args:\n      subnetwork: The `Subnetwork` to weight.\n      num_subnetworks: The number of subnetworks in the ensemble.\n      weight_initializer: Initializer for the weight variable.\n\n    Returns:\n      A `WeightedSubnetwork` instance.\n\n    Raises:\n      ValueError: When the subnetwork\'s last layer and logits dimension do\n        not match and requiring a SCALAR or VECTOR mixture weight.\n    """"""\n\n    if isinstance(subnetwork.last_layer, dict):\n      logits, weight = {}, {}\n      for i, key in enumerate(sorted(subnetwork.last_layer)):\n        logits[key], weight[key] = self._build_weighted_subnetwork_helper(\n            subnetwork, num_subnetworks,\n            _lookup_if_dict(weight_initializer, key), key, i)\n    else:\n      logits, weight = self._build_weighted_subnetwork_helper(\n          subnetwork, num_subnetworks, weight_initializer)\n\n    return WeightedSubnetwork(\n        subnetwork=subnetwork, logits=logits, weight=weight)\n\n  def _build_weighted_subnetwork_helper(self,\n                                        subnetwork,\n                                        num_subnetworks,\n                                        weight_initializer=None,\n                                        key=None,\n                                        index=None):\n    """"""Returns the logits and weight of the `WeightedSubnetwork` for key.""""""\n\n    # Treat subnetworks as if their weights are frozen, and ensure that\n    # mixture weight gradients do not propagate through.\n    last_layer = _lookup_if_dict(subnetwork.last_layer, key)\n    logits = _lookup_if_dict(subnetwork.logits, key)\n    weight_shape = None\n    last_layer_size = last_layer.get_shape().as_list()[-1]\n    logits_size = logits.get_shape().as_list()[-1]\n    batch_size = tf.shape(input=last_layer)[0]\n\n    if weight_initializer is None:\n      weight_initializer = self._select_mixture_weight_initializer(\n          num_subnetworks)\n      if self._mixture_weight_type == MixtureWeightType.SCALAR:\n        weight_shape = []\n      if self._mixture_weight_type == MixtureWeightType.VECTOR:\n        weight_shape = [logits_size]\n      if self._mixture_weight_type == MixtureWeightType.MATRIX:\n        weight_shape = [last_layer_size, logits_size]\n\n    with tf_compat.v1.variable_scope(\n        ""logits_{}"".format(index) if index else ""logits""):\n      weight = tf_compat.v1.get_variable(\n          name=""mixture_weight"",\n          shape=weight_shape,\n          initializer=weight_initializer)\n      if self._mixture_weight_type == MixtureWeightType.MATRIX:\n        # TODO: Add Unit tests for the ndims == 3 path.\n        ndims = len(last_layer.get_shape().as_list())\n        if ndims > 3:\n          raise NotImplementedError(\n              ""Last Layer with more than 3 dimensions are not supported with ""\n              ""matrix mixture weights."")\n        # This is reshaping [batch_size, timesteps, emb_dim ] to\n        # [batch_size x timesteps, emb_dim] for matrix multiplication\n        # and reshaping back.\n        if ndims == 3:\n          logging.info(""Rank 3 tensors like [batch_size, timesteps, d]  are ""\n                       ""reshaped to rank 2 [ batch_size x timesteps, d] for ""\n                       ""the weight matrix multiplication, and are reshaped ""\n                       ""to their original shape afterwards."")\n          last_layer = tf.reshape(last_layer, [-1, last_layer_size])\n        logits = tf.matmul(last_layer, weight)\n        if ndims == 3:\n          logits = tf.reshape(logits, [batch_size, -1, logits_size])\n      else:\n        logits = tf.multiply(logits, weight)\n    return logits, weight\n\n  def _create_bias_term(self,\n                        weighted_subnetworks,\n                        prior=None,\n                        previous_iteration_checkpoint=None):\n    """"""Returns a bias term vector.\n\n    If `use_bias` is set, then it returns a trainable bias variable initialized\n    to zero, or warm-started with the given prior. Otherwise it returns\n    a non-trainable zero variable.\n\n    Args:\n      weighted_subnetworks: List of `WeightedSubnetwork` instances that form\n        this ensemble. Ordered from first to most recent.\n      prior: Prior bias term `Tensor` of dict of string to `Tensor` (for multi-\n        head) for warm-starting the bias term variable.\n      previous_iteration_checkpoint: `tf.train.Checkpoint` for iteration t-1.\n\n    Returns:\n      A bias term `Tensor` or dict of string to bias term `Tensor` (for multi-\n        head).\n    """"""\n\n    if not isinstance(weighted_subnetworks[0].subnetwork.logits, dict):\n      return self._create_bias_term_helper(weighted_subnetworks, prior,\n                                           previous_iteration_checkpoint)\n    bias_terms = {}\n    for i, key in enumerate(sorted(weighted_subnetworks[0].subnetwork.logits)):\n      bias_terms[key] = self._create_bias_term_helper(\n          weighted_subnetworks, prior, previous_iteration_checkpoint, key, i)\n    return bias_terms\n\n  def _create_bias_term_helper(self,\n                               weighted_subnetworks,\n                               prior,\n                               previous_iteration_checkpoint,\n                               key=None,\n                               index=None):\n    """"""Returns a bias term for weights with the given key.""""""\n\n    shape = None\n    if prior is None or not self._warm_start_mixture_weights:\n      prior = tf_compat.v1.zeros_initializer()\n      logits = _lookup_if_dict(weighted_subnetworks[0].subnetwork.logits, key)\n      dims = logits.shape.as_list()\n\n      if len(dims) == 1:\n        num_dims = 1\n      else:\n        assert len(dims) == 2\n        num_dims = dims[-1]\n        assert num_dims is not None\n      shape = num_dims\n\n    else:\n      prior = self._load_variable(\n          _lookup_if_dict(prior, key), previous_iteration_checkpoint)\n    return tf_compat.v1.get_variable(\n        name=""bias_{}"".format(index) if index else ""bias"",\n        shape=shape,\n        initializer=prior,\n        trainable=self._use_bias)\n\n  def _create_ensemble_logits(self, weighted_subnetworks, bias, summary):\n    """"""Computes the AdaNet weighted ensemble logits.\n\n    Args:\n      weighted_subnetworks: List of `WeightedSubnetwork` instances that form\n        this ensemble. Ordered from first to most recent.\n      bias: Bias term `Tensor` or dict of string to `Tensor` (for multi-head)\n        for the AdaNet-weighted ensemble logits.\n      summary: A `_ScopedSummary` instance for recording ensemble summaries.\n\n    Returns:\n      A two-tuple of:\n       1. Ensemble logits `Tensor` or dict of string to logits `Tensor` (for\n         multi-head).\n       2. Ensemble complexity regularization\n    """"""\n\n    if not isinstance(weighted_subnetworks[0].subnetwork.logits, dict):\n      return self._create_ensemble_logits_helper(weighted_subnetworks, bias,\n                                                 summary)\n    logits_dict = weighted_subnetworks[0].subnetwork.logits\n    return {\n        key: self._create_ensemble_logits_helper(\n            weighted_subnetworks, bias, summary, key=key, index=i)\n        for i, key in enumerate(sorted(logits_dict))\n    }\n\n  def _create_ensemble_logits_helper(self,\n                                     weighted_subnetworks,\n                                     bias,\n                                     summary,\n                                     key=None,\n                                     index=None):\n    """"""Returns the AdaNet ensemble logits and regularization term for key.""""""\n\n    subnetwork_logits = []\n    for weighted_subnetwork in weighted_subnetworks:\n      subnetwork_logits.append(_lookup_if_dict(weighted_subnetwork.logits, key))\n    with tf_compat.v1.variable_scope(\n        ""logits_{}"".format(index) if index else ""logits""):\n      ensemble_logits = _lookup_if_dict(bias, key)\n      for logits in subnetwork_logits:\n        ensemble_logits = tf.add(ensemble_logits, logits)\n    return ensemble_logits\n\n  def _compute_complexity_regularization(self,\n                                         weighted_subnetworks,\n                                         summary,\n                                         key=None):\n    """"""Returns the AdaNet regularization term contribution for a key.""""""\n\n    ensemble_complexity_regularization = 0\n    total_weight_l1_norms = 0\n    weights = []\n    for weighted_subnetwork in weighted_subnetworks:\n      weight_l1_norm = tf.norm(\n          tensor=_lookup_if_dict(weighted_subnetwork.weight, key), ord=1)\n      total_weight_l1_norms += weight_l1_norm\n      ensemble_complexity_regularization += (\n          self._compute_complexity_regularization_helper(\n              weight_l1_norm, weighted_subnetwork.subnetwork.complexity))\n      weights.append(weight_l1_norm)\n\n    with summary.current_scope():\n      # Append a suffix for multi head summaries.\n      suffix = ""_{}"".format(_stringify(key)) if key else """"\n      summary.scalar(\n          ""complexity_regularization/adanet/adanet_weighted_ensemble"" + suffix,\n          ensemble_complexity_regularization)\n      summary.histogram(\n          ""mixture_weights/adanet/adanet_weighted_ensemble"" + suffix, weights)\n      for iteration, weight in enumerate(weights):\n        scope = ""adanet/adanet_weighted_ensemble/subnetwork{}_{}"".format(\n            suffix, iteration)\n        summary.scalar(""mixture_weight_norms/{}"".format(scope), weight)\n        fraction = weight / total_weight_l1_norms\n        summary.scalar(""mixture_weight_fractions/{}"".format(scope), fraction)\n    return ensemble_complexity_regularization\n\n  def _compute_complexity_regularization_helper(self, weight_l1_norm,\n                                                complexity):\n    """"""For a subnetwork, computes: (lambda * r(h) + beta) * |w|.""""""\n\n    # Note: Unsafe comparison against float zero.\n    if self._adanet_lambda == 0. and self._adanet_beta == 0.:\n      return tf.constant(0., name=""zero"")\n    return tf.scalar_mul(self._compute_adanet_gamma(complexity), weight_l1_norm)\n\n  def build_train_op(self, ensemble, loss, var_list, labels, iteration_step,\n                     summary, previous_ensemble):\n    del labels, iteration_step, summary, previous_ensemble  # unused\n    optimizer = self._optimizer\n    if callable(optimizer):\n      optimizer = optimizer()\n    if optimizer is None:\n      return tf.no_op()\n\n    # The AdaNet Estimator is responsible for incrementing the global step.\n    return optimizer.minimize(\n        loss=loss + ensemble.complexity_regularization, var_list=var_list)\n'"
adanet/ensemble/weighted_test.py,16,"b'""""""Test AdaNet single weighted subnetwork and ensembler implementation.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\n\nfrom absl.testing import parameterized\nfrom adanet import ensemble\nfrom adanet import subnetwork\nfrom adanet import tf_compat\nfrom adanet.core.summary import Summary\nimport mock\nimport tensorflow.compat.v2 as tf\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass _FakeCheckpoint(object):\n  """"""A fake `tf.train.Checkpoint`.""""""\n\n  def restore(self, save_path):\n    del save_path  # unused\n\n\nclass _FakeSummary(Summary):\n  """"""A fake adanet.Summary.""""""\n  scalars = collections.defaultdict(list)\n\n  def scalar(self, name, tensor, family=None):\n    self.scalars[name].append(tensor)\n    return \'fake_scalar\'\n\n  def image(self, name, tensor, max_outputs=3, family=None):\n    return \'fake_image\'\n\n  def histogram(self, name, values, family=None):\n    return \'fake_histogram\'\n\n  def audio(self, name, tensor, sample_rate, max_outputs=3, family=None):\n    return \'fake_audio\'\n\n  def clear_scalars(self):\n    self.scalars.clear()\n\n  @contextlib.contextmanager\n  def current_scope(self):\n    yield\n\n\ndef _get_norm_summary_key(subnetwork_index):\n  return (\'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_%s\' %\n          subnetwork_index)\n\n\ndef _get_fractions_summary_key(subnetwork_index):\n  return (\n      \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_%s\' %\n      subnetwork_index)\n\n\ndef _get_complexity_regularization_summary_key():\n  return \'complexity_regularization/adanet/adanet_weighted_ensemble\'\n\n\nclass ComplexityRegularizedEnsemblerTest(parameterized.TestCase,\n                                         tf.test.TestCase):\n\n  def setUp(self):\n    super(ComplexityRegularizedEnsemblerTest, self).setUp()\n\n    self._optimizer = tf_compat.v1.train.GradientDescentOptimizer(\n        learning_rate=.1)\n    self.easy_ensembler = ensemble.ComplexityRegularizedEnsembler(\n        optimizer=self._optimizer)\n\n    mock.patch.object(tf.train, \'load_variable\', autospec=False).start()\n    mock.patch.object(\n        tf.compat.v1.train, \'load_variable\', autospec=False).start()\n    mock.patch.object(\n        tf.compat.v2.train, \'load_variable\', autospec=False).start()\n    mock.patch.object(\n        ensemble.ComplexityRegularizedEnsembler, \'_load_variable\',\n        autospec=False).start()\n\n    def _load_variable(var, previous_iteration_checkpoint):\n      del var  # unused\n      assert previous_iteration_checkpoint is not None\n      return 1.0\n\n    complexity_regularized_ensembler = ensemble.ComplexityRegularizedEnsembler\n    complexity_regularized_ensembler._load_variable.side_effect = _load_variable\n\n    self.summary = _FakeSummary()\n\n  def _build_easy_ensemble(self, subnetworks):\n    return self.easy_ensembler.build_ensemble(\n        subnetworks=subnetworks,\n        previous_ensemble_subnetworks=None,\n        features=None,\n        labels=None,\n        logits_dimension=None,\n        training=None,\n        iteration_step=None,\n        summary=self.summary,\n        previous_ensemble=None,\n        previous_iteration_checkpoint=None)\n\n  def _build_subnetwork(self, multi_head=False):\n\n    last_layer = tf.Variable(\n        tf_compat.random_normal(shape=(2, 3)), trainable=False).read_value()\n\n    def new_logits():\n      return tf_compat.v1.layers.dense(\n          last_layer,\n          units=1,\n          kernel_initializer=tf_compat.v1.glorot_uniform_initializer())\n\n    if multi_head:\n      logits = {k: new_logits() for k in multi_head}\n      last_layer = {k: last_layer for k in multi_head}\n    else:\n      logits = new_logits()\n\n    return subnetwork.Subnetwork(last_layer=logits, logits=logits, complexity=2)\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'default\',\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [1],\n              _get_fractions_summary_key(0): [1],\n              _get_complexity_regularization_summary_key(): [0.],\n          },\n          \'expected_complexity_regularization\': 0.,\n      }, {\n          \'testcase_name\': \'one_previous_network\',\n          \'num_previous_ensemble_subnetworks\': 1,\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [0.5],\n              _get_norm_summary_key(1): [0.5],\n              _get_fractions_summary_key(0): [0.5],\n              _get_fractions_summary_key(1): [0.5],\n              _get_complexity_regularization_summary_key(): [0.],\n          },\n          \'expected_complexity_regularization\': 0.,\n      }, {\n          \'testcase_name\': \'one_previous_network_with_lambda\',\n          \'adanet_lambda\': 0.1,\n          \'num_previous_ensemble_subnetworks\': 1,\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [0.5],\n              _get_norm_summary_key(1): [0.5],\n              _get_fractions_summary_key(0): [0.5],\n              _get_fractions_summary_key(1): [0.5],\n              _get_complexity_regularization_summary_key(): [0.2],\n          },\n          \'expected_complexity_regularization\': 0.2,\n      }, {\n          \'testcase_name\': \'two_subnetworks_one_previous_network_with_lambda\',\n          \'adanet_lambda\': 0.1,\n          \'num_previous_ensemble_subnetworks\': 1,\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [0.5],\n              _get_norm_summary_key(1): [0.5],\n              _get_fractions_summary_key(0): [0.5],\n              _get_fractions_summary_key(1): [0.5],\n              _get_complexity_regularization_summary_key(): [0.2],\n          },\n          \'expected_complexity_regularization\': 0.2,\n      }, {\n          \'testcase_name\': \'all_previous_networks_with_lambda\',\n          \'adanet_lambda\': 0.1,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [1 / 3.],\n              _get_norm_summary_key(1): [1 / 3.],\n              _get_norm_summary_key(2): [1 / 3.],\n              _get_fractions_summary_key(0): [1 / 3.],\n              _get_fractions_summary_key(1): [1 / 3.],\n              _get_fractions_summary_key(2): [1 / 3.],\n              _get_complexity_regularization_summary_key(): [1 / 5.],\n          },\n          \'expected_complexity_regularization\': 1 / 5.,\n      }, {\n          \'testcase_name\': \'all_previous_networks_and_two_subnetworks\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              _get_norm_summary_key(0): [1 / 4.],\n              _get_norm_summary_key(1): [1 / 4.],\n              _get_norm_summary_key(2): [1 / 4.],\n              _get_norm_summary_key(3): [1 / 4.],\n              _get_fractions_summary_key(0): [1 / 4.],\n              _get_fractions_summary_key(1): [1 / 4.],\n              _get_fractions_summary_key(2): [1 / 4.],\n              _get_fractions_summary_key(3): [1 / 4.],\n              _get_complexity_regularization_summary_key(): [1 / 5.],\n          },\n          \'expected_complexity_regularization\': 1 / 5.,\n      }, {\n          \'testcase_name\': \'all_nets_and_string_multihead\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'multi_head\': [\'head1\', \'head2\'],\n          \'use_bias\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_head2\':\n                  [0.2],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_head1\':\n                  [0.2],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_3\':\n                  [0.25]\n          },\n          \'expected_complexity_regularization\': 2 / 5.,\n      }, {\n          \'testcase_name\': \'all_nets_and_string_tuple_multihead\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'multi_head\': [(\'bar\', \'baz\'), (\'foo\', \'bar\')],\n          \'use_bias\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_baz_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_0\':\n                  [0.25],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_foo_bar\':\n                  [0.2],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_bar_baz\':\n                  [0.2],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_bar_0\':\n                  [0.25]\n          },\n          \'expected_complexity_regularization\': 2 / 5.,\n      }, {\n          \'testcase_name\': \'all_nets_and_tuple_multihead\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'multi_head\': [(\'bar\', 0), (\'foo\', 1)],\n          \'use_bias\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_0_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_1_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_1_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_1_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_foo_1_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_0_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_0_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_bar_0_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_0_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_0_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_1_3\':\n                  [0.25],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_bar_0\':\n                  [0.2],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_1_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_1_0\':\n                  [0.25],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_foo_1\':\n                  [0.2],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_0_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_bar_0_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_foo_1_2\':\n                  [0.25]\n          },\n          \'expected_complexity_regularization\': 2 / 5.,\n      }, {\n          \'testcase_name\': \'all_nets_and_number_multihead\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'multi_head\': [0, 1],\n          \'use_bias\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1_3\':\n                  [0.25],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_1\':\n                  [0.2],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1_1\':\n                  [0.25],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble\':\n                  [0.2],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_0\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_3\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1_0\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1_1\':\n                  [0.25]\n          },\n          \'expected_complexity_regularization\': 2 / 5.,\n      }, {\n          \'testcase_name\': \'all_nets_with_warm_start\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'warm_start_mixture_weights\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_2\':\n                  [0.1],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_3\':\n                  [0.1],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble\':\n                  [0.5],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_1\':\n                  [0.4],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_2\':\n                  [0.25],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_0\':\n                  [0.4],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_0\':\n                  [1.0],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_1\':\n                  [1.0],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_3\':\n                  [0.25]\n          },\n          \'expected_complexity_regularization\': 1 / 2.,\n      }, {\n          \'testcase_name\': \'all_nets_with_warm_start_and_multihead\',\n          \'num_subnetworks\': 2,\n          \'adanet_lambda\': 0.1,\n          \'multi_head\': [\'head1\', \'head2\'],\n          \'use_bias\': True,\n          \'warm_start_mixture_weights\': True,\n          \'num_previous_ensemble_subnetworks\': 2,\n          \'expected_summary_scalars\': {\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_head2\':\n                  [0.5],\n              \'complexity_regularization/adanet/adanet_weighted_ensemble_head1\':\n                  [0.5],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_1\':\n                  [1.0],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_0\':\n                  [1.0],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head2_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_2\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_3\':\n                  [0.25],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_0\':\n                  [1.0],\n              \'mixture_weight_norms/adanet/adanet_weighted_ensemble/subnetwork_head1_1\':\n                  [1.0],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_2\':\n                  [0.1],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_3\':\n                  [0.1],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_0\':\n                  [0.4],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head1_1\':\n                  [0.4],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_1\':\n                  [0.4],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_0\':\n                  [0.4],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_3\':\n                  [0.1],\n              \'mixture_weight_fractions/adanet/adanet_weighted_ensemble/subnetwork_head2_2\':\n                  [0.1]\n          },\n          \'expected_complexity_regularization\': 1.,\n          \'name\': \'with_bias\',\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_ensemble(self,\n                          mixture_weight_type=ensemble.MixtureWeightType.SCALAR,\n                          mixture_weight_initializer=None,\n                          warm_start_mixture_weights=False,\n                          adanet_lambda=0.,\n                          adanet_beta=0.,\n                          multi_head=None,\n                          use_bias=False,\n                          num_subnetworks=1,\n                          num_previous_ensemble_subnetworks=0,\n                          expected_complexity_regularization=0.,\n                          expected_summary_scalars=None,\n                          name=None):\n    with context.graph_mode():\n      model_dir = None\n      if warm_start_mixture_weights:\n        model_dir = \'fake_checkpoint_dir\'\n      ensembler = ensemble.ComplexityRegularizedEnsembler(\n          optimizer=self._optimizer,\n          mixture_weight_type=mixture_weight_type,\n          mixture_weight_initializer=mixture_weight_initializer,\n          warm_start_mixture_weights=warm_start_mixture_weights,\n          model_dir=model_dir,\n          adanet_lambda=adanet_lambda,\n          adanet_beta=adanet_beta,\n          use_bias=use_bias,\n          name=name)\n\n      if name:\n        self.assertEqual(ensembler.name, name)\n      else:\n        self.assertEqual(ensembler.name, \'complexity_regularized\')\n\n      with tf_compat.v1.variable_scope(\'dummy_adanet_scope_iteration_0\'):\n        previous_ensemble_subnetworks_all = [\n            self._build_subnetwork(multi_head),\n            self._build_subnetwork(multi_head)\n        ]\n\n        previous_ensemble = self._build_easy_ensemble(\n            previous_ensemble_subnetworks_all)\n\n      with tf_compat.v1.variable_scope(\'dummy_adanet_scope_iteration_1\'):\n        subnetworks_pool = [\n            self._build_subnetwork(multi_head),\n            self._build_subnetwork(multi_head),\n        ]\n\n        subnetworks = subnetworks_pool[:num_subnetworks]\n\n        previous_ensemble_subnetworks = previous_ensemble_subnetworks_all[:(\n            num_previous_ensemble_subnetworks)]\n\n        self.summary.clear_scalars()\n\n        built_ensemble = ensembler.build_ensemble(\n            subnetworks=subnetworks,\n            previous_ensemble_subnetworks=previous_ensemble_subnetworks,\n            features=None,\n            labels=None,\n            logits_dimension=None,\n            training=None,\n            iteration_step=None,\n            summary=self.summary,\n            previous_ensemble=previous_ensemble,\n            previous_iteration_checkpoint=_FakeCheckpoint())\n\n        with self.test_session() as sess:\n          sess.run(tf_compat.v1.global_variables_initializer())\n\n          summary_scalars, complexity_regularization = sess.run(\n              (self.summary.scalars, built_ensemble.complexity_regularization))\n\n          if expected_summary_scalars:\n            for key in expected_summary_scalars.keys():\n              print(summary_scalars)\n              self.assertAllClose(expected_summary_scalars[key],\n                                  summary_scalars[key])\n\n          self.assertEqual(\n              [l.subnetwork for l in built_ensemble.weighted_subnetworks],\n              previous_ensemble_subnetworks + subnetworks)\n\n          self.assertAllClose(expected_complexity_regularization,\n                              complexity_regularization)\n          self.assertIsNotNone(sess.run(built_ensemble.logits))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_ensemble_subnetwork_has_scalar_logits(self):\n    with context.graph_mode():\n      logits = tf.ones(shape=(100,))\n      ensemble_spec = self._build_easy_ensemble([\n          subnetwork.Subnetwork(\n              last_layer=logits, logits=logits, complexity=0.)\n      ])\n      self.assertEqual([1], ensemble_spec.bias.shape.as_list())\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_train_op_no_op(self):\n    with context.graph_mode():\n      train_op = ensemble.ComplexityRegularizedEnsembler().build_train_op(\n          *[None] * 7)  # arguments unused\n      self.assertEqual(train_op.type, tf.no_op().type)\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_train_op_callable_optimizer(self):\n    with context.graph_mode():\n      dummy_weight = tf.Variable(0., name=\'dummy_weight\')\n      dummy_loss = dummy_weight * 2.\n      ensembler = ensemble.ComplexityRegularizedEnsembler(\n          optimizer=lambda: tf_compat.v1.train.GradientDescentOptimizer(.1))\n      train_op = ensembler.build_train_op(\n          self._build_easy_ensemble([self._build_subnetwork()]),\n          dummy_loss, [dummy_weight],\n          labels=None,\n          iteration_step=None,\n          summary=None,\n          previous_ensemble=None)\n      config = tf.compat.v1.ConfigProto(\n          gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n      with tf_compat.v1.Session(config=config) as sess:\n        sess.run(tf_compat.v1.global_variables_initializer())\n        sess.run(train_op)\n        self.assertAllClose(-.2, sess.run(dummy_weight))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_build_train_op(self):\n    with context.graph_mode():\n      dummy_weight = tf.Variable(0., name=\'dummy_weight\')\n      dummy_loss = dummy_weight * 2.\n      ensembler = ensemble.ComplexityRegularizedEnsembler(\n          optimizer=tf_compat.v1.train.GradientDescentOptimizer(.1))\n      train_op = ensembler.build_train_op(\n          self._build_easy_ensemble([self._build_subnetwork()]),\n          dummy_loss, [dummy_weight],\n          labels=None,\n          iteration_step=None,\n          summary=None,\n          previous_ensemble=None)\n      config = tf.compat.v1.ConfigProto(\n          gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n      with tf_compat.v1.Session(config=config) as sess:\n        sess.run(tf_compat.v1.global_variables_initializer())\n        sess.run(train_op)\n        self.assertAllClose(-.2, sess.run(dummy_weight))\n\n  def tearDown(self):\n    self.summary.clear_scalars()\n    mock.patch.stopall()\n    tf_compat.v1.reset_default_graph()\n    super(ComplexityRegularizedEnsemblerTest, self).tearDown()\n\n\nif __name__ == \'__main__\':\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/examples/__init__.py,0,"b'""""""Some examples using AdaNet.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n'"
adanet/examples/simple_dnn.py,12,"b'""""""A simple dense neural network search space.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport adanet\nfrom six.moves import range\nimport tensorflow.compat.v2 as tf\n\n_NUM_LAYERS_KEY = ""num_layers""\n\n\nclass _SimpleDNNBuilder(adanet.subnetwork.Builder):\n  """"""Builds a DNN subnetwork for AdaNet.""""""\n\n  def __init__(self, feature_columns, optimizer, layer_size, num_layers,\n               learn_mixture_weights, dropout, seed):\n    """"""Initializes a `_DNNBuilder`.\n\n    Args:\n      feature_columns: An iterable containing all the feature columns used by\n        the model. All items in the set should be instances of classes derived\n        from `FeatureColumn`.\n      optimizer: An `Optimizer` instance for training both the subnetwork and\n        the mixture weights.\n      layer_size: The number of nodes to output at each hidden layer.\n      num_layers: The number of hidden layers.\n      learn_mixture_weights: Whether to solve a learning problem to find the\n        best mixture weights, or use their default value according to the\n        mixture weight type. When `False`, the subnetworks will return a no_op\n        for the mixture weight train op.\n      dropout: The dropout rate, between 0 and 1. E.g. ""rate=0.1"" would drop out\n        10% of input units.\n      seed: A random seed.\n\n    Returns:\n      An instance of `_DNNBuilder`.\n    """"""\n\n    self._feature_columns = feature_columns\n    self._optimizer = optimizer\n    self._layer_size = layer_size\n    self._num_layers = num_layers\n    self._learn_mixture_weights = learn_mixture_weights\n    self._dropout = dropout\n    self._seed = seed\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    input_layer = tf.compat.v1.feature_column.input_layer(\n        features=features, feature_columns=self._feature_columns)\n    last_layer = input_layer\n    for _ in range(self._num_layers):\n      last_layer = tf.compat.v1.layers.dense(\n          last_layer,\n          units=self._layer_size,\n          activation=tf.nn.relu,\n          kernel_initializer=tf.compat.v1.glorot_uniform_initializer(\n              seed=self._seed))\n      last_layer = tf.compat.v1.layers.dropout(\n          last_layer, rate=self._dropout, seed=self._seed, training=training)\n    logits = tf.compat.v1.layers.dense(\n        last_layer,\n        units=logits_dimension,\n        kernel_initializer=tf.compat.v1.glorot_uniform_initializer(\n            seed=self._seed))\n\n    # Approximate the Rademacher complexity of this subnetwork as the square-\n    # root of its depth.\n    complexity = tf.sqrt(tf.cast(self._num_layers, dtype=tf.float32))\n\n    with tf.name_scope(""""):\n      summary.scalar(""complexity"", complexity)\n      summary.scalar(""num_layers"", self._num_layers)\n\n    shared = {_NUM_LAYERS_KEY: self._num_layers}\n    return adanet.Subnetwork(\n        last_layer=last_layer,\n        logits=logits,\n        complexity=complexity,\n        shared=shared)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    # NOTE: The `adanet.Estimator` increments the global step.\n    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      return self._optimizer.minimize(loss=loss, var_list=var_list)\n\n  # TODO: Delete deprecated build_mixture_weights_train_op method.\n  # Use adanet.ensemble.Ensembler instead.\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    if not self._learn_mixture_weights:\n      return tf.no_op(""mixture_weights_train_op"")\n\n    # NOTE: The `adanet.Estimator` increments the global step.\n    return self._optimizer.minimize(loss=loss, var_list=var_list)\n\n  @property\n  def name(self):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    if self._num_layers == 0:\n      # A DNN with no hidden layers is a linear model.\n      return ""linear""\n    return ""{}_layer_dnn"".format(self._num_layers)\n\n\nclass Generator(adanet.subnetwork.Generator):\n  """"""Generates a two DNN subnetworks at each iteration.\n\n  The first DNN has an identical shape to the most recently added subnetwork\n  in `previous_ensemble`. The second has the same shape plus one more dense\n  layer on top. This is similar to the adaptive network presented in Figure 2 of\n  [Cortes et al. ICML 2017](https://arxiv.org/abs/1607.01097), without the\n  connections to hidden layers of networks from previous iterations.\n  """"""\n\n  def __init__(self,\n               feature_columns,\n               optimizer,\n               layer_size=32,\n               initial_num_layers=0,\n               learn_mixture_weights=False,\n               dropout=0.,\n               seed=None):\n    """"""Initializes a DNN `Generator`.\n\n    Args:\n      feature_columns: An iterable containing all the feature columns used by\n        DNN models. All items in the set should be instances of classes derived\n        from `FeatureColumn`.\n      optimizer: An `Optimizer` instance for training both the subnetwork and\n        the mixture weights.\n      layer_size: Number of nodes in each hidden layer of the subnetwork\n        candidates. Note that this parameter is ignored in a DNN with no hidden\n        layers.\n      initial_num_layers: Minimum number of layers for each DNN subnetwork. At\n        iteration 0, the subnetworks will be `initial_num_layers` deep.\n        Subnetworks at subsequent iterations will be at least as deep.\n      learn_mixture_weights: Whether to solve a learning problem to find the\n        best mixture weights, or use their default value according to the\n        mixture weight type. When `False`, the subnetworks will return a no_op\n        for the mixture weight train op.\n      dropout: The dropout rate, between 0 and 1. E.g. ""rate=0.1"" would drop out\n        10% of input units.\n      seed: A random seed.\n\n    Returns:\n      An instance of `Generator`.\n\n    Raises:\n      ValueError: If feature_columns is empty.\n      ValueError: If layer_size < 1.\n      ValueError: If initial_num_layers < 0.\n    """"""\n\n    if not feature_columns:\n      raise ValueError(""feature_columns must not be empty"")\n\n    if layer_size < 1:\n      raise ValueError(""layer_size must be >= 1"")\n\n    if initial_num_layers < 0:\n      raise ValueError(""initial_num_layers must be >= 0"")\n\n    self._initial_num_layers = initial_num_layers\n    self._dnn_builder_fn = functools.partial(\n        _SimpleDNNBuilder,\n        feature_columns=feature_columns,\n        optimizer=optimizer,\n        layer_size=layer_size,\n        learn_mixture_weights=learn_mixture_weights,\n        dropout=dropout,\n        seed=seed)\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports):\n    """"""See `adanet.subnetwork.Generator`.""""""\n\n    num_layers = self._initial_num_layers\n    if previous_ensemble:\n      num_layers = previous_ensemble.weighted_subnetworks[-1].subnetwork.shared[\n          _NUM_LAYERS_KEY]\n    return [\n        self._dnn_builder_fn(num_layers=num_layers),\n        self._dnn_builder_fn(num_layers=num_layers + 1),\n    ]\n'"
adanet/examples/simple_dnn_test.py,21,"b'""""""Tests for a simple dense neural network search space.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport adanet\nfrom adanet.examples import simple_dnn\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass _FakeEnsemble(object):\n  """"""A fake ensemble of one subnetwork.""""""\n\n  def __init__(self, num_layers):\n    shared_tensors = {""num_layers"": num_layers}\n    self._weighted_subnetworks = [\n        adanet.WeightedSubnetwork(\n            name=None,\n            iteration_number=None,\n            weight=None,\n            logits=None,\n            subnetwork=adanet.Subnetwork(\n                last_layer=[1],\n                logits=[1],\n                complexity=1,\n                shared=shared_tensors))\n    ]\n\n  @property\n  def weighted_subnetworks(self):\n    return self._weighted_subnetworks\n\n\nclass GeneratorTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""defaults"",\n      ""want_names"": [""linear"", ""1_layer_dnn""],\n      ""want_subnetwork_losses"": [.871, .932],\n      ""want_mixture_weight_losses"": [.871, .932],\n      ""want_complexities"": [0., 1.],\n  }, {\n      ""testcase_name"": ""learn_mixture_weights"",\n      ""learn_mixture_weights"": True,\n      ""want_names"": [""linear"", ""1_layer_dnn""],\n      ""want_subnetwork_losses"": [.871, .932],\n      ""want_mixture_weight_losses"": [.842, .892],\n      ""want_complexities"": [0., 1.],\n  }, {\n      ""testcase_name"": ""one_initial_num_layers"",\n      ""initial_num_layers"": 1,\n      ""want_names"": [""1_layer_dnn"", ""2_layer_dnn""],\n      ""want_subnetwork_losses"": [.932, .660],\n      ""want_mixture_weight_losses"": [.932, .660],\n      ""want_complexities"": [1., 1.414],\n  }, {\n      ""testcase_name"": ""previous_ensemble"",\n      ""previous_ensemble"": _FakeEnsemble(1),\n      ""want_names"": [""1_layer_dnn"", ""2_layer_dnn""],\n      ""want_subnetwork_losses"": [.932, .660],\n      ""want_mixture_weight_losses"": [.932, .660],\n      ""want_complexities"": [1., 1.414],\n  })\n  @test_util.run_in_graph_and_eager_modes\n  def test_generate_candidates(self,\n                               want_names,\n                               want_subnetwork_losses,\n                               want_mixture_weight_losses,\n                               want_complexities,\n                               learn_mixture_weights=False,\n                               initial_num_layers=0,\n                               previous_ensemble=None):\n    feature_columns = [tf.feature_column.numeric_column(""x"")]\n    generator = simple_dnn.Generator(\n        feature_columns=feature_columns,\n        optimizer=tf.compat.v1.train.GradientDescentOptimizer(.1),\n        layer_size=3,\n        initial_num_layers=initial_num_layers,\n        learn_mixture_weights=learn_mixture_weights,\n        seed=42)\n    with context.graph_mode(), tf.Graph().as_default() as g:\n      iteration_step = tf.compat.v1.train.create_global_step()\n      features = {""x"": [[1.], [2.]]}\n      labels = tf.constant([[0.], [1.]])\n      names = []\n      subnetwork_losses = []\n      mixture_weight_losses = []\n      complexities = []\n      for builder in generator.generate_candidates(\n          previous_ensemble,\n          # The following arguments are not used by\n          # simple_dnn.BuilderGenerator\'s generate_candidates.\n          iteration_number=0,\n          previous_ensemble_reports=[],\n          all_reports=[]):\n        names.append(builder.name)\n\n        # 1. Build subnetwork graph.\n        subnetwork = builder.build_subnetwork(\n            features,\n            logits_dimension=1,\n            training=True,\n            iteration_step=iteration_step,\n            summary=tf.summary,\n            previous_ensemble=previous_ensemble)\n\n        # 2. Build subnetwork train ops.\n        subnetwork_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                logits=subnetwork.logits, labels=labels))\n        subnetwork_train_op = builder.build_subnetwork_train_op(\n            subnetwork,\n            subnetwork_loss,\n            var_list=None,\n            labels=labels,\n            iteration_step=iteration_step,\n            summary=tf.summary,\n            previous_ensemble=None)\n\n        # 3. Build mixture weight train ops.\n\n        # Stop gradients since mixture weights should have not propagate\n        # beyond top layer.\n        subnetwork_logits = tf.stop_gradient(subnetwork.logits)\n\n        # Mixture weight will initialize to a one-valued scalar.\n        mixture_weight_logits = tf.compat.v1.layers.dense(\n            subnetwork_logits,\n            units=1,\n            use_bias=False,\n            kernel_initializer=tf.ones_initializer())\n        mixture_weight_loss = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                logits=mixture_weight_logits, labels=labels))\n        mixture_weight_train_op = builder.build_mixture_weights_train_op(\n            mixture_weight_loss,\n            var_list=None,\n            labels=labels,\n            logits=mixture_weight_logits,\n            iteration_step=iteration_step,\n            summary=tf.summary)\n\n        with self.test_session(graph=g) as sess:\n          sess.run(tf.compat.v1.global_variables_initializer())\n          sess.run(subnetwork_train_op)\n          sess.run(mixture_weight_train_op)\n          subnetwork_losses.append(sess.run(subnetwork_loss))\n          mixture_weight_losses.append(sess.run(mixture_weight_loss))\n          complexities.append(sess.run(subnetwork.complexity))\n\n    self.assertEqual(want_names, names)\n    self.assertAllClose(want_subnetwork_losses, subnetwork_losses, atol=1e-3)\n    self.assertAllClose(\n        want_mixture_weight_losses, mixture_weight_losses, atol=1e-3)\n    self.assertAllClose(want_complexities, complexities, atol=1e-3)\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""empty_feature_columns"",\n      ""feature_columns"": [],\n  }, {\n      ""testcase_name"": ""zero_layer_size"",\n      ""feature_columns"": [tf.feature_column.numeric_column(""x"")],\n      ""layer_size"": 0,\n  }, {\n      ""testcase_name"": ""negative_initial_num_layers"",\n      ""feature_columns"": [tf.feature_column.numeric_column(""x"")],\n      ""initial_num_layers"": -1,\n  })\n  def test_constructor_errors(self,\n                              feature_columns,\n                              layer_size=3,\n                              initial_num_layers=0):\n    with self.assertRaises(ValueError):\n      simple_dnn.Generator(\n          feature_columns=feature_columns,\n          optimizer=tf.compat.v1.train.GradientDescentOptimizer(.1),\n          layer_size=layer_size,\n          initial_num_layers=initial_num_layers)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/experimental/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet experimental directory.""""""\n\nfrom adanet.experimental import controllers\nfrom adanet.experimental import keras\nfrom adanet.experimental import phases\nfrom adanet.experimental import schedulers\nfrom adanet.experimental import storages\nfrom adanet.experimental import work_units\n\n\n__all__ = [\n    ""controllers"",\n    ""keras"",\n    ""phases"",\n    ""schedulers"",\n    ""storages"",\n    ""work_units"",\n]\n'"
adanet/pip_package/setup.py,0,"b'# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Setup for pip package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet import version\nimport setuptools\n\n# Can\'t import the module during setup.py.\n# Use execfile to find __version__.\nwith open(\'adanet/version.py\') as in_file:\n  exec(in_file.read())\n\nREQUIRED_PACKAGES = [\n    \'absl-py>=0.7,<1.0\',\n    \'six>=1.11,<2.0\',\n    \'numpy>=1.15,<2.0\',\n    \'nose>=1.3,<2.0\',\n    \'rednose>=1.3,<2.0\',\n    \'coverage>=4.5,<5.0\',\n    \'protobuf>=3.6,<4.0\',\n    \'mock>=3.0,<4.0\',\n]\n\nsetuptools.setup(\n    name=\'adanet\',  # Automatic: adanet, etc. Case insensitive.\n    version=version.__version__.replace(\'-\', \'\'),\n    description=(\n        \'adanet is a lightweight and scalable TensorFlow AutoML framework for \'\n        \'training and deploying adaptive neural networks using the AdaNet \'\n        \'algorithm [Cortes et al. ICML 2017](https://arxiv.org/abs/1607.01097).\'\n    ),\n    long_description=\'\',\n    url=\'https://github.com/tensorflow/adanet\',\n    author=\'Google LLC\',\n    install_requires=REQUIRED_PACKAGES,\n    packages=setuptools.find_packages(),\n    # PyPI package information.\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n    license=\'Apache 2.0\',\n    keywords=(\'tensorflow machine learning automl module subgraph framework \'\n              \'ensemble neural network adaptive metalearning\'),\n)\n'"
adanet/replay/__init__.py,0,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Defines mechanisms for deterministically replaying an AdaNet model search.""""""\n\n# TODO: Add more detailed documentation.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\n\nimport tensorflow.compat.v1 as tf\n\n\nclass Config(object):  # pylint: disable=g-classes-have-attributes\n  # pyformat: disable\n  """"""Defines how to deterministically replay an AdaNet model search.\n\n  Specifically, it reconstructs the previous model and trains its components\n  in the correct order without performing any search.\n\n  Args:\n    best_ensemble_indices: A list of the best ensemble indices (one per\n      iteration).\n\n  Returns:\n    An :class:`adanet.replay.Config` instance.\n  """"""\n  # pyformat: enable\n\n  def __init__(self, best_ensemble_indices=None):\n    self._best_ensemble_indices = best_ensemble_indices\n\n  @property\n  def best_ensemble_indices(self):\n    """"""The best ensemble indices per iteration.""""""\n    return self._best_ensemble_indices\n\n  def get_best_ensemble_index(self, iteration_number):\n    """"""Returns the best ensemble index given an iteration number.""""""\n    # If we are provided the list\n    if (self._best_ensemble_indices\n        and iteration_number < len(self._best_ensemble_indices)):\n      return self._best_ensemble_indices[iteration_number]\n\n    return None\n\n\n__all__ = [""Config""]\n'"
adanet/subnetwork/__init__.py,0,"b'# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Low-level APIs for defining custom subnetworks and search spaces.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.subnetwork.generator import Builder\nfrom adanet.subnetwork.generator import Generator\nfrom adanet.subnetwork.generator import SimpleGenerator\nfrom adanet.subnetwork.generator import Subnetwork\nfrom adanet.subnetwork.generator import TrainOpSpec\nfrom adanet.subnetwork.report import MaterializedReport\nfrom adanet.subnetwork.report import Report\n\n__all__ = [\n    ""Builder"",\n    ""Generator"",\n    ""MaterializedReport"",\n    ""Report"",\n    ""SimpleGenerator"",\n    ""Subnetwork"",\n    ""TrainOpSpec"",\n]\n'"
adanet/subnetwork/generator.py,27,"b'""""""An AdaNet subnetwork definition in Tensorflow using a single graph.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport collections\n\nimport six\n\n\ndef _validate_nested_persisted_tensors(persisted_tensors):\n  """"""Raises a ValueError when a nested dict is empty in persisted_tensors.""""""\n\n  for key, entry in persisted_tensors.items():\n    if not isinstance(entry, dict):\n      continue\n    if not entry:\n      raise ValueError(""Got empty nested dictionary for key: \'{}\'"".format(key))\n    _validate_nested_persisted_tensors(entry)\n\n\nclass TrainOpSpec(\n    collections.namedtuple(""TrainOpSpec"",\n                           [""train_op"", ""chief_hooks"", ""hooks""])):\n  """"""A data structure for specifying training operations.\n\n  Args:\n    train_op: Op for the training step.\n    chief_hooks: Iterable of :class:`tf.train.SessionRunHook` objects to run on\n      the chief worker during training.\n    hooks: Iterable of :class:`tf.train.SessionRunHook` objects to run on all\n      workers during training.\n\n  Returns:\n    A :class:`adanet.subnetwork.TrainOpSpec` object.\n  """"""\n\n  def __new__(cls, train_op, chief_hooks=None, hooks=None):\n    # Make hooks immutable.\n    chief_hooks = tuple(chief_hooks) if chief_hooks else ()\n    hooks = tuple(hooks) if hooks else ()\n    return super(TrainOpSpec, cls).__new__(cls, train_op, chief_hooks, hooks)\n\n\nclass Subnetwork(\n    collections.namedtuple(""Subnetwork"", [\n        ""last_layer"", ""logits"", ""complexity"", ""persisted_tensors"", ""shared"",\n        ""local_init_ops""\n    ])):\n  # pyformat: disable\n  """"""An AdaNet subnetwork.\n\n  In the AdaNet paper, an :class:`adanet.subnetwork.Subnetwork` is are called a\n  *subnetwork*, and indicated by *h*. A collection of weighted subnetworks form\n  an AdaNet ensemble.\n\n  Args:\n    last_layer: :class:`tf.Tensor` output or dict of string to\n      :class:`tf.Tensor` outputs (for multi-head) of the last layer of the\n      subnetwork, i.e the layer before the logits layer. When the mixture weight\n      type is :class:`MATRIX`, the AdaNet algorithm takes care of computing\n      ensemble mixture weights matrices (one per subnetwork) that multiply the\n      various last layers of the ensemble\'s subnetworks, and regularize them\n      using their subnetwork\'s complexity. This field is represented by *h* in\n      the AdaNet paper.\n    logits: :class:`tf.Tensor` logits or dict of string to :class:`tf.Tensor`\n      logits (for multi-head) for training the subnetwork. These logits are not\n      used in the ensemble\'s outputs if the mixture weight type is\n      :class:`MATRIX`, instead AdaNet learns its own logits (mixture weights)\n      from the subnetwork\'s `last_layers` with complexity regularization. The\n      logits are used in the ensemble only when the mixture weights type is\n      :class:`SCALAR` or :class:`VECTOR`. Even though the logits are not used\n      in the ensemble in some cases, they should always be supplied as adanet\n      uses the logits to train the subnetworks.\n    complexity: A scalar :class:`tf.Tensor` representing the complexity of the\n      subnetwork\'s architecture. It is used for choosing the best subnetwork at\n      each iteration, and for regularizing the weighted outputs of more complex\n      subnetworks.\n    persisted_tensors: DEPRECATED. See `shared`. Optional nested dictionary of\n      string to :class:`tf.Tensor` to persist across iterations. At the end of\n      an iteration, the :class:`tf.Tensor` instances will be available to\n      subnetworks in the next iterations, whereas others that are not part of\n      the `Subnetwork` will be pruned. This allows later\n      :class:`adanet.subnetwork.Subnetwork` instances to dynamically build\n      upon arbitrary :class:`tf.Tensors` from previous\n      :class:`adanet.subnetwork.Subnetwork` instances.\n    shared: Optional Python object(s), primitive(s), or function(s) to share\n      with subnetworks within the same iteration or in future iterations.\n    local_init_ops: Iterable of :class:`tf.Operation` objects to run to\n      initialize local variables.\n\n  Returns:\n    A validated :class:`adanet.subnetwork.Subnetwork` object.\n\n  Raises:\n    ValueError: If last_layer is None.\n    ValueError: If logits is None.\n    ValueError: If logits is a dict but last_layer is not.\n    ValueError: If last_layer is a dict but logits is not.\n    ValueError: If complexity is None.\n    ValueError: If persisted_tensors is present but not a dictionary.\n    ValueError: If persisted_tensors contains an empty nested dictionary.\n  """"""\n  # pyformat: enable\n\n  # Import here to avoid strict BUILD deps check.\n  from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import,g-import-not-at-top\n\n  @deprecation.deprecated_args(\n      None, ""`persisted_tensors` is deprecated, please use `shared` instead."",\n      ""persisted_tensors"")\n  def __new__(cls,\n              last_layer,\n              logits,\n              complexity,\n              persisted_tensors=None,\n              shared=None,\n              local_init_ops=None):\n    if last_layer is None:\n      raise ValueError(""last_layer not provided"")\n    if logits is None:\n      raise ValueError(""logits not provided"")\n    if isinstance(logits, dict) and not isinstance(last_layer, dict):\n      raise ValueError(""if logits is a dict last_layer must also be a dict"")\n    if isinstance(last_layer, dict) and not isinstance(logits, dict):\n      raise ValueError(""if last_layer is a dict logits must also be a dict"")\n    if complexity is None:\n      raise ValueError(""complexity not provided"")\n    if persisted_tensors is not None:\n      if not isinstance(persisted_tensors, dict):\n        raise ValueError(""persisted_tensors must be a dict"")\n      _validate_nested_persisted_tensors(persisted_tensors)\n    local_init_ops = tuple(local_init_ops) if local_init_ops else ()\n    return super(Subnetwork, cls).__new__(\n        cls,\n        last_layer=last_layer,\n        logits=logits,\n        complexity=complexity,\n        persisted_tensors=persisted_tensors,\n        shared=shared,\n        local_init_ops=local_init_ops)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Builder(object):\n  """"""Interface for a subnetwork builder.\n\n  Given features, labels, and the best ensemble of subnetworks at iteration\n  t-1, a `Builder` creates a `Subnetwork` to add to a candidate\n  ensemble at iteration t. These candidate ensembles are evaluated against one\n  another at the end of the iteration, and the best one is selected based on its\n  complexity-regularized loss.\n  """"""\n\n  @abc.abstractproperty\n  def name(self):\n    r""""""Returns the unique name of this subnetwork within an iteration.\n\n    Returns:\n      String name of this subnetwork.\n    """"""\n\n    # TODO: Validate name matches ^[A-Za-z0-9_.\\\\-/]*$\n\n  @abc.abstractmethod\n  def build_subnetwork(self,\n                       features,\n                       labels,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    # pyformat: disable\n    """"""Returns the candidate `Subnetwork` to add to the ensemble.\n\n    This method will be called only once before\n    :meth:`build_subnetwork_train_op`. This method should construct the\n    candidate subnetwork\'s graph operations and variables.\n\n    Accessing the global step via :meth:`tf.train.get_or_create_global_step()`\n    or :meth:`tf.train.get_global_step()` within this scope will return an\n    incrementable iteration step since the beginning of the iteration.\n\n    Args:\n      features: Input `dict` of :class:`tf.Tensor` objects.\n      labels: Labels :class:`tf.Tensor` or a dictionary of string label name to\n        :class:`tf.Tensor` (for multi-head). Can be `None`.\n      logits_dimension: Size of the last dimension of the logits\n        :class:`tf.Tensor`. Typically, logits have for shape `[batch_size,\n        logits_dimension]`.\n      training: A python boolean indicating whether the graph is in training\n        mode or prediction mode.\n      iteration_step: Integer :class:`tf.Tensor` representing the step since the\n        beginning of the current iteration, as opposed to the global step.\n      summary: An :class:`adanet.Summary` for scoping summaries to individual\n        subnetworks in Tensorboard. Using :meth:`tf.summary` within this scope\n        will use this :class:`adanet.Summary` under the hood.\n      previous_ensemble: The best :class:`adanet.Ensemble` from iteration t-1.\n        The created subnetwork will extend the previous ensemble to form the\n        :class:`adanet.Ensemble` at iteration t.\n\n    Returns:\n      An :class:`adanet.subnetwork.Subnetwork` instance.\n    """"""\n    # pyformat: enable\n\n  @abc.abstractmethod\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    """"""Returns an op for training a new subnetwork.\n\n    This method will be called once after :meth:`build_subnetwork`.\n\n    Accessing the global step via :meth:`tf.train.get_or_create_global_step()`\n    or\n    :meth:`tf.train.get_global_step()` within this scope will return an\n    incrementable\n    iteration step since the beginning of the iteration.\n\n    Args:\n      subnetwork: Newest subnetwork, that is not part of the\n        `previous_ensemble`.\n      loss: A :class:`tf.Tensor` containing the subnetwork\'s loss to minimize.\n      var_list: List of subnetwork :class:`tf.Variable` parameters to update as\n        part of the training operation.\n      labels: Labels :class:`tf.Tensor` or a dictionary of string label name to\n        :class:`tf.Tensor` (for multi-head).\n      iteration_step: Integer :class:`tf.Tensor` representing the step since the\n        beginning of the current iteration, as opposed to the global step.\n      summary: An :class:`adanet.Summary` for scoping summaries to individual\n        subnetworks in Tensorboard. Using `tf.summary` within this scope will\n        use this :class:`adanet.Summary` under the hood.\n      previous_ensemble: The best `Ensemble` from iteration t-1. The created\n        subnetwork will extend the previous ensemble to form the `Ensemble` at\n        iteration t. Is None for iteration 0.\n\n    Returns:\n      Either a train op or an :class:`adanet.subnetwork.TrainOpSpec`.\n    """"""\n\n  def build_subnetwork_report(self):\n    """"""Returns a `subnetwork.Report` to materialize and record.\n\n    This method will be called once after :meth:`build_subnetwork`.\n    Do NOT depend on variables created in :meth:`build_subnetwork_train_op`,\n    because they are not called before :meth:`build_subnetwork_report` is\n    called.\n\n    If it returns None, AdaNet records the name and standard eval metrics.\n    """"""\n\n    return None\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Generator(object):\n  """"""Interface for a candidate subnetwork generator.\n\n  Given the ensemble of subnetworks at iteration t-1, this object is\n  responsible for generating the set of candidate subnetworks for iteration t\n  that minimize the objective as part of an ensemble.\n  """"""\n\n  @abc.abstractmethod\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports, config):\n    # pyformat: disable\n    """"""Generates :class:`adanet.subnetwork.Builder` instances for an iteration.\n\n    NOTE: Every call to :meth:`generate_candidates` must be deterministic for\n    the given arguments.\n\n    Args:\n      previous_ensemble: The best :class:`adanet.Ensemble` from iteration t-1.\n        DEPRECATED. We are transitioning away from the use of previous_ensemble\n        in generate_candidates. New Generators should *not* use\n        previous_ensemble in their implementation of generate_candidates --\n        please only use iteration_number, previous_ensemble_reports and\n        all_reports.\n      iteration_number: Python integer AdaNet iteration t, starting from 0.\n      previous_ensemble_reports: List of\n        :class:`adanet.subnetwork.MaterializedReport` instances corresponding to\n        the Builders composing :class:`adanet.Ensemble` from iteration t-1. The\n        first element in the list corresponds to the Builder added in the\n        first iteration. If a :class:`adanet.subnetwork.MaterializedReport` is\n        not supplied to the estimator, previous_ensemble_report is `None`.\n      all_reports: List of :class:`adanet.subnetwork.MaterializedReport`\n        instances. If an :class:`adanet.subnetwork.ReportMaterializer` is not\n        supplied to the estimator, `all_reports` is `None`. If\n        :class:`adanet.subnetwork.ReportMaterializer` is supplied to the\n        estimator and t=0, `all_reports` is an empty List. Otherwise,\n        `all_reports` is a sequence of Lists. Each element of the sequence is a\n        List containing all the :class:`adanet.subnetwork.MaterializedReport`\n        instances in an AdaNet iteration, starting from iteration 0, and\n        ending at iteration t-1.\n      config: The current :class:`tf.estimator.RunConfig` object to configure\n        the runtime settings.\n\n    Returns:\n      A list of :class:`adanet.subnetwork.Builder` instances.\n    """"""\n    # pyformat: enable\n\n\nclass SimpleGenerator(Generator):\n  """"""Always generates the given :class:`adanet.subnetwork.Builder` instances.\n\n  Args:\n    subnetwork_builders: List of :class:`adanet.subnetwork.Builder` instances to\n      return at each iteration when `generate_candidates` is called.\n\n  Returns:\n    A :class:`adanet.SimpleGenerator` instance.\n  """"""\n\n  def __init__(self, subnetwork_builders):\n    self._subnetwork_builders = subnetwork_builders\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports):\n    return self._subnetwork_builders\n'"
adanet/subnetwork/generator_test.py,3,"b'""""""Test AdaNet single graph subnetwork implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet import tf_compat\nfrom adanet.subnetwork.generator import Builder\nfrom adanet.subnetwork.generator import Subnetwork\nimport tensorflow.compat.v2 as tf\n\n\ndef dummy_tensor(shape=(), random_seed=42):\n  """"""Returns a randomly initialized tensor.""""""\n\n  return tf.Variable(\n      tf_compat.random_normal(shape=shape, seed=random_seed),\n      trainable=False).read_value()\n\n\nclass FakeSubnetwork(Builder):\n  """"""Fake subnetwork builder.""""""\n\n  @property\n  def name(self):\n    return ""fake_subnetwork""\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    return\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    return\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    return\n\n\nclass SubnetworkTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""no_persisted_tensors_nor_shared"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n      }, {\n          ""testcase_name"": ""empty_persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""dict_logits_and_last_layer"",\n          ""last_layer"": {\n              ""head1"": dummy_tensor()\n          },\n          ""logits"": {\n              ""head1"": dummy_tensor()\n          },\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {\n              ""hidden_layer"": dummy_tensor(),\n          },\n      }, {\n          ""testcase_name"": ""nested_persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {\n              ""hidden_layer"": dummy_tensor(),\n              ""nested"": {\n                  ""foo"": dummy_tensor(),\n                  ""nested"": {\n                      ""foo"": dummy_tensor(),\n                  },\n              },\n          },\n      }, {\n          ""testcase_name"": ""shared_primitive"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""shared"": 1,\n      }, {\n          ""testcase_name"": ""shared_dict"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""shared"": {},\n      }, {\n          ""testcase_name"": ""shared_lambda"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""shared"": lambda x: x,\n      }, {\n          ""testcase_name"": ""shared_object"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""shared"": dummy_tensor(),\n      })\n  def test_new(self,\n               last_layer,\n               logits,\n               complexity,\n               persisted_tensors=None,\n               shared=None):\n    with self.test_session():\n      got = Subnetwork(last_layer, logits, complexity, persisted_tensors,\n                       shared)\n      self.assertEqual(got.last_layer, last_layer)\n      self.assertEqual(got.logits, logits)\n      self.assertEqual(got.complexity, complexity)\n      self.assertEqual(got.persisted_tensors, persisted_tensors)\n      self.assertEqual(got.shared, shared)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""none_last_layer"",\n          ""last_layer"": None,\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""none_logits"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": None,\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""none_complexity"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": None,\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""empty_list_persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": [],\n      }, {\n          ""testcase_name"": ""list_persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": [1.],\n      }, {\n          ""testcase_name"": ""empty_nested_persisted_tensors"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {\n              ""value"": dummy_tensor(),\n              ""nested"": {},\n          },\n      }, {\n          ""testcase_name"": ""empty_nested_persisted_tensors_recursive"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {\n              ""value"": dummy_tensor(),\n              ""nested"": {\n                  ""value"": dummy_tensor(),\n                  ""nested"": {\n                      ""value"": dummy_tensor(),\n                      ""nested"": {},\n                  },\n              },\n          },\n      }, {\n          ""testcase_name"": ""only_dict_logits"",\n          ""last_layer"": dummy_tensor(),\n          ""logits"": {\n              ""head"": dummy_tensor()\n          },\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      }, {\n          ""testcase_name"": ""only_dict_last_layer"",\n          ""last_layer"": {\n              ""head"": dummy_tensor()\n          },\n          ""logits"": dummy_tensor(),\n          ""complexity"": dummy_tensor(),\n          ""persisted_tensors"": {},\n      })\n  def test_new_errors(self, last_layer, logits, complexity, persisted_tensors):\n    with self.test_session():\n      with self.assertRaises(ValueError):\n        Subnetwork(last_layer, logits, complexity, persisted_tensors)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/subnetwork/report.py,8,"b'""""""Container for an `adanet.Subnetwork`\'s attributes and metrics.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom adanet import tf_compat\nimport six\nimport tensorflow.compat.v1 as tf\n\n\nclass Report(\n    collections.namedtuple(""Report"", [""hparams"", ""attributes"", ""metrics""])):\n  # pyformat: disable\n  """"""A container for data to be collected about a :class:`Subnetwork`.\n\n  Args:\n    hparams: A dict mapping strings to python strings, ints, bools, or floats.\n      It is meant to contain the constants that define the\n      :class:`adanet.subnetwork.Builder`, such as dropout, number of layers, or\n      initial learning rate.\n    attributes: A dict mapping strings to rank 0 Tensors of dtype string, int32,\n      or float32. It is meant to contain properties that may or may not change\n      over the course of training the :class:`adanet.subnetwork.Subnetwork`,\n      such as the number of parameters, the Lipschitz constant, the :math:`L2`\n      norm of the weights, or learning rate at materialization time.\n    metrics: Dict of metric results keyed by name. The values of the dict are\n      the results of calling a metric function, namely a `(metric_tensor,\n      update_op)` tuple. `metric_tensor` should be evaluated without any impact\n      on state (typically is a pure computation results based on variables.).\n      For example, it should not trigger the :code:`update_op` or requires any\n      input fetching. This is meant to contain metrics of interest, such as the\n      training loss, complexity regularized loss, or standard deviation of the\n      last layer outputs.\n\n  Returns:\n    A validated :class:`adanet.subnetwork.Report` object.\n\n  Raises:\n    ValueError: If validation fails.\n  """"""\n  # pyformat: enable\n\n  def __new__(cls, hparams, attributes, metrics):\n\n    def _is_scalar(tensor):\n      """"""Returns True iff tensor is scalar.""""""\n      return tensor.shape.ndims == 0\n\n    def _is_accepted_dtype(tensor):\n      """"""Returns True iff tensor has the dtype we can handle.""""""\n      return tensor.dtype.base_dtype in (tf.bool, tf.int32, tf.float32,\n                                         tf.float64, tf.string)\n\n    # Validate hparams\n    for key, value in hparams.items():\n      if not isinstance(value, (bool, int, float, six.string_types)):\n        raise ValueError(\n            ""hparam \'{}\' refers to invalid value {}, type {}. type must be ""\n            ""python primitive int, float, bool, or string."".format(\n                key, value, type(value)))\n\n    # Validate attributes\n    for key, value in attributes.items():\n      if not isinstance(value, tf.Tensor):\n        raise ValueError(""attribute \'{}\' refers to invalid value: {}, type: {}.""\n                         ""type must be Tensor."".format(key, value, type(value)))\n\n      if not (_is_scalar(value) and _is_accepted_dtype(value)):\n        raise ValueError(\n            ""attribute \'{}\' refers to invalid tensor {}. Shape: {}"".format(\n                key, value, value.get_shape()))\n\n    # Validate metrics\n    metrics_copy = {}\n    for key, value in metrics.items():\n      value = tf_compat.metric_op(value)\n      if not isinstance(value, tuple):\n        raise ValueError(\n            ""metric \'{}\' has invalid type {}. Must be a tuple."".format(\n                key, type(value)))\n\n      if len(value) < 2:\n        raise ValueError(\n            ""metric tuple \'{}\' has fewer than 2 elements"".format(key))\n\n      if not isinstance(value[0], (tf.Tensor, tf.Variable)):\n        raise ValueError(\n            ""First element of metric tuple \'{}\' has value {} and type {}. ""\n            ""Must be a Tensor or Variable."".format(key, value[0],\n                                                   type(value[0])))\n\n      if not _is_accepted_dtype(value[0]):\n        raise ValueError(\n            ""First element of metric \'{}\' refers to Tensor of the wrong ""\n            ""dtype {}. Must be one of tf.bool, tf.int32, tf.float32, ""\n            ""tf.float64 or tf.string."".format(key, value[0].dtype))\n\n      if not _is_scalar(value[0]):\n        tf.logging.warn(\n            ""First element of metric \'{}\' refers to Tensor of rank > 0. ""\n            ""AdaNet is currently unable to store metrics of rank > 0 -- this ""\n            ""metric will be dropped from the report. ""\n            ""value: {}"".format(key, value[0]))\n        continue\n\n      if not isinstance(value[1], (tf.Tensor, tf.Operation, tf.Variable)):\n        raise ValueError(\n            ""Second element of metric tuple \'{}\' has value {} and type {}. ""\n            ""Must be a Tensor, Operation, or Variable."".format(\n                key, value[1], type(value[1])))\n\n      metrics_copy[key] = value\n\n    return super(Report, cls).__new__(\n        cls, hparams=hparams, attributes=attributes, metrics=metrics_copy)\n\n\nclass MaterializedReport(\n    collections.namedtuple(""MaterializedReport"", [\n        ""iteration_number"", ""name"", ""hparams"", ""attributes"", ""metrics"",\n        ""included_in_final_ensemble""\n    ])):\n  # pyformat: disable\n  """"""Data collected about a :class:`adanet.subnetwork.Subnetwork`.\n\n  Args:\n    iteration_number: A python integer for the AdaNet iteration number, starting\n      from 0.\n    name: A string, which is either the name of the corresponding Builder, or\n      ""previous_ensemble"" if it refers to the previous_ensemble.\n    hparams: A dict mapping strings to python strings, ints, or floats. These\n      are constants passed from the author of the\n      :class:`adanet.subnetwork.Builder` that was used to construct this\n      :class:`adanet.subnetwork.Subnetwork`. It is meant to contain the\n      arguments that defined the :class:`adanet.subnetwork.Builder`, such as\n      dropout, number of layers, or initial learning rate.\n    attributes: A dict mapping strings to python strings, ints, bools, or\n      floats. These are python primitives that come from materialized Tensors;\n      these Tensors were defined by the author of the\n      :class:`adanet.subnetwork.Builder` that was used\n      to construct this :class:`adanet.subnetwork.Subnetwork`. It is meant to\n      contain properties that may or may not change over the course of\n      training the :class:`adanet.subnetwork.Subnetwork`, such as the number of\n      parameters, the Lipschitz constant, or the :math:`L2` norm of the weights.\n    metrics: A dict mapping strings to python strings, ints, or floats. These\n      are python primitives that come from metrics that were evaluated on the\n      trained :class:`adanet.subnetwork.Subnetwork` over some dataset; these\n      metrics were defined by the author of the\n      :class:`adanet.subnetwork.Builder` that was used to construct this\n      :class:`adanet.subnetwork.Subnetwork`. It is meant to contain\n      performance metrics or measures that could predict generalization, such\n      as the training loss, complexity regularized loss, or standard deviation\n      of the last layer outputs.\n    included_in_final_ensemble: A boolean denoting whether the associated\n      :class:`adanet.subnetwork.Subnetwork` was included in the ensemble at the\n      end of the AdaNet iteration.\n\n  Returns:\n    An :class:`adanet.subnetwork.MaterializedReport` object.\n  """"""\n  # pyformat: enable\n\n  def __new__(cls,\n              iteration_number,\n              name,\n              hparams,\n              attributes,\n              metrics,\n              included_in_final_ensemble=False):\n\n    return super(MaterializedReport, cls).__new__(\n        cls,\n        iteration_number=iteration_number,\n        name=name,\n        hparams=hparams,\n        attributes=attributes,\n        metrics=metrics,\n        included_in_final_ensemble=included_in_final_ensemble)\n'"
adanet/subnetwork/report_test.py,13,"b'""""""Test AdaNet single graph subnetwork implementation.\n\nCopyright 2018 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet.subnetwork.report import Report\nimport tensorflow.compat.v2 as tf\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import test_util\n# pylint: enable=g-direct-tensorflow-import\n\n\nclass ReportTest(parameterized.TestCase, tf.test.TestCase):\n\n  # pylint: disable=g-long-lambda\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""empty"",\n          ""hparams"": {},\n          ""attributes"": lambda: {},\n          ""metrics"": lambda: {},\n      }, {\n          ""testcase_name"": ""non_empty"",\n          ""hparams"": {\n              ""hoo"": 1\n          },\n          ""attributes"": lambda: {\n              ""aoo"": tf.constant(1)\n          },\n          ""metrics"": lambda: {\n              ""moo"": (tf.constant(1), tf.constant(1))\n          },\n      }, {\n          ""testcase_name"": ""non_tensor_update_op"",\n          ""hparams"": {\n              ""hoo"": 1\n          },\n          ""attributes"": lambda: {\n              ""aoo"": tf.constant(1)\n          },\n          ""metrics"": lambda: {\n              ""moo"": (tf.constant(1), tf.no_op())\n          },\n      })\n  # pylint: enable=g-long-lambda\n  @test_util.run_in_graph_and_eager_modes\n  def test_new(self, hparams, attributes, metrics):\n    with context.graph_mode():\n      _ = tf.constant(0)  # Just to have a non-empty graph.\n      report = Report(\n          hparams=hparams, attributes=attributes(), metrics=metrics())\n      self.assertEqual(hparams, report.hparams)\n      self.assertEqual(\n          self.evaluate(attributes()), self.evaluate(report.attributes))\n      self.assertEqual(self.evaluate(metrics()), self.evaluate(report.metrics))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_drop_non_scalar_metric(self):\n    """"""Tests b/118632346.""""""\n\n    hparams = {""hoo"": 1}\n    attributes = {""aoo"": tf.constant(1)}\n    metrics = {\n        ""moo1"": (tf.constant(1), tf.constant(1)),\n        ""moo2"": (tf.constant([1, 1]), tf.constant([1, 1])),\n    }\n    want_metrics = metrics.copy()\n    del want_metrics[""moo2""]\n    with self.test_session():\n      report = Report(hparams=hparams, attributes=attributes, metrics=metrics)\n      self.assertEqual(hparams, report.hparams)\n      self.assertEqual(attributes, report.attributes)\n      self.assertEqual(want_metrics, report.metrics)\n\n  @parameterized.named_parameters(\n      {\n          ""testcase_name"": ""tensor_hparams"",\n          ""hparams"": {\n              ""hoo"": tf.constant(1)\n          },\n          ""attributes"": {},\n          ""metrics"": {},\n      }, {\n          ""testcase_name"": ""non_tensor_attributes"",\n          ""hparams"": {},\n          ""attributes"": {\n              ""aoo"": 1,\n          },\n          ""metrics"": {},\n      }, {\n          ""testcase_name"": ""non_tuple_metrics"",\n          ""hparams"": {},\n          ""attributes"": {},\n          ""metrics"": {\n              ""moo"": tf.constant(1)\n          },\n      }, {\n          ""testcase_name"": ""one_item_tuple_metrics"",\n          ""hparams"": {},\n          ""attributes"": {},\n          ""metrics"": {\n              ""moo"": (tf.constant(1),)\n          },\n      })\n  @test_util.run_in_graph_and_eager_modes\n  def test_new_errors(self, hparams, attributes, metrics):\n    with self.assertRaises(ValueError):\n      Report(hparams=hparams, attributes=attributes, metrics=metrics)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
adanet/tf_compat/__init__.py,29,"b'# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""TensorFlow major version compatibility code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom distutils.version import LooseVersion\nimport tensorflow.compat.v1 as tf\nimport tensorflow.compat.v2 as tf_v2\n# pylint: disable=unused-import\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python import tf2\nfrom tensorflow.python.keras.metrics import Metric\nfrom tensorflow.python.tpu import tpu_function\nfrom tensorflow_estimator.python.estimator.head import regression_head\n# pylint: enable=g-direct-tensorflow-import\n# pylint: enable=unused-import\n\nDatasetV1 = tf.compat.v1.data.Dataset\nDatasetV2 = tf.compat.v2.data.Dataset\n\nv1 = tf.compat.v1\nv2 = tf.compat.v2\n\ntry:\n  SessionRunHook = tf.estimator.SessionRunHook\nexcept AttributeError:\n  SessionRunHook = tf.train.SessionRunHook\n\ntry:\n  SessionRunArgs = tf.estimator.SessionRunArgs\nexcept AttributeError:\n  SessionRunArgs = tf.train.SessionRunArgs\n\ntry:\n  SummarySaverHook = tf.estimator.SummarySaverHook\nexcept AttributeError:\n  SummarySaverHook = tf.train.SummarySaverHook\n\ntry:\n  CheckpointSaverHook = tf.estimator.CheckpointSaverHook\nexcept AttributeError:\n  CheckpointSaverHook = tf.train.CheckpointSaverHook\n\ntry:\n  # Loss reduction strings change between TF 1.13 and TF 1.14, which causes\n  # Heads to raise errors.\n  regression_head.RegressionHead(\n      loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n  SUM_OVER_BATCH_SIZE = tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n  SUM = tf.losses.Reduction.SUM\nexcept ValueError:\n  SUM_OVER_BATCH_SIZE = ""sum_over_batch_size""\n  SUM = ""sum""\n\n\ndef tensor_name(tensor):\n  """"""Returns the Tensor\'s name.\n\n  Tensor names always have the structure <op_name>:<int>. This method\n  returns the portion before the \':\'.\n\n  Args:\n    tensor: Tensor.\n\n  Returns:\n    String name of the Tensor.\n  """"""\n\n  return tensor.name.split("":"")[-2]\n\n\ndef version_greater_or_equal(semver):\n  """"""Returns whether the current TF version is >= to semver string.""""""\n\n  try:\n    tf_version = tf.version.VERSION\n  except AttributeError:\n    tf_version = tf.VERSION\n  return LooseVersion(tf_version) >= LooseVersion(semver)\n\n\ndef make_one_shot_iterator(dataset):\n  """"""Returns a dataset\'s one-shot iterator.""""""\n\n  try:\n    return v1.data.make_one_shot_iterator(dataset)\n  except AttributeError:\n    return dataset.make_one_shot_iterator()\n\n\ndef random_normal(*args, **kwargs):\n  """"""Returns a random normal distribution Tensor.""""""\n\n  try:\n    return tf.random.normal(*args, **kwargs)\n  except AttributeError:\n    return tf.random_normal(*args, **kwargs)\n\n\ndef metric_op(metric):\n  """"""Converts Keras metrics into a metric op tuple.\n\n  NOTE: If this method is called in for loop, the runtime is O(n^2). However\n  the number of eval metrics at any given time should be small enough that\n  this does not affect performance. Any impact is only during graph construction\n  time, and therefore has no effect on steps/s.\n\n  Args:\n    metric: Either a `tf.keras.metric.Metric` instance or a tuple of Tensor\n      value and update op.\n\n  Returns:\n    A tuple of metric Tensor value and update op.\n  """"""\n\n  if not isinstance(metric, tf.keras.metrics.Metric):\n    return metric\n  vars_to_add = {}\n  for var in metric.variables:\n    vars_to_add[_hashable_var_key(var)] = var\n  metric = (metric.result(), metric.updates[0])\n  _update_variable_collection(v1.GraphKeys.LOCAL_VARIABLES, vars_to_add)\n  _update_variable_collection(v1.GraphKeys.METRIC_VARIABLES, vars_to_add)\n  return metric\n\n\ndef _hashable_var_key(var):\n  """"""Returns a hashable key to identify the given Variable.""""""\n\n  # In TF 2, Variables themselves are not hashable, so cannot be dict keys.\n  # Error is ""Tensor is unhashable if Tensor equality is enabled. Instead, use\n  # tensor.experimental_ref() as the key"". For a related issue, see:\n  # https://github.com/tensorflow/tensorflow/issues/32139\n  ref_op = getattr(var, ""experimental_ref"", None)\n  if callable(ref_op):\n    return ref_op()\n  return var\n\n\ndef _update_variable_collection(collection_name, vars_to_add):\n  """"""Add variables to collection.""""""\n  collection = {}\n  for var in v1.get_collection(collection_name):\n    collection[_hashable_var_key(var)] = var\n  # Skip variables that are in the collection already: O(n) runtime.\n  for var_ref in vars_to_add:\n    if var_ref in collection:\n      continue\n    v1.add_to_collection(collection_name, vars_to_add[var_ref])\n\n\ndef skip_for_tf2(f):\n  """"""Decorator that skips tests when using TensorFlow 2.""""""\n\n  def test_wrapper(*args, **kwargs):\n    """"""Wraps the decorated function to determine whether to skip.""""""\n\n    # Extract test case instance from args.\n    self = args[0]\n    try:\n      # If tf.contrib doesn\'t exist, we are in TF 2.0.\n      _ = tf.contrib\n      _ = tf.contrib.estimator.regression_head(\n          loss_reduction=tf.compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE)\n    except (AttributeError, ImportError):\n      self.skipTest(""Skipping test in TF 2.0."")\n    return f(*args, **kwargs)\n\n  return test_wrapper\n\n\ndef skip_for_tf1(f):\n  """"""Decorator that skips tests when using TensorFlow 1.""""""\n\n  def test_wrapper(*args, **kwargs):\n    """"""Wraps the decorated function to determine whether to skip.""""""\n\n    # Extract test case instance from args.\n    self = args[0]\n    try:\n      # If tf.contrib doesn\'t exist, we are in TF 2.0.\n      _ = tf_v2.contrib\n    except (AttributeError, ImportError):\n      return f(*args, **kwargs)\n    self.skipTest(""Skipping test in TF 1.0."")\n    return f(*args, **kwargs)\n\n  return test_wrapper\n\n\ndef is_v2_behavior_enabled():\n  """"""Returns if user called tf.enable_v2_behavior.""""""\n\n  # Since there is no actual tf.is_v2_behavior enabled, check that the\n  # settings were enabled.\n  return tf2.enabled()\n\n\ndef load_variable(checkpoint_path, var_name, shape, dtype):\n  """"""Loads a variable from a given checkpoint.""""""\n  with tf.Graph().as_default():\n    variable = v1.get_variable(\n        var_name,\n        shape=shape,\n        dtype=dtype,\n        initializer=v1.zeros_initializer(),\n        trainable=False)\n    trackable_vars = {var_name: variable}\n    checkpoint = v2.train.Checkpoint(**trackable_vars)\n    status = checkpoint.restore(checkpoint_path)\n    status.expect_partial()\n    with v1.Session() as session:\n      status.initialize_or_restore(session)\n      return session.run(variable)\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n# Copyright 2018 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Configuration file for the Sphinx documentation builder.\n\nThis file does only contain a selection of the most common options. For a\nfull list see the documentation:\nhttp://www.sphinx-doc.org/en/master/usage/configuration.html\n""""""\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\nfrom adanet import version as ver  # pylint: disable=g-import-not-at-top\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'adanet\'\ncopyright = u\'2018, AdaNet Authors\'  # pylint: disable=redefined-builtin\nauthor = u\'AdaNet Authors\'\n\n# The short X.Y version\nversion = ver.__version__\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'recommonmark\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\n# templates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.txt\': \'markdown\',\n    \'.md\': \'markdown\',\n}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [u\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\nhtml_logo = \'./assets/adanet_tangram_logo.png\'\n\nhtml_context = {\n    \'css_files\': [\'_static/custom.css\'],\n}\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'adanetdoc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'adanet.tex\', u\'adanet Documentation\', u\'AdaNet Authors\',\n     \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'adanet\', u\'adanet Documentation\', [author], 1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'adanet\', u\'adanet Documentation\', author, \'adanet\',\n     \'One line description of project.\', \'Miscellaneous\'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# -- Extension configuration -------------------------------------------------\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n'"
research/improve_nas/setup.py,0,"b'""""""Setup file.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = [\'tensorflow>=1.12\',\n                     \'adanet==0.5.0\']\n\nsetup(\n    name=\'trainer\',\n    version=\'0.1\',\n    install_requires=REQUIRED_PACKAGES,\n    packages=find_packages(),\n    include_package_data=True,\n    description=\'improve nas model\'\n)\n'"
adanet/experimental/controllers/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet ModelFlow controllers.""""""\n\nfrom adanet.experimental.controllers.sequential_controller import SequentialController\n\n\n__all__ = [\n    ""SequentialController"",\n]\n'"
adanet/experimental/controllers/controller.py,1,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The AutoML controller for AdaNet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom typing import Iterator, Sequence\n\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass Controller(abc.ABC):\n  """"""Defines the machine learning workflow to produce high-quality models.""""""\n\n  @abc.abstractmethod\n  def work_units(self) -> Iterator[WorkUnit]:\n    """"""Yields `WorkUnit` instances.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_best_models(self, num_models) -> Sequence[tf.keras.Model]:\n    """"""Returns the top models produced from executing the controller.""""""\n    pass\n'"
adanet/experimental/controllers/sequential_controller.py,1,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A manual controller for model search.""""""\n\nfrom typing import Iterator, Sequence\nfrom adanet.experimental.controllers.controller import Controller\nfrom adanet.experimental.phases.phase import ModelProvider\nfrom adanet.experimental.phases.phase import Phase\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass SequentialController(Controller):\n  """"""A controller where the user specifies the sequences of phase to execute.""""""\n\n  # TODO: Add checks to make sure phases are valid.\n  def __init__(self, phases: Sequence[Phase]):\n    """"""Initializes a SequentialController.\n\n    Args:\n      phases: A list of `Phase` instances.\n    """"""\n\n    self._phases = phases\n\n  def work_units(self) -> Iterator[WorkUnit]:\n    previous_phase = None\n    for phase in self._phases:\n      for work_unit in phase.work_units(previous_phase):\n        yield work_unit\n      previous_phase = phase\n\n  def get_best_models(self, num_models: int) -> Sequence[tf.keras.Model]:\n    final_phase = self._phases[-1]\n    if isinstance(final_phase, ModelProvider):\n      return self._phases[-1].get_best_models(num_models)\n    raise RuntimeError(\'Final phase does not provide models.\')\n'"
adanet/experimental/keras/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet Keras models.""""""\n\nfrom adanet.experimental.keras.ensemble_model import EnsembleModel\nfrom adanet.experimental.keras.ensemble_model import MeanEnsemble\nfrom adanet.experimental.keras.ensemble_model import WeightedEnsemble\nfrom adanet.experimental.keras.model_search import ModelSearch\n\n\n__all__ = [\n    ""EnsembleModel"",\n    ""MeanEnsemble"",\n    ""WeightedEnsemble"",\n    ""ModelSearch"",\n]\n'"
adanet/experimental/keras/ensemble_model.py,8,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An AdaNet ensemble implementation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom typing import Sequence\n\nimport tensorflow.compat.v2 as tf\n\n\nclass EnsembleModel(tf.keras.Model):\n  """"""An ensemble of Keras models.""""""\n\n  def __init__(self, submodels: Sequence[tf.keras.Model],\n               freeze_submodels: bool = True):\n    """"""Initializes an EnsembleModel.\n\n    Args:\n      submodels: A list of `tf.keras.Model` that compose the ensemble.\n      freeze_submodels: Whether to freeze the weights of submodels.\n    """"""\n\n    super().__init__()\n    if freeze_submodels:\n      for submodel in submodels:\n        for layer in submodel.layers:\n          layer.trainable = False\n    self._submodels = submodels\n\n  @property\n  def submodels(self) -> Sequence[tf.keras.Model]:\n    return self._submodels\n\n  def call(self, inputs):\n    raise NotImplementedError\n\n\nclass MeanEnsemble(EnsembleModel):\n  """"""An ensemble that averages submodel outputs.""""""\n\n  def call(self, inputs):\n    if len(self._submodels) == 1:\n      return self._submodels[0](inputs)\n\n    submodel_outputs = []\n    for submodel in self._submodels:\n      submodel_outputs.append(submodel(inputs))\n    return tf.keras.layers.average(submodel_outputs)\n\n\nclass WeightedEnsemble(EnsembleModel):\n  """"""An ensemble that linearly combines submodel outputs.""""""\n\n  # TODO: Extract output shapes from submodels instead of passing in\n  # as argument.\n  def __init__(self, submodels: Sequence[tf.keras.Model], output_units: int):\n    """"""Initializes a WeightedEnsemble.\n\n    Args:\n        submodels: A list of `adanet.keras.SubModel` that compose the ensemble.\n        output_units: The output size of the last layer of each submodel.\n    """"""\n\n    super().__init__(submodels)\n    self.dense = tf.keras.layers.Dense(units=output_units)\n\n  def call(self, inputs):\n    submodel_outputs = []\n    for submodel in self.submodels:\n      submodel_outputs.append(submodel(inputs))\n    return self.dense(tf.stack(submodel_outputs))\n'"
adanet/experimental/keras/ensemble_model_test.py,15,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for adanet.experimental.keras.EnsembleModel.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom adanet.experimental.keras import testing_utils\nfrom adanet.experimental.keras.ensemble_model import MeanEnsemble\nfrom adanet.experimental.keras.ensemble_model import WeightedEnsemble\n\nimport tensorflow.compat.v2 as tf\n\n\nclass EnsembleModelTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'mean_ensemble\',\n          \'ensemble\': MeanEnsemble,\n          \'want_results\': [0.07671691, 0.20448962],\n      }, {\n          \'testcase_name\': \'weighted_ensemble\',\n          \'ensemble\': WeightedEnsemble,\n          \'output_units\': 2,\n          \'want_results\': [0.42579408, 0.53439462],\n      })\n  def test_lifecycle(self, ensemble, want_results, output_units=None):\n    train_dataset, test_dataset = testing_utils.get_holdout_data(\n        train_samples=128,\n        test_samples=64,\n        input_shape=(10,),\n        num_classes=2,\n        random_seed=42)\n\n    # TODO: Consider performing `tf.data.Dataset` transformations\n    # within get_test_data function.\n    train_dataset = train_dataset.batch(32).repeat(10)\n    test_dataset = test_dataset.batch(32).repeat(10)\n\n    model1 = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(2),\n    ])\n    model1.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01),\n        loss=\'mse\')\n    model1.fit(train_dataset)\n    model1.trainable = False  # Since models inside ensemble should be trained.\n    model1_pre_train_weights = model1.get_weights()\n\n    model2 = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(2),\n    ])\n    model2.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01),\n        loss=\'mse\')\n    model2.fit(train_dataset)\n    model2.trainable = False  # Since models inside ensemble should be trained.\n    model2_pre_train_weights = model2.get_weights()\n\n    if output_units:\n      ensemble = ensemble(submodels=[model1, model2],\n                          output_units=output_units)\n    else:\n      ensemble = ensemble(submodels=[model1, model2])\n    ensemble.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01),\n        loss=\'mse\',\n        metrics=[\'mae\'])\n\n    ensemble.fit(train_dataset)\n\n    # Make sure submodel weights were not altered during ensemble training.\n    model1_post_train_weights = model1.get_weights()\n    model2_post_train_weights = model2.get_weights()\n    self.assertAllClose(model1_pre_train_weights, model1_post_train_weights)\n    self.assertAllClose(model2_pre_train_weights, model2_post_train_weights)\n\n    eval_results = ensemble.evaluate(test_dataset)\n    self.assertAllClose(eval_results, want_results)\n\n\nif __name__ == \'__main__\':\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/experimental/keras/model_search.py,1,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An AdaNet interface for model search.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom typing import Sequence\n\nfrom adanet.experimental.controllers.controller import Controller\nfrom adanet.experimental.schedulers.in_process_scheduler import InProcessScheduler\nfrom adanet.experimental.schedulers.scheduler import Scheduler\nimport tensorflow.compat.v2 as tf\n\n\nclass ModelSearch(object):\n  """"""An AutoML pipeline manager.""""""\n\n  def __init__(self,\n               controller: Controller,\n               scheduler: Scheduler = InProcessScheduler()):\n    """"""Initializes a ModelSearch.\n\n    Args:\n      controller: A `Controller` instance.\n      scheduler: A `Scheduler` instance.\n    """"""\n\n    self._controller = controller\n    self._scheduler = scheduler\n\n  def run(self):\n    """"""Executes the training workflow to generate models.""""""\n    self._scheduler.schedule(self._controller.work_units())\n\n  def get_best_models(self, num_models) -> Sequence[tf.keras.Model]:\n    """"""Returns the top models from the run.""""""\n    return self._controller.get_best_models(num_models)\n'"
adanet/experimental/keras/model_search_test.py,25,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tests for adanet.experimental.keras.ModelSearch.""""""\n\nimport os\nimport shutil\nimport sys\nimport time\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom adanet.experimental.controllers.sequential_controller import SequentialController\nfrom adanet.experimental.keras import testing_utils\nfrom adanet.experimental.keras.ensemble_model import MeanEnsemble\nfrom adanet.experimental.keras.model_search import ModelSearch\nfrom adanet.experimental.phases.autoensemble_phase import AutoEnsemblePhase\nfrom adanet.experimental.phases.autoensemble_phase import GrowStrategy\nfrom adanet.experimental.phases.autoensemble_phase import MeanEnsembler\nfrom adanet.experimental.phases.input_phase import InputPhase\nfrom adanet.experimental.phases.keras_trainer_phase import KerasTrainerPhase\nfrom adanet.experimental.phases.keras_tuner_phase import KerasTunerPhase\nfrom adanet.experimental.phases.repeat_phase import RepeatPhase\nfrom adanet.experimental.storages.in_memory_storage import InMemoryStorage\nfrom kerastuner import tuners\nimport tensorflow.compat.v2 as tf\n\n\nclass ModelSearchTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(ModelSearchTest, self).setUp()\n    # Setup and cleanup test directory.\n    # Flags are not automatically parsed at this point.\n    flags.FLAGS(sys.argv)\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(ModelSearchTest, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n  def test_phases_end_to_end(self):\n    train_dataset, test_dataset = testing_utils.get_holdout_data(\n        train_samples=128,\n        test_samples=64,\n        input_shape=(10,),\n        num_classes=10,\n        random_seed=42)\n\n    # TODO: Consider performing `tf.data.Dataset` transformations\n    # within get_test_data function.\n    train_dataset = train_dataset.batch(32)\n    test_dataset = test_dataset.batch(32)\n\n    model1 = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(10),\n    ])\n    model1.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01), loss=\'mse\', metrics=[\'mae\'])\n\n    model2 = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(64, activation=\'relu\'),\n        tf.keras.layers.Dense(10),\n    ])\n    model2.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01), loss=\'mse\', metrics=[\'mae\'])\n\n    # TODO: This test could potentially have the best model be\n    # a non-ensemble Keras model. Therefore, need to address this issue and\n    # remove the freeze_submodels flag.\n    ensemble = MeanEnsemble(submodels=[model1, model2], freeze_submodels=False)\n    ensemble.compile(\n        optimizer=tf.keras.optimizers.Adam(0.01), loss=\'mse\', metrics=[\'mae\'])\n\n    controller = SequentialController(phases=[\n        InputPhase(train_dataset, test_dataset),\n        KerasTrainerPhase([model1, model2]),\n        KerasTrainerPhase([ensemble]),\n    ])\n\n    model_search = ModelSearch(controller)\n    model_search.run()\n    self.assertIsInstance(\n        model_search.get_best_models(num_models=1)[0], MeanEnsemble)\n\n  def test_tuner_end_to_end(self):\n    train_dataset, test_dataset = testing_utils.get_holdout_data(\n        train_samples=128,\n        test_samples=64,\n        input_shape=(10,),\n        num_classes=10,\n        random_seed=42)\n\n    # TODO: Consider performing `tf.data.Dataset` transformations\n    # within get_holdout_data function.\n    train_dataset = train_dataset.batch(32)\n    test_dataset = test_dataset.batch(32)\n\n    def build_model(hp):\n      model = tf.keras.Sequential()\n      model.add(\n          tf.keras.layers.Dense(\n              units=hp.Int(\'units\', min_value=32, max_value=512, step=32),\n              activation=\'relu\'))\n      model.add(tf.keras.layers.Dense(10, activation=\'softmax\'))\n      model.compile(\n          optimizer=tf.keras.optimizers.Adam(\n              hp.Choice(\'learning_rate\', values=[1e-2, 1e-3, 1e-4])),\n          loss=\'sparse_categorical_crossentropy\',\n          metrics=[\'accuracy\'])\n      return model\n\n    # Define phases.\n    tuner = tuners.RandomSearch(\n        build_model,\n        objective=\'val_accuracy\',\n        max_trials=3,\n        executions_per_trial=1,\n        directory=self.test_subdirectory,\n        project_name=\'helloworld_tuner\',\n        overwrite=True)\n\n    tuner_phase = KerasTunerPhase(tuner)\n\n    def build_ensemble():\n      ensemble = MeanEnsemble(\n          submodels=tuner_phase.get_best_models(num_models=2))\n      ensemble.compile(\n          optimizer=tf.keras.optimizers.Adam(0.01), loss=\'mse\', metrics=[\'mae\'])\n      return [ensemble]\n\n    ensemble_phase = KerasTrainerPhase(build_ensemble)\n    input_phase = InputPhase(train_dataset, test_dataset)\n\n    controller = SequentialController(phases=[input_phase,\n                                              tuner_phase,\n                                              ensemble_phase])\n\n    # Execute phases.\n    model_search = ModelSearch(controller)\n    model_search.run()\n    self.assertIsInstance(\n        model_search.get_best_models(num_models=1)[0], MeanEnsemble)\n\n  def test_autoensemble_end_to_end(self):\n\n    train_dataset, test_dataset = testing_utils.get_holdout_data(\n        train_samples=128,\n        test_samples=64,\n        input_shape=(10,),\n        num_classes=10,\n        random_seed=42)\n\n    # TODO: Consider performing `tf.data.Dataset` transformations\n    # within get_holdout_data function.\n    train_dataset = train_dataset.batch(32)\n    test_dataset = test_dataset.batch(32)\n\n    def build_model(hp):\n      model = tf.keras.Sequential()\n      model.add(\n          tf.keras.layers.Dense(\n              units=hp.Int(\'units\', min_value=32, max_value=512, step=32),\n              activation=\'relu\'))\n      model.add(tf.keras.layers.Dense(10, activation=\'softmax\'))\n      model.compile(\n          optimizer=tf.keras.optimizers.Adam(\n              hp.Choice(\'learning_rate\', values=[1e-2, 1e-3, 1e-4])),\n          loss=\'sparse_categorical_crossentropy\',\n          metrics=[\'accuracy\'])\n      return model\n\n    # This allows us to have a shared storage for all the autoensemble phases\n    # that occur in the repeat phase.\n    autoensemble_storage = InMemoryStorage()\n    input_phase = InputPhase(train_dataset, test_dataset)\n    # pylint: disable=g-long-lambda\n    repeat_phase = RepeatPhase(\n        [\n            lambda: KerasTunerPhase(\n                tuners.RandomSearch(\n                    build_model,\n                    objective=\'val_accuracy\',\n                    max_trials=3,\n                    executions_per_trial=1,\n                    directory=self.test_subdirectory,\n                    project_name=\'helloworld_\' + str(int(time.time())),\n                    overwrite=True)),\n            lambda: AutoEnsemblePhase(\n                ensemblers=[\n                    MeanEnsembler(\'sparse_categorical_crossentropy\', \'adam\',\n                                  [\'accuracy\'])\n                ],\n                ensemble_strategies=[GrowStrategy()],\n                storage=autoensemble_storage)\n        ], repetitions=3)\n    # pylint: enable=g-long-lambda\n\n    controller = SequentialController(phases=[input_phase, repeat_phase])\n\n    model_search = ModelSearch(controller)\n    model_search.run()\n    self.assertIsInstance(\n        model_search.get_best_models(num_models=1)[0], MeanEnsemble)\n\n\nif __name__ == \'__main__\':\n  tf.enable_v2_behavior()\n  tf.test.main()\n'"
adanet/experimental/keras/testing_utils.py,4,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for unit-testing AdaNet Keras.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\n\n\n# TODO: Add ability to choose the problem type: regression,\n# classification, multi-class etc.\ndef get_holdout_data(\n    train_samples: int,\n    test_samples: int,\n    input_shape: Tuple[int],\n    num_classes: int,\n    random_seed: Optional[int] = None\n) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n  """"""Generates training and test data.\n\n  Args:\n    train_samples: Number of training samples to generate.\n    test_samples: Number of training samples to generate.\n    input_shape: Shape of the inputs.\n    num_classes: Number of classes for the data and targets.\n    random_seed: A random seed for numpy to use.\n\n  Returns:\n    A tuple of `tf.data.Datasets`.\n  """"""\n  if random_seed:\n    np.random.seed(random_seed)\n\n  num_sample = train_samples + test_samples\n  templates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\n  y = np.random.randint(0, num_classes, size=(num_sample,))\n  x = np.zeros((num_sample,) + input_shape, dtype=np.float32)\n  for i in range(num_sample):\n    x[i] = templates[y[i]] + np.random.normal(loc=0, scale=1., size=input_shape)\n\n  train_dataset = tf.data.Dataset.from_tensor_slices(\n      (x[:train_samples], y[:train_samples]))\n  test_dataset = tf.data.Dataset.from_tensor_slices(\n      (x[train_samples:], y[train_samples:]))\n  return train_dataset, test_dataset\n'"
adanet/experimental/phases/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet ModelFlow phases.""""""\n\nfrom adanet.experimental.phases.autoensemble_phase import AutoEnsemblePhase\nfrom adanet.experimental.phases.input_phase import InputPhase\nfrom adanet.experimental.phases.keras_trainer_phase import KerasTrainerPhase\nfrom adanet.experimental.phases.keras_tuner_phase import KerasTunerPhase\nfrom adanet.experimental.phases.repeat_phase import RepeatPhase\n\n\n__all__ = [\n    ""AutoEnsemblePhase"",\n    ""InputPhase"",\n    ""KerasTrainerPhase"",\n    ""KerasTunerPhase"",\n    ""RepeatPhase"",\n]\n'"
adanet/experimental/phases/autoensemble_phase.py,11,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase that automatically ensembles models.""""""\n\nimport abc\nimport random\n\nfrom typing import Iterable, Iterator, List\nfrom adanet.experimental.keras.ensemble_model import EnsembleModel\nfrom adanet.experimental.keras.ensemble_model import MeanEnsemble\nfrom adanet.experimental.phases.phase import DatasetProvider\nfrom adanet.experimental.phases.phase import ModelProvider\nfrom adanet.experimental.storages.in_memory_storage import InMemoryStorage\nfrom adanet.experimental.storages.storage import Storage\nfrom adanet.experimental.work_units.keras_trainer_work_unit import KerasTrainerWorkUnit\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass EnsembleStrategy(abc.ABC):\n  """"""An abstract ensemble strategy.""""""\n\n  @abc.abstractmethod\n  def __call__(\n      self, candidates: List[tf.keras.Model]) -> Iterable[List[tf.keras.Model]]:\n    pass\n\n\nclass Ensembler(abc.ABC):\n  """"""An abstract ensembler.""""""\n\n  def __init__(self, loss, optimizer, metrics):\n    self._loss = loss\n    self._optimizer = optimizer\n    self._metrics = metrics\n\n  @abc.abstractmethod\n  def __call__(self, submodels: List[tf.keras.Model]) -> EnsembleModel:\n    pass\n\n\nclass MeanEnsembler(Ensembler):\n  """"""An ensembler that averages the weights of submodel outputs.""""""\n\n  def __init__(self, loss, optimizer, metrics, freeze_submodels=True):\n    super().__init__(loss, optimizer, metrics)\n    self._freeze_submodels = freeze_submodels\n\n  def __call__(self, submodels: List[tf.keras.Model]) -> EnsembleModel:\n    ensemble = MeanEnsemble(submodels, freeze_submodels=self._freeze_submodels)\n    if self._freeze_submodels:\n      for layer in ensemble.layers:\n        layer.trainable = False\n    # Compile SGD with learning rate set to 0 for no weight updates.\n    ensemble.compile(\n        loss=self._loss, optimizer=tf.keras.optimizers.SGD(0),\n        metrics=self._metrics)\n    return ensemble\n\n\nclass GrowStrategy(EnsembleStrategy):\n  """"""An ensemble strategy that adds one candidate to the ensemble at a time.""""""\n\n  def __call__(\n      self, candidates: List[tf.keras.Model]) -> Iterable[List[tf.keras.Model]]:\n    return [[candidate] for candidate in candidates]\n\n\nclass AllStrategy(EnsembleStrategy):\n  """"""An ensemble strategy that adds all candidates to the ensemble.""""""\n\n  def __call__(\n      self, candidates: List[tf.keras.Model]) -> Iterable[List[tf.keras.Model]]:\n    return [candidates]\n\n\nclass RandomKStrategy(EnsembleStrategy):\n  """"""An ensemble strategy that adds k random candidates (with replacement).""""""\n\n  def __init__(self, k, seed=None):\n    """"""Initializes a RandomKStrategy ensemble strategy.\n\n    Args:\n      k: Number of candidates to sample.\n      seed: Random seed.\n    """"""\n    self._k = k\n    self._seed = seed\n\n  def __call__(\n      self, candidates: List[tf.keras.Model]) -> Iterable[List[tf.keras.Model]]:\n    if self._seed:\n      random_state = random.getstate()\n      random.seed(self._seed)\n      candidates = [random.choices(candidates, k=self._k)]\n      random_state = random.setstate(random_state)\n    else:\n      candidates = [random.choices(candidates, k=self._k)]\n    return [candidates]\n\n\nclass AutoEnsemblePhase(DatasetProvider, ModelProvider):\n  """"""A phase that automatically ensembles models from a prior phase.""""""\n\n  def __init__(self,\n               ensemblers: List[Ensembler],\n               ensemble_strategies: List[EnsembleStrategy],\n               storage: Storage = InMemoryStorage(),\n               num_candidates: int = None):\n    """"""Initializes an AutoEnsemblePhase.\n\n    Args:\n      ensemblers: A list of `Ensembler` instances to determine how to combine\n        subnetworks.\n      ensemble_strategies: A list of `EnsembleStrategy` instances to determine\n        which subnetworks compose an ensemble.\n      storage: A `Storage` instance to store models and model metadata.\n      num_candidates: The number of subnetwork candidates to consider from the\n        previous phase. If `None` then all of the subnetworks generated in the\n        previous phase will be considered.\n    """"""\n\n    super().__init__(storage)\n    self._ensemblers = ensemblers\n    self._ensemble_strategies = ensemble_strategies\n    self._num_candidates = num_candidates\n\n  def work_units(self, previous_phase) -> Iterator[WorkUnit]:\n    self._train_dataset = previous_phase.get_train_dataset()\n    self._eval_dataset = previous_phase.get_eval_dataset()\n    if self._num_candidates:\n      candidates = previous_phase.get_best_models(\n          num_models=self._num_candidates)\n    else:\n      candidates = previous_phase.get_models()\n    if self.get_best_models():\n      current_best_ensemble = list(self.get_best_models())[0]\n    else:\n      current_best_ensemble = None\n\n    for ensemble_strategy in self._ensemble_strategies:\n      for submodels in ensemble_strategy(candidates):\n        for ensembler in self._ensemblers:\n          if current_best_ensemble:\n            previous_ensemble = current_best_ensemble.submodels\n          else:\n            previous_ensemble = []\n          ensemble = ensembler(previous_ensemble + submodels)\n          yield KerasTrainerWorkUnit(ensemble,\n                                     previous_phase.get_train_dataset(),\n                                     previous_phase.get_eval_dataset(),\n                                     self._storage)\n\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    return self._storage.get_models()\n\n  def get_best_models(self, num_models=1) -> Iterable[tf.keras.Model]:\n    return self._storage.get_best_models(num_models)\n\n  # TODO: Add some way to check that work_units has to be called\n  # before accessing these methods.\n  def get_train_dataset(self) -> tf.data.Dataset:\n    return self._train_dataset\n\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    return self._eval_dataset\n'"
adanet/experimental/phases/input_phase.py,6,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase that provides datasets.""""""\n\nfrom typing import Optional\nfrom adanet.experimental.phases.phase import DatasetProvider\nfrom adanet.experimental.phases.phase import Phase\nimport tensorflow.compat.v2 as tf\n\n\nclass InputPhase(DatasetProvider):\n  """"""A phase that simply relays train and eval datasets.""""""\n\n  def __init__(self, train_dataset: tf.data.Dataset,\n               eval_dataset: tf.data.Dataset):\n    """"""Initializes an InputPhase.\n\n    Args:\n      train_dataset: A `tf.data.Dataset` for training.\n      eval_dataset: A `tf.data.Dataset` for evaluation.\n    """"""\n\n    self._train_dataset = train_dataset\n    self._eval_dataset = eval_dataset\n\n  def get_train_dataset(self) -> tf.data.Dataset:\n    return self._train_dataset\n\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    return self._eval_dataset\n\n  def work_units(self, previous_phase: Optional[Phase]):\n    return []\n'"
adanet/experimental/phases/keras_trainer_phase.py,8,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase in the AdaNet workflow.""""""\n\nfrom typing import Callable, Iterable, Iterator, Union\nfrom adanet.experimental.phases.phase import DatasetProvider\nfrom adanet.experimental.phases.phase import ModelProvider\nfrom adanet.experimental.storages.in_memory_storage import InMemoryStorage\nfrom adanet.experimental.storages.storage import Storage\nfrom adanet.experimental.work_units.keras_trainer_work_unit import KerasTrainerWorkUnit\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass KerasTrainerPhase(DatasetProvider, ModelProvider):\n  """"""Trains Keras models.""""""\n\n  def __init__(self,\n               models: Union[Iterable[tf.keras.Model],\n                             Callable[[], Iterable[tf.keras.Model]]],\n               storage: Storage = InMemoryStorage()):\n    """"""Initializes a KerasTrainerPhase.\n\n    Args:\n      models: A list of `tf.keras.Model` instances or a list of callables that\n        return `tf.keras.Model` instances.\n      storage: A `Storage` instance.\n    """"""\n    # TODO: Consume arbitary fit inputs.\n    # Dataset should be wrapped inside a work unit.\n    # For instance when you create KerasTrainer work unit the dataset is\n    # encapsulated inside that work unit.\n    # What if you want to run on different (parts of the) datasets\n    # what if a work units consumes numpy arrays?\n    super().__init__(storage)\n    self._models = models\n\n  def work_units(self, previous_phase: DatasetProvider) -> Iterator[WorkUnit]:\n    self._train_dataset = previous_phase.get_train_dataset()\n    self._eval_dataset = previous_phase.get_eval_dataset()\n    models = self._models\n    if callable(models):\n      models = models()\n    for model in models:\n      yield KerasTrainerWorkUnit(model, self._train_dataset, self._eval_dataset,\n                                 self._storage)\n\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    return self._storage.get_models()\n\n  def get_best_models(self, num_models) -> Iterable[tf.keras.Model]:\n    return self._storage.get_best_models(num_models)\n\n  def get_train_dataset(self) -> tf.data.Dataset:\n    return self._train_dataset\n\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    return self._eval_dataset\n'"
adanet/experimental/phases/keras_tuner_phase.py,4,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase in the AdaNet workflow.""""""\n\nimport sys\n\nfrom typing import Callable, Iterable, Iterator, Union\nfrom adanet.experimental.phases.phase import DatasetProvider\nfrom adanet.experimental.phases.phase import ModelProvider\nfrom adanet.experimental.work_units.keras_tuner_work_unit import KerasTunerWorkUnit\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nfrom kerastuner.engine.tuner import Tuner\nimport tensorflow.compat.v2 as tf\n\n\nclass KerasTunerPhase(DatasetProvider, ModelProvider):\n  """"""Tunes Keras Model hyperparameters using the Keras Tuner.""""""\n\n  def __init__(self, tuner: Union[Callable[..., Tuner], Tuner], *search_args,\n               **search_kwargs):\n    """"""Initializes a KerasTunerPhase.\n\n    Args:\n      tuner: A `kerastuner.tuners.tuner.Tuner` instance or a callable that\n        returns a `kerastuner.tuners.tuner.Tuner` instance.\n      *search_args: Arguments to pass to the tuner search method.\n      **search_kwargs: Keyword arguments to pass to the tuner search method.\n    """"""\n\n    if callable(tuner):\n      self._tuner = tuner()\n    else:\n      self._tuner = tuner\n    self._search_args = search_args\n    self._search_kwargs = search_kwargs\n\n  def work_units(self, previous_phase: DatasetProvider) -> Iterator[WorkUnit]:\n    self._train_dataset = previous_phase.get_train_dataset()\n    self._eval_dataset = previous_phase.get_eval_dataset()\n    yield KerasTunerWorkUnit(\n        self._tuner,\n        x=self._train_dataset,\n        validation_data=self._eval_dataset,\n        *self._search_args,\n        **self._search_kwargs)\n\n  # TODO: Find a better way to get all models than to pass in a\n  # large number.\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    return self._tuner.get_best_models(num_models=sys.maxsize)\n\n  def get_best_models(self, num_models) -> Iterable[tf.keras.Model]:\n    return self._tuner.get_best_models(num_models=num_models)\n\n  def get_train_dataset(self) -> tf.data.Dataset:\n    return self._train_dataset\n\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    return self._eval_dataset\n'"
adanet/experimental/phases/phase.py,4,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase in the AdaNet workflow.""""""\n\nimport abc\n\nfrom typing import Iterable, Iterator, Optional\nfrom adanet.experimental.storages.in_memory_storage import InMemoryStorage\nfrom adanet.experimental.storages.storage import Storage\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass Phase(abc.ABC):\n  """"""A stage in a linear workflow.""""""\n\n  def __init__(self, storage: Storage = InMemoryStorage()):\n    self._storage = storage\n\n  # TODO: Find a better way to ensure work_units only gets called\n  # once per phase.\n  @abc.abstractmethod\n  def work_units(self, previous_phase: Optional[\'Phase\']) -> Iterator[WorkUnit]:\n    pass\n\n\nclass DatasetProvider(Phase, abc.ABC):\n  """"""An interface for a phase that produces datasets.""""""\n\n  def __init__(self, storage: Storage = InMemoryStorage()):\n    """"""Initializes a Phase.\n\n    Args:\n      storage: A `Storage` instance.\n    """"""\n\n    super().__init__(storage)\n    self._train_dataset = None\n    self._eval_dataset = None\n\n  @abc.abstractmethod\n  def get_train_dataset(self) -> tf.data.Dataset:\n    """"""Returns the dataset for train data.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    """"""Returns the dataset for eval data.""""""\n    pass\n\n\nclass ModelProvider(Phase, abc.ABC):\n  """"""An interface for a phase that produces models.""""""\n\n  @abc.abstractmethod\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    """"""Returns the models produced by this phase.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_best_models(self, num_models: int = 1) -> Iterable[tf.keras.Model]:\n    """"""Returns the `k` best models produced by this phase.""""""\n    pass\n\n'"
adanet/experimental/phases/repeat_phase.py,4,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A phase that repeats its inner phases.""""""\n\nfrom typing import Callable, Iterable, Iterator, List\nfrom adanet.experimental.phases.phase import DatasetProvider\nfrom adanet.experimental.phases.phase import ModelProvider\nfrom adanet.experimental.phases.phase import Phase\nfrom adanet.experimental.work_units.work_unit import WorkUnit\nimport tensorflow.compat.v2 as tf\n\n\nclass RepeatPhase(DatasetProvider, ModelProvider):\n  """"""A phase that repeats its inner phases.""""""\n\n  def __init__(self,\n               phase_factory: List[Callable[..., Phase]],\n               repetitions: int):\n    self._phase_factory = phase_factory\n    self._repetitions = repetitions\n    self._final_phase = None\n    """"""Initializes a RepeatPhase.\n\n    Args:\n      phase_factory: A list of callables that return `Phase` instances.\n      repetitions: Number of times to repeat the phases in the phase factory.\n    """"""\n\n  def work_units(self, previous_phase: DatasetProvider) -> Iterator[WorkUnit]:\n    for _ in range(self._repetitions):\n      # Each repetition, the ""first"" previous phase is the one preceeding the\n      # repeat phase itself.\n      prev_phase = previous_phase\n      for phase in self._phase_factory:\n        phase = phase()\n        for work_unit in phase.work_units(prev_phase):\n          yield work_unit\n        prev_phase = phase\n    self._final_phase = prev_phase\n\n  def get_train_dataset(self) -> tf.data.Dataset:\n    if not isinstance(self._final_phase, DatasetProvider):\n      raise NotImplementedError(\n          \'The last phase in repetition does not provide datasets.\')\n    return self._final_phase.get_train_dataset()\n\n  def get_eval_dataset(self) -> tf.data.Dataset:\n    if not isinstance(self._final_phase, DatasetProvider):\n      raise NotImplementedError(\n          \'The last phase in repetition does not provide datasets.\')\n    return self._final_phase.get_eval_dataset()\n\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    if not isinstance(self._final_phase, ModelProvider):\n      raise NotImplementedError(\n          \'The last phase in repetition does not provide models.\')\n    return self._final_phase.get_models()\n\n  def get_best_models(self, num_models=1) -> Iterable[tf.keras.Model]:\n    if not isinstance(self._final_phase, ModelProvider):\n      raise NotImplementedError(\n          \'The last phase in repetition does not provide models.\')\n    return self._final_phase.get_best_models(num_models)\n'"
adanet/experimental/schedulers/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet ModelFlow schedulers.""""""\n\nfrom adanet.experimental.schedulers.in_process_scheduler import InProcessScheduler\n\n\n__all__ = [\n    ""InProcessScheduler"",\n]\n'"
adanet/experimental/schedulers/in_process_scheduler.py,0,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""An in process scheduler for managing AdaNet phases.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom typing import Iterator\n\nfrom adanet.experimental.schedulers import scheduler\nfrom adanet.experimental.work_units.work_unit import WorkUnit\n\n\nclass InProcessScheduler(scheduler.Scheduler):\n  """"""A scheduler that executes in a single process.""""""\n\n  def schedule(self, work_units: Iterator[WorkUnit]):\n    """"""Schedules and execute work units in a single process.\n\n    Args:\n      work_units: An iterator that yields `WorkUnit` instances.\n    """"""\n\n    for work_unit in work_units:\n      work_unit.execute()\n'"
adanet/experimental/schedulers/scheduler.py,0,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A scheduler for managing AdaNet phases.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nfrom typing import Iterator\n\nfrom adanet.experimental.work_units.work_unit import WorkUnit\n\n\nclass Scheduler(abc.ABC):\n  """"""Abstract interface for a scheduler to be used in ModelFlow pipelines.""""""\n\n  @abc.abstractmethod\n  def schedule(self, work_units: Iterator[WorkUnit]):\n    """"""Schedules and executes work units.\n\n    Args:\n      work_units: An iterator that yields `WorkUnit` instances.\n    """"""\n    pass\n'"
adanet/experimental/storages/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet ModelFlow storages.""""""\n\nfrom adanet.experimental.storages.in_memory_storage import InMemoryStorage\n\n\n__all__ = [\n    ""InMemoryStorage"",\n]\n'"
adanet/experimental/storages/in_memory_storage.py,2,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A storage for persisting results and managing stage.""""""\n\nimport heapq\n\nfrom typing import List\nfrom adanet.experimental.storages.storage import ModelContainer\nfrom adanet.experimental.storages.storage import Storage\nimport tensorflow.compat.v2 as tf\n\n\nclass InMemoryStorage(Storage):\n  """"""In memory storage for testing-only.\n\n  Uses a priority queue under the hood to sort the models according to their\n  score.\n\n  Currently the only supported score is \'loss\'.\n  """"""\n\n  def __init__(self):\n    self._model_containers = []\n\n  def save_model(self, model_container: ModelContainer):\n    """"""Stores a model.\n\n    Args:\n      model_container: A `ModelContainer` instance.\n    """"""\n    # We use a counter since heappush will compare on the second item in the\n    # tuple in the case of a tie in the first item comparison. This is for the\n    # off chance that two models have the same loss.\n    heapq.heappush(self._model_containers, model_container)\n\n  def get_models(self) -> List[tf.keras.Model]:\n    """"""Returns all stored models.""""""\n    return [c.model for c in self._model_containers]\n\n  def get_best_models(self, num_models: int = 1) -> List[tf.keras.Model]:\n    """"""Returns the top `num_models` stored models in descending order.""""""\n    return [c.model\n            for c in heapq.nsmallest(num_models, self._model_containers)]\n\n  def get_model_metrics(self) -> List[List[float]]:\n    """"""Returns the metrics for all stored models.""""""\n    return [c.metrics for c in self._model_containers]\n'"
adanet/experimental/storages/storage.py,3,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A storage for persisting results and managing stage.""""""\n\nimport abc\n\nfrom typing import Iterable, List\nimport tensorflow.compat.v2 as tf\n\n\nclass ModelContainer:\n  """"""A container for a model and its metadata.""""""\n\n  def __init__(self, score: float, model: tf.keras.Model, metrics: List[float]):\n    self.score = score\n    self.model = model\n    self.metrics = metrics\n\n  def __eq__(self, other: \'ModelContainer\'):\n    return self.score == other.score\n\n  def __lt__(self, other: \'ModelContainer\'):\n    return self.score < other.score\n\n\nclass Storage(abc.ABC):\n  """"""A storage for persisting results and managing state.""""""\n\n  @abc.abstractmethod\n  def save_model(self, model_container: ModelContainer):\n    """"""Stores a model and its metadata.""""""\n    # TODO: How do we enforce that save_model is called only once per\n    # model?\n    pass\n\n  @abc.abstractmethod\n  def get_models(self) -> Iterable[tf.keras.Model]:\n    """"""Returns all stored models.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_best_models(self, num_models: int = 1) -> Iterable[tf.keras.Model]:\n    """"""Returns the top `num_models` stored models in descending order.""""""\n    pass\n\n  @abc.abstractmethod\n  def get_model_metrics(self) -> Iterable[Iterable[float]]:\n    """"""Returns the metrics for all stored models.""""""\n    pass\n'"
adanet/experimental/work_units/__init__.py,0,"b'# Lint as: python3\n# Copyright 2020 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""AdaNet ModelFlow work units.""""""\n\nfrom adanet.experimental.work_units.keras_trainer_work_unit import KerasTrainerWorkUnit\nfrom adanet.experimental.work_units.keras_tuner_work_unit import KerasTunerWorkUnit\n\n\n__all__ = [\n    ""KerasTrainerWorkUnit"",\n    ""KerasTunerWorkUnit"",\n]\n'"
adanet/experimental/work_units/keras_trainer_work_unit.py,4,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A work unit for training, evaluating, and saving a Keras model.""""""\n\nimport os\nimport time\n\nfrom adanet.experimental.storages.storage import ModelContainer\nfrom adanet.experimental.storages.storage import Storage\nfrom adanet.experimental.work_units import work_unit\nimport tensorflow.compat.v2 as tf\n\n\nclass KerasTrainerWorkUnit(work_unit.WorkUnit):\n  """"""Trains, evaluates, and saves a Keras model.""""""\n\n  def __init__(self, model: tf.keras.Model,\n               train_dataset: tf.data.Dataset,\n               eval_dataset: tf.data.Dataset,\n               storage: Storage,\n               tensorboard_base_dir: str = \'/tmp\'):\n    self._model = model\n    self._train_dataset = train_dataset\n    self._eval_dataset = eval_dataset\n    self._storage = storage\n    self._tensorboard_base_dir = tensorboard_base_dir\n\n  # TODO: Allow better customization of TensorBoard log_dir.\n  def execute(self):\n    log_dir = os.path.join(self._tensorboard_base_dir, str(int(time.time())))\n    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                                 update_freq=\'batch\')\n    if self._model.trainable:\n      self._model.fit(self._train_dataset, callbacks=[tensorboard])\n    else:\n      print(\'Skipping training since model.trainable set to false.\')\n    results = self._model.evaluate(self._eval_dataset, callbacks=[tensorboard])\n    # If the model was compiled with metrics, the results is a list of loss +\n    # metric values. If the model was compiled without metrics, it is a loss\n    # scalar.\n    if not isinstance(results, list):\n      results = [results]\n    self._storage.save_model(ModelContainer(results[0], self._model, results))\n'"
adanet/experimental/work_units/keras_tuner_work_unit.py,1,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A work unit for training, evaluating, and saving a Keras model.""""""\n\nimport os\nimport time\n\nfrom adanet.experimental.work_units import work_unit\nfrom kerastuner.engine.tuner import Tuner\nimport tensorflow.compat.v2 as tf\n\n\nclass KerasTunerWorkUnit(work_unit.WorkUnit):\n  """"""Trains, evaluates and saves a tuned Keras model.""""""\n\n  def __init__(self, tuner: Tuner, *search_args, **search_kwargs):\n    self._tuner = tuner\n    self._search_args = search_args\n    self._search_kwargs = search_kwargs\n\n  # TODO: Allow better customization of TensorBoard log_dir.\n  def execute(self):\n    log_dir = os.path.join(\'/tmp\', str(int(time.time())))\n    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                                 update_freq=\'batch\')\n    # We don\'t need to eval and store, because the Tuner does it for us.\n    self._tuner.search(callbacks=[tensorboard], *self._search_args,\n                       **self._search_kwargs)\n'"
adanet/experimental/work_units/work_unit.py,0,"b'# Lint as: python3\n# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A work unit for an AdaNet scheduler.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\n\nclass WorkUnit(abc.ABC):\n\n  @abc.abstractproperty\n  def execute(self):\n    pass\n'"
research/improve_nas/trainer/__init__.py,0,"b'""""""Copyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n'"
research/improve_nas/trainer/adanet_improve_nas.py,4,"b'# Lint as: python3\n""""""Defines adanet estimator builder.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport adanet\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import improve_nas\n  from adanet.research.improve_nas.trainer import optimizer\nexcept ImportError as e:\n  from trainer import improve_nas\n  from trainer import optimizer\n# pylint: enable=g-import-not-at-top\n\n\nclass GeneratorType(object):\n  """"""Controls what generator is used.""""""\n  DYNAMIC = ""dynamic""\n  SIMPLE = ""simple""\n\n\nclass Builder(object):\n  """"""An AdaNet estimator builder.""""""\n\n  def estimator(self,\n                data_provider,\n                run_config,\n                hparams,\n                train_steps=None,\n                seed=None):\n    """"""Returns an AdaNet `Estimator` for train and evaluation.\n\n    Args:\n      data_provider: Data `Provider` for dataset to model.\n      run_config: `RunConfig` object to configure the runtime settings.\n      hparams: `HParams` instance defining custom hyperparameters.\n      train_steps: number of train steps.\n      seed: An integer seed if determinism is required.\n\n    Returns:\n      Returns an `Estimator`.\n    """"""\n\n    max_iteration_steps = int(train_steps / hparams.boosting_iterations)\n\n    optimizer_fn = optimizer.fn_with_name(\n        hparams.optimizer,\n        learning_rate_schedule=hparams.learning_rate_schedule,\n        cosine_decay_steps=max_iteration_steps)\n    hparams.add_hparam(""total_training_steps"", max_iteration_steps)\n\n    if hparams.generator == GeneratorType.SIMPLE:\n      subnetwork_generator = improve_nas.Generator(\n          feature_columns=data_provider.get_feature_columns(),\n          optimizer_fn=optimizer_fn,\n          iteration_steps=max_iteration_steps,\n          checkpoint_dir=run_config.model_dir,\n          hparams=hparams,\n          seed=seed)\n    elif hparams.generator == GeneratorType.DYNAMIC:\n      subnetwork_generator = improve_nas.DynamicGenerator(\n          feature_columns=data_provider.get_feature_columns(),\n          optimizer_fn=optimizer_fn,\n          iteration_steps=max_iteration_steps,\n          checkpoint_dir=run_config.model_dir,\n          hparams=hparams,\n          seed=seed)\n    else:\n      raise ValueError(""Invalid generator: `%s`"" % hparams.generator)\n\n    evaluator = None\n    if hparams.use_evaluator:\n      evaluator = adanet.Evaluator(\n          input_fn=data_provider.get_input_fn(\n              partition=""train"",\n              mode=tf.estimator.ModeKeys.EVAL,\n              batch_size=hparams.evaluator_batch_size),\n          steps=hparams.evaluator_steps)\n\n    return adanet.Estimator(\n        head=data_provider.get_head(),\n        subnetwork_generator=subnetwork_generator,\n        max_iteration_steps=max_iteration_steps,\n        adanet_lambda=hparams.adanet_lambda,\n        adanet_beta=hparams.adanet_beta,\n        mixture_weight_type=hparams.mixture_weight_type,\n        force_grow=hparams.force_grow,\n        evaluator=evaluator,\n        config=run_config,\n        model_dir=run_config.model_dir)\n\n  def hparams(self, default_batch_size, hparams_string):\n    """"""Returns hyperparameters, including any flag value overrides.\n\n    In order to allow for automated hyperparameter tuning, model hyperparameters\n    are aggregated within a tf.HParams object.  In this case, here are the\n    hyperparameters and their descriptions:\n    - optimizer: Name of the optimizer to use. See `optimizers.fn_with_name`.\n    - learning_rate_schedule: Learning rate schedule string.\n    - initial_learning_rate: The initial learning rate to use during training.\n    - num_cells: Number of cells in the model. Must be divisible by 3.\n    - num_conv_filters: The initial number of convolutional filters. The final\n        layer will have 24*num_conv_filters channels.\n    - weight_decay: Float amount of weight decay to apply to train loss.\n    - use_aux_head: Whether to create an auxiliary head for training. This adds\n        some non-determinism to training.\n    - knowledge_distillation: Whether subnetworks should learn from the\n        logits of the \'previous ensemble\'/\'previous subnetwork\' in addition to\n        the labels to distill/transfer/compress the knowledge in a manner\n        inspired by Born Again Networks [Furlanello et al., 2018]\n        (https://arxiv.org/abs/1805.04770) and Distilling the Knowledge in\n        a Neural Network [Hinton at al., 2015]\n        (https://arxiv.org/abs/1503.02531).\n    - model_version: See `improve_nas.ModelVersion`.\n    - adanet_lambda: See `adanet.Estimator`.\n    - adanet_beta: See `adanet.Estimator`.\n    - generator: Type of generator. `simple` generator is just ensembling,\n        `dynamic` generator gradually grows the network.\n    - boosting_iterations: The number of boosting iterations to perform. The\n      final ensemble will have at most this many subnetworks comprising it.\n    - evaluator_batch_size: Batch size for the evaluator to use when comparing\n        candidates.\n    - evaluator_steps: Number of batches for the evaluator to use when\n        comparing candidates.\n    - learn_mixture_weights: Whether to learn adanet mixture weights.\n    - mixture_weight_type: Type of mxture weights.\n    - batch_size: Batch size for training.\n    - force_grow: Force AdaNet to add a candidate in each itteration, even if it\n        would decreases the performance of the ensemble.\n    - label_smoothing: Strength of label smoothing that will be applied (even\n        non true labels will have a non zero representation in one hot encoding\n        when computing loss).\n    - clip_gradients: Clip gradient to this value.\n    - aux_head_weight: NASNet cell parameter. Weight of auxiliary loss.\n    - stem_multiplier: NASNet cell parameter.\n    - drop_path_keep_prob: NASNet cell parameter. Propability for drop_path\n        regularization.\n    - dense_dropout_keep_prob: NASNet cell parameter. Dropout keep probability.\n    - filter_scaling_rate: NASNet cell parameter. Controls growth of number of\n        filters.\n    - num_reduction_layers: NASNet cell parameter. Number of reduction layers\n        that will be added to the architecture.\n    - data_format: NASNet cell parameter. Controls whether data is in channels\n        last or channels first format.\n    - skip_reduction_layer_input: NASNet cell parameter. Whether to skip\n        reduction layer.\n    - use_bounded_activation: NASNet cell parameter. Whether to use bounded\n        activations.\n    - use_evaluator: Boolean whether to use the adanet.Evaluator to choose the\n        best ensemble at each round.\n\n    Args:\n      default_batch_size: The default batch_size specified for training.\n      hparams_string: If the hparams_string is given, then it will use any\n        values specified in hparams to override any individually-set\n        hyperparameter. This logic allows tuners to override hyperparameter\n        settings to find optimal values.\n\n    Returns:\n      The hyperparameters as a tf.HParams object.\n    """"""\n    hparams = tf.contrib.training.HParams(\n        # Nasnet config hparams (default cifar config)\n        num_cells=3,\n        num_conv_filters=10,\n        aux_head_weight=0.4,\n        stem_multiplier=3.0,\n        drop_path_keep_prob=0.6,\n        use_aux_head=True,\n        dense_dropout_keep_prob=1.0,\n        filter_scaling_rate=2.0,\n        num_reduction_layers=2,\n        data_format=""NHWC"",\n        skip_reduction_layer_input=0,\n        use_bounded_activation=False,\n        # Other hparams\n        clip_gradients=5,\n        optimizer=""momentum"",\n        learning_rate_schedule=""cosine"",\n        initial_learning_rate=.025,\n        weight_decay=5e-4,\n        label_smoothing=0.1,\n        knowledge_distillation=improve_nas.KnowledgeDistillation.ADAPTIVE,\n        model_version=""cifar"",\n        adanet_lambda=0.,\n        adanet_beta=0.,\n        generator=GeneratorType.SIMPLE,\n        boosting_iterations=3,\n        force_grow=True,\n        evaluator_batch_size=-1,\n        evaluator_steps=-1,\n        batch_size=default_batch_size,\n        learn_mixture_weights=False,\n        mixture_weight_type=adanet.MixtureWeightType.SCALAR,\n        use_evaluator=True,\n    )\n    if hparams_string:\n      hparams = hparams.parse(hparams_string)\n    if hparams.evaluator_batch_size < 0:\n      hparams.evaluator_batch_size = default_batch_size\n    if hparams.evaluator_steps < 0:\n      hparams.evaluator_steps = None\n    return hparams\n'"
research/improve_nas/trainer/adanet_improve_nas_test.py,8,"b'# Lint as: python3\n""""""Tests for improve_nas.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom adanet.research.improve_nas.trainer import adanet_improve_nas\nfrom adanet.research.improve_nas.trainer import fake_data\nimport tensorflow.compat.v1 as tf\n\n\nclass AdaNetQuetzalBuilderTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""simple_generator"",\n      ""hparams_string"": (""optimizer=sgd,boosting_iterations=2,generator=simple,""\n                         ""initial_learning_rate=.1,use_aux_head=False,""\n                         ""num_cells=3,num_conv_filters=2,use_evaluator=False""),\n  }, {\n      ""testcase_name"": ""dynamic_generator"",\n      ""hparams_string"":\n          (""optimizer=sgd,boosting_iterations=1,generator=dynamic,""\n           ""initial_learning_rate=.1,use_aux_head=False,""\n           ""num_cells=3,num_conv_filters=2,use_evaluator=False""),\n  })\n  def test_estimator(self,\n                     hparams_string,\n                     batch_size=1):\n    """"""Structural test to make sure Estimator Builder works.""""""\n\n    seed = 42\n\n    # Set up and clean test directory.\n    model_dir = os.path.join(flags.FLAGS.test_tmpdir,\n                             ""AdanetImproveNasBuilderTest"")\n    if tf.gfile.Exists(model_dir):\n      tf.gfile.DeleteRecursively(model_dir)\n    tf.gfile.MkDir(model_dir)\n\n    data_provider = fake_data.FakeImageProvider(seed=seed)\n    estimator_builder = adanet_improve_nas.Builder()\n    hparams = estimator_builder.hparams(\n        default_batch_size=3, hparams_string=hparams_string)\n    run_config = tf.estimator.RunConfig(\n        tf_random_seed=seed, model_dir=model_dir)\n    _ = data_provider.get_input_fn(\n        ""train"",\n        tf.estimator.ModeKeys.TRAIN,\n        batch_size=batch_size)\n    test_input_fn = data_provider.get_input_fn(\n        ""test"",\n        tf.estimator.ModeKeys.EVAL,\n        batch_size=batch_size)\n\n    estimator = estimator_builder.estimator(\n        data_provider=data_provider,\n        run_config=run_config,\n        hparams=hparams,\n        train_steps=10,\n        seed=seed)\n    eval_metrics = estimator.evaluate(input_fn=test_input_fn, steps=1)\n\n    self.assertGreater(eval_metrics[""loss""], 0.0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
research/improve_nas/trainer/cifar10.py,10,"b'# Lint as: python3\n""""""CIFAR-10 data and convenience functions.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.keras.datasets import cifar10\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import image_processing\nexcept ImportError as e:\n  from trainer import image_processing\n# pylint: enable=g-import-not-at-top\n\nFEATURES = \'x\'\nPreprocessingType = image_processing.PreprocessingType\n\n\nclass Provider(object):\n  """"""A CIFAR-10 data provider.""""""\n\n  def __init__(self,\n               params_string=\'\',\n               seed=None):\n    """"""Returns a CIFAR-10 `Provider`.""""""\n    # For testing\n    self._seed = seed\n    default_params = tf.contrib.training.HParams(\n        cutout=True, augmentation=PreprocessingType.BASIC)\n    self._params = default_params.parse(params_string)\n\n  def _preprocess_data(self, image, label, training, preprocess):\n    """"""Apply Inception data augmentation and preprocessing.""""""\n\n    # Unpack `Element` tuple.\n    # image, label = element\n\n    if preprocess:\n      image_height, image_width = self._shape()[:2]\n      if self._params.augmentation == PreprocessingType.BASIC:\n        image = image_processing.resize_and_normalize(image, image_height,\n                                                      image_width)\n        if training:\n          image = image_processing.basic_augmentation(image, image_height,\n                                                      image_width, self._seed)\n      else:\n        raise ValueError(\'Unsupported data augmentation type: `%s`\' %\n                         self._params.augmentation)\n\n      if training and self._params.cutout:\n        # According to https://arxiv.org/abs/1708.04552, cutting out 16x16\n        # works best.\n        image = image_processing.cutout(image, pad_size=8, seed=self._seed)\n\n    # Set shapes so that they are defined.\n    image.set_shape(self._shape())\n    if label is not None:\n      label.set_shape([1])\n    return {FEATURES: image}, label\n\n  def _cifar10_dataset(self, partition):\n    """"""Returns a partition of the CIFAR-10 `Dataset`.""""""\n    cifar10_data = None\n    try:\n      cifar10_data = cifar10.load_data()\n      tf.logging.info(\'Loaded cifar10.\')\n    except:  # pylint: disable=bare-except\n      tf.logging.info(\n          \'Can not load cifar10 from internet. Creating dummy data for \'\n          \'testing.\')\n      data = np.zeros((3, 32, 32, 3))\n      labels = np.array([[5], [3], [9]])\n      data[:, 0, 0] = [148, 141, 174]\n      data[:, -1, 0, 0] = 128\n      cifar10_data = ((data, labels), (data, labels))\n    (x_train, y_train), (x_test, y_test) = cifar10_data\n    x = None\n    y = None\n    if partition == \'train\':\n      x, y = x_train, y_train\n    else:\n      x, y = x_test, y_test\n\n    dataset = tf.data.Dataset.from_tensor_slices((x, y.astype(np.int32)))\n    return dataset.cache()\n\n  def _shape(self):\n    """"""Returns a 3-dimensional list with the shape of the image.""""""\n    return [32, 32, 3]\n\n  def get_input_fn(self,\n                   partition,\n                   mode,\n                   batch_size,\n                   preprocess=True,\n                   use_tpu=False):\n    """"""See `data.Provider` get_input_fn.""""""\n\n    def input_fn(params=None):\n      """"""Provides batches of CIFAR images.\n\n      Args:\n        params: A dict containing the batch_size on TPU, otherwise None.\n\n      Returns:\n        images: A `Tensor` of size [batch_size, 32, 32, 3]\n        labels: A `Tensor` of size [batch_size, 1],\n      """"""\n\n      batch_size_ = batch_size\n      if use_tpu:\n        batch_size_ = params.get(\'batch_size\', batch_size)\n\n      training = mode == tf.estimator.ModeKeys.TRAIN\n      dataset = self._cifar10_dataset(partition)\n      dataset = dataset.map(\n          functools.partial(\n              self._preprocess_data, training=training, preprocess=preprocess))\n      if training:\n        dataset = dataset.apply(\n            tf.contrib.data.shuffle_and_repeat(\n                buffer_size=500, seed=self._seed))\n      return dataset.batch(\n          batch_size_,\n          drop_remainder=use_tpu).prefetch(tf.data.experimental.AUTOTUNE\n                                          ).make_one_shot_iterator().get_next()\n\n    return input_fn\n\n  def get_head(self, name=None):\n    """"""Returns a `Head` instance for multiclass CIFAR-10 with the given name.""""""\n    return tf.contrib.estimator.multi_class_head(\n        10, name=name, loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n  def get_feature_columns(self):\n    """"""Returns feature columns.""""""\n    feature_columns = [\n        tf.feature_column.numeric_column(key=FEATURES, shape=self._shape())\n    ]\n    return feature_columns\n'"
research/improve_nas/trainer/cifar100.py,10,"b'# Lint as: python3\n""""""CIFAR-100 data and convenience functions.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.keras.datasets import cifar100\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import image_processing\nexcept ImportError as e:\n  from trainer import image_processing\n# pylint: enable=g-import-not-at-top\n\nFEATURES = \'x\'\nPreprocessingType = image_processing.PreprocessingType\n\n\nclass Provider(object):\n  """"""A CIFAR-100 data provider.""""""\n\n  def __init__(self,\n               params_string=\'\',\n               seed=None):\n    """"""Returns a CIFAR-100 `Provider`.""""""\n    # For testing\n    self._seed = seed\n    default_params = tf.contrib.training.HParams(\n        cutout=True, augmentation=PreprocessingType.BASIC)\n    self._params = default_params.parse(params_string)\n\n  def _preprocess_data(self, image, label, training, preprocess):\n    """"""Apply Inception data augmentation and preprocessing.""""""\n\n    # Unpack `Element` tuple.\n    # image, label = element\n\n    if preprocess:\n      image_height, image_width = self._shape()[:2]\n      if self._params.augmentation == PreprocessingType.BASIC:\n        image = image_processing.resize_and_normalize(image, image_height,\n                                                      image_width)\n        if training:\n          image = image_processing.basic_augmentation(image, image_height,\n                                                      image_width, self._seed)\n      else:\n        raise ValueError(\'Unsupported data augmentation type: `%s`\' %\n                         self._params.augmentation)\n\n      if training and self._params.cutout:\n        # According to https://arxiv.org/abs/1708.04552, cutting out 16x16\n        # works best.\n        image = image_processing.cutout(image, pad_size=8, seed=self._seed)\n\n    # Set shapes so that they are defined.\n    image.set_shape(self._shape())\n    if label is not None:\n      label.set_shape([1])\n    return {FEATURES: image}, label\n\n  def _cifar100_dataset(self, partition):\n    """"""Returns a partition of the CIFAR-100 `Dataset`.""""""\n\n    cifar100_data = None\n    try:\n      cifar100_data = cifar100.load_data()\n      tf.logging.info(\'Loaded cifar100.\')\n    except:  # pylint: disable=bare-except\n      tf.logging.info(\n          \'Can not load cifar100 from internet. Creating dummy data for \'\n          \'testing.\')\n      data = np.zeros((3, 32, 32, 3))\n      labels = np.array([[47], [52], [5]])\n      data[:, 0, 0] = [220, 25, 47]\n      data[:, -1, 0, 0] = 128\n      cifar100_data = ((data, labels), (data, labels))\n\n    (x_train, y_train), (x_test, y_test) = cifar100_data\n\n    x = None\n    y = None\n    if partition == \'train\':\n      x, y = x_train, y_train\n    else:\n      x, y = x_test, y_test\n\n    dataset = tf.data.Dataset.from_tensor_slices((x, y.astype(np.int32)))\n    return dataset.cache()\n\n  def _shape(self):\n    """"""Returns a 3-dimensional list with the shape of the image.""""""\n    return [32, 32, 3]\n\n  def get_input_fn(self,\n                   partition,\n                   mode,\n                   batch_size,\n                   preprocess=True,\n                   use_tpu=False):\n    """"""See `data.Provider` get_input_fn.""""""\n\n    def input_fn(params=None):\n      """"""Provides batches of CIFAR images.\n\n      Args:\n        params: A dict containing the batch_size on TPU, otherwise None.\n\n      Returns:\n        images: A `Tensor` of size [batch_size, 32, 32, 3]\n        labels: A `Tensor` of size [batch_size, 1],\n      """"""\n\n      batch_size_ = batch_size\n      if use_tpu:\n        batch_size_ = params.get(\'batch_size\', batch_size)\n\n      training = mode == tf.estimator.ModeKeys.TRAIN\n      dataset = self._cifar100_dataset(partition)\n      dataset = dataset.map(\n          functools.partial(\n              self._preprocess_data, training=training, preprocess=preprocess))\n      if training:\n        dataset = dataset.apply(\n            tf.contrib.data.shuffle_and_repeat(\n                buffer_size=500, seed=self._seed))\n      return dataset.batch(\n          batch_size_,\n          drop_remainder=use_tpu).prefetch(tf.data.experimental.AUTOTUNE\n                                          ).make_one_shot_iterator().get_next()\n\n    return input_fn\n\n  def get_head(self, name=None):\n    """"""Returns a `Head` instance for CIFAR-100 with the given name.""""""\n    return tf.contrib.estimator.multi_class_head(\n        100, name=name, loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)\n\n  def get_feature_columns(self):\n    """"""Returns feature columns.""""""\n    feature_columns = [\n        tf.feature_column.numeric_column(key=FEATURES, shape=self._shape())\n    ]\n    return feature_columns\n'"
research/improve_nas/trainer/cifar100_test.py,11,"b'# Lint as: python3\n""""""Tests for cifar100 dataset.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.research.improve_nas.trainer import cifar100\nimport tensorflow.compat.v1 as tf\n\n\nclass Cifar100Test(tf.test.TestCase):\n\n  def _check_dimensions(self, partition):\n    provider = cifar100.Provider(seed=4)\n    input_fn = provider.get_input_fn(\n        partition, tf.contrib.learn.ModeKeys.TRAIN, batch_size=3)\n    data, labels = input_fn()\n    self.assertIn(cifar100.FEATURES, data)\n    features = data[cifar100.FEATURES]\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      self.assertEqual((3, 32, 32, 3), sess.run(features).shape)\n      self.assertEqual((3, 1), sess.run(labels).shape)\n\n  def test_read_cifar100(self):\n    for partition in [""train"", ""test""]:\n      self._check_dimensions(partition)\n\n  def test_no_preprocess(self):\n    provider = cifar100.Provider(seed=4)\n    input_fn = provider.get_input_fn(\n        ""train"",\n        tf.contrib.learn.ModeKeys.TRAIN,\n        batch_size=3,\n        preprocess=False)\n    data, label = input_fn()\n\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      self.assertAllEqual([220, 25, 47], sess.run(data[""x""])[0][0][0])\n      self.assertAllEqual([[47], [5], [52]], sess.run(label))\n\n  def test_basic_preprocess(self):\n    provider = cifar100.Provider(\n        params_string=""augmentation=basic"", seed=4)\n    input_fn = provider.get_input_fn(\n        ""train"",\n        tf.contrib.learn.ModeKeys.TRAIN,\n        batch_size=3,\n        preprocess=True)\n    data, label = input_fn()\n\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      data_result = sess.run(data[""x""])\n      self.assertEqual((3, 32, 32, 3), data_result.shape)\n      self.assertAllEqual([0, 0, 0], data_result[0, 0, 0])\n      self.assertAlmostEqual(0.0, data_result[0, -1, 0, 0], places=3)\n      self.assertAllEqual([[47], [5], [52]], sess.run(label))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
research/improve_nas/trainer/cifar10_test.py,11,"b'# Lint as: python3\n""""""Tests for cifar10 dataset.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom adanet.research.improve_nas.trainer import cifar10\nimport tensorflow.compat.v1 as tf\n\n\nclass Cifar10Test(tf.test.TestCase):\n\n  def _check_dimensions(self, partition):\n    provider = cifar10.Provider(seed=4)\n    input_fn = provider.get_input_fn(\n        partition, tf.contrib.learn.ModeKeys.TRAIN, batch_size=3)\n    data, labels = input_fn()\n    self.assertIn(cifar10.FEATURES, data)\n    features = data[cifar10.FEATURES]\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      self.assertEqual((3, 32, 32, 3), sess.run(features).shape)\n      self.assertEqual((3, 1), sess.run(labels).shape)\n\n  def test_read_cifar10(self):\n    for partition in [""train"", ""test""]:\n      self._check_dimensions(partition)\n\n  def test_no_preprocess(self):\n    provider = cifar10.Provider(seed=4)\n    input_fn = provider.get_input_fn(\n        ""train"",\n        tf.contrib.learn.ModeKeys.TRAIN,\n        batch_size=3,\n        preprocess=False)\n    data, label = input_fn()\n\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      data_result = sess.run(data[""x""])\n      self.assertEqual((3, 32, 32, 3), data_result.shape)\n      self.assertAllEqual([148, 141, 174], data_result[0][0][0])\n      self.assertAllEqual([[5], [9], [3]], sess.run(label))\n\n  def test_basic_preprocess(self):\n    provider = cifar10.Provider(\n        params_string=""augmentation=basic"", seed=4)\n    input_fn = provider.get_input_fn(\n        ""train"",\n        tf.contrib.learn.ModeKeys.TRAIN,\n        batch_size=3,\n        preprocess=True)\n    data, label = input_fn()\n\n    init = tf.group(tf.global_variables_initializer(),\n                    tf.local_variables_initializer())\n    with self.test_session() as sess:\n      sess.run(init)\n      data_result = sess.run(data[""x""])\n      self.assertEqual((3, 32, 32, 3), data_result.shape)\n      self.assertAllEqual([0, 0, 0], data_result[0, 0, 0])\n      self.assertAlmostEqual(0.0, data_result[0, -1, 0, 0], places=3)\n      self.assertAllEqual([[5], [9], [3]], sess.run(label))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
research/improve_nas/trainer/fake_data.py,9,"b'# Lint as: python3\n""""""Fake dataset for testing and debugging.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\nclass FakeImageProvider(object):\n  """"""A fake image data provider.""""""\n\n  def __init__(self,\n               num_examples=3,\n               num_classes=3,\n               image_dim=8,\n               channels=1,\n               seed=42):\n    self._num_examples = num_examples\n    self._num_classes = num_classes\n    self._seed = seed\n    self._channels = channels\n    self._image_dim = image_dim\n\n  def get_head(self, name=None):\n    return tf.contrib.estimator.multi_class_head(\n        self._num_classes, name=name, loss_reduction=tf.losses.Reduction.SUM)\n\n  def _shape(self):\n    return [self._image_dim, self._image_dim, self._channels]\n\n  def get_input_fn(self,\n                   partition,\n                   mode,\n                   batch_size):\n    """"""See `data.Provider` get_input_fn.""""""\n\n    del partition\n    def input_fn(params=None):\n      """"""Input_fn to return.""""""\n\n      del params  # Unused.\n\n      np.random.seed(self._seed)\n      if mode == tf.estimator.ModeKeys.EVAL:\n        np.random.seed(self._seed + 1)\n\n      images = tf.to_float(\n          tf.convert_to_tensor(\n              np.random.rand(self._num_examples, *self._shape())))\n      labels = tf.convert_to_tensor(\n          np.random.randint(0, high=2, size=(self._num_examples, 1)))\n      dataset = tf.data.Dataset.from_tensor_slices(({""x"": images}, labels))\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        dataset = dataset.repeat()\n      dataset = dataset.batch(batch_size)\n      iterator = dataset.make_one_shot_iterator()\n      return iterator.get_next()\n\n    return input_fn\n\n  def get_feature_columns(self):\n    feature_columns = [\n        tf.feature_column.numeric_column(key=""x"", shape=self._shape()),\n    ]\n    return feature_columns\n'"
research/improve_nas/trainer/image_processing.py,29,"b'# Lint as: python3\n""""""Image preprocessing and augmentation function for a single image.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\n\nclass PreprocessingType(object):\n  """"""Type of preprocessing to be applied on the image.\n\n  * `INCEPTION`: Preprocessing used in inception.\n  * `BASIC`: Minimalistic preprocessing used in NasNet for cifar.\n\n  """"""\n  INCEPTION = ""inception""\n  BASIC = ""basic""\n\n\ndef basic_augmentation(image, image_height, image_width, seed=None):\n  """"""Augment image according to NasNet paper (random flip + random crop).""""""\n\n  # source: https://arxiv.org/pdf/1707.07012.pdf appendix A.1\n  padding = 4\n  image = tf.image.random_flip_left_right(image, seed=seed)\n\n  image = tf.pad(image, [[padding, padding], [padding, padding], [0, 0]])\n  image = tf.random_crop(image, [image_height, image_width, 3], seed=seed)\n  return image\n\n\ndef resize_and_normalize(image, height, width):\n  """"""Convert image to float, resize and normalize to zero mean and [-1, 1].""""""\n  if image.dtype != tf.float32:\n    # Rescale pixel values to float in interval [0.0, 1.0].\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n  # Resize the image to the specified height and width.\n  image = tf.expand_dims(image, 0)\n  image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n  image = tf.squeeze(image, [0])\n  # Rescale pixels to range [-0.5, 0.5].\n  image = tf.subtract(image, 0.5)\n  # Rescale pixels to range [-1, 1].\n  image = tf.multiply(image, 2.0)\n  return image\n\n\ndef cutout(image, pad_size, replace=0, seed=None):\n  """"""Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n\n  Forked from learning/brain/research/meta_architect/image/image_processing.py?\n    l=1172&rcl=193953073\n\n  Args:\n    image: Image `Tensor` with shape [height, width, channels].\n    pad_size: The cutout shape will be at most [pad_size * 2, pad_size * 2].\n    replace: Value for replacing cutout values.\n    seed: Random seed.\n\n  Returns:\n    Image `Tensor` with cutout applied.\n  """"""\n\n  with tf.variable_scope(""cutout""):\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n    image_depth = tf.shape(image)[2]\n\n    # Sample the location in the image where the zero mask will be applied.\n    cutout_center_height = tf.random_uniform(\n        shape=[], minval=0, maxval=image_height, seed=seed, dtype=tf.int32)\n\n    cutout_center_width = tf.random_uniform(\n        shape=[], minval=0, maxval=image_width, seed=seed, dtype=tf.int32)\n\n    lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n    upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n    left_pad = tf.maximum(0, cutout_center_width - pad_size)\n    right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n\n    cutout_shape = [\n        image_height - (lower_pad + upper_pad),\n        image_width - (left_pad + right_pad)\n    ]\n    padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n    mask = tf.pad(\n        tf.zeros(cutout_shape, dtype=image.dtype),\n        padding_dims,\n        constant_values=1)\n    mask = tf.expand_dims(mask, -1)\n    mask = tf.tile(mask, [1, 1, image_depth])\n    image = tf.where(\n        tf.equal(mask, 0),\n        tf.ones_like(image, dtype=image.dtype) * replace, image)\n  return image\n'"
research/improve_nas/trainer/improve_nas.py,21,"b'# Lint as: python3\n""""""Defines NASNet subnetwork and subnetwork generators.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport adanet\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import nasnet\n  from adanet.research.improve_nas.trainer import subnetwork_utils\nexcept ImportError as e:\n  from trainer import nasnet\n  from trainer import subnetwork_utils\n# pylint: enable=g-import-not-at-top\n\n\n_PREVIOUS_NUM_CELLS = ""num_cells""\n_PREVIOUS_CONV_FILTERS = ""num_conv_filters""\n\n\nclass KnowledgeDistillation(object):\n  """"""Controls what type of knowledge distillation is used.\n\n  In knowledge distillation we want the new subnetwork to learn from the logits\n  of previous ensemble or previous subnetwork.\n\n  The following distillations are defined:\n\n  * `ADAPTIVE`: Distill previous ensemble. Inspired by Distilling the Knowledge\n        in a Neural Network [Hinton at al., 2015]\n        (https://arxiv.org/abs/1503.02531).\n  * `BORN_AGAIN`: Distill previous subnetwork. Introduced in Born Again Networks\n        [Furlanello et al., 2018](https://arxiv.org/abs/1805.04770).\n  * `NONE`: Do not use knowledge distillation.\n  """"""\n\n  ADAPTIVE = ""adaptive""\n  BORN_AGAIN = ""born_again""\n  NONE = ""none""\n\n\nclass Builder(adanet.subnetwork.Builder):\n  """"""Builds a NASNet subnetwork for AdaNet.""""""\n\n  def __init__(self, feature_columns, optimizer_fn, checkpoint_dir, hparams,\n               seed):\n    """"""Initializes a `Builder`.\n\n    Args:\n      feature_columns: The input feature columns of the problem.\n      optimizer_fn: Function that accepts a float \'learning_rate\' argument and\n        returns an `Optimizer` instance and learning rate `Tensor` which may\n        have a custom learning rate schedule applied.\n      checkpoint_dir: Checkpoint directory.\n      hparams: A `HParams` instance.\n      seed: A Python integer. Used to create random seeds. See\n        tf.set_random_seed for behavior.\n\n    Returns:\n      An instance of `Subnetwork`.\n    """"""\n\n    self._feature_columns = feature_columns\n    self._optimizer_fn = optimizer_fn\n    self._checkpoint_dir = checkpoint_dir\n    self._hparams = hparams\n\n    self._aux_head_weight = hparams.aux_head_weight\n    self._learn_mixture_weights = hparams.learn_mixture_weights\n    self._initial_learning_rate = hparams.initial_learning_rate\n    self._knowledge_distillation = hparams.knowledge_distillation\n    self._label_smoothing = hparams.label_smoothing\n    self._model_version = hparams.model_version\n    self._weight_decay = hparams.weight_decay\n    # `num_cells` and `num_conv_filters` are not directly used here. They are\n    # passed inside hparams to build_nasnet function. They are just saved in\n    # `shared`.\n    self._num_cells = hparams.num_cells\n    self._num_conv_filters = hparams.num_conv_filters\n    self._seed = seed\n\n  def build_subnetwork(self,\n                       features,\n                       logits_dimension,\n                       training,\n                       iteration_step,\n                       summary,\n                       previous_ensemble=None):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    # Prepare the input.\n    assert len(self._feature_columns) == 1, ""Got feature columns: {}"".format(\n        self._feature_columns)\n    images = tf.to_float(features[self._feature_columns[0].name])\n    self._name_scope = tf.get_default_graph().get_name_scope()\n\n    seed = self._seed\n    if seed is not None and previous_ensemble:\n      # Deterministically change the seed for different iterations so that\n      # subnetworks are not correlated.\n      seed += len(previous_ensemble.weighted_subnetworks)\n\n    arg_scope = nasnet.nasnet_cifar_arg_scope(weight_decay=self._weight_decay)\n\n    with tf.contrib.slim.arg_scope(arg_scope):\n      build_fn = nasnet.build_nasnet_cifar\n      logits, end_points = build_fn(\n          images,\n          num_classes=logits_dimension,\n          is_training=training,\n          config=self._hparams)\n    last_layer = end_points[""global_pool""]\n\n    subnetwork_shared_data = {\n        _PREVIOUS_NUM_CELLS: tf.constant(self._num_cells),\n        _PREVIOUS_CONV_FILTERS: tf.constant(self._num_conv_filters)\n    }\n\n    return adanet.Subnetwork(\n        last_layer=last_layer,\n        logits=logits,\n        complexity=1,\n        shared=subnetwork_shared_data)\n\n  def build_subnetwork_train_op(self, subnetwork, loss, var_list, labels,\n                                iteration_step, summary, previous_ensemble):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    del loss  # Loss for training is defined below.\n\n    # The AdaNet Estimator is responsible for incrementing the global step.\n    optimizer, learning_rate = self._optimizer_fn(\n        learning_rate=self._initial_learning_rate)\n    with tf.name_scope(""""):\n      summary.scalar(""learning_rate/adanet/subnetwork"", learning_rate)\n\n    onehot_labels = tf.one_hot(\n        tf.reshape(labels, [-1]), subnetwork.logits.shape[-1], dtype=tf.int32)\n\n    loss = tf.losses.softmax_cross_entropy(\n        onehot_labels=onehot_labels,\n        logits=subnetwork.logits,\n        weights=1.0,\n        label_smoothing=self._label_smoothing)\n\n    # Add knowledge ditillation loss.\n    if previous_ensemble:\n      if self._knowledge_distillation == KnowledgeDistillation.ADAPTIVE:\n        loss += tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.nn.softmax(previous_ensemble.logits),\n            logits=subnetwork.logits,\n            weights=1.0,\n            scope=""loss_adaptive_kd"")\n\n      if self._knowledge_distillation == KnowledgeDistillation.BORN_AGAIN:\n        loss += tf.losses.softmax_cross_entropy(\n            onehot_labels=tf.nn.softmax(\n                previous_ensemble.weighted_subnetworks[-1].logits),\n            logits=subnetwork.logits,\n            weights=1.0,\n            scope=""loss_born_again_kd"")\n\n    # Add weight decay.\n    loss += tf.losses.get_regularization_loss(scope=self._name_scope)\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      if self._hparams.clip_gradients > 0:\n        optimizer = tf.contrib.estimator.clip_gradients_by_norm(\n            optimizer, self._hparams.clip_gradients)\n      return optimizer.minimize(loss, var_list=var_list)\n\n  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n                                     iteration_step, summary):\n    """"""See `adanet.subnetwork.Builder`.""""""\n\n    if not self._learn_mixture_weights:\n      return tf.no_op(""mixture_weights_train_op"")\n\n    # The AdaNet Estimator is responsible for incrementing the global step.\n    optimizer, learning_rate = self._optimizer_fn(\n        learning_rate=self._initial_learning_rate)\n    summary.scalar(""learning_rate/adanet/mixture_weights"", learning_rate)\n    return optimizer.minimize(loss=loss, var_list=var_list)\n\n  @property\n  def name(self):\n    """"""Returns this subnetwork\'s name.""""""\n    name = ""NasNet_A_{}_{}"".format(self._hparams.num_cells / 3,\n                                   self._hparams.num_conv_filters * 24)\n    if self._knowledge_distillation != KnowledgeDistillation.NONE:\n      name += ""_"" + self._knowledge_distillation\n    name += ""_"" + self._model_version\n    return name\n\n\nclass Generator(adanet.subnetwork.Generator):\n  """"""Generates a list of Builders.""""""\n\n  def __init__(self,\n               feature_columns,\n               optimizer_fn,\n               iteration_steps,\n               checkpoint_dir,\n               hparams,\n               seed=None):\n    """"""Initializes a `Generator`.\n\n    Args:\n      feature_columns: The input feature columns of the problem.\n      optimizer_fn: Function that accepts a float \'learning_rate\' argument and\n        returns an `Optimizer` instance and learning rate `Tensor` which may\n        have a custom learning rate schedule applied.\n      iteration_steps: The number of train steps in per iteration. Required for\n        ScheduleDropPath algorithm.\n      checkpoint_dir: Checkpoint directory.\n      hparams: Hyper-parameters.\n      seed: A Python integer. Used to create random seeds. See\n        tf.set_random_seed for behavior.\n\n    Returns:\n      An instance of `Generator`.\n\n    Raises:\n      ValueError: If num_cells is not divisible by 3.\n    """"""\n\n    if hparams.num_cells % 3 != 0:\n      raise ValueError(""num_cells must be a multiple of 3."")\n\n    self._builder_fn = functools.partial(\n        Builder,\n        feature_columns=feature_columns,\n        optimizer_fn=optimizer_fn,\n        checkpoint_dir=checkpoint_dir,\n        seed=seed,\n        hparams=hparams)\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports):\n    """"""See `adanet.subnetwork.Generator`.""""""\n\n    return [self._builder_fn()]\n\n\nclass DynamicGenerator(adanet.subnetwork.Generator):\n  """"""Generates a list of `Builders`.""""""\n\n  def __init__(self,\n               feature_columns,\n               optimizer_fn,\n               iteration_steps,\n               checkpoint_dir,\n               hparams,\n               seed=None):\n    """"""Generator that gradually grows the architecture.\n\n    In each iteration, we generate one deeper candidate and one wider candidate.\n\n    Args:\n      feature_columns: The input feature columns of the problem.\n      optimizer_fn: Function that accepts a float \'learning_rate\' argument and\n        returns an `Optimizer` instance and learning rate `Tensor` which may\n        have a custom learning rate schedule applied.\n      iteration_steps: The number of train steps in per iteration. Required for\n        ScheduleDropPath algorithm.\n      checkpoint_dir: Checkpoint directory.\n      hparams: Hyper-parameters.\n      seed: A Python integer. Used to create random seeds. See\n        tf.set_random_seed for behavior.\n\n    Returns:\n      An instance of `Generator`.\n\n    Raises:\n      ValueError: If num_cells is not divisible by 3.\n    """"""\n\n    if hparams.num_cells % 3 != 0:\n      raise ValueError(""num_cells must be a multiple of 3."")\n\n    self._hparams = hparams\n    self._builder_fn = functools.partial(\n        Builder,\n        feature_columns=feature_columns,\n        optimizer_fn=optimizer_fn,\n        checkpoint_dir=checkpoint_dir,\n        seed=seed)\n\n  def generate_candidates(self, previous_ensemble, iteration_number,\n                          previous_ensemble_reports, all_reports):\n    """"""See `adanet.subnetwork.Generator`.""""""\n\n    num_cells = self._hparams.num_cells\n    num_conv_filters = self._hparams.num_conv_filters\n    # Get the architecture of the last subnetwork.\n    if previous_ensemble:\n      num_cells = int(\n          subnetwork_utils.get_persisted_value_from_ensemble(\n              previous_ensemble, _PREVIOUS_NUM_CELLS))\n\n      num_conv_filters = int(\n          subnetwork_utils.get_persisted_value_from_ensemble(\n              previous_ensemble, _PREVIOUS_CONV_FILTERS))\n\n    candidates = [\n        self._builder_fn(\n            hparams=subnetwork_utils.copy_update(\n                self._hparams,\n                num_cells=num_cells + 3,\n                num_conv_filters=num_conv_filters)),\n        self._builder_fn(\n            hparams=subnetwork_utils.copy_update(\n                self._hparams,\n                num_cells=num_cells,\n                num_conv_filters=num_conv_filters + 10)),\n    ]\n    return candidates\n'"
research/improve_nas/trainer/improve_nas_test.py,20,"b'# Lint as: python3\n""""""Tests for improve_nas.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\nfrom absl import flags\nfrom absl.testing import parameterized\nimport adanet\nfrom adanet.research.improve_nas.trainer import improve_nas\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\n_IMAGE_DIM = 32\n\n\nclass _FakeSummary(object):\n  """"""A fake `Summary`.""""""\n\n  def scalar(self, name, tensor):\n    del name  # Unused\n    del tensor  # Unused\n\n\ndef _optimizer(learning_rate):\n  return tf.train.GradientDescentOptimizer(learning_rate), learning_rate\n\n\ndef _builder(snapshot=False,\n             knowledge_distillation=improve_nas.KnowledgeDistillation.NONE,\n             checkpoint_dir=None,\n             use_aux_head=False,\n             learn_mixture_weights=False,\n             model_version=""cifar""):\n  hparams = tf.contrib.training.HParams(\n      clip_gradients=5.,\n      stem_multiplier=3.0,\n      drop_path_keep_prob=0.6,\n      num_cells=3,\n      use_aux_head=use_aux_head,\n      aux_head_weight=0.4,\n      label_smoothing=0.1,\n      num_conv_filters=4,\n      dense_dropout_keep_prob=1.0,\n      filter_scaling_rate=2.0,\n      num_reduction_layers=2,\n      data_format=""NHWC"",\n      use_bounded_activation=False,\n      skip_reduction_layer_input=0,\n      initial_learning_rate=.01,\n      complexity_decay_rate=0.9,\n      weight_decay=.0001,\n      knowledge_distillation=knowledge_distillation,\n      snapshot=snapshot,\n      learn_mixture_weights=learn_mixture_weights,\n      mixture_weight_type=adanet.MixtureWeightType.SCALAR,\n      model_version=model_version,\n      total_training_steps=100)\n  return improve_nas.Builder(\n      [tf.feature_column.numeric_column(key=""x"", shape=[32, 32, 3])],\n      seed=11,\n      optimizer_fn=_optimizer,\n      checkpoint_dir=checkpoint_dir,\n      hparams=hparams)\n\n\ndef _subnetwork_generator(checkpoint_dir):\n  hparams = tf.contrib.training.HParams(\n      clip_gradients=5.,\n      stem_multiplier=3.0,\n      drop_path_keep_prob=0.6,\n      num_cells=3,\n      use_aux_head=False,\n      aux_head_weight=0.4,\n      label_smoothing=0.1,\n      num_conv_filters=4,\n      dense_dropout_keep_prob=1.0,\n      filter_scaling_rate=2.0,\n      complexity_decay_rate=0.9,\n      num_reduction_layers=2,\n      data_format=""NHWC"",\n      skip_reduction_layer_input=0,\n      initial_learning_rate=.01,\n      use_bounded_activation=False,\n      weight_decay=.0001,\n      knowledge_distillation=improve_nas.KnowledgeDistillation.NONE,\n      snapshot=False,\n      learn_mixture_weights=False,\n      mixture_weight_type=adanet.MixtureWeightType.SCALAR,\n      model_version=""cifar"",\n      total_training_steps=100)\n  return improve_nas.Generator(\n      [tf.feature_column.numeric_column(key=""x"", shape=[32, 32, 3])],\n      seed=11,\n      optimizer_fn=_optimizer,\n      iteration_steps=3,\n      checkpoint_dir=checkpoint_dir,\n      hparams=hparams)\n\n\nclass ImproveNasBuilderTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(ImproveNasBuilderTest, self).setUp()\n    # Setup and cleanup test directory.\n    self.test_subdirectory = os.path.join(tf.flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.makedirs(self.test_subdirectory)\n\n  def tearDown(self):\n    super(ImproveNasBuilderTest, self).tearDown()\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n\n  @parameterized.named_parameters({\n      ""testcase_name"": ""two_subnetworks_adaptive_knowledge_distillation_aux"",\n      ""builder_params"": [\n          {\n              ""knowledge_distillation"":\n                  improve_nas.KnowledgeDistillation.ADAPTIVE,\n              ""use_aux_head"": True,\n          },\n          {\n              ""knowledge_distillation"":\n                  improve_nas.KnowledgeDistillation.ADAPTIVE,\n              ""use_aux_head"": True,\n          },\n      ],\n      ""want_name"": ""NasNet_A_1.0_96_adaptive_cifar"",\n  }, {\n      ""testcase_name"": ""two_subnetworks_born_again_knowledge_distillation_w"",\n      ""builder_params"": [\n          {\n              ""knowledge_distillation"":\n                  improve_nas.KnowledgeDistillation.BORN_AGAIN,\n              ""use_aux_head"":\n                  True,\n              ""learn_mixture_weights"": True,\n          },\n          {\n              ""knowledge_distillation"":\n                  improve_nas.KnowledgeDistillation.BORN_AGAIN,\n              ""use_aux_head"":\n                  True,\n              ""learn_mixture_weights"": True,\n          },\n      ],\n      ""want_name"": ""NasNet_A_1.0_96_born_again_cifar"",\n  })\n  def test_build_subnetwork(self, builder_params, want_name):\n    with tf.Graph().as_default() as g, self.test_session(graph=g) as sess:\n      data = np.concatenate([\n          np.ones((1, _IMAGE_DIM, _IMAGE_DIM, 1)), 2. * np.ones(\n              (1, _IMAGE_DIM, _IMAGE_DIM, 1))\n      ])\n      features = {""x"": tf.constant(data)}\n      labels = tf.constant([0, 1])\n      training = True\n      mode = tf.estimator.ModeKeys.TRAIN\n      head = tf.contrib.estimator.binary_classification_head(\n          loss_reduction=tf.losses.Reduction.SUM)\n      ensemble = None\n      name = None\n      subnetwork = None\n      builders = []\n      for builder_param in builder_params:\n        builders.append(\n            _builder(checkpoint_dir=self.test_subdirectory, **builder_param))\n      for idx, builder in enumerate(builders):\n        name = builder.name\n        # Pass the subnetworks of previous builders to the next builder.\n        with tf.variable_scope(""subnetwork_{}"".format(idx)):\n          subnetwork = builder.build_subnetwork(\n              features=features,\n              logits_dimension=head.logits_dimension,\n              training=training,\n              iteration_step=tf.train.get_or_create_global_step(),\n              summary=_FakeSummary(),\n              previous_ensemble=ensemble)\n          logits = subnetwork.logits\n          weighted_subnetworks = []\n          if ensemble:\n            logits += ensemble.logits\n            weighted_subnetworks = ensemble.weighted_subnetworks\n          ensemble = adanet.Ensemble(\n              weighted_subnetworks=weighted_subnetworks + [\n                  adanet.WeightedSubnetwork(\n                      name=None,\n                      logits=logits,\n                      weight=None,\n                      subnetwork=subnetwork)\n              ],\n              logits=logits,\n              bias=0.)\n\n      estimator_spec = head.create_estimator_spec(\n          features=features,\n          labels=labels,\n          mode=mode,\n          train_op_fn=lambda loss: tf.no_op(),\n          logits=ensemble.logits)\n      sess.run(tf.global_variables_initializer())\n      train_op = builders[-1].build_subnetwork_train_op(\n          subnetwork,\n          estimator_spec.loss,\n          var_list=None,\n          labels=labels,\n          iteration_step=tf.train.get_or_create_global_step(),\n          summary=_FakeSummary(),\n          previous_ensemble=ensemble)\n      for _ in range(10):\n        sess.run(train_op)\n      self.assertEqual(want_name, name)\n      self.assertGreater(sess.run(estimator_spec.loss), 0.0)\n\n\nclass QuetzalGeneratorTest(tf.test.TestCase):\n\n  def test_candidate_generation(self):\n    self.test_subdirectory = os.path.join(flags.FLAGS.test_tmpdir, self.id())\n    shutil.rmtree(self.test_subdirectory, ignore_errors=True)\n    os.mkdir(self.test_subdirectory)\n\n    subnetwork_generator = _subnetwork_generator(self.test_subdirectory)\n    subnetwork_builders = subnetwork_generator.generate_candidates(\n        previous_ensemble=None,\n        # The following arguments are unused by\n        # quetzal.Generator.\n        iteration_number=0,\n        previous_ensemble_reports=[],\n        all_reports=[])\n    self.assertEqual(1, len(subnetwork_builders))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
research/improve_nas/trainer/nasnet.py,28,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Lint as: python3\n""""""Contains the definition for the NASNet classification networks.\n\nPaper: https://arxiv.org/abs/1707.07012\nCopy of: https://github.com/tensorflow/models/blob/master/research/slim/nets/\nnasnet/nasnet.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import nasnet_utils\nexcept ImportError as e:\n  from trainer import nasnet_utils\n# pylint: enable=g-import-not-at-top\n\narg_scope = tf.contrib.framework.arg_scope\nslim = tf.contrib.slim\n\n\n# Notes for training NASNet Cifar Model\n# -------------------------------------\n# batch_size: 32\n# learning rate: 0.025\n# cosine (single period) learning rate decay\n# auxiliary head loss weighting: 0.4\n# clip global norm of all gradients by 5\ndef cifar_config():\n  return tf.contrib.training.HParams(\n      stem_multiplier=3.0,\n      drop_path_keep_prob=0.6,\n      num_cells=18,\n      use_aux_head=1,\n      num_conv_filters=32,\n      dense_dropout_keep_prob=1.0,\n      filter_scaling_rate=2.0,\n      num_reduction_layers=2,\n      data_format=\'NHWC\',\n      skip_reduction_layer_input=0,\n      # 600 epochs with a batch size of 32\n      # This is used for the drop path probabilities since it needs to increase\n      # the drop out probability over the course of training.\n      total_training_steps=937500,\n      use_bounded_activation=False,\n  )\n\n\n# Notes for training large NASNet model on ImageNet\n# -------------------------------------\n# batch size (per replica): 16\n# learning rate: 0.015 * 100\n# learning rate decay factor: 0.97\n# num epochs per decay: 2.4\n# sync sgd with 100 replicas\n# auxiliary head loss weighting: 0.4\n# label smoothing: 0.1\n# clip global norm of all gradients by 10\ndef large_imagenet_config():\n  return tf.contrib.training.HParams(\n      stem_multiplier=3.0,\n      dense_dropout_keep_prob=0.5,\n      num_cells=18,\n      filter_scaling_rate=2.0,\n      num_conv_filters=168,\n      drop_path_keep_prob=0.7,\n      use_aux_head=1,\n      num_reduction_layers=2,\n      data_format=\'NHWC\',\n      skip_reduction_layer_input=1,\n      total_training_steps=250000,\n      use_bounded_activation=False,\n  )\n\n\n# Notes for training the mobile NASNet ImageNet model\n# -------------------------------------\n# batch size (per replica): 32\n# learning rate: 0.04 * 50\n# learning rate scaling factor: 0.97\n# num epochs per decay: 2.4\n# sync sgd with 50 replicas\n# auxiliary head weighting: 0.4\n# label smoothing: 0.1\n# clip global norm of all gradients by 10\ndef mobile_imagenet_config():\n  return tf.contrib.training.HParams(\n      stem_multiplier=1.0,\n      dense_dropout_keep_prob=0.5,\n      num_cells=12,\n      filter_scaling_rate=2.0,\n      drop_path_keep_prob=1.0,\n      num_conv_filters=44,\n      use_aux_head=1,\n      num_reduction_layers=2,\n      data_format=\'NHWC\',\n      skip_reduction_layer_input=0,\n      total_training_steps=250000,\n      use_bounded_activation=False,\n  )\n\n\ndef _update_hparams(hparams, is_training):\n  """"""Update hparams for given is_training option.""""""\n  if not is_training:\n    hparams.set_hparam(\'drop_path_keep_prob\', 1.0)\n\n\ndef nasnet_cifar_arg_scope(weight_decay=5e-4,\n                           batch_norm_decay=0.9,\n                           batch_norm_epsilon=1e-5):\n  """"""Defines the default arg scope for the NASNet-A Cifar model.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the NASNet Cifar Model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': True,\n      \'fused\': True,\n  }\n  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  weights_initializer = tf.contrib.layers.variance_scaling_initializer(\n      mode=\'FAN_OUT\')\n  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],\n                 weights_regularizer=weights_regularizer,\n                 weights_initializer=weights_initializer):\n    with arg_scope([slim.fully_connected],\n                   activation_fn=None, scope=\'FC\'):\n      with arg_scope([slim.conv2d, slim.separable_conv2d],\n                     activation_fn=None, biases_initializer=None):\n        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n          return sc\n\n\ndef nasnet_mobile_arg_scope(weight_decay=4e-5,\n                            batch_norm_decay=0.9997,\n                            batch_norm_epsilon=1e-3):\n  """"""Defines the default arg scope for the NASNet-A Mobile ImageNet model.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the NASNet Mobile Model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': True,\n      \'fused\': True,\n  }\n  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  weights_initializer = tf.contrib.layers.variance_scaling_initializer(\n      mode=\'FAN_OUT\')\n  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],\n                 weights_regularizer=weights_regularizer,\n                 weights_initializer=weights_initializer):\n    with arg_scope([slim.fully_connected],\n                   activation_fn=None, scope=\'FC\'):\n      with arg_scope([slim.conv2d, slim.separable_conv2d],\n                     activation_fn=None, biases_initializer=None):\n        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n          return sc\n\n\ndef nasnet_large_arg_scope(weight_decay=5e-5,\n                           batch_norm_decay=0.9997,\n                           batch_norm_epsilon=1e-3):\n  """"""Defines the default arg scope for the NASNet-A Large ImageNet model.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the NASNet Large Model.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': True,\n      \'fused\': True,\n  }\n  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  weights_initializer = tf.contrib.layers.variance_scaling_initializer(\n      mode=\'FAN_OUT\')\n  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],\n                 weights_regularizer=weights_regularizer,\n                 weights_initializer=weights_initializer):\n    with arg_scope([slim.fully_connected],\n                   activation_fn=None, scope=\'FC\'):\n      with arg_scope([slim.conv2d, slim.separable_conv2d],\n                     activation_fn=None, biases_initializer=None):\n        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n          return sc\n\n\ndef _build_aux_head(net, end_points, num_classes, hparams, scope):\n  """"""Auxiliary head used for all models across all datasets.""""""\n  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu\n  with tf.variable_scope(scope):\n    aux_logits = tf.identity(net)\n    with tf.variable_scope(\'aux_logits\'):\n      aux_logits = slim.avg_pool2d(\n          aux_logits, [5, 5], stride=3, padding=\'VALID\')\n      aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope=\'proj\')\n      aux_logits = slim.batch_norm(aux_logits, scope=\'aux_bn0\')\n      aux_logits = activation_fn(aux_logits)\n      # Shape of feature map before the final layer.\n      shape = aux_logits.shape\n      if hparams.data_format == \'NHWC\':\n        shape = shape[1:3]\n      else:\n        shape = shape[2:4]\n      aux_logits = slim.conv2d(aux_logits, 768, shape, padding=\'VALID\')\n      aux_logits = slim.batch_norm(aux_logits, scope=\'aux_bn1\')\n      aux_logits = activation_fn(aux_logits)\n      aux_logits = tf.contrib.layers.flatten(aux_logits)\n      aux_logits = slim.fully_connected(aux_logits, num_classes)\n      end_points[\'AuxLogits\'] = aux_logits\n\n\ndef _imagenet_stem(inputs, hparams, stem_cell, current_step=None):\n  """"""Stem used for models trained on ImageNet.""""""\n  num_stem_cells = 2\n\n  # 149 x 149 x 32\n  num_stem_filters = int(32 * hparams.stem_multiplier)\n  net = slim.conv2d(\n      inputs, num_stem_filters, [3, 3], stride=2, scope=\'conv0\',\n      padding=\'VALID\')\n  net = slim.batch_norm(net, scope=\'conv0_bn\')\n\n  # Run the reduction cells\n  cell_outputs = [None, net]\n  filter_scaling = 1.0 / (hparams.filter_scaling_rate**num_stem_cells)\n  for cell_num in range(num_stem_cells):\n    net = stem_cell(\n        net,\n        scope=\'cell_stem_{}\'.format(cell_num),\n        filter_scaling=filter_scaling,\n        stride=2,\n        prev_layer=cell_outputs[-2],\n        cell_num=cell_num,\n        current_step=current_step)\n    cell_outputs.append(net)\n    filter_scaling *= hparams.filter_scaling_rate\n  return net, cell_outputs\n\n\ndef _cifar_stem(inputs, hparams):\n  """"""Stem used for models trained on Cifar.""""""\n  num_stem_filters = int(hparams.num_conv_filters * hparams.stem_multiplier)\n  net = slim.conv2d(\n      inputs,\n      num_stem_filters,\n      3,\n      scope=\'l1_stem_3x3\')\n  net = slim.batch_norm(net, scope=\'l1_stem_bn\')\n  return net, [None, net]\n\n\ndef build_nasnet_cifar(images, num_classes,\n                       is_training=True,\n                       config=None,\n                       current_step=None):\n  """"""Build NASNet model for the Cifar Dataset.""""""\n  hparams = cifar_config() if config is None else copy.deepcopy(config)\n  _update_hparams(hparams, is_training)\n\n  if tf.test.is_gpu_available() and hparams.data_format == \'NHWC\':\n    tf.logging.info(\'A GPU is available on the machine, consider using NCHW \'\n                    \'data format for increased speed on GPU.\')\n\n  if hparams.data_format == \'NCHW\':\n    images = tf.transpose(images, [0, 3, 1, 2])\n\n  # Calculate the total number of cells in the network\n  # Add 2 for the reduction cells\n  total_num_cells = hparams.num_cells + 2\n\n  normal_cell = nasnet_utils.NasNetANormalCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  reduction_cell = nasnet_utils.NasNetAReductionCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],\n                 is_training=is_training):\n    with arg_scope([slim.avg_pool2d,\n                    slim.max_pool2d,\n                    slim.conv2d,\n                    slim.batch_norm,\n                    slim.separable_conv2d,\n                    nasnet_utils.factorized_reduction,\n                    nasnet_utils.global_avg_pool,\n                    nasnet_utils.get_channel_index,\n                    nasnet_utils.get_channel_dim],\n                   data_format=hparams.data_format):\n      return _build_nasnet_base(images,\n                                normal_cell=normal_cell,\n                                reduction_cell=reduction_cell,\n                                num_classes=num_classes,\n                                hparams=hparams,\n                                is_training=is_training,\n                                stem_type=\'cifar\',\n                                current_step=current_step)\nbuild_nasnet_cifar.default_image_size = 32\n\n\ndef build_nasnet_mobile(images, num_classes,\n                        is_training=True,\n                        final_endpoint=None,\n                        config=None,\n                        current_step=None):\n  """"""Build NASNet Mobile model for the ImageNet Dataset.""""""\n  hparams = (mobile_imagenet_config() if config is None\n             else copy.deepcopy(config))\n  _update_hparams(hparams, is_training)\n\n  if tf.test.is_gpu_available() and hparams.data_format == \'NHWC\':\n    tf.logging.info(\'A GPU is available on the machine, consider using NCHW \'\n                    \'data format for increased speed on GPU.\')\n\n  if hparams.data_format == \'NCHW\':\n    images = tf.transpose(images, [0, 3, 1, 2])\n\n  # Calculate the total number of cells in the network\n  # Add 2 for the reduction cells\n  total_num_cells = hparams.num_cells + 2\n  # If ImageNet, then add an additional two for the stem cells\n  total_num_cells += 2\n\n  normal_cell = nasnet_utils.NasNetANormalCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  reduction_cell = nasnet_utils.NasNetAReductionCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],\n                 is_training=is_training):\n    with arg_scope([slim.avg_pool2d,\n                    slim.max_pool2d,\n                    slim.conv2d,\n                    slim.batch_norm,\n                    slim.separable_conv2d,\n                    nasnet_utils.factorized_reduction,\n                    nasnet_utils.global_avg_pool,\n                    nasnet_utils.get_channel_index,\n                    nasnet_utils.get_channel_dim],\n                   data_format=hparams.data_format):\n      return _build_nasnet_base(images,\n                                normal_cell=normal_cell,\n                                reduction_cell=reduction_cell,\n                                num_classes=num_classes,\n                                hparams=hparams,\n                                is_training=is_training,\n                                stem_type=\'imagenet\',\n                                final_endpoint=final_endpoint,\n                                current_step=current_step)\nbuild_nasnet_mobile.default_image_size = 224\n\n\ndef build_nasnet_large(images, num_classes,\n                       is_training=True,\n                       final_endpoint=None,\n                       config=None,\n                       current_step=None):\n  """"""Build NASNet Large model for the ImageNet Dataset.""""""\n  hparams = (large_imagenet_config() if config is None\n             else copy.deepcopy(config))\n  _update_hparams(hparams, is_training)\n\n  if tf.test.is_gpu_available() and hparams.data_format == \'NHWC\':\n    tf.logging.info(\'A GPU is available on the machine, consider using NCHW \'\n                    \'data format for increased speed on GPU.\')\n\n  if hparams.data_format == \'NCHW\':\n    images = tf.transpose(images, [0, 3, 1, 2])\n\n  # Calculate the total number of cells in the network\n  # Add 2 for the reduction cells\n  total_num_cells = hparams.num_cells + 2\n  # If ImageNet, then add an additional two for the stem cells\n  total_num_cells += 2\n\n  normal_cell = nasnet_utils.NasNetANormalCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  reduction_cell = nasnet_utils.NasNetAReductionCell(\n      hparams.num_conv_filters, hparams.drop_path_keep_prob,\n      total_num_cells, hparams.total_training_steps,\n      hparams.use_bounded_activation)\n  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],\n                 is_training=is_training):\n    with arg_scope([slim.avg_pool2d,\n                    slim.max_pool2d,\n                    slim.conv2d,\n                    slim.batch_norm,\n                    slim.separable_conv2d,\n                    nasnet_utils.factorized_reduction,\n                    nasnet_utils.global_avg_pool,\n                    nasnet_utils.get_channel_index,\n                    nasnet_utils.get_channel_dim],\n                   data_format=hparams.data_format):\n      return _build_nasnet_base(images,\n                                normal_cell=normal_cell,\n                                reduction_cell=reduction_cell,\n                                num_classes=num_classes,\n                                hparams=hparams,\n                                is_training=is_training,\n                                stem_type=\'imagenet\',\n                                final_endpoint=final_endpoint,\n                                current_step=current_step)\nbuild_nasnet_large.default_image_size = 331\n\n\ndef _build_nasnet_base(images,\n                       normal_cell,\n                       reduction_cell,\n                       num_classes,\n                       hparams,\n                       is_training,\n                       stem_type,\n                       final_endpoint=None,\n                       current_step=None):\n  """"""Constructs a NASNet image model.""""""\n\n  end_points = {}\n  def add_and_check_endpoint(endpoint_name, net):\n    end_points[endpoint_name] = net\n    return final_endpoint and (endpoint_name == final_endpoint)\n\n  # Find where to place the reduction cells or stride normal cells\n  reduction_indices = nasnet_utils.calc_reduction_layers(\n      hparams.num_cells, hparams.num_reduction_layers)\n  stem_cell = reduction_cell\n\n  if stem_type == \'imagenet\':\n    stem = lambda: _imagenet_stem(images, hparams, stem_cell)\n  elif stem_type == \'cifar\':\n    stem = lambda: _cifar_stem(images, hparams)\n  else:\n    raise ValueError(\'Unknown stem_type: \', stem_type)\n  net, cell_outputs = stem()\n  if add_and_check_endpoint(\'Stem\', net): return net, end_points\n\n  # Setup for building in the auxiliary head.\n  aux_head_cell_idxes = []\n  if len(reduction_indices) >= 2:\n    aux_head_cell_idxes.append(reduction_indices[1] - 1)\n\n  # Run the cells\n  filter_scaling = 1.0\n  # true_cell_num accounts for the stem cells\n  true_cell_num = 2 if stem_type == \'imagenet\' else 0\n  activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu\n  for cell_num in range(hparams.num_cells):\n    stride = 1\n    if hparams.skip_reduction_layer_input:\n      prev_layer = cell_outputs[-2]\n    if cell_num in reduction_indices:\n      filter_scaling *= hparams.filter_scaling_rate\n      net = reduction_cell(\n          net,\n          scope=\'reduction_cell_{}\'.format(reduction_indices.index(cell_num)),\n          filter_scaling=filter_scaling,\n          stride=2,\n          prev_layer=cell_outputs[-2],\n          cell_num=true_cell_num,\n          current_step=current_step)\n      if add_and_check_endpoint(\n          \'Reduction_Cell_{}\'.format(reduction_indices.index(cell_num)), net):\n        return net, end_points\n      true_cell_num += 1\n      cell_outputs.append(net)\n    if not hparams.skip_reduction_layer_input:\n      prev_layer = cell_outputs[-2]\n    net = normal_cell(\n        net,\n        scope=\'cell_{}\'.format(cell_num),\n        filter_scaling=filter_scaling,\n        stride=stride,\n        prev_layer=prev_layer,\n        cell_num=true_cell_num,\n        current_step=current_step)\n\n    if add_and_check_endpoint(\'Cell_{}\'.format(cell_num), net):\n      return net, end_points\n    true_cell_num += 1\n    if (hparams.use_aux_head and cell_num in aux_head_cell_idxes and\n        num_classes and is_training):\n      aux_net = activation_fn(net)\n      _build_aux_head(aux_net, end_points, num_classes, hparams,\n                      scope=\'aux_{}\'.format(cell_num))\n    cell_outputs.append(net)\n\n  # Final softmax layer\n  with tf.variable_scope(\'final_layer\'):\n    net = activation_fn(net)\n    net = nasnet_utils.global_avg_pool(net)\n    if add_and_check_endpoint(\'global_pool\', net) or not num_classes:\n      return net, end_points\n    net = slim.dropout(net, hparams.dense_dropout_keep_prob, scope=\'dropout\')\n    logits = slim.fully_connected(net, num_classes)\n\n    if add_and_check_endpoint(\'Logits\', logits):\n      return net, end_points\n\n    predictions = tf.nn.softmax(logits, name=\'predictions\')\n    if add_and_check_endpoint(\'Predictions\', predictions):\n      return net, end_points\n  return logits, end_points\n'"
research/improve_nas/trainer/nasnet_utils.py,48,"b'# Copyright 2019 The AdaNet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Lint as: python3\n""""""A custom module for some common operations used by NASNet.\n\nFunctions exposed in this file:\n- calc_reduction_layers\n- get_channel_index\n- get_channel_dim\n- global_avg_pool\n- factorized_reduction\n- drop_path\n\nClasses exposed in this file:\n- NasNetABaseCell\n- NasNetANormalCell\n- NasNetAReductionCell\n\nCopy of: https://github.com/tensorflow/models/blob/master/research/slim/nets/\nnasnet/nasnet_utils.py\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\n\narg_scope = tf.contrib.framework.arg_scope\nslim = tf.contrib.slim\n\nDATA_FORMAT_NCHW = \'NCHW\'\nDATA_FORMAT_NHWC = \'NHWC\'\nINVALID = \'null\'\n# The cap for tf.clip_by_value, it\'s hinted from the activation distribution\n# that the majority of activation values are in the range [-6, 6].\nCLIP_BY_VALUE_CAP = 6\n\n\ndef calc_reduction_layers(num_cells, num_reduction_layers):\n  """"""Figure out what layers should have reductions.""""""\n  reduction_layers = []\n  for pool_num in range(1, num_reduction_layers + 1):\n    layer_num = (float(pool_num) / (num_reduction_layers + 1)) * num_cells\n    layer_num = int(layer_num)\n    reduction_layers.append(layer_num)\n  return reduction_layers\n\n\n@tf.contrib.framework.add_arg_scope\ndef get_channel_index(data_format=INVALID):\n  assert data_format != INVALID\n  axis = 3 if data_format == \'NHWC\' else 1\n  return axis\n\n\n@tf.contrib.framework.add_arg_scope\ndef get_channel_dim(shape, data_format=INVALID):\n  assert data_format != INVALID\n  assert len(shape) == 4\n  if data_format == \'NHWC\':\n    return int(shape[3])\n  elif data_format == \'NCHW\':\n    return int(shape[1])\n  else:\n    raise ValueError(\'Not a valid data_format\', data_format)\n\n\n@tf.contrib.framework.add_arg_scope\ndef global_avg_pool(x, data_format=INVALID):\n  """"""Average pool away the height and width spatial dimensions of x.""""""\n  assert data_format != INVALID\n  assert data_format in [\'NHWC\', \'NCHW\']\n  assert x.shape.ndims == 4\n  if data_format == \'NHWC\':\n    return tf.reduce_mean(x, [1, 2])\n  else:\n    return tf.reduce_mean(x, [2, 3])\n\n\n@tf.contrib.framework.add_arg_scope\ndef factorized_reduction(net, output_filters, stride, data_format=INVALID):\n  """"""Reduces the shape of net without information loss due to striding.""""""\n  assert data_format != INVALID\n  if stride == 1:\n    net = slim.conv2d(net, output_filters, 1, scope=\'path_conv\')\n    net = slim.batch_norm(net, scope=\'path_bn\')\n    return net\n  if data_format == \'NHWC\':\n    stride_spec = [1, stride, stride, 1]\n  else:\n    stride_spec = [1, 1, stride, stride]\n\n  # Skip path 1\n  path1 = tf.nn.avg_pool(\n      net, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=data_format)\n  path1 = slim.conv2d(path1, int(output_filters / 2), 1, scope=\'path1_conv\')\n\n  # Skip path 2\n  # First pad with 0\'s on the right and bottom, then shift the filter to\n  # include those 0\'s that were added.\n  if data_format == \'NHWC\':\n    pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]\n    path2 = tf.pad(net, pad_arr)[:, 1:, 1:, :]\n    concat_axis = 3\n  else:\n    pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]\n    path2 = tf.pad(net, pad_arr)[:, :, 1:, 1:]\n    concat_axis = 1\n\n  path2 = tf.nn.avg_pool(\n      path2, [1, 1, 1, 1], stride_spec, \'VALID\', data_format=data_format)\n\n  # If odd number of filters, add an additional one to the second path.\n  final_filter_size = int(output_filters / 2) + int(output_filters % 2)\n  path2 = slim.conv2d(path2, final_filter_size, 1, scope=\'path2_conv\')\n\n  # Concat and apply BN\n  final_path = tf.concat(values=[path1, path2], axis=concat_axis)\n  final_path = slim.batch_norm(final_path, scope=\'final_path_bn\')\n  return final_path\n\n\n@tf.contrib.framework.add_arg_scope\ndef drop_path(net, keep_prob, is_training=True):\n  """"""Drops out a whole example hiddenstate with the specified probability.""""""\n  if is_training:\n    batch_size = tf.shape(net)[0]\n    noise_shape = [batch_size, 1, 1, 1]\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)\n    binary_tensor = tf.cast(tf.floor(random_tensor), net.dtype)\n    keep_prob_inv = tf.cast(1.0 / keep_prob, net.dtype)\n    net = net * keep_prob_inv * binary_tensor\n\n  return net\n\n\ndef _operation_to_filter_shape(operation):\n  splitted_operation = operation.split(\'x\')\n  filter_shape = int(splitted_operation[0][-1])\n  assert filter_shape == int(\n      splitted_operation[1][0]), \'Rectangular filters not supported.\'\n  return filter_shape\n\n\ndef _operation_to_num_layers(operation):\n  splitted_operation = operation.split(\'_\')\n  if \'x\' in splitted_operation[-1]:\n    return 1\n  return int(splitted_operation[-1])\n\n\ndef _operation_to_info(operation):\n  """"""Takes in operation name and returns meta information.\n\n  An example would be \'separable_3x3_4\' -> (3, 4).\n\n  Args:\n    operation: String that corresponds to convolution operation.\n\n  Returns:\n    Tuple of (filter shape, num layers).\n  """"""\n  num_layers = _operation_to_num_layers(operation)\n  filter_shape = _operation_to_filter_shape(operation)\n  return num_layers, filter_shape\n\n\ndef _stacked_separable_conv(net, stride, operation, filter_size,\n                            use_bounded_activation):\n  """"""Takes in an operations and parses it to the correct sep operation.""""""\n  num_layers, kernel_size = _operation_to_info(operation)\n  activation_fn = tf.nn.relu6 if use_bounded_activation else tf.nn.relu\n  for layer_num in range(num_layers - 1):\n    net = activation_fn(net)\n    net = slim.separable_conv2d(\n        net,\n        filter_size,\n        kernel_size,\n        depth_multiplier=1,\n        scope=\'separable_{0}x{0}_{1}\'.format(kernel_size, layer_num + 1),\n        stride=stride)\n    net = slim.batch_norm(\n        net, scope=\'bn_sep_{0}x{0}_{1}\'.format(kernel_size, layer_num + 1))\n    stride = 1\n  net = activation_fn(net)\n  net = slim.separable_conv2d(\n      net,\n      filter_size,\n      kernel_size,\n      depth_multiplier=1,\n      scope=\'separable_{0}x{0}_{1}\'.format(kernel_size, num_layers),\n      stride=stride)\n  net = slim.batch_norm(\n      net, scope=\'bn_sep_{0}x{0}_{1}\'.format(kernel_size, num_layers))\n  return net\n\n\ndef _operation_to_pooling_type(operation):\n  """"""Takes in the operation string and returns the pooling type.""""""\n  splitted_operation = operation.split(\'_\')\n  return splitted_operation[0]\n\n\ndef _operation_to_pooling_shape(operation):\n  """"""Takes in the operation string and returns the pooling kernel shape.""""""\n  splitted_operation = operation.split(\'_\')\n  shape = splitted_operation[-1]\n  assert \'x\' in shape\n  filter_height, filter_width = shape.split(\'x\')\n  assert filter_height == filter_width\n  return int(filter_height)\n\n\ndef _operation_to_pooling_info(operation):\n  """"""Parses the pooling operation string to return its type and shape.""""""\n  pooling_type = _operation_to_pooling_type(operation)\n  pooling_shape = _operation_to_pooling_shape(operation)\n  return pooling_type, pooling_shape\n\n\ndef _pooling(net, stride, operation, use_bounded_activation):\n  """"""Parses operation and performs the correct pooling operation on net.""""""\n  padding = \'SAME\'\n  pooling_type, pooling_shape = _operation_to_pooling_info(operation)\n  if use_bounded_activation:\n    net = tf.nn.relu6(net)\n  if pooling_type == \'avg\':\n    net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=padding)\n  elif pooling_type == \'max\':\n    net = slim.max_pool2d(net, pooling_shape, stride=stride, padding=padding)\n  else:\n    raise NotImplementedError(\'Unimplemented pooling type: \', pooling_type)\n  return net\n\n\nclass NasNetABaseCell(object):\n  """"""NASNet Cell class that is used as a \'layer\' in image architectures.""""""\n\n  def __init__(self, num_conv_filters, operations, used_hiddenstates,\n               hiddenstate_indices, drop_path_keep_prob, total_num_cells,\n               total_training_steps, use_bounded_activation=False):\n    """"""Constructs a NasNetABaseCell.\n\n    Args:\n      num_conv_filters: The number of filters for each convolution operation.\n      operations: List of operations that are performed in the NASNet Cell in\n        order.\n      used_hiddenstates: Binary array that signals if the hiddenstate was used\n        within the cell. This is used to determine what outputs of the cell\n        should be concatenated together.\n      hiddenstate_indices: Determines what hiddenstates should be combined\n        together with the specified operations to create the NASNet cell.\n      drop_path_keep_prob: Keep probability during DropPath regularization.\n      total_num_cells: Total number of cells.\n      total_training_steps: Total training steps.\n      use_bounded_activation: Whether or not to use bounded activations. Bounded\n        activations better lend themselves to quantized inference.\n    """"""\n    self._num_conv_filters = num_conv_filters\n    self._operations = operations\n    self._used_hiddenstates = used_hiddenstates\n    self._hiddenstate_indices = hiddenstate_indices\n    self._drop_path_keep_prob = drop_path_keep_prob\n    self._total_num_cells = total_num_cells\n    self._total_training_steps = total_training_steps\n    self._use_bounded_activation = use_bounded_activation\n\n  def _reduce_prev_layer(self, prev_layer, curr_layer):\n    """"""Matches dimension of prev_layer to the curr_layer.""""""\n    # Set the prev layer to the current layer if it is none\n    if prev_layer is None:\n      return curr_layer\n    curr_num_filters = self._filter_size\n    prev_num_filters = get_channel_dim(prev_layer.shape)\n    curr_filter_shape = int(curr_layer.shape[2])\n    prev_filter_shape = int(prev_layer.shape[2])\n    activation_fn = tf.nn.relu6 if self._use_bounded_activation else tf.nn.relu\n    if curr_filter_shape != prev_filter_shape:\n      prev_layer = activation_fn(prev_layer)\n      prev_layer = factorized_reduction(\n          prev_layer, curr_num_filters, stride=2)\n    elif curr_num_filters != prev_num_filters:\n      prev_layer = activation_fn(prev_layer)\n      prev_layer = slim.conv2d(\n          prev_layer, curr_num_filters, 1, scope=\'prev_1x1\')\n      prev_layer = slim.batch_norm(prev_layer, scope=\'prev_bn\')\n    return prev_layer\n\n  def _cell_base(self, net, prev_layer):\n    """"""Runs the beginning of the conv cell before the predicted ops are run.""""""\n    num_filters = self._filter_size\n\n    # Check to be sure prev layer stuff is setup correctly\n    prev_layer = self._reduce_prev_layer(prev_layer, net)\n\n    net = tf.nn.relu6(net) if self._use_bounded_activation else tf.nn.relu(net)\n    net = slim.conv2d(net, num_filters, 1, scope=\'1x1\')\n    net = slim.batch_norm(net, scope=\'beginning_bn\')\n    # num_or_size_splits=1\n    net = [net]\n    net.append(prev_layer)\n    return net\n\n  def __call__(self, net, scope=None, filter_scaling=1, stride=1,\n               prev_layer=None, cell_num=-1, current_step=None):\n    """"""Runs the conv cell.""""""\n    self._cell_num = cell_num\n    self._filter_scaling = filter_scaling\n    self._filter_size = int(self._num_conv_filters * filter_scaling)\n\n    i = 0\n    with tf.variable_scope(scope):\n      net = self._cell_base(net, prev_layer)\n      for iteration in range(5):\n        with tf.variable_scope(\'comb_iter_{}\'.format(iteration)):\n          left_hiddenstate_idx, right_hiddenstate_idx = (\n              self._hiddenstate_indices[i],\n              self._hiddenstate_indices[i + 1])\n          original_input_left = left_hiddenstate_idx < 2\n          original_input_right = right_hiddenstate_idx < 2\n          h1 = net[left_hiddenstate_idx]\n          h2 = net[right_hiddenstate_idx]\n\n          operation_left = self._operations[i]\n          operation_right = self._operations[i+1]\n          i += 2\n          # Apply conv operations\n          with tf.variable_scope(\'left\'):\n            h1 = self._apply_conv_operation(h1, operation_left,\n                                            stride, original_input_left,\n                                            current_step)\n          with tf.variable_scope(\'right\'):\n            h2 = self._apply_conv_operation(h2, operation_right,\n                                            stride, original_input_right,\n                                            current_step)\n\n          # Combine hidden states using \'add\'.\n          with tf.variable_scope(\'combine\'):\n            h = h1 + h2\n            if self._use_bounded_activation:\n              h = tf.nn.relu6(h)\n\n          # Add hiddenstate to the list of hiddenstates we can choose from\n          net.append(h)\n\n      with tf.variable_scope(\'cell_output\'):\n        net = self._combine_unused_states(net)\n\n      return net\n\n  def _apply_conv_operation(self, net, operation,\n                            stride, is_from_original_input, current_step):\n    """"""Applies the predicted conv operation to net.""""""\n    # Dont stride if this is not one of the original hiddenstates\n    if stride > 1 and not is_from_original_input:\n      stride = 1\n    input_filters = get_channel_dim(net.shape)\n    filter_size = self._filter_size\n    if \'separable\' in operation:\n      net = _stacked_separable_conv(net, stride, operation, filter_size,\n                                    self._use_bounded_activation)\n      if self._use_bounded_activation:\n        net = tf.clip_by_value(net, -CLIP_BY_VALUE_CAP, CLIP_BY_VALUE_CAP)\n    elif operation in [\'none\']:\n      if self._use_bounded_activation:\n        net = tf.nn.relu6(net)\n      # Check if a stride is needed, then use a strided 1x1 here\n      if stride > 1 or (input_filters != filter_size):\n        if not self._use_bounded_activation:\n          net = tf.nn.relu(net)\n        net = slim.conv2d(net, filter_size, 1, stride=stride, scope=\'1x1\')\n        net = slim.batch_norm(net, scope=\'bn_1\')\n        if self._use_bounded_activation:\n          net = tf.clip_by_value(net, -CLIP_BY_VALUE_CAP, CLIP_BY_VALUE_CAP)\n    elif \'pool\' in operation:\n      net = _pooling(net, stride, operation, self._use_bounded_activation)\n      if input_filters != filter_size:\n        net = slim.conv2d(net, filter_size, 1, stride=1, scope=\'1x1\')\n        net = slim.batch_norm(net, scope=\'bn_1\')\n      if self._use_bounded_activation:\n        net = tf.clip_by_value(net, -CLIP_BY_VALUE_CAP, CLIP_BY_VALUE_CAP)\n    else:\n      raise ValueError(\'Unimplemented operation\', operation)\n\n    if operation != \'none\':\n      net = self._apply_drop_path(net, current_step=current_step)\n    return net\n\n  def _combine_unused_states(self, net):\n    """"""Concatenate the unused hidden states of the cell.""""""\n    used_hiddenstates = self._used_hiddenstates\n\n    final_height = int(net[-1].shape[2])\n    final_num_filters = get_channel_dim(net[-1].shape)\n    assert len(used_hiddenstates) == len(net)\n    for idx, used_h in enumerate(used_hiddenstates):\n      curr_height = int(net[idx].shape[2])\n      curr_num_filters = get_channel_dim(net[idx].shape)\n\n      # Determine if a reduction should be applied to make the number of\n      # filters match.\n      should_reduce = final_num_filters != curr_num_filters\n      should_reduce = (final_height != curr_height) or should_reduce\n      should_reduce = should_reduce and not used_h\n      if should_reduce:\n        stride = 2 if final_height != curr_height else 1\n        with tf.variable_scope(\'reduction_{}\'.format(idx)):\n          net[idx] = factorized_reduction(\n              net[idx], final_num_filters, stride)\n\n    states_to_combine = (\n        [h for h, is_used in zip(net, used_hiddenstates) if not is_used])\n\n    # Return the concat of all the states\n    concat_axis = get_channel_index()\n    net = tf.concat(values=states_to_combine, axis=concat_axis)\n    return net\n\n  @tf.contrib.framework.add_arg_scope  # No public API. For internal use only.\n  def _apply_drop_path(self, net, current_step=None,\n                       use_summaries=False, drop_connect_version=\'v3\'):\n    """"""Apply drop_path regularization.\n\n    Args:\n      net: the Tensor that gets drop_path regularization applied.\n      current_step: a float32 Tensor with the current global_step value,\n        to be divided by hparams.total_training_steps. Usually None, which\n        defaults to tf.train.get_or_create_global_step() properly casted.\n      use_summaries: a Python boolean. If set to False, no summaries are output.\n      drop_connect_version: one of \'v1\', \'v2\', \'v3\', controlling whether\n        the dropout rate is scaled by current_step (v1), layer (v2), or\n        both (v3, the default).\n\n    Returns:\n      The dropped-out value of `net`.\n    """"""\n    drop_path_keep_prob = self._drop_path_keep_prob\n    if drop_path_keep_prob < 1.0:\n      assert drop_connect_version in [\'v1\', \'v2\', \'v3\']\n      if drop_connect_version in [\'v2\', \'v3\']:\n        # Scale keep prob by layer number\n        assert self._cell_num != -1\n        # The added 2 is for the reduction cells\n        num_cells = self._total_num_cells\n        layer_ratio = (self._cell_num + 1)/float(num_cells)\n        if use_summaries:\n          with tf.device(\'/cpu:0\'):\n            tf.summary.scalar(\'layer_ratio\', layer_ratio)\n        drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\n      if drop_connect_version in [\'v1\', \'v3\']:\n        # Decrease the keep probability over time\n        if current_step is None:\n          current_step = tf.train.get_or_create_global_step()\n        current_step = tf.cast(current_step, tf.float32)\n        drop_path_burn_in_steps = self._total_training_steps\n        current_ratio = current_step / drop_path_burn_in_steps\n        current_ratio = tf.minimum(1.0, current_ratio)\n        if use_summaries:\n          with tf.device(\'/cpu:0\'):\n            tf.summary.scalar(\'current_ratio\', current_ratio)\n        drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob))\n      if use_summaries:\n        with tf.device(\'/cpu:0\'):\n          tf.summary.scalar(\'drop_path_keep_prob\', drop_path_keep_prob)\n      net = drop_path(net, drop_path_keep_prob)\n    return net\n\n\nclass NasNetANormalCell(NasNetABaseCell):\n  """"""NASNetA Normal Cell.""""""\n\n  def __init__(self, num_conv_filters, drop_path_keep_prob, total_num_cells,\n               total_training_steps, use_bounded_activation=False):\n    operations = [\'separable_5x5_2\',\n                  \'separable_3x3_2\',\n                  \'separable_5x5_2\',\n                  \'separable_3x3_2\',\n                  \'avg_pool_3x3\',\n                  \'none\',\n                  \'avg_pool_3x3\',\n                  \'avg_pool_3x3\',\n                  \'separable_3x3_2\',\n                  \'none\']\n    used_hiddenstates = [1, 0, 0, 0, 0, 0, 0]\n    hiddenstate_indices = [0, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n    super(NasNetANormalCell, self).__init__(num_conv_filters, operations,\n                                            used_hiddenstates,\n                                            hiddenstate_indices,\n                                            drop_path_keep_prob,\n                                            total_num_cells,\n                                            total_training_steps,\n                                            use_bounded_activation)\n\n\nclass NasNetAReductionCell(NasNetABaseCell):\n  """"""NASNetA Reduction Cell.""""""\n\n  def __init__(self, num_conv_filters, drop_path_keep_prob, total_num_cells,\n               total_training_steps, use_bounded_activation=False):\n    operations = [\'separable_5x5_2\',\n                  \'separable_7x7_2\',\n                  \'max_pool_3x3\',\n                  \'separable_7x7_2\',\n                  \'avg_pool_3x3\',\n                  \'separable_5x5_2\',\n                  \'none\',\n                  \'avg_pool_3x3\',\n                  \'separable_3x3_2\',\n                  \'max_pool_3x3\']\n    used_hiddenstates = [1, 1, 1, 0, 0, 0, 0]\n    hiddenstate_indices = [0, 1, 0, 1, 0, 1, 3, 2, 2, 0]\n    super(NasNetAReductionCell, self).__init__(num_conv_filters, operations,\n                                               used_hiddenstates,\n                                               hiddenstate_indices,\n                                               drop_path_keep_prob,\n                                               total_num_cells,\n                                               total_training_steps,\n                                               use_bounded_activation)\n'"
research/improve_nas/trainer/optimizer.py,10,"b'# Lint as: python3\n""""""Definition of optimizers and learning rate schedules.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport functools\n\nimport tensorflow.compat.v1 as tf\n\n\nclass LearningRateSchedule(object):\n  """"""A learning rate decay schedule interface.""""""\n\n  __metaclass__ = abc.ABCMeta\n\n  @abc.abstractmethod\n  def apply(self, learning_rate):\n    """"""Applies the learning rate decay schedule to the given learning rate.\n\n    Args:\n      learning_rate: Float `Tensor` learning rate.\n\n    Returns:\n      Float `Tensor` learning rate with applied decay schedule.\n    """"""\n\n\nclass Constant(LearningRateSchedule):\n  """"""A constant schedule.""""""\n\n  def apply(self, learning_rate):\n    """"""See `LearningRateSchedule`.""""""\n\n    return learning_rate\n\n\nclass Cosine(LearningRateSchedule):\n  """"""Cosine.""""""\n\n  def __init__(self, decay_steps, alpha):\n    """"""Returns a `Cosine` instance.\n\n    Args:\n      decay_steps: Number of steps to decay over.\n      alpha: Minimum learning rate value as a fraction of learning_rate.\n\n    Returns:\n      A `Cosine` instance.\n    """"""\n\n    self._decay_fn = functools.partial(\n        tf.train.cosine_decay, decay_steps=decay_steps, alpha=alpha)\n\n  def apply(self, learning_rate):\n    """"""See `LearningRateSchedule`.""""""\n\n    # Start at -1 since we increment before reading.\n    global_step = tf.get_variable(""decay_step"", initializer=-1, trainable=False)\n    increment_op = tf.assign_add(global_step, 1)\n    with tf.control_dependencies([increment_op]):\n      learning_rate = self._decay_fn(\n          learning_rate=learning_rate, global_step=global_step.read_value())\n    return learning_rate\n\n\ndef fn_with_name(optimizer_name,\n                 learning_rate_schedule=""constant"",\n                 cosine_decay_steps=None):\n  """"""Returns an optimizer_fn with the given name.\n\n  Args:\n    optimizer_name: Optimizer name string for identifying the optimizer. Either\n      \'adagrad\', \'adam\', \'momentum\', or \'sgd\'.\n    learning_rate_schedule: Type of learning rate schedule to use. Opened for\n      future extensions.\n    cosine_decay_steps: See `Cosine`.\n\n  Returns:\n    An optimizer_fn which takes a `learning_rate` scalar `Tensor` argument and\n      returns an `Optimizer` instance.\n\n  Raises:\n    ValueError: If `optimizer_name` is invalid.\n  """"""\n\n  optimizers = {\n      ""adagrad"": tf.train.AdagradOptimizer,\n      ""adam"": tf.train.AdamOptimizer,\n      ""lazy_adam"": tf.contrib.opt.LazyAdamOptimizer,\n      ""momentum"": functools.partial(tf.train.MomentumOptimizer, momentum=.9),\n      ""rmsprop"": tf.train.RMSPropOptimizer,\n      ""sgd"": tf.train.GradientDescentOptimizer,\n  }\n  optimizer_name = optimizer_name.lower()\n  if optimizer_name not in optimizers:\n    raise ValueError(""Invalid optimizer \'{}\'"".format(optimizer_name))\n  optimizer_fn = optimizers[optimizer_name]\n  schedules = {\n      ""constant"":\n          Constant(),\n      ""cosine"":\n          Cosine(decay_steps=cosine_decay_steps, alpha=0.0),\n  }\n  schedule_name = learning_rate_schedule.lower()\n  if schedule_name not in schedules:\n    raise ValueError(\n        ""Invalid learning_rate_schedule \'{}\'"".format(schedule_name))\n  schedule = schedules[schedule_name]\n\n  def _optimizer_with_schedule(learning_rate):\n    learning_rate = schedule.apply(learning_rate)\n    optimizer = optimizer_fn(learning_rate)\n    return optimizer, learning_rate\n  return _optimizer_with_schedule\n'"
research/improve_nas/trainer/subnetwork_utils.py,3,"b'# Lint as: python3\n""""""Definition of helpful functions to work with AdaNet subnetworks.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport copy\nimport tensorflow.compat.v1 as tf\n\n\ndef capture_variables(fn):\n  """"""Utility function that captures which tf variables were created by `fn`.\n\n  This function encourages style that is easy to write, resonably easy to\n  understand but against google codestyle.\n\n  In general, you have an function `f` that takes some arguments (`a` and `b`)\n  and returns some output. You may enclose it in lambda and get\n  `fn == lambda: f(a,b)`, which is a function without arguments that does the\n  same as `f`.\n\n  This idiom makes variable management much easier and less error prone. Usable\n  for prototyping or debugging.\n\n  Args:\n    fn: function with no arguments.\n\n  Returns:\n    tuple: First element of this touple is a list of tf variables created by\n        fn, second is the actual output of fn\n\n  """"""\n  vars_before_fn = tf.trainable_variables()\n  fn_return = fn()\n  vars_after_fn = tf.trainable_variables()\n  fn_vars = list(set(vars_after_fn) - set(vars_before_fn))\n  return set(fn_vars), fn_return\n\n\ndef copy_update(hparams, **kwargs):\n  """"""Deep copy hparams with values updated by kwargs.\n\n  This enables to use hparams in an immutable manner.\n  Args:\n    hparams: hyperparameters.\n    **kwargs: keyword arguments to change in hparams.\n\n  Returns:\n    updated hyperparameters object. Change in this object is not propagated to\n    the original hparams\n  """"""\n  values = hparams.values()\n  values.update(kwargs)\n  values = copy.deepcopy(values)\n  hp = tf.contrib.training.HParams(**values)\n  return hp\n\n\ndef get_persisted_value_from_ensemble(ensemble, key):\n  """"""Return constant persisted tensor values from the previous subnetwork.\n\n  Args:\n    ensemble: Previous ensemble.\n    key: Name of constant to get from eprsisted tensor.\n\n  Returns:\n    int|float value of the constant.\n  """"""\n  previous_subnetwork = ensemble.weighted_subnetworks[-1].subnetwork\n  persisted_tensor = previous_subnetwork.shared[key]\n  return persisted_tensor\n'"
research/improve_nas/trainer/trainer.py,11,"b'# Lint as: python3\n""""""Script to any experiment from paper.\n\nCopyright 2019 The AdaNet Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\n\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=g-import-not-at-top\ntry:\n  from adanet.research.improve_nas.trainer import adanet_improve_nas\n  from adanet.research.improve_nas.trainer import cifar10\n  from adanet.research.improve_nas.trainer import cifar100\n  from adanet.research.improve_nas.trainer import fake_data\n  print(""Imported from adanet."")\nexcept ImportError as e:\n  from trainer import adanet_improve_nas\n  from trainer import cifar10\n  from trainer import cifar100\n  from trainer import fake_data\n  print(""Imported from trainer."")\n# pylint: enable=g-import-not-at-top\n\n\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(""batch_size"", 32,\n                     ""Batch size used for training, eval and inference."")\nflags.DEFINE_integer(""train_steps"", 1000000, ""Number of training steps."")\nflags.DEFINE_integer(""save_summary_steps"", 2000,\n                     ""Save summaries every this many steps."")\nflags.DEFINE_string(\n    ""hparams"", """",\n    """"""A comma-separated list of `name=value` hyperparameter values."""""")\nflags.DEFINE_string(\n    ""dataset"", """",\n    ""Dataset name: \'cifar10\', \'cifar100\' or \'fake\'. \'fake\' dataset is mainly ""\n    ""for test runs."")\nflags.DEFINE_integer(""tf_random_seed"", None,\n                     ""Graph level random seed for TensorFlow."")\nflags.DEFINE_integer(""eval_steps"", None,\n                     ""Number of batches used for evaluation. If `None`, the ""\n                     ""whole eval dataset is used"")\nflags.DEFINE_integer(\n    ""save_checkpoints_secs"", 600, ""Number of seconds between checkpoint saves. ""\n    ""This flag is ignored when autotune is used. ""\n    ""Cannot be used with save_checkpoints_steps -- exactly one of ""\n    ""save_checkpoints_secs and save_checkpoints_steps must be zero, and the ""\n    ""other must be a strictly positive integer. Defaults to 120s."")\nflags.DEFINE_integer(\n    ""save_checkpoints_steps"", 0,\n    ""Number of global steps between checkpoint saves.""\n    ""This flag is ignored when autotune is used. ""\n    ""Cannot be used with save_checkpoints_secs -- exactly one of ""\n    ""save_checkpoints_secs and save_checkpoints_steps must be zero, and the ""\n    ""other must be a strictly positive integer. Defaults to 0, which means ""\n    ""save_checkpoints_steps is ignored. To use save_checkpoints_steps ""\n    ""instead, set save_checkpoints_secs to 0 and set save_checkpoints_steps ""\n    ""to a positive integer."")\nflags.DEFINE_string(\n    ""data_params"", """",\n    """"""A comma-separated list of `name=value` data provider parameter values.\n    This flag is used to override data provider default settings for\n    preprocessing or selecting different configurations for a given data\n    provider."""""")\nflags.DEFINE_integer(\n    ""keep_checkpoint_max"", 5,\n    ""The maximum number of recent checkpoint files to keep. As new files are ""\n    ""created, older files are deleted. If None or 0, all checkpoint files are ""\n    ""kept. Defaults to 5 (i.e. the 5 most recent checkpoint files are kept.)"")\n\nflags.DEFINE_string(\n    ""job-dir"", """",\n    ""Unused. Must be here because of ml-engine."")\nflags.DEFINE_string(\n    ""model_dir"", None, """"""Directory for saving models and logs."""""")\n\n\ndef make_run_config():\n  """"""Makes a RunConfig object with FLAGS.\n\n  Returns:\n    tf.estimator.RunConfig.\n  Raises:\n    ValueError: If not exactly one of `save_checkpoints_secs` and\n      `save_checkpoints_steps` is specified.\n  """"""\n  save_checkpoints_secs = FLAGS.save_checkpoints_secs or None\n  save_checkpoints_steps = FLAGS.save_checkpoints_steps or None\n  if save_checkpoints_secs and save_checkpoints_steps:\n    raise ValueError(""save_checkpoints_secs and save_checkpoints_steps ""\n                     ""cannot both be non-zero."")\n  if not (save_checkpoints_secs or save_checkpoints_steps):\n    raise ValueError(""save_checkpoints_secs and save_checkpoints_steps ""\n                     ""cannot both be zero."")\n\n  # An error is thrown by absl.flags if train.sh passes tf_random_seed=None, so\n  # it passes -1 instead.\n  if FLAGS.tf_random_seed == -1:\n    tf_random_seed = None\n  else:\n    tf_random_seed = FLAGS.tf_random_seed\n\n  return tf.estimator.RunConfig(\n      save_summary_steps=FLAGS.save_summary_steps,\n      model_dir=FLAGS.model_dir,\n      save_checkpoints_secs=save_checkpoints_secs,\n      save_checkpoints_steps=save_checkpoints_steps,\n      keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n      tf_random_seed=tf_random_seed)\n\n\ndef main(argv):\n  del argv\n\n  run_config = make_run_config()\n  estimator_builder = adanet_improve_nas.Builder()\n  hparams = estimator_builder.hparams(FLAGS.batch_size, FLAGS.hparams)\n\n  tf.logging.info(""Running Experiment with HParams: %s"", hparams)\n  if FLAGS.dataset == ""cifar10"":\n    data_provider = cifar10.Provider()\n  elif FLAGS.dataset == ""cifar100"":\n    data_provider = cifar100.Provider()\n  elif FLAGS.dataset == ""fake"":\n    data_provider = fake_data.FakeImageProvider(\n        num_examples=10,\n        num_classes=10,\n        image_dim=32,\n        channels=3,\n        seed=42)\n  else:\n    raise ValueError(""Invalid dataset"")\n\n  estimator = estimator_builder.estimator(\n      data_provider=data_provider,\n      run_config=run_config,\n      hparams=hparams,\n      train_steps=FLAGS.train_steps)\n\n  train_spec = tf.estimator.TrainSpec(\n      input_fn=data_provider.get_input_fn(\n          partition=""train"",\n          mode=tf.estimator.ModeKeys.TRAIN,\n          batch_size=FLAGS.batch_size),\n      max_steps=FLAGS.train_steps\n  )\n\n  eval_spec = tf.estimator.EvalSpec(\n      input_fn=data_provider.get_input_fn(\n          partition=""test"",\n          mode=tf.estimator.ModeKeys.EVAL,\n          batch_size=FLAGS.batch_size),\n      steps=FLAGS.eval_steps,\n      start_delay_secs=10,\n      throttle_secs=1800\n  )\n\n  tf.logging.info(""Training!"")\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  tf.logging.info(""Done training!"")\n\n\nif __name__ == ""__main__"":\n  tf.app.run(main)\n'"
