file_path,api_count,code
configure.py,5,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Usage: python configure.py\n#\n\n\nimport os\nimport pathlib\nimport platform\nimport logging\n\nimport tensorflow as tf\n\n_TFA_BAZELRC = "".bazelrc""\n\n\n# Writes variables to bazelrc file\ndef write(line):\n    with open(_TFA_BAZELRC, ""a"") as f:\n        f.write(line + ""\\n"")\n\n\ndef write_action_env(var_name, var):\n    write(\'build --action_env {}=""{}""\'.format(var_name, var))\n\n\ndef is_macos():\n    return platform.system() == ""Darwin""\n\n\ndef is_windows():\n    return platform.system() == ""Windows""\n\n\ndef get_tf_header_dir():\n    import tensorflow as tf\n\n    tf_header_dir = tf.sysconfig.get_compile_flags()[0][2:]\n    if is_windows():\n        tf_header_dir = tf_header_dir.replace(""\\\\"", ""/"")\n    return tf_header_dir\n\n\ndef get_tf_shared_lib_dir():\n    import tensorflow as tf\n\n    # OS Specific parsing\n    if is_windows():\n        tf_shared_lib_dir = tf.sysconfig.get_compile_flags()[0][2:-7] + ""python""\n        return tf_shared_lib_dir.replace(""\\\\"", ""/"")\n    else:\n        return tf.sysconfig.get_link_flags()[0][2:]\n\n\n# Converts the linkflag namespec to the full shared library name\ndef get_shared_lib_name():\n    import tensorflow as tf\n\n    namespec = tf.sysconfig.get_link_flags()\n    if is_macos():\n        # MacOS\n        return ""lib"" + namespec[1][2:] + "".dylib""\n    elif is_windows():\n        # Windows\n        return ""_pywrap_tensorflow_internal.lib""\n    else:\n        # Linux\n        return namespec[1][3:]\n\n\ndef create_build_configuration():\n    print()\n    print(""Configuring TensorFlow Addons to be built from source..."")\n\n    if os.path.isfile(_TFA_BAZELRC):\n        os.remove(_TFA_BAZELRC)\n\n    logging.disable(logging.WARNING)\n\n    write_action_env(""TF_HEADER_DIR"", get_tf_header_dir())\n    write_action_env(""TF_SHARED_LIBRARY_DIR"", get_tf_shared_lib_dir())\n    write_action_env(""TF_SHARED_LIBRARY_NAME"", get_shared_lib_name())\n    write_action_env(""TF_CXX11_ABI_FLAG"", tf.sysconfig.CXX11_ABI_FLAG)\n\n    write(""build --spawn_strategy=standalone"")\n    write(""build --strategy=Genrule=standalone"")\n    write(""build -c opt"")\n\n    if os.getenv(""TF_NEED_CUDA"", ""0"") == ""1"":\n        print(""> Building GPU & CPU ops"")\n        configure_cuda()\n    else:\n        print(""> Building only CPU ops"")\n\n    print()\n    print(""Build configurations successfully written to"", _TFA_BAZELRC, "":\\n"")\n    print(pathlib.Path(_TFA_BAZELRC).read_text())\n\n\ndef configure_cuda():\n    write_action_env(""TF_NEED_CUDA"", ""1"")\n    write_action_env(\n        ""CUDA_TOOLKIT_PATH"", os.getenv(""CUDA_TOOLKIT_PATH"", ""/usr/local/cuda"")\n    )\n    write_action_env(\n        ""CUDNN_INSTALL_PATH"",\n        os.getenv(""CUDNN_INSTALL_PATH"", ""/usr/lib/x86_64-linux-gnu""),\n    )\n    write_action_env(""TF_CUDA_VERSION"", os.getenv(""TF_CUDA_VERSION"", ""10.1""))\n    write_action_env(""TF_CUDNN_VERSION"", os.getenv(""TF_CUDNN_VERSION"", ""7""))\n\n    write(""test --config=cuda"")\n    write(""build --config=cuda"")\n    write(""build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true"")\n    write(""build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain"")\n\n\nif __name__ == ""__main__"":\n    create_build_configuration()\n'"
setup.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TensorFlow Addons.\n\nTensorFlow Addons is a repository of contributions that conform to well-\nestablished API patterns, but implement new functionality not available\nin core TensorFlow. TensorFlow natively supports a large number of\noperators, layers, metrics, losses, and optimizers. However, in a fast\nmoving field like ML, there are many interesting new developments that\ncannot be integrated into core TensorFlow (because their broad\napplicability is not yet clear, or it is mostly used by a smaller subset\nof the community).\n""""""\n\nimport os\nfrom pathlib import Path\nimport sys\n\nfrom datetime import datetime\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom setuptools.dist import Distribution\nfrom setuptools import Extension\n\nDOCLINES = __doc__.split(""\\n"")\n\n\ndef get_last_commit_time() -> str:\n    string_time = os.getenv(""NIGHTLY_TIME"").replace(\'""\', """")\n    return datetime.strptime(string_time, ""%Y-%m-%dT%H:%M:%SZ"").strftime(""%Y%m%d%H%M%S"")\n\n\ndef get_project_name_version():\n    # Version\n    version = {}\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    with open(os.path.join(base_dir, ""tensorflow_addons"", ""version.py"")) as fp:\n        exec(fp.read(), version)\n\n    project_name = ""tensorflow-addons""\n    if ""--nightly"" in sys.argv:\n        project_name = ""tfa-nightly""\n        version[""__version__""] += get_last_commit_time()\n        sys.argv.remove(""--nightly"")\n\n    return project_name, version\n\n\ndef get_ext_modules():\n    ext_modules = []\n    if ""--platlib-patch"" in sys.argv:\n        if sys.platform.startswith(""linux""):\n            # Manylinux2010 requires a patch for platlib\n            ext_modules = [Extension(""_foo"", [""stub.cc""])]\n        sys.argv.remove(""--platlib-patch"")\n    return ext_modules\n\n\nclass BinaryDistribution(Distribution):\n    """"""This class is needed in order to create OS specific wheels.""""""\n\n    def has_ext_modules(self):\n        return True\n\n\nproject_name, version = get_project_name_version()\nsetup(\n    name=project_name,\n    version=version[""__version__""],\n    description=DOCLINES[0],\n    long_description=""\\n"".join(DOCLINES[2:]),\n    author=""Google Inc."",\n    author_email=""opensource@google.com"",\n    packages=find_packages(),\n    ext_modules=get_ext_modules(),\n    install_requires=Path(""requirements.txt"").read_text().splitlines(),\n    include_package_data=True,\n    zip_safe=False,\n    distclass=BinaryDistribution,\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Topic :: Scientific/Engineering :: Mathematics"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n        ""Topic :: Software Development :: Libraries"",\n    ],\n    license=""Apache 2.0"",\n    keywords=""tensorflow addons machine learning"",\n)\n'"
docs/build_docs.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Modified from the tfdocs example api reference docs generation script.\n\nThis script generates API reference docs.\n\nInstall pre-requisites:\n$> pip install -U git+https://github.com/tensorflow/docs\n$> pip install artifacts/tensorflow_addons-*.whl\n\nGenerate Docs:\n$> from the repo root run: python docs/build_docs.py\n""""""\n\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow_addons as tfa\n\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nPROJECT_SHORT_NAME = ""tfa""\nPROJECT_FULL_NAME = ""TensorFlow Addons""\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    ""git_branch"", default=None, help=""The name of the corresponding branch on github.""\n)\n\nCODE_PREFIX_TEMPLATE = (\n    ""https://github.com/tensorflow/addons/tree/{git_branch}/tensorflow_addons""\n)\nflags.DEFINE_string(""code_url_prefix"", None, ""The url prefix for links to the code."")\nflags.mark_flags_as_mutual_exclusive([""code_url_prefix"", ""git_branch""])\n\nflags.DEFINE_string(""output_dir"", ""/tmp/addons_api"", ""Where to output the docs"")\n\nflags.DEFINE_bool(\n    ""search_hints"", True, ""Include metadata search hints in the generated files""\n)\n\nflags.DEFINE_string(\n    ""site_path"", ""addons/api_docs/python"", ""Path prefix in the _toc.yaml""\n)\n\n\ndef main(argv):\n    if argv[1:]:\n        raise ValueError(""Unrecognized arguments: {}"".format(argv[1:]))\n\n    if FLAGS.code_url_prefix:\n        code_url_prefix = FLAGS.code_url_prefix\n    elif FLAGS.git_branch:\n        code_url_prefix = CODE_PREFIX_TEMPLATE.format(git_branch=FLAGS.git_branch)\n    else:\n        code_url_prefix = CODE_PREFIX_TEMPLATE.format(git_branch=""master"")\n\n    doc_generator = generate_lib.DocGenerator(\n        root_title=PROJECT_FULL_NAME,\n        py_modules=[(PROJECT_SHORT_NAME, tfa)],\n        code_url_prefix=code_url_prefix,\n        private_map={""tfa"": [""__version__"", ""utils"", ""version""]},\n        # This callback usually cleans up a lot of aliases caused by internal imports.\n        callbacks=[public_api.local_definitions_filter],\n        search_hints=FLAGS.search_hints,\n        site_path=FLAGS.site_path,\n    )\n\n    doc_generator.build(FLAGS.output_dir)\n\n    print(""Output docs to: "", FLAGS.output_dir)\n\n\nif __name__ == ""__main__"":\n    app.run(main)\n'"
tensorflow_addons/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Useful extra functionality for TensorFlow maintained by SIG-addons.""""""\nfrom tensorflow_addons.utils.ensure_tf_install import _check_tf_version\n\n_check_tf_version()\n\n# Local project imports\nfrom tensorflow_addons import activations\nfrom tensorflow_addons import callbacks\nfrom tensorflow_addons import image\nfrom tensorflow_addons import layers\nfrom tensorflow_addons import losses\nfrom tensorflow_addons import metrics\nfrom tensorflow_addons import optimizers\nfrom tensorflow_addons import rnn\nfrom tensorflow_addons import seq2seq\nfrom tensorflow_addons import text\nfrom tensorflow_addons.register import register_all\n\nfrom tensorflow_addons.version import __version__\n'"
tensorflow_addons/conftest.py,0,"b'from tensorflow_addons.utils.test_utils import (  # noqa: F401\n    maybe_run_functions_eagerly,\n    pytest_make_parametrize_id,\n    data_format,\n    set_seeds,\n    pytest_addoption,\n    set_global_variables,\n    pytest_configure,\n    device,\n    pytest_generate_tests,\n    pytest_collection_modifyitems,\n)\n\n# fixtures present in this file will be available\n# when running tests and can be referenced with strings\n# https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functions\n'"
tensorflow_addons/options.py,0,"b'import os\nimport platform\nimport warnings\nimport traceback\n\ntry:\n    TF_ADDONS_PY_OPS = bool(int(os.environ[""TF_ADDONS_PY_OPS""]))\nexcept KeyError:\n    if platform.system() == ""Linux"":\n        TF_ADDONS_PY_OPS = False\n    else:\n        TF_ADDONS_PY_OPS = True\n\n\nFALLBACK_WARNING_TEMPLATE = """"""{}\n\nThe {} C++/CUDA custom op could not be loaded.\nFor this reason, Addons will fallback to an implementation written\nin Python with public TensorFlow ops. There worst you might experience with\nthis is a moderate slowdown on GPU. There can be multiple\nreason for this loading error, one of them may be an ABI incompatibility between\nthe TensorFlow installed on your system and the TensorFlow used to compile\nTensorFlow Addons\' custom ops. The stacktrace generated when loading the\nshared object file was displayed above.\n\nIf you want this warning to disappear, either make sure the TensorFlow installed\nis compatible with this version of Addons, or tell TensorFlow Addons to\nprefer using Python implementations and not custom C++/CUDA ones. You can do that\nby changing the TF_ADDONS_PY_OPS flag\neither with the environment variable:\n```bash\nTF_ADDONS_PY_OPS=1 python my_script.py\n```\nor in your code, after your imports:\n```python\nimport tensorflow_addons as tfa\nimport ...\nimport ...\n\ntfa.options.TF_ADDONS_PY_OPS = True\n```\n""""""\n\n\ndef warn_fallback(op_name):\n    warning_msg = FALLBACK_WARNING_TEMPLATE.format(traceback.format_exc(), op_name)\n    warnings.warn(warning_msg, RuntimeWarning)\n    global TF_ADDONS_PY_OPS\n    TF_ADDONS_PY_OPS = True\n'"
tensorflow_addons/register.py,8,"b'import glob\nimport os\nfrom pathlib import Path\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.resource_loader import get_project_root\n\n\ndef register_all(keras_objects: bool = True, custom_kernels: bool = True) -> None:\n    """"""Register TensorFlow Addons\' objects in TensorFlow global dictionaries.\n\n    When loading a Keras model that has a TF Addons\' function, it is needed\n    for this function to be known by the Keras deserialization process.\n\n    There are two ways to do this, either do\n\n    ```python\n    tf.keras.models.load_model(\n        ""my_model.tf"",\n        custom_objects={""LAMB"": tfa.image.optimizer.LAMB}\n    )\n    ```\n\n    or you can do:\n    ```python\n    tfa.register_all()\n    tf.tf.keras.models.load_model(""my_model.tf"")\n    ```\n\n    If the model contains custom ops (compiled ops) of TensorFlow Addons,\n    and the graph is loaded with `tf.saved_model.load`, then custom ops need\n    to be registered before to avoid an error of the type:\n\n    ```\n    tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered\n    \'...\' in binary running on ... Make sure the Op and Kernel are\n    registered in the binary running in this process.\n    ```\n\n    In this case, the only way to make sure that the ops are registered is to call\n    this function:\n\n    ```python\n    tfa.register_all()\n    tf.saved_model.load(""my_model.tf"")\n    ```\n\n    Note that you can call this function multiple times in the same process,\n    it only has an effect the first time. Afterward, it\'s just a no-op.\n\n    Args:\n        keras_objects: boolean, `True` by default. If `True`, register all\n            Keras objects\n            with `tf.keras.utils.register_keras_serializable(package=""Addons"")`\n            If set to False, doesn\'t register any Keras objects\n            of Addons in TensorFlow.\n        custom_kernels: boolean, `True` by default. If `True`, loads all\n            custom kernels of TensorFlow Addons with\n            `tf.load_op_library(""path/to/so/file.so"")`. Loading the SO files\n            register them automatically. If `False` doesn\'t load and register\n            the shared objects files. Not that it might be useful to turn it off\n            if your installation of Addons doesn\'t work well with custom ops.\n    Returns:\n        None\n    """"""\n    if keras_objects:\n        register_keras_objects()\n    if custom_kernels:\n        register_custom_kernels()\n\n\ndef register_keras_objects() -> None:\n    # TODO: once layer_test is replaced by a public API\n    # and we can used unregistered objects with it\n    # we can remove all decorators.\n    # And register Keras objects here.\n    pass\n\n\ndef register_custom_kernels() -> None:\n    all_shared_objects = _get_all_shared_objects()\n    if not all_shared_objects:\n        raise FileNotFoundError(\n            ""No shared objects files were found in the custom ops ""\n            ""directory in Tensorflow Addons, check your installation again,""\n            ""or, if you don\'t need custom ops, call `tfa.register_all(custom_kernels=False)`""\n            "" instead.""\n        )\n    try:\n        for shared_object in all_shared_objects:\n            tf.load_op_library(shared_object)\n    except tf.errors.NotFoundError as e:\n        raise RuntimeError(\n            ""One of the shared objects ({}) could not be loaded. This may be ""\n            ""due to a number of reasons (incompatible TensorFlow version, buiding from ""\n            ""source with different flags, broken install of TensorFlow Addons...). If you""\n            ""wanted to register the shared objects because you needed them when loading your ""\n            ""model, you should fix your install of TensorFlow Addons. If you don\'t ""\n            ""use custom ops in your model, you can skip registering custom ops with ""\n            ""`tfa.register_all(custom_kernels=False)`"".format(shared_object)\n        ) from e\n\n\ndef _get_all_shared_objects():\n    custom_ops_dir = os.path.join(get_project_root(), ""custom_ops"")\n    all_shared_objects = glob.glob(custom_ops_dir + ""/**/*.so"", recursive=True)\n    all_shared_objects = [x for x in all_shared_objects if Path(x).is_file()]\n    return all_shared_objects\n'"
tensorflow_addons/version.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Define TensorFlow Addons version information.""""""\n\n# We follow Semantic Versioning (https://semver.org/)\n_MAJOR_VERSION = ""0""\n_MINOR_VERSION = ""11""\n_PATCH_VERSION = ""0""\n\n# When building releases, we can update this value on the release branch to\n# reflect the current release candidate (\'rc0\', \'rc1\') or, finally, the official\n# stable release (indicated by `_VERSION_SUFFIX = \'\'`). Outside the context of a\n# release branch, the current version is by default assumed to be a\n# \'development\' version, labeled \'dev\'.\n_VERSION_SUFFIX = ""dev""\n\n# Example, \'0.1.0-dev\'\n__version__ = ""."".join([_MAJOR_VERSION, _MINOR_VERSION, _PATCH_VERSION,])\nif _VERSION_SUFFIX:\n    __version__ = ""{}-{}"".format(__version__, _VERSION_SUFFIX)\n'"
tools/format.py,0,"b'#!/usr/bin/env python\nfrom subprocess import check_call, CalledProcessError\n\n\ndef check_bash_call(string):\n    check_call([""bash"", ""-c"", string])\n\n\ndef _run_format_and_flake8():\n    files_changed = False\n\n    try:\n        check_bash_call(""python -m black --check ./"")\n    except CalledProcessError:\n        check_bash_call(""python -m black ./"")\n        files_changed = True\n\n    try:\n        check_bash_call(""buildifier -mode=check -r ."")\n    except CalledProcessError:\n        check_bash_call(""buildifier -r ."")\n        files_changed = True\n\n    # todo: find a way to check if files changed\n    # see https://github.com/DoozyX/clang-format-lint-action for inspiration\n    check_bash_call(\n        ""shopt -s globstar && clang-format-9 -i --style=google **/*.cc **/*.h"",\n    )\n\n    if files_changed:\n        print(""Some files have changed."")\n        print(""Please do git add and git commit again"")\n    else:\n        print(""No formatting needed."")\n\n    print(""Running flake8."")\n    check_bash_call(""flake8"")\n    print(""Done"")\n\n    if files_changed:\n        exit(1)\n\n\ndef run_format_and_flake8():\n    try:\n        _run_format_and_flake8()\n    except CalledProcessError as error:\n        print(""Pre-commit returned exit code"", error.returncode)\n        exit(error.returncode)\n\n\nif __name__ == ""__main__"":\n    run_format_and_flake8()\n'"
.github/workflows/notify_codeowners.py,0,"b'import github\nimport click\nimport urllib.request\nfrom pathlib import Path\nfrom typing import List, Tuple\nimport re\nimport fnmatch\nimport glob\nimport os\nimport json\n\n# Github already take care\n# of notifying users with write access\nBLACKLIST = [\n    ""tensorflow/sig-addons-maintainers"",\n    ""facaiy"",\n    ""seanpmorgan"",\n    ""squadrick"",\n    ""shun-lin"",\n    ""windqaq"",\n    ""qlzh727"",\n    ""guillaumekln"",\n]\n\n\ndef xor_strings(a, b):\n    result = int(a, 16) ^ int(b, 16)\n    return ""{:x}"".format(result)\n\n\ndef get_github_client():\n    bot_token = ""1353d990cdb8b8ceb1b73d301dce83cc0da3db29""\n    bot_token_key = ""a1b2c3d47311f8e29e204f85a81b4df4a44e252c""\n\n    return github.Github(xor_strings(bot_token, bot_token_key))\n\n\nCLIENT = get_github_client()\n\n\ndef check_user(user: str, line_idx: int):\n    if user[0] != ""@"":\n        raise ValueError(\n            f""User \'{user}\' at line {line_idx} of CODEOWNERS ""\n            f""doesn\'t start with \'@\' ""\n        )\n    user = user[1:]\n    user = user.lower()  # in github, user names are case insensitive\n    if user in BLACKLIST:\n        return None\n    try:\n        CLIENT.get_user(user)\n    except github.UnknownObjectException:\n        raise KeyError(\n            f""User \'{user}\' line {line_idx} does not exist. Did you make a typo?""\n        )\n    return user\n\n\ndef check_pattern(pattern: str, line_idx: int):\n    if pattern[0] == ""/"":\n        pattern = pattern[1:]\n\n    pattern = Pattern(pattern)\n\n    if not pattern.match_in_dir("".""):\n        raise FileNotFoundError(\n            f""\'{pattern.string}\' present in CODEOWNERS line""\n            f"" {line_idx} does not match any file in the repository. ""\n            f""Did you make a typo?""\n        )\n\n    return pattern\n\n\nclass Pattern:\n    def __init__(self, pattern: str):\n        self.string = pattern\n        if ""*"" in self.string:\n            self.regex = re.compile(fnmatch.translate(pattern))\n        else:\n            self.regex = None\n\n    def match(self, file):\n        if self.regex:\n            return re.match(self.regex, file) is not None\n        else:\n            return file.startswith(self.string)\n\n    def match_in_dir(self, directory):\n        list_files = glob.glob(f""{directory}/**/*"", recursive=True)\n        list_files = [os.path.relpath(x, directory) for x in list_files]\n        matching_files = list(filter(self.match, list_files))\n        return bool(matching_files)\n\n\nCodeOwners = List[Tuple[Pattern, List[str]]]\n\n\ndef parse_codeowners(text: str) -> CodeOwners:\n    result = []\n\n    for i, line in enumerate(text.splitlines()):\n        line = line.strip()\n        if line == """":\n            continue\n        if line[0] == ""#"":  # comment\n            continue\n        elements = list(filter(lambda x: x != """", line.split("" "")))\n\n        pattern = check_pattern(elements[0], i)\n        users = [check_user(user, i) for user in elements[1:]]\n        users = [user for user in users if user is not None]\n        if users:\n            result.append((pattern, users))\n\n    return result\n\n\nnice_message = """"""\n\nYou are owner{} of some files modified in this pull request.\nWould you kindly review the changes whenever you have the time to?\nThank you very much.\n""""""\n\n\ndef craft_message(codeowners: CodeOwners, pull_request):\n\n    owners = set()\n    for file in pull_request.get_files():\n        for pattern, users in codeowners:\n            if not pattern.match(file.filename):\n                continue\n            owners.update(users)\n\n    author = pull_request.user.login.lower()\n    try:\n        owners.remove(author)  # no need to notify the author\n    except KeyError:\n        pass\n\n    owners = [f""@{owner}"" for owner in owners]\n    if not owners:\n        return None\n    if len(owners) >= 2:\n        plural = ""s""\n    else:\n        plural = """"\n    return "" "".join(owners + [nice_message.format(plural)])\n\n\ndef get_pull_request_id_from_gh_actions():\n    actions_file = Path(os.environ[""GITHUB_EVENT_PATH""])\n    return json.loads(actions_file.read_text())[""number""]\n\n\n@click.command()\n@click.option(""--pull-request-id"")\n@click.option(""--no-dry-run"", is_flag=True)\n@click.argument(""file"")\ndef notify_codeowners(pull_request_id, no_dry_run, file):\n    if file.startswith(""http""):\n        text = urllib.request.urlopen(file).read().decode(""utf-8"")\n    else:\n        text = Path(file).read_text()\n    codeowners = parse_codeowners(text)\n\n    if pull_request_id is not None:\n        if pull_request_id == ""auto"":\n            pull_request_id = get_pull_request_id_from_gh_actions()\n        pull_request_id = int(pull_request_id)\n        pull_request = CLIENT.get_repo(""tensorflow/addons"").get_pull(pull_request_id)\n        msg = craft_message(codeowners, pull_request)\n        print(msg)\n        if no_dry_run and msg is not None:\n            pull_request.create_issue_comment(msg)\n\n\nif __name__ == ""__main__"":\n    notify_codeowners()\n'"
tensorflow_addons/activations/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional activation functions.""""""\n\nfrom tensorflow_addons.activations.gelu import gelu\nfrom tensorflow_addons.activations.hardshrink import hardshrink\nfrom tensorflow_addons.activations.lisht import lisht\nfrom tensorflow_addons.activations.mish import mish\nfrom tensorflow_addons.activations.softshrink import softshrink\nfrom tensorflow_addons.activations.rrelu import rrelu\nfrom tensorflow_addons.activations.sparsemax import sparsemax\nfrom tensorflow_addons.activations.tanhshrink import tanhshrink\n'"
tensorflow_addons/activations/gelu.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nimport math\nimport warnings\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef gelu(x: types.TensorLike, approximate: bool = True) -> tf.Tensor:\n    """"""Gaussian Error Linear Unit.\n\n    Computes gaussian error linear:\n    `0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))` or\n    `x * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2)))`, where P(X) ~ N(0, 1),\n    depending on whether approximation is enabled.\n\n    See [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)\n    and [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n        approximate: bool, whether to enable approximation.\n    Returns:\n        A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _gelu_custom_op(x, approximate)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""gelu"")\n\n    return _gelu_py(x, approximate)\n\n\ndef _gelu_custom_op(x, approximate):\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be removed in TensorFlow Addons ""\n        ""v0.12.0. \\nPlease use the pure python version of Gelu instead by using the ""\n        ""`TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_gelu(x, approximate)\n\n\n@tf.RegisterGradient(""Addons>Gelu"")\ndef _gelu_grad(op, grad):\n    return _activation_so.ops.addons_gelu_grad(\n        grad, op.inputs[0], op.get_attr(""approximate"")\n    )\n\n\ndef _gelu_py(x: types.TensorLike, approximate: bool = True) -> tf.Tensor:\n    x = tf.convert_to_tensor(x)\n    if approximate:\n        pi = tf.cast(math.pi, x.dtype)\n        coeff = tf.cast(0.044715, x.dtype)\n        return 0.5 * x * (1.0 + tf.tanh(tf.sqrt(2.0 / pi) * (x + coeff * tf.pow(x, 3))))\n    else:\n        return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(tf.sqrt(2.0), x.dtype)))\n'"
tensorflow_addons/activations/hardshrink.py,8,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import Number\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef hardshrink(\n    x: types.TensorLike, lower: Number = -0.5, upper: Number = 0.5\n) -> tf.Tensor:\n    """"""Hard shrink function.\n\n    Computes hard shrink function:\n    `x if x < lower or x > upper else 0`.\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n        lower: `float`, lower bound for setting values to zeros.\n        upper: `float`, upper bound for setting values to zeros.\n    Returns:\n        A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _hardshrink_custom_op(x, lower, upper)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""hardshrink"")\n\n    return _hardshrink_py(x, lower, upper)\n\n\ndef _hardshrink_custom_op(x, lower=-0.5, upper=0.5):\n    """"""Alias with lazy loading of the .so file""""""\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be removed in ""\n        ""TensorFlow Addons v0.12.0. \\nPlease use the pure python version of ""\n        ""hardshrink instead by using the ""\n        ""`TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_hardshrink(x, lower, upper)\n\n\n@tf.RegisterGradient(""Addons>Hardshrink"")\ndef _hardshrink_grad(op, grad):\n    return _activation_so.ops.addons_hardshrink_grad(\n        grad, op.inputs[0], op.get_attr(""lower""), op.get_attr(""upper"")\n    )\n\n\ndef _hardshrink_py(\n    x: types.TensorLike, lower: Number = -0.5, upper: Number = 0.5\n) -> tf.Tensor:\n    if lower > upper:\n        raise ValueError(\n            ""The value of lower is {} and should""\n            "" not be higher than the value ""\n            ""variable upper, which is {} ."".format(lower, upper)\n        )\n    mask_lower = x < lower\n    mask_upper = upper < x\n    mask = tf.logical_or(mask_lower, mask_upper)\n    mask = tf.cast(mask, x.dtype)\n    return x * mask\n'"
tensorflow_addons/activations/lisht.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef lisht(x: types.TensorLike) -> tf.Tensor:\n    """"""LiSHT: Non-Parameteric Linearly Scaled Hyperbolic Tangent Activation Function.\n\n    Computes linearly scaled hyperbolic tangent (LiSHT): `x * tanh(x)`\n\n    See [LiSHT: Non-Parameteric Linearly Scaled Hyperbolic Tangent Activation Function for Neural Networks](https://arxiv.org/abs/1901.05894).\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n    Returns:\n        A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _lisht_custom_op(x)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""lisht"")\n\n    return _lisht_py(x)\n\n\ndef _lisht_custom_op(x):\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be removed in TensorFlow ""\n        ""Addons v0.12.0. \\nPlease use the pure python version of lisht instead by ""\n        ""using the `TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_lisht(x)\n\n\n@tf.RegisterGradient(""Addons>Lisht"")\ndef _lisht_grad(op, grad):\n    return _activation_so.ops.addons_lisht_grad(grad, op.inputs[0])\n\n\ndef _lisht_py(x):\n    return x * tf.math.tanh(x)\n'"
tensorflow_addons/activations/mish.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef mish(x: types.TensorLike) -> tf.Tensor:\n    """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function.\n\n    Computes mish activation: x * tanh(softplus(x))\n\n    See [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/abs/1908.08681).\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n    Returns:\n        A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _mish_custom_op(x)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""mish"")\n\n    return _mish_py(x)\n\n\ndef _mish_custom_op(x):\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be removed in TensorFlow ""\n        ""Addons v0.12.0. \\nPlease use the pure python version of mish instead by using ""\n        ""the `TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_mish(x)\n\n\n@tf.RegisterGradient(""Addons>Mish"")\ndef _mish_grad(op, grad):\n    return _activation_so.ops.addons_mish_grad(grad, op.inputs[0])\n\n\ndef _mish_py(x):\n    return x * tf.math.tanh(tf.math.softplus(x))\n'"
tensorflow_addons/activations/rrelu.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import TensorLike, Number\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef rrelu(\n    x: TensorLike,\n    lower: Number = 0.125,\n    upper: Number = 0.3333333333333333,\n    training: Optional[bool] = None,\n    seed: Optional[int] = None,\n    rng: Optional[tf.random.Generator] = None,\n) -> tf.Tensor:\n    """"""rrelu function.\n\n    Computes rrelu function:\n    `x if x > 0 else random(lower, upper) * x` or\n    `x if x > 0 else x * (lower + upper) / 2`\n    depending on whether training is enabled.\n\n    See [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853).\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n        lower: `float`, lower bound for random alpha.\n        upper: `float`, upper bound for random alpha.\n        training: `bool`, indicating whether the `call`\n        is meant for training or inference.\n        seed: `int`, this sets the operation-level seed.\n        rng: A `Generator`.\n    Returns:\n        result: A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n    lower = tf.cast(lower, x.dtype)\n    upper = tf.cast(upper, x.dtype)\n\n    if training is None:\n        training = tf.keras.backend.learning_phase()\n        training = bool(tf.keras.backend.get_value(training))\n\n    if training:\n        if rng is not None and seed is not None:\n            raise ValueError(\n                ""Either seed or rng should be specified. Not both at the same time.""\n            )\n        elif rng is not None:\n            alpha = rng.uniform(tf.shape(x), minval=lower, maxval=upper, dtype=x.dtype)\n        else:\n            alpha = tf.random.uniform(\n                tf.shape(x), minval=lower, maxval=upper, dtype=x.dtype, seed=seed\n            )\n    else:\n        alpha = (lower + upper) / 2\n\n    return tf.where(x >= 0, x, alpha * x)\n'"
tensorflow_addons/activations/softshrink.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import Number\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef softshrink(\n    x: types.TensorLike, lower: Number = -0.5, upper: Number = 0.5\n) -> tf.Tensor:\n    """"""Soft shrink function.\n\n    Computes soft shrink function:\n    `x - lower if x < lower, x - upper if x > upper else 0`.\n\n    Args:\n        x: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n        lower: `float`, lower bound for setting values to zeros.\n        upper: `float`, upper bound for setting values to zeros.\n    Returns:\n        A `Tensor`. Has the same type as `x`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _softshrink_custom_op(x, lower, upper)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""softshrink"")\n\n    return _softshrink_py(x, lower, upper)\n\n\ndef _softshrink_custom_op(x, lower, upper):\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be ""\n        ""removed in TensorFlow Addons ""\n        ""v0.12.0. \\nPlease use the pure python version of softshrink instead ""\n        ""by using the ""\n        ""`TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_softshrink(x, lower, upper)\n\n\n@tf.RegisterGradient(""Addons>Softshrink"")\ndef _softshrink_grad(op, grad):\n    return _activation_so.ops.addons_softshrink_grad(\n        grad, op.inputs[0], op.get_attr(""lower""), op.get_attr(""upper"")\n    )\n\n\ndef _softshrink_py(x, lower, upper):\n    if lower > upper:\n        raise ValueError(\n            ""The value of lower is {} and should""\n            "" not be higher than the value ""\n            ""variable upper, which is {} ."".format(lower, upper)\n        )\n    values_below_lower = tf.where(x < lower, x - lower, 0)\n    values_above_upper = tf.where(upper < x, x - upper, 0)\n    return values_below_lower + values_above_upper\n'"
tensorflow_addons/activations/sparsemax.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef sparsemax(logits: types.TensorLike, axis: int = -1) -> tf.Tensor:\n    """"""Sparsemax activation function [1].\n\n    For each batch `i` and class `j` we have\n      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n\n    [1]: https://arxiv.org/abs/1602.02068\n\n    Args:\n        logits: Input tensor.\n        axis: Integer, axis along which the sparsemax operation is applied.\n    Returns:\n        Tensor, output of sparsemax transformation. Has the same type and\n        shape as `logits`.\n    Raises:\n        ValueError: In case `dim(logits) == 1`.\n    """"""\n    logits = tf.convert_to_tensor(logits, name=""logits"")\n\n    # We need its original shape for shape inference.\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n\n    # If dim is not the last dimension, we have to do a transpose so that we can\n    # still perform softmax on its last dimension.\n\n    # Swap logits\' dimension of dim and its last dimension.\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Do the actual softmax on its last dimension.\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n\n    # Make shape inference work since transpose may erase its static shape.\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n\ndef _compute_2d_sparsemax(logits):\n    """"""Performs the sparsemax operation when axis=-1.""""""\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n\n    # In the paper, they call the logits z.\n    # The mean(logits) can be substracted from logits to make the algorithm\n    # more numerically stable. the instability in this algorithm comes mostly\n    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n    # to zero. However, in practise the numerical instability issues are very\n    # minor and substacting the mean causes extra issues with inf and nan\n    # input.\n    # Reshape to [obs, dims] as it is almost free and means the remanining\n    # code doesn\'t need to worry about the rank.\n    z = tf.reshape(logits, [obs, dims])\n\n    # sort z\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n\n    # calculate k(z)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n    # (index + 1) of the last `1` is the same as just summing the number of 1.\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n\n    # calculate tau(z)\n    # If there are inf values or all values are -inf, the k_z will be zero,\n    # this is mathematically invalid and will also cause the gather_nd to fail.\n    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n    # fixed later (see p_safe) by returning p = nan. This results in the same\n    # behavior as softmax.\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n\n    # calculate p\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    # If k_z = 0 or if z = nan, then the input is invalid\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(""nan""), logits.dtype)),\n        p,\n    )\n\n    # Reshape back to original size\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe\n'"
tensorflow_addons/activations/tanhshrink.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons import options\n\n_activation_so = LazySO(""custom_ops/activations/_activation_ops.so"")\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef tanhshrink(x: types.TensorLike) -> tf.Tensor:\n    """"""Applies the element-wise function: x - tanh(x)\n\n    Args:\n        features: A `Tensor`. Must be one of the following types:\n            `float16`, `float32`, `float64`.\n    Returns:\n        A `Tensor`. Has the same type as `features`.\n    """"""\n    x = tf.convert_to_tensor(x)\n\n    if not options.TF_ADDONS_PY_OPS:\n        try:\n            return _tanhshrink_custom_op(x)\n        except tf.errors.NotFoundError:\n            options.warn_fallback(""tanhshrink"")\n\n    return _tanhshrink_py(x)\n\n\ndef _tanhshrink_custom_op(x):\n    warnings.warn(\n        ""The activations custom ops are deprecated and will be removed in ""\n        ""TensorFlow Addons v0.12.0. \\nPlease use the pure python version of ""\n        ""tanhshrink instead by using the ""\n        ""`TF_ADDONS_PY_OPS` flag. \\nFor more info about this flag, see ""\n        ""https://github.com/tensorflow/addons#gpucpu-custom-ops "",\n        DeprecationWarning,\n    )\n    return _activation_so.ops.addons_tanhshrink(x)\n\n\n@tf.RegisterGradient(""Addons>Tanhshrink"")\ndef _tanhshrink_grad(op, grad):\n    return _activation_so.ops.addons_tanhshrink_grad(grad, op.inputs[0])\n\n\ndef _tanhshrink_py(x):\n    return x - tf.math.tanh(x)\n'"
tensorflow_addons/callbacks/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional callbacks that conform to Keras API.""""""\n\nfrom tensorflow_addons.callbacks.average_model_checkpoint import AverageModelCheckpoint\nfrom tensorflow_addons.callbacks.time_stopping import TimeStopping\nfrom tensorflow_addons.callbacks.tqdm_progress_bar import TQDMProgressBar\n'"
tensorflow_addons/callbacks/average_model_checkpoint.py,2,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport tensorflow as tf\nfrom typeguard import typechecked\nfrom tensorflow_addons.optimizers.average_wrapper import AveragedOptimizerWrapper\n\n\nclass AverageModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n    r""""""The callback that should be used with optimizers that extend\n    AverageWrapper, i.e., MovingAverage and StochasticAverage optimizers.\n    It saves and, optionally, assigns the averaged weights.\n\n    Args:\n        update_weights: If True, assign the moving average weights\n            to the model, and save them. If False, keep the old\n            non-averaged weights, but the saved model uses the\n            average weights.\n\n        See `tf.keras.callbacks.ModelCheckpoint` for the other args.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        update_weights: bool,\n        filepath: str,\n        monitor: str = ""val_loss"",\n        verbose: int = 0,\n        save_best_only: bool = False,\n        save_weights_only: bool = False,\n        mode: str = ""auto"",\n        save_freq: str = ""epoch"",\n        **kwargs\n    ):\n        self.update_weights = update_weights\n        super().__init__(\n            filepath,\n            monitor,\n            verbose,\n            save_best_only,\n            save_weights_only,\n            mode,\n            save_freq,\n            **kwargs,\n        )\n\n    def set_model(self, model):\n        if not isinstance(model.optimizer, AveragedOptimizerWrapper):\n            raise TypeError(\n                ""AverageModelCheckpoint is only used when training""\n                ""with MovingAverage or StochasticAverage""\n            )\n        return super().set_model(model)\n\n    def _save_model(self, epoch, logs):\n        assert isinstance(self.model.optimizer, AveragedOptimizerWrapper)\n\n        if self.update_weights:\n            self.model.optimizer.assign_average_vars(self.model.variables)\n            return super()._save_model(epoch, logs)\n        else:\n            # Note: `model.get_weights()` gives us the weights (non-ref)\n            # whereas `model.variables` returns references to the variables.\n            non_avg_weights = self.model.get_weights()\n            self.model.optimizer.assign_average_vars(self.model.variables)\n            # result is currently None, since `super._save_model` doesn\'t\n            # return anything, but this may change in the future.\n            result = super()._save_model(epoch, logs)\n            self.model.set_weights(non_avg_weights)\n            return result\n'"
tensorflow_addons/callbacks/time_stopping.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Callback that stops training when a specified amount of time has passed.""""""\n\nimport datetime\nimport time\nfrom typeguard import typechecked\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TimeStopping(Callback):\n    """"""Stop training when a specified amount of time has passed.\n\n    Args:\n        seconds: maximum amount of time before stopping.\n            Defaults to 86400 (1 day).\n        verbose: verbosity mode. Defaults to 0.\n    """"""\n\n    @typechecked\n    def __init__(self, seconds: int = 86400, verbose: int = 0):\n        super().__init__()\n\n        self.seconds = seconds\n        self.verbose = verbose\n        self.stopped_epoch = None\n\n    def on_train_begin(self, logs=None):\n        self.stopping_time = time.time() + self.seconds\n\n    def on_epoch_end(self, epoch, logs={}):\n        if time.time() >= self.stopping_time:\n            self.model.stop_training = True\n            self.stopped_epoch = epoch\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch is not None and self.verbose > 0:\n            formatted_time = datetime.timedelta(seconds=self.seconds)\n            msg = ""Timed stopping at epoch {} after training for {}"".format(\n                self.stopped_epoch + 1, formatted_time\n            )\n            print(msg)\n\n    def get_config(self):\n        config = {\n            ""seconds"": self.seconds,\n            ""verbose"": self.verbose,\n        }\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/callbacks/tqdm_progress_bar.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TQDM Progress Bar.""""""\n\nimport time\nimport tensorflow as tf\nfrom collections import defaultdict\nfrom typeguard import typechecked\n\nfrom tensorflow.keras.callbacks import Callback\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TQDMProgressBar(Callback):\n    """"""TQDM Progress Bar for Tensorflow Keras.\n\n    Args:\n        metrics_separator: Custom separator between metrics.\n            Defaults to \' - \'.\n        overall_bar_format: Custom bar format for overall\n            (outer) progress bar, see https://github.com/tqdm/tqdm#parameters\n            for more detail.\n        epoch_bar_format: Custom bar format for epoch\n            (inner) progress bar, see https://github.com/tqdm/tqdm#parameters\n            for more detail.\n        update_per_second: Maximum number of updates in the epochs bar\n            per second, this is to prevent small batches from slowing down\n            training. Defaults to 10.\n        metrics_format: Custom format for how metrics are formatted.\n            See https://github.com/tqdm/tqdm#parameters for more detail.\n        leave_epoch_progress: True to leave epoch progress bars.\n        leave_overall_progress: True to leave overall progress bar.\n        show_epoch_progress: False to hide epoch progress bars.\n        show_overall_progress: False to hide overall progress bar.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        metrics_separator: str = "" - "",\n        overall_bar_format: str = ""{l_bar}{bar} {n_fmt}/{total_fmt} ETA: ""\n        ""{remaining}s,  {rate_fmt}{postfix}"",\n        epoch_bar_format: str = ""{n_fmt}/{total_fmt}{bar} ETA: ""\n        ""{remaining}s - {desc}"",\n        metrics_format: str = ""{name}: {value:0.4f}"",\n        update_per_second: int = 10,\n        leave_epoch_progress: bool = True,\n        leave_overall_progress: bool = True,\n        show_epoch_progress: bool = True,\n        show_overall_progress: bool = True,\n    ):\n\n        try:\n            # import tqdm here because tqdm is not a required package\n            # for addons\n            import tqdm\n\n            version_message = ""Please update your TQDM version to >= 4.36.1, ""\n            ""you have version {}. To update, run !pip install -U tqdm""\n            assert tqdm.__version__ >= ""4.36.1"", version_message.format(\n                tqdm.__version__\n            )\n            from tqdm.auto import tqdm\n\n            self.tqdm = tqdm\n        except ImportError:\n            raise ImportError(""Please install tqdm via pip install tqdm"")\n\n        self.metrics_separator = metrics_separator\n        self.overall_bar_format = overall_bar_format\n        self.epoch_bar_format = epoch_bar_format\n        self.leave_epoch_progress = leave_epoch_progress\n        self.leave_overall_progress = leave_overall_progress\n        self.show_epoch_progress = show_epoch_progress\n        self.show_overall_progress = show_overall_progress\n        self.metrics_format = metrics_format\n\n        # compute update interval (inverse of update per second)\n        self.update_interval = 1 / update_per_second\n\n        self.last_update_time = time.time()\n        self.overall_progress_tqdm = None\n        self.epoch_progress_tqdm = None\n        self.is_training = False\n        self.num_epochs = None\n        self.logs = None\n        super().__init__()\n\n    def _initialize_progbar(self, hook, epoch, logs=None):\n        self.num_samples_seen = 0\n        self.steps_to_update = 0\n        self.steps_so_far = 0\n        self.logs = defaultdict(float)\n        self.num_epochs = self.params[""epochs""]\n        self.mode = ""steps""\n        self.total_steps = self.params[""steps""]\n        if hook == ""train_overall"":\n            if self.show_overall_progress:\n                self.overall_progress_tqdm = self.tqdm(\n                    desc=""Training"",\n                    total=self.num_epochs,\n                    bar_format=self.overall_bar_format,\n                    leave=self.leave_overall_progress,\n                    dynamic_ncols=True,\n                    unit=""epochs"",\n                )\n        elif hook == ""test"":\n            if self.show_epoch_progress:\n                self.epoch_progress_tqdm = self.tqdm(\n                    total=self.total_steps,\n                    desc=""Evaluating"",\n                    bar_format=self.epoch_bar_format,\n                    leave=self.leave_epoch_progress,\n                    dynamic_ncols=True,\n                    unit=self.mode,\n                )\n        elif hook == ""train_epoch"":\n            current_epoch_description = ""Epoch {epoch}/{num_epochs}"".format(\n                epoch=epoch + 1, num_epochs=self.num_epochs\n            )\n            if self.show_epoch_progress:\n                print(current_epoch_description)\n                self.epoch_progress_tqdm = self.tqdm(\n                    total=self.total_steps,\n                    bar_format=self.epoch_bar_format,\n                    leave=self.leave_epoch_progress,\n                    dynamic_ncols=True,\n                    unit=self.mode,\n                )\n\n    def _clean_up_progbar(self, hook, logs):\n        if hook == ""train_overall"":\n            if self.show_overall_progress:\n                self.overall_progress_tqdm.close()\n        else:\n            if hook == ""test"":\n                metrics = self.format_metrics(logs, self.num_samples_seen)\n            else:\n                metrics = self.format_metrics(logs)\n            if self.show_epoch_progress:\n                self.epoch_progress_tqdm.desc = metrics\n                # set miniters and mininterval to 0 so last update displays\n                self.epoch_progress_tqdm.miniters = 0\n                self.epoch_progress_tqdm.mininterval = 0\n                # update the rest of the steps in epoch progress bar\n                self.epoch_progress_tqdm.update(\n                    self.total_steps - self.epoch_progress_tqdm.n\n                )\n                self.epoch_progress_tqdm.close()\n\n    def _update_progbar(self, logs):\n        if self.mode == ""samples"":\n            batch_size = logs[""size""]\n        else:\n            batch_size = 1\n\n        self.num_samples_seen += batch_size\n        self.steps_to_update += 1\n        self.steps_so_far += 1\n\n        if self.steps_so_far <= self.total_steps:\n            for metric, value in logs.items():\n                self.logs[metric] += value * batch_size\n\n            now = time.time()\n            time_diff = now - self.last_update_time\n            if self.show_epoch_progress and time_diff >= self.update_interval:\n\n                # update the epoch progress bar\n                metrics = self.format_metrics(self.logs, self.num_samples_seen)\n                self.epoch_progress_tqdm.desc = metrics\n                self.epoch_progress_tqdm.update(self.steps_to_update)\n\n                # reset steps to update\n                self.steps_to_update = 0\n\n                # update timestamp for last update\n                self.last_update_time = now\n\n    def on_train_begin(self, logs=None):\n        self.is_training = True\n        self._initialize_progbar(""train_overall"", None, logs)\n\n    def on_train_end(self, logs={}):\n        self.is_training = False\n        self._clean_up_progbar(""train_overall"", logs)\n\n    def on_test_begin(self, logs={}):\n        if not self.is_training:\n            self._initialize_progbar(""test"", None, logs)\n\n    def on_test_end(self, logs={}):\n        if not self.is_training:\n            self._clean_up_progbar(""test"", self.logs)\n\n    def on_epoch_begin(self, epoch, logs={}):\n        self._initialize_progbar(""train_epoch"", epoch, logs)\n\n    def on_epoch_end(self, epoch, logs={}):\n        self._clean_up_progbar(""train_epoch"", logs)\n        if self.show_overall_progress:\n            self.overall_progress_tqdm.update(1)\n\n    def on_test_batch_end(self, batch, logs={}):\n        if not self.is_training:\n            self._update_progbar(logs)\n\n    def on_batch_end(self, batch, logs={}):\n        self._update_progbar(logs)\n\n    def format_metrics(self, logs={}, factor=1):\n        """"""Format metrics in logs into a string.\n\n        Arguments:\n            logs: dictionary of metrics and their values. Defaults to\n                empty dictionary.\n            factor (int): The factor we want to divide the metrics in logs\n                by, useful when we are computing the logs after each batch.\n                Defaults to 1.\n\n        Returns:\n            metrics_string: a string displaying metrics using the given\n            formators passed in through the constructor.\n        """"""\n\n        metric_value_pairs = []\n        for key, value in logs.items():\n            if key in [""batch"", ""size""]:\n                continue\n            pair = self.metrics_format.format(name=key, value=value / factor)\n            metric_value_pairs.append(pair)\n        metrics_string = self.metrics_separator.join(metric_value_pairs)\n        return metrics_string\n\n    def get_config(self):\n        config = {\n            ""metrics_separator"": self.metrics_separator,\n            ""overall_bar_format"": self.overall_bar_format,\n            ""epoch_bar_format"": self.epoch_bar_format,\n            ""leave_epoch_progress"": self.leave_epoch_progress,\n            ""leave_overall_progress"": self.leave_overall_progress,\n            ""show_epoch_progress"": self.show_epoch_progress,\n            ""show_overall_progress"": self.show_overall_progress,\n        }\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/image/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional image manipulation ops.""""""\n\nfrom tensorflow_addons.image.distort_image_ops import adjust_hsv_in_yiq\nfrom tensorflow_addons.image.compose_ops import blend\nfrom tensorflow_addons.image.color_ops import equalize\nfrom tensorflow_addons.image.color_ops import sharpness\nfrom tensorflow_addons.image.connected_components import connected_components\nfrom tensorflow_addons.image.cutout_ops import cutout\nfrom tensorflow_addons.image.dense_image_warp import dense_image_warp\nfrom tensorflow_addons.image.distance_transform import euclidean_dist_transform\nfrom tensorflow_addons.image.dense_image_warp import interpolate_bilinear\nfrom tensorflow_addons.image.interpolate_spline import interpolate_spline\nfrom tensorflow_addons.image.filters import gaussian_filter2d\nfrom tensorflow_addons.image.filters import mean_filter2d\nfrom tensorflow_addons.image.filters import median_filter2d\nfrom tensorflow_addons.image.cutout_ops import random_cutout\nfrom tensorflow_addons.image.distort_image_ops import random_hsv_in_yiq\nfrom tensorflow_addons.image.resampler_ops import resampler\nfrom tensorflow_addons.image.transform_ops import rotate\nfrom tensorflow_addons.image.transform_ops import shear_x\nfrom tensorflow_addons.image.transform_ops import shear_y\nfrom tensorflow_addons.image.sparse_image_warp import sparse_image_warp\nfrom tensorflow_addons.image.transform_ops import transform\nfrom tensorflow_addons.image.translate_ops import translate\nfrom tensorflow_addons.image.translate_ops import translate_xy\n'"
tensorflow_addons/image/color_ops.py,34,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Color operations.\n    equalize: Equalizes image histogram\n    sharpness: Sharpen image\n""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.types import TensorLike, Number\nfrom tensorflow_addons.image.utils import to_4D_image, from_4D_image\nfrom tensorflow_addons.image.compose_ops import blend\n\nfrom typing import Optional\nfrom functools import partial\n\n\ndef equalize_image(image: TensorLike, data_format: str = ""channels_last"") -> tf.Tensor:\n    """"""Implements Equalize function from PIL using TF ops.""""""\n\n    @tf.function\n    def scale_channel(image, channel):\n        """"""Scale the data in the channel to implement equalize.""""""\n        image_dtype = image.dtype\n\n        if data_format == ""channels_last"":\n            image = tf.cast(image[:, :, channel], tf.int32)\n        elif data_format == ""channels_first"":\n            image = tf.cast(image[channel], tf.int32)\n        else:\n            raise ValueError(\n                ""data_format can either be channels_last or channels_first""\n            )\n        # Compute the histogram of the image channel.\n        histo = tf.histogram_fixed_width(image, [0, 255], nbins=256)\n\n        # For the purposes of computing the step, filter out the nonzeros.\n        nonzero = tf.where(tf.not_equal(histo, 0))\n        nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n        step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n        def build_lut(histo, step):\n            # Compute the cumulative sum, shifting by step // 2\n            # and then normalization by step.\n            lut = (tf.cumsum(histo) + (step // 2)) // step\n            # Shift lut, prepending with 0.\n            lut = tf.concat([[0], lut[:-1]], 0)\n            # Clip the counts to be in range.  This is done\n            # in the C code for image.point.\n            return tf.clip_by_value(lut, 0, 255)\n\n        # If step is zero, return the original image.  Otherwise, build\n        # lut from the full histogram and step and then index from it.\n\n        if step == 0:\n            result = image\n        else:\n            result = tf.gather(build_lut(histo, step), image)\n\n        return tf.cast(result, image_dtype)\n\n    idx = 2 if data_format == ""channels_last"" else 0\n    image = tf.stack([scale_channel(image, c) for c in range(image.shape[idx])], idx)\n\n    return image\n\n\ndef equalize(\n    image: TensorLike, data_format: str = ""channels_last"", name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Equalize image(s)\n\n    Args:\n      images: A tensor of shape\n          (num_images, num_rows, num_columns, num_channels) (NHWC), or\n          (num_images, num_channels, num_rows, num_columns) (NCHW), or\n          (num_rows, num_columns, num_channels) (HWC), or\n          (num_channels, num_rows, num_columns) (CHW), or\n          (num_rows, num_columns) (HW). The rank must be statically known (the\n          shape is not `TensorShape(None)`).\n      data_format: Either \'channels_first\' or \'channels_last\'\n      name: The name of the op.\n    Returns:\n      Image(s) with the same type and shape as `images`, equalized.\n    """"""\n    with tf.name_scope(name or ""equalize""):\n        image_dims = tf.rank(image)\n        image = to_4D_image(image)\n        fn = partial(equalize_image, data_format=data_format)\n        image = tf.map_fn(fn, image)\n        return from_4D_image(image, image_dims)\n\n\ndef sharpness_image(image: TensorLike, factor: Number) -> tf.Tensor:\n    """"""Implements Sharpness function from PIL using TF ops.""""""\n    orig_image = image\n    image_dtype = image.dtype\n    # SMOOTH PIL Kernel.\n    image = tf.cast(image, tf.float32)\n    kernel = (\n        tf.constant(\n            [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32, shape=[3, 3, 1, 1]\n        )\n        / 13.0\n    )\n    # Tile across channel dimension.\n    kernel = tf.tile(kernel, [1, 1, 3, 1])\n    strides = [1, 1, 1, 1]\n    degenerate = tf.nn.depthwise_conv2d(\n        image, kernel, strides, padding=""VALID"", dilations=[1, 1]\n    )\n    degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n    degenerate = tf.cast(degenerate, image_dtype)\n\n    # For the borders of the resulting image, fill in the values of the\n    # original image.\n    mask = tf.ones_like(degenerate)\n    padded_mask = tf.pad(mask, [[0, 0], [1, 1], [1, 1], [0, 0]])\n    padded_degenerate = tf.pad(degenerate, [[0, 0], [1, 1], [1, 1], [0, 0]])\n    result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n    # Blend the final result.\n    blended = blend(result, orig_image, factor)\n    return tf.cast(blended, image_dtype)\n\n\ndef sharpness(image: TensorLike, factor: Number) -> tf.Tensor:\n    """"""Change sharpness of image(s).\n\n    Args:\n      images: A tensor of shape\n          (num_images, num_rows, num_columns, num_channels) (NHWC), or\n          (num_rows, num_columns, num_channels) (HWC)\n      factor: A floating point value or Tensor above 0.0.\n    Returns:\n      Image(s) with the same type and shape as `images`, sharper.\n    """"""\n    image = tf.convert_to_tensor(image)\n    image_dims = tf.rank(image)\n    image = to_4D_image(image)\n    image = sharpness_image(image, factor=factor)\n    return from_4D_image(image, image_dims)\n'"
tensorflow_addons/image/compose_ops.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Compose Ops""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.types import TensorLike, Number\n\n\ndef blend(image1: TensorLike, image2: TensorLike, factor: Number) -> tf.Tensor:\n    """"""Blend image1 and image2 using \'factor\'.\n\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 ""extrapolates"" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n\n  Args:\n    image1: An image Tensor of shape (num_rows, num_columns,\n        num_channels) (HWC), or (num_rows, num_columns) (HW),\n        or (num_channels, num_rows, num_columns).\n    image2: An image Tensor of shape (num_rows, num_columns,\n        num_channels) (HWC), or (num_rows, num_columns) (HW),\n        or (num_channels, num_rows, num_columns).\n    factor: A floating point value or Tensor of type tf.float32 above 0.0.\n\n  Returns:\n    A blended image Tensor of tf.float32.\n\n  """"""\n    with tf.name_scope(""blend""):\n\n        if factor == 0.0:\n            return tf.convert_to_tensor(image1)\n        if factor == 1.0:\n            return tf.convert_to_tensor(image2)\n\n        image1 = tf.cast(image1, dtype=tf.dtypes.float32)\n        image2 = tf.cast(image2, dtype=tf.dtypes.float32)\n\n        difference = image2 - image1\n        scaled = factor * difference\n\n        # Do addition in float.\n        temp = image1 + scaled\n\n        # Interpolate\n        if factor > 0.0 and factor < 1.0:\n            # Interpolation means we always stay within 0 and 255.\n            temp = tf.round(temp)\n            return temp\n\n        # Extrapolate:\n        #\n        # We need to clip and then cast.\n        temp = tf.round(tf.clip_by_value(temp, 0.0, 255.0))\n        return temp\n'"
tensorflow_addons/image/connected_components.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Connected Components.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\n\nfrom typing import Optional, Text\n\n_image_so = LazySO(""custom_ops/image/_image_ops.so"")\n\n\n@tf.function\ndef connected_components(\n    images: types.TensorLike, name: Optional[Text] = None\n) -> tf.Tensor:\n    """"""Labels the connected components in a batch of images.\n\n    A component is a set of pixels in a single input image, which are\n    all adjacent and all have the same non-zero value. The components\n    using a squared connectivity of one (all equal entries are joined with\n    their neighbors above,below, left, and right). Components across all\n    images have consecutive ids 1 through n.\n    Components are labeled according to the first pixel of the\n    component appearing in row-major order (lexicographic order by\n    image_index_in_batch, row, col).\n    Zero entries all have an output id of 0.\n    This op is equivalent with `scipy.ndimage.measurements.label`\n    on a 2D array with the default structuring element\n    (which is the connectivity used here).\n\n    Args:\n      images: A 2D (H, W) or 3D (N, H, W) Tensor of image (integer,\n      floating point and boolean types are supported).\n      name: The name of the op.\n\n    Returns:\n      Components with the same shape as `images`.\n      entries that evaluate to False (e.g. 0/0.0f, False) in `images` have\n      value 0, and all other entries map to a component id > 0.\n\n    Raises:\n      TypeError: if `images` is not 2D or 3D.\n    """"""\n    with tf.name_scope(name or ""connected_components""):\n        image_or_images = tf.convert_to_tensor(images, name=""images"")\n        if len(image_or_images.get_shape()) == 2:\n            images = image_or_images[None, :, :]\n        elif len(image_or_images.get_shape()) == 3:\n            images = image_or_images\n        else:\n            raise TypeError(\n                ""images should have rank 2 (HW) or 3 (NHW). Static shape is %s""\n                % image_or_images.get_shape()\n            )\n        components = _image_so.ops.addons_image_connected_components(images)\n\n        # TODO(ringwalt): Component id renaming should be done in the op,\n        # to avoid constructing multiple additional large tensors.\n        components_flat = tf.reshape(components, [-1])\n        unique_ids, id_index = tf.unique(components_flat)\n        id_is_zero = tf.where(tf.equal(unique_ids, 0))[:, 0]\n        # Map each nonzero id to consecutive values.\n        nonzero_consecutive_ids = (\n            tf.range(tf.shape(unique_ids)[0] - tf.shape(id_is_zero)[0]) + 1\n        )\n\n        def no_zero():\n            # No need to insert a zero into the ids.\n            return nonzero_consecutive_ids\n\n        def has_zero():\n            # Insert a zero in the consecutive ids\n            # where zero appears in unique_ids.\n            # id_is_zero has length 1.\n            zero_id_ind = tf.cast(id_is_zero[0], tf.int32)\n            ids_before = nonzero_consecutive_ids[:zero_id_ind]\n            ids_after = nonzero_consecutive_ids[zero_id_ind:]\n            return tf.concat([ids_before, [0], ids_after], axis=0)\n\n        new_ids = tf.cond(tf.equal(tf.shape(id_is_zero)[0], 0), no_zero, has_zero)\n        components = tf.reshape(tf.gather(new_ids, id_index), tf.shape(components))\n        if len(image_or_images.get_shape()) == 2:\n            return components[0, :, :]\n        else:\n            return components\n'"
tensorflow_addons/image/cutout_ops.py,36,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Cutout op""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils import keras_utils\nfrom tensorflow_addons.utils.types import TensorLike, Number\n\n\ndef _get_image_wh(images, data_format):\n    if data_format == ""channels_last"":\n        image_height, image_width = tf.shape(images)[1], tf.shape(images)[2]\n    else:\n        image_height, image_width = tf.shape(images)[2], tf.shape(images)[3]\n\n    return image_height, image_width\n\n\ndef _norm_params(images, mask_size, data_format):\n    mask_size = tf.convert_to_tensor(mask_size)\n    if tf.executing_eagerly():\n        tf.assert_equal(\n            tf.reduce_any(mask_size % 2 != 0),\n            False,\n            ""mask_size should be divisible by 2"",\n        )\n    if tf.rank(mask_size) == 0:\n        mask_size = tf.stack([mask_size, mask_size])\n    data_format = keras_utils.normalize_data_format(data_format)\n    image_height, image_width = _get_image_wh(images, data_format)\n    return mask_size, data_format, image_height, image_width\n\n\ndef random_cutout(\n    images: TensorLike,\n    mask_size: TensorLike,\n    constant_values: Number = 0,\n    seed: Number = None,\n    data_format: str = ""channels_last"",\n) -> tf.Tensor:\n    """"""Apply cutout (https://arxiv.org/abs/1708.04552) to images.\n\n    This operation applies a (mask_height x mask_width) mask of zeros to\n    a random location within `img`. The pixel values filled in will be of the\n    value `replace`. The located where the mask will be applied is randomly\n    chosen uniformly over the whole images.\n\n    Args:\n      images: A tensor of shape\n        (batch_size, height, width, channels)\n        (NHWC), (batch_size, channels, height, width)(NCHW).\n      mask_size: Specifies how big the zero mask that will be generated is that\n        is applied to the images. The mask will be of size\n        (mask_height x mask_width). Note: mask_size should be divisible by 2.\n      constant_values: What pixel value to fill in the images in the area that has\n        the cutout mask applied to it.\n      seed: A Python integer. Used in combination with `tf.random.set_seed` to\n        create a reproducible sequence of tensors across multiple calls.\n      data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch_size, ..., channels)` while `channels_first` corresponds to\n        inputs with shape `(batch_size, channels, ...)`.\n    Returns:\n      An image Tensor.\n    Raises:\n      InvalidArgumentError: if mask_size can\'t be divisible by 2.\n    """"""\n    batch_size = tf.shape(images)[0]\n    mask_size, data_format, image_height, image_width = _norm_params(\n        images, mask_size, data_format\n    )\n\n    cutout_center_height = tf.random.uniform(\n        shape=[batch_size], minval=0, maxval=image_height, dtype=tf.int32, seed=seed\n    )\n    cutout_center_width = tf.random.uniform(\n        shape=[batch_size], minval=0, maxval=image_width, dtype=tf.int32, seed=seed\n    )\n\n    offset = tf.transpose([cutout_center_height, cutout_center_width], [1, 0])\n    return cutout(images, mask_size, offset, constant_values, data_format,)\n\n\ndef cutout(\n    images: TensorLike,\n    mask_size: TensorLike,\n    offset: TensorLike = (0, 0),\n    constant_values: Number = 0,\n    data_format: str = ""channels_last"",\n) -> tf.Tensor:\n    """"""Apply cutout (https://arxiv.org/abs/1708.04552) to images.\n\n    This operation applies a (mask_height x mask_width) mask of zeros to\n    a location within `img` specified by the offset. The pixel values filled in will be of the\n    value `replace`. The located where the mask will be applied is randomly\n    chosen uniformly over the whole images.\n\n    Args:\n      images: A tensor of shape (batch_size, height, width, channels)\n        (NHWC), (batch_size, channels, height, width)(NCHW).\n      mask_size: Specifies how big the zero mask that will be generated is that\n        is applied to the images. The mask will be of size\n        (mask_height x mask_width). Note: mask_size should be divisible by 2.\n      offset: A tuple of (height, width) or (batch_size, 2)\n      constant_values: What pixel value to fill in the images in the area that has\n        the cutout mask applied to it.\n      data_format: A string, one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch_size, ..., channels)` while `channels_first` corresponds to\n        inputs with shape `(batch_size, channels, ...)`.\n    Returns:\n      An image Tensor.\n    Raises:\n      InvalidArgumentError: if mask_size can\'t be divisible by 2.\n    """"""\n    with tf.name_scope(""cutout""):\n        origin_shape = images.shape\n        offset = tf.convert_to_tensor(offset)\n        mask_size, data_format, image_height, image_width = _norm_params(\n            images, mask_size, data_format\n        )\n        mask_size = mask_size // 2\n\n        if tf.rank(offset) == 1:\n            offset = tf.expand_dims(offset, 0)\n        cutout_center_heights = offset[:, 0]\n        cutout_center_widths = offset[:, 1]\n\n        lower_pads = tf.maximum(0, cutout_center_heights - mask_size[0])\n        upper_pads = tf.maximum(0, image_height - cutout_center_heights - mask_size[0])\n        left_pads = tf.maximum(0, cutout_center_widths - mask_size[1])\n        right_pads = tf.maximum(0, image_width - cutout_center_widths - mask_size[1])\n\n        cutout_shape = tf.transpose(\n            [\n                image_height - (lower_pads + upper_pads),\n                image_width - (left_pads + right_pads),\n            ],\n            [1, 0],\n        )\n        masks = tf.TensorArray(images.dtype, 0, dynamic_size=True)\n        for i in tf.range(tf.shape(cutout_shape)[0]):\n            padding_dims = [\n                [lower_pads[i], upper_pads[i]],\n                [left_pads[i], right_pads[i]],\n            ]\n            mask = tf.pad(\n                tf.zeros(cutout_shape[i], dtype=images.dtype),\n                padding_dims,\n                constant_values=1,\n            )\n            masks = masks.write(i, mask)\n\n        if data_format == ""channels_last"":\n            mask_4d = tf.expand_dims(masks.stack(), -1)\n            mask = tf.tile(mask_4d, [1, 1, 1, tf.shape(images)[-1]])\n        else:\n            mask_4d = tf.expand_dims(masks.stack(), 1)\n            mask = tf.tile(mask_4d, [1, tf.shape(images)[1], 1, 1])\n        images = tf.where(\n            mask == 0,\n            tf.ones_like(images, dtype=images.dtype) * constant_values,\n            images,\n        )\n        images.set_shape(origin_shape)\n        return images\n'"
tensorflow_addons/image/dense_image_warp.py,53,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image warping using per-pixel flow vectors.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom typing import Optional\n\n\n@tf.function\ndef interpolate_bilinear(\n    grid: types.TensorLike,\n    query_points: types.TensorLike,\n    indexing: str = ""ij"",\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Similar to Matlab\'s interp2 function.\n\n    Finds values for query points on a grid using bilinear interpolation.\n\n    Args:\n      grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`.\n      query_points: a 3-D float `Tensor` of N points with shape\n        `[batch, N, 2]`.\n      indexing: whether the query points are specified as row and column (ij),\n        or Cartesian coordinates (xy).\n      name: a name for the operation (optional).\n\n    Returns:\n      values: a 3-D `Tensor` with shape `[batch, N, channels]`\n\n    Raises:\n      ValueError: if the indexing mode is invalid, or if the shape of the\n        inputs invalid.\n    """"""\n    if indexing != ""ij"" and indexing != ""xy"":\n        raise ValueError(""Indexing mode must be \'ij\' or \'xy\'"")\n\n    with tf.name_scope(name or ""interpolate_bilinear""):\n        grid = tf.convert_to_tensor(grid)\n        query_points = tf.convert_to_tensor(query_points)\n\n        # grid shape checks\n        grid_static_shape = grid.shape\n        grid_shape = tf.shape(grid)\n        if grid_static_shape.dims is not None:\n            if len(grid_static_shape) != 4:\n                raise ValueError(""Grid must be 4D Tensor"")\n            if grid_static_shape[1] is not None and grid_static_shape[1] < 2:\n                raise ValueError(""Grid height must be at least 2."")\n            if grid_static_shape[2] is not None and grid_static_shape[2] < 2:\n                raise ValueError(""Grid width must be at least 2."")\n        else:\n            with tf.control_dependencies(\n                [\n                    tf.debugging.assert_greater_equal(\n                        grid_shape[1], 2, message=""Grid height must be at least 2.""\n                    ),\n                    tf.debugging.assert_greater_equal(\n                        grid_shape[2], 2, message=""Grid width must be at least 2.""\n                    ),\n                    tf.debugging.assert_less_equal(\n                        tf.cast(\n                            grid_shape[0] * grid_shape[1] * grid_shape[2],\n                            dtype=tf.dtypes.float32,\n                        ),\n                        np.iinfo(np.int32).max / 8.0,\n                        message=""The image size or batch size is sufficiently ""\n                        ""large that the linearized addresses used by ""\n                        ""tf.gather may exceed the int32 limit."",\n                    ),\n                ]\n            ):\n                pass\n\n        # query_points shape checks\n        query_static_shape = query_points.shape\n        query_shape = tf.shape(query_points)\n        if query_static_shape.dims is not None:\n            if len(query_static_shape) != 3:\n                raise ValueError(""Query points must be 3 dimensional."")\n            query_hw = query_static_shape[2]\n            if query_hw is not None and query_hw != 2:\n                raise ValueError(""Query points last dimension must be 2."")\n        else:\n            with tf.control_dependencies(\n                [\n                    tf.debugging.assert_equal(\n                        query_shape[2],\n                        2,\n                        message=""Query points last dimension must be 2."",\n                    )\n                ]\n            ):\n                pass\n\n        batch_size, height, width, channels = (\n            grid_shape[0],\n            grid_shape[1],\n            grid_shape[2],\n            grid_shape[3],\n        )\n\n        num_queries = query_shape[1]\n\n        query_type = query_points.dtype\n        grid_type = grid.dtype\n\n        alphas = []\n        floors = []\n        ceils = []\n        index_order = [0, 1] if indexing == ""ij"" else [1, 0]\n        unstacked_query_points = tf.unstack(query_points, axis=2, num=2)\n\n        for i, dim in enumerate(index_order):\n            with tf.name_scope(""dim-"" + str(dim)):\n                queries = unstacked_query_points[dim]\n\n                size_in_indexing_dimension = grid_shape[i + 1]\n\n                # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1\n                # is still a valid index into the grid.\n                max_floor = tf.cast(size_in_indexing_dimension - 2, query_type)\n                min_floor = tf.constant(0.0, dtype=query_type)\n                floor = tf.math.minimum(\n                    tf.math.maximum(min_floor, tf.math.floor(queries)), max_floor\n                )\n                int_floor = tf.cast(floor, tf.dtypes.int32)\n                floors.append(int_floor)\n                ceil = int_floor + 1\n                ceils.append(ceil)\n\n                # alpha has the same type as the grid, as we will directly use alpha\n                # when taking linear combinations of pixel values from the image.\n                alpha = tf.cast(queries - floor, grid_type)\n                min_alpha = tf.constant(0.0, dtype=grid_type)\n                max_alpha = tf.constant(1.0, dtype=grid_type)\n                alpha = tf.math.minimum(tf.math.maximum(min_alpha, alpha), max_alpha)\n\n                # Expand alpha to [b, n, 1] so we can use broadcasting\n                # (since the alpha values don\'t depend on the channel).\n                alpha = tf.expand_dims(alpha, 2)\n                alphas.append(alpha)\n\n            flattened_grid = tf.reshape(grid, [batch_size * height * width, channels])\n            batch_offsets = tf.reshape(\n                tf.range(batch_size) * height * width, [batch_size, 1]\n            )\n\n        # This wraps tf.gather. We reshape the image data such that the\n        # batch, y, and x coordinates are pulled into the first dimension.\n        # Then we gather. Finally, we reshape the output back. It\'s possible this\n        # code would be made simpler by using tf.gather_nd.\n        def gather(y_coords, x_coords, name):\n            with tf.name_scope(""gather-"" + name):\n                linear_coordinates = batch_offsets + y_coords * width + x_coords\n                gathered_values = tf.gather(flattened_grid, linear_coordinates)\n                return tf.reshape(gathered_values, [batch_size, num_queries, channels])\n\n        # grab the pixel values in the 4 corners around each query point\n        top_left = gather(floors[0], floors[1], ""top_left"")\n        top_right = gather(floors[0], ceils[1], ""top_right"")\n        bottom_left = gather(ceils[0], floors[1], ""bottom_left"")\n        bottom_right = gather(ceils[0], ceils[1], ""bottom_right"")\n\n        # now, do the actual interpolation\n        with tf.name_scope(""interpolate""):\n            interp_top = alphas[1] * (top_right - top_left) + top_left\n            interp_bottom = alphas[1] * (bottom_right - bottom_left) + bottom_left\n            interp = alphas[0] * (interp_bottom - interp_top) + interp_top\n\n        return interp\n\n\n@tf.function\ndef dense_image_warp(\n    image: types.TensorLike, flow: types.TensorLike, name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Image warping using per-pixel flow vectors.\n\n    Apply a non-linear warp to the image, where the warp is specified by a\n    dense flow field of offset vectors that define the correspondences of\n    pixel values in the output image back to locations in the source image.\n    Specifically, the pixel value at `output[b, j, i, c]` is\n    `images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c]`.\n\n    The locations specified by this formula do not necessarily map to an int\n    index. Therefore, the pixel value is obtained by bilinear\n    interpolation of the 4 nearest pixels around\n    `(b, j - flow[b, j, i, 0], i - flow[b, j, i, 1])`. For locations outside\n    of the image, we use the nearest pixel values at the image boundary.\n\n    PLEASE NOTE: The definition of the flow field above is different from that\n    of optical flow. This function expects the negative forward flow from\n    output image to source image. Given two images `I_1` and `I_2` and the\n    optical flow `F_12` from `I_1` to `I_2`, the image `I_1` can be\n    reconstructed by `I_1_rec = dense_image_warp(I_2, -F_12)`.\n\n    Args:\n      image: 4-D float `Tensor` with shape `[batch, height, width, channels]`.\n      flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`.\n      name: A name for the operation (optional).\n\n      Note that image and flow can be of type tf.half, tf.float32, or\n      tf.float64, and do not necessarily have to be the same type.\n\n    Returns:\n      A 4-D float `Tensor` with shape`[batch, height, width, channels]`\n        and same type as input image.\n\n    Raises:\n      ValueError: if height < 2 or width < 2 or the inputs have the wrong\n        number of dimensions.\n    """"""\n    with tf.name_scope(name or ""dense_image_warp""):\n        image = tf.convert_to_tensor(image)\n        flow = tf.convert_to_tensor(flow)\n        batch_size, height, width, channels = (\n            tf.shape(image)[0],\n            tf.shape(image)[1],\n            tf.shape(image)[2],\n            tf.shape(image)[3],\n        )\n\n        # The flow is defined on the image grid. Turn the flow into a list of query\n        # points in the grid space.\n        grid_x, grid_y = tf.meshgrid(tf.range(width), tf.range(height))\n        stacked_grid = tf.cast(tf.stack([grid_y, grid_x], axis=2), flow.dtype)\n        batched_grid = tf.expand_dims(stacked_grid, axis=0)\n        query_points_on_grid = batched_grid - flow\n        query_points_flattened = tf.reshape(\n            query_points_on_grid, [batch_size, height * width, 2]\n        )\n        # Compute values at the query points, then reshape the result back to the\n        # image grid.\n        interpolated = interpolate_bilinear(image, query_points_flattened)\n        interpolated = tf.reshape(interpolated, [batch_size, height, width, channels])\n        return interpolated\n'"
tensorflow_addons/image/distance_transform.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Distance transform ops.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import utils as img_utils\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons.utils.types import TensorLike\n\nfrom typing import Optional, Type\n\n_image_so = LazySO(""custom_ops/image/_image_ops.so"")\n\ntf.no_gradient(""Addons>EuclideanDistanceTransform"")\n\n\ndef euclidean_dist_transform(\n    images: TensorLike,\n    dtype: Type[tf.dtypes.DType] = tf.float32,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Applies euclidean distance transform(s) to the image(s).\n\n    Args:\n      images: A tensor of shape (num_images, num_rows, num_columns, 1) (NHWC),\n        or (num_rows, num_columns, 1) (HWC) or (num_rows, num_columns) (HW).\n      dtype: DType of the output tensor.\n      name: The name of the op.\n\n    Returns:\n      Image(s) with the type `dtype` and same shape as `images`, with the\n      transform applied. If a tensor of all ones is given as input, the\n      output tensor will be filled with the max value of the `dtype`.\n\n    Raises:\n      TypeError: If `image` is not tf.uint8, or `dtype` is not floating point.\n      ValueError: If `image` more than one channel, or `image` is not of\n        rank between 2 and 4.\n    """"""\n\n    with tf.name_scope(name or ""euclidean_distance_transform""):\n        image_or_images = tf.convert_to_tensor(images, name=""images"")\n\n        if image_or_images.dtype.base_dtype != tf.uint8:\n            raise TypeError(""Invalid dtype %s. Expected uint8."" % image_or_images.dtype)\n\n        images = img_utils.to_4D_image(image_or_images)\n        original_ndims = img_utils.get_ndims(image_or_images)\n\n        if images.get_shape()[3] != 1 and images.get_shape()[3] is not None:\n            raise ValueError(""`images` must have only one channel"")\n\n        if dtype not in [tf.float16, tf.float32, tf.float64]:\n            raise TypeError(""`dtype` must be float16, float32 or float64"")\n\n        images = tf.cast(images, dtype)\n        output = _image_so.ops.addons_euclidean_distance_transform(images)\n\n        return img_utils.from_4D_image(output, original_ndims)\n'"
tensorflow_addons/image/distort_image_ops.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Python layer for distort_image_ops.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons.utils.types import Number, TensorLike\n\nfrom typing import Optional\n\n_distort_image_so = LazySO(""custom_ops/image/_distort_image_ops.so"")\n\n\ndef random_hsv_in_yiq(\n    image: TensorLike,\n    max_delta_hue: Number = 0,\n    lower_saturation: Number = 1,\n    upper_saturation: Number = 1,\n    lower_value: Number = 1,\n    upper_value: Number = 1,\n    seed: Optional[int] = None,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Adjust hue, saturation, value of an RGB image randomly in YIQ color space.\n\n    Equivalent to `adjust_yiq_hsv()` but uses a `delta_h` randomly\n    picked in the interval `[-max_delta_hue, max_delta_hue]`, a\n    `scale_saturation` randomly picked in the interval\n    `[lower_saturation, upper_saturation]`, and a `scale_value`\n    randomly picked in the interval `[lower_saturation, upper_saturation]`.\n\n    Args:\n      image: RGB image or images. Size of the last dimension must be 3.\n      max_delta_hue: float. Maximum value for the random delta_hue. Passing 0\n        disables adjusting hue.\n      lower_saturation: float. Lower bound for the random scale_saturation.\n      upper_saturation: float. Upper bound for the random scale_saturation.\n      lower_value: float. Lower bound for the random scale_value.\n      upper_value: float. Upper bound for the random scale_value.\n      seed: An operation-specific seed. It will be used in conjunction\n        with the graph-level seed to determine the real seeds that will be\n        used in this operation. Please see the documentation of\n        set_random_seed for its interaction with the graph-level random seed.\n      name: A name for this operation (optional).\n\n    Returns:\n      3-D float tensor of shape `[height, width, channels]`.\n\n    Raises:\n      ValueError: if `max_delta`, `lower_saturation`, `upper_saturation`,\n        `lower_value`, or `upper_value` is invalid.\n    """"""\n    if max_delta_hue < 0:\n        raise ValueError(""max_delta must be non-negative."")\n\n    if lower_saturation < 0:\n        raise ValueError(""lower_saturation must be non-negative."")\n\n    if lower_value < 0:\n        raise ValueError(""lower_value must be non-negative."")\n\n    if lower_saturation > upper_saturation:\n        raise ValueError(\n            ""lower_saturation must be not greater than "" ""upper_saturation.""\n        )\n\n    if lower_value > upper_value:\n        raise ValueError(""lower_value must be not greater than upper_value."")\n\n    with tf.name_scope(name or ""random_hsv_in_yiq"") as scope:\n        if max_delta_hue == 0:\n            delta_hue = 0\n        else:\n            delta_hue = tf.random.uniform([], -max_delta_hue, max_delta_hue, seed=seed)\n        if lower_saturation == upper_saturation:\n            scale_saturation = lower_saturation\n        else:\n            scale_saturation = tf.random.uniform(\n                [], lower_saturation, upper_saturation, seed=seed\n            )\n        if lower_value == upper_value:\n            scale_value = lower_value\n        else:\n            scale_value = tf.random.uniform([], lower_value, upper_value, seed=seed)\n        return adjust_hsv_in_yiq(\n            image, delta_hue, scale_saturation, scale_value, name=scope\n        )\n\n\ndef adjust_hsv_in_yiq(\n    image: TensorLike,\n    delta_hue: Number = 0,\n    scale_saturation: Number = 1,\n    scale_value: Number = 1,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Adjust hue, saturation, value of an RGB image in YIQ color space.\n\n    This is a convenience method that converts an RGB image to float\n    representation, converts it to YIQ, rotates the color around the\n    Y channel by delta_hue in radians, scales the chrominance channels\n    (I, Q) by scale_saturation, scales all channels (Y, I, Q) by scale_value,\n    converts back to RGB, and then back to the original data type.\n\n    `image` is an RGB image. The image hue is adjusted by converting the\n    image to YIQ, rotating around the luminance channel (Y) by\n    `delta_hue` in radians, multiplying the chrominance channels (I, Q) by\n    `scale_saturation`, and multiplying all channels (Y, I, Q) by\n    `scale_value`. The image is then converted back to RGB.\n\n    Args:\n      image: RGB image or images. Size of the last dimension must be 3.\n      delta_hue: float, the hue rotation amount, in radians.\n      scale_saturation: float, factor to multiply the saturation by.\n      scale_value: float, factor to multiply the value by.\n      name: A name for this operation (optional).\n\n    Returns:\n      Adjusted image(s), same shape and dtype as `image`.\n    """"""\n    with tf.name_scope(name or ""adjust_hsv_in_yiq""):\n        image = tf.convert_to_tensor(image, name=""image"")\n\n        # Remember original dtype to so we can convert back if needed\n        orig_dtype = image.dtype\n        flt_image = tf.image.convert_image_dtype(image, tf.dtypes.float32)\n\n        rgb_altered = _distort_image_so.ops.addons_adjust_hsv_in_yiq(\n            flt_image, delta_hue, scale_saturation, scale_value\n        )\n\n        return tf.image.convert_image_dtype(rgb_altered, orig_dtype)\n'"
tensorflow_addons/image/filters.py,47,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import utils as img_utils\nfrom tensorflow_addons.utils import keras_utils\nfrom tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n\nfrom typing import Optional, Union, List, Tuple\n\n\ndef _pad(\n    image: TensorLike,\n    filter_shape: Union[List[int], Tuple[int]],\n    mode: str = ""CONSTANT"",\n    constant_values: TensorLike = 0,\n) -> tf.Tensor:\n    """"""Explicitly pad a 4-D image.\n\n    Equivalent to the implicit padding method offered in `tf.nn.conv2d` and\n    `tf.nn.depthwise_conv2d`, but supports non-zero, reflect and symmetric\n    padding mode. For the even-sized filter, it pads one more value to the\n    right or the bottom side.\n\n    Args:\n      image: A 4-D `Tensor` of shape `[batch_size, height, width, channels]`.\n      filter_shape: A `tuple`/`list` of 2 integers, specifying the height\n        and width of the 2-D filter.\n      mode: A `string`, one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\n        The type of padding algorithm to use, which is compatible with\n        `mode` argument in `tf.pad`. For more details, please refer to\n        https://www.tensorflow.org/api_docs/python/tf/pad.\n      constant_values: A `scalar`, the pad value to use in ""CONSTANT""\n        padding mode.\n    """"""\n    assert mode in [""CONSTANT"", ""REFLECT"", ""SYMMETRIC""]\n    filter_height, filter_width = filter_shape\n    pad_top = (filter_height - 1) // 2\n    pad_bottom = filter_height - 1 - pad_top\n    pad_left = (filter_width - 1) // 2\n    pad_right = filter_width - 1 - pad_left\n    paddings = [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n    return tf.pad(image, paddings, mode=mode, constant_values=constant_values)\n\n\n@tf.function\ndef mean_filter2d(\n    image: TensorLike,\n    filter_shape: Union[List[int], Tuple[int]] = [3, 3],\n    padding: str = ""REFLECT"",\n    constant_values: TensorLike = 0,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Perform mean filtering on image(s).\n\n    Args:\n      image: Either a 2-D `Tensor` of shape `[height, width]`,\n        a 3-D `Tensor` of shape `[height, width, channels]`,\n        or a 4-D `Tensor` of shape `[batch_size, height, width, channels]`.\n      filter_shape: An `integer` or `tuple`/`list` of 2 integers, specifying\n        the height and width of the 2-D mean filter. Can be a single integer\n        to specify the same value for all spatial dimensions.\n      padding: A `string`, one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\n        The type of padding algorithm to use, which is compatible with\n        `mode` argument in `tf.pad`. For more details, please refer to\n        https://www.tensorflow.org/api_docs/python/tf/pad.\n      constant_values: A `scalar`, the pad value to use in ""CONSTANT""\n        padding mode.\n      name: A name for this operation (optional).\n    Returns:\n      2-D, 3-D or 4-D `Tensor` of the same dtype as input.\n    Raises:\n      ValueError: If `image` is not 2, 3 or 4-dimensional,\n        if `padding` is other than ""REFLECT"", ""CONSTANT"" or ""SYMMETRIC"",\n        or if `filter_shape` is invalid.\n    """"""\n    with tf.name_scope(name or ""mean_filter2d""):\n        image = tf.convert_to_tensor(image, name=""image"")\n        original_ndims = img_utils.get_ndims(image)\n        image = img_utils.to_4D_image(image)\n\n        if padding not in [""REFLECT"", ""CONSTANT"", ""SYMMETRIC""]:\n            raise ValueError(\n                \'padding should be one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\'\n            )\n\n        filter_shape = keras_utils.normalize_tuple(filter_shape, 2, ""filter_shape"")\n\n        # Keep the precision if it\'s float;\n        # otherwise, convert to float32 for computing.\n        orig_dtype = image.dtype\n        if not image.dtype.is_floating:\n            image = tf.dtypes.cast(image, tf.dtypes.float32)\n\n        # Explicitly pad the image\n        image = _pad(image, filter_shape, mode=padding, constant_values=constant_values)\n\n        # Filter of shape (filter_width, filter_height, in_channels, 1)\n        # has the value of 1 for each element.\n        area = tf.constant(filter_shape[0] * filter_shape[1], dtype=image.dtype)\n        filter_shape += (tf.shape(image)[-1], 1)\n        kernel = tf.ones(shape=filter_shape, dtype=image.dtype)\n\n        output = tf.nn.depthwise_conv2d(\n            image, kernel, strides=(1, 1, 1, 1), padding=""VALID""\n        )\n\n        output /= area\n\n        output = img_utils.from_4D_image(output, original_ndims)\n        return tf.dtypes.cast(output, orig_dtype)\n\n\n@tf.function\ndef median_filter2d(\n    image: TensorLike,\n    filter_shape: Union[List[int], Tuple[int]] = [3, 3],\n    padding: str = ""REFLECT"",\n    constant_values: FloatTensorLike = 0,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Perform median filtering on image(s).\n\n    Args:\n      image: Either a 2-D `Tensor` of shape `[height, width]`,\n        a 3-D `Tensor` of shape `[height, width, channels]`,\n        or a 4-D `Tensor` of shape `[batch_size, height, width, channels]`.\n      filter_shape: An `integer` or `tuple`/`list` of 2 integers, specifying\n        the height and width of the 2-D median filter. Can be a single integer\n        to specify the same value for all spatial dimensions.\n      padding: A `string`, one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\n        The type of padding algorithm to use, which is compatible with\n        `mode` argument in `tf.pad`. For more details, please refer to\n        https://www.tensorflow.org/api_docs/python/tf/pad.\n      constant_values: A `scalar`, the pad value to use in ""CONSTANT""\n        padding mode.\n      name: A name for this operation (optional).\n    Returns:\n      2-D, 3-D or 4-D `Tensor` of the same dtype as input.\n    Raises:\n      ValueError: If `image` is not 2, 3 or 4-dimensional,\n        if `padding` is other than ""REFLECT"", ""CONSTANT"" or ""SYMMETRIC"",\n        or if `filter_shape` is invalid.\n    """"""\n    with tf.name_scope(name or ""median_filter2d""):\n        image = tf.convert_to_tensor(image, name=""image"")\n        original_ndims = img_utils.get_ndims(image)\n        image = img_utils.to_4D_image(image)\n\n        if padding not in [""REFLECT"", ""CONSTANT"", ""SYMMETRIC""]:\n            raise ValueError(\n                \'padding should be one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\'\n            )\n\n        filter_shape = keras_utils.normalize_tuple(filter_shape, 2, ""filter_shape"")\n\n        image_shape = tf.shape(image)\n        batch_size = image_shape[0]\n        height = image_shape[1]\n        width = image_shape[2]\n        channels = image_shape[3]\n\n        # Explicitly pad the image\n        image = _pad(image, filter_shape, mode=padding, constant_values=constant_values)\n\n        area = filter_shape[0] * filter_shape[1]\n\n        floor = (area + 1) // 2\n        ceil = area // 2 + 1\n\n        patches = tf.image.extract_patches(\n            image,\n            sizes=[1, filter_shape[0], filter_shape[1], 1],\n            strides=[1, 1, 1, 1],\n            rates=[1, 1, 1, 1],\n            padding=""VALID"",\n        )\n\n        patches = tf.reshape(patches, shape=[batch_size, height, width, area, channels])\n\n        patches = tf.transpose(patches, [0, 1, 2, 4, 3])\n\n        # Note the returned median is casted back to the original type\n        # Take [5, 6, 7, 8] for example, the median is (6 + 7) / 2 = 3.5\n        # It turns out to be int(6.5) = 6 if the original type is int\n        top = tf.nn.top_k(patches, k=ceil).values\n        if area % 2 == 1:\n            median = top[:, :, :, :, floor - 1]\n        else:\n            median = (top[:, :, :, :, floor - 1] + top[:, :, :, :, ceil - 1]) / 2\n\n        output = tf.cast(median, image.dtype)\n        output = img_utils.from_4D_image(output, original_ndims)\n        return output\n\n\ndef _get_gaussian_kernel(sigma, filter_shape):\n    """"""Compute 1D Gaussian kernel.""""""\n    sigma = tf.convert_to_tensor(sigma)\n    x = tf.range(-filter_shape // 2 + 1, filter_shape // 2 + 1)\n    x = tf.cast(x ** 2, sigma.dtype)\n    x = tf.exp(-x / (2.0 * (sigma ** 2)))\n    x = x / tf.math.reduce_sum(x)\n    return x\n\n\ndef _get_gaussian_kernel_2d(gaussian_filter_x, gaussian_filter_y):\n    """"""Compute 2D Gaussian kernel given 1D kernels.""""""\n    gaussian_kernel = tf.matmul(gaussian_filter_x, gaussian_filter_y)\n    return gaussian_kernel\n\n\n@tf.function\ndef gaussian_filter2d(\n    image: FloatTensorLike,\n    filter_shape: Union[List[int], Tuple[int]] = [3, 3],\n    sigma: Union[List[float], Tuple[float]] = 1.0,\n    padding: str = ""REFLECT"",\n    constant_values: TensorLike = 0,\n    name: Optional[str] = None,\n) -> FloatTensorLike:\n    """"""Perform Gaussian blur on image(s).\n\n    Args:\n      image: Either a 2-D `Tensor` of shape `[height, width]`,\n        a 3-D `Tensor` of shape `[height, width, channels]`,\n        or a 4-D `Tensor` of shape `[batch_size, height, width, channels]`.\n      filter_shape: An `integer` or `tuple`/`list` of 2 integers, specifying\n        the height and width of the 2-D gaussian filter. Can be a single\n        integer to specify the same value for all spatial dimensions.\n      sigma: A `float` or `tuple`/`list` of 2 floats, specifying\n        the standard deviation in x and y direction the 2-D gaussian filter.\n        Can be a single float to specify the same value for all spatial\n        dimensions.\n      padding: A `string`, one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\n        The type of padding algorithm to use, which is compatible with\n        `mode` argument in `tf.pad`. For more details, please refer to\n        https://www.tensorflow.org/api_docs/python/tf/pad.\n      constant_values: A `scalar`, the pad value to use in ""CONSTANT""\n        padding mode.\n      name: A name for this operation (optional).\n    Returns:\n      2-D, 3-D or 4-D `Tensor` of the same dtype as input.\n    Raises:\n      ValueError: If `image` is not 2, 3 or 4-dimensional,\n        if `padding` is other than ""REFLECT"", ""CONSTANT"" or ""SYMMETRIC"",\n        if `filter_shape` is invalid,\n        or if `sigma` is invalid.\n    """"""\n    with tf.name_scope(name or ""gaussian_filter2d""):\n        if isinstance(sigma, (list, tuple)):\n            if len(sigma) != 2:\n                raise ValueError(""sigma should be a float or a tuple/list of 2 floats"")\n        else:\n            sigma = (sigma,) * 2\n\n        if any(s < 0 for s in sigma):\n            raise ValueError(""sigma should be greater than or equal to 0."")\n\n        if padding not in [""REFLECT"", ""CONSTANT"", ""SYMMETRIC""]:\n            raise ValueError(\n                \'padding should be one of ""REFLECT"", ""CONSTANT"", or ""SYMMETRIC"".\'\n            )\n\n        image = tf.convert_to_tensor(image, name=""image"")\n        sigma = tf.convert_to_tensor(sigma, name=""sigma"")\n\n        original_ndims = img_utils.get_ndims(image)\n        image = img_utils.to_4D_image(image)\n\n        # Keep the precision if it\'s float;\n        # otherwise, convert to float32 for computing.\n        orig_dtype = image.dtype\n        if not image.dtype.is_floating:\n            image = tf.cast(image, tf.float32)\n\n        channels = tf.shape(image)[3]\n        filter_shape = keras_utils.normalize_tuple(filter_shape, 2, ""filter_shape"")\n\n        sigma = tf.cast(sigma, image.dtype)\n        gaussian_kernel_x = _get_gaussian_kernel(sigma[1], filter_shape[1])\n        gaussian_kernel_x = tf.reshape(gaussian_kernel_x, [1, filter_shape[1]])\n\n        gaussian_kernel_y = _get_gaussian_kernel(sigma[0], filter_shape[0])\n        gaussian_kernel_y = tf.reshape(gaussian_kernel_y, [filter_shape[0], 1])\n\n        gaussian_kernel_2d = _get_gaussian_kernel_2d(\n            gaussian_kernel_y, gaussian_kernel_x\n        )\n        gaussian_kernel_2d = tf.repeat(gaussian_kernel_2d, channels)\n        gaussian_kernel_2d = tf.reshape(\n            gaussian_kernel_2d, [filter_shape[0], filter_shape[1], channels, 1]\n        )\n\n        image = _pad(\n            image, filter_shape, mode=padding, constant_values=constant_values,\n        )\n\n        output = tf.nn.depthwise_conv2d(\n            input=image,\n            filter=gaussian_kernel_2d,\n            strides=(1, 1, 1, 1),\n            padding=""VALID"",\n        )\n        output = img_utils.from_4D_image(output, original_ndims)\n        return tf.cast(output, orig_dtype)\n'"
tensorflow_addons/image/interpolate_spline.py,44,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Polyharmonic spline interpolation.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n\nEPSILON = 0.0000000001\n\n\ndef _cross_squared_distance_matrix(x: TensorLike, y: TensorLike) -> tf.Tensor:\n    """"""Pairwise squared distance between two (batch) matrices\' rows (2nd dim).\n\n    Computes the pairwise distances between rows of x and rows of y\n    Args:\n      x: [batch_size, n, d] float `Tensor`\n      y: [batch_size, m, d] float `Tensor`\n\n    Returns:\n      squared_dists: [batch_size, n, m] float `Tensor`, where\n      squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2\n    """"""\n    x_norm_squared = tf.reduce_sum(tf.square(x), 2)\n    y_norm_squared = tf.reduce_sum(tf.square(y), 2)\n\n    # Expand so that we can broadcast.\n    x_norm_squared_tile = tf.expand_dims(x_norm_squared, 2)\n    y_norm_squared_tile = tf.expand_dims(y_norm_squared, 1)\n\n    x_y_transpose = tf.matmul(x, y, adjoint_b=True)\n\n    # squared_dists[b,i,j] = ||x_bi - y_bj||^2 =\n    # x_bi\'x_bi- 2x_bi\'x_bj + x_bj\'x_bj\n    squared_dists = x_norm_squared_tile - 2 * x_y_transpose + y_norm_squared_tile\n\n    return squared_dists\n\n\ndef _pairwise_squared_distance_matrix(x: TensorLike) -> tf.Tensor:\n    """"""Pairwise squared distance among a (batch) matrix\'s rows (2nd dim).\n\n    This saves a bit of computation vs. using\n    _cross_squared_distance_matrix(x,x)\n\n    Args:\n      x: `[batch_size, n, d]` float `Tensor`\n\n    Returns:\n      squared_dists: `[batch_size, n, n]` float `Tensor`, where\n      squared_dists[b,i,j] = ||x[b,i,:] - x[b,j,:]||^2\n    """"""\n\n    x_x_transpose = tf.matmul(x, x, adjoint_b=True)\n    x_norm_squared = tf.linalg.diag_part(x_x_transpose)\n    x_norm_squared_tile = tf.expand_dims(x_norm_squared, 2)\n\n    # squared_dists[b,i,j] = ||x_bi - x_bj||^2 =\n    # = x_bi\'x_bi- 2x_bi\'x_bj + x_bj\'x_bj\n    squared_dists = (\n        x_norm_squared_tile\n        - 2 * x_x_transpose\n        + tf.transpose(x_norm_squared_tile, [0, 2, 1])\n    )\n\n    return squared_dists\n\n\ndef _solve_interpolation(\n    train_points: TensorLike,\n    train_values: TensorLike,\n    order: int,\n    regularization_weight: FloatTensorLike,\n) -> TensorLike:\n    """"""Solve for interpolation coefficients.\n\n    Computes the coefficients of the polyharmonic interpolant for the\n    \'training\' data defined by (train_points, train_values) using the kernel\n    phi.\n\n    Args:\n      train_points: `[b, n, d]` interpolation centers\n      train_values: `[b, n, k]` function values\n      order: order of the interpolation\n      regularization_weight: weight to place on smoothness regularization term\n\n    Returns:\n      w: `[b, n, k]` weights on each interpolation center\n      v: `[b, d, k]` weights on each input dimension\n    Raises:\n      ValueError: if d or k is not fully specified.\n    """"""\n\n    # These dimensions are set dynamically at runtime.\n    b, n, _ = tf.unstack(tf.shape(train_points), num=3)\n\n    d = train_points.shape[-1]\n    if d is None:\n        raise ValueError(\n            ""The dimensionality of the input points (d) must be ""\n            ""statically-inferrable.""\n        )\n\n    k = train_values.shape[-1]\n    if k is None:\n        raise ValueError(\n            ""The dimensionality of the output values (k) must be ""\n            ""statically-inferrable.""\n        )\n\n    # First, rename variables so that the notation (c, f, w, v, A, B, etc.)\n    # follows https://en.wikipedia.org/wiki/Polyharmonic_spline.\n    # To account for python style guidelines we use\n    # matrix_a for A and matrix_b for B.\n\n    c = train_points\n    f = train_values\n\n    # Next, construct the linear system.\n    with tf.name_scope(""construct_linear_system""):\n\n        matrix_a = _phi(_pairwise_squared_distance_matrix(c), order)  # [b, n, n]\n        if regularization_weight > 0:\n            batch_identity_matrix = tf.expand_dims(tf.eye(n, dtype=c.dtype), 0)\n            matrix_a += regularization_weight * batch_identity_matrix\n\n        # Append ones to the feature values for the bias term\n        # in the linear model.\n        ones = tf.ones_like(c[..., :1], dtype=c.dtype)\n        matrix_b = tf.concat([c, ones], 2)  # [b, n, d + 1]\n\n        # [b, n + d + 1, n]\n        left_block = tf.concat([matrix_a, tf.transpose(matrix_b, [0, 2, 1])], 1)\n\n        num_b_cols = matrix_b.get_shape()[2]  # d + 1\n        lhs_zeros = tf.zeros([b, num_b_cols, num_b_cols], train_points.dtype)\n        right_block = tf.concat([matrix_b, lhs_zeros], 1)  # [b, n + d + 1, d + 1]\n        lhs = tf.concat([left_block, right_block], 2)  # [b, n + d + 1, n + d + 1]\n\n        rhs_zeros = tf.zeros([b, d + 1, k], train_points.dtype)\n        rhs = tf.concat([f, rhs_zeros], 1)  # [b, n + d + 1, k]\n\n    # Then, solve the linear system and unpack the results.\n    with tf.name_scope(""solve_linear_system""):\n        w_v = tf.linalg.solve(lhs, rhs)\n        w = w_v[:, :n, :]\n        v = w_v[:, n:, :]\n\n    return w, v\n\n\ndef _apply_interpolation(\n    query_points: TensorLike,\n    train_points: TensorLike,\n    w: TensorLike,\n    v: TensorLike,\n    order: int,\n) -> TensorLike:\n    """"""Apply polyharmonic interpolation model to data.\n\n    Given coefficients w and v for the interpolation model, we evaluate\n    interpolated function values at query_points.\n\n    Args:\n      query_points: `[b, m, d]` x values to evaluate the interpolation at\n      train_points: `[b, n, d]` x values that act as the interpolation centers\n                      ( the c variables in the wikipedia article)\n      w: `[b, n, k]` weights on each interpolation center\n      v: `[b, d, k]` weights on each input dimension\n      order: order of the interpolation\n\n    Returns:\n      Polyharmonic interpolation evaluated at points defined in query_points.\n    """"""\n\n    # First, compute the contribution from the rbf term.\n    pairwise_dists = _cross_squared_distance_matrix(query_points, train_points)\n    phi_pairwise_dists = _phi(pairwise_dists, order)\n\n    rbf_term = tf.matmul(phi_pairwise_dists, w)\n\n    # Then, compute the contribution from the linear term.\n    # Pad query_points with ones, for the bias term in the linear model.\n    query_points_pad = tf.concat(\n        [query_points, tf.ones_like(query_points[..., :1], train_points.dtype)], 2\n    )\n    linear_term = tf.matmul(query_points_pad, v)\n\n    return rbf_term + linear_term\n\n\ndef _phi(r: FloatTensorLike, order: int) -> FloatTensorLike:\n    """"""Coordinate-wise nonlinearity used to define the order of the\n    interpolation.\n\n    See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition.\n\n    Args:\n      r: input op\n      order: interpolation order\n\n    Returns:\n      phi_k evaluated coordinate-wise on r, for k = r\n    """"""\n\n    # using EPSILON prevents log(0), sqrt0), etc.\n    # sqrt(0) is well-defined, but its gradient is not\n    with tf.name_scope(""phi""):\n        if order == 1:\n            r = tf.maximum(r, EPSILON)\n            r = tf.sqrt(r)\n            return r\n        elif order == 2:\n            return 0.5 * r * tf.math.log(tf.maximum(r, EPSILON))\n        elif order == 4:\n            return 0.5 * tf.square(r) * tf.math.log(tf.maximum(r, EPSILON))\n        elif order % 2 == 0:\n            r = tf.maximum(r, EPSILON)\n            return 0.5 * tf.pow(r, 0.5 * order) * tf.math.log(r)\n        else:\n            r = tf.maximum(r, EPSILON)\n            return tf.pow(r, 0.5 * order)\n\n\ndef interpolate_spline(\n    train_points: TensorLike,\n    train_values: TensorLike,\n    query_points: TensorLike,\n    order: int,\n    regularization_weight: FloatTensorLike = 0.0,\n    name: str = ""interpolate_spline"",\n) -> tf.Tensor:\n    r""""""Interpolate signal using polyharmonic interpolation.\n\n    The interpolant has the form\n    $$f(x) = \\sum_{i = 1}^n w_i \\phi(||x - c_i||) + v^T x + b.$$\n\n    This is a sum of two terms: (1) a weighted sum of radial basis function\n    (RBF) terms, with the centers \\\\(c_1, ... c_n\\\\), and (2) a linear term\n    with a bias. The \\\\(c_i\\\\) vectors are \'training\' points.\n    In the code, b is absorbed into v\n    by appending 1 as a final dimension to x. The coefficients w and v are\n    estimated such that the interpolant exactly fits the value of the function\n    at the \\\\(c_i\\\\) points, the vector w is orthogonal to each \\\\(c_i\\\\),\n    and the vector w sums to 0. With these constraints, the coefficients\n    can be obtained by solving a linear system.\n\n    \\\\(\\phi\\\\) is an RBF, parametrized by an interpolation\n    order. Using order=2 produces the well-known thin-plate spline.\n\n    We also provide the option to perform regularized interpolation. Here, the\n    interpolant is selected to trade off between the squared loss on the\n    training data and a certain measure of its curvature\n    ([details](https://en.wikipedia.org/wiki/Polyharmonic_spline)).\n    Using a regularization weight greater than zero has the effect that the\n    interpolant will no longer exactly fit the training data. However, it may\n    be less vulnerable to overfitting, particularly for high-order\n    interpolation.\n\n    Note the interpolation procedure is differentiable with respect to all\n    inputs besides the order parameter.\n\n    We support dynamically-shaped inputs, where batch_size, n, and m are None\n    at graph construction time. However, d and k must be known.\n\n    Args:\n      train_points: `[batch_size, n, d]` float `Tensor` of n d-dimensional\n        locations. These do not need to be regularly-spaced.\n      train_values: `[batch_size, n, k]` float `Tensor` of n c-dimensional\n        values evaluated at train_points.\n      query_points: `[batch_size, m, d]` `Tensor` of m d-dimensional locations\n        where we will output the interpolant\'s values.\n      order: order of the interpolation. Common values are 1 for\n        \\\\(\\phi(r) = r\\\\), 2 for \\\\(\\phi(r) = r^2 * log(r)\\\\)\n        (thin-plate spline), or 3 for \\\\(\\phi(r) = r^3\\\\).\n      regularization_weight: weight placed on the regularization term.\n        This will depend substantially on the problem, and it should always be\n        tuned. For many problems, it is reasonable to use no regularization.\n        If using a non-zero value, we recommend a small value like 0.001.\n      name: name prefix for ops created by this function\n\n    Returns:\n      `[b, m, k]` float `Tensor` of query values. We use train_points and\n      train_values to perform polyharmonic interpolation. The query values are\n      the values of the interpolant evaluated at the locations specified in\n      query_points.\n  """"""\n    with tf.name_scope(name or ""interpolate_spline""):\n        train_points = tf.convert_to_tensor(train_points)\n        train_values = tf.convert_to_tensor(train_values)\n        query_points = tf.convert_to_tensor(query_points)\n\n        # First, fit the spline to the observed data.\n        with tf.name_scope(""solve""):\n            w, v = _solve_interpolation(\n                train_points, train_values, order, regularization_weight\n            )\n\n        # Then, evaluate the spline at the query locations.\n        with tf.name_scope(""predict""):\n            query_values = _apply_interpolation(query_points, train_points, w, v, order)\n\n    return query_values\n'"
tensorflow_addons/image/resampler_ops.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Python layer for Resampler.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import types\nfrom tensorflow_addons.utils.resource_loader import LazySO\n\nfrom typing import Optional\n\n_resampler_so = LazySO(""custom_ops/image/_resampler_ops.so"")\n\n\n@tf.function\ndef resampler(\n    data: types.TensorLike, warp: types.TensorLike, name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Resamples input data at user defined coordinates.\n\n    The resampler currently only supports bilinear interpolation of 2D data.\n\n    Args:\n      data: Tensor of shape `[batch_size, data_height, data_width,\n        data_num_channels]` containing 2D data that will be resampled.\n      warp: Tensor of minimum rank 2 containing the coordinates at\n      which resampling will be performed. Since only bilinear\n      interpolation is currently supported, the last dimension of the\n      `warp` tensor must be 2, representing the (x, y) coordinate where\n      x is the index for width and y is the index for height.\n      name: Optional name of the op.\n    Returns:\n      Tensor of resampled values from `data`. The output tensor shape\n      is determined by the shape of the warp tensor. For example, if `data`\n      is of shape `[batch_size, data_height, data_width, data_num_channels]`\n      and warp of shape `[batch_size, dim_0, ... , dim_n, 2]` the output will\n      be of shape `[batch_size, dim_0, ... , dim_n, data_num_channels]`.\n    Raises:\n      ImportError: if the wrapper generated during compilation is not\n      present when the function is called.\n    """"""\n    with tf.name_scope(name or ""resampler""):\n        data_tensor = tf.convert_to_tensor(data, name=""data"")\n        warp_tensor = tf.convert_to_tensor(warp, name=""warp"")\n        return _resampler_so.ops.addons_resampler(data_tensor, warp_tensor)\n\n\n@tf.RegisterGradient(""Addons>Resampler"")\ndef _resampler_grad(op: types.TensorLike, grad_output: types.TensorLike) -> tf.Tensor:\n    data, warp = op.inputs\n    grad_output_tensor = tf.convert_to_tensor(grad_output, name=""grad_output"")\n    return _resampler_so.ops.addons_resampler_grad(data, warp, grad_output_tensor)\n\n\ntf.no_gradient(""Addons>ResamplerGrad"")\n'"
tensorflow_addons/image/sparse_image_warp.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image warping using sparse flow defined at control points.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.image import dense_image_warp\nfrom tensorflow_addons.image import interpolate_spline\nfrom tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n\n\ndef _get_grid_locations(\n    image_height: TensorLike, image_width: TensorLike\n) -> TensorLike:\n    """"""Wrapper for np.meshgrid.""""""\n\n    y_range = np.linspace(0, image_height - 1, image_height)\n    x_range = np.linspace(0, image_width - 1, image_width)\n    y_grid, x_grid = np.meshgrid(y_range, x_range, indexing=""ij"")\n    return np.stack((y_grid, x_grid), -1)\n\n\ndef _expand_to_minibatch(np_array: TensorLike, batch_size: TensorLike) -> TensorLike:\n    """"""Tile arbitrarily-sized np_array to include new batch dimension.""""""\n    tiles = [batch_size] + [1] * np_array.ndim\n    return np.tile(np.expand_dims(np_array, 0), tiles)\n\n\ndef _get_boundary_locations(\n    image_height: TensorLike, image_width: TensorLike, num_points_per_edge: TensorLike,\n) -> TensorLike:\n    """"""Compute evenly-spaced indices along edge of image.""""""\n    y_range = np.linspace(0, image_height - 1, num_points_per_edge + 2)\n    x_range = np.linspace(0, image_width - 1, num_points_per_edge + 2)\n    ys, xs = np.meshgrid(y_range, x_range, indexing=""ij"")\n    is_boundary = np.logical_or(\n        np.logical_or(xs == 0, xs == image_width - 1),\n        np.logical_or(ys == 0, ys == image_height - 1),\n    )\n    return np.stack([ys[is_boundary], xs[is_boundary]], axis=-1)\n\n\ndef _add_zero_flow_controls_at_boundary(\n    control_point_locations: TensorLike,\n    control_point_flows: TensorLike,\n    image_height: TensorLike,\n    image_width: TensorLike,\n    boundary_points_per_edge: TensorLike,\n) -> tf.Tensor:\n    """"""Add control points for zero-flow boundary conditions.\n\n    Augment the set of control points with extra points on the\n    boundary of the image that have zero flow.\n\n    Args:\n      control_point_locations: input control points\n      control_point_flows: their flows\n      image_height: image height\n      image_width: image width\n      boundary_points_per_edge: number of points to add in the middle of each\n                            edge (not including the corners).\n                            The total number of points added is\n                            4 + 4*(boundary_points_per_edge).\n\n    Returns:\n      merged_control_point_locations: augmented set of control point locations\n      merged_control_point_flows: augmented set of control point flows\n    """"""\n\n    batch_size = tf.compat.dimension_value(control_point_locations.shape[0])\n\n    boundary_point_locations = _get_boundary_locations(\n        image_height, image_width, boundary_points_per_edge\n    )\n\n    boundary_point_flows = np.zeros([boundary_point_locations.shape[0], 2])\n\n    type_to_use = control_point_locations.dtype\n    boundary_point_locations = tf.constant(\n        _expand_to_minibatch(boundary_point_locations, batch_size), dtype=type_to_use\n    )\n\n    boundary_point_flows = tf.constant(\n        _expand_to_minibatch(boundary_point_flows, batch_size), dtype=type_to_use\n    )\n\n    merged_control_point_locations = tf.concat(\n        [control_point_locations, boundary_point_locations], 1\n    )\n\n    merged_control_point_flows = tf.concat(\n        [control_point_flows, boundary_point_flows], 1\n    )\n\n    return merged_control_point_locations, merged_control_point_flows\n\n\ndef sparse_image_warp(\n    image: TensorLike,\n    source_control_point_locations: TensorLike,\n    dest_control_point_locations: TensorLike,\n    interpolation_order: int = 2,\n    regularization_weight: FloatTensorLike = 0.0,\n    num_boundary_points: int = 0,\n    name: str = ""sparse_image_warp"",\n) -> tf.Tensor:\n    """"""Image warping using correspondences between sparse control points.\n\n    Apply a non-linear warp to the image, where the warp is specified by\n    the source and destination locations of a (potentially small) number of\n    control points. First, we use a polyharmonic spline\n    (`tfa.image.interpolate_spline`) to interpolate the displacements\n    between the corresponding control points to a dense flow field.\n    Then, we warp the image using this dense flow field\n    (`tfa.image.dense_image_warp`).\n\n    Let t index our control points. For `regularization_weight = 0`, we have:\n    warped_image[b, dest_control_point_locations[b, t, 0],\n                    dest_control_point_locations[b, t, 1], :] =\n    image[b, source_control_point_locations[b, t, 0],\n             source_control_point_locations[b, t, 1], :].\n\n    For `regularization_weight > 0`, this condition is met approximately, since\n    regularized interpolation trades off smoothness of the interpolant vs.\n    reconstruction of the interpolant at the control points.\n    See `tfa.image.interpolate_spline` for further documentation of the\n    `interpolation_order` and `regularization_weight` arguments.\n\n\n    Args:\n      image: `[batch, height, width, channels]` float `Tensor`\n      source_control_point_locations: `[batch, num_control_points, 2]` float\n        `Tensor`\n      dest_control_point_locations: `[batch, num_control_points, 2]` float\n        `Tensor`\n      interpolation_order: polynomial order used by the spline interpolation\n      regularization_weight: weight on smoothness regularizer in interpolation\n      num_boundary_points: How many zero-flow boundary points to include at\n        each image edge.Usage:\n          num_boundary_points=0: don\'t add zero-flow points\n          num_boundary_points=1: 4 corners of the image\n          num_boundary_points=2: 4 corners and one in the middle of each edge\n            (8 points total)\n          num_boundary_points=n: 4 corners and n-1 along each edge\n      name: A name for the operation (optional).\n\n      Note that image and offsets can be of type tf.half, tf.float32, or\n      tf.float64, and do not necessarily have to be the same type.\n\n    Returns:\n      warped_image: `[batch, height, width, channels]` float `Tensor` with same\n        type as input image.\n      flow_field: `[batch, height, width, 2]` float `Tensor` containing the\n        dense flow field produced by the interpolation.\n    """"""\n\n    image = tf.convert_to_tensor(image)\n    source_control_point_locations = tf.convert_to_tensor(\n        source_control_point_locations\n    )\n    dest_control_point_locations = tf.convert_to_tensor(dest_control_point_locations)\n\n    control_point_flows = dest_control_point_locations - source_control_point_locations\n\n    clamp_boundaries = num_boundary_points > 0\n    boundary_points_per_edge = num_boundary_points - 1\n\n    with tf.name_scope(name or ""sparse_image_warp""):\n\n        batch_size, image_height, image_width, _ = image.get_shape().as_list()\n\n        # This generates the dense locations where the interpolant\n        # will be evaluated.\n        grid_locations = _get_grid_locations(image_height, image_width)\n\n        flattened_grid_locations = np.reshape(\n            grid_locations, [image_height * image_width, 2]\n        )\n\n        flattened_grid_locations = tf.constant(\n            _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype\n        )\n\n        if clamp_boundaries:\n            (\n                dest_control_point_locations,\n                control_point_flows,\n            ) = _add_zero_flow_controls_at_boundary(\n                dest_control_point_locations,\n                control_point_flows,\n                image_height,\n                image_width,\n                boundary_points_per_edge,\n            )\n\n        flattened_flows = interpolate_spline(\n            dest_control_point_locations,\n            control_point_flows,\n            flattened_grid_locations,\n            interpolation_order,\n            regularization_weight,\n        )\n\n        dense_flows = tf.reshape(\n            flattened_flows, [batch_size, image_height, image_width, 2]\n        )\n\n        warped_image = dense_image_warp(image, dense_flows)\n\n        return warped_image, dense_flows\n'"
tensorflow_addons/image/transform_ops.py,58,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image transform ops.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import utils as img_utils\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons.utils.types import TensorLike\nfrom tensorflow_addons.image.utils import wrap, unwrap\n\nfrom typing import Optional\n\n_image_so = LazySO(""custom_ops/image/_image_ops.so"")\n\n_IMAGE_DTYPES = {\n    tf.dtypes.uint8,\n    tf.dtypes.int32,\n    tf.dtypes.int64,\n    tf.dtypes.float16,\n    tf.dtypes.float32,\n    tf.dtypes.float64,\n}\n\n\n@tf.function\ndef transform(\n    images: TensorLike,\n    transforms: TensorLike,\n    interpolation: str = ""NEAREST"",\n    output_shape: Optional[list] = None,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Applies the given transform(s) to the image(s).\n\n    Args:\n      images: A tensor of shape (num_images, num_rows, num_columns,\n        num_channels) (NHWC), (num_rows, num_columns, num_channels) (HWC), or\n        (num_rows, num_columns) (HW).\n      transforms: Projective transform matrix/matrices. A vector of length 8 or\n        tensor of size N x 8. If one row of transforms is\n        [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point\n        `(x, y)` to a transformed *input* point\n        `(x\', y\') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n        where `k = c0 x + c1 y + 1`. The transforms are *inverted* compared to\n        the transform mapping input points to output points. Note that\n        gradients are not backpropagated into transformation parameters.\n      interpolation: Interpolation mode.\n        Supported values: ""NEAREST"", ""BILINEAR"".\n      output_shape: Output dimesion after the transform, [height, width].\n        If None, output is the same size as input image.\n\n      name: The name of the op.\n\n    Returns:\n      Image(s) with the same type and shape as `images`, with the given\n      transform(s) applied. Transformed coordinates outside of the input image\n      will be filled with zeros.\n\n    Raises:\n      TypeError: If `image` is an invalid type.\n      ValueError: If output shape is not 1-D int32 Tensor.\n    """"""\n    with tf.name_scope(name or ""transform""):\n        image_or_images = tf.convert_to_tensor(images, name=""images"")\n        transform_or_transforms = tf.convert_to_tensor(\n            transforms, name=""transforms"", dtype=tf.dtypes.float32\n        )\n        if image_or_images.dtype.base_dtype not in _IMAGE_DTYPES:\n            raise TypeError(""Invalid dtype %s."" % image_or_images.dtype)\n        images = img_utils.to_4D_image(image_or_images)\n        original_ndims = img_utils.get_ndims(image_or_images)\n\n        if output_shape is None:\n            output_shape = tf.shape(images)[1:3]\n\n        output_shape = tf.convert_to_tensor(\n            output_shape, tf.dtypes.int32, name=""output_shape""\n        )\n\n        if not output_shape.get_shape().is_compatible_with([2]):\n            raise ValueError(\n                ""output_shape must be a 1-D Tensor of 2 elements: ""\n                ""new_height, new_width""\n            )\n\n        if len(transform_or_transforms.get_shape()) == 1:\n            transforms = transform_or_transforms[None]\n        elif transform_or_transforms.get_shape().ndims is None:\n            raise ValueError(""transforms rank must be statically known"")\n        elif len(transform_or_transforms.get_shape()) == 2:\n            transforms = transform_or_transforms\n        else:\n            transforms = transform_or_transforms\n            raise ValueError(\n                ""transforms should have rank 1 or 2, but got rank %d""\n                % len(transforms.get_shape())\n            )\n\n        output = _image_so.ops.addons_image_projective_transform_v2(\n            images,\n            output_shape=output_shape,\n            transforms=transforms,\n            interpolation=interpolation.upper(),\n        )\n        return img_utils.from_4D_image(output, original_ndims)\n\n\ndef compose_transforms(transforms: TensorLike, name: Optional[str] = None) -> tf.Tensor:\n    """"""Composes the transforms tensors.\n\n    Args:\n      transforms: List of image projective transforms to be composed. Each\n        transform is length 8 (single transform) or shape (N, 8) (batched\n        transforms). The shapes of all inputs must be equal, and at least one\n        input must be given.\n      name: The name for the op.\n\n    Returns:\n      A composed transform tensor. When passed to `transform` op,\n          equivalent to applying each of the given transforms to the image in\n          order.\n    """"""\n    assert transforms, ""transforms cannot be empty""\n    with tf.name_scope(name or ""compose_transforms""):\n        composed = flat_transforms_to_matrices(transforms[0])\n        for tr in transforms[1:]:\n            # Multiply batches of matrices.\n            composed = tf.matmul(composed, flat_transforms_to_matrices(tr))\n        return matrices_to_flat_transforms(composed)\n\n\ndef flat_transforms_to_matrices(\n    transforms: TensorLike, name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Converts projective transforms to affine matrices.\n\n    Note that the output matrices map output coordinates to input coordinates.\n    For the forward transformation matrix, call `tf.linalg.inv` on the result.\n\n    Args:\n      transforms: Vector of length 8, or batches of transforms with shape\n        `(N, 8)`.\n      name: The name for the op.\n\n    Returns:\n      3D tensor of matrices with shape `(N, 3, 3)`. The output matrices map the\n        *output coordinates* (in homogeneous coordinates) of each transform to\n        the corresponding *input coordinates*.\n\n    Raises:\n      ValueError: If `transforms` have an invalid shape.\n    """"""\n    with tf.name_scope(name or ""flat_transforms_to_matrices""):\n        transforms = tf.convert_to_tensor(transforms, name=""transforms"")\n        if transforms.shape.ndims not in (1, 2):\n            raise ValueError(""Transforms should be 1D or 2D, got: %s"" % transforms)\n        # Make the transform(s) 2D in case the input is a single transform.\n        transforms = tf.reshape(transforms, tf.constant([-1, 8]))\n        num_transforms = tf.shape(transforms)[0]\n        # Add a column of ones for the implicit last entry in the matrix.\n        return tf.reshape(\n            tf.concat([transforms, tf.ones([num_transforms, 1])], axis=1),\n            tf.constant([-1, 3, 3]),\n        )\n\n\ndef matrices_to_flat_transforms(\n    transform_matrices: TensorLike, name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Converts affine matrices to projective transforms.\n\n    Note that we expect matrices that map output coordinates to input\n    coordinates. To convert forward transformation matrices,\n    call `tf.linalg.inv` on the matrices and use the result here.\n\n    Args:\n      transform_matrices: One or more affine transformation matrices, for the\n        reverse transformation in homogeneous coordinates. Shape `(3, 3)` or\n        `(N, 3, 3)`.\n      name: The name for the op.\n\n    Returns:\n      2D tensor of flat transforms with shape `(N, 8)`, which may be passed\n      into `transform` op.\n\n    Raises:\n      ValueError: If `transform_matrices` have an invalid shape.\n    """"""\n    with tf.name_scope(name or ""matrices_to_flat_transforms""):\n        transform_matrices = tf.convert_to_tensor(\n            transform_matrices, name=""transform_matrices""\n        )\n        if transform_matrices.shape.ndims not in (2, 3):\n            raise ValueError(\n                ""Matrices should be 2D or 3D, got: %s"" % transform_matrices\n            )\n        # Flatten each matrix.\n        transforms = tf.reshape(transform_matrices, tf.constant([-1, 9]))\n        # Divide each matrix by the last entry (normally 1).\n        transforms /= transforms[:, 8:9]\n        return transforms[:, :8]\n\n\ndef angles_to_projective_transforms(\n    angles: TensorLike,\n    image_height: TensorLike,\n    image_width: TensorLike,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Returns projective transform(s) for the given angle(s).\n\n    Args:\n      angles: A scalar angle to rotate all images by, or (for batches of\n        images) a vector with an angle to rotate each image in the batch. The\n        rank must be statically known (the shape is not `TensorShape(None)`.\n      image_height: Height of the image(s) to be transformed.\n      image_width: Width of the image(s) to be transformed.\n\n    Returns:\n      A tensor of shape (num_images, 8). Projective transforms which can be\n      given to `transform` op.\n    """"""\n    with tf.name_scope(name or ""angles_to_projective_transforms""):\n        angle_or_angles = tf.convert_to_tensor(\n            angles, name=""angles"", dtype=tf.dtypes.float32\n        )\n        if len(angle_or_angles.get_shape()) == 0:\n            angles = angle_or_angles[None]\n        elif len(angle_or_angles.get_shape()) == 1:\n            angles = angle_or_angles\n        else:\n            raise ValueError(""angles should have rank 0 or 1."")\n        x_offset = (\n            (image_width - 1)\n            - (\n                tf.math.cos(angles) * (image_width - 1)\n                - tf.math.sin(angles) * (image_height - 1)\n            )\n        ) / 2.0\n        y_offset = (\n            (image_height - 1)\n            - (\n                tf.math.sin(angles) * (image_width - 1)\n                + tf.math.cos(angles) * (image_height - 1)\n            )\n        ) / 2.0\n        num_angles = tf.shape(angles)[0]\n        return tf.concat(\n            values=[\n                tf.math.cos(angles)[:, None],\n                -tf.math.sin(angles)[:, None],\n                x_offset[:, None],\n                tf.math.sin(angles)[:, None],\n                tf.math.cos(angles)[:, None],\n                y_offset[:, None],\n                tf.zeros((num_angles, 2), tf.dtypes.float32),\n            ],\n            axis=1,\n        )\n\n\n@tf.RegisterGradient(""Addons>ImageProjectiveTransformV2"")\ndef _image_projective_transform_grad(op, grad):\n    """"""Computes the gradient for ImageProjectiveTransform.""""""\n    images = op.inputs[0]\n    transforms = op.inputs[1]\n    interpolation = op.get_attr(""interpolation"")\n\n    image_or_images = tf.convert_to_tensor(images, name=""images"")\n    transform_or_transforms = tf.convert_to_tensor(\n        transforms, name=""transforms"", dtype=tf.dtypes.float32\n    )\n\n    if image_or_images.dtype.base_dtype not in _IMAGE_DTYPES:\n        raise ValueError(""Invalid dtype %s."" % image_or_images.dtype)\n    if len(transform_or_transforms.get_shape()) == 1:\n        transforms = transform_or_transforms[None]\n    elif len(transform_or_transforms.get_shape()) == 2:\n        transforms = transform_or_transforms\n    else:\n        transforms = transform_or_transforms\n        raise ValueError(\n            ""transforms should have rank 1 or 2, but got rank %d""\n            % len(transforms.get_shape())\n        )\n\n    # Invert transformations\n    transforms = flat_transforms_to_matrices(transforms=transforms)\n    inverse = tf.linalg.inv(transforms)\n    transforms = matrices_to_flat_transforms(inverse)\n    output = _image_so.ops.addons_image_projective_transform_v2(\n        images=grad,\n        transforms=transforms,\n        output_shape=tf.shape(image_or_images)[1:3],\n        interpolation=interpolation,\n    )\n    return [output, None, None]\n\n\ndef rotate(\n    images: TensorLike,\n    angles: TensorLike,\n    interpolation: str = ""NEAREST"",\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Rotate image(s) counterclockwise by the passed angle(s) in radians.\n\n    Args:\n      images: A tensor of shape\n        (num_images, num_rows, num_columns, num_channels)\n        (NHWC), (num_rows, num_columns, num_channels) (HWC), or\n        (num_rows, num_columns) (HW).\n      angles: A scalar angle to rotate all images by, or (if images has rank 4)\n        a vector of length num_images, with an angle for each image in the\n        batch.\n      interpolation: Interpolation mode. Supported values: ""NEAREST"",\n        ""BILINEAR"".\n      name: The name of the op.\n\n    Returns:\n      Image(s) with the same type and shape as `images`, rotated by the given\n      angle(s). Empty space due to the rotation will be filled with zeros.\n\n    Raises:\n      TypeError: If `image` is an invalid type.\n    """"""\n    with tf.name_scope(name or ""rotate""):\n        image_or_images = tf.convert_to_tensor(images)\n        if image_or_images.dtype.base_dtype not in _IMAGE_DTYPES:\n            raise TypeError(""Invalid dtype %s."" % image_or_images.dtype)\n        images = img_utils.to_4D_image(image_or_images)\n        original_ndims = img_utils.get_ndims(image_or_images)\n\n        image_height = tf.cast(tf.shape(images)[1], tf.dtypes.float32)[None]\n        image_width = tf.cast(tf.shape(images)[2], tf.dtypes.float32)[None]\n        output = transform(\n            images,\n            angles_to_projective_transforms(angles, image_height, image_width),\n            interpolation=interpolation,\n        )\n        return img_utils.from_4D_image(output, original_ndims)\n\n\ndef shear_x(image: TensorLike, level: float, replace: int) -> TensorLike:\n    """"""Perform shear operation on an image (x-axis)\n    Args:\n        image: A 3D image Tensor.\n        level: A float denoting shear element along y-axis\n        replace: A one or three value 1D tensor to fill empty pixels.\n    Returns:\n        Transformed image along X or Y axis, with space outside image\n        filled with replace.\n    """"""\n    # Shear parallel to x axis is a projective transform\n    # with a matrix form of:\n    # [1  level\n    #  0  1].\n    image = transform(wrap(image), [1.0, level, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n    return unwrap(image, replace)\n\n\ndef shear_y(image: TensorLike, level: float, replace: int) -> TensorLike:\n    """"""Perform shear operation on an image (y-axis)\n    Args:\n        image: A 3D image Tensor.\n        level: A float denoting shear element along x-axis\n        replace: A one or three value 1D tensor to fill empty pixels.\n    Returns:\n        Transformed image along X or Y axis, with space outside image\n        filled with replace.\n    """"""\n    # Shear parallel to y axis is a projective transform\n    # with a matrix form of:\n    # [1  0\n    #  level  1].\n    image = transform(wrap(image), [1.0, 0.0, 0.0, level, 1.0, 0.0, 0.0, 0.0])\n    return unwrap(image, replace)\n'"
tensorflow_addons/image/translate_ops.py,16,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image translate ops.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.image.transform_ops import transform\nfrom tensorflow_addons.image.utils import wrap, unwrap\nfrom tensorflow_addons.utils.types import TensorLike\n\nfrom typing import Optional\n\n\ndef translations_to_projective_transforms(\n    translations: TensorLike, name: Optional[str] = None\n) -> tf.Tensor:\n    """"""Returns projective transform(s) for the given translation(s).\n\n    Args:\n        translations: A 2-element list representing [dx, dy] or a matrix of\n            2-element lists representing [dx, dy] to translate for each image\n            (for a batch of images). The rank must be statically known\n            (the shape is not `TensorShape(None)`).\n        name: The name of the op.\n    Returns:\n        A tensor of shape (num_images, 8) projective transforms which can be\n        given to `tfa.image.transform`.\n    """"""\n    with tf.name_scope(name or ""translations_to_projective_transforms""):\n        translation_or_translations = tf.convert_to_tensor(\n            translations, name=""translations"", dtype=tf.dtypes.float32\n        )\n        if translation_or_translations.get_shape().ndims is None:\n            raise TypeError(""translation_or_translations rank must be statically known"")\n        elif len(translation_or_translations.get_shape()) == 1:\n            translations = translation_or_translations[None]\n        elif len(translation_or_translations.get_shape()) == 2:\n            translations = translation_or_translations\n        else:\n            raise TypeError(""Translations should have rank 1 or 2."")\n        num_translations = tf.shape(translations)[0]\n        # The translation matrix looks like:\n        #     [[1 0 -dx]\n        #      [0 1 -dy]\n        #      [0 0 1]]\n        # where the last entry is implicit.\n        # Translation matrices are always float32.\n        return tf.concat(\n            values=[\n                tf.ones((num_translations, 1), tf.dtypes.float32),\n                tf.zeros((num_translations, 1), tf.dtypes.float32),\n                -translations[:, 0, None],\n                tf.zeros((num_translations, 1), tf.dtypes.float32),\n                tf.ones((num_translations, 1), tf.dtypes.float32),\n                -translations[:, 1, None],\n                tf.zeros((num_translations, 2), tf.dtypes.float32),\n            ],\n            axis=1,\n        )\n\n\n@tf.function\ndef translate(\n    images: TensorLike,\n    translations: TensorLike,\n    interpolation: str = ""NEAREST"",\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Translate image(s) by the passed vectors(s).\n\n    Args:\n      images: A tensor of shape\n          (num_images, num_rows, num_columns, num_channels) (NHWC),\n          (num_rows, num_columns, num_channels) (HWC), or\n          (num_rows, num_columns) (HW). The rank must be statically known (the\n          shape is not `TensorShape(None)`).\n      translations: A vector representing [dx, dy] or (if images has rank 4)\n          a matrix of length num_images, with a [dx, dy] vector for each image\n          in the batch.\n      interpolation: Interpolation mode. Supported values: ""NEAREST"",\n          ""BILINEAR"".\n      name: The name of the op.\n    Returns:\n      Image(s) with the same type and shape as `images`, translated by the\n      given vector(s). Empty space due to the translation will be filled with\n      zeros.\n    Raises:\n      TypeError: If `images` is an invalid type.\n    """"""\n    with tf.name_scope(name or ""translate""):\n        return transform(\n            images,\n            translations_to_projective_transforms(translations),\n            interpolation=interpolation,\n        )\n\n\ndef translate_xy(\n    image: TensorLike, translate_to: TensorLike, replace: int\n) -> TensorLike:\n    """"""Translates image in X or Y dimension.\n\n    Args:\n        image: A 3D image Tensor.\n        translate_to: A 1D tensor to translate [x, y]\n        replace: A one or three value 1D tensor to fill empty pixels.\n    Returns:\n        Translated image along X or Y axis, with space outside image\n        filled with replace.\n    Raises:\n        ValueError: if axis is neither 0 nor 1.\n    """"""\n    image = tf.convert_to_tensor(image)\n    image = wrap(image)\n    trans = tf.convert_to_tensor(translate_to)\n    image = translate(image, [trans[0], trans[1]])\n    return unwrap(image, replace)\n'"
tensorflow_addons/image/utils.py,36,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Image util ops.""""""\n\nimport tensorflow as tf\n\n\ndef get_ndims(image):\n    return image.get_shape().ndims or tf.rank(image)\n\n\ndef to_4D_image(image):\n    """"""Convert 2/3/4D image to 4D image.\n\n    Args:\n      image: 2/3/4D tensor.\n\n    Returns:\n      4D tensor with the same type.\n    """"""\n    with tf.control_dependencies(\n        [\n            tf.debugging.assert_rank_in(\n                image, [2, 3, 4], message=""`image` must be 2/3/4D tensor""\n            )\n        ]\n    ):\n        ndims = image.get_shape().ndims\n        if ndims is None:\n            return _dynamic_to_4D_image(image)\n        elif ndims == 2:\n            return image[None, :, :, None]\n        elif ndims == 3:\n            return image[None, :, :, :]\n        else:\n            return image\n\n\ndef _dynamic_to_4D_image(image):\n    shape = tf.shape(image)\n    original_rank = tf.rank(image)\n    # 4D image => [N, H, W, C] or [N, C, H, W]\n    # 3D image => [1, H, W, C] or [1, C, H, W]\n    # 2D image => [1, H, W, 1]\n    left_pad = tf.cast(tf.less_equal(original_rank, 3), dtype=tf.int32)\n    right_pad = tf.cast(tf.equal(original_rank, 2), dtype=tf.int32)\n    new_shape = tf.concat(\n        [\n            tf.ones(shape=left_pad, dtype=tf.int32),\n            shape,\n            tf.ones(shape=right_pad, dtype=tf.int32),\n        ],\n        axis=0,\n    )\n    return tf.reshape(image, new_shape)\n\n\ndef from_4D_image(image, ndims):\n    """"""Convert back to an image with `ndims` rank.\n\n    Args:\n      image: 4D tensor.\n      ndims: The original rank of the image.\n\n    Returns:\n      `ndims`-D tensor with the same type.\n    """"""\n    with tf.control_dependencies(\n        [tf.debugging.assert_rank(image, 4, message=""`image` must be 4D tensor"")]\n    ):\n        if isinstance(ndims, tf.Tensor):\n            return _dynamic_from_4D_image(image, ndims)\n        elif ndims == 2:\n            return tf.squeeze(image, [0, 3])\n        elif ndims == 3:\n            return tf.squeeze(image, [0])\n        else:\n            return image\n\n\ndef _dynamic_from_4D_image(image, original_rank):\n    shape = tf.shape(image)\n    # 4D image <= [N, H, W, C] or [N, C, H, W]\n    # 3D image <= [1, H, W, C] or [1, C, H, W]\n    # 2D image <= [1, H, W, 1]\n    begin = tf.cast(tf.less_equal(original_rank, 3), dtype=tf.int32)\n    end = 4 - tf.cast(tf.equal(original_rank, 2), dtype=tf.int32)\n    new_shape = shape[begin:end]\n    return tf.reshape(image, new_shape)\n\n\ndef wrap(image):\n    """"""Returns \'image\' with an extra channel set to all 1s.""""""\n    shape = tf.shape(image)\n    extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n    extended = tf.concat([image, extended_channel], 2)\n    return extended\n\n\ndef unwrap(image, replace):\n    """"""Unwraps an image produced by wrap.\n\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the \'average\' value, rather than pure black.\n\n\n  Args:\n    image: A 3D Image Tensor with 4 channels.\n    replace: A one or three value 1D tensor to fill empty pixels.\n\n  Returns:\n    image: A 3D image Tensor with 3 channels.\n  """"""\n    image_shape = tf.shape(image)\n    # Flatten the spatial dimensions.\n    flattened_image = tf.reshape(image, [-1, image_shape[2]])\n\n    # Find all pixels where the last channel is zero.\n    alpha_channel = flattened_image[:, 3]\n\n    replace = tf.constant(replace, tf.uint8)\n    if tf.rank(replace) == 0:\n        replace = tf.expand_dims(replace, 0)\n        replace = tf.concat([replace, replace, replace], 0)\n    replace = tf.concat([replace, tf.ones([1], dtype=image.dtype)], 0)\n\n    # Where they are zero, fill them in with \'replace\'.\n    cond = tf.equal(alpha_channel, 1)\n    cond = tf.expand_dims(cond, 1)\n    cond = tf.concat([cond, cond, cond, cond], 1)\n    flattened_image = tf.where(cond, flattened_image, replace)\n\n    image = tf.reshape(flattened_image, image_shape)\n    image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n    return image\n'"
tensorflow_addons/layers/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional layers that conform to Keras API.""""""\n\nfrom tensorflow_addons.layers.adaptive_pooling import (\n    AdaptiveAveragePooling1D,\n    AdaptiveMaxPooling1D,\n    AdaptiveAveragePooling2D,\n    AdaptiveMaxPooling2D,\n    AdaptiveAveragePooling3D,\n    AdaptiveMaxPooling3D,\n)\nfrom tensorflow_addons.layers.gelu import GELU\nfrom tensorflow_addons.layers.maxout import Maxout\nfrom tensorflow_addons.layers.multihead_attention import MultiHeadAttention\nfrom tensorflow_addons.layers.normalizations import FilterResponseNormalization\nfrom tensorflow_addons.layers.normalizations import GroupNormalization\nfrom tensorflow_addons.layers.normalizations import InstanceNormalization\nfrom tensorflow_addons.layers.optical_flow import CorrelationCost\nfrom tensorflow_addons.layers.poincare import PoincareNormalize\nfrom tensorflow_addons.layers.polynomial import PolynomialCrossing\nfrom tensorflow_addons.layers.sparsemax import Sparsemax\nfrom tensorflow_addons.layers.spatial_pyramid_pooling import SpatialPyramidPooling2D\nfrom tensorflow_addons.layers.tlu import TLU\nfrom tensorflow_addons.layers.wrappers import WeightNormalization\nfrom tensorflow_addons.layers.esn import ESN\n'"
tensorflow_addons/layers/adaptive_pooling.py,51,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Pooling layers with fixed size outputs""""""\n\nimport tensorflow as tf\nimport tensorflow_addons.utils.keras_utils as conv_utils\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable, Iterable\n\n\nclass AdaptivePooling1D(tf.keras.layers.Layer):\n    """"""Parent class for 1D pooling layers with adaptive kernel size.\n\n    This class only exists for code reuse. It will never be an exposed API.\n\n    Arguments:\n      reduce_function: The reduction method to apply, e.g. `tf.reduce_max`.\n      output_size: An integer or tuple/list of a single integer, specifying pooled_features.\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, features)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, features, steps)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        reduce_function: Callable,\n        output_size: Union[int, Iterable[int]],\n        data_format=None,\n        **kwargs\n    ):\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.reduce_function = reduce_function\n        self.output_size = conv_utils.normalize_tuple(output_size, 1, ""output_size"")\n        super().__init__(**kwargs)\n\n    def call(self, inputs, *args):\n        bins = self.output_size[0]\n        if self.data_format == ""channels_last"":\n            splits = tf.split(inputs, bins, axis=1)\n            splits = tf.stack(splits, axis=1)\n            out_vect = self.reduce_function(splits, axis=2)\n        else:\n            splits = tf.split(inputs, bins, axis=2)\n            splits = tf.stack(splits, axis=2)\n            out_vect = self.reduce_function(splits, axis=3)\n        return out_vect\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        if self.data_format == ""channels_last"":\n            shape = tf.TensorShape(\n                [input_shape[0], self.output_size[0], input_shape[2]]\n            )\n        else:\n            shape = tf.TensorShape(\n                [input_shape[0], input_shape[1], self.output_size[0]]\n            )\n\n        return shape\n\n    def get_config(self):\n        config = {\n            ""output_size"": self.output_size,\n            ""data_format"": self.data_format,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveAveragePooling1D(AdaptivePooling1D):\n    """"""Average Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: An integer or tuple/list of a single integer, specifying pooled_features.\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, steps)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        3D tensor with shape `(batch, steps, channels)`.\n      - If `data_format=\'channels_first\'`:\n        3D tensor with shape `(batch, channels, steps)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        3D tensor with shape `(batch_size, pooled_steps, channels)`.\n      - If `data_format=\'channels_first\'`:\n        3D tensor with shape `(batch_size, channels, pooled_steps)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_mean, output_size, data_format, **kwargs)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveMaxPooling1D(AdaptivePooling1D):\n    """"""Max Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: An integer or tuple/list of a single integer, specifying pooled_features.\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, steps, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, steps)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        3D tensor with shape `(batch, steps, channels)`.\n      - If `data_format=\'channels_first\'`:\n        3D tensor with shape `(batch, channels, steps)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        3D tensor with shape `(batch_size, pooled_steps, channels)`.\n      - If `data_format=\'channels_first\'`:\n        3D tensor with shape `(batch_size, channels, pooled_steps)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_max, output_size, data_format, **kwargs)\n\n\nclass AdaptivePooling2D(tf.keras.layers.Layer):\n    """"""Parent class for 2D pooling layers with adaptive kernel size.\n\n    This class only exists for code reuse. It will never be an exposed API.\n\n    Arguments:\n      reduce_function: The reduction method to apply, e.g. `tf.reduce_max`.\n      output_size: An integer or tuple/list of 2 integers specifying (pooled_rows, pooled_cols).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, height, width)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        reduce_function: Callable,\n        output_size: Union[int, Iterable[int]],\n        data_format=None,\n        **kwargs\n    ):\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.reduce_function = reduce_function\n        self.output_size = conv_utils.normalize_tuple(output_size, 2, ""output_size"")\n        super().__init__(**kwargs)\n\n    def call(self, inputs, *args):\n        h_bins = self.output_size[0]\n        w_bins = self.output_size[1]\n        if self.data_format == ""channels_last"":\n            split_cols = tf.split(inputs, h_bins, axis=1)\n            split_cols = tf.stack(split_cols, axis=1)\n            split_rows = tf.split(split_cols, w_bins, axis=3)\n            split_rows = tf.stack(split_rows, axis=3)\n            out_vect = self.reduce_function(split_rows, axis=[2, 4])\n        else:\n            split_cols = tf.split(inputs, h_bins, axis=2)\n            split_cols = tf.stack(split_cols, axis=2)\n            split_rows = tf.split(split_cols, w_bins, axis=4)\n            split_rows = tf.stack(split_rows, axis=4)\n            out_vect = self.reduce_function(split_rows, axis=[3, 5])\n        return out_vect\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        if self.data_format == ""channels_last"":\n            shape = tf.TensorShape(\n                [\n                    input_shape[0],\n                    self.output_size[0],\n                    self.output_size[1],\n                    input_shape[3],\n                ]\n            )\n        else:\n            shape = tf.TensorShape(\n                [\n                    input_shape[0],\n                    input_shape[1],\n                    self.output_size[0],\n                    self.output_size[1],\n                ]\n            )\n\n        return shape\n\n    def get_config(self):\n        config = {\n            ""output_size"": self.output_size,\n            ""data_format"": self.data_format,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveAveragePooling2D(AdaptivePooling2D):\n    """"""Average Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: Tuple of integers specifying (pooled_rows, pooled_cols).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, height, width)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        4D tensor with shape `(batch_size, height, width, channels)`.\n      - If `data_format=\'channels_first\'`:\n        4D tensor with shape `(batch_size, channels, height, width)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n      - If `data_format=\'channels_first\'`:\n        4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_mean, output_size, data_format, **kwargs)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveMaxPooling2D(AdaptivePooling2D):\n    """"""Max Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: Tuple of integers specifying (pooled_rows, pooled_cols).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, height, width)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        4D tensor with shape `(batch_size, height, width, channels)`.\n      - If `data_format=\'channels_first\'`:\n        4D tensor with shape `(batch_size, channels, height, width)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n      - If `data_format=\'channels_first\'`:\n        4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_max, output_size, data_format, **kwargs)\n\n\nclass AdaptivePooling3D(tf.keras.layers.Layer):\n    """"""Parent class for 3D pooling layers with adaptive kernel size.\n\n    This class only exists for code reuse. It will never be an exposed API.\n\n    Arguments:\n      reduce_function: The reduction method to apply, e.g. `tf.reduce_max`.\n      output_size: An integer or tuple/list of 3 integers specifying (pooled_dim1, pooled_dim2, pooled_dim3).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)` while `channels_first`\n        corresponds to inputs with shape\n        `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        reduce_function: Callable,\n        output_size: Union[int, Iterable[int]],\n        data_format=None,\n        **kwargs\n    ):\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.reduce_function = reduce_function\n        self.output_size = conv_utils.normalize_tuple(output_size, 3, ""output_size"")\n        super().__init__(**kwargs)\n\n    def call(self, inputs, *args):\n        h_bins = self.output_size[0]\n        w_bins = self.output_size[1]\n        d_bins = self.output_size[2]\n        if self.data_format == ""channels_last"":\n            split_cols = tf.split(inputs, h_bins, axis=1)\n            split_cols = tf.stack(split_cols, axis=1)\n            split_rows = tf.split(split_cols, w_bins, axis=3)\n            split_rows = tf.stack(split_rows, axis=3)\n            split_depth = tf.split(split_rows, d_bins, axis=5)\n            split_depth = tf.stack(split_depth, axis=5)\n            out_vect = self.reduce_function(split_depth, axis=[2, 4, 6])\n        else:\n            split_cols = tf.split(inputs, h_bins, axis=2)\n            split_cols = tf.stack(split_cols, axis=2)\n            split_rows = tf.split(split_cols, w_bins, axis=4)\n            split_rows = tf.stack(split_rows, axis=4)\n            split_depth = tf.split(split_rows, d_bins, axis=6)\n            split_depth = tf.stack(split_depth, axis=6)\n            out_vect = self.reduce_function(split_depth, axis=[3, 5, 7])\n        return out_vect\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        if self.data_format == ""channels_last"":\n            shape = tf.TensorShape(\n                [\n                    input_shape[0],\n                    self.output_size[0],\n                    self.output_size[1],\n                    self.output_size[2],\n                    input_shape[4],\n                ]\n            )\n        else:\n            shape = tf.TensorShape(\n                [\n                    input_shape[0],\n                    input_shape[1],\n                    self.output_size[0],\n                    self.output_size[1],\n                    self.output_size[2],\n                ]\n            )\n\n        return shape\n\n    def get_config(self):\n        config = {\n            ""output_size"": self.output_size,\n            ""data_format"": self.data_format,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveAveragePooling3D(AdaptivePooling3D):\n    """"""Average Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: An integer or tuple/list of 3 integers specifying (pooled_depth, pooled_height, pooled_width).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, height, width)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        5D tensor with shape `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`.\n      - If `data_format=\'channels_first\'`:\n        5D tensor with shape `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        5D tensor with shape `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`.\n      - If `data_format=\'channels_first\'`:\n        5D tensor with shape `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_mean, output_size, data_format, **kwargs)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdaptiveMaxPooling3D(AdaptivePooling3D):\n    """"""Max Pooling with adaptive kernel size.\n\n    Arguments:\n      output_size: An integer or tuple/list of 3 integers specifying (pooled_depth, pooled_height, pooled_width).\n        The new size of output channels.\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, height, width)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        5D tensor with shape `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`.\n      - If `data_format=\'channels_first\'`:\n        5D tensor with shape `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n\n    Output shape:\n      - If `data_format=\'channels_last\'`:\n        5D tensor with shape `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`.\n      - If `data_format=\'channels_first\'`:\n        5D tensor with shape `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, output_size: Union[int, Iterable[int]], data_format=None, **kwargs\n    ):\n        super().__init__(tf.reduce_max, output_size, data_format, **kwargs)\n'"
tensorflow_addons/layers/esn.py,6,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Echo State recurrent Network (ESN) layer.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.rnn import ESNCell\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils.types import (\n    Activation,\n    FloatTensorLike,\n    TensorLike,\n    Initializer,\n)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass ESN(tf.keras.layers.RNN):\n    """"""Echo State Network layer.\n\n    This implements the recurrent layer using the ESNCell.\n\n    This is based on the paper\n        H. Jaeger\n        [""The ""echo state"" approach to analysing and training recurrent neural networks""]\n        (https://www.researchgate.net/publication/215385037).\n        GMD Report148, German National Research Center for Information Technology, 2001.\n\n    Arguments:\n        units: Positive integer, dimensionality of the reservoir.\n        connectivity: Float between 0 and 1.\n            Connection probability between two reservoir units.\n            Default: 0.1.\n        leaky: Float between 0 and 1.\n            Leaking rate of the reservoir.\n            If you pass 1, it\'s the special case the model does not have leaky integration.\n            Default: 1.\n        spectral_radius: Float between 0 and 1.\n            Desired spectral radius of recurrent weight matrix.\n            Default: 0.9.\n        use_norm2: Boolean, whether to use the p-norm function (with p=2) as an upper\n            bound of the spectral radius so that the echo state property is satisfied.\n            It  avoids to compute the eigenvalues which has an exponential complexity.\n            Default: False.\n        use_bias: Boolean, whether the layer uses a bias vector.\n            Default: True.\n        activation: Activation function to use.\n            Default: hyperbolic tangent (`tanh`).\n            If you pass `None`, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            Default: `glorot_uniform`.\n        recurrent_initializer: Initializer for the `recurrent_kernel` weights matrix,\n            used for the linear transformation of the recurrent state.\n            Default: `glorot_uniform`.\n        bias_initializer: Initializer for the bias vector.\n            Default: `zeros`.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n\n    Call arguments:\n        inputs: A 3D tensor.\n        mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n            a given timestep should be masked.\n        training: Python boolean indicating whether the layer should behave in\n            training mode or in inference mode. This argument is passed to the cell\n            when calling it. This is only relevant if `dropout` or\n            `recurrent_dropout` is used.\n        initial_state: List of initial state tensors to be passed to the first\n            call of the cell.\n     """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        connectivity: FloatTensorLike = 0.1,\n        leaky: FloatTensorLike = 1,\n        spectral_radius: FloatTensorLike = 0.9,\n        use_norm2: bool = False,\n        use_bias: bool = True,\n        activation: Activation = ""tanh"",\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        recurrent_initializer: Initializer = ""glorot_uniform"",\n        bias_initializer: Initializer = ""zeros"",\n        return_sequences=False,\n        go_backwards=False,\n        unroll=False,\n        **kwargs\n    ):\n        cell = ESNCell(\n            units,\n            connectivity=connectivity,\n            leaky=leaky,\n            spectral_radius=spectral_radius,\n            use_norm2=use_norm2,\n            use_bias=use_bias,\n            activation=activation,\n            kernel_initializer=kernel_initializer,\n            recurrent_initializer=recurrent_initializer,\n            bias_initializer=bias_initializer,\n            dtype=kwargs.get(""dtype""),\n        )\n        super().__init__(\n            cell,\n            return_sequences=return_sequences,\n            go_backwards=go_backwards,\n            unroll=unroll,\n            **kwargs,\n        )\n\n    def call(self, inputs, mask=None, training=None, initial_state=None):\n        return super().call(\n            inputs,\n            mask=mask,\n            training=training,\n            initial_state=initial_state,\n            constants=None,\n        )\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def connectivity(self):\n        return self.cell.connectivity\n\n    @property\n    def leaky(self):\n        return self.cell.leaky\n\n    @property\n    def spectral_radius(self):\n        return self.cell.spectral_radius\n\n    @property\n    def use_norm2(self):\n        return self.cell.use_norm2\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    def get_config(self):\n        config = {\n            ""units"": self.units,\n            ""connectivity"": self.connectivity,\n            ""leaky"": self.leaky,\n            ""spectral_radius"": self.spectral_radius,\n            ""use_norm2"": self.use_norm2,\n            ""use_bias"": self.use_bias,\n            ""activation"": tf.keras.activations.serialize(self.activation),\n            ""kernel_initializer"": tf.keras.initializers.serialize(\n                self.kernel_initializer\n            ),\n            ""recurrent_initializer"": tf.keras.initializers.serialize(\n                self.recurrent_initializer\n            ),\n            ""bias_initializer"": tf.keras.initializers.serialize(self.bias_initializer),\n        }\n        base_config = super().get_config()\n        del base_config[""cell""]\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n'"
tensorflow_addons/layers/gelu.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements GELU activation.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.activations import gelu\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass GELU(tf.keras.layers.Layer):\n    """"""Gaussian Error Linear Unit.\n\n    A smoother version of ReLU generally used\n    in the BERT or BERT architecture based models.\n    Original paper: https://arxiv.org/abs/1606.08415\n\n    Input shape:\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    Output shape:\n        Same shape as the input.\n    """"""\n\n    @typechecked\n    def __init__(self, approximate: bool = True, **kwargs):\n        super().__init__(**kwargs)\n        self.approximate = approximate\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return gelu(inputs, approximate=self.approximate)\n\n    def get_config(self):\n        config = {""approximate"": self.approximate}\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n'"
tensorflow_addons/layers/maxout.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementing Maxout layer.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass Maxout(tf.keras.layers.Layer):\n    """"""Applies Maxout to the input.\n\n    ""Maxout Networks"" Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron\n    Courville, Yoshua Bengio. https://arxiv.org/abs/1302.4389\n\n    Usually the operation is performed in the filter/channel dimension. This\n    can also be used after Dense layers to reduce number of features.\n\n    Arguments:\n      num_units: Specifies how many features will remain after maxout\n        in the `axis` dimension (usually channel).\n        This must be a factor of number of features.\n      axis: The dimension where max pooling will be performed. Default is the\n        last dimension.\n\n    Input shape:\n      nD tensor with shape: `(batch_size, ..., axis_dim, ...)`.\n\n    Output shape:\n      nD tensor with shape: `(batch_size, ..., num_units, ...)`.\n    """"""\n\n    @typechecked\n    def __init__(self, num_units: int, axis: int = -1, **kwargs):\n        super().__init__(**kwargs)\n        self.num_units = num_units\n        self.axis = axis\n\n    def call(self, inputs):\n        inputs = tf.convert_to_tensor(inputs)\n        shape = inputs.get_shape().as_list()\n        # Dealing with batches with arbitrary sizes\n        for i in range(len(shape)):\n            if shape[i] is None:\n                shape[i] = tf.shape(inputs)[i]\n\n        num_channels = shape[self.axis]\n        if not isinstance(num_channels, tf.Tensor) and num_channels % self.num_units:\n            raise ValueError(\n                ""number of features({}) is not ""\n                ""a multiple of num_units({})"".format(num_channels, self.num_units)\n            )\n\n        if self.axis < 0:\n            axis = self.axis + len(shape)\n        else:\n            axis = self.axis\n        assert axis >= 0, ""Find invalid axis: {}"".format(self.axis)\n\n        expand_shape = shape[:]\n        expand_shape[axis] = self.num_units\n        k = num_channels // self.num_units\n        expand_shape.insert(axis, k)\n\n        outputs = tf.math.reduce_max(\n            tf.reshape(inputs, expand_shape), axis, keepdims=False\n        )\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        input_shape[self.axis] = self.num_units\n        return tf.TensorShape(input_shape)\n\n    def get_config(self):\n        config = {""num_units"": self.num_units, ""axis"": self.axis}\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/layers/multihead_attention.py,31,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport typing\n\nimport tensorflow as tf\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    r""""""\n    MultiHead Attention layer.\n\n    Defines the MultiHead Attention operation as described in\n    [Attention Is All You Need](https://arxiv.org/abs/1706.03762) which takes\n    in the tensors `query`, `key`, and `value`, and returns the dot-product attention\n    between them:\n\n        ```python\n        mha = MultiHeadAttention(head_size=128, num_heads=128)\n\n        query = tf.random.uniform((32, 20, 200)) # (batch_size, query_elements, query_depth)\n        key = tf.random.uniform((32, 15, 300)) # (batch_size, key_elements, key_depth)\n        value = tf.random.uniform((32, 15, 400)) # (batch_size, key_elements, value_depth)\n\n        attention = mha([query, key, value]) # (batch_size, query_elements, value_depth)\n        ```\n\n    If `value` is not given then internally `value = key` will be used:\n\n         ```python\n        mha = MultiHeadAttention(head_size=128, num_heads=128)\n\n        query = tf.random.uniform((32, 20, 200)) # (batch_size, query_elements, query_depth)\n        key = tf.random.uniform((32, 15, 300)) # (batch_size, key_elements, key_depth)\n\n        attention = mha([query, key]) # (batch_size, query_elements, key_depth)\n        ```\n\n    Arguments:\n        head_size: int, dimensionality of the `query`, `key` and `value` tensors\n        after the linear transformation.\n        num_heads: int, number of attention heads.\n        output_size: int, dimensionality of the output space, if `None` then the\n        input dimension of\n        `value` or `key` will be used, default `None`.\n        dropout: float, `rate` parameter for the dropout layer that is\n        applied to attention after softmax,\n        default `0`.\n        use_projection_bias: bool, whether to use a bias term after the linear\n        output projection.\n        return_attn_coef: bool, if `True`, return the attention coefficients as\n        an additional output argument.\n        kernel_initializer: initializer, initializer for the kernel weights.\n        kernel_regularizer: regularizer, regularizer for the kernel weights.\n        kernel_constraint: constraint, constraint for the kernel weights.\n        bias_initializer: initializer, initializer for the bias weights.\n        bias_regularizer: regularizer, regularizer for the bias weights.\n        bias_constraint: constraint, constraint for the bias weights.\n\n    Call Arguments:\n        inputs:  List of `[query, key, value]` where\n            * `query`: Tensor of shape `(..., query_elements, query_depth)`\n            * `key`: `Tensor of shape \'(..., key_elements, key_depth)`\n            * `value`: Tensor of shape `(..., key_elements, value_depth)`, optional, if not given `key` will be used.\n        mask: a binary Tensor of shape `[batch_size?, num_heads?, query_elements, key_elements]`\n        which specifies which query elements can attendo to which key elements,\n        `1` indicates attention and `0` indicates no attention.\n\n    Output shape:\n        * `(..., query_elements, output_size)` if `output_size` is given, else\n        * `(..., query_elements, value_depth)` if `value` is given, else\n        * `(..., query_elements, key_depth)`\n    """"""\n\n    def __init__(\n        self,\n        head_size: int,\n        num_heads: int,\n        output_size: int = None,\n        dropout: float = 0.0,\n        use_projection_bias: bool = True,\n        return_attn_coef: bool = False,\n        kernel_initializer: typing.Union[str, typing.Callable] = ""glorot_uniform"",\n        kernel_regularizer: typing.Union[str, typing.Callable] = None,\n        kernel_constraint: typing.Union[str, typing.Callable] = None,\n        bias_initializer: typing.Union[str, typing.Callable] = ""zeros"",\n        bias_regularizer: typing.Union[str, typing.Callable] = None,\n        bias_constraint: typing.Union[str, typing.Callable] = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        if output_size is not None and output_size < 1:\n            raise ValueError(""output_size must be a positive number"")\n\n        self.head_size = head_size\n        self.num_heads = num_heads\n        self.output_size = output_size\n        self.use_projection_bias = use_projection_bias\n        self.return_attn_coef = return_attn_coef\n\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self._droput_rate = dropout\n\n    def build(self, input_shape):\n\n        num_query_features = input_shape[0][-1]\n        num_key_features = input_shape[1][-1]\n        num_value_features = (\n            input_shape[2][-1] if len(input_shape) > 2 else num_key_features\n        )\n        output_size = (\n            self.output_size if self.output_size is not None else num_value_features\n        )\n\n        self.query_kernel = self.add_weight(\n            name=""query_kernel"",\n            shape=[self.num_heads, num_query_features, self.head_size],\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        self.key_kernel = self.add_weight(\n            name=""key_kernel"",\n            shape=[self.num_heads, num_key_features, self.head_size],\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        self.value_kernel = self.add_weight(\n            name=""value_kernel"",\n            shape=[self.num_heads, num_value_features, self.head_size],\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        self.projection_kernel = self.add_weight(\n            name=""projection_kernel"",\n            shape=[self.num_heads, self.head_size, output_size],\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n\n        if self.use_projection_bias:\n            self.projection_bias = self.add_weight(\n                name=""projection_bias"",\n                shape=[output_size],\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n            )\n        else:\n            self.projection_bias = None\n\n        super().build(input_shape)\n\n    def call(self, inputs, training=None, mask=None):\n\n        # einsum nomenclature\n        # ------------------------\n        # N = query elements\n        # M = key/value elements\n        # H = heads\n        # I = input features\n        # O = output features\n\n        query = inputs[0]\n        key = inputs[1]\n        value = inputs[2] if len(inputs) > 2 else key\n\n        # verify shapes\n        if key.shape[-2] != value.shape[-2]:\n            raise ValueError(\n                ""the number of elements in \'key\' must be equal to the same as the number of elements in \'value\'""\n            )\n\n        if mask is not None:\n            if len(mask.shape) < 2:\n                raise ValueError(""\'mask\' must have atleast 2 dimensions"")\n            if query.shape[-2] != mask.shape[-2]:\n                raise ValueError(\n                    ""mask\'s second to last dimension must be equal to the number of elements in \'query\'""\n                )\n            if key.shape[-2] != mask.shape[-1]:\n                raise ValueError(\n                    ""mask\'s last dimension must be equal to the number of elements in \'key\'""\n                )\n\n        # Linear transformations\n        query = tf.einsum(""...NI , HIO -> ...NHO"", query, self.query_kernel)\n        key = tf.einsum(""...MI , HIO -> ...MHO"", key, self.key_kernel)\n        value = tf.einsum(""...MI , HIO -> ...MHO"", value, self.value_kernel)\n\n        # Scale dot-product, doing the division to either query or key\n        # instead of their product saves some computation\n        depth = tf.constant(self.head_size, dtype=tf.float32)\n        query /= tf.sqrt(depth)\n\n        # Calculate dot product attention\n        logits = tf.einsum(""...NHO,...MHO->...HNM"", query, key)\n\n        # apply mask\n        if mask is not None:\n            mask = tf.cast(mask, tf.float32)\n\n            # possibly expand on the head dimension so broadcasting works\n            if len(mask.shape) != len(logits.shape):\n                mask = tf.expand_dims(mask, -3)\n\n            logits += -10e9 * (1.0 - mask)\n\n        attn_coef = tf.nn.softmax(logits)\n\n        # attention dropout\n        attn_coef_dropout = self.dropout(attn_coef, training=training)\n\n        # attention * value\n        multihead_output = tf.einsum(""...HNM,...MHI->...NHI"", attn_coef_dropout, value)\n\n        # Run the outputs through another linear projection layer. Recombining heads\n        # is automatically done.\n        output = tf.einsum(\n            ""...NHI,HIO->...NO"", multihead_output, self.projection_kernel\n        )\n\n        if self.projection_bias is not None:\n            output += self.projection_bias\n\n        if self.return_attn_coef:\n            return output, attn_coef\n        else:\n            return output\n\n    def compute_output_shape(self, input_shape):\n        num_value_features = (\n            input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]\n        )\n        output_size = (\n            self.output_size if self.output_size is not None else num_value_features\n        )\n\n        output_shape = input_shape[0][:-1] + (output_size,)\n\n        if self.return_attn_coef:\n            num_query_elements = input_shape[0][-2]\n            num_key_elements = input_shape[1][-2]\n            attn_coef_shape = input_shape[0][:-2] + (\n                self.num_heads,\n                num_query_elements,\n                num_key_elements,\n            )\n\n            return output_shape, attn_coef_shape\n        else:\n            return output_shape\n\n    def get_config(self):\n        config = super().get_config()\n\n        config.update(\n            head_size=self.head_size,\n            num_heads=self.num_heads,\n            output_size=self.output_size,\n            dropout=self._droput_rate,\n            use_projection_bias=self.use_projection_bias,\n            return_attn_coef=self.return_attn_coef,\n            kernel_initializer=tf.keras.initializers.serialize(self.kernel_initializer),\n            kernel_regularizer=tf.keras.regularizers.serialize(self.kernel_regularizer),\n            kernel_constraint=tf.keras.constraints.serialize(self.kernel_constraint),\n            bias_initializer=tf.keras.initializers.serialize(self.bias_initializer),\n            bias_regularizer=tf.keras.regularizers.serialize(self.bias_regularizer),\n            bias_constraint=tf.keras.constraints.serialize(self.bias_constraint),\n        )\n\n        return config\n'"
tensorflow_addons/layers/netvlad.py,18,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""NetVLAD keras layer.""""""\n\nimport math\nimport tensorflow as tf\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass NetVLAD(tf.keras.layers.Layer):\n    """"""Applies NetVLAD to the input.\n\n        This is a fully-differentiable version of ""Vector of Locally Aggregated Descriptors"" commonly used in image\n        retrieval. It is also used in audio retrieval, and audio represenation learning (ex\n        ""Towards Learning a Universal Non-Semantic Representation of Speech"", https://arxiv.org/abs/2002.12764).\n\n        ""NetVLAD: CNN architecture for weakly supervised place recognition""\n        Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic.\n        https://arxiv.org/abs/1511.07247\n\n    Arguments:\n        num_clusters: The number of clusters to use.\n    Input shape:\n        3D tensor with shape: `(batch_size, time, feature_dim)`.\n    Output shape:\n        2D tensor with shape: `(batch_size, feature_dim * num_clusters)`.\n    """"""\n\n    @typechecked\n    def __init__(self, num_clusters: int, **kwargs):\n        super().__init__(**kwargs)\n        if num_clusters <= 0:\n            raise ValueError(""`num_clusters` must be greater than 1: %i"" % num_clusters)\n        self.num_clusters = num_clusters\n\n    def build(self, input_shape):\n        """"""Keras build method.""""""\n        feature_dim = input_shape[-1]\n        if not isinstance(feature_dim, int):\n            feature_dim = feature_dim.value\n        self.fc = tf.keras.layers.Dense(\n            units=self.num_clusters,\n            activation=tf.nn.softmax,\n            kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n        )\n        self.cluster_centers = self.add_weight(\n            name=""cluster_centers"",\n            shape=(1, feature_dim, self.num_clusters),\n            initializer=tf.keras.initializers.TruncatedNormal(\n                stddev=1.0 / math.sqrt(feature_dim)\n            ),\n            trainable=True,\n        )\n        super(NetVLAD, self).build(input_shape)\n\n    def call(self, frames):\n        """"""Apply the NetVLAD module to the given frames.\n\n        Args:\n            frames: A tensor with shape [batch_size, max_frames, feature_dim].\n\n        Returns:\n            A tensor with shape [batch_size, feature_dim * num_clusters].\n\n        Raises:\n            ValueError: If the `feature_dim` of input is not defined.\n        """"""\n        frames.shape.assert_has_rank(3)\n        feature_dim = frames.shape.as_list()[-1]\n        if feature_dim is None:\n            raise ValueError(""Last dimension must be defined."")\n        max_frames = tf.shape(frames)[-2]\n\n        # Compute soft-assignment from frames to clusters.\n        # Essentially: softmax(w*x + b), although BN can be used instead of bias.\n        frames = tf.reshape(frames, (-1, feature_dim))\n        activation = self.fc(frames)\n        activation = tf.reshape(activation, (-1, max_frames, self.num_clusters))\n\n        # Soft-count of number of frames assigned to each cluster.\n        # Output shape: [batch_size, 1, num_clusters]\n        a_sum = tf.math.reduce_sum(activation, axis=-2, keepdims=True)\n\n        # Compute sum_{i=1}^N softmax(w_k * x_i + b_k) * c_k(j),\n        # for all clusters and dimensions.\n        # Output shape: [batch_size, feature_dim, num_clusters]\n        a = a_sum * self.cluster_centers\n\n        # Compute sum_{i=1}^N softmax(w_k * x_i + b_k) * x_i(j),\n        # for all clusters and dimensions.\n        # Output shape: (batch_size, feature_dim, num_clusters)\n        frames = tf.reshape(frames, (-1, max_frames, feature_dim))\n        b = tf.transpose(\n            tf.matmul(tf.transpose(activation, perm=(0, 2, 1)), frames), perm=(0, 2, 1)\n        )\n\n        # Output shape: (batch_size, feature_dim, num_clusters)\n        vlad = b - a\n\n        # Normalize first across the feature dimensions.\n        vlad = tf.nn.l2_normalize(vlad, 1)\n\n        # Output shape: [batch_size, feature_dim * num_clusters]\n        vlad = tf.reshape(vlad, (-1, feature_dim * self.num_clusters))\n\n        # Renormalize across both the feature dimensions (already normalized) and\n        # the cluster centers.\n        vlad = tf.nn.l2_normalize(vlad, 1)\n\n        return vlad\n\n    def compute_output_shape(self, input_shape):\n        input_shape = tf.TensorShape(input_shape).as_list()\n        return tf.TensorShape([input_shape[0], input_shape[-1] * self.num_clusters])\n\n    def get_config(self):\n        config = {""num_clusters"": self.num_clusters}\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
tensorflow_addons/layers/normalizations.py,52,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Orginal implementation from keras_contrib/layer/normalization\n# =============================================================================\n\nimport logging\nimport tensorflow as tf\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils import types\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass GroupNormalization(tf.keras.layers.Layer):\n    """"""Group normalization layer.\n\n    Group Normalization divides the channels into groups and computes\n    within each group the mean and variance for normalization.\n    Empirically, its accuracy is more stable than batch norm in a wide\n    range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n\n    Relation to Layer Normalization:\n    If the number of groups is set to 1, then this operation becomes identical\n    to Layer Normalization.\n\n    Relation to Instance Normalization:\n    If the number of groups is set to the\n    input dimension (number of groups is equal\n    to number of channels), then this operation becomes\n    identical to Instance Normalization.\n\n    Arguments\n        groups: Integer, the number of groups for Group Normalization.\n            Can be in the range [1, N] where N is the input dimension.\n            The input dimension must be divisible by the number of groups.\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    Output shape\n        Same shape as input.\n    References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        groups: int = 2,\n        axis: int = -1,\n        epsilon: float = 1e-3,\n        center: bool = True,\n        scale: bool = True,\n        beta_initializer: types.Initializer = ""zeros"",\n        gamma_initializer: types.Initializer = ""ones"",\n        beta_regularizer: types.Regularizer = None,\n        gamma_regularizer: types.Regularizer = None,\n        beta_constraint: types.Constraint = None,\n        gamma_constraint: types.Constraint = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self._check_axis()\n\n    def build(self, input_shape):\n\n        self._check_if_input_shape_is_none(input_shape)\n        self._set_number_of_groups_for_instance_norm(input_shape)\n        self._check_size_of_dimensions(input_shape)\n        self._create_input_spec(input_shape)\n\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        self.built = True\n        super().build(input_shape)\n\n    def call(self, inputs):\n\n        input_shape = tf.keras.backend.int_shape(inputs)\n        tensor_input_shape = tf.shape(inputs)\n\n        reshaped_inputs, group_shape = self._reshape_into_groups(\n            inputs, input_shape, tensor_input_shape\n        )\n\n        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n\n        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            ""groups"": self.groups,\n            ""axis"": self.axis,\n            ""epsilon"": self.epsilon,\n            ""center"": self.center,\n            ""scale"": self.scale,\n            ""beta_initializer"": tf.keras.initializers.serialize(self.beta_initializer),\n            ""gamma_initializer"": tf.keras.initializers.serialize(\n                self.gamma_initializer\n            ),\n            ""beta_regularizer"": tf.keras.regularizers.serialize(self.beta_regularizer),\n            ""gamma_regularizer"": tf.keras.regularizers.serialize(\n                self.gamma_regularizer\n            ),\n            ""beta_constraint"": tf.keras.constraints.serialize(self.beta_constraint),\n            ""gamma_constraint"": tf.keras.constraints.serialize(self.gamma_constraint),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n\n        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n        group_shape[self.axis] = input_shape[self.axis] // self.groups\n        group_shape.insert(self.axis, self.groups)\n        group_shape = tf.stack(group_shape)\n        reshaped_inputs = tf.reshape(inputs, group_shape)\n        return reshaped_inputs, group_shape\n\n    def _apply_normalization(self, reshaped_inputs, input_shape):\n\n        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n        group_reduction_axes = list(range(1, len(group_shape)))\n        axis = -2 if self.axis == -1 else self.axis - 1\n        group_reduction_axes.pop(axis)\n\n        mean, variance = tf.nn.moments(\n            reshaped_inputs, group_reduction_axes, keepdims=True\n        )\n\n        gamma, beta = self._get_reshaped_weights(input_shape)\n        normalized_inputs = tf.nn.batch_normalization(\n            reshaped_inputs,\n            mean=mean,\n            variance=variance,\n            scale=gamma,\n            offset=beta,\n            variance_epsilon=self.epsilon,\n        )\n        return normalized_inputs\n\n    def _get_reshaped_weights(self, input_shape):\n        broadcast_shape = self._create_broadcast_shape(input_shape)\n        gamma = None\n        beta = None\n        if self.scale:\n            gamma = tf.reshape(self.gamma, broadcast_shape)\n\n        if self.center:\n            beta = tf.reshape(self.beta, broadcast_shape)\n        return gamma, beta\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim = input_shape[self.axis]\n        if dim is None:\n            raise ValueError(\n                ""Axis "" + str(self.axis) + "" of ""\n                ""input tensor should have a defined dimension ""\n                ""but the layer received an input with shape "" + str(input_shape) + "".""\n            )\n\n    def _set_number_of_groups_for_instance_norm(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if self.groups == -1:\n            self.groups = dim\n\n    def _check_size_of_dimensions(self, input_shape):\n\n        dim = input_shape[self.axis]\n        if dim < self.groups:\n            raise ValueError(\n                ""Number of groups ("" + str(self.groups) + "") cannot be ""\n                ""more than the number of channels ("" + str(dim) + "").""\n            )\n\n        if dim % self.groups != 0:\n            raise ValueError(\n                ""Number of groups ("" + str(self.groups) + "") must be a ""\n                ""multiple of the number of channels ("" + str(dim) + "").""\n            )\n\n    def _check_axis(self):\n\n        if self.axis == 0:\n            raise ValueError(\n                ""You are trying to normalize your batch axis. Do you want to ""\n                ""use tf.layer.batch_normalization instead""\n            )\n\n    def _create_input_spec(self, input_shape):\n\n        dim = input_shape[self.axis]\n        self.input_spec = tf.keras.layers.InputSpec(\n            ndim=len(input_shape), axes={self.axis: dim}\n        )\n\n    def _add_gamma_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                name=""gamma"",\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n            )\n        else:\n            self.gamma = None\n\n    def _add_beta_weight(self, input_shape):\n\n        dim = input_shape[self.axis]\n        shape = (dim,)\n\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                name=""beta"",\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n            )\n        else:\n            self.beta = None\n\n    def _create_broadcast_shape(self, input_shape):\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n        broadcast_shape.insert(self.axis, self.groups)\n        return broadcast_shape\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass InstanceNormalization(GroupNormalization):\n    """"""Instance normalization layer.\n\n    Instance Normalization is an specific case of ```GroupNormalization```since\n    it normalizes all features of one channel. The Groupsize is equal to the\n    channel size. Empirically, its accuracy is more stable than batch norm in a\n    wide range of small batch sizes, if learning rate is adjusted linearly\n    with batch sizes.\n\n    Arguments\n        axis: Integer, the axis that should be normalized.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    Output shape\n        Same shape as input.\n\n    References\n        - [Instance Normalization: The Missing Ingredient for Fast Stylization]\n        (https://arxiv.org/abs/1607.08022)\n    """"""\n\n    def __init__(self, **kwargs):\n        if ""groups"" in kwargs:\n            logging.warning(""The given value for groups will be overwritten."")\n\n        kwargs[""groups""] = -1\n        super().__init__(**kwargs)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass FilterResponseNormalization(tf.keras.layers.Layer):\n    """"""Filter response normalization layer.\n\n    Filter Response Normalization (FRN), a normalization\n    method that enables models trained with per-channel\n    normalization to achieve high accuracy. It performs better than\n    all other normalization techniques for small batches and is par\n    with Batch Normalization for bigger batch sizes.\n\n    Arguments\n        axis: List of axes that should be normalized. This should represent the\n              spatial dimensions.\n        epsilon: Small positive float value added to variance to avoid dividing by zero.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n        learned_epsilon: (bool) Whether to add another learnable\n        epsilon parameter or not.\n        name: Optional name for the layer\n\n    Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model. This layer, as of now,\n        works on a 4-D tensor where the tensor should have the shape [N X H X W X C]\n\n        TODO: Add support for NCHW data format and FC layers.\n\n    Output shape\n        Same shape as input.\n\n    References\n        - [Filter Response Normalization Layer: Eliminating Batch Dependence\n        in the training of Deep Neural Networks]\n        (https://arxiv.org/abs/1911.09737)\n    """"""\n\n    def __init__(\n        self,\n        epsilon: float = 1e-6,\n        axis: list = [1, 2],\n        beta_initializer: types.Initializer = ""zeros"",\n        gamma_initializer: types.Initializer = ""ones"",\n        beta_regularizer: types.Regularizer = None,\n        gamma_regularizer: types.Regularizer = None,\n        beta_constraint: types.Constraint = None,\n        gamma_constraint: types.Constraint = None,\n        learned_epsilon: bool = False,\n        learned_epsilon_constraint: types.Constraint = None,\n        name: str = None,\n        **kwargs\n    ):\n        super().__init__(name=name, **kwargs)\n        self.epsilon = tf.math.abs(tf.cast(epsilon, dtype=self.dtype))\n        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n        self.use_eps_learned = learned_epsilon\n        self.supports_masking = True\n\n        if self.use_eps_learned:\n            self.eps_learned_initializer = tf.keras.initializers.Constant(1e-4)\n            self.eps_learned_constraint = tf.keras.constraints.get(\n                learned_epsilon_constraint\n            )\n            self.eps_learned = self.add_weight(\n                shape=(1,),\n                name=""learned_epsilon"",\n                dtype=self.dtype,\n                initializer=tf.keras.initializers.get(self.eps_learned_initializer),\n                regularizer=None,\n                constraint=self.eps_learned_constraint,\n            )\n        else:\n            self.eps_learned_initializer = None\n            self.eps_learned_constraint = None\n\n        self._check_axis(axis)\n\n    def build(self, input_shape):\n        if len(tf.TensorShape(input_shape)) != 4:\n            raise ValueError(\n                """"""Only 4-D tensors (CNNs) are supported\n        as of now.""""""\n            )\n        self._check_if_input_shape_is_none(input_shape)\n        self._create_input_spec(input_shape)\n        self._add_gamma_weight(input_shape)\n        self._add_beta_weight(input_shape)\n        super().build(input_shape)\n\n    def call(self, inputs):\n        epsilon = self.epsilon\n        if self.use_eps_learned:\n            epsilon += tf.math.abs(self.eps_learned)\n        nu2 = tf.reduce_mean(tf.square(inputs), axis=self.axis, keepdims=True)\n        normalized_inputs = inputs * tf.math.rsqrt(nu2 + epsilon)\n        return self.gamma * normalized_inputs + self.beta\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            ""axis"": self.axis,\n            ""epsilon"": self.epsilon,\n            ""learned_epsilon"": self.use_eps_learned,\n            ""beta_initializer"": tf.keras.initializers.serialize(self.beta_initializer),\n            ""gamma_initializer"": tf.keras.initializers.serialize(\n                self.gamma_initializer\n            ),\n            ""beta_regularizer"": tf.keras.regularizers.serialize(self.beta_regularizer),\n            ""gamma_regularizer"": tf.keras.regularizers.serialize(\n                self.gamma_regularizer\n            ),\n            ""beta_constraint"": tf.keras.constraints.serialize(self.beta_constraint),\n            ""gamma_constraint"": tf.keras.constraints.serialize(self.gamma_constraint),\n            ""learned_epsilon_constraint"": tf.keras.constraints.serialize(\n                self.eps_learned_constraint\n            ),\n        }\n        base_config = super().get_config()\n        return dict(**base_config, **config)\n\n    def _create_input_spec(self, input_shape):\n        ndims = len(tf.TensorShape(input_shape))\n        for idx, x in enumerate(self.axis):\n            if x < 0:\n                self.axis[idx] = ndims + x\n\n        # Validate axes\n        for x in self.axis:\n            if x < 0 or x >= ndims:\n                raise ValueError(""Invalid axis: %d"" % x)\n\n        if len(self.axis) != len(set(self.axis)):\n            raise ValueError(""Duplicate axis: %s"" % self.axis)\n\n        axis_to_dim = {x: input_shape[x] for x in self.axis}\n        self.input_spec = tf.keras.layers.InputSpec(ndim=ndims, axes=axis_to_dim)\n\n    def _check_axis(self, axis):\n        if not isinstance(axis, list):\n            raise TypeError(\n                """"""Expected a list of values but got {}."""""".format(type(axis))\n            )\n        else:\n            self.axis = axis\n\n        if self.axis != [1, 2]:\n            raise ValueError(\n                """"""FilterResponseNormalization operates on per-channel basis.\n                Axis values should be a list of spatial dimensions.""""""\n            )\n\n    def _check_if_input_shape_is_none(self, input_shape):\n        dim1, dim2 = input_shape[self.axis[0]], input_shape[self.axis[1]]\n        if dim1 is None or dim2 is None:\n            raise ValueError(\n                """"""Axis {} of input tensor should have a defined dimension but\n                the layer received an input with shape {}."""""".format(\n                    self.axis, input_shape\n                )\n            )\n\n    def _add_gamma_weight(self, input_shape):\n        # Get the channel dimension\n        dim = input_shape[-1]\n        shape = [1, 1, 1, dim]\n        # Initialize gamma with shape (1, 1, 1, C)\n        self.gamma = self.add_weight(\n            shape=shape,\n            name=""gamma"",\n            dtype=self.dtype,\n            initializer=self.gamma_initializer,\n            regularizer=self.gamma_regularizer,\n            constraint=self.gamma_constraint,\n        )\n\n    def _add_beta_weight(self, input_shape):\n        # Get the channel dimension\n        dim = input_shape[-1]\n        shape = [1, 1, 1, dim]\n        # Initialize beta with shape (1, 1, 1, C)\n        self.beta = self.add_weight(\n            shape=shape,\n            name=""beta"",\n            dtype=self.dtype,\n            initializer=self.beta_initializer,\n            regularizer=self.beta_regularizer,\n            constraint=self.beta_constraint,\n        )\n'"
tensorflow_addons/layers/optical_flow.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tensorflow op performing correlation cost operation.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\nfrom tensorflow_addons.utils.resource_loader import LazySO\n\n_correlation_cost_so = LazySO(""custom_ops/layers/_correlation_cost_ops.so"")\n\n\ndef _correlation_cost(\n    input_a,\n    input_b,\n    kernel_size,\n    max_displacement,\n    stride_1,\n    stride_2,\n    pad,\n    data_format=""channels_last"",\n    name=None,\n):\n    """"""Correlation Cost Volume computation.\n\n    ""FlowNet: Learning Optical Flow with Convolutional Networks""\n    Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Hausser,\n    Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt,\n    Daniel Cremers, Thomas Brox. https://arxiv.org/abs/1504.06852\n\n    Computes a cost volume using correlation for two inputs. For feature\n    maps A, B with spatial dimensions w, h, c it computes\n\n      output(a, b) = sum_{l in [-k,k]**2}  < I(a+l), J(b+l) >\n\n    where the patches of size K=2d + 1 are centered in position a resp. b.\n\n    The output shape is [B, C\', H\', W\'], where\n\n      r = max_displacement / stride_2;\n      bd = max_displacement + (kernel_size - 1) / 2\n      C\' = (2 * r + 1) ** 2\n      H\' = H + 2 * (pad - bd) / stride_1\n      W\' = W + 2 * (pad - bd) / stride_1\n\n    Note: When the data_format requests ""channels_last"", an additional explicit\n      transpose operation is executed.\n\n    Args:\n      input_a: A `Tensor` of the format specified by `data_format`.\n      input_b: A `Tensor` of the format specified by `data_format`.\n      kernel_size: An integer specifying the height and width of the\n          patch used to compute the per-patch costs.\n      max_displacement: An integer specifying the maximum search radius\n          for each position.\n      stride_1: An integer specifying the stride length in the input.\n      stride_2: An integer specifying the stride length in the patch.\n      pad: An integer specifying the paddings in height and width.\n      data_format: Specifies the data format.\n          Possible values are:\n          ""channels_last"" float [batch, height, width, channels]\n          ""channels_first"" float [batch, channels, height, width]\n          Defaults to `""channels_last""`.\n      name: A name for the operation (optional).\n\n    Returns:\n      A `Tensor` of the format specified by `data_format`.\n    """"""\n\n    with tf.name_scope(name or ""correlation_cost""):\n        op_call = _correlation_cost_so.ops.addons_correlation_cost\n\n        if data_format == ""channels_last"":\n            op_data_format = ""NHWC""\n        elif data_format == ""channels_first"":\n            op_data_format = ""NCHW""\n        else:\n            raise ValueError(\n                ""`data_format` must be either `channels_last` or"" ""`channels_first`""\n            )\n\n        ret = op_call(\n            input_a,\n            input_b,\n            kernel_size=kernel_size,\n            max_displacement=max_displacement,\n            stride_1=stride_1,\n            stride_2=stride_2,\n            pad=pad,\n            data_format=op_data_format,\n        )\n        if data_format == ""channels_last"":\n            # this is easier to maintain without\n            # specializing an additional cuda kernel\n            return tf.transpose(ret, [0, 2, 3, 1])\n        return ret\n\n\n@tf.RegisterGradient(""Addons>CorrelationCost"")\ndef _correlation_cost_grad(op, grad_output):\n    kernel_size = op.get_attr(""kernel_size"")\n    max_displacement = op.get_attr(""max_displacement"")\n    stride_1 = op.get_attr(""stride_1"")\n    stride_2 = op.get_attr(""stride_2"")\n    pad = op.get_attr(""pad"")\n    data_format = op.get_attr(""data_format"")\n\n    input_a = tf.convert_to_tensor(op.inputs[0], name=""input_a"")\n    input_b = tf.convert_to_tensor(op.inputs[1], name=""input_b"")\n    grad_output_tensor = tf.convert_to_tensor(grad_output, name=""grad_output"")\n\n    op_call = _correlation_cost_so.ops.addons_correlation_cost_grad\n    grads = op_call(\n        input_a,\n        input_b,\n        grad_output_tensor,\n        kernel_size=kernel_size,\n        max_displacement=max_displacement,\n        stride_1=stride_1,\n        stride_2=stride_2,\n        pad=pad,\n        data_format=data_format,\n    )\n\n    grad_input_a = tf.convert_to_tensor(grads[0], name=""grad_input_a"")\n    grad_input_b = tf.convert_to_tensor(grads[1], name=""grad_input_b"")\n    return [grad_input_a, grad_input_b]\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass CorrelationCost(tf.keras.layers.Layer):\n    """"""Correlation Cost Layer.\n\n    This layer implements the correlation operation from FlowNet Learning\n    Optical Flow with Convolutional Networks (Fischer et al.):\n    https://arxiv.org/abs/1504.06\n\n    Args:\n        kernel_size: An integer specifying the height and width of the\n            patch used to compute the per-patch costs.\n        max_displacement: An integer specifying the maximum search radius\n            for each position.\n        stride_1: An integer specifying the stride length in the input.\n        stride_2: An integer specifying the stride length in the patch.\n        pad: An integer specifying the paddings in height and width.\n        data_format: Specifies the data format.\n            Possible values are:\n                ""channels_last"" float [batch, height, width, channels]\n                ""channels_first"" float [batch, channels, height, width]\n                Defaults to `""channels_last""`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        kernel_size: int,\n        max_displacement: int,\n        stride_1: int,\n        stride_2: int,\n        pad: int,\n        data_format: str,\n        **kwargs\n    ):\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride_1 = stride_1\n        self.stride_2 = stride_2\n        self.pad = pad\n\n        if data_format != ""channels_last"" and data_format != ""channels_first"":\n            raise ValueError(\n                ""`data_format` must be either `channels_last` or""\n                ""`channels_first`, instead got %s"" % data_format\n            )\n\n        self.data_format = data_format\n\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        if not isinstance(input_shape, list):\n            raise ValueError(""Input must be a list of two Tensors to process"")\n        super().build(input_shape)\n\n    def call(self, inputs):\n        if not isinstance(inputs, list):\n            raise ValueError(""Input must be a list of two Tensors to process"")\n\n        input_a = tf.convert_to_tensor(inputs[0])\n        input_b = tf.convert_to_tensor(inputs[1])\n\n        return _correlation_cost(\n            input_a,\n            input_b,\n            kernel_size=self.kernel_size,\n            max_displacement=self.max_displacement,\n            stride_1=self.stride_1,\n            stride_2=self.stride_2,\n            pad=self.pad,\n            data_format=self.data_format,\n        )\n\n    def compute_output_shape(self, input_shape):\n        assert isinstance(input_shape, list)\n\n        #  Input validation\n        if len(input_shape) != 2:\n            raise ValueError(""Input must be a list of two shapes"")\n\n        for idx in range(4):\n            if input_shape[0][idx] != input_shape[1][idx]:\n                raise ValueError(""Input shapes must match"")\n\n        n = input_shape[0][0]\n        r = self.max_displacement // self.stride_2\n        bd = self.max_displacement + (self.kernel_size - 1) // 2\n        output_c = (2 * r + 1) ** 2\n\n        if self.data_format == ""channels_first"":\n            output_h = input_shape[0][2] + 2 * (self.pad - bd) // self.stride_1\n            output_w = input_shape[0][3] + 2 * (self.pad - bd) // self.stride_1\n            return [(n, output_c, output_h, output_w)]\n\n        elif self.data_format == ""channels_last"":\n            output_h = input_shape[0][1] + 2 * (self.pad - bd) // self.stride_1\n            output_w = input_shape[0][2] + 2 * (self.pad - bd) // self.stride_1\n            return [(n, output_h, output_w, output_c)]\n        else:\n            raise ValueError(\n                ""`data_format` must be either `channels_last` or"" ""`channels_first`""\n            )\n\n    def get_config(self):\n        config = {\n            ""kernel_size"": self.kernel_size,\n            ""max_displacement"": self.max_displacement,\n            ""stride_1"": self.stride_1,\n            ""stride_2"": self.stride_2,\n            ""pad"": self.pad,\n            ""data_format"": self.data_format,\n        }\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/layers/poincare.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementing PoincareNormalize layer.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\nfrom typing import Union, List\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass PoincareNormalize(tf.keras.layers.Layer):\n    """"""Project into the Poincare ball with norm <= 1.0 - epsilon.\n\n    https://en.wikipedia.org/wiki/Poincare_ball_model\n\n    Used in Poincare Embeddings for Learning Hierarchical Representations\n    Maximilian Nickel, Douwe Kiela https://arxiv.org/pdf/1705.08039.pdf\n\n    For a 1-D tensor with `axis = 0`, computes\n\n                  (x * (1 - epsilon)) / ||x||     if ||x|| > 1 - epsilon\n        output =\n                   x                              otherwise\n\n    For `x` with more dimensions, independently normalizes each 1-D slice along\n    dimension `axis`.\n\n    Arguments:\n      axis: Axis along which to normalize.  A scalar or a vector of integers.\n      epsilon: A small deviation from the edge of the unit sphere for\n        numerical stability.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, axis: Union[None, int, List[int]] = 1, epsilon: float = 1e-5, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.axis = axis\n        self.epsilon = epsilon\n\n    def call(self, inputs):\n        x = tf.convert_to_tensor(inputs)\n        square_sum = tf.math.reduce_sum(tf.math.square(x), self.axis, keepdims=True)\n        x_inv_norm = tf.math.rsqrt(square_sum)\n        x_inv_norm = tf.math.minimum((1.0 - self.epsilon) * x_inv_norm, 1.0)\n        outputs = tf.math.multiply(x, x_inv_norm)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {""axis"": self.axis, ""epsilon"": self.epsilon}\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/layers/polynomial.py,19,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Polynomial Crossing Layer.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils import types\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass PolynomialCrossing(tf.keras.layers.Layer):\n    """"""Layer for Deep & Cross Network to learn explicit feature interactions.\n\n    A layer that applies feature crossing in learning certain explicit\n    bounded-degree feature interactions more efficiently. The `call` method\n    accepts `inputs` as a tuple of size 2 tensors. The first input `x0` should be\n    the input to the first `PolynomialCrossing` layer in the stack, or the input\n    to the network (usually after the embedding layer), the second input `xi`\n    is the output of the previous `PolynomialCrossing` layer in the stack, i.e.,\n    the i-th `PolynomialCrossing` layer.\n\n    The output is x_{i+1} = x0 .* (W * x_i + diag_scale * x_i) + bias + xi, where .* designates elementwise\n    multiplication, W could be a full rank matrix, or a low rank matrix U*V to reduce the computational cost,\n    and diag_scale increases the diagonal of W to improve training stability (especially for the low rank case).\n\n    References\n        See [R. Wang](https://arxiv.org/pdf/1708.05123.pdf)\n\n    Example:\n\n        ```python\n        # after embedding layer in a functional model:\n        input = tf.keras.Input(shape=(None,), name=\'index\', dtype=tf.int64)\n        x0 = tf.keras.layers.Embedding(input_dim=32, output_dim=6))\n        x1 = PolynomialCrossing(projection_dim=None)((x0, x0))\n        x2 = PolynomialCrossing(projection_dim=None)((x0, x1))\n        logits = tf.keras.layers.Dense(units=10)(x2)\n        model = tf.keras.Model(input, logits)\n        ```\n\n    Arguments:\n        projection_dim: project dimension to reduce the computational cost.\n          Default is `None` such that a full (`input_dim` by `input_dim`)\n          matrix W is used. If enabled, a low-rank matrix W = U*V will be used,\n          where U is of size `input_dim` by `projection_dim` and V is of size\n          `projection_dim` by `input_dim`. `projection_dim` need to be smaller\n          than `input_dim`/2 to improve the model efficiency.\n        diag_scale: a non-negative float used to increase the diagonal of the\n           kernel W by `diag_scale`.\n        use_bias: whether to calculate the bias/intercept for this layer. If set to\n          False, no bias/intercept will be used in calculations, e.g., the data is\n          already centered.\n        kernel_initializer: Initializer instance to use on the kernel matrix.\n        bias_initializer: Initializer instance to use on the bias vector.\n        kernel_regularizer: Regularizer instance to use on the kernel matrix.\n        bias_regularizer: Regularizer instance to use on bias vector.\n\n    Input shape:\n        A tuple of 2 (batch_size, `input_dim`) dimensional inputs.\n\n    Output shape:\n        A single (batch_size, `input_dim`) dimensional output.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        projection_dim: int = None,\n        diag_scale: float = 0.0,\n        use_bias: bool = True,\n        kernel_initializer: types.Initializer = ""truncated_normal"",\n        bias_initializer: types.Initializer = ""zeros"",\n        kernel_regularizer: types.Regularizer = None,\n        bias_regularizer: types.Regularizer = None,\n        **kwargs\n    ):\n        super(PolynomialCrossing, self).__init__(**kwargs)\n\n        self.projection_dim = projection_dim\n        self.diag_scale = diag_scale\n        self.use_bias = use_bias\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        if not isinstance(input_shape, (tuple, list)) or len(input_shape) != 2:\n            raise ValueError(\n                ""Input shapes must be a tuple or list of size 2, ""\n                ""got {}"".format(input_shape)\n            )\n        last_dim = input_shape[-1][-1]\n        if self.projection_dim is None:\n            self.kernel = self.add_weight(\n                ""kernel"",\n                shape=[last_dim, last_dim],\n                initializer=self.kernel_initializer,\n                regularizer=self.kernel_regularizer,\n                dtype=self.dtype,\n                trainable=True,\n            )\n        else:\n            if self.projection_dim < 0 or self.projection_dim > last_dim / 2:\n                raise ValueError(\n                    ""`projection_dim` should be smaller than last_dim / 2 to improve""\n                    ""the model efficiency, and should be positive. Got ""\n                    ""`projection_dim` {}, and last dimension of input {}"".format(\n                        self.projection_dim, last_dim\n                    )\n                )\n            self.kernel_u = self.add_weight(\n                ""kernel_u"",\n                shape=[last_dim, self.projection_dim],\n                initializer=self.kernel_initializer,\n                regularizer=self.kernel_regularizer,\n                dtype=self.dtype,\n                trainable=True,\n            )\n            self.kernel_v = self.add_weight(\n                ""kernel_v"",\n                shape=[self.projection_dim, last_dim],\n                initializer=self.kernel_initializer,\n                regularizer=self.kernel_regularizer,\n                dtype=self.dtype,\n                trainable=True,\n            )\n        if self.use_bias:\n            self.bias = self.add_weight(\n                ""bias"",\n                shape=[last_dim],\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                dtype=self.dtype,\n                trainable=True,\n            )\n        self.built = True\n\n    def call(self, inputs):\n        if not isinstance(inputs, (tuple, list)) or len(inputs) != 2:\n            raise ValueError(\n                ""Inputs to the layer must be a tuple or list of size 2, ""\n                ""got {}"".format(inputs)\n            )\n        x0, x = inputs\n        if self.projection_dim is None:\n            prod_output = tf.matmul(x, self.kernel)\n        else:\n            prod_output = tf.matmul(x, self.kernel_u)\n            prod_output = tf.matmul(prod_output, self.kernel_v)\n        if self.diag_scale:\n            prod_output = tf.add(prod_output, self.diag_scale * x)\n        outputs = x0 * prod_output + x\n        if self.use_bias:\n            outputs = tf.add(outputs, self.bias)\n        return outputs\n\n    def get_config(self):\n        config = {\n            ""projection_dim"": self.projection_dim,\n            ""diag_scale"": self.diag_scale,\n            ""use_bias"": self.use_bias,\n            ""kernel_initializer"": tf.keras.initializers.serialize(\n                self.kernel_initializer\n            ),\n            ""bias_initializer"": tf.keras.initializers.serialize(self.bias_initializer),\n            ""kernel_regularizer"": tf.keras.regularizers.serialize(\n                self.kernel_regularizer\n            ),\n            ""bias_regularizer"": tf.keras.regularizers.serialize(self.bias_regularizer),\n        }\n        base_config = super(PolynomialCrossing, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, (tuple, list)):\n            raise ValueError(\n                ""A `PolynomialCrossing` layer should be called "" ""on a list of inputs.""\n            )\n        return input_shape[0]\n'"
tensorflow_addons/layers/sparsemax.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow_addons.activations.sparsemax import sparsemax\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass Sparsemax(tf.keras.layers.Layer):\n    """"""Sparsemax activation function [1].\n\n    The output shape is the same as the input shape.\n\n    [1]: https://arxiv.org/abs/1602.02068\n\n    Arguments:\n        axis: Integer, axis along which the sparsemax normalization is applied.\n    """"""\n\n    @typechecked\n    def __init__(self, axis: int = -1, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n\n    def call(self, inputs):\n        return sparsemax(inputs, axis=self.axis)\n\n    def get_config(self):\n        config = {""axis"": self.axis}\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n'"
tensorflow_addons/layers/spatial_pyramid_pooling.py,10,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Spatial Pyramid Pooling layers""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.layers.adaptive_pooling import AdaptiveAveragePooling2D\nimport tensorflow_addons.utils.keras_utils as conv_utils\n\nfrom typeguard import typechecked\nfrom typing import Union, Iterable\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass SpatialPyramidPooling2D(tf.keras.layers.Layer):\n    """"""Performs Spatial Pyramid Pooling.\n\n    Original Paper: https://arxiv.org/pdf/1406.4729.pdf\n\n    Spatial Pyramid Pooling generates a fixed-length representation\n    regardless of input size/scale. It is typically used before a layer\n    that requires a constant input shape, for example before a Dense Layer.\n\n    Arguments:\n      bins: Either a collection of integers or a collection of collections of 2 integers.\n        Each element in the inner collection must contain 2 integers, (pooled_rows, pooled_cols)\n        For example, providing [1, 3, 5] or [[1, 1], [3, 3], [5, 5]] preforms pooling\n        using three different pooling layers, having outputs with dimensions 1x1, 3x3 and 5x5 respectively.\n        These are flattened along height and width to give an output of shape\n        [batch_size, (1 + 9 + 25), channels] = [batch_size, 35, channels].\n      data_format: A string,\n        one of `channels_last` (default) or `channels_first`.\n        The ordering of the dimensions in the inputs.\n        `channels_last` corresponds to inputs with shape\n        `(batch, height, width, channels)` while `channels_first`\n        corresponds to inputs with shape `(batch, channels, height, width)`.\n\n    Input shape:\n      - If `data_format=\'channels_last\'`:\n        4D tensor with shape `(batch_size, height, width, channels)`.\n      - If `data_format=\'channels_first\'`:\n        4D tensor with shape `(batch_size, channels, height, width)`.\n\n    Output shape:\n      The output is the pooled image, flattened across its height and width\n      - If `data_format=\'channels_last\'`:\n        3D tensor with shape `(batch_size, num_bins, channels)`.\n      - If `data_format=\'channels_first\'`:\n        3D tensor with shape `(batch_size, channels, num_bins)`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        bins: Union[Iterable[int], Iterable[Iterable[int]]],\n        data_format=None,\n        *args,\n        **kwargs\n    ):\n        self.bins = [conv_utils.normalize_tuple(bin, 2, ""bin"") for bin in bins]\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.pool_layers = []\n        for bin in self.bins:\n            self.pool_layers.append(AdaptiveAveragePooling2D(bin, self.data_format))\n        super().__init__(*args, **kwargs)\n\n    def call(self, inputs, **kwargs):\n        dynamic_input_shape = tf.shape(inputs)\n        outputs = []\n        index = 0\n        if self.data_format == ""channels_last"":\n            for bin in self.bins:\n                height_overflow = dynamic_input_shape[1] % bin[0]\n                width_overflow = dynamic_input_shape[2] % bin[1]\n                new_input_height = dynamic_input_shape[1] - height_overflow\n                new_input_width = dynamic_input_shape[2] - width_overflow\n\n                new_inp = inputs[\n                    :, :new_input_height, :new_input_width, :,\n                ]\n                output = self.pool_layers[index](new_inp)\n                output = tf.reshape(\n                    output, [dynamic_input_shape[0], bin[0] * bin[1], inputs.shape[-1]]\n                )\n                outputs.append(output)\n                index += 1\n            outputs = tf.concat(outputs, axis=1)\n        else:\n            for bin in self.bins:\n                height_overflow = dynamic_input_shape[2] % bin[0]\n                width_overflow = dynamic_input_shape[3] % bin[1]\n                new_input_height = dynamic_input_shape[2] - height_overflow\n                new_input_width = dynamic_input_shape[3] - width_overflow\n\n                new_inp = inputs[\n                    :, :, :new_input_height, :new_input_width,\n                ]\n                output = self.pool_layers[index](new_inp)\n                output = tf.reshape(\n                    output, [dynamic_input_shape[0], inputs.shape[1], bin[0] * bin[1]]\n                )\n                outputs.append(output)\n                index += 1\n\n            outputs = tf.concat(outputs, axis=2)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        pooled_shape = 0\n        for bin in self.bins:\n            pooled_shape += tf.reduce_prod(bin)\n        if self.data_format == ""channels_last"":\n            return tf.TensorShape([input_shape[0], pooled_shape, input_shape[-1]])\n        else:\n            return tf.TensorShape([input_shape[0], input_shape[1], pooled_shape])\n\n    def get_config(self):\n        config = {""bins"": self.bins, ""data_format"": self.data_format}\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/layers/tlu.py,21,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Thresholded Linear Unit.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils import types\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TLU(tf.keras.layers.Layer):\n    """"""Thresholded Linear Unit. An activation function which is similar to ReLU\n    but with a learned threshold that benefits models using FRN(Filter Response\n    Normalization). Original paper: https://arxiv.org/pdf/1911.09737.\n\n    Input shape:\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n\n    Output shape:\n        Same shape as the input.\n\n    Arguments:\n        affine: bool. Whether to make it TLU-Affine or not\n        which has the form `max(x, alpha*x + tau)`\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        affine: bool = False,\n        tau_initializer: types.Initializer = ""zeros"",\n        tau_regularizer: types.Regularizer = None,\n        tau_constraint: types.Constraint = None,\n        alpha_initializer: types.Initializer = ""zeros"",\n        alpha_regularizer: types.Regularizer = None,\n        alpha_constraint: types.Constraint = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.affine = affine\n        self.tau_initializer = tf.keras.initializers.get(tau_initializer)\n        self.tau_regularizer = tf.keras.regularizers.get(tau_regularizer)\n        self.tau_constraint = tf.keras.constraints.get(tau_constraint)\n        if self.affine:\n            self.alpha_initializer = tf.keras.initializers.get(alpha_initializer)\n            self.alpha_regularizer = tf.keras.regularizers.get(alpha_regularizer)\n            self.alpha_constraint = tf.keras.constraints.get(alpha_constraint)\n\n    def build(self, input_shape):\n        param_shape = list(input_shape[1:])\n        self.tau = self.add_weight(\n            shape=param_shape,\n            name=""tau"",\n            initializer=self.tau_initializer,\n            regularizer=self.tau_regularizer,\n            constraint=self.tau_constraint,\n            synchronization=tf.VariableSynchronization.AUTO,\n            aggregation=tf.VariableAggregation.MEAN,\n        )\n        if self.affine:\n            self.alpha = self.add_weight(\n                shape=param_shape,\n                name=""alpha"",\n                initializer=self.alpha_initializer,\n                regularizer=self.alpha_regularizer,\n                constraint=self.alpha_constraint,\n                synchronization=tf.VariableSynchronization.AUTO,\n                aggregation=tf.VariableAggregation.MEAN,\n            )\n\n        axes = {i: input_shape[i] for i in range(1, len(input_shape))}\n        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes=axes)\n        self.built = True\n\n    def call(self, inputs):\n        if self.affine:\n            return tf.maximum(inputs, self.alpha * inputs + self.tau)\n        else:\n            return tf.maximum(inputs, self.tau)\n\n    def get_config(self):\n        config = {\n            ""tau_initializer"": tf.keras.initializers.serialize(self.tau_initializer),\n            ""tau_regularizer"": tf.keras.regularizers.serialize(self.tau_regularizer),\n            ""tau_constraint"": tf.keras.constraints.serialize(self.tau_constraint),\n            ""affine"": self.affine,\n        }\n\n        if self.affine:\n            config[""alpha_initializer""] = tf.keras.initializers.serialize(\n                self.alpha_initializer\n            )\n            config[""alpha_regularizer""] = tf.keras.regularizers.serialize(\n                self.alpha_regularizer\n            )\n            config[""alpha_constraint""] = tf.keras.constraints.serialize(\n                self.alpha_constraint\n            )\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n'"
tensorflow_addons/layers/wrappers.py,36,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport logging\n\nimport tensorflow as tf\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass WeightNormalization(tf.keras.layers.Wrapper):\n    """"""This wrapper reparameterizes a layer by decoupling the weight\'s\n    magnitude and direction.\n\n    This speeds up convergence by improving the\n    conditioning of the optimization problem.\n    Weight Normalization: A Simple Reparameterization to Accelerate\n    Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868\n    Tim Salimans, Diederik P. Kingma (2016)\n    WeightNormalization wrapper works for keras and tf layers.\n    ```python\n      net = WeightNormalization(\n          tf.keras.layers.Conv2D(2, 2, activation=\'relu\'),\n          input_shape=(32, 32, 3),\n          data_init=True)(x)\n      net = WeightNormalization(\n          tf.keras.layers.Conv2D(16, 5, activation=\'relu\'),\n          data_init=True)(net)\n      net = WeightNormalization(\n          tf.keras.layers.Dense(120, activation=\'relu\'),\n          data_init=True)(net)\n      net = WeightNormalization(\n          tf.keras.layers.Dense(n_classes),\n          data_init=True)(net)\n    ```\n    Arguments:\n      layer: a layer instance.\n      data_init: If `True` use data dependent variable initialization\n    Raises:\n      ValueError: If not initialized with a `Layer` instance.\n      ValueError: If `Layer` does not contain a `kernel` of weights\n      NotImplementedError: If `data_init` is True and running graph execution\n    """"""\n\n    @typechecked\n    def __init__(self, layer: tf.keras.layers, data_init: bool = True, **kwargs):\n        super().__init__(layer, **kwargs)\n        self.data_init = data_init\n        self._track_trackable(layer, name=""layer"")\n        self.is_rnn = isinstance(self.layer, tf.keras.layers.RNN)\n\n        if self.data_init and self.is_rnn:\n            logging.warning(\n                ""WeightNormalization: Using `data_init=True` with RNNs ""\n                ""is advised against by the paper. Use `data_init=False`.""\n            )\n\n    def build(self, input_shape):\n        """"""Build `Layer`""""""\n        input_shape = tf.TensorShape(input_shape)\n        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\n\n        if not self.layer.built:\n            self.layer.build(input_shape)\n\n        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n\n        if not hasattr(kernel_layer, ""kernel""):\n            raise ValueError(\n                ""`WeightNormalization` must wrap a layer that""\n                "" contains a `kernel` for weights""\n            )\n\n        if self.is_rnn:\n            kernel = kernel_layer.recurrent_kernel\n        else:\n            kernel = kernel_layer.kernel\n\n        # The kernel\'s filter or unit dimension is -1\n        self.layer_depth = int(kernel.shape[-1])\n        self.kernel_norm_axes = list(range(kernel.shape.rank - 1))\n\n        self.g = self.add_weight(\n            name=""g"",\n            shape=(self.layer_depth,),\n            initializer=""ones"",\n            dtype=kernel.dtype,\n            trainable=True,\n        )\n        self.v = kernel\n\n        self._initialized = self.add_weight(\n            name=""initialized"",\n            shape=None,\n            initializer=""zeros"",\n            dtype=tf.dtypes.bool,\n            trainable=False,\n        )\n\n        if self.data_init:\n            # Used for data initialization in self._data_dep_init.\n            with tf.name_scope(""data_dep_init""):\n                layer_config = tf.keras.layers.serialize(self.layer)\n                layer_config[""config""][""trainable""] = False\n                self._naked_clone_layer = tf.keras.layers.deserialize(layer_config)\n                self._naked_clone_layer.build(input_shape)\n                self._naked_clone_layer.set_weights(self.layer.get_weights())\n                if not self.is_rnn:\n                    self._naked_clone_layer.activation = None\n\n        self.built = True\n\n    def call(self, inputs):\n        """"""Call `Layer`""""""\n\n        def _do_nothing():\n            return tf.identity(self.g)\n\n        def _update_weights():\n            # Ensure we read `self.g` after _update_weights.\n            with tf.control_dependencies(self._initialize_weights(inputs)):\n                return tf.identity(self.g)\n\n        g = tf.cond(self._initialized, _do_nothing, _update_weights)\n\n        with tf.name_scope(""compute_weights""):\n            # Replace kernel by normalized weight variable.\n            kernel = tf.nn.l2_normalize(self.v, axis=self.kernel_norm_axes) * g\n\n            if self.is_rnn:\n                self.layer.cell.recurrent_kernel = kernel\n                update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n            else:\n                self.layer.kernel = kernel\n                update_kernel = tf.identity(self.layer.kernel)\n\n            # Ensure we calculate result after updating kernel.\n            with tf.control_dependencies([update_kernel]):\n                outputs = self.layer(inputs)\n                return outputs\n\n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n\n    def _initialize_weights(self, inputs):\n        """"""Initialize weight g.\n\n        The initial value of g could either from the initial value in v,\n        or by the input value if self.data_init is True.\n        """"""\n        with tf.control_dependencies(\n            [\n                tf.debugging.assert_equal(  # pylint: disable=bad-continuation\n                    self._initialized, False, message=""The layer has been initialized.""\n                )\n            ]\n        ):\n            if self.data_init:\n                assign_tensors = self._data_dep_init(inputs)\n            else:\n                assign_tensors = self._init_norm()\n            assign_tensors.append(self._initialized.assign(True))\n            return assign_tensors\n\n    def _init_norm(self):\n        """"""Set the weight g with the norm of the weight vector.""""""\n        with tf.name_scope(""init_norm""):\n            v_flat = tf.reshape(self.v, [-1, self.layer_depth])\n            v_norm = tf.linalg.norm(v_flat, axis=0)\n            g_tensor = self.g.assign(tf.reshape(v_norm, (self.layer_depth,)))\n            return [g_tensor]\n\n    def _data_dep_init(self, inputs):\n        """"""Data dependent initialization.""""""\n        with tf.name_scope(""data_dep_init""):\n            # Generate data dependent init values\n            x_init = self._naked_clone_layer(inputs)\n            data_norm_axes = list(range(x_init.shape.rank - 1))\n            m_init, v_init = tf.nn.moments(x_init, data_norm_axes)\n            scale_init = 1.0 / tf.math.sqrt(v_init + 1e-10)\n\n            # RNNs have fused kernels that are tiled\n            # Repeat scale_init to match the shape of fused kernel\n            # Note: This is only to support the operation,\n            # the paper advises against RNN+data_dep_init\n            if scale_init.shape[0] != self.g.shape[0]:\n                rep = int(self.g.shape[0] / scale_init.shape[0])\n                scale_init = tf.tile(scale_init, [rep])\n\n            # Assign data dependent init values\n            g_tensor = self.g.assign(self.g * scale_init)\n            if hasattr(self.layer, ""bias"") and self.layer.bias is not None:\n                bias_tensor = self.layer.bias.assign(-m_init * scale_init)\n                return [g_tensor, bias_tensor]\n            else:\n                return [g_tensor]\n\n    def get_config(self):\n        config = {""data_init"": self.data_init}\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def remove(self):\n        kernel = tf.Variable(\n            tf.nn.l2_normalize(self.v, axis=self.kernel_norm_axes) * self.g,\n            name=""recurrent_kernel"" if self.is_rnn else ""kernel"",\n        )\n\n        if self.is_rnn:\n            self.layer.cell.recurrent_kernel = kernel\n        else:\n            self.layer.kernel = kernel\n\n        return self.layer\n'"
tensorflow_addons/losses/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional losses that conform to Keras API.""""""\n\nfrom tensorflow_addons.losses.contrastive import contrastive_loss, ContrastiveLoss\nfrom tensorflow_addons.losses.focal_loss import (\n    sigmoid_focal_crossentropy,\n    SigmoidFocalCrossEntropy,\n)\nfrom tensorflow_addons.losses.giou_loss import giou_loss, GIoULoss\nfrom tensorflow_addons.losses.lifted import lifted_struct_loss, LiftedStructLoss\nfrom tensorflow_addons.losses.sparsemax_loss import sparsemax_loss, SparsemaxLoss\nfrom tensorflow_addons.losses.triplet import (\n    triplet_semihard_loss,\n    triplet_hard_loss,\n    TripletSemiHardLoss,\n    TripletHardLoss,\n)\nfrom tensorflow_addons.losses.quantiles import pinball_loss, PinballLoss\n\n\nfrom tensorflow_addons.losses.npairs import (\n    npairs_loss,\n    NpairsLoss,\n    npairs_multilabel_loss,\n    NpairsMultilabelLoss,\n)\nfrom tensorflow_addons.losses.kappa_loss import WeightedKappaLoss\n'"
tensorflow_addons/losses/contrastive.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements contrastive loss.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import TensorLike, Number\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef contrastive_loss(\n    y_true: TensorLike, y_pred: TensorLike, margin: Number = 1.0\n) -> tf.Tensor:\n    r""""""Computes the contrastive loss between `y_true` and `y_pred`.\n\n    This loss encourages the embedding to be close to each other for\n    the samples of the same label and the embedding to be far apart at least\n    by the margin constant for the samples of different labels.\n\n    The euclidean distances `y_pred` between two embedding matrices\n    `a` and `b` with shape [batch_size, hidden_size] can be computed\n    as follows:\n\n    ```python\n    # y_pred = \\sqrt (\\sum_i (a[:, i] - b[:, i])^2)\n    y_pred = tf.linalg.norm(a - b, axis=1)\n    ```\n\n    See: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\n    Args:\n      y_true: 1-D integer `Tensor` with shape [batch_size] of\n        binary labels indicating positive vs negative pair.\n      y_pred: 1-D float `Tensor` with shape [batch_size] of\n        distances between two embedding matrices.\n      margin: margin term in the loss definition.\n\n    Returns:\n      contrastive_loss: 1-D float `Tensor` with shape [batch_size].\n    """"""\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n    return y_true * tf.math.square(y_pred) + (1.0 - y_true) * tf.math.square(\n        tf.math.maximum(margin - y_pred, 0.0)\n    )\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass ContrastiveLoss(LossFunctionWrapper):\n    r""""""Computes the contrastive loss between `y_true` and `y_pred`.\n\n    This loss encourages the embedding to be close to each other for\n    the samples of the same label and the embedding to be far apart at least\n    by the margin constant for the samples of different labels.\n\n    See: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n\n    We expect labels `y_true` to be provided as 1-D integer `Tensor`\n    with shape [batch_size] of binary integer labels. And `y_pred` must be\n    1-D float `Tensor` with shape [batch_size] of distances between two\n    embedding matrices.\n\n    The euclidean distances `y_pred` between two embedding matrices\n    `a` and `b` with shape [batch_size, hidden_size] can be computed\n    as follows:\n\n    ```python\n    # y_pred = \\sqrt (\\sum_i (a[:, i] - b[:, i])^2)\n    y_pred = tf.linalg.norm(a - b, axis=1)\n    ```\n\n    Args:\n      margin: `Float`, margin term in the loss definition.\n        Default value is 1.0.\n      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply.\n        Default value is `SUM_OVER_BATCH_SIZE`.\n      name: (Optional) name for the loss.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        margin: Number = 1.0,\n        reduction: str = tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name: str = ""contrasitve_loss"",\n    ):\n        super().__init__(\n            contrastive_loss, reduction=reduction, name=name, margin=margin\n        )\n'"
tensorflow_addons/losses/focal_loss.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Focal loss.""""""\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass SigmoidFocalCrossEntropy(LossFunctionWrapper):\n    """"""Implements the focal loss function.\n\n    Focal loss was first introduced in the RetinaNet paper\n    (https://arxiv.org/pdf/1708.02002.pdf). Focal loss is extremely useful for\n    classification when you have highly imbalanced classes. It down-weights\n    well-classified examples and focuses on hard examples. The loss value is\n    much high for a sample which is misclassified by the classifier as compared\n    to the loss value corresponding to a well-classified example. One of the\n    best use-cases of focal loss is its usage in object detection where the\n    imbalance between the background class and other classes is extremely high.\n\n    Usage:\n\n    ```python\n    fl = tfa.losses.SigmoidFocalCrossEntropy()\n    loss = fl(\n      y_true = [[1.0], [1.0], [0.0]],\n      y_pred = [[0.97], [0.91], [0.03]])\n    print(\'Loss: \', loss.numpy())  # Loss: [6.8532745e-06,\n                                            1.9097870e-04,\n                                            2.0559824e-05]\n    ```\n    Usage with tf.keras API:\n\n    ```python\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\'sgd\', loss=tf.keras.losses.SigmoidFocalCrossEntropy())\n    ```\n\n    Args\n      alpha: balancing factor, default value is 0.25\n      gamma: modulating factor, default value is 2.0\n\n    Returns:\n      Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same\n          shape as `y_true`; otherwise, it is scalar.\n\n    Raises:\n        ValueError: If the shape of `sample_weight` is invalid or value of\n          `gamma` is less than zero\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        from_logits: bool = False,\n        alpha: FloatTensorLike = 0.25,\n        gamma: FloatTensorLike = 2.0,\n        reduction: str = tf.keras.losses.Reduction.NONE,\n        name: str = ""sigmoid_focal_crossentropy"",\n    ):\n        super().__init__(\n            sigmoid_focal_crossentropy,\n            name=name,\n            reduction=reduction,\n            from_logits=from_logits,\n            alpha=alpha,\n            gamma=gamma,\n        )\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef sigmoid_focal_crossentropy(\n    y_true: TensorLike,\n    y_pred: TensorLike,\n    alpha: FloatTensorLike = 0.25,\n    gamma: FloatTensorLike = 2.0,\n    from_logits: bool = False,\n) -> tf.Tensor:\n    """"""\n    Args\n        y_true: true targets tensor.\n        y_pred: predictions tensor.\n        alpha: balancing factor.\n        gamma: modulating factor.\n\n    Returns:\n        Weighted loss float `Tensor`. If `reduction` is `NONE`,this has the\n        same shape as `y_true`; otherwise, it is scalar.\n    """"""\n    if gamma and gamma < 0:\n        raise ValueError(""Value of gamma should be greater than or equal to zero"")\n\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.convert_to_tensor(y_true, dtype=y_pred.dtype)\n\n    # Get the cross_entropy for each entry\n    ce = K.binary_crossentropy(y_true, y_pred, from_logits=from_logits)\n\n    # If logits are provided then convert the predictions into probabilities\n    if from_logits:\n        pred_prob = tf.sigmoid(y_pred)\n    else:\n        pred_prob = y_pred\n\n    p_t = (y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob))\n    alpha_factor = 1.0\n    modulating_factor = 1.0\n\n    if alpha:\n        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n\n    if gamma:\n        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n        modulating_factor = tf.pow((1.0 - p_t), gamma)\n\n    # compute the final loss and return\n    return tf.reduce_sum(alpha_factor * modulating_factor * ce, axis=-1)\n'"
tensorflow_addons/losses/giou_loss.py,34,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements GIoU loss.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import TensorLike\nfrom typing import Optional\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass GIoULoss(LossFunctionWrapper):\n    """"""Implements the GIoU loss function.\n\n    GIoU loss was first introduced in the\n    [Generalized Intersection over Union:\n    A Metric and A Loss for Bounding Box Regression]\n    (https://giou.stanford.edu/GIoU.pdf).\n    GIoU is an enhancement for models which use IoU in object detection.\n\n    Usage:\n\n    ```python\n    gl = tfa.losses.GIoULoss()\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])\n    loss = gl(boxes1, boxes2)\n    print(\'Loss: \', loss.numpy())  # Loss: [1.07500000298023224, 1.9333333373069763]\n    ```\n    Usage with tf.keras API:\n\n    ```python\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\'sgd\', loss=tfa.losses.GIoULoss())\n    ```\n\n    Args:\n      mode: one of [\'giou\', \'iou\'], decided to calculate GIoU or IoU loss.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        mode: str = ""giou"",\n        reduction: str = tf.keras.losses.Reduction.AUTO,\n        name: Optional[str] = ""giou_loss"",\n    ):\n        super().__init__(giou_loss, name=name, reduction=reduction, mode=mode)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef giou_loss(y_true: TensorLike, y_pred: TensorLike, mode: str = ""giou"") -> tf.Tensor:\n    """"""\n    Args:\n        y_true: true targets tensor. The coordinates of the each bounding\n            box in boxes are encoded as [y_min, x_min, y_max, x_max].\n        y_pred: predictions tensor. The coordinates of the each bounding\n            box in boxes are encoded as [y_min, x_min, y_max, x_max].\n        mode: one of [\'giou\', \'iou\'], decided to calculate GIoU or IoU loss.\n\n    Returns:\n        GIoU loss float `Tensor`.\n    """"""\n    if mode not in [""giou"", ""iou""]:\n        raise ValueError(""Value of mode should be \'iou\' or \'giou\'"")\n    y_pred = tf.convert_to_tensor(y_pred)\n    if not y_pred.dtype.is_floating:\n        y_pred = tf.cast(y_pred, tf.float32)\n    y_true = tf.cast(y_true, y_pred.dtype)\n    giou = tf.squeeze(_calculate_giou(y_pred, y_true, mode))\n\n    return 1 - giou\n\n\ndef _calculate_giou(b1: TensorLike, b2: TensorLike, mode: str = ""giou"") -> tf.Tensor:\n    """"""\n    Args:\n        b1: bounding box. The coordinates of the each bounding box in boxes are\n            encoded as [y_min, x_min, y_max, x_max].\n        b2: the other bounding box. The coordinates of the each bounding box\n            in boxes are encoded as [y_min, x_min, y_max, x_max].\n        mode: one of [\'giou\', \'iou\'], decided to calculate GIoU or IoU loss.\n\n    Returns:\n        GIoU loss float `Tensor`.\n    """"""\n    zero = tf.convert_to_tensor(0.0, b1.dtype)\n    b1_ymin, b1_xmin, b1_ymax, b1_xmax = tf.unstack(b1, 4, axis=-1)\n    b2_ymin, b2_xmin, b2_ymax, b2_xmax = tf.unstack(b2, 4, axis=-1)\n    b1_width = tf.maximum(zero, b1_xmax - b1_xmin)\n    b1_height = tf.maximum(zero, b1_ymax - b1_ymin)\n    b2_width = tf.maximum(zero, b2_xmax - b2_xmin)\n    b2_height = tf.maximum(zero, b2_ymax - b2_ymin)\n    b1_area = b1_width * b1_height\n    b2_area = b2_width * b2_height\n\n    intersect_ymin = tf.maximum(b1_ymin, b2_ymin)\n    intersect_xmin = tf.maximum(b1_xmin, b2_xmin)\n    intersect_ymax = tf.minimum(b1_ymax, b2_ymax)\n    intersect_xmax = tf.minimum(b1_xmax, b2_xmax)\n    intersect_width = tf.maximum(zero, intersect_xmax - intersect_xmin)\n    intersect_height = tf.maximum(zero, intersect_ymax - intersect_ymin)\n    intersect_area = intersect_width * intersect_height\n\n    union_area = b1_area + b2_area - intersect_area\n    iou = tf.math.divide_no_nan(intersect_area, union_area)\n    if mode == ""iou"":\n        return iou\n\n    enclose_ymin = tf.minimum(b1_ymin, b2_ymin)\n    enclose_xmin = tf.minimum(b1_xmin, b2_xmin)\n    enclose_ymax = tf.maximum(b1_ymax, b2_ymax)\n    enclose_xmax = tf.maximum(b1_xmax, b2_xmax)\n    enclose_width = tf.maximum(zero, enclose_xmax - enclose_xmin)\n    enclose_height = tf.maximum(zero, enclose_ymax - enclose_ymin)\n    enclose_area = enclose_width * enclose_height\n    giou = iou - tf.math.divide_no_nan((enclose_area - union_area), enclose_area)\n    return giou\n'"
tensorflow_addons/losses/kappa_loss.py,29,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Weighted kappa loss.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import Number\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass WeightedKappaLoss(tf.keras.losses.Loss):\n    """"""Implements the Weighted Kappa loss function.\n\n    Weighted Kappa loss was introduced in the\n    [Weighted kappa loss function for multi-class classification\n    of ordinal data in deep learning]\n    (https://www.sciencedirect.com/science/article/abs/pii/S0167865517301666).\n    Weighted Kappa is widely used in Ordinal Classification Problems.\n    The loss value lies in [-inf, log 2], where log 2\n     means the random prediction.\n\n    Usage:\n\n    ```python\n    kappa_loss = WeightedKappaLoss(num_classes=4)\n    y_true = tf.constant([[0, 0, 1, 0], [0, 1, 0, 0],\n                          [1, 0, 0, 0], [0, 0, 0, 1]])\n    y_pred = tf.constant([[0.1, 0.2, 0.6, 0.1], [0.1, 0.5, 0.3, 0.1],\n                          [0.8, 0.05, 0.05, 0.1], [0.01, 0.09, 0.1, 0.8]])\n    loss = kappa_loss(y_true, y_pred)\n    print(\'Loss: \', loss.numpy())  # Loss: -1.1611923\n    ```\n\n    Usage with `tf.keras` API:\n    ```python\n    # outputs should be softmax results\n    # if you want to weight the samples, just multiply the outputs\n    # by the sample weight.\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\'sgd\', loss=tfa.losses.WeightedKappa(num_classes=4))\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: int,\n        weightage: Optional[str] = ""quadratic"",\n        name: Optional[str] = ""cohen_kappa_loss"",\n        epsilon: Optional[Number] = 1e-6,\n        dtype: Optional[tf.DType] = tf.float32,\n        reduction: str = tf.keras.losses.Reduction.NONE,\n    ):\n        """"""Creates a `WeightedKappa` instance.\n\n        Args:\n          num_classes: Number of unique classes in your dataset.\n          weightage: (Optional) Weighting to be considered for calculating\n            kappa statistics. A valid value is one of\n            [\'linear\', \'quadratic\']. Defaults to `quadratic` since it\'s\n            mostly used.\n          name: (Optional) String name of the metric instance.\n          epsilon: (Optional) increment to avoid log zero,\n            so the loss will be log(1 - k + epsilon), where k belongs to\n            [-1, 1], usually you can use the default value which is 1e-6.\n          dtype: (Optional) Data type of the metric result.\n            Defaults to `tf.float32`.\n        Raises:\n          ValueError: If the value passed for `weightage` is invalid\n            i.e. not any one of [\'linear\', \'quadratic\']\n        """"""\n\n        super().__init__(name=name, reduction=reduction)\n\n        if weightage not in (""linear"", ""quadratic""):\n            raise ValueError(""Unknown kappa weighting type."")\n\n        self.weightage = weightage\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.dtype = dtype\n        label_vec = tf.range(num_classes, dtype=dtype)\n        self.row_label_vec = tf.reshape(label_vec, [1, num_classes])\n        self.col_label_vec = tf.reshape(label_vec, [num_classes, 1])\n        col_mat = tf.tile(self.col_label_vec, [1, num_classes])\n        row_mat = tf.tile(self.row_label_vec, [num_classes, 1])\n        if weightage == ""linear"":\n            self.weight_mat = tf.abs(col_mat - row_mat)\n        else:\n            self.weight_mat = (col_mat - row_mat) ** 2\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, dtype=self.dtype)\n        batch_size = tf.shape(y_true)[0]\n        cat_labels = tf.matmul(y_true, self.col_label_vec)\n        cat_label_mat = tf.tile(cat_labels, [1, self.num_classes])\n        row_label_mat = tf.tile(self.row_label_vec, [batch_size, 1])\n        if self.weightage == ""linear"":\n            weight = tf.abs(cat_label_mat - row_label_mat)\n        else:\n            weight = (cat_label_mat - row_label_mat) ** 2\n        numerator = tf.reduce_sum(weight * y_pred)\n        label_dist = tf.reduce_sum(y_true, axis=0, keepdims=True)\n        pred_dist = tf.reduce_sum(y_pred, axis=0, keepdims=True)\n        w_pred_dist = tf.matmul(self.weight_mat, pred_dist, transpose_b=True)\n        denominator = tf.reduce_sum(tf.matmul(label_dist, w_pred_dist))\n        denominator /= tf.cast(batch_size, dtype=self.dtype)\n        loss = tf.math.divide_no_nan(numerator, denominator)\n        return tf.math.log(loss + self.epsilon)\n\n    def get_config(self):\n        config = {\n            ""num_classes"": self.num_classes,\n            ""weightage"": self.weightage,\n            ""epsilon"": self.epsilon,\n            ""dtype"": self.dtype,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/losses/lifted.py,34,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements lifted_struct_loss.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.losses import metric_learning\n\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef lifted_struct_loss(\n    labels: TensorLike, embeddings: TensorLike, margin: FloatTensorLike = 1.0\n) -> tf.Tensor:\n    """"""Computes the lifted structured loss.\n\n    Args:\n      labels: 1-D tf.int32 `Tensor` with shape [batch_size] of\n        multiclass integer labels.\n      embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should\n        not be l2 normalized.\n      margin: Float, margin term in the loss definition.\n\n    Returns:\n      lifted_loss: float scalar with dtype of embeddings.\n    """"""\n    convert_to_float32 = (\n        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n    )\n    precise_embeddings = (\n        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n    )\n\n    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n    lshape = tf.shape(labels)\n    labels = tf.reshape(labels, [lshape[0], 1])\n\n    # Build pairwise squared distance matrix.\n    pairwise_distances = metric_learning.pairwise_distance(precise_embeddings)\n\n    # Build pairwise binary adjacency matrix.\n    adjacency = tf.math.equal(labels, tf.transpose(labels))\n    # Invert so we can select negatives only.\n    adjacency_not = tf.math.logical_not(adjacency)\n\n    batch_size = tf.size(labels)\n\n    diff = margin - pairwise_distances\n    mask = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n    # Safe maximum: Temporarily shift negative distances\n    #   above zero before taking max.\n    #     this is to take the max only among negatives.\n    row_minimums = tf.math.reduce_min(diff, 1, keepdims=True)\n    row_negative_maximums = (\n        tf.math.reduce_max(\n            tf.math.multiply(diff - row_minimums, mask), 1, keepdims=True\n        )\n        + row_minimums\n    )\n\n    # Compute the loss.\n    # Keep track of matrix of maximums where M_ij = max(m_i, m_j)\n    #   where m_i is the max of alpha - negative D_i\'s.\n    # This matches the Caffe loss layer implementation at:\n    #   https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16/blob/0efd7544a9846f58df923c8b992198ba5c355454/src/caffe/layers/lifted_struct_similarity_softmax_layer.cpp\n\n    max_elements = tf.math.maximum(\n        row_negative_maximums, tf.transpose(row_negative_maximums)\n    )\n    diff_tiled = tf.tile(diff, [batch_size, 1])\n    mask_tiled = tf.tile(mask, [batch_size, 1])\n    max_elements_vect = tf.reshape(tf.transpose(max_elements), [-1, 1])\n\n    loss_exp_left = tf.reshape(\n        tf.math.reduce_sum(\n            tf.math.multiply(tf.math.exp(diff_tiled - max_elements_vect), mask_tiled),\n            1,\n            keepdims=True,\n        ),\n        [batch_size, batch_size],\n    )\n\n    loss_mat = max_elements + tf.math.log(loss_exp_left + tf.transpose(loss_exp_left))\n    # Add the positive distance.\n    loss_mat += pairwise_distances\n\n    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n        tf.ones([batch_size])\n    )\n\n    # *0.5 for upper triangular, and another *0.5 for 1/2 factor for loss^2.\n    num_positives = tf.math.reduce_sum(mask_positives) / 2.0\n\n    lifted_loss = tf.math.truediv(\n        0.25\n        * tf.math.reduce_sum(\n            tf.math.square(\n                tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)\n            )\n        ),\n        num_positives,\n    )\n\n    if convert_to_float32:\n        return tf.cast(lifted_loss, embeddings.dtype)\n    else:\n        return lifted_loss\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass LiftedStructLoss(LossFunctionWrapper):\n    """"""Computes the lifted structured loss.\n\n    The loss encourages the positive distances (between a pair of embeddings\n    with the same labels) to be smaller than any negative distances (between\n    a pair of embeddings with different labels) in the mini-batch in a way\n    that is differentiable with respect to the embedding vectors.\n    See: https://arxiv.org/abs/1511.06452.\n\n    Args:\n      margin: Float, margin term in the loss definition.\n      name: Optional name for the op.\n    """"""\n\n    @typechecked\n    def __init__(\n        self, margin: FloatTensorLike = 1.0, name: Optional[str] = None, **kwargs\n    ):\n        super().__init__(\n            lifted_struct_loss,\n            name=name,\n            reduction=tf.keras.losses.Reduction.NONE,\n            margin=margin,\n        )\n'"
tensorflow_addons/losses/metric_learning.py,20,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions of metric learning.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import TensorLike\n\n\n@tf.function\ndef pairwise_distance(feature: TensorLike, squared: bool = False):\n    """"""Computes the pairwise distance matrix with numerical stability.\n\n    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n\n    Args:\n      feature: 2-D Tensor of size [number of data, feature dimension].\n      squared: Boolean, whether or not to square the pairwise distances.\n\n    Returns:\n      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n    """"""\n    pairwise_distances_squared = tf.math.add(\n        tf.math.reduce_sum(tf.math.square(feature), axis=[1], keepdims=True),\n        tf.math.reduce_sum(\n            tf.math.square(tf.transpose(feature)), axis=[0], keepdims=True\n        ),\n    ) - 2.0 * tf.matmul(feature, tf.transpose(feature))\n\n    # Deal with numerical inaccuracies. Set small negatives to zero.\n    pairwise_distances_squared = tf.math.maximum(pairwise_distances_squared, 0.0)\n    # Get the mask where the zero distances are at.\n    error_mask = tf.math.less_equal(pairwise_distances_squared, 0.0)\n\n    # Optionally take the sqrt.\n    if squared:\n        pairwise_distances = pairwise_distances_squared\n    else:\n        pairwise_distances = tf.math.sqrt(\n            pairwise_distances_squared\n            + tf.cast(error_mask, dtype=tf.dtypes.float32) * 1e-16\n        )\n\n    # Undo conditionally adding 1e-16.\n    pairwise_distances = tf.math.multiply(\n        pairwise_distances,\n        tf.cast(tf.math.logical_not(error_mask), dtype=tf.dtypes.float32),\n    )\n\n    num_data = tf.shape(feature)[0]\n    # Explicitly set diagonals to zero.\n    mask_offdiagonals = tf.ones_like(pairwise_distances) - tf.linalg.diag(\n        tf.ones([num_data])\n    )\n    pairwise_distances = tf.math.multiply(pairwise_distances, mask_offdiagonals)\n    return pairwise_distances\n\n\n@tf.function\ndef angular_distance(feature: TensorLike):\n    """"""Computes the angular distance matrix.\n\n    output[i, j] = 1 - cosine_similarity(feature[i, :], feature[j, :])\n\n    Args:\n      feature: 2-D Tensor of size [number of data, feature dimension].\n\n    Returns:\n      angular_distances: 2-D Tensor of size [number of data, number of data].\n    """"""\n    # normalize input\n    feature = tf.math.l2_normalize(feature, axis=1)\n\n    # create adjaceny matrix of cosine similarity\n    angular_distances = 1 - tf.matmul(feature, feature, transpose_b=True)\n\n    # ensure all distances > 1e-16\n    angular_distances = tf.maximum(angular_distances, 0.0)\n\n    return angular_distances\n'"
tensorflow_addons/losses/npairs.py,32,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements npairs loss.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.types import TensorLike\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef npairs_loss(y_true: TensorLike, y_pred: TensorLike) -> tf.Tensor:\n    """"""Computes the npairs loss between `y_true` and `y_pred`.\n\n    Npairs loss expects paired data where a pair is composed of samples from\n    the same labels and each pairs in the minibatch have different labels.\n    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n    as logits and the remapped multi-class labels, `y_true`, as labels.\n\n    The similarity matrix `y_pred` between two embedding matrices `a` and `b`\n    with shape `[batch_size, hidden_size]` can be computed as follows:\n\n    ```python\n    # y_pred = a * b^T\n    y_pred = tf.matmul(a, b, transpose_a=False, transpose_b=True)\n    ```\n\n    See: http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n\n    Args:\n      y_true: 1-D integer `Tensor` with shape `[batch_size]` of\n        multi-class labels.\n      y_pred: 2-D float `Tensor` with shape `[batch_size, batch_size]` of\n        similarity matrix between embedding matrices.\n\n    Returns:\n      npairs_loss: float scalar.\n    """"""\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n\n    # Expand to [batch_size, 1]\n    y_true = tf.expand_dims(y_true, -1)\n    y_true = tf.cast(tf.equal(y_true, tf.transpose(y_true)), y_pred.dtype)\n    y_true /= tf.math.reduce_sum(y_true, 1, keepdims=True)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n\n    return tf.math.reduce_mean(loss)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef npairs_multilabel_loss(y_true: TensorLike, y_pred: TensorLike) -> tf.Tensor:\n    r""""""Computes the npairs loss between multilabel data `y_true` and `y_pred`.\n\n    Npairs loss expects paired data where a pair is composed of samples from\n    the same labels and each pairs in the minibatch have different labels.\n    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n    as logits and the remapped multi-class labels, `y_true`, as labels.\n\n    To deal with multilabel inputs, the count of label intersection\n    is computed as follows:\n\n    ```\n    L_{i,j} = | set_of_labels_for(i) \\cap set_of_labels_for(j) |\n    ```\n\n    Each row of the count based label matrix is further normalized so that\n    each row sums to one.\n\n    `y_true` should be a binary indicator for classes.\n    That is, if `y_true[i, j] = 1`, then `i`th sample is in `j`th class;\n    if `y_true[i, j] = 0`, then `i`th sample is not in `j`th class.\n\n    The similarity matrix `y_pred` between two embedding matrices `a` and `b`\n    with shape `[batch_size, hidden_size]` can be computed as follows:\n\n    ```python\n    # y_pred = a * b^T\n    y_pred = tf.matmul(a, b, transpose_a=False, transpose_b=True)\n    ```\n\n    See: http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n\n    Args:\n      y_true: Either 2-D integer `Tensor` with shape\n        `[batch_size, num_classes]`, or `SparseTensor` with dense shape\n        `[batch_size, num_classes]`. If `y_true` is a `SparseTensor`, then\n        it will be converted to `Tensor` via `tf.sparse.to_dense` first.\n\n      y_pred: 2-D float `Tensor` with shape `[batch_size, batch_size]` of\n        similarity matrix between embedding matrices.\n\n    Returns:\n      npairs_multilabel_loss: float scalar.\n    """"""\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n\n    # Convert to dense tensor if `y_true` is a `SparseTensor`\n    if isinstance(y_true, tf.SparseTensor):\n        y_true = tf.sparse.to_dense(y_true)\n\n    # Enable efficient multiplication because y_true contains lots of zeros\n    # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n    y_true = tf.linalg.matmul(\n        y_true, y_true, transpose_b=True, a_is_sparse=True, b_is_sparse=True\n    )\n    y_true /= tf.math.reduce_sum(y_true, 1, keepdims=True)\n\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true)\n\n    return tf.math.reduce_mean(loss)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass NpairsLoss(tf.keras.losses.Loss):\n    """"""Computes the npairs loss between `y_true` and `y_pred`.\n\n    Npairs loss expects paired data where a pair is composed of samples from\n    the same labels and each pairs in the minibatch have different labels.\n    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n    as logits and the remapped multi-class labels, `y_true`, as labels.\n\n    The similarity matrix `y_pred` between two embedding matrices `a` and `b`\n    with shape `[batch_size, hidden_size]` can be computed as follows:\n\n    ```python\n    # y_pred = a * b^T\n    y_pred = tf.matmul(a, b, transpose_a=False, transpose_b=True)\n    ```\n\n    See: http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n\n    Args:\n      name: (Optional) name for the loss.\n    """"""\n\n    @typechecked\n    def __init__(self, name: str = ""npairs_loss""):\n        super().__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n\n    def call(self, y_true, y_pred):\n        return npairs_loss(y_true, y_pred)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass NpairsMultilabelLoss(tf.keras.losses.Loss):\n    r""""""Computes the npairs loss between multilabel data `y_true` and `y_pred`.\n\n    Npairs loss expects paired data where a pair is composed of samples from\n    the same labels and each pairs in the minibatch have different labels.\n    The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n    as logits and the remapped multi-class labels, `y_true`, as labels.\n\n    To deal with multilabel inputs, the count of label intersection\n    is computed as follows:\n\n    ```\n    L_{i,j} = | set_of_labels_for(i) \\cap set_of_labels_for(j) |\n    ```\n\n    Each row of the count based label matrix is further normalized so that\n    each row sums to one.\n\n    `y_true` should be a binary indicator for classes.\n    That is, if `y_true[i, j] = 1`, then `i`th sample is in `j`th class;\n    if `y_true[i, j] = 0`, then `i`th sample is not in `j`th class.\n\n    The similarity matrix `y_pred` between two embedding matrices `a` and `b`\n    with shape `[batch_size, hidden_size]` can be computed as follows:\n\n    ```python\n    # y_pred = a * b^T\n    y_pred = tf.matmul(a, b, transpose_a=False, transpose_b=True)\n    ```\n\n    See: http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf\n\n    Args:\n      name: (Optional) name for the loss.\n    """"""\n\n    @typechecked\n    def __init__(self, name: str = ""npairs_multilabel_loss""):\n        super().__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n\n    def call(self, y_true, y_pred):\n        return npairs_multilabel_loss(y_true, y_pred)\n'"
tensorflow_addons/losses/quantiles.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements quantiles losses.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n\n\n@tf.function\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef pinball_loss(\n    y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5\n) -> tf.Tensor:\n    """"""Computes the pinball loss between `y_true` and `y_pred`.\n\n    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n\n    In the context of regression this, loss yields an estimator of the tau\n    conditional quantile.\n\n    See: https://en.wikipedia.org/wiki/Quantile_regression\n\n    Usage:\n    ```python\n    loss = pinball_loss([0., 0., 1., 1.], [1., 1., 1., 0.], tau=.1)\n\n    # loss = max(0.1 * (y_true - y_pred), (0.1 - 1) * (y_true - y_pred))\n    #      = (0.9 + 0.9 + 0 + 0.1) / 4\n\n    print(\'Loss: \', loss.numpy())  # Loss: 0.475\n    ```\n\n    Args:\n      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`\n      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n        the context of quantile regression, the value of tau determines the\n        conditional quantile level. When tau = 0.5, this amounts to l1\n        regression, an estimator of the conditional median (0.5 quantile).\n\n    Returns:\n        pinball_loss: 1-D float `Tensor` with shape [batch_size].\n\n    References:\n      - https://en.wikipedia.org/wiki/Quantile_regression\n      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n    """"""\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(y_true, y_pred.dtype)\n\n    # Broadcast the pinball slope along the batch dimension\n    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n    one = tf.cast(1, tau.dtype)\n\n    delta_y = y_true - y_pred\n    pinball = tf.math.maximum(tau * delta_y, (tau - one) * delta_y)\n    return tf.reduce_mean(pinball, axis=-1)\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass PinballLoss(LossFunctionWrapper):\n    """"""Computes the pinball loss between `y_true` and `y_pred`.\n\n    `loss = maximum(tau * (y_true - y_pred), (tau - 1) * (y_true - y_pred))`\n\n    In the context of regression, this loss yields an estimator of the tau\n    conditional quantile.\n\n    See: https://en.wikipedia.org/wiki/Quantile_regression\n\n    Usage:\n    ```python\n    pinball = tfa.losses.PinballLoss(tau=.1)\n    loss = pinball([0., 0., 1., 1.], [1., 1., 1., 0.])\n\n    # loss = max(0.1 * (y_true - y_pred), (0.1 - 1) * (y_true - y_pred))\n    #      = (0.9 + 0.9 + 0 + 0.1) / 4\n\n    print(\'Loss: \', loss.numpy())  # Loss: 0.475\n    ```\n\n    Usage with the `compile` API:\n\n    ```python\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\'sgd\', loss=tfa.losses.PinballLoss(tau=.1))\n    ```\n\n    Args:\n      tau: (Optional) Float in [0, 1] or a tensor taking values in [0, 1] and\n        shape = `[d0,..., dn]`.  It defines the slope of the pinball loss. In\n        the context of quantile regression, the value of tau determines the\n        conditional quantile level. When tau = 0.5, this amounts to l1\n        regression, an estimator of the conditional median (0.5 quantile).\n      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n        loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n        option will be determined by the usage context. For almost all cases\n        this defaults to `SUM_OVER_BATCH_SIZE`.\n        When used with `tf.distribute.Strategy`, outside of built-in training\n        loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n        `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n        https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n        for more details on this.\n      name: Optional name for the op.\n\n    References:\n      - https://en.wikipedia.org/wiki/Quantile_regression\n      - https://projecteuclid.org/download/pdfview_1/euclid.bj/1297173840\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        tau: FloatTensorLike = 0.5,\n        reduction: str = tf.keras.losses.Reduction.AUTO,\n        name: str = ""pinball_loss"",\n    ):\n        super().__init__(pinball_loss, reduction=reduction, name=name, tau=tau)\n'"
tensorflow_addons/losses/sparsemax_loss.py,19,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow_addons.activations.sparsemax import sparsemax\n\nfrom tensorflow_addons.utils.types import TensorLike\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef sparsemax_loss(\n    logits: TensorLike,\n    sparsemax: TensorLike,\n    labels: TensorLike,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Sparsemax loss function [1].\n\n    Computes the generalized multi-label classification loss for the sparsemax\n    function. The implementation is a reformulation of the original loss\n    function such that it uses the sparsemax properbility output instead of the\n    internal \\tau variable. However, the output is identical to the original\n    loss function.\n\n    [1]: https://arxiv.org/abs/1602.02068\n\n    Args:\n      logits: A `Tensor`. Must be one of the following types: `float32`,\n        `float64`.\n      sparsemax: A `Tensor`. Must have the same type as `logits`.\n      labels: A `Tensor`. Must have the same type as `logits`.\n      name: A name for the operation (optional).\n    Returns:\n      A `Tensor`. Has the same type as `logits`.\n    """"""\n    logits = tf.convert_to_tensor(logits, name=""logits"")\n    sparsemax = tf.convert_to_tensor(sparsemax, name=""sparsemax"")\n    labels = tf.convert_to_tensor(labels, name=""labels"")\n\n    # In the paper, they call the logits z.\n    # A constant can be substracted from logits to make the algorithm\n    # more numerically stable in theory. However, there are really no major\n    # source numerical instability in this algorithm.\n    z = logits\n\n    # sum over support\n    # Use a conditional where instead of a multiplication to support z = -inf.\n    # If z = -inf, and there is no support (sparsemax = 0), a multiplication\n    # would cause 0 * -inf = nan, which is not correct in this case.\n    sum_s = tf.where(\n        tf.math.logical_or(sparsemax > 0, tf.math.is_nan(sparsemax)),\n        sparsemax * (z - 0.5 * sparsemax),\n        tf.zeros_like(sparsemax),\n    )\n\n    # - z_k + ||q||^2\n    q_part = labels * (0.5 * labels - z)\n    # Fix the case where labels = 0 and z = -inf, where q_part would\n    # otherwise be 0 * -inf = nan. But since the lables = 0, no cost for\n    # z = -inf should be consideredself.\n    # The code below also coveres the case where z = inf. Howeverm in this\n    # caose the sparsemax will be nan, which means the sum_s will also be nan,\n    # therefor this case doesn\'t need addtional special treatment.\n    q_part_safe = tf.where(\n        tf.math.logical_and(tf.math.equal(labels, 0), tf.math.is_inf(z)),\n        tf.zeros_like(z),\n        q_part,\n    )\n\n    return tf.math.reduce_sum(sum_s + q_part_safe, axis=1)\n\n\n@tf.function\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\ndef sparsemax_loss_from_logits(\n    y_true: TensorLike, logits_pred: TensorLike\n) -> tf.Tensor:\n    y_pred = sparsemax(logits_pred)\n    loss = sparsemax_loss(logits_pred, y_pred, y_true)\n    return loss\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass SparsemaxLoss(tf.keras.losses.Loss):\n    """"""Sparsemax loss function.\n\n    Computes the generalized multi-label classification loss for the sparsemax\n    function.\n\n    Because the sparsemax loss function needs both the properbility output and\n    the logits to compute the loss value, `from_logits` must be `True`.\n\n    Because it computes the generalized multi-label loss, the shape of both\n    `y_pred` and `y_true` must be `[batch_size, num_classes]`.\n\n    Args:\n      from_logits: Whether `y_pred` is expected to be a logits tensor. Default\n        is `True`, meaning `y_pred` is the logits.\n      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n        loss. Default value is `SUM_OVER_BATCH_SIZE`.\n      name: Optional name for the op.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        from_logits: bool = True,\n        reduction: str = tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name: str = ""sparsemax_loss"",\n    ):\n        if from_logits is not True:\n            raise ValueError(""from_logits must be True"")\n\n        super().__init__(name=name, reduction=reduction)\n        self.from_logits = from_logits\n\n    def call(self, y_true, y_pred):\n        return sparsemax_loss_from_logits(y_true, y_pred)\n\n    def get_config(self):\n        config = {\n            ""from_logits"": self.from_logits,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/losses/triplet.py,62,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements triplet loss.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.losses import metric_learning\nfrom tensorflow_addons.utils.keras_utils import LossFunctionWrapper\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike\nfrom typeguard import typechecked\nfrom typing import Optional, Union, Callable\n\n\ndef _masked_maximum(data, mask, dim=1):\n    """"""Computes the axis wise maximum over chosen elements.\n\n    Args:\n      data: 2-D float `Tensor` of size [n, m].\n      mask: 2-D Boolean `Tensor` of size [n, m].\n      dim: The dimension over which to compute the maximum.\n\n    Returns:\n      masked_maximums: N-D `Tensor`.\n        The maximized dimension is of size 1 after the operation.\n    """"""\n    axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n    masked_maximums = (\n        tf.math.reduce_max(\n            tf.math.multiply(data - axis_minimums, mask), dim, keepdims=True\n        )\n        + axis_minimums\n    )\n    return masked_maximums\n\n\ndef _masked_minimum(data, mask, dim=1):\n    """"""Computes the axis wise minimum over chosen elements.\n\n    Args:\n      data: 2-D float `Tensor` of size [n, m].\n      mask: 2-D Boolean `Tensor` of size [n, m].\n      dim: The dimension over which to compute the minimum.\n\n    Returns:\n      masked_minimums: N-D `Tensor`.\n        The minimized dimension is of size 1 after the operation.\n    """"""\n    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n    masked_minimums = (\n        tf.math.reduce_min(\n            tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n        )\n        + axis_maximums\n    )\n    return masked_minimums\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef triplet_semihard_loss(\n    y_true: TensorLike,\n    y_pred: TensorLike,\n    margin: FloatTensorLike = 1.0,\n    distance_metric: Union[str, Callable] = ""L2"",\n) -> tf.Tensor:\n    """"""Computes the triplet loss with semi-hard negative mining.\n\n    Args:\n      y_true: 1-D integer `Tensor` with shape [batch_size] of\n        multiclass integer labels.\n      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n        be l2 normalized.\n      margin: Float, margin term in the loss definition.\n      distance_metric: str or function, determines distance metric:\n                       ""L1"" for l1-norm distance\n                       ""L2"" for l2-norm distance\n                       ""angular"" for cosine similarity\n                        A custom function returning a 2d adjacency\n                          matrix of a chosen distance metric can\n                          also be passed here. e.g.\n\n                          def custom_distance(batch):\n                              batch = 1 - batch @ batch.T\n                              return batch\n\n                          triplet_semihard_loss(batch, labels,\n                                        distance_metric=custom_distance\n                                    )\n\n\n    Returns:\n      triplet_loss: float scalar with dtype of y_pred.\n    """"""\n\n    labels, embeddings = y_true, y_pred\n\n    convert_to_float32 = (\n        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n    )\n    precise_embeddings = (\n        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n    )\n\n    # Reshape label tensor to [batch_size, 1].\n    lshape = tf.shape(labels)\n    labels = tf.reshape(labels, [lshape[0], 1])\n\n    # Build pairwise squared distance matrix\n\n    if distance_metric == ""L1"":\n        pdist_matrix = metric_learning.pairwise_distance(\n            precise_embeddings, squared=False\n        )\n\n    elif distance_metric == ""L2"":\n        pdist_matrix = metric_learning.pairwise_distance(\n            precise_embeddings, squared=True\n        )\n\n    elif distance_metric == ""angular"":\n        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n\n    else:\n        pdist_matrix = distance_metric(precise_embeddings)\n\n    # Build pairwise binary adjacency matrix.\n    adjacency = tf.math.equal(labels, tf.transpose(labels))\n    # Invert so we can select negatives only.\n    adjacency_not = tf.math.logical_not(adjacency)\n\n    batch_size = tf.size(labels)\n\n    # Compute the mask.\n    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n    mask = tf.math.logical_and(\n        tf.tile(adjacency_not, [batch_size, 1]),\n        tf.math.greater(\n            pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n        ),\n    )\n    mask_final = tf.reshape(\n        tf.math.greater(\n            tf.math.reduce_sum(\n                tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n            ),\n            0.0,\n        ),\n        [batch_size, batch_size],\n    )\n    mask_final = tf.transpose(mask_final)\n\n    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n\n    # negatives_outside: smallest D_an where D_an > D_ap.\n    negatives_outside = tf.reshape(\n        _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size]\n    )\n    negatives_outside = tf.transpose(negatives_outside)\n\n    # negatives_inside: largest D_an.\n    negatives_inside = tf.tile(\n        _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size]\n    )\n    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n\n    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n\n    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n        tf.ones([batch_size])\n    )\n\n    # In lifted-struct, the authors multiply 0.5 for upper triangular\n    #   in semihard, they take all positive pairs except the diagonal.\n    num_positives = tf.math.reduce_sum(mask_positives)\n\n    triplet_loss = tf.math.truediv(\n        tf.math.reduce_sum(\n            tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)\n        ),\n        num_positives,\n    )\n\n    if convert_to_float32:\n        return tf.cast(triplet_loss, embeddings.dtype)\n    else:\n        return triplet_loss\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\n@tf.function\ndef triplet_hard_loss(\n    y_true: TensorLike,\n    y_pred: TensorLike,\n    margin: FloatTensorLike = 1.0,\n    soft: bool = False,\n    distance_metric: Union[str, Callable] = ""L2"",\n) -> tf.Tensor:\n    """"""Computes the triplet loss with hard negative and hard positive mining.\n\n    Args:\n      y_true: 1-D integer `Tensor` with shape [batch_size] of\n        multiclass integer labels.\n      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n        be l2 normalized.\n      margin: Float, margin term in the loss definition.\n      soft: Boolean, if set, use the soft margin version.\n      distance_metric: str or function, determines distance metric:\n                       ""L1"" for l1-norm distance\n                       ""L2"" for l2-norm distance\n                       ""angular"" for cosine similarity\n                        A custom function returning a 2d adjacency\n                          matrix of a chosen distance metric can\n                          also be passed here. e.g.\n\n                          def custom_distance(batch):\n                              batch = 1 - batch @ batch.T\n                              return batch\n\n                          triplet_semihard_loss(batch, labels,\n                                        distance_metric=custom_distance\n                                    )\n\n    Returns:\n      triplet_loss: float scalar with dtype of y_pred.\n    """"""\n    labels, embeddings = y_true, y_pred\n\n    convert_to_float32 = (\n        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n    )\n    precise_embeddings = (\n        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n    )\n\n    # Reshape label tensor to [batch_size, 1].\n    lshape = tf.shape(labels)\n    labels = tf.reshape(labels, [lshape[0], 1])\n\n    # Build pairwise squared distance matrix.\n    if distance_metric == ""L1"":\n        pdist_matrix = metric_learning.pairwise_distance(\n            precise_embeddings, squared=False\n        )\n\n    elif distance_metric == ""L2"":\n        pdist_matrix = metric_learning.pairwise_distance(\n            precise_embeddings, squared=True\n        )\n\n    elif distance_metric == ""angular"":\n        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n\n    else:\n        pdist_matrix = distance_metric(precise_embeddings)\n\n    # Build pairwise binary adjacency matrix.\n    adjacency = tf.math.equal(labels, tf.transpose(labels))\n    # Invert so we can select negatives only.\n    adjacency_not = tf.math.logical_not(adjacency)\n\n    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n    # hard negatives: smallest D_an.\n    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n\n    batch_size = tf.size(labels)\n\n    adjacency = tf.cast(adjacency, dtype=tf.dtypes.float32)\n\n    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n        tf.ones([batch_size])\n    )\n\n    # hard positives: largest D_ap.\n    hard_positives = _masked_maximum(pdist_matrix, mask_positives)\n\n    if soft:\n        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n    else:\n        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n\n    # Get final mean triplet loss\n    triplet_loss = tf.reduce_mean(triplet_loss)\n\n    if convert_to_float32:\n        return tf.cast(triplet_loss, embeddings.dtype)\n    else:\n        return triplet_loss\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TripletSemiHardLoss(LossFunctionWrapper):\n    """"""Computes the triplet loss with semi-hard negative mining.\n\n    The loss encourages the positive distances (between a pair of embeddings\n    with the same labels) to be smaller than the minimum negative distance\n    among which are at least greater than the positive distance plus the\n    margin constant (called semi-hard negative) in the mini-batch.\n    If no such negative exists, uses the largest negative distance instead.\n    See: https://arxiv.org/abs/1503.03832.\n\n    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n    2-D float `Tensor` of l2 normalized embedding vectors.\n\n    Args:\n      margin: Float, margin term in the loss definition. Default value is 1.0.\n      name: Optional name for the op.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        margin: FloatTensorLike = 1.0,\n        distance_metric: Union[str, Callable] = ""L2"",\n        name: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(\n            triplet_semihard_loss,\n            name=name,\n            reduction=tf.keras.losses.Reduction.NONE,\n            margin=margin,\n            distance_metric=distance_metric,\n        )\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TripletHardLoss(LossFunctionWrapper):\n    """"""Computes the triplet loss with hard negative and hard positive mining.\n\n    The loss encourages the maximum positive distance (between a pair of embeddings\n    with the same labels) to be smaller than the minimum negative distance plus the\n    margin constant in the mini-batch.\n    The loss selects the hardest positive and the hardest negative samples\n    within the batch when forming the triplets for computing the loss.\n    See: https://arxiv.org/pdf/1703.07737.\n\n    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n    2-D float `Tensor` of l2 normalized embedding vectors.\n\n    Args:\n      margin: Float, margin term in the loss definition. Default value is 1.0.\n      soft: Boolean, if set, use the soft margin version. Default value is False.\n      name: Optional name for the op.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        margin: FloatTensorLike = 1.0,\n        soft: bool = False,\n        distance_metric: Union[str, Callable] = ""L2"",\n        name: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(\n            triplet_hard_loss,\n            name=name,\n            reduction=tf.keras.losses.Reduction.NONE,\n            margin=margin,\n            soft=soft,\n            distance_metric=distance_metric,\n        )\n'"
tensorflow_addons/metrics/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional metrics that conform to Keras API.""""""\n\nfrom tensorflow_addons.metrics.cohens_kappa import CohenKappa\nfrom tensorflow_addons.metrics.f_scores import F1Score, FBetaScore\nfrom tensorflow_addons.metrics.hamming import HammingLoss, hamming_distance\nfrom tensorflow_addons.metrics.utils import MeanMetricWrapper\nfrom tensorflow_addons.metrics.matthews_correlation_coefficient import (\n    MatthewsCorrelationCoefficient,\n)\nfrom tensorflow_addons.metrics.multilabel_confusion_matrix import (\n    MultiLabelConfusionMatrix,\n)\nfrom tensorflow_addons.metrics.r_square import RSquare\n'"
tensorflow_addons/metrics/cohens_kappa.py,41,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Cohen\'s Kappa.""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass CohenKappa(Metric):\n    """"""Computes Kappa score between two raters.\n\n    The score lies in the range [-1, 1]. A score of -1 represents\n    complete disagreement between two raters whereas a score of 1\n    represents complete agreement between the two raters.\n    A score of 0 means agreement by chance.\n\n    Note: As of now, this implementation considers all labels\n    while calculating the Cohen\'s Kappa score.\n\n    Usage:\n\n    ```python\n    actuals = np.array([4, 4, 3, 4, 2, 4, 1, 1], dtype=np.int32)\n    preds = np.array([4, 4, 3, 4, 4, 2, 1, 1], dtype=np.int32)\n    weights = np.array([1, 1, 2, 5, 10, 2, 3, 3], dtype=np.int32)\n\n    m = tfa.metrics.CohenKappa(num_classes=5, sparse_labels=True)\n    m.update_state(actuals, preds)\n    print(\'Final result: \', m.result().numpy()) # Result: 0.61904764\n\n    # To use this with weights, sample_weight argument can be used.\n    m = tfa.metrics.CohenKappa(num_classes=5, sparse_labels=True)\n    m.update_state(actuals, preds, sample_weight=weights)\n    print(\'Final result: \', m.result().numpy()) # Result: 0.37209308\n    ```\n\n    Usage with tf.keras API:\n\n    ```python\n    model = tf.keras.models.Model(inputs, outputs)\n    model.add_metric(tfa.metrics.CohenKappa(num_classes=5)(outputs))\n    model.compile(\'sgd\', loss=\'mse\')\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: FloatTensorLike,\n        name: str = ""cohen_kappa"",\n        weightage: Optional[str] = None,\n        sparse_labels: bool = False,\n        regression: bool = False,\n        dtype: AcceptableDTypes = None,\n    ):\n        """"""Creates a `CohenKappa` instance.\n\n        Args:\n          num_classes: Number of unique classes in your dataset.\n          weightage: (optional) Weighting to be considered for calculating\n            kappa statistics. A valid value is one of\n            [None, \'linear\', \'quadratic\']. Defaults to `None`\n          sparse_lables: (bool) Valid only for multi-class scenario.\n            If True, ground truth labels are expected tp be integers\n            and not one-hot encoded\n          regression: (bool) If set, that means the problem is being treated\n            as a regression problem where you are regressing the predictions.\n            **Note:** If you are regressing for the values, the the output layer\n            should contain a single unit.\n          name: (optional) String name of the metric instance\n          dtype: (optional) Data type of the metric result. Defaults to `None`\n\n        Raises:\n          ValueError: If the value passed for `weightage` is invalid\n            i.e. not any one of [None, \'linear\', \'quadratic\']\n        """"""\n        super().__init__(name=name, dtype=dtype)\n\n        if weightage not in (None, ""linear"", ""quadratic""):\n            raise ValueError(""Unknown kappa weighting type."")\n\n        if num_classes == 2:\n            self._update = self._update_binary_class_model\n        elif num_classes > 2:\n            self._update = self._update_multi_class_model\n        else:\n            raise ValueError(\n                """"""Number of classes must be\n                              greater than or euqal to two""""""\n            )\n\n        self.weightage = weightage\n        self.num_classes = num_classes\n        self.regression = regression\n        self.sparse_labels = sparse_labels\n        self.conf_mtx = self.add_weight(\n            ""conf_mtx"",\n            shape=(self.num_classes, self.num_classes),\n            initializer=tf.keras.initializers.zeros,\n            dtype=tf.float32,\n        )\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        """"""Accumulates the confusion matrix condition statistics.\n\n        Args:\n          y_true: Labels assigned by the first annotator with shape\n            `[num_samples,]`.\n          y_pred: Labels assigned by the second annotator with shape\n            `[num_samples,]`. The kappa statistic is symmetric,\n            so swapping `y_true` and `y_pred` doesn\'t change the value.\n          sample_weight (optional): for weighting labels in confusion matrix\n            Defaults to `None`. The dtype for weights should be the same\n            as the dtype for confusion matrix. For more details,\n            please check `tf.math.confusion_matrix`.\n\n        Returns:\n          Update op.\n        """"""\n        return self._update(y_true, y_pred, sample_weight)\n\n    def _update_binary_class_model(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, dtype=tf.int64)\n        y_pred = tf.cast(y_pred, dtype=tf.float32)\n        y_pred = tf.cast(y_pred > 0.5, dtype=tf.int64)\n        return self._update_confusion_matrix(y_true, y_pred, sample_weight)\n\n    @tf.function\n    def _update_multi_class_model(self, y_true, y_pred, sample_weight=None):\n        if not self.sparse_labels:\n            y_true = tf.cast(tf.argmax(y_true, axis=-1), dtype=tf.int64)\n        else:\n            y_true = tf.cast(y_true, dtype=tf.int64)\n\n        y_pred = self._cast_ypred(y_pred)\n\n        return self._update_confusion_matrix(y_true, y_pred, sample_weight)\n\n    @tf.function\n    def _cast_ypred(self, y_pred):\n        if tf.rank(y_pred) > 1:\n            if not self.regression:\n                y_pred = tf.cast(tf.argmax(y_pred, axis=-1), dtype=tf.int64)\n            else:\n                y_pred = tf.math.round(tf.math.abs(y_pred))\n                y_pred = tf.cast(y_pred, dtype=tf.int64)\n        else:\n            y_pred = tf.cast(y_pred, dtype=tf.int64)\n        return y_pred\n\n    def _update_confusion_matrix(self, y_true, y_pred, sample_weight):\n        y_true = tf.squeeze(y_true)\n        y_pred = tf.squeeze(y_pred)\n\n        new_conf_mtx = tf.math.confusion_matrix(\n            labels=y_true,\n            predictions=y_pred,\n            num_classes=self.num_classes,\n            weights=sample_weight,\n            dtype=tf.float32,\n        )\n\n        return self.conf_mtx.assign_add(new_conf_mtx)\n\n    def result(self):\n        nb_ratings = tf.shape(self.conf_mtx)[0]\n        weight_mtx = tf.ones([nb_ratings, nb_ratings], dtype=tf.float32)\n\n        # 2. Create a weight matrix\n        if self.weightage is None:\n            diagonal = tf.zeros([nb_ratings], dtype=tf.float32)\n            weight_mtx = tf.linalg.set_diag(weight_mtx, diagonal=diagonal)\n        else:\n            weight_mtx += tf.cast(tf.range(nb_ratings), dtype=tf.float32)\n            weight_mtx = tf.cast(weight_mtx, dtype=self.dtype)\n\n            if self.weightage == ""linear"":\n                weight_mtx = tf.abs(weight_mtx - tf.transpose(weight_mtx))\n            else:\n                weight_mtx = tf.pow((weight_mtx - tf.transpose(weight_mtx)), 2)\n\n        weight_mtx = tf.cast(weight_mtx, dtype=self.dtype)\n\n        # 3. Get counts\n        actual_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=1)\n        pred_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=0)\n\n        # 4. Get the outer product\n        out_prod = pred_ratings_hist[..., None] * actual_ratings_hist[None, ...]\n\n        # 5. Normalize the confusion matrix and outer product\n        conf_mtx = self.conf_mtx / tf.reduce_sum(self.conf_mtx)\n        out_prod = out_prod / tf.reduce_sum(out_prod)\n\n        conf_mtx = tf.cast(conf_mtx, dtype=self.dtype)\n        out_prod = tf.cast(out_prod, dtype=self.dtype)\n\n        # 6. Calculate Kappa score\n        numerator = tf.reduce_sum(conf_mtx * weight_mtx)\n        denominator = tf.reduce_sum(out_prod * weight_mtx)\n        return tf.cond(\n            tf.math.is_nan(denominator),\n            true_fn=lambda: 0.0,\n            false_fn=lambda: 1 - (numerator / denominator),\n        )\n\n    def get_config(self):\n        """"""Returns the serializable config of the metric.""""""\n\n        config = {\n            ""num_classes"": self.num_classes,\n            ""weightage"": self.weightage,\n            ""sparse_labels"": self.sparse_labels,\n            ""regression"": self.regression,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def reset_states(self):\n        """"""Resets all of the metric state variables.""""""\n\n        for v in self.variables:\n            K.set_value(\n                v,\n                np.zeros((self.num_classes, self.num_classes), v.dtype.as_numpy_dtype),\n            )\n'"
tensorflow_addons/metrics/f_scores.py,22,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements F scores.""""""\n\nimport tensorflow as tf\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass FBetaScore(tf.keras.metrics.Metric):\n    """"""Computes F-Beta score.\n\n    It is the weighted harmonic mean of precision\n    and recall. Output range is [0, 1]. Works for\n    both multi-class and multi-label classification.\n\n    F-Beta = (1 + beta^2) * (prec * recall) / ((beta^2 * prec) + recall)\n\n    Args:\n        num_classes: Number of unique classes in the dataset.\n        average: Type of averaging to be performed on data.\n            Acceptable values are `None`, `micro`, `macro` and\n            `weighted`. Default value is None.\n        beta: Determines the weight of precision and recall\n            in harmonic mean. Determines the weight given to the\n            precision and recall. Default value is 1.\n        threshold: Elements of `y_pred` greater than threshold are\n            converted to be 1, and the rest 0. If threshold is\n            None, the argmax is converted to 1, and the rest 0.\n\n    Returns:\n        F-Beta Score: float\n\n    Raises:\n        ValueError: If the `average` has values other than\n        [None, micro, macro, weighted].\n\n        ValueError: If the `beta` value is less than or equal\n        to 0.\n\n    `average` parameter behavior:\n        None: Scores for each class are returned\n\n        micro: True positivies, false positives and\n            false negatives are computed globally.\n\n        macro: True positivies, false positives and\n            false negatives are computed for each class\n            and their unweighted mean is returned.\n\n        weighted: Metrics are computed for each class\n            and returns the mean weighted by the\n            number of true instances in each class.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: FloatTensorLike,\n        average: Optional[str] = None,\n        beta: FloatTensorLike = 1.0,\n        threshold: Optional[FloatTensorLike] = None,\n        name: str = ""fbeta_score"",\n        dtype: AcceptableDTypes = None,\n        **kwargs\n    ):\n        super().__init__(name=name, dtype=dtype)\n\n        if average not in (None, ""micro"", ""macro"", ""weighted""):\n            raise ValueError(\n                ""Unknown average type. Acceptable values ""\n                ""are: [None, micro, macro, weighted]""\n            )\n\n        if not isinstance(beta, float):\n            raise TypeError(""The value of beta should be a python float"")\n\n        if beta <= 0.0:\n            raise ValueError(""beta value should be greater than zero"")\n\n        if threshold is not None:\n            if not isinstance(threshold, float):\n                raise TypeError(""The value of threshold should be a python float"")\n            if threshold > 1.0 or threshold <= 0.0:\n                raise ValueError(""threshold should be between 0 and 1"")\n\n        self.num_classes = num_classes\n        self.average = average\n        self.beta = beta\n        self.threshold = threshold\n        self.axis = None\n        self.init_shape = []\n\n        if self.average != ""micro"":\n            self.axis = 0\n            self.init_shape = [self.num_classes]\n\n        def _zero_wt_init(name):\n            return self.add_weight(\n                name, shape=self.init_shape, initializer=""zeros"", dtype=self.dtype\n            )\n\n        self.true_positives = _zero_wt_init(""true_positives"")\n        self.false_positives = _zero_wt_init(""false_positives"")\n        self.false_negatives = _zero_wt_init(""false_negatives"")\n        self.weights_intermediate = _zero_wt_init(""weights_intermediate"")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        if self.threshold is None:\n            threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n            # make sure [0, 0, 0] doesn\'t become [1, 1, 1]\n            # Use abs(x) > eps, instead of x != 0 to check for zero\n            y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n        else:\n            y_pred = y_pred > self.threshold\n\n        y_true = tf.cast(y_true, self.dtype)\n        y_pred = tf.cast(y_pred, self.dtype)\n\n        def _weighted_sum(val, sample_weight):\n            if sample_weight is not None:\n                val = tf.math.multiply(val, tf.expand_dims(sample_weight, 1))\n            return tf.reduce_sum(val, axis=self.axis)\n\n        self.true_positives.assign_add(_weighted_sum(y_pred * y_true, sample_weight))\n        self.false_positives.assign_add(\n            _weighted_sum(y_pred * (1 - y_true), sample_weight)\n        )\n        self.false_negatives.assign_add(\n            _weighted_sum((1 - y_pred) * y_true, sample_weight)\n        )\n        self.weights_intermediate.assign_add(_weighted_sum(y_true, sample_weight))\n\n    def result(self):\n        precision = tf.math.divide_no_nan(\n            self.true_positives, self.true_positives + self.false_positives\n        )\n        recall = tf.math.divide_no_nan(\n            self.true_positives, self.true_positives + self.false_negatives\n        )\n\n        mul_value = precision * recall\n        add_value = (tf.math.square(self.beta) * precision) + recall\n        mean = tf.math.divide_no_nan(mul_value, add_value)\n        f1_score = mean * (1 + tf.math.square(self.beta))\n\n        if self.average == ""weighted"":\n            weights = tf.math.divide_no_nan(\n                self.weights_intermediate, tf.reduce_sum(self.weights_intermediate)\n            )\n            f1_score = tf.reduce_sum(f1_score * weights)\n\n        elif self.average is not None:  # [micro, macro]\n            f1_score = tf.reduce_mean(f1_score)\n\n        return f1_score\n\n    def get_config(self):\n        """"""Returns the serializable config of the metric.""""""\n\n        config = {\n            ""num_classes"": self.num_classes,\n            ""average"": self.average,\n            ""beta"": self.beta,\n            ""threshold"": self.threshold,\n        }\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def reset_states(self):\n        self.true_positives.assign(tf.zeros(self.init_shape, self.dtype))\n        self.false_positives.assign(tf.zeros(self.init_shape, self.dtype))\n        self.false_negatives.assign(tf.zeros(self.init_shape, self.dtype))\n        self.weights_intermediate.assign(tf.zeros(self.init_shape, self.dtype))\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass F1Score(FBetaScore):\n    """"""Computes F-1 Score.\n\n    It is the harmonic mean of precision and recall.\n    Output range is [0, 1]. Works for both multi-class\n    and multi-label classification.\n\n    F-1 = 2 * (precision * recall) / (precision + recall)\n\n    Args:\n        num_classes: Number of unique classes in the dataset.\n        average: Type of averaging to be performed on data.\n            Acceptable values are `None`, `micro`, `macro`\n            and `weighted`. Default value is None.\n        threshold: Elements of `y_pred` above threshold are\n            considered to be 1, and the rest 0. If threshold is\n            None, the argmax is converted to 1, and the rest 0.\n\n    Returns:\n        F-1 Score: float\n\n    Raises:\n        ValueError: If the `average` has values other than\n        [None, micro, macro, weighted].\n\n    `average` parameter behavior:\n        None: Scores for each class are returned\n\n        micro: True positivies, false positives and\n            false negatives are computed globally.\n\n        macro: True positivies, false positives and\n            false negatives are computed for each class\n            and their unweighted mean is returned.\n\n        weighted: Metrics are computed for each class\n            and returns the mean weighted by the\n            number of true instances in each class.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: FloatTensorLike,\n        average: str = None,\n        threshold: Optional[FloatTensorLike] = None,\n        name: str = ""f1_score"",\n        dtype: AcceptableDTypes = None,\n    ):\n        super().__init__(num_classes, average, 1.0, threshold, name=name, dtype=dtype)\n\n    def get_config(self):\n        base_config = super().get_config()\n        del base_config[""beta""]\n        return base_config\n'"
tensorflow_addons/metrics/hamming.py,23,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Hamming distance and loss.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.metrics.utils import MeanMetricWrapper\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike, AcceptableDTypes\n\nfrom typeguard import typechecked\nfrom typing import Union, Optional\n\n\ndef hamming_distance(actuals: TensorLike, predictions: TensorLike) -> tf.Tensor:\n    """"""Computes hamming distance.\n\n    Hamming distance is for comparing two binary strings.\n    It is the number of bit positions in which two bits\n    are different.\n\n    Args:\n        actuals: actual target value\n        predictions: predicted value\n\n    Returns:\n        hamming distance: float\n\n    Usage:\n\n    ```python\n    actuals = tf.constant([1, 1, 0, 0, 1, 0, 1, 0, 0, 1],\n                          dtype=tf.int32)\n    predictions = tf.constant([1, 0, 0, 0, 1, 0, 0, 1, 0, 1],\n                              dtype=tf.int32)\n    result = hamming_distance(actuals, predictions)\n    print(\'Hamming distance: \', result.numpy())\n    ```\n    """"""\n    result = tf.not_equal(actuals, predictions)\n    not_eq = tf.reduce_sum(tf.cast(result, tf.float32))\n    ham_distance = tf.math.divide_no_nan(not_eq, len(result))\n    return ham_distance\n\n\ndef hamming_loss_fn(\n    y_true: TensorLike,\n    y_pred: TensorLike,\n    threshold: Union[FloatTensorLike, None],\n    mode: str,\n) -> tf.Tensor:\n    """"""Computes hamming loss.\n\n    Hamming loss is the fraction of wrong labels to the total number\n    of labels.\n\n    In multi-class classification, hamming loss is calculated as the\n    hamming distance between `actual` and `predictions`.\n    In multi-label classification, hamming loss penalizes only the\n    individual labels.\n\n    Args:\n\n        y_true: actual target value\n        y_pred: predicted target value\n        threshold: Elements of `y_pred` greater than threshold are\n            converted to be 1, and the rest 0. If threshold is\n            None, the argmax is converted to 1, and the rest 0.\n        mode: multi-class or multi-label\n\n    Returns:\n\n        hamming loss: float\n\n    Usage:\n\n    ```python\n    # multi-class hamming loss\n    hl = HammingLoss(mode=\'multiclass\', threshold=0.6)\n    actuals = tf.constant([[1, 0, 0, 0],[0, 0, 1, 0],\n                       [0, 0, 0, 1],[0, 1, 0, 0]],\n                      dtype=tf.float32)\n    predictions = tf.constant([[0.8, 0.1, 0.1, 0],\n                               [0.2, 0, 0.8, 0],\n                               [0.05, 0.05, 0.1, 0.8],\n                               [1, 0, 0, 0]],\n                          dtype=tf.float32)\n    hl.update_state(actuals, predictions)\n    print(\'Hamming loss: \', hl.result().numpy()) # 0.25\n\n    # multi-label hamming loss\n    hl = HammingLoss(mode=\'multilabel\', threshold=0.8)\n    actuals = tf.constant([[1, 0, 1, 0],[0, 1, 0, 1],\n                       [0, 0, 0,1]], dtype=tf.int32)\n    predictions = tf.constant([[0.82, 0.5, 0.90, 0],\n                               [0, 1, 0.4, 0.98],\n                               [0.89, 0.79, 0, 0.3]],\n                               dtype=tf.float32)\n    hl.update_state(actuals, predictions)\n    print(\'Hamming loss: \', hl.result().numpy()) # 0.16666667\n    ```\n    """"""\n    if mode not in [""multiclass"", ""multilabel""]:\n        raise TypeError(""mode must be either multiclass or multilabel]"")\n\n    if threshold is None:\n        threshold = tf.reduce_max(y_pred, axis=-1, keepdims=True)\n        # make sure [0, 0, 0] doesn\'t become [1, 1, 1]\n        # Use abs(x) > eps, instead of x != 0 to check for zero\n        y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n    else:\n        y_pred = y_pred > threshold\n\n    y_true = tf.cast(y_true, tf.int32)\n    y_pred = tf.cast(y_pred, tf.int32)\n\n    if mode == ""multiclass"":\n        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n        return 1.0 - nonzero\n\n    else:\n        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n        return nonzero / y_true.get_shape()[-1]\n\n\nclass HammingLoss(MeanMetricWrapper):\n    """"""Computes hamming loss.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        mode: str,\n        name: str = ""hamming_loss"",\n        threshold: Optional[FloatTensorLike] = None,\n        dtype: AcceptableDTypes = None,\n        **kwargs\n    ):\n        super().__init__(\n            hamming_loss_fn, name=name, dtype=dtype, mode=mode, threshold=threshold\n        )\n'"
tensorflow_addons/metrics/matthews_correlation_coefficient.py,26,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Matthews Correlation Coefficient Implementation.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass MatthewsCorrelationCoefficient(tf.keras.metrics.Metric):\n    """"""Computes the Matthews Correlation Coefficient.\n\n    The statistic is also known as the phi coefficient.\n    The Matthews correlation coefficient (MCC) is used in\n    machine learning as a measure of the quality of binary\n    and multiclass classifications. It takes into account\n    true and false positives and negatives and is generally\n    regarded as a balanced measure which can be used even\n    if the classes are of very different sizes. The correlation\n    coefficient value of MCC is between -1 and +1. A\n    coefficient of +1 represents a perfect prediction,\n    0 an average random prediction and -1 an inverse\n    prediction. The statistic is also known as\n    the phi coefficient.\n\n    MCC = (TP * TN) - (FP * FN) /\n          ((TP + FP) * (TP + FN) * (TN + FP ) * (TN + FN))^(1/2)\n\n    Usage:\n    ```python\n    actuals = tf.constant([[1.0], [1.0], [1.0], [0.0]],\n             dtype=tf.float32)\n    preds = tf.constant([[1.0], [0.0], [1.0], [1.0]],\n             dtype=tf.float32)\n    # Matthews correlation coefficient\n    mcc = MatthewsCorrelationCoefficient(num_classes=1)\n    mcc.update_state(actuals, preds)\n    print(\'Matthews correlation coefficient is:\',\n    mcc.result().numpy())\n    # Matthews correlation coefficient is : -0.33333334\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: FloatTensorLike,\n        name: str = ""MatthewsCorrelationCoefficient"",\n        dtype: AcceptableDTypes = None,\n        **kwargs\n    ):\n        """"""Creates a Matthews Correlation Coefficient instance.\n\n        Args:\n            num_classes : Number of unique classes in the dataset.\n            name: (Optional) String name of the metric instance.\n            dtype: (Optional) Data type of the metric result.\n            Defaults to `tf.float32`.\n        """"""\n        super().__init__(name=name, dtype=dtype)\n        self.num_classes = num_classes\n        self.true_positives = self.add_weight(\n            ""true_positives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.false_positives = self.add_weight(\n            ""false_positives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.false_negatives = self.add_weight(\n            ""false_negatives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.true_negatives = self.add_weight(\n            ""true_negatives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n\n    # TODO: sample_weights\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.cast(y_true, dtype=self.dtype)\n        y_pred = tf.cast(y_pred, dtype=self.dtype)\n\n        true_positive = tf.math.count_nonzero(y_true * y_pred, 0)\n        # true_negative\n        y_true_negative = tf.math.not_equal(y_true, 1.0)\n        y_pred_negative = tf.math.not_equal(y_pred, 1.0)\n        true_negative = tf.math.count_nonzero(\n            tf.math.logical_and(y_true_negative, y_pred_negative), axis=0\n        )\n        # predicted sum\n        pred_sum = tf.math.count_nonzero(y_pred, 0)\n        # Ground truth label sum\n        true_sum = tf.math.count_nonzero(y_true, 0)\n        false_positive = pred_sum - true_positive\n        false_negative = true_sum - true_positive\n\n        # true positive state_update\n        self.true_positives.assign_add(tf.cast(true_positive, self.dtype))\n        # false positive state_update\n        self.false_positives.assign_add(tf.cast(false_positive, self.dtype))\n        # false negative state_update\n        self.false_negatives.assign_add(tf.cast(false_negative, self.dtype))\n        # true negative state_update\n        self.true_negatives.assign_add(tf.cast(true_negative, self.dtype))\n\n    def result(self):\n        # numerator\n        numerator1 = self.true_positives * self.true_negatives\n        numerator2 = self.false_positives * self.false_negatives\n        numerator = numerator1 - numerator2\n        # denominator\n        denominator1 = self.true_positives + self.false_positives\n        denominator2 = self.true_positives + self.false_negatives\n        denominator3 = self.true_negatives + self.false_positives\n        denominator4 = self.true_negatives + self.false_negatives\n        denominator = tf.math.sqrt(\n            denominator1 * denominator2 * denominator3 * denominator4\n        )\n        mcc = tf.math.divide_no_nan(numerator, denominator)\n        return mcc\n\n    def get_config(self):\n        """"""Returns the serializable config of the metric.""""""\n\n        config = {\n            ""num_classes"": self.num_classes,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def reset_states(self):\n        """"""Resets all of the metric state variables.""""""\n        self.true_positives.assign(tf.zeros((self.num_classes), self.dtype))\n        self.false_positives.assign(tf.zeros((self.num_classes), self.dtype))\n        self.false_negatives.assign(tf.zeros((self.num_classes), self.dtype))\n        self.true_negatives.assign(tf.zeros((self.num_classes), self.dtype))\n'"
tensorflow_addons/metrics/multilabel_confusion_matrix.py,23,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements Multi-label confusion matrix scores.""""""\n\nimport warnings\n\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Metric\nimport numpy as np\n\nfrom typeguard import typechecked\nfrom tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\n\n\nclass MultiLabelConfusionMatrix(Metric):\n    """"""Computes Multi-label confusion matrix.\n\n    Class-wise confusion matrix is computed for the\n    evaluation of classification.\n\n    If multi-class input is provided, it will be treated\n    as multilabel data.\n\n    Consider classification problem with two classes\n    (i.e num_classes=2).\n\n    Resultant matrix `M` will be in the shape of (num_classes, 2, 2).\n\n    Every class `i` has a dedicated 2*2 matrix that contains:\n\n    - true negatives for class i in M(0,0)\n    - false positives for class i in M(0,1)\n    - false negatives for class i in M(1,0)\n    - true positives for class i in M(1,1)\n\n    ```python\n    # multilabel confusion matrix\n    y_true = tf.constant([[1, 0, 1], [0, 1, 0]],\n             dtype=tf.int32)\n    y_pred = tf.constant([[1, 0, 0],[0, 1, 1]],\n             dtype=tf.int32)\n    output = MultiLabelConfusionMatrix(num_classes=3)\n    output.update_state(y_true, y_pred)\n    print(\'Confusion matrix:\', output.result().numpy())\n\n    # Confusion matrix: [[[1 0] [0 1]] [[1 0] [0 1]]\n                      [[0 1] [1 0]]]\n\n    # if multiclass input is provided\n    y_true = tf.constant([[1, 0, 0], [0, 1, 0]],\n             dtype=tf.int32)\n    y_pred = tf.constant([[1, 0, 0],[0, 0, 1]],\n             dtype=tf.int32)\n    output = MultiLabelConfusionMatrix(num_classes=3)\n    output.update_state(y_true, y_pred)\n    print(\'Confusion matrix:\', output.result().numpy())\n\n    # Confusion matrix: [[[1 0] [0 1]] [[1 0] [1 0]] [[1 1] [0 0]]]\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        num_classes: FloatTensorLike,\n        name: str = ""Multilabel_confusion_matrix"",\n        dtype: AcceptableDTypes = None,\n        **kwargs\n    ):\n        super().__init__(name=name, dtype=dtype)\n        self.num_classes = num_classes\n        self.true_positives = self.add_weight(\n            ""true_positives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.false_positives = self.add_weight(\n            ""false_positives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.false_negatives = self.add_weight(\n            ""false_negatives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n        self.true_negatives = self.add_weight(\n            ""true_negatives"",\n            shape=[self.num_classes],\n            initializer=""zeros"",\n            dtype=self.dtype,\n        )\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        if sample_weight is not None:\n            warnings.warn(\n                ""`sample_weight` is not None. Be aware that MultiLabelConfusionMatrix ""\n                ""does not take `sample_weight` into account when computing the metric ""\n                ""value.""\n            )\n\n        y_true = tf.cast(y_true, tf.int32)\n        y_pred = tf.cast(y_pred, tf.int32)\n        # true positive\n        true_positive = tf.math.count_nonzero(y_true * y_pred, 0)\n        # predictions sum\n        pred_sum = tf.math.count_nonzero(y_pred, 0)\n        # true labels sum\n        true_sum = tf.math.count_nonzero(y_true, 0)\n        false_positive = pred_sum - true_positive\n        false_negative = true_sum - true_positive\n        y_true_negative = tf.math.not_equal(y_true, 1)\n        y_pred_negative = tf.math.not_equal(y_pred, 1)\n        true_negative = tf.math.count_nonzero(\n            tf.math.logical_and(y_true_negative, y_pred_negative), axis=0\n        )\n\n        # true positive state update\n        self.true_positives.assign_add(tf.cast(true_positive, self.dtype))\n        # false positive state update\n        self.false_positives.assign_add(tf.cast(false_positive, self.dtype))\n        # false negative state update\n        self.false_negatives.assign_add(tf.cast(false_negative, self.dtype))\n        # true negative state update\n        self.true_negatives.assign_add(tf.cast(true_negative, self.dtype))\n\n    def result(self):\n        flat_confusion_matrix = tf.convert_to_tensor(\n            [\n                self.true_negatives,\n                self.false_positives,\n                self.false_negatives,\n                self.true_positives,\n            ]\n        )\n        # reshape into 2*2 matrix\n        confusion_matrix = tf.reshape(tf.transpose(flat_confusion_matrix), [-1, 2, 2])\n\n        return confusion_matrix\n\n    def get_config(self):\n        """"""Returns the serializable config of the metric.""""""\n\n        config = {\n            ""num_classes"": self.num_classes,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def reset_states(self):\n        self.true_positives.assign(np.zeros(self.num_classes), np.int32)\n        self.false_positives.assign(np.zeros(self.num_classes), np.int32)\n        self.false_negatives.assign(np.zeros(self.num_classes), np.int32)\n        self.true_negatives.assign(np.zeros(self.num_classes), np.int32)\n'"
tensorflow_addons/metrics/r_square.py,18,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implements R^2 scores.""""""\nfrom typing import Tuple\n\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow.python.ops import weights_broadcast_ops\n\nfrom typeguard import typechecked\nfrom tensorflow_addons.utils.types import AcceptableDTypes\n\n\nVALID_MULTIOUTPUT = {""raw_values"", ""uniform_average"", ""variance_weighted""}\n\n\ndef _reduce_average(\n    input_tensor: tf.Tensor, axis=None, keepdims=False, weights=None\n) -> tf.Tensor:\n    """"""Computes the (weighted) mean of elements across dimensions of a tensor.\n  """"""\n    if weights is None:\n        return tf.reduce_mean(input_tensor, axis=axis, keepdims=keepdims)\n\n    weighted_sum = tf.reduce_sum(weights * input_tensor, axis=axis, keepdims=keepdims)\n    sum_of_weights = tf.reduce_sum(weights, axis=axis, keepdims=keepdims)\n    average = weighted_sum / sum_of_weights\n    return average\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass RSquare(Metric):\n    """"""Compute R^2 score.\n\n     This is also called the [coefficient of determination\n     ](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n     It tells how close are data to the fitted regression line.\n\n     - Highest score can be 1.0 and it indicates that the predictors\n       perfectly accounts for variation in the target.\n     - Score 0.0 indicates that the predictors do not\n       account for variation in the target.\n     - It can also be negative if the model is worse.\n\n     The sample weighting for this metric implementation mimics the\n     behaviour of the [scikit-learn implementation\n     ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n     of the same metric.\n\n     Usage:\n     ```python\n     actuals = tf.constant([1, 4, 3], dtype=tf.float32)\n     preds = tf.constant([2, 4, 4], dtype=tf.float32)\n     result = tf.keras.metrics.RSquare()\n     result.update_state(actuals, preds)\n     print(\'R^2 score is: \', r1.result().numpy()) # 0.57142866\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        name: str = ""r_square"",\n        dtype: AcceptableDTypes = None,\n        y_shape: Tuple[int, ...] = (),\n        multioutput: str = ""uniform_average"",\n        **kwargs\n    ):\n        super().__init__(name=name, dtype=dtype, **kwargs)\n        self.y_shape = y_shape\n\n        if multioutput not in VALID_MULTIOUTPUT:\n            raise ValueError(\n                ""The multioutput argument must be one of {}, but was: {}"".format(\n                    VALID_MULTIOUTPUT, multioutput\n                )\n            )\n        self.multioutput = multioutput\n        self.squared_sum = self.add_weight(\n            name=""squared_sum"", shape=y_shape, initializer=""zeros"", dtype=dtype\n        )\n        self.sum = self.add_weight(\n            name=""sum"", shape=y_shape, initializer=""zeros"", dtype=dtype\n        )\n        self.res = self.add_weight(\n            name=""residual"", shape=y_shape, initializer=""zeros"", dtype=dtype\n        )\n        self.count = self.add_weight(\n            name=""count"", shape=y_shape, initializer=""zeros"", dtype=dtype\n        )\n\n    def update_state(self, y_true, y_pred, sample_weight=None) -> None:\n        y_true = tf.cast(y_true, dtype=self._dtype)\n        y_pred = tf.cast(y_pred, dtype=self._dtype)\n        if sample_weight is None:\n            sample_weight = 1\n        sample_weight = tf.cast(sample_weight, dtype=self._dtype)\n        sample_weight = weights_broadcast_ops.broadcast_weights(\n            weights=sample_weight, values=y_true\n        )\n\n        weighted_y_true = y_true * sample_weight\n        self.sum.assign_add(tf.reduce_sum(weighted_y_true, axis=0))\n        self.squared_sum.assign_add(tf.reduce_sum(y_true * weighted_y_true, axis=0))\n        self.res.assign_add(\n            tf.reduce_sum((y_true - y_pred) ** 2 * sample_weight, axis=0,)\n        )\n        self.count.assign_add(tf.reduce_sum(sample_weight, axis=0))\n\n    def result(self) -> tf.Tensor:\n        mean = self.sum / self.count\n        total = self.squared_sum - self.sum * mean\n        raw_scores = 1 - (self.res / total)\n\n        if self.multioutput == ""raw_values"":\n            return raw_scores\n        elif self.multioutput == ""uniform_average"":\n            return tf.reduce_mean(raw_scores)\n        elif self.multioutput == ""variance_weighted"":\n            return _reduce_average(raw_scores, weights=total)\n        else:\n            raise RuntimeError(\n                ""The multioutput attribute must be one of {}, but was: {}"".format(\n                    VALID_MULTIOUTPUT, self.multioutput\n                )\n            )\n\n    def reset_states(self) -> None:\n        # The state of the metric will be reset at the start of each epoch.\n        self.squared_sum.assign(0)\n        self.sum.assign(0)\n        self.res.assign(0)\n        self.count.assign(0)\n'"
tensorflow_addons/metrics/utils.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for metrics.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import AcceptableDTypes\n\nfrom typeguard import typechecked\nfrom typing import Optional, Callable\n\n\nclass MeanMetricWrapper(tf.keras.metrics.Mean):\n    """"""Wraps a stateless metric function with the Mean metric.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        fn: Callable,\n        name: Optional[str] = None,\n        dtype: AcceptableDTypes = None,\n        **kwargs\n    ):\n        """"""Creates a `MeanMetricWrapper` instance.\n        Args:\n          fn: The metric function to wrap, with signature\n            `fn(y_true, y_pred, **kwargs)`.\n          name: (Optional) string name of the metric instance.\n          dtype: (Optional) data type of the metric result.\n          **kwargs: The keyword arguments that are passed on to `fn`.\n        """"""\n        super().__init__(name=name, dtype=dtype)\n        self._fn = fn\n        self._fn_kwargs = kwargs\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        """"""Accumulates metric statistics.\n\n        `y_true` and `y_pred` should have the same shape.\n        Args:\n          y_true: The ground truth values.\n          y_pred: The predicted values.\n          sample_weight: Optional weighting of each example. Defaults to 1.\n            Can be a `Tensor` whose rank is either 0, or the same rank as\n            `y_true`, and must be broadcastable to `y_true`.\n        Returns:\n          Update op.\n        """"""\n        y_true = tf.cast(y_true, self._dtype)\n        y_pred = tf.cast(y_pred, self._dtype)\n        # TODO: Add checks for ragged tensors and dimensions:\n        #   `ragged_assert_compatible_and_get_flat_values`\n        #   and `squeeze_or_expand_dimensions`\n        matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n        return super().update_state(matches, sample_weight=sample_weight)\n\n    def get_config(self):\n        config = {}\n        for k, v in self._fn_kwargs.items():\n            config[k] = v\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\ndef _get_model(metric, num_output):\n    # Test API comptibility with tf.keras Model\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(64, activation=""relu""))\n    model.add(tf.keras.layers.Dense(num_output, activation=""softmax""))\n    model.compile(\n        optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""acc"", metric]\n    )\n\n    data = np.random.random((10, 3))\n    labels = np.random.random((10, num_output))\n    model.fit(data, labels, epochs=1, batch_size=5, verbose=0)\n'"
tensorflow_addons/optimizers/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional optimizers that conform to Keras API.""""""\n\nfrom tensorflow_addons.optimizers.average_wrapper import AveragedOptimizerWrapper\nfrom tensorflow_addons.optimizers.conditional_gradient import ConditionalGradient\nfrom tensorflow_addons.optimizers.cyclical_learning_rate import CyclicalLearningRate\nfrom tensorflow_addons.optimizers.cyclical_learning_rate import (\n    TriangularCyclicalLearningRate,\n)\nfrom tensorflow_addons.optimizers.cyclical_learning_rate import (\n    Triangular2CyclicalLearningRate,\n)\nfrom tensorflow_addons.optimizers.cyclical_learning_rate import (\n    ExponentialCyclicalLearningRate,\n)\nfrom tensorflow_addons.optimizers.lamb import LAMB\nfrom tensorflow_addons.optimizers.lazy_adam import LazyAdam\nfrom tensorflow_addons.optimizers.lookahead import Lookahead\nfrom tensorflow_addons.optimizers.moving_average import MovingAverage\nfrom tensorflow_addons.optimizers.novograd import NovoGrad\nfrom tensorflow_addons.optimizers.rectified_adam import RectifiedAdam\nfrom tensorflow_addons.optimizers.stochastic_weight_averaging import SWA\nfrom tensorflow_addons.optimizers.weight_decay_optimizers import AdamW\nfrom tensorflow_addons.optimizers.weight_decay_optimizers import SGDW\nfrom tensorflow_addons.optimizers.weight_decay_optimizers import (\n    extend_with_decoupled_weight_decay,\n)\nfrom tensorflow_addons.optimizers.yogi import Yogi\n'"
tensorflow_addons/optimizers/average_wrapper.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport abc\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils import types\n\nimport warnings\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\nclass AveragedOptimizerWrapper(tf.keras.optimizers.Optimizer, metaclass=abc.ABCMeta):\n    @typechecked\n    def __init__(\n        self,\n        optimizer: types.Optimizer,\n        sequential_update: Optional[bool] = None,\n        name: str = ""AverageOptimizer"",\n        **kwargs\n    ):\n        super().__init__(name, **kwargs)\n\n        if isinstance(optimizer, str):\n            optimizer = tf.keras.optimizers.get(optimizer)\n\n        if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n            raise TypeError(\n                ""optimizer is not an object of tf.keras.optimizers.Optimizer""\n            )\n\n        if not isinstance(sequential_update, bool):\n            raise TypeError(""sequential_update must be of bool type"")\n\n        self._optimizer = optimizer\n\n        if sequential_update is not None:\n            warnings.warn(\n                ""The parameter `sequential_update` is redundant due to AutoGraph. ""\n                ""This behavior is deprecated and in Addons 0.12, this will raise an error. "",\n                DeprecationWarning,\n            )\n\n    def _create_slots(self, var_list):\n        self._optimizer._create_slots(var_list=var_list)\n        for var in var_list:\n            self.add_slot(var, ""average"")\n\n    def _create_hypers(self):\n        self._optimizer._create_hypers()\n\n    def _prepare(self, var_list):\n        return self._optimizer._prepare(var_list=var_list)\n\n    def apply_gradients(self, grads_and_vars, name=None):\n        self._optimizer._iterations = self.iterations\n        return super().apply_gradients(grads_and_vars, name)\n\n    @abc.abstractmethod\n    def average_op(self, var, average_var):\n        raise NotImplementedError\n\n    def _apply_average_op(self, train_op, var):\n        average_var = self.get_slot(var, ""average"")\n        return self.average_op(var, average_var)\n\n    def _resource_apply_dense(self, grad, var):\n        train_op = self._optimizer._resource_apply_dense(grad, var)\n        average_op = self._apply_average_op(train_op, var)\n        return tf.group(train_op, average_op)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        train_op = self._optimizer._resource_apply_sparse(grad, var, indices)\n        average_op = self._apply_average_op(train_op, var)\n        return tf.group(train_op, average_op)\n\n    def _resource_apply_sparse_duplicate_indices(self, grad, var, indices):\n        train_op = self._optimizer._resource_apply_sparse_duplicate_indices(\n            grad, var, indices\n        )\n        average_op = self._apply_average_op(train_op, var)\n        return tf.group(train_op, average_op)\n\n    def assign_average_vars(self, var_list):\n        """"""Assign variables in var_list with their respective averages.\n\n        Args:\n            var_list: List of model variables to be assigned to their average.\n\n        Returns:\n            assign_op: The op corresponding to the assignment operation of\n            variables to their average.\n\n        Example:\n        ```python\n        model = tf.Sequential([...])\n        opt = tfa.optimizers.SWA(\n                tf.keras.optimizers.SGD(lr=2.0), 100, 10)\n        model.compile(opt, ...)\n        model.fit(x, y, ...)\n\n        # Update the weights to their mean before saving\n        opt.assign_average_vars(model.variables)\n\n        model.save(\'model.h5\')\n        ```\n        """"""\n        assign_op = tf.group(\n            [\n                var.assign(self.get_slot(var, ""average""))\n                for var in var_list\n                if var.trainable\n            ]\n        )\n        return assign_op\n\n    def get_config(self):\n        config = {\n            ""optimizer"": tf.keras.optimizers.serialize(self._optimizer),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        optimizer = tf.keras.optimizers.deserialize(\n            config.pop(""optimizer""), custom_objects=custom_objects,\n        )\n        return cls(optimizer, **config)\n\n    @property\n    def weights(self):\n        return self._weights + self._optimizer.weights\n\n    @property\n    def lr(self):\n        return self._optimizer._get_hyper(""learning_rate"")\n\n    @lr.setter\n    def lr(self, lr):\n        self._optimizer._set_hyper(""learning_rate"", lr)  #\n\n    @property\n    def learning_rate(self):\n        return self._optimizer._get_hyper(""learning_rate"")\n\n    @learning_rate.setter\n    def learning_rate(self, learning_rate):\n        self._optimizer._set_hyper(""learning_rate"", learning_rate)\n'"
tensorflow_addons/optimizers/conditional_gradient.py,36,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Conditional Gradient optimizer.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass ConditionalGradient(tf.keras.optimizers.Optimizer):\n    """"""Optimizer that implements the Conditional Gradient optimization.\n\n    This optimizer helps handle constraints well.\n\n    Currently only supports frobenius norm constraint or nuclear norm\n    constraint.\n    See https://arxiv.org/pdf/1803.06453.pdf\n\n    ```\n    variable -= (1-learning_rate) * (variable + lambda_ * gradient\n        / (frobenius_norm(gradient) + epsilon))\n    ```\n\n    Note that `lambda_` here refers to the constraint ""lambda"" in\n    the paper. `epsilon` is constant with tiny value as compared to\n    the value of frobenius norm of gradient. The purpose of `epsilon`\n    here is to avoid the case that the value of frobenius norm of\n    gradient is 0.\n\n    In this implementation, `epsilon` defaults to $10^{-7}$.\n\n    For nucler norm constraint, the formula is as following:\n\n    ```\n    variable -= (1-learning_rate) * (variable\n        + lambda_ * top_singular_vector(gradient))\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable],\n        lambda_: Union[FloatTensorLike, Callable] = 0.01,\n        epsilon: FloatTensorLike = 1e-7,\n        ord: str = ""fro"",\n        use_locking: bool = False,\n        name: str = ""ConditionalGradient"",\n        **kwargs\n    ):\n        """"""Construct a new conditional gradient optimizer.\n\n        Args:\n            learning_rate: A `Tensor` or a floating point value. or a schedule\n                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\n                The learning rate.\n            lambda_: A `Tensor` or a floating point value. The constraint.\n            epsilon: A `Tensor` or a floating point value. A small constant\n                for numerical stability when handling the case of norm of\n                gradient to be zero.\n            ord: Order of the norm. Supported values are `\'fro\'`\n                and `\'nuclear\'`. Default is `\'fro\'`, which is frobenius norm.\n            use_locking: If `True`, use locks for update operations.\n            name: Optional name prefix for the operations created when\n                applying gradients. Defaults to \'ConditionalGradient\'.\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n                by norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(name=name, **kwargs)\n        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))\n        self._set_hyper(""lambda_"", lambda_)\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\n        supported_norms = [""fro"", ""nuclear""]\n        if ord not in supported_norms:\n            raise ValueError(\n                ""\'ord\' must be a supported matrix norm in %s, got \'%s\' instead""\n                % (supported_norms, ord)\n            )\n        self.ord = ord\n        self._set_hyper(""use_locking"", use_locking)\n\n    def get_config(self):\n        config = {\n            ""learning_rate"": self._serialize_hyperparameter(""learning_rate""),\n            ""lambda_"": self._serialize_hyperparameter(""lambda_""),\n            ""epsilon"": self.epsilon,\n            ""ord"": self.ord,\n            ""use_locking"": self._serialize_hyperparameter(""use_locking""),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def _create_slots(self, var_list):\n        for v in var_list:\n            self.add_slot(v, ""conditional_gradient"")\n\n    def _prepare_local(self, var_device, var_dtype, apply_state):\n        super()._prepare_local(var_device, var_dtype, apply_state)\n        apply_state[(var_device, var_dtype)][""learning_rate""] = tf.identity(\n            self._get_hyper(""learning_rate"", var_dtype)\n        )\n        apply_state[(var_device, var_dtype)][""lambda_""] = tf.identity(\n            self._get_hyper(""lambda_"", var_dtype)\n        )\n        apply_state[(var_device, var_dtype)][""epsilon""] = tf.convert_to_tensor(\n            self.epsilon, var_dtype\n        )\n\n    @staticmethod\n    def _frobenius_norm(m):\n        return tf.reduce_sum(m ** 2) ** 0.5\n\n    @staticmethod\n    def _top_singular_vector(m):\n        # handle the case where m is a tensor of rank 0 or rank 1.\n        # Example:\n        #   scalar (rank 0) a, shape []=> [[a]], shape [1,1]\n        #   vector (rank 1) [a,b], shape [2] => [[a,b]], shape [1,2]\n        original_rank = tf.rank(m)\n        shape = tf.shape(m)\n        first_pad = tf.cast(tf.less(original_rank, 2), dtype=tf.int32)\n        second_pad = tf.cast(tf.equal(original_rank, 0), dtype=tf.int32)\n        new_shape = tf.concat(\n            [\n                tf.ones(shape=first_pad, dtype=tf.int32),\n                tf.ones(shape=second_pad, dtype=tf.int32),\n                shape,\n            ],\n            axis=0,\n        )\n        n = tf.reshape(m, new_shape)\n        st, ut, vt = tf.linalg.svd(n, full_matrices=False)\n        n_size = tf.shape(n)\n        ut = tf.reshape(ut[:, 0], [n_size[0], 1])\n        vt = tf.reshape(vt[:, 0], [n_size[1], 1])\n        st = tf.matmul(ut, tf.transpose(vt))\n        # when we return the top singular vector, we have to remove the\n        # dimension we have added on\n        st_shape = tf.shape(st)\n        begin = tf.cast(tf.less(original_rank, 2), dtype=tf.int32)\n        end = 2 - tf.cast(tf.equal(original_rank, 0), dtype=tf.int32)\n        new_shape = st_shape[begin:end]\n        return tf.reshape(st, new_shape)\n\n    def _resource_apply_dense(self, grad, var, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n        lr = coefficients[""learning_rate""]\n        lambda_ = coefficients[""lambda_""]\n        epsilon = coefficients[""epsilon""]\n        if self.ord == ""fro"":\n            norm = tf.convert_to_tensor(\n                self._frobenius_norm(grad), name=""norm"", dtype=var.dtype.base_dtype\n            )\n            s = grad / (norm + epsilon)\n        else:\n            top_singular_vector = tf.convert_to_tensor(\n                self._top_singular_vector(grad),\n                name=""top_singular_vector"",\n                dtype=var.dtype.base_dtype,\n            )\n            s = top_singular_vector\n\n        var_update_tensor = tf.math.multiply(var, lr) - (1 - lr) * lambda_ * s\n        var_update_kwargs = {\n            ""resource"": var.handle,\n            ""value"": var_update_tensor,\n        }\n        var_update_op = tf.raw_ops.AssignVariableOp(**var_update_kwargs)\n        return tf.group(var_update_op)\n\n    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n        lr = coefficients[""learning_rate""]\n        lambda_ = coefficients[""lambda_""]\n        epsilon = coefficients[""epsilon""]\n        var_slice = tf.gather(var, indices)\n        if self.ord == ""fro"":\n            norm = tf.convert_to_tensor(\n                self._frobenius_norm(grad), name=""norm"", dtype=var.dtype.base_dtype\n            )\n            s = grad / (norm + epsilon)\n        else:\n            top_singular_vector = tf.convert_to_tensor(\n                self._top_singular_vector(grad),\n                name=""top_singular_vector"",\n                dtype=var.dtype.base_dtype,\n            )\n            s = top_singular_vector\n\n        var_update_value = tf.math.multiply(var_slice, lr) - (1 - lr) * lambda_ * s\n        var_update_kwargs = {\n            ""resource"": var.handle,\n            ""indices"": indices,\n            ""updates"": var_update_value,\n        }\n        var_update_op = tf.raw_ops.ResourceScatterUpdate(**var_update_kwargs)\n        return tf.group(var_update_op)\n'"
tensorflow_addons/optimizers/cyclical_learning_rate.py,24,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Cyclical Learning Rate Schedule policies for TensorFlow.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass CyclicalLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n    """"""A LearningRateSchedule that uses cyclical schedule.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        initial_learning_rate: Union[FloatTensorLike, Callable],\n        maximal_learning_rate: Union[FloatTensorLike, Callable],\n        step_size: FloatTensorLike,\n        scale_fn: Callable,\n        scale_mode: str = ""cycle"",\n        name: str = ""CyclicalLearningRate"",\n    ):\n        """"""Applies cyclical schedule to the learning rate.\n\n        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186\n\n\n        ```python\n        lr_schedule = tf.keras.optimizers.schedules.CyclicalLearningRate(\n            initial_learning_rate=1e-4,\n            maximal_learning_rate=1e-2,\n            step_size=2000,\n            scale_fn=lambda x: 1.,\n            scale_mode=""cycle"",\n            name=""MyCyclicScheduler"")\n\n        model.compile(optimizer=tf.keras.optimizers.SGD(\n                                                    learning_rate=lr_schedule),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        model.fit(data, labels, epochs=5)\n        ```\n\n        You can pass this schedule directly into a\n        `tf.keras.optimizers.Optimizer` as the learning rate.\n\n        Args:\n            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The initial learning rate.\n            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The maximum learning rate.\n            step_size: A scalar `float32` or `float64` `Tensor` or a\n                Python number. Step size.\n            scale_fn: A function. Scheduling function applied in cycle\n            scale_mode: [\'cycle\', \'iterations\']. Mode to apply during cyclic\n                schedule\n            name: (Optional) Name for the operation.\n\n        Returns:\n            Updated learning rate value.\n        """"""\n        super().__init__()\n        self.initial_learning_rate = initial_learning_rate\n        self.maximal_learning_rate = maximal_learning_rate\n        self.step_size = step_size\n        self.scale_fn = scale_fn\n        self.scale_mode = scale_mode\n        self.name = name\n\n    def __call__(self, step):\n        with tf.name_scope(self.name or ""CyclicalLearningRate""):\n            initial_learning_rate = tf.convert_to_tensor(\n                self.initial_learning_rate, name=""initial_learning_rate""\n            )\n            dtype = initial_learning_rate.dtype\n            maximal_learning_rate = tf.cast(self.maximal_learning_rate, dtype)\n            step_size = tf.cast(self.step_size, dtype)\n            cycle = tf.floor(1 + step / (2 * step_size))\n            x = tf.abs(step / step_size - 2 * cycle + 1)\n\n            mode_step = cycle if self.scale_mode == ""cycle"" else step\n\n            return initial_learning_rate + (\n                maximal_learning_rate - initial_learning_rate\n            ) * tf.maximum(tf.cast(0, dtype), (1 - x)) * self.scale_fn(mode_step)\n\n    def get_config(self):\n        return {\n            ""initial_learning_rate"": self.initial_learning_rate,\n            ""maximal_learning_rate"": self.maximal_learning_rate,\n            ""scale_fn"": self.scale_fn,\n            ""step_size"": self.step_size,\n            ""scale_mode"": self.scale_mode,\n        }\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass TriangularCyclicalLearningRate(CyclicalLearningRate):\n    @typechecked\n    def __init__(\n        self,\n        initial_learning_rate: Union[FloatTensorLike, Callable],\n        maximal_learning_rate: Union[FloatTensorLike, Callable],\n        step_size: FloatTensorLike,\n        scale_mode: str = ""cycle"",\n        name: str = ""TriangularCyclicalLearningRate"",\n    ):\n        """"""Applies triangular cyclical schedule to the learning rate.\n\n        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186\n\n\n        ```python\n        from tf.keras.optimizers import schedules\n\n        lr_schedule = schedules.TriangularCyclicalLearningRate(\n            initial_learning_rate=1e-4,\n            maximal_learning_rate=1e-2,\n            step_size=2000,\n            scale_mode=""cycle"",\n            name=""MyCyclicScheduler"")\n\n        model.compile(optimizer=tf.keras.optimizers.SGD(\n                                                    learning_rate=lr_schedule),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        model.fit(data, labels, epochs=5)\n        ```\n\n        You can pass this schedule directly into a\n        `tf.keras.optimizers.Optimizer` as the learning rate.\n\n        Args:\n            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The initial learning rate.\n            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The maximum learning rate.\n            step_size: A scalar `float32` or `float64` `Tensor` or a\n                Python number. Step size.\n            scale_mode: [\'cycle\', \'iterations\']. Mode to apply during cyclic\n                schedule\n            name: (Optional) Name for the operation.\n\n        Returns:\n            Updated learning rate value.\n        """"""\n        super().__init__(\n            initial_learning_rate=initial_learning_rate,\n            maximal_learning_rate=maximal_learning_rate,\n            step_size=step_size,\n            scale_fn=lambda x: 1.0,\n            scale_mode=scale_mode,\n            name=name,\n        )\n\n    def get_config(self):\n        return {\n            ""initial_learning_rate"": self.initial_learning_rate,\n            ""maximal_learning_rate"": self.maximal_learning_rate,\n            ""step_size"": self.step_size,\n            ""scale_mode"": self.scale_mode,\n        }\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass Triangular2CyclicalLearningRate(CyclicalLearningRate):\n    @typechecked\n    def __init__(\n        self,\n        initial_learning_rate: Union[FloatTensorLike, Callable],\n        maximal_learning_rate: Union[FloatTensorLike, Callable],\n        step_size: FloatTensorLike,\n        scale_mode: str = ""cycle"",\n        name: str = ""Triangular2CyclicalLearningRate"",\n    ):\n        """"""Applies triangular2 cyclical schedule to the learning rate.\n\n        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186\n\n\n        ```python\n        from tf.keras.optimizers import schedules\n\n        lr_schedule = schedules.Triangular2CyclicalLearningRate(\n            initial_learning_rate=1e-4,\n            maximal_learning_rate=1e-2,\n            step_size=2000,\n            scale_mode=""cycle"",\n            name=""MyCyclicScheduler"")\n\n        model.compile(optimizer=tf.keras.optimizers.SGD(\n                                                    learning_rate=lr_schedule),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        model.fit(data, labels, epochs=5)\n        ```\n\n        You can pass this schedule directly into a\n        `tf.keras.optimizers.Optimizer` as the learning rate.\n\n        Args:\n            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The initial learning rate.\n            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The maximum learning rate.\n            step_size: A scalar `float32` or `float64` `Tensor` or a\n                Python number. Step size.\n            scale_mode: [\'cycle\', \'iterations\']. Mode to apply during cyclic\n                schedule\n            name: (Optional) Name for the operation.\n\n        Returns:\n            Updated learning rate value.\n        """"""\n        super().__init__(\n            initial_learning_rate=initial_learning_rate,\n            maximal_learning_rate=maximal_learning_rate,\n            step_size=step_size,\n            scale_fn=lambda x: 1 / (2.0 ** (x - 1)),\n            scale_mode=scale_mode,\n            name=name,\n        )\n\n    def get_config(self):\n        return {\n            ""initial_learning_rate"": self.initial_learning_rate,\n            ""maximal_learning_rate"": self.maximal_learning_rate,\n            ""step_size"": self.step_size,\n            ""scale_mode"": self.scale_mode,\n        }\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass ExponentialCyclicalLearningRate(CyclicalLearningRate):\n    @typechecked\n    def __init__(\n        self,\n        initial_learning_rate: Union[FloatTensorLike, Callable],\n        maximal_learning_rate: Union[FloatTensorLike, Callable],\n        step_size: FloatTensorLike,\n        scale_mode: str = ""iterations"",\n        gamma: FloatTensorLike = 1.0,\n        name: str = ""ExponentialCyclicalLearningRate"",\n    ):\n        """"""Applies exponential cyclical schedule to the learning rate.\n\n        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186\n\n\n        ```python\n        from tf.keras.optimizers import schedules\n\n        lr_schedule = ExponentialCyclicalLearningRate(\n            initial_learning_rate=1e-4,\n            maximal_learning_rate=1e-2,\n            step_size=2000,\n            scale_mode=""cycle"",\n            gamma=0.96,\n            name=""MyCyclicScheduler"")\n\n        model.compile(optimizer=tf.keras.optimizers.SGD(\n                                                    learning_rate=lr_schedule),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        model.fit(data, labels, epochs=5)\n        ```\n\n        You can pass this schedule directly into a\n        `tf.keras.optimizers.Optimizer` as the learning rate.\n\n        Args:\n            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The initial learning rate.\n            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or\n                a Python number.  The maximum learning rate.\n            step_size: A scalar `float32` or `float64` `Tensor` or a\n                Python number. Step size.\n            scale_mode: [\'cycle\', \'iterations\']. Mode to apply during cyclic\n                schedule\n            gamma: A scalar `float32` or `float64` `Tensor` or a\n                Python number.  Gamma value.\n            name: (Optional) Name for the operation.\n\n        Returns:\n            Updated learning rate value.\n        """"""\n        self.gamma = gamma\n        super().__init__(\n            initial_learning_rate=initial_learning_rate,\n            maximal_learning_rate=maximal_learning_rate,\n            step_size=step_size,\n            scale_fn=lambda x: gamma ** x,\n            scale_mode=scale_mode,\n            name=name,\n        )\n\n    def get_config(self):\n        return {\n            ""initial_learning_rate"": self.initial_learning_rate,\n            ""maximal_learning_rate"": self.maximal_learning_rate,\n            ""step_size"": self.step_size,\n            ""scale_mode"": self.scale_mode,\n            ""gamma"": self.gamma,\n        }\n'"
tensorflow_addons/optimizers/lamb.py,26,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Layer-wise Adaptive Moments (LAMB) optimizer.\n\nSee paper [Large Batch Optimization for Deep Learning: Training BERT in\n76 minutes](https://arxiv.org/abs/1904.00962).\n""""""\n\nimport re\nfrom typing import Optional, Union, Callable, List\nfrom typeguard import typechecked\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass LAMB(tf.keras.optimizers.Optimizer):\n    """"""Optimizer that implements the Layer-wise Adaptive Moments (LAMB).\n\n    See paper [Large Batch Optimization for Deep Learning: Training BERT\n    in 76 minutes](https://arxiv.org/abs/1904.00962).\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        beta_1: FloatTensorLike = 0.9,\n        beta_2: FloatTensorLike = 0.999,\n        epsilon: FloatTensorLike = 1e-6,\n        weight_decay_rate: FloatTensorLike = 0.0,\n        exclude_from_weight_decay: Optional[List[str]] = None,\n        exclude_from_layer_adaptation: Optional[List[str]] = None,\n        name: str = ""LAMB"",\n        **kwargs\n    ):\n        """"""Construct a new LAMB optimizer.\n\n        Args:\n            learning_rate: A `Tensor` or a floating point value. or a schedule\n                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\n                The learning rate.\n            beta_1: A `float` value or a constant `float` tensor.\n              The exponential decay rate for the 1st moment estimates.\n            beta_2: A `float` value or a constant `float` tensor.\n              The exponential decay rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability.\n            weight_decay_rate: weight decay rate.\n            exclude_from_weight_decay: List of regex patterns of\n              variables excluded from weight decay. Variables whose name\n              contain a substring matching the pattern will be excluded.\n            exclude_from_layer_adaptation: List of regex patterns of\n              variables excluded from layer adaptation. Variables whose name\n              contain a substring matching the pattern will be excluded.\n            name: Optional name for the operations created when applying\n              gradients. Defaults to ""LAMB"".\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n              `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by\n              norm; `clipvalue` is clip gradients by value, `decay` is\n              included for backward compatibility to allow time inverse\n              decay of learning rate. `lr` is included for backward\n              compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(name, **kwargs)\n\n        # Just adding the square of the weights to the loss function is *not*\n        # the correct way of using L2 regularization/weight decay with Adam,\n        # since that will interact with the m and v parameters in strange ways.\n        #\n        # Instead we want to decay the weights in a manner that doesn\'t interact\n        # with the m/v parameters.\n        self._set_hyper(""weight_decay_rate"", weight_decay_rate)\n        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))\n\n        # This is learning rate decay for using keras learning rate schedule.\n        self._set_hyper(""decay"", self._initial_decay)\n        self._set_hyper(""beta_1"", beta_1)\n        self._set_hyper(""beta_2"", beta_2)\n        self.epsilon = epsilon or tf.backend_config.epsilon()\n        self.exclude_from_weight_decay = exclude_from_weight_decay\n        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if\n        # the arg is None.\n        if exclude_from_layer_adaptation:\n            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n        else:\n            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        # Separate for-loops to respect the ordering of slot variables from v1.\n        for var in var_list:\n            self.add_slot(var, ""m"")\n        for var in var_list:\n            self.add_slot(var, ""v"")\n\n    def _prepare_local(self, var_device, var_dtype, apply_state):\n        super()._prepare_local(var_device, var_dtype, apply_state)\n\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_t = tf.identity(self._get_hyper(""beta_1"", var_dtype))\n        beta_2_t = tf.identity(self._get_hyper(""beta_2"", var_dtype))\n        weight_decay_rate = tf.identity(self._get_hyper(""weight_decay_rate"", var_dtype))\n        beta_1_power = tf.pow(beta_1_t, local_step)\n        beta_2_power = tf.pow(beta_2_t, local_step)\n        apply_state[(var_device, var_dtype)].update(\n            dict(\n                weight_decay_rate=weight_decay_rate,\n                epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),\n                beta_1_t=beta_1_t,\n                beta_1_power=beta_1_power,\n                one_minus_beta_1_t=1 - beta_1_t,\n                beta_2_t=beta_2_t,\n                beta_2_power=beta_2_power,\n                one_minus_beta_2_t=1 - beta_2_t,\n            )\n        )\n\n    def _resource_apply_dense(self, grad, var, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * coefficients[""one_minus_beta_1_t""]\n        m_t = m * coefficients[""beta_1_t""] + m_scaled_g_values\n        m_t = m.assign(m_t, use_locking=self._use_locking)\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * coefficients[""one_minus_beta_2_t""]\n        v_t = v * coefficients[""beta_2_t""] + v_scaled_g_values\n        v_t = v.assign(v_t, use_locking=self._use_locking)\n\n        m_t_hat = m_t / (1.0 - coefficients[""beta_1_power""])\n        v_t_hat = v_t / (1.0 - coefficients[""beta_2_power""])\n\n        v_sqrt = tf.sqrt(v_t_hat)\n        update = m_t_hat / (v_sqrt + coefficients[""epsilon""])\n\n        var_name = self._get_variable_name(var.name)\n        if self._do_use_weight_decay(var_name):\n            update += coefficients[""weight_decay_rate""] * var\n\n        ratio = 1.0\n        if self._do_layer_adaptation(var_name):\n            w_norm = tf.norm(var, ord=2)\n            g_norm = tf.norm(update, ord=2)\n            ratio = tf.where(\n                tf.greater(w_norm, 0),\n                tf.where(tf.greater(g_norm, 0), (w_norm / g_norm), 1.0),\n                1.0,\n            )\n\n        var_update = var - ratio * coefficients[""lr_t""] * update\n        return var.assign(var_update, use_locking=self._use_locking).op\n\n    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n\n        # m_t = beta1 * m + (1 - beta1) * g_t\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * coefficients[""one_minus_beta_1_t""]\n        m_t = m.assign(m * coefficients[""beta_1_t""], use_locking=self._use_locking)\n        with tf.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n\n        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * coefficients[""one_minus_beta_2_t""]\n        v_t = v.assign(v * coefficients[""beta_2_t""], use_locking=self._use_locking)\n        with tf.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        m_t_hat = m_t / (1.0 - coefficients[""beta_1_power""])\n        v_t_hat = v_t / (1.0 - coefficients[""beta_2_power""])\n\n        v_sqrt = tf.sqrt(v_t_hat)\n        update = m_t_hat / (v_sqrt + coefficients[""epsilon""])\n\n        var_name = self._get_variable_name(var.name)\n        if self._do_use_weight_decay(var_name):\n            update += coefficients[""weight_decay_rate""] * var\n\n        ratio = 1.0\n        if self._do_layer_adaptation(var_name):\n            w_norm = tf.norm(var, ord=2)\n            g_norm = tf.norm(update, ord=2)\n            ratio = tf.where(\n                tf.greater(w_norm, 0),\n                tf.where(tf.greater(g_norm, 0), (w_norm / g_norm), 1.0),\n                1.0,\n            )\n\n        var_update = var.assign_sub(\n            ratio * coefficients[""lr_t""] * update, use_locking=self._use_locking\n        )\n        return tf.group(*[var_update, m_t, v_t])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                ""learning_rate"": self._serialize_hyperparameter(""learning_rate""),\n                ""weight_decay_rate"": self._serialize_hyperparameter(\n                    ""weight_decay_rate""\n                ),\n                ""decay"": self._serialize_hyperparameter(""decay""),\n                ""beta_1"": self._serialize_hyperparameter(""beta_1""),\n                ""beta_2"": self._serialize_hyperparameter(""beta_2""),\n                ""epsilon"": self.epsilon,\n            }\n        )\n        return config\n\n    def _do_use_weight_decay(self, param_name):\n        """"""Whether to use L2 weight decay for `param_name`.""""""\n        if self.exclude_from_weight_decay:\n            for r in self.exclude_from_weight_decay:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def _do_layer_adaptation(self, param_name):\n        """"""Whether to do layer-wise learning rate adaptation for\n        `param_name`.""""""\n        if self.exclude_from_layer_adaptation:\n            for r in self.exclude_from_layer_adaptation:\n                if re.search(r, param_name) is not None:\n                    return False\n        return True\n\n    def _get_variable_name(self, param_name):\n        """"""Get the variable name from the tensor name.""""""\n        m = re.match(""^(.*):\\\\d+$"", param_name)\n        if m is not None:\n            param_name = m.group(1)\n        return param_name\n'"
tensorflow_addons/optimizers/lazy_adam.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Variant of the Adam optimizer that handles sparse updates more efficiently.\n\nCompared with the original Adam optimizer, the one in this file can\nprovide a large improvement in model training throughput for some\napplications. However, it provides slightly different semantics than the\noriginal Adam algorithm, and may lead to different empirical results.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass LazyAdam(tf.keras.optimizers.Adam):\n    """"""Variant of the Adam optimizer that handles sparse updates more\n    efficiently.\n\n    The original Adam algorithm maintains two moving-average accumulators for\n    each trainable variable; the accumulators are updated at every step.\n    This class provides lazier handling of gradient updates for sparse\n    variables.  It only updates moving-average accumulators for sparse variable\n    indices that appear in the current batch, rather than updating the\n    accumulators for all indices. Compared with the original Adam optimizer,\n    it can provide large improvements in model training throughput for some\n    applications. However, it provides slightly different semantics than the\n    original Adam algorithm, and may lead to different empirical results.\n\n    Note, amsgrad is currently not supported and the argument can only be\n    False.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        beta_1: FloatTensorLike = 0.9,\n        beta_2: FloatTensorLike = 0.999,\n        epsilon: FloatTensorLike = 1e-7,\n        amsgrad: bool = False,\n        name: str = ""LazyAdam"",\n        **kwargs\n    ):\n        """"""Constructs a new LazyAdam optimizer.\n\n        Args:\n          learning_rate: A `Tensor` or a floating point value. or a schedule\n            that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\n            The learning rate.\n          beta_1: A `float` value or a constant `float` tensor.\n            The exponential decay rate for the 1st moment estimates.\n          beta_2: A `float` value or a constant `float` tensor.\n            The exponential decay rate for the 2nd moment estimates.\n          epsilon: A small constant for numerical stability.\n            This epsilon is ""epsilon hat"" in\n            [Adam: A Method for Stochastic Optimization. Kingma et al., 2014]\n            (http://arxiv.org/abs/1412.6980) (in the formula just\n            before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n          amsgrad: `boolean`. Whether to apply AMSGrad variant of this\n            algorithm from the paper ""On the Convergence of Adam and beyond"".\n            Note that this argument is currently not supported and the\n            argument can only be `False`.\n          name: Optional name for the operations created when applying\n            gradients. Defaults to ""LazyAdam"".\n          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`,\n            `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue`\n            is clip gradients by value, `decay` is included for backward\n            compatibility to allow time inverse decay of learning rate. `lr`\n            is included for backward compatibility, recommended to use\n            `learning_rate` instead.\n        """"""\n        super().__init__(\n            learning_rate=learning_rate,\n            beta_1=beta_1,\n            beta_2=beta_2,\n            epsilon=epsilon,\n            amsgrad=amsgrad,\n            name=name,\n            **kwargs,\n        )\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper(""beta_1"", var_dtype)\n        beta_2_t = self._get_hyper(""beta_2"", var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_power = tf.math.pow(beta_1_t, local_step)\n        beta_2_power = tf.math.pow(beta_2_t, local_step)\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n        lr = lr_t * tf.math.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n\n        # \\\\(m := beta1 * m + (1 - beta1) * g_t\\\\)\n        m = self.get_slot(var, ""m"")\n        m_t_slice = beta_1_t * tf.gather(m, indices) + (1 - beta_1_t) * grad\n\n        m_update_kwargs = {\n            ""resource"": m.handle,\n            ""indices"": indices,\n            ""updates"": m_t_slice,\n        }\n        m_update_op = tf.raw_ops.ResourceScatterUpdate(**m_update_kwargs)\n\n        # \\\\(v := beta2 * v + (1 - beta2) * (g_t * g_t)\\\\)\n        v = self.get_slot(var, ""v"")\n        v_t_slice = beta_2_t * tf.gather(v, indices) + (1 - beta_2_t) * tf.math.square(\n            grad\n        )\n\n        v_update_kwargs = {\n            ""resource"": v.handle,\n            ""indices"": indices,\n            ""updates"": v_t_slice,\n        }\n        v_update_op = tf.raw_ops.ResourceScatterUpdate(**v_update_kwargs)\n\n        # \\\\(variable -= learning_rate * m_t / (epsilon_t + sqrt(v_t))\\\\)\n        var_slice = lr * m_t_slice / (tf.math.sqrt(v_t_slice) + epsilon_t)\n\n        var_update_kwargs = {\n            ""resource"": var.handle,\n            ""indices"": indices,\n            ""updates"": var_slice,\n        }\n        var_update_op = tf.raw_ops.ResourceScatterSub(**var_update_kwargs)\n\n        return tf.group(*[var_update_op, m_update_op, v_update_op])\n'"
tensorflow_addons/optimizers/lookahead.py,24,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils import types\n\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass Lookahead(tf.keras.optimizers.Optimizer):\n    """"""This class allows to extend optimizers with the lookahead mechanism.\n\n    The mechanism is proposed by Michael R. Zhang et.al in the paper\n    [Lookahead Optimizer: k steps forward, 1 step back]\n    (https://arxiv.org/abs/1907.08610v1). The optimizer iteratively updates two\n    sets of weights: the search directions for weights are chosen by the inner\n    optimizer, while the ""slow weights"" are updated each `k` steps based on the\n    directions of the ""fast weights"" and the two sets of weights are\n    synchronized. This method improves the learning stability and lowers the\n    variance of its inner optimizer.\n\n    Example of usage:\n\n    ```python\n    opt = tf.keras.optimizers.SGD(learning_rate)\n    opt = tfa.optimizers.Lookahead(opt)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        optimizer: types.Optimizer,\n        sync_period: int = 6,\n        slow_step_size: types.FloatTensorLike = 0.5,\n        name: str = ""Lookahead"",\n        **kwargs\n    ):\n        r""""""Wrap optimizer with the lookahead mechanism.\n\n        Args:\n            optimizer: The original optimizer that will be used to compute\n                and apply the gradients.\n            sync_period: An integer. The synchronization period of lookahead.\n                Enable lookahead mechanism by setting it with a positive value.\n            slow_step_size: A floating point value.\n                The ratio for updating the slow weights.\n            name: Optional name for the operations created when applying\n                gradients. Defaults to ""Lookahead"".\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n                by norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(name, **kwargs)\n\n        if isinstance(optimizer, str):\n            optimizer = tf.keras.optimizers.get(optimizer)\n        if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n            raise TypeError(\n                ""optimizer is not an object of tf.keras.optimizers.Optimizer""\n            )\n\n        self._optimizer = optimizer\n        self._set_hyper(""sync_period"", sync_period)\n        self._set_hyper(""slow_step_size"", slow_step_size)\n        self._initialized = False\n\n    def _create_slots(self, var_list):\n        self._optimizer._create_slots(\n            var_list=var_list\n        )  # pylint: disable=protected-access\n        for var in var_list:\n            self.add_slot(var, ""slow"")\n\n    def _create_hypers(self):\n        self._optimizer._create_hypers()  # pylint: disable=protected-access\n\n    def _prepare(self, var_list):\n        return self._optimizer._prepare(\n            var_list=var_list\n        )  # pylint: disable=protected-access\n\n    def apply_gradients(self, grads_and_vars, name=None):\n        self._optimizer._iterations = (\n            self.iterations\n        )  # pylint: disable=protected-access\n        return super().apply_gradients(grads_and_vars, name)\n\n    def _init_op(self, var):\n        slow_var = self.get_slot(var, ""slow"")\n        return slow_var.assign(\n            tf.where(\n                tf.equal(self.iterations, tf.constant(0, dtype=self.iterations.dtype)),\n                var,\n                slow_var,\n            ),\n            use_locking=self._use_locking,\n        )\n\n    def _look_ahead_op(self, var):\n        var_dtype = var.dtype.base_dtype\n        slow_var = self.get_slot(var, ""slow"")\n        local_step = tf.cast(self.iterations + 1, tf.dtypes.int64)\n        sync_period = self._get_hyper(""sync_period"", tf.dtypes.int64)\n        slow_step_size = self._get_hyper(""slow_step_size"", var_dtype)\n        step_back = slow_var + slow_step_size * (var - slow_var)\n        sync_cond = tf.equal(\n            tf.math.floordiv(local_step, sync_period) * sync_period, local_step\n        )\n        with tf.control_dependencies([step_back]):\n            slow_update = slow_var.assign(\n                tf.where(sync_cond, step_back, slow_var,), use_locking=self._use_locking\n            )\n            var_update = var.assign(\n                tf.where(sync_cond, step_back, var,), use_locking=self._use_locking\n            )\n        return tf.group(slow_update, var_update)\n\n    @property\n    def weights(self):\n        return self._weights + self._optimizer.weights\n\n    def _resource_apply_dense(self, grad, var):\n        init_op = self._init_op(var)\n        with tf.control_dependencies([init_op]):\n            train_op = self._optimizer._resource_apply_dense(\n                grad, var\n            )  # pylint: disable=protected-access\n            with tf.control_dependencies([train_op]):\n                look_ahead_op = self._look_ahead_op(var)\n        return tf.group(init_op, train_op, look_ahead_op)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        init_op = self._init_op(var)\n        with tf.control_dependencies([init_op]):\n            train_op = self._optimizer._resource_apply_sparse(  # pylint: disable=protected-access\n                grad, var, indices\n            )\n            with tf.control_dependencies([train_op]):\n                look_ahead_op = self._look_ahead_op(var)\n        return tf.group(init_op, train_op, look_ahead_op)\n\n    def get_config(self):\n        config = {\n            ""optimizer"": tf.keras.optimizers.serialize(self._optimizer),\n            ""sync_period"": self._serialize_hyperparameter(""sync_period""),\n            ""slow_step_size"": self._serialize_hyperparameter(""slow_step_size""),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @property\n    def learning_rate(self):\n        return self._optimizer._get_hyper(""learning_rate"")\n\n    @learning_rate.setter\n    def learning_rate(self, learning_rate):\n        self._optimizer._set_hyper(""learning_rate"", learning_rate)\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, lr):\n        self.learning_rate = lr\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        optimizer = tf.keras.optimizers.deserialize(\n            config.pop(""optimizer""), custom_objects=custom_objects,\n        )\n        return cls(optimizer, **config)\n'"
tensorflow_addons/optimizers/moving_average.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport tensorflow as tf\nfrom tensorflow.python.training.moving_averages import assign_moving_average\nfrom tensorflow_addons.optimizers import AveragedOptimizerWrapper\nfrom tensorflow_addons.utils import types\n\nfrom typing import Optional\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass MovingAverage(AveragedOptimizerWrapper):\n    """"""Optimizer that computes a moving average of the variables.\n\n    Empirically it has been found that using the moving average of the trained\n    parameters of a deep network is better than using its trained parameters\n    directly. This optimizer allows you to compute this moving average and swap\n    the variables at save time so that any code outside of the training loop\n    will use by default the average values instead of the original ones.\n\n    Example of usage:\n\n    ```python\n    opt = tf.keras.optimizers.SGD(learning_rate)\n    opt = tfa.optimizers.MovingAverage(opt)\n\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        optimizer: types.Optimizer,\n        sequential_update: bool = True,\n        average_decay: types.FloatTensorLike = 0.99,\n        num_updates: Optional[str] = None,\n        name: str = ""MovingAverage"",\n        **kwargs\n    ):\n        r""""""Construct a new MovingAverage optimizer.\n\n        Args:\n            optimizer: str or `tf.keras.optimizers.Optimizer` that will be\n                used to compute and apply gradients.\n            sequential_update: Bool. If False, will compute the moving average\n                at the same time as the model is updated, potentially doing\n                benign data races. If True, will update the moving average\n                after gradient updates.\n            average_decay: float. Decay to use to maintain the moving averages\n                of trained variables.\n            num_updates: Optional count of the number of updates applied to\n                variables.\n            name: Optional name for the operations created when applying\n                gradients. Defaults to ""MovingAverage"".\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by\n                norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(optimizer, sequential_update, name, **kwargs)\n        self._num_updates = num_updates\n        if self._num_updates is not None:\n            num_updates = tf.cast(self._num_updates, tf.float32, name=""num_updates"")\n            average_decay = tf.minimum(\n                average_decay, (1.0 + num_updates) / (10.0 + num_updates)\n            )\n\n        self._set_hyper(""average_decay"", average_decay)\n\n    def average_op(self, var, average_var):\n        decay = self._get_hyper(""average_decay"", tf.dtypes.float32)\n        return assign_moving_average(average_var, var, decay, False)\n\n    def get_config(self):\n        config = {\n            ""average_decay"": self._serialize_hyperparameter(""average_decay""),\n            ""num_updates"": self._num_updates,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def _create_slots(self, var_list):\n        self._optimizer._create_slots(\n            var_list=var_list\n        )  # pylint: disable=protected-access\n        for var in var_list:\n            self.add_slot(var, ""average"", var.read_value())\n'"
tensorflow_addons/optimizers/novograd.py,29,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""NovoGrad for TensorFlow.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typing import Union, Callable\nfrom typeguard import typechecked\n\n# TODO: Find public API alternatives to these\nfrom tensorflow.python.training import training_ops\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass NovoGrad(tf.keras.optimizers.Optimizer):\n    """"""The NovoGrad Optimizer was first proposed in [Stochastic Gradient\n    Methods with Layerwise Adaptvie Moments for training of Deep\n    Networks](https://arxiv.org/pdf/1905.11286.pdf)\n\n    NovoGrad is a first-order SGD-based algorithm, which computes second\n    moments per layer instead of per weight as in Adam. Compared to Adam,\n    NovoGrad takes less memory, and has been found to be more numerically\n    stable. More specifically we compute (for more information on the\n    computation please refer to this\n    [link](https://nvidia.github.io/OpenSeq2Seq/html/optimizers.html):\n\n    Second order moment = exponential moving average of Layer-wise square\n    of grads:\n        v_t <-- beta_2 * v_{t-1} + (1-beta_2) * (g_t)^2\n    First order moment in one of four modes:\n        1. moment of grads normalized by v_t:\n            m_t <- beta_1 * m_{t-1} + [ g_t / (sqrt(v_t)+epsilon)]\n        2. moment similar to Adam: exponential moving average of grads\n        normalized by v_t (set grad_averaging = True to use this):\n            m_t <- beta_1 * m_{t-1} +\n                   [(1 - beta_1) * (g_t / (sqrt(v_t) + epsilon))]\n        3. weight decay adds a w_d term after grads are rescaled by\n        1/sqrt(v_t) (set weight_decay > 0 to use this0:\n            m_t <- beta_1 * m_{t-1} +\n                   [(g_t / (sqrt(v_t) + epsilon)) + (w_d * w_{t-1})]\n        4. weight decay + exponential moving average from Adam:\n            m_t <- beta_1 * m_{t-1} +\n                   [(1 - beta_1) * ((g_t / (sqrt(v_t + epsilon)) +\n                   (w_d * w_{t-1}))]\n    Weight update:\n        w_t <- w_{t-1} - lr_t * m_t\n\n    Example of usage:\n    ```python\n    opt = tfa.optimizers.NovoGrad(\n        lr=1e-3,\n        beta_1=0.9,\n        beta_2=0.999,\n        weight_decay=0.001,\n        grad_averaging=False\n    )\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        beta_1: FloatTensorLike = 0.9,\n        beta_2: FloatTensorLike = 0.999,\n        epsilon: FloatTensorLike = 1e-7,\n        weight_decay: FloatTensorLike = 0.0,\n        grad_averaging: bool = False,\n        amsgrad: bool = False,\n        name: str = ""NovoGrad"",\n        **kwargs\n    ):\n        r""""""Construct a new NovoGrad optimizer.\n\n        Args:\n            learning_rate: A `Tensor` or a floating point value. or a schedule\n                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\n                The learning rate.\n            beta_1: A float value or a constant float tensor.\n                The exponential decay rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor.\n                The exponential decay rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability.\n            weight_decay: A floating point value. Weight decay for each param.\n            grad_averaging: determines whether to use Adam style exponential\n                moving averaging for the first order moments.\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n                by norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(name, **kwargs)\n        if weight_decay < 0.0:\n            raise ValueError(""Weight decay rate cannot be negative"")\n        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))\n        self._set_hyper(""decay"", self._initial_decay)\n        self._set_hyper(""beta_1"", beta_1)\n        self._set_hyper(""beta_2"", beta_2)\n        self._set_hyper(""weight_decay"", weight_decay)\n        self._set_hyper(""grad_averaging"", grad_averaging)\n        self.amsgrad = amsgrad\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\n\n    def _create_slots(self, var_list):\n        # Create slots for the first and second moments.\n        # Separate for-loops to respect the ordering of slot variables from v1.\n        for var in var_list:\n            self.add_slot(var=var, slot_name=""m"", initializer=""zeros"")\n        for var in var_list:\n            self.add_slot(\n                var=var, slot_name=""v"", initializer=tf.zeros(shape=[], dtype=var.dtype)\n            )\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, ""vhat"")\n\n    def _prepare_local(self, var_device, var_dtype, apply_state):\n        super()._prepare_local(var_device, var_dtype, apply_state)\n        beta_1_t = tf.identity(self._get_hyper(""beta_1"", var_dtype))\n        beta_2_t = tf.identity(self._get_hyper(""beta_2"", var_dtype))\n        apply_state[(var_device, var_dtype)].update(\n            dict(\n                epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),\n                beta_1_t=beta_1_t,\n                beta_2_t=beta_2_t,\n                one_minus_beta_2_t=1 - beta_2_t,\n                one_minus_beta_1_t=1 - beta_1_t,\n            )\n        )\n\n    def set_weights(self, weights):\n        params = self.weights\n        # If the weights are generated by Keras V1 optimizer, it includes vhats\n        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[: len(params)]\n        super().set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n        weight_decay = self._get_hyper(""weight_decay"")\n        grad_averaging = self._get_hyper(""grad_averaging"")\n\n        v = self.get_slot(var, ""v"")\n        g_2 = tf.reduce_sum(tf.square(tf.cast(grad, tf.float32)))\n        v_t = tf.cond(\n            tf.equal(self.iterations, 0),\n            lambda: g_2,\n            lambda: v * coefficients[""beta_2_t""]\n            + g_2 * coefficients[""one_minus_beta_2_t""],\n        )\n        v_t = v.assign(v_t, use_locking=self._use_locking)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, ""vhat"")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            grad = grad / (tf.sqrt(vhat_t) + self.epsilon)\n        else:\n            grad = grad / (tf.sqrt(v_t) + self.epsilon)\n        grad = tf.cond(\n            tf.greater(weight_decay, 0), lambda: grad + weight_decay * var, lambda: grad\n        )\n        grad = tf.cond(\n            tf.logical_and(grad_averaging, tf.not_equal(self.iterations, 0)),\n            lambda: grad * coefficients[""one_minus_beta_1_t""],\n            lambda: grad,\n        )\n        m = self.get_slot(var, ""m"")\n        return training_ops.resource_apply_keras_momentum(\n            var.handle,\n            m.handle,\n            coefficients[""lr_t""],\n            grad,\n            coefficients[""beta_1_t""],\n            use_locking=self._use_locking,\n            use_nesterov=False,\n        )\n\n    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        coefficients = (apply_state or {}).get(\n            (var_device, var_dtype)\n        ) or self._fallback_apply_state(var_device, var_dtype)\n        weight_decay = self._get_hyper(""weight_decay"")\n        grad_averaging = self._get_hyper(""grad_averaging"")\n\n        v = self.get_slot(var, ""v"")\n        g_2 = tf.reduce_sum(tf.square(tf.cast(grad, tf.float32)))\n        # v is just a scalar and does not need to involve sparse tensors.\n        v_t = tf.cond(\n            tf.equal(self.iterations, 0),\n            lambda: g_2,\n            lambda: v * coefficients[""beta_2_t""]\n            + g_2 * coefficients[""one_minus_beta_2_t""],\n        )\n        v_t = v.assign(v_t, use_locking=self._use_locking)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, ""vhat"")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            grad = grad / (tf.sqrt(vhat_t) + self.epsilon)\n        else:\n            grad = grad / (tf.sqrt(v_t) + self.epsilon)\n        grad = tf.cond(\n            tf.greater(weight_decay, 0), lambda: grad + weight_decay * var, lambda: grad\n        )\n        grad = tf.cond(\n            tf.logical_and(grad_averaging, tf.not_equal(self.iterations, 0)),\n            lambda: grad * coefficients[""one_minus_beta_1_t""],\n            lambda: grad,\n        )\n        m = self.get_slot(var, ""m"")\n        return training_ops.resource_sparse_apply_keras_momentum(\n            var.handle,\n            m.handle,\n            coefficients[""lr_t""],\n            tf.gather(grad, indices),\n            indices,\n            coefficients[""beta_1_t""],\n            use_locking=self._use_locking,\n            use_nesterov=False,\n        )\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                ""learning_rate"": self._serialize_hyperparameter(""learning_rate""),\n                ""beta_1"": self._serialize_hyperparameter(""beta_1""),\n                ""beta_2"": self._serialize_hyperparameter(""beta_2""),\n                ""epsilon"": self.epsilon,\n                ""weight_decay"": self._serialize_hyperparameter(""weight_decay""),\n                ""grad_averaging"": self._serialize_hyperparameter(""grad_averaging""),\n            }\n        )\n        return config\n'"
tensorflow_addons/optimizers/rectified_adam.py,35,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Rectified Adam (RAdam) optimizer.""""""\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typing import Union, Callable\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass RectifiedAdam(tf.keras.optimizers.Optimizer):\n    """"""Variant of the Adam optimizer whose adaptive learning rate is rectified\n    so as to have a consistent variance.\n\n    It implements the Rectified Adam (a.k.a. RAdam) proposed by\n    Liyuan Liu et al. in [On The Variance Of The Adaptive Learning Rate\n    And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n\n    Example of usage:\n\n    ```python\n    opt = tfa.optimizers.RectifiedAdam(lr=1e-3)\n    ```\n\n    Note: `amsgrad` is not described in the original paper. Use it with\n          caution.\n\n    RAdam is not a placement of the heuristic warmup, the settings should be\n    kept if warmup has already been employed and tuned in the baseline method.\n    You can enable warmup by setting `total_steps` and `warmup_proportion`:\n\n    ```python\n    opt = tfa.optimizers.RectifiedAdam(\n        lr=1e-3,\n        total_steps=10000,\n        warmup_proportion=0.1,\n        min_lr=1e-5,\n    )\n    ```\n\n    In the above example, the learning rate will increase linearly\n    from 0 to `lr` in 1000 steps, then decrease linearly from `lr` to `min_lr`\n    in 9000 steps.\n\n    Lookahead, proposed by Michael R. Zhang et.al in the paper\n    [Lookahead Optimizer: k steps forward, 1 step back]\n    (https://arxiv.org/abs/1907.08610v1), can be integrated with RAdam,\n    which is announced by Less Wright and the new combined optimizer can also\n    be called ""Ranger"". The mechanism can be enabled by using the lookahead\n    wrapper. For example:\n\n    ```python\n    radam = tfa.optimizers.RectifiedAdam()\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        beta_1: FloatTensorLike = 0.9,\n        beta_2: FloatTensorLike = 0.999,\n        epsilon: FloatTensorLike = 1e-7,\n        weight_decay: FloatTensorLike = 0.0,\n        amsgrad: bool = False,\n        sma_threshold: FloatTensorLike = 5.0,\n        total_steps: int = 0,\n        warmup_proportion: FloatTensorLike = 0.1,\n        min_lr: FloatTensorLike = 0.0,\n        name: str = ""RectifiedAdam"",\n        **kwargs\n    ):\n        r""""""Construct a new RAdam optimizer.\n\n        Args:\n            learning_rate: A `Tensor` or a floating point value. or a schedule\n                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`\n                The learning rate.\n            beta_1: A float value or a constant float tensor.\n                The exponential decay rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor.\n                The exponential decay rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this\n                algorithm from the paper ""On the Convergence of Adam and\n                beyond"".\n            sma_threshold. A float value.\n                The threshold for simple mean average.\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value.\n                The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying\n                gradients. Defaults to ""RectifiedAdam"".\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n                by norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(name, **kwargs)\n        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))\n        self._set_hyper(""beta_1"", beta_1)\n        self._set_hyper(""beta_2"", beta_2)\n        self._set_hyper(""decay"", self._initial_decay)\n        self._set_hyper(""weight_decay"", weight_decay)\n        self._set_hyper(""sma_threshold"", sma_threshold)\n        self._set_hyper(""total_steps"", int(total_steps))\n        self._set_hyper(""warmup_proportion"", warmup_proportion)\n        self._set_hyper(""min_lr"", min_lr)\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, ""m"")\n        for var in var_list:\n            self.add_slot(var, ""v"")\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, ""vhat"")\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[: len(params)]\n        super().set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, ""m"")\n        v = self.get_slot(var, ""v"")\n        beta_1_t = self._get_hyper(""beta_1"", var_dtype)\n        beta_2_t = self._get_hyper(""beta_2"", var_dtype)\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_power = tf.pow(beta_1_t, local_step)\n        beta_2_power = tf.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(""total_steps"", var_dtype)\n            warmup_steps = total_steps * self._get_hyper(""warmup_proportion"", var_dtype)\n            min_lr = self._get_hyper(""min_lr"", var_dtype)\n            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m_t = m.assign(\n            beta_1_t * m + (1.0 - beta_1_t) * grad, use_locking=self._use_locking\n        )\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v_t = v.assign(\n            beta_2_t * v + (1.0 - beta_2_t) * tf.square(grad),\n            use_locking=self._use_locking,\n        )\n        if self.amsgrad:\n            vhat = self.get_slot(var, ""vhat"")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = tf.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = tf.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = tf.sqrt(\n            (sma_t - 4.0)\n            / (sma_inf - 4.0)\n            * (sma_t - 2.0)\n            / (sma_inf - 2.0)\n            * sma_inf\n            / sma_t\n        )\n\n        sma_threshold = self._get_hyper(""sma_threshold"", var_dtype)\n        var_t = tf.where(\n            sma_t >= sma_threshold, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t\n        )\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper(""weight_decay"", var_dtype) * var\n\n        var_update = var.assign_sub(lr_t * var_t, use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return tf.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper(""beta_1"", var_dtype)\n        beta_2_t = self._get_hyper(""beta_2"", var_dtype)\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta_1_power = tf.pow(beta_1_t, local_step)\n        beta_2_power = tf.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper(""total_steps"", var_dtype)\n            warmup_steps = total_steps * self._get_hyper(""warmup_proportion"", var_dtype)\n            min_lr = self._get_hyper(""min_lr"", var_dtype)\n            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m = self.get_slot(var, ""m"")\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = m.assign(m * beta_1_t, use_locking=self._use_locking)\n        with tf.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v = self.get_slot(var, ""v"")\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = v.assign(v * beta_2_t, use_locking=self._use_locking)\n        with tf.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, ""vhat"")\n            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n            v_corr_t = tf.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = tf.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = tf.sqrt(\n            (sma_t - 4.0)\n            / (sma_inf - 4.0)\n            * (sma_t - 2.0)\n            / (sma_inf - 2.0)\n            * sma_inf\n            / sma_t\n        )\n\n        sma_threshold = self._get_hyper(""sma_threshold"", var_dtype)\n        var_t = tf.where(\n            sma_t >= sma_threshold, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t\n        )\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper(""weight_decay"", var_dtype) * var\n\n        with tf.control_dependencies([var_t]):\n            var_update = self._resource_scatter_add(\n                var, indices, tf.gather(-lr_t * var_t, indices)\n            )\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return tf.group(*updates)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                ""learning_rate"": self._serialize_hyperparameter(""learning_rate""),\n                ""beta_1"": self._serialize_hyperparameter(""beta_1""),\n                ""beta_2"": self._serialize_hyperparameter(""beta_2""),\n                ""decay"": self._serialize_hyperparameter(""decay""),\n                ""weight_decay"": self._serialize_hyperparameter(""weight_decay""),\n                ""sma_threshold"": self._serialize_hyperparameter(""sma_threshold""),\n                ""epsilon"": self.epsilon,\n                ""amsgrad"": self.amsgrad,\n                ""total_steps"": self._serialize_hyperparameter(""total_steps""),\n                ""warmup_proportion"": self._serialize_hyperparameter(\n                    ""warmup_proportion""\n                ),\n                ""min_lr"": self._serialize_hyperparameter(""min_lr""),\n            }\n        )\n        return config\n'"
tensorflow_addons/optimizers/stochastic_weight_averaging.py,13,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""An implementation of the Stochastic Weight Averaging optimizer.\n\nThe Stochastic Weight Averaging mechanism was proposed by Pavel Izmailov\net. al in the paper [Averaging Weights Leads to Wider Optima and Better\nGeneralization](https://arxiv.org/abs/1803.05407). The optimizer\nimplements averaging of multiple points along the trajectory of SGD.\nThis averaging has shown to improve model performance on validation/test\nsets whilst possibly causing a small increase in loss on the training\nset.\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.optimizers.average_wrapper import AveragedOptimizerWrapper\nfrom tensorflow_addons.utils import types\n\nfrom typeguard import typechecked\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass SWA(AveragedOptimizerWrapper):\n    """"""This class extends optimizers with Stochastic Weight Averaging (SWA).\n\n    The Stochastic Weight Averaging mechanism was proposed by Pavel Izmailov\n    et. al in the paper [Averaging Weights Leads to Wider Optima and\n    Better Generalization](https://arxiv.org/abs/1803.05407). The optimizer\n    implements averaging of multiple points along the trajectory of SGD. The\n    optimizer expects an inner optimizer which will be used to apply the\n    gradients to the variables and itself computes a running average of the\n    variables every `k` steps (which generally corresponds to the end\n    of a cycle when a cyclic learning rate is employed).\n\n    We also allow the specification of the number of steps averaging\n    should first happen after. Let\'s say, we want averaging to happen every `k`\n    steps after the first `m` steps. After step `m` we\'d take a snapshot of the\n    variables and then average the weights appropriately at step `m + k`,\n    `m + 2k` and so on. The assign_average_vars function can be called at the\n    end of training to obtain the averaged_weights from the optimizer.\n\n    Note: If your model has batch-normalization layers you would need to run\n    the final weights through the data to compute the running mean and\n    variance corresponding to the activations for each layer of the network.\n    From the paper: If the DNN uses batch normalization we run one\n    additional pass over the data, to compute the running mean and standard\n    deviation of the activations for each layer of the network with SWA\n    weights after the training is finished, since these statistics are not\n    collected during training. For most deep learning libraries, such as\n    PyTorch or Tensorflow, one can typically collect these statistics by\n    making a forward pass over the data in training mode\n    ([Averaging Weights Leads to Wider Optima and Better\n    Generalization](https://arxiv.org/abs/1803.05407))\n\n    Example of usage:\n\n    ```python\n    opt = tf.keras.optimizers.SGD(learning_rate)\n    opt = tfa.optimizers.SWA(opt, start_averaging=m, average_period=k)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        optimizer: types.Optimizer,\n        start_averaging: int = 0,\n        average_period: int = 10,\n        name: str = ""SWA"",\n        sequential_update: bool = True,\n        **kwargs\n    ):\n        r""""""Wrap optimizer with the Stochastic Weight Averaging mechanism.\n\n        Args:\n            optimizer: The original optimizer that will be used to compute and\n                apply the gradients.\n            start_averaging: An integer. Threshold to start averaging using\n                SWA. Averaging only occurs at `start_averaging` iters, must\n                be >= 0. If start_averaging = m, the first snapshot will be\n                taken after the mth application of gradients (where the first\n                iteration is iteration 0).\n            average_period: An integer. The synchronization period of SWA. The\n                averaging occurs every average_period steps. Averaging period\n                needs to be >= 1.\n            name: Optional name for the operations created when applying\n                gradients. Defaults to \'SWA\'.\n            sequential_update: Bool. If False, will compute the moving average\n                at the same time as the model is updated, potentially doing\n                benign data races. If True, will update the moving average\n                after gradient updates.\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by\n                norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse\n                decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(optimizer, sequential_update, name, **kwargs)\n\n        if average_period < 1:\n            raise ValueError(""average_period must be >= 1"")\n        if start_averaging < 0:\n            raise ValueError(""start_averaging must be >= 0"")\n\n        self._set_hyper(""average_period"", average_period)\n        self._set_hyper(""start_averaging"", start_averaging)\n\n    def average_op(self, var, average_var):\n        average_period = self._get_hyper(""average_period"", tf.dtypes.int64)\n        start_averaging = self._get_hyper(""start_averaging"", tf.dtypes.int64)\n        # check if the correct number of iterations has taken place to start\n        # averaging.\n        thresold_cond = tf.greater_equal(self.iterations, start_averaging)\n        # number of times snapshots of weights have been taken (using max to\n        # avoid negative values of num_snapshots).\n        num_snapshots = tf.math.maximum(\n            tf.cast(0, tf.int64),\n            tf.math.floordiv(self.iterations - start_averaging, average_period),\n        )\n        # checks if the iteration is one in which a snapshot should be taken.\n        sync_cond = tf.equal(\n            start_averaging + num_snapshots * average_period, self.iterations\n        )\n        num_snapshots = tf.cast(num_snapshots, tf.float32)\n        average_value = (average_var * num_snapshots + var) / (num_snapshots + 1.0)\n        average_cond = tf.reduce_all([thresold_cond, sync_cond])\n        with tf.control_dependencies([average_value]):\n            average_update = average_var.assign(\n                tf.where(average_cond, average_value, average_var,),\n                use_locking=self._use_locking,\n            )\n        return average_update\n\n    def get_config(self):\n        config = {\n            ""average_period"": self._serialize_hyperparameter(""average_period""),\n            ""start_averaging"": self._serialize_hyperparameter(""start_averaging""),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/optimizers/utils.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional Utilities used for tfa.optimizers.""""""\n\nimport tensorflow as tf\n\n\ndef fit_bn(model, *args, **kwargs):\n    """"""Resets batch normalization layers of model, and recalculates the\n    statistics for each batchnorm layer by running a pass on the data.\n\n    Args:\n        model: An instance of tf.keras.Model\n        *args, **kwargs: Params that\'ll be passed to `.fit` method of model\n    """"""\n    kwargs[""epochs""] = 1\n    if not isinstance(model, tf.keras.Model):\n        raise TypeError(""model must be an instance of tf.keras.Model"")\n\n    if not model.built:\n        raise ValueError(""Call `fit_bn` after the model is built and trained"")\n\n    assign_ops = []\n    for layer in model.layers:\n        if isinstance(layer, tf.keras.layers.BatchNormalization):\n            assign_ops.extend(\n                [\n                    layer.moving_mean.assign(tf.zeros_like(layer.moving_mean)),\n                    layer.moving_variance.assign(tf.ones_like(layer.moving_variance)),\n                ]\n            )\n\n    _trainable = model.trainable\n    _metrics = model._metrics\n    model.trainable = False\n    model._metrics = []\n\n    model.fit(*args, **kwargs)\n\n    model.trainable = _trainable\n    model._metrics = _metrics\n'"
tensorflow_addons/optimizers/weight_decay_optimizers.py,30,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base class to make optimizers weight decay ready.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable, Type\n\n\ndef _ref(var):\n    return var.ref() if hasattr(var, ""ref"") else var.experimental_ref()\n\n\nclass DecoupledWeightDecayExtension:\n    """"""This class allows to extend optimizers with decoupled weight decay.\n\n    It implements the decoupled weight decay described by Loshchilov & Hutter\n    (https://arxiv.org/pdf/1711.05101.pdf), in which the weight decay is\n    decoupled from the optimization steps w.r.t. to the loss function.\n    For SGD variants, this simplifies hyperparameter search since it decouples\n    the settings of weight decay and learning rate.\n    For adaptive gradient algorithms, it regularizes variables with large\n    gradients more than L2 regularization would, which was shown to yield\n    better training loss and generalization error in the paper above.\n\n    This class alone is not an optimizer but rather extends existing\n    optimizers with decoupled weight decay. We explicitly define the two\n    examples used in the above paper (SGDW and AdamW), but in general this can\n    extend any OptimizerX class by using\n        `ExtendedCls = extend_with_decoupled_weight_decay(OptimizerX)`.\n    Weight decay can then be set when instantiating the optimizer:\n        `optimizerX = ExtendedCls(weight_decay=0.001, learning_rate=0.001)`.\n    In order for it to work, it must be the first class the Optimizer with\n    weight decay inherits from, e.g.\n\n    ```python\n    class AdamW(DecoupledWeightDecayExtension, tf.keras.optimizers.Adam):\n      def __init__(self, weight_decay, *args, **kwargs):\n        super(AdamW, self).__init__(weight_decay, *args, **kwargs).\n    ```\n\n    Note: this extension decays weights BEFORE applying the update based\n    on the gradient, i.e. this extension only has the desired behaviour for\n    optimizers which do not depend on the value of\'var\' in the update step!\n\n    Note: when applying a decay to the learning rate, be sure to manually apply\n    the decay to the `weight_decay` as well. For example:\n\n    ```python\n    step = tf.Variable(0, trainable=False)\n    schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n        [10000, 15000], [1e-0, 1e-1, 1e-2])\n    # lr and wd can be a function or a tensor\n    lr = 1e-1 * schedule(step)\n    wd = lambda: 1e-4 * schedule(step)\n\n    # ...\n\n    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(self, weight_decay: Union[FloatTensorLike, Callable], **kwargs):\n        """"""Extension class that adds weight decay to an optimizer.\n\n        Args:\n            weight_decay: A `Tensor` or a floating point value, the factor by\n                which a variable is decayed in the update step.\n            **kwargs: Optional list or tuple or set of `Variable` objects to\n                decay.\n        """"""\n        wd = kwargs.pop(""weight_decay"", weight_decay)\n        super().__init__(**kwargs)\n        self._decay_var_list = None  # is set in minimize or apply_gradients\n        self._set_hyper(""weight_decay"", wd)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {""weight_decay"": self._serialize_hyperparameter(""weight_decay""),}\n        )\n        return config\n\n    def minimize(self, loss, var_list, grad_loss=None, name=None, decay_var_list=None):\n        """"""Minimize `loss` by updating `var_list`.\n\n        This method simply computes gradient using `tf.GradientTape` and calls\n        `apply_gradients()`. If you want to process the gradient before\n        applying then call `tf.GradientTape` and `apply_gradients()` explicitly\n        instead of using this function.\n\n        Args:\n            loss: A callable taking no arguments which returns the value to\n                minimize.\n            var_list: list or tuple of `Variable` objects to update to\n                minimize `loss`, or a callable returning the list or tuple of\n                `Variable` objects. Use callable when the variable list would\n                otherwise be incomplete before `minimize` since the variables\n                are created at the first time `loss` is called.\n            grad_loss: Optional. A `Tensor` holding the gradient computed for\n                `loss`.\n            decay_var_list: Optional list of variables to be decayed. Defaults\n                to all variables in var_list.\n            name: Optional name for the returned operation.\n        Returns:\n            An Operation that updates the variables in `var_list`.\n        Raises:\n            ValueError: If some of the variables are not `Variable` objects.\n        """"""\n        self._decay_var_list = (\n            set([_ref(v) for v in decay_var_list]) if decay_var_list else False\n        )\n        return super().minimize(loss, var_list=var_list, grad_loss=grad_loss, name=name)\n\n    def apply_gradients(self, grads_and_vars, name=None, decay_var_list=None, **kwargs):\n        """"""Apply gradients to variables.\n\n        This is the second part of `minimize()`. It returns an `Operation` that\n        applies gradients.\n\n        Args:\n            grads_and_vars: List of (gradient, variable) pairs.\n            name: Optional name for the returned operation.  Default to the\n                name passed to the `Optimizer` constructor.\n            decay_var_list: Optional list of variables to be decayed. Defaults\n                to all variables in var_list.\n            **kwargs: Additional arguments to pass to the base optimizer\'s\n                apply_gradient method, e.g., TF2.2 added an argument\n                `experimental_aggregate_gradients`.\n        Returns:\n            An `Operation` that applies the specified gradients.\n        Raises:\n            TypeError: If `grads_and_vars` is malformed.\n            ValueError: If none of the variables have gradients.\n        """"""\n        self._decay_var_list = (\n            set([_ref(v) for v in decay_var_list]) if decay_var_list else False\n        )\n        return super().apply_gradients(grads_and_vars, name=name, **kwargs)\n\n    def _decay_weights_op(self, var):\n        if not self._decay_var_list or _ref(var) in self._decay_var_list:\n            return var.assign_sub(\n                self._get_hyper(""weight_decay"", var.dtype) * var, self._use_locking\n            )\n        return tf.no_op()\n\n    def _decay_weights_sparse_op(self, var, indices):\n        if not self._decay_var_list or _ref(var) in self._decay_var_list:\n            update = -self._get_hyper(""weight_decay"", var.dtype) * tf.gather(\n                var, indices\n            )\n            return self._resource_scatter_add(var, indices, update)\n        return tf.no_op()\n\n    # Here, we overwrite the apply functions that the base optimizer calls.\n    # super().apply_x resolves to the apply_x function of the BaseOptimizer.\n\n    def _resource_apply_dense(self, grad, var):\n        with tf.control_dependencies([self._decay_weights_op(var)]):\n            return super()._resource_apply_dense(grad, var)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        decay_op = self._decay_weights_sparse_op(var, indices)\n        with tf.control_dependencies([decay_op]):\n            return super()._resource_apply_sparse(grad, var, indices)\n\n\n@typechecked\ndef extend_with_decoupled_weight_decay(\n    base_optimizer: Type[tf.keras.optimizers.Optimizer],\n) -> Type[tf.keras.optimizers.Optimizer]:\n    """"""Factory function returning an optimizer class with decoupled weight\n    decay.\n\n    Returns an optimizer class. An instance of the returned class computes the\n    update step of `base_optimizer` and additionally decays the weights.\n    E.g., the class returned by\n    `extend_with_decoupled_weight_decay(tf.keras.optimizers.Adam)` is\n    equivalent to `tfa.optimizers.AdamW`.\n\n    The API of the new optimizer class slightly differs from the API of the\n    base optimizer:\n    - The first argument to the constructor is the weight decay rate.\n    - `minimize` and `apply_gradients` accept the optional keyword argument\n      `decay_var_list`, which specifies the variables that should be decayed.\n      If `None`, all variables that are optimized are decayed.\n\n    Usage example:\n    ```python\n    # MyAdamW is a new class\n    MyAdamW = extend_with_decoupled_weight_decay(tf.keras.optimizers.Adam)\n    # Create a MyAdamW object\n    optimizer = MyAdamW(weight_decay=0.001, learning_rate=0.001)\n    # update var1, var2 but only decay var1\n    optimizer.minimize(loss, var_list=[var1, var2], decay_variables=[var1])\n\n    Note: this extension decays weights BEFORE applying the update based\n    on the gradient, i.e. this extension only has the desired behaviour for\n    optimizers which do not depend on the value of \'var\' in the update step!\n\n    Note: when applying a decay to the learning rate, be sure to manually apply\n    the decay to the `weight_decay` as well. For example:\n\n    ```python\n    step = tf.Variable(0, trainable=False)\n    schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n        [10000, 15000], [1e-0, 1e-1, 1e-2])\n    # lr and wd can be a function or a tensor\n    lr = 1e-1 * schedule(step)\n    wd = lambda: 1e-4 * schedule(step)\n\n    # ...\n\n    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n    ```\n\n    Note: you might want to register your own custom optimizer using\n    `tf.keras.utils.get_custom_objects()`.\n\n    Args:\n        base_optimizer: An optimizer class that inherits from\n            tf.optimizers.Optimizer.\n\n    Returns:\n        A new optimizer class that inherits from DecoupledWeightDecayExtension\n        and base_optimizer.\n    """"""\n\n    class OptimizerWithDecoupledWeightDecay(\n        DecoupledWeightDecayExtension, base_optimizer\n    ):\n        """"""Base_optimizer with decoupled weight decay.\n\n        This class computes the update step of `base_optimizer` and\n        additionally decays the variable with the weight decay being\n        decoupled from the optimization steps w.r.t. to the loss\n        function, as described by Loshchilov & Hutter\n        (https://arxiv.org/pdf/1711.05101.pdf). For SGD variants, this\n        simplifies hyperparameter search since it decouples the settings\n        of weight decay and learning rate. For adaptive gradient\n        algorithms, it regularizes variables with large gradients more\n        than L2 regularization would, which was shown to yield better\n        training loss and generalization error in the paper above.\n        """"""\n\n        @typechecked\n        def __init__(\n            self, weight_decay: Union[FloatTensorLike, Callable], *args, **kwargs\n        ):\n            # super delegation is necessary here\n            super().__init__(weight_decay, *args, **kwargs)\n\n    return OptimizerWithDecoupledWeightDecay\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass SGDW(DecoupledWeightDecayExtension, tf.keras.optimizers.SGD):\n    """"""Optimizer that implements the Momentum algorithm with weight_decay.\n\n    This is an implementation of the SGDW optimizer described in ""Decoupled\n    Weight Decay Regularization"" by Loshchilov & Hutter\n    (https://arxiv.org/abs/1711.05101)\n    ([pdf])(https://arxiv.org/pdf/1711.05101.pdf).\n    It computes the update step of `tf.keras.optimizers.SGD` and additionally\n    decays the variable. Note that this is different from adding\n    L2 regularization on the variables to the loss. Decoupling the weight decay\n    from other hyperparameters (in particular the learning rate) simplifies\n    hyperparameter search.\n\n    For further information see the documentation of the SGD Optimizer.\n\n    This optimizer can also be instantiated as\n    ```python\n    extend_with_decoupled_weight_decay(tf.keras.optimizers.SGD,\n                                       weight_decay=weight_decay)\n    ```\n\n    Note: when applying a decay to the learning rate, be sure to manually apply\n    the decay to the `weight_decay` as well. For example:\n\n    ```python\n    step = tf.Variable(0, trainable=False)\n    schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n        [10000, 15000], [1e-0, 1e-1, 1e-2])\n    # lr and wd can be a function or a tensor\n    lr = 1e-1 * schedule(step)\n    wd = lambda: 1e-4 * schedule(step)\n\n    # ...\n\n    optimizer = tfa.optimizers.SGDW(\n        learning_rate=lr, weight_decay=wd, momentum=0.9)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        weight_decay: Union[FloatTensorLike, Callable],\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        momentum: Union[FloatTensorLike, Callable] = 0.0,\n        nesterov: bool = False,\n        name: str = ""SGDW"",\n        **kwargs\n    ):\n        """"""Construct a new SGDW optimizer.\n\n        For further information see the documentation of the SGD Optimizer.\n\n        Args:\n            learning_rate: float hyperparameter >= 0. Learning rate.\n            momentum: float hyperparameter >= 0 that accelerates SGD in the\n                relevant direction and dampens oscillations.\n            nesterov: boolean. Whether to apply Nesterov momentum.\n            name: Optional name prefix for the operations created when applying\n                gradients.  Defaults to \'SGD\'.\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by\n                norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse decay\n                of learning rate. `lr` is included for backward compatibility,\n                recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(\n            weight_decay,\n            learning_rate=learning_rate,\n            momentum=momentum,\n            nesterov=nesterov,\n            name=name,\n            **kwargs,\n        )\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass AdamW(DecoupledWeightDecayExtension, tf.keras.optimizers.Adam):\n    """"""Optimizer that implements the Adam algorithm with weight decay.\n\n    This is an implementation of the AdamW optimizer described in ""Decoupled\n    Weight Decay Regularization"" by Loshch ilov & Hutter\n    (https://arxiv.org/abs/1711.05101)\n    ([pdf])(https://arxiv.org/pdf/1711.05101.pdf).\n\n    It computes the update step of `tf.keras.optimizers.Adam` and additionally\n    decays the variable. Note that this is different from adding L2\n    regularization on the variables to the loss: it regularizes variables with\n    large gradients more than L2 regularization would, which was shown to yield\n    better training loss and generalization error in the paper above.\n\n    For further information see the documentation of the Adam Optimizer.\n\n    This optimizer can also be instantiated as\n    ```python\n    extend_with_decoupled_weight_decay(tf.keras.optimizers.Adam,\n                                       weight_decay=weight_decay)\n    ```\n\n    Note: when applying a decay to the learning rate, be sure to manually apply\n    the decay to the `weight_decay` as well. For example:\n\n    ```python\n    step = tf.Variable(0, trainable=False)\n    schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n        [10000, 15000], [1e-0, 1e-1, 1e-2])\n    # lr and wd can be a function or a tensor\n    lr = 1e-1 * schedule(step)\n    wd = lambda: 1e-4 * schedule(step)\n\n    # ...\n\n    optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        weight_decay: Union[FloatTensorLike, Callable],\n        learning_rate: Union[FloatTensorLike, Callable] = 0.001,\n        beta_1: Union[FloatTensorLike, Callable] = 0.9,\n        beta_2: Union[FloatTensorLike, Callable] = 0.999,\n        epsilon: FloatTensorLike = 1e-07,\n        amsgrad: bool = False,\n        name: str = ""AdamW"",\n        **kwargs\n    ):\n        """"""Construct a new AdamW optimizer.\n\n        For further information see the documentation of the Adam Optimizer.\n\n        Args:\n            weight_decay: A Tensor or a floating point value. The weight decay.\n            learning_rate: A Tensor or a floating point value. The learning\n                rate.\n            beta_1: A float value or a constant float tensor. The exponential\n                decay rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor. The exponential\n                decay rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                ""epsilon hat"" in the Kingma and Ba paper (in the formula just\n                before Section 2.1), not the epsilon in Algorithm 1 of the\n                paper.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this\n                algorithm from the paper ""On the Convergence of Adam and\n                beyond"".\n            name: Optional name for the operations created when applying\n                gradients. Defaults to ""AdamW"".\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients by\n                norm; `clipvalue` is clip gradients by value, `decay` is\n                included for backward compatibility to allow time inverse decay\n                of learning rate. `lr` is included for backward compatibility,\n                recommended to use `learning_rate` instead.\n        """"""\n        super().__init__(\n            weight_decay,\n            learning_rate=learning_rate,\n            beta_1=beta_1,\n            beta_2=beta_2,\n            epsilon=epsilon,\n            amsgrad=amsgrad,\n            name=name,\n            **kwargs,\n        )\n'"
tensorflow_addons/optimizers/yogi.py,38,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Yogi: Extension of yogi adaptive nonconvex optimizer in Keras.\n\nImplementation of Additive Averaging.\nm_t+1 = beta1*m_t + (1-beta1)*g_t\nv_t+1 = v_t + sign(g_t-v_t)(g_t^2)\nExperiments show better performance across NLP and Vision tasks.\nPaper:\nhttps://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf\n""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typeguard import typechecked\nfrom typing import Union, Callable\n\n\ndef _solve(a, b, c):\n    """"""Return solution of a quadratic minimization.\n\n    The optimization equation is:\n         f(a, b, c) = argmin_w{1/2 * a * w^2 + b * w + c * |w|}\n    we get optimal solution w*:\n         w* = -(b - sign(b)*c)/a if |b| > c else w* = 0\n    REQUIRES: Dimensionality of a and b must be same\n    Args:\n      a: A Tensor\n      b: A Tensor\n      c: A Tensor with one element.\n    Returns:\n      A Tensor w, which is solution for the equation\n    """"""\n    w = (c * tf.sign(b) - b) / a\n    w = tf.cast(tf.abs(b) > c, dtype=b.dtype) * w\n    return w\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass Yogi(tf.keras.optimizers.Optimizer):\n    """"""Optimizer that implements the Yogi algorithm in Keras.\n\n    See Algorithm 2 of\n    https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        learning_rate: Union[FloatTensorLike, Callable] = 0.01,\n        beta1: FloatTensorLike = 0.9,\n        beta2: FloatTensorLike = 0.999,\n        epsilon: FloatTensorLike = 1e-3,\n        l1_regularization_strength: FloatTensorLike = 0.0,\n        l2_regularization_strength: FloatTensorLike = 0.0,\n        initial_accumulator_value: FloatTensorLike = 1e-6,\n        activation: str = ""sign"",\n        name: str = ""Yogi"",\n        **kwargs\n    ):\n        """"""Construct a new Yogi optimizer.\n\n        Args:\n          learning_rate: A Tensor or a floating point value.\n            The learning rate.\n          beta1: A float value or a constant float tensor.\n            The exponential decay rate for the 1st moment estimates.\n          beta2: A float value or a constant float tensor.\n            The exponential decay rate for the 2nd moment estimates.\n          epsilon: A constant trading off adaptivity and noise.\n          l1_regularization_strength: A float value, must be greater than or\n            equal to zero.\n          l2_regularization_strength: A float value, must be greater than or\n            equal to zero.\n          initial_accumulator_value: The starting value for accumulators.\n            Only positive values are allowed.\n          activation: Use hard sign or soft tanh to determin sign.\n          name: Optional name for the operations created when applying\n            gradients. Defaults to ""Yogi"".\n          **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`,\n            `lr`, `decay`}. `clipnorm` is clip gradients by norm; `clipvalue`\n            is clip gradients by value, `decay` is included for backward\n            compatibility to allow time inverse decay of learning rate. `lr`\n            is included for backward compatibility, recommended to use\n            `learning_rate` instead.\n        """"""\n        super().__init__(name, **kwargs)\n        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))\n        self._set_hyper(""decay"", self._initial_decay)\n        self._set_hyper(""beta_1"", beta1)\n        self._set_hyper(""beta_2"", beta2)\n        self._set_hyper(""epsilon"", epsilon)\n        self._set_hyper(""l1_regularization_strength"", l1_regularization_strength)\n        self._set_hyper(""l2_regularization_strength"", l2_regularization_strength)\n\n        self._beta1 = beta1\n        self._activation = activation\n        self._initial_accumulator_value = initial_accumulator_value\n        self._l1_regularization_strength = l1_regularization_strength\n        self._l2_regularization_strength = l2_regularization_strength\n\n    def _create_slots(self, var_list):\n        """"""See `tf.train.Optimizer._create_slots()`.""""""\n        # Create slots for the first and second moments, and maximum second moments.\n        for var in var_list:\n            init = tf.constant_initializer(self._initial_accumulator_value)\n            self.add_slot(var, ""v"", init)\n            if self._beta1 > 0.0:\n                self.add_slot(var, ""m"")\n\n    def _resource_apply_dense(self, grad, var):\n        """"""See `tf.train.Optimizer._apply_dense()`.""""""\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta1_t = self._get_hyper(""beta_1"", var_dtype)\n        beta2_t = self._get_hyper(""beta_2"", var_dtype)\n        epsilon_t = self._get_hyper(""epsilon"", var_dtype)\n        l1_t = self._get_hyper(""l1_regularization_strength"", var_dtype)\n        l2_t = self._get_hyper(""l2_regularization_strength"", var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta1_power = tf.pow(beta1_t, local_step)\n        beta2_power = tf.pow(beta2_t, local_step)\n\n        lr = lr_t * tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n\n        update_vs = []\n        if self._beta1 == 0.0:\n            # v_t = v + sign(g_t^2-v)(g_t^2)\n            v = self.get_slot(var, ""v"")\n            grad2 = grad * grad\n            if self._activation == ""sign"":\n                sign = tf.sign(grad2 - v)\n            elif self._activation == ""tanh"":\n                sign = tf.tanh(10 * (grad2 - v))\n            else:\n                raise NotImplementedError(""Activation function can be sign or tanh"")\n            v_t = v.assign_add(\n                (1 - beta2_t) * sign * grad2, use_locking=self._use_locking\n            )\n            v_sqrt = tf.sqrt(v_t)\n\n            # Yogi effective LR\n            per_coord_lr = lr / (v_sqrt + epsilon_t)\n\n            # Variable update\n            # Step 1: Gradient descent\n            new_var = var - per_coord_lr * grad\n            # Step 2: Prox operator\n            if self._l1_regularization_strength > 0:\n                new_var = _solve(1 + l2_t * per_coord_lr, -new_var, l1_t * per_coord_lr)\n            elif self._l2_regularization_strength > 0:\n                new_var = new_var / (1 + l2_t * per_coord_lr)\n            # Step 3: Update\n            var_update = var.assign(new_var, use_locking=self._use_locking)\n\n            update_vs.append(var_update)\n            update_vs.append(v_t)\n\n        else:\n            # m_t = beta1 * m + (1 - beta1) * g_t\n            m = self.get_slot(var, ""m"")\n            m_t = m.assign(\n                m * beta1_t + grad * (1 - beta1_t), use_locking=self._use_locking\n            )\n\n            # v_t = v + sign(g_t^2-v)(g_t^2)\n            v = self.get_slot(var, ""v"")\n            grad2 = grad * grad\n            if self._activation == ""sign"":\n                sign = tf.sign(grad2 - v)\n            elif self._activation == ""tanh"":\n                sign = tf.tanh(10 * (grad2 - v))\n            else:\n                raise NotImplementedError(""Activation function can be sign or tanh"")\n            v_t = v.assign_add(\n                (1 - beta2_t) * sign * grad2, use_locking=self._use_locking\n            )\n            v_sqrt = tf.sqrt(v_t)\n\n            # Yogi effective LR\n            per_coord_lr = lr / (v_sqrt + epsilon_t)\n\n            # Variable update\n            # Step 1: Gradient descent\n            new_var = var - per_coord_lr * m_t\n            # Step 2: Prox operator\n            if self._l1_regularization_strength > 0:\n                new_var = _solve(1 + l2_t * per_coord_lr, -new_var, l1_t * per_coord_lr)\n            elif self._l2_regularization_strength > 0:\n                new_var = new_var / (1 + l2_t * per_coord_lr)\n            # Step 3: Update\n            var_update = var.assign(new_var, use_locking=self._use_locking)\n            update_vs.append(var_update)\n            update_vs.append(m_t)\n            update_vs.append(v_t)\n\n        # Create an op that groups all the above operations\n        return tf.group(*update_vs)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        """"""Applies sparse gradients to a variable.\n\n        Args:\n          grad: A tensor for the `values` of `tf.IndexedSlices`.\n          var: A `tf.Variable` object.\n          indices: A tensor for the `indices` of `tf.IndexedSlices`.\n        Returns:\n          An op which updates `var` with `grad` and `indices`.\n        """"""\n\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta1_t = self._get_hyper(""beta_1"", var_dtype)\n        beta2_t = self._get_hyper(""beta_2"", var_dtype)\n        epsilon_t = self._get_hyper(""epsilon"", var_dtype)\n        l1_t = self._get_hyper(""l1_regularization_strength"", var_dtype)\n        l2_t = self._get_hyper(""l2_regularization_strength"", var_dtype)\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n        beta1_power = tf.pow(beta1_t, local_step)\n        beta2_power = tf.pow(beta2_t, local_step)\n\n        lr = lr_t * tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n\n        update_vs = []\n        if self._beta1 == 0.0:\n            # v_t = v + sign(g_t^2-v)(g_t^2)\n            v = self.get_slot(var, ""v"")\n            grad2 = grad * grad\n            v_slice = tf.gather(v, indices)\n            if self._activation == ""sign"":\n                sign = tf.sign(grad2 - v_slice)\n            elif self._activation == ""tanh"":\n                sign = tf.tanh(10 * (grad2 - v_slice))\n            else:\n                raise NotImplementedError(""Activation function can be sign or tanh"")\n            v_scaled_g_values = v_slice + (1 - beta2_t) * sign * grad2\n            v_t = self._resource_scatter_update(v, indices, v_scaled_g_values)\n            v_sqrt = tf.sqrt(v_scaled_g_values)\n\n            # Yogi effective LR\n            per_coord_lr = lr / (v_sqrt + epsilon_t)\n\n            # Variable update\n            # Step 1: Gradient descent\n            var_slice = tf.gather(var, indices)\n            new_var = var_slice - per_coord_lr * grad\n            # Step 2: Prox operator\n            if self._l1_regularization_strength > 0:\n                new_var = _solve(1 + l2_t * per_coord_lr, -new_var, l1_t * per_coord_lr)\n            elif self._l2_regularization_strength > 0:\n                new_var = new_var / (1 + l2_t * per_coord_lr)\n            # Step 3: Update\n            var_update = self._resource_scatter_update(var, indices, new_var)\n            update_vs.append(var_update)\n            update_vs.append(v_t)\n\n        else:\n            # m_t = beta1 * m + (1 - beta1) * g_t\n            m = self.get_slot(var, ""m"")\n            m_scaled_g_values = grad * (1 - beta1_t)\n            m_t = m.assign(m * beta1_t, use_locking=self._use_locking)\n            with tf.control_dependencies([m_t]):\n                m_slice = tf.gather(m, indices) + m_scaled_g_values\n                m_t = self._resource_scatter_update(m, indices, m_slice)\n\n            # v_t = v + sign(g_t^2-v)(g_t^2)\n            v = self.get_slot(var, ""v"")\n            grad2 = grad * grad\n            v_slice = tf.gather(v, indices)\n            if self._activation == ""sign"":\n                sign = tf.sign(grad2 - tf.gather(v, indices))\n            elif self._activation == ""tanh"":\n                sign = tf.tanh(10 * (grad2 - tf.gather(v, indices)))\n            else:\n                raise NotImplementedError(""Activation function can be sign or tanh"")\n            v_scaled_g_values = v_slice + (1 - beta2_t) * sign * grad2\n            v_t = self._resource_scatter_update(v, indices, v_scaled_g_values)\n            v_sqrt = tf.sqrt(v_scaled_g_values)\n\n            # Yogi effective LR\n            per_coord_lr = lr / (v_sqrt + epsilon_t)\n\n            # Variable update\n            # Step 1: Gradient descent\n            var_slice = tf.gather(var, indices)\n            new_var = var_slice - per_coord_lr * m_slice\n            # Step 2: Prox operator\n            if self._l1_regularization_strength > 0:\n                new_var = _solve(1 + l2_t * per_coord_lr, -new_var, l1_t * per_coord_lr)\n            elif self._l2_regularization_strength > 0:\n                new_var = new_var / (1 + l2_t * per_coord_lr)\n            # Step 3: Update\n            var_update = self._resource_scatter_update(var, indices, new_var)\n            update_vs.append(var_update)\n            update_vs.append(m_t)\n            update_vs.append(v_t)\n\n        # Create an op that groups all the above operations\n        return tf.group(*update_vs)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                ""learning_rate"": self._serialize_hyperparameter(""learning_rate""),\n                ""decay"": self._serialize_hyperparameter(""decay""),\n                ""beta1"": self._serialize_hyperparameter(""beta_1""),\n                ""beta2"": self._serialize_hyperparameter(""beta_2""),\n                ""epsilon"": self._serialize_hyperparameter(""epsilon""),\n                ""l1_regularization_strength"": self._serialize_hyperparameter(\n                    ""l1_regularization_strength""\n                ),\n                ""l2_regularization_strength"": self._serialize_hyperparameter(\n                    ""l2_regularization_strength""\n                ),\n                ""activation"": self._activation,\n                ""initial_accumulator_value"": self._initial_accumulator_value,\n            }\n        )\n        return config\n'"
tensorflow_addons/rnn/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional RNN cells that corform to Keras API.""""""\n\nfrom tensorflow_addons.rnn.cell import LayerNormLSTMCell\nfrom tensorflow_addons.rnn.cell import NASCell\nfrom tensorflow_addons.rnn.cell import LayerNormSimpleRNNCell\nfrom tensorflow_addons.rnn.cell import ESNCell\n'"
tensorflow_addons/rnn/cell.py,39,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Module for RNN Cells.""""""\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom typeguard import typechecked\n\nfrom tensorflow_addons.utils.types import (\n    Activation,\n    FloatTensorLike,\n    TensorLike,\n    Initializer,\n    Constraint,\n    Regularizer,\n)\nfrom typing import Optional\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass NASCell(keras.layers.AbstractRNNCell):\n    """"""Neural Architecture Search (NAS) recurrent network cell.\n\n    This implements the recurrent cell from the paper:\n\n      https://arxiv.org/abs/1611.01578\n\n    Barret Zoph and Quoc V. Le.\n    ""Neural Architecture Search with Reinforcement Learning"" Proc. ICLR 2017.\n\n    The class uses an optional projection layer.\n    """"""\n\n    # NAS cell\'s architecture base.\n    _NAS_BASE = 8\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        projection: Optional[FloatTensorLike] = None,\n        use_bias: bool = False,\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        recurrent_initializer: Initializer = ""glorot_uniform"",\n        projection_initializer: Initializer = ""glorot_uniform"",\n        bias_initializer: Initializer = ""zeros"",\n        **kwargs\n    ):\n        """"""Initialize the parameters for a NAS cell.\n\n        Args:\n          units: int, The number of units in the NAS cell.\n          projection: (optional) int, The output dimensionality for the\n            projection matrices.  If None, no projection is performed.\n          use_bias: (optional) bool, If True then use biases within the cell.\n            This is False by default.\n          kernel_initializer: Initializer for kernel weight.\n          recurrent_initializer: Initializer for recurrent kernel weight.\n          projection_initializer: Initializer for projection weight, used when\n            projection is not None.\n          bias_initializer: Initializer for bias, used when use_bias is True.\n          **kwargs: Additional keyword arguments.\n        """"""\n        super().__init__(**kwargs)\n        self.units = units\n        self.projection = projection\n        self.use_bias = use_bias\n        self.kernel_initializer = kernel_initializer\n        self.recurrent_initializer = recurrent_initializer\n        self.projection_initializer = projection_initializer\n        self.bias_initializer = bias_initializer\n\n        if projection is not None:\n            self._state_size = [units, projection]\n            self._output_size = projection\n        else:\n            self._state_size = [units, units]\n            self._output_size = units\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._output_size\n\n    def build(self, inputs_shape):\n        input_size = tf.compat.dimension_value(\n            tf.TensorShape(inputs_shape).with_rank(2)[1]\n        )\n        if input_size is None:\n            raise ValueError(""Could not infer input size from inputs.get_shape()[-1]"")\n\n        # Variables for the NAS cell. `recurrent_kernel` is all matrices\n        # multiplying the hidden state and `kernel` is all matrices multiplying\n        # the inputs.\n        self.recurrent_kernel = self.add_weight(\n            name=""recurrent_kernel"",\n            shape=[self.output_size, self._NAS_BASE * self.units],\n            initializer=self.recurrent_initializer,\n        )\n        self.kernel = self.add_weight(\n            name=""kernel"",\n            shape=[input_size, self._NAS_BASE * self.units],\n            initializer=self.kernel_initializer,\n        )\n\n        if self.use_bias:\n            self.bias = self.add_weight(\n                name=""bias"",\n                shape=[self._NAS_BASE * self.units],\n                initializer=self.bias_initializer,\n            )\n        # Projection layer if specified\n        if self.projection is not None:\n            self.projection_weights = self.add_weight(\n                name=""projection_weights"",\n                shape=[self.units, self.projection],\n                initializer=self.projection_initializer,\n            )\n\n        self.built = True\n\n    def call(self, inputs, state):\n        """"""Run one step of NAS Cell.\n\n        Args:\n          inputs: input Tensor, 2D, batch x num_units.\n          state: This must be a list of state Tensors, both `2-D`, with column\n            sizes `c_state` and `m_state`.\n\n        Returns:\n          A tuple containing:\n          - A `2-D, [batch x output_dim]`, Tensor representing the output of\n            the NAS Cell after reading `inputs` when previous state was\n            `state`.\n            Here output_dim is:\n               projection if projection was set, units otherwise.\n          - Tensor(s) representing the new state of NAS Cell after reading\n            `inputs` when the previous state was `state`.  Same type and\n            shape(s) as `state`.\n\n        Raises:\n          ValueError: If input size cannot be inferred from inputs via\n            static shape inference.\n        """"""\n        sigmoid = tf.math.sigmoid\n        tanh = tf.math.tanh\n        relu = tf.nn.relu\n\n        c_prev, m_prev = state\n\n        m_matrix = tf.matmul(m_prev, self.recurrent_kernel)\n        inputs_matrix = tf.matmul(inputs, self.kernel)\n\n        if self.use_bias:\n            m_matrix = tf.nn.bias_add(m_matrix, self.bias)\n\n        # The NAS cell branches into 8 different splits for both the hidden\n        # state and the input\n        m_matrix_splits = tf.split(\n            axis=1, num_or_size_splits=self._NAS_BASE, value=m_matrix\n        )\n        inputs_matrix_splits = tf.split(\n            axis=1, num_or_size_splits=self._NAS_BASE, value=inputs_matrix\n        )\n\n        # First layer\n        layer1_0 = sigmoid(inputs_matrix_splits[0] + m_matrix_splits[0])\n        layer1_1 = relu(inputs_matrix_splits[1] + m_matrix_splits[1])\n        layer1_2 = sigmoid(inputs_matrix_splits[2] + m_matrix_splits[2])\n        layer1_3 = relu(inputs_matrix_splits[3] * m_matrix_splits[3])\n        layer1_4 = tanh(inputs_matrix_splits[4] + m_matrix_splits[4])\n        layer1_5 = sigmoid(inputs_matrix_splits[5] + m_matrix_splits[5])\n        layer1_6 = tanh(inputs_matrix_splits[6] + m_matrix_splits[6])\n        layer1_7 = sigmoid(inputs_matrix_splits[7] + m_matrix_splits[7])\n\n        # Second layer\n        l2_0 = tanh(layer1_0 * layer1_1)\n        l2_1 = tanh(layer1_2 + layer1_3)\n        l2_2 = tanh(layer1_4 * layer1_5)\n        l2_3 = sigmoid(layer1_6 + layer1_7)\n\n        # Inject the cell\n        l2_0 = tanh(l2_0 + c_prev)\n\n        # Third layer\n        l3_0_pre = l2_0 * l2_1\n        new_c = l3_0_pre  # create new cell\n        l3_0 = l3_0_pre\n        l3_1 = tanh(l2_2 + l2_3)\n\n        # Final layer\n        new_m = tanh(l3_0 * l3_1)\n\n        # Projection layer if specified\n        if self.projection is not None:\n            new_m = tf.matmul(new_m, self.projection_weights)\n\n        return new_m, [new_c, new_m]\n\n    def get_config(self):\n        config = {\n            ""units"": self.units,\n            ""projection"": self.projection,\n            ""use_bias"": self.use_bias,\n            ""kernel_initializer"": self.kernel_initializer,\n            ""recurrent_initializer"": self.recurrent_initializer,\n            ""bias_initializer"": self.bias_initializer,\n            ""projection_initializer"": self.projection_initializer,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass LayerNormLSTMCell(keras.layers.LSTMCell):\n    """"""LSTM cell with layer normalization and recurrent dropout.\n\n    This class adds layer normalization and recurrent dropout to a LSTM unit.\n    Layer normalization implementation is based on:\n\n      https://arxiv.org/abs/1607.06450.\n\n    ""Layer Normalization"" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\n\n    and is applied before the internal nonlinearities.\n    Recurrent dropout is based on:\n\n      https://arxiv.org/abs/1603.05118\n\n    ""Recurrent Dropout without Memory Loss""\n    Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        activation: Activation = ""tanh"",\n        recurrent_activation: Activation = ""sigmoid"",\n        use_bias: bool = True,\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        recurrent_initializer: Initializer = ""orthogonal"",\n        bias_initializer: Initializer = ""zeros"",\n        unit_forget_bias: bool = True,\n        kernel_regularizer: Regularizer = None,\n        recurrent_regularizer: Regularizer = None,\n        bias_regularizer: Regularizer = None,\n        kernel_constraint: Constraint = None,\n        recurrent_constraint: Constraint = None,\n        bias_constraint: Constraint = None,\n        dropout: FloatTensorLike = 0.0,\n        recurrent_dropout: FloatTensorLike = 0.0,\n        norm_gamma_initializer: Initializer = ""ones"",\n        norm_beta_initializer: Initializer = ""zeros"",\n        norm_epsilon: FloatTensorLike = 1e-3,\n        **kwargs\n    ):\n        """"""Initializes the LSTM cell.\n\n        Args:\n          units: Positive integer, dimensionality of the output space.\n          activation: Activation function to use. Default: hyperbolic tangent\n            (`tanh`). If you pass `None`, no activation is applied (ie.\n            ""linear"" activation: `a(x) = x`).\n          recurrent_activation: Activation function to use for the recurrent\n            step. Default: sigmoid (`sigmoid`). If you pass `None`, no\n            activation is applied (ie. ""linear"" activation: `a(x) = x`).\n          use_bias: Boolean, whether the layer uses a bias vector.\n          kernel_initializer: Initializer for the `kernel` weights matrix, used\n            for the linear transformation of the inputs.\n          recurrent_initializer: Initializer for the `recurrent_kernel` weights\n            matrix, used for the linear transformation of the recurrent state.\n          bias_initializer: Initializer for the bias vector.\n          unit_forget_bias: Boolean. If True, add 1 to the bias of the forget\n            gate at initialization. Setting it to true will also force\n            `bias_initializer=""zeros""`. This is recommended in [Jozefowicz et\n              al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n          kernel_regularizer: Regularizer function applied to the `kernel`\n            weights matrix.\n          recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix.\n          bias_regularizer: Regularizer function applied to the bias vector.\n          kernel_constraint: Constraint function applied to the `kernel`\n            weights matrix.\n          recurrent_constraint: Constraint function applied to the\n            `recurrent_kernel` weights matrix.\n          bias_constraint: Constraint function applied to the bias vector.\n          dropout: Float between 0 and 1. Fraction of the units to drop for the\n            linear transformation of the inputs.\n          recurrent_dropout: Float between 0 and 1. Fraction of the units to\n            drop for the linear transformation of the recurrent state.\n          norm_gamma_initializer: Initializer for the layer normalization gain\n            initial value.\n          norm_beta_initializer: Initializer for the layer normalization shift\n            initial value.\n          norm_epsilon: Float, the epsilon value for normalization layers.\n          **kwargs: Dict, the other keyword arguments for layer creation.\n        """"""\n        super().__init__(\n            units,\n            activation=activation,\n            recurrent_activation=recurrent_activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            recurrent_initializer=recurrent_initializer,\n            bias_initializer=bias_initializer,\n            unit_forget_bias=unit_forget_bias,\n            kernel_regularizer=kernel_regularizer,\n            recurrent_regularizer=recurrent_regularizer,\n            bias_regularizer=bias_regularizer,\n            kernel_constraint=kernel_constraint,\n            recurrent_constraint=recurrent_constraint,\n            bias_constraint=bias_constraint,\n            dropout=dropout,\n            recurrent_dropout=recurrent_dropout,\n            **kwargs,\n        )\n        self.norm_gamma_initializer = keras.initializers.get(norm_gamma_initializer)\n        self.norm_beta_initializer = keras.initializers.get(norm_beta_initializer)\n        self.norm_epsilon = norm_epsilon\n        self.kernel_norm = self._create_norm_layer(""kernel_norm"")\n        self.recurrent_norm = self._create_norm_layer(""recurrent_norm"")\n        self.state_norm = self._create_norm_layer(""state_norm"")\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        self.kernel_norm.build([input_shape[0], self.units * 4])\n        self.recurrent_norm.build([input_shape[0], self.units * 4])\n        self.state_norm.build([input_shape[0], self.units])\n\n    def call(self, inputs, states, training=None):\n        h_tm1 = states[0]  # previous memory state\n        c_tm1 = states[1]  # previous carry state\n\n        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(h_tm1, training, count=4)\n        if 0.0 < self.dropout < 1.0:\n            inputs *= dp_mask[0]\n        z = self.kernel_norm(keras.backend.dot(inputs, self.kernel))\n\n        if 0.0 < self.recurrent_dropout < 1.0:\n            h_tm1 *= rec_dp_mask[0]\n        z += self.recurrent_norm(keras.backend.dot(h_tm1, self.recurrent_kernel))\n        if self.use_bias:\n            z = keras.backend.bias_add(z, self.bias)\n\n        z = tf.split(z, num_or_size_splits=4, axis=1)\n        c, o = self._compute_carry_and_output_fused(z, c_tm1)\n        c = self.state_norm(c)\n        h = o * self.activation(c)\n        return h, [h, c]\n\n    def get_config(self):\n        config = {\n            ""norm_gamma_initializer"": keras.initializers.serialize(\n                self.norm_gamma_initializer\n            ),\n            ""norm_beta_initializer"": keras.initializers.serialize(\n                self.norm_beta_initializer\n            ),\n            ""norm_epsilon"": self.norm_epsilon,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def _create_norm_layer(self, name):\n        return keras.layers.LayerNormalization(\n            beta_initializer=self.norm_beta_initializer,\n            gamma_initializer=self.norm_gamma_initializer,\n            epsilon=self.norm_epsilon,\n            name=name,\n        )\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass LayerNormSimpleRNNCell(keras.layers.SimpleRNNCell):\n    """"""Cell class for LayerNormSimpleRNN.\n\n    References:\n    [1] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n        ""Layer Normalization."" ArXiv:1607.06450 [Cs, Stat],\n        July 21, 2016. http://arxiv.org/abs/1607.06450\n\n    Arguments:\n      units: Positive integer, dimensionality of the output space.\n      activation: Activation function to use.\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. ""linear"" activation: `a(x) = x`).\n      use_bias: Boolean, (default `True`), whether the layer uses a bias\n        vector.\n      layernorm_epsilon: Float, (default `1e-5`), Small float added to variance\n        to avoid dividing by zero.\n      kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs. Default:\n        `glorot_uniform`.\n      recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix, used for the linear transformation of the recurrent\n        state. Default: `orthogonal`.\n      bias_initializer: Initializer for the bias vector (`use_bias=True`).\n         Default: `zeros`.\n      gamma_initializer: Initializer for the gamma vector of the layer\n         normalization layer. Default: `ones`.\n      kernel_regularizer: Regularizer function applied to the `kernel` weights\n        matrix. Default: `None`.\n      recurrent_regularizer: Regularizer function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n      bias_regularizer: Regularizer function applied to the bias vector\n         (`use_bias=True`). Default: `None`.\n      gamma_regularizer: Regularizer function applied to the gamma vector\n         of the layer normalization layer. Default: `None`.\n      kernel_constraint: Constraint function applied to the `kernel` weights\n        matrix. Default: `None`.\n      recurrent_constraint: Constraint function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n      bias_constraint: Constraint function applied to the bias vector\n         (`use_bias=True`). Default: `None`.\n      gamma_constraint: Constraint function applied to the gamma vector\n         of the layer normalization layer. Default: `None`.\n      dropout: Float between 0 and 1. Fraction of the units to drop for the\n        linear transformation of the inputs. Default: 0.\n      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n        for the linear transformation of the recurrent state. Default: 0.\n\n    Call arguments:\n      inputs: A 2D tensor, with shape of `[batch, feature]`.\n      states: A 2D tensor with shape of `[batch, units]`, which is the state\n        from the previous time step. For timestep 0, the initial state provided\n        by the user will be feed to cell.\n      training: Python boolean indicating whether the layer should behave in\n        training mode or in inference mode. Only relevant when `dropout` or\n        `recurrent_dropout` is used.\n\n    Examples:\n\n    ```python\n    import numpy as np\n    import tensorflow.keras as keras\n    import tensorflow_addons as tfa\n\n    inputs = np.random.random([32, 10, 8]).astype(np.float32)\n    rnn = keras.layers.RNN(tfa.rnn.LayerNormSimpleRNNCell(4))\n\n    output = rnn(inputs)  # The output has shape `[32, 4]`.\n\n    rnn = keras.layers.RNN(\n        tfa.rnn.LayerNormSimpleRNNCell(4),\n        return_sequences=True,\n        return_state=True)\n\n    # whole_sequence_output has shape `[32, 10, 4]`.\n    # final_state has shape `[32, 4]`.\n    whole_sequence_output, final_state = rnn(inputs)\n    ```\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        activation: Activation = ""tanh"",\n        use_bias: bool = True,\n        layernorm_epsilon: FloatTensorLike = 1e-05,\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        recurrent_initializer: Initializer = ""orthogonal"",\n        bias_initializer: Initializer = ""zeros"",\n        gamma_initializer: Initializer = ""ones"",\n        kernel_regularizer: Regularizer = None,\n        recurrent_regularizer: Regularizer = None,\n        bias_regularizer: Regularizer = None,\n        gamma_regularizer: Regularizer = None,\n        kernel_constraint: Regularizer = None,\n        recurrent_constraint: Constraint = None,\n        bias_constraint: Constraint = None,\n        gamma_constraint: Constraint = None,\n        dropout: FloatTensorLike = 0.0,\n        recurrent_dropout: FloatTensorLike = 0.0,\n        **kwargs\n    ):\n        super(LayerNormSimpleRNNCell, self).__init__(\n            units,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            recurrent_initializer=recurrent_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            recurrent_regularizer=recurrent_regularizer,\n            bias_regularizer=bias_regularizer,\n            kernel_constraint=kernel_constraint,\n            recurrent_constraint=recurrent_constraint,\n            bias_constraint=bias_constraint,\n            dropout=dropout,\n            recurrent_dropout=recurrent_dropout,\n            **kwargs,\n        )\n        self.layernorm = keras.layers.LayerNormalization(\n            axis=-1,\n            epsilon=layernorm_epsilon,\n            center=False,\n            scale=True,\n            beta_initializer=None,\n            gamma_initializer=gamma_initializer,\n            beta_regularizer=None,\n            gamma_regularizer=gamma_regularizer,\n            beta_constraint=None,\n            gamma_constraint=gamma_constraint,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        super(LayerNormSimpleRNNCell, self).build(input_shape)\n        self.layernorm.build((None, self.units))\n\n    def call(self, inputs, states, training=None):\n        """"""Formulas.\n\n        Notation:\n            y_t : Cell output at t (`output`)\n            y_{t-1} : Previous cell output at t-1 (`prev_output`)\n            x_t : The new input at t (`inputs`)\n            W_xh : Weight matrix for inputs x_t (`self.kernel`)\n            W_hh : Weights for prev. outputs y_{t-1} (`self.recurrent_kernel`)\n            b : Bias term for centering (`self.bias`)\n            d1 : Dropout function for x_t (`inputs * dp_mask`)\n            d2 : Dropout function for y_{t-1} (`prev_output * rec_dp_mask`)\n            ln : Scaling function from layer normalization (`self.layernorm`)\n            f : Activation function (`self.activation`)\n\n        Case 1:\n            Keras\' SimpleRNN. Only with bias and activation\n              y_t = f(x_t * W_xh + y_{t-1} * W_hh + b)\n            or\n              net = x_t * W_xh + y_{t-1} * W_hh\n              y_t = f(net + b)\n\n        Case 2:\n            addons\' LayerNormSimpleRNNCell. Like case 1 but with layer\n            normalization (only scaling).\n              y_t = f(ln(x_t * W_xh + y_{t-1} * W_hh) + b)\n            or\n              net = x_t * W_xh + y_{t-1} * W_hh\n              y_t = f(ln(net) + b)\n\n            Layer normalization with scaling and centering in one go (see Ba et\n            al (2016), page 3, formula 4, https://arxiv.org/abs/1607.06450)\n            is the same as layer normalization only with scaling, and\n            centering directly afterwards.\n\n        Case 3:\n            Keras\' SimpleRNN. with dropout, bias, and activation\n              y_t = f(d1(x_t) * W_xh + d2(y_{t-1}) * W_hh + b)\n            or\n              net = d1(x_t) * W_xh + d2(y_{t-1}) * W_hh\n              y_t = f(net + b)\n\n        Case 4:\n            addons\' LayerNormSimpleRNNCell. Like case 3 but with layer\n            normalization (only scaling).\n              y_t = f(ln(d1(x_t) * W_xh + d2(y_{t-1}) * W_hh) + b)\n            or\n              net = d1(x_t) * W_xh + d2(y_{t-1}) * W_hh\n              y_t = f(ln(net) + b)\n        """"""\n        prev_output = states[0]\n        dp_mask = self.get_dropout_mask_for_cell(inputs, training)\n        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(prev_output, training)\n\n        if dp_mask is not None:\n            h = keras.backend.dot(inputs * dp_mask, self.kernel)\n        else:\n            h = keras.backend.dot(inputs, self.kernel)\n\n        # don\'t add bias to ""h"" here\n        # add bias after scaling with layer normalization to ""output""\n\n        if rec_dp_mask is not None:\n            prev_output = prev_output * rec_dp_mask\n        output = h + keras.backend.dot(prev_output, self.recurrent_kernel)  # ""net""\n\n        output = self.layernorm(output)\n\n        if self.bias is not None:\n            output = keras.backend.bias_add(output, self.bias)\n\n        if self.activation is not None:\n            output = self.activation(output)\n\n        return output, [output]\n\n    # use SimpleRNNCell\'s get_initial_state method\n\n    def get_config(self):\n        cell_config = super(LayerNormSimpleRNNCell, self).get_config()\n        del cell_config[""name""]\n\n        ln_config = self.layernorm.get_config()\n        ln_config = {\n            k: v\n            for k, v in ln_config.items()\n            if k\n            in [""epsilon"", ""gamma_initializer"", ""gamma_regularizer"", ""gamma_constraint""]\n        }\n\n        ln_config[""layernorm_epsilon""] = ln_config.pop(""epsilon"")\n        return dict(list(cell_config.items()) + list(ln_config.items()))\n\n\n@tf.keras.utils.register_keras_serializable(package=""Addons"")\nclass ESNCell(keras.layers.AbstractRNNCell):\n    """"""Echo State recurrent Network (ESN) cell.\n\n    This implements the recurrent cell from the paper:\n        H. Jaeger\n        ""The ""echo state"" approach to analysing and training recurrent neural networks"".\n        GMD Report148, German National Research Center for Information Technology, 2001.\n        https://www.researchgate.net/publication/215385037\n\n    Arguments:\n        units: Positive integer, dimensionality in the reservoir.\n        connectivity: Float between 0 and 1.\n            Connection probability between two reservoir units.\n            Default: 0.1.\n        leaky: Float between 0 and 1.\n            Leaking rate of the reservoir.\n            If you pass 1, it is the special case the model does not have leaky\n            integration.\n            Default: 1.\n        spectral_radius: Float between 0 and 1.\n            Desired spectral radius of recurrent weight matrix.\n            Default: 0.9.\n        use_norm2: Boolean, whether to use the p-norm function (with p=2) as an upper\n            bound of the spectral radius so that the echo state property is satisfied.\n            It  avoids to compute the eigenvalues which has an exponential complexity.\n            Default: False.\n        use_bias: Boolean, whether the layer uses a bias vector.\n            Default: True.\n        activation: Activation function to use.\n            Default: hyperbolic tangent (`tanh`).\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            Default: `glorot_uniform`.\n        recurrent_initializer: Initializer for the `recurrent_kernel` weights matrix,\n            used for the linear transformation of the recurrent state.\n            Default: `glorot_uniform`.\n        bias_initializer: Initializer for the bias vector.\n            Default: `zeros`.\n    Call arguments:\n        inputs: A 2D tensor (batch x num_units).\n        states: List of state tensors corresponding to the previous timestep.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: int,\n        connectivity: float = 0.1,\n        leaky: float = 1,\n        spectral_radius: float = 0.9,\n        use_norm2: bool = False,\n        use_bias: bool = True,\n        activation: Activation = ""tanh"",\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        recurrent_initializer: Initializer = ""glorot_uniform"",\n        bias_initializer: Initializer = ""zeros"",\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.units = units\n        self.connectivity = connectivity\n        self.leaky = leaky\n        self.spectral_radius = spectral_radius\n        self.use_norm2 = use_norm2\n        self.use_bias = use_bias\n        self.activation = tf.keras.activations.get(activation)\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.recurrent_initializer = tf.keras.initializers.get(recurrent_initializer)\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n\n        self._state_size = units\n        self._output_size = units\n\n    @property\n    def state_size(self):\n        return self._state_size\n\n    @property\n    def output_size(self):\n        return self._output_size\n\n    def build(self, inputs_shape):\n        input_size = tf.compat.dimension_value(tf.TensorShape(inputs_shape)[-1])\n        if input_size is None:\n            raise ValueError(\n                ""Could not infer input size from inputs.get_shape()[-1]. Shape received is %s""\n                % inputs_shape\n            )\n\n        def _esn_recurrent_initializer(shape, dtype, partition_info=None):\n            recurrent_weights = tf.keras.initializers.get(self.recurrent_initializer)(\n                shape, dtype\n            )\n\n            connectivity_mask = tf.cast(\n                tf.math.less_equal(tf.random.uniform(shape), self.connectivity,), dtype\n            )\n            recurrent_weights = tf.math.multiply(recurrent_weights, connectivity_mask)\n\n            # Satisfy the necessary condition for the echo state property `max(eig(W)) < 1`\n            if self.use_norm2:\n                # This condition is approximated scaling the norm 2 of the reservoir matrix\n                # which is an upper bound of the spectral radius.\n                recurrent_norm2 = tf.math.sqrt(\n                    tf.math.reduce_sum(tf.math.square(recurrent_weights))\n                )\n                is_norm2_0 = tf.cast(tf.math.equal(recurrent_norm2, 0), dtype)\n                scaling_factor = self.spectral_radius / (\n                    recurrent_norm2 + 1 * is_norm2_0\n                )\n            else:\n                abs_eig_values = tf.abs(tf.linalg.eig(recurrent_weights)[0])\n                scaling_factor = tf.math.divide_no_nan(\n                    self.spectral_radius, tf.reduce_max(abs_eig_values)\n                )\n\n            recurrent_weights = tf.multiply(recurrent_weights, scaling_factor)\n\n            return recurrent_weights\n\n        self.recurrent_kernel = self.add_weight(\n            name=""recurrent_kernel"",\n            shape=[self.units, self.units],\n            initializer=_esn_recurrent_initializer,\n            trainable=False,\n            dtype=self.dtype,\n        )\n        self.kernel = self.add_weight(\n            name=""kernel"",\n            shape=[input_size, self.units],\n            initializer=self.kernel_initializer,\n            trainable=False,\n            dtype=self.dtype,\n        )\n\n        if self.use_bias:\n            self.bias = self.add_weight(\n                name=""bias"",\n                shape=[self.units],\n                initializer=self.bias_initializer,\n                trainable=False,\n                dtype=self.dtype,\n            )\n\n        self.built = True\n\n    def call(self, inputs, state):\n        in_matrix = tf.concat([inputs, state[0]], axis=1)\n        weights_matrix = tf.concat([self.kernel, self.recurrent_kernel], axis=0)\n\n        output = tf.linalg.matmul(in_matrix, weights_matrix)\n        if self.use_bias:\n            output = output + self.bias\n        output = self.activation(output)\n        output = (1 - self.leaky) * state[0] + self.leaky * output\n\n        return output, output\n\n    def get_config(self):\n        config = {\n            ""units"": self.units,\n            ""connectivity"": self.connectivity,\n            ""leaky"": self.leaky,\n            ""spectral_radius"": self.spectral_radius,\n            ""use_norm2"": self.use_norm2,\n            ""use_bias"": self.use_bias,\n            ""activation"": tf.keras.activations.serialize(self.activation),\n            ""kernel_initializer"": tf.keras.initializers.serialize(\n                self.kernel_initializer\n            ),\n            ""recurrent_initializer"": tf.keras.initializers.serialize(\n                self.recurrent_initializer\n            ),\n            ""bias_initializer"": tf.keras.initializers.serialize(self.bias_initializer),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n'"
tensorflow_addons/seq2seq/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional ops for building neural network sequence to sequence decoders and\nlosses.""""""\n\nfrom tensorflow_addons.seq2seq.attention_wrapper import AttentionMechanism\nfrom tensorflow_addons.seq2seq.attention_wrapper import AttentionWrapper\nfrom tensorflow_addons.seq2seq.attention_wrapper import AttentionWrapperState\nfrom tensorflow_addons.seq2seq.attention_wrapper import BahdanauAttention\nfrom tensorflow_addons.seq2seq.attention_wrapper import BahdanauMonotonicAttention\nfrom tensorflow_addons.seq2seq.attention_wrapper import LuongAttention\nfrom tensorflow_addons.seq2seq.attention_wrapper import LuongMonotonicAttention\nfrom tensorflow_addons.seq2seq.attention_wrapper import hardmax\nfrom tensorflow_addons.seq2seq.attention_wrapper import monotonic_attention\nfrom tensorflow_addons.seq2seq.attention_wrapper import safe_cumprod\n\nfrom tensorflow_addons.seq2seq.basic_decoder import BasicDecoder\nfrom tensorflow_addons.seq2seq.basic_decoder import BasicDecoderOutput\n\nfrom tensorflow_addons.seq2seq.beam_search_decoder import BeamSearchDecoder\nfrom tensorflow_addons.seq2seq.beam_search_decoder import BeamSearchDecoderOutput\nfrom tensorflow_addons.seq2seq.beam_search_decoder import BeamSearchDecoderState\nfrom tensorflow_addons.seq2seq.beam_search_decoder import FinalBeamSearchDecoderOutput\nfrom tensorflow_addons.seq2seq.beam_search_decoder import gather_tree\nfrom tensorflow_addons.seq2seq.beam_search_decoder import gather_tree_from_array\nfrom tensorflow_addons.seq2seq.beam_search_decoder import tile_batch\n\nfrom tensorflow_addons.seq2seq.decoder import BaseDecoder\nfrom tensorflow_addons.seq2seq.decoder import Decoder\nfrom tensorflow_addons.seq2seq.decoder import dynamic_decode\n\nfrom tensorflow_addons.seq2seq.loss import SequenceLoss\nfrom tensorflow_addons.seq2seq.loss import sequence_loss\n\nfrom tensorflow_addons.seq2seq.sampler import CustomSampler\nfrom tensorflow_addons.seq2seq.sampler import GreedyEmbeddingSampler\nfrom tensorflow_addons.seq2seq.sampler import InferenceSampler\nfrom tensorflow_addons.seq2seq.sampler import SampleEmbeddingSampler\nfrom tensorflow_addons.seq2seq.sampler import Sampler\nfrom tensorflow_addons.seq2seq.sampler import ScheduledEmbeddingTrainingSampler\nfrom tensorflow_addons.seq2seq.sampler import ScheduledOutputTrainingSampler\nfrom tensorflow_addons.seq2seq.sampler import TrainingSampler\n'"
tensorflow_addons/seq2seq/attention_wrapper.py,137,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A powerful dynamic attention wrapper object.""""""\n\nimport collections\nimport functools\nimport math\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import keras_utils\nfrom tensorflow_addons.utils.types import (\n    AcceptableDTypes,\n    FloatTensorLike,\n    TensorLike,\n    Initializer,\n    Number,\n)\n\nfrom typeguard import typechecked\nfrom typing import Optional, Callable, Union, List\n\n# TODO: Find public API alternatives to these\nfrom tensorflow.python.keras.engine import base_layer_utils\n\n\nclass AttentionMechanism:\n    @property\n    def alignments_size(self):\n        raise NotImplementedError\n\n    @property\n    def state_size(self):\n        raise NotImplementedError\n\n\nclass _BaseAttentionMechanism(AttentionMechanism, tf.keras.layers.Layer):\n    """"""A base AttentionMechanism class providing common functionality.\n\n    Common functionality includes:\n      1. Storing the query and memory layers.\n      2. Preprocessing and storing the memory.\n\n    Note that this layer takes memory as its init parameter, which is an\n    anti-pattern of Keras API, we have to keep the memory as init parameter for\n    performance and dependency reason. Under the hood, during `__init__()`, it\n    will invoke `base_layer.__call__(memory, setup_memory=True)`. This will let\n    keras to keep track of the memory tensor as the input of this layer. Once\n    the `__init__()` is done, then user can query the attention by\n    `score = att_obj([query, state])`, and use it as a normal keras layer.\n\n    Special attention is needed when adding using this class as the base layer\n    for new attention:\n      1. Build() could be invoked at least twice. So please make sure weights\n         are not duplicated.\n      2. Layer.get_weights() might return different set of weights if the\n         instance has `query_layer`. The query_layer weights is not initialized\n         until the memory is configured.\n\n    Also note that this layer does not work with Keras model when\n    `model.compile(run_eagerly=True)` due to the fact that this layer is\n    stateful. The support for that will be added in a future version.\n    """"""\n\n    def __init__(\n        self,\n        memory,\n        probability_fn,\n        query_layer=None,\n        memory_layer=None,\n        memory_sequence_length=None,\n        **kwargs\n    ):\n        """"""Construct base AttentionMechanism class.\n\n        Args:\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          probability_fn: A `callable`. Converts the score and previous\n            alignments to probabilities. Its signature should be:\n            `probabilities = probability_fn(score, state)`.\n          query_layer:  (optional): Instance of `tf.keras.Layer`.  The layer\'s\n            depth must match the depth of `memory_layer`.  If `query_layer` is\n            not provided, the shape of `query` must match that of\n            `memory_layer`.\n          memory_layer: (optional): Instance of `tf.keras.Layer`. The layer\'s\n            depth must match the depth of `query_layer`.\n            If `memory_layer` is not provided, the shape of `memory` must match\n            that of `query_layer`.\n          memory_sequence_length (optional): Sequence lengths for the batch\n            entries in memory. If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          **kwargs: Dictionary that contains other common arguments for layer\n            creation.\n        """"""\n        if query_layer is not None and not isinstance(\n            query_layer, tf.keras.layers.Layer\n        ):\n            raise TypeError(\n                ""query_layer is not a Layer: %s"" % type(query_layer).__name__\n            )\n        if memory_layer is not None and not isinstance(\n            memory_layer, tf.keras.layers.Layer\n        ):\n            raise TypeError(\n                ""memory_layer is not a Layer: %s"" % type(memory_layer).__name__\n            )\n        self.query_layer = query_layer\n        self.memory_layer = memory_layer\n        if self.memory_layer is not None and ""dtype"" not in kwargs:\n            kwargs[""dtype""] = self.memory_layer.dtype\n        super().__init__(**kwargs)\n        if not callable(probability_fn):\n            raise TypeError(\n                ""probability_fn must be callable, saw type: %s""\n                % type(probability_fn).__name__\n            )\n        self.default_probability_fn = probability_fn\n        self.probability_fn = probability_fn\n\n        self.keys = None\n        self.values = None\n        self.batch_size = None\n        self._memory_initialized = False\n        self._check_inner_dims_defined = True\n        self.supports_masking = True\n\n        if memory is not None:\n            # Setup the memory by self.__call__() with memory and\n            # memory_seq_length. This will make the attention follow the keras\n            # convention which takes all the tensor inputs via __call__().\n            if memory_sequence_length is None:\n                inputs = memory\n            else:\n                inputs = [memory, memory_sequence_length]\n\n            self.values = super().__call__(inputs, setup_memory=True)\n\n    @property\n    def memory_initialized(self):\n        """"""Returns `True` if this attention mechanism has been initialized with\n        a memory.""""""\n        return self._memory_initialized\n\n    def build(self, input_shape):\n        if not self._memory_initialized:\n            # This is for setting up the memory, which contains memory and\n            # optional memory_sequence_length. Build the memory_layer with\n            # memory shape.\n            if self.memory_layer is not None and not self.memory_layer.built:\n                if isinstance(input_shape, list):\n                    self.memory_layer.build(input_shape[0])\n                else:\n                    self.memory_layer.build(input_shape)\n        else:\n            # The input_shape should be query.shape and state.shape. Use the\n            # query to init the query layer.\n            if self.query_layer is not None and not self.query_layer.built:\n                self.query_layer.build(input_shape[0])\n\n    def __call__(self, inputs, **kwargs):\n        """"""Preprocess the inputs before calling `base_layer.__call__()`.\n\n        Note that there are situation here, one for setup memory, and one with\n        actual query and state.\n        1. When the memory has not been configured, we just pass all the param\n           to base_layer.__call__(), which will then invoke self.call() with\n           proper inputs, which allows this class to setup memory.\n        2. When the memory has already been setup, the input should contain\n           query and state, and optionally processed memory. If the processed\n           memory is not included in the input, we will have to append it to\n           the inputs and give it to the base_layer.__call__(). The processed\n           memory is the output of first invocation of self.__call__(). If we\n           don\'t add it here, then from keras perspective, the graph is\n           disconnected since the output from previous call is never used.\n\n        Args:\n          inputs: the inputs tensors.\n          **kwargs: dict, other keyeword arguments for the `__call__()`\n        """"""\n        # Allow manual memory reset\n        if kwargs.get(""setup_memory"", False):\n            self._memory_initialized = False\n\n        if self._memory_initialized:\n            if len(inputs) not in (2, 3):\n                raise ValueError(\n                    ""Expect the inputs to have 2 or 3 tensors, got %d"" % len(inputs)\n                )\n            if len(inputs) == 2:\n                # We append the calculated memory here so that the graph will be\n                # connected.\n                inputs.append(self.values)\n\n        return super().__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, setup_memory=False, **kwargs):\n        """"""Setup the memory or query the attention.\n\n        There are two case here, one for setup memory, and the second is query\n        the attention score. `setup_memory` is the flag to indicate which mode\n        it is. The input list will be treated differently based on that flag.\n\n        Args:\n          inputs: a list of tensor that could either be `query` and `state`, or\n            `memory` and `memory_sequence_length`.\n            `query` is the tensor of dtype matching `memory` and shape\n            `[batch_size, query_depth]`.\n            `state` is the tensor of dtype matching `memory` and shape\n            `[batch_size, alignments_size]`. (`alignments_size` is memory\'s\n            `max_time`).\n            `memory` is the memory to query; usually the output of an RNN\n            encoder. The tensor should be shaped `[batch_size, max_time, ...]`.\n            `memory_sequence_length` (optional) is the sequence lengths for the\n             batch entries in memory. If provided, the memory tensor rows are\n            masked with zeros for values past the respective sequence lengths.\n          mask: optional bool tensor with shape `[batch, max_time]` for the\n            mask of memory. If it is not None, the corresponding item of the\n            memory should be filtered out during calculation.\n          setup_memory: boolean, whether the input is for setting up memory, or\n            query attention.\n          **kwargs: Dict, other keyword arguments for the call method.\n        Returns:\n          Either processed memory or attention score, based on `setup_memory`.\n        """"""\n        if setup_memory:\n            if isinstance(inputs, list):\n                if len(inputs) not in (1, 2):\n                    raise ValueError(\n                        ""Expect inputs to have 1 or 2 tensors, got %d"" % len(inputs)\n                    )\n                memory = inputs[0]\n                memory_sequence_length = inputs[1] if len(inputs) == 2 else None\n                memory_mask = mask\n            else:\n                memory, memory_sequence_length = inputs, None\n                memory_mask = mask\n            self.setup_memory(memory, memory_sequence_length, memory_mask)\n            # We force the self.built to false here since only memory is,\n            # initialized but the real query/state has not been call() yet. The\n            # layer should be build and call again.\n            self.built = False\n            # Return the processed memory in order to create the Keras\n            # connectivity data for it.\n            return self.values\n        else:\n            if not self._memory_initialized:\n                raise ValueError(\n                    ""Cannot query the attention before the setup of "" ""memory""\n                )\n            if len(inputs) not in (2, 3):\n                raise ValueError(\n                    ""Expect the inputs to have query, state, and optional ""\n                    ""processed memory, got %d items"" % len(inputs)\n                )\n            # Ignore the rest of the inputs and only care about the query and\n            # state\n            query, state = inputs[0], inputs[1]\n            return self._calculate_attention(query, state)\n\n    def setup_memory(self, memory, memory_sequence_length=None, memory_mask=None):\n        """"""Pre-process the memory before actually query the memory.\n\n        This should only be called once at the first invocation of call().\n\n        Args:\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          memory_sequence_length (optional): Sequence lengths for the batch\n            entries in memory. If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          memory_mask: (Optional) The boolean tensor with shape `[batch_size,\n            max_time]`. For any value equal to False, the corresponding value\n            in memory should be ignored.\n        """"""\n        if memory_sequence_length is not None and memory_mask is not None:\n            raise ValueError(\n                ""memory_sequence_length and memory_mask cannot be ""\n                ""used at same time for attention.""\n            )\n        with tf.name_scope(self.name or ""BaseAttentionMechanismInit""):\n            self.values = _prepare_memory(\n                memory,\n                memory_sequence_length=memory_sequence_length,\n                memory_mask=memory_mask,\n                check_inner_dims_defined=self._check_inner_dims_defined,\n            )\n            # Mark the value as check since the memory and memory mask might not\n            # passed from __call__(), which does not have proper keras metadata.\n            # TODO(omalleyt12): Remove this hack once the mask the has proper\n            # keras history.\n            base_layer_utils.mark_checked(self.values)\n            if self.memory_layer is not None:\n                self.keys = self.memory_layer(self.values)\n            else:\n                self.keys = self.values\n            self.batch_size = self.keys.shape[0] or tf.shape(self.keys)[0]\n            self._alignments_size = self.keys.shape[1] or tf.shape(self.keys)[1]\n            if memory_mask is not None or memory_sequence_length is not None:\n                unwrapped_probability_fn = self.default_probability_fn\n\n                def _mask_probability_fn(score, prev):\n                    return unwrapped_probability_fn(\n                        _maybe_mask_score(\n                            score,\n                            memory_mask=memory_mask,\n                            memory_sequence_length=memory_sequence_length,\n                            score_mask_value=score.dtype.min,\n                        ),\n                        prev,\n                    )\n\n                self.probability_fn = _mask_probability_fn\n        self._memory_initialized = True\n\n    def _calculate_attention(self, query, state):\n        raise NotImplementedError(\n            ""_calculate_attention need to be implemented by subclasses.""\n        )\n\n    def compute_mask(self, inputs, mask=None):\n        # There real input of the attention is query and state, and the memory\n        # layer mask shouldn\'t be pass down. Returning None for all output mask\n        # here.\n        return None, None\n\n    def get_config(self):\n        config = {}\n        # Since the probability_fn is likely to be a wrapped function, the child\n        # class should preserve the original function and how its wrapped.\n\n        if self.query_layer is not None:\n            config[""query_layer""] = {\n                ""class_name"": self.query_layer.__class__.__name__,\n                ""config"": self.query_layer.get_config(),\n            }\n        if self.memory_layer is not None:\n            config[""memory_layer""] = {\n                ""class_name"": self.memory_layer.__class__.__name__,\n                ""config"": self.memory_layer.get_config(),\n            }\n        # memory is a required init parameter and its a tensor. It cannot be\n        # serialized to config, so we put a placeholder for it.\n        config[""memory""] = None\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def _process_probability_fn(self, func_name):\n        """"""Helper method to retrieve the probably function by string input.""""""\n        valid_probability_fns = {\n            ""softmax"": tf.nn.softmax,\n            ""hardmax"": hardmax,\n        }\n        if func_name not in valid_probability_fns.keys():\n            raise ValueError(\n                ""Invalid probability function: %s, options are %s""\n                % (func_name, valid_probability_fns.keys())\n            )\n        return valid_probability_fns[func_name]\n\n    @classmethod\n    def deserialize_inner_layer_from_config(cls, config, custom_objects):\n        """"""Helper method that reconstruct the query and memory from the config.\n\n        In the get_config() method, the query and memory layer configs are\n        serialized into dict for persistence, this method perform the reverse\n        action to reconstruct the layer from the config.\n\n        Args:\n          config: dict, the configs that will be used to reconstruct the\n            object.\n          custom_objects: dict mapping class names (or function names) of\n            custom (non-Keras) objects to class/functions.\n        Returns:\n          config: dict, the config with layer instance created, which is ready\n            to be used as init parameters.\n        """"""\n        # Reconstruct the query and memory layer for parent class.\n        # Instead of updating the input, create a copy and use that.\n        config = config.copy()\n        query_layer_config = config.pop(""query_layer"", None)\n        if query_layer_config:\n            query_layer = tf.keras.layers.deserialize(\n                query_layer_config, custom_objects=custom_objects\n            )\n            config[""query_layer""] = query_layer\n        memory_layer_config = config.pop(""memory_layer"", None)\n        if memory_layer_config:\n            memory_layer = tf.keras.layers.deserialize(\n                memory_layer_config, custom_objects=custom_objects\n            )\n            config[""memory_layer""] = memory_layer\n        return config\n\n    @property\n    def alignments_size(self):\n        if isinstance(self._alignments_size, int):\n            return self._alignments_size\n        else:\n            return tf.TensorShape([None])\n\n    @property\n    def state_size(self):\n        return self.alignments_size\n\n    def initial_alignments(self, batch_size, dtype):\n        """"""Creates the initial alignment values for the `AttentionWrapper`\n        class.\n\n        This is important for AttentionMechanisms that use the previous\n        alignment to calculate the alignment at the next time step\n        (e.g. monotonic attention).\n\n        The default behavior is to return a tensor of all zeros.\n\n        Args:\n          batch_size: `int32` scalar, the batch_size.\n          dtype: The `dtype`.\n\n        Returns:\n          A `dtype` tensor shaped `[batch_size, alignments_size]`\n          (`alignments_size` is the values\' `max_time`).\n        """"""\n        return tf.zeros([batch_size, self._alignments_size], dtype=dtype)\n\n    def initial_state(self, batch_size, dtype):\n        """"""Creates the initial state values for the `AttentionWrapper` class.\n\n        This is important for AttentionMechanisms that use the previous\n        alignment to calculate the alignment at the next time step\n        (e.g. monotonic attention).\n\n        The default behavior is to return the same output as\n        initial_alignments.\n\n        Args:\n          batch_size: `int32` scalar, the batch_size.\n          dtype: The `dtype`.\n\n        Returns:\n          A structure of all-zero tensors with shapes as described by\n          `state_size`.\n        """"""\n        return self.initial_alignments(batch_size, dtype)\n\n\ndef _luong_score(query, keys, scale):\n    """"""Implements Luong-style (multiplicative) scoring function.\n\n    This attention has two forms.  The first is standard Luong attention,\n    as described in:\n\n    Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n    ""Effective Approaches to Attention-based Neural Machine Translation.""\n    EMNLP 2015.  https://arxiv.org/abs/1508.04025\n\n    The second is the scaled form inspired partly by the normalized form of\n    Bahdanau attention.\n\n    To enable the second form, call this function with `scale=True`.\n\n    Args:\n      query: Tensor, shape `[batch_size, num_units]` to compare to keys.\n      keys: Processed memory, shape `[batch_size, max_time, num_units]`.\n      scale: the optional tensor to scale the attention score.\n\n    Returns:\n      A `[batch_size, max_time]` tensor of unnormalized score values.\n\n    Raises:\n      ValueError: If `key` and `query` depths do not match.\n    """"""\n    depth = query.shape[-1]\n    key_units = keys.shape[-1]\n    if depth != key_units:\n        raise ValueError(\n            ""Incompatible or unknown inner dimensions between query and keys. ""\n            ""Query (%s) has units: %s.  Keys (%s) have units: %s.  ""\n            ""Perhaps you need to set num_units to the keys\' dimension (%s)?""\n            % (query, depth, keys, key_units, key_units)\n        )\n\n    # Reshape from [batch_size, depth] to [batch_size, 1, depth]\n    # for matmul.\n    query = tf.expand_dims(query, 1)\n\n    # Inner product along the query units dimension.\n    # matmul shapes: query is [batch_size, 1, depth] and\n    #                keys is [batch_size, max_time, depth].\n    # the inner product is asked to **transpose keys\' inner shape** to get a\n    # batched matmul on:\n    #   [batch_size, 1, depth] . [batch_size, depth, max_time]\n    # resulting in an output shape of:\n    #   [batch_size, 1, max_time].\n    # we then squeeze out the center singleton dimension.\n    score = tf.matmul(query, keys, transpose_b=True)\n    score = tf.squeeze(score, [1])\n\n    if scale is not None:\n        score = scale * score\n    return score\n\n\nclass LuongAttention(_BaseAttentionMechanism):\n    """"""Implements Luong-style (multiplicative) attention scoring.\n\n    This attention has two forms.  The first is standard Luong attention,\n    as described in:\n\n    Minh-Thang Luong, Hieu Pham, Christopher D. Manning.\n    [Effective Approaches to Attention-based Neural Machine Translation.\n    EMNLP 2015.](https://arxiv.org/abs/1508.04025)\n\n    The second is the scaled form inspired partly by the normalized form of\n    Bahdanau attention.\n\n    To enable the second form, construct the object with parameter\n    `scale=True`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        memory: Optional[TensorLike] = None,\n        memory_sequence_length: Optional[TensorLike] = None,\n        scale: bool = False,\n        probability_fn: str = ""softmax"",\n        dtype: AcceptableDTypes = None,\n        name: str = ""LuongAttention"",\n        **kwargs\n    ):\n        """"""Construct the AttentionMechanism mechanism.\n\n        Args:\n          units: The depth of the attention mechanism.\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          memory_sequence_length: (optional): Sequence lengths for the batch\n            entries in memory.  If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          scale: Python boolean. Whether to scale the energy term.\n          probability_fn: (optional) string, the name of function to convert\n            the attention score to probabilities. The default is `softmax`\n            which is `tf.nn.softmax`. Other options is `hardmax`, which is\n            hardmax() within this module. Any other value will result\n            intovalidation error. Default to use `softmax`.\n          dtype: The data type for the memory layer of the attention mechanism.\n          name: Name to use when creating ops.\n          **kwargs: Dictionary that contains other common arguments for layer\n            creation.\n        """"""\n        # For LuongAttention, we only transform the memory layer; thus\n        # num_units **must** match expected the query depth.\n        self.probability_fn_name = probability_fn\n        probability_fn = self._process_probability_fn(self.probability_fn_name)\n\n        def wrapped_probability_fn(score, _):\n            return probability_fn(score)\n\n        if dtype is None:\n            dtype = tf.float32\n        memory_layer = kwargs.pop(""memory_layer"", None)\n        if not memory_layer:\n            memory_layer = tf.keras.layers.Dense(\n                units, name=""memory_layer"", use_bias=False, dtype=dtype\n            )\n        self.units = units\n        self.scale = scale\n        self.scale_weight = None\n        super().__init__(\n            memory=memory,\n            memory_sequence_length=memory_sequence_length,\n            query_layer=None,\n            memory_layer=memory_layer,\n            probability_fn=wrapped_probability_fn,\n            name=name,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        if self.scale and self.scale_weight is None:\n            self.scale_weight = self.add_weight(\n                ""attention_g"", initializer=tf.ones_initializer, shape=()\n            )\n        self.built = True\n\n    def _calculate_attention(self, query, state):\n        """"""Score the query based on the keys and values.\n\n        Args:\n          query: Tensor of dtype matching `self.values` and shape\n            `[batch_size, query_depth]`.\n          state: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]`\n            (`alignments_size` is memory\'s `max_time`).\n\n        Returns:\n          alignments: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n            `max_time`).\n          next_state: Same as the alignments.\n        """"""\n        score = _luong_score(query, self.keys, self.scale_weight)\n        alignments = self.probability_fn(score, state)\n        next_state = alignments\n        return alignments, next_state\n\n    def get_config(self):\n        config = {\n            ""units"": self.units,\n            ""scale"": self.scale,\n            ""probability_fn"": self.probability_fn_name,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        config = _BaseAttentionMechanism.deserialize_inner_layer_from_config(\n            config, custom_objects=custom_objects\n        )\n        return cls(**config)\n\n\ndef _bahdanau_score(\n    processed_query, keys, attention_v, attention_g=None, attention_b=None\n):\n    """"""Implements Bahdanau-style (additive) scoring function.\n\n    This attention has two forms.  The first is Bhandanau attention,\n    as described in:\n\n    Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n    ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n    ICLR 2015. https://arxiv.org/abs/1409.0473\n\n    The second is the normalized form.  This form is inspired by the\n    weight normalization article:\n\n    Tim Salimans, Diederik P. Kingma.\n    ""Weight Normalization: A Simple Reparameterization to Accelerate\n     Training of Deep Neural Networks.""\n    https://arxiv.org/abs/1602.07868\n\n    To enable the second form, set please pass in attention_g and attention_b.\n\n    Args:\n      processed_query: Tensor, shape `[batch_size, num_units]` to compare to\n        keys.\n      keys: Processed memory, shape `[batch_size, max_time, num_units]`.\n      attention_v: Tensor, shape `[num_units]`.\n      attention_g: Optional scalar tensor for normalization.\n      attention_b: Optional tensor with shape `[num_units]` for normalization.\n\n    Returns:\n      A `[batch_size, max_time]` tensor of unnormalized score values.\n    """"""\n    # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.\n    processed_query = tf.expand_dims(processed_query, 1)\n    if attention_g is not None and attention_b is not None:\n        normed_v = (\n            attention_g\n            * attention_v\n            * tf.math.rsqrt(tf.reduce_sum(tf.square(attention_v)))\n        )\n        return tf.reduce_sum(\n            normed_v * tf.tanh(keys + processed_query + attention_b), [2]\n        )\n    else:\n        return tf.reduce_sum(attention_v * tf.tanh(keys + processed_query), [2])\n\n\nclass BahdanauAttention(_BaseAttentionMechanism):\n    """"""Implements Bahdanau-style (additive) attention.\n\n    This attention has two forms.  The first is Bahdanau attention,\n    as described in:\n\n    Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.\n    ""Neural Machine Translation by Jointly Learning to Align and Translate.""\n    ICLR 2015. https://arxiv.org/abs/1409.0473\n\n    The second is the normalized form.  This form is inspired by the\n    weight normalization article:\n\n    Tim Salimans, Diederik P. Kingma.\n    ""Weight Normalization: A Simple Reparameterization to Accelerate\n     Training of Deep Neural Networks.""\n    https://arxiv.org/abs/1602.07868\n\n    To enable the second form, construct the object with parameter\n    `normalize=True`.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        memory: Optional[TensorLike] = None,\n        memory_sequence_length: Optional[TensorLike] = None,\n        normalize: bool = False,\n        probability_fn: str = ""softmax"",\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        dtype: AcceptableDTypes = None,\n        name: str = ""BahdanauAttention"",\n        **kwargs\n    ):\n        """"""Construct the Attention mechanism.\n\n        Args:\n          units: The depth of the query mechanism.\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          memory_sequence_length: (optional): Sequence lengths for the batch\n            entries in memory.  If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          normalize: Python boolean.  Whether to normalize the energy term.\n          probability_fn: (optional) string, the name of function to convert\n            the attention score to probabilities. The default is `softmax`\n            which is `tf.nn.softmax`. Other options is `hardmax`, which is\n            hardmax() within this module. Any other value will result into\n            validation error. Default to use `softmax`.\n          kernel_initializer: (optional), the name of the initializer for the\n            attention kernel.\n          dtype: The data type for the query and memory layers of the attention\n            mechanism.\n          name: Name to use when creating ops.\n          **kwargs: Dictionary that contains other common arguments for layer\n            creation.\n        """"""\n        self.probability_fn_name = probability_fn\n        probability_fn = self._process_probability_fn(self.probability_fn_name)\n\n        def wrapped_probability_fn(score, _):\n            return probability_fn(score)\n\n        if dtype is None:\n            dtype = tf.float32\n        query_layer = kwargs.pop(""query_layer"", None)\n        if not query_layer:\n            query_layer = tf.keras.layers.Dense(\n                units, name=""query_layer"", use_bias=False, dtype=dtype\n            )\n        memory_layer = kwargs.pop(""memory_layer"", None)\n        if not memory_layer:\n            memory_layer = tf.keras.layers.Dense(\n                units, name=""memory_layer"", use_bias=False, dtype=dtype\n            )\n        self.units = units\n        self.normalize = normalize\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.attention_v = None\n        self.attention_g = None\n        self.attention_b = None\n        super().__init__(\n            memory=memory,\n            memory_sequence_length=memory_sequence_length,\n            query_layer=query_layer,\n            memory_layer=memory_layer,\n            probability_fn=wrapped_probability_fn,\n            name=name,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        if self.attention_v is None:\n            self.attention_v = self.add_weight(\n                ""attention_v"",\n                [self.units],\n                dtype=self.dtype,\n                initializer=self.kernel_initializer,\n            )\n        if self.normalize and self.attention_g is None and self.attention_b is None:\n            self.attention_g = self.add_weight(\n                ""attention_g"",\n                initializer=tf.constant_initializer(math.sqrt(1.0 / self.units)),\n                shape=(),\n            )\n            self.attention_b = self.add_weight(\n                ""attention_b"", shape=[self.units], initializer=tf.zeros_initializer()\n            )\n        self.built = True\n\n    def _calculate_attention(self, query, state):\n        """"""Score the query based on the keys and values.\n\n        Args:\n          query: Tensor of dtype matching `self.values` and shape\n            `[batch_size, query_depth]`.\n          state: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]`\n            (`alignments_size` is memory\'s `max_time`).\n\n        Returns:\n          alignments: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n            `max_time`).\n          next_state: same as alignments.\n        """"""\n        processed_query = self.query_layer(query) if self.query_layer else query\n        score = _bahdanau_score(\n            processed_query,\n            self.keys,\n            self.attention_v,\n            attention_g=self.attention_g,\n            attention_b=self.attention_b,\n        )\n        alignments = self.probability_fn(score, state)\n        next_state = alignments\n        return alignments, next_state\n\n    def get_config(self):\n        # yapf: disable\n        config = {\n            ""units"": self.units,\n            ""normalize"": self.normalize,\n            ""probability_fn"": self.probability_fn_name,\n            ""kernel_initializer"": tf.keras.initializers.serialize(\n                self.kernel_initializer)\n        }\n        # yapf: enable\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        config = _BaseAttentionMechanism.deserialize_inner_layer_from_config(\n            config, custom_objects=custom_objects\n        )\n        return cls(**config)\n\n\ndef safe_cumprod(x: TensorLike, *args, **kwargs) -> tf.Tensor:\n    """"""Computes cumprod of x in logspace using cumsum to avoid underflow.\n\n    The cumprod function and its gradient can result in numerical instabilities\n    when its argument has very small and/or zero values.  As long as the\n    argument is all positive, we can instead compute the cumulative product as\n    exp(cumsum(log(x))).  This function can be called identically to\n    tf.cumprod.\n\n    Args:\n      x: Tensor to take the cumulative product of.\n      *args: Passed on to cumsum; these are identical to those in cumprod.\n      **kwargs: Passed on to cumsum; these are identical to those in cumprod.\n    Returns:\n      Cumulative product of x.\n    """"""\n    with tf.name_scope(""SafeCumprod""):\n        x = tf.convert_to_tensor(x, name=""x"")\n        tiny = np.finfo(x.dtype.as_numpy_dtype).tiny\n        return tf.exp(\n            tf.cumsum(tf.math.log(tf.clip_by_value(x, tiny, 1)), *args, **kwargs)\n        )\n\n\ndef monotonic_attention(\n    p_choose_i: FloatTensorLike, previous_attention: FloatTensorLike, mode: str\n) -> tf.Tensor:\n    """"""Compute monotonic attention distribution from choosing probabilities.\n\n    Monotonic attention implies that the input sequence is processed in an\n    explicitly left-to-right manner when generating the output sequence.  In\n    addition, once an input sequence element is attended to at a given output\n    timestep, elements occurring before it cannot be attended to at subsequent\n    output timesteps.  This function generates attention distributions\n    according to these assumptions.  For more information, see `Online and\n    Linear-Time Attention by Enforcing Monotonic Alignments`.\n\n    Args:\n      p_choose_i: Probability of choosing input sequence/memory element i.\n        Should be of shape (batch_size, input_sequence_length), and should all\n        be in the range [0, 1].\n      previous_attention: The attention distribution from the previous output\n        timestep.  Should be of shape (batch_size, input_sequence_length).  For\n        the first output timestep, preevious_attention[n] should be\n        [1, 0, 0, ..., 0] for all n in [0, ... batch_size - 1].\n      mode: How to compute the attention distribution.  Must be one of\n        \'recursive\', \'parallel\', or \'hard\'.\n          * \'recursive\' uses tf.scan to recursively compute the distribution.\n            This is slowest but is exact, general, and does not suffer from\n            numerical instabilities.\n          * \'parallel\' uses parallelized cumulative-sum and cumulative-product\n            operations to compute a closed-form solution to the recurrence\n            relation defining the attention distribution.  This makes it more\n            efficient than \'recursive\', but it requires numerical checks which\n            make the distribution non-exact.  This can be a problem in\n            particular when input_sequence_length is long and/or p_choose_i has\n            entries very close to 0 or 1.\n          * \'hard\' requires that the probabilities in p_choose_i are all either\n            0 or 1, and subsequently uses a more efficient and exact solution.\n\n    Returns:\n      A tensor of shape (batch_size, input_sequence_length) representing the\n      attention distributions for each sequence in the batch.\n\n    Raises:\n      ValueError: mode is not one of \'recursive\', \'parallel\', \'hard\'.\n    """"""\n    # Force things to be tensors\n    p_choose_i = tf.convert_to_tensor(p_choose_i, name=""p_choose_i"")\n    previous_attention = tf.convert_to_tensor(\n        previous_attention, name=""previous_attention""\n    )\n    if mode == ""recursive"":\n        # Use .shape[0] when it\'s not None, or fall back on symbolic shape\n        batch_size = p_choose_i.shape[0] or tf.shape(p_choose_i)[0]\n        # Compute [1, 1 - p_choose_i[0], 1 - p_choose_i[1], ..., 1 - p_choose_\n        # i[-2]]\n        shifted_1mp_choose_i = tf.concat(\n            [tf.ones((batch_size, 1)), 1 - p_choose_i[:, :-1]], 1\n        )\n        # Compute attention distribution recursively as\n        # q[i] = (1 - p_choose_i[i - 1])*q[i - 1] + previous_attention[i]\n        # attention[i] = p_choose_i[i]*q[i]\n        attention = p_choose_i * tf.transpose(\n            tf.scan(\n                # Need to use reshape to remind TF of the shape between loop\n                # iterations\n                lambda x, yz: tf.reshape(yz[0] * x + yz[1], (batch_size,)),\n                # Loop variables yz[0] and yz[1]\n                [tf.transpose(shifted_1mp_choose_i), tf.transpose(previous_attention)],\n                # Initial value of x is just zeros\n                tf.zeros((batch_size,)),\n            )\n        )\n    elif mode == ""parallel"":\n        # safe_cumprod computes cumprod in logspace with numeric checks\n        cumprod_1mp_choose_i = safe_cumprod(1 - p_choose_i, axis=1, exclusive=True)\n        # Compute recurrence relation solution\n        attention = (\n            p_choose_i\n            * cumprod_1mp_choose_i\n            * tf.cumsum(\n                previous_attention /\n                # Clip cumprod_1mp to avoid divide-by-zero\n                tf.clip_by_value(cumprod_1mp_choose_i, 1e-10, 1.0),\n                axis=1,\n            )\n        )\n    elif mode == ""hard"":\n        # Remove any probabilities before the index chosen last time step\n        p_choose_i *= tf.cumsum(previous_attention, axis=1)\n        # Now, use exclusive cumprod to remove probabilities after the first\n        # chosen index, like so:\n        # p_choose_i = [0, 0, 0, 1, 1, 0, 1, 1]\n        # cumprod(1 - p_choose_i, exclusive=True) = [1, 1, 1, 1, 0, 0, 0, 0]\n        # Product of above: [0, 0, 0, 1, 0, 0, 0, 0]\n        attention = p_choose_i * tf.math.cumprod(1 - p_choose_i, axis=1, exclusive=True)\n    else:\n        raise ValueError(""mode must be \'recursive\', \'parallel\', or \'hard\'."")\n    return attention\n\n\ndef _monotonic_probability_fn(\n    score, previous_alignments, sigmoid_noise, mode, seed=None\n):\n    """"""Attention probability function for monotonic attention.\n\n    Takes in unnormalized attention scores, adds pre-sigmoid noise to encourage\n    the model to make discrete attention decisions, passes them through a\n    sigmoid to obtain ""choosing"" probabilities, and then calls\n    monotonic_attention to obtain the attention distribution.  For more\n    information, see\n\n    Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.  https://arxiv.org/abs/1704.00784\n\n    Args:\n      score: Unnormalized attention scores, shape\n        `[batch_size, alignments_size]`\n      previous_alignments: Previous attention distribution, shape\n        `[batch_size, alignments_size]`\n      sigmoid_noise: Standard deviation of pre-sigmoid noise. Setting this\n        larger than 0 will encourage the model to produce large attention\n        scores, effectively making the choosing probabilities discrete and the\n        resulting attention distribution one-hot.  It should be set to 0 at\n        test-time, and when hard attention is not desired.\n      mode: How to compute the attention distribution.  Must be one of\n        \'recursive\', \'parallel\', or \'hard\'.  See the docstring for\n        `tfa.seq2seq.monotonic_attention` for more information.\n      seed: (optional) Random seed for pre-sigmoid noise.\n\n    Returns:\n      A `[batch_size, alignments_size]`-shape tensor corresponding to the\n      resulting attention distribution.\n    """"""\n    # Optionally add pre-sigmoid noise to the scores\n    if sigmoid_noise > 0:\n        noise = tf.random.normal(tf.shape(score), dtype=score.dtype, seed=seed)\n        score += sigmoid_noise * noise\n    # Compute ""choosing"" probabilities from the attention scores\n    if mode == ""hard"":\n        # When mode is hard, use a hard sigmoid\n        p_choose_i = tf.cast(score > 0, score.dtype)\n    else:\n        p_choose_i = tf.sigmoid(score)\n    # Convert from choosing probabilities to attention distribution\n    return monotonic_attention(p_choose_i, previous_alignments, mode)\n\n\nclass _BaseMonotonicAttentionMechanism(_BaseAttentionMechanism):\n    """"""Base attention mechanism for monotonic attention.\n\n    Simply overrides the initial_alignments function to provide a dirac\n    distribution, which is needed in order for the monotonic attention\n    distributions to have the correct behavior.\n    """"""\n\n    def initial_alignments(self, batch_size, dtype):\n        """"""Creates the initial alignment values for the monotonic attentions.\n\n        Initializes to dirac distributions, i.e.\n        [1, 0, 0, ...memory length..., 0] for all entries in the batch.\n\n        Args:\n          batch_size: `int32` scalar, the batch_size.\n          dtype: The `dtype`.\n\n        Returns:\n          A `dtype` tensor shaped `[batch_size, alignments_size]`\n          (`alignments_size` is the values\' `max_time`).\n        """"""\n        max_time = self._alignments_size\n        return tf.one_hot(\n            tf.zeros((batch_size,), dtype=tf.int32), max_time, dtype=dtype\n        )\n\n\nclass BahdanauMonotonicAttention(_BaseMonotonicAttentionMechanism):\n    """"""Monotonic attention mechanism with Bahadanau-style energy function.\n\n    This type of attention enforces a monotonic constraint on the attention\n    distributions; that is once the model attends to a given point in the\n    memory it can\'t attend to any prior points at subsequence output timesteps.\n    It achieves this by using the _monotonic_probability_fn instead of softmax\n    to construct its attention distributions.  Since the attention scores are\n    passed through a sigmoid, a learnable scalar bias parameter is applied\n    after the score function and before the sigmoid.  Otherwise, it is\n    equivalent to BahdanauAttention.  This approach is proposed in\n\n    Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.  https://arxiv.org/abs/1704.00784\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        memory: Optional[TensorLike] = None,\n        memory_sequence_length: Optional[TensorLike] = None,\n        normalize: bool = False,\n        sigmoid_noise: FloatTensorLike = 0.0,\n        sigmoid_noise_seed: Optional[FloatTensorLike] = None,\n        score_bias_init: FloatTensorLike = 0.0,\n        mode: str = ""parallel"",\n        kernel_initializer: Initializer = ""glorot_uniform"",\n        dtype: AcceptableDTypes = None,\n        name: str = ""BahdanauMonotonicAttention"",\n        **kwargs\n    ):\n        """"""Construct the Attention mechanism.\n\n        Args:\n          units: The depth of the query mechanism.\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          memory_sequence_length: (optional): Sequence lengths for the batch\n            entries in memory.  If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          normalize: Python boolean. Whether to normalize the energy term.\n          sigmoid_noise: Standard deviation of pre-sigmoid noise. See the\n            docstring for `_monotonic_probability_fn` for more information.\n          sigmoid_noise_seed: (optional) Random seed for pre-sigmoid noise.\n          score_bias_init: Initial value for score bias scalar. It\'s\n            recommended to initialize this to a negative value when the length\n            of the memory is large.\n          mode: How to compute the attention distribution. Must be one of\n            \'recursive\', \'parallel\', or \'hard\'. See the docstring for\n            `tfa.seq2seq.monotonic_attention` for more information.\n          kernel_initializer: (optional), the name of the initializer for the\n            attention kernel.\n          dtype: The data type for the query and memory layers of the attention\n            mechanism.\n          name: Name to use when creating ops.\n          **kwargs: Dictionary that contains other common arguments for layer\n            creation.\n        """"""\n        # Set up the monotonic probability fn with supplied parameters\n        if dtype is None:\n            dtype = tf.float32\n        wrapped_probability_fn = functools.partial(\n            _monotonic_probability_fn,\n            sigmoid_noise=sigmoid_noise,\n            mode=mode,\n            seed=sigmoid_noise_seed,\n        )\n        query_layer = kwargs.pop(""query_layer"", None)\n        if not query_layer:\n            query_layer = tf.keras.layers.Dense(\n                units, name=""query_layer"", use_bias=False, dtype=dtype\n            )\n        memory_layer = kwargs.pop(""memory_layer"", None)\n        if not memory_layer:\n            memory_layer = tf.keras.layers.Dense(\n                units, name=""memory_layer"", use_bias=False, dtype=dtype\n            )\n        self.units = units\n        self.normalize = normalize\n        self.sigmoid_noise = sigmoid_noise\n        self.sigmoid_noise_seed = sigmoid_noise_seed\n        self.score_bias_init = score_bias_init\n        self.mode = mode\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n        self.attention_v = None\n        self.attention_score_bias = None\n        self.attention_g = None\n        self.attention_b = None\n        super().__init__(\n            memory=memory,\n            memory_sequence_length=memory_sequence_length,\n            query_layer=query_layer,\n            memory_layer=memory_layer,\n            probability_fn=wrapped_probability_fn,\n            name=name,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        if self.attention_v is None:\n            self.attention_v = self.add_weight(\n                ""attention_v"",\n                [self.units],\n                dtype=self.dtype,\n                initializer=self.kernel_initializer,\n            )\n        if self.attention_score_bias is None:\n            self.attention_score_bias = self.add_weight(\n                ""attention_score_bias"",\n                shape=(),\n                dtype=self.dtype,\n                initializer=tf.constant_initializer(self.score_bias_init),\n            )\n        if self.normalize and self.attention_g is None and self.attention_b is None:\n            self.attention_g = self.add_weight(\n                ""attention_g"",\n                dtype=self.dtype,\n                initializer=tf.constant_initializer(math.sqrt(1.0 / self.units)),\n                shape=(),\n            )\n            self.attention_b = self.add_weight(\n                ""attention_b"",\n                [self.units],\n                dtype=self.dtype,\n                initializer=tf.zeros_initializer(),\n            )\n        self.built = True\n\n    def _calculate_attention(self, query, state):\n        """"""Score the query based on the keys and values.\n\n        Args:\n          query: Tensor of dtype matching `self.values` and shape\n            `[batch_size, query_depth]`.\n          state: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]`\n            (`alignments_size` is memory\'s `max_time`).\n\n        Returns:\n          alignments: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n            `max_time`).\n        """"""\n        processed_query = self.query_layer(query) if self.query_layer else query\n        score = _bahdanau_score(\n            processed_query,\n            self.keys,\n            self.attention_v,\n            attention_g=self.attention_g,\n            attention_b=self.attention_b,\n        )\n        score += self.attention_score_bias\n        alignments = self.probability_fn(score, state)\n        next_state = alignments\n        return alignments, next_state\n\n    def get_config(self):\n\n        # yapf: disable\n        config = {\n            ""units"": self.units,\n            ""normalize"": self.normalize,\n            ""sigmoid_noise"": self.sigmoid_noise,\n            ""sigmoid_noise_seed"": self.sigmoid_noise_seed,\n            ""score_bias_init"": self.score_bias_init,\n            ""mode"": self.mode,\n            ""kernel_initializer"": tf.keras.initializers.serialize(\n                self.kernel_initializer),\n        }\n        # yapf: enable\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        config = _BaseAttentionMechanism.deserialize_inner_layer_from_config(\n            config, custom_objects=custom_objects\n        )\n        return cls(**config)\n\n\nclass LuongMonotonicAttention(_BaseMonotonicAttentionMechanism):\n    """"""Monotonic attention mechanism with Luong-style energy function.\n\n    This type of attention enforces a monotonic constraint on the attention\n    distributions; that is once the model attends to a given point in the\n    memory it can\'t attend to any prior points at subsequence output timesteps.\n    It achieves this by using the _monotonic_probability_fn instead of softmax\n    to construct its attention distributions.  Otherwise, it is equivalent to\n    LuongAttention.  This approach is proposed in\n\n    [Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,\n    ""Online and Linear-Time Attention by Enforcing Monotonic Alignments.""\n    ICML 2017.](https://arxiv.org/abs/1704.00784)\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        units: TensorLike,\n        memory: Optional[TensorLike] = None,\n        memory_sequence_length: Optional[TensorLike] = None,\n        scale: bool = False,\n        sigmoid_noise: FloatTensorLike = 0.0,\n        sigmoid_noise_seed: Optional[FloatTensorLike] = None,\n        score_bias_init: FloatTensorLike = 0.0,\n        mode: str = ""parallel"",\n        dtype: AcceptableDTypes = None,\n        name: str = ""LuongMonotonicAttention"",\n        **kwargs\n    ):\n        """"""Construct the Attention mechanism.\n\n        Args:\n          units: The depth of the query mechanism.\n          memory: The memory to query; usually the output of an RNN encoder.\n            This tensor should be shaped `[batch_size, max_time, ...]`.\n          memory_sequence_length: (optional): Sequence lengths for the batch\n            entries in memory.  If provided, the memory tensor rows are masked\n            with zeros for values past the respective sequence lengths.\n          scale: Python boolean.  Whether to scale the energy term.\n          sigmoid_noise: Standard deviation of pre-sigmoid noise.  See the\n            docstring for `_monotonic_probability_fn` for more information.\n          sigmoid_noise_seed: (optional) Random seed for pre-sigmoid noise.\n          score_bias_init: Initial value for score bias scalar.  It\'s\n            recommended to initialize this to a negative value when the length\n            of the memory is large.\n          mode: How to compute the attention distribution.  Must be one of\n            \'recursive\', \'parallel\', or \'hard\'.  See the docstring for\n            `tfa.seq2seq.monotonic_attention` for more information.\n          dtype: The data type for the query and memory layers of the attention\n            mechanism.\n          name: Name to use when creating ops.\n          **kwargs: Dictionary that contains other common arguments for layer\n            creation.\n        """"""\n        # Set up the monotonic probability fn with supplied parameters\n        if dtype is None:\n            dtype = tf.float32\n        wrapped_probability_fn = functools.partial(\n            _monotonic_probability_fn,\n            sigmoid_noise=sigmoid_noise,\n            mode=mode,\n            seed=sigmoid_noise_seed,\n        )\n        memory_layer = kwargs.pop(""memory_layer"", None)\n        if not memory_layer:\n            memory_layer = tf.keras.layers.Dense(\n                units, name=""memory_layer"", use_bias=False, dtype=dtype\n            )\n        self.units = units\n        self.scale = scale\n        self.sigmoid_noise = sigmoid_noise\n        self.sigmoid_noise_seed = sigmoid_noise_seed\n        self.score_bias_init = score_bias_init\n        self.mode = mode\n        self.attention_g = None\n        self.attention_score_bias = None\n        super().__init__(\n            memory=memory,\n            memory_sequence_length=memory_sequence_length,\n            query_layer=None,\n            memory_layer=memory_layer,\n            probability_fn=wrapped_probability_fn,\n            name=name,\n            dtype=dtype,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        if self.scale and self.attention_g is None:\n            self.attention_g = self.add_weight(\n                ""attention_g"", initializer=tf.ones_initializer, shape=()\n            )\n        if self.attention_score_bias is None:\n            self.attention_score_bias = self.add_weight(\n                ""attention_score_bias"",\n                shape=(),\n                initializer=tf.constant_initializer(self.score_bias_init),\n            )\n        self.built = True\n\n    def _calculate_attention(self, query, state):\n        """"""Score the query based on the keys and values.\n\n        Args:\n          query: Tensor of dtype matching `self.values` and shape\n            `[batch_size, query_depth]`.\n          state: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]`\n            (`alignments_size` is memory\'s `max_time`).\n\n        Returns:\n          alignments: Tensor of dtype matching `self.values` and shape\n            `[batch_size, alignments_size]` (`alignments_size` is memory\'s\n            `max_time`).\n          next_state: Same as alignments\n        """"""\n        score = _luong_score(query, self.keys, self.attention_g)\n        score += self.attention_score_bias\n        alignments = self.probability_fn(score, state)\n        next_state = alignments\n        return alignments, next_state\n\n    def get_config(self):\n        config = {\n            ""units"": self.units,\n            ""scale"": self.scale,\n            ""sigmoid_noise"": self.sigmoid_noise,\n            ""sigmoid_noise_seed"": self.sigmoid_noise_seed,\n            ""score_bias_init"": self.score_bias_init,\n            ""mode"": self.mode,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        config = _BaseAttentionMechanism.deserialize_inner_layer_from_config(\n            config, custom_objects=custom_objects\n        )\n        return cls(**config)\n\n\nclass AttentionWrapperState(\n    collections.namedtuple(\n        ""AttentionWrapperState"",\n        (\n            ""cell_state"",\n            ""attention"",\n            ""alignments"",\n            ""alignment_history"",\n            ""attention_state"",\n        ),\n    )\n):\n    """"""`namedtuple` storing the state of a `AttentionWrapper`.\n\n    Contains:\n\n      - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n        step.\n      - `attention`: The attention emitted at the previous time step.\n      - `alignments`: A single or tuple of `Tensor`(s) containing the\n         alignments emitted at the previous time step for each attention\n         mechanism.\n      - `alignment_history`: (if enabled) a single or tuple of `TensorArray`(s)\n         containing alignment matrices from all time steps for each attention\n         mechanism. Call `stack()` on each to convert to a `Tensor`.\n      - `attention_state`: A single or tuple of nested objects\n         containing attention mechanism state for each attention mechanism.\n         The objects may contain Tensors or TensorArrays.\n    """"""\n\n    def clone(self, **kwargs):\n        """"""Clone this object, overriding components provided by kwargs.\n\n        The new state fields\' shape must match original state fields\' shape.\n        This will be validated, and original fields\' shape will be propagated\n        to new fields.\n\n        Example:\n\n        ```python\n        initial_state = attention_wrapper.get_initial_state(\n            batch_size=..., dtype=...)\n        initial_state = initial_state.clone(cell_state=encoder_state)\n        ```\n\n        Args:\n          **kwargs: Any properties of the state object to replace in the\n            returned `AttentionWrapperState`.\n\n        Returns:\n          A new `AttentionWrapperState` whose properties are the same as\n          this one, except any overridden properties as provided in `kwargs`.\n        """"""\n\n        def with_same_shape(old, new):\n            """"""Check and set new tensor\'s shape.""""""\n            if isinstance(old, tf.Tensor) and isinstance(new, tf.Tensor):\n                if not tf.executing_eagerly():\n                    new_shape = tf.shape(new)\n                    old_shape = tf.shape(old)\n                    assert_equal = tf.debugging.assert_equal(new_shape, old_shape)\n                    with tf.control_dependencies([assert_equal]):\n                        # Add an identity op so that control deps can kick in.\n                        return tf.identity(new)\n                else:\n                    if old.shape.as_list() != new.shape.as_list():\n                        raise ValueError(\n                            ""The shape of the AttentionWrapperState is ""\n                            ""expected to be same as the one to clone. ""\n                            ""self.shape: %s, input.shape: %s"" % (old.shape, new.shape)\n                        )\n                    return new\n            return new\n\n        return tf.nest.map_structure(with_same_shape, self, super()._replace(**kwargs))\n\n\ndef _prepare_memory(\n    memory, memory_sequence_length=None, memory_mask=None, check_inner_dims_defined=True\n):\n    """"""Convert to tensor and possibly mask `memory`.\n\n    Args:\n      memory: `Tensor`, shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.\n      memory_mask: `boolean` tensor with shape [batch_size, max_time]. The\n        memory should be skipped when the corresponding mask is False.\n      check_inner_dims_defined: Python boolean.  If `True`, the `memory`\n        argument\'s shape is checked to ensure all but the two outermost\n        dimensions are fully defined.\n\n    Returns:\n      A (possibly masked), checked, new `memory`.\n\n    Raises:\n      ValueError: If `check_inner_dims_defined` is `True` and not\n        `memory.shape[2:].is_fully_defined()`.\n    """"""\n    memory = tf.nest.map_structure(\n        lambda m: tf.convert_to_tensor(m, name=""memory""), memory\n    )\n    if memory_sequence_length is not None and memory_mask is not None:\n        raise ValueError(\n            ""memory_sequence_length and memory_mask can\'t be provided "" ""at same time.""\n        )\n    if memory_sequence_length is not None:\n        memory_sequence_length = tf.convert_to_tensor(\n            memory_sequence_length, name=""memory_sequence_length""\n        )\n    if check_inner_dims_defined:\n\n        def _check_dims(m):\n            if not m.shape[2:].is_fully_defined():\n                raise ValueError(\n                    ""Expected memory %s to have fully defined inner dims, ""\n                    ""but saw shape: %s"" % (m.name, m.shape)\n                )\n\n        tf.nest.map_structure(_check_dims, memory)\n    if memory_sequence_length is None and memory_mask is None:\n        return memory\n    elif memory_sequence_length is not None:\n        seq_len_mask = tf.sequence_mask(\n            memory_sequence_length,\n            maxlen=tf.shape(tf.nest.flatten(memory)[0])[1],\n            dtype=tf.nest.flatten(memory)[0].dtype,\n        )\n    else:\n        # For memory_mask is not None\n        seq_len_mask = tf.cast(memory_mask, dtype=tf.nest.flatten(memory)[0].dtype)\n\n    def _maybe_mask(m, seq_len_mask):\n        """"""Mask the memory based on the memory mask.""""""\n        rank = m.shape.ndims\n        rank = rank if rank is not None else tf.rank(m)\n        extra_ones = tf.ones(rank - 2, dtype=tf.int32)\n        seq_len_mask = tf.reshape(\n            seq_len_mask, tf.concat((tf.shape(seq_len_mask), extra_ones), 0)\n        )\n        return m * seq_len_mask\n\n    return tf.nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory)\n\n\ndef _maybe_mask_score(\n    score, memory_sequence_length=None, memory_mask=None, score_mask_value=None\n):\n    """"""Mask the attention score based on the masks.""""""\n    if memory_sequence_length is None and memory_mask is None:\n        return score\n    if memory_sequence_length is not None and memory_mask is not None:\n        raise ValueError(\n            ""memory_sequence_length and memory_mask can\'t be provided "" ""at same time.""\n        )\n    if memory_sequence_length is not None:\n        message = ""All values in memory_sequence_length must greater than "" ""zero.""\n        with tf.control_dependencies(\n            [\n                tf.debugging.assert_positive(  # pylint: disable=bad-continuation\n                    memory_sequence_length, message=message\n                )\n            ]\n        ):\n            memory_mask = tf.sequence_mask(\n                memory_sequence_length, maxlen=tf.shape(score)[1]\n            )\n    score_mask_values = score_mask_value * tf.ones_like(score)\n    return tf.where(memory_mask, score, score_mask_values)\n\n\ndef hardmax(logits: TensorLike, name: Optional[str] = None) -> tf.Tensor:\n    """"""Returns batched one-hot vectors.\n\n    The depth index containing the `1` is that of the maximum logit value.\n\n    Args:\n      logits: A batch tensor of logit values.\n      name: Name to use when creating ops.\n    Returns:\n      A batched one-hot tensor.\n    """"""\n    with tf.name_scope(name or ""Hardmax""):\n        logits = tf.convert_to_tensor(logits, name=""logits"")\n        depth = logits.shape[-1] or tf.shape(logits)[-1]\n        return tf.one_hot(tf.argmax(logits, -1), depth, dtype=logits.dtype)\n\n\ndef _compute_attention(\n    attention_mechanism, cell_output, attention_state, attention_layer\n):\n    """"""Computes the attention and alignments for a given\n    attention_mechanism.""""""\n    if isinstance(attention_mechanism, _BaseAttentionMechanism):\n        alignments, next_attention_state = attention_mechanism(\n            [cell_output, attention_state]\n        )\n    else:\n        # For other class, assume they are following _BaseAttentionMechanism,\n        # which takes query and state as separate parameter.\n        alignments, next_attention_state = attention_mechanism(\n            cell_output, state=attention_state\n        )\n\n    # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n    expanded_alignments = tf.expand_dims(alignments, 1)\n    # Context is the inner product of alignments and values along the\n    # memory time dimension.\n    # alignments shape is\n    #   [batch_size, 1, memory_time]\n    # attention_mechanism.values shape is\n    #   [batch_size, memory_time, memory_size]\n    # the batched matmul is over memory_time, so the output shape is\n    #   [batch_size, 1, memory_size].\n    # we then squeeze out the singleton dim.\n    context_ = tf.matmul(expanded_alignments, attention_mechanism.values)\n    context_ = tf.squeeze(context_, [1])\n\n    if attention_layer is not None:\n        attention = attention_layer(tf.concat([cell_output, context_], 1))\n    else:\n        attention = context_\n\n    return attention, alignments, next_attention_state\n\n\nclass AttentionWrapper(tf.keras.layers.AbstractRNNCell):\n    """"""Wraps another `RNNCell` with attention.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        cell: tf.keras.layers.Layer,\n        attention_mechanism: Union[AttentionMechanism, List[AttentionMechanism]],\n        attention_layer_size: Optional[Union[Number, List[Number]]] = None,\n        alignment_history: bool = False,\n        cell_input_fn: Optional[Callable] = None,\n        output_attention: bool = True,\n        initial_cell_state: Optional[TensorLike] = None,\n        name: Optional[str] = None,\n        attention_layer: Optional[\n            Union[tf.keras.layers.Layer, List[tf.keras.layers.Layer]]\n        ] = None,\n        attention_fn: Optional[Callable] = None,\n        **kwargs\n    ):\n        """"""Construct the `AttentionWrapper`.\n\n        **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped\n        in `AttentionWrapper`, then you must ensure that:\n\n        - The encoder output has been tiled to `beam_width` via\n          `tfa.seq2seq.tile_batch` (NOT `tf.tile`).\n        - The `batch_size` argument passed to the `get_initial_state` method of\n          this wrapper is equal to `true_batch_size * beam_width`.\n        - The initial state created with `get_initial_state` above contains a\n          `cell_state` value containing properly tiled final state from the\n          encoder.\n\n        An example:\n\n        ```\n        tiled_encoder_outputs = tfa.seq2seq.tile_batch(\n            encoder_outputs, multiplier=beam_width)\n        tiled_encoder_final_state = tfa.seq2seq.tile_batch(\n            encoder_final_state, multiplier=beam_width)\n        tiled_sequence_length = tfa.seq2seq.tile_batch(\n            sequence_length, multiplier=beam_width)\n        attention_mechanism = MyFavoriteAttentionMechanism(\n            num_units=attention_depth,\n            memory=tiled_inputs,\n            memory_sequence_length=tiled_sequence_length)\n        attention_cell = AttentionWrapper(cell, attention_mechanism, ...)\n        decoder_initial_state = attention_cell.get_initial_state(\n            batch_size=true_batch_size * beam_width, dtype=dtype)\n        decoder_initial_state = decoder_initial_state.clone(\n            cell_state=tiled_encoder_final_state)\n        ```\n\n        Args:\n          cell: An instance of `RNNCell`.\n          attention_mechanism: A list of `AttentionMechanism` instances or a\n            single instance.\n          attention_layer_size: A list of Python integers or a single Python\n            integer, the depth of the attention (output) layer(s). If None\n            (default), use the context as attention at each time step.\n            Otherwise, feed the context and cell output into the attention\n            layer to generate attention at each time step. If\n            attention_mechanism is a list, attention_layer_size must be a list\n            of the same length. If attention_layer is set, this must be None.\n            If attention_fn is set, it must guaranteed that the outputs of\n            attention_fn also meet the above requirements.\n          alignment_history: Python boolean, whether to store alignment history\n            from all time steps in the final output state (currently stored as\n            a time major `TensorArray` on which you must call `stack()`).\n          cell_input_fn: (optional) A `callable`.  The default is:\n            `lambda inputs, attention:\n              tf.concat([inputs, attention], -1)`.\n          output_attention: Python bool.  If `True` (default), the output at\n            each time step is the attention value.  This is the behavior of\n            Luong-style attention mechanisms.  If `False`, the output at each\n            time step is the output of `cell`.  This is the behavior of\n            Bhadanau-style attention mechanisms.  In both cases, the\n            `attention` tensor is propagated to the next time step via the\n            state and is used there. This flag only controls whether the\n            attention mechanism is propagated up to the next cell in an RNN\n            stack or to the top RNN output.\n          initial_cell_state: The initial state value to use for the cell when\n            the user calls `get_initial_state()`.  Note that if this value is\n            provided now, and the user uses a `batch_size` argument of\n            `get_initial_state` which does not match the batch size of\n            `initial_cell_state`, proper behavior is not guaranteed.\n          name: Name to use when creating ops.\n          attention_layer: A list of `tf.tf.keras.layers.Layer` instances or a\n            single `tf.tf.keras.layers.Layer` instance taking the context\n            and cell output as inputs to generate attention at each time step.\n            If None (default), use the context as attention at each time step.\n            If attention_mechanism is a list, attention_layer must be a list of\n            the same length. If attention_layer_size is set, this must be\n            None.\n          attention_fn: An optional callable function that allows users to\n            provide their own customized attention function, which takes input\n            (attention_mechanism, cell_output, attention_state,\n            attention_layer) and outputs (attention, alignments,\n            next_attention_state). If provided, the attention_layer_size should\n            be the size of the outputs of attention_fn.\n          **kwargs: Other keyword arguments for layer creation.\n\n        Raises:\n          TypeError: `attention_layer_size` is not None and\n            (`attention_mechanism` is a list but `attention_layer_size` is not;\n            or vice versa).\n          ValueError: if `attention_layer_size` is not None,\n            `attention_mechanism` is a list, and its length does not match that\n            of `attention_layer_size`; if `attention_layer_size` and\n            `attention_layer` are set simultaneously.\n        """"""\n        super().__init__(name=name, **kwargs)\n        keras_utils.assert_like_rnncell(""cell"", cell)\n        if isinstance(attention_mechanism, (list, tuple)):\n            self._is_multi = True\n            attention_mechanisms = list(attention_mechanism)\n        else:\n            self._is_multi = False\n            attention_mechanisms = [attention_mechanism]\n\n        if cell_input_fn is None:\n\n            def cell_input_fn(inputs, attention):\n                return tf.concat([inputs, attention], -1)\n\n        if attention_layer_size is not None and attention_layer is not None:\n            raise ValueError(\n                ""Only one of attention_layer_size and attention_layer "" ""should be set""\n            )\n\n        if attention_layer_size is not None:\n            attention_layer_sizes = tuple(\n                attention_layer_size\n                if isinstance(attention_layer_size, (list, tuple))\n                else (attention_layer_size,)\n            )\n            if len(attention_layer_sizes) != len(attention_mechanisms):\n                raise ValueError(\n                    ""If provided, attention_layer_size must contain exactly ""\n                    ""one integer per attention_mechanism, saw: %d vs %d""\n                    % (len(attention_layer_sizes), len(attention_mechanisms))\n                )\n            self._attention_layers = list(\n                tf.keras.layers.Dense(\n                    attention_layer_size,\n                    name=""attention_layer"",\n                    use_bias=False,\n                    dtype=attention_mechanisms[i].dtype,\n                )\n                for i, attention_layer_size in enumerate(attention_layer_sizes)\n            )\n        elif attention_layer is not None:\n            self._attention_layers = list(\n                attention_layer\n                if isinstance(attention_layer, (list, tuple))\n                else (attention_layer,)\n            )\n            if len(self._attention_layers) != len(attention_mechanisms):\n                raise ValueError(\n                    ""If provided, attention_layer must contain exactly one ""\n                    ""layer per attention_mechanism, saw: %d vs %d""\n                    % (len(self._attention_layers), len(attention_mechanisms))\n                )\n        else:\n            self._attention_layers = None\n\n        if attention_fn is None:\n            attention_fn = _compute_attention\n        self._attention_fn = attention_fn\n        self._attention_layer_size = None\n\n        self._cell = cell\n        self._attention_mechanisms = attention_mechanisms\n        self._cell_input_fn = cell_input_fn\n        self._output_attention = output_attention\n        self._alignment_history = alignment_history\n        with tf.name_scope(name or ""AttentionWrapperInit""):\n            if initial_cell_state is None:\n                self._initial_cell_state = None\n            else:\n                final_state_tensor = tf.nest.flatten(initial_cell_state)[-1]\n                state_batch_size = (\n                    final_state_tensor.shape[0] or tf.shape(final_state_tensor)[0]\n                )\n                error_message = (\n                    ""When constructing AttentionWrapper %s: "" % self.name\n                    + ""Non-matching batch sizes between the memory ""\n                    ""(encoder output) and initial_cell_state.  Are you using ""\n                    ""the BeamSearchDecoder?  You may need to tile your ""\n                    ""initial state via the tfa.seq2seq.tile_batch ""\n                    ""function with argument multiple=beam_width.""\n                )\n                with tf.control_dependencies(\n                    self._batch_size_checks(  # pylint: disable=bad-continuation\n                        state_batch_size, error_message\n                    )\n                ):\n                    self._initial_cell_state = tf.nest.map_structure(\n                        lambda s: tf.identity(s, name=""check_initial_cell_state""),\n                        initial_cell_state,\n                    )\n\n    def _attention_mechanisms_checks(self):\n        for attention_mechanism in self._attention_mechanisms:\n            if not attention_mechanism.memory_initialized:\n                raise ValueError(\n                    ""The AttentionMechanism instances passed to ""\n                    ""this AttentionWrapper should be initialized ""\n                    ""with a memory first, either by passing it ""\n                    ""to the AttentionMechanism constructor or ""\n                    ""calling attention_mechanism.setup_memory()""\n                )\n\n    def _batch_size_checks(self, batch_size, error_message):\n        self._attention_mechanisms_checks()\n        return [\n            tf.debugging.assert_equal(\n                batch_size, attention_mechanism.batch_size, message=error_message\n            )\n            for attention_mechanism in self._attention_mechanisms\n        ]\n\n    def _get_attention_layer_size(self):\n        if self._attention_layer_size is not None:\n            return self._attention_layer_size\n        self._attention_mechanisms_checks()\n        attention_output_sizes = (\n            attention_mechanism.values.shape[-1]\n            for attention_mechanism in self._attention_mechanisms\n        )\n        if self._attention_layers is None:\n            self._attention_layer_size = sum(attention_output_sizes)\n        else:\n            # Compute the layer output size from its input which is the\n            # concatenation of the cell output and the attention mechanism\n            # output.\n            self._attention_layer_size = sum(\n                layer.compute_output_shape(\n                    [None, self._cell.output_size + attention_output_size]\n                )[-1]\n                for layer, attention_output_size in zip(\n                    self._attention_layers, attention_output_sizes\n                )\n            )\n        return self._attention_layer_size\n\n    def _item_or_tuple(self, seq):\n        """"""Returns `seq` as tuple or the singular element.\n\n        Which is returned is determined by how the AttentionMechanism(s) were\n        passed to the constructor.\n\n        Args:\n          seq: A non-empty sequence of items or generator.\n\n        Returns:\n          Either the values in the sequence as a tuple if\n          AttentionMechanism(s) were passed to the constructor as a sequence\n          or the singular element.\n        """"""\n        t = tuple(seq)\n        if self._is_multi:\n            return t\n        else:\n            return t[0]\n\n    @property\n    def output_size(self):\n        if self._output_attention:\n            return self._get_attention_layer_size()\n        else:\n            return self._cell.output_size\n\n    @property\n    def state_size(self):\n        """"""The `state_size` property of `AttentionWrapper`.\n\n        Returns:\n          An `AttentionWrapperState` tuple containing shapes used\n          by this object.\n        """"""\n        return AttentionWrapperState(\n            cell_state=self._cell.state_size,\n            attention=self._get_attention_layer_size(),\n            alignments=self._item_or_tuple(\n                a.alignments_size for a in self._attention_mechanisms\n            ),\n            attention_state=self._item_or_tuple(\n                a.state_size for a in self._attention_mechanisms\n            ),\n            alignment_history=self._item_or_tuple(\n                a.alignments_size if self._alignment_history else ()\n                for a in self._attention_mechanisms\n            ),\n        )  # sometimes a TensorArray\n\n    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n        """"""Return an initial (zero) state tuple for this `AttentionWrapper`.\n\n        **NOTE** Please see the initializer documentation for details of how\n        to call `get_initial_state` if using an `AttentionWrapper` with a\n        `BeamSearchDecoder`.\n\n        Args:\n          inputs: The inputs that will be fed to this cell.\n          batch_size: `0D` integer tensor: the batch size.\n          dtype: The internal state data type.\n\n        Returns:\n          An `AttentionWrapperState` tuple containing zeroed out tensors and,\n          possibly, empty `TensorArray` objects.\n\n        Raises:\n          ValueError: (or, possibly at runtime, InvalidArgument), if\n            `batch_size` does not match the output size of the encoder passed\n            to the wrapper object at initialization time.\n        """"""\n        if inputs is not None:\n            batch_size = tf.shape(inputs)[0]\n            dtype = inputs.dtype\n        with tf.name_scope(\n            type(self).__name__ + ""ZeroState""\n        ):  # pylint: disable=bad-continuation\n            if self._initial_cell_state is not None:\n                cell_state = self._initial_cell_state\n            else:\n                cell_state = self._cell.get_initial_state(\n                    batch_size=batch_size, dtype=dtype\n                )\n            error_message = (\n                ""When calling get_initial_state of AttentionWrapper %s: "" % self.name\n                + ""Non-matching batch sizes between the memory ""\n                ""(encoder output) and the requested batch size. Are you using ""\n                ""the BeamSearchDecoder?  If so, make sure your encoder output ""\n                ""has been tiled to beam_width via ""\n                ""tfa.seq2seq.tile_batch, and the batch_size= argument ""\n                ""passed to get_initial_state is batch_size * beam_width.""\n            )\n            with tf.control_dependencies(\n                self._batch_size_checks(batch_size, error_message)\n            ):  # pylint: disable=bad-continuation\n                cell_state = tf.nest.map_structure(\n                    lambda s: tf.identity(s, name=""checked_cell_state""), cell_state\n                )\n            initial_alignments = [\n                attention_mechanism.initial_alignments(batch_size, dtype)\n                for attention_mechanism in self._attention_mechanisms\n            ]\n            return AttentionWrapperState(\n                cell_state=cell_state,\n                attention=tf.zeros(\n                    [batch_size, self._get_attention_layer_size()], dtype=dtype\n                ),\n                alignments=self._item_or_tuple(initial_alignments),\n                attention_state=self._item_or_tuple(\n                    attention_mechanism.initial_state(batch_size, dtype)\n                    for attention_mechanism in self._attention_mechanisms\n                ),\n                alignment_history=self._item_or_tuple(\n                    tf.TensorArray(\n                        dtype, size=0, dynamic_size=True, element_shape=alignment.shape\n                    )\n                    if self._alignment_history\n                    else ()\n                    for alignment in initial_alignments\n                ),\n            )\n\n    def call(self, inputs, state, **kwargs):\n        """"""Perform a step of attention-wrapped RNN.\n\n        - Step 1: Mix the `inputs` and previous step\'s `attention` output via\n          `cell_input_fn`.\n        - Step 2: Call the wrapped `cell` with this input and its previous\n          state.\n        - Step 3: Score the cell\'s output with `attention_mechanism`.\n        - Step 4: Calculate the alignments by passing the score through the\n          `normalizer`.\n        - Step 5: Calculate the context vector as the inner product between the\n          alignments and the attention_mechanism\'s values (memory).\n        - Step 6: Calculate the attention output by concatenating the cell\n          output and context through the attention layer (a linear layer with\n          `attention_layer_size` outputs).\n\n        Args:\n          inputs: (Possibly nested tuple of) Tensor, the input at this time\n            step.\n          state: An instance of `AttentionWrapperState` containing\n            tensors from the previous time step.\n          **kwargs: Dict, other keyword arguments for the cell call method.\n\n        Returns:\n          A tuple `(attention_or_cell_output, next_state)`, where:\n\n          - `attention_or_cell_output` depending on `output_attention`.\n          - `next_state` is an instance of `AttentionWrapperState`\n             containing the state calculated at this time step.\n\n        Raises:\n          TypeError: If `state` is not an instance of `AttentionWrapperState`.\n        """"""\n        if not isinstance(state, AttentionWrapperState):\n            try:\n                state = AttentionWrapperState(*state)\n            except TypeError:\n                raise TypeError(\n                    ""Expected state to be instance of AttentionWrapperState or ""\n                    ""values that can construct AttentionWrapperState. ""\n                    ""Received type %s instead."" % type(state)\n                )\n\n        # Step 1: Calculate the true inputs to the cell based on the\n        # previous attention value.\n        cell_inputs = self._cell_input_fn(inputs, state.attention)\n        cell_state = state.cell_state\n        cell_output, next_cell_state = self._cell(cell_inputs, cell_state, **kwargs)\n        next_cell_state = tf.nest.pack_sequence_as(\n            cell_state, tf.nest.flatten(next_cell_state)\n        )\n\n        cell_batch_size = cell_output.shape[0] or tf.shape(cell_output)[0]\n        error_message = (\n            ""When applying AttentionWrapper %s: "" % self.name\n            + ""Non-matching batch sizes between the memory ""\n            ""(encoder output) and the query (decoder output).  Are you using ""\n            ""the BeamSearchDecoder?  You may need to tile your memory input ""\n            ""via the tfa.seq2seq.tile_batch function with argument ""\n            ""multiple=beam_width.""\n        )\n        with tf.control_dependencies(\n            self._batch_size_checks(cell_batch_size, error_message)\n        ):  # pylint: disable=bad-continuation\n            cell_output = tf.identity(cell_output, name=""checked_cell_output"")\n\n        if self._is_multi:\n            previous_attention_state = state.attention_state\n            previous_alignment_history = state.alignment_history\n        else:\n            previous_attention_state = [state.attention_state]\n            previous_alignment_history = [state.alignment_history]\n\n        all_alignments = []\n        all_attentions = []\n        all_attention_states = []\n        maybe_all_histories = []\n        for i, attention_mechanism in enumerate(self._attention_mechanisms):\n            attention, alignments, next_attention_state = self._attention_fn(\n                attention_mechanism,\n                cell_output,\n                previous_attention_state[i],\n                self._attention_layers[i] if self._attention_layers else None,\n            )\n            alignment_history = (\n                previous_alignment_history[i].write(\n                    previous_alignment_history[i].size(), alignments\n                )\n                if self._alignment_history\n                else ()\n            )\n\n            all_attention_states.append(next_attention_state)\n            all_alignments.append(alignments)\n            all_attentions.append(attention)\n            maybe_all_histories.append(alignment_history)\n\n        attention = tf.concat(all_attentions, 1)\n        next_state = AttentionWrapperState(\n            cell_state=next_cell_state,\n            attention=attention,\n            attention_state=self._item_or_tuple(all_attention_states),\n            alignments=self._item_or_tuple(all_alignments),\n            alignment_history=self._item_or_tuple(maybe_all_histories),\n        )\n\n        if self._output_attention:\n            return attention, next_state\n        else:\n            return cell_output, next_state\n'"
tensorflow_addons/seq2seq/basic_decoder.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A class of Decoders that may sample to generate the next input.""""""\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import decoder\nfrom tensorflow_addons.seq2seq import sampler as sampler_py\nfrom tensorflow_addons.utils import keras_utils\n\nfrom typeguard import typechecked\nfrom typing import Optional\n\n\nclass BasicDecoderOutput(\n    collections.namedtuple(""BasicDecoderOutput"", (""rnn_output"", ""sample_id""))\n):\n    """"""Outputs of a `BasicDecoder` step.\n\n    Contains:\n\n      - `rnn_output`: The output for this step. If the `output_layer` argument\n         of `BasicDecoder` was set, it is the output of this layer, otherwise it\n         is the output of the RNN cell.\n      - `sample_id`: The token IDs sampled for this step, as returned by the\n        `sampler` instance passed to `BasicDecoder`.\n    """"""\n\n    pass\n\n\nclass BasicDecoder(decoder.BaseDecoder):\n    """"""Basic sampling decoder.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        cell: tf.keras.layers.Layer,\n        sampler: sampler_py.Sampler,\n        output_layer: Optional[tf.keras.layers.Layer] = None,\n        **kwargs\n    ):\n        """"""Initialize BasicDecoder.\n\n        Args:\n          cell: A layer that implements the `tf.keras.layers.AbstractRNNCell`\n            interface.\n          sampler: A `Sampler` instance.\n          output_layer: (Optional) An instance of `tf.keras.layers.Layer`, i.e.,\n            `tf.keras.layers.Dense`. Optional layer to apply to the RNN output\n             prior to storing the result or sampling.\n          **kwargs: Other keyword arguments for layer creation.\n        """"""\n        keras_utils.assert_like_rnncell(""cell"", cell)\n        self.cell = cell\n        self.sampler = sampler\n        self.output_layer = output_layer\n        super().__init__(**kwargs)\n\n    def initialize(self, inputs, initial_state=None, **kwargs):\n        """"""Initialize the decoder.""""""\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        self._cell_dtype = tf.nest.flatten(initial_state)[0].dtype\n        return self.sampler.initialize(inputs, **kwargs) + (initial_state,)\n\n    @property\n    def batch_size(self):\n        return self.sampler.batch_size\n\n    def _rnn_output_size(self):\n        size = tf.TensorShape(self.cell.output_size)\n        if self.output_layer is None:\n            return size\n        else:\n            # To use layer\'s compute_output_shape, we need to convert the\n            # RNNCell\'s output_size entries into shapes with an unknown\n            # batch size.  We then pass this through the layer\'s\n            # compute_output_shape and read off all but the first (batch)\n            # dimensions to get the output size of the rnn with the layer\n            # applied to the top.\n            output_shape_with_unknown_batch = tf.nest.map_structure(\n                lambda s: tf.TensorShape([None]).concatenate(s), size\n            )\n            layer_output_shape = self.output_layer.compute_output_shape(\n                output_shape_with_unknown_batch\n            )\n            return tf.nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n    @property\n    def output_size(self):\n        # Return the cell output and the id\n        return BasicDecoderOutput(\n            rnn_output=self._rnn_output_size(), sample_id=self.sampler.sample_ids_shape\n        )\n\n    @property\n    def output_dtype(self):\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        # Return that structure and the sample_ids_dtype from the helper.\n        dtype = self._cell_dtype\n        return BasicDecoderOutput(\n            tf.nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n            self.sampler.sample_ids_dtype,\n        )\n\n    def step(self, time, inputs, state, training=None):\n        """"""Perform a decoding step.\n\n        Args:\n          time: scalar `int32` tensor.\n          inputs: A (structure of) input tensors.\n          state: A (structure of) state tensors and TensorArrays.\n          training: Python boolean.\n\n        Returns:\n          `(outputs, next_state, next_inputs, finished)`.\n        """"""\n        cell_outputs, cell_state = self.cell(inputs, state, training=training)\n        cell_state = tf.nest.pack_sequence_as(state, tf.nest.flatten(cell_state))\n        if self.output_layer is not None:\n            cell_outputs = self.output_layer(cell_outputs)\n        sample_ids = self.sampler.sample(\n            time=time, outputs=cell_outputs, state=cell_state\n        )\n        (finished, next_inputs, next_state) = self.sampler.next_inputs(\n            time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids\n        )\n        outputs = BasicDecoderOutput(cell_outputs, sample_ids)\n        return (outputs, next_state, next_inputs, finished)\n'"
tensorflow_addons/seq2seq/beam_search_decoder.py,168,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A decoder that performs beam search.""""""\n\nimport collections\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import attention_wrapper\nfrom tensorflow_addons.seq2seq import decoder\nfrom tensorflow_addons.utils import keras_utils\nfrom tensorflow_addons.utils.resource_loader import LazySO\nfrom tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n\nfrom typeguard import typechecked\nfrom typing import Callable, Optional\n\n_beam_search_so = LazySO(""custom_ops/seq2seq/_beam_search_ops.so"")\n\n\ndef gather_tree(*args, **kwargs) -> tf.Tensor:\n    return _beam_search_so.ops.addons_gather_tree(*args, **kwargs)\n\n\nclass BeamSearchDecoderState(\n    collections.namedtuple(\n        ""BeamSearchDecoderState"",\n        (\n            ""cell_state"",\n            ""log_probs"",\n            ""finished"",\n            ""lengths"",\n            ""accumulated_attention_probs"",\n        ),\n    )\n):\n    """"""State of a `BeamSearchDecoder`.\n\n    Contains:\n\n      - `cell_state`: The cell state returned at the previous time step.\n      - `log_probs`: The accumulated log probabilities of each beam.\n        A `float32` `Tensor` of shape `[batch_size, beam_width]`.\n      - `finished`: The finished status of each beam.\n        A `bool` `Tensor` of shape `[batch_size, beam_width]`.\n      - `lengths`: The accumulated length of each beam.\n        An `int64` `Tensor` of shape `[batch_size, beam_width]`.\n      - `accumulated_attention_prob`: Accumulation of the attention\n        probabilities (used to compute the coverage penalty)\n    """"""\n\n    pass\n\n\nclass BeamSearchDecoderOutput(\n    collections.namedtuple(\n        ""BeamSearchDecoderOutput"", (""scores"", ""predicted_ids"", ""parent_ids"")\n    )\n):\n    """"""Outputs of a `BeamSearchDecoder` step.\n\n    Contains:\n\n      - `scores`: The scores for this step, which are the log probabilities\n        over the output vocabulary, possibly penalized by length and attention\n        coverage.\n        A `float32` `Tensor` of shape `[batch_size, beam_width, vocab_size]`.\n      - `predicted_ids`: The token IDs predicted for this step.\n        A `int32` `Tensor` of shape `[batch_size, beam_width]`.\n      - `parent_ids`: The indices of the parent beam of each beam.\n        A `int32` `Tensor` of shape `[batch_size, beam_width]`.\n    """"""\n\n    pass\n\n\nclass FinalBeamSearchDecoderOutput(\n    collections.namedtuple(\n        ""FinalBeamDecoderOutput"", [""predicted_ids"", ""beam_search_decoder_output""]\n    )\n):\n    """"""Final outputs returned by the beam search after all decoding is\n    finished.\n\n    Args:\n      predicted_ids: The final prediction. A tensor of shape\n        `[batch_size, T, beam_width]` (or `[T, batch_size, beam_width]` if\n        `output_time_major` is True). Beams are ordered from best to worst.\n      beam_search_decoder_output: An instance of `BeamSearchDecoderOutput` that\n        describes the state of the beam search.\n    """"""\n\n    pass\n\n\ndef _tile_batch(t, multiplier):\n    """"""Core single-tensor implementation of tile_batch.""""""\n    t = tf.convert_to_tensor(t, name=""t"")\n    shape_t = tf.shape(t)\n    if t.shape.ndims is None or t.shape.ndims < 1:\n        raise ValueError(""t must have statically known rank"")\n    tiling = [1] * (t.shape.ndims + 1)\n    tiling[1] = multiplier\n    tiled_static_batch_size = (\n        t.shape[0] * multiplier if t.shape[0] is not None else None\n    )\n    tiled = tf.tile(tf.expand_dims(t, 1), tiling)\n    tiled = tf.reshape(tiled, tf.concat(([shape_t[0] * multiplier], shape_t[1:]), 0))\n    tiled.set_shape(tf.TensorShape([tiled_static_batch_size]).concatenate(t.shape[1:]))\n    return tiled\n\n\ndef tile_batch(t: TensorLike, multiplier: int, name: Optional[str] = None) -> tf.Tensor:\n    """"""Tile the batch dimension of a (possibly nested structure of) tensor(s)\n    t.\n\n    For each tensor t in a (possibly nested structure) of tensors,\n    this function takes a tensor t shaped `[batch_size, s0, s1, ...]` composed\n    of minibatch entries `t[0], ..., t[batch_size - 1]` and tiles it to have a\n    shape `[batch_size * multiplier, s0, s1, ...]` composed of minibatch\n    entries `t[0], t[0], ..., t[1], t[1], ...` where each minibatch entry is\n    repeated `multiplier` times.\n\n    Args:\n      t: `Tensor` shaped `[batch_size, ...]`.\n      multiplier: Python int.\n      name: Name scope for any created operations.\n\n    Returns:\n      A (possibly nested structure of) `Tensor` shaped\n      `[batch_size * multiplier, ...]`.\n\n    Raises:\n      ValueError: if tensor(s) `t` do not have a statically known rank or\n      the rank is < 1.\n    """"""\n    with tf.name_scope(name or ""tile_batch""):\n        return tf.nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n\n\ndef gather_tree_from_array(\n    t: TensorLike, parent_ids: TensorLike, sequence_length: TensorLike\n) -> tf.Tensor:\n    """"""Calculates the full beams for `TensorArray`s.\n\n    Args:\n      t: A stacked `TensorArray` of size `max_time` that contains `Tensor`s of\n        shape `[batch_size, beam_width, s]` or `[batch_size * beam_width, s]`\n        where `s` is the depth shape.\n      parent_ids: The parent ids of shape `[max_time, batch_size, beam_width]`.\n      sequence_length: The sequence length of shape `[batch_size, beam_width]`.\n\n    Returns:\n      A `Tensor` which is a stacked `TensorArray` of the same size and type as\n      `t` and where beams are sorted in each `Tensor` according to\n      `parent_ids`.\n    """"""\n    max_time = parent_ids.shape[0] or tf.shape(parent_ids)[0]\n    batch_size = parent_ids.shape[1] or tf.shape(parent_ids)[1]\n    beam_width = parent_ids.shape[2] or tf.shape(parent_ids)[2]\n\n    # Generate beam ids that will be reordered by gather_tree.\n    beam_ids = tf.reshape(tf.range(beam_width), [1, 1, -1])\n    beam_ids = tf.tile(beam_ids, [max_time, batch_size, 1])\n\n    max_sequence_lengths = tf.cast(tf.reduce_max(sequence_length, axis=1), tf.int32)\n    sorted_beam_ids = gather_tree(\n        step_ids=beam_ids,\n        parent_ids=parent_ids,\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=beam_width + 1,\n    )\n\n    # For out of range steps, simply copy the same beam.\n    in_bound_steps = tf.transpose(\n        tf.sequence_mask(sequence_length, maxlen=max_time), perm=[2, 0, 1]\n    )\n    sorted_beam_ids = tf.where(in_bound_steps, x=sorted_beam_ids, y=beam_ids)\n\n    # Gather from a tensor with collapsed additional dimensions.\n    final_shape = tf.shape(t)\n    gather_from = tf.reshape(t, [max_time, batch_size, beam_width, -1])\n    ordered = tf.gather(gather_from, sorted_beam_ids, axis=2, batch_dims=2)\n    ordered = tf.reshape(ordered, final_shape)\n\n    return ordered\n\n\ndef _check_ndims(t):\n    if t.shape.ndims is None:\n        raise ValueError(\n            ""Expected tensor (%s) to have known rank, but ndims == None."" % t\n        )\n\n\ndef _check_static_batch_beam_maybe(shape, batch_size, beam_width):\n    """"""Raises an exception if dimensions are known statically and can not be\n    reshaped to [batch_size, beam_size, -1].""""""\n    reshaped_shape = tf.TensorShape([batch_size, beam_width, None])\n    assert len(shape.dims) > 0\n    if batch_size is None or shape[0] is None:\n        return True  # not statically known => no check\n    if shape[0] == batch_size * beam_width:\n        return True  # flattened, matching\n    has_second_dim = shape.ndims >= 2 and shape[1] is not None\n    if has_second_dim and shape[0] == batch_size and shape[1] == beam_width:\n        return True  # non-flattened, matching\n    # Otherwise we could not find a match and warn:\n    tf.get_logger().warn(\n        ""TensorArray reordering expects elements to be ""\n        ""reshapable to %s which is incompatible with the ""\n        ""current shape %s. Consider setting ""\n        ""reorder_tensor_arrays to False to disable TensorArray ""\n        ""reordering during the beam search."" % (reshaped_shape, shape)\n    )\n    return False\n\n\ndef _check_batch_beam(t, batch_size, beam_width):\n    """"""Returns an Assert operation checking that the elements of the stacked\n    TensorArray can be reshaped to [batch_size, beam_size, -1].\n\n    At this point, the TensorArray elements have a known rank of at\n    least 1.\n    """"""\n    error_message = (\n        ""TensorArray reordering expects elements to be ""\n        ""reshapable to [batch_size, beam_size, -1] which is ""\n        ""incompatible with the dynamic shape of %s elements. ""\n        ""Consider setting reorder_tensor_arrays to False to disable ""\n        ""TensorArray reordering during the beam search.""\n        % (t if tf.executing_eagerly() else t.name)\n    )\n    rank = t.shape.ndims\n    shape = tf.shape(t)\n    if rank == 2:\n        condition = tf.equal(shape[1], batch_size * beam_width)\n    else:\n        condition = tf.logical_or(\n            tf.equal(shape[1], batch_size * beam_width),\n            tf.logical_and(\n                tf.equal(shape[1], batch_size), tf.equal(shape[2], beam_width)\n            ),\n        )\n    return tf.Assert(condition, [error_message])\n\n\ndef _as_shape(value):\n    """"""Converts the argument to a TensorShape if not already one.""""""\n    if not isinstance(value, tf.TensorShape):\n        if isinstance(value, tf.Tensor):\n            value = tf.get_static_value(value)\n        value = tf.TensorShape(value)\n    return value\n\n\nclass BeamSearchDecoderMixin:\n    """"""BeamSearchDecoderMixin contains the common methods for\n    BeamSearchDecoder.\n\n    It is expected to be used a base class for concrete\n    BeamSearchDecoder. Since this is a mixin class, it is expected to be\n    used together with other class as base.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        cell: tf.keras.layers.Layer,\n        beam_width: int,\n        output_layer: Optional[tf.keras.layers.Layer] = None,\n        length_penalty_weight: FloatTensorLike = 0.0,\n        coverage_penalty_weight: FloatTensorLike = 0.0,\n        reorder_tensor_arrays: bool = True,\n        **kwargs\n    ):\n        """"""Initialize the BeamSearchDecoderMixin.\n\n        Args:\n          cell: A layer that implements the `tf.keras.layers.AbstractRNNCell`\n            interface.\n          beam_width:  Python integer, the number of beams.\n          output_layer: (Optional) An instance of `tf.keras.layers.Layer`,\n            i.e., `tf.keras.layers.Dense`.  Optional layer to apply to the RNN\n            output prior to storing the result or sampling.\n          length_penalty_weight: Float weight to penalize length. Disabled with\n             0.0.\n          coverage_penalty_weight: Float weight to penalize the coverage of\n            source sentence. Disabled with 0.0.\n          reorder_tensor_arrays: If `True`, `TensorArray`s\' elements within the\n            cell state will be reordered according to the beam search path. If\n            the `TensorArray` can be reordered, the stacked form will be\n            returned. Otherwise, the `TensorArray` will be returned as is. Set\n            this flag to `False` if the cell state contains `TensorArray`s that\n            are not amenable to reordering.\n          **kwargs: Dict, other keyword arguments for parent class.\n        """"""\n        keras_utils.assert_like_rnncell(""cell"", cell)\n        self._cell = cell\n        self._output_layer = output_layer\n        self._reorder_tensor_arrays = reorder_tensor_arrays\n\n        self._start_tokens = None\n        self._end_token = None\n        self._batch_size = None\n        self._beam_width = beam_width\n        self._length_penalty_weight = length_penalty_weight\n        self._coverage_penalty_weight = coverage_penalty_weight\n        super().__init__(**kwargs)\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    def _rnn_output_size(self):\n        """"""Get the output shape from the RNN layer.""""""\n        size = self._cell.output_size\n        if self._output_layer is None:\n            return size\n        else:\n            # To use layer\'s compute_output_shape, we need to convert the\n            # RNNCell\'s output_size entries into shapes with an unknown\n            # batch size.  We then pass this through the layer\'s\n            # compute_output_shape and read off all but the first (batch)\n            # dimensions to get the output size of the rnn with the layer\n            # applied to the top.\n            output_shape_with_unknown_batch = tf.nest.map_structure(\n                lambda s: tf.TensorShape([None]).concatenate(s), size\n            )\n            layer_output_shape = self._output_layer.compute_output_shape(\n                output_shape_with_unknown_batch\n            )\n            return tf.nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n    @property\n    def tracks_own_finished(self):\n        """"""The BeamSearchDecoder shuffles its beams and their finished state.\n\n        For this reason, it conflicts with the `dynamic_decode` function\'s\n        tracking of finished states.  Setting this property to true avoids\n        early stopping of decoding due to mismanagement of the finished state\n        in `dynamic_decode`.\n\n        Returns:\n          `True`.\n        """"""\n        return True\n\n    @property\n    def output_size(self):\n        # Return the cell output and the id\n        return BeamSearchDecoderOutput(\n            scores=tf.TensorShape([self._beam_width]),\n            predicted_ids=tf.TensorShape([self._beam_width]),\n            parent_ids=tf.TensorShape([self._beam_width]),\n        )\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        """"""Finalize and return the predicted_ids.\n\n        Args:\n          outputs: An instance of BeamSearchDecoderOutput.\n          final_state: An instance of BeamSearchDecoderState. Passed through to\n            the output.\n          sequence_lengths: An `int64` tensor shaped\n            `[batch_size, beam_width]`. The sequence lengths determined for\n            each beam during decode. **NOTE** These are ignored; the updated\n            sequence lengths are stored in `final_state.lengths`.\n\n        Returns:\n          outputs: An instance of `FinalBeamSearchDecoderOutput` where the\n            predicted_ids are the result of calling _gather_tree.\n          final_state: The same input instance of `BeamSearchDecoderState`.\n        """"""\n        del sequence_lengths\n        # Get max_sequence_length across all beams for each batch.\n        max_sequence_lengths = tf.cast(\n            tf.reduce_max(final_state.lengths, axis=1), tf.int32\n        )\n        predicted_ids = gather_tree(\n            outputs.predicted_ids,\n            outputs.parent_ids,\n            max_sequence_lengths=max_sequence_lengths,\n            end_token=self._end_token,\n        )\n        if self._reorder_tensor_arrays:\n            final_state = final_state._replace(\n                cell_state=tf.nest.map_structure(\n                    lambda t: self._maybe_sort_array_beams(\n                        t, outputs.parent_ids, final_state.lengths\n                    ),\n                    final_state.cell_state,\n                )\n            )\n        outputs = FinalBeamSearchDecoderOutput(\n            beam_search_decoder_output=outputs, predicted_ids=predicted_ids\n        )\n        return outputs, final_state\n\n    def _merge_batch_beams(self, t, s=None):\n        """"""Merges the tensor from a batch of beams into a batch by beams.\n\n        More exactly, t is a tensor of dimension [batch_size, beam_width, s].\n        We reshape this into [batch_size*beam_width, s]\n\n        Args:\n          t: Tensor of dimension [batch_size, beam_width, s]\n          s: (Possibly known) depth shape.\n\n        Returns:\n          A reshaped version of t with dimension [batch_size * beam_width, s].\n        """"""\n        s = _as_shape(s)\n        t_shape = tf.shape(t)\n        static_batch_size = tf.get_static_value(self._batch_size)\n        batch_size_beam_width = (\n            None if static_batch_size is None else static_batch_size * self._beam_width\n        )\n        reshaped_t = tf.reshape(\n            t, tf.concat(([self._batch_size * self._beam_width], t_shape[2:]), 0)\n        )\n        reshaped_t.set_shape(tf.TensorShape([batch_size_beam_width]).concatenate(s))\n        return reshaped_t\n\n    def _split_batch_beams(self, t, s=None):\n        """"""Splits the tensor from a batch by beams into a batch of beams.\n\n        More exactly, t is a tensor of dimension [batch_size*beam_width, s]. We\n        reshape this into [batch_size, beam_width, s]\n\n        Args:\n          t: Tensor of dimension [batch_size*beam_width, s].\n          s: (Possibly known) depth shape.\n\n        Returns:\n          A reshaped version of t with dimension [batch_size, beam_width, s].\n\n        Raises:\n          ValueError: If, after reshaping, the new tensor is not shaped\n            `[batch_size, beam_width, s]` (assuming batch_size and beam_width\n            are known statically).\n        """"""\n        s = _as_shape(s)\n        t_shape = tf.shape(t)\n        reshaped_t = tf.reshape(\n            t, tf.concat(([self._batch_size, self._beam_width], t_shape[1:]), 0)\n        )\n        static_batch_size = tf.get_static_value(self._batch_size)\n        expected_reshaped_shape = tf.TensorShape(\n            [static_batch_size, self._beam_width]\n        ).concatenate(s)\n        if not reshaped_t.shape.is_compatible_with(expected_reshaped_shape):\n            raise ValueError(\n                ""Unexpected behavior when reshaping between beam width ""\n                ""and batch size.  The reshaped tensor has shape: %s.  ""\n                ""We expected it to have shape ""\n                ""(batch_size, beam_width, depth) == %s.  Perhaps you ""\n                ""forgot to call get_initial_state with ""\n                ""batch_size=encoder_batch_size * beam_width?""\n                % (reshaped_t.shape, expected_reshaped_shape)\n            )\n        reshaped_t.set_shape(expected_reshaped_shape)\n        return reshaped_t\n\n    def _maybe_split_batch_beams(self, t, s):\n        """"""Maybe splits the tensor from a batch by beams into a batch of beams.\n\n        We do this so that we can use nest and not run into problems with\n        shapes.\n\n        Args:\n          t: `Tensor`, either scalar or shaped `[batch_size * beam_width] + s`.\n          s: `Tensor`, Python int, or `TensorShape`.\n\n        Returns:\n          If `t` is a matrix or higher order tensor, then the return value is\n          `t` reshaped to `[batch_size, beam_width] + s`.  Otherwise `t` is\n          returned unchanged.\n\n        Raises:\n          ValueError: If the rank of `t` is not statically known.\n        """"""\n        if isinstance(t, tf.TensorArray):\n            return t\n        _check_ndims(t)\n        if t.shape.ndims >= 1:\n            return self._split_batch_beams(t, s)\n        else:\n            return t\n\n    def _maybe_merge_batch_beams(self, t, s):\n        """"""Splits the tensor from a batch by beams into a batch of beams.\n\n        More exactly, `t` is a tensor of dimension\n        `[batch_size * beam_width] + s`, then we reshape it to\n        `[batch_size, beam_width] + s`.\n\n        Args:\n          t: `Tensor` of dimension `[batch_size * beam_width] + s`.\n          s: `Tensor`, Python int, or `TensorShape`.\n\n        Returns:\n          A reshaped version of t with shape `[batch_size, beam_width] + s`.\n\n        Raises:\n          ValueError:  If the rank of `t` is not statically known.\n        """"""\n        if isinstance(t, tf.TensorArray):\n            return t\n        _check_ndims(t)\n        if t.shape.ndims >= 2:\n            return self._merge_batch_beams(t, s)\n        else:\n            return t\n\n    def _maybe_sort_array_beams(self, t, parent_ids, sequence_length):\n        """"""Maybe sorts beams within a `TensorArray`.\n\n        Args:\n          t: A `TensorArray` of size `max_time` that contains `Tensor`s of\n            shape `[batch_size, beam_width, s]` or\n            `[batch_size * beam_width, s]` where `s` is the depth shape.\n          parent_ids: The parent ids of shape\n            `[max_time, batch_size, beam_width]`.\n          sequence_length: The sequence length of shape\n            `[batch_size, beam_width]`.\n\n        Returns:\n          A `TensorArray` where beams are sorted in each `Tensor` or `t` itself\n            if it is not a `TensorArray` or does not meet shape requirements.\n        """"""\n        if not isinstance(t, tf.TensorArray):\n            return t\n        if t.element_shape.ndims is None or t.element_shape.ndims < 1:\n            tf.get_logger().warn(\n                ""The TensorArray %s in the cell state is not amenable to ""\n                ""sorting based on the beam search result. For a ""\n                ""TensorArray to be sorted, its elements shape must be ""\n                ""defined and have at least a rank of 1, but saw shape: %s""\n                % (t.handle.name, t.element_shape)\n            )\n            return t\n        if not _check_static_batch_beam_maybe(\n            t.element_shape, tf.get_static_value(self._batch_size), self._beam_width\n        ):\n            return t\n        t = t.stack()\n        with tf.control_dependencies(\n            [_check_batch_beam(t, self._batch_size, self._beam_width)]\n        ):\n            return gather_tree_from_array(t, parent_ids, sequence_length)\n\n    def step(self, time, inputs, state, training=None, name=None):\n        """"""Perform a decoding step.\n\n        Args:\n          time: scalar `int32` tensor.\n          inputs: A (structure of) input tensors.\n          state: A (structure of) state tensors and TensorArrays.\n          training: Python boolean. Indicates whether the layer should\n              behave in training mode or in inference mode. Only relevant\n              when `dropout` or `recurrent_dropout` is used.\n          name: Name scope for any created operations.\n\n        Returns:\n          `(outputs, next_state, next_inputs, finished)`.\n        """"""\n        batch_size = self._batch_size\n        beam_width = self._beam_width\n        end_token = self._end_token\n        length_penalty_weight = self._length_penalty_weight\n        coverage_penalty_weight = self._coverage_penalty_weight\n\n        with tf.name_scope(name or ""BeamSearchDecoderStep""):\n            cell_state = state.cell_state\n            inputs = tf.nest.map_structure(\n                lambda inp: self._merge_batch_beams(inp, s=inp.shape[2:]), inputs\n            )\n            cell_state = tf.nest.map_structure(\n                self._maybe_merge_batch_beams, cell_state, self._cell.state_size\n            )\n            cell_outputs, next_cell_state = self._cell(\n                inputs, cell_state, training=training\n            )\n            cell_outputs = tf.nest.map_structure(\n                lambda out: self._split_batch_beams(out, out.shape[1:]), cell_outputs\n            )\n            next_cell_state = tf.nest.pack_sequence_as(\n                cell_state, tf.nest.flatten(next_cell_state)\n            )\n            next_cell_state = tf.nest.map_structure(\n                self._maybe_split_batch_beams, next_cell_state, self._cell.state_size\n            )\n\n            if self._output_layer is not None:\n                cell_outputs = self._output_layer(cell_outputs)\n\n            beam_search_output, beam_search_state = _beam_search_step(\n                time=time,\n                logits=cell_outputs,\n                next_cell_state=next_cell_state,\n                beam_state=state,\n                batch_size=batch_size,\n                beam_width=beam_width,\n                end_token=end_token,\n                length_penalty_weight=length_penalty_weight,\n                coverage_penalty_weight=coverage_penalty_weight,\n            )\n\n            finished = beam_search_state.finished\n            sample_ids = beam_search_output.predicted_ids\n            next_inputs = tf.cond(\n                tf.reduce_all(finished),\n                lambda: self._start_inputs,\n                lambda: self._embedding_fn(sample_ids),\n            )\n\n        return (beam_search_output, beam_search_state, next_inputs, finished)\n\n\nclass BeamSearchDecoder(BeamSearchDecoderMixin, decoder.BaseDecoder):\n    # Note that the inheritance hierarchy is important here. The Mixin has to be\n    # the first parent class since we will use super().__init__(), and Mixin\n    # which is a object will properly invoke the __init__ method of other parent\n    # class.\n    """"""BeamSearch sampling decoder.\n\n    **NOTE** If you are using the `BeamSearchDecoder` with a cell wrapped in\n    `AttentionWrapper`, then you must ensure that:\n\n    - The encoder output has been tiled to `beam_width` via\n      `tfa.seq2seq.tile_batch` (NOT `tf.tile`).\n    - The `batch_size` argument passed to the `get_initial_state` method of\n      this wrapper is equal to `true_batch_size * beam_width`.\n    - The initial state created with `get_initial_state` above contains a\n      `cell_state` value containing properly tiled final state from the\n      encoder.\n\n    An example:\n\n    ```\n    tiled_encoder_outputs = tfa.seq2seq.tile_batch(\n        encoder_outputs, multiplier=beam_width)\n    tiled_encoder_final_state = tfa.seq2seq.tile_batch(\n        encoder_final_state, multiplier=beam_width)\n    tiled_sequence_length = tfa.seq2seq.tile_batch(\n        sequence_length, multiplier=beam_width)\n    attention_mechanism = MyFavoriteAttentionMechanism(\n        num_units=attention_depth,\n        memory=tiled_inputs,\n        memory_sequence_length=tiled_sequence_length)\n    attention_cell = AttentionWrapper(cell, attention_mechanism, ...)\n    decoder_initial_state = attention_cell.get_initial_state(\n        batch_size=true_batch_size * beam_width, dtype=dtype)\n    decoder_initial_state = decoder_initial_state.clone(\n        cell_state=tiled_encoder_final_state)\n    ```\n\n    Meanwhile, with `AttentionWrapper`, coverage penalty is suggested to use\n    when computing scores (https://arxiv.org/pdf/1609.08144.pdf). It encourages\n    the decoding to cover all inputs.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        cell: tf.keras.layers.Layer,\n        beam_width: int,\n        embedding_fn: Optional[Callable] = None,\n        output_layer: Optional[tf.keras.layers.Layer] = None,\n        length_penalty_weight: FloatTensorLike = 0.0,\n        coverage_penalty_weight: FloatTensorLike = 0.0,\n        reorder_tensor_arrays: bool = True,\n        **kwargs\n    ):\n        """"""Initialize the BeamSearchDecoder.\n\n        Args:\n          cell: A layer that implements the `tf.keras.layers.AbstractRNNCell`\n            interface.\n          beam_width:  Python integer, the number of beams.\n          embedding_fn: A callable that takes a `int32` `Tensor` of token IDs\n            and returns embedding tensors. If set, the `embedding` argument in\n            the decoder call should be set to `None`.\n          output_layer: (Optional) An instance of `tf.keras.layers.Layer`,\n            i.e., `tf.keras.layers.Dense`.  Optional layer to apply to the RNN\n            output prior to storing the result or sampling.\n          length_penalty_weight: Float weight to penalize length. Disabled with\n            0.0.\n          coverage_penalty_weight: Float weight to penalize the coverage of\n            source sentence. Disabled with 0.0.\n          reorder_tensor_arrays: If `True`, `TensorArray`s\' elements within the\n            cell state will be reordered according to the beam search path. If\n            the `TensorArray` can be reordered, the stacked form will be\n            returned. Otherwise, the `TensorArray` will be returned as is. Set\n            this flag to `False` if the cell state contains `TensorArray`s that\n            are not amenable to reordering.\n          **kwargs: Dict, other keyword arguments for initialization.\n        """"""\n        super().__init__(\n            cell,\n            beam_width,\n            output_layer=output_layer,\n            length_penalty_weight=length_penalty_weight,\n            coverage_penalty_weight=coverage_penalty_weight,\n            reorder_tensor_arrays=reorder_tensor_arrays,\n            **kwargs,\n        )\n\n        self._embedding_fn = embedding_fn\n\n    def initialize(self, embedding, start_tokens, end_token, initial_state):\n        """"""Initialize the decoder.\n\n        Args:\n          embedding: A `Tensor` (or `Variable`) to pass as the `params` argument\n            for `tf.nn.embedding_lookup`. This overrides `embedding_fn` set in\n            the constructor.\n          start_tokens: Start the decoding from these tokens.\n            A `int32` `Tensor` of shape `[batch_size]`.\n          end_token: The token that marks the end of decoding.\n            A `int32` scalar `Tensor`.\n          initial_state: The initial cell state as a (possibly nested) structure\n            of `Tensor` and `TensorArray`.\n\n        Returns:\n          `(finished, start_inputs, initial_state)`.\n\n        Raises:\n          ValueError: If `embedding` is `None` and `embedding_fn` was not set\n            in the constructor.\n          ValueError: If `start_tokens` is not a vector or `end_token` is not a\n            scalar.\n        """"""\n        if embedding is not None:\n            self._embedding_fn = lambda ids: tf.nn.embedding_lookup(embedding, ids)\n        elif self._embedding_fn is None:\n            raise ValueError(\n                ""You should either pass an embedding variable when calling the ""\n                ""BeamSearchDecoder or set embedding_fn in the constructor.""\n            )\n\n        self._start_tokens = tf.convert_to_tensor(\n            start_tokens, dtype=tf.int32, name=""start_tokens""\n        )\n        if self._start_tokens.shape.ndims != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        self._end_token = tf.convert_to_tensor(\n            end_token, dtype=tf.int32, name=""end_token""\n        )\n        if self._end_token.shape.ndims != 0:\n            raise ValueError(""end_token must be a scalar"")\n\n        self._batch_size = tf.size(start_tokens)\n        self._initial_cell_state = tf.nest.map_structure(\n            self._maybe_split_batch_beams, initial_state, self._cell.state_size\n        )\n        self._start_tokens = tf.tile(\n            tf.expand_dims(self._start_tokens, 1), [1, self._beam_width]\n        )\n        self._start_inputs = self._embedding_fn(self._start_tokens)\n\n        self._finished = tf.one_hot(\n            tf.zeros([self._batch_size], dtype=tf.int32),\n            depth=self._beam_width,\n            on_value=False,\n            off_value=True,\n            dtype=tf.bool,\n        )\n\n        finished, start_inputs = self._finished, self._start_inputs\n\n        dtype = tf.nest.flatten(self._initial_cell_state)[0].dtype\n        log_probs = tf.one_hot(  # shape(batch_sz, beam_sz)\n            tf.zeros([self._batch_size], dtype=tf.int32),\n            depth=self._beam_width,\n            on_value=tf.convert_to_tensor(0.0, dtype=dtype),\n            off_value=tf.convert_to_tensor(-np.Inf, dtype=dtype),\n            dtype=dtype,\n        )\n        init_attention_probs = get_attention_probs(\n            self._initial_cell_state, self._coverage_penalty_weight\n        )\n        if init_attention_probs is None:\n            init_attention_probs = ()\n\n        initial_state = BeamSearchDecoderState(\n            cell_state=self._initial_cell_state,\n            log_probs=log_probs,\n            finished=finished,\n            lengths=tf.zeros([self._batch_size, self._beam_width], dtype=tf.int64),\n            accumulated_attention_probs=init_attention_probs,\n        )\n\n        return (finished, start_inputs, initial_state)\n\n    @property\n    def output_dtype(self):\n        # Assume the dtype of the cell is the output_size structure\n        # containing the input_state\'s first component\'s dtype.\n        # Return that structure and int32 (the id)\n        dtype = tf.nest.flatten(self._initial_cell_state)[0].dtype\n        return BeamSearchDecoderOutput(\n            scores=tf.nest.map_structure(lambda _: dtype, self._rnn_output_size()),\n            predicted_ids=tf.int32,\n            parent_ids=tf.int32,\n        )\n\n    def call(\n        self, embedding, start_tokens, end_token, initial_state, training=None, **kwargs\n    ):\n        init_kwargs = kwargs\n        init_kwargs[""start_tokens""] = start_tokens\n        init_kwargs[""end_token""] = end_token\n        init_kwargs[""initial_state""] = initial_state\n        return decoder.dynamic_decode(\n            self,\n            output_time_major=self.output_time_major,\n            impute_finished=self.impute_finished,\n            maximum_iterations=self.maximum_iterations,\n            parallel_iterations=self.parallel_iterations,\n            swap_memory=self.swap_memory,\n            training=training,\n            decoder_init_input=embedding,\n            decoder_init_kwargs=init_kwargs,\n        )\n\n\ndef _beam_search_step(\n    time,\n    logits,\n    next_cell_state,\n    beam_state,\n    batch_size,\n    beam_width,\n    end_token,\n    length_penalty_weight,\n    coverage_penalty_weight,\n):\n    """"""Performs a single step of Beam Search Decoding.\n\n    Args:\n      time: Beam search time step, should start at 0. At time 0 we assume\n        that all beams are equal and consider only the first beam for\n        continuations.\n      logits: Logits at the current time step. A tensor of shape\n        `[batch_size, beam_width, vocab_size]`\n      next_cell_state: The next state from the cell, e.g. an instance of\n        AttentionWrapperState if the cell is attentional.\n      beam_state: Current state of the beam search.\n        An instance of `BeamSearchDecoderState`.\n      batch_size: The batch size for this input.\n      beam_width: Python int.  The size of the beams.\n      end_token: The int32 end token.\n      length_penalty_weight: Float weight to penalize length. Disabled with\n        0.0.\n      coverage_penalty_weight: Float weight to penalize the coverage of source\n        sentence. Disabled with 0.0.\n\n    Returns:\n      A new beam state.\n    """"""\n    static_batch_size = tf.get_static_value(batch_size)\n\n    # Calculate the current lengths of the predictions\n    prediction_lengths = beam_state.lengths\n    previously_finished = beam_state.finished\n    not_finished = tf.logical_not(previously_finished)\n\n    # Calculate the total log probs for the new hypotheses\n    # Final Shape: [batch_size, beam_width, vocab_size]\n    step_log_probs = tf.nn.log_softmax(logits)\n    step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished)\n    total_probs = tf.expand_dims(beam_state.log_probs, 2) + step_log_probs\n\n    # Calculate the continuation lengths by adding to all continuing beams.\n    vocab_size = logits.shape[-1] or tf.shape(logits)[-1]\n    lengths_to_add = tf.one_hot(\n        indices=tf.fill([batch_size, beam_width], end_token),\n        depth=vocab_size,\n        on_value=np.int64(0),\n        off_value=np.int64(1),\n        dtype=tf.int64,\n    )\n    add_mask = tf.cast(not_finished, tf.int64)\n    lengths_to_add *= tf.expand_dims(add_mask, 2)\n    new_prediction_lengths = lengths_to_add + tf.expand_dims(prediction_lengths, 2)\n\n    # Calculate the accumulated attention probabilities if coverage penalty is\n    # enabled.\n    accumulated_attention_probs = None\n    attention_probs = get_attention_probs(next_cell_state, coverage_penalty_weight)\n    if attention_probs is not None:\n        attention_probs *= tf.expand_dims(tf.cast(not_finished, tf.float32), 2)\n        accumulated_attention_probs = (\n            beam_state.accumulated_attention_probs + attention_probs\n        )\n\n    # Calculate the scores for each beam\n    scores = _get_scores(\n        log_probs=total_probs,\n        sequence_lengths=new_prediction_lengths,\n        length_penalty_weight=length_penalty_weight,\n        coverage_penalty_weight=coverage_penalty_weight,\n        finished=previously_finished,\n        accumulated_attention_probs=accumulated_attention_probs,\n    )\n\n    time = tf.convert_to_tensor(time, name=""time"")\n    # During the first time step we only consider the initial beam\n    scores_flat = tf.reshape(scores, [batch_size, -1])\n\n    # Pick the next beams according to the specified successors function\n    next_beam_size = tf.convert_to_tensor(beam_width, dtype=tf.int32, name=""beam_width"")\n    next_beam_scores, word_indices = tf.math.top_k(scores_flat, k=next_beam_size)\n\n    next_beam_scores.set_shape([static_batch_size, beam_width])\n    word_indices.set_shape([static_batch_size, beam_width])\n\n    # Pick out the probs, beam_ids, and states according to the chosen\n    # predictions\n    next_beam_probs = _tensor_gather_helper(\n        gather_indices=word_indices,\n        gather_from=total_probs,\n        batch_size=batch_size,\n        range_size=beam_width * vocab_size,\n        gather_shape=[-1],\n        name=""next_beam_probs"",\n    )\n    # Note: just doing the following\n    #   tf.to_int32(word_indices % vocab_size,\n    #       name=""next_beam_word_ids"")\n    # would be a lot cleaner but for reasons unclear, that hides the results of\n    # the op which prevents capturing it with tfdbg debug ops.\n    raw_next_word_ids = tf.math.floormod(\n        word_indices, vocab_size, name=""next_beam_word_ids""\n    )\n    next_word_ids = tf.cast(raw_next_word_ids, tf.int32)\n    next_beam_ids = tf.cast(\n        word_indices / vocab_size, tf.int32, name=""next_beam_parent_ids""\n    )\n\n    # Append new ids to current predictions\n    previously_finished = _tensor_gather_helper(\n        gather_indices=next_beam_ids,\n        gather_from=previously_finished,\n        batch_size=batch_size,\n        range_size=beam_width,\n        gather_shape=[-1],\n    )\n    next_finished = tf.logical_or(\n        previously_finished,\n        tf.equal(next_word_ids, end_token),\n        name=""next_beam_finished"",\n    )\n\n    # Calculate the length of the next predictions.\n    # 1. Finished beams remain unchanged.\n    # 2. Beams that are now finished (EOS predicted) have their length\n    #    increased by 1.\n    # 3. Beams that are not yet finished have their length increased by 1.\n    lengths_to_add = tf.cast(tf.logical_not(previously_finished), tf.int64)\n    next_prediction_len = _tensor_gather_helper(\n        gather_indices=next_beam_ids,\n        gather_from=beam_state.lengths,\n        batch_size=batch_size,\n        range_size=beam_width,\n        gather_shape=[-1],\n    )\n    next_prediction_len += lengths_to_add\n    next_accumulated_attention_probs = ()\n    if accumulated_attention_probs is not None:\n        next_accumulated_attention_probs = _tensor_gather_helper(\n            gather_indices=next_beam_ids,\n            gather_from=accumulated_attention_probs,\n            batch_size=batch_size,\n            range_size=beam_width,\n            gather_shape=[batch_size * beam_width, -1],\n            name=""next_accumulated_attention_probs"",\n        )\n\n    # Pick out the cell_states according to the next_beam_ids. We use a\n    # different gather_shape here because the cell_state tensors, i.e.\n    # the tensors that would be gathered from, all have dimension\n    # greater than two and we need to preserve those dimensions.\n    next_cell_state = tf.nest.map_structure(\n        lambda gather_from: _maybe_tensor_gather_helper(\n            gather_indices=next_beam_ids,\n            gather_from=gather_from,\n            batch_size=batch_size,\n            range_size=beam_width,\n            gather_shape=[batch_size * beam_width, -1],\n        ),\n        next_cell_state,\n    )\n\n    next_state = BeamSearchDecoderState(\n        cell_state=next_cell_state,\n        log_probs=next_beam_probs,\n        lengths=next_prediction_len,\n        finished=next_finished,\n        accumulated_attention_probs=next_accumulated_attention_probs,\n    )\n\n    output = BeamSearchDecoderOutput(\n        scores=next_beam_scores, predicted_ids=next_word_ids, parent_ids=next_beam_ids\n    )\n\n    return output, next_state\n\n\ndef get_attention_probs(next_cell_state, coverage_penalty_weight):\n    """"""Get attention probabilities from the cell state.\n\n    Args:\n      next_cell_state: The next state from the cell, e.g. an instance of\n        AttentionWrapperState if the cell is attentional.\n      coverage_penalty_weight: Float weight to penalize the coverage of source\n        sentence. Disabled with 0.0.\n\n    Returns:\n      The attention probabilities with shape\n        `[batch_size, beam_width, max_time]` if coverage penalty is enabled.\n        Otherwise, returns None.\n\n    Raises:\n      ValueError: If no cell is attentional but coverage penalty is enabled.\n    """"""\n    if coverage_penalty_weight == 0.0:\n        return None\n\n    # Attention probabilities of each attention layer. Each with shape\n    # `[batch_size, beam_width, max_time]`.\n    probs_per_attn_layer = []\n    if isinstance(next_cell_state, attention_wrapper.AttentionWrapperState):\n        probs_per_attn_layer = [attention_probs_from_attn_state(next_cell_state)]\n    elif isinstance(next_cell_state, tuple):\n        for state in next_cell_state:\n            if isinstance(state, attention_wrapper.AttentionWrapperState):\n                probs_per_attn_layer.append(attention_probs_from_attn_state(state))\n\n    if not probs_per_attn_layer:\n        raise ValueError(\n            ""coverage_penalty_weight must be 0.0 if no cell is attentional.""\n        )\n\n    if len(probs_per_attn_layer) == 1:\n        attention_probs = probs_per_attn_layer[0]\n    else:\n        # Calculate the average attention probabilities from all attention\n        # layers.\n        attention_probs = [tf.expand_dims(prob, -1) for prob in probs_per_attn_layer]\n        attention_probs = tf.concat(attention_probs, -1)\n        attention_probs = tf.reduce_mean(attention_probs, -1)\n\n    return attention_probs\n\n\ndef _get_scores(\n    log_probs,\n    sequence_lengths,\n    length_penalty_weight,\n    coverage_penalty_weight,\n    finished,\n    accumulated_attention_probs,\n):\n    """"""Calculates scores for beam search hypotheses.\n\n    Args:\n      log_probs: The log probabilities with shape\n        `[batch_size, beam_width, vocab_size]`.\n      sequence_lengths: The array of sequence lengths.\n      length_penalty_weight: Float weight to penalize length. Disabled with\n        0.0.\n      coverage_penalty_weight: Float weight to penalize the coverage of source\n        sentence. Disabled with 0.0.\n      finished: A boolean tensor of shape `[batch_size, beam_width]` that\n        specifies which elements in the beam are finished already.\n      accumulated_attention_probs: Accumulated attention probabilities up to\n        the current time step, with shape `[batch_size, beam_width, max_time]`\n        if coverage_penalty_weight is not 0.0.\n\n    Returns:\n      The scores normalized by the length_penalty and coverage_penalty.\n\n    Raises:\n      ValueError: accumulated_attention_probs is None when coverage penalty is\n        enabled.\n    """"""\n    length_penalty_ = _length_penalty(\n        sequence_lengths=sequence_lengths, penalty_factor=length_penalty_weight\n    )\n    length_penalty_ = tf.cast(length_penalty_, dtype=log_probs.dtype)\n    scores = log_probs / length_penalty_\n\n    coverage_penalty_weight = tf.convert_to_tensor(\n        coverage_penalty_weight, name=""coverage_penalty_weight""\n    )\n    if coverage_penalty_weight.shape.ndims != 0:\n        raise ValueError(\n            ""coverage_penalty_weight should be a scalar, ""\n            ""but saw shape: %s"" % coverage_penalty_weight.shape\n        )\n\n    if tf.get_static_value(coverage_penalty_weight) == 0.0:\n        return scores\n\n    if accumulated_attention_probs is None:\n        raise ValueError(\n            ""accumulated_attention_probs can be None only if coverage penalty ""\n            ""is disabled.""\n        )\n\n    # Add source sequence length mask before computing coverage penalty.\n    accumulated_attention_probs = tf.where(\n        tf.equal(accumulated_attention_probs, 0.0),\n        tf.ones_like(accumulated_attention_probs),\n        accumulated_attention_probs,\n    )\n\n    # coverage penalty =\n    #     sum over `max_time` {log(min(accumulated_attention_probs, 1.0))}\n    coverage_penalty = tf.reduce_sum(\n        tf.math.log(tf.minimum(accumulated_attention_probs, 1.0)), 2\n    )\n    # Apply coverage penalty to finished predictions.\n    coverage_penalty *= tf.cast(finished, tf.float32)\n    weighted_coverage_penalty = coverage_penalty * coverage_penalty_weight\n    # Reshape from [batch_size, beam_width] to [batch_size, beam_width, 1]\n    weighted_coverage_penalty = tf.expand_dims(weighted_coverage_penalty, 2)\n    return scores + weighted_coverage_penalty\n\n\ndef attention_probs_from_attn_state(attention_state):\n    """"""Calculates the average attention probabilities.\n\n    Args:\n      attention_state: An instance of `AttentionWrapperState`.\n\n    Returns:\n      The attention probabilities in the given AttentionWrapperState.\n      If there\'re multiple attention mechanisms, return the average value from\n      all attention mechanisms.\n    """"""\n    # Attention probabilities over time steps, with shape\n    # `[batch_size, beam_width, max_time]`.\n    attention_probs = attention_state.alignments\n    if isinstance(attention_probs, tuple):\n        attention_probs = [tf.expand_dims(prob, -1) for prob in attention_probs]\n        attention_probs = tf.concat(attention_probs, -1)\n        attention_probs = tf.reduce_mean(attention_probs, -1)\n    return attention_probs\n\n\ndef _length_penalty(sequence_lengths, penalty_factor):\n    """"""Calculates the length penalty. See https://arxiv.org/abs/1609.08144.\n\n    Returns the length penalty tensor:\n    ```\n    [(5+sequence_lengths)/6]**penalty_factor\n    ```\n    where all operations are performed element-wise.\n\n    Args:\n      sequence_lengths: `Tensor`, the sequence lengths of each hypotheses.\n      penalty_factor: A scalar that weights the length penalty.\n\n    Returns:\n      If the penalty is `0`, returns the scalar `1.0`.  Otherwise returns\n      the length penalty factor, a tensor with the same shape as\n      `sequence_lengths`.\n    """"""\n    penalty_factor = tf.convert_to_tensor(penalty_factor, name=""penalty_factor"")\n    penalty_factor.set_shape(())  # penalty should be a scalar.\n    static_penalty = tf.get_static_value(penalty_factor)\n    if static_penalty is not None and static_penalty == 0:\n        return 1.0\n    return tf.math.divide(\n        (5.0 + tf.cast(sequence_lengths, tf.float32)) ** penalty_factor,\n        (5.0 + 1.0) ** penalty_factor,\n    )\n\n\ndef _mask_probs(probs, eos_token, finished):\n    """"""Masks log probabilities.\n\n    The result is that finished beams allocate all probability mass to eos and\n    unfinished beams remain unchanged.\n\n    Args:\n      probs: Log probabilities of shape `[batch_size, beam_width, vocab_size]`\n      eos_token: An int32 id corresponding to the EOS token to allocate\n        probability to.\n      finished: A boolean tensor of shape `[batch_size, beam_width]` that\n        specifies which elements in the beam are finished already.\n\n    Returns:\n      A tensor of shape `[batch_size, beam_width, vocab_size]`, where\n      unfinished beams stay unchanged and finished beams are replaced with a\n      tensor with all probability on the EOS token.\n    """"""\n    vocab_size = tf.shape(probs)[2]\n    # All finished examples are replaced with a vector that has all\n    # probability on EOS\n    finished_row = tf.one_hot(\n        eos_token,\n        vocab_size,\n        dtype=probs.dtype,\n        on_value=tf.convert_to_tensor(0.0, dtype=probs.dtype),\n        off_value=probs.dtype.min,\n    )\n    finished_probs = tf.tile(\n        tf.reshape(finished_row, [1, 1, -1]), tf.concat([tf.shape(finished), [1]], 0)\n    )\n    finished_mask = tf.tile(tf.expand_dims(finished, 2), [1, 1, vocab_size])\n\n    return tf.where(finished_mask, finished_probs, probs)\n\n\ndef _maybe_tensor_gather_helper(\n    gather_indices, gather_from, batch_size, range_size, gather_shape\n):\n    """"""Maybe applies _tensor_gather_helper.\n\n    This applies _tensor_gather_helper when the gather_from dims is at least as\n    big as the length of gather_shape. This is used in conjunction with nest so\n    that we don\'t apply _tensor_gather_helper to inapplicable values like\n    scalars.\n\n    Args:\n      gather_indices: The tensor indices that we use to gather.\n      gather_from: The tensor that we are gathering from.\n      batch_size: The batch size.\n      range_size: The number of values in each range. Likely equal to\n        beam_width.\n      gather_shape: What we should reshape gather_from to in order to preserve\n        the correct values. An example is when gather_from is the attention\n        from an AttentionWrapperState with shape\n        [batch_size, beam_width, attention_size]. There, we want to preserve\n        the attention_size elements, so gather_shape is\n        [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n        attention_size as desired.\n\n    Returns:\n      output: Gathered tensor of shape\n        tf.shape(gather_from)[:1+len(gather_shape)] or the original tensor if\n        its dimensions are too small.\n    """"""\n    if isinstance(gather_from, tf.TensorArray):\n        return gather_from\n    _check_ndims(gather_from)\n    if gather_from.shape.ndims >= len(gather_shape):\n        return _tensor_gather_helper(\n            gather_indices=gather_indices,\n            gather_from=gather_from,\n            batch_size=batch_size,\n            range_size=range_size,\n            gather_shape=gather_shape,\n        )\n    else:\n        return gather_from\n\n\ndef _tensor_gather_helper(\n    gather_indices, gather_from, batch_size, range_size, gather_shape, name=None\n):\n    """"""Helper for gathering the right indices from the tensor.\n\n    This works by reshaping gather_from to gather_shape (e.g. [-1]) and then\n    gathering from that according to the gather_indices, which are offset by\n    the right amounts in order to preserve the batch order.\n\n    Args:\n      gather_indices: The tensor indices that we use to gather.\n      gather_from: The tensor that we are gathering from.\n      batch_size: The input batch size.\n      range_size: The number of values in each range. Likely equal to\n        beam_width.\n      gather_shape: What we should reshape gather_from to in order to preserve\n        the correct values. An example is when gather_from is the attention\n        from an AttentionWrapperState with shape\n        [batch_size, beam_width, attention_size]. There, we want to preserve\n        the attention_size elements, so gather_shape is\n        [batch_size * beam_width, -1]. Then, upon reshape, we still have the\n        attention_size as desired.\n      name: The tensor name for set of operations. By default this is\n        \'tensor_gather_helper\'. The final output is named \'output\'.\n\n    Returns:\n      output: Gathered tensor of shape\n        tf.shape(gather_from)[:1+len(gather_shape)]\n    """"""\n    with tf.name_scope(name or ""tensor_gather_helper""):\n        range_ = tf.expand_dims(tf.range(batch_size) * range_size, 1)\n        gather_indices = tf.reshape(gather_indices + range_, [-1])\n        output = tf.gather(tf.reshape(gather_from, gather_shape), gather_indices)\n        final_shape = tf.shape(gather_from)[: 1 + len(gather_shape)]\n        static_batch_size = tf.get_static_value(batch_size)\n        final_static_shape = tf.TensorShape([static_batch_size]).concatenate(\n            gather_from.shape[1 : 1 + len(gather_shape)]\n        )\n        output = tf.reshape(output, final_shape, name=""output"")\n        output.set_shape(final_static_shape)\n        return output\n'"
tensorflow_addons/seq2seq/decoder.py,47,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Seq2seq layer operations for use in neural networks.""""""\n\nimport abc\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import TensorLike\nfrom typeguard import typechecked\nfrom typing import Any, Optional, Tuple, Union\n\n# TODO: Find public API alternatives to these\nfrom tensorflow.python.ops import control_flow_util\n\n\nclass Decoder(metaclass=abc.ABCMeta):\n    """"""An RNN Decoder abstract interface object.\n\n    Concepts used by this interface:\n    - `inputs`: (structure of) tensors and TensorArrays that is passed as input\n      to the RNNCell composing the decoder, at each time step.\n    - `state`: (structure of) tensors and TensorArrays that is passed to the\n      RNNCell instance as the state.\n    - `finished`: boolean tensor telling whether each sequence in the batch is\n      finished.\n    - `training`: boolean whether it should behave in training mode or in\n      inference mode.\n    - `outputs`: Instance of BasicDecoderOutput. Result of the decoding, at\n      each time step.\n    """"""\n\n    @property\n    def batch_size(self):\n        """"""The batch size of input values.""""""\n        raise NotImplementedError\n\n    @property\n    def output_size(self):\n        """"""A (possibly nested tuple of...) integer[s] or `TensorShape`\n        object[s].""""""\n        raise NotImplementedError\n\n    @property\n    def output_dtype(self):\n        """"""A (possibly nested tuple of...) dtype[s].""""""\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def initialize(self, name=None):\n        """"""Called before any decoding iterations.\n\n        This methods must compute initial input values and initial state.\n\n        Args:\n          name: Name scope for any created operations.\n\n        Returns:\n          `(finished, initial_inputs, initial_state)`: initial values of\n          \'finished\' flags, inputs and state.\n        """"""\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def step(self, time, inputs, state, training=None, name=None):\n        """"""Called per step of decoding (but only once for dynamic decoding).\n\n        Args:\n          time: Scalar `int32` tensor. Current step number.\n          inputs: RNNCell input (possibly nested tuple of) tensor[s] for this\n            time step.\n          state: RNNCell state (possibly nested tuple of) tensor[s] from\n            previous time step.\n          training: Python boolean. Indicates whether the layer should behave\n            in training  mode or in inference mode. Only relevant\n            when `dropout` or `recurrent_dropout` is used.\n          name: Name scope for any created operations.\n\n        Returns:\n          `(outputs, next_state, next_inputs, finished)`: `outputs` is an\n          object containing the decoder output, `next_state` is a (structure\n          of) state tensors and TensorArrays, `next_inputs` is the tensor that\n          should be used as input for the next step, `finished` is a boolean\n          tensor telling whether the sequence is complete, for each sequence in\n          the batch.\n        """"""\n        raise NotImplementedError\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        raise NotImplementedError\n\n    @property\n    def tracks_own_finished(self):\n        """"""Describes whether the Decoder keeps track of finished states.\n\n        Most decoders will emit a true/false `finished` value independently\n        at each time step.  In this case, the `dynamic_decode` function keeps\n        track of which batch entries are already finished, and performs a\n        logical OR to insert new batches to the finished set.\n\n        Some decoders, however, shuffle batches / beams between time steps and\n        `dynamic_decode` will mix up the finished state across these entries\n        because it does not track the reshuffle across time steps. In this\n        case, it is up to the decoder to declare that it will keep track of its\n        own finished state by setting this property to `True`.\n\n        Returns:\n          Python bool.\n        """"""\n        return False\n\n\nclass BaseDecoder(tf.keras.layers.Layer):\n    """"""An RNN Decoder that is based on a Keras layer.\n\n    Concepts used by this interface:\n    - `inputs`: (structure of) tensors and TensorArrays that is passed as input\n      to the RNNCell composing the decoder, at each time step.\n    - `state`: (structure of) tensors and TensorArrays that is passed to the\n      RNNCell instance as the state.\n    - `memory`: (sturecute of) tensors that is usually the full output of the\n      encoder, which will be used for the attention wrapper for the RNNCell.\n    - `finished`: boolean tensor telling whether each sequence in the batch is\n      finished.\n    - `training`: boolean whether it should behave in training mode or in\n      inference mode.\n    - `outputs`: Instance of BasicDecoderOutput. Result of the decoding, at\n      each time step.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        output_time_major: bool = False,\n        impute_finished: bool = False,\n        maximum_iterations: Optional[TensorLike] = None,\n        parallel_iterations: int = 32,\n        swap_memory: bool = False,\n        **kwargs\n    ):\n        self.output_time_major = output_time_major\n        self.impute_finished = impute_finished\n        self.maximum_iterations = maximum_iterations\n        self.parallel_iterations = parallel_iterations\n        self.swap_memory = swap_memory\n        super().__init__(**kwargs)\n\n    def call(self, inputs, initial_state=None, training=None, **kwargs):\n        init_kwargs = kwargs\n        init_kwargs[""initial_state""] = initial_state\n        return dynamic_decode(\n            self,\n            output_time_major=self.output_time_major,\n            impute_finished=self.impute_finished,\n            maximum_iterations=self.maximum_iterations,\n            parallel_iterations=self.parallel_iterations,\n            swap_memory=self.swap_memory,\n            training=training,\n            decoder_init_input=inputs,\n            decoder_init_kwargs=init_kwargs,\n        )\n\n    @property\n    def batch_size(self):\n        """"""The batch size of input values.""""""\n        raise NotImplementedError\n\n    @property\n    def output_size(self):\n        """"""A (possibly nested tuple of...) integer[s] or `TensorShape`\n        object[s].""""""\n        raise NotImplementedError\n\n    @property\n    def output_dtype(self):\n        """"""A (possibly nested tuple of...) dtype[s].""""""\n        raise NotImplementedError\n\n    def initialize(self, inputs, initial_state=None, **kwargs):\n        """"""Called before any decoding iterations.\n\n        This methods must compute initial input values and initial state.\n\n        Args:\n          inputs: (structure of) tensors that contains the input for the\n            decoder. In the normal case, it\'s a tensor with shape\n            [batch, timestep, embedding].\n          initial_state: (structure of) tensors that contains the initial state\n            for the RNNCell.\n          **kwargs: Other arguments that are passed in from layer.call()\n            method. It could contains item like input sequence_length, or\n            masking for input.\n\n        Returns:\n          `(finished, initial_inputs, initial_state)`: initial values of\n          \'finished\' flags, inputs and state.\n        """"""\n        raise NotImplementedError\n\n    def step(self, time, inputs, state, training):\n        """"""Called per step of decoding (but only once for dynamic decoding).\n\n        Args:\n          time: Scalar `int32` tensor. Current step number.\n          inputs: RNNCell input (possibly nested tuple of) tensor[s] for this\n            time step.\n          state: RNNCell state (possibly nested tuple of) tensor[s] from\n            previous time step.\n          training: Python boolean. Indicates whether the layer should\n            behave in training mode or in inference mode.\n\n        Returns:\n          `(outputs, next_state, next_inputs, finished)`: `outputs` is an\n          object containing the decoder output, `next_state` is a\n          (structure of) state tensors and TensorArrays, `next_inputs` is the\n          tensor that should be used as input for the next step, `finished` is\n          a boolean tensor telling whether the sequence is complete, for each\n          sequence in the batch.\n        """"""\n        raise NotImplementedError\n\n    def finalize(self, outputs, final_state, sequence_lengths):\n        raise NotImplementedError\n\n    @property\n    def tracks_own_finished(self):\n        """"""Describes whether the Decoder keeps track of finished states.\n\n        Most decoders will emit a true/false `finished` value independently\n        at each time step.  In this case, the `dynamic_decode` function keeps\n        track of which batch entries are already finished, and performs a\n        logical OR to insert new batches to the finished set.\n\n        Some decoders, however, shuffle batches / beams between time steps and\n        `dynamic_decode` will mix up the finished state across these entries\n        because it does not track the reshuffle across time steps. In this\n        case, it is up to the decoder to declare that it will keep track of its\n        own finished state by setting this property to `True`.\n\n        Returns:\n          Python bool.\n        """"""\n        return False\n\n    # TODO(scottzhu): Add build/get_config/from_config and other layer methods.\n\n\n@typechecked\ndef dynamic_decode(\n    decoder: Union[Decoder, BaseDecoder],\n    output_time_major: bool = False,\n    impute_finished: bool = False,\n    maximum_iterations: Optional[TensorLike] = None,\n    parallel_iterations: int = 32,\n    swap_memory: bool = False,\n    training: Optional[bool] = None,\n    scope: Optional[str] = None,\n    **kwargs\n) -> Tuple[Any, Any, Any]:\n    """"""Perform dynamic decoding with `decoder`.\n\n    Calls initialize() once and step() repeatedly on the Decoder object.\n\n    Args:\n      decoder: A `Decoder` instance.\n      output_time_major: Python boolean.  Default: `False` (batch major). If\n        `True`, outputs are returned as time major tensors (this mode is\n        faster). Otherwise, outputs are returned as batch major tensors (this\n        adds extra time to the computation).\n      impute_finished: Python boolean.  If `True`, then states for batch\n        entries which are marked as finished get copied through and the\n        corresponding outputs get zeroed out.  This causes some slowdown at\n        each time step, but ensures that the final state and outputs have\n        the correct values and that backprop ignores time steps that were\n        marked as finished.\n      maximum_iterations: `int32` scalar, maximum allowed number of decoding\n         steps.  Default is `None` (decode until the decoder is fully done).\n      parallel_iterations: Argument passed to `tf.while_loop`.\n      swap_memory: Argument passed to `tf.while_loop`.\n      training: Python boolean. Indicates whether the layer should behave\n          in training  mode or in inference mode. Only relevant\n          when `dropout` or `recurrent_dropout` is used.\n      scope: Optional name scope to use.\n      **kwargs: dict, other keyword arguments for dynamic_decode. It might\n        contain arguments for `BaseDecoder` to initialize, which takes all\n        tensor inputs during call().\n\n    Returns:\n      `(final_outputs, final_state, final_sequence_lengths)`.\n\n    Raises:\n      ValueError: if `maximum_iterations` is provided but is not a scalar.\n    """"""\n    with tf.name_scope(scope or ""decoder""):\n        is_xla = not tf.executing_eagerly() and control_flow_util.GraphOrParentsInXlaContext(\n            tf.compat.v1.get_default_graph()\n        )\n\n        if maximum_iterations is not None:\n            maximum_iterations = tf.convert_to_tensor(\n                maximum_iterations, dtype=tf.int32, name=""maximum_iterations""\n            )\n            if maximum_iterations.shape.ndims != 0:\n                raise ValueError(""maximum_iterations must be a scalar"")\n        elif is_xla:\n            raise ValueError(""maximum_iterations is required for XLA compilation."")\n\n        if isinstance(decoder, Decoder):\n            initial_finished, initial_inputs, initial_state = decoder.initialize()\n        else:\n            # For BaseDecoder that takes tensor inputs during call.\n            decoder_init_input = kwargs.pop(""decoder_init_input"", None)\n            decoder_init_kwargs = kwargs.pop(""decoder_init_kwargs"", {})\n            initial_finished, initial_inputs, initial_state = decoder.initialize(\n                decoder_init_input, **decoder_init_kwargs\n            )\n\n        zero_outputs = tf.nest.map_structure(\n            lambda shape, dtype: tf.zeros(\n                _prepend_batch(decoder.batch_size, shape), dtype=dtype\n            ),\n            decoder.output_size,\n            decoder.output_dtype,\n        )\n\n        if maximum_iterations is not None:\n            initial_finished = tf.logical_or(initial_finished, 0 >= maximum_iterations)\n        initial_sequence_lengths = tf.zeros_like(initial_finished, dtype=tf.int32)\n        initial_time = tf.constant(0, dtype=tf.int32)\n\n        def _shape(batch_size, from_shape):\n            if not isinstance(from_shape, tf.TensorShape) or from_shape.ndims == 0:\n                return None\n            else:\n                batch_size = tf.get_static_value(\n                    tf.convert_to_tensor(batch_size, name=""batch_size"")\n                )\n                return tf.TensorShape([batch_size]).concatenate(from_shape)\n\n        dynamic_size = maximum_iterations is None or not is_xla\n\n        def _create_ta(s, d):\n            return tf.TensorArray(\n                dtype=d,\n                size=0 if dynamic_size else maximum_iterations,\n                dynamic_size=dynamic_size,\n                element_shape=_shape(decoder.batch_size, s),\n            )\n\n        initial_outputs_ta = tf.nest.map_structure(\n            _create_ta, decoder.output_size, decoder.output_dtype\n        )\n\n        def condition(\n            unused_time,\n            unused_outputs_ta,\n            unused_state,\n            unused_inputs,\n            finished,\n            unused_sequence_lengths,\n        ):\n            return tf.logical_not(tf.reduce_all(finished))\n\n        def body(time, outputs_ta, state, inputs, finished, sequence_lengths):\n            """"""Internal while_loop body.\n\n            Args:\n              time: scalar int32 tensor.\n              outputs_ta: structure of TensorArray.\n              state: (structure of) state tensors and TensorArrays.\n              inputs: (structure of) input tensors.\n              finished: bool tensor (keeping track of what\'s finished).\n              sequence_lengths: int32 tensor (keeping track of time of finish).\n\n            Returns:\n              `(time + 1, outputs_ta, next_state, next_inputs, next_finished,\n                next_sequence_lengths)`.\n              ```\n            """"""\n            (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(\n                time, inputs, state, training\n            )\n            decoder_state_sequence_lengths = False\n            if decoder.tracks_own_finished:\n                next_finished = decoder_finished\n                lengths = getattr(decoder_state, ""lengths"", None)\n                if lengths is not None:\n                    # sequence lengths are provided by decoder_state.lengths;\n                    # overwrite our sequence lengths.\n                    decoder_state_sequence_lengths = True\n                    sequence_lengths = tf.cast(lengths, tf.int32)\n            else:\n                next_finished = tf.logical_or(decoder_finished, finished)\n\n            if decoder_state_sequence_lengths:\n                # Just pass something through the loop; at the next iteration\n                # we\'ll pull the sequence lengths from the decoder_state again.\n                next_sequence_lengths = sequence_lengths\n            else:\n                next_sequence_lengths = tf.where(\n                    tf.logical_not(finished),\n                    tf.fill(tf.shape(sequence_lengths), time + 1),\n                    sequence_lengths,\n                )\n\n            tf.nest.assert_same_structure(state, decoder_state)\n            tf.nest.assert_same_structure(outputs_ta, next_outputs)\n            tf.nest.assert_same_structure(inputs, next_inputs)\n\n            # Zero out output values past finish\n            if impute_finished:\n\n                def zero_out_finished(out, zero):\n                    if finished.shape.rank < zero.shape.rank:\n                        broadcast_finished = tf.broadcast_to(\n                            tf.expand_dims(finished, axis=-1), zero.shape\n                        )\n                        return tf.where(broadcast_finished, zero, out)\n                    else:\n                        return tf.where(finished, zero, out)\n\n                emit = tf.nest.map_structure(\n                    zero_out_finished, next_outputs, zero_outputs\n                )\n            else:\n                emit = next_outputs\n\n            # Copy through states past finish\n            def _maybe_copy_state(new, cur):\n                # TensorArrays and scalar states get passed through.\n                if isinstance(cur, tf.TensorArray):\n                    pass_through = True\n                else:\n                    new.set_shape(cur.shape)\n                    pass_through = new.shape.ndims == 0\n                if not pass_through:\n                    broadcast_finished = tf.broadcast_to(\n                        tf.expand_dims(finished, axis=-1), new.shape\n                    )\n                    return tf.where(broadcast_finished, cur, new)\n                else:\n                    return new\n\n            if impute_finished:\n                next_state = tf.nest.map_structure(\n                    _maybe_copy_state, decoder_state, state\n                )\n            else:\n                next_state = decoder_state\n\n            outputs_ta = tf.nest.map_structure(\n                lambda ta, out: ta.write(time, out), outputs_ta, emit\n            )\n            return (\n                time + 1,\n                outputs_ta,\n                next_state,\n                next_inputs,\n                next_finished,\n                next_sequence_lengths,\n            )\n\n        res = tf.while_loop(\n            condition,\n            body,\n            loop_vars=(\n                initial_time,\n                initial_outputs_ta,\n                initial_state,\n                initial_inputs,\n                initial_finished,\n                initial_sequence_lengths,\n            ),\n            parallel_iterations=parallel_iterations,\n            maximum_iterations=maximum_iterations,\n            swap_memory=swap_memory,\n        )\n\n        final_outputs_ta = res[1]\n        final_state = res[2]\n        final_sequence_lengths = res[5]\n\n        final_outputs = tf.nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n\n        try:\n            final_outputs, final_state = decoder.finalize(\n                final_outputs, final_state, final_sequence_lengths\n            )\n        except NotImplementedError:\n            pass\n\n        if not output_time_major:\n            final_outputs = tf.nest.map_structure(_transpose_batch_time, final_outputs)\n\n    return final_outputs, final_state, final_sequence_lengths\n\n\ndef _prepend_batch(batch_size, shape):\n    """"""Prepends the batch dimension to the shape.\n\n    If the batch_size value is known statically, this function returns a\n    TensorShape, otherwise a Tensor.\n    """"""\n    if isinstance(batch_size, tf.Tensor):\n        static_batch_size = tf.get_static_value(batch_size)\n    else:\n        static_batch_size = batch_size\n    if static_batch_size is None:\n        return tf.concat(([batch_size], shape), axis=0)\n    return [static_batch_size] + shape\n\n\ndef _transpose_batch_time(tensor):\n    """"""Transposes the batch and time dimension of tensor if its rank is at\n    least 2.""""""\n    shape = tensor.shape\n    if shape.rank is not None and shape.rank < 2:\n        return tensor\n    perm = tf.concat(([1, 0], tf.range(2, tf.rank(tensor))), axis=0)\n    return tf.transpose(tensor, perm)\n'"
tensorflow_addons/seq2seq/loss.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Seq2seq loss operations for use in sequence models.""""""\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import TensorLike\n\nfrom typeguard import typechecked\nfrom typing import Callable, Optional\n\n\ndef sequence_loss(\n    logits: TensorLike,\n    targets: TensorLike,\n    weights: TensorLike,\n    average_across_timesteps: bool = True,\n    average_across_batch: bool = True,\n    sum_over_timesteps: bool = False,\n    sum_over_batch: bool = False,\n    softmax_loss_function: Optional[Callable] = None,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Weighted cross-entropy loss for a sequence of logits.\n\n    Depending on the values of `average_across_timesteps` /\n    `sum_over_timesteps` and `average_across_batch` / `sum_over_batch`, the\n    return Tensor will have rank 0, 1, or 2 as these arguments reduce the\n    cross-entropy at each target, which has shape\n    `[batch_size, sequence_length]`, over their respective dimensions. For\n    example, if `average_across_timesteps` is `True` and `average_across_batch`\n    is `False`, then the return Tensor will have shape `[batch_size]`.\n\n    Note that `average_across_timesteps` and `sum_over_timesteps` cannot be\n    True at same time. Same for `average_across_batch` and `sum_over_batch`.\n\n    The recommended loss reduction in tf 2.0 has been changed to sum_over,\n    instead of weighted average. User are recommend to use `sum_over_timesteps`\n    and `sum_over_batch` for reduction.\n\n    Args:\n      logits: A Tensor of shape\n        `[batch_size, sequence_length, num_decoder_symbols]` and dtype float.\n        The logits correspond to the prediction across all classes at each\n        timestep.\n      targets: A Tensor of shape `[batch_size, sequence_length]` and dtype\n        int. The target represents the true class at each timestep.\n      weights: A Tensor of shape `[batch_size, sequence_length]` and dtype\n        float. `weights` constitutes the weighting of each prediction in the\n        sequence. When using `weights` as masking, set all valid timesteps to 1\n        and all padded timesteps to 0, e.g. a mask returned by\n        `tf.sequence_mask`.\n      average_across_timesteps: If set, sum the cost across the sequence\n        dimension and divide the cost by the total label weight across\n        timesteps.\n      average_across_batch: If set, sum the cost across the batch dimension and\n        divide the returned cost by the batch size.\n      sum_over_timesteps: If set, sum the cost across the sequence dimension\n        and divide the size of the sequence. Note that any element with 0\n        weights will be excluded from size calculation.\n      sum_over_batch: if set, sum the cost across the batch dimension and\n        divide the total cost by the batch size. Not that any element with 0\n        weights will be excluded from size calculation.\n      softmax_loss_function: Function (labels, logits) -> loss-batch\n        to be used instead of the standard softmax (the default if this is\n        None). **Note that to avoid confusion, it is required for the function\n        to accept named arguments.**\n      name: Optional name for this operation, defaults to ""sequence_loss"".\n\n    Returns:\n      A float Tensor of rank 0, 1, or 2 depending on the\n      `average_across_timesteps` and `average_across_batch` arguments. By\n      default, it has rank 0 (scalar) and is the weighted average cross-entropy\n      (log-perplexity) per symbol.\n\n    Raises:\n      ValueError: logits does not have 3 dimensions or targets does not have 2\n                  dimensions or weights does not have 2 dimensions.\n    """"""\n    if len(logits.shape) != 3:\n        raise ValueError(\n            ""Logits must be a "" ""[batch_size x sequence_length x logits] tensor""\n        )\n\n    targets_rank = len(targets.shape)\n    if targets_rank != 2 and targets_rank != 3:\n        raise ValueError(\n            ""Targets must be either a [batch_size x sequence_length] tensor ""\n            + ""where each element contains the labels\' index""\n            + ""or a [batch_size x sequence_length x num_classes] tensor ""\n            + ""where the third axis is a one-hot representation of the labels""\n        )\n\n    if len(weights.shape) != 2:\n        raise ValueError(""Weights must be a [batch_size x sequence_length] tensor"")\n\n    if average_across_timesteps and sum_over_timesteps:\n        raise ValueError(\n            ""average_across_timesteps and sum_over_timesteps cannot ""\n            ""be set to True at same time.""\n        )\n    if average_across_batch and sum_over_batch:\n        raise ValueError(\n            ""average_across_batch and sum_over_batch cannot be set ""\n            ""to True at same time.""\n        )\n    if average_across_batch and sum_over_timesteps:\n        raise ValueError(\n            ""average_across_batch and sum_over_timesteps cannot be set ""\n            ""to True at same time because of ambiguous order.""\n        )\n    if sum_over_batch and average_across_timesteps:\n        raise ValueError(\n            ""sum_over_batch and average_across_timesteps cannot be set ""\n            ""to True at same time because of ambiguous order.""\n        )\n    with tf.name_scope(name or ""sequence_loss""):\n        num_classes = tf.shape(input=logits)[2]\n        logits_flat = tf.reshape(logits, [-1, num_classes])\n        if softmax_loss_function is None:\n            if targets_rank == 2:\n                targets = tf.reshape(targets, [-1])\n                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                    labels=targets, logits=logits_flat\n                )\n            else:\n                targets = tf.reshape(targets, [-1, num_classes])\n                crossent = tf.nn.softmax_cross_entropy_with_logits(\n                    labels=targets, logits=logits_flat\n                )\n        else:\n            targets = tf.reshape(targets, [-1])\n            crossent = softmax_loss_function(labels=targets, logits=logits_flat)\n        crossent *= tf.reshape(weights, [-1])\n        if average_across_timesteps and average_across_batch:\n            crossent = tf.reduce_sum(input_tensor=crossent)\n            total_size = tf.reduce_sum(input_tensor=weights)\n            crossent = tf.math.divide_no_nan(crossent, total_size)\n        elif sum_over_timesteps and sum_over_batch:\n            crossent = tf.reduce_sum(input_tensor=crossent)\n            total_count = tf.cast(tf.math.count_nonzero(weights), crossent.dtype)\n            crossent = tf.math.divide_no_nan(crossent, total_count)\n        else:\n            crossent = tf.reshape(crossent, tf.shape(input=logits)[0:2])\n            if average_across_timesteps or average_across_batch:\n                reduce_axis = [0] if average_across_batch else [1]\n                crossent = tf.reduce_sum(input_tensor=crossent, axis=reduce_axis)\n                total_size = tf.reduce_sum(input_tensor=weights, axis=reduce_axis)\n                crossent = tf.math.divide_no_nan(crossent, total_size)\n            elif sum_over_timesteps or sum_over_batch:\n                reduce_axis = [0] if sum_over_batch else [1]\n                crossent = tf.reduce_sum(input_tensor=crossent, axis=reduce_axis)\n                total_count = tf.cast(\n                    tf.math.count_nonzero(weights, axis=reduce_axis),\n                    dtype=crossent.dtype,\n                )\n                crossent = tf.math.divide_no_nan(crossent, total_count)\n        return crossent\n\n\nclass SequenceLoss(tf.keras.losses.Loss):\n    """"""Weighted cross-entropy loss for a sequence of logits.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        average_across_timesteps: bool = False,\n        average_across_batch: bool = False,\n        sum_over_timesteps: bool = True,\n        sum_over_batch: bool = True,\n        softmax_loss_function: Optional[Callable] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(reduction=tf.keras.losses.Reduction.NONE, name=name)\n        self.average_across_timesteps = average_across_timesteps\n        self.average_across_batch = average_across_batch\n        self.sum_over_timesteps = sum_over_timesteps\n        self.sum_over_batch = sum_over_batch\n        self.softmax_loss_function = softmax_loss_function\n\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        """"""Override the parent __call__ to have a customized reduce\n        behavior.""""""\n        return sequence_loss(\n            y_pred,\n            y_true,\n            sample_weight,\n            average_across_timesteps=self.average_across_timesteps,\n            average_across_batch=self.average_across_batch,\n            sum_over_timesteps=self.sum_over_timesteps,\n            sum_over_batch=self.sum_over_batch,\n            softmax_loss_function=self.softmax_loss_function,\n            name=self.name,\n        )\n\n    def call(self, y_true, y_pred):\n        # Skip this method since the __call__ contains real implementation.\n        pass\n'"
tensorflow_addons/seq2seq/sampler.py,126,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A library of sampler for use with SamplingDecoders.""""""\n\nimport abc\n\nimport tensorflow as tf\nfrom tensorflow_addons.seq2seq import decoder\nfrom tensorflow_addons.utils.types import Initializer, TensorLike\nfrom typeguard import typechecked\nfrom typing import Callable, Optional\nfrom tensorflow_addons.utils import types\n\n_transpose_batch_time = decoder._transpose_batch_time\n\n\nclass Sampler(metaclass=abc.ABCMeta):\n    """"""Interface for implementing sampling in seq2seq decoders.\n\n    Sampler instances are used by `BasicDecoder`. The normal usage of a sampler\n    is like below:\n\n    ```python\n    sampler = Sampler(init_args)\n    (initial_finished, initial_inputs) = sampler.initialize(input_tensors)\n    cell_input = initial_inputs\n    cell_state = cell.get_initial_state(...)\n    for time_step in tf.range(max_output_length):\n        cell_output, cell_state = cell(cell_input, cell_state)\n        sample_ids = sampler.sample(time_step, cell_output, cell_state)\n        (finished, cell_input, cell_state) = sampler.next_inputs(\n            time_step, cell_output, cell_state, sample_ids)\n        if tf.reduce_all(finished):\n            break\n    ```\n\n    Note that the input_tensors should not be fed to the Sampler as __init__()\n    parameters. Instead, they should be fed by decoders via initialize().\n    """"""\n\n    @abc.abstractmethod\n    def initialize(self, inputs, **kwargs):\n        """"""initialize the sampler with the input tensors.\n\n        This method must be invoked exactly once before calling other\n        methods of the Sampler.\n\n        Args:\n          inputs: A (structure of) input tensors, it could be a nested tuple or\n            a single tensor.\n          **kwargs: Other kwargs for initialization. It could contain tensors\n            like mask for inputs, or non tensor parameter.\n\n        Returns:\n          `(initial_finished, initial_inputs)`.\n        """"""\n        pass\n\n    @abc.abstractmethod\n    def sample(self, time, outputs, state):\n        """"""Returns `sample_ids`.""""""\n        pass\n\n    @abc.abstractmethod\n    def next_inputs(self, time, outputs, state, sample_ids):\n        """"""Returns `(finished, next_inputs, next_state)`.""""""\n        pass\n\n    @abc.abstractproperty\n    def batch_size(self):\n        """"""Batch size of tensor returned by `sample`.\n\n        Returns a scalar int32 tensor. The return value might not\n        available before the invocation of initialize(), in this case,\n        ValueError is raised.\n        """"""\n        raise NotImplementedError(""batch_size has not been implemented"")\n\n    @abc.abstractproperty\n    def sample_ids_shape(self):\n        """"""Shape of tensor returned by `sample`, excluding the batch dimension.\n\n        Returns a `TensorShape`. The return value might not available\n        before the invocation of initialize().\n        """"""\n        raise NotImplementedError(""sample_ids_shape has not been implemented"")\n\n    @abc.abstractproperty\n    def sample_ids_dtype(self):\n        """"""DType of tensor returned by `sample`.\n\n        Returns a DType. The return value might not available before the\n        invocation of initialize().\n        """"""\n        raise NotImplementedError(""sample_ids_dtype has not been implemented"")\n\n\nclass CustomSampler(Sampler):\n    """"""Base abstract class that allows the user to customize sampling.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        initialize_fn: Initializer,\n        sample_fn: Callable,\n        next_inputs_fn: Callable,\n        sample_ids_shape: Optional[TensorLike] = None,\n        sample_ids_dtype: types.AcceptableDTypes = None,\n    ):\n        """"""Initializer.\n\n        Args:\n          initialize_fn: callable that returns `(finished, next_inputs)` for\n            the first iteration.\n          sample_fn: callable that takes `(time, outputs, state)` and emits\n            tensor `sample_ids`.\n          next_inputs_fn: callable that takes\n            `(time, outputs, state, sample_ids)` and emits\n            `(finished, next_inputs, next_state)`.\n          sample_ids_shape: Either a list of integers, or a 1-D Tensor of type\n            `int32`, the shape of each value in the `sample_ids` batch.\n            Defaults to a scalar.\n          sample_ids_dtype: The dtype of the `sample_ids` tensor. Defaults to\n            int32.\n        """"""\n        self._initialize_fn = initialize_fn\n        self._sample_fn = sample_fn\n        self._next_inputs_fn = next_inputs_fn\n        self._batch_size = None\n        self._sample_ids_shape = tf.TensorShape(sample_ids_shape or [])\n        self._sample_ids_dtype = sample_ids_dtype or tf.int32\n\n    @property\n    def batch_size(self):\n        if self._batch_size is None:\n            raise ValueError(""batch_size accessed before initialize was called"")\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return self._sample_ids_shape\n\n    @property\n    def sample_ids_dtype(self):\n        return self._sample_ids_dtype\n\n    def initialize(self, inputs, **kwargs):\n        (finished, next_inputs) = self._initialize_fn(inputs, **kwargs)\n        if self._batch_size is None:\n            self._batch_size = tf.size(finished)\n        return (finished, next_inputs)\n\n    def sample(self, time, outputs, state):\n        return self._sample_fn(time=time, outputs=outputs, state=state)\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        return self._next_inputs_fn(\n            time=time, outputs=outputs, state=state, sample_ids=sample_ids\n        )\n\n\nclass TrainingSampler(Sampler):\n    """"""A Sampler for use during training.\n\n    Only reads inputs.\n\n    Returned sample_ids are the argmax of the RNN output logits.\n    """"""\n\n    @typechecked\n    def __init__(self, time_major: bool = False):\n        """"""Initializer.\n\n        Args:\n          time_major: Python bool.  Whether the tensors in `inputs` are time\n            major. If `False` (default), they are assumed to be batch major.\n\n        Raises:\n          ValueError: if `sequence_length` is not a 1D tensor or `mask` is\n            not a 2D boolean tensor.\n        """"""\n        self.time_major = time_major\n        self._batch_size = None\n\n    @property\n    def batch_size(self):\n        if self._batch_size is None:\n            raise ValueError(""batch_size accessed before initialize was called"")\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return tf.TensorShape([])\n\n    @property\n    def sample_ids_dtype(self):\n        return tf.int32\n\n    def initialize(self, inputs, sequence_length=None, mask=None):\n        """"""Initialize the TrainSampler.\n\n        Args:\n          inputs: A (structure of) input tensors.\n          sequence_length: An int32 vector tensor.\n          mask: A boolean 2D tensor.\n\n        Returns:\n          (finished, next_inputs), a tuple of two items. The first item is a\n            boolean vector to indicate whether the item in the batch has\n            finished. The second item is the first slide of input data based on\n            the timestep dimension (usually the second dim of the input).\n        """"""\n        self.inputs = tf.convert_to_tensor(inputs, name=""inputs"")\n        if not self.time_major:\n            inputs = tf.nest.map_structure(_transpose_batch_time, inputs)\n\n        self._batch_size = tf.shape(tf.nest.flatten(inputs)[0])[1]\n\n        self.input_tas = tf.nest.map_structure(_unstack_ta, inputs)\n        if sequence_length is not None and mask is not None:\n            raise ValueError(\n                ""sequence_length and mask can\'t be provided "" ""at the same time.""\n            )\n        if sequence_length is not None:\n            self.sequence_length = tf.convert_to_tensor(\n                sequence_length, name=""sequence_length""\n            )\n            if self.sequence_length.shape.ndims != 1:\n                raise ValueError(\n                    ""Expected sequence_length to be vector, but received ""\n                    ""shape: %s"" % self.sequence_length.shape\n                )\n        elif mask is not None:\n            mask = tf.convert_to_tensor(mask)\n            if mask.shape.ndims != 2:\n                raise ValueError(\n                    ""Expected mask to a 2D tensor, but received shape: %s"" % mask\n                )\n            if not mask.dtype.is_bool:\n                raise ValueError(\n                    ""Expected mask to be a boolean tensor, but received ""\n                    ""dtype: %s"" % repr(mask.dtype)\n                )\n\n            axis = 1 if not self.time_major else 0\n            with tf.control_dependencies(\n                [_check_sequence_is_right_padded(mask, self.time_major)]\n            ):\n                self.sequence_length = tf.math.reduce_sum(\n                    tf.cast(mask, tf.int32), axis=axis, name=""sequence_length""\n                )\n        else:\n            # As the input tensor has been converted to time major,\n            # the maximum sequence length should be inferred from\n            # the first dimension.\n            max_seq_len = tf.shape(tf.nest.flatten(inputs)[0])[0]\n            self.sequence_length = tf.fill(\n                [self.batch_size], max_seq_len, name=""sequence_length""\n            )\n\n        self.zero_inputs = tf.nest.map_structure(\n            lambda inp: tf.zeros_like(inp[0, :]), inputs\n        )\n\n        finished = tf.equal(0, self.sequence_length)\n        all_finished = tf.reduce_all(finished)\n        next_inputs = tf.cond(\n            all_finished,\n            lambda: self.zero_inputs,\n            lambda: tf.nest.map_structure(lambda inp: inp.read(0), self.input_tas),\n        )\n        return (finished, next_inputs)\n\n    def sample(self, time, outputs, state):\n        del state\n        sample_ids = tf.cast(tf.argmax(outputs, axis=-1), tf.int32)\n        return sample_ids\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        del sample_ids\n        next_time = time + 1\n        finished = next_time >= self.sequence_length\n        all_finished = tf.reduce_all(finished)\n\n        def read_from_ta(inp):\n            return inp.read(next_time)\n\n        next_inputs = tf.cond(\n            all_finished,\n            lambda: self.zero_inputs,\n            lambda: tf.nest.map_structure(read_from_ta, self.input_tas),\n        )\n        return (finished, next_inputs, state)\n\n\nclass ScheduledEmbeddingTrainingSampler(TrainingSampler):\n    """"""A training sampler that adds scheduled sampling.\n\n    Returns -1s for sample_ids where no sampling took place; valid\n    sample id values elsewhere.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        sampling_probability: TensorLike,\n        embedding_fn: Optional[Callable] = None,\n        time_major: bool = False,\n        seed: Optional[int] = None,\n        scheduling_seed: Optional[TensorLike] = None,\n    ):\n        """"""Initializer.\n\n        Args:\n          sampling_probability: A `float32` 0-D or 1-D tensor: the probability\n            of sampling categorically from the output ids instead of reading\n            directly from the inputs.\n          embedding_fn: A callable that takes a vector tensor of `ids`\n            (argmax ids).\n          time_major: Python bool. Whether the tensors in `inputs` are time\n            major. If `False` (default), they are assumed to be batch major.\n          seed: The sampling seed.\n          scheduling_seed: The schedule decision rule sampling seed.\n\n        Raises:\n          ValueError: if `sampling_probability` is not a scalar or vector.\n        """"""\n        self.embedding_fn = embedding_fn\n        if isinstance(sampling_probability, tf.Variable):\n            self.sampling_probability = sampling_probability\n        else:\n            self.sampling_probability = tf.convert_to_tensor(\n                sampling_probability, name=""sampling_probability""\n            )\n        if self.sampling_probability.shape.ndims not in (0, 1):\n            raise ValueError(\n                ""sampling_probability must be either a scalar or a vector. ""\n                ""saw shape: %s"" % (self.sampling_probability.shape)\n            )\n        self.seed = seed\n        self.scheduling_seed = scheduling_seed\n        super().__init__(time_major=time_major)\n\n    def initialize(self, inputs, sequence_length=None, mask=None, embedding=None):\n        if self.embedding_fn is None:\n            if embedding is None:\n                raise ValueError(\n                    ""embedding is required as a keyword argument for ""\n                    ""ScheduledEmbeddingTrainingSampler""\n                )\n            self.embedding_fn = lambda ids: tf.nn.embedding_lookup(embedding, ids)\n        return super().initialize(inputs, sequence_length=sequence_length, mask=mask)\n\n    def sample(self, time, outputs, state):\n        del state\n        # Return -1s where we did not sample, and sample_ids elsewhere\n        select_sample = bernoulli_sample(\n            probs=self.sampling_probability,\n            dtype=tf.bool,\n            sample_shape=self.batch_size,\n            seed=self.scheduling_seed,\n        )\n        return tf.where(\n            select_sample,\n            categorical_sample(logits=outputs, seed=self.seed),\n            tf.fill([self.batch_size], -1),\n        )\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        (finished, base_next_inputs, state) = super().next_inputs(\n            time=time, outputs=outputs, state=state, sample_ids=sample_ids\n        )\n\n        def maybe_sample():\n            """"""Perform scheduled sampling.""""""\n            where_sampling = tf.cast(tf.where(sample_ids > -1), tf.int32)\n            where_not_sampling = tf.cast(tf.where(sample_ids <= -1), tf.int32)\n            sample_ids_sampling = tf.gather_nd(sample_ids, where_sampling)\n            inputs_not_sampling = tf.gather_nd(base_next_inputs, where_not_sampling)\n            sampled_next_inputs = self.embedding_fn(sample_ids_sampling)\n            base_shape = tf.shape(base_next_inputs)\n            return tf.scatter_nd(\n                indices=where_sampling, updates=sampled_next_inputs, shape=base_shape\n            ) + tf.scatter_nd(\n                indices=where_not_sampling,\n                updates=inputs_not_sampling,\n                shape=base_shape,\n            )\n\n        all_finished = tf.reduce_all(finished)\n        next_inputs = tf.cond(all_finished, lambda: base_next_inputs, maybe_sample)\n        return (finished, next_inputs, state)\n\n\nclass ScheduledOutputTrainingSampler(TrainingSampler):\n    """"""A training sampler that adds scheduled sampling directly to outputs.\n\n    Returns False for sample_ids where no sampling took place; True\n    elsewhere.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        sampling_probability: TensorLike,\n        time_major: bool = False,\n        seed: Optional[int] = None,\n        next_inputs_fn: Optional[Callable] = None,\n    ):\n        """"""Initializer.\n\n        Args:\n          sampling_probability: A `float32` scalar tensor: the probability of\n            sampling from the outputs instead of reading directly from the\n            inputs.\n          time_major: Python bool. Whether the tensors in `inputs` are time\n            major. If `False` (default), they are assumed to be batch major.\n          seed: The sampling seed.\n          next_inputs_fn: (Optional) callable to apply to the RNN outputs to\n            create the next input when sampling. If `None` (default), the RNN\n            outputs will be used as the next inputs.\n\n        Raises:\n          ValueError: if `sampling_probability` is not a scalar or vector.\n        """"""\n        if isinstance(sampling_probability, tf.Variable):\n            self.sampling_probability = sampling_probability\n        else:\n            self.sampling_probability = tf.convert_to_tensor(\n                sampling_probability, name=""sampling_probability""\n            )\n        if self.sampling_probability.shape.ndims not in (0, 1):\n            raise ValueError(\n                ""sampling_probability must be either a scalar or a vector. ""\n                ""saw shape: %s"" % (self.sampling_probability.shape)\n            )\n\n        self.seed = seed\n        self.next_inputs_fn = next_inputs_fn\n\n        super().__init__(time_major=time_major)\n\n    def initialize(\n        self, inputs, sequence_length=None, mask=None, auxiliary_inputs=None\n    ):\n        if auxiliary_inputs is None:\n            maybe_concatenated_inputs = inputs\n        else:\n            inputs = tf.convert_to_tensor(inputs)\n            auxiliary_inputs = tf.convert_to_tensor(auxiliary_inputs)\n            maybe_concatenated_inputs = tf.nest.map_structure(\n                lambda x, y: tf.concat((x, y), -1), inputs, auxiliary_inputs\n            )\n            if not self.time_major:\n                auxiliary_inputs = tf.nest.map_structure(\n                    _transpose_batch_time, auxiliary_inputs\n                )\n        if auxiliary_inputs is not None:\n            self._auxiliary_input_tas = tf.nest.map_structure(\n                _unstack_ta, auxiliary_inputs\n            )\n        else:\n            self._auxiliary_input_tas = None\n\n        return super().initialize(\n            maybe_concatenated_inputs, sequence_length=sequence_length, mask=mask\n        )\n\n    def sample(self, time, outputs, state):\n        del state\n        return bernoulli_sample(\n            probs=self.sampling_probability,\n            sample_shape=self.batch_size,\n            seed=self.seed,\n        )\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        (finished, base_next_inputs, state) = super().next_inputs(\n            time=time, outputs=outputs, state=state, sample_ids=sample_ids\n        )\n        sample_ids = tf.cast(sample_ids, tf.bool)\n\n        def maybe_sample():\n            """"""Perform scheduled sampling.""""""\n\n            def maybe_concatenate_auxiliary_inputs(outputs_, indices=None):\n                """"""Concatenate outputs with auxiliary inputs, if they exist.""""""\n                if self._auxiliary_input_tas is None:\n                    return outputs_\n\n                next_time = time + 1\n                auxiliary_inputs = tf.nest.map_structure(\n                    lambda ta: ta.read(next_time), self._auxiliary_input_tas\n                )\n                if indices is not None:\n                    auxiliary_inputs = tf.gather_nd(auxiliary_inputs, indices)\n                return tf.nest.map_structure(\n                    lambda x, y: tf.concat((x, y), -1), outputs_, auxiliary_inputs\n                )\n\n            if self.next_inputs_fn is None:\n                return tf.where(\n                    tf.broadcast_to(\n                        tf.expand_dims(sample_ids, axis=-1), base_next_inputs.shape\n                    ),\n                    maybe_concatenate_auxiliary_inputs(outputs),\n                    base_next_inputs,\n                )\n\n            where_sampling = tf.cast(tf.where(sample_ids), tf.int32)\n            where_not_sampling = tf.cast(tf.where(tf.logical_not(sample_ids)), tf.int32)\n            outputs_sampling = tf.gather_nd(outputs, where_sampling)\n            inputs_not_sampling = tf.gather_nd(base_next_inputs, where_not_sampling)\n            sampled_next_inputs = maybe_concatenate_auxiliary_inputs(\n                self.next_inputs_fn(outputs_sampling), where_sampling\n            )\n\n            base_shape = tf.shape(base_next_inputs)\n            return tf.scatter_nd(\n                indices=where_sampling, updates=sampled_next_inputs, shape=base_shape\n            ) + tf.scatter_nd(\n                indices=where_not_sampling,\n                updates=inputs_not_sampling,\n                shape=base_shape,\n            )\n\n        all_finished = tf.reduce_all(finished)\n        no_samples = tf.logical_not(tf.reduce_any(sample_ids))\n        next_inputs = tf.cond(\n            tf.logical_or(all_finished, no_samples),\n            lambda: base_next_inputs,\n            maybe_sample,\n        )\n        return (finished, next_inputs, state)\n\n\nclass GreedyEmbeddingSampler(Sampler):\n    """"""A sampler for use during inference.\n\n    Uses the argmax of the output (treated as logits) and passes the\n    result through an embedding layer to get the next input.\n    """"""\n\n    @typechecked\n    def __init__(self, embedding_fn: Optional[Callable] = None):\n        """"""Initializer.\n\n        Args:\n          embedding_fn: A optional callable that takes a vector tensor of `ids`\n            (argmax ids). The returned tensor will be passed to the decoder\n            input. Default to use `tf.nn.embedding_lookup`.\n        """"""\n        self.embedding_fn = embedding_fn\n        self._batch_size = None\n\n    @property\n    def batch_size(self):\n        if self._batch_size is None:\n            raise ValueError(""batch_size accessed before initialize was called"")\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return tf.TensorShape([])\n\n    @property\n    def sample_ids_dtype(self):\n        return tf.int32\n\n    def initialize(self, embedding, start_tokens=None, end_token=None):\n        """"""Initialize the GreedyEmbeddingSampler.\n\n        Args:\n          embedding: tensor that contains embedding states matrix. It will be\n            used to generate generate outputs with start_tokens and end_tokens.\n            The embedding will be ignored if the embedding_fn has been provided\n            at __init__().\n          start_tokens: `int32` vector shaped `[batch_size]`, the start tokens.\n          end_token: `int32` scalar, the token that marks end of decoding.\n\n        Returns:\n          Tuple of two items: `(finished, self.start_inputs)`.\n        Raises:\n          ValueError: if `start_tokens` is not a 1D tensor or `end_token` is\n            not a scalar.\n        """"""\n        if self.embedding_fn is None:\n            self.embedding_fn = lambda ids: tf.nn.embedding_lookup(embedding, ids)\n\n        self.start_tokens = tf.convert_to_tensor(\n            start_tokens, dtype=tf.int32, name=""start_tokens""\n        )\n        self.end_token = tf.convert_to_tensor(\n            end_token, dtype=tf.int32, name=""end_token""\n        )\n        if self.start_tokens.shape.ndims != 1:\n            raise ValueError(""start_tokens must be a vector"")\n        self._batch_size = tf.size(start_tokens)\n        if self.end_token.shape.ndims != 0:\n            raise ValueError(""end_token must be a scalar"")\n        self.start_inputs = self.embedding_fn(self.start_tokens)\n\n        finished = tf.tile([False], [self._batch_size])\n        return (finished, self.start_inputs)\n\n    def sample(self, time, outputs, state):\n        """"""sample for GreedyEmbeddingHelper.""""""\n        del time, state  # unused by sample_fn\n        # Outputs are logits, use argmax to get the most probable id\n        if not isinstance(outputs, tf.Tensor):\n            raise TypeError(\n                ""Expected outputs to be a single Tensor, got: %s"" % type(outputs)\n            )\n        sample_ids = tf.argmax(outputs, axis=-1, output_type=tf.int32)\n        return sample_ids\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        """"""next_inputs_fn for GreedyEmbeddingHelper.""""""\n        del time, outputs  # unused by next_inputs_fn\n        finished = tf.equal(sample_ids, self.end_token)\n        all_finished = tf.reduce_all(finished)\n        next_inputs = tf.cond(\n            all_finished,\n            # If we\'re finished, the next_inputs value doesn\'t matter\n            lambda: self.start_inputs,\n            lambda: self.embedding_fn(sample_ids),\n        )\n        return (finished, next_inputs, state)\n\n\nclass SampleEmbeddingSampler(GreedyEmbeddingSampler):\n    """"""A sampler for use during inference.\n\n    Uses sampling (from a distribution) instead of argmax and passes the\n    result through an embedding layer to get the next input.\n    """"""\n\n    @typechecked\n    def __init__(\n        self,\n        embedding_fn: Optional[Callable] = None,\n        softmax_temperature: Optional[TensorLike] = None,\n        seed: Optional[TensorLike] = None,\n    ):\n        """"""Initializer.\n\n        Args:\n          embedding_fn: (Optional) A callable that takes a vector tensor of\n            `ids` (argmax ids). The returned tensor will be passed to the\n            decoder input.\n          softmax_temperature: (Optional) `float32` scalar, value to divide the\n            logits by before computing the softmax. Larger values (above 1.0)\n            result in more random samples, while smaller values push the\n            sampling distribution towards the argmax. Must be strictly greater\n            than 0. Defaults to 1.0.\n          seed: (Optional) The sampling seed.\n\n        Raises:\n          ValueError: if `start_tokens` is not a 1D tensor or `end_token` is\n            not a scalar.\n        """"""\n        super().__init__(embedding_fn)\n        self.softmax_temperature = softmax_temperature\n        self.seed = seed\n\n    def sample(self, time, outputs, state):\n        """"""sample for SampleEmbeddingHelper.""""""\n        del time, state  # unused by sample_fn\n        # Outputs are logits, we sample instead of argmax (greedy).\n        if not isinstance(outputs, tf.Tensor):\n            raise TypeError(\n                ""Expected outputs to be a single Tensor, got: %s"" % type(outputs)\n            )\n        if self.softmax_temperature is None:\n            logits = outputs\n        else:\n            logits = outputs / self.softmax_temperature\n\n        return categorical_sample(logits=logits, seed=self.seed)\n\n\nclass InferenceSampler(Sampler):\n    """"""A helper to use during inference with a custom sampling function.""""""\n\n    @typechecked\n    def __init__(\n        self,\n        sample_fn: Callable,\n        sample_shape: TensorLike,\n        sample_dtype: types.AcceptableDTypes,\n        end_fn: Callable,\n        next_inputs_fn: Optional[Callable] = None,\n    ):\n        """"""Initializer.\n\n        Args:\n          sample_fn: A callable that takes `outputs` and emits tensor\n            `sample_ids`.\n          sample_shape: Either a list of integers, or a 1-D Tensor of type\n            `int32`, the shape of the each sample in the batch returned by\n            `sample_fn`.\n          sample_dtype: the dtype of the sample returned by `sample_fn`.\n          end_fn: A callable that takes `sample_ids` and emits a `bool` vector\n            shaped `[batch_size]` indicating whether each sample is an end\n            token.\n          next_inputs_fn: (Optional) A callable that takes `sample_ids` and\n            returns the next batch of inputs. If not provided, `sample_ids` is\n            used as the next batch of inputs.\n        """"""\n        self.sample_fn = sample_fn\n        self.sample_shape = tf.TensorShape(sample_shape)\n        self.sample_dtype = sample_dtype\n        self.end_fn = end_fn\n        self.next_inputs_fn = next_inputs_fn\n        self._batch_size = None\n\n    @property\n    def batch_size(self):\n        if self._batch_size is None:\n            raise ValueError(""batch_size accessed before initialize was called"")\n        return self._batch_size\n\n    @property\n    def sample_ids_shape(self):\n        return self.sample_shape\n\n    @property\n    def sample_ids_dtype(self):\n        return self.sample_dtype\n\n    def initialize(self, start_inputs):\n        self.start_inputs = tf.convert_to_tensor(start_inputs, name=""start_inputs"")\n        self._batch_size = tf.shape(start_inputs)[0]\n        finished = tf.tile([False], [self._batch_size])\n        return (finished, self.start_inputs)\n\n    def sample(self, time, outputs, state):\n        del time, state  # unused by sample\n        return self.sample_fn(outputs)\n\n    def next_inputs(self, time, outputs, state, sample_ids):\n        del time, outputs  # unused by next_inputs\n        if self.next_inputs_fn is None:\n            next_inputs = sample_ids\n        else:\n            next_inputs = self.next_inputs_fn(sample_ids)\n        finished = self.end_fn(sample_ids)\n        return (finished, next_inputs, state)\n\n\n# The following sample functions (_call_sampler, bernoulli_sample,\n# categorical_sample) mimic TensorFlow Probability distribution semantics.\ndef _call_sampler(sample_n_fn, sample_shape, name=None):\n    """"""Reshapes vector of samples.""""""\n    with tf.name_scope(name or ""call_sampler""):\n        sample_shape = tf.convert_to_tensor(\n            sample_shape, dtype=tf.int32, name=""sample_shape""\n        )\n        # Ensure sample_shape is a vector (vs just a scalar).\n        pad = tf.cast(tf.equal(tf.rank(sample_shape), 0), tf.int32)\n        sample_shape = tf.reshape(\n            sample_shape,\n            tf.pad(tf.shape(sample_shape), paddings=[[pad, 0]], constant_values=1),\n        )\n        samples = sample_n_fn(tf.reduce_prod(sample_shape))\n        batch_event_shape = tf.shape(samples)[1:]\n        final_shape = tf.concat([sample_shape, batch_event_shape], 0)\n        return tf.reshape(samples, final_shape)\n\n\ndef bernoulli_sample(\n    probs=None, logits=None, dtype=tf.int32, sample_shape=(), seed=None\n):\n    """"""Samples from Bernoulli distribution.""""""\n    if probs is None:\n        probs = tf.sigmoid(logits, name=""probs"")\n    else:\n        probs = tf.convert_to_tensor(probs, name=""probs"")\n    batch_shape_tensor = tf.shape(probs)\n\n    def _sample_n(n):\n        """"""Sample vector of Bernoullis.""""""\n        new_shape = tf.concat([[n], batch_shape_tensor], 0)\n        uniform = tf.random.uniform(new_shape, seed=seed, dtype=probs.dtype)\n        return tf.cast(tf.less(uniform, probs), dtype)\n\n    return _call_sampler(_sample_n, sample_shape)\n\n\ndef categorical_sample(logits, dtype=tf.int32, sample_shape=(), seed=None):\n    """"""Samples from categorical distribution.""""""\n    logits = tf.convert_to_tensor(logits, name=""logits"")\n    event_size = tf.shape(logits)[-1]\n    batch_shape_tensor = tf.shape(logits)[:-1]\n\n    def _sample_n(n):\n        """"""Sample vector of categoricals.""""""\n        if logits.shape.ndims == 2:\n            logits_2d = logits\n        else:\n            logits_2d = tf.reshape(logits, [-1, event_size])\n        sample_dtype = tf.int64 if logits.dtype.size > 4 else tf.int32\n        draws = tf.random.categorical(logits_2d, n, dtype=sample_dtype, seed=seed)\n        draws = tf.reshape(tf.transpose(draws), tf.concat([[n], batch_shape_tensor], 0))\n        return tf.cast(draws, dtype)\n\n    return _call_sampler(_sample_n, sample_shape)\n\n\ndef _unstack_ta(inp):\n    return tf.TensorArray(\n        dtype=inp.dtype, size=tf.shape(inp)[0], element_shape=inp.shape[1:]\n    ).unstack(inp)\n\n\ndef _check_sequence_is_right_padded(mask, time_major):\n    """"""Returns an Assert operation checking that if the mask tensor is right\n    padded.""""""\n    if time_major:\n        mask = tf.transpose(mask)\n    sequence_length = tf.math.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n    max_seq_length = tf.shape(mask)[1]\n    right_padded_mask = tf.sequence_mask(\n        sequence_length, maxlen=max_seq_length, dtype=tf.bool\n    )\n    all_equal = tf.math.equal(mask, right_padded_mask)\n\n    condition = tf.math.reduce_all(all_equal)\n    error_message = ""The input sequence should be right padded.""\n\n    return tf.Assert(condition, [error_message])\n'"
tensorflow_addons/testing/__init__.py,0,b''
tensorflow_addons/testing/serialization.py,0,"b'from typing import Union\nimport inspect\n\nimport numpy as np\nfrom tensorflow.keras.metrics import Metric\nimport typeguard\n\n\n@typeguard.typechecked\ndef check_metric_serialization(\n    metric: Metric,\n    y_true: Union[tuple, np.ndarray],\n    y_pred: Union[tuple, np.ndarray],\n    sample_weight: Union[tuple, np.ndarray, None] = None,\n    strict: bool = True,\n):\n    config = metric.get_config()\n    class_ = metric.__class__\n\n    check_config(config, class_, strict)\n\n    metric_copy = class_(**config)\n    metric_copy.set_weights(metric.get_weights())\n\n    if isinstance(y_true, tuple):\n        y_true = get_random_array(y_true)\n    if isinstance(y_pred, tuple):\n        y_pred = get_random_array(y_pred)\n    if isinstance(sample_weight, tuple) and sample_weight is not None:\n        sample_weight = get_random_array(sample_weight)\n\n    # the behavior should be the same for the original and the copy\n    if sample_weight is None:\n        metric.update_state(y_true, y_pred)\n        metric_copy.update_state(y_true, y_pred)\n    else:\n        metric.update_state(y_true, y_pred, sample_weight)\n        metric_copy.update_state(y_true, y_pred, sample_weight)\n\n    assert_all_arrays_close(metric.get_weights(), metric_copy.get_weights())\n    metric_result = metric.result().numpy()\n    metric_copy_result = metric_copy.result().numpy()\n\n    try:\n        np.testing.assert_allclose(metric_result, metric_copy_result)\n    except AssertionError as e:\n        raise ValueError(\n            ""The original and the copy of the metric give different results after ""\n            ""the same `.update_states()` call.""\n        ) from e\n\n\ndef check_config(config, class_, strict):\n    init_signature = inspect.signature(class_.__init__)\n\n    for parameter_name in init_signature.parameters:\n        if parameter_name == ""self"":\n            continue\n        elif parameter_name == ""args"" and strict:\n            raise KeyError(\n                ""Please do not use args in the class constructor of {}, ""\n                ""as it hides the real signature ""\n                ""and degrades the user experience. ""\n                ""If you have no alternative to *args, ""\n                ""use `strict=False` in check_metric_serialization."".format(\n                    class_.__name__\n                )\n            )\n        elif parameter_name == ""kwargs"" and strict:\n            raise KeyError(\n                ""Please do not use kwargs in the class constructor of {}, ""\n                ""as it hides the real signature ""\n                ""and degrades the user experience. ""\n                ""If you have no alternative to **kwargs, ""\n                ""use `strict=False` in check_metric_serialization."".format(\n                    class_.__name__\n                )\n            )\n        if parameter_name not in config:\n            raise KeyError(\n                ""The constructor parameter {} is not present in the config dict ""\n                ""obtained with `.get_config()` of {}. All parameters should be set to ""\n                ""ensure a perfect copy of the keras object can be obtained when ""\n                ""serialized."".format(parameter_name, class_.__name__)\n            )\n\n\ndef assert_all_arrays_close(list1, list2):\n    for array1, array2 in zip(list1, list2):\n        np.testing.assert_allclose(array1, array2)\n\n\ndef get_random_array(shape):\n    return np.random.uniform(size=shape).astype(np.float32)\n'"
tensorflow_addons/tests/__init__.py,0,b''
tensorflow_addons/tests/register_test.py,1,"b'import sys\n\nimport pytest\nimport tensorflow as tf\nfrom tensorflow_addons.register import register_all, _get_all_shared_objects\nfrom tensorflow_addons.utils import resource_loader\n\n\ndef test_multiple_register():\n    if resource_loader.SKIP_CUSTOM_OPS:\n        pytest.skip(\n            ""Skipping the test because a custom ops ""\n            ""was being loaded while --skip-custom-ops was set.""\n        )\n    register_all()\n    register_all()\n\n\ndef test_get_all_shared_objects():\n    if resource_loader.SKIP_CUSTOM_OPS:\n        pytest.skip(\n            ""Skipping the test because a custom ops ""\n            ""was being loaded while --skip-custom-ops was set.""\n        )\n    all_shared_objects = _get_all_shared_objects()\n    assert len(all_shared_objects) >= 4\n\n    for file in all_shared_objects:\n        tf.load_op_library(file)\n\n\nif __name__ == ""__main__"":\n    sys.exit(pytest.main([__file__]))\n'"
tensorflow_addons/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/text/__init__.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Additional text-processing ops.""""""\n\n# Conditional Random Field\nfrom tensorflow_addons.text.crf import crf_binary_score\nfrom tensorflow_addons.text.crf import crf_decode\nfrom tensorflow_addons.text.crf import crf_decode_backward\nfrom tensorflow_addons.text.crf import crf_decode_forward\nfrom tensorflow_addons.text.crf import crf_forward\nfrom tensorflow_addons.text.crf import crf_log_likelihood\nfrom tensorflow_addons.text.crf import crf_log_norm\nfrom tensorflow_addons.text.crf import crf_multitag_sequence_score\nfrom tensorflow_addons.text.crf import crf_sequence_score\nfrom tensorflow_addons.text.crf import crf_unary_score\nfrom tensorflow_addons.text.crf import viterbi_decode\nfrom tensorflow_addons.text.parse_time_op import parse_time\n\n# Skip Gram Sampling\nfrom tensorflow_addons.text.skip_gram_ops import skip_gram_sample\nfrom tensorflow_addons.text.skip_gram_ops import skip_gram_sample_with_text_vocab\n'"
tensorflow_addons/text/crf.py,117,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.types import TensorLike\nfrom typeguard import typechecked\nfrom typing import Optional\n\n# TODO: Wrap functions in @tf.function once\n# https://github.com/tensorflow/tensorflow/issues/29075 is resolved\n\n\ndef crf_sequence_score(\n    inputs: TensorLike,\n    tag_indices: TensorLike,\n    sequence_lengths: TensorLike,\n    transition_params: TensorLike,\n) -> tf.Tensor:\n    """"""Computes the unnormalized score for a tag sequence.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n          to use as input to the CRF layer.\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which\n          we compute the unnormalized score.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] transition matrix.\n    Returns:\n      sequence_scores: A [batch_size] vector of unnormalized sequence scores.\n    """"""\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    # If max_seq_len is 1, we skip the score calculation and simply gather the\n    # unary potentials of the single tag.\n    def _single_seq_fn():\n        batch_size = tf.shape(inputs, out_type=tf.int32)[0]\n        batch_inds = tf.reshape(tf.range(batch_size), [-1, 1])\n        indices = tf.concat([batch_inds, tf.zeros_like(batch_inds)], axis=1)\n\n        tag_inds = tf.gather_nd(tag_indices, indices)\n        tag_inds = tf.reshape(tag_inds, [-1, 1])\n        indices = tf.concat([indices, tag_inds], axis=1)\n\n        sequence_scores = tf.gather_nd(inputs, indices)\n\n        sequence_scores = tf.where(\n            tf.less_equal(sequence_lengths, 0),\n            tf.zeros_like(sequence_scores),\n            sequence_scores,\n        )\n        return sequence_scores\n\n    def _multi_seq_fn():\n        # Compute the scores of the given tag sequence.\n        unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)\n        binary_scores = crf_binary_score(\n            tag_indices, sequence_lengths, transition_params\n        )\n        sequence_scores = unary_scores + binary_scores\n        return sequence_scores\n\n    return tf.cond(tf.equal(tf.shape(inputs)[1], 1), _single_seq_fn, _multi_seq_fn)\n\n\ndef crf_multitag_sequence_score(\n    inputs: TensorLike,\n    tag_bitmap: TensorLike,\n    sequence_lengths: TensorLike,\n    transition_params: TensorLike,\n) -> tf.Tensor:\n    """"""Computes the unnormalized score of all tag sequences matching\n    tag_bitmap.\n\n    tag_bitmap enables more than one tag to be considered correct at each time\n    step. This is useful when an observed output at a given time step is\n    consistent with more than one tag, and thus the log likelihood of that\n    observation must take into account all possible consistent tags.\n\n    Using one-hot vectors in tag_bitmap gives results identical to\n    crf_sequence_score.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n          to use as input to the CRF layer.\n      tag_bitmap: A [batch_size, max_seq_len, num_tags] boolean tensor\n          representing all active tags at each index for which to calculate the\n          unnormalized score.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] transition matrix.\n    Returns:\n      sequence_scores: A [batch_size] vector of unnormalized sequence scores.\n    """"""\n    tag_bitmap = tf.cast(tag_bitmap, dtype=tf.bool)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    filtered_inputs = tf.where(\n        tag_bitmap, inputs, tf.fill(tf.shape(inputs), float(""-inf""))\n    )\n\n    # If max_seq_len is 1, we skip the score calculation and simply gather the\n    # unary potentials of all active tags.\n    def _single_seq_fn():\n        return tf.reduce_logsumexp(filtered_inputs, axis=[1, 2], keepdims=False)\n\n    def _multi_seq_fn():\n        # Compute the logsumexp of all scores of sequences\n        # matching the given tags.\n        return crf_log_norm(\n            inputs=filtered_inputs,\n            sequence_lengths=sequence_lengths,\n            transition_params=transition_params,\n        )\n\n    return tf.cond(tf.equal(tf.shape(inputs)[1], 1), _single_seq_fn, _multi_seq_fn)\n\n\ndef crf_log_norm(\n    inputs: TensorLike, sequence_lengths: TensorLike, transition_params: TensorLike\n) -> tf.Tensor:\n    """"""Computes the normalization for a CRF.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n          to use as input to the CRF layer.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] transition matrix.\n    Returns:\n      log_norm: A [batch_size] vector of normalizers for a CRF.\n    """"""\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    # Split up the first and rest of the inputs in preparation for the forward\n    # algorithm.\n    first_input = tf.slice(inputs, [0, 0, 0], [-1, 1, -1])\n    first_input = tf.squeeze(first_input, [1])\n\n    # If max_seq_len is 1, we skip the algorithm and simply reduce_logsumexp\n    # over the ""initial state"" (the unary potentials).\n    def _single_seq_fn():\n        log_norm = tf.reduce_logsumexp(first_input, [1])\n        # Mask `log_norm` of the sequences with length <= zero.\n        log_norm = tf.where(\n            tf.less_equal(sequence_lengths, 0), tf.zeros_like(log_norm), log_norm\n        )\n        return log_norm\n\n    def _multi_seq_fn():\n        """"""Forward computation of alpha values.""""""\n        rest_of_input = tf.slice(inputs, [0, 1, 0], [-1, -1, -1])\n        # Compute the alpha values in the forward algorithm in order to get the\n        # partition function.\n\n        alphas = crf_forward(\n            rest_of_input, first_input, transition_params, sequence_lengths\n        )\n        log_norm = tf.reduce_logsumexp(alphas, [1])\n        # Mask `log_norm` of the sequences with length <= zero.\n        log_norm = tf.where(\n            tf.less_equal(sequence_lengths, 0), tf.zeros_like(log_norm), log_norm\n        )\n        return log_norm\n\n    return tf.cond(tf.equal(tf.shape(inputs)[1], 1), _single_seq_fn, _multi_seq_fn)\n\n\ndef crf_log_likelihood(\n    inputs: TensorLike,\n    tag_indices: TensorLike,\n    sequence_lengths: TensorLike,\n    transition_params: Optional[TensorLike] = None,\n) -> tf.Tensor:\n    """"""Computes the log-likelihood of tag sequences in a CRF.\n\n    Args:\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n          to use as input to the CRF layer.\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which\n          we compute the log-likelihood.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] transition matrix,\n          if available.\n    Returns:\n      log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of\n        each example, given the sequence of tag indices.\n      transition_params: A [num_tags, num_tags] transition matrix. This is\n          either provided by the caller or created in this function.\n    """"""\n    num_tags = inputs.shape[2]\n\n    # cast type to handle different types\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    if transition_params is None:\n        initializer = tf.keras.initializers.GlorotUniform()\n        transition_params = tf.Variable(\n            initializer([num_tags, num_tags]), ""transitions""\n        )\n\n    sequence_scores = crf_sequence_score(\n        inputs, tag_indices, sequence_lengths, transition_params\n    )\n    log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)\n\n    # Normalize the scores to get the log-likelihood per example.\n    log_likelihood = sequence_scores - log_norm\n    return log_likelihood, transition_params\n\n\ndef crf_unary_score(\n    tag_indices: TensorLike, sequence_lengths: TensorLike, inputs: TensorLike\n) -> tf.Tensor:\n    """"""Computes the unary scores of tag sequences.\n\n    Args:\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials.\n    Returns:\n      unary_scores: A [batch_size] vector of unary scores.\n    """"""\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    batch_size = tf.shape(inputs)[0]\n    max_seq_len = tf.shape(inputs)[1]\n    num_tags = tf.shape(inputs)[2]\n\n    flattened_inputs = tf.reshape(inputs, [-1])\n\n    offsets = tf.expand_dims(tf.range(batch_size) * max_seq_len * num_tags, 1)\n    offsets += tf.expand_dims(tf.range(max_seq_len) * num_tags, 0)\n    # Use int32 or int64 based on tag_indices\' dtype.\n    if tag_indices.dtype == tf.int64:\n        offsets = tf.cast(offsets, tf.int64)\n    flattened_tag_indices = tf.reshape(offsets + tag_indices, [-1])\n\n    unary_scores = tf.reshape(\n        tf.gather(flattened_inputs, flattened_tag_indices), [batch_size, max_seq_len]\n    )\n\n    masks = tf.sequence_mask(\n        sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32\n    )\n\n    unary_scores = tf.reduce_sum(unary_scores * masks, 1)\n    return unary_scores\n\n\ndef crf_binary_score(\n    tag_indices: TensorLike, sequence_lengths: TensorLike, transition_params: TensorLike\n) -> tf.Tensor:\n    """"""Computes the binary scores of tag sequences.\n\n    Args:\n      tag_indices: A [batch_size, max_seq_len] matrix of tag indices.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n    Returns:\n      binary_scores: A [batch_size] vector of binary scores.\n    """"""\n    tag_indices = tf.cast(tag_indices, dtype=tf.int32)\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    num_tags = tf.shape(transition_params)[0]\n    num_transitions = tf.shape(tag_indices)[1] - 1\n\n    # Truncate by one on each side of the sequence to get the start and end\n    # indices of each transition.\n    start_tag_indices = tf.slice(tag_indices, [0, 0], [-1, num_transitions])\n    end_tag_indices = tf.slice(tag_indices, [0, 1], [-1, num_transitions])\n\n    # Encode the indices in a flattened representation.\n    flattened_transition_indices = start_tag_indices * num_tags + end_tag_indices\n    flattened_transition_params = tf.reshape(transition_params, [-1])\n\n    # Get the binary scores based on the flattened representation.\n    binary_scores = tf.gather(flattened_transition_params, flattened_transition_indices)\n\n    masks = tf.sequence_mask(\n        sequence_lengths, maxlen=tf.shape(tag_indices)[1], dtype=tf.float32\n    )\n    truncated_masks = tf.slice(masks, [0, 1], [-1, -1])\n    binary_scores = tf.reduce_sum(binary_scores * truncated_masks, 1)\n    return binary_scores\n\n\ndef crf_forward(\n    inputs: TensorLike,\n    state: TensorLike,\n    transition_params: TensorLike,\n    sequence_lengths: TensorLike,\n) -> tf.Tensor:\n    """"""Computes the alpha values in a linear-chain CRF.\n\n    See http://www.cs.columbia.edu/~mcollins/fb.pdf for reference.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of unary potentials.\n      state: A [batch_size, num_tags] matrix containing the previous alpha\n         values.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n          This matrix is expanded into a [1, num_tags, num_tags] in preparation\n          for the broadcast summation occurring within the cell.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      new_alphas: A [batch_size, num_tags] matrix containing the\n          new alpha values.\n    """"""\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n\n    last_index = tf.maximum(\n        tf.constant(0, dtype=sequence_lengths.dtype), sequence_lengths - 1\n    )\n    inputs = tf.transpose(inputs, [1, 0, 2])\n    transition_params = tf.expand_dims(transition_params, 0)\n\n    def _scan_fn(_state, _inputs):\n        _state = tf.expand_dims(_state, 2)\n        transition_scores = _state + transition_params\n        new_alphas = _inputs + tf.reduce_logsumexp(transition_scores, [1])\n        return new_alphas\n\n    all_alphas = tf.transpose(tf.scan(_scan_fn, inputs, state), [1, 0, 2])\n    # add first state for sequences of length 1\n    all_alphas = tf.concat([tf.expand_dims(state, 1), all_alphas], 1)\n\n    idxs = tf.stack([tf.range(tf.shape(last_index)[0]), last_index], axis=1)\n    return tf.gather_nd(all_alphas, idxs)\n\n\ndef viterbi_decode(score: TensorLike, transition_params: TensorLike) -> tf.Tensor:\n    """"""Decode the highest scoring sequence of tags outside of TensorFlow.\n\n    This should only be used at test time.\n\n    Args:\n      score: A [seq_len, num_tags] matrix of unary potentials.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n\n    Returns:\n      viterbi: A [seq_len] list of integers containing the highest scoring tag\n          indices.\n      viterbi_score: A float containing the score for the Viterbi sequence.\n    """"""\n    trellis = np.zeros_like(score)\n    backpointers = np.zeros_like(score, dtype=np.int32)\n    trellis[0] = score[0]\n\n    for t in range(1, score.shape[0]):\n        v = np.expand_dims(trellis[t - 1], 1) + transition_params\n        trellis[t] = score[t] + np.max(v, 0)\n        backpointers[t] = np.argmax(v, 0)\n\n    viterbi = [np.argmax(trellis[-1])]\n    for bp in reversed(backpointers[1:]):\n        viterbi.append(bp[viterbi[-1]])\n    viterbi.reverse()\n\n    viterbi_score = np.max(trellis[-1])\n    return viterbi, viterbi_score\n\n\nclass CrfDecodeForwardRnnCell(tf.keras.layers.AbstractRNNCell):\n    """"""Computes the forward decoding in a linear-chain CRF.""""""\n\n    @typechecked\n    def __init__(self, transition_params: TensorLike, **kwargs):\n        """"""Initialize the CrfDecodeForwardRnnCell.\n\n        Args:\n          transition_params: A [num_tags, num_tags] matrix of binary\n            potentials. This matrix is expanded into a\n            [1, num_tags, num_tags] in preparation for the broadcast\n            summation occurring within the cell.\n        """"""\n        super().__init__(**kwargs)\n        self._transition_params = tf.expand_dims(transition_params, 0)\n        self._num_tags = transition_params.shape[0]\n\n    @property\n    def state_size(self):\n        return self._num_tags\n\n    @property\n    def output_size(self):\n        return self._num_tags\n\n    def build(self, input_shape):\n        super().build(input_shape)\n\n    def call(self, inputs, state):\n        """"""Build the CrfDecodeForwardRnnCell.\n\n        Args:\n          inputs: A [batch_size, num_tags] matrix of unary potentials.\n          state: A [batch_size, num_tags] matrix containing the previous step\'s\n                score values.\n\n        Returns:\n          backpointers: A [batch_size, num_tags] matrix of backpointers.\n          new_state: A [batch_size, num_tags] matrix of new score values.\n        """"""\n        state = tf.expand_dims(state[0], 2)\n        transition_scores = state + self._transition_params\n        new_state = inputs + tf.reduce_max(transition_scores, [1])\n        backpointers = tf.argmax(transition_scores, 1)\n        backpointers = tf.cast(backpointers, dtype=tf.int32)\n        return backpointers, new_state\n\n\ndef crf_decode_forward(\n    inputs: TensorLike,\n    state: TensorLike,\n    transition_params: TensorLike,\n    sequence_lengths: TensorLike,\n) -> tf.Tensor:\n    """"""Computes forward decoding in a linear-chain CRF.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of unary potentials.\n      state: A [batch_size, num_tags] matrix containing the previous step\'s\n            score values.\n      transition_params: A [num_tags, num_tags] matrix of binary potentials.\n      sequence_lengths: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      backpointers: A [batch_size, num_tags] matrix of backpointers.\n      new_state: A [batch_size, num_tags] matrix of new score values.\n    """"""\n    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)\n    mask = tf.sequence_mask(sequence_lengths, tf.shape(inputs)[1])\n    crf_fwd_cell = CrfDecodeForwardRnnCell(transition_params)\n    crf_fwd_layer = tf.keras.layers.RNN(\n        crf_fwd_cell, return_sequences=True, return_state=True\n    )\n    return crf_fwd_layer(inputs, state, mask=mask)\n\n\ndef crf_decode_backward(inputs: TensorLike, state: TensorLike) -> tf.Tensor:\n    """"""Computes backward decoding in a linear-chain CRF.\n\n    Args:\n      inputs: A [batch_size, num_tags] matrix of\n            backpointer of next step (in time order).\n      state: A [batch_size, 1] matrix of tag index of next step.\n\n    Returns:\n      new_tags: A [batch_size, num_tags]\n        tensor containing the new tag indices.\n    """"""\n    inputs = tf.transpose(inputs, [1, 0, 2])\n\n    def _scan_fn(state, inputs):\n        state = tf.squeeze(state, axis=[1])\n        idxs = tf.stack([tf.range(tf.shape(inputs)[0]), state], axis=1)\n        new_tags = tf.expand_dims(tf.gather_nd(inputs, idxs), axis=-1)\n        return new_tags\n\n    return tf.transpose(tf.scan(_scan_fn, inputs, state), [1, 0, 2])\n\n\ndef crf_decode(\n    potentials: TensorLike, transition_params: TensorLike, sequence_length: TensorLike\n) -> tf.Tensor:\n    """"""Decode the highest scoring sequence of tags.\n\n    Args:\n      potentials: A [batch_size, max_seq_len, num_tags] tensor of\n                unary potentials.\n      transition_params: A [num_tags, num_tags] matrix of\n                binary potentials.\n      sequence_length: A [batch_size] vector of true sequence lengths.\n\n    Returns:\n      decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.\n                  Contains the highest scoring tag indices.\n      best_score: A [batch_size] vector, containing the score of `decode_tags`.\n    """"""\n    sequence_length = tf.cast(sequence_length, dtype=tf.int32)\n\n    # If max_seq_len is 1, we skip the algorithm and simply return the\n    # argmax tag and the max activation.\n    def _single_seq_fn():\n        decode_tags = tf.cast(tf.argmax(potentials, axis=2), dtype=tf.int32)\n        best_score = tf.reshape(tf.reduce_max(potentials, axis=2), shape=[-1])\n        return decode_tags, best_score\n\n    def _multi_seq_fn():\n        # Computes forward decoding. Get last score and backpointers.\n        initial_state = tf.slice(potentials, [0, 0, 0], [-1, 1, -1])\n        initial_state = tf.squeeze(initial_state, axis=[1])\n        inputs = tf.slice(potentials, [0, 1, 0], [-1, -1, -1])\n\n        sequence_length_less_one = tf.maximum(\n            tf.constant(0, dtype=tf.int32), sequence_length - 1\n        )\n\n        backpointers, last_score = crf_decode_forward(\n            inputs, initial_state, transition_params, sequence_length_less_one\n        )\n\n        backpointers = tf.reverse_sequence(\n            backpointers, sequence_length_less_one, seq_axis=1\n        )\n\n        initial_state = tf.cast(tf.argmax(last_score, axis=1), dtype=tf.int32)\n        initial_state = tf.expand_dims(initial_state, axis=-1)\n\n        decode_tags = crf_decode_backward(backpointers, initial_state)\n        decode_tags = tf.squeeze(decode_tags, axis=[2])\n        decode_tags = tf.concat([initial_state, decode_tags], axis=1)\n        decode_tags = tf.reverse_sequence(decode_tags, sequence_length, seq_axis=1)\n\n        best_score = tf.reduce_max(last_score, axis=1)\n        return decode_tags, best_score\n\n    if potentials.shape[1] is not None:\n        # shape is statically know, so we just execute\n        # the appropriate code path\n        if potentials.shape[1] == 1:\n            return _single_seq_fn()\n        else:\n            return _multi_seq_fn()\n    else:\n        return tf.cond(\n            tf.equal(tf.shape(potentials)[1], 1), _single_seq_fn, _multi_seq_fn\n        )\n'"
tensorflow_addons/text/parse_time_op.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Parse time ops.""""""\nimport platform\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.resource_loader import LazySO\n\nIS_WINDOWS = platform.system() == ""Windows""\n\n_parse_time_so = LazySO(""custom_ops/text/_parse_time_op.so"")\n\ntf.no_gradient(""Addons>ParseTime"")\n\n\ndef parse_time(time_string: str, time_format: str, output_unit: str) -> tf.Tensor:\n    """"""Parse an input string according to the provided format string into a\n    Unix time.\n\n    Parse an input string according to the provided format string into a Unix\n    time, the number of seconds / milliseconds / microseconds / nanoseconds\n    elapsed since January 1, 1970 UTC.\n\n    Uses strftime()-like formatting options, with the same extensions as\n    FormatTime(), but with the exceptions that %E#S is interpreted as %E*S, and\n    %E#f as %E*f.  %Ez and %E*z also accept the same inputs.\n\n    %Y consumes as many numeric characters as it can, so the matching\n    data should always be terminated with a non-numeric. %E4Y always\n    consumes exactly four characters, including any sign.\n\n    Unspecified fields are taken from the default date and time of ...\n\n      ""1970-01-01 00:00:00.0 +0000""\n\n    For example, parsing a string of ""15:45"" (%H:%M) will return an\n    Unix time that represents ""1970-01-01 15:45:00.0 +0000"".\n\n    Note that ParseTime only heeds the fields year, month, day, hour,\n    minute, (fractional) second, and UTC offset.  Other fields, like\n    weekday (%a or %A), while parsed for syntactic validity, are\n    ignored in the conversion.\n\n    Date and time fields that are out-of-range will be treated as\n    errors rather than normalizing them like `absl::CivilSecond` does.\n    For example, it is an error to parse the date ""Oct 32, 2013""\n    because 32 is out of range.\n\n    A leap second of "":60"" is normalized to "":00"" of the following\n    minute with fractional seconds discarded.  The following table\n    shows how the given seconds and subseconds will be parsed:\n\n      ""59.x"" -> 59.x  // exact\n      ""60.x"" -> 00.0  // normalized\n      ""00.x"" -> 00.x  // exact\n\n    Args:\n      time_string: The input time string to be parsed.\n      time_format: The time format.\n      output_unit: The output unit of the parsed unix time. Can only be SECOND,\n        MILLISECOND, MICROSECOND, NANOSECOND.\n\n    Returns:\n      the number of seconds / milliseconds / microseconds / nanoseconds elapsed\n        since January 1, 1970 UTC.\n\n    Raises:\n      ValueError: If `output_unit` is not a valid value,\n        if parsing `time_string` according to `time_format` failed.\n    """"""\n    if IS_WINDOWS:\n        raise NotImplementedError(""parse_time is not yet implemented on Windows."")\n    return _parse_time_so.ops.addons_parse_time(time_string, time_format, output_unit)\n'"
tensorflow_addons/text/skip_gram_ops.py,30,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Skip-gram sampling ops from https://arxiv.org/abs/1301.3781.""""""\n\nimport csv\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils.resource_loader import LazySO\n\nfrom tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike, TensorLike\nfrom typing import Optional\n\n_skip_gram_so = LazySO(""custom_ops/text/_skip_gram_ops.so"")\n\ntf.no_gradient(""Addons>SkipGramGenerateCandidates"")\n\n\ndef skip_gram_sample(\n    input_tensor: TensorLike,\n    min_skips: FloatTensorLike = 1,\n    max_skips: FloatTensorLike = 5,\n    start: FloatTensorLike = 0,\n    limit: FloatTensorLike = -1,\n    emit_self_as_target: bool = False,\n    vocab_freq_table: tf.lookup.KeyValueTensorInitializer = None,\n    vocab_min_count: Optional[FloatTensorLike] = None,\n    vocab_subsampling: Optional[FloatTensorLike] = None,\n    corpus_size: Optional[FloatTensorLike] = None,\n    batch_size: Optional[FloatTensorLike] = None,\n    batch_capacity: Optional[FloatTensorLike] = None,\n    seed: Optional[FloatTensorLike] = None,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Generates skip-gram token and label paired Tensors from the input\n    tensor.\n\n    Generates skip-gram `(""token"", ""label"")` pairs using each element in the\n    rank-1 `input_tensor` as a token. The window size used for each token will\n    be randomly selected from the range specified by `[min_skips, max_skips]`,\n    inclusive. See https://arxiv.org/abs/1301.3781 for more details about\n    skip-gram.\n\n    For example, given `input_tensor = [""the"", ""quick"", ""brown"", ""fox"",\n    ""jumps""]`, `min_skips = 1`, `max_skips = 2`, `emit_self_as_target = False`,\n    the output `(tokens, labels)` pairs for the token ""quick"" will be randomly\n    selected from either `(tokens=[""quick"", ""quick""], labels=[""the"", ""brown""])`\n    for 1 skip, or `(tokens=[""quick"", ""quick"", ""quick""],\n    labels=[""the"", ""brown"", ""fox""])` for 2 skips.\n\n    If `emit_self_as_target = True`, each token will also be emitted as a label\n    for itself. From the previous example, the output will be either\n    `(tokens=[""quick"", ""quick"", ""quick""], labels=[""the"", ""quick"", ""brown""])`\n    for 1 skip, or `(tokens=[""quick"", ""quick"", ""quick"", ""quick""],\n    labels=[""the"", ""quick"", ""brown"", ""fox""])` for 2 skips.\n\n    The same process is repeated for each element of `input_tensor` and\n    concatenated together into the two output rank-1 `Tensors` (one for all the\n    tokens, another for all the labels).\n\n    If `vocab_freq_table` is specified, tokens in `input_tensor` that are not\n    present in the vocabulary are discarded. Tokens whose frequency counts are\n    below `vocab_min_count` are also discarded. Tokens whose frequency\n    proportions in the corpus exceed `vocab_subsampling` may be randomly\n    down-sampled. See Eq. 5 in http://arxiv.org/abs/1310.4546 for more details\n    about subsampling.\n\n    Due to the random window sizes used for each token, the lengths of the\n    outputs are non-deterministic, unless `batch_size` is specified to batch\n    the outputs to always return `Tensors` of length `batch_size`.\n\n    Args:\n      input_tensor: A rank-1 `Tensor` from which to generate skip-gram\n        candidates.\n      min_skips: `int` or scalar `Tensor` specifying the minimum window size to\n        randomly use for each token. Must be >= 0 and <= `max_skips`. If\n        `min_skips` and `max_skips` are both 0, the only label outputted will\n        be the token itself when `emit_self_as_target = True` -\n        or no output otherwise.\n      max_skips: `int` or scalar `Tensor` specifying the maximum window size to\n        randomly use for each token. Must be >= 0.\n      start: `int` or scalar `Tensor` specifying the position in\n        `input_tensor` from which to start generating skip-gram candidates.\n      limit: `int` or scalar `Tensor` specifying the maximum number of\n        elements in `input_tensor` to use in generating skip-gram candidates.\n        -1 means to use the rest of the `Tensor` after `start`.\n      emit_self_as_target: `bool` or scalar `Tensor` specifying whether to emit\n        each token as a label for itself.\n      vocab_freq_table: (Optional) A lookup table (subclass of\n        `lookup.InitializableLookupTableBase`) that maps tokens to their raw\n        frequency counts. If specified, any token in `input_tensor` that is not\n        found in `vocab_freq_table` will be filtered out before generating\n        skip-gram candidates. While this will typically map to integer raw\n        frequency counts, it could also map to float frequency proportions.\n        `vocab_min_count` and `corpus_size` should be in the same units\n        as this.\n      vocab_min_count: (Optional) `int`, `float`, or scalar `Tensor` specifying\n        minimum frequency threshold (from `vocab_freq_table`) for a token to be\n        kept in `input_tensor`. If this is specified, `vocab_freq_table` must\n        also be specified - and they should both be in the same units.\n      vocab_subsampling: (Optional) `float` specifying frequency proportion\n        threshold for tokens from `input_tensor`. Tokens that occur more\n        frequently (based on the ratio of the token\'s `vocab_freq_table` value\n        to the `corpus_size`) will be randomly down-sampled. Reasonable\n        starting values may be around 1e-3 or 1e-5. If this is specified, both\n        `vocab_freq_table` and `corpus_size` must also be specified. See Eq. 5\n        in http://arxiv.org/abs/1310.4546 for more details.\n      corpus_size: (Optional) `int`, `float`, or scalar `Tensor` specifying the\n        total number of tokens in the corpus (e.g., sum of all the frequency\n        counts of `vocab_freq_table`). Used with `vocab_subsampling` for\n        down-sampling frequently occurring tokens. If this is specified,\n        `vocab_freq_table` and `vocab_subsampling` must also be specified.\n      batch_size: (Optional) `int` specifying batch size of returned `Tensors`.\n      batch_capacity: (Optional) `int` specifying batch capacity for the queue\n        used for batching returned `Tensors`. Only has an effect if\n        `batch_size` > 0. Defaults to 100 * `batch_size` if not specified.\n      seed: (Optional) `int` used to create a random seed for window size and\n        subsampling. See `set_random_seed` docs for behavior.\n      name: (Optional) A `string` name or a name scope for the operations.\n\n    Returns:\n      A `tuple` containing (token, label) `Tensors`. Each output `Tensor` is of\n      rank-1 and has the same type as `input_tensor`. The `Tensors` will be of\n      length `batch_size`; if `batch_size` is not specified, they will be of\n      random length, though they will be in sync with each other as long as\n      they are evaluated together.\n\n    Raises:\n      ValueError: If `vocab_freq_table` is not provided, but `vocab_min_count`,\n        `vocab_subsampling`, or `corpus_size` is specified.\n        If `vocab_subsampling` and `corpus_size` are not both present or\n        both absent.\n    """"""\n\n    if vocab_freq_table is None and (\n        vocab_min_count is not None\n        or vocab_subsampling is not None\n        or corpus_size is not None\n    ):\n        raise ValueError(\n            ""vocab_freq_table is not provided, but vocab_min_count={}, ""\n            ""vocab_subsampling={}, or corpus_size={} is not None.""\n            ""These settings are useless without a vocab_freq_table."".format(\n                vocab_min_count, vocab_subsampling, corpus_size\n            )\n        )\n\n    if (vocab_subsampling is None) != (corpus_size is None):\n        raise ValueError(\n            ""vocab_subsampling is {} while corpus_size is {} - both must be ""\n            ""provided in order for subsampling to work."".format(\n                vocab_subsampling, corpus_size\n            )\n        )\n\n    with tf.name_scope(name or ""skip_gram_sample""):\n\n        input_tensor = _filter_input(\n            input_tensor=input_tensor,\n            vocab_freq_table=vocab_freq_table,\n            vocab_min_count=vocab_min_count,\n            vocab_subsampling=vocab_subsampling,\n            corpus_size=corpus_size,\n            seed=seed,\n        )\n\n        seed1, seed2 = tf.compat.v1.get_seed(seed)\n        tokens, labels = _skip_gram_so.ops.addons_skip_gram_generate_candidates(\n            input_tensor=input_tensor,\n            min_skips=min_skips,\n            max_skips=max_skips,\n            start=start,\n            limit=limit,\n            emit_self_as_target=emit_self_as_target,\n            # Note that seed here should be seed1! This is due to\n            # GuardedPhiloxRandom\'s hard-coded attributes of ""seed"" and ""seed2"".\n            seed=seed1,\n            seed2=seed2,\n        )\n\n        # TODO(weiho): If the need arises, add support for sparse input_tensor that\n        # figures out sentence boundaries, then calls\n        # skip_gram_generate_candidates() on each sentence.\n\n        # Batches the (tokens, labels) outputs so that they will be of deterministic\n        # batch_size, to facilitate feeding them into the rest of the network.\n        if batch_size is not None and batch_size > 0:\n            batch_capacity = (\n                batch_capacity\n                if (batch_capacity is not None and batch_capacity > 0)\n                else 100 * batch_size\n            )\n            return tf.train.batch(\n                [tokens, labels], batch_size, capacity=batch_capacity, enqueue_many=True\n            )\n\n        return tokens, labels\n\n\ndef skip_gram_sample_with_text_vocab(\n    input_tensor: TensorLike,\n    vocab_freq_file: str,\n    vocab_token_index: FloatTensorLike = 0,\n    vocab_token_dtype: Optional[AcceptableDTypes] = tf.dtypes.string,\n    vocab_freq_index: FloatTensorLike = 1,\n    vocab_freq_dtype: Optional[AcceptableDTypes] = tf.dtypes.float64,\n    vocab_delimiter: str = "","",\n    vocab_min_count: Optional[FloatTensorLike] = None,\n    vocab_subsampling: Optional[FloatTensorLike] = None,\n    corpus_size: Optional[FloatTensorLike] = None,\n    min_skips: FloatTensorLike = 1,\n    max_skips: FloatTensorLike = 5,\n    start: FloatTensorLike = 0,\n    limit: FloatTensorLike = -1,\n    emit_self_as_target: bool = False,\n    batch_size: Optional[FloatTensorLike] = None,\n    batch_capacity: Optional[FloatTensorLike] = None,\n    seed: Optional[FloatTensorLike] = None,\n    name: Optional[str] = None,\n) -> tf.Tensor:\n    """"""Skip-gram sampling with a text vocabulary file.\n\n    Wrapper around `skip_gram_sample()` for use with a text vocabulary file.\n    The vocabulary file is expected to be a plain-text file, with lines of\n    `vocab_delimiter`-separated columns. The `vocab_token_index` column should\n    contain the vocabulary term, while the `vocab_freq_index` column should\n    contain the number of times that term occurs in the corpus. For example,\n    with a text vocabulary file of:\n\n      ```\n      bonjour,fr,42\n      hello,en,777\n      hola,es,99\n      ```\n\n    You should set `vocab_delimiter="",""`, `vocab_token_index=0`, and\n    `vocab_freq_index=2`.\n\n    See `skip_gram_sample()` documentation for more details about the skip-gram\n    sampling process.\n\n    Args:\n      input_tensor:\n        A rank-1 `Tensor` from which to generate skip-gram candidates.\n      vocab_freq_file:\n        `string` specifying full file path to the text vocab file.\n      vocab_token_index: `int` specifying which column in the text vocab file\n        contains the tokens.\n      vocab_token_dtype:\n        `DType` specifying the format of the tokens in the text vocab file.\n      vocab_freq_index: `int` specifying which column in the text vocab file\n        contains the frequency counts of the tokens.\n      vocab_freq_dtype: `DType` specifying the format of the frequency counts\n        in the text vocab file.\n      vocab_delimiter: `string` specifying the delimiter used in the text vocab\n        file.\n      vocab_min_count: `int`, `float`, or scalar `Tensor` specifying\n        minimum frequency threshold (from `vocab_freq_file`) for a token to be\n        kept in `input_tensor`. This should correspond with `vocab_freq_dtype`.\n      vocab_subsampling: (Optional) `float` specifying frequency proportion\n        threshold for tokens from `input_tensor`. Tokens that occur more\n        frequently will be randomly down-sampled. Reasonable starting values\n        may be around 1e-3 or 1e-5. See Eq. 5 in http://arxiv.org/abs/1310.4546\n        for more details.\n      corpus_size: (Optional) `int`, `float`, or scalar `Tensor` specifying the\n        total number of tokens in the corpus (e.g., sum of all the frequency\n        counts of `vocab_freq_file`). Used with `vocab_subsampling` for\n        down-sampling frequently occurring tokens. If this is specified,\n        `vocab_freq_file` and `vocab_subsampling` must also be specified.\n        If `corpus_size` is needed but not supplied, then it will be calculated\n        from `vocab_freq_file`. You might want to supply your own value if you\n        have already eliminated infrequent tokens from your vocabulary files\n        (where frequency < vocab_min_count) to save memory in the internal\n        token lookup table. Otherwise, the unused tokens\' variables will waste\n        memory.  The user-supplied `corpus_size` value must be greater than or\n        equal to the sum of all the frequency counts of `vocab_freq_file`.\n      min_skips: `int` or scalar `Tensor` specifying the minimum window size to\n        randomly use for each token. Must be >= 0 and <= `max_skips`. If\n        `min_skips` and `max_skips` are both 0, the only label outputted will\n        be the token itself.\n      max_skips: `int` or scalar `Tensor` specifying the maximum window size to\n        randomly use for each token. Must be >= 0.\n      start: `int` or scalar `Tensor` specifying the position in `input_tensor`\n        from which to start generating skip-gram candidates.\n      limit: `int` or scalar `Tensor` specifying the maximum number of elements\n        in `input_tensor` to use in generating skip-gram candidates. -1 means\n        to use the rest of the `Tensor` after `start`.\n      emit_self_as_target: `bool` or scalar `Tensor` specifying whether to emit\n        each token as a label for itself.\n      batch_size: (Optional) `int` specifying batch size of returned `Tensors`.\n      batch_capacity: (Optional) `int` specifying batch capacity for the queue\n        used for batching returned `Tensors`. Only has an effect if\n        `batch_size` > 0. Defaults to 100 * `batch_size` if not specified.\n      seed: (Optional) `int` used to create a random seed for window size and\n        subsampling. See\n        [`set_random_seed`](../../g3doc/python/constant_op.md#set_random_seed)\n        for behavior.\n      name: (Optional) A `string` name or a name scope for the operations.\n\n    Returns:\n      A `tuple` containing (token, label) `Tensors`. Each output `Tensor` is of\n      rank-1 and has the same type as `input_tensor`. The `Tensors` will be of\n      length `batch_size`; if `batch_size` is not specified, they will be of\n      random length, though they will be in sync with each other as long as\n      they are evaluated together.\n\n    Raises:\n      ValueError: If `vocab_token_index` or `vocab_freq_index` is less than 0\n        or exceeds the number of columns in `vocab_freq_file`.\n        If `vocab_token_index` and `vocab_freq_index` are both set to the same\n        column. If any token in `vocab_freq_file` has a negative frequency.\n    """"""\n\n    if vocab_token_index < 0 or vocab_freq_index < 0:\n        raise ValueError(\n            ""vocab_token_index={} and vocab_freq_index={} must both be >= 0."".format(\n                vocab_token_index, vocab_freq_index\n            )\n        )\n    if vocab_token_index == vocab_freq_index:\n        raise ValueError(\n            ""vocab_token_index and vocab_freq_index should be different, ""\n            ""but are both {}."".format(vocab_token_index)\n        )\n\n    # Iterates through the vocab file and calculates the number of vocab terms as\n    # well as the total corpus size (by summing the frequency counts of all the\n    # vocab terms).\n    calculated_corpus_size = 0.0\n    vocab_size = 0\n    with tf.io.gfile.GFile(vocab_freq_file, mode=""r"") as f:\n        reader = csv.reader(f, delimiter=vocab_delimiter)\n        for row in reader:\n            if vocab_token_index >= len(row) or vocab_freq_index >= len(row):\n                raise ValueError(\n                    ""Row in vocab file only has {} columns, ""\n                    ""so vocab_token_index={} or ""\n                    ""vocab_freq_index={} is out of bounds. Row content: {}"".format(\n                        len(row), vocab_token_index, vocab_freq_index, row\n                    )\n                )\n            vocab_size += 1\n            freq = vocab_freq_dtype.as_numpy_dtype(row[vocab_freq_index])\n            if freq < 0:\n                raise ValueError(\n                    ""Row in vocab file has negative frequency of {}. ""\n                    ""Row content: {}"".format(freq, row)\n                )\n            # Note: tokens whose frequencies are below vocab_min_count will still\n            # contribute to the total corpus size used for vocab subsampling.\n            calculated_corpus_size += freq\n\n    if not corpus_size:\n        corpus_size = calculated_corpus_size\n    elif calculated_corpus_size - corpus_size > 1e-6:\n        raise ValueError(\n            ""`corpus_size`={} must be greater than or equal to the ""\n            ""sum of all the frequency counts ({}) of `vocab_freq_file` ({})."".format(\n                corpus_size, calculated_corpus_size, vocab_freq_file\n            )\n        )\n\n    vocab_freq_table = tf.lookup.StaticHashTable(\n        tf.lookup.TextFileInitializer(\n            filename=vocab_freq_file,\n            key_dtype=vocab_token_dtype,\n            key_index=vocab_token_index,\n            value_dtype=vocab_freq_dtype,\n            value_index=vocab_freq_index,\n            vocab_size=vocab_size,\n            delimiter=vocab_delimiter,\n        ),\n        # For vocab terms not in vocab file, use a default value of -1.\n        default_value=-1,\n    )\n\n    return skip_gram_sample(\n        input_tensor,\n        min_skips=min_skips,\n        max_skips=max_skips,\n        start=start,\n        limit=limit,\n        emit_self_as_target=emit_self_as_target,\n        vocab_freq_table=vocab_freq_table,\n        vocab_min_count=vocab_min_count,\n        vocab_subsampling=vocab_subsampling,\n        # corpus_size is not used unless vocab_subsampling is specified.\n        corpus_size=None if vocab_subsampling is None else corpus_size,\n        batch_size=batch_size,\n        batch_capacity=batch_capacity,\n        seed=seed,\n        name=name,\n    )\n\n\ndef _filter_input(\n    input_tensor,\n    vocab_freq_table,\n    vocab_min_count,\n    vocab_subsampling,\n    corpus_size,\n    seed,\n):\n    input_tensor = tf.convert_to_tensor(input_tensor)\n    """"""Filters input tensor based on vocab freq, threshold, and subsampling.""""""\n    if vocab_freq_table is None:\n        return input_tensor\n\n    if not isinstance(vocab_freq_table, tf.lookup.StaticHashTable):\n        raise ValueError(\n            ""vocab_freq_table must be a subclass of ""\n            ""InitializableLookupTableBase (such as HashTable) instead of type ""\n            ""{}."".format(type(vocab_freq_table))\n        )\n\n    with tf.name_scope(""filter_vocab""):\n        freq = vocab_freq_table.lookup(input_tensor)\n        # Filters out elements in input_tensor that are not found in\n        # vocab_freq_table (table returns a default value of -1 specified above when\n        # an element is not found).\n        mask = tf.math.not_equal(freq, vocab_freq_table.default_value)\n\n        # Filters out elements whose vocab frequencies are less than the threshold.\n        if vocab_min_count is not None:\n            cast_threshold = tf.cast(vocab_min_count, freq.dtype)\n            mask = tf.math.logical_and(\n                mask, tf.math.greater_equal(freq, cast_threshold)\n            )\n\n        input_tensor = tf.boolean_mask(input_tensor, mask)\n        freq = tf.boolean_mask(freq, mask)\n\n    if not vocab_subsampling:\n        return input_tensor\n\n    if vocab_subsampling < 0 or vocab_subsampling > 1:\n        raise ValueError(\n            ""Invalid vocab_subsampling={} - it should be within range [0, 1]."".format(\n                vocab_subsampling\n            )\n        )\n\n    # Subsamples the input tokens based on vocabulary frequency and\n    # vocab_subsampling threshold (ie randomly discard commonly appearing\n    # tokens).\n    with tf.name_scope(""subsample_vocab""):\n        corpus_size = tf.cast(corpus_size, tf.dtypes.float64)\n        freq = tf.cast(freq, tf.dtypes.float64)\n        vocab_subsampling = tf.cast(vocab_subsampling, tf.dtypes.float64)\n\n        # From tensorflow_models/tutorials/embedding/word2vec_kernels.cc, which is\n        # suppose to correlate with Eq. 5 in http://arxiv.org/abs/1310.4546.\n        keep_prob = (tf.math.sqrt(freq / (vocab_subsampling * corpus_size)) + 1.0) * (\n            vocab_subsampling * corpus_size / freq\n        )\n        random_prob = tf.random.uniform(\n            tf.shape(freq), minval=0, maxval=1, dtype=tf.dtypes.float64, seed=seed\n        )\n\n        mask = tf.math.less_equal(random_prob, keep_prob)\n        return tf.boolean_mask(input_tensor, mask)\n'"
tensorflow_addons/utils/__init__.py,0,b''
tensorflow_addons/utils/ensure_tf_install.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n# Ensure the TensorFlow version is in the right range. This\n# needs to happen before anything else, since the imports below will try to\n# import TensorFlow, too.\n\nfrom distutils.version import LooseVersion\nimport warnings\n\nimport tensorflow as tf\n\nMIN_TF_VERSION = ""2.2.0""\nMAX_TF_VERSION = ""2.3.0""\n\n\ndef _check_tf_version():\n    """"""Warn the user if the version of TensorFlow used is not supported.\n\n    This is not a check for custom ops compatibility. This check only ensure that\n    we support this TensorFlow version if the user uses only Addons\' Python code.\n    """"""\n\n    if ""dev"" in tf.__version__:\n        warnings.warn(\n            ""You are currently using a nightly version of TensorFlow ({}). \\n""\n            ""TensorFlow Addons offers no support for the nightly versions of ""\n            ""TensorFlow. Some things might work, some other might not. \\n""\n            ""If you encounter a bug, do not file an issue on GitHub.""\n            """".format(tf.__version__),\n            UserWarning,\n        )\n        return\n\n    min_version = LooseVersion(MIN_TF_VERSION)\n    max_version = LooseVersion(MAX_TF_VERSION)\n\n    if min_version <= LooseVersion(tf.__version__) < max_version:\n        return\n\n    warnings.warn(\n        ""Tensorflow Addons supports using Python ops for all Tensorflow versions ""\n        ""above or equal to {} and strictly below {} (nightly versions are not ""\n        ""supported). \\n ""\n        ""The versions of TensorFlow you are currently using is {} and is not ""\n        ""supported. \\n""\n        ""Some things might work, some things might not.\\n""\n        ""If you were to encounter a bug, do not file an issue.\\n""\n        ""If you want to make sure you\'re using a tested and supported configuration, ""\n        ""either change the TensorFlow version or the TensorFlow Addons\'s version. \\n""\n        ""You can find the compatibility matrix in TensorFlow Addon\'s readme:\\n""\n        ""https://github.com/tensorflow/addons"".format(\n            MIN_TF_VERSION, MAX_TF_VERSION, tf.__version__\n        ),\n        UserWarning,\n    )\n'"
tensorflow_addons/utils/keras_utils.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for tf.keras.""""""\n\nimport tensorflow as tf\n\n\ndef is_tensor_or_variable(x):\n    return tf.is_tensor(x) or isinstance(x, tf.Variable)\n\n\nclass LossFunctionWrapper(tf.keras.losses.Loss):\n    """"""Wraps a loss function in the `Loss` class.""""""\n\n    def __init__(\n        self, fn, reduction=tf.keras.losses.Reduction.AUTO, name=None, **kwargs\n    ):\n        """"""Initializes `LossFunctionWrapper` class.\n\n    Args:\n      fn: The loss function to wrap, with signature `fn(y_true, y_pred,\n        **kwargs)`.\n      reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n        loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n        option will be determined by the usage context. For almost all cases\n        this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n        `tf.distribute.Strategy`, outside of built-in training loops such as\n        `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n        will raise an error. Please see this custom training [tutorial](\n          https://www.tensorflow.org/tutorials/distribute/custom_training)\n        for more details.\n      name: (Optional) name for the loss.\n      **kwargs: The keyword arguments that are passed on to `fn`.\n    """"""\n        super().__init__(reduction=reduction, name=name)\n        self.fn = fn\n        self._fn_kwargs = kwargs\n\n    def call(self, y_true, y_pred):\n        """"""Invokes the `LossFunctionWrapper` instance.\n\n    Args:\n      y_true: Ground truth values.\n      y_pred: The predicted values.\n\n    Returns:\n      Loss values per sample.\n    """"""\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n\n    def get_config(self):\n        config = {}\n        for k, v in iter(self._fn_kwargs.items()):\n            config[k] = tf.keras.backend.eval(v) if is_tensor_or_variable(v) else v\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n\ndef normalize_data_format(value):\n    if value is None:\n        value = tf.keras.backend.image_data_format()\n    data_format = value.lower()\n    if data_format not in {""channels_first"", ""channels_last""}:\n        raise ValueError(\n            ""The `data_format` argument must be one of ""\n            \'""channels_first"", ""channels_last"". Received: \' + str(value)\n        )\n    return data_format\n\n\ndef normalize_tuple(value, n, name):\n    """"""Transforms an integer or iterable of integers into an integer tuple.\n\n    A copy of tensorflow.python.keras.util.\n\n    Arguments:\n      value: The value to validate and convert. Could an int, or any iterable\n        of ints.\n      n: The size of the tuple to be returned.\n      name: The name of the argument being validated, e.g. ""strides"" or\n        ""kernel_size"". This is only used to format error messages.\n\n    Returns:\n      A tuple of n integers.\n\n    Raises:\n      ValueError: If something else than an int/long or iterable thereof was\n        passed.\n    """"""\n    if isinstance(value, int):\n        return (value,) * n\n    else:\n        try:\n            value_tuple = tuple(value)\n        except TypeError:\n            raise TypeError(\n                ""The `""\n                + name\n                + ""` argument must be a tuple of ""\n                + str(n)\n                + "" integers. Received: ""\n                + str(value)\n            )\n        if len(value_tuple) != n:\n            raise ValueError(\n                ""The `""\n                + name\n                + ""` argument must be a tuple of ""\n                + str(n)\n                + "" integers. Received: ""\n                + str(value)\n            )\n        for single_value in value_tuple:\n            try:\n                int(single_value)\n            except (ValueError, TypeError):\n                raise ValueError(\n                    ""The `""\n                    + name\n                    + ""` argument must be a tuple of ""\n                    + str(n)\n                    + "" integers. Received: ""\n                    + str(value)\n                    + "" ""\n                    ""including element ""\n                    + str(single_value)\n                    + "" of type""\n                    + "" ""\n                    + str(type(single_value))\n                )\n        return value_tuple\n\n\ndef _hasattr(obj, attr_name):\n    # If possible, avoid retrieving the attribute as the object might run some\n    # lazy computation in it.\n    if attr_name in dir(obj):\n        return True\n    try:\n        getattr(obj, attr_name)\n    except AttributeError:\n        return False\n    else:\n        return True\n\n\ndef assert_like_rnncell(cell_name, cell):\n    """"""Raises a TypeError if cell is not like a\n    tf.keras.layers.AbstractRNNCell.\n\n    Args:\n      cell_name: A string to give a meaningful error referencing to the name\n        of the function argument.\n      cell: The object which should behave like a\n        tf.keras.layers.AbstractRNNCell.\n\n    Raises:\n      TypeError: A human-friendly exception.\n    """"""\n    conditions = [\n        _hasattr(cell, ""output_size""),\n        _hasattr(cell, ""state_size""),\n        _hasattr(cell, ""get_initial_state""),\n        callable(cell),\n    ]\n\n    errors = [\n        ""\'output_size\' property is missing"",\n        ""\'state_size\' property is missing"",\n        ""\'get_initial_state\' method is required"",\n        ""is not callable"",\n    ]\n\n    if not all(conditions):\n        errors = [error for error, cond in zip(errors, conditions) if not cond]\n        raise TypeError(\n            ""The argument {!r} ({}) is not an RNNCell: {}."".format(\n                cell_name, cell, "", "".join(errors)\n            )\n        )\n'"
tensorflow_addons/utils/resource_loader.py,5,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities similar to tf.python.platform.resource_loader.""""""\n\nfrom distutils.version import LooseVersion\nimport os\nimport warnings\n\nimport tensorflow as tf\n\nMIN_TF_VERSION_FOR_ABI_COMPATIBILITY = ""2.2.0""\nMAX_TF_VERSION_FOR_ABI_COMPATIBILITY = ""2.3.0""\nabi_warning_already_raised = False\nSKIP_CUSTOM_OPS = False\n\n\ndef get_project_root():\n    """"""Returns project root folder.""""""\n    return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef get_path_to_datafile(path):\n    """"""Get the path to the specified file in the data dependencies.\n\n    The path is relative to tensorflow_addons/\n\n    Args:\n      path: a string resource path relative to tensorflow_addons/\n    Returns:\n      The path to the specified data file\n    """"""\n    root_dir = get_project_root()\n    return os.path.join(root_dir, path.replace(""/"", os.sep))\n\n\nclass LazySO:\n    def __init__(self, relative_path):\n        self.relative_path = relative_path\n        self._ops = None\n\n    @property\n    def ops(self):\n        if SKIP_CUSTOM_OPS:\n            import pytest\n\n            pytest.skip(\n                ""Skipping the test because a custom ops ""\n                ""was being loaded while --skip-custom-ops was set.""\n            )\n        if self._ops is None:\n            self.display_warning_if_incompatible()\n            self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))\n        return self._ops\n\n    def display_warning_if_incompatible(self):\n        global abi_warning_already_raised\n        if abi_is_compatible() or abi_warning_already_raised:\n            return\n\n        warnings.warn(\n            ""You are currently using TensorFlow {} and trying to load a custom op ({}).""\n            ""\\n""\n            ""TensorFlow Addons has compiled its custom ops against TensorFlow {}, ""\n            ""and there are no compatibility guarantees between the two versions. ""\n            ""\\n""\n            ""This means that you might get segfaults when loading the custom op, ""\n            ""or other kind of low-level errors.\\n If you do, do not file an issue ""\n            ""on Github. This is a known limitation.""\n            ""\\n\\n""\n            ""It might help you to fallback to pure Python ""\n            ""ops with TF_ADDONS_PY_OPS . To do that, see ""\n            ""https://github.com/tensorflow/addons#gpucpu-custom-ops ""\n            ""\\n\\n""\n            ""You can also change the TensorFlow version installed on your system. ""\n            ""You would need a TensorFlow version equal to or above {} and strictly ""\n            ""below {}.\\n Note that nightly versions of TensorFlow, ""\n            ""as well as non-pip TensorFlow like `conda install tensorflow` or compiled ""\n            ""from source are not supported.""\n            ""\\n\\n""\n            ""The last solution is to find the TensorFlow Addons version that has ""\n            ""custom ops compatible with the TensorFlow installed on your ""\n            ""system. To do that, refer to the readme: ""\n            ""https://github.com/tensorflow/addons""\n            """".format(\n                tf.__version__,\n                self.relative_path,\n                MIN_TF_VERSION_FOR_ABI_COMPATIBILITY,\n                MIN_TF_VERSION_FOR_ABI_COMPATIBILITY,\n                MAX_TF_VERSION_FOR_ABI_COMPATIBILITY,\n            ),\n            UserWarning,\n        )\n        abi_warning_already_raised = True\n\n\ndef abi_is_compatible():\n    if ""dev"" in tf.__version__:\n        # tf-nightly\n        return False\n\n    min_version = LooseVersion(MIN_TF_VERSION_FOR_ABI_COMPATIBILITY)\n    max_version = LooseVersion(MAX_TF_VERSION_FOR_ABI_COMPATIBILITY)\n    return min_version <= LooseVersion(tf.__version__) < max_version\n'"
tensorflow_addons/utils/test_utils.py,16,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for testing Addons.""""""\n\nimport os\nimport random\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import resource_loader\n\n# TODO: copy the layer_test implementation in Addons.\nfrom tensorflow.python.keras.testing_utils import layer_test  # noqa: F401\n\n\nNUMBER_OF_WORKERS = int(os.environ.get(""PYTEST_XDIST_WORKER_COUNT"", ""1""))\nWORKER_ID = int(os.environ.get(""PYTEST_XDIST_WORKER"", ""gw0"")[2])\nNUMBER_OF_GPUS = len(tf.config.list_physical_devices(""GPU""))\n\n\ndef is_gpu_available():\n    return NUMBER_OF_GPUS >= 1\n\n\n# Some configuration before starting the tests.\n\n# we only need one core per worker.\n# This avoids context switching for speed, but it also prevents TensorFlow to go\n# crazy on systems with many cores (kokoro has 30+ cores).\ntf.config.threading.set_intra_op_parallelism_threads(1)\ntf.config.threading.set_inter_op_parallelism_threads(1)\n\nif is_gpu_available():\n    # We use only the first gpu at the moment. That\'s enough for most use cases.\n    # split the first gpu into chunks of 100MB per virtual device.\n    # It\'s the user\'s job to limit the amount of pytest workers depending\n    # on the available memory.\n    # In practice, each process takes a bit more memory.\n    # There must be some kind of overhead but it\'s not very big (~200MB more)\n    # Each worker has two virtual devices.\n    # When running on gpu, only the first device is used. The other one is used\n    # in distributed strategies.\n    first_gpu = tf.config.list_physical_devices(""GPU"")[0]\n    virtual_gpus = [\n        tf.config.LogicalDeviceConfiguration(memory_limit=100) for _ in range(2)\n    ]\n\n    tf.config.set_logical_device_configuration(first_gpu, virtual_gpus)\n\n\ndef finalizer():\n    tf.config.experimental_run_functions_eagerly(False)\n\n\ndef pytest_make_parametrize_id(config, val, argname):\n    if isinstance(val, tf.DType):\n        return val.name\n    if val is False:\n        return ""no_"" + argname\n    if val is True:\n        return argname\n\n\n@pytest.fixture(scope=""function"", params=[""eager_mode"", ""tf_function""])\ndef maybe_run_functions_eagerly(request):\n    if request.param == ""eager_mode"":\n        tf.config.experimental_run_functions_eagerly(True)\n    elif request.param == ""tf_function"":\n        tf.config.experimental_run_functions_eagerly(False)\n\n    request.addfinalizer(finalizer)\n\n\n@pytest.fixture(scope=""function"", params=[""channels_first"", ""channels_last""])\ndef data_format(request):\n    return request.param\n\n\n@pytest.fixture(scope=""function"", autouse=True)\ndef set_seeds():\n    random.seed(0)\n    np.random.seed(0)\n    tf.random.set_seed(0)\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--skip-custom-ops"",\n        action=""store_true"",\n        help=""When a custom op is being loaded in a test, skip this test."",\n    )\n\n\ndef gpus_for_testing():\n    """"""For the moment it\'s very simple, but it might change in the future,\n    with multiple physical gpus for example. So it\'s better if this function\n    is called rather than hardcoding the gpu devices in the tests.\n    """"""\n    if not is_gpu_available():\n        raise SystemError(\n            ""You are trying to get some gpus for testing but no gpu is available on ""\n            ""your system. \\nDid you forget to use `@pytest.mark.needs_gpu` on your test""\n            "" so that it\'s skipped automatically when no gpu is available?""\n        )\n    return [""gpu:0"", ""gpu:1""]\n\n\n@pytest.fixture(scope=""session"", autouse=True)\ndef set_global_variables(request):\n    if request.config.getoption(""--skip-custom-ops""):\n        resource_loader.SKIP_CUSTOM_OPS = True\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        ""markers"", ""with_device(devices): mark test to run on specific devices.""\n    )\n    config.addinivalue_line(""markers"", ""needs_gpu: mark test that needs a gpu."")\n\n\n@pytest.fixture(autouse=True, scope=""function"")\ndef device(request):\n    requested_device = request.param\n    if requested_device == ""no_device"":\n        yield requested_device\n    elif requested_device == tf.distribute.MirroredStrategy:\n        strategy = requested_device(gpus_for_testing())\n        with strategy.scope():\n            yield strategy\n    elif isinstance(requested_device, str):\n        if requested_device in [""cpu"", ""gpu""]:\n            # we use GPU:0 because the virtual device we created is the\n            # only one in the first GPU (so first in the list of virtual devices).\n            requested_device += "":0""\n        else:\n            raise KeyError(""Invalid device: "" + requested_device)\n        with tf.device(requested_device):\n            yield requested_device\n\n\ndef get_marks(device_name):\n    if device_name == ""gpu"" or device_name == tf.distribute.MirroredStrategy:\n        return [pytest.mark.needs_gpu]\n    else:\n        return []\n\n\ndef pytest_generate_tests(metafunc):\n    marker = metafunc.definition.get_closest_marker(""with_device"")\n    if marker is None:\n        # tests which don\'t have the ""with_device"" mark are executed on CPU\n        # to ensure reproducibility. We can\'t let TensorFlow decide\n        # where to place the ops.\n        devices = [""cpu""]\n    else:\n        devices = marker.args[0]\n\n    parameters = [pytest.param(x, marks=get_marks(x)) for x in devices]\n    metafunc.parametrize(""device"", parameters, indirect=True)\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if item.get_closest_marker(""needs_gpu"") is not None:\n            if not is_gpu_available():\n                item.add_marker(pytest.mark.skip(""The gpu is not available.""))\n\n\ndef assert_allclose_according_to_type(\n    a,\n    b,\n    rtol=1e-6,\n    atol=1e-6,\n    float_rtol=1e-6,\n    float_atol=1e-6,\n    half_rtol=1e-3,\n    half_atol=1e-3,\n    bfloat16_rtol=1e-2,\n    bfloat16_atol=1e-2,\n):\n    """"""\n    Similar to tf.test.TestCase.assertAllCloseAccordingToType()\n    but this doesn\'t need a subclassing to run.\n    """"""\n    a = np.array(a)\n    b = np.array(b)\n    # types with lower tol are put later to overwrite previous ones.\n    if (\n        a.dtype == np.float32\n        or b.dtype == np.float32\n        or a.dtype == np.complex64\n        or b.dtype == np.complex64\n    ):\n        rtol = max(rtol, float_rtol)\n        atol = max(atol, float_atol)\n    if a.dtype == np.float16 or b.dtype == np.float16:\n        rtol = max(rtol, half_rtol)\n        atol = max(atol, half_atol)\n    if a.dtype == tf.bfloat16.as_numpy_dtype or b.dtype == tf.bfloat16.as_numpy_dtype:\n        rtol = max(rtol, bfloat16_rtol)\n        atol = max(atol, bfloat16_atol)\n\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n'"
tensorflow_addons/utils/types.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Types for typing functions signatures.""""""\n\nfrom typing import Union, Callable, List\n\nimport numpy as np\nimport tensorflow as tf\n\n\nNumber = Union[\n    float,\n    int,\n    np.float16,\n    np.float32,\n    np.float64,\n    np.int8,\n    np.int16,\n    np.int32,\n    np.int64,\n    np.uint8,\n    np.uint16,\n    np.uint32,\n    np.uint64,\n]\n\nInitializer = Union[None, dict, str, Callable]\nRegularizer = Union[None, dict, str, Callable]\nConstraint = Union[None, dict, str, Callable]\nActivation = Union[None, str, Callable]\nOptimizer = Union[tf.keras.optimizers.Optimizer, str]\n\nTensorLike = Union[\n    List[Union[Number, list]],\n    tuple,\n    Number,\n    np.ndarray,\n    tf.Tensor,\n    tf.SparseTensor,\n    tf.Variable,\n]\nFloatTensorLike = Union[tf.Tensor, float, np.float16, np.float32, np.float64]\nAcceptableDTypes = Union[tf.DType, np.dtype, type, int, str, None]\n'"
tools/testing/source_code_test.py,14,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#\nimport glob\nimport os\n\nfrom typedapi import ensure_api_is_typed\n\nimport tensorflow_addons as tfa\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ""../..""))\n\n\ndef test_api_typed():\n    modules_list = [\n        tfa,\n        tfa.activations,\n        tfa.callbacks,\n        tfa.image,\n        tfa.losses,\n        tfa.metrics,\n        tfa.optimizers,\n        tfa.rnn,\n        tfa.seq2seq,\n        tfa.text,\n    ]\n    # Files within this list will be exempt from verification.\n    exception_list = []\n    help_message = (\n        ""You can also take a look at the section about it in the CONTRIBUTING.md:\\n""\n        ""https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md#about-type-hints""\n    )\n    ensure_api_is_typed(\n        modules_list, exception_list, init_only=True, additional_message=help_message,\n    )\n\n\ndef test_case_insensitive_filesystems():\n    # Make sure BASE_DIR is project root.\n    # If it doesn\'t, we probably computed the wrong directory.\n    if not os.path.isdir(os.path.join(BASE_DIR, ""tensorflow_addons"")):\n        raise AssertionError(""BASE_DIR = {} is not project root"".format(BASE_DIR))\n\n    for dirpath, dirnames, filenames in os.walk(BASE_DIR, followlinks=True):\n        lowercase_directories = [x.lower() for x in dirnames]\n        lowercase_files = [x.lower() for x in filenames]\n\n        lowercase_dir_contents = lowercase_directories + lowercase_files\n        if len(lowercase_dir_contents) != len(set(lowercase_dir_contents)):\n            raise AssertionError(\n                ""Files with same name but different case detected ""\n                ""in directory: {}"".format(dirpath)\n            )\n\n\ndef get_lines_of_source_code(blacklist=None):\n    blacklist = blacklist or []\n    source_dir = os.path.join(BASE_DIR, ""tensorflow_addons"")\n    for path in glob.glob(source_dir + ""/**/*.py"", recursive=True):\n        if in_blacklist(path, blacklist):\n            continue\n        with open(path) as f:\n            for line_idx, line in enumerate(f):\n                yield path, line_idx, line\n\n\ndef in_blacklist(file_path, blacklist):\n    for blacklisted_file in blacklist:\n        if file_path.endswith(blacklisted_file):\n            return True\n    return False\n\n\ndef test_no_private_tf_api():\n    # TODO: remove all elements of the list and remove the blacklist\n    # Unlike the exception list for functions/classes missing types,\n    # this blacklist should not grow. Do not add elements to this list.\n    blacklist = [\n        ""tensorflow_addons/optimizers/novograd.py"",\n        ""tensorflow_addons/optimizers/moving_average.py"",\n        ""tensorflow_addons/metrics/r_square.py"",\n        ""tensorflow_addons/utils/test_utils.py"",\n        ""tensorflow_addons/seq2seq/decoder.py"",\n        ""tensorflow_addons/seq2seq/attention_wrapper.py"",\n    ]\n\n    for file_path, line_idx, line in get_lines_of_source_code(blacklist):\n\n        if ""import tensorflow.python"" in line or ""from tensorflow.python"" in line:\n            raise ImportError(\n                ""A private tensorflow API import was found in {} at line {}.\\n""\n                ""tensorflow.python refers to TensorFlow\'s internal source ""\n                ""code and private functions/classes.\\n""\n                ""The use of those is forbidden in Addons for stability reasons.""\n                ""\\nYou should find a public alternative or ask the ""\n                ""TensorFlow team to expose publicly the function/class ""\n                ""that you are using.\\n""\n                ""If you\'re trying to do `import tensorflow.python.keras` ""\n                ""it can be replaced with `import tensorflow.keras`.""\n                """".format(file_path, line_idx + 1)\n            )\n\n\ndef test_no_tf_cond():\n    # TODO: remove all elements of the list and remove the blacklist\n    # Unlike the exception list for functions/classes missing types,\n    # this blacklist should not grow. Do not add elements to this list.\n    blacklist = [\n        ""tensorflow_addons/text/crf.py"",\n        ""tensorflow_addons/layers/wrappers.py"",\n        ""tensorflow_addons/image/connected_components.py"",\n        ""tensorflow_addons/optimizers/novograd.py"",\n        ""tensorflow_addons/metrics/cohens_kappa.py"",\n        ""tensorflow_addons/seq2seq/sampler.py"",\n        ""tensorflow_addons/seq2seq/beam_search_decoder.py"",\n    ]\n    for file_path, line_idx, line in get_lines_of_source_code(blacklist):\n\n        if ""tf.cond("" in line:\n            raise NameError(\n                ""The usage of a tf.cond() function call was found in ""\n                ""file {} at line {}:\\n\\n""\n                ""   {}\\n""\n                ""In TensorFlow 2.x, using a simple `if` in a function decorated ""\n                ""with `@tf.function` is equivalent to a tf.cond() thanks to Autograph. \\n""\n                ""TensorFlow Addons aims to be written with idiomatic TF 2.x code. \\n""\n                ""As such, using tf.cond() is not allowed in the codebase. \\n""\n                ""Use a `if` and decorate your function with @tf.function instead. \\n""\n                ""You can take a look at ""\n                ""https://www.tensorflow.org/guide/function#use_python_control_flow""\n                """".format(file_path, line_idx, line)\n            )\n\n\ndef test_no_experimental_api():\n    # TODO: remove all elements of the list and remove the blacklist\n    # Unlike the exception list for functions/classes missing types,\n    # this blacklist should not grow. Do not add elements to this list.\n    blacklist = [\n        ""tensorflow_addons/optimizers/weight_decay_optimizers.py"",\n    ]\n    for file_path, line_idx, line in get_lines_of_source_code(blacklist):\n\n        if file_path.endswith(""_test.py"") or file_path.endswith(""conftest.py""):\n            continue\n        if file_path.endswith(""tensorflow_addons/utils/test_utils.py""):\n            continue\n\n        if ""experimental"" in line:\n            raise NameError(\n                ""The usage of a TensorFlow experimental API was found in file {} ""\n                ""at line {}:\\n\\n""\n                ""   {}\\n""\n                ""Experimental APIs are ok in tests but not in user-facing code. ""\n                ""This is because Experimental APIs might have bugs and are not ""\n                ""widely used yet.\\n""\n                ""Addons should show how to write TensorFlow ""\n                ""code in a stable and forward-compatible way.""\n                """".format(file_path, line_idx, line)\n            )\n\n\ndef test_no_tf_control_dependencies():\n    # TODO: remove all elements of the list and remove the blacklist\n    # Unlike the exception list for functions/classes missing types,\n    # this blacklist should not grow. Do not add elements to this list.\n    blacklist = [\n        ""tensorflow_addons/layers/wrappers.py"",\n        ""tensorflow_addons/image/utils.py"",\n        ""tensorflow_addons/image/dense_image_warp.py"",\n        ""tensorflow_addons/optimizers/stochastic_weight_averaging.py"",\n        ""tensorflow_addons/optimizers/average_wrapper.py"",\n        ""tensorflow_addons/optimizers/yogi.py"",\n        ""tensorflow_addons/optimizers/lookahead.py"",\n        ""tensorflow_addons/optimizers/weight_decay_optimizers.py"",\n        ""tensorflow_addons/optimizers/rectified_adam.py"",\n        ""tensorflow_addons/optimizers/lamb.py"",\n        ""tensorflow_addons/seq2seq/sampler.py"",\n        ""tensorflow_addons/seq2seq/beam_search_decoder.py"",\n        ""tensorflow_addons/seq2seq/attention_wrapper.py"",\n    ]\n    for file_path, line_idx, line in get_lines_of_source_code(blacklist):\n\n        if ""tf.control_dependencies("" in line:\n\n            raise NameError(\n                ""The usage of a tf.control_dependencies() function call was found in ""\n                ""file {} at line {}:\\n\\n""\n                ""   {}\\n""\n                ""In TensorFlow 2.x, in a function decorated ""\n                ""with `@tf.function` the dependencies are controlled automatically""\n                "" thanks to Autograph. \\n""\n                ""TensorFlow Addons aims to be written with idiomatic TF 2.x code. \\n""\n                ""As such, using tf.control_dependencies() is not allowed in the codebase. \\n""\n                ""Decorate your function with @tf.function instead. \\n""\n                ""You can take a look at \\n""\n                ""https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md#program-order-semantics--control-dependencies""\n                """".format(file_path, line_idx, line)\n            )\n\n\ndef test_no_deprecated_v1():\n    # TODO: remove all elements of the list and remove the blacklist\n    # Unlike the exception list for functions/classes missing types,\n    # this blacklist should not grow. Do not add elements to this list.\n    blacklist = [\n        ""tensorflow_addons/text/skip_gram_ops.py"",\n        ""tensorflow_addons/metrics/tests/f_scores_test.py"",\n        ""tensorflow_addons/seq2seq/tests/basic_decoder_test.py"",\n        ""tensorflow_addons/seq2seq/tests/beam_search_decoder_test.py"",\n        ""tensorflow_addons/seq2seq/decoder.py"",\n        ""tensorflow_addons/seq2seq/tests/attention_wrapper_test.py"",\n    ]\n    for file_path, line_idx, line in get_lines_of_source_code(blacklist):\n\n        if ""tf.compat.v1"" in line:\n            raise NameError(\n                ""The usage of a tf.compat.v1 API was found in file {} at line {}:\\n\\n""\n                ""   {}\\n""\n                ""TensorFlow Addons doesn\'t support running programs with ""\n                ""`tf.compat.v1.disable_v2_behavior()`.\\n""\n                ""As such, there should be no need for the compatibility module ""\n                ""tf.compat. Please find an alternative using only the TF2.x API.""\n                """".format(file_path, line_idx, line)\n            )\n'"
build_deps/toolchains/gpu/find_cuda_config.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Prints CUDA library and header directories and versions found on the system.\n\nThe script searches for CUDA library and header files on the system, inspects\nthem to determine their version and prints the configuration to stdout.\nThe paths to inspect and the required versions are specified through\nenvironment variables. If no valid configuration is found, the script prints\nto stderr and returns an error code.\n\nThe list of libraries to find is specified as arguments. Supported libraries\nare CUDA (includes cuBLAS), cuDNN, NCCL, and TensorRT.\n\nThe script takes a list of base directories specified by the TF_CUDA_PATHS\nenvironment variable as comma-separated glob list. The script looks for headers\nand library files in a hard-coded set of subdirectories from these base paths.\nIf TF_CUDA_PATHS is not specified, a OS specific default is used:\n\n  Linux:   /usr/local/cuda, /usr, and paths from \'ldconfig -p\'.\n  Windows: CUDA_PATH environment variable, or\n           C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\*\n\nFor backwards compatibility, some libraries also use alternative base\ndirectories from other environment variables if they are specified. List of\nlibrary-specific environment variables:\n\n  Library   Version env variable  Additional base directories\n  ----------------------------------------------------------------\n  CUDA      TF_CUDA_VERSION       CUDA_TOOLKIT_PATH\n  cuBLAS    TF_CUBLAS_VERSION     CUDA_TOOLKIT_PATH\n  cuDNN     TF_CUDNN_VERSION      CUDNN_INSTALL_PATH\n  NCCL      TF_NCCL_VERSION       NCCL_INSTALL_PATH, NCCL_HDR_PATH\n  TensorRT  TF_TENSORRT_VERSION   TENSORRT_INSTALL_PATH\n\nVersions environment variables can be of the form \'x\' or \'x.y\' to request a\nspecific version, empty or unspecified to accept any version.\n\nThe output of a found library is of the form:\ntf_<library>_version: x.y.z\ntf_<library>_header_dir: ...\ntf_<library>_library_dir: ...\n""""""\n\nimport os\nimport glob\nimport platform\nimport re\nimport subprocess\nimport sys\n\ntry:\n    from shutil import which\nexcept ImportError:\n    from distutils.spawn import find_executable as which\n\n\nclass ConfigError(Exception):\n    pass\n\n\ndef _is_linux():\n    return platform.system() == ""Linux""\n\n\ndef _is_windows():\n    return platform.system() == ""Windows""\n\n\ndef _is_macos():\n    return platform.system() == ""Darwin""\n\n\ndef _matches_version(actual_version, required_version):\n    """"""Checks whether some version meets the requirements.\n\n    All elements of the required_version need to be present in the\n    actual_version.\n\n        required_version  actual_version  result\n        -----------------------------------------\n        1                 1.1             True\n        1.2               1               False\n        1.2               1.3             False\n                          1               True\n\n    Args:\n      required_version: The version specified by the user.\n      actual_version: The version detected from the CUDA installation.\n    Returns: Whether the actual version matches the required one.\n    """"""\n    if actual_version is None:\n        return False\n\n    # Strip spaces from the versions.\n    actual_version = actual_version.strip()\n    required_version = required_version.strip()\n    return actual_version.startswith(required_version)\n\n\ndef _at_least_version(actual_version, required_version):\n    actual = [int(v) for v in actual_version.split(""."")]\n    required = [int(v) for v in required_version.split(""."")]\n    return actual >= required\n\n\ndef _get_header_version(path, name):\n    """"""Returns preprocessor defines in C header file.""""""\n    for line in open(path, ""r"", encoding=""utf-8"").readlines():\n        match = re.match(r""#define %s +(\\d+)"" % name, line)\n        if match:\n            return match.group(1)\n    return """"\n\n\ndef _cartesian_product(first, second):\n    """"""Returns all path combinations of first and second.""""""\n    return [os.path.join(f, s) for f in first for s in second]\n\n\ndef _get_ld_config_paths():\n    """"""Returns all directories from \'ldconfig -p\'.""""""\n    if not _is_linux():\n        return []\n    ldconfig_path = which(""ldconfig"") or ""/sbin/ldconfig""\n    output = subprocess.check_output([ldconfig_path, ""-p""])\n    pattern = re.compile("".* => (.*)"")\n    result = set()\n    for line in output.splitlines():\n        try:\n            match = pattern.match(line.decode(""ascii""))\n        except UnicodeDecodeError:\n            match = False\n        if match:\n            result.add(os.path.dirname(match.group(1)))\n    return sorted(list(result))\n\n\ndef _get_default_cuda_paths(cuda_version):\n    if not cuda_version:\n        cuda_version = ""*""\n    elif ""."" not in cuda_version:\n        cuda_version = cuda_version + "".*""\n\n    if _is_windows():\n        return [\n            os.environ.get(\n                ""CUDA_PATH"",\n                ""C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v%s\\\\""\n                % cuda_version,\n            )\n        ]\n    return [\n        ""/usr/local/cuda-%s"" % cuda_version,\n        ""/usr/local/cuda"",\n        ""/usr"",\n        ""/usr/local/cudnn"",\n    ] + _get_ld_config_paths()\n\n\ndef _header_paths():\n    """"""Returns hard-coded set of relative paths to look for header files.""""""\n    return [\n        """",\n        ""include"",\n        ""include/cuda"",\n        ""include/*-linux-gnu"",\n        ""extras/CUPTI/include"",\n        ""include/cuda/CUPTI"",\n    ]\n\n\ndef _library_paths():\n    """"""Returns hard-coded set of relative paths to look for library files.""""""\n    return [\n        """",\n        ""lib64"",\n        ""lib"",\n        ""lib/*-linux-gnu"",\n        ""lib/x64"",\n        ""extras/CUPTI/*"",\n    ]\n\n\ndef _not_found_error(base_paths, relative_paths, filepattern):\n    base_paths = """".join([""\\n        \'%s\'"" % path for path in sorted(base_paths)])\n    relative_paths = """".join([""\\n        \'%s\'"" % path for path in relative_paths])\n    return ConfigError(\n        ""Could not find any %s in any subdirectory:%s\\nof:%s\\n""\n        % (filepattern, relative_paths, base_paths)\n    )\n\n\ndef _find_file(base_paths, relative_paths, filepattern):\n    for path in _cartesian_product(base_paths, relative_paths):\n        for file_path in glob.glob(os.path.join(path, filepattern)):\n            return file_path\n    raise _not_found_error(base_paths, relative_paths, filepattern)\n\n\ndef _find_library(base_paths, library_name, required_version):\n    """"""Returns first valid path to the requested library.""""""\n    if _is_windows():\n        filepattern = library_name + "".lib""\n    elif _is_macos():\n        filepattern = ""%s*.dylib"" % (\n            ""."".join([""lib"" + library_name] + required_version.split(""."")[:1])\n        )\n    else:\n        filepattern = (\n            ""."".join([""lib"" + library_name, ""so""] + required_version.split(""."")[:1])\n            + ""*""\n        )\n    return _find_file(base_paths, _library_paths(), filepattern)\n\n\ndef _find_versioned_file(\n    base_paths, relative_paths, filepattern, required_version, get_version\n):\n    """"""Returns first valid path to a file that matches the requested\n    version.""""""\n    for path in _cartesian_product(base_paths, relative_paths):\n        for file_path in glob.glob(os.path.join(path, filepattern)):\n            actual_version = get_version(file_path)\n            if _matches_version(actual_version, required_version):\n                return file_path, actual_version\n    raise _not_found_error(\n        base_paths,\n        relative_paths,\n        filepattern + "" matching version \'%s\'"" % required_version,\n    )\n\n\ndef _find_header(base_paths, header_name, required_version, get_version):\n    """"""Returns first valid path to a header that matches the requested\n    version.""""""\n    return _find_versioned_file(\n        base_paths, _header_paths(), header_name, required_version, get_version\n    )\n\n\ndef _find_cuda_config(base_paths, required_version):\n    def get_header_version(path):\n        version = int(_get_header_version(path, ""CUDA_VERSION""))\n        if not version:\n            return None\n        return ""%d.%d"" % (version // 1000, version % 1000 // 10)\n\n    cuda_header_path, header_version = _find_header(\n        base_paths, ""cuda.h"", required_version, get_header_version\n    )\n    cuda_version = header_version  # x.y, see above.\n\n    cuda_library_path = _find_library(base_paths, ""cudart"", cuda_version)\n\n    def get_nvcc_version(path):\n        pattern = r""Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)""\n        for line in subprocess.check_output([path, ""--version""]).splitlines():\n            match = re.match(pattern, line.decode(""ascii""))\n            if match:\n                return match.group(1)\n        return None\n\n    nvcc_name = ""nvcc.exe"" if _is_windows() else ""nvcc""\n    nvcc_path, nvcc_version = _find_versioned_file(\n        base_paths, ["""", ""bin"",], nvcc_name, cuda_version, get_nvcc_version\n    )\n\n    nvvm_path = _find_file(\n        base_paths,\n        [""nvvm/libdevice"", ""share/cuda"", ""lib/nvidia-cuda-toolkit/libdevice"",],\n        ""libdevice*.10.bc"",\n    )\n\n    cupti_header_path = _find_file(base_paths, _header_paths(), ""cupti.h"")\n    cupti_library_path = _find_library(base_paths, ""cupti"", required_version)\n\n    cuda_binary_dir = os.path.dirname(nvcc_path)\n    nvvm_library_dir = os.path.dirname(nvvm_path)\n\n    # XLA requires the toolkit path to find ptxas and libdevice.\n    # TODO(csigg): pass in both directories instead.\n    cuda_toolkit_paths = (\n        os.path.normpath(os.path.join(cuda_binary_dir, "".."")),\n        os.path.normpath(os.path.join(nvvm_library_dir, ""../.."")),\n    )\n    if cuda_toolkit_paths[0] != cuda_toolkit_paths[1]:\n        raise ConfigError(\n            ""Inconsistent CUDA toolkit path: %s vs %s"" % cuda_toolkit_paths\n        )\n\n    return {\n        ""cuda_version"": cuda_version,\n        ""cuda_include_dir"": os.path.dirname(cuda_header_path),\n        ""cuda_library_dir"": os.path.dirname(cuda_library_path),\n        ""cuda_binary_dir"": cuda_binary_dir,\n        ""nvvm_library_dir"": nvvm_library_dir,\n        ""cupti_include_dir"": os.path.dirname(cupti_header_path),\n        ""cupti_library_dir"": os.path.dirname(cupti_library_path),\n        ""cuda_toolkit_path"": cuda_toolkit_paths[0],\n    }\n\n\ndef _find_cublas_config(base_paths, required_version, cuda_version):\n\n    if _at_least_version(cuda_version, ""10.1""):\n\n        def get_header_version(path):\n            version = (\n                _get_header_version(path, name)\n                for name in (""CUBLAS_VER_MAJOR"", ""CUBLAS_VER_MINOR"", ""CUBLAS_VER_PATCH"")\n            )\n            return ""."".join(version)\n\n        header_path, cublas_version = _find_header(\n            base_paths, ""cublas_api.h"", required_version, get_header_version\n        )\n\n        cublas_major_version = cublas_version.split(""."")[0]\n        if not _matches_version(cuda_version, cublas_major_version):\n            raise ConfigError(\n                ""cuBLAS version %s does not match CUDA version %s""\n                % (cublas_major_version, cuda_version)\n            )\n\n    else:\n        # There is no version info available before CUDA 10.1, just find the file.\n        header_path = _find_file(base_paths, _header_paths(), ""cublas_api.h"")\n        # cuBLAS version is the same as CUDA version (x.y).\n        cublas_major_version = required_version\n\n    library_path = _find_library(base_paths, ""cublas"", cublas_major_version)\n\n    return {\n        ""cublas_include_dir"": os.path.dirname(header_path),\n        ""cublas_library_dir"": os.path.dirname(library_path),\n    }\n\n\ndef _find_cudnn_config(base_paths, required_version):\n    def get_header_version(path):\n        version = (\n            _get_header_version(path, name)\n            for name in (""CUDNN_MAJOR"", ""CUDNN_MINOR"", ""CUDNN_PATCHLEVEL"")\n        )\n        return ""."".join(version)\n\n    header_path, header_version = _find_header(\n        base_paths, ""cudnn.h"", required_version, get_header_version\n    )\n    cudnn_version = header_version.split(""."")[0]\n\n    library_path = _find_library(base_paths, ""cudnn"", cudnn_version)\n\n    return {\n        ""cudnn_version"": cudnn_version,\n        ""cudnn_include_dir"": os.path.dirname(header_path),\n        ""cudnn_library_dir"": os.path.dirname(library_path),\n    }\n\n\ndef _find_nccl_config(base_paths, required_version):\n    def get_header_version(path):\n        version = (\n            _get_header_version(path, name)\n            for name in (""NCCL_MAJOR"", ""NCCL_MINOR"", ""NCCL_PATCH"")\n        )\n        return ""."".join(version)\n\n    header_path, header_version = _find_header(\n        base_paths, ""nccl.h"", required_version, get_header_version\n    )\n    nccl_version = header_version.split(""."")[0]\n\n    library_path = _find_library(base_paths, ""nccl"", nccl_version)\n\n    return {\n        ""nccl_version"": nccl_version,\n        ""nccl_include_dir"": os.path.dirname(header_path),\n        ""nccl_library_dir"": os.path.dirname(library_path),\n    }\n\n\ndef _find_tensorrt_config(base_paths, required_version):\n    def get_header_version(path):\n        version = (\n            _get_header_version(path, name)\n            for name in (""NV_TENSORRT_MAJOR"", ""NV_TENSORRT_MINOR"", ""NV_TENSORRT_PATCH"")\n        )\n        # `version` is a generator object, so we convert it to a list before using\n        # it (muitiple times below).\n        version = list(version)\n        if not all(version):\n            return None  # Versions not found, make _matches_version returns False.\n        return ""."".join(version)\n\n    try:\n        header_path, header_version = _find_header(\n            base_paths, ""NvInfer.h"", required_version, get_header_version\n        )\n    except ConfigError:\n        # TensorRT 6 moved the version information to NvInferVersion.h.\n        header_path, header_version = _find_header(\n            base_paths, ""NvInferVersion.h"", required_version, get_header_version\n        )\n\n    tensorrt_version = header_version.split(""."")[0]\n    library_path = _find_library(base_paths, ""nvinfer"", tensorrt_version)\n\n    return {\n        ""tensorrt_version"": tensorrt_version,\n        ""tensorrt_include_dir"": os.path.dirname(header_path),\n        ""tensorrt_library_dir"": os.path.dirname(library_path),\n    }\n\n\ndef _list_from_env(env_name, default=[]):\n    """"""Returns comma-separated list from environment variable.""""""\n    if env_name in os.environ:\n        return os.environ[env_name].split("","")\n    return default\n\n\ndef _get_legacy_path(env_name, default=[]):\n    """"""Returns a path specified by a legacy environment variable.\n\n    CUDNN_INSTALL_PATH, NCCL_INSTALL_PATH, TENSORRT_INSTALL_PATH set to\n    \'/usr/lib/x86_64-linux-gnu\' would previously find both library and\n    header paths. Detect those and return \'/usr\', otherwise forward to\n    _list_from_env().\n    """"""\n    if env_name in os.environ:\n        match = re.match(r""^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$"", os.environ[env_name])\n        if match:\n            return [match.group(1)]\n    return _list_from_env(env_name, default)\n\n\ndef _normalize_path(path):\n    """"""Returns normalized path, with forward slashes on Windows.""""""\n    path = os.path.normpath(path)\n    if _is_windows():\n        path = path.replace(""\\\\"", ""/"")\n    return path\n\n\ndef find_cuda_config():\n    """"""Returns a dictionary of CUDA library and header file paths.""""""\n    libraries = [argv.lower() for argv in sys.argv[1:]]\n    cuda_version = os.environ.get(""TF_CUDA_VERSION"", """")\n    base_paths = _list_from_env(""TF_CUDA_PATHS"", _get_default_cuda_paths(cuda_version))\n\n    base_paths = [path for path in base_paths if os.path.exists(path)]\n    result = {}\n    if ""cuda"" in libraries:\n        cuda_paths = _list_from_env(""CUDA_TOOLKIT_PATH"", base_paths)\n        result.update(_find_cuda_config(cuda_paths, cuda_version))\n\n        cuda_version = result[""cuda_version""]\n        cublas_paths = base_paths\n        if tuple(int(v) for v in cuda_version.split(""."")) < (10, 1):\n            # Before CUDA 10.1, cuBLAS was in the same directory as the toolkit.\n            cublas_paths = cuda_paths\n        cublas_version = os.environ.get(""TF_CUBLAS_VERSION"", """")\n        result.update(_find_cublas_config(cublas_paths, cublas_version, cuda_version))\n\n    if ""cudnn"" in libraries:\n        cudnn_paths = _get_legacy_path(""CUDNN_INSTALL_PATH"", base_paths)\n        cudnn_version = os.environ.get(""TF_CUDNN_VERSION"", """")\n        result.update(_find_cudnn_config(cudnn_paths, cudnn_version))\n\n    if ""nccl"" in libraries:\n        nccl_paths = _get_legacy_path(""NCCL_INSTALL_PATH"", base_paths)\n        nccl_version = os.environ.get(""TF_NCCL_VERSION"", """")\n        result.update(_find_nccl_config(nccl_paths, nccl_version))\n\n    if ""tensorrt"" in libraries:\n        tensorrt_paths = _get_legacy_path(""TENSORRT_INSTALL_PATH"", base_paths)\n        tensorrt_version = os.environ.get(""TF_TENSORRT_VERSION"", """")\n        result.update(_find_tensorrt_config(tensorrt_paths, tensorrt_version))\n\n    for k, v in result.items():\n        if k.endswith(""_dir"") or k.endswith(""_path""):\n            result[k] = _normalize_path(v)\n\n    return result\n\n\ndef main():\n    try:\n        for key, value in sorted(find_cuda_config().items()):\n            print(""{}: {}"".format(key, value))\n    except ConfigError as e:\n        sys.stderr.write(str(e))\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tensorflow_addons/activations/tests/__init__.py,0,b''
tensorflow_addons/activations/tests/activations_test.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\nimport tensorflow as tf\nfrom tensorflow_addons import activations\n\n\nALL_ACTIVATIONS = [\n    ""gelu"",\n    ""hardshrink"",\n    ""lisht"",\n    ""mish"",\n    ""rrelu"",\n    ""softshrink"",\n    ""sparsemax"",\n    ""tanhshrink"",\n]\n\n\n@pytest.mark.parametrize(""name"", ALL_ACTIVATIONS)\ndef test_serialization(name):\n    fn = tf.keras.activations.get(""Addons>"" + name)\n    ref_fn = getattr(activations, name)\n    assert fn == ref_fn\n    config = tf.keras.activations.serialize(fn)\n    fn = tf.keras.activations.deserialize(config)\n    assert fn == ref_fn\n\n\n@pytest.mark.parametrize(""name"", ALL_ACTIVATIONS)\ndef test_serialization_with_layers(name):\n    layer = tf.keras.layers.Dense(3, activation=getattr(activations, name))\n    config = tf.keras.layers.serialize(layer)\n    deserialized_layer = tf.keras.layers.deserialize(config)\n    assert deserialized_layer.__class__.__name__ == layer.__class__.__name__\n    assert deserialized_layer.activation.__name__ == name\n'"
tensorflow_addons/activations/tests/gelu_test.py,7,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import gelu\nfrom tensorflow_addons.activations.gelu import _gelu_py\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_gelu(dtype):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    expected_result = tf.constant(\n        [-0.04540229, -0.158808, 0.0, 0.841192, 1.9545977], dtype=dtype\n    )\n    test_utils.assert_allclose_according_to_type(gelu(x), expected_result)\n\n    expected_result = tf.constant(\n        [-0.04550028, -0.15865526, 0.0, 0.8413447, 1.9544997], dtype=dtype\n    )\n    test_utils.assert_allclose_according_to_type(gelu(x, False), expected_result)\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\n@pytest.mark.parametrize(""approximate"", [True, False])\ndef test_same_as_py_func(dtype, approximate):\n    np.random.seed(100)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype, approximate)\n\n\ndef verify_funcs_are_equivalent(dtype, approximate):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = gelu(x, approximate=approximate)\n        y_py = _gelu_py(x, approximate=approximate)\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n    # TODO: lower atol to 1e-6\n    # currently it doesn\'t work.\n    # It necessitates changing the Python or C++ implementation.\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py, atol=1e-5)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\n@pytest.mark.parametrize(""approximate"", [True, False])\ndef test_theoretical_gradients(dtype, approximate):\n    # Only test theoretical gradients for float32 and float64\n    # because of the instability of float16 while computing jacobian\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n\n    theoretical, numerical = tf.test.compute_gradient(\n        lambda x: gelu(x, approximate=approximate), [x]\n    )\n    test_utils.assert_allclose_according_to_type(theoretical, numerical, atol=1e-4)\n'"
tensorflow_addons/activations/tests/hardshrink_test.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations.hardshrink import _hardshrink_custom_op\nfrom tensorflow_addons.activations.hardshrink import _hardshrink_py\nfrom tensorflow_addons.utils import test_utils\n\n\ndef test_invalid():\n    with pytest.raises(\n        tf.errors.OpError, match=""lower must be less than or equal to upper.""\n    ):\n        y = _hardshrink_custom_op(tf.ones(shape=(1, 2, 3)), lower=2.0, upper=-2.0)\n        y.numpy()\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_hardshrink(dtype):\n    x = tf.constant([-2.0, -0.5, 0.0, 0.5, 2.0], dtype=dtype)\n    expected_result = tf.constant([-2.0, 0.0, 0.0, 0.0, 2.0], dtype=dtype)\n    test_utils.assert_allclose_according_to_type(\n        _hardshrink_custom_op(x), expected_result\n    )\n\n    expected_result = tf.constant([-2.0, 0.0, 0.0, 0.0, 2.0], dtype=dtype)\n    test_utils.assert_allclose_according_to_type(\n        _hardshrink_custom_op(x, lower=-1.0, upper=1.0), expected_result\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_same_as_py_func(dtype):\n    np.random.seed(1234)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype)\n\n\ndef verify_funcs_are_equivalent(dtype):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n    lower = np.random.uniform(-10, 10)\n    upper = lower + np.random.uniform(0, 10)\n\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = _hardshrink_custom_op(x, lower, upper)\n        y_py = _hardshrink_py(x, lower, upper)\n\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_theoretical_gradients(dtype):\n    # Only test theoretical gradients for float32 and float64\n    # because of the instability of float16 while computing jacobian\n\n    # Hardshrink is not continuous at `lower` and `upper`.\n    # Avoid these two points to make gradients smooth.\n    x = tf.constant([-2.0, -1.5, 0.0, 1.5, 2.0], dtype=dtype)\n\n    theoretical, numerical = tf.test.compute_gradient(_hardshrink_custom_op, [x])\n    test_utils.assert_allclose_according_to_type(theoretical, numerical, atol=1e-4)\n'"
tensorflow_addons/activations/tests/lisht_test.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import lisht\nfrom tensorflow_addons.activations.lisht import _lisht_py\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_theoretical_gradients(dtype):\n    # Only test theoretical gradients for float32 and float64\n    # because of the instability of float16 while computing jacobian\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n\n    theoretical, numerical = tf.test.compute_gradient(lisht, [x])\n    test_utils.assert_allclose_according_to_type(\n        theoretical, numerical, rtol=5e-4, atol=5e-4\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_lisht(dtype):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    expected_result = tf.constant(\n        [1.9280552, 0.7615942, 0.0, 0.7615942, 1.9280552], dtype=dtype\n    )\n    test_utils.assert_allclose_according_to_type(lisht(x), expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_same_as_py_func(dtype):\n    np.random.seed(1234)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype)\n\n\ndef verify_funcs_are_equivalent(dtype):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = lisht(x)\n        y_py = _lisht_py(x)\n\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py)\n'"
tensorflow_addons/activations/tests/mish_test.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import mish\nfrom tensorflow_addons.activations.mish import _mish_py\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_mish(dtype):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    expected_result = tf.constant(\n        [-0.2525015, -0.30340144, 0.0, 0.86509836, 1.943959], dtype=dtype\n    )\n    test_utils.assert_allclose_according_to_type(mish(x), expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_theoretical_gradients(dtype):\n    # Only test theoretical gradients for float32 and float64\n    # because of the instability of float16 while computing jacobian\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n\n    theoretical, numerical = tf.test.compute_gradient(mish, [x])\n    test_utils.assert_allclose_according_to_type(theoretical, numerical, atol=1e-4)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_same_as_py_func(dtype):\n    np.random.seed(1234)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype)\n\n\ndef verify_funcs_are_equivalent(dtype):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = mish(x)\n        y_py = _mish_py(x)\n\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py, atol=1e-5)\n'"
tensorflow_addons/activations/tests/rrelu_test.py,10,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import rrelu\nfrom tensorflow_addons.utils import test_utils\n\nSEED = 111111\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\n@pytest.mark.parametrize(""training"", [True, False])\ndef test_rrelu_old(dtype, training):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    lower = 0.1\n    upper = 0.2\n\n    tf.random.set_seed(SEED)\n    training_results = {\n        np.float16: [-0.288330078, -0.124206543, 0, 1, 2],\n        np.float32: [-0.26851666, -0.116421416, 0, 1, 2],\n        np.float64: [-0.3481333923206531, -0.17150176242558851, 0, 1, 2],\n    }\n    result = rrelu(x, lower, upper, training=training, seed=SEED)\n    if training:\n        expect_result = training_results.get(dtype)\n    else:\n        expect_result = [\n            -0.30000001192092896,\n            -0.15000000596046448,\n            0,\n            1,\n            2,\n        ]\n    test_utils.assert_allclose_according_to_type(result, expect_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\n@pytest.mark.parametrize(""training"", [True, False])\ndef test_rrelu(dtype, training):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    lower = 0.1\n    upper = 0.2\n    training_results = {\n        np.float16: [-0.3826, -0.165, 0, 1, 2],\n        np.float32: [-0.282151192, -0.199812651, 0, 1, 2],\n        np.float64: [-0.25720977, -0.1221586, 0, 1, 2],\n    }\n    result = rrelu(\n        x,\n        lower,\n        upper,\n        training=training,\n        seed=None,\n        rng=tf.random.Generator.from_seed(SEED),\n    )\n    if training:\n        expect_result = training_results.get(dtype)\n    else:\n        expect_result = [-0.30000001192092896, -0.15000000596046448, 0, 1, 2]\n    test_utils.assert_allclose_according_to_type(result, expect_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\n@pytest.mark.parametrize(""training"", [True, False])\ndef test_theoretical_gradients_old(dtype, training):\n    def rrelu_wrapper(lower, upper, training):\n        def inner(x):\n            tf.random.set_seed(SEED)\n            return rrelu(x, lower, upper, training=training, seed=SEED)\n\n        return inner\n\n    x = tf.constant([-2.0, -1.0, -0.1, 0.1, 1.0, 2.0], dtype=dtype)\n    lower = 0.1\n    upper = 0.2\n\n    theoretical, numerical = tf.test.compute_gradient(\n        rrelu_wrapper(lower, upper, training), [x]\n    )\n    test_utils.assert_allclose_according_to_type(\n        theoretical, numerical, rtol=5e-4, atol=5e-4\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\n@pytest.mark.parametrize(""training"", [True, False])\ndef test_theoretical_gradients(dtype, training):\n    x = tf.constant([-2.0, -1.0, -0.1, 0.1, 1.0, 2.0], dtype=dtype)\n    lower = 0.1\n    upper = 0.2\n\n    theoretical, numerical = tf.test.compute_gradient(\n        lambda x: rrelu(\n            x,\n            lower,\n            upper,\n            training=training,\n            seed=None,\n            rng=tf.random.Generator.from_seed(SEED),\n        ),\n        [x],\n    )\n    test_utils.assert_allclose_according_to_type(\n        theoretical, numerical, rtol=5e-4, atol=5e-4\n    )\n'"
tensorflow_addons/activations/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/activations/tests/softshrink_test.py,9,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import softshrink\nfrom tensorflow_addons.activations.softshrink import (\n    _softshrink_py,\n    _softshrink_custom_op,\n)\nfrom tensorflow_addons.utils import test_utils\n\n\ndef test_invalid():\n    with pytest.raises(\n        tf.errors.OpError, match=""lower must be less than or equal to upper.""\n    ):\n        y = _softshrink_custom_op(tf.ones(shape=(1, 2, 3)), lower=2.0, upper=-2.0)\n        y.numpy()\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_softshrink(dtype):\n    x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=dtype)\n    expected_result = tf.constant([-1.5, -0.5, 0.0, 0.5, 1.5], dtype=dtype)\n    test_utils.assert_allclose_according_to_type(softshrink(x), expected_result)\n\n    expected_result = tf.constant([-1.0, 0.0, 0.0, 0.0, 1.0], dtype=dtype)\n    test_utils.assert_allclose_according_to_type(\n        softshrink(x, lower=-1.0, upper=1.0), expected_result\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32])\ndef test_same_as_py_func(dtype):\n    np.random.seed(1234)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype)\n\n\ndef verify_funcs_are_equivalent(dtype):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n    lower = np.random.uniform(-10, 10)\n    upper = lower + np.random.uniform(0, 10)\n\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = softshrink(x, lower, upper)\n        y_py = _softshrink_py(x, lower, upper)\n\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_theoretical_gradients(dtype):\n    # Only test theoretical gradients for float32 and float64\n    # because of the instability of float16 while computing jacobian\n\n    # Softshrink is not continuous at `lower` and `upper`.\n    # Avoid these two points to make gradients smooth.\n    x = tf.constant([-2.0, -1.5, 0.0, 1.5, 2.0], dtype=dtype)\n\n    theoretical, numerical = tf.test.compute_gradient(softshrink, [x])\n    test_utils.assert_allclose_according_to_type(theoretical, numerical, atol=1e-4)\n'"
tensorflow_addons/activations/tests/sparsemax_test.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.activations import sparsemax\nfrom tensorflow_addons.utils import test_utils\n\ntest_obs = 17\n\n\ndef _np_sparsemax(z):\n    z = z - np.mean(z, axis=1)[:, np.newaxis]\n\n    # sort z\n    z_sorted = np.sort(z, axis=1)[:, ::-1]\n\n    # calculate k(z)\n    z_cumsum = np.cumsum(z_sorted, axis=1)\n    k = np.arange(1, z.shape[1] + 1)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # use argmax to get the index by row as .nonzero() doesn\'t\n    # take an axis argument. np.argmax return the first index, but the last\n    # index is required here, use np.flip to get the last index and\n    # `z.shape[axis]` to compensate for np.flip afterwards.\n    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)\n\n    # calculate tau(z)\n    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]\n    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)\n\n    # calculate p\n    return np.maximum(0, z - tau_z)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_against_numpy_axis(dtype):\n    """"""check sparsemax kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype), axis=0).numpy()\n    np_sparsemax = np.transpose(_np_sparsemax(np.transpose(z))).astype(dtype)\n\n    test_utils.assert_allclose_according_to_type(\n        np_sparsemax, tf_sparsemax_out, half_atol=5e-3\n    )\n    assert np_sparsemax.shape == tf_sparsemax_out.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_against_numpy_low_rank(dtype):\n    """"""check sparsemax kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(10))\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype)).numpy()\n    np_sparsemax = np.reshape(_np_sparsemax(np.reshape(z, [1, 10])), [10]).astype(dtype)\n\n    test_utils.assert_allclose_according_to_type(\n        np_sparsemax, tf_sparsemax_out, half_atol=5e-3\n    )\n    assert np_sparsemax.shape == tf_sparsemax_out.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_against_numpy(dtype):\n    """"""check sparsemax kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype))\n    np_sparsemax = _np_sparsemax(z).astype(dtype)\n\n    test_utils.assert_allclose_according_to_type(np_sparsemax, tf_sparsemax_out)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_against_numpy_high_rank(dtype):\n    """"""check sparsemax kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, test_obs, 10))\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype))\n    np_sparsemax = np.reshape(\n        _np_sparsemax(np.reshape(z, [test_obs * test_obs, 10])),\n        [test_obs, test_obs, 10],\n    ).astype(dtype)\n\n    test_utils.assert_allclose_according_to_type(np_sparsemax, tf_sparsemax_out)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_of_nan(dtype):\n    """"""check sparsemax transfers nan.""""""\n    z_nan = np.asarray(\n        [[0, np.nan, 0], [0, np.nan, np.nan], [np.nan, np.nan, np.nan],]\n    ).astype(dtype)\n\n    tf_sparsemax_nan = sparsemax(z_nan)\n    np.testing.assert_equal(\n        np.array(\n            [\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n            ]\n        ),\n        tf_sparsemax_nan,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_of_inf(dtype):\n    """"""check sparsemax is infinity safe.""""""\n    z_neg = np.asarray(\n        [[0, -np.inf, 0], [0, -np.inf, -np.inf], [-np.inf, -np.inf, -np.inf],]\n    ).astype(dtype)\n    z_pos = np.asarray(\n        [[0, np.inf, 0], [0, np.inf, np.inf], [np.inf, np.inf, np.inf]]\n    ).astype(dtype)\n    z_mix = np.asarray(\n        [[0, np.inf, 0], [0, np.inf, -np.inf], [-np.inf, np.inf, -np.inf]]\n    ).astype(dtype)\n\n    tf_sparsemax_neg = sparsemax(z_neg)\n    np.testing.assert_equal(\n        np.array([[0.5, 0, 0.5], [1, 0, 0], [np.nan, np.nan, np.nan]]), tf_sparsemax_neg\n    )\n\n    tf_sparsemax_pos = sparsemax(z_pos)\n    np.testing.assert_equal(\n        np.array(\n            [\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n            ]\n        ),\n        tf_sparsemax_pos,\n    )\n\n    tf_sparsemax_mix = sparsemax(z_mix)\n    np.testing.assert_equal(\n        np.array(\n            [\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n                [np.nan, np.nan, np.nan],\n            ]\n        ),\n        tf_sparsemax_mix,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_of_zero(dtype):\n    """"""check sparsemax proposition 1, part 1.""""""\n    z = np.zeros((1, 10))\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype))\n    np_sparsemax = np.ones_like(z, dtype=dtype) / z.size\n\n    test_utils.assert_allclose_according_to_type(np_sparsemax, tf_sparsemax_out)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_of_to_inf(dtype):\n    """"""check sparsemax proposition 1, part 2.""""""\n    random = np.random.RandomState(4)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n\n    # assume |A(z)| = 1, as z is continues random\n    z_sort_arg = np.argsort(z, axis=1)[:, ::-1]\n    z_sort = np.sort(z, axis=-1)[:, ::-1]\n    gamma_z = z_sort[:, 0] - z_sort[:, 1]\n    epsilon = (0.99 * gamma_z * 1).reshape(-1, 1)\n\n    # construct the expected 1_A(z) array\n    p_expected = np.zeros((test_obs, 10), dtype=dtype)\n    p_expected[np.arange(0, test_obs), z_sort_arg[:, 0]] = 1\n\n    tf_sparsemax_out = sparsemax(((1 / epsilon) * z).astype(dtype))\n\n    test_utils.assert_allclose_according_to_type(p_expected, tf_sparsemax_out)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_constant_add(dtype):\n    """"""check sparsemax proposition 2.""""""\n    random = np.random.RandomState(5)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10)).astype(dtype)\n    c = random.uniform(low=-3, high=3, size=(test_obs, 1)).astype(dtype)\n\n    tf_sparsemax_zpc = sparsemax((z + c))\n\n    tf_sparsemax_z = sparsemax(z)\n\n    test_utils.assert_allclose_according_to_type(\n        tf_sparsemax_zpc, tf_sparsemax_z, half_atol=5e-3\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_two_dimentional(dtype):\n    """"""check two dimentation sparsemax case.""""""\n    t = np.linspace(-2, 2, test_obs, dtype=dtype)\n    z = np.vstack([t, np.zeros(test_obs, dtype=dtype)]).T\n\n    tf_sparsemax_out = sparsemax(z.astype(dtype)).numpy()\n\n    p0_expected = np.select([t < -1, t <= 1, t > 1], [0, (t + 1) / 2, 1])\n\n    test_utils.assert_allclose_according_to_type(p0_expected, tf_sparsemax_out[:, 0])\n    test_utils.assert_allclose_according_to_type(\n        1 - p0_expected, tf_sparsemax_out[:, 1]\n    )\n    assert z.shape == tf_sparsemax_out.shape\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_diffrence(dtype):\n    """"""check sparsemax proposition 4.""""""\n    random = np.random.RandomState(7)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    p = sparsemax(z.astype(dtype)).numpy()\n\n    etol = {np.float32: 1e-6, np.float64: 1e-9}[dtype]\n\n    for val in range(0, test_obs):\n        for i in range(0, 10):\n            for j in range(0, 10):\n                # check condition, the obesite pair will be checked anyway\n                if z[val, i] > z[val, j]:\n                    continue\n\n                assert 0 <= p[val, j] - p[val, i] <= z[val, j] - z[val, i] + etol\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_permutation(dtype):\n    """"""check sparsemax proposition 3.""""""\n    random = np.random.RandomState(6)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    p = sparsemax(z.astype(dtype)).numpy()\n\n    for i in range(test_obs):\n        per = random.permutation(10)\n\n        tf_sparsemax_out = sparsemax(z[i, per].reshape(1, -1).astype(dtype))\n        p_expected = p[i, per].reshape(1, -1)\n\n        test_utils.assert_allclose_according_to_type(\n            p_expected, tf_sparsemax_out, half_atol=5e-3\n        )\n        assert p_expected.shape == tf_sparsemax_out.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_gradient_against_estimate(dtype):\n    """"""check sparsemax Rop, against estimated Rop.""""""\n    random = np.random.RandomState(9)\n\n    # sparsemax is not a smooth function so gradient estimation is only\n    # possible for float64.\n    if dtype != ""float64"":\n        return\n\n    z = random.uniform(low=-1, high=1, size=(test_obs, 10)).astype(dtype)\n\n    (jacob_sym,), (jacob_num,) = tf.test.compute_gradient(\n        lambda logits: sparsemax(logits), [z], delta=1e-6\n    )\n    np.testing.assert_allclose(jacob_sym, jacob_num)\n'"
tensorflow_addons/activations/tests/tanhshrink_test.py,2,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.activations import tanhshrink\nfrom tensorflow_addons.activations.tanhshrink import _tanhshrink_py\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_same_as_py_func(dtype):\n    np.random.seed(1234)\n    for _ in range(20):\n        verify_funcs_are_equivalent(dtype)\n\n\ndef verify_funcs_are_equivalent(dtype):\n    x_np = np.random.uniform(-10, 10, size=(4, 4)).astype(dtype)\n    x = tf.convert_to_tensor(x_np)\n    with tf.GradientTape(persistent=True) as t:\n        t.watch(x)\n        y_native = tanhshrink(x)\n        y_py = _tanhshrink_py(x)\n    test_utils.assert_allclose_according_to_type(y_native, y_py)\n    grad_native = t.gradient(y_native, x)\n    grad_py = t.gradient(y_py, x)\n    test_utils.assert_allclose_according_to_type(grad_native, grad_py)\n'"
tensorflow_addons/callbacks/tests/__init__.py,0,b''
tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py,3,"b'import os\nimport pytest\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_addons.callbacks import AverageModelCheckpoint\nfrom tensorflow_addons.optimizers import MovingAverage\n\nTRAIN_SAMPLES = 10\nNUM_CLASSES = 2\nINPUT_DIM = 3\nNUM_HIDDEN = 5\nBATCH_SIZE = 5\nEPOCHS = 5\n\n\ndef get_data_and_model(optimizer=""moving_avg""):\n    x = tf.random.normal([TRAIN_SAMPLES, INPUT_DIM])\n    y = tf.random.normal([TRAIN_SAMPLES, NUM_CLASSES])\n    moving_avg = MovingAverage(\n        tf.keras.optimizers.SGD(lr=2.0), sequential_update=True, average_decay=0.5\n    )\n    if optimizer == ""moving_avg"":\n        optimizer = moving_avg\n    inputs = keras.layers.Input(INPUT_DIM)\n    hidden_layer = keras.layers.Dense(\n        NUM_HIDDEN, input_dim=INPUT_DIM, activation=""relu""\n    )(inputs)\n    outputs = keras.layers.Dense(NUM_CLASSES, activation=""softmax"")(hidden_layer)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""acc""])\n    return x, y, model\n\n\ndef test_compatibility_with_some_opts_only(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model(optimizer=""rmsprop"")\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True, filepath=test_model_filepath\n    )\n    with pytest.raises(\n        TypeError,\n        match=""AverageModelCheckpoint is only used when trainingwith""\n        "" MovingAverage or StochasticAverage"",\n    ):\n        model.fit(\n            x, y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[avg_model_ckpt]\n        )\n\n\ndef test_model_file_creation(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True, filepath=test_model_filepath\n    )\n    model.fit(x, y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[avg_model_ckpt])\n    assert os.path.exists(test_model_filepath)\n\n\ndef test_mode_auto(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    monitor = ""val_loss""\n    save_best_only = False\n    mode = ""auto""\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True,\n        filepath=test_model_filepath,\n        monitor=monitor,\n        save_best_only=save_best_only,\n        mode=mode,\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert os.path.exists(test_model_filepath)\n\n\ndef test_mode_min(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    monitor = ""val_loss""\n    save_best_only = False\n    mode = ""min""\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True,\n        filepath=test_model_filepath,\n        monitor=monitor,\n        save_best_only=save_best_only,\n        mode=mode,\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert os.path.exists(test_model_filepath)\n\n\ndef test_mode_max(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    mode = ""max""\n    monitor = ""val_acc""\n    save_best_only = False\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True,\n        filepath=test_model_filepath,\n        monitor=monitor,\n        save_best_only=save_best_only,\n        mode=mode,\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert os.path.exists(test_model_filepath)\n\n\ndef test_save_best_only(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    save_best_only = True\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=True, filepath=test_model_filepath, save_best_only=save_best_only\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert os.path.exists(test_model_filepath)\n\n\ndef test_metric_unavailable(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    x, y, model = get_data_and_model()\n    monitor = ""unknown""\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=False,\n        filepath=test_model_filepath,\n        monitor=monitor,\n        save_best_only=True,\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert not os.path.exists(test_model_filepath)\n\n\ndef test_save_freq(tmp_path):\n    test_filepath = str(tmp_path / ""test_model.{epoch:02d}.h5"")\n    x, y, model = get_data_and_model()\n    save_freq = ""epoch""\n    avg_model_ckpt = AverageModelCheckpoint(\n        update_weights=False, filepath=test_filepath, save_freq=save_freq\n    )\n    model.fit(\n        x,\n        y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(x, y),\n        callbacks=[avg_model_ckpt],\n    )\n    assert os.path.exists(test_filepath.format(epoch=1))\n    assert os.path.exists(test_filepath.format(epoch=2))\n    assert os.path.exists(test_filepath.format(epoch=3))\n    assert os.path.exists(test_filepath.format(epoch=4))\n    assert os.path.exists(test_filepath.format(epoch=5))\n\n\ndef test_invalid_save_freq(tmp_path):\n    test_model_filepath = str(tmp_path / ""test_model.h5"")\n    save_freq = ""invalid_save_freq""\n    with pytest.raises(ValueError, match=""Unrecognized save_freq""):\n        AverageModelCheckpoint(\n            update_weights=True, filepath=test_model_filepath, save_freq=save_freq\n        )\n'"
tensorflow_addons/callbacks/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/callbacks/tests/time_stopping_test.py,1,"b'import time\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nfrom tensorflow_addons.callbacks.time_stopping import TimeStopping\n\n\nclass SleepLayer(tf.keras.layers.Layer):\n    def __init__(self, secs):\n        self.secs = secs\n        super().__init__(dynamic=True)\n\n    def call(self, inputs):\n        time.sleep(self.secs)\n        return inputs\n\n\ndef get_data_and_model(secs):\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([[0], [1], [1], [0]])\n\n    model = Sequential()\n    model.add(SleepLayer(secs))\n    model.add(Dense(1))\n    model.compile(loss=""mean_squared_error"")\n\n    # In case there is some initialization going on.\n    model.fit(X, y, epochs=1, verbose=0)\n    return X, y, model\n\n\ndef test_stop_at_the_right_time():\n    X, y, model = get_data_and_model(0.1)\n\n    time_stopping = TimeStopping(2, verbose=0)\n    history = model.fit(X, y, epochs=30, verbose=0, callbacks=[time_stopping])\n\n    assert len(history.epoch) <= 20\n\n\ndef test_default_value():\n    X, y, model = get_data_and_model(0.1)\n\n    time_stopping = TimeStopping()\n    history = model.fit(X, y, epochs=15, verbose=0, callbacks=[time_stopping])\n\n    assert len(history.epoch) == 15\n\n\n@pytest.mark.parametrize(""verbose"", [0, 1])\ndef test_time_stopping_verbose(capsys, verbose):\n    X, y, model = get_data_and_model(0.25)\n\n    time_stopping = TimeStopping(1, verbose=verbose)\n\n    capsys.readouterr()  # flush the stdout/stderr buffer.\n    history = model.fit(X, y, epochs=10, verbose=0, callbacks=[time_stopping])\n    fit_stdout = capsys.readouterr().out\n    nb_epochs_run = len(history.epoch)\n    message = ""Timed stopping at epoch "" + str(nb_epochs_run)\n    if verbose:\n        assert message in fit_stdout\n    else:\n        assert message not in fit_stdout\n    assert len(history.epoch) <= 4\n'"
tensorflow_addons/callbacks/tests/tqdm_progress_bar_test.py,3,"b'import re\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nimport tensorflow_addons as tfa\n\n\ndef get_data_and_model():\n    x = np.random.random((12, 1))\n    y = np.random.randint(0, 2, (12, 1), dtype=np.int)\n\n    inputs = tf.keras.layers.Input(shape=(1,))\n    outputs = tf.keras.layers.Dense(1)(inputs)\n    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(optimizer=""sgd"", loss=""mse"", metrics=[""acc""])\n    return x, y, model\n\n\ndef test_tqdm_progress_bar(capsys):\n\n    x, y, model = get_data_and_model()\n\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, epochs=1, verbose=0, callbacks=[tfa.callbacks.TQDMProgressBar()])\n    fit_stderr = capsys.readouterr().err\n    assert ""loss:"" in fit_stderr\n    assert ""acc:"" in fit_stderr\n\n\ndef test_tqdm_progress_bar_overall_bar_format(capsys):\n\n    x, y, model = get_data_and_model()\n    overall_bar_format = (\n        ""{l_bar}{bar} {n_fmt}/{total_fmt} ETA: dodo""\n        ""{remaining}s,  {rate_fmt}{postfix}""\n    )\n    pb = tfa.callbacks.TQDMProgressBar(\n        overall_bar_format=overall_bar_format, show_epoch_progress=False\n    )\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, epochs=1, verbose=0, callbacks=[pb])\n    fit_stderr = capsys.readouterr().err\n    assert ""ETA: dodo"" in fit_stderr\n\n\ndef test_tqdm_progress_bar_epoch_bar_format(capsys):\n\n    x, y, model = get_data_and_model()\n    epoch_bar_format = ""{n_fmt}/{total_fmt}{bar} ETA: dodo {remaining}s - {desc}""\n    pb = tfa.callbacks.TQDMProgressBar(\n        epoch_bar_format=epoch_bar_format, show_overall_progress=False\n    )\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, epochs=1, verbose=0, callbacks=[pb])\n    fit_stderr = capsys.readouterr().err\n    assert ""ETA: dodo"" in fit_stderr\n\n\ndef test_tqdm_progress_bar_epoch_bar_format_missing_parameter(capsys):\n\n    x, y, model = get_data_and_model()\n    epoch_bar_format = ""{n_fmt} {bar} ETA: dodo {remaining}s - {desc}""\n    pb = tfa.callbacks.TQDMProgressBar(\n        epoch_bar_format=epoch_bar_format, show_overall_progress=False\n    )\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, batch_size=4, epochs=2, verbose=0, callbacks=[pb])\n    fit_stderr = capsys.readouterr().err\n    assert ""/3"" not in fit_stderr\n\n\ndef test_tqdm_progress_bar_metrics_format(capsys):\n\n    x, y, model = get_data_and_model()\n\n    pb = tfa.callbacks.TQDMProgressBar(\n        metrics_format=""{name}: dodo {value:0.6f}"", show_overall_progress=False\n    )\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, epochs=1, verbose=0, callbacks=[pb])\n    fit_stderr = capsys.readouterr().err\n    assert ""acc: dodo"" in fit_stderr\n    assert re.search(r""acc: dodo [0-9]\\.[0-9][0-9][0-9][0-9][0-9][0-9]"", fit_stderr)\n\n\n@pytest.mark.parametrize(""show_epoch_progress"", [True, False])\n@pytest.mark.parametrize(""show_overall_progress"", [True, False])\ndef test_tqdm_progress_bar_show(capsys, show_epoch_progress, show_overall_progress):\n\n    x, y, model = get_data_and_model()\n\n    pb = tfa.callbacks.TQDMProgressBar(\n        show_epoch_progress=show_epoch_progress,\n        show_overall_progress=show_overall_progress,\n    )\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, batch_size=4, epochs=2, verbose=0, callbacks=[pb])\n    fit_stderr = capsys.readouterr().err\n\n    assert (""/3"" in fit_stderr) is show_epoch_progress\n    assert (""epochs/s"" in fit_stderr) is show_overall_progress\n\n    if show_epoch_progress and not show_overall_progress:\n        assert ""size"" not in fit_stderr\n        assert ""batch"" not in fit_stderr\n\n\ndef test_tqdm_progress_bar_validation(capsys):\n    x, y, model = get_data_and_model()\n\n    pb = tfa.callbacks.TQDMProgressBar()\n    capsys.readouterr()  # flush the buffer\n    model.fit(x, y, epochs=1, verbose=0, callbacks=[pb], validation_data=(x, y))\n    fit_stderr = capsys.readouterr().err\n    assert re.search(r""val_loss: [0-9]\\.[0-9][0-9][0-9][0-9]"", fit_stderr)\n    assert re.search(r""val_acc: [0-9]\\.[0-9][0-9][0-9][0-9]"", fit_stderr)\n\n\ndef test_tqdm_progress_bar_evaluate(capsys):\n    x, y, model = get_data_and_model()\n\n    pb = tfa.callbacks.TQDMProgressBar()\n    capsys.readouterr()  # flush the buffer\n    model.evaluate(x, y, callbacks=[pb], verbose=0)\n    evaluate_stderr = capsys.readouterr().err\n    assert re.search(r""loss: [0-9]\\.[0-9][0-9][0-9][0-9]"", evaluate_stderr)\n    assert re.search(r""acc: [0-9]\\.[0-9][0-9][0-9][0-9]"", evaluate_stderr)\n'"
tensorflow_addons/image/tests/__init__.py,0,b''
tensorflow_addons/image/tests/color_ops_test.py,5,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of color ops""""""\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_addons.image import color_ops\nfrom PIL import Image, ImageOps, ImageEnhance\n\n_DTYPES = {\n    np.uint8,\n    np.int32,\n    np.int64,\n    np.float16,\n    np.float32,\n    np.float64,\n}\n\n\n@pytest.mark.parametrize(""dtype"", _DTYPES)\n@pytest.mark.parametrize(""shape"", [(7, 7), (5, 5, 1), (5, 5, 3), (5, 7, 7, 3)])\ndef test_equalize_dtype_shape(dtype, shape):\n    image = np.ones(shape=shape, dtype=dtype)\n    equalized = color_ops.equalize(tf.constant(image)).numpy()\n    np.testing.assert_equal(equalized, image)\n    assert equalized.dtype == image.dtype\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_equalize_with_PIL():\n    np.random.seed(0)\n    image = np.random.randint(low=0, high=255, size=(4, 3, 3, 3), dtype=np.uint8)\n    equalized = np.stack([ImageOps.equalize(Image.fromarray(i)) for i in image])\n    np.testing.assert_equal(color_ops.equalize(tf.constant(image)).numpy(), equalized)\n\n\n@pytest.mark.parametrize(""shape"", [(1, 5, 5), (3, 5, 5), (10, 3, 7, 7)])\ndef test_equalize_channel_first(shape):\n    image = tf.ones(shape=shape, dtype=tf.uint8)\n    equalized = color_ops.equalize(image, ""channels_first"")\n    np.testing.assert_equal(equalized.numpy(), image.numpy())\n\n\n@pytest.mark.parametrize(""dtype"", _DTYPES)\n@pytest.mark.parametrize(""shape"", [(5, 5, 3), (10, 5, 5, 3)])\ndef test_sharpness_dtype_shape(dtype, shape):\n    image = np.ones(shape=shape, dtype=dtype)\n    sharp = color_ops.sharpness(tf.constant(image), 0).numpy()\n    np.testing.assert_equal(sharp, image)\n    assert sharp.dtype == image.dtype\n\n\n@pytest.mark.parametrize(""factor"", [0, 0.25, 0.5, 0.75, 1])\ndef test_sharpness_with_PIL(factor):\n    np.random.seed(0)\n    image = np.random.randint(low=0, high=255, size=(10, 5, 5, 3), dtype=np.uint8)\n    sharpened = np.stack(\n        [ImageEnhance.Sharpness(Image.fromarray(i)).enhance(factor) for i in image]\n    )\n    np.testing.assert_allclose(\n        color_ops.sharpness(tf.constant(image), factor).numpy(), sharpened, atol=1\n    )\n'"
tensorflow_addons/image/tests/compose_ops_test.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of augmentation ops""""""\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_addons.image import compose_ops\n\n_DTYPES = {\n    tf.dtypes.uint8,\n    tf.dtypes.int32,\n    tf.dtypes.int64,\n    tf.dtypes.float16,\n    tf.dtypes.float32,\n    tf.dtypes.float64,\n}\n\n\ndef blend_np(image1, image2, factor):\n    image1 = image1.astype(""float32"")\n    image2 = image2.astype(""float32"")\n    difference = image2 - image1\n    scaled = factor * difference\n    temp = image1 + scaled\n    if factor >= 0.0 and factor <= 1.0:\n        temp = np.round(temp)\n        return temp\n    temp = np.round(np.clip(temp, 0.0, 255.0))\n    return temp\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_blend(dtype):\n    image1 = tf.constant(\n        [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=dtype\n    )\n    image2 = tf.constant(\n        [\n            [255, 255, 255, 255],\n            [255, 255, 255, 255],\n            [255, 255, 255, 255],\n            [255, 255, 255, 255],\n        ],\n        dtype=dtype,\n    )\n    blended = compose_ops.blend(image1, image2, 0.5).numpy()\n    np.testing.assert_equal(\n        blended,\n        [\n            [128, 128, 128, 128],\n            [128, 128, 128, 128],\n            [128, 128, 128, 128],\n            [128, 128, 128, 128],\n        ],\n    )\n\n    np.random.seed(0)\n    image1 = np.random.randint(0, 255, (3, 5, 5), np.uint8)\n    image2 = np.random.randint(0, 255, (3, 5, 5), np.uint8)\n    tf.random.set_seed(0)\n    factor = tf.random.uniform(shape=[], maxval=1, dtype=tf.dtypes.float32, seed=0)\n    blended = compose_ops.blend(\n        tf.convert_to_tensor(image1), tf.convert_to_tensor(image2), factor\n    ).numpy()\n    expected = blend_np(image1, image2, factor.numpy())\n    np.testing.assert_equal(blended, expected)\n    assert blended.dtype == expected.dtype\n'"
tensorflow_addons/image/tests/connected_components_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for connected component analysis.""""""\n\nimport pytest\nimport logging\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_addons.image.connected_components import connected_components\n\n# Image for testing connected_components, with a single, winding component.\nSNAKE = np.asarray(\n    [\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 1, 1, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 1, 1, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 1, 1, 1, 1, 1, 1, 1, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 1, 1, 1, 1, 1, 0],\n        [0, 1, 0, 0, 0, 0, 0, 1, 0],\n        [0, 1, 1, 1, 1, 1, 1, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n    ]\n)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_disconnected():\n    arr = tf.cast(\n        [\n            [1, 0, 0, 1, 0, 0, 0, 0, 1],\n            [0, 1, 0, 0, 0, 1, 0, 1, 0],\n            [1, 0, 1, 0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0, 0, 0],\n        ],\n        tf.bool,\n    )\n    expected = [\n        [1, 0, 0, 2, 0, 0, 0, 0, 3],\n        [0, 4, 0, 0, 0, 5, 0, 6, 0],\n        [7, 0, 8, 0, 0, 0, 9, 0, 0],\n        [0, 0, 0, 0, 10, 0, 0, 0, 0],\n        [0, 0, 11, 0, 0, 0, 0, 0, 0],\n    ]\n    np.testing.assert_equal(connected_components(arr).numpy(), expected)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_simple():\n    arr = [[0, 1, 0], [1, 1, 1], [0, 1, 0]]\n\n    # Single component with id 1.\n    np.testing.assert_equal(connected_components(tf.cast(arr, tf.bool)).numpy(), arr)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_snake():\n    # Single component with id 1.\n    np.testing.assert_equal(\n        connected_components(tf.cast(SNAKE, tf.bool)).numpy(), SNAKE\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_snake_disconnected():\n    for i in range(SNAKE.shape[0]):\n        for j in range(SNAKE.shape[1]):\n\n            # If we disconnect any part of the snake except for the endpoints,\n            # there will be 2 components.\n            if SNAKE[i, j] and (i, j) not in [(1, 1), (6, 3)]:\n                disconnected_snake = SNAKE.copy()\n                disconnected_snake[i, j] = 0\n                components = connected_components(tf.cast(disconnected_snake, tf.bool))\n                assert np.max(components) == 2\n\n                bins = np.bincount(components.numpy().ravel())\n                # Nonzero number of pixels labeled 0, 1, or 2.\n                assert bins[0] > 0\n                assert bins[1] > 0\n                assert bins[2] > 0\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_multiple_images():\n    images = tf.cast(\n        [\n            [[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]],\n            [[1, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 1]],\n            [[1, 1, 0, 1], [0, 1, 1, 0], [1, 0, 1, 0], [0, 0, 1, 1]],\n        ],\n        tf.bool,\n    )\n    expected = [\n        [[1, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1], [1, 1, 1, 1]],\n        [[2, 0, 0, 3], [0, 0, 0, 0], [0, 0, 0, 0], [4, 0, 0, 5]],\n        [[6, 6, 0, 7], [0, 6, 6, 0], [8, 0, 6, 0], [0, 0, 6, 6]],\n    ]\n\n    np.testing.assert_equal(connected_components(images).numpy(), expected)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_zeros():\n    np.testing.assert_equal(\n        connected_components(tf.zeros((100, 20, 50), tf.bool)), np.zeros((100, 20, 50)),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_ones():\n    np.testing.assert_equal(\n        connected_components(tf.ones((100, 20, 50), tf.bool)),\n        np.tile(np.arange(100)[:, None, None] + 1, [1, 20, 50]),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_ones_small():\n\n    np.testing.assert_equal(\n        connected_components(tf.ones((3, 5), tf.bool)).numpy(), np.ones((3, 5)),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_random_scipy():\n    np.random.seed(42)\n    images = np.random.randint(0, 2, size=(10, 100, 200)).astype(np.bool)\n    expected = connected_components_reference_implementation(images)\n    if expected is None:\n        return\n\n    np.testing.assert_equal(connected_components(images).numpy(), expected)\n\n\ndef connected_components_reference_implementation(images):\n    try:\n        from scipy.ndimage import measurements\n    except ImportError:\n        logging.exception(""Skipping test method because scipy could not be loaded"")\n        return\n    image_or_images = np.asarray(images)\n    if len(image_or_images.shape) == 2:\n        images = image_or_images[None, :, :]\n    elif len(image_or_images.shape) == 3:\n        images = image_or_images\n    components = np.asarray([measurements.label(image)[0] for image in images])\n    # Get the count of nonzero ids for each image, and offset each image\'s nonzero\n    # ids using the cumulative sum.\n    num_ids_per_image = components.reshape(\n        [-1, components.shape[1] * components.shape[2]]\n    ).max(axis=-1)\n    positive_id_start_per_image = np.cumsum(num_ids_per_image)\n    for i in range(components.shape[0]):\n        new_id_start = positive_id_start_per_image[i - 1] if i > 0 else 0\n        components[i, components[i] > 0] += new_id_start\n    if len(image_or_images.shape) == 2:\n        return components[0, :, :]\n    else:\n        return components\n'"
tensorflow_addons/image/tests/cutout_ops_test.py,21,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for cutout.""""""\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow_addons.image.cutout_ops import cutout, random_cutout\nfrom tensorflow_addons.image.utils import to_4D_image\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.uint8])\ndef test_different_dtypes(dtype):\n    test_image = tf.ones([1, 40, 40, 1], dtype=dtype)\n    result_image = cutout(test_image, 4, [2, 2])\n    cutout_area = tf.zeros([4, 4], dtype=dtype)\n    cutout_area = tf.pad(cutout_area, ((0, 36), (0, 36)), constant_values=1)\n    expect_image = to_4D_image(cutout_area)\n    np.testing.assert_allclose(result_image, expect_image)\n    assert result_image.dtype == dtype\n\n\ndef test_different_channels():\n    for channel in [0, 1, 3, 4]:\n        test_image = tf.ones([1, 40, 40, channel], dtype=np.uint8)\n        cutout_area = tf.zeros([4, 4], dtype=np.uint8)\n        cutout_area = tf.pad(cutout_area, ((0, 36), (0, 36)), constant_values=1)\n        expect_image = to_4D_image(cutout_area)\n        expect_image = tf.tile(expect_image, [1, 1, 1, channel])\n        result_image = random_cutout(test_image, 20, seed=1234)\n        np.testing.assert_allclose(tf.shape(result_image), tf.shape(expect_image))\n\n\ndef test_batch_size():\n    test_image = tf.random.uniform([10, 40, 40, 1], dtype=np.float32, seed=1234)\n    result_image = random_cutout(test_image, 20, seed=1234)\n    np.testing.assert_allclose(tf.shape(result_image), [10, 40, 40, 1])\n    means = np.mean(result_image, axis=(1, 2, 3))\n    np.testing.assert_allclose(len(set(means)), 10)\n\n\ndef test_channel_first():\n    test_image = tf.ones([10, 1, 40, 40], dtype=np.uint8)\n    cutout_area = tf.zeros([4, 4], dtype=np.uint8)\n    cutout_area = tf.pad(cutout_area, ((0, 36), (0, 36)), constant_values=1)\n    expect_image = tf.expand_dims(cutout_area, 0)\n    expect_image = tf.expand_dims(expect_image, 0)\n    expect_image = tf.tile(expect_image, [10, 1, 1, 1])\n    result_image = random_cutout(\n        test_image, 20, seed=1234, data_format=""channels_first""\n    )\n    np.testing.assert_allclose(tf.shape(result_image), tf.shape(expect_image))\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_with_tf_function():\n    test_image = tf.ones([1, 40, 40, 1], dtype=tf.uint8)\n    result_image = tf.function(random_cutout)(test_image, 2)\n    cutout_area = tf.zeros([4, 4], dtype=tf.uint8)\n    cutout_area = tf.pad(cutout_area, ((0, 36), (0, 36)), constant_values=1)\n    expect_image = to_4D_image(cutout_area)\n    np.testing.assert_equal(result_image.shape, expect_image.shape)\n'"
tensorflow_addons/image/tests/dense_image_warp_test.py,22,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for dense_image_warp.""""""\n\nimport pytest\nimport math\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.image import dense_image_warp\nfrom tensorflow_addons.image import interpolate_bilinear\n\n\ndef test_interpolate_small_grid_ij():\n    grid = tf.constant(\n        [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]],\n        shape=[1, 4, 3, 1],\n    )\n    query_points = tf.constant(\n        [[0.0, 0.0], [1.0, 0.0], [2.0, 0.5], [1.5, 1.5], [3.0, 2.0]], shape=[1, 5, 2],\n    )\n    expected_results = np.reshape(np.array([0.0, 3.0, 6.5, 6.0, 11.0]), [1, 5, 1])\n\n    interp = interpolate_bilinear(grid, query_points)\n\n    np.testing.assert_allclose(expected_results, interp)\n\n\ndef test_interpolate_small_grid_xy():\n    grid = tf.constant(\n        [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]],\n        shape=[1, 4, 3, 1],\n    )\n    query_points = tf.constant(\n        [[0.0, 0.0], [0.0, 1.0], [0.5, 2.0], [1.5, 1.5], [2.0, 3.0]], shape=[1, 5, 2],\n    )\n    expected_results = np.reshape(np.array([0.0, 3.0, 6.5, 6.0, 11.0]), [1, 5, 1])\n\n    interp = interpolate_bilinear(grid, query_points, indexing=""xy"")\n\n    np.testing.assert_allclose(expected_results, interp)\n\n\ndef test_interpolate_small_grid_batched():\n    grid = tf.constant(\n        [[[0.0, 1.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]], shape=[2, 2, 2, 1]\n    )\n    query_points = tf.constant(\n        [[[0.0, 0.0], [1.0, 0.0], [0.5, 0.5]], [[0.5, 0.0], [1.0, 0.0], [1.0, 1.0]]]\n    )\n    expected_results = np.reshape(\n        np.array([[0.0, 3.0, 2.0], [6.0, 7.0, 8.0]]), [2, 3, 1]\n    )\n\n    interp = interpolate_bilinear(grid, query_points)\n\n    np.testing.assert_allclose(expected_results, interp)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_unknown_shape():\n    query_points = tf.constant(\n        [[0.0, 0.0], [0.0, 1.0], [0.5, 2.0], [1.5, 1.5]], shape=[1, 4, 2]\n    )\n    fn = interpolate_bilinear.get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float32),\n        tf.TensorSpec(shape=None, dtype=tf.float32),\n    )\n    for shape in (2, 4, 3, 6), (6, 2, 4, 3), (1, 2, 4, 3):\n        image = tf.ones(shape=shape)\n        res = fn(image, query_points)\n        assert res.shape == (shape[0], 4, shape[3])\n\n\ndef _check_zero_flow_correctness(shape, image_type, flow_type):\n    """"""Assert using zero flows doesn\'t change the input image.""""""\n    rand_image, rand_flows = _get_random_image_and_flows(shape, image_type, flow_type)\n    rand_flows *= 0\n\n    interp = dense_image_warp(\n        image=tf.convert_to_tensor(rand_image), flow=tf.convert_to_tensor(rand_flows),\n    )\n\n    np.testing.assert_allclose(rand_image, interp, rtol=1e-6, atol=1e-6)\n\n\ndef test_zero_flows():\n    """"""Apply _check_zero_flow_correctness() for a few sizes and types.""""""\n    shapes_to_try = [[3, 4, 5, 6], [1, 2, 2, 1]]\n    for shape in shapes_to_try:\n        _check_zero_flow_correctness(shape, image_type=""float32"", flow_type=""float32"")\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_gradients_exist():\n    """"""Check that backprop can run.\n\n    The correctness of the gradients is assumed, since the forward\n    propagation is tested to be correct and we only use built-in tf\n    ops. However, we perform a simple test to make sure that\n    backprop can actually run.\n    """"""\n    batch_size, height, width, num_channels = [4, 5, 6, 7]\n    image_shape = [batch_size, height, width, num_channels]\n    image = tf.random.normal(image_shape)\n    flow_shape = [batch_size, height, width, 2]\n    flows = tf.Variable(tf.random.normal(shape=flow_shape) * 0.25, dtype=tf.float32)\n\n    with tf.GradientTape() as t:\n        interp = dense_image_warp(image, flows)\n\n    grads = t.gradient(interp, flows).numpy()\n    assert np.sum(np.abs(grads)) != 0\n\n\ndef _assert_correct_interpolation_value(\n    image,\n    flows,\n    pred_interpolation,\n    batch_index,\n    y_index,\n    x_index,\n    low_precision=False,\n):\n    """"""Assert that the tf interpolation matches hand-computed value.""""""\n    height = image.shape[1]\n    width = image.shape[2]\n    displacement = flows[batch_index, y_index, x_index, :]\n    float_y = y_index - displacement[0]\n    float_x = x_index - displacement[1]\n    floor_y = max(min(height - 2, math.floor(float_y)), 0)\n    floor_x = max(min(width - 2, math.floor(float_x)), 0)\n    ceil_y = floor_y + 1\n    ceil_x = floor_x + 1\n\n    alpha_y = min(max(0.0, float_y - floor_y), 1.0)\n    alpha_x = min(max(0.0, float_x - floor_x), 1.0)\n\n    floor_y = int(floor_y)\n    floor_x = int(floor_x)\n    ceil_y = int(ceil_y)\n    ceil_x = int(ceil_x)\n\n    top_left = image[batch_index, floor_y, floor_x, :]\n    top_right = image[batch_index, floor_y, ceil_x, :]\n    bottom_left = image[batch_index, ceil_y, floor_x, :]\n    bottom_right = image[batch_index, ceil_y, ceil_x, :]\n\n    interp_top = alpha_x * (top_right - top_left) + top_left\n    interp_bottom = alpha_x * (bottom_right - bottom_left) + bottom_left\n    interp = alpha_y * (interp_bottom - interp_top) + interp_top\n    atol = 1e-6\n    rtol = 1e-6\n    if low_precision:\n        atol = 1e-2\n        rtol = 1e-3\n    np.testing.assert_allclose(\n        interp,\n        pred_interpolation[batch_index, y_index, x_index, :],\n        atol=atol,\n        rtol=rtol,\n    )\n\n\ndef _get_random_image_and_flows(shape, image_type, flow_type):\n    batch_size, height, width, num_channels = shape\n    image_shape = [batch_size, height, width, num_channels]\n    image = np.random.normal(size=image_shape)\n    flow_shape = [batch_size, height, width, 2]\n    flows = np.random.normal(size=flow_shape) * 3\n    return image.astype(image_type), flows.astype(flow_type)\n\n\ndef _check_interpolation_correctness(\n    shape, image_type, flow_type, call_with_unknown_shapes=False, num_probes=5\n):\n    """"""Interpolate, and then assert correctness for a few query\n    locations.""""""\n    low_precision = image_type == ""float16"" or flow_type == ""float16""\n    rand_image, rand_flows = _get_random_image_and_flows(shape, image_type, flow_type)\n\n    if call_with_unknown_shapes:\n        fn = dense_image_warp.get_concrete_function(\n            tf.TensorSpec(shape=None, dtype=image_type),\n            tf.TensorSpec(shape=None, dtype=flow_type),\n        )\n        interp = fn(\n            image=tf.convert_to_tensor(rand_image),\n            flow=tf.convert_to_tensor(rand_flows),\n        )\n    else:\n        interp = dense_image_warp(\n            image=tf.convert_to_tensor(rand_image),\n            flow=tf.convert_to_tensor(rand_flows),\n        )\n\n    for _ in range(num_probes):\n        batch_index = np.random.randint(0, shape[0])\n        y_index = np.random.randint(0, shape[1])\n        x_index = np.random.randint(0, shape[2])\n\n        _assert_correct_interpolation_value(\n            rand_image,\n            rand_flows,\n            interp,\n            batch_index,\n            y_index,\n            x_index,\n            low_precision=low_precision,\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_interpolation():\n    """"""Apply _check_interpolation_correctness() for a few sizes and\n    types.""""""\n    shapes_to_try = [[3, 4, 5, 6], [1, 2, 2, 1]]\n    for im_type in [""float32"", ""float64"", ""float16""]:\n        for flow_type in [""float32"", ""float64"", ""float16""]:\n            for shape in shapes_to_try:\n                _check_interpolation_correctness(shape, im_type, flow_type)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_size_exception():\n    """"""Make sure it throws an exception for images that are too small.""""""\n    shape = [1, 2, 1, 1]\n    errors = (ValueError, tf.errors.InvalidArgumentError)\n    with pytest.raises(errors) as exception_raised:\n        _check_interpolation_correctness(shape, ""float32"", ""float32"")\n    assert ""Grid width must be at least 2."" in str(exception_raised.value)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_unknown_shapes():\n    """"""Apply _check_interpolation_correctness() for a few sizes and check\n    for tf.Dataset compatibility.""""""\n    shapes_to_try = [[3, 4, 5, 6], [1, 2, 2, 1]]\n    for shape in shapes_to_try:\n        _check_interpolation_correctness(shape, ""float32"", ""float32"", True)\n'"
tensorflow_addons/image/tests/distance_transform_test.py,13,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for distance transform ops.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.image import distance_transform as dist_ops\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [tf.float16, tf.float32, tf.float64])\ndef test_single_binary_image(dtype):\n    image = [\n        [[1], [1], [1], [1], [1]],\n        [[1], [1], [1], [1], [1]],\n        [[0], [1], [0], [1], [0]],\n        [[1], [0], [1], [0], [1]],\n        [[0], [1], [0], [1], [0]],\n    ]\n    expected_output = np.array(\n        [\n            2,\n            2.23606801,\n            2,\n            2.23606801,\n            2,\n            1,\n            1.41421354,\n            1,\n            1.41421354,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n            1,\n            0,\n        ]\n    )\n    image = tf.constant(image, dtype=tf.uint8)\n\n    output = dist_ops.euclidean_dist_transform(image, dtype=dtype)\n    output_flat = tf.reshape(output, [-1])\n\n    assert output.dtype == dtype\n    assert output.shape == [5, 5, 1]\n    test_utils.assert_allclose_according_to_type(output_flat, expected_output)\n\n\n@pytest.mark.parametrize(""dtype"", [tf.float16, tf.float32, tf.float64])\ndef test_batch_binary_images(dtype):\n    batch_size = 3\n    image = [\n        [[0], [0], [0], [0], [0]],\n        [[0], [1], [1], [1], [0]],\n        [[0], [1], [1], [1], [0]],\n        [[0], [1], [1], [1], [0]],\n        [[0], [0], [0], [0], [0]],\n    ]\n    expected_output = np.array(\n        [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n        * batch_size\n    )\n    images = tf.constant([image] * batch_size, dtype=tf.uint8)\n\n    output = dist_ops.euclidean_dist_transform(images, dtype=dtype)\n    output_flat = tf.reshape(output, [-1])\n\n    assert output.shape == [batch_size, 5, 5, 1]\n    test_utils.assert_allclose_according_to_type(output_flat, expected_output)\n\n\n@pytest.mark.parametrize(""dtype"", [tf.uint8, tf.int32, tf.int64])\ndef test_image_with_invalid_dtype(dtype):\n    image = [\n        [[1], [1], [1], [1], [1]],\n        [[1], [1], [1], [1], [1]],\n        [[0], [1], [0], [1], [0]],\n        [[1], [0], [1], [0], [1]],\n        [[0], [1], [0], [1], [0]],\n    ]\n    image = tf.constant(image, dtype=tf.uint8)\n\n    with pytest.raises(TypeError, match=""`dtype` must be float16, float32 or float64""):\n        _ = dist_ops.euclidean_dist_transform(image, dtype=dtype)\n\n\ndef test_image_with_invalid_shape():\n    image = tf.zeros([2, 4, 3], tf.uint8)\n    with pytest.raises(ValueError, match=""`images` must have only one channel""):\n        _ = dist_ops.euclidean_dist_transform(image)\n\n\ndef test_all_zeros():\n    image = tf.zeros([10, 10], tf.uint8)\n    expected_output = np.zeros([10, 10])\n\n    for output_dtype in [tf.float16, tf.float32, tf.float64]:\n        output = dist_ops.euclidean_dist_transform(image, dtype=output_dtype)\n        np.testing.assert_allclose(output, expected_output)\n\n\ndef test_all_ones():\n    image = tf.ones([10, 10, 1], tf.uint8)\n    output = dist_ops.euclidean_dist_transform(image)\n    expected_output = np.full([10, 10, 1], tf.float32.max)\n    np.testing.assert_allclose(output, expected_output)\n'"
tensorflow_addons/image/tests/distort_image_ops_test.py,17,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may noa use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for python distort_image_ops.""""""\n\nimport pytest\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import distort_image_ops\n\n\ndef _adjust_hue_in_yiq_np(x_np, delta_h):\n    """"""Rotate hue in YIQ space.\n\n    Mathematically we first convert rgb color to yiq space, rotate the hue\n    degrees, and then convert back to rgb.\n\n    Args:\n        x_np: input x with last dimension = 3.\n        delta_h: degree of hue rotation, in radians.\n\n    Returns:\n        Adjusted y with the same shape as x_np.\n    """"""\n    assert x_np.shape[-1] == 3\n    x_v = x_np.reshape([-1, 3])\n    y_v = np.ndarray(x_v.shape, dtype=x_v.dtype)\n    u = np.cos(delta_h)\n    w = np.sin(delta_h)\n    # Projection matrix from RGB to YIQ. Numbers from wikipedia\n    # https://en.wikipedia.org/wiki/YIQ\n    tyiq = np.array(\n        [[0.299, 0.587, 0.114], [0.596, -0.274, -0.322], [0.211, -0.523, 0.312]]\n    )\n    y_v = np.dot(x_v, tyiq.T)\n    # Hue rotation matrix in YIQ space.\n    hue_rotation = np.array([[1.0, 0.0, 0.0], [0.0, u, -w], [0.0, w, u]])\n    y_v = np.dot(y_v, hue_rotation.T)\n    # Projecting back to RGB space.\n    y_v = np.dot(y_v, np.linalg.inv(tyiq).T)\n    return y_v.reshape(x_np.shape)\n\n\ndef _adjust_hue_in_yiq_tf(x_np, delta_h):\n    x = tf.constant(x_np)\n    y = distort_image_ops.adjust_hsv_in_yiq(x, delta_h, 1, 1)\n    return y\n\n\ndef test_adjust_random_hue_in_yiq():\n    x_shapes = [\n        [2, 2, 3],\n        [4, 2, 3],\n        [2, 4, 3],\n        [2, 5, 3],\n        [1000, 1, 3],\n    ]\n    test_styles = [\n        ""all_random"",\n        ""rg_same"",\n        ""rb_same"",\n        ""gb_same"",\n        ""rgb_same"",\n    ]\n    for x_shape in x_shapes:\n        for test_style in test_styles:\n            x_np = np.random.rand(*x_shape) * 255.0\n            delta_h = (np.random.rand() * 2.0 - 1.0) * np.pi\n            if test_style == ""all_random"":\n                pass\n            elif test_style == ""rg_same"":\n                x_np[..., 1] = x_np[..., 0]\n            elif test_style == ""rb_same"":\n                x_np[..., 2] = x_np[..., 0]\n            elif test_style == ""gb_same"":\n                x_np[..., 2] = x_np[..., 1]\n            elif test_style == ""rgb_same"":\n                x_np[..., 1] = x_np[..., 0]\n                x_np[..., 2] = x_np[..., 0]\n            else:\n                raise AssertionError(""Invalid test style: %s"" % (test_style))\n            y_np = _adjust_hue_in_yiq_np(x_np, delta_h)\n            y_tf = _adjust_hue_in_yiq_tf(x_np, delta_h)\n            np.testing.assert_allclose(y_tf, y_np, rtol=2e-4, atol=1e-4)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_invalid_rank_hsv():\n    x_np = np.random.rand(2, 3) * 255.0\n    delta_h = np.random.rand() * 2.0 - 1.0\n    with pytest.raises(\n        tf.errors.InvalidArgumentError, match=""input must be at least 3-D""\n    ):\n        _adjust_hue_in_yiq_tf(x_np, delta_h)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_invalid_channels_hsv():\n    x_np = np.random.rand(4, 2, 4) * 255.0\n    delta_h = np.random.rand() * 2.0 - 1.0\n    with pytest.raises(\n        tf.errors.InvalidArgumentError,\n        match=""input must have 3 channels but instead has 4"",\n    ):\n        _adjust_hue_in_yiq_tf(x_np, delta_h)\n\n\ndef test_adjust_hsv_in_yiq_unknown_shape():\n    fn = tf.function(distort_image_ops.adjust_hsv_in_yiq).get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float64)\n    )\n    for shape in (2, 3, 3), (4, 2, 3, 3):\n        image_np = np.random.rand(*shape) * 255.0\n        image_tf = tf.constant(image_np)\n        np.testing.assert_allclose(\n            _adjust_hue_in_yiq_np(image_np, 0), fn(image_tf), rtol=2e-4, atol=1e-4,\n        )\n\n\ndef test_random_hsv_in_yiq_unknown_shape():\n    fn = tf.function(distort_image_ops.random_hsv_in_yiq).get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float32)\n    )\n    for shape in (2, 3, 3), (4, 2, 3, 3):\n        image_tf = tf.ones(shape)\n        np.testing.assert_equal(fn(image_tf).numpy(), fn(image_tf).numpy())\n\n\ndef _adjust_value_in_yiq_np(x_np, scale):\n    return x_np * scale\n\n\ndef _adjust_value_in_yiq_tf(x_np, scale):\n    x = tf.constant(x_np)\n    y = distort_image_ops.adjust_hsv_in_yiq(x, 0, 1, scale)\n    return y\n\n\ndef test_adjust_random_value_in_yiq():\n    x_shapes = [\n        [2, 2, 3],\n        [4, 2, 3],\n        [2, 4, 3],\n        [2, 5, 3],\n        [1000, 1, 3],\n    ]\n    test_styles = [\n        ""all_random"",\n        ""rg_same"",\n        ""rb_same"",\n        ""gb_same"",\n        ""rgb_same"",\n    ]\n    for x_shape in x_shapes:\n        for test_style in test_styles:\n            x_np = np.random.rand(*x_shape) * 255.0\n            scale = np.random.rand() * 2.0 - 1.0\n            if test_style == ""all_random"":\n                pass\n            elif test_style == ""rg_same"":\n                x_np[..., 1] = x_np[..., 0]\n            elif test_style == ""rb_same"":\n                x_np[..., 2] = x_np[..., 0]\n            elif test_style == ""gb_same"":\n                x_np[..., 2] = x_np[..., 1]\n            elif test_style == ""rgb_same"":\n                x_np[..., 1] = x_np[..., 0]\n                x_np[..., 2] = x_np[..., 0]\n            else:\n                raise AssertionError(""Invalid test style: %s"" % (test_style))\n            y_np = _adjust_value_in_yiq_np(x_np, scale)\n            y_tf = _adjust_value_in_yiq_tf(x_np, scale)\n            np.testing.assert_allclose(y_tf, y_np, rtol=2e-4, atol=1e-4)\n\n\ndef test_invalid_rank_value():\n    x_np = np.random.rand(2, 3) * 255.0\n    scale = np.random.rand() * 2.0 - 1.0\n    if tf.executing_eagerly():\n        with pytest.raises(\n            tf.errors.InvalidArgumentError, match=""input must be at least 3-D""\n        ):\n            _adjust_value_in_yiq_tf(x_np, scale)\n    else:\n        with pytest.raises(\n            ValueError, match=""Shape must be at least rank 3 but is rank 2""\n        ):\n            _adjust_value_in_yiq_tf(x_np, scale)\n\n\ndef test_invalid_channels_value():\n    x_np = np.random.rand(4, 2, 4) * 255.0\n    scale = np.random.rand() * 2.0 - 1.0\n    if tf.executing_eagerly():\n        with pytest.raises(\n            tf.errors.InvalidArgumentError,\n            match=""input must have 3 channels but instead has 4"",\n        ):\n            _adjust_value_in_yiq_tf(x_np, scale)\n    else:\n        with pytest.raises(ValueError, match=""Dimension must be 3 but is 4""):\n            _adjust_value_in_yiq_tf(x_np, scale)\n\n\ndef _adjust_saturation_in_yiq_tf(x_np, scale):\n    x = tf.constant(x_np)\n    y = distort_image_ops.adjust_hsv_in_yiq(x, 0, scale, 1)\n    return y\n\n\ndef _adjust_saturation_in_yiq_np(x_np, scale):\n    """"""Adjust saturation using linear interpolation.""""""\n    rgb_weights = np.array([0.299, 0.587, 0.114])\n    gray = np.sum(x_np * rgb_weights, axis=-1, keepdims=True)\n    y_v = x_np * scale + gray * (1 - scale)\n    return y_v\n\n\ndef test_adjust_random_saturation_in_yiq():\n    x_shapes = [\n        [2, 2, 3],\n        [4, 2, 3],\n        [2, 4, 3],\n        [2, 5, 3],\n        [1000, 1, 3],\n    ]\n    test_styles = [\n        ""all_random"",\n        ""rg_same"",\n        ""rb_same"",\n        ""gb_same"",\n        ""rgb_same"",\n    ]\n    for x_shape in x_shapes:\n        for test_style in test_styles:\n            x_np = np.random.rand(*x_shape) * 255.0\n            scale = np.random.rand() * 2.0 - 1.0\n            if test_style == ""all_random"":\n                pass\n            elif test_style == ""rg_same"":\n                x_np[..., 1] = x_np[..., 0]\n            elif test_style == ""rb_same"":\n                x_np[..., 2] = x_np[..., 0]\n            elif test_style == ""gb_same"":\n                x_np[..., 2] = x_np[..., 1]\n            elif test_style == ""rgb_same"":\n                x_np[..., 1] = x_np[..., 0]\n                x_np[..., 2] = x_np[..., 0]\n            else:\n                raise AssertionError(""Invalid test style: %s"" % (test_style))\n            y_baseline = _adjust_saturation_in_yiq_np(x_np, scale)\n            y_tf = _adjust_saturation_in_yiq_tf(x_np, scale)\n            np.testing.assert_allclose(y_tf, y_baseline, rtol=2e-4, atol=1e-4)\n\n\ndef test_invalid_rank():\n    x_np = np.random.rand(2, 3) * 255.0\n    scale = np.random.rand() * 2.0 - 1.0\n\n    msg = ""input must be at least 3-D""\n    with pytest.raises(tf.errors.InvalidArgumentError, match=msg):\n        _adjust_saturation_in_yiq_tf(x_np, scale).numpy()\n\n\ndef test_invalid_channels():\n    x_np = np.random.rand(4, 2, 4) * 255.0\n    scale = np.random.rand() * 2.0 - 1.0\n    msg = ""input must have 3 channels but instead has 4 ""\n    with pytest.raises(tf.errors.InvalidArgumentError, match=msg):\n        _adjust_saturation_in_yiq_tf(x_np, scale).numpy()\n'"
tensorflow_addons/image/tests/filters_test.py,46,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may noa use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.image import mean_filter2d\nfrom tensorflow_addons.image import median_filter2d\nfrom tensorflow_addons.image import gaussian_filter2d\nfrom tensorflow_addons.utils import test_utils\nfrom scipy.ndimage.filters import gaussian_filter\n\n_dtypes_to_test = {\n    tf.dtypes.uint8,\n    tf.dtypes.int32,\n    tf.dtypes.float16,\n    tf.dtypes.float32,\n    tf.dtypes.float64,\n}\n\n_image_shapes_to_test = [\n    (3, 3, 1),\n    (3, 3, 3),\n    (1, 3, 3, 1),\n    (1, 3, 3, 3),\n    (2, 3, 3, 1),\n    (2, 3, 3, 3),\n]\n\n\ndef tile_image(plane, image_shape):\n    """"""Tile a 2-D image `plane` into 3-D or 4-D as per `image_shape`.""""""\n    assert 3 <= len(image_shape) <= 4\n    plane = tf.convert_to_tensor(plane)\n    plane = tf.expand_dims(plane, -1)\n    channels = image_shape[-1]\n    image = tf.tile(plane, (1, 1, channels))\n\n    if len(image_shape) == 4:\n        batch_size = image_shape[0]\n        image = tf.expand_dims(image, 0)\n        image = tf.tile(image, (batch_size, 1, 1, 1))\n\n    return image\n\n\ndef setup_values(\n    filter2d_fn, image_shape, filter_shape, padding, constant_values, dtype\n):\n    assert 3 <= len(image_shape) <= 4\n    height, width = image_shape[-3], image_shape[-2]\n    plane = tf.constant(\n        [x for x in range(1, height * width + 1)], shape=(height, width), dtype=dtype,\n    )\n    image = tile_image(plane, image_shape=image_shape)\n\n    result = filter2d_fn(\n        image,\n        filter_shape=filter_shape,\n        padding=padding,\n        constant_values=constant_values,\n    )\n\n    return result\n\n\ndef verify_values(\n    filter2d_fn, image_shape, filter_shape, padding, constant_values, expected_plane\n):\n    expected_output = tile_image(expected_plane, image_shape)\n    for dtype in _dtypes_to_test:\n        result = setup_values(\n            filter2d_fn, image_shape, filter_shape, padding, constant_values, dtype\n        )\n        np.testing.assert_allclose(\n            result.numpy(),\n            tf.dtypes.cast(expected_output, dtype).numpy(),\n            rtol=1e-02,\n            atol=1e-02,\n        )\n\n    def setUp(self):\n        self._filter2d_fn = mean_filter2d\n        super().setUp()\n\n\n@pytest.mark.parametrize(""image_shape"", [(1,), (16, 28, 28, 1, 1)])\ndef test_invalid_image_mean(image_shape):\n    with pytest.raises((ValueError, tf.errors.InvalidArgumentError)):\n        image = tf.ones(shape=image_shape)\n        mean_filter2d(image)\n\n\n@pytest.mark.parametrize(""filter_shape"", [(3, 3, 3), (3, None, 3)])\ndef test_invalid_filter_shape_mean(filter_shape):\n    image = tf.ones(shape=(1, 28, 28, 1))\n\n    with pytest.raises(ValueError):\n        mean_filter2d(image, filter_shape=filter_shape)\n\n    filter_shape = None\n    with pytest.raises(TypeError):\n        mean_filter2d(image, filter_shape=filter_shape)\n\n\ndef test_invalid_padding_mean():\n    image = tf.ones(shape=(1, 28, 28, 1))\n\n    with pytest.raises(ValueError):\n        mean_filter2d(image, padding=""TEST"")\n\n\ndef test_none_channels_mean():\n    # 3-D image\n    fn = mean_filter2d.get_concrete_function(\n        tf.TensorSpec(dtype=tf.dtypes.float32, shape=(3, 3, None))\n    )\n    fn(tf.ones(shape=(3, 3, 1)))\n    fn(tf.ones(shape=(3, 3, 3)))\n\n    # 4-D image\n    fn = mean_filter2d.get_concrete_function(\n        tf.TensorSpec(dtype=tf.dtypes.float32, shape=(1, 3, 3, None))\n    )\n    fn(tf.ones(shape=(1, 3, 3, 1)))\n    fn(tf.ones(shape=(1, 3, 3, 3)))\n\n\n@pytest.mark.parametrize(""shape"", [(3, 3), (3, 3, 3), (1, 3, 3, 3)])\ndef test_unknown_shape_mean(shape):\n    fn = mean_filter2d.get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.dtypes.float32),\n        padding=""CONSTANT"",\n        constant_values=1.0,\n    )\n\n    image = tf.ones(shape=shape)\n    np.testing.assert_equal(image.numpy(), fn(image).numpy())\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_reflect_padding_with_3x3_filter_mean(image_shape):\n    expected_plane = tf.constant(\n        [\n            [3.6666667, 4.0, 4.3333335],\n            [4.6666665, 5.0, 5.3333335],\n            [5.6666665, 6.0, 6.3333335],\n        ]\n    )\n\n    verify_values(\n        mean_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""REFLECT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_reflect_padding_with_4x4_filter_mean(image_shape):\n    expected_plane = tf.constant([[5.0, 5.0, 5.0], [5.0, 5.0, 5.0], [5.0, 5.0, 5.0],])\n\n    verify_values(\n        mean_filter2d,\n        image_shape=image_shape,\n        filter_shape=(4, 4),\n        padding=""REFLECT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_constant_padding_with_3x3_filter_mean(image_shape):\n    expected_plane = tf.constant(\n        [\n            [1.3333334, 2.3333333, 1.7777778],\n            [3.0, 5.0, 3.6666667],\n            [2.6666667, 4.3333335, 3.1111112],\n        ]\n    )\n\n    verify_values(\n        mean_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""CONSTANT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n    expected_plane = tf.constant(\n        [\n            [1.8888888, 2.6666667, 2.3333333],\n            [3.3333333, 5.0, 4.0],\n            [3.2222223, 4.6666665, 3.6666667],\n        ]\n    )\n\n    verify_values(\n        mean_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""CONSTANT"",\n        constant_values=1,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_symmetric_padding_with_3x3_filter_mean(image_shape):\n    expected_plane = tf.constant(\n        [\n            [2.3333333, 3.0, 3.6666667],\n            [4.3333335, 5.0, 5.6666665],\n            [6.3333335, 7.0, 7.6666665],\n        ]\n    )\n\n    verify_values(\n        mean_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""SYMMETRIC"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.parametrize(""image_shape"", [(1,), (16, 28, 28, 1, 1)])\ndef test_invalid_image_median(image_shape):\n    with pytest.raises((ValueError, tf.errors.InvalidArgumentError)):\n        image = tf.ones(shape=image_shape)\n        median_filter2d(image)\n\n\n@pytest.mark.parametrize(""filter_shape"", [(3, 3, 3), (3, None, 3)])\ndef test_invalid_filter_shape_median(filter_shape):\n    image = tf.ones(shape=(1, 28, 28, 1))\n\n    with pytest.raises(ValueError):\n        median_filter2d(image, filter_shape=filter_shape)\n\n    filter_shape = None\n    with pytest.raises(TypeError):\n        mean_filter2d(image, filter_shape=filter_shape)\n\n\ndef test_invalid_padding_median():\n    image = tf.ones(shape=(1, 28, 28, 1))\n\n    with pytest.raises(ValueError):\n        median_filter2d(image, padding=""TEST"")\n\n\ndef test_none_channels_median():\n    # 3-D image\n    fn = median_filter2d.get_concrete_function(\n        tf.TensorSpec(dtype=tf.dtypes.float32, shape=(3, 3, None))\n    )\n    fn(tf.ones(shape=(3, 3, 1)))\n    fn(tf.ones(shape=(3, 3, 3)))\n\n    # 4-D image\n    fn = median_filter2d.get_concrete_function(\n        tf.TensorSpec(dtype=tf.dtypes.float32, shape=(1, 3, 3, None))\n    )\n    fn(tf.ones(shape=(1, 3, 3, 1)))\n    fn(tf.ones(shape=(1, 3, 3, 3)))\n\n\n@pytest.mark.parametrize(""shape"", [(3, 3), (3, 3, 3), (1, 3, 3, 3)])\ndef test_unknown_shape_median(shape):\n    fn = median_filter2d.get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.dtypes.float32),\n        padding=""CONSTANT"",\n        constant_values=1.0,\n    )\n\n    image = tf.ones(shape=shape)\n    np.testing.assert_equal(image.numpy(), fn(image).numpy())\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_reflect_padding_with_3x3_filter_median(image_shape):\n    expected_plane = tf.constant([[4, 4, 5], [5, 5, 5], [5, 6, 6]])\n\n    verify_values(\n        median_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""REFLECT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_reflect_padding_with_4x4_filter_median(image_shape):\n    expected_plane = tf.constant([[5, 5, 5], [5, 5, 5], [5, 5, 5]])\n\n    verify_values(\n        median_filter2d,\n        image_shape=image_shape,\n        filter_shape=(4, 4),\n        padding=""REFLECT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_constant_padding_with_3x3_filter(image_shape):\n    expected_plane = tf.constant([[0, 2, 0], [2, 5, 3], [0, 5, 0]])\n\n    verify_values(\n        median_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""CONSTANT"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n    expected_plane = tf.constant([[1, 2, 1], [2, 5, 3], [1, 5, 1]])\n\n    verify_values(\n        median_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""CONSTANT"",\n        constant_values=1,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""image_shape"", _image_shapes_to_test)\ndef test_symmetric_padding_with_3x3_filter_median(image_shape):\n    expected_plane = tf.constant([[2, 3, 3], [4, 5, 6], [7, 7, 8]])\n\n    verify_values(\n        median_filter2d,\n        image_shape=image_shape,\n        filter_shape=(3, 3),\n        padding=""SYMMETRIC"",\n        constant_values=0,\n        expected_plane=expected_plane,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""shape"", [[10, 10], [10, 10, 3], [2, 10, 10, 3]])\n@pytest.mark.parametrize(""padding"", [""SYMMETRIC"", ""CONSTANT"", ""REFLECT""])\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_gaussian_filter2d(shape, padding, dtype):\n    modes = {\n        ""SYMMETRIC"": ""reflect"",\n        ""CONSTANT"": ""constant"",\n        ""REFLECT"": ""mirror"",\n    }\n\n    image = np.arange(np.prod(shape)).reshape(*shape).astype(dtype)\n\n    ndims = len(shape)\n    sigma = [1.0, 1.0]\n    if ndims == 3:\n        sigma = [1.0, 1.0, 0.0]\n    elif ndims == 4:\n        sigma = [0.0, 1.0, 1.0, 0.0]\n\n    test_utils.assert_allclose_according_to_type(\n        gaussian_filter2d(image, 9, 1, padding=padding).numpy(),\n        gaussian_filter(image, sigma, mode=modes[padding]),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_gaussian_filter2d_different_sigma():\n    image = np.arange(40 * 40).reshape(40, 40).astype(np.float32)\n    sigma = [1.0, 2.0]\n\n    test_utils.assert_allclose_according_to_type(\n        gaussian_filter2d(image, [9, 17], sigma).numpy(),\n        gaussian_filter(image, sigma, mode=""mirror""),\n    )\n'"
tensorflow_addons/image/tests/interpolate_spline_test.py,21,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for interpolate_spline.""""""\n\nimport numpy as np\nimport pytest\nfrom scipy import interpolate as sc_interpolate\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import interpolate_spline\n\n\nclass _InterpolationProblem:\n    """"""Abstract class for interpolation problem descriptions.""""""\n\n    def get_problem(self, optimizable=False, extrapolate=True, dtype=""float32""):\n        """"""Make data for an interpolation problem where all x vectors are n-d.\n\n        Args:\n            optimizable: If True, then make train_points a tf.Variable.\n            extrapolate: If False, then clamp the query_points values to be\n                within the max and min of train_points.\n            dtype: The data type to use.\n\n        Returns:\n            query_points, query_values, train_points, train_values: training\n                and test tensors for interpolation problem.\n        """"""\n\n        # The values generated here depend on a seed of 0.\n        np.random.seed(0)\n\n        batch_size = 1\n        num_training_points = 10\n        num_query_points = 4\n\n        init_points = np.random.uniform(\n            size=[batch_size, num_training_points, self.DATA_DIM]\n        )\n\n        init_points = init_points.astype(dtype)\n        train_points = (\n            tf.Variable(init_points) if optimizable else tf.constant(init_points)\n        )\n        train_values = self.test_function(train_points)\n\n        query_points_np = np.random.uniform(\n            size=[batch_size, num_query_points, self.DATA_DIM]\n        )\n        query_points_np = query_points_np.astype(dtype)\n        if not extrapolate:\n            query_points_np = np.clip(\n                query_points_np, np.min(init_points), np.max(init_points)\n            )\n\n        query_points = tf.constant(query_points_np)\n        query_values = self.test_function(query_points_np)\n\n        return query_points, query_values, train_points, train_values\n\n\nclass _QuadraticPlusSinProblem1D(_InterpolationProblem):\n    """"""1D interpolation problem used for regression testing.""""""\n\n    DATA_DIM = 1\n    HARDCODED_QUERY_VALUES = {\n        (1.0, 0.0): [6.2647187603, -7.84362604077, -5.63690142322, 1.42928896387],\n        (1.0, 0.01): [6.77688289946, -8.02163669853, -5.79491157027, 1.4063285693],\n        (2.0, 0.0): [8.67110264937, -8.41281390883, -5.80190044693, 1.50155606059],\n        (2.0, 0.01): [6.70797816797, -7.49709587663, -5.28965776238, 1.52284731741],\n        (3.0, 0.0): [9.37691802935, -8.50390141515, -5.80786417426, 1.63467762122],\n        (3.0, 0.01): [4.47106304758, -5.71266128361, -3.92529303296, 1.86755293857],\n        (4.0, 0.0): [9.58172461111, -8.51432104771, -5.80967675388, 1.63361164256],\n        (4.0, 0.01): [-3.87902711352, -0.0253462273846, 1.79857618022, -0.769339675725],\n    }\n\n    def test_function(self, x):\n        """"""Takes a tensor, evaluates the test function, and returns a\n        tensor.""""""\n        return tf.reduce_mean(\n            tf.pow((x - 0.5), 3) - 0.25 * x + 10 * tf.sin(x * 10), 2, keepdims=True\n        )\n\n\nclass _QuadraticPlusSinProblemND(_InterpolationProblem):\n    """"""3D interpolation problem used for regression testing.""""""\n\n    DATA_DIM = 3\n    HARDCODED_QUERY_VALUES = {\n        (1.0, 0.0): [1.06609663962, 1.28894849357, 1.10882405595, 1.63966936885],\n        (1.0, 0.01): [1.03123780748, 1.2952930985, 1.10366822954, 1.65265118569],\n        (2.0, 0.0): [0.627787735064, 1.43802857251, 1.00194632358, 1.91667538215],\n        (2.0, 0.01): [0.730159985046, 1.41702471595, 1.0065827217, 1.85758519312],\n        (3.0, 0.0): [0.350460417862, 1.67223539464, 1.00475331246, 2.31580322491],\n        (3.0, 0.01): [0.624557250556, 1.63138876667, 0.976588193162, 2.12511237866],\n        (4.0, 0.0): [0.898129669986, 1.24434133638, -0.938056116931, 1.59910338833],\n        (4.0, 0.01): [0.0930360338179, -3.38791305538, -1.00969032567, 0.745535080382],\n    }\n\n    def test_function(self, x):\n        """"""Takes a tensor, evaluates the test function, and returns a\n        tensor.""""""\n        return tf.reduce_sum(\n            tf.square(x - 0.5) + 0.25 * x + 1 * tf.sin(x * 15), 2, keepdims=True\n        )\n\n\ndef test_1d_linear_interpolation():\n    """"""For 1d linear interpolation, we can compare directly to scipy.""""""\n    tp = _QuadraticPlusSinProblem1D()\n    (query_points, _, train_points, train_values) = tp.get_problem(\n        extrapolate=False, dtype=""float64""\n    )\n    interpolation_order = 1\n\n    with tf.name_scope(""interpolator""):\n        interp = interpolate_spline(\n            train_points, train_values, query_points, interpolation_order\n        ).numpy()\n\n        # Just look at the first element of the minibatch.\n        # Also, trim the final singleton dimension.\n        interp = interp[0, :, 0]\n        query_points = query_points.numpy()[0, :, 0]\n        train_points = train_points.numpy()[0, :, 0]\n        train_values = train_values.numpy()[0, :, 0]\n\n        # Compute scipy interpolation.\n        scipy_interp_function = sc_interpolate.interp1d(\n            train_points, train_values, kind=""linear""\n        )\n\n        scipy_interpolation = scipy_interp_function(query_points)\n        scipy_interpolation_on_train = scipy_interp_function(train_points)\n\n        # Even with float64 precision, the interpolants disagree with scipy a\n        # bit due to the fact that we add the EPSILON to prevent sqrt(0), etc.\n        tol = 1e-3\n\n        np.testing.assert_allclose(\n            train_values, scipy_interpolation_on_train, atol=tol, rtol=tol\n        )\n        np.testing.assert_allclose(interp, scipy_interpolation, atol=tol, rtol=tol)\n\n\ndef test_1d_interpolation():\n    """"""Regression test for interpolation with 1-D points.""""""\n\n    tp = _QuadraticPlusSinProblem1D()\n    (query_points, _, train_points, train_values) = tp.get_problem(dtype=""float64"")\n\n    for order in (1, 2, 3):\n        for reg_weight in (0, 0.01):\n            interp = interpolate_spline(\n                train_points, train_values, query_points, order, reg_weight\n            )\n\n            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]\n            target_interpolation = np.array(target_interpolation)\n\n            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)\n\n\ndef test_nd_linear_interpolation():\n    """"""Regression test for interpolation with N-D points.""""""\n\n    tp = _QuadraticPlusSinProblemND()\n    (query_points, _, train_points, train_values) = tp.get_problem(dtype=""float64"")\n\n    for order in (1, 2, 3):\n        for reg_weight in (0, 0.01):\n            interp = interpolate_spline(\n                train_points, train_values, query_points, order, reg_weight\n            )\n\n            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]\n            target_interpolation = np.array(target_interpolation)\n\n            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)\n\n\ndef test_nd_linear_interpolation_unspecified_shape():\n    """"""Ensure that interpolation supports dynamic batch_size and\n    num_points.""""""\n\n    tp = _QuadraticPlusSinProblemND()\n    (query_points, _, train_points, train_values) = tp.get_problem(dtype=""float64"")\n\n    feature_dim = query_points.shape[-1]\n    value_dim = train_values.shape[-1]\n\n    order = 1\n    reg_weight = 0.01\n\n    # Get concrete functions such that the batch size, number of train points,\n    # and number of query points are not known at graph construction time.\n    fn = tf.function(interpolate_spline).get_concrete_function(\n        tf.TensorSpec(shape=[None, None, feature_dim], dtype=train_points.dtype),\n        tf.TensorSpec(shape=[None, None, value_dim], dtype=train_values.dtype),\n        tf.TensorSpec(shape=[None, None, feature_dim], dtype=query_points.dtype),\n        order,\n        reg_weight,\n    )\n\n    target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]\n    target_interpolation = np.array(target_interpolation)\n\n    interp_val = fn(train_points, train_values, query_points)\n\n    np.testing.assert_allclose(interp_val[0, :, 0], target_interpolation)\n\n\ndef test_fully_unspecified_shape():\n    """"""Ensure that erreor is thrown when input/output dim unspecified.""""""\n    tp = _QuadraticPlusSinProblemND()\n    (query_points, _, train_points, train_values) = tp.get_problem(dtype=""float64"")\n\n    feature_dim = query_points.shape[-1]\n    value_dim = train_values.shape[-1]\n\n    order = 1\n    reg_weight = 0.01\n\n    # Get concrete functions such that the batch size, number of train points,\n    # and number of query points are not known at graph construction time.\n    with pytest.raises(ValueError):\n        tf.function(interpolate_spline).get_concrete_function(\n            tf.TensorSpec(shape=[None, None, None], dtype=train_points.dtype),\n            tf.TensorSpec(shape=[None, None, value_dim], dtype=train_values.dtype),\n            tf.TensorSpec(shape=[None, None, feature_dim], dtype=query_points.dtype),\n            order,\n            reg_weight,\n        )\n\n    with pytest.raises(ValueError):\n        tf.function(interpolate_spline).get_concrete_function(\n            tf.TensorSpec(shape=[None, None, feature_dim], dtype=train_points.dtype),\n            tf.TensorSpec(shape=[None, None, None], dtype=train_values.dtype),\n            tf.TensorSpec(shape=[None, None, feature_dim], dtype=query_points.dtype),\n            order,\n            reg_weight,\n        )\n\n\ndef test_interpolation_gradient():\n    """"""Correctness of gradients is assumed. We compute them\n    and check they exist.\n    """"""\n    tp = _QuadraticPlusSinProblemND()\n    (query_points, _, train_points, train_values) = tp.get_problem(optimizable=True)\n\n    regularization = 0.001\n    for interpolation_order in (1, 2, 3, 4):\n\n        with tf.GradientTape() as g:\n            interpolator = interpolate_spline(\n                train_points,\n                train_values,\n                query_points,\n                interpolation_order,\n                regularization,\n            )\n\n        gradients = g.gradient(interpolator, train_points).numpy()\n        assert np.sum(np.abs(gradients)) != 0\n'"
tensorflow_addons/image/tests/resampler_ops_test.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for resampler.""""""\n\nimport numpy as np\nimport pytest\n\nimport tensorflow as tf\nfrom tensorflow_addons.image import resampler_ops\nfrom tensorflow_addons.utils import test_utils\n\n\ndef _bilinearly_interpolate(data, x, y):\n    """"""Performs bilinenar interpolation of grid data at user defined\n    coordinates.\n\n    This interpolation function:\n      a) implicitly pads the input data with 0s.\n      b) returns 0 when sampling outside the (padded) image.\n    The effect is that the sampled signal smoothly goes to 0 outside the\n    original input domain, rather than producing a jump discontinuity at\n    the image boundaries.\n    Args:\n      data: numpy array of shape `[data_height, data_width]` containing data\n        samples assumed to be defined at the corresponding pixel coordinates.\n      x: numpy array of shape `[warp_height, warp_width]` containing\n        x coordinates at which interpolation will be performed.\n      y: numpy array of shape `[warp_height, warp_width]` containing\n        y coordinates at which interpolation will be performed.\n    Returns:\n      Numpy array of shape `[warp_height, warp_width]` containing interpolated\n        values.\n    """"""\n    shape = x.shape\n    x = np.asarray(x) + 1\n    y = np.asarray(y) + 1\n    data = np.pad(data, 1, ""constant"", constant_values=0)\n\n    x_0 = np.floor(x).astype(int)\n    x_1 = x_0 + 1\n    y_0 = np.floor(y).astype(int)\n    y_1 = y_0 + 1\n\n    x_0 = np.clip(x_0, 0, data.shape[1] - 1)\n    x_1 = np.clip(x_1, 0, data.shape[1] - 1)\n    y_0 = np.clip(y_0, 0, data.shape[0] - 1)\n    y_1 = np.clip(y_1, 0, data.shape[0] - 1)\n\n    i_a = data[y_0, x_0]\n    i_b = data[y_1, x_0]\n    i_c = data[y_0, x_1]\n    i_d = data[y_1, x_1]\n\n    w_a = (x_1 - x) * (y_1 - y)\n    w_b = (x_1 - x) * (y - y_0)\n    w_c = (x - x_0) * (y_1 - y)\n    w_d = (x - x_0) * (y - y_0)\n\n    samples = w_a * i_a + w_b * i_b + w_c * i_c + w_d * i_d\n    samples = samples.reshape(shape)\n\n    return samples\n\n\ndef _make_warp(batch_size, warp_height, warp_width, dtype):\n    """"""Creates batch of warping coordinates.""""""\n    x, y = np.meshgrid(\n        np.linspace(0, warp_width - 1, warp_width),\n        np.linspace(0, warp_height - 1, warp_height),\n    )\n    warp = np.concatenate(\n        (\n            x.reshape([warp_height, warp_width, 1]),\n            y.reshape([warp_height, warp_width, 1]),\n        ),\n        2,\n    )\n    warp = np.tile(warp.reshape([1, warp_height, warp_width, 2]), [batch_size, 1, 1, 1])\n    warp += np.random.randn(*warp.shape)\n    return warp.astype(dtype)\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_op_forward_pass(dtype):\n    np.random.seed(0)\n    data_width = 7\n    data_height = 9\n    data_channels = 5\n    warp_width = 4\n    warp_height = 8\n    batch_size = 10\n\n    warp = _make_warp(batch_size, warp_height, warp_width, dtype)\n    data_shape = (batch_size, data_height, data_width, data_channels)\n    data = np.random.rand(*data_shape).astype(dtype)\n    data_ph = tf.constant(data)\n    warp_ph = tf.constant(warp)\n    outputs = resampler_ops.resampler(data=data_ph, warp=warp_ph)\n    assert outputs.shape == (10, warp_height, warp_width, data_channels)\n\n    # Generate reference output via bilinear interpolation in numpy\n    reference_output = np.zeros_like(outputs)\n    for batch in range(batch_size):\n        for c in range(data_channels):\n            reference_output[batch, :, :, c] = _bilinearly_interpolate(\n                data[batch, :, :, c], warp[batch, :, :, 0], warp[batch, :, :, 1]\n            )\n\n    test_utils.assert_allclose_according_to_type(\n        outputs, reference_output, half_rtol=5e-3, half_atol=5e-3\n    )\n\n\ndef test_op_errors():\n    batch_size = 10\n    data_height = 9\n    data_width = 7\n    data_depth = 3\n    data_channels = 5\n    warp_width = 4\n    warp_height = 8\n\n    # Input data shape is not defined over a 2D grid, i.e. its shape is not like\n    # (batch_size, data_height, data_width, data_channels).\n    data_shape = (batch_size, data_height, data_width, data_depth, data_channels)\n    data = np.zeros(data_shape)\n    warp_shape = (batch_size, warp_height, warp_width, 2)\n    warp = np.zeros(warp_shape)\n\n    with pytest.raises(\n        tf.errors.UnimplementedError,\n        match=""Only bilinear interpolation is currently supported."",\n    ):\n        resampler_ops.resampler(data, warp)\n\n    # Warp tensor must be at least a matrix, with shape [batch_size, 2].\n    data_shape = (batch_size, data_height, data_width, data_channels)\n    data = np.zeros(data_shape)\n    warp_shape = (batch_size,)\n    warp = np.zeros(warp_shape)\n\n    with pytest.raises(\n        tf.errors.InvalidArgumentError, match=""warp should be at least a matrix""\n    ):\n        resampler_ops.resampler(data, warp)\n\n    # The batch size of the data and warp tensors must be the same.\n    data_shape = (batch_size, data_height, data_width, data_channels)\n    data = np.zeros(data_shape)\n    warp_shape = (batch_size + 1, warp_height, warp_width, 2)\n    warp = np.zeros(warp_shape)\n\n    with pytest.raises(\n        tf.errors.InvalidArgumentError, match=""Batch size of data and warp tensor""\n    ):\n        resampler_ops.resampler(data, warp)\n\n    # The warp tensor must contain 2D coordinates, i.e. its shape last dimension\n    # must be 2.\n    data_shape = (batch_size, data_height, data_width, data_channels)\n    data = np.zeros(data_shape)\n    warp_shape = (batch_size, warp_height, warp_width, 3)\n    warp = np.zeros(warp_shape)\n\n    with pytest.raises(\n        tf.errors.UnimplementedError,\n        match=""Only bilinear interpolation is supported, warping"",\n    ):\n        resampler_ops.resampler(data, warp)\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_op_backward_pass(dtype):\n    np.random.seed(13)\n    data_width = 5\n    data_height = 4\n    data_channels = 3\n    warp_width = 2\n    warp_height = 6\n    batch_size = 3\n\n    warp = _make_warp(batch_size, warp_height, warp_width, dtype)\n    data_shape = (batch_size, data_height, data_width, data_channels)\n    data = np.random.rand(*data_shape).astype(dtype)\n    data_tensor = tf.constant(data)\n    warp_tensor = tf.constant(warp)\n    theoretical, _ = tf.test.compute_gradient(\n        resampler_ops.resampler, [data_tensor, warp_tensor]\n    )\n    data_tensor_64 = tf.constant(data, dtype=tf.float64)\n    warp_tensor_64 = tf.constant(warp, dtype=tf.float64)\n    _, numerical_64 = tf.test.compute_gradient(\n        resampler_ops.resampler, [data_tensor_64, warp_tensor_64]\n    )\n\n    for t, n in zip(theoretical, numerical_64):\n        test_utils.assert_allclose_according_to_type(\n            t, n, float_rtol=5e-5, float_atol=5e-5\n        )\n'"
tensorflow_addons/image/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/image/tests/sparse_image_warp_test.py,17,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for sparse_image_warp.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.image import sparse_image_warp\nfrom tensorflow_addons.image.sparse_image_warp import _get_boundary_locations\nfrom tensorflow_addons.image.sparse_image_warp import _get_grid_locations\nfrom tensorflow_addons.utils.resource_loader import get_path_to_datafile\n\n\ndef test_zero_shift():\n    """"""Run assert_zero_shift for various hyperparameters.""""""\n    for order in (1, 2):\n        for regularization in (0, 0.01):\n            for num_boundary_points in (0, 1):\n                assert_zero_shift(order, regularization, num_boundary_points)\n\n\ndef assert_zero_shift(order, regularization, num_boundary_points):\n    """"""Check that warping with zero displacements doesn\'t change the\n    image.""""""\n    batch_size = 1\n    image_height = 4\n    image_width = 4\n    channels = 3\n\n    image = np.random.uniform(size=[batch_size, image_height, image_width, channels])\n\n    input_image = tf.constant(np.float32(image))\n\n    control_point_locations = [[1.0, 1.0], [2.0, 2.0], [2.0, 1.0]]\n    control_point_locations = tf.constant(\n        np.float32(np.expand_dims(control_point_locations, 0))\n    )\n\n    control_point_displacements = np.zeros(control_point_locations.shape.as_list())\n    control_point_displacements = tf.constant(np.float32(control_point_displacements))\n\n    (warped_image, _) = sparse_image_warp(\n        input_image,\n        control_point_locations,\n        control_point_locations + control_point_displacements,\n        interpolation_order=order,\n        regularization_weight=regularization,\n        num_boundary_points=num_boundary_points,\n    )\n\n    np.testing.assert_allclose(warped_image, input_image, rtol=1e-6, atol=1e-6)\n\n\ndef test_get_boundary_locations():\n    image_height = 11\n    image_width = 11\n    num_points_per_edge = 4\n    locs = _get_boundary_locations(image_height, image_width, num_points_per_edge)\n    num_points = locs.shape[0]\n    assert num_points == (4 + 4 * num_points_per_edge)\n    locs = [(locs[i, 0], locs[i, 1]) for i in range(num_points)]\n    for i in (0, image_height - 1):\n        for j in (0, image_width - 1):\n            assert (i, j) in locs\n\n        for i in (2, 4, 6, 8):\n            for j in (0, image_width - 1):\n                assert (i, j) in locs\n\n        for i in (0, image_height - 1):\n            for j in (2, 4, 6, 8):\n                assert (i, j) in locs\n\n\ndef test_get_grid_locations():\n    image_height = 5\n    image_width = 3\n    grid = _get_grid_locations(image_height, image_width)\n    for i in range(image_height):\n        for j in range(image_width):\n            assert grid[i, j, 0] == i\n            assert grid[i, j, 1] == j\n\n\ndef test_move_single_pixel():\n    """"""Run assert_move_single_pixel for various hyperparameters and data\n    types.""""""\n    for order in (1, 2):\n        for num_boundary_points in (1, 2):\n            for type_to_use in (tf.dtypes.float32, tf.dtypes.float64):\n                assert_move_single_pixel(order, num_boundary_points, type_to_use)\n\n\ndef assert_move_single_pixel(order, num_boundary_points, type_to_use):\n    """"""Move a single block in a small grid using warping.""""""\n    batch_size = 1\n    image_height = 7\n    image_width = 7\n    channels = 3\n\n    image = np.zeros([batch_size, image_height, image_width, channels])\n    image[:, 3, 3, :] = 1.0\n    input_image = tf.constant(image, dtype=type_to_use)\n\n    # Place a control point at the one white pixel.\n    control_point_locations = [[3.0, 3.0]]\n    control_point_locations = tf.constant(\n        np.float32(np.expand_dims(control_point_locations, 0)), dtype=type_to_use\n    )\n    # Shift it one pixel to the right.\n    control_point_displacements = [[0.0, 1.0]]\n    control_point_displacements = tf.constant(\n        np.float32(np.expand_dims(control_point_displacements, 0)), dtype=type_to_use,\n    )\n\n    (warped_image, flow) = sparse_image_warp(\n        input_image,\n        control_point_locations,\n        control_point_locations + control_point_displacements,\n        interpolation_order=order,\n        num_boundary_points=num_boundary_points,\n    )\n\n    # Check that it moved the pixel correctly.\n    np.testing.assert_allclose(\n        warped_image[0, 4, 5, :], input_image[0, 4, 4, :], atol=1e-5, rtol=1e-5\n    )\n\n    # Test that there is no flow at the corners.\n    for i in (0, image_height - 1):\n        for j in (0, image_width - 1):\n            np.testing.assert_allclose(\n                flow[0, i, j, :], np.zeros([2]), atol=1e-5, rtol=1e-5\n            )\n\n\ndef load_image(image_file):\n    image = tf.image.decode_png(\n        tf.io.read_file(image_file), dtype=tf.dtypes.uint8, channels=4\n    )[:, :, 0:3]\n    return image\n\n\ndef test_smiley_face():\n    """"""Check warping accuracy by comparing to hardcoded warped images.""""""\n\n    input_file = get_path_to_datafile(""image/tests/test_data/Yellow_Smiley_Face.png"")\n    input_image = load_image(input_file)\n    control_points = np.asarray(\n        [\n            [64, 59],\n            [180 - 64, 59],\n            [39, 111],\n            [180 - 39, 111],\n            [90, 143],\n            [58, 134],\n            [180 - 58, 134],\n        ]\n    )  # pyformat: disable\n    control_point_displacements = np.asarray(\n        [\n            [-10.5, 10.5],\n            [10.5, 10.5],\n            [0, 0],\n            [0, 0],\n            [0, -10],\n            [-20, 10.25],\n            [10, 10.75],\n        ]\n    )\n    control_points = tf.constant(\n        np.expand_dims(np.float32(control_points[:, [1, 0]]), 0)\n    )\n    control_point_displacements = tf.constant(\n        np.expand_dims(np.float32(control_point_displacements[:, [1, 0]]), 0)\n    )\n    float_image = np.expand_dims(np.float32(input_image) / 255, 0)\n    input_image = tf.constant(float_image)\n\n    for interpolation_order in (1, 2, 3):\n        for num_boundary_points in (0, 1, 4):\n            warped_image, _ = sparse_image_warp(\n                input_image,\n                control_points,\n                control_points + control_point_displacements,\n                interpolation_order=interpolation_order,\n                num_boundary_points=num_boundary_points,\n            )\n\n            warped_image = warped_image\n            out_image = np.uint8(warped_image[0, :, :, :] * 255)\n            target_file = get_path_to_datafile(\n                ""image/tests/test_data/Yellow_Smiley_Face_Warp-interp""\n                + ""-{}-clamp-{}.png"".format(interpolation_order, num_boundary_points)\n            )\n\n            target_image = load_image(target_file)\n\n            # Check that the target_image and out_image difference is no\n            # bigger than 2 (on a scale of 0-255). Due to differences in\n            # floating point computation on different devices, the float\n            # output in warped_image may get rounded to a different int\n            # than that in the saved png file loaded into target_image.\n            np.testing.assert_allclose(target_image, out_image, atol=2, rtol=1e-3)\n\n\ndef test_that_backprop_runs():\n    """"""Making sure the gradients can be computed.""""""\n    batch_size = 1\n    image_height = 9\n    image_width = 12\n    image = tf.Variable(\n        np.random.uniform(size=[batch_size, image_height, image_width, 3]),\n        dtype=tf.float32,\n    )\n    control_point_locations = [[3.0, 3.0]]\n    control_point_locations = tf.constant(\n        np.float32(np.expand_dims(control_point_locations, 0))\n    )\n    control_point_displacements = [[0.25, -0.5]]\n    control_point_displacements = tf.constant(\n        np.float32(np.expand_dims(control_point_displacements, 0))\n    )\n\n    with tf.GradientTape() as t:\n        warped_image, _ = sparse_image_warp(\n            image,\n            control_point_locations,\n            control_point_locations + control_point_displacements,\n            num_boundary_points=3,\n        )\n\n    gradients = t.gradient(warped_image, image).numpy()\n    assert np.sum(np.abs(gradients)) != 0\n'"
tensorflow_addons/image/tests/transform_ops_test.py,45,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for transform ops.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.image import transform_ops\nfrom skimage import transform\n\n_DTYPES = {\n    tf.dtypes.uint8,\n    tf.dtypes.int32,\n    tf.dtypes.int64,\n    tf.dtypes.float16,\n    tf.dtypes.float32,\n    tf.dtypes.float64,\n}\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_compose(dtype):\n    image = tf.constant(\n        [[1, 1, 1, 0], [1, 0, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0]], dtype=dtype,\n    )\n    # Rotate counter-clockwise by pi / 2.\n    rotation = transform_ops.angles_to_projective_transforms(np.pi / 2, 4, 4)\n    # Translate right by 1 (the transformation matrix is always inverted,\n    # hence the -1).\n    translation = tf.constant([1, 0, -1, 0, 1, 0, 0, 0], dtype=tf.dtypes.float32)\n    composed = transform_ops.compose_transforms([rotation, translation])\n    image_transformed = transform_ops.transform(image, composed)\n    np.testing.assert_equal(\n        [[0, 0, 0, 0], [0, 1, 0, 1], [0, 1, 0, 1], [0, 1, 1, 1]],\n        image_transformed.numpy(),\n    )\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_extreme_projective_transform(dtype):\n    image = tf.constant(\n        [[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1]], dtype=dtype,\n    )\n    transformation = tf.constant([1, 0, 0, 0, 1, 0, -1, 0], tf.dtypes.float32)\n    image_transformed = transform_ops.transform(image, transformation)\n    np.testing.assert_equal(\n        [[1, 0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0]],\n        image_transformed.numpy(),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_transform_static_output_shape():\n    image = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    result = transform_ops.transform(\n        image, tf.random.uniform([8], -1, 1), output_shape=[3, 5]\n    )\n    np.testing.assert_equal([3, 5], result.shape)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_transform_unknown_shape():\n    fn = tf.function(transform_ops.transform).get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float32), [1, 0, 0, 0, 1, 0, 0, 0]\n    )\n    for shape in (2, 4), (2, 4, 3), (1, 2, 4, 3):\n        image = tf.ones(shape=shape)\n        np.testing.assert_equal(image.numpy(), fn(image).numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef _test_grad(input_shape, output_shape=None):\n    image_size = tf.math.cumprod(input_shape)[-1]\n    image_size = tf.cast(image_size, tf.float32)\n    test_image = tf.reshape(tf.range(0, image_size, dtype=tf.float32), input_shape)\n    # Scale test image to range [0, 0.01]\n    test_image = (test_image / image_size) * 0.01\n\n    def transform_fn(x):\n        x.set_shape(input_shape)\n        transform = transform_ops.angles_to_projective_transforms(np.pi / 2, 4, 4)\n        return transform_ops.transform(\n            images=x, transforms=transform, output_shape=output_shape\n        )\n\n    theoretical, numerical = tf.test.compute_gradient(transform_fn, [test_image])\n\n    np.testing.assert_almost_equal(theoretical[0], numerical[0])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_grad():\n    _test_grad([8, 8])\n    _test_grad([8, 8], [4, 4])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_transform_data_types(dtype):\n    image = tf.constant([[1, 2], [3, 4]], dtype=dtype)\n    np.testing.assert_equal(\n        np.array([[4, 4], [4, 4]]).astype(dtype.as_numpy_dtype),\n        transform_ops.transform(image, [1] * 8),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_transform_eager():\n    image = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    np.testing.assert_equal(\n        np.array([[4, 4], [4, 4]]), transform_ops.transform(image, [1] * 8)\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_zeros(dtype):\n    for shape in [(5, 5), (24, 24), (2, 24, 24, 3)]:\n        for angle in [0, 1, np.pi / 2.0]:\n            image = tf.zeros(shape, dtype)\n            np.testing.assert_equal(\n                transform_ops.rotate(image, angle),\n                np.zeros(shape, dtype.as_numpy_dtype),\n            )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_rotate_even(dtype):\n    image = tf.reshape(tf.cast(tf.range(36), dtype), (6, 6))\n    image_rep = tf.tile(image[None, :, :, None], [3, 1, 1, 1])\n    angles = tf.constant([0.0, np.pi / 4.0, np.pi / 2.0], tf.float32)\n    image_rotated = transform_ops.rotate(image_rep, angles)\n    np.testing.assert_equal(\n        image_rotated.numpy()[:, :, :, 0],\n        [\n            [\n                [0, 1, 2, 3, 4, 5],\n                [6, 7, 8, 9, 10, 11],\n                [12, 13, 14, 15, 16, 17],\n                [18, 19, 20, 21, 22, 23],\n                [24, 25, 26, 27, 28, 29],\n                [30, 31, 32, 33, 34, 35],\n            ],\n            [\n                [0, 3, 4, 11, 17, 0],\n                [2, 3, 9, 16, 23, 23],\n                [1, 8, 15, 21, 22, 29],\n                [6, 13, 20, 21, 27, 34],\n                [12, 18, 19, 26, 33, 33],\n                [0, 18, 24, 31, 32, 0],\n            ],\n            [\n                [5, 11, 17, 23, 29, 35],\n                [4, 10, 16, 22, 28, 34],\n                [3, 9, 15, 21, 27, 33],\n                [2, 8, 14, 20, 26, 32],\n                [1, 7, 13, 19, 25, 31],\n                [0, 6, 12, 18, 24, 30],\n            ],\n        ],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_rotate_odd(dtype):\n    image = tf.reshape(tf.cast(tf.range(25), dtype), (5, 5))\n    image_rep = tf.tile(image[None, :, :, None], [3, 1, 1, 1])\n    angles = tf.constant([np.pi / 4.0, 1.0, -np.pi / 2.0], tf.float32)\n    image_rotated = transform_ops.rotate(image_rep, angles)\n    np.testing.assert_equal(\n        image_rotated.numpy()[:, :, :, 0],\n        [\n            [\n                [0, 3, 8, 9, 0],\n                [1, 7, 8, 13, 19],\n                [6, 6, 12, 18, 18],\n                [5, 11, 16, 17, 23],\n                [0, 15, 16, 21, 0],\n            ],\n            [\n                [0, 3, 9, 14, 0],\n                [2, 7, 8, 13, 19],\n                [1, 6, 12, 18, 23],\n                [5, 11, 16, 17, 22],\n                [0, 10, 15, 21, 0],\n            ],\n            [\n                [20, 15, 10, 5, 0],\n                [21, 16, 11, 6, 1],\n                [22, 17, 12, 7, 2],\n                [23, 18, 13, 8, 3],\n                [24, 19, 14, 9, 4],\n            ],\n        ],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_compose_rotate(dtype):\n    image = tf.constant(\n        [[1, 1, 1, 0], [1, 0, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0]], dtype=dtype\n    )\n    # Rotate counter-clockwise by pi / 2.\n    rotation = transform_ops.angles_to_projective_transforms(np.pi / 2, 4, 4)\n    # Translate right by 1 (the transformation matrix is always inverted,\n    # hence the -1).\n    translation = tf.constant([1, 0, -1, 0, 1, 0, 0, 0], dtype=tf.float32)\n    composed = transform_ops.compose_transforms([rotation, translation])\n    image_transformed = transform_ops.transform(image, composed)\n    np.testing.assert_equal(\n        image_transformed.numpy(),\n        [[0, 0, 0, 0], [0, 1, 0, 1], [0, 1, 0, 1], [0, 1, 1, 1]],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_bilinear():\n    image = tf.constant(\n        [\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 0, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0],\n        ],\n        tf.float32,\n    )\n    # The following result matches:\n    # >>> scipy.ndimage.rotate(image, 45, order=1, reshape=False)\n    # which uses spline interpolation of order 1, equivalent to bilinear\n    # interpolation.\n    transformed = transform_ops.rotate(image, np.pi / 4.0, interpolation=""BILINEAR"")\n    np.testing.assert_allclose(\n        transformed.numpy(),\n        [\n            [0.000, 0.000, 0.343, 0.000, 0.000],\n            [0.000, 0.586, 0.914, 0.586, 0.000],\n            [0.343, 0.914, 0.000, 0.914, 0.343],\n            [0.000, 0.586, 0.914, 0.586, 0.000],\n            [0.000, 0.000, 0.343, 0.000, 0.000],\n        ],\n        atol=0.001,\n    )\n    transformed = transform_ops.rotate(image, np.pi / 4.0, interpolation=""NEAREST"")\n    np.testing.assert_allclose(\n        transformed.numpy(),\n        [\n            [0, 0, 1, 0, 0],\n            [0, 1, 1, 1, 0],\n            [1, 1, 0, 1, 1],\n            [0, 1, 1, 1, 0],\n            [0, 0, 1, 0, 0],\n        ],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_bilinear_uint8():\n    image = tf.constant(\n        np.asarray(\n            [\n                [0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 255, 255, 255, 0.0],\n                [0.0, 255, 0.0, 255, 0.0],\n                [0.0, 255, 255, 255, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0],\n            ],\n            np.uint8,\n        ),\n        tf.uint8,\n    )\n    # == np.rint((expected image above) * 255)\n    transformed = transform_ops.rotate(image, np.pi / 4.0, interpolation=""BILINEAR"")\n    np.testing.assert_equal(\n        transformed.numpy(),\n        [\n            [0.0, 0.0, 87.0, 0.0, 0.0],\n            [0.0, 149, 233, 149, 0.0],\n            [87.0, 233, 0.0, 233, 87.0],\n            [0.0, 149, 233, 149, 0.0],\n            [0.0, 0.0, 87.0, 0.0, 0.0],\n        ],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_rotate_static_shape():\n    image = tf.linalg.diag([1.0, 2.0, 3.0])\n    result = transform_ops.rotate(\n        image, tf.random.uniform((), -1, 1), interpolation=""BILINEAR""\n    )\n    np.testing.assert_equal(image.get_shape(), result.get_shape())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_unknown_shape():\n    fn = tf.function(transform_ops.rotate).get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float32), 0\n    )\n    for shape in (2, 4), (2, 4, 3), (1, 2, 4, 3):\n        image = tf.ones(shape=shape)\n        np.testing.assert_equal(image.numpy(), fn(image).numpy())\n\n\n# TODO: Parameterize on dtypes\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_shear_x():\n    image = np.random.randint(low=0, high=255, size=(4, 4, 3), dtype=np.uint8)\n    color = tf.constant([255, 0, 255], tf.uint8)\n    level = tf.random.uniform(shape=(), minval=0, maxval=1)\n\n    tf_image = tf.constant(image)\n    sheared_img = transform_ops.shear_x(tf_image, level, replace=color)\n    transform_matrix = transform.AffineTransform(\n        np.array([[1, level.numpy(), 0], [0, 1, 0], [0, 0, 1]])\n    )\n    expected_img = transform.warp(\n        image, transform_matrix, order=0, cval=-1, preserve_range=True\n    )\n\n    mask = np.where(expected_img == -1)\n    expected_img[mask[0], mask[1], :] = color\n\n    np.testing.assert_equal(sheared_img.numpy(), expected_img)\n\n\n# TODO: Parameterize on dtypes\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_shear_y():\n    image = np.random.randint(low=0, high=255, size=(4, 4, 3), dtype=np.uint8)\n    color = tf.constant([255, 0, 255], tf.dtypes.uint8)\n    level = tf.random.uniform(shape=(), minval=0, maxval=1)\n\n    tf_image = tf.constant(image)\n    sheared_img = transform_ops.shear_y(image=tf_image, level=level, replace=color)\n    transform_matrix = transform.AffineTransform(\n        np.array([[1, 0, 0], [level.numpy(), 1, 0], [0, 0, 1]])\n    )\n    expected_img = transform.warp(\n        image, transform_matrix, order=0, cval=-1, preserve_range=True\n    )\n\n    mask = np.where(expected_img == -1)\n    expected_img[mask[0], mask[1], :] = color\n\n    np.testing.assert_equal(sheared_img.numpy(), expected_img)\n'"
tensorflow_addons/image/tests/translate_ops_test.py,13,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for translate ops.""""""\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_addons.image import translate_ops\nfrom PIL import Image\n\n_DTYPES = {\n    tf.dtypes.uint8,\n    tf.dtypes.int32,\n    tf.dtypes.int64,\n    tf.dtypes.float16,\n    tf.dtypes.float32,\n    tf.dtypes.float64,\n}\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", _DTYPES)\ndef test_translate(dtype):\n    image = tf.constant(\n        [[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1]], dtype=dtype\n    )\n    translation = tf.constant([-1, -1], dtype=tf.float32)\n    image_translated = translate_ops.translate(image, translation)\n    np.testing.assert_equal(\n        image_translated.numpy(),\n        [[1, 0, 1, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0]],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_translations_to_projective_transforms():\n    translation = tf.constant([-1, -1], dtype=tf.float32)\n    transform = translate_ops.translations_to_projective_transforms(translation)\n    np.testing.assert_equal(transform.numpy(), [[1, 0, 1, 0, 1, 1, 0, 0]])\n\n\n# TODO: Parameterize on dtypes\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_translate_xy():\n    image = np.random.randint(low=0, high=255, size=(4, 4, 3), dtype=np.uint8)\n    translate = np.random.randint(low=0, high=4, size=(2,), dtype=np.uint8)\n    translate = tf.constant(translate)\n    color = tf.constant([255, 0, 255], tf.dtypes.uint8)\n\n    tf_image = tf.constant(image)\n    pil_image = Image.fromarray(image)\n\n    translated = translate_ops.translate_xy(\n        image=tf_image, translate_to=tf.constant(translate), replace=color\n    )\n    expected = pil_image.rotate(\n        angle=0,\n        resample=Image.NEAREST,\n        translate=tuple(translate.numpy()),\n        fillcolor=tuple(color.numpy()),\n    )\n\n    np.testing.assert_equal(translated.numpy(), expected)\n'"
tensorflow_addons/image/tests/utils_test.py,24,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for util ops.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.image import utils as img_utils\n\n\ndef test_to_4D_image_with_unknown_shape():\n    fn = tf.function(img_utils.to_4D_image).get_concrete_function(\n        tf.TensorSpec(shape=None, dtype=tf.float32)\n    )\n    for shape in (2, 4), (2, 4, 1), (1, 2, 4, 1):\n        exp = tf.ones(shape=(1, 2, 4, 1))\n        res = fn(tf.ones(shape=shape))\n        np.testing.assert_equal(exp.numpy(), res.numpy())\n\n\ndef test_to_4D_image_with_invalid_shape():\n    errors = (ValueError, tf.errors.InvalidArgumentError)\n    with pytest.raises(errors, match=""`image` must be 2/3/4D tensor""):\n        img_utils.to_4D_image(tf.ones(shape=(1,)))\n\n    with pytest.raises(errors, match=""`image` must be 2/3/4D tensor""):\n        img_utils.to_4D_image(tf.ones(shape=(1, 2, 4, 3, 2)))\n\n\ndef test_from_4D_image():\n    for shape in (2, 4), (2, 4, 1), (1, 2, 4, 1):\n        exp = tf.ones(shape=shape)\n        res = img_utils.from_4D_image(tf.ones(shape=(1, 2, 4, 1)), len(shape))\n        # static shape:\n        assert exp.get_shape() == res.get_shape()\n        np.testing.assert_equal(exp.numpy(), res.numpy())\n\n\ndef test_to_4D_image():\n    for shape in (2, 4), (2, 4, 1), (1, 2, 4, 1):\n        exp = tf.ones(shape=(1, 2, 4, 1))\n        res = img_utils.to_4D_image(tf.ones(shape=shape))\n        # static shape:\n        assert exp.get_shape() == res.get_shape()\n        np.testing.assert_equal(exp.numpy(), res.numpy())\n\n\ndef test_from_4D_image_with_invalid_data():\n    with np.testing.assert_raises((ValueError, tf.errors.InvalidArgumentError)):\n        img_utils.from_4D_image(tf.ones(shape=(2, 2, 4, 1)), 2)\n\n    with np.testing.assert_raises((ValueError, tf.errors.InvalidArgumentError)):\n        img_utils.from_4D_image(tf.ones(shape=(2, 2, 4, 1)), tf.constant(2))\n\n\ndef test_from_4D_image_with_unknown_shape():\n    for shape in (2, 4), (2, 4, 1), (1, 2, 4, 1):\n        exp = tf.ones(shape=shape)\n        fn = tf.function(img_utils.from_4D_image).get_concrete_function(\n            tf.TensorSpec(shape=None, dtype=tf.float32), tf.size(shape)\n        )\n        res = fn(tf.ones(shape=(1, 2, 4, 1)), tf.size(shape))\n        np.testing.assert_equal(exp.numpy(), res.numpy())\n\n\n@pytest.mark.parametrize(""rank"", [2, tf.constant(2)])\ndef test_from_4d_image_with_invalid_shape(rank):\n    errors = (ValueError, tf.errors.InvalidArgumentError)\n    with pytest.raises(errors, match=""`image` must be 4D tensor""):\n        img_utils.from_4D_image(tf.ones(shape=(2, 4)), rank)\n\n    with pytest.raises(errors, match=""`image` must be 4D tensor""):\n        img_utils.from_4D_image(tf.ones(shape=(2, 4, 1)), rank)\n\n    with pytest.raises(errors, match=""`image` must be 4D tensor""):\n        img_utils.from_4D_image(tf.ones(shape=(1, 2, 4, 1, 1)), rank)\n'"
tensorflow_addons/layers/tests/__init__.py,0,b''
tensorflow_addons/layers/tests/adaptive_pooling_test.py,0,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for AdaptivePooling layers.""""""\n\nimport pytest\nimport numpy as np\nfrom tensorflow_addons.layers.adaptive_pooling import (\n    AdaptiveAveragePooling1D,\n    AdaptiveMaxPooling1D,\n    AdaptiveAveragePooling2D,\n    AdaptiveMaxPooling2D,\n    AdaptiveAveragePooling3D,\n    AdaptiveMaxPooling3D,\n)\n\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_avg_1d():\n    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 12, 1))\n    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)\n    output = np.reshape(output, (1, 4, 1))\n    test_utils.layer_test(\n        AdaptiveAveragePooling1D,\n        kwargs={""output_size"": 4, ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 12))\n    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)\n    output = np.reshape(output, (1, 1, 4))\n    test_utils.layer_test(\n        AdaptiveAveragePooling1D,\n        kwargs={""output_size"": 4, ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_avg_2d():\n    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 4, 10, 1))\n    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)\n    output = np.reshape(output, (1, 2, 2, 1))\n    test_utils.layer_test(\n        AdaptiveAveragePooling2D,\n        kwargs={""output_size"": (2, 2), ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 4, 10))\n    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)\n    output = np.reshape(output, (1, 1, 2, 2))\n    test_utils.layer_test(\n        AdaptiveAveragePooling2D,\n        kwargs={""output_size"": (2, 2), ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_avg_3d():\n    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))\n    output = np.array(\n        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]\n    ).astype(np.float32)\n    output = np.reshape(output, (1, 2, 2, 2, 1))\n    test_utils.layer_test(\n        AdaptiveAveragePooling3D,\n        kwargs={""output_size"": (2, 2, 2), ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))\n    output = np.array(\n        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]\n    ).astype(np.float32)\n    output = np.reshape(output, (1, 1, 2, 2, 2))\n    test_utils.layer_test(\n        AdaptiveAveragePooling3D,\n        kwargs={""output_size"": (2, 2, 2), ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_max_1d():\n    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 12, 1))\n    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)\n    output = np.reshape(output, (1, 4, 1))\n    test_utils.layer_test(\n        AdaptiveMaxPooling1D,\n        kwargs={""output_size"": 4, ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 12))\n    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)\n    output = np.reshape(output, (1, 1, 4))\n    test_utils.layer_test(\n        AdaptiveMaxPooling1D,\n        kwargs={""output_size"": 4, ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_max_2d():\n    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 4, 10, 1))\n    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)\n    output = np.reshape(output, (1, 2, 2, 1))\n    test_utils.layer_test(\n        AdaptiveMaxPooling2D,\n        kwargs={""output_size"": (2, 2), ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 4, 10))\n    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)\n    output = np.reshape(output, (1, 1, 2, 2))\n    test_utils.layer_test(\n        AdaptiveMaxPooling2D,\n        kwargs={""output_size"": (2, 2), ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_max_3d():\n    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))\n    output = np.array(\n        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]\n    ).astype(np.float32)\n    output = np.reshape(output, (1, 2, 2, 2, 1))\n    test_utils.layer_test(\n        AdaptiveMaxPooling3D,\n        kwargs={""output_size"": (2, 2, 2), ""data_format"": ""channels_last""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n\n    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)\n    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))\n    output = np.array(\n        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]\n    ).astype(np.float32)\n    output = np.reshape(output, (1, 1, 2, 2, 2))\n    test_utils.layer_test(\n        AdaptiveMaxPooling3D,\n        kwargs={""output_size"": (2, 2, 2), ""data_format"": ""channels_first""},\n        input_data=valid_input,\n        expected_output=output,\n    )\n'"
tensorflow_addons/layers/tests/esn_test.py,3,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Echo State recurrent Network (ESN).""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.layers.esn import ESN\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef layer_test_esn(dtype):\n    inp = np.asanyarray(\n        [[[1.0, 1.0, 1.0, 1.0]], [[2.0, 2.0, 2.0, 2.0]], [[3.0, 3.0, 3.0, 3.0]]]\n    ).astype(dtype)\n    out = np.asarray([[2.5, 2.5, 2.5], [4.5, 4.5, 4.5], [6.5, 6.5, 6.5]]).astype(dtype)\n\n    const_initializer = tf.constant_initializer(0.5)\n    kwargs = {\n        ""units"": 3,\n        ""connectivity"": 1,\n        ""leaky"": 1,\n        ""spectral_radius"": 0.9,\n        ""use_norm2"": True,\n        ""use_bias"": True,\n        ""activation"": None,\n        ""kernel_initializer"": const_initializer,\n        ""recurrent_initializer"": const_initializer,\n        ""bias_initializer"": const_initializer,\n        ""dtype"": dtype,\n    }\n\n    test_utils.layer_test(ESN, kwargs=kwargs, input_data=inp, expected_output=out)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_serialization(dtype):\n    esn = ESN(\n        units=3,\n        connectivity=1,\n        leaky=1,\n        spectral_radius=0.9,\n        use_norm2=False,\n        use_bias=True,\n        activation=None,\n        kernel_initializer=""ones"",\n        recurrent_initializer=""ones"",\n        bias_initializer=""ones"",\n    )\n    serialized_esn = tf.keras.layers.serialize(esn)\n    new_layer = tf.keras.layers.deserialize(serialized_esn)\n    assert esn.get_config() == new_layer.get_config()\n'"
tensorflow_addons/layers/tests/gelu_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for GELU activation.""""""\n\n\nimport pytest\nimport numpy as np\nfrom tensorflow_addons.layers.gelu import GELU\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_random(dtype):\n    x = np.array([[0.5, 1.2, -0.3]]).astype(dtype)\n    val = np.array([[0.345714, 1.0617027, -0.11462909]]).astype(dtype)\n    test_utils.layer_test(\n        GELU, kwargs={""dtype"": dtype}, input_data=x, expected_output=val\n    )\n'"
tensorflow_addons/layers/tests/maxout_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Maxout layer.""""""\n\n\nimport pytest\nimport numpy as np\n\nfrom tensorflow_addons.layers.maxout import Maxout\nfrom tensorflow_addons.utils import test_utils\n\n\npytestmark = pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n\n\ndef test_simple():\n    test_utils.layer_test(Maxout, kwargs={""num_units"": 3}, input_shape=(5, 4, 2, 18))\n\n\ndef test_nchw():\n    test_utils.layer_test(\n        Maxout, kwargs={""num_units"": 4, ""axis"": 1}, input_shape=(2, 20, 3, 6)\n    )\n\n    test_utils.layer_test(\n        Maxout, kwargs={""num_units"": 4, ""axis"": -3}, input_shape=(2, 20, 3, 6)\n    )\n\n\ndef test_unknown():\n    inputs = np.random.random((5, 4, 2, 18)).astype(""float32"")\n    test_utils.layer_test(\n        Maxout, kwargs={""num_units"": 3}, input_shape=(5, 4, 2, None), input_data=inputs,\n    )\n\n    test_utils.layer_test(\n        Maxout,\n        kwargs={""num_units"": 3},\n        input_shape=(None, None, None, None),\n        input_data=inputs,\n    )\n\n\ndef test_invalid_shape():\n    with pytest.raises(ValueError, match=""number of features""):\n        test_utils.layer_test(Maxout, kwargs={""num_units"": 3}, input_shape=(5, 4, 2, 7))\n'"
tensorflow_addons/layers/tests/multihead_attention_test.py,40,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.layers.multihead_attention import MultiHeadAttention\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_output_size():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n    output_size = 20\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, output_size=output_size\n    )\n\n    output = mha([q, k, v])\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == output_size\n\n\ndef test_output_shape():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads)\n\n    output = mha([q, k, v])\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == v.shape[2]\n\n\ndef test_no_batch():\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((5, 9), dtype=np.float32)\n    k = tf.random.uniform((7, 11), dtype=np.float32)\n    v = tf.random.uniform((7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads)\n\n    output = mha([q, k, v])\n\n    assert output.shape[0] == q.shape[0]\n    assert output.shape[1] == v.shape[1]\n\n\ndef test_extra_dims():\n    batch_size = 10\n    extra_dim = 17\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, extra_dim, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, extra_dim, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, extra_dim, 7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads)\n\n    output = mha([q, k, v])\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == extra_dim\n    assert output.shape[2] == q.shape[2]\n    assert output.shape[3] == v.shape[3]\n\n\ndef test_extra_dims_atten_coef():\n    batch_size = 10\n    extra_dim = 17\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, extra_dim, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, extra_dim, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, extra_dim, 7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output, attn_coef = mha([q, k, v])\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == extra_dim\n    assert output.shape[2] == q.shape[2]\n    assert output.shape[3] == v.shape[3]\n\n    assert attn_coef.shape[0] == batch_size\n    assert attn_coef.shape[1] == extra_dim\n    assert attn_coef.shape[2] == num_heads\n    assert attn_coef.shape[3] == q.shape[2]\n    assert attn_coef.shape[4] == k.shape[2]\n\n\ndef test_attention_coefficients_shape():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output, attn_coef = mha([q, k, v])\n\n    assert attn_coef.shape[0] == batch_size\n    assert attn_coef.shape[1] == num_heads\n    assert attn_coef.shape[2] == q.shape[1]\n    assert attn_coef.shape[3] == k.shape[1]\n\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == v.shape[2]\n\n\ndef test_compute_output_shape():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads)\n\n    output_shape = mha.compute_output_shape(\n        [(batch_size, 5, 9), (batch_size, 7, 11), (batch_size, 7, 13)]\n    )\n\n    assert output_shape[1] == 5\n    assert output_shape[2] == 13\n\n\ndef test_compute_output_shape_return_attn():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output_shape, attn_coef_shape = mha.compute_output_shape(\n        [(batch_size, 5, 9), (batch_size, 7, 11), (batch_size, 7, 13)]\n    )\n\n    assert output_shape[1] == 5\n    assert output_shape[2] == 13\n\n    assert attn_coef_shape[0] == batch_size\n    assert attn_coef_shape[1] == num_heads\n    assert attn_coef_shape[2] == 5\n    assert attn_coef_shape[3] == 7\n\n\ndef test_no_value():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output, attn_coef = mha([q, k])\n\n    assert attn_coef.shape[0] == batch_size\n    assert attn_coef.shape[1] == num_heads\n    assert attn_coef.shape[2] == q.shape[1]\n    assert attn_coef.shape[3] == k.shape[1]\n\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == k.shape[2]\n\n\ndef test_mask_no_batch():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)\n    mask = tf.random.uniform((5, 7), dtype=np.float32) > 0.1\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output, attn_coef = mha([q, k, v], mask=mask)\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == v.shape[2]\n\n    if tf.executing_eagerly():\n        attn_coef = attn_coef.numpy()\n        mask = mask.numpy()\n\n        assert ((attn_coef != 0) == mask).all()\n\n\ndef test_from_to_config():\n    num_heads = 8\n    head_size = 12\n\n    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads, dropout=0.5)\n\n    config = mha.get_config()\n\n    new_mha = MultiHeadAttention.from_config(config)\n\n    assert mha.head_size == new_mha.head_size\n    assert mha.num_heads == new_mha.num_heads\n    assert mha._droput_rate == new_mha._droput_rate\n\n\ndef test_save_load_model():\n\n    num_heads = 8\n    head_size = 12\n\n    inputs = tf.keras.layers.Input(shape=[42, 13])\n\n    net, attn_coef = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, dropout=0.5, return_attn_coef=True\n    )([inputs, inputs, inputs])\n    net = tf.keras.layers.GlobalAveragePooling1D()(net)\n    net = tf.keras.layers.Dense(10, activation=""softmax"")(net)\n\n    model = tf.keras.Model(inputs=inputs, outputs=[net, attn_coef])\n\n    # initialize model\n    model.predict(np.random.uniform(size=(10, 42, 13)))\n\n    with tempfile.TemporaryDirectory() as model_dir:\n        model_path = str(Path(model_dir) / ""saved_model"")\n        model.save(model_path)\n        new_model = tf.keras.models.load_model(model_path)\n\n    assert model.layers[1].get_config() == new_model.layers[1].get_config()\n\n\ndef test_fit_predict_eval():\n\n    num_heads = 8\n    head_size = 12\n\n    inputs = tf.keras.layers.Input(shape=[42, 13])\n\n    net = MultiHeadAttention(head_size=head_size, num_heads=num_heads, dropout=0.5)(\n        [inputs, inputs, inputs]\n    )\n    net = tf.keras.layers.GlobalAveragePooling1D()(net)\n    net = tf.keras.layers.Dense(10, activation=""softmax"")(net)\n\n    model = tf.keras.Model(inputs=inputs, outputs=net)\n\n    model.compile(\n        loss=tf.losses.SparseCategoricalCrossentropy(),\n        optimizer=tf.keras.optimizers.Adam(0.001),\n    )\n\n    model.fit(\n        x=np.random.uniform(size=(50, 42, 13)),\n        y=np.random.randint(10, size=(50,)),\n        batch_size=10,\n        epochs=2,\n    )\n\n    model.predict(np.random.uniform(size=(10, 42, 13)))\n\n    model.evaluate(\n        x=np.random.uniform(size=(20, 42, 13)),\n        y=np.random.randint(0, 10, size=(20,)),\n        batch_size=10,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_mask():\n    batch_size = 10\n    num_heads = 8\n    head_size = 12\n\n    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)\n    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)\n    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)\n    mask = tf.random.uniform((batch_size, num_heads, 5, 7), dtype=np.float32) > 0.1\n\n    mha = MultiHeadAttention(\n        head_size=head_size, num_heads=num_heads, return_attn_coef=True\n    )\n\n    output, attn_coef = mha([q, k, v], mask=mask)\n\n    assert attn_coef.shape[0] == batch_size\n    assert attn_coef.shape[1] == num_heads\n    assert attn_coef.shape[2] == q.shape[1]\n    assert attn_coef.shape[3] == k.shape[1]\n\n    assert output.shape[0] == batch_size\n    assert output.shape[1] == q.shape[1]\n    assert output.shape[2] == v.shape[2]\n\n    np.testing.assert_array_equal((attn_coef != 0), mask)\n'"
tensorflow_addons/layers/tests/netvlad_test.py,0,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for NetVLAD layer.""""""\n\n\nimport pytest\nimport numpy as np\nfrom tensorflow_addons.layers.netvlad import NetVLAD\nfrom tensorflow_addons.utils import test_utils\n\n\npytestmark = pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n\n\n@pytest.mark.parametrize(""num_clusters"", [1, 4])\ndef test_simple(num_clusters):\n    test_utils.layer_test(\n        NetVLAD,\n        kwargs={""num_clusters"": num_clusters},\n        input_shape=(5, 4, 100),\n        expected_output_shape=(None, num_clusters * 100),\n    )\n\n\ndef test_unknown():\n    inputs = np.random.random((5, 4, 100)).astype(""float32"")\n    test_utils.layer_test(\n        NetVLAD,\n        kwargs={""num_clusters"": 3},\n        input_shape=(None, None, 100),\n        input_data=inputs,\n        expected_output_shape=(None, 3 * 100),\n    )\n\n\ndef test_invalid_shape():\n    with pytest.raises(ValueError) as exception_info:\n        test_utils.layer_test(\n            NetVLAD, kwargs={""num_clusters"": 0}, input_shape=(5, 4, 20)\n        )\n    assert ""`num_clusters` must be greater than 1"" in str(exception_info.value)\n\n    with pytest.raises(ValueError) as exception_info:\n        test_utils.layer_test(\n            NetVLAD, kwargs={""num_clusters"": 2}, input_shape=(5, 4, 4, 20)\n        )\n    assert ""must have rank 3"" in str(exception_info.value)\n'"
tensorflow_addons/layers/tests/normalizations_test.py,35,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.layers.normalizations import FilterResponseNormalization\nfrom tensorflow_addons.layers.normalizations import GroupNormalization\nfrom tensorflow_addons.layers.normalizations import InstanceNormalization\n\n\n# ------------Tests to ensure proper inheritance. If these suceed you can\n# test for Instance norm by setting Groupnorm groups = -1\ndef test_inheritance():\n    assert issubclass(InstanceNormalization, GroupNormalization)\n    assert InstanceNormalization.build == GroupNormalization.build\n    assert InstanceNormalization.call == GroupNormalization.call\n\n\ndef test_groups_after_init():\n    layers = InstanceNormalization()\n    assert layers.groups == -1\n\n\ndef test_weights():\n    # Check if weights get initialized correctly\n    layer = GroupNormalization(groups=1, scale=False, center=False)\n    layer.build((None, 3, 4))\n    assert len(layer.trainable_weights) == 0\n    assert len(layer.weights) == 0\n\n    layer = InstanceNormalization()\n    layer.build((None, 3, 4))\n    assert len(layer.trainable_weights) == 2\n    assert len(layer.weights) == 2\n\n\ndef test_apply_normalization():\n    input_shape = (1, 4)\n    reshaped_inputs = tf.constant([[[2.0, 2.0], [3.0, 3.0]]])\n    layer = GroupNormalization(groups=2, axis=1, scale=False, center=False)\n    normalized_input = layer._apply_normalization(reshaped_inputs, input_shape)\n    np.testing.assert_equal(normalized_input, np.array([[[0.0, 0.0], [0.0, 0.0]]]))\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_reshape():\n    def run_reshape_test(axis, group, input_shape, expected_shape):\n        group_layer = GroupNormalization(groups=group, axis=axis)\n        group_layer._set_number_of_groups_for_instance_norm(input_shape)\n\n        inputs = np.ones(input_shape)\n        tensor_input_shape = tf.convert_to_tensor(input_shape)\n        reshaped_inputs, group_shape = group_layer._reshape_into_groups(\n            inputs, (10, 10, 10), tensor_input_shape\n        )\n        for i in range(len(expected_shape)):\n            assert group_shape[i] == expected_shape[i]\n\n    input_shape = (10, 10, 10)\n    expected_shape = [10, 10, 5, 2]\n    run_reshape_test(2, 5, input_shape, expected_shape)\n\n    input_shape = (10, 10, 10)\n    expected_shape = [10, 2, 5, 10]\n    run_reshape_test(1, 2, input_shape, expected_shape)\n\n    input_shape = (10, 10, 10)\n    expected_shape = [10, 10, 1, 10]\n    run_reshape_test(1, -1, input_shape, expected_shape)\n\n    input_shape = (10, 10, 10)\n    expected_shape = [10, 1, 10, 10]\n    run_reshape_test(1, 1, input_shape, expected_shape)\n\n\n@pytest.mark.parametrize(""center"", [True, False])\n@pytest.mark.parametrize(""scale"", [True, False])\ndef test_feature_input(center, scale):\n    shape = (10, 100)\n    for groups in [-1, 1, 2, 5]:\n        _test_random_shape_on_all_axis_except_batch(shape, groups, center, scale)\n\n\n@pytest.mark.parametrize(""center"", [True, False])\n@pytest.mark.parametrize(""scale"", [True, False])\ndef test_picture_input(center, scale):\n    shape = (10, 30, 30, 3)\n    for groups in [-1, 1, 3]:\n        _test_random_shape_on_all_axis_except_batch(shape, groups, center, scale)\n\n\ndef _test_random_shape_on_all_axis_except_batch(shape, groups, center, scale):\n    inputs = tf.random.normal(shape)\n    for axis in range(1, len(shape)):\n        _test_specific_layer(inputs, axis, groups, center, scale)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef _test_specific_layer(inputs, axis, groups, center, scale):\n\n    input_shape = inputs.shape\n\n    # Get Output from Keras model\n    layer = GroupNormalization(axis=axis, groups=groups, center=center, scale=scale)\n    model = tf.keras.models.Sequential()\n    model.add(layer)\n    outputs = model.predict(inputs, steps=1)\n    assert not np.isnan(outputs).any()\n\n    # Create shapes\n    if groups == -1:\n        groups = input_shape[axis]\n    np_inputs = inputs\n    reshaped_dims = list(np_inputs.shape)\n    reshaped_dims[axis] = reshaped_dims[axis] // groups\n    reshaped_dims.insert(axis, groups)\n    reshaped_inputs = np.reshape(np_inputs, tuple(reshaped_dims))\n\n    group_reduction_axes = list(range(1, len(reshaped_dims)))\n    axis = -2 if axis == -1 else axis - 1\n    group_reduction_axes.pop(axis)\n\n    # Calculate mean and variance\n    mean = np.mean(reshaped_inputs, axis=tuple(group_reduction_axes), keepdims=True)\n    variance = np.var(reshaped_inputs, axis=tuple(group_reduction_axes), keepdims=True)\n\n    # Get gamma and beta initalized by layer\n    gamma, beta = layer._get_reshaped_weights(input_shape)\n    if gamma is None:\n        gamma = 1.0\n    if beta is None:\n        beta = 0.0\n\n    # Get ouput from Numpy\n    zeroed = reshaped_inputs - mean\n    rsqrt = 1 / np.sqrt(variance + 1e-5)\n    output_test = gamma * zeroed * rsqrt + beta\n\n    # compare outputs\n    output_test = tf.reshape(output_test, input_shape)\n    np.testing.assert_almost_equal(tf.reduce_mean(output_test - outputs), 0, decimal=7)\n\n\ndef _create_and_fit_sequential_model(layer, shape):\n    # Helperfunction for quick evaluation\n    np.random.seed(0x2020)\n    model = tf.keras.models.Sequential()\n    model.add(layer)\n    model.add(tf.keras.layers.Dense(32))\n    model.add(tf.keras.layers.Dense(1))\n\n    model.compile(\n        optimizer=tf.keras.optimizers.RMSprop(0.01), loss=""categorical_crossentropy""\n    )\n    layer_shape = (10,) + shape\n    input_batch = np.random.rand(*layer_shape)\n    output_batch = np.random.rand(*(10, 1))\n    model.fit(x=input_batch, y=output_batch, epochs=1, batch_size=1)\n    return model\n\n\ndef test_groupnorm_flat():\n    # Check basic usage of groupnorm_flat\n    # Testing for 1 == LayerNorm, 16 == GroupNorm, -1 == InstanceNorm\n\n    groups = [-1, 16, 1]\n    shape = (64,)\n    for i in groups:\n        model = _create_and_fit_sequential_model(GroupNormalization(groups=i), shape)\n        assert hasattr(model.layers[0], ""gamma"")\n        assert hasattr(model.layers[0], ""beta"")\n\n\ndef test_instancenorm_flat():\n    # Check basic usage of instancenorm\n    model = _create_and_fit_sequential_model(InstanceNormalization(), (64,))\n    assert hasattr(model.layers[0], ""gamma"")\n    assert hasattr(model.layers[0], ""beta"")\n\n\ndef test_initializer():\n    # Check if the initializer for gamma and beta is working correctly\n    layer = GroupNormalization(\n        groups=32,\n        beta_initializer=""random_normal"",\n        beta_constraint=""NonNeg"",\n        gamma_initializer=""random_normal"",\n        gamma_constraint=""NonNeg"",\n    )\n\n    model = _create_and_fit_sequential_model(layer, (64,))\n\n    weights = np.array(model.layers[0].get_weights())\n    negativ = weights[weights < 0.0]\n    assert len(negativ) == 0\n\n\ndef test_axis_error():\n    with pytest.raises(ValueError):\n        GroupNormalization(axis=0)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_groupnorm_conv():\n    # Check if Axis is working for CONV nets\n    # Testing for 1 == LayerNorm, 5 == GroupNorm, -1 == InstanceNorm\n    np.random.seed(0x2020)\n    groups = [-1, 5, 1]\n    for i in groups:\n        model = tf.keras.models.Sequential()\n        model.add(GroupNormalization(axis=1, groups=i, input_shape=(20, 20, 3)))\n        model.add(tf.keras.layers.Conv2D(5, (1, 1), padding=""same""))\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dense(1, activation=""softmax""))\n        model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=""mse"")\n        x = np.random.randint(1000, size=(10, 20, 20, 3))\n        y = np.random.randint(1000, size=(10, 1))\n        model.fit(x=x, y=y, epochs=1)\n        assert hasattr(model.layers[0], ""gamma"")\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_regularizations():\n    layer = GroupNormalization(\n        gamma_regularizer=""l1"", beta_regularizer=""l1"", groups=4, axis=2\n    )\n    layer.build((None, 4, 4))\n    assert len(layer.losses) == 2\n    max_norm = tf.keras.constraints.max_norm\n    layer = GroupNormalization(gamma_constraint=max_norm, beta_constraint=max_norm)\n    layer.build((None, 3, 4))\n    assert layer.gamma.constraint == max_norm\n    assert layer.beta.constraint == max_norm\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_groupnorm_correctness_1d():\n    np.random.seed(0x2020)\n    model = tf.keras.models.Sequential()\n    norm = GroupNormalization(input_shape=(10,), groups=2)\n    model.add(norm)\n    model.compile(loss=""mse"", optimizer=""rmsprop"")\n\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n    model.fit(x, x, epochs=5, verbose=0)\n    out = model.predict(x)\n    out -= norm.beta.numpy()\n    out /= norm.gamma.numpy()\n\n    np.testing.assert_allclose(out.mean(), 0.0, atol=1e-1)\n    np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_groupnorm_2d_different_groups():\n    np.random.seed(0x2020)\n    groups = [2, 1, 10]\n    for i in groups:\n        model = tf.keras.models.Sequential()\n        norm = GroupNormalization(axis=1, groups=i, input_shape=(10, 3))\n        model.add(norm)\n        # centered and variance are 5.0 and 10.0, respectively\n        model.compile(loss=""mse"", optimizer=""rmsprop"")\n        x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 3))\n        model.fit(x, x, epochs=5, verbose=0)\n        out = model.predict(x)\n        out -= np.reshape(norm.beta.numpy(), (1, 10, 1))\n        out /= np.reshape(norm.gamma.numpy(), (1, 10, 1))\n\n        np.testing.assert_allclose(\n            out.mean(axis=(0, 1), dtype=np.float32), (0.0, 0.0, 0.0), atol=1e-1\n        )\n        np.testing.assert_allclose(\n            out.std(axis=(0, 1), dtype=np.float32), (1.0, 1.0, 1.0), atol=1e-1\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_groupnorm_convnet():\n    np.random.seed(0x2020)\n    model = tf.keras.models.Sequential()\n    norm = GroupNormalization(axis=1, input_shape=(3, 4, 4), groups=3)\n    model.add(norm)\n    model.compile(loss=""mse"", optimizer=""sgd"")\n\n    # centered = 5.0, variance  = 10.0\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n    model.fit(x, x, epochs=4, verbose=0)\n    out = model.predict(x)\n    out -= np.reshape(norm.beta.numpy(), (1, 3, 1, 1))\n    out /= np.reshape(norm.gamma.numpy(), (1, 3, 1, 1))\n\n    np.testing.assert_allclose(\n        np.mean(out, axis=(0, 2, 3), dtype=np.float32), (0.0, 0.0, 0.0), atol=1e-1\n    )\n    np.testing.assert_allclose(\n        np.std(out, axis=(0, 2, 3), dtype=np.float32), (1.0, 1.0, 1.0), atol=1e-1\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_groupnorm_convnet_no_center_no_scale():\n    np.random.seed(0x2020)\n    model = tf.keras.models.Sequential()\n    norm = GroupNormalization(\n        axis=-1, groups=2, center=False, scale=False, input_shape=(3, 4, 4)\n    )\n    model.add(norm)\n    model.compile(loss=""mse"", optimizer=""sgd"")\n    # centered and variance are  5.0 and 10.0, respectively\n    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n    model.fit(x, x, epochs=4, verbose=0)\n    out = model.predict(x)\n\n    np.testing.assert_allclose(\n        np.mean(out, axis=(0, 2, 3), dtype=np.float32), (0.0, 0.0, 0.0), atol=1e-1\n    )\n    np.testing.assert_allclose(\n        np.std(out, axis=(0, 2, 3), dtype=np.float32), (1.0, 1.0, 1.0), atol=1e-1\n    )\n\n\ndef calculate_frn(\n    x, beta=0.2, gamma=1, eps=1e-6, learned_epsilon=False, dtype=np.float32\n):\n    if learned_epsilon:\n        eps = eps + 1e-4\n    eps = tf.cast(eps, dtype=dtype)\n    nu2 = tf.reduce_mean(tf.square(x), axis=[1, 2], keepdims=True)\n    x = x * tf.math.rsqrt(nu2 + tf.abs(eps))\n    return gamma * x + beta\n\n\ndef set_random_seed():\n    seed = 0x2020\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_with_beta(dtype):\n    set_random_seed()\n    inputs = np.random.rand(28, 28, 1).astype(dtype)\n    inputs = np.expand_dims(inputs, axis=0)\n    frn = FilterResponseNormalization(\n        beta_initializer=""ones"", gamma_initializer=""ones"", dtype=dtype\n    )\n    frn.build((None, 28, 28, 1))\n    observed = frn(inputs)\n    expected = calculate_frn(inputs, beta=1, gamma=1, dtype=dtype)\n    np.testing.assert_allclose(expected[0], observed[0])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_with_gamma(dtype):\n    set_random_seed()\n    inputs = np.random.rand(28, 28, 1).astype(dtype)\n    inputs = np.expand_dims(inputs, axis=0)\n    frn = FilterResponseNormalization(\n        beta_initializer=""zeros"", gamma_initializer=""ones"", dtype=dtype\n    )\n    frn.build((None, 28, 28, 1))\n    observed = frn(inputs)\n    expected = calculate_frn(inputs, beta=0, gamma=1, dtype=dtype)\n    np.testing.assert_allclose(expected[0], observed[0])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_with_epsilon(dtype):\n    set_random_seed()\n    inputs = np.random.rand(28, 28, 1).astype(dtype)\n    inputs = np.expand_dims(inputs, axis=0)\n    frn = FilterResponseNormalization(\n        beta_initializer=tf.keras.initializers.Constant(0.5),\n        gamma_initializer=""ones"",\n        learned_epsilon=True,\n        dtype=dtype,\n    )\n    frn.build((None, 28, 28, 1))\n    observed = frn(inputs)\n    expected = calculate_frn(\n        inputs, beta=0.5, gamma=1, learned_epsilon=True, dtype=dtype\n    )\n    np.testing.assert_allclose(expected[0], observed[0])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_keras_model(dtype):\n    set_random_seed()\n    frn = FilterResponseNormalization(\n        beta_initializer=""ones"", gamma_initializer=""ones"", dtype=dtype\n    )\n    random_inputs = np.random.rand(10, 32, 32, 3).astype(dtype)\n    random_labels = np.random.randint(2, size=(10,)).astype(dtype)\n    input_layer = tf.keras.layers.Input(shape=(32, 32, 3))\n    x = frn(input_layer)\n    x = tf.keras.layers.Flatten()(x)\n    out = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)\n    model = tf.keras.models.Model(input_layer, out)\n    model.compile(loss=""binary_crossentropy"", optimizer=""sgd"")\n    model.fit(random_inputs, random_labels, epochs=2)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_serialization(dtype):\n    frn = FilterResponseNormalization(\n        beta_initializer=""ones"", gamma_initializer=""ones"", dtype=dtype\n    )\n    serialized_frn = tf.keras.layers.serialize(frn)\n    new_layer = tf.keras.layers.deserialize(serialized_frn)\n    assert frn.get_config() == new_layer.get_config()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_eps_gards(dtype):\n    set_random_seed()\n    random_inputs = np.random.rand(10, 32, 32, 3).astype(np.float32)\n    random_labels = np.random.randint(2, size=(10,)).astype(np.float32)\n    input_layer = tf.keras.layers.Input(shape=(32, 32, 3))\n    frn = FilterResponseNormalization(\n        beta_initializer=""ones"", gamma_initializer=""ones"", learned_epsilon=True\n    )\n    initial_eps_value = frn.eps_learned.numpy()[0]\n    x = frn(input_layer)\n    x = tf.keras.layers.Flatten()(x)\n    out = tf.keras.layers.Dense(1, activation=""sigmoid"")(x)\n    model = tf.keras.models.Model(input_layer, out)\n    model.compile(loss=""binary_crossentropy"", optimizer=""sgd"")\n    model.fit(random_inputs, random_labels, epochs=1)\n    final_eps_value = frn.eps_learned.numpy()[0]\n    assert initial_eps_value != final_eps_value\n'"
tensorflow_addons/layers/tests/optical_flow_test.py,18,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.layers.optical_flow import CorrelationCost\n\n\ndef _forward(\n    input_a,\n    input_b,\n    kernel_size,\n    max_displacement,\n    stride_1,\n    stride_2,\n    pad,\n    data_format,\n):\n    input_a_op = tf.convert_to_tensor(input_a, dtype=tf.float32)\n    input_b_op = tf.convert_to_tensor(input_b, dtype=tf.float32)\n\n    output = CorrelationCost(\n        kernel_size=kernel_size,\n        max_displacement=max_displacement,\n        stride_1=stride_1,\n        stride_2=stride_2,\n        pad=pad,\n        data_format=data_format,\n    )([input_a_op, input_b_op])\n\n    return output\n\n\ndef _create_test_data(data_format):\n    # Produce test data for _forward_simple and _keras methods\n    val_a = np.array(\n        [\n            [\n                [[0, -6, 9, 5], [1, -5, 10, 3], [2, -4, 11, 1]],\n                [[3, -3, 12, -1], [4, -2, 13, -3], [5, -1, 14, -5]],\n            ],\n            [\n                [[6, 0, 15, -7], [7, 1, 16, -9], [8, 2, 17, -11]],\n                [[9, 3, 18, -13], [10, 4, 19, -15], [11, 5, 20, -17]],\n            ],\n        ],\n        dtype=np.float32,\n    )\n\n    # pylint: disable=too-many-function-args\n    val_b = val_a.transpose(2, 3, 0, 1).reshape(2, 2, 3, 4)\n    # pylint: enable=too-many-function-args\n\n    if data_format == ""channels_last"":\n        val_a = np.moveaxis(val_a, 1, -1)\n        val_b = np.moveaxis(val_b, 1, -1)\n\n    return val_a, val_b\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_forward_simple(data_format):\n    # We are just testing where the output has vanishing values.\n    val_a, val_b = _create_test_data(data_format)\n    input_a = tf.constant(val_a, dtype=tf.float32)\n    input_b = tf.constant(val_b, dtype=tf.float32)\n\n    input_a_tensor = tf.convert_to_tensor(input_a, dtype=tf.float32)\n    input_b_tensor = tf.convert_to_tensor(input_b, dtype=tf.float32)\n\n    kernel_size = 1\n    max_displacement = 2\n    stride_1 = 1\n    stride_2 = 2\n    pad = 4\n\n    actual = _forward(\n        input_a_tensor,\n        input_b_tensor,\n        kernel_size=kernel_size,\n        max_displacement=max_displacement,\n        stride_1=stride_1,\n        stride_2=stride_2,\n        pad=pad,\n        data_format=data_format,\n    )\n\n    if data_format == ""channels_last"":\n        # NHWC -> NCHW\n        actual = tf.transpose(actual, [0, 3, 1, 2])\n\n    # We can test fixed ids, as output is independent from data_format\n    expected_ids = np.concatenate([np.zeros(464,), np.ones(464,)])\n    np.testing.assert_allclose(tf.where(actual == 0)[:, 0].numpy(), expected_ids)\n\n    counts = [54, 52, 54, 50, 44, 50, 54, 52, 54]\n    expected_ids = np.concatenate([k * np.ones(v,) for k, v in enumerate(counts)])\n    expected_ids = np.concatenate([expected_ids, expected_ids])\n    np.testing.assert_allclose(tf.where(actual == 0)[:, 1], expected_ids)\n    assert actual.shape == (2, 9, 7, 8)\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\ndef test_gradients(data_format):\n    batch, channels, height, width = 2, 3, 5, 6\n    input_a = np.random.randn(batch, channels, height, width).astype(np.float32)\n    input_b = np.random.randn(batch, channels, height, width).astype(np.float32)\n\n    kernel_size = 1\n    max_displacement = 2\n    stride_1 = 1\n    stride_2 = 2\n    pad = 4\n\n    if data_format == ""channels_last"":\n        input_a = tf.transpose(input_a, [0, 2, 3, 1])\n        input_b = tf.transpose(input_b, [0, 2, 3, 1])\n\n    input_a_op = tf.convert_to_tensor(input_a)\n    input_b_op = tf.convert_to_tensor(input_b)\n\n    def correlation_fn(input_a, input_b):\n        return CorrelationCost(\n            kernel_size=kernel_size,\n            max_displacement=max_displacement,\n            stride_1=stride_1,\n            stride_2=stride_2,\n            pad=pad,\n            data_format=data_format,\n        )([input_a, input_b])\n\n    theoretical, numerical = tf.test.compute_gradient(\n        correlation_fn, [input_a_op, input_b_op]\n    )\n\n    np.testing.assert_allclose(theoretical[0], numerical[0], atol=1e-3)\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\ndef test_keras(data_format):\n    # Unable to use `layer_test` as this layer has multiple inputs.\n    val_a, val_b = _create_test_data(data_format)\n\n    input_a = tf.keras.Input(shape=val_a.shape[1:])\n    input_b = tf.keras.Input(shape=val_b.shape[1:])\n\n    layer = CorrelationCost(\n        kernel_size=1,\n        max_displacement=2,\n        stride_1=1,\n        stride_2=2,\n        pad=4,\n        data_format=data_format,\n    )\n\n    expected_output_shape = tuple(\n        layer.compute_output_shape([input_a.shape, input_b.shape])\n    )\n\n    x = [input_a, input_b]\n    y = layer(x)\n    model = tf.keras.models.Model(x, y)\n    actual_output = model([val_a, val_b])\n\n    expected_output_type = ""float32""\n    assert tf.keras.backend.dtype(y[0]) == expected_output_type\n    assert actual_output.shape[1:] == expected_output_shape[0][1:]\n'"
tensorflow_addons/layers/tests/poincare_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for PoincareNormalize layer.""""""\n\n\nimport pytest\nimport numpy as np\n\nfrom tensorflow_addons.layers.poincare import PoincareNormalize\nfrom tensorflow_addons.utils import test_utils\n\n\ndef _poincare_normalize(x, dim, epsilon=1e-5):\n    if isinstance(dim, list):\n        norm = np.linalg.norm(x, axis=tuple(dim))\n        for d in dim:\n            norm = np.expand_dims(norm, d)\n        norm_x = ((1.0 - epsilon) * x) / norm\n    else:\n        norm = np.expand_dims(np.apply_along_axis(np.linalg.norm, dim, x), dim)\n        norm_x = ((1.0 - epsilon) * x) / norm\n    return np.where(norm > 1.0 - epsilon, norm_x, x)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_poincare_normalize():\n    x_shape = [20, 7, 3]\n    epsilon = 1e-5\n    tol = 1e-6\n    np.random.seed(1)\n    inputs = np.random.random_sample(x_shape).astype(np.float32)\n\n    for dim in range(len(x_shape)):\n        outputs_expected = _poincare_normalize(inputs, dim, epsilon)\n\n        outputs = test_utils.layer_test(\n            PoincareNormalize,\n            kwargs={""axis"": dim, ""epsilon"": epsilon},\n            input_data=inputs,\n            expected_output=outputs_expected,\n        )\n        for y in outputs_expected, outputs:\n            norm = np.linalg.norm(y, axis=dim)\n            assert norm.max() <= 1.0 - epsilon + tol\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_poincare_normalize_dim_array():\n    x_shape = [20, 7, 3]\n    epsilon = 1e-5\n    tol = 1e-6\n    np.random.seed(1)\n    inputs = np.random.random_sample(x_shape).astype(np.float32)\n    dim = [1, 2]\n\n    outputs_expected = _poincare_normalize(inputs, dim, epsilon)\n\n    outputs = test_utils.layer_test(\n        PoincareNormalize,\n        kwargs={""axis"": dim, ""epsilon"": epsilon},\n        input_data=inputs,\n        expected_output=outputs_expected,\n    )\n    for y in outputs_expected, outputs:\n        norm = np.linalg.norm(y, axis=tuple(dim))\n        assert norm.max() <= 1.0 - epsilon + tol\n'"
tensorflow_addons/layers/tests/polynomial_test.py,2,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for PolynomialCrossing layer.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.layers.polynomial import PolynomialCrossing\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_full_matrix():\n    x0 = np.asarray([[0.1, 0.2, 0.3]]).astype(np.float32)\n    x = np.asarray([[0.4, 0.5, 0.6]]).astype(np.float32)\n    layer = PolynomialCrossing(projection_dim=None, kernel_initializer=""ones"")\n    output = layer([x0, x])\n    np.testing.assert_allclose([[0.55, 0.8, 1.05]], output)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_low_rank_matrix():\n    x0 = np.asarray([[0.1, 0.2, 0.3]]).astype(np.float32)\n    x = np.asarray([[0.4, 0.5, 0.6]]).astype(np.float32)\n    layer = PolynomialCrossing(projection_dim=1, kernel_initializer=""ones"")\n    output = layer([x0, x])\n    np.testing.assert_allclose([[0.55, 0.8, 1.05]], output)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_invalid_proj_dim():\n    with pytest.raises(ValueError, match=""should be smaller than last_dim / 2""):\n        x0 = np.random.random((12, 5))\n        x = np.random.random((12, 5))\n        layer = PolynomialCrossing(projection_dim=6)\n        layer([x0, x])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_invalid_inputs():\n    with pytest.raises(ValueError, match=""must be a tuple or list of size 2""):\n        x0 = np.random.random((12, 5))\n        x = np.random.random((12, 5))\n        x1 = np.random.random((12, 5))\n        layer = PolynomialCrossing(projection_dim=6)\n        layer([x0, x, x1])\n\n\ndef test_serialization():\n    layer = PolynomialCrossing(projection_dim=None)\n    serialized_layer = tf.keras.layers.serialize(layer)\n    new_layer = tf.keras.layers.deserialize(serialized_layer)\n    assert layer.get_config() == new_layer.get_config()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_diag_scale():\n    x0 = np.asarray([[0.1, 0.2, 0.3]]).astype(np.float32)\n    x = np.asarray([[0.4, 0.5, 0.6]]).astype(np.float32)\n    layer = PolynomialCrossing(\n        projection_dim=None, diag_scale=1.0, kernel_initializer=""ones""\n    )\n    output = layer([x0, x])\n    np.testing.assert_allclose([[0.59, 0.9, 1.23]], output)\n'"
tensorflow_addons/layers/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/layers/tests/sparsemax_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport pytest\nimport numpy as np\n\nfrom tensorflow_addons.layers import Sparsemax\nfrom tensorflow_addons.utils import test_utils\n\ntest_obs = 17\n\n\ndef _np_sparsemax(z):\n    z = z - np.mean(z, axis=1)[:, np.newaxis]\n\n    # sort z\n    z_sorted = np.sort(z, axis=1)[:, ::-1]\n\n    # calculate k(z)\n    z_cumsum = np.cumsum(z_sorted, axis=1)\n    k = np.arange(1, z.shape[1] + 1)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # use argmax to get the index by row as .nonzero() doesn\'t\n    # take an axis argument. np.argmax return the first index, but the last\n    # index is required here, use np.flip to get the last index and\n    # `z.shape[axis]` to compensate for np.flip afterwards.\n    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)\n\n    # calculate tau(z)\n    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]\n    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)\n\n    # calculate p\n    return np.maximum(0, z - tau_z)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_sparsemax_layer_against_numpy(dtype):\n    """"""check sparsemax kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10)).astype(dtype)\n\n    test_utils.layer_test(\n        Sparsemax,\n        kwargs={""dtype"": dtype},\n        input_data=z,\n        expected_output=_np_sparsemax(z).astype(dtype),\n    )\n'"
tensorflow_addons/layers/tests/spatial_pyramid_pooling_test.py,5,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for SpatialPyramidPooling layers""""""\n\nimport pytest\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow_addons.layers.spatial_pyramid_pooling import SpatialPyramidPooling2D\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_spp_shape_2d():\n    spp = SpatialPyramidPooling2D([1, 3, 5])\n    output_shape = [256, 35, 64]\n    assert spp.compute_output_shape([256, None, None, 64]).as_list() == output_shape\n\n    spp = SpatialPyramidPooling2D([1, 3, 5], data_format=""channels_first"")\n    output_shape = [256, 64, 35]\n    assert spp.compute_output_shape([256, 64, None, None]).as_list() == output_shape\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_spp_output_2d():\n    inputs = np.arange(start=0.0, stop=16.0, step=1.0).astype(np.float32)\n    inputs = np.reshape(inputs, (1, 4, 4, 1))\n    output = np.array([[[7.5], [2.5], [4.5], [10.5], [12.5]]]).astype(np.float32)\n    test_utils.layer_test(\n        SpatialPyramidPooling2D,\n        kwargs={""bins"": [[1, 1], [2, 2]], ""data_format"": ""channels_last""},\n        input_data=inputs,\n        expected_output=output,\n    )\n\n    inputs = np.arange(start=0.0, stop=16.0, step=1.0).astype(np.float32)\n    inputs = np.reshape(inputs, (1, 1, 4, 4))\n    output = np.array([[[7.5, 2.5, 4.5, 10.5, 12.5]]]).astype(np.float32)\n    test_utils.layer_test(\n        SpatialPyramidPooling2D,\n        kwargs={""bins"": [[1, 1], [2, 2]], ""data_format"": ""channels_first""},\n        input_data=inputs,\n        expected_output=output,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_serialization():\n    layer = SpatialPyramidPooling2D([[1, 1], [3, 3]])\n    serialized_layer = tf.keras.layers.serialize(layer)\n    new_layer = tf.keras.layers.deserialize(serialized_layer)\n    assert layer.get_config() == new_layer.get_config()\n\n\ndef test_keras(tmpdir):\n    test_inputs = np.arange(start=0.0, stop=16.0, step=1.0).astype(np.float32)\n    test_inputs = np.reshape(test_inputs, (1, 4, 4, 1))\n    test_output = [[[7.5], [2.5], [4.5], [10.5], [12.5]]]\n\n    inputs = tf.keras.layers.Input((None, None, 1))\n    spp = SpatialPyramidPooling2D([1, 2])(inputs)\n    model = tf.keras.Model(inputs=[inputs], outputs=[spp])\n\n    model_path = str(tmpdir / ""spp_model.h5"")\n    model.save(model_path)\n    model = tf.keras.models.load_model(model_path)\n    model_output = model.predict(test_inputs).tolist()\n    assert model_output == test_output\n'"
tensorflow_addons/layers/tests/tlu_test.py,2,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for TLU activation.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.layers.tlu import TLU\nfrom tensorflow_addons.utils import test_utils\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_random(dtype):\n    x = np.array([[-2.5, 0.0, 0.3]]).astype(dtype)\n    val = np.array([[0.0, 0.0, 0.3]]).astype(dtype)\n    test_utils.layer_test(\n        TLU, kwargs={""dtype"": dtype}, input_data=x, expected_output=val\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_affine(dtype):\n    x = np.array([[-2.5, 0.0, 0.3]]).astype(dtype)\n    val = np.array([[-1.5, 1.0, 1.3]]).astype(dtype)\n    test_utils.layer_test(\n        TLU,\n        kwargs={\n            ""affine"": True,\n            ""dtype"": dtype,\n            ""alpha_initializer"": ""ones"",\n            ""tau_initializer"": ""ones"",\n        },\n        input_data=x,\n        expected_output=val,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_serialization(dtype):\n    tlu = TLU(\n        affine=True, alpha_initializer=""ones"", tau_initializer=""ones"", dtype=dtype\n    )\n    serialized_tlu = tf.keras.layers.serialize(tlu)\n    new_layer = tf.keras.layers.deserialize(serialized_tlu)\n    assert tlu.get_config() == new_layer.get_config()\n'"
tensorflow_addons/layers/tests/wrappers_test.py,35,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport os\nimport tempfile\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.layers import wrappers\nfrom tensorflow_addons.utils import test_utils\n\n\ndef test_basic():\n    test_utils.layer_test(\n        wrappers.WeightNormalization,\n        kwargs={""layer"": tf.keras.layers.Conv2D(5, (2, 2)),},\n        input_shape=(2, 4, 4, 3),\n    )\n\n\ndef test_no_bias():\n    test_utils.layer_test(\n        wrappers.WeightNormalization,\n        kwargs={""layer"": tf.keras.layers.Dense(5, use_bias=False),},\n        input_shape=(2, 4),\n    )\n\n\ndef _check_data_init(data_init, input_data, expected_output):\n    layer = tf.keras.layers.Dense(\n        input_data.shape[-1],\n        activation=None,\n        kernel_initializer=""identity"",\n        bias_initializer=""zeros"",\n    )\n    test_utils.layer_test(\n        wrappers.WeightNormalization,\n        kwargs={""layer"": layer, ""data_init"": data_init,},\n        input_data=input_data,\n        expected_output=expected_output,\n    )\n\n\ndef test_with_data_init_is_false():\n    input_data = np.array([[[-4, -4], [4, 4]]], dtype=np.float32)\n    _check_data_init(data_init=False, input_data=input_data, expected_output=input_data)\n\n\ndef test_with_data_init_is_true():\n    input_data = np.array([[[-4, -4], [4, 4]]], dtype=np.float32)\n    _check_data_init(\n        data_init=True, input_data=input_data, expected_output=input_data / 4\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_non_layer():\n    images = tf.random.uniform((2, 4, 3))\n    with pytest.raises(AssertionError):\n        wrappers.WeightNormalization(images)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_non_kernel_layer():\n    images = tf.random.uniform((2, 2, 2))\n    with pytest.raises(ValueError, match=""contains a `kernel`""):\n        non_kernel_layer = tf.keras.layers.MaxPooling2D(2, 2)\n        wn_wrapper = wrappers.WeightNormalization(non_kernel_layer)\n        wn_wrapper(images)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_with_time_dist():\n    batch_shape = (8, 8, 16, 16, 3)\n    inputs = tf.keras.layers.Input(batch_shape=batch_shape)\n    a = tf.keras.layers.Conv2D(3, 3)\n    b = wrappers.WeightNormalization(a)\n    out = tf.keras.layers.TimeDistributed(b)(inputs)\n    tf.keras.Model(inputs, out)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(\n    ""base_layer, rnn"",\n    [\n        (lambda: tf.keras.layers.Dense(1), False),\n        (lambda: tf.keras.layers.SimpleRNN(1), True),\n        (lambda: tf.keras.layers.Conv2D(3, 1), False),\n        (lambda: tf.keras.layers.LSTM(1), True),\n    ],\n)\ndef test_serialization(base_layer, rnn):\n    base_layer = base_layer()\n    wn_layer = wrappers.WeightNormalization(base_layer, not rnn)\n    new_wn_layer = tf.keras.layers.deserialize(tf.keras.layers.serialize(wn_layer))\n    assert wn_layer.data_init == new_wn_layer.data_init\n    assert wn_layer.is_rnn == new_wn_layer.is_rnn\n    assert wn_layer.is_rnn == rnn\n    if not isinstance(base_layer, tf.keras.layers.LSTM):\n        # Issue with LSTM serialization, check with TF-core\n        # Before serialization: tensorflow.python.keras.layers.recurrent_v2.LSTM\n        # After serialization: tensorflow.python.keras.layers.recurrent.LSTM\n        assert isinstance(new_wn_layer.layer, base_layer.__class__)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""data_init"", [True, False])\n@pytest.mark.parametrize(\n    ""base_layer_fn, input_shape"",\n    [\n        (lambda: tf.keras.layers.Dense(1), [1]),\n        (lambda: tf.keras.layers.SimpleRNN(1), [None, 10]),\n        (lambda: tf.keras.layers.Conv2D(3, 1), [3, 3, 1]),\n        (lambda: tf.keras.layers.LSTM(1), [10, 10]),\n    ],\n)\ndef test_model_build(base_layer_fn, input_shape, data_init):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n    base_layer = base_layer_fn()\n    wt_layer = wrappers.WeightNormalization(base_layer, data_init)\n    model = tf.keras.models.Sequential(layers=[inputs, wt_layer])\n    model.build()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(\n    ""base_layer, input_shape"",\n    [\n        (lambda: tf.keras.layers.Dense(1), [1]),\n        (lambda: tf.keras.layers.SimpleRNN(1), [10, 10]),\n        (lambda: tf.keras.layers.Conv2D(3, 1), [3, 3, 1]),\n        (lambda: tf.keras.layers.LSTM(1), [10, 10]),\n    ],\n)\ndef test_save_file_h5(base_layer, input_shape):\n    base_layer = base_layer()\n    wn_conv = wrappers.WeightNormalization(base_layer)\n    model = tf.keras.Sequential(layers=[wn_conv])\n    model.build([None] + input_shape)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        model.save_weights(os.path.join(tmp_dir, ""wrapper_test_model.h5""))\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(\n    ""base_layer, input_shape"",\n    [\n        (lambda: tf.keras.layers.Dense(1), [1]),\n        (lambda: tf.keras.layers.SimpleRNN(1), [10, 10]),\n        (lambda: tf.keras.layers.Conv2D(3, 1), [3, 3, 1]),\n        (lambda: tf.keras.layers.LSTM(1), [10, 10]),\n    ],\n)\ndef test_forward_pass(base_layer, input_shape):\n    sample_data = np.ones([1] + input_shape, dtype=np.float32)\n    base_layer = base_layer()\n    base_output = base_layer(sample_data)\n    wn_layer = wrappers.WeightNormalization(base_layer, False)\n    wn_output = wn_layer(sample_data)\n    np.testing.assert_allclose(base_output, wn_output, rtol=1e-6, atol=1e-6)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""data_init"", [True, False])\n@pytest.mark.parametrize(\n    ""base_layer_fn, input_shape"",\n    [\n        (lambda: tf.keras.layers.Dense(1), [1]),\n        (lambda: tf.keras.layers.SimpleRNN(1), [10, 10]),\n        (lambda: tf.keras.layers.Conv2D(3, 1), [3, 3, 1]),\n        (lambda: tf.keras.layers.LSTM(1), [10, 10]),\n    ],\n)\ndef test_removal(base_layer_fn, input_shape, data_init):\n    sample_data = np.ones([1] + input_shape, dtype=np.float32)\n\n    base_layer = base_layer_fn()\n    wn_layer = wrappers.WeightNormalization(base_layer, data_init)\n    wn_output = wn_layer(sample_data)\n    wn_removed_layer = wn_layer.remove()\n    wn_removed_output = wn_removed_layer(sample_data)\n    np.testing.assert_allclose(wn_removed_output.numpy(), wn_output.numpy())\n    assert isinstance(wn_removed_layer, base_layer.__class__)\n'"
tensorflow_addons/losses/tests/__init__.py,0,b''
tensorflow_addons/losses/tests/contrastive_test.py,21,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for contrastive loss.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.losses import contrastive\n\n\ndef test_config():\n    cl_obj = contrastive.ContrastiveLoss(\n        reduction=tf.keras.losses.Reduction.SUM, name=""cl""\n    )\n    assert cl_obj.name == ""cl""\n    assert cl_obj.reduction == tf.keras.losses.Reduction.SUM\n\n\ndef test_zero_loss():\n    cl_obj = contrastive.ContrastiveLoss()\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([1.0, 1.0, 0.0, 0.0, 1.0, 0.0], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred)\n    np.testing.assert_allclose(loss, 0.0)\n\n\ndef test_unweighted():\n    cl_obj = contrastive.ContrastiveLoss()\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(1 - 0.1, 0)^2, max(1 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(1 - 1.1, 0)^2, 0.5^2]\n    #      = [0.9^2, 0.7^2, 1.3^2, 0.7^2, 0^2, 0.5^2]\n    #      = [0.81, 0.49, 1.69, 0.49, 0, 0.25]\n    # Reduced loss = (0.81 + 0.49 + 1.69 + 0.49 + 0 + 0.25) / 6\n    #              = 0.621666\n\n    np.testing.assert_allclose(loss, 0.621666, atol=1e-6, rtol=1e-6)\n\n\ndef test_sample_weighted():\n    cl_obj = contrastive.ContrastiveLoss()\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    sample_weight = tf.constant([1.2, 0.8, 0.5, 0.4, 1.5, 1.0], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred, sample_weight=sample_weight)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(1 - 0.1, 0)^2, max(1 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(1 - 1.1, 0)^2, 0.5^2]\n    #      = [0.9^2, 0.7^2, 1.3^2, 0.7^2, 0^2, 0.5^2]\n    #      = [0.81, 0.49, 1.69, 0.49, 0, 0.25]\n    # Weighted loss = [0.81 * 1.2, 0.49 * 0.8, 1.69 * 0.5,\n    #                  0.49 * 0.4, 0 * 1.5, 0.25 * 1.0]\n    #               = [0.972, 0.392, 0.845, 0.196, 0, 0.25]\n    # Reduced loss = (0.972 + 0.392 + 0.845 + 0.196 + 0 + 0.25) / 6\n    #              = 0.4425\n\n    np.testing.assert_allclose(loss, 0.4425)\n\n\ndef test_non_default_margin():\n    cl_obj = contrastive.ContrastiveLoss(margin=2.0)\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(2 - 0.1, 0)^2, max(2 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(2 - 1.1, 0)^2, 0.5^2]\n    #      = [1.9^2, 1.7^2, 1.3^2, 0.7^2, 0.9^2, 0.5^2]\n    #      = [3.61, 2.89, 1.69, 0.49, 0.81, 0.25]\n    # Reduced loss = (3.61 + 2.89 + 1.69 + 0.49 + 0.81 + 0.25) / 6\n    #              = 1.623333\n\n    np.testing.assert_allclose(loss, 1.623333, atol=1e-6, rtol=1e-6)\n\n\ndef test_scalar_weighted():\n    cl_obj = contrastive.ContrastiveLoss()\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred, sample_weight=6.0)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(1 - 0.1, 0)^2, max(1 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(1 - 1.1, 0)^2, 0.5^2]\n    #      = [0.9^2, 0.7^2, 1.3^2, 0.7^2, 0^2, 0.5^2]\n    #      = [0.81, 0.49, 1.69, 0.49, 0, 0.25]\n    # Weighted loss = [0.81 * 6, 0.49 * 6, 1.69 * 6,\n    #                  0.49 * 6, 0 * 6, 0.25 * 6]\n    # Reduced loss = (0.81 + 0.49 + 1.69 + 0.49 + 0 + 0.25) * 6 / 6\n    #              = 3.73\n\n    np.testing.assert_allclose(loss, 3.73, atol=1e-6, rtol=1e-6)\n\n\ndef test_zero_weighted():\n    cl_obj = contrastive.ContrastiveLoss()\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred, sample_weight=0.0)\n    np.testing.assert_allclose(loss, 0.0)\n\n\ndef test_no_reduction():\n    cl_obj = contrastive.ContrastiveLoss(reduction=tf.keras.losses.Reduction.NONE)\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(1 - 0.1, 0)^2, max(1 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(1 - 1.1, 0)^2, 0.5^2]\n    #      = [0.9^2, 0.7^2, 1.3^2, 0.7^2, 0^2, 0.5^2]\n    #      = [0.81, 0.49, 1.69, 0.49, 0, 0.25]\n\n    np.testing.assert_allclose(loss, [0.81, 0.49, 1.69, 0.49, 0.0, 0.25], rtol=1e-5)\n\n\ndef test_sum_reduction():\n    cl_obj = contrastive.ContrastiveLoss(reduction=tf.keras.losses.Reduction.SUM)\n    y_true = tf.constant([0, 0, 1, 1, 0, 1], dtype=tf.dtypes.int64)\n    y_pred = tf.constant([0.1, 0.3, 1.3, 0.7, 1.1, 0.5], dtype=tf.dtypes.float32)\n    loss = cl_obj(y_true, y_pred)\n\n    # Loss = y * (y`)^2 + (1 - y) * (max(m - y`, 0))^2\n    #      = [max(1 - 0.1, 0)^2, max(1 - 0.3, 0)^2,\n    #         1.3^2, 0.7^2, max(1 - 1.1, 0)^2, 0.5^2]\n    #      = [0.9^2, 0.7^2, 1.3^2, 0.7^2, 0^2, 0.5^2]\n    #      = [0.81, 0.49, 1.69, 0.49, 0, 0.25]\n    # Reduced loss = 0.81 + 0.49 + 1.69 + 0.49 + 0 + 0.25\n    #              = 3.73\n\n    np.testing.assert_allclose(loss, 3.73)\n'"
tensorflow_addons/losses/tests/focal_loss_test.py,19,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for focal loss.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow_addons.losses import (\n    sigmoid_focal_crossentropy,\n    SigmoidFocalCrossEntropy,\n)\n\n\ndef test_config():\n    bce_obj = SigmoidFocalCrossEntropy(\n        reduction=tf.keras.losses.Reduction.NONE, name=""sigmoid_focal_crossentropy""\n    )\n    assert bce_obj.name == ""sigmoid_focal_crossentropy""\n    assert bce_obj.reduction == tf.keras.losses.Reduction.NONE\n\n\ndef to_logit(prob):\n    logit = np.log(prob / (1.0 - prob))\n    return logit\n\n\n# Test with logits\ndef test_with_logits():\n    # predictiions represented as logits\n    prediction_tensor = tf.constant(\n        [\n            [to_logit(0.97)],\n            [to_logit(0.91)],\n            [to_logit(0.73)],\n            [to_logit(0.27)],\n            [to_logit(0.09)],\n            [to_logit(0.03)],\n        ],\n        tf.float32,\n    )\n    # Ground truth\n    target_tensor = tf.constant([[1], [1], [1], [0], [0], [0]], tf.float32)\n\n    fl = sigmoid_focal_crossentropy(\n        y_true=target_tensor,\n        y_pred=prediction_tensor,\n        from_logits=True,\n        alpha=None,\n        gamma=None,\n    )\n    bce = tf.reduce_sum(\n        K.binary_crossentropy(target_tensor, prediction_tensor, from_logits=True),\n        axis=-1,\n    )\n\n    # When alpha and gamma are None, it should be equal to BCE\n    np.testing.assert_allclose(fl, bce)\n\n    # When gamma==2.0\n    fl = sigmoid_focal_crossentropy(\n        y_true=target_tensor,\n        y_pred=prediction_tensor,\n        from_logits=True,\n        alpha=None,\n        gamma=2.0,\n    )\n\n    # order_of_ratio = np.power(10, np.floor(np.log10(bce/FL)))\n    order_of_ratio = tf.pow(10.0, tf.math.floor(log10(bce / fl)))\n    pow_values = tf.constant([1000, 100, 10, 10, 100, 1000])\n    np.testing.assert_allclose(order_of_ratio, pow_values)\n\n\ndef test_keras_model_compile():\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Input(shape=(100,)),\n            tf.keras.layers.Dense(5, activation=""softmax""),\n        ]\n    )\n    model.compile(loss=""Addons>sigmoid_focal_crossentropy"")\n\n\ndef log10(x):\n    numerator = tf.math.log(x)\n    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n    return numerator / denominator\n\n\n# Test without logits\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_without_logits():\n    # predictiions represented as logits\n    prediction_tensor = tf.constant(\n        [[0.97], [0.91], [0.73], [0.27], [0.09], [0.03]], tf.float32\n    )\n    # Ground truth\n    target_tensor = tf.constant([[1], [1], [1], [0], [0], [0]], tf.float32)\n\n    fl = sigmoid_focal_crossentropy(\n        y_true=target_tensor, y_pred=prediction_tensor, alpha=None, gamma=None\n    )\n    bce = tf.reduce_sum(\n        K.binary_crossentropy(target_tensor, prediction_tensor), axis=-1\n    )\n\n    # When alpha and gamma are None, it should be equal to BCE\n    assert np.allclose(fl, bce)\n\n    # When gamma==2.0\n    fl = sigmoid_focal_crossentropy(\n        y_true=target_tensor, y_pred=prediction_tensor, alpha=None, gamma=2.0\n    )\n\n    order_of_ratio = tf.pow(10.0, tf.math.floor(log10(bce / fl)))\n    pow_values = tf.constant([1000, 100, 10, 10, 100, 1000])\n    assert np.allclose(order_of_ratio, pow_values)\n'"
tensorflow_addons/losses/tests/giou_loss_test.py,25,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for GIoU loss.""""""\n\n\nimport pytest\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.utils import test_utils\nfrom tensorflow_addons.losses import giou_loss, GIoULoss\n\n\ndef test_config():\n    gl_obj = GIoULoss(reduction=tf.keras.losses.Reduction.NONE, name=""giou_loss"")\n    assert gl_obj.name == ""giou_loss""\n    assert gl_obj.reduction == tf.keras.losses.Reduction.NONE\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_iou(dtype):\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]], dtype=dtype)\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]], dtype=dtype)\n    expected_result = tf.constant([0.875, 1.0], dtype=dtype)\n    loss = giou_loss(boxes1, boxes2, mode=""iou"")\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_giou_loss(dtype):\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]], dtype=dtype)\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]], dtype=dtype)\n    expected_result = tf.constant(\n        [1.07500000298023224, 1.9333333373069763], dtype=dtype\n    )\n    loss = giou_loss(boxes1, boxes2)\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_different_shapes(dtype):\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]], dtype=dtype)\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0]], dtype=dtype)\n    expand_boxes1 = tf.expand_dims(boxes1, -2)\n    expand_boxes2 = tf.expand_dims(boxes2, 0)\n    expected_result = tf.constant([1.07500000298023224, 1.366071], dtype=dtype)\n    loss = giou_loss(expand_boxes1, expand_boxes2)\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_one_bbox(dtype):\n    boxes1 = tf.constant([4.0, 3.0, 7.0, 5.0], dtype=dtype)\n    boxes2 = tf.constant([3.0, 4.0, 6.0, 8.0], dtype=dtype)\n    expected_result = tf.constant(1.07500000298023224, dtype=dtype)\n    loss = giou_loss(boxes1, boxes2)\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float16, np.float32, np.float64])\ndef test_keras_model(dtype):\n    boxes1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]], dtype=dtype)\n    boxes2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]], dtype=dtype)\n    expected_result = tf.constant(1.5041667222976685, dtype=dtype)\n    model = tf.keras.Sequential()\n    model.compile(\n        optimizer=""adam"",\n        loss=GIoULoss(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE),\n    )\n    loss = model.evaluate(boxes1, boxes2, batch_size=2, steps=1)\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_with_integer():\n    boxes1 = tf.constant([[4, 3, 7, 5], [5, 6, 10, 7]], dtype=tf.int32)\n    boxes2 = tf.constant([[3, 4, 6, 8], [14, 14, 15, 15]], dtype=tf.int32)\n    expected_result = tf.constant(\n        [1.07500000298023224, 1.9333333373069763], dtype=tf.float32\n    )\n    loss = giou_loss(boxes1, boxes2)\n    test_utils.assert_allclose_according_to_type(loss, expected_result)\n'"
tensorflow_addons/losses/tests/kappa_loss_test.py,1,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Weighted Kappa Loss.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.losses.kappa_loss import WeightedKappaLoss\n\n\ndef weighted_kappa_loss_np(y_true, y_pred, weightage=""quadratic"", eps=1e-6):\n    num_samples, num_classes = y_true.shape\n    cat_labels = y_true.argmax(axis=1).reshape((-1, 1))\n    label_mat = np.tile(cat_labels, (1, num_classes))\n    row_label_vec = np.arange(num_classes).reshape((1, num_classes))\n    label_mat_ = np.tile(row_label_vec, (num_samples, 1))\n    if weightage == ""linear"":\n        weight = np.abs(label_mat - label_mat_)\n    else:\n        weight = (label_mat - label_mat_) ** 2\n    numerator = (y_pred * weight).sum()\n    label_dist = y_true.sum(axis=0, keepdims=True)\n    pred_dist = y_pred.sum(axis=0, keepdims=True)\n\n    col_label_vec = row_label_vec.T\n    row_mat = np.tile(row_label_vec, (num_classes, 1))\n    col_mat = np.tile(col_label_vec, (1, num_classes))\n    if weightage == ""quadratic"":\n        weight_ = (col_mat - row_mat) ** 2\n    else:\n        weight_ = np.abs(col_mat - row_mat)\n    weighted_pred_dist = np.matmul(weight_, pred_dist.T)\n    denominator = np.matmul(label_dist, weighted_pred_dist).sum()\n    denominator /= num_samples\n    return np.log(np.nan_to_num(numerator / denominator) + eps)\n\n\ndef gen_labels_and_preds(num_samples, num_classes, seed):\n    np.random.seed(seed)\n    rands = np.random.uniform(size=(num_samples, num_classes))\n    cat_labels = rands.argmax(axis=1)\n    y_true = np.eye(num_classes, dtype=""int"")[cat_labels]\n    y_pred = np.random.uniform(size=(num_samples, num_classes))\n    y_pred /= y_pred.sum(axis=1, keepdims=True)\n    return y_true, y_pred\n\n\n@pytest.mark.parametrize(""np_seed"", [0, 1, 2, 3])\ndef test_linear_weighted_kappa_loss(np_seed):\n    y_true, y_pred = gen_labels_and_preds(50, 4, np_seed)\n    kappa_loss = WeightedKappaLoss(num_classes=4, weightage=""linear"")\n    y_pred = y_pred.astype(kappa_loss.dtype.as_numpy_dtype)\n    loss = kappa_loss(y_true, y_pred)\n    loss_np = weighted_kappa_loss_np(y_true, y_pred, weightage=""linear"")\n    np.testing.assert_allclose(loss, loss_np, rtol=1e-5, atol=1e-5)\n\n\n@pytest.mark.parametrize(""np_seed"", [0, 1, 2, 3])\ndef test_quadratic_weighted_kappa_loss(np_seed):\n    y_true, y_pred = gen_labels_and_preds(100, 3, np_seed)\n    kappa_loss = WeightedKappaLoss(num_classes=3)\n    y_pred = y_pred.astype(kappa_loss.dtype.as_numpy_dtype)\n    loss = kappa_loss(y_true, y_pred)\n    loss_np = weighted_kappa_loss_np(y_true, y_pred)\n    np.testing.assert_allclose(loss, loss_np, rtol=1e-5, atol=1e-5)\n\n\ndef test_config():\n    kappa_loss = WeightedKappaLoss(\n        num_classes=4, weightage=""linear"", name=""kappa_loss"", epsilon=0.001,\n    )\n    assert kappa_loss.num_classes == 4\n    assert kappa_loss.weightage == ""linear""\n    assert kappa_loss.name == ""kappa_loss""\n    np.testing.assert_allclose(kappa_loss.epsilon, 0.001, 1e-6)\n\n\ndef test_serialization():\n    loss = WeightedKappaLoss(num_classes=3)\n    tf.keras.losses.deserialize(tf.keras.losses.serialize(loss))\n'"
tensorflow_addons/losses/tests/lifted_test.py,6,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for lifted loss.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.losses import lifted\nfrom tensorflow_addons.utils import test_utils\n\n\ndef pairwise_distance_np(feature, squared=False):\n    """"""Computes the pairwise distance matrix in numpy.\n\n    Args:\n      feature: 2-D numpy array of size [number of data, feature dimension]\n      squared: Boolean. If true, output is the pairwise squared euclidean\n        distance matrix; else, output is the pairwise euclidean distance\n        matrix.\n\n    Returns:\n      pairwise_distances: 2-D numpy array of size\n        [number of data, number of data].\n    """"""\n    triu = np.triu_indices(feature.shape[0], 1)\n    upper_tri_pdists = np.linalg.norm(feature[triu[1]] - feature[triu[0]], axis=1)\n    if squared:\n        upper_tri_pdists **= 2.0\n    num_data = feature.shape[0]\n    pairwise_distances = np.zeros((num_data, num_data))\n    pairwise_distances[np.triu_indices(num_data, 1)] = upper_tri_pdists\n    # Make symmetrical.\n    pairwise_distances = (\n        pairwise_distances\n        + pairwise_distances.T\n        - np.diag(pairwise_distances.diagonal())\n    )\n    return pairwise_distances\n\n\ndef lifted_struct_loss_np(labels, embedding, margin):\n\n    num_data = embedding.shape[0]\n    # Reshape labels to compute adjacency matrix.\n    labels_reshaped = np.reshape(labels, (labels.shape[0], 1))\n\n    adjacency = np.equal(labels_reshaped, labels_reshaped.T)\n    pdist_matrix = pairwise_distance_np(embedding)\n    loss_np = 0.0\n    num_constraints = 0.0\n    for i in range(num_data):\n        for j in range(num_data):\n            if adjacency[i][j] > 0.0 and i != j:\n                d_pos = pdist_matrix[i][j]\n                negs = []\n                for k in range(num_data):\n                    if not adjacency[i][k]:\n                        negs.append(margin - pdist_matrix[i][k])\n                for l in range(num_data):\n                    if not adjacency[j][l]:\n                        negs.append(margin - pdist_matrix[j][l])\n\n                negs = np.array(negs)\n                max_elem = np.max(negs)\n                negs -= max_elem\n                negs = np.exp(negs)\n                soft_maximum = np.log(np.sum(negs)) + max_elem\n\n                num_constraints += 1.0\n                this_loss = max(soft_maximum + d_pos, 0)\n                loss_np += this_loss * this_loss\n\n    loss_np = loss_np / num_constraints / 2.0\n    return loss_np\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [tf.float32, tf.float16, tf.bfloat16])\ndef test_lifted_struct(dtype):\n    num_data = 10\n    feat_dim = 6\n    margin = 1.0\n    num_classes = 4\n\n    embedding = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=num_data).astype(np.float32)\n\n    # Compute the loss in NP\n    loss_np = lifted_struct_loss_np(labels, embedding, margin)\n\n    # Compute the loss in TF.\n    y_true = tf.constant(labels)\n    y_pred = tf.constant(embedding, dtype=dtype)\n    cce_obj = lifted.LiftedStructLoss()\n    loss = cce_obj(y_true, y_pred)\n    test_utils.assert_allclose_according_to_type(loss.numpy(), loss_np)\n\n\ndef test_keras_model_compile():\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Input(shape=(784,)), tf.keras.layers.Dense(10),]\n    )\n    model.compile(loss=""Addons>lifted_struct_loss"", optimizer=""adam"")\n\n\ndef test_serialization():\n    loss = lifted.LiftedStructLoss()\n    tf.keras.losses.deserialize(tf.keras.losses.serialize(loss))\n'"
tensorflow_addons/losses/tests/metric_test.py,5,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for metric learning.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.losses.metric_learning import pairwise_distance\n\n\ndef test_zero_distance():\n    """"""Test that equal embeddings have a pairwise distance of 0.""""""\n    equal_embeddings = tf.constant([[1.0, 0.5], [1.0, 0.5]])\n\n    distances = pairwise_distance(equal_embeddings, squared=False)\n    np.testing.assert_allclose(tf.math.reduce_sum(distances), 0, 1e-6, 1e-6)\n\n\ndef test_positive_distances():\n    """"""Test that the pairwise distances are always positive.""""""\n\n    # Create embeddings very close to each other in [1.0 - 2e-7, 1.0 + 2e-7]\n    # This will encourage errors in the computation\n    embeddings = 1.0 + 2e-7 * tf.random.uniform([64, 6], dtype=tf.float32)\n    distances = pairwise_distance(embeddings, squared=False)\n    assert np.all(distances >= 0)\n\n\ndef test_correct_distance():\n    """"""Compare against numpy caluclation.""""""\n    tf_embeddings = tf.constant([[0.5, 0.5], [1.0, 1.0]])\n\n    expected_distance = np.array([[0, np.sqrt(2) / 2], [np.sqrt(2) / 2, 0]])\n\n    distances = pairwise_distance(tf_embeddings, squared=False)\n    np.testing.assert_allclose(expected_distance, distances, 1e-6, 1e-6)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_correct_distance_squared():\n    """"""Compare against numpy caluclation for squared distances.""""""\n    tf_embeddings = tf.constant([[0.5, 0.5], [1.0, 1.0]])\n\n    expected_distance = np.array([[0, 0.5], [0.5, 0]])\n\n    distances = pairwise_distance(tf_embeddings, squared=True)\n    np.testing.assert_allclose(expected_distance, distances, 1e-6, 1e-6)\n'"
tensorflow_addons/losses/tests/npairs_test.py,24,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for npairs loss.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.losses import npairs\n\n\ndef test_config():\n    nl_obj = npairs.NpairsLoss(name=""nl"")\n    assert nl_obj.name == ""nl""\n    assert nl_obj.reduction == tf.keras.losses.Reduction.NONE\n\n\ndef test_unweighted():\n    nl_obj = npairs.NpairsLoss()\n    # batch size = 4, hidden size = 2\n    y_true = tf.constant([0, 1, 2, 3], dtype=tf.int64)\n    # features of anchors\n    f = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # features of positive samples\n    fp = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # similarity matrix\n    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)\n    loss = nl_obj(y_true, y_pred)\n\n    # Loss = 1/4 * \\sum_i log(1 + \\sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))\n    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4\n    # i = 0 => log(1 + sum([exp(-2), exp(-2), exp(-4)])) = 0.253846\n    # i = 1 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846\n    # i = 2 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846\n    # i = 3 => log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 0.253846\n    # Loss = (0.253856 + 0.253856 + 0.253856 + 0.253856) / 4 = 0.253856\n\n    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)\n\n\ndef config():\n    nml_obj = npairs.NpairsMultilabelLoss(name=""nml"")\n    assert nml_obj.name == ""nml""\n    assert nml_obj.reduction == tf.keras.losses.Reduction.NONE\n\n\ndef test_single_label():\n    """"""Test single label, which is the same with `NpairsLoss`.""""""\n    nml_obj = npairs.NpairsMultilabelLoss()\n    # batch size = 4, hidden size = 2\n    y_true = tf.constant(\n        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=tf.int64\n    )\n    # features of anchors\n    f = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # features of positive samples\n    fp = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # similarity matrix\n    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)\n    loss = nml_obj(y_true, y_pred)\n\n    # Loss = 1/4 * \\sum_i log(1 + \\sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))\n    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4\n    # i = 0 => log(1 + sum([exp(-2), exp(-2), exp(-4)])) = 0.253846\n    # i = 1 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846\n    # i = 2 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846\n    # i = 3 => log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 0.253846\n    # Loss = (0.253856 + 0.253856 + 0.253856 + 0.253856) / 4 = 0.253856\n\n    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)\n\n    # Test sparse tensor\n    y_true = tf.sparse.from_dense(y_true)\n    loss = nml_obj(y_true, y_pred)\n    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)\n\n\ndef test_multilabel():\n    nml_obj = npairs.NpairsMultilabelLoss()\n    # batch size = 4, hidden size = 2\n    y_true = tf.constant(\n        [[1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1], [0, 0, 0, 1]], dtype=tf.int64\n    )\n    # features of anchors\n    f = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # features of positive samples\n    fp = tf.constant(\n        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32\n    )\n    # similarity matrix\n    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)\n    loss = nml_obj(y_true, y_pred)\n\n    # Loss = \\sum_i log(1 + \\sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))\n    # Because of multilabel, the label matrix is normalized so that each\n    # row sums to one. That\'s why the multiplier before log exists.\n    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4\n    # i = 0 => 2/3 * log(1 + sum([exp(-2), exp(-2), exp(-4)])) +\n    #          1/3 * log(1 + sum([exp(2) , exp(0) , exp(-2)])) = 0.920522\n    # i = 1 => 1/4 * log(1 + sum([exp(2) , exp(-2), exp(0) ])) +\n    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +\n    #          1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) = 1.753856\n    # i = 2 => 1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) +\n    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +\n    #          1/4 * log(1 + sum([exp(0) , exp(-2), exp(2) ])) = 1.753856\n    # i = 4 => 1/2 * log(1 + sum([exp(-2), exp(0) , exp(2) ])) +\n    #          1/2 * log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 1.253856\n    # Loss = (0.920522 + 1.753856 + 1.753856 + 1.253856) / 4 = 1.420522\n\n    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)\n\n    # Test sparse tensor\n    y_true = tf.sparse.from_dense(y_true)\n    loss = nml_obj(y_true, y_pred)\n    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)\n'"
tensorflow_addons/losses/tests/quantiles_test.py,34,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for pinball loss.""""""\n\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.losses import quantiles\nfrom distutils.version import LooseVersion\n\n\ndef test_config():\n    pin_obj = quantiles.PinballLoss(\n        reduction=tf.keras.losses.Reduction.SUM, name=""pin_1""\n    )\n    assert pin_obj.name == ""pin_1""\n    assert pin_obj.reduction == tf.keras.losses.Reduction.SUM\n\n\ndef test_all_correct_unweighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n    loss = pin_obj(y_true, y_true)\n    assert loss == 0\n\n\ndef test_unweighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred)\n    np.testing.assert_almost_equal(loss, 2.75, 3)\n\n\ndef test_scalar_weighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred, sample_weight=2.3)\n    np.testing.assert_almost_equal(loss, 6.325, 3)\n\n\ndef test_sample_weighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n    loss = pin_obj(y_true, y_pred, sample_weight=sample_weight)\n    np.testing.assert_almost_equal(loss, 40.7 / 6, 3)\n\n\ndef test_timestep_weighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1), dtype=tf.dtypes.float32)\n    sample_weight = tf.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n    loss = pin_obj(y_true, y_pred, sample_weight=sample_weight)\n    np.testing.assert_almost_equal(loss, 41.5 / 6, 3)\n\n\ndef test_zero_weighted():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred, sample_weight=0)\n    np.testing.assert_almost_equal(loss, 0.0, 3)\n\n\ndef test_invalid_sample_weight():\n    pin_obj = quantiles.PinballLoss()\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n    sample_weight = tf.constant([3, 6, 5, 0], shape=(2, 2))\n    if LooseVersion(tf.__version__) >= ""2.2"":\n        with pytest.raises(tf.errors.InvalidArgumentError, match=""Incompatible shapes""):\n            pin_obj(y_true, y_pred, sample_weight=sample_weight)\n    else:\n        with pytest.raises(ValueError, match=""weights can not be broadcast to values""):\n            pin_obj(y_true, y_pred, sample_weight=sample_weight)\n\n\ndef test_unweighted_quantile_0pc():\n    pin_obj = quantiles.PinballLoss(tau=0.0)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred)\n    np.testing.assert_almost_equal(loss, 4.8333, 3)\n\n\ndef test_unweighted_quantile_10pc():\n    pin_obj = quantiles.PinballLoss(tau=0.1)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred)\n    np.testing.assert_almost_equal(loss, 4.4166, 3)\n\n\ndef test_unweighted_quantile_90pc():\n    pin_obj = quantiles.PinballLoss(tau=0.9)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred)\n    np.testing.assert_almost_equal(loss, 1.0833, 3)\n\n\ndef test_unweighted_quantile_100pc():\n    pin_obj = quantiles.PinballLoss(tau=1.0)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred)\n    np.testing.assert_almost_equal(loss, 0.6666, 3)\n\n\ndef test_no_reduction():\n    pin_obj = quantiles.PinballLoss(reduction=tf.keras.losses.Reduction.NONE)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred, sample_weight=2.3)\n    np.testing.assert_almost_equal(loss, [5.3666, 7.28333], 1e-3)\n\n\ndef test_sum_reduction():\n    pin_obj = quantiles.PinballLoss(reduction=tf.keras.losses.Reduction.SUM)\n    y_true = tf.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n    y_pred = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3), dtype=tf.dtypes.float32)\n    loss = pin_obj(y_true, y_pred, sample_weight=2.3)\n    np.testing.assert_almost_equal(loss, 12.65, 3)\n'"
tensorflow_addons/losses/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/losses/tests/sparsemax_loss_test.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport pytest\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow_addons.activations import sparsemax\nfrom tensorflow_addons.losses import sparsemax_loss, SparsemaxLoss\nfrom tensorflow_addons.utils import test_utils\n\ntest_obs = 17\n\n\ndef _np_sparsemax(z):\n    z = z - np.mean(z, axis=1)[:, np.newaxis]\n\n    # sort z\n    z_sorted = np.sort(z, axis=1)[:, ::-1]\n\n    # calculate k(z)\n    z_cumsum = np.cumsum(z_sorted, axis=1)\n    k = np.arange(1, z.shape[1] + 1)\n    z_check = 1 + k * z_sorted > z_cumsum\n    # use argmax to get the index by row as .nonzero() doesn\'t\n    # take an axis argument. np.argmax return the first index, but the last\n    # index is required here, use np.flip to get the last index and\n    # `z.shape[axis]` to compensate for np.flip afterwards.\n    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)\n\n    # calculate tau(z)\n    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]\n    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)\n\n    # calculate p\n    return np.maximum(0, z - tau_z)\n\n\ndef _np_sparsemax_loss(z, q):\n    z = z - np.mean(z, axis=1)[:, np.newaxis]\n\n    # Calculate q^T * z\n    z_k = np.sum(q * z, axis=1)\n\n    # calculate sum over S(z)\n    p = _np_sparsemax(z)\n    s = p > 0\n    # z_i^2 - tau(z)^2 = p_i (2 * z_i - p_i) for i \\in S(z)\n    s_sum = np.sum(s * p * (2 * z - p), axis=1)\n\n    # because q is binary, sum([q_1^2, q_2^2, ...]) is just sum(q)\n    q_norm = np.sum(q, axis=1)\n\n    return 0.5 * s_sum + 0.5 * q_norm - z_k\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_constructor_aginst_numpy(dtype):\n    """"""check sparsemax-loss construcor against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    q = np.zeros((test_obs, 10))\n    q[np.arange(0, test_obs), random.randint(0, 10, size=test_obs)] = 1\n\n    loss_object = SparsemaxLoss()\n    tf_loss_op = loss_object(q, z)\n    np_loss = np.mean(_np_sparsemax_loss(z, q).astype(dtype))\n\n    test_utils.assert_allclose_according_to_type(np_loss, tf_loss_op)\n    assert np_loss.shape == tf_loss_op.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_gradient_against_estimate(dtype):\n    """"""check sparsemax-loss Rop, against estimated-loss Rop.""""""\n    random = np.random.RandomState(7)\n\n    # sparsemax is not a smooth function so gradient estimation is only\n    # possible for float64.\n    if dtype != ""float64"":\n        return\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10)).astype(dtype)\n    q = np.zeros((test_obs, 10)).astype(dtype)\n    q[np.arange(0, test_obs), np.random.randint(0, 10, size=test_obs)] = 1\n\n    (jacob_sym,), (jacob_num,) = tf.test.compute_gradient(\n        lambda logits: sparsemax_loss(logits, sparsemax(logits), q), [z]\n    )\n    test_utils.assert_allclose_according_to_type(jacob_sym, jacob_num)\n\n\ndef _tf_sparsemax_loss(z, q, dtype):\n    z = z.astype(dtype)\n    q = q.astype(dtype)\n\n    tf_sparsemax_op = sparsemax(z)\n    tf_loss_op = sparsemax_loss(z, tf_sparsemax_op, q)\n    tf_loss_out = tf_loss_op\n\n    return tf_loss_op, tf_loss_out\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_positive(dtype):\n    """"""check sparsemax-loss proposition 4.""""""\n    random = np.random.RandomState(5)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    q = np.zeros((test_obs, 10))\n    q[np.arange(0, test_obs), random.randint(0, 10, size=test_obs)] = 1\n\n    tf_loss_op, tf_loss_out = _tf_sparsemax_loss(z, q, dtype)\n\n    test_utils.assert_allclose_according_to_type(np.abs(tf_loss_out), tf_loss_out)\n    assert np.zeros(test_obs).shape == tf_loss_op.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_constructor_not_from_logits(dtype):\n    """"""check sparsemax-loss construcor throws when from_logits=True.""""""\n    with pytest.raises(ValueError):\n        SparsemaxLoss(from_logits=False)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_against_numpy(dtype):\n    """"""check sparsemax-loss kernel against numpy.""""""\n    random = np.random.RandomState(1)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    q = np.zeros((test_obs, 10))\n    q[np.arange(0, test_obs), random.randint(0, 10, size=test_obs)] = 1\n\n    tf_loss_op, tf_loss_out = _tf_sparsemax_loss(z, q, dtype)\n    np_loss = _np_sparsemax_loss(z, q).astype(dtype)\n\n    test_utils.assert_allclose_according_to_type(np_loss, tf_loss_out)\n    assert np_loss.shape == tf_loss_op.shape\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_of_nan(dtype):\n    """"""check sparsemax-loss transfers nan.""""""\n    q = np.asarray([[0, 0, 1], [0, 0, 1], [0, 0, 1]])\n    z_nan = np.asarray(\n        [[0, np.nan, 0], [0, np.nan, np.nan], [np.nan, np.nan, np.nan]]\n    ).astype(dtype)\n\n    _, tf_loss_nan = _tf_sparsemax_loss(z_nan, q, dtype)\n    np.testing.assert_equal(np.asanyarray([np.nan, np.nan, np.nan]), tf_loss_nan)\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_of_inf(dtype):\n    """"""check sparsemax-loss is infinity safe.""""""\n    q = np.asarray([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]])\n    z_neg = np.asarray(\n        [\n            [0, -np.inf, 0],\n            [0, -np.inf, -np.inf],\n            [-np.inf, -np.inf, 0],\n            [-np.inf, -np.inf, -np.inf],\n        ]\n    ).astype(dtype)\n    z_pos = np.asarray(\n        [\n            [0, np.inf, 0],\n            [0, np.inf, np.inf],\n            [np.inf, np.inf, 0],\n            [np.inf, np.inf, np.inf],\n        ]\n    ).astype(dtype)\n    z_mix = np.asarray(\n        [\n            [0, np.inf, 0],\n            [0, np.inf, -np.inf],\n            [-np.inf, np.inf, 0],\n            [-np.inf, np.inf, -np.inf],\n        ]\n    ).astype(dtype)\n\n    _, tf_loss_neg = _tf_sparsemax_loss(z_neg, q, dtype)\n    np.testing.assert_equal(np.asanyarray([0.25, np.inf, 0, np.nan]), tf_loss_neg)\n\n    _, tf_loss_pos = _tf_sparsemax_loss(z_pos, q, dtype)\n    np.testing.assert_equal(\n        np.asanyarray([np.nan, np.nan, np.nan, np.nan]), tf_loss_pos\n    )\n\n    _, tf_loss_mix = _tf_sparsemax_loss(z_mix, q, dtype)\n    np.testing.assert_equal(\n        np.asanyarray([np.nan, np.nan, np.nan, np.nan]), tf_loss_mix\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_constant_add(dtype):\n    """"""check sparsemax-loss proposition 3.""""""\n    random = np.random.RandomState(4)\n\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    c = random.uniform(low=-3, high=3, size=(test_obs, 1))\n    q = np.zeros((test_obs, 10))\n    q[np.arange(0, test_obs), np.random.randint(0, 10, size=test_obs)] = 1\n\n    _, tf_loss_zpc = _tf_sparsemax_loss(z + c, q, dtype)\n    _, tf_loss_z = _tf_sparsemax_loss(z, q, dtype)\n\n    test_utils.assert_allclose_according_to_type(\n        tf_loss_zpc, tf_loss_z, float_atol=5e-6, float_rtol=5e-6\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [""float32"", ""float64""])\ndef test_sparsemax_loss_zero(dtype):\n    """"""check sparsemax-loss proposition 5.""""""\n    random = np.random.RandomState(6)\n\n    # construct z and q, such that z_k >= 1 + max_{j!=k} z_k holds for\n    # delta_0 = 1.\n    z = random.uniform(low=-3, high=3, size=(test_obs, 10))\n    z[:, 0] = np.max(z, axis=1) + 1.05\n\n    q = np.zeros((test_obs, 10))\n    q[:, 0] = 1\n\n    tf_loss_op, tf_loss_out = _tf_sparsemax_loss(z, q, dtype)\n\n    tf_sparsemax_op = sparsemax(z.astype(dtype))\n\n    test_utils.assert_allclose_according_to_type(np.zeros(test_obs), tf_loss_out)\n    assert np.zeros(test_obs).shape == tf_loss_op.shape\n\n    test_utils.assert_allclose_according_to_type(q, tf_sparsemax_op)\n    assert q.shape == tf_sparsemax_op.shape\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_serialization():\n    ref_fn = sparsemax_loss\n    config = tf.keras.losses.serialize(ref_fn)\n    fn = tf.keras.losses.deserialize(config)\n    assert ref_fn == fn\n'"
tensorflow_addons/losses/tests/triplet_test.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for triplet loss.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.losses import triplet\nfrom tensorflow_addons.utils import test_utils\n\n\ndef pairwise_distance_np(feature, squared=False):\n    """"""Computes the pairwise distance matrix in numpy.\n\n    Args:\n      feature: 2-D numpy array of size [number of data, feature dimension]\n      squared: Boolean. If true, output is the pairwise squared euclidean\n        distance matrix; else, output is the pairwise euclidean distance\n        matrix.\n\n    Returns:\n      pairwise_distances: 2-D numpy array of size\n        [number of data, number of data].\n    """"""\n    triu = np.triu_indices(feature.shape[0], 1)\n    upper_tri_pdists = np.linalg.norm(feature[triu[1]] - feature[triu[0]], axis=1)\n    if squared:\n        upper_tri_pdists **= 2.0\n    num_data = feature.shape[0]\n    pairwise_distances = np.zeros((num_data, num_data))\n    pairwise_distances[np.triu_indices(num_data, 1)] = upper_tri_pdists\n    # Make symmetrical.\n    pairwise_distances = (\n        pairwise_distances\n        + pairwise_distances.T\n        - np.diag(pairwise_distances.diagonal())\n    )\n    return pairwise_distances\n\n\ndef l_2_dists(embs):\n    return pairwise_distance_np(embs, True)\n\n\ndef l_1_dists(embs):\n    return pairwise_distance_np(embs, False)\n\n\ndef angular_distance_np(feature):\n    """"""Computes the angular distance matrix in numpy.\n    Args:\n      feature: 2-D numpy array of size [number of data, feature dimension]\n    Returns:\n      angular_distances: 2-D numpy array of size\n        [number of data, number of data].\n    """"""\n\n    # l2-normalize all features\n    normed = feature / np.linalg.norm(feature, ord=2, axis=1, keepdims=True)\n    cosine_similarity = normed @ normed.T\n    inverse_cos_sim = 1 - cosine_similarity\n\n    return inverse_cos_sim\n\n\ndef triplet_semihard_loss_np(labels, embedding, margin, dist_func):\n\n    num_data = embedding.shape[0]\n    # Reshape labels to compute adjacency matrix.\n    labels_reshaped = np.reshape(labels.astype(np.float32), (labels.shape[0], 1))\n\n    adjacency = np.equal(labels_reshaped, labels_reshaped.T)\n    pdist_matrix = dist_func(embedding)\n    loss_np = 0.0\n    num_positives = 0.0\n    for i in range(num_data):\n        for j in range(num_data):\n            if adjacency[i][j] > 0.0 and i != j:\n                num_positives += 1.0\n\n                pos_distance = pdist_matrix[i][j]\n                neg_distances = []\n\n                for k in range(num_data):\n                    if adjacency[i][k] == 0:\n                        neg_distances.append(pdist_matrix[i][k])\n\n                # Sort by distance.\n                neg_distances.sort()\n                chosen_neg_distance = neg_distances[0]\n\n                for l in range(len(neg_distances)):\n                    chosen_neg_distance = neg_distances[l]\n                    if chosen_neg_distance > pos_distance:\n                        break\n\n                loss_np += np.maximum(0.0, margin - chosen_neg_distance + pos_distance)\n\n    loss_np /= num_positives\n    return loss_np\n\n\ndef triplet_hard_loss_np(labels, embedding, margin, dist_func, soft=False):\n\n    num_data = embedding.shape[0]\n    # Reshape labels to compute adjacency matrix.\n    labels_reshaped = np.reshape(labels.astype(np.float32), (labels.shape[0], 1))\n\n    adjacency = np.equal(labels_reshaped, labels_reshaped.T)\n    pdist_matrix = dist_func(embedding)\n    loss_np = 0.0\n    for i in range(num_data):\n        pos_distances = []\n        neg_distances = []\n        for j in range(num_data):\n            if adjacency[i][j] == 0:\n                neg_distances.append(pdist_matrix[i][j])\n            if adjacency[i][j] > 0.0 and i != j:\n                pos_distances.append(pdist_matrix[i][j])\n\n        # if there are no positive pairs, distance is 0\n        if len(pos_distances) == 0:\n            pos_distances.append(0)\n\n        # Sort by distance.\n        neg_distances.sort()\n        min_neg_distance = neg_distances[0]\n        pos_distances.sort(reverse=True)\n        max_pos_distance = pos_distances[0]\n\n        if soft:\n            loss_np += np.log1p(np.exp(max_pos_distance - min_neg_distance))\n        else:\n            loss_np += np.maximum(0.0, max_pos_distance - min_neg_distance + margin)\n\n    loss_np /= num_data\n    return loss_np\n\n\n# triplet semihard\n@pytest.mark.parametrize(""dtype"", [tf.float32, tf.float16, tf.bfloat16])\n@pytest.mark.parametrize(\n    ""dist_func, dist_metric"",\n    [(angular_distance_np, ""angular""), (l_2_dists, ""L2""), (l_1_dists, ""L1"")],\n)\ndef test_semihard_tripled_loss_angular(dtype, dist_func, dist_metric):\n    num_data = 10\n    feat_dim = 6\n    margin = 1.0\n    num_classes = 4\n\n    embedding = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=(num_data))\n\n    # Compute the loss in NP.\n    loss_np = triplet_semihard_loss_np(labels, embedding, margin, dist_func)\n\n    # Compute the loss in TF.\n    y_true = tf.constant(labels)\n    y_pred = tf.constant(embedding, dtype=dtype)\n    cce_obj = triplet.TripletSemiHardLoss(distance_metric=dist_metric)\n    loss = cce_obj(y_true, y_pred)\n    test_utils.assert_allclose_according_to_type(loss.numpy(), loss_np)\n\n\ndef test_keras_model_compile_semihard():\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Input(shape=(784,)), tf.keras.layers.Dense(10),]\n    )\n    model.compile(loss=""Addons>triplet_semihard_loss"", optimizer=""adam"")\n\n\ndef test_serialization_semihard():\n    loss = triplet.TripletSemiHardLoss()\n    tf.keras.losses.deserialize(tf.keras.losses.serialize(loss))\n\n\n# test cosine similarity\n@pytest.mark.parametrize(""dtype"", [tf.float32, tf.float16, tf.bfloat16])\n@pytest.mark.parametrize(""soft"", [False, True])\n@pytest.mark.parametrize(\n    ""dist_func, dist_metric"",\n    [(angular_distance_np, ""angular""), (l_2_dists, ""L2""), (l_1_dists, ""L1"")],\n)\ndef test_hard_tripled_loss_angular(dtype, soft, dist_func, dist_metric):\n    num_data = 20\n    feat_dim = 6\n    margin = 1.0\n    num_classes = 4\n\n    embedding = np.random.rand(num_data, feat_dim).astype(np.float32)\n    labels = np.random.randint(0, num_classes, size=(num_data))\n\n    # Compute the loss in NP.\n    loss_np = triplet_hard_loss_np(labels, embedding, margin, dist_func, soft)\n\n    # Compute the loss in TF.\n    y_true = tf.constant(labels)\n    y_pred = tf.constant(embedding, dtype=dtype)\n    cce_obj = triplet.TripletHardLoss(soft=soft, distance_metric=dist_metric)\n    loss = cce_obj(y_true, y_pred)\n    test_utils.assert_allclose_according_to_type(loss.numpy(), loss_np)\n\n\ndef test_keras_model_compile_hard():\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Input(shape=(784,)), tf.keras.layers.Dense(10),]\n    )\n    model.compile(loss=""Addons>triplet_hard_loss"", optimizer=""adam"")\n\n\ndef test_serialization_hard():\n    loss = triplet.TripletHardLoss()\n    tf.keras.losses.deserialize(tf.keras.losses.serialize(loss))\n'"
tensorflow_addons/metrics/tests/__init__.py,0,b''
tensorflow_addons/metrics/tests/cohens_kappa_test.py,27,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Cohen\'s Kappa Metric.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow_addons.metrics import CohenKappa\nfrom tensorflow_addons.testing.serialization import check_metric_serialization\n\n\ndef test_config():\n    kp_obj = CohenKappa(name=""cohen_kappa"", num_classes=5)\n    assert kp_obj.name == ""cohen_kappa""\n    assert kp_obj.dtype == tf.float32\n    assert kp_obj.num_classes == 5\n\n    # Check save and restore config\n    kb_obj2 = CohenKappa.from_config(kp_obj.get_config())\n    assert kb_obj2.name == ""cohen_kappa""\n    assert kb_obj2.dtype == tf.float32\n    assert kp_obj.num_classes == 5\n\n\ndef initialize_vars():\n    kp_obj1 = CohenKappa(num_classes=5, sparse_labels=True)\n    kp_obj2 = CohenKappa(num_classes=5, sparse_labels=True, weightage=""linear"")\n    kp_obj3 = CohenKappa(num_classes=5, sparse_labels=True, weightage=""quadratic"")\n\n    return kp_obj1, kp_obj2, kp_obj3\n\n\ndef update_obj_states(obj1, obj2, obj3, actuals, preds, weights):\n    obj1.update_state(actuals, preds, sample_weight=weights)\n    obj2.update_state(actuals, preds, sample_weight=weights)\n    obj3.update_state(actuals, preds, sample_weight=weights)\n\n\ndef reset_obj_states(obj1, obj2, obj3):\n    obj1.reset_states()\n    obj2.reset_states()\n    obj3.reset_states()\n\n\ndef check_results(objs, values):\n    obj1, obj2, obj3 = objs\n    val1, val2, val3 = values\n\n    np.testing.assert_allclose(val1, obj1.result(), atol=1e-5)\n    np.testing.assert_allclose(val2, obj2.result(), atol=1e-5)\n    np.testing.assert_allclose(val3, obj3.result(), atol=1e-5)\n\n\ndef test_kappa_random_score():\n    actuals = [4, 4, 3, 4, 2, 4, 1, 1]\n    preds = [4, 4, 3, 4, 4, 2, 1, 1]\n    actuals = tf.constant(actuals, dtype=tf.int32)\n    preds = tf.constant(preds, dtype=tf.int32)\n\n    # Initialize\n    kp_obj1, kp_obj2, kp_obj3 = initialize_vars()\n\n    # Update\n    update_obj_states(kp_obj1, kp_obj2, kp_obj3, actuals, preds, None)\n\n    # Check results\n    check_results([kp_obj1, kp_obj2, kp_obj3], [0.61904761, 0.62790697, 0.68932038])\n\n\ndef test_kappa_perfect_score():\n    actuals = [4, 4, 3, 3, 2, 2, 1, 1]\n    preds = [4, 4, 3, 3, 2, 2, 1, 1]\n    actuals = tf.constant(actuals, dtype=tf.int32)\n    preds = tf.constant(preds, dtype=tf.int32)\n\n    # Initialize\n    kp_obj1, kp_obj2, kp_obj3 = initialize_vars()\n\n    # Update\n    update_obj_states(kp_obj1, kp_obj2, kp_obj3, actuals, preds, None)\n\n    # Check results\n    check_results([kp_obj1, kp_obj2, kp_obj3], [1.0, 1.0, 1.0])\n\n\ndef test_kappa_worse_than_random():\n    actuals = [4, 4, 3, 3, 2, 2, 1, 1]\n    preds = [1, 2, 4, 1, 3, 3, 4, 4]\n    actuals = tf.constant(actuals, dtype=tf.int32)\n    preds = tf.constant(preds, dtype=tf.int32)\n\n    # Initialize\n    kp_obj1, kp_obj2, kp_obj3 = initialize_vars()\n\n    # Update\n    update_obj_states(kp_obj1, kp_obj2, kp_obj3, actuals, preds, None)\n\n    # check results\n    check_results([kp_obj1, kp_obj2, kp_obj3], [-0.3333333, -0.52380952, -0.72727272])\n\n\ndef test_kappa_with_sample_weights():\n    actuals = [4, 4, 3, 3, 2, 2, 1, 1]\n    preds = [1, 2, 4, 1, 3, 3, 4, 4]\n    weights = [1, 1, 2, 5, 10, 2, 3, 3]\n    actuals = tf.constant(actuals, dtype=tf.int32)\n    preds = tf.constant(preds, dtype=tf.int32)\n    weights = tf.constant(weights, dtype=tf.int32)\n\n    # Initialize\n    kp_obj1, kp_obj2, kp_obj3 = initialize_vars()\n\n    # Update\n    update_obj_states(kp_obj1, kp_obj2, kp_obj3, actuals, preds, weights)\n\n    # check results\n    check_results([kp_obj1, kp_obj2, kp_obj3], [-0.25473321, -0.38992332, -0.60695344])\n\n\ndef test_kappa_reset_states():\n    # Initialize\n    kp_obj1, kp_obj2, kp_obj3 = initialize_vars()\n\n    # reset states\n    reset_obj_states(kp_obj1, kp_obj2, kp_obj3)\n\n    # check results\n    check_results([kp_obj1, kp_obj2, kp_obj3], [0.0, 0.0, 0.0])\n\n\ndef test_large_values():\n    y_true = [1] * 10000 + [0] * 20000 + [1] * 20000\n    y_pred = [0] * 20000 + [1] * 30000\n\n    y_true = tf.convert_to_tensor(y_true)\n    y_pred = tf.convert_to_tensor(y_pred)\n\n    obj = CohenKappa(num_classes=2)\n\n    obj.update_state(y_true, y_pred)\n    np.testing.assert_allclose(0.166666666, obj.result(), 1e-6, 1e-6)\n\n\ndef test_with_sparse_labels():\n    y_true = np.array([4, 4, 3, 4], dtype=np.int32)\n    y_pred = np.array([4, 4, 1, 2], dtype=np.int32)\n\n    obj = CohenKappa(num_classes=5, sparse_labels=True)\n\n    obj.update_state(y_true, y_pred)\n    np.testing.assert_allclose(0.19999999, obj.result())\n\n\ndef test_keras_binary_reg_model():\n    kp = CohenKappa(num_classes=2)\n    inputs = tf.keras.layers.Input(shape=(10,))\n    outputs = tf.keras.layers.Dense(1)(inputs)\n    model = tf.keras.models.Model(inputs, outputs)\n    model.compile(optimizer=""sgd"", loss=""mse"", metrics=[kp])\n\n    x = np.random.rand(1000, 10).astype(np.float32)\n    y = np.random.randint(2, size=(1000, 1)).astype(np.float32)\n\n    model.fit(x, y, epochs=1, verbose=0, batch_size=32)\n\n\ndef test_keras_multiclass_reg_model():\n    kp = CohenKappa(num_classes=5, regression=True, sparse_labels=True)\n    inputs = tf.keras.layers.Input(shape=(10,))\n    outputs = tf.keras.layers.Dense(1)(inputs)\n    model = tf.keras.models.Model(inputs, outputs)\n    model.compile(optimizer=""sgd"", loss=""mse"", metrics=[kp])\n\n    x = np.random.rand(1000, 10).astype(np.float32)\n    y = np.random.randint(5, size=(1000,)).astype(np.float32)\n\n    model.fit(x, y, epochs=1, verbose=0, batch_size=32)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_keras_binary_clasasification_model():\n    kp = CohenKappa(num_classes=2)\n    inputs = tf.keras.layers.Input(shape=(10,))\n    outputs = tf.keras.layers.Dense(1, activation=""sigmoid"")(inputs)\n    model = tf.keras.models.Model(inputs, outputs)\n    model.compile(optimizer=""sgd"", loss=""binary_crossentropy"", metrics=[kp])\n\n    x = np.random.rand(1000, 10).astype(np.float32)\n    y = np.random.randint(2, size=(1000, 1)).astype(np.float32)\n\n    model.fit(x, y, epochs=1, verbose=0, batch_size=32)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_keras_multiclass_classification_model():\n    kp = CohenKappa(num_classes=5)\n    inputs = tf.keras.layers.Input(shape=(10,))\n    outputs = tf.keras.layers.Dense(5, activation=""softmax"")(inputs)\n    model = tf.keras.models.Model(inputs, outputs)\n    model.compile(optimizer=""sgd"", loss=""categorical_crossentropy"", metrics=[kp])\n\n    x = np.random.rand(1000, 10).astype(np.float32)\n    y = np.random.randint(5, size=(1000,)).astype(np.float32)\n    y = tf.keras.utils.to_categorical(y, num_classes=5)\n\n    model.fit(x, y, epochs=1, verbose=0, batch_size=32)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_with_ohe_labels():\n    y_true = np.array([4, 4, 3, 4], dtype=np.int32)\n    y_true = tf.keras.utils.to_categorical(y_true, num_classes=5)\n    y_pred = np.array([4, 4, 1, 2], dtype=np.int32)\n\n    obj = CohenKappa(num_classes=5, sparse_labels=False)\n\n    obj.update_state(y_true, y_pred)\n    np.testing.assert_allclose(0.19999999, obj.result().numpy())\n\n\ndef test_cohen_kappa_serialization():\n    actuals = np.array([4, 4, 3, 3, 2, 2, 1, 1], dtype=np.int32)\n    preds = np.array([1, 2, 4, 1, 3, 3, 4, 4], dtype=np.int32)\n    weights = np.array([1, 1, 2, 5, 10, 2, 3, 3], dtype=np.int32)\n\n    ck = CohenKappa(num_classes=5, sparse_labels=True, weightage=""quadratic"")\n    check_metric_serialization(ck, actuals, preds, weights)\n'"
tensorflow_addons/metrics/tests/f_scores_test.py,4,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests F beta metrics.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow_addons.metrics import FBetaScore, F1Score, utils\nfrom tensorflow_addons.testing.serialization import check_metric_serialization\n\n\ndef test_config_fbeta():\n    fbeta_obj = FBetaScore(num_classes=3, beta=0.5, threshold=0.3, average=None)\n    assert fbeta_obj.beta == 0.5\n    assert fbeta_obj.average is None\n    assert fbeta_obj.threshold == 0.3\n    assert fbeta_obj.num_classes == 3\n    assert fbeta_obj.dtype == tf.float32\n\n    # Check save and restore config\n    fbeta_obj2 = FBetaScore.from_config(fbeta_obj.get_config())\n    assert fbeta_obj2.beta == 0.5\n    assert fbeta_obj2.average is None\n    assert fbeta_obj2.threshold == 0.3\n    assert fbeta_obj2.num_classes == 3\n    assert fbeta_obj2.dtype == tf.float32\n\n\ndef _test_tf(avg, beta, act, pred, sample_weights, threshold):\n    act = tf.constant(act, tf.float32)\n    pred = tf.constant(pred, tf.float32)\n\n    fbeta = FBetaScore(3, avg, beta, threshold)\n    fbeta.update_state(act, pred, sample_weights)\n    return fbeta.result().numpy()\n\n\ndef _test_fbeta_score(actuals, preds, sample_weights, avg, beta_val, result, threshold):\n    tf_score = _test_tf(avg, beta_val, actuals, preds, sample_weights, threshold)\n    np.testing.assert_allclose(tf_score, result, atol=1e-7, rtol=1e-6)\n\n\ndef test_fbeta_perfect_score():\n    preds = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n    actuals = [[1, 1, 1], [1, 0, 0], [1, 1, 0]]\n\n    for avg_val in [""micro"", ""macro"", ""weighted""]:\n        for beta in [0.5, 1.0, 2.0]:\n            _test_fbeta_score(actuals, preds, None, avg_val, beta, 1.0, 0.66)\n\n\ndef test_fbeta_worst_score():\n    preds = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n    actuals = [[0, 0, 0], [0, 1, 0], [0, 0, 1]]\n\n    for avg_val in [""micro"", ""macro"", ""weighted""]:\n        for beta in [0.5, 1.0, 2.0]:\n            _test_fbeta_score(actuals, preds, None, avg_val, beta, 0.0, 0.66)\n\n\n@pytest.mark.parametrize(\n    ""avg_val, beta, result"",\n    [\n        (None, 0.5, [0.71428573, 0.5, 0.833334]),\n        (None, 1.0, [0.8, 0.5, 0.6666667]),\n        (None, 2.0, [0.9090904, 0.5, 0.555556]),\n        (""micro"", 0.5, 0.6666667),\n        (""micro"", 1.0, 0.6666667),\n        (""micro"", 2.0, 0.6666667),\n        (""macro"", 0.5, 0.6825397),\n        (""macro"", 1.0, 0.6555555),\n        (""macro"", 2.0, 0.6548822),\n        (""weighted"", 0.5, 0.6825397),\n        (""weighted"", 1.0, 0.6555555),\n        (""weighted"", 2.0, 0.6548822),\n    ],\n)\ndef test_fbeta_random_score(avg_val, beta, result):\n    preds = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n    actuals = [[0, 0, 1], [1, 1, 0], [1, 1, 1]]\n    _test_fbeta_score(actuals, preds, None, avg_val, beta, result, 0.66)\n\n\n@pytest.mark.parametrize(\n    ""avg_val, beta, result"",\n    [\n        (None, 0.5, [0.9090904, 0.555556, 1.0]),\n        (None, 1.0, [0.8, 0.6666667, 1.0]),\n        (None, 2.0, [0.71428573, 0.833334, 1.0]),\n        (""micro"", 0.5, 0.833334),\n        (""micro"", 1.0, 0.833334),\n        (""micro"", 2.0, 0.833334),\n        (""macro"", 0.5, 0.821549),\n        (""macro"", 1.0, 0.822222),\n        (""macro"", 2.0, 0.849206),\n        (""weighted"", 0.5, 0.880471),\n        (""weighted"", 1.0, 0.844445),\n        (""weighted"", 2.0, 0.829365),\n    ],\n)\ndef test_fbeta_random_score_none(avg_val, beta, result):\n    preds = [\n        [0.9, 0.1, 0],\n        [0.2, 0.6, 0.2],\n        [0, 0, 1],\n        [0.4, 0.3, 0.3],\n        [0, 0.9, 0.1],\n        [0, 0, 1],\n    ]\n    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]\n    _test_fbeta_score(actuals, preds, None, avg_val, beta, result, None)\n\n\n@pytest.mark.parametrize(\n    ""avg_val, beta, sample_weights, result"",\n    [\n        (None, 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.909091, 0.555556, 1.0]),\n        (None, 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n        (None, 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.9375, 0.714286, 1.0]),\n        (None, 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.8, 0.666667, 1.0]),\n        (None, 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n        (None, 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.857143, 0.8, 1.0]),\n        (None, 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.714286, 0.833333, 1.0]),\n        (None, 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n        (None, 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.789474, 0.909091, 1.0]),\n        (""micro"", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n        (""micro"", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""micro"", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n        (""micro"", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n        (""micro"", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""micro"", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n        (""micro"", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n        (""micro"", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""micro"", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n        (""macro"", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.821549),\n        (""macro"", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n        (""macro"", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.883929),\n        (""macro"", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.822222),\n        (""macro"", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n        (""macro"", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.885714),\n        (""macro"", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.849206),\n        (""macro"", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n        (""macro"", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.899522),\n        (""weighted"", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.880471),\n        (""weighted"", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""weighted"", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.917857),\n        (""weighted"", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.844444),\n        (""weighted"", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""weighted"", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.902857),\n        (""weighted"", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.829365),\n        (""weighted"", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n        (""weighted"", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.897608),\n    ],\n)\ndef test_fbeta_weighted_random_score_none(avg_val, beta, sample_weights, result):\n    preds = [\n        [0.9, 0.1, 0],\n        [0.2, 0.6, 0.2],\n        [0, 0, 1],\n        [0.4, 0.3, 0.3],\n        [0, 0.9, 0.1],\n        [0, 0, 1],\n    ]\n    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]\n    _test_fbeta_score(actuals, preds, sample_weights, avg_val, beta, result, None)\n\n\ndef test_keras_model():\n    fbeta = FBetaScore(5, ""micro"", 1.0)\n    utils._get_model(fbeta, 5)\n\n\ndef test_eq():\n    f1 = F1Score(3)\n    fbeta = FBetaScore(3, beta=1.0)\n\n    preds = [\n        [0.9, 0.1, 0],\n        [0.2, 0.6, 0.2],\n        [0, 0, 1],\n        [0.4, 0.3, 0.3],\n        [0, 0.9, 0.1],\n        [0, 0, 1],\n    ]\n    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]\n\n    fbeta.update_state(actuals, preds)\n    f1.update_state(actuals, preds)\n    np.testing.assert_allclose(fbeta.result().numpy(), f1.result().numpy())\n\n\ndef test_sample_eq():\n    f1 = F1Score(3)\n    f1_weighted = F1Score(3)\n\n    preds = [\n        [0.9, 0.1, 0],\n        [0.2, 0.6, 0.2],\n        [0, 0, 1],\n        [0.4, 0.3, 0.3],\n        [0, 0.9, 0.1],\n        [0, 0, 1],\n    ]\n    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]\n    sample_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n    f1.update_state(actuals, preds)\n    f1_weighted(actuals, preds, sample_weights)\n    np.testing.assert_allclose(f1.result().numpy(), f1_weighted.result().numpy())\n\n\ndef test_keras_model_f1():\n    f1 = F1Score(5)\n    utils._get_model(f1, 5)\n\n\ndef test_config_f1():\n    f1 = F1Score(3)\n    config = f1.get_config()\n    assert ""beta"" not in config\n\n\n@pytest.mark.parametrize(""average"", [None, ""micro"", ""macro"", ""weighted""])\n@pytest.mark.parametrize(""threshold"", [None, 0.2])\ndef test_serialization_f1_score(average, threshold):\n    f1 = F1Score(3, average, threshold)\n    preds = [\n        [0.9, 0.1, 0],\n        [0.2, 0.6, 0.2],\n        [0, 0, 1],\n        [0.4, 0.3, 0.3],\n        [0, 0.9, 0.1],\n        [0, 0, 1],\n    ]\n    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]\n\n    check_metric_serialization(f1, np.array(actuals), np.array(preds))\n'"
tensorflow_addons/metrics/tests/hamming_test.py,19,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests Hamming metrics.""""""\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nfrom tensorflow_addons.metrics import HammingLoss, hamming_distance\n\n\ndef test_config():\n    hl_obj = HammingLoss(mode=""multilabel"", threshold=0.8)\n    assert hl_obj.name == ""hamming_loss""\n    assert hl_obj.dtype == tf.float32\n\n\ndef check_results(obj, value):\n    np.testing.assert_allclose(value, obj.result().numpy(), atol=1e-5)\n\n\ndef test_mc_4_classes():\n    actuals = tf.constant(\n        [\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 1, 0, 0],\n            [0, 1, 0, 0],\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n        ],\n        dtype=tf.float32,\n    )\n    predictions = tf.constant(\n        [\n            [0.85, 0.12, 0.03, 0],\n            [0, 0, 1, 0],\n            [0.10, 0.045, 0.045, 0.81],\n            [1, 0, 0, 0],\n            [0.80, 0.10, 0.10, 0],\n            [1, 0, 0, 0],\n            [0.05, 0, 0.90, 0.05],\n        ],\n        dtype=tf.float32,\n    )\n    # Initialize\n    hl_obj = HammingLoss(""multiclass"", threshold=0.8)\n    hl_obj.update_state(actuals, predictions)\n    # Check results\n    check_results(hl_obj, 0.2857143)\n\n\ndef test_mc_5_classes():\n    actuals = tf.constant(\n        [\n            [1, 0, 0, 0, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 1],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n        ],\n        dtype=tf.float32,\n    )\n\n    predictions = tf.constant(\n        [\n            [0.85, 0, 0.15, 0, 0],\n            [0, 0, 0, 1, 0],\n            [0, 1, 0, 0, 0],\n            [0.05, 0.90, 0.04, 0, 0.01],\n            [0.10, 0, 0.81, 0.09, 0],\n            [0.10, 0.045, 0, 0.81, 0.045],\n            [1, 0, 0, 0, 0],\n            [0, 0.85, 0, 0, 0.15],\n        ],\n        dtype=tf.float32,\n    )\n    # Initialize\n    hl_obj = HammingLoss(""multiclass"", threshold=0.8)\n    hl_obj.update_state(actuals, predictions)\n    # Check results\n    check_results(hl_obj, 0.25)\n\n\ndef test_ml_4_classes():\n    actuals = tf.constant([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 0, 1]], dtype=tf.float32)\n    predictions = tf.constant(\n        [[0.97, 0.56, 0.83, 0.77], [0.34, 0.95, 0.7, 0.89], [0.95, 0.45, 0.23, 0.56],],\n        dtype=tf.float32,\n    )\n    # Initialize\n    hl_obj = HammingLoss(""multilabel"", threshold=0.8)\n    hl_obj.update_state(actuals, predictions)\n    # Check results\n    check_results(hl_obj, 0.16666667)\n\n\ndef test_ml_5_classes():\n    actuals = tf.constant(\n        [\n            [1, 0, 0, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 1, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 0, 1, 1, 0],\n            [1, 0, 0, 0, 1],\n            [0, 1, 1, 0, 0],\n        ],\n        dtype=tf.float32,\n    )\n    predictions = tf.constant(\n        [\n            [1, 0.75, 0.2, 0.55, 0],\n            [0.65, 0.22, 0.97, 0.88, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0.85, 0.9, 0.34, 0.5],\n            [0.4, 0.65, 0.87, 0, 0.12],\n            [0.66, 0.55, 1, 0.98, 0],\n            [0.95, 0.34, 0.67, 0.65, 0.10],\n            [0.45, 0.97, 0.89, 0.67, 0.46],\n        ],\n        dtype=tf.float32,\n    )\n    # Initialize\n    hl_obj = HammingLoss(""multilabel"", threshold=0.7)\n    hl_obj.update_state(actuals, predictions)\n    # Check results\n    check_results(hl_obj, 0.075)\n\n\ndef hamming_distance_test():\n    actuals = tf.constant([1, 1, 0, 0, 1, 0, 1, 0, 0, 1], dtype=tf.int32)\n    predictions = tf.constant([1, 0, 0, 0, 1, 0, 0, 1, 0, 1], dtype=tf.int32)\n    test_result = hamming_distance(actuals, predictions)\n    np.testing.assert_allclose(0.3, test_result, atol=1e-5)\n\n\n# Keras model check\ndef test_keras_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(64, activation=""relu""))\n    model.add(layers.Dense(3, activation=""softmax""))\n    h1 = HammingLoss(mode=""multiclass"")\n    model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", metrics=[h1])\n    data = np.random.random((100, 10))\n    labels = np.random.random((100, 3))\n    model.fit(data, labels, epochs=1, batch_size=32, verbose=0)\n'"
tensorflow_addons/metrics/tests/matthews_correlation_coefficient_test.py,12,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Matthews Correlation Coefficient Test.""""""\n\n\nimport tensorflow as tf\n\nimport numpy as np\nfrom tensorflow_addons.metrics import MatthewsCorrelationCoefficient\n\n\ndef test_config():\n    # mcc object\n    mcc1 = MatthewsCorrelationCoefficient(num_classes=1)\n    assert mcc1.num_classes == 1\n    assert mcc1.dtype == tf.float32\n    # check configure\n    mcc2 = MatthewsCorrelationCoefficient.from_config(mcc1.get_config())\n    assert mcc2.num_classes == 1\n    assert mcc2.dtype == tf.float32\n\n\ndef check_results(obj, value):\n    np.testing.assert_allclose(value, obj.result().numpy(), atol=1e-6)\n\n\ndef test_binary_classes():\n    gt_label = tf.constant([[1.0], [1.0], [1.0], [0.0]], dtype=tf.float32)\n    preds = tf.constant([[1.0], [0.0], [1.0], [1.0]], dtype=tf.float32)\n    # Initialize\n    mcc = MatthewsCorrelationCoefficient(1)\n    # Update\n    mcc.update_state(gt_label, preds)\n    # Check results\n    check_results(mcc, [-0.33333334])\n\n\ndef test_multiple_classes():\n    gt_label = tf.constant(\n        [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 1.0]],\n        dtype=tf.float32,\n    )\n    preds = tf.constant(\n        [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 1.0], [1.0, 1.0, 0.0]],\n        dtype=tf.float32,\n    )\n    # Initialize\n    mcc = MatthewsCorrelationCoefficient(3)\n    mcc.update_state(gt_label, preds)\n    # Check results\n    check_results(mcc, [-0.33333334, 1.0, 0.57735026])\n\n\n# Keras model API check\ndef test_keras_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(64, activation=""relu""))\n    model.add(tf.keras.layers.Dense(64, activation=""relu""))\n    model.add(tf.keras.layers.Dense(1, activation=""softmax""))\n    mcc = MatthewsCorrelationCoefficient(num_classes=1)\n    model.compile(\n        optimizer=""Adam"", loss=""binary_crossentropy"", metrics=[""accuracy"", mcc]\n    )\n    # data preparation\n    data = np.random.random((10, 1))\n    labels = np.random.random((10, 1))\n    labels = np.where(labels > 0.5, 1.0, 0.0)\n    model.fit(data, labels, epochs=1, batch_size=32, verbose=0)\n'"
tensorflow_addons/metrics/tests/metrics_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport inspect\n\nfrom tensorflow.keras.metrics import Metric\nfrom tensorflow_addons import metrics\n\n\ndef test_update_state_signature():\n    for name, obj in inspect.getmembers(metrics):\n        if inspect.isclass(obj) and issubclass(obj, Metric):\n            check_update_state_signature(obj)\n\n\ndef check_update_state_signature(metric_class):\n    update_state_signature = inspect.signature(metric_class.update_state)\n    for expected_parameter in [""y_true"", ""y_pred"", ""sample_weight""]:\n        if expected_parameter not in update_state_signature.parameters.keys():\n            raise ValueError(\n                ""Class {} is missing the parameter {} in the `update_state` ""\n                ""method. If the method doesn\'t use this argument, declare ""\n                ""it anyway and raise a UserWarning if it is ""\n                ""not None."".format(metric_class.__name__, expected_parameter)\n            )\n'"
tensorflow_addons/metrics/tests/multilabel_confusion_matrix_test.py,11,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Multilabel Confusion Matrix Metric.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow_addons.metrics import MultiLabelConfusionMatrix\n\n\ndef test_config():\n    mcm_obj = MultiLabelConfusionMatrix(num_classes=3)\n    assert mcm_obj.num_classes == 3\n    assert mcm_obj.dtype == tf.float32\n    # Check save and restore config\n    mcm_obj2 = MultiLabelConfusionMatrix.from_config(mcm_obj.get_config())\n    assert mcm_obj2.num_classes == 3\n    assert mcm_obj2.dtype == tf.float32\n\n\ndef check_results(obj, value):\n    np.testing.assert_allclose(value, obj.result().numpy(), atol=1e-6)\n\n\n@pytest.mark.parametrize(""dtype"", [tf.int32, tf.int64, tf.float32, tf.float64])\ndef test_mcm_3_classes(dtype):\n    actuals = tf.constant([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=dtype)\n    preds = tf.constant([[1, 0, 0], [0, 1, 1], [1, 0, 0], [0, 1, 1]], dtype=dtype)\n    # Initialize\n    mcm_obj = MultiLabelConfusionMatrix(num_classes=3, dtype=dtype)\n    mcm_obj.update_state(actuals, preds)\n    # Check results\n    check_results(mcm_obj, [[[2, 0], [0, 2]], [[2, 0], [0, 2]], [[0, 2], [2, 0]]])\n\n\n@pytest.mark.parametrize(""dtype"", [tf.int32, tf.int64, tf.float32, tf.float64])\ndef test_mcm_4_classes(dtype):\n    actuals = tf.constant(\n        [\n            [1, 0, 0, 1],\n            [0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [1, 1, 0, 0],\n            [0, 1, 0, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 1, 1, 0],\n            [0, 1, 0, 1],\n        ],\n        dtype=dtype,\n    )\n    preds = tf.constant(\n        [\n            [1, 0, 1, 0],\n            [0, 0, 1, 1],\n            [0, 0, 0, 1],\n            [1, 1, 0, 0],\n            [1, 0, 0, 0],\n            [1, 0, 0, 1],\n            [0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 1, 0, 0],\n            [0, 0, 0, 1],\n        ],\n        dtype=dtype,\n    )\n\n    # Initialize\n    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)\n    mcm_obj.update_state(actuals, preds)\n    # Check results\n    check_results(\n        mcm_obj,\n        [[[4, 1], [1, 4]], [[6, 0], [2, 2]], [[6, 1], [1, 2]], [[2, 0], [2, 6]],],\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [tf.int32, tf.int64, tf.float32, tf.float64])\ndef test_multiclass(dtype):\n    actuals = tf.constant(\n        [\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 1, 0, 0],\n            [0, 1, 0, 0],\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n        ],\n        dtype=dtype,\n    )\n    preds = tf.constant(\n        [\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [1, 0, 0, 0],\n            [1, 0, 0, 0],\n            [1, 0, 0, 0],\n            [0, 0, 1, 0],\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 0, 1],\n        ],\n        dtype=dtype,\n    )\n\n    # Initialize\n    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)\n    mcm_obj.update_state(actuals, preds)\n    # Check results\n    check_results(\n        mcm_obj,\n        [[[5, 2], [0, 3]], [[7, 1], [2, 0]], [[7, 0], [1, 2]], [[8, 0], [0, 2]],],\n    )\n'"
tensorflow_addons/metrics/tests/r_square_test.py,20,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for R-Square Metric.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom sklearn.metrics import r2_score as sklearn_r2_score\nfrom tensorflow_addons.metrics import RSquare\nfrom tensorflow_addons.metrics.r_square import VALID_MULTIOUTPUT\n\n\ndef test_config():\n    r2_obj = RSquare(name=""r_square"")\n    assert r2_obj.name == ""r_square""\n    assert r2_obj.dtype == tf.float32\n    # Check save and restore config\n    r2_obj2 = RSquare.from_config(r2_obj.get_config())\n    assert r2_obj2.name == ""r_square""\n    assert r2_obj2.dtype == tf.float32\n\n\ndef initialize_vars(y_shape=(), multioutput: str = ""uniform_average""):\n    return RSquare(y_shape=y_shape, multioutput=multioutput)\n\n\ndef update_obj_states(obj, actuals, preds, sample_weight=None):\n    obj.update_state(actuals, preds, sample_weight=sample_weight)\n\n\ndef check_results(obj, value):\n    np.testing.assert_allclose(value, obj.result(), atol=1e-5)\n\n\ndef test_r2_perfect_score():\n    actuals = tf.constant([100, 700, 40, 5.7], dtype=tf.float32)\n    preds = tf.constant([100, 700, 40, 5.7], dtype=tf.float32)\n    actuals = tf.cast(actuals, dtype=tf.float32)\n    preds = tf.cast(preds, dtype=tf.float32)\n    # Initialize\n    r2_obj = initialize_vars()\n    # Update\n    update_obj_states(r2_obj, actuals, preds)\n    # Check results\n    check_results(r2_obj, 1.0)\n\n\ndef test_r2_worst_score():\n    actuals = tf.constant([10, 600, 4, 9.77], dtype=tf.float32)\n    preds = tf.constant([1, 70, 40, 5.7], dtype=tf.float32)\n    actuals = tf.cast(actuals, dtype=tf.float32)\n    preds = tf.cast(preds, dtype=tf.float32)\n    # Initialize\n    r2_obj = initialize_vars()\n    # Update\n    update_obj_states(r2_obj, actuals, preds)\n    # Check results\n    check_results(r2_obj, -0.073607)\n\n\ndef test_r2_random_score():\n    actuals = tf.constant([10, 600, 3, 9.77], dtype=tf.float32)\n    preds = tf.constant([1, 340, 40, 5.7], dtype=tf.float32)\n    actuals = tf.cast(actuals, dtype=tf.float32)\n    preds = tf.cast(preds, dtype=tf.float32)\n    # Initialize\n    r2_obj = initialize_vars()\n    # Update\n    update_obj_states(r2_obj, actuals, preds)\n    # Check results\n    check_results(r2_obj, 0.7376327)\n\n\ndef test_r2_sklearn_comparison():\n    """"""Test that RSquare behaves similarly to the scikit-learn\n    implementation of the same metric, given random input.\n    """"""\n    for multioutput in VALID_MULTIOUTPUT:\n        for i in range(10):\n            actuals = np.random.rand(64, 3)\n            preds = np.random.rand(64, 3)\n            sample_weight = np.random.rand(64, 1)\n            tensor_actuals = tf.constant(actuals, dtype=tf.float32)\n            tensor_preds = tf.constant(preds, dtype=tf.float32)\n            tensor_sample_weight = tf.constant(sample_weight, dtype=tf.float32)\n            tensor_actuals = tf.cast(tensor_actuals, dtype=tf.float32)\n            tensor_preds = tf.cast(tensor_preds, dtype=tf.float32)\n            tensor_sample_weight = tf.cast(tensor_sample_weight, dtype=tf.float32)\n            # Initialize\n            r2_obj = initialize_vars(y_shape=(3,), multioutput=multioutput)\n            # Update\n            update_obj_states(\n                r2_obj,\n                tensor_actuals,\n                tensor_preds,\n                sample_weight=tensor_sample_weight,\n            )\n            # Check results by comparing to results of scikit-learn r2 implementation\n            sklearn_result = sklearn_r2_score(\n                actuals, preds, sample_weight=sample_weight, multioutput=multioutput\n            )\n            check_results(r2_obj, sklearn_result)\n\n\ndef test_unrecognized_multioutput():\n    with pytest.raises(ValueError):\n        initialize_vars(multioutput=""meadian"")\n'"
tensorflow_addons/metrics/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/optimizers/tests/__init__.py,0,b''
tensorflow_addons/optimizers/tests/conditional_gradient_test.py,104,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Conditional Gradient.""""""\n\nimport numpy as np\nimport pytest\nimport platform\n\nimport tensorflow as tf\nfrom tensorflow_addons.utils import test_utils\nfrom tensorflow_addons.optimizers import conditional_gradient as cg_lib\n\n\ndef _dtypes_to_test(use_gpu):\n    # Based on issue #347 in the following link,\n    #        ""https://github.com/tensorflow/addons/issues/347""\n    # tf.half is not registered for \'ResourceScatterUpdate\' OpKernel\n    # for \'GPU\' devices.\n    # So we have to remove tf.half when testing with gpu.\n    # The function ""_DtypesToTest"" is from\n    #       ""https://github.com/tensorflow/tensorflow/blob/5d4a6cee737a1dc6c20172a1dc1\n    #        5df10def2df72/tensorflow/python/kernel_tests/conv_ops_3d_test.py#L53-L62""\n    if use_gpu:\n        return [tf.float32, tf.float64]\n    else:\n        return [tf.half, tf.float32, tf.float64]\n\n\ndef _dtypes_with_checking_system(use_gpu, system):\n    # Based on issue #36764 in the following link,\n    #        ""https://github.com/tensorflow/tensorflow/issues/36764""\n    # tf.half is not registered for tf.linalg.svd function on Windows\n    # CPU version.\n    # So we have to remove tf.half when testing with Windows CPU version.\n    if system == ""Windows"":\n        return [tf.float32, tf.float64]\n    else:\n        return _dtypes_to_test(use_gpu)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_like_dist_belief_nuclear_cg01():\n    db_grad, db_out = _db_params_nuclear_cg01()\n    num_samples = len(db_grad)\n    var0 = tf.Variable([0.0] * num_samples)\n    grads0 = tf.constant([0.0] * num_samples)\n    ord = ""nuclear""\n    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)\n\n    for i in range(num_samples):\n        grads0 = tf.constant(db_grad[i])\n        cg_opt.apply_gradients(zip([grads0], [var0]))\n        np.testing.assert_allclose(\n            np.array(db_out[i]), var0.numpy(), rtol=1e-6, atol=1e-6\n        )\n\n\n@pytest.mark.with_device([""cpu"", ""gpu""])\n@pytest.mark.parametrize(""dtype"", [tf.float16, tf.float32, tf.float64])\ndef test_minimize_sparse_resource_variable_frobenius(dtype, device):\n    if ""gpu"" in device and dtype == tf.float16:\n        pytest.xfail(""See https://github.com/tensorflow/addons/issues/347"")\n    var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n\n    def loss():\n        x = tf.constant([[4.0], [5.0]], dtype=dtype)\n        pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)\n        return pred * pred\n\n    # the gradient based on the current loss function\n    grads0_0 = 32 * 1.0 + 40 * 2.0\n    grads0_1 = 40 * 1.0 + 50 * 2.0\n    grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)\n    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5\n\n    learning_rate = 0.1\n    lambda_ = 0.1\n    ord = ""fro""\n    opt = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    _ = opt.minimize(loss, var_list=[var0])\n    test_utils.assert_allclose_according_to_type(\n        [\n            [\n                1.0 * learning_rate - (1 - learning_rate) * lambda_ * grads0_0 / norm0,\n                2.0 * learning_rate - (1 - learning_rate) * lambda_ * grads0_1 / norm0,\n            ]\n        ],\n        var0.numpy(),\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""use_resource"", [True, False])\ndef test_basic_frobenius(dtype, use_resource):\n    if use_resource:\n        var0 = tf.Variable([1.0, 2.0], dtype=dtype[0], name=""var0_%d"" % dtype[1])\n        var1 = tf.Variable([3.0, 4.0], dtype=dtype[0], name=""var0_%d"" % dtype[1])\n    else:\n        var0 = tf.Variable([1.0, 2.0], dtype=dtype[0])\n        var1 = tf.Variable([3.0, 4.0], dtype=dtype[0])\n    grads0 = tf.constant([0.1, 0.1], dtype=dtype[0])\n    grads1 = tf.constant([0.01, 0.01], dtype=dtype[0])\n    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5\n    norm1 = tf.math.reduce_sum(grads1 ** 2) ** 0.5\n\n    def learning_rate():\n        return 0.5\n\n    def lambda_():\n        return 0.01\n\n    ord = ""fro""\n\n    cg_opt = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n    # Check we have slots\n    assert [""conditional_gradient""] == cg_opt.get_slot_names()\n    slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n    assert slot0.get_shape() == var0.get_shape()\n    slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n    assert slot1.get_shape() == var1.get_shape()\n\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,\n                2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,\n            ]\n        ),\n        var0.numpy(),\n    )\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,\n                4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,\n            ]\n        ),\n        var1.numpy(),\n    )\n\n    # Step 2: the conditional_gradient contain the previous update.\n    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5\n                - (1 - 0.5) * 0.01 * 0.1 / norm0,\n                (2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5\n                - (1 - 0.5) * 0.01 * 0.1 / norm0,\n            ]\n        ),\n        var0.numpy(),\n    )\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5\n                - (1 - 0.5) * 0.01 * 0.01 / norm1,\n                (4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5\n                - (1 - 0.5) * 0.01 * 0.01 / norm1,\n            ]\n        ),\n        var1.numpy(),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""use_resource"", [True, False])\ndef test_basic_nuclear(use_resource):\n    # TODO:\n    #       to address issue #36764\n    for i, dtype in enumerate(\n        _dtypes_with_checking_system(\n            use_gpu=test_utils.is_gpu_available(), system=platform.system()\n        )\n    ):\n\n        if use_resource:\n            var0 = tf.Variable([1.0, 2.0], dtype=dtype, name=""var0_%d"" % i)\n            var1 = tf.Variable([3.0, 4.0], dtype=dtype, name=""var1_%d"" % i)\n        else:\n            var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n            var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n\n        grads0 = tf.constant([0.1, 0.1], dtype=dtype)\n        grads1 = tf.constant([0.01, 0.01], dtype=dtype)\n        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)\n        top_singular_vector1 = cg_lib.ConditionalGradient._top_singular_vector(grads1)\n\n        def learning_rate():\n            return 0.5\n\n        def lambda_():\n            return 0.01\n\n        ord = ""nuclear""\n\n        cg_opt = cg_lib.ConditionalGradient(\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\n        )\n        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check we have slots\n        assert [""conditional_gradient""] == cg_opt.get_slot_names()\n        slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n        assert slot0.get_shape() == var0.get_shape()\n        slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n        assert slot1.get_shape() == var1.get_shape()\n\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],\n                    2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],\n                ]\n            ),\n            var0.numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],\n                    4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],\n                ]\n            ),\n            var1.numpy(),\n        )\n\n        # Step 2: the conditional_gradient contain the previous update.\n        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector0[0],\n                    (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector0[1],\n                ]\n            ),\n            var0.numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],\n                    (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],\n                ]\n            ),\n            var1.numpy(),\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_minimize_sparse_resource_variable_nuclear():\n    # TODO:\n    #       to address issue #347 and #36764.\n    for dtype in _dtypes_with_checking_system(\n        use_gpu=test_utils.is_gpu_available(), system=platform.system()\n    ):\n        var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n\n        def loss():\n            x = tf.constant([[4.0], [5.0]], dtype=dtype)\n            pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)\n            return pred * pred\n\n        # the gradient based on the current loss function\n        grads0_0 = 32 * 1.0 + 40 * 2.0\n        grads0_1 = 40 * 1.0 + 50 * 2.0\n        grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)\n        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)\n\n        learning_rate = 0.1\n        lambda_ = 0.1\n        ord = ""nuclear""\n        opt = cg_lib.ConditionalGradient(\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\n        )\n        _ = opt.minimize(loss, var_list=[var0])\n\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(\n            [\n                [\n                    1.0 * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[0][0],\n                    2.0 * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[0][1],\n                ]\n            ],\n            var0.numpy(),\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_tensor_learning_rate_and_conditional_gradient_nuclear():\n    for dtype in _dtypes_with_checking_system(\n        use_gpu=test_utils.is_gpu_available(), system=platform.system()\n    ):\n        # TODO:\n        # Based on issue #36764 in the following link,\n        #        ""https://github.com/tensorflow/tensorflow/issues/36764""\n        # tf.half is not registered for tf.linalg.svd function on Windows\n        # CPU version.\n        # So we have to remove tf.half when testing with Windows CPU version.\n        var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n        var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n        grads0 = tf.constant([0.1, 0.1], dtype=dtype)\n        grads1 = tf.constant([0.01, 0.01], dtype=dtype)\n        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)\n        top_singular_vector1 = cg_lib.ConditionalGradient._top_singular_vector(grads1)\n        ord = ""nuclear""\n        cg_opt = cg_lib.ConditionalGradient(\n            learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord\n        )\n        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check we have slots\n        assert [""conditional_gradient""] == cg_opt.get_slot_names()\n        slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n        assert slot0.get_shape() == var0.get_shape()\n        slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n        assert slot1.get_shape() == var1.get_shape()\n\n        # Check that the parameters have been updated.\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],\n                    2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],\n                ]\n            ),\n            var0.numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],\n                    4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],\n                ]\n            ),\n            var1.numpy(),\n        )\n        # Step 2: the conditional_gradient contain the\n        # previous update.\n        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check that the parameters have been updated.\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector0[0],\n                    (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector0[1],\n                ]\n            ),\n            var0.numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector1[0],\n                    (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1]) * 0.5\n                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],\n                ]\n            ),\n            var1.numpy(),\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_variables_across_graphs_frobenius():\n    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord=""fro"")\n    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name=""var0"")\n    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name=""var1"")\n\n    def loss():\n        return tf.math.reduce_sum(var0 + var1)\n\n    optimizer.minimize(loss, var_list=[var0, var1])\n    optimizer_variables = optimizer.variables()\n    # There should be three items. The first item is iteration,\n    # and one item for each variable.\n    assert optimizer_variables[1].name.startswith(""ConditionalGradient/var0"")\n    assert optimizer_variables[2].name.startswith(""ConditionalGradient/var1"")\n    assert 3 == len(optimizer_variables)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_variables_across_graphs_nuclear():\n    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord=""nuclear"")\n    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name=""var0"")\n    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name=""var1"")\n\n    def loss():\n        return tf.math.reduce_sum(var0 + var1)\n\n    optimizer.minimize(loss, var_list=[var0, var1])\n    optimizer_variables = optimizer.variables()\n    # There should be three items. The first item is iteration,\n    # and one item for each variable.\n    assert optimizer_variables[1].name.startswith(""ConditionalGradient/var0"")\n    assert optimizer_variables[2].name.startswith(""ConditionalGradient/var1"")\n    assert 3 == len(optimizer_variables)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_minimize_with_2D_indicies_for_embedding_lookup_frobenius():\n    # This test invokes the ResourceSparseApplyConditionalGradient\n    # operation.\n    var0 = tf.Variable(tf.ones([2, 2]))\n\n    def loss():\n        return tf.math.reduce_sum(tf.nn.embedding_lookup(var0, [[1]]))\n\n    # the gradient for this loss function:\n    grads0 = tf.constant([[0, 0], [1, 1]], dtype=tf.float32)\n    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5\n\n    learning_rate = 0.1\n    lambda_ = 0.1\n    ord = ""fro""\n    opt = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    _ = opt.minimize(loss, var_list=[var0])\n\n    # Run 1 step of cg_op\n    test_utils.assert_allclose_according_to_type(\n        [\n            [1, 1],\n            [\n                learning_rate * 1 - (1 - learning_rate) * lambda_ * 1 / norm0,\n                learning_rate * 1 - (1 - learning_rate) * lambda_ * 1 / norm0,\n            ],\n        ],\n        var0.numpy(),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_minimize_with_2D_indicies_for_embedding_lookup_nuclear():\n    # This test invokes the ResourceSparseApplyConditionalGradient\n    # operation.\n    var0 = tf.Variable(tf.ones([2, 2]))\n\n    def loss():\n        return tf.math.reduce_sum(tf.nn.embedding_lookup(var0, [[1]]))\n\n    # the gradient for this loss function:\n    grads0 = tf.constant([[0, 0], [1, 1]], dtype=tf.float32)\n    top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)\n\n    learning_rate = 0.1\n    lambda_ = 0.1\n    ord = ""nuclear""\n    opt = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    _ = opt.minimize(loss, var_list=[var0])\n\n    # Run 1 step of cg_op\n    test_utils.assert_allclose_according_to_type(\n        [\n            learning_rate * 1\n            - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0],\n            learning_rate * 1\n            - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1],\n        ],\n        var0[1],\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_tensor_learning_rate_and_conditional_gradient_frobenius(dtype):\n    var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n    var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n    grads0 = tf.constant([0.1, 0.1], dtype=dtype)\n    grads1 = tf.constant([0.01, 0.01], dtype=dtype)\n    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5\n    norm1 = tf.math.reduce_sum(grads1 ** 2) ** 0.5\n    ord = ""fro""\n    cg_opt = cg_lib.ConditionalGradient(\n        learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord\n    )\n    _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n    # Check we have slots\n    assert [""conditional_gradient""] == cg_opt.get_slot_names()\n    slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n    assert slot0.get_shape() == var0.get_shape()\n    slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n    assert slot1.get_shape() == var1.get_shape()\n\n    # Check that the parameters have been updated.\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,\n                2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,\n            ]\n        ),\n        var0.numpy(),\n    )\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,\n                4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,\n            ]\n        ),\n        var1.numpy(),\n    )\n    # Step 2: the conditional_gradient contain the\n    # previous update.\n    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n    # Check that the parameters have been updated.\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5\n                - (1 - 0.5) * 0.01 * 0.1 / norm0,\n                (2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5\n                - (1 - 0.5) * 0.01 * 0.1 / norm0,\n            ]\n        ),\n        var0.numpy(),\n    )\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5\n                - (1 - 0.5) * 0.01 * 0.01 / norm1,\n                (4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5\n                - (1 - 0.5) * 0.01 * 0.01 / norm1,\n            ]\n        ),\n        var1.numpy(),\n    )\n\n\ndef _db_params_frobenius_cg01():\n    """"""Return dist-belief conditional_gradient values.\n\n    Return values been generated from the dist-belief\n    conditional_gradient unittest, running with a learning rate of 0.1\n    and a lambda_ of 0.1.\n\n    These values record how a parameter vector of size 10, initialized\n    with 0.0, gets updated with 10 consecutive conditional_gradient\n    steps.\n    It uses random gradients.\n\n    Returns:\n        db_grad: The gradients to apply\n        db_out: The parameters after the conditional_gradient update.\n    """"""\n    db_grad = [[]] * 10\n    db_out = [[]] * 10\n    db_grad[0] = [\n        0.00096264342,\n        0.17914793,\n        0.93945462,\n        0.41396621,\n        0.53037018,\n        0.93197989,\n        0.78648776,\n        0.50036013,\n        0.55345792,\n        0.96722615,\n    ]\n    db_out[0] = [\n        -4.1555551e-05,\n        -7.7334875e-03,\n        -4.0554531e-02,\n        -1.7870162e-02,\n        -2.2895107e-02,\n        -4.0231861e-02,\n        -3.3951234e-02,\n        -2.1599628e-02,\n        -2.3891762e-02,\n        -4.1753378e-02,\n    ]\n    db_grad[1] = [\n        0.17075552,\n        0.88821375,\n        0.20873757,\n        0.25236958,\n        0.57578111,\n        0.15312378,\n        0.5513742,\n        0.94687688,\n        0.16012503,\n        0.22159521,\n    ]\n    db_out[1] = [\n        -0.00961733,\n        -0.0507779,\n        -0.01580694,\n        -0.01599489,\n        -0.03470477,\n        -0.01264373,\n        -0.03443632,\n        -0.05546713,\n        -0.01140388,\n        -0.01665068,\n    ]\n    db_grad[2] = [\n        0.35077485,\n        0.47304362,\n        0.44412705,\n        0.44368884,\n        0.078527533,\n        0.81223965,\n        0.31168157,\n        0.43203235,\n        0.16792089,\n        0.24644311,\n    ]\n    db_out[2] = [\n        -0.02462724,\n        -0.03699233,\n        -0.03154434,\n        -0.03153357,\n        -0.00876844,\n        -0.05606323,\n        -0.02447166,\n        -0.03469437,\n        -0.0124694,\n        -0.01829169,\n    ]\n    db_grad[3] = [\n        0.9694621,\n        0.75035888,\n        0.28171822,\n        0.83813518,\n        0.53807181,\n        0.3728098,\n        0.81454384,\n        0.03848977,\n        0.89759839,\n        0.93665648,\n    ]\n    db_out[3] = [\n        -0.04124615,\n        -0.03371741,\n        -0.0144246,\n        -0.03668303,\n        -0.02240246,\n        -0.02052062,\n        -0.03503307,\n        -0.00500922,\n        -0.03715545,\n        -0.0393002,\n    ]\n    db_grad[4] = [\n        0.38578293,\n        0.8536852,\n        0.88722926,\n        0.66276771,\n        0.13678469,\n        0.94036359,\n        0.69107032,\n        0.81897682,\n        0.5433259,\n        0.67860287,\n    ]\n    db_out[4] = [\n        -0.01979208,\n        -0.0380417,\n        -0.03747472,\n        -0.0305847,\n        -0.00779536,\n        -0.04024222,\n        -0.03156913,\n        -0.0337613,\n        -0.02578116,\n        -0.03148952,\n    ]\n    db_grad[5] = [\n        0.27885768,\n        0.76100707,\n        0.24625534,\n        0.81354135,\n        0.18959245,\n        0.48038563,\n        0.84163809,\n        0.41172323,\n        0.83259648,\n        0.44941229,\n    ]\n    db_out[5] = [\n        -0.01555188,\n        -0.04084422,\n        -0.01573331,\n        -0.04265549,\n        -0.01000746,\n        -0.02740575,\n        -0.04412147,\n        -0.02341569,\n        -0.0431026,\n        -0.02502293,\n    ]\n    db_grad[6] = [\n        0.27233034,\n        0.056316052,\n        0.5039115,\n        0.24105175,\n        0.35697976,\n        0.75913221,\n        0.73577434,\n        0.16014607,\n        0.57500273,\n        0.071136251,\n    ]\n    db_out[6] = [\n        -0.01890448,\n        -0.00767214,\n        -0.03367592,\n        -0.01962219,\n        -0.02374279,\n        -0.05110247,\n        -0.05128598,\n        -0.01254396,\n        -0.04094185,\n        -0.00703416,\n    ]\n    db_grad[7] = [\n        0.58697265,\n        0.2494842,\n        0.08106143,\n        0.39954534,\n        0.15892942,\n        0.12683646,\n        0.74053431,\n        0.16033,\n        0.66625422,\n        0.73515922,\n    ]\n    db_out[7] = [\n        -0.03772914,\n        -0.01599993,\n        -0.00831695,\n        -0.02635719,\n        -0.01207801,\n        -0.01285448,\n        -0.05034328,\n        -0.01104364,\n        -0.04477356,\n        -0.04558991,\n    ]\n    db_grad[8] = [\n        0.8215279,\n        0.41994119,\n        0.95172721,\n        0.68000203,\n        0.79439718,\n        0.43384039,\n        0.55561525,\n        0.22567581,\n        0.93331909,\n        0.29438227,\n    ]\n    db_out[8] = [\n        -0.03919835,\n        -0.01970845,\n        -0.04187151,\n        -0.03195836,\n        -0.03546333,\n        -0.01999326,\n        -0.02899324,\n        -0.01083582,\n        -0.04472339,\n        -0.01725317,\n    ]\n    db_grad[9] = [\n        0.68297005,\n        0.67758518,\n        0.1748755,\n        0.13266537,\n        0.70697063,\n        0.055731893,\n        0.68593478,\n        0.50580865,\n        0.12602448,\n        0.093537711,\n    ]\n    db_out[9] = [\n        -0.04510314,\n        -0.04282944,\n        -0.0147322,\n        -0.0111956,\n        -0.04617687,\n        -0.00535998,\n        -0.0442614,\n        -0.03158399,\n        -0.01207165,\n        -0.00736567,\n    ]\n    return db_grad, db_out\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_like_dist_belief_frobenius_cg01():\n    db_grad, db_out = _db_params_frobenius_cg01()\n    num_samples = len(db_grad)\n    var0 = tf.Variable([0.0] * num_samples)\n    grads0 = tf.constant([0.0] * num_samples)\n    ord = ""fro""\n    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)\n\n    for i in range(num_samples):\n        grads0 = tf.constant(db_grad[i])\n        cg_opt.apply_gradients(zip([grads0], [var0]))\n        np.testing.assert_allclose(\n            np.array(db_out[i]), var0.numpy(), rtol=1e-06, atol=1e-06\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_frobenius():\n    # TODO:\n    #       To address the issue #347.\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        var0 = tf.Variable(tf.zeros([4, 2], dtype=dtype))\n        var1 = tf.Variable(tf.constant(1.0, dtype, [4, 2]))\n        grads0 = tf.IndexedSlices(\n            tf.constant([[0.1, 0.1]], dtype=dtype),\n            tf.constant([1]),\n            tf.constant([4, 2]),\n        )\n        grads1 = tf.IndexedSlices(\n            tf.constant([[0.01, 0.01], [0.01, 0.01]], dtype=dtype),\n            tf.constant([2, 3]),\n            tf.constant([4, 2]),\n        )\n        norm0 = tf.math.reduce_sum(tf.math.multiply(grads0, grads0)) ** 0.5\n        norm1 = tf.math.reduce_sum(tf.math.multiply(grads1, grads1)) ** 0.5\n        learning_rate = 0.1\n        lambda_ = 0.1\n        ord = ""fro""\n        cg_opt = cg_lib.ConditionalGradient(\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\n        )\n        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check we have slots\n        assert [""conditional_gradient""] == cg_opt.get_slot_names()\n        slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n        assert slot0.get_shape() == var0.get_shape()\n        slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n        assert slot1.get_shape() == var1.get_shape()\n\n        # Check that the parameters have been updated.\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    0 - (1 - learning_rate) * lambda_ * 0 / norm0,\n                    0 - (1 - learning_rate) * lambda_ * 0 / norm0,\n                ]\n            ),\n            var0[0].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    0 - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n                    0 - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n                ]\n            ),\n            var0[1].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    1.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n                    1.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n                ]\n            ),\n            var1[2].numpy(),\n        )\n        # Step 2: the conditional_gradient contain the\n        # previous update.\n        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n        # Check that the parameters have been updated.\n        np.testing.assert_allclose(np.array([0, 0]), var0[0].numpy())\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (0 - (1 - learning_rate) * lambda_ * 0.1 / norm0) * learning_rate\n                    - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n                    (0 - (1 - learning_rate) * lambda_ * 0.1 / norm0) * learning_rate\n                    - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n                ]\n            ),\n            var0[1].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (1.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1)\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n                    (1.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1)\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n                ]\n            ),\n            var1[2].numpy(),\n        )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_sharing_frobenius(dtype):\n    var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n    var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n    grads0 = tf.constant([0.1, 0.1], dtype=dtype)\n    grads1 = tf.constant([0.01, 0.01], dtype=dtype)\n    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5\n    norm1 = tf.math.reduce_sum(grads1 ** 2) ** 0.5\n    learning_rate = 0.1\n    lambda_ = 0.1\n    ord = ""fro""\n    cg_opt = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n    # Check we have slots\n    assert [""conditional_gradient""] == cg_opt.get_slot_names()\n    slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n    assert slot0.get_shape() == var0.get_shape()\n    slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n    assert slot1.get_shape() == var1.get_shape()\n\n    # Because in the eager mode, as we declare two cg_update\n    # variables, it already altomatically finish executing them.\n    # Thus, we cannot test the param value at this time for\n    # eager mode. We can only test the final value of param\n    # after the second execution.\n\n    # Step 2: the second conditional_gradient contain\n    # the previous update.\n    # Check that the parameters have been updated.\n    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (1.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.1 / norm0)\n                * learning_rate\n                - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n                (2.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.1 / norm0)\n                * learning_rate\n                - (1 - learning_rate) * lambda_ * 0.1 / norm0,\n            ]\n        ),\n        var0.numpy(),\n    )\n    test_utils.assert_allclose_according_to_type(\n        np.array(\n            [\n                (3.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1)\n                * learning_rate\n                - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n                (4.0 * learning_rate - (1 - learning_rate) * lambda_ * 0.01 / norm1)\n                * learning_rate\n                - (1 - learning_rate) * lambda_ * 0.01 / norm1,\n            ]\n        ),\n        var1.numpy(),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sharing_nuclear():\n    # TODO:\n    #       To address the issue #36764.\n    for dtype in _dtypes_with_checking_system(\n        use_gpu=test_utils.is_gpu_available(), system=platform.system()\n    ):\n        var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n        var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n        grads0 = tf.constant([0.1, 0.1], dtype=dtype)\n        grads1 = tf.constant([0.01, 0.01], dtype=dtype)\n        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)\n        top_singular_vector1 = cg_lib.ConditionalGradient._top_singular_vector(grads1)\n        learning_rate = 0.1\n        lambda_ = 0.1\n        ord = ""nuclear""\n        cg_opt = cg_lib.ConditionalGradient(\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\n        )\n        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check we have slots\n        assert [""conditional_gradient""] == cg_opt.get_slot_names()\n        slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n        assert slot0.get_shape() == var0.get_shape()\n        slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n        assert slot1.get_shape() == var1.get_shape()\n\n        # Because in the eager mode, as we declare two cg_update\n        # variables, it already altomatically finish executing them.\n        # Thus, we cannot test the param value at this time for\n        # eager mode. We can only test the final value of param\n        # after the second execution.\n\n        # Step 2: the second conditional_gradient contain\n        # the previous update.\n        # Check that the parameters have been updated.\n        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (\n                        1.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector0[0]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[0],\n                    (\n                        2.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector0[1]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[1],\n                ]\n            ),\n            var0.numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (\n                        3.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector1[0]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[0],\n                    (\n                        4.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector1[1]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[1],\n                ]\n            ),\n            var1.numpy(),\n        )\n\n\ndef _db_params_nuclear_cg01():\n    """"""Return dist-belief conditional_gradient values.\n\n    Return values been generated from the dist-belief\n    conditional_gradient unittest, running with a learning rate of 0.1\n    and a lambda_ of 0.1.\n\n    These values record how a parameter vector of size 10, initialized\n    with 0.0, gets updated with 10 consecutive conditional_gradient\n    steps.\n    It uses random gradients.\n\n    Returns:\n        db_grad: The gradients to apply\n        db_out: The parameters after the conditional_gradient update.\n    """"""\n    db_grad = [[]] * 10\n    db_out = [[]] * 10\n    db_grad[0] = [\n        0.00096264342,\n        0.17914793,\n        0.93945462,\n        0.41396621,\n        0.53037018,\n        0.93197989,\n        0.78648776,\n        0.50036013,\n        0.55345792,\n        0.96722615,\n    ]\n    db_out[0] = [\n        -4.1552783e-05,\n        -7.7334875e-03,\n        -4.0554535e-02,\n        -1.7870164e-02,\n        -2.2895109e-02,\n        -4.0231861e-02,\n        -3.3951234e-02,\n        -2.1599628e-02,\n        -2.3891764e-02,\n        -4.1753381e-02,\n    ]\n    db_grad[1] = [\n        0.17075552,\n        0.88821375,\n        0.20873757,\n        0.25236958,\n        0.57578111,\n        0.15312378,\n        0.5513742,\n        0.94687688,\n        0.16012503,\n        0.22159521,\n    ]\n    db_out[1] = [\n        -0.00961733,\n        -0.0507779,\n        -0.01580694,\n        -0.01599489,\n        -0.03470477,\n        -0.01264373,\n        -0.03443632,\n        -0.05546713,\n        -0.01140388,\n        -0.01665068,\n    ]\n    db_grad[2] = [\n        0.35077485,\n        0.47304362,\n        0.44412705,\n        0.44368884,\n        0.078527533,\n        0.81223965,\n        0.31168157,\n        0.43203235,\n        0.16792089,\n        0.24644311,\n    ]\n    db_out[2] = [\n        -0.02462724,\n        -0.03699233,\n        -0.03154433,\n        -0.03153357,\n        -0.00876844,\n        -0.05606324,\n        -0.02447166,\n        -0.03469437,\n        -0.0124694,\n        -0.01829169,\n    ]\n    db_grad[3] = [\n        0.9694621,\n        0.75035888,\n        0.28171822,\n        0.83813518,\n        0.53807181,\n        0.3728098,\n        0.81454384,\n        0.03848977,\n        0.89759839,\n        0.93665648,\n    ]\n    db_out[3] = [\n        -0.04124615,\n        -0.03371741,\n        -0.0144246,\n        -0.03668303,\n        -0.02240246,\n        -0.02052062,\n        -0.03503307,\n        -0.00500922,\n        -0.03715545,\n        -0.0393002,\n    ]\n    db_grad[4] = [\n        0.38578293,\n        0.8536852,\n        0.88722926,\n        0.66276771,\n        0.13678469,\n        0.94036359,\n        0.69107032,\n        0.81897682,\n        0.5433259,\n        0.67860287,\n    ]\n    db_out[4] = [\n        -0.01979207,\n        -0.0380417,\n        -0.03747472,\n        -0.0305847,\n        -0.00779536,\n        -0.04024221,\n        -0.03156913,\n        -0.0337613,\n        -0.02578116,\n        -0.03148951,\n    ]\n    db_grad[5] = [\n        0.27885768,\n        0.76100707,\n        0.24625534,\n        0.81354135,\n        0.18959245,\n        0.48038563,\n        0.84163809,\n        0.41172323,\n        0.83259648,\n        0.44941229,\n    ]\n    db_out[5] = [\n        -0.01555188,\n        -0.04084422,\n        -0.01573331,\n        -0.04265549,\n        -0.01000746,\n        -0.02740575,\n        -0.04412147,\n        -0.02341569,\n        -0.0431026,\n        -0.02502293,\n    ]\n    db_grad[6] = [\n        0.27233034,\n        0.056316052,\n        0.5039115,\n        0.24105175,\n        0.35697976,\n        0.75913221,\n        0.73577434,\n        0.16014607,\n        0.57500273,\n        0.071136251,\n    ]\n    db_out[6] = [\n        -0.01890448,\n        -0.00767214,\n        -0.03367592,\n        -0.01962219,\n        -0.02374278,\n        -0.05110246,\n        -0.05128598,\n        -0.01254396,\n        -0.04094184,\n        -0.00703416,\n    ]\n    db_grad[7] = [\n        0.58697265,\n        0.2494842,\n        0.08106143,\n        0.39954534,\n        0.15892942,\n        0.12683646,\n        0.74053431,\n        0.16033,\n        0.66625422,\n        0.73515922,\n    ]\n    db_out[7] = [\n        -0.03772915,\n        -0.01599993,\n        -0.00831695,\n        -0.0263572,\n        -0.01207801,\n        -0.01285448,\n        -0.05034329,\n        -0.01104364,\n        -0.04477356,\n        -0.04558992,\n    ]\n    db_grad[8] = [\n        0.8215279,\n        0.41994119,\n        0.95172721,\n        0.68000203,\n        0.79439718,\n        0.43384039,\n        0.55561525,\n        0.22567581,\n        0.93331909,\n        0.29438227,\n    ]\n    db_out[8] = [\n        -0.03919835,\n        -0.01970845,\n        -0.04187151,\n        -0.03195836,\n        -0.03546333,\n        -0.01999326,\n        -0.02899324,\n        -0.01083582,\n        -0.04472339,\n        -0.01725317,\n    ]\n    db_grad[9] = [\n        0.68297005,\n        0.67758518,\n        0.1748755,\n        0.13266537,\n        0.70697063,\n        0.055731893,\n        0.68593478,\n        0.50580865,\n        0.12602448,\n        0.093537711,\n    ]\n    db_out[9] = [\n        -0.04510314,\n        -0.04282944,\n        -0.0147322,\n        -0.0111956,\n        -0.04617687,\n        -0.00535998,\n        -0.0442614,\n        -0.031584,\n        -0.01207165,\n        -0.00736567,\n    ]\n    return db_grad, db_out\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_nuclear():\n    # TODO:\n    #       To address the issue #347 and issue #36764.\n    for dtype in _dtypes_with_checking_system(\n        use_gpu=test_utils.is_gpu_available(), system=platform.system()\n    ):\n        var0 = tf.Variable(tf.zeros([4, 2], dtype=dtype))\n        var1 = tf.Variable(tf.constant(1.0, dtype, [4, 2]))\n        grads0 = tf.IndexedSlices(\n            tf.constant([[0.1, 0.1]], dtype=dtype),\n            tf.constant([1]),\n            tf.constant([4, 2]),\n        )\n        grads1 = tf.IndexedSlices(\n            tf.constant([[0.01, 0.01], [0.01, 0.01]], dtype=dtype),\n            tf.constant([2, 3]),\n            tf.constant([4, 2]),\n        )\n        top_singular_vector0 = tf.constant(\n            [[0.0, 0.0], [0.7071067, 0.7071067], [0.0, 0.0], [0.0, 0.0]], dtype=dtype,\n        )\n        top_singular_vector1 = tf.constant(\n            [\n                [-4.2146844e-08, -4.2146844e-08],\n                [0.0000000e00, 0.0000000e00],\n                [4.9999994e-01, 4.9999994e-01],\n                [4.9999994e-01, 4.9999994e-01],\n            ],\n            dtype=dtype,\n        )\n        learning_rate = 0.1\n        lambda_ = 0.1\n        ord = ""nuclear""\n        cg_opt = cg_lib.ConditionalGradient(\n            learning_rate=learning_rate, lambda_=lambda_, ord=ord\n        )\n        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check we have slots\n        assert [""conditional_gradient""] == cg_opt.get_slot_names()\n        slot0 = cg_opt.get_slot(var0, ""conditional_gradient"")\n        assert slot0.get_shape() == var0.get_shape()\n        slot1 = cg_opt.get_slot(var1, ""conditional_gradient"")\n        assert slot1.get_shape() == var1.get_shape()\n\n        # Check that the parameters have been updated.\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    0 - (1 - learning_rate) * lambda_ * top_singular_vector0[0][0],\n                    0 - (1 - learning_rate) * lambda_ * top_singular_vector0[0][1],\n                ]\n            ),\n            var0[0].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    0 - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0],\n                    0 - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1],\n                ]\n            ),\n            var0[1].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    1.0 * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[2][0],\n                    1.0 * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[2][1],\n                ]\n            ),\n            var1[2].numpy(),\n        )\n        # Step 2: the conditional_gradient contain the\n        # previous update.\n        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        # Check that the parameters have been updated.\n        np.testing.assert_allclose(np.array([0, 0]), var0[0].numpy())\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (0 - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0])\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[1][0],\n                    (0 - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1])\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector0[1][1],\n                ]\n            ),\n            var0[1].numpy(),\n        )\n        test_utils.assert_allclose_according_to_type(\n            np.array(\n                [\n                    (\n                        1.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector1[2][0]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[2][0],\n                    (\n                        1.0 * learning_rate\n                        - (1 - learning_rate) * lambda_ * top_singular_vector1[2][1]\n                    )\n                    * learning_rate\n                    - (1 - learning_rate) * lambda_ * top_singular_vector1[2][1],\n                ]\n            ),\n            var1[2].numpy(),\n        )\n\n\ndef test_serialization():\n    learning_rate = 0.1\n    lambda_ = 0.1\n    ord = ""nuclear""\n    optimizer = cg_lib.ConditionalGradient(\n        learning_rate=learning_rate, lambda_=lambda_, ord=ord\n    )\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert optimizer.get_config() == new_optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/cyclical_learning_rate_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Cyclical Learning Rate.""""""\n\nimport pytest\nimport numpy as np\n\nfrom tensorflow_addons.optimizers import cyclical_learning_rate\n\n\ndef _maybe_serialized(lr_decay, serialize_and_deserialize):\n    if serialize_and_deserialize:\n        serialized = lr_decay.get_config()\n        return lr_decay.from_config(serialized)\n    else:\n        return lr_decay\n\n\n@pytest.mark.parametrize(""serialize"", [True, False])\ndef test_triangular_cyclical_learning_rate(serialize):\n    initial_learning_rate = 0.1\n    max_learning_rate = 1\n    step_size = 40\n    triangular_cyclical_lr = cyclical_learning_rate.TriangularCyclicalLearningRate(\n        initial_learning_rate=initial_learning_rate,\n        maximal_learning_rate=max_learning_rate,\n        step_size=step_size,\n    )\n    triangular_cyclical_lr = _maybe_serialized(triangular_cyclical_lr, serialize)\n\n    expected = np.concatenate(\n        [\n            np.linspace(initial_learning_rate, max_learning_rate, num=step_size + 1),\n            np.linspace(max_learning_rate, initial_learning_rate, num=step_size + 1)[\n                1:\n            ],\n        ]\n    )\n\n    for step, expected_value in enumerate(expected):\n        np.testing.assert_allclose(triangular_cyclical_lr(step), expected_value, 1e-6)\n\n\n@pytest.mark.parametrize(""serialize"", [True, False])\ndef test_triangular2_cyclical_learning_rate(serialize):\n    initial_lr = 0.1\n    maximal_lr = 1\n    step_size = 30\n    triangular2_lr = cyclical_learning_rate.Triangular2CyclicalLearningRate(\n        initial_learning_rate=initial_lr,\n        maximal_learning_rate=maximal_lr,\n        step_size=step_size,\n    )\n    triangular2_lr = _maybe_serialized(triangular2_lr, serialize)\n\n    middle_lr = (maximal_lr + initial_lr) / 2\n    expected = np.concatenate(\n        [\n            np.linspace(initial_lr, maximal_lr, num=step_size + 1),\n            np.linspace(maximal_lr, initial_lr, num=step_size + 1)[1:],\n            np.linspace(initial_lr, middle_lr, num=step_size + 1)[1:],\n            np.linspace(middle_lr, initial_lr, num=step_size + 1)[1:],\n        ]\n    )\n\n    for step, expected_value in enumerate(expected):\n        np.testing.assert_allclose(triangular2_lr(step).numpy(), expected_value, 1e-6)\n\n\n@pytest.mark.parametrize(""serialize"", [True, False])\ndef test_exponential_cyclical_learning_rate(serialize):\n    initial_learning_rate = 0.1\n    maximal_learning_rate = 1\n    step_size = 2000\n    gamma = 0.996\n\n    step = 0\n    exponential_cyclical_lr = cyclical_learning_rate.ExponentialCyclicalLearningRate(\n        initial_learning_rate=initial_learning_rate,\n        maximal_learning_rate=maximal_learning_rate,\n        step_size=step_size,\n        gamma=gamma,\n    )\n    exponential_cyclical_lr = _maybe_serialized(exponential_cyclical_lr, serialize)\n\n    for i in range(0, 8001):\n        non_bounded_value = np.abs(i / 2000.0 - 2 * np.floor(1 + i / (2 * 2000)) + 1)\n        expected = initial_learning_rate + (\n            maximal_learning_rate - initial_learning_rate\n        ) * np.maximum(0, (1 - non_bounded_value)) * (gamma ** i)\n        computed = exponential_cyclical_lr(step).numpy()\n        np.testing.assert_allclose(computed, expected, 1e-6)\n        step += 1\n\n\n@pytest.mark.parametrize(""serialize"", [True, False])\ndef test_custom_cyclical_learning_rate(serialize):\n    initial_learning_rate = 0.1\n    maximal_learning_rate = 1\n    step_size = 4000\n\n    def scale_fn(x):\n        return 1 / (5 ** (x * 0.0001))\n\n    custom_cyclical_lr = cyclical_learning_rate.CyclicalLearningRate(\n        initial_learning_rate=initial_learning_rate,\n        maximal_learning_rate=maximal_learning_rate,\n        step_size=step_size,\n        scale_fn=scale_fn,\n    )\n    custom_cyclical_lr = _maybe_serialized(custom_cyclical_lr, serialize)\n\n    for step in range(1, 8001):\n        cycle = np.floor(1 + step / (2 * step_size))\n        non_bounded_value = np.abs(step / step_size - 2 * cycle + 1)\n        expected = initial_learning_rate + (\n            maximal_learning_rate - initial_learning_rate\n        ) * np.maximum(0, 1 - non_bounded_value) * scale_fn(cycle)\n        np.testing.assert_allclose(\n            custom_cyclical_lr(step).numpy(), expected, 1e-6, 1e-6\n        )\n\n\n@pytest.mark.parametrize(""serialize"", [True, False])\ndef test_custom_cyclical_learning_rate_with_scale_mode(serialize):\n    initial_learning_rate = 0.1\n    maximal_learning_rate = 1\n    step_size = 4000\n    scale_mode = ""iterations""\n\n    def scale_fn(x):\n        return 1 / (5 ** (x * 0.0001))\n\n    custom_cyclical_lr = cyclical_learning_rate.CyclicalLearningRate(\n        initial_learning_rate=initial_learning_rate,\n        maximal_learning_rate=maximal_learning_rate,\n        step_size=step_size,\n        scale_fn=scale_fn,\n        scale_mode=scale_mode,\n    )\n    custom_cyclical_lr = _maybe_serialized(custom_cyclical_lr, serialize)\n\n    for step in range(1, 8001):\n        cycle = np.floor(1 + step / (2 * step_size))\n        non_bounded_value = np.abs(step / step_size - 2 * cycle + 1)\n        expected = initial_learning_rate + (\n            maximal_learning_rate - initial_learning_rate\n        ) * np.maximum(0, 1 - non_bounded_value) * scale_fn(step)\n        np.testing.assert_allclose(\n            custom_cyclical_lr(step).numpy(), expected, 1e-6, 1e-6\n        )\n'"
tensorflow_addons/optimizers/tests/lamb_test.py,46,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for LAMB Optimizer.""""""\n\n\nimport numpy as np\nfrom numpy import linalg\nimport pytest\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import lamb\nfrom tensorflow_addons.utils import test_utils\n\n\ndef _dtypes_to_test(use_gpu):\n    # Based on issue #347 (https://github.com/tensorflow/addons/issues/347)\n    # tf.half is not registered for \'ResourceScatterUpdate\' OpKernel for \'GPU\'.\n    # So we have to remove tf.half when testing with gpu.\n    if use_gpu:\n        return [tf.float32, tf.float64]\n    else:\n        return [tf.half, tf.float32, tf.float64]\n\n\ndef lamb_update_numpy(\n    param, g_t, t, m, v, lr=0.001, lamb_wd=0.0, beta1=0.9, beta2=0.999, epsilon=1e-6\n):\n\n    m_t = beta1 * m + (1 - beta1) * g_t\n    v_t = beta2 * v + (1 - beta2) * g_t * g_t\n\n    m_t_hat = m_t / (1 - beta1 ** (t + 1))\n    v_t_hat = v_t / (1 - beta2 ** (t + 1))\n    update = m_t_hat / (np.sqrt(v_t_hat) + epsilon)\n\n    update += lamb_wd * param\n\n    w_norm = linalg.norm(param, ord=2)\n    g_norm = linalg.norm(update, ord=2)\n    ratio = np.where(w_norm > 0, np.where(g_norm > 0, (w_norm / g_norm), 1.0), 1.0)\n\n    param_t = param - ratio * lr * update\n    return param_t, m_t, v_t\n\n\ndef get_beta_accumulators(opt, dtype):\n    local_step = tf.cast(opt.iterations + 1, dtype)\n    beta_1_t = tf.cast(opt._get_hyper(""beta_1""), dtype)\n    beta_1_power = tf.math.pow(beta_1_t, local_step)\n    beta_2_t = tf.cast(opt._get_hyper(""beta_2""), dtype)\n    beta_2_power = tf.math.pow(beta_2_t, local_step)\n    return (beta_1_power, beta_2_power)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize tf for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.0, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.0, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0_np_indices = np.array([0, 2], dtype=np.int32)\n        grads0 = tf.IndexedSlices(\n            tf.constant(grads0_np[grads0_np_indices]),\n            tf.constant(grads0_np_indices),\n            tf.constant([3]),\n        )\n        grads1_np_indices = np.array([0, 2], dtype=np.int32)\n        grads1 = tf.IndexedSlices(\n            tf.constant(grads1_np[grads1_np_indices]),\n            tf.constant(grads1_np_indices),\n            tf.constant([3]),\n        )\n        opt = lamb.LAMB()\n\n        # Fetch params to validate initial values\n        np.testing.assert_allclose(np.asanyarray([1.0, 1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of LAMB\n        for t in range(3):\n            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic_with_learning_rate_decay():\n    for i, dtype in enumerate(_dtypes_to_test(use_gpu=test_utils.is_gpu_available())):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np, name=""var0_%d"" % i)\n        var1 = tf.Variable(var1_np, name=""var1_%d"" % i)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n\n        learning_rate = 0.001\n        beta_1 = 0.9\n        beta_2 = 0.999\n        epsilon = 1e-7\n        decay = 0.5\n        lamb_wd = 0.01\n\n        opt = lamb.LAMB(\n            learning_rate=learning_rate,\n            beta_1=beta_1,\n            beta_2=beta_2,\n            epsilon=epsilon,\n            weight_decay_rate=lamb_wd,\n            decay=decay,\n        )\n\n        # Run 3 steps of LAMB\n        for t in range(3):\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            lr_np = learning_rate / (1 + decay * t)\n\n            var0_np, m0, v0 = lamb_update_numpy(\n                var0_np, grads0_np, t, m0, v0, lr=lr_np, lamb_wd=lamb_wd\n            )\n            var1_np, m1, v1 = lamb_update_numpy(\n                var1_np, grads1_np, t, m1, v1, lr=lr_np, lamb_wd=lamb_wd\n            )\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic_with_learning_rate_inverse_time_decay():\n    for i, dtype in enumerate(_dtypes_to_test(use_gpu=test_utils.is_gpu_available())):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np, name=""var0_%d"" % i)\n        var1 = tf.Variable(var1_np, name=""var1_%d"" % i)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n\n        learning_rate = 0.001\n        decay = 0.5\n        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n            learning_rate, decay_steps=1.0, decay_rate=decay\n        )\n        beta_1 = 0.9\n        beta_2 = 0.999\n        epsilon = 1e-7\n\n        opt = lamb.LAMB(\n            learning_rate=lr_schedule, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon\n        )\n\n        # Run 3 steps of LAMB\n        for t in range(3):\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            lr_np = learning_rate / (1 + decay * t)\n\n            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0, lr=lr_np)\n            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1, lr=lr_np)\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_tensor_learning_rate():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n        opt = lamb.LAMB(tf.constant(0.001))\n\n        # Fetch params to validate initial values\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of LAMB\n        for t in range(3):\n            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sharing():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n        opt = lamb.LAMB()\n\n        # Fetch params to validate initial values\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of intertwined LAMB1 and LAMB2.\n        for t in range(3):\n            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\ndef test_minimize_mean_square_loss_with_weight_decay():\n    w = tf.Variable([0.1, -0.2, -0.1])\n    x = tf.constant([0.4, 0.2, -0.5])\n\n    def loss():\n        return tf.reduce_mean(tf.square(x - w))\n\n    opt = lamb.LAMB(0.02, weight_decay_rate=0.01)\n\n    # Run 200 steps\n    for _ in range(200):\n        opt.minimize(loss, [w])\n    # Validate updated params\n    np.testing.assert_allclose(\n        w.numpy(), np.asanyarray([0.4, 0.2, -0.5]), rtol=1e-2, atol=1e-2\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_resource():\n    for i, dtype in enumerate(_dtypes_to_test(use_gpu=test_utils.is_gpu_available())):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np, name=""var0_%d"" % i)\n        var1 = tf.Variable(var1_np, name=""var1_%d"" % i)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n\n        def learning_rate():\n            return 0.001\n\n        opt = lamb.LAMB(learning_rate=learning_rate)\n\n        # Run 3 steps of LAMB\n        for t in range(3):\n            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\ndef test_get_config():\n    opt = lamb.LAMB(1e-4)\n    config = opt.get_config()\n    assert config[""learning_rate""] == 1e-4\n\n\ndef test_exclude_weight_decay():\n    opt = lamb.LAMB(0.01, weight_decay_rate=0.01, exclude_from_weight_decay=[""var1""])\n    assert opt._do_use_weight_decay(""var0"")\n    assert not opt._do_use_weight_decay(""var1"")\n    assert not opt._do_use_weight_decay(""var1_weight"")\n\n\ndef test_exclude_layer_adaptation():\n    opt = lamb.LAMB(0.01, exclude_from_layer_adaptation=[""var1""])\n    assert opt._do_layer_adaptation(""var0"")\n    assert not opt._do_layer_adaptation(""var1"")\n    assert not opt._do_layer_adaptation(""var1_weight"")\n\n\ndef test_serialization():\n    optimizer = lamb.LAMB(1e-4)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/lazy_adam_test.py,55,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for LazyAdam.""""""\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import lazy_adam\nfrom tensorflow_addons.utils import test_utils\nimport pytest\n\n\ndef adam_update_numpy(\n    param, g_t, t, m, v, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7\n):\n    lr_t = lr * np.sqrt(1 - beta2 ** (t + 1)) / (1 - beta1 ** (t + 1))\n\n    m_t = beta1 * m + (1 - beta1) * g_t\n    v_t = beta2 * v + (1 - beta2) * g_t * g_t\n\n    param_t = param - lr_t * m_t / (np.sqrt(v_t) + epsilon)\n    return param_t, m_t, v_t\n\n\ndef get_beta_accumulators(opt, dtype):\n    local_step = tf.cast(opt.iterations + 1, dtype)\n    beta_1_t = tf.cast(opt._get_hyper(""beta_1""), dtype)\n    beta_1_power = tf.math.pow(beta_1_t, local_step)\n    beta_2_t = tf.cast(opt._get_hyper(""beta_2""), dtype)\n    beta_2_power = tf.math.pow(beta_2_t, local_step)\n    return (beta_1_power, beta_2_power)\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse(dtype):\n    # TODO: remove the with tf.device when the execution on cpu is enforced\n    # See #1682 to track it.\n    with tf.device(""CPU:0""):\n        _test_sparse(dtype)\n\n\ndef _test_sparse(dtype):\n    # Initialize tf for numpy implementation.\n    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n    var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)\n    grads0_np = np.array([0.1, 0.0, 0.1], dtype=dtype.as_numpy_dtype)\n    var1_np = np.array([3.0, 3.0, 4.0], dtype=dtype.as_numpy_dtype)\n    grads1_np = np.array([0.01, 0.0, 0.01], dtype=dtype.as_numpy_dtype)\n\n    var0 = tf.Variable(var0_np)\n    var1 = tf.Variable(var1_np)\n    grads0_np_indices = np.array([0, 2], dtype=np.int32)\n    grads0 = tf.IndexedSlices(\n        tf.constant(grads0_np[grads0_np_indices]),\n        tf.constant(grads0_np_indices),\n        tf.constant([3]),\n    )\n    grads1_np_indices = np.array([0, 2], dtype=np.int32)\n    grads1 = tf.IndexedSlices(\n        tf.constant(grads1_np[grads1_np_indices]),\n        tf.constant(grads1_np_indices),\n        tf.constant([3]),\n    )\n    opt = lazy_adam.LazyAdam()\n\n    # Fetch params to validate initial values\n    np.testing.assert_allclose([1.0, 1.0, 2.0], var0.numpy(), 1e-6, 1e-6)\n    np.testing.assert_allclose([3.0, 3.0, 4.0], var1.numpy(), 1e-6, 1e-6)\n\n    # Run 3 steps of Adam\n    for t in range(3):\n        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)\n        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.parametrize(""dtype"", [tf.int32, tf.int64])\n@pytest.mark.with_device([""cpu"", ""gpu""])\ndef test_sparse_device_placement(dtype):\n\n    # If a GPU is available, tests that all optimizer ops can be placed on\n    # it (i.e. they have GPU kernels).\n    var = tf.Variable([[1.0], [2.0]])\n    indices = tf.constant([0, 1], dtype=dtype)\n\n    def g_sum():\n        return tf.math.reduce_sum(tf.gather(var, indices))\n\n    optimizer = lazy_adam.LazyAdam(3.0)\n    optimizer.minimize(g_sum, var_list=[var])\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_sparse_repeated_indices(dtype):\n    # todo: remove the with tf.device once the placement on cpu is enforced.\n    with tf.device(""CPU:0""):\n        repeated_index_update_var = tf.Variable([[1], [2]], dtype=dtype)\n        aggregated_update_var = tf.Variable([[1], [2]], dtype=dtype)\n        grad_repeated_index = tf.IndexedSlices(\n            tf.constant([0.1, 0.1], shape=[2, 1], dtype=dtype),\n            tf.constant([1, 1]),\n            tf.constant([2, 1]),\n        )\n        grad_aggregated = tf.IndexedSlices(\n            tf.constant([0.2], shape=[1, 1], dtype=dtype),\n            tf.constant([1]),\n            tf.constant([2, 1]),\n        )\n        repeated_update_opt = lazy_adam.LazyAdam()\n        aggregated_update_opt = lazy_adam.LazyAdam()\n        for _ in range(3):\n            repeated_update_opt.apply_gradients(\n                [(grad_repeated_index, repeated_index_update_var)]\n            )\n            aggregated_update_opt.apply_gradients(\n                [(grad_aggregated, aggregated_update_var)]\n            )\n            np.testing.assert_allclose(\n                aggregated_update_var.numpy(), repeated_index_update_var.numpy()\n            )\n\n\n@pytest.mark.parametrize(""use_callable_params"", [True, False])\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_basic(use_callable_params, dtype):\n    # Initialize tf for numpy implementation.\n    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n    var0 = tf.Variable(var0_np)\n    var1 = tf.Variable(var1_np)\n    grads0 = tf.constant(grads0_np)\n    grads1 = tf.constant(grads1_np)\n\n    def learning_rate():\n        return 0.001\n\n    if not use_callable_params:\n        learning_rate = learning_rate()\n\n    opt = lazy_adam.LazyAdam(learning_rate=learning_rate)\n\n    # Run 3 steps of Adam\n    for t in range(3):\n        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)\n        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_tensor_learning_rate(dtype):\n    # Initialize tf for numpy implementation.\n    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n    var0 = tf.Variable(var0_np)\n    var1 = tf.Variable(var1_np)\n    grads0 = tf.constant(grads0_np)\n    grads1 = tf.constant(grads1_np)\n    opt = lazy_adam.LazyAdam(tf.constant(0.001))\n\n    # Run 3 steps of Adam\n    for t in range(3):\n        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)\n        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sharing(dtype):\n    # Initialize tf for numpy implementation.\n    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0\n    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n    var0 = tf.Variable(var0_np)\n    var1 = tf.Variable(var1_np)\n    grads0 = tf.constant(grads0_np)\n    grads1 = tf.constant(grads1_np)\n    opt = lazy_adam.LazyAdam()\n\n    # Fetch params to validate initial values\n    np.testing.assert_allclose([1.0, 2.0], var0.numpy())\n    np.testing.assert_allclose([3.0, 4.0], var1.numpy())\n\n    # Run 3 steps of intertwined Adam1 and Adam2.\n    for t in range(3):\n        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)\n        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)\n        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)\n        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)\n        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\ndef test_slots_unique_eager():\n    v1 = tf.Variable(1.0)\n    v2 = tf.Variable(1.0)\n    opt = lazy_adam.LazyAdam(1.0)\n    opt.minimize(lambda: v1 + v2, var_list=[v1, v2])\n    # There should be iteration, and two unique slot variables for v1 and v2.\n    assert 5 == len(opt.variables())\n    assert opt.variables()[0] == opt.iterations\n\n\ndef test_serialization():\n    optimizer = lazy_adam.LazyAdam()\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/lookahead_test.py,24,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Lookahead optimizer.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import Lookahead\n\n\ndef run_dense_sample(iterations, optimizer, seed=0x2019):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    val_0 = np.random.random((2,))\n    val_1 = np.random.random((2,))\n\n    var_0 = tf.Variable(val_0, dtype=tf.dtypes.float32)\n    var_1 = tf.Variable(val_1, dtype=tf.dtypes.float32)\n\n    grad_0 = tf.constant(np.random.standard_normal((2,)), dtype=tf.dtypes.float32)\n    grad_1 = tf.constant(np.random.standard_normal((2,)), dtype=tf.dtypes.float32)\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    return [val_0, val_1], [var_0, var_1]\n\n\ndef run_sparse_sample(iterations, optimizer, seed=0x2019):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    val_0 = np.random.random((2,))\n    val_1 = np.random.random((2,))\n\n    var_0 = tf.Variable(val_0, dtype=tf.dtypes.float32)\n    var_1 = tf.Variable(val_1, dtype=tf.dtypes.float32)\n\n    grad_0 = tf.IndexedSlices(\n        tf.constant([np.random.standard_normal()]), tf.constant([0]), tf.constant([2]),\n    )\n    grad_1 = tf.IndexedSlices(\n        tf.constant([np.random.standard_normal()]), tf.constant([1]), tf.constant([2]),\n    )\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    return [val_0, val_1], [var_0, var_1]\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_exact_ratio():\n    for k in [5, 10, 100]:\n        for alpha in [0.3, 0.7]:\n            optimizer = tf.keras.optimizers.get(""adam"")\n            vals, quick_vars = run_dense_sample(k, optimizer)\n            optimizer = Lookahead(""adam"", sync_period=k, slow_step_size=alpha)\n            _, slow_vars = run_dense_sample(k, optimizer)\n            for val, quick, slow in zip(vals, quick_vars, slow_vars):\n                expected = val + (quick - val) * alpha\n                np.testing.assert_allclose(\n                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06\n                )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_exact_ratio():\n    for k in [5, 10, 100]:\n        for alpha in [0.3, 0.7]:\n            optimizer = tf.keras.optimizers.get(""adam"")\n            vals, quick_vars = run_sparse_sample(k, optimizer)\n            optimizer = Lookahead(""adam"", sync_period=k, slow_step_size=alpha)\n            _, slow_vars = run_sparse_sample(k, optimizer)\n            for val, quick, slow in zip(vals, quick_vars, slow_vars):\n                expected = val + (quick - val) * alpha\n                np.testing.assert_allclose(\n                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06\n                )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_fit_simple_linear_model():\n    np.random.seed(0x2019)\n    tf.random.set_seed(0x2019)\n\n    x = np.random.standard_normal((10000, 3))\n    w = np.random.standard_normal((3, 1))\n    y = np.dot(x, w) + np.random.standard_normal((10000, 1)) * 1e-4\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))\n    model.compile(Lookahead(""sgd""), loss=""mse"")\n\n    model.fit(x, y, epochs=3)\n\n    x = np.random.standard_normal((100, 3))\n    y = np.dot(x, w)\n    predicted = model.predict(x)\n\n    max_abs_diff = np.max(np.abs(predicted - y))\n    assert max_abs_diff < 1e-3\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_model_dynamic_lr():\n    grad = tf.Variable([[0.1]])\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(\n                1,\n                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n                use_bias=False,\n            )\n        ]\n    )\n    model.build(input_shape=[1, 1])\n\n    opt = Lookahead(""adam"", sync_period=10, slow_step_size=0.4)\n    _ = opt.apply_gradients(list(zip([grad], model.variables)))\n\n    np.testing.assert_allclose(opt.lr.read_value(), 1e-3)\n\n    opt.lr = 1e-4\n    np.testing.assert_allclose(opt.lr.read_value(), 1e-4)\n\n\ndef test_get_config():\n    opt = Lookahead(""adam"", sync_period=10, slow_step_size=0.4)\n    opt = tf.keras.optimizers.deserialize(tf.keras.optimizers.serialize(opt))\n    config = opt.get_config()\n    assert config[""sync_period""] == 10\n    assert config[""slow_step_size""] == 0.4\n\n\ndef test_serialization():\n    optimizer = Lookahead(""adam"", sync_period=10, slow_step_size=0.4)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/moving_average_test.py,22,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for MovingAverage optimizers.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import MovingAverage\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_run():\n    var0 = tf.Variable([1.0, 2.0])\n    var1 = tf.Variable([3.0, 4.0])\n\n    grads0 = tf.constant([0.1, 0.1])\n    grads1 = tf.constant([0.01, 0.01])\n\n    grads_and_vars = list(zip([grads0, grads1], [var0, var1]))\n\n    opt = MovingAverage(tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5,)\n\n    opt.apply_gradients(grads_and_vars)\n    opt.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var0.read_value(), [0.6, 1.6])\n    np.testing.assert_allclose(var1.read_value(), [2.96, 3.96])\n\n    ema_var0 = opt.get_slot(var0, ""average"")\n    ema_var1 = opt.get_slot(var1, ""average"")\n\n    np.testing.assert_allclose(ema_var0.read_value(), [0.75, 1.75])\n    np.testing.assert_allclose(ema_var1.read_value(), [2.975, 3.975])\n\n    _ = opt.assign_average_vars([var0, var1])\n\n    np.testing.assert_allclose(var0.read_value(), [0.75, 1.75])\n    np.testing.assert_allclose(var1.read_value(), [2.975, 3.975])\n\n    var0.assign_add([1.0, 1.0]),\n    var1.assign_add([2.0, 2.0]),\n    ema_var0.assign_add([3.0, 3.0]),\n    ema_var1.assign_add([4.0, 4.0]),\n\n    np.testing.assert_allclose(var0.read_value(), [1.75, 2.75])\n    np.testing.assert_allclose(var1.read_value(), [4.975, 5.975])\n    np.testing.assert_allclose(ema_var0.read_value(), [3.75, 4.75])\n    np.testing.assert_allclose(ema_var1.read_value(), [6.975, 7.975])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_opt_failure():\n    base_opt = None\n    with pytest.raises(TypeError):\n        MovingAverage(base_opt, 0.5)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_model_weights_update():\n    grad = tf.Variable([[0.1]])\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(\n                1,\n                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n                use_bias=False,\n            )\n        ]\n    )\n    model.build(input_shape=[1, 1])\n\n    opt = MovingAverage(tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5)\n    _ = opt.apply_gradients(list(zip([grad], model.variables)))\n    np.testing.assert_allclose(model.variables[0].read_value(), [[0.8]])\n    _ = opt.assign_average_vars(model.variables)\n    np.testing.assert_allclose(model.variables[0].read_value(), [[0.9]])\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_model_dynamic_lr():\n    grad = tf.Variable([[0.1]])\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(\n                1,\n                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n                use_bias=False,\n            )\n        ]\n    )\n    model.build(input_shape=[1, 1])\n\n    opt = MovingAverage(tf.keras.optimizers.SGD(lr=1e-3), average_decay=0.5)\n    _ = opt.apply_gradients(list(zip([grad], model.variables)))\n    np.testing.assert_allclose(opt.lr.read_value(), 1e-3)\n    opt.lr = 1e-4\n    np.testing.assert_allclose(opt.lr.read_value(), 1e-4)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_optimizer_string():\n    _ = MovingAverage(""adam"")\n\n\ndef test_config():\n    sgd_opt = tf.keras.optimizers.SGD(lr=2.0, nesterov=True, momentum=0.3, decay=0.1)\n    opt = MovingAverage(sgd_opt, average_decay=0.5, num_updates=None)\n    config = opt.get_config()\n\n    assert config[""average_decay""] == 0.5\n    assert config[""num_updates""] is None\n\n    new_opt = MovingAverage.from_config(config)\n    old_sgd_config = opt._optimizer.get_config()\n    new_sgd_config = new_opt._optimizer.get_config()\n\n    for k1, k2 in zip(old_sgd_config, new_sgd_config):\n        assert old_sgd_config[k1] == new_sgd_config[k2]\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_fit_simple_linear_model():\n    seed = 0x2019\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    num_examples = 5000\n    x = np.random.standard_normal((num_examples, 3))\n    w = np.random.standard_normal((3, 1))\n    y = np.dot(x, w) + np.random.standard_normal((num_examples, 1)) * 1e-4\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))\n\n    opt = MovingAverage(""sgd"")\n    model.compile(opt, loss=""mse"")\n\n    model.fit(x, y, epochs=5)\n    opt.assign_average_vars(model.variables)\n\n    x = np.random.standard_normal((100, 3))\n    y = np.dot(x, w)\n\n    predicted = model.predict(x)\n\n    max_abs_diff = np.max(np.abs(predicted - y))\n    assert max_abs_diff < 5e-3\n\n\ndef test_serialization():\n    sgd_opt = tf.keras.optimizers.SGD(lr=2.0, nesterov=True, momentum=0.3, decay=0.1)\n    optimizer = MovingAverage(sgd_opt, average_decay=0.5, num_updates=None)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/novograd_test.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for NovoGrad Optimizer.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import NovoGrad\n\n\ndef run_dense_sample(iterations, expected, optimizer):\n    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)\n    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)\n\n    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)\n    grad_1 = tf.constant([0.3, 0.4], dtype=tf.dtypes.float32)\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)\n    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample():\n    run_dense_sample(\n        iterations=1,\n        expected=[[0.9552786425, 1.9105572849], [2.9400000012, 3.9200000016]],\n        optimizer=NovoGrad(lr=0.1, epsilon=1e-8),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_weight_decay():\n    run_dense_sample(\n        iterations=1,\n        expected=[[0.945278642, 1.8905572849], [2.9100000012, 3.8800000016]],\n        optimizer=NovoGrad(lr=0.1, weight_decay=0.1, epsilon=1e-8),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_grad_averaging():\n    run_dense_sample(\n        iterations=2,\n        expected=[[0.9105572849, 1.8211145698], [2.8800000024, 3.8400000032]],\n        optimizer=NovoGrad(lr=0.1, grad_averaging=True, epsilon=1e-8),\n    )\n\n\ndef run_sparse_sample(iterations, expected, optimizer):\n    var_0 = tf.Variable([1.0, 2.0])\n    var_1 = tf.Variable([3.0, 4.0])\n\n    grad_0 = tf.IndexedSlices(\n        tf.constant([0.1, 0.2]), tf.constant([0, 1]), tf.constant([2])\n    )\n    grad_1 = tf.IndexedSlices(\n        tf.constant([0.3, 0.4]), tf.constant([0, 1]), tf.constant([2])\n    )\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)\n    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample():\n    run_sparse_sample(\n        iterations=1,\n        expected=[[0.9552786425, 1.9105572849], [2.9400000012, 3.9200000016]],\n        optimizer=NovoGrad(lr=0.1, epsilon=1e-8),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_weight_decay():\n    run_sparse_sample(\n        iterations=1,\n        expected=[[0.945278642, 1.8905572849], [2.9100000012, 3.8800000016]],\n        optimizer=NovoGrad(lr=0.1, weight_decay=0.1, epsilon=1e-8),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_grad_averaging():\n    run_sparse_sample(\n        iterations=2,\n        expected=[[0.9105572849, 1.8211145698], [2.8800000024, 3.8400000032]],\n        optimizer=NovoGrad(lr=0.1, grad_averaging=True, epsilon=1e-8),\n    )\n\n\ndef test_fit_simple_linear_model():\n    np.random.seed(0x2020)\n    tf.random.set_seed(0x2020)\n\n    x = np.random.standard_normal((100000, 3))\n    w = np.random.standard_normal((3, 1))\n    y = np.dot(x, w) + np.random.standard_normal((100000, 1)) * 1e-5\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))\n    model.compile(NovoGrad(), loss=""mse"")\n\n    model.fit(x, y, epochs=2)\n\n    x = np.random.standard_normal((100, 3))\n    y = np.dot(x, w)\n    predicted = model.predict(x)\n\n    max_abs_diff = np.max(np.abs(predicted - y))\n    assert max_abs_diff < 1e-2\n\n\ndef test_get_config():\n    opt = NovoGrad(lr=1e-4, weight_decay=0.0, grad_averaging=False)\n    config = opt.get_config()\n    assert config[""learning_rate""] == 1e-4\n    assert config[""weight_decay""] == 0.0\n    assert config[""grad_averaging""] is False\n\n\ndef test_serialization():\n    optimizer = NovoGrad(lr=1e-4, weight_decay=0.0, grad_averaging=False)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/rectified_adam_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Rectified Adam optimizer.""""""\n\nimport numpy as np\nimport pytest\n\nimport tensorflow as tf\nfrom tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n\n\ndef run_dense_sample(iterations, expected, optimizer):\n    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)\n    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)\n\n    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)\n    grad_1 = tf.constant([0.03, 0.04], dtype=tf.dtypes.float32)\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)\n    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)\n\n\ndef run_sparse_sample(iterations, expected, optimizer):\n    var_0 = tf.Variable([1.0, 2.0])\n    var_1 = tf.Variable([3.0, 4.0])\n\n    grad_0 = tf.IndexedSlices(tf.constant([0.1]), tf.constant([0]), tf.constant([2]))\n    grad_1 = tf.IndexedSlices(tf.constant([0.04]), tf.constant([1]), tf.constant([2]))\n\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    for _ in range(iterations):\n        optimizer.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)\n    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample():\n    # Expected values are obtained from the previous implementation\n    run_dense_sample(\n        iterations=100,\n        expected=[[0.985769, 1.985269], [2.986119, 3.986068]],\n        optimizer=RectifiedAdam(lr=1e-3),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample():\n    # Expected values are obtained from the previous implementation\n    run_sparse_sample(\n        iterations=200,\n        expected=[[0.959333, 2.0], [3.0, 3.959632]],\n        optimizer=RectifiedAdam(lr=1e-3),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_amsgrad():\n    # Expected values are obtained from the official implementation\n    # `amsgrad` has no effect because the gradient is fixed\n    run_dense_sample(\n        iterations=100,\n        expected=[[0.985769, 1.985269], [2.986119, 3.986068]],\n        optimizer=RectifiedAdam(lr=1e-3, amsgrad=True),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_amsgrad():\n    # Expected values are obtained from the official implementation\n    # `amsgrad` has no effect because the gradient is fixed\n    run_sparse_sample(\n        iterations=200,\n        expected=[[0.959333, 2.0], [3.0, 3.959632]],\n        optimizer=RectifiedAdam(lr=1e-3, amsgrad=True),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_weight_decay():\n    # Expected values are obtained from the previous implementation\n    run_dense_sample(\n        iterations=100,\n        expected=[[0.984775, 1.983276], [2.983125, 3.982076]],\n        optimizer=RectifiedAdam(lr=1e-3, weight_decay=0.01),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_weight_decay():\n    # Expected values are obtained from the previous implementation\n    run_sparse_sample(\n        iterations=200,\n        expected=[[0.957368, 2.0], [3.0, 3.951673]],\n        optimizer=RectifiedAdam(lr=1e-3, weight_decay=0.01),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_warmup():\n    run_dense_sample(\n        iterations=100,\n        expected=[[0.994062, 1.993912], [2.994167, 3.994152]],\n        optimizer=RectifiedAdam(\n            lr=1e-3, total_steps=100, warmup_proportion=0.1, min_lr=1e-5,\n        ),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_warmup():\n    run_sparse_sample(\n        iterations=200,\n        expected=[[0.982629, 2.0], [3.0, 3.982674]],\n        optimizer=RectifiedAdam(\n            lr=1e-3, total_steps=200, warmup_proportion=0.1, min_lr=1e-5,\n        ),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_dense_sample_with_lookahead():\n    # Expected values are obtained from the original implementation\n    # of Ranger\n    run_dense_sample(\n        iterations=100,\n        expected=[[0.993126, 1.992901], [2.993283, 3.993261]],\n        optimizer=Lookahead(\n            RectifiedAdam(lr=1e-3, beta_1=0.95,), sync_period=6, slow_step_size=0.45,\n        ),\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_sample_with_lookahead():\n    # Expected values are obtained from the previous implementation\n    # of Ranger.\n    run_sparse_sample(\n        iterations=150,\n        expected=[[0.988156, 2.0], [3.0, 3.988291]],\n        optimizer=Lookahead(\n            RectifiedAdam(lr=1e-3, beta_1=0.95,), sync_period=6, slow_step_size=0.45,\n        ),\n    )\n\n\ndef test_get_config():\n    opt = RectifiedAdam(lr=1e-4)\n    config = opt.get_config()\n    assert config[""learning_rate""] == 1e-4\n    assert config[""total_steps""] == 0\n\n\ndef test_serialization():\n    optimizer = RectifiedAdam(\n        lr=1e-3, total_steps=10000, warmup_proportion=0.1, min_lr=1e-5,\n    )\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/optimizers/tests/stochastic_weight_averaging_test.py,17,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Stochastic Weight Averaging optimizer.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import stochastic_weight_averaging\nfrom tensorflow_addons.optimizers.utils import fit_bn\n\nSWA = stochastic_weight_averaging.SWA\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_averaging():\n    start_averaging = 0\n    average_period = 1\n    sgd = tf.keras.optimizers.SGD(lr=1.0)\n    optimizer = SWA(sgd, start_averaging, average_period)\n\n    val_0 = [1.0, 1.0]\n    val_1 = [2.0, 2.0]\n    var_0 = tf.Variable(val_0)\n    var_1 = tf.Variable(val_1)\n\n    grad_val_0 = [0.1, 0.1]\n    grad_val_1 = [0.1, 0.1]\n    grad_0 = tf.constant(grad_val_0)\n    grad_1 = tf.constant(grad_val_1)\n    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))\n\n    optimizer.apply_gradients(grads_and_vars)\n    optimizer.apply_gradients(grads_and_vars)\n    optimizer.apply_gradients(grads_and_vars)\n\n    np.testing.assert_allclose(var_1.read_value(), [1.7, 1.7], rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(var_0.read_value(), [0.7, 0.7], rtol=1e-06, atol=1e-06)\n\n    optimizer.assign_average_vars([var_0, var_1])\n\n    np.testing.assert_allclose(var_0.read_value(), [0.8, 0.8])\n    np.testing.assert_allclose(var_1.read_value(), [1.8, 1.8])\n\n\ndef test_optimizer_failure():\n    with pytest.raises(TypeError):\n        _ = SWA(None, average_period=10)\n\n\ndef test_optimizer_string():\n    _ = SWA(""adam"", average_period=10)\n\n\ndef test_get_config():\n    opt = SWA(""adam"", average_period=10, start_averaging=0)\n    opt = tf.keras.optimizers.deserialize(tf.keras.optimizers.serialize(opt))\n    config = opt.get_config()\n    assert config[""average_period""] == 10\n    assert config[""start_averaging""] == 0\n\n\ndef test_assign_batchnorm():\n    x = np.random.standard_normal((10, 64))\n    y = np.random.standard_normal((10, 1))\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(16, activation=""relu""))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dense(1))\n\n    opt = SWA(tf.keras.optimizers.SGD())\n    model.compile(optimizer=opt, loss=""mean_squared_error"")\n    model.fit(x, y, epochs=1)\n\n    opt.assign_average_vars(model.variables)\n    fit_bn(model, x, y)\n\n\ndef test_fit_simple_linear_model():\n    seed = 0x2019\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    num_examples = 100000\n    x = np.random.standard_normal((num_examples, 3))\n    w = np.random.standard_normal((3, 1))\n    y = np.dot(x, w) + np.random.standard_normal((num_examples, 1)) * 1e-4\n\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))\n    # using num_examples - 1 since steps starts from 0.\n    optimizer = SWA(""sgd"", start_averaging=num_examples // 32 - 1, average_period=100)\n    model.compile(optimizer, loss=""mse"")\n    model.fit(x, y, epochs=2)\n    optimizer.assign_average_vars(model.variables)\n\n    x = np.random.standard_normal((100, 3))\n    y = np.dot(x, w)\n\n    predicted = model.predict(x)\n\n    max_abs_diff = np.max(np.abs(predicted - y))\n    assert max_abs_diff < 1e-3\n\n\ndef test_serialization():\n    start_averaging = 0\n    average_period = 1\n    sgd = tf.keras.optimizers.SGD(lr=1.0)\n    optimizer = SWA(sgd, start_averaging, average_period)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/weight_decay_optimizers_test.py,36,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for optimizers with weight decay.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import test_utils\nfrom tensorflow_addons.optimizers import weight_decay_optimizers\n\nWEIGHT_DECAY = 0.01\n\n\ndef do_test(\n    dtype,\n    optimizer,\n    update_fn,\n    do_sparse=False,\n    do_decay_var_list=False,\n    **optimizer_kwargs\n):\n    """"""The major test function.\n\n    Args:\n        optimizer: The tensorflow optimizer class to be tested.\n        update_fn: The numpy update function of the optimizer, the function\n            signature must be\n            update_fn(var: np.array,\n                        grad_t: np.array,\n                        slot_vars: dict,\n                        **kwargs) -> (updated_var, updated_slot_vars)\n            Note that slot_vars will be initialized to an empty dictionary\n            for each variable, initial values should be handled in the\n            update_fn.\n        do_sparse: If True, test sparse update. Defaults to False, i.e.,\n            dense update.\n        do_decay_var_list: If True, test by passing a list of vars to ensure hashing is handled correctly\n        **optimizer_kwargs:The parameters to pass to the construcor of the\n            optimizer. Either a constant or a callable. This also passed to\n            the optimizer_params in the update_fn.\n    """"""\n    # TODO: Fix #347 issue\n    if do_sparse and test_utils.is_gpu_available():\n        pytest.skip(""Wait #347 to be fixed"")\n\n    # Initialize variables for numpy implementation.\n    np_slot_vars0, np_slot_vars1 = {}, {}\n    var0_np = np.array([1.0, 2.0], dtype=dtype[0].as_numpy_dtype)\n    grads0_np = np.array([0.1, 0.1], dtype=dtype[0].as_numpy_dtype)\n    var1_np = np.array([3.0, 4.0], dtype=dtype[0].as_numpy_dtype)\n    grads1_np = np.array([0.01, 0.01], dtype=dtype[0].as_numpy_dtype)\n    # Create Tensorflow variables.\n    var0 = tf.Variable(var0_np, name=""var0_%d"" % dtype[1])\n    var1 = tf.Variable(var1_np, name=""var1_%d"" % dtype[1])\n    if do_sparse:\n        grads0_np_indices = np.array([0, 1], dtype=np.int32)\n        grads0 = tf.IndexedSlices(\n            tf.constant(grads0_np), tf.constant(grads0_np_indices), tf.constant([2]),\n        )\n        grads1_np_indices = np.array([0, 1], dtype=np.int32)\n        grads1 = tf.IndexedSlices(\n            tf.constant(grads1_np), tf.constant(grads1_np_indices), tf.constant([2]),\n        )\n    else:\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n    opt = optimizer(**optimizer_kwargs)\n    # Create the update op.\n    # Run 3 steps of the optimizer\n    for _ in range(3):\n        if do_decay_var_list:\n            opt.apply_gradients(\n                zip([grads0, grads1], [var0, var1]), decay_var_list=[var0, var1],\n            )\n        else:\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n        var0_np, np_slot_vars0 = update_fn(\n            var0_np, grads0_np, np_slot_vars0, **optimizer_kwargs\n        )\n        var1_np, np_slot_vars1 = update_fn(\n            var1_np, grads1_np, np_slot_vars1, **optimizer_kwargs\n        )\n        # Validate updated params\n        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\ndef do_test_sparse_repeated_indices(dtype, optimizer, **optimizer_kwargs):\n    """"""Test for repeated indices in sparse updates.\n\n    This test verifies that an update with repeated indices is the same as\n    an update with two times the gradient.\n\n    Args:\n        optimizer: The tensorflow optimizer class to be tested.\n        **optimizer_kwargs: The parameters to pass to the construcor of the\n            optimizer. Either a constant or a callable. This also passed to\n            the optimizer_params in the update_fn.\n    """"""\n    # TODO: Fix #347 issue\n    if test_utils.is_gpu_available():\n        pytest.skip(""Wait #347 to be fixed"")\n\n    repeated_index_update_var = tf.Variable([[1.0], [2.0]], dtype=dtype)\n    aggregated_update_var = tf.Variable([[1.0], [2.0]], dtype=dtype)\n    grad_repeated_index = tf.IndexedSlices(\n        tf.constant([0.1, 0.1], shape=[2, 1], dtype=dtype),\n        tf.constant([1, 1]),\n        tf.constant([2, 1]),\n    )\n    grad_aggregated = tf.IndexedSlices(\n        tf.constant([0.2], shape=[1, 1], dtype=dtype),\n        tf.constant([1]),\n        tf.constant([2, 1]),\n    )\n    opt_repeated = optimizer(**optimizer_kwargs)\n    _ = opt_repeated.apply_gradients([(grad_repeated_index, repeated_index_update_var)])\n    opt_aggregated = optimizer(**optimizer_kwargs)\n    _ = opt_aggregated.apply_gradients([(grad_aggregated, aggregated_update_var)])\n    np.testing.assert_allclose(\n        aggregated_update_var.numpy(), repeated_index_update_var.numpy()\n    )\n    for _ in range(3):\n        opt_repeated.apply_gradients([(grad_repeated_index, repeated_index_update_var)])\n        opt_aggregated.apply_gradients([(grad_aggregated, aggregated_update_var)])\n        np.testing.assert_allclose(\n            aggregated_update_var.numpy(), repeated_index_update_var.numpy()\n        )\n\n\ndef adamw_update_numpy(\n    param, grad_t, slot_vars, learning_rate, beta_1, beta_2, epsilon, weight_decay\n):\n    """"""Numpy update function for AdamW.""""""\n    lr, beta1, beta2, eps, wd = (\n        v() if callable(v) else v\n        for v in (learning_rate, beta_1, beta_2, epsilon, weight_decay)\n    )\n    t = slot_vars.get(""t"", 0) + 1\n    lr_t = lr * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n    slot_vars[""m""] = beta1 * slot_vars.get(""m"", 0) + (1 - beta1) * grad_t\n    slot_vars[""v""] = beta2 * slot_vars.get(""v"", 0) + (1 - beta2) * grad_t ** 2\n    param_t = param * (1 - wd) - lr_t * slot_vars[""m""] / (np.sqrt(slot_vars[""v""]) + eps)\n    slot_vars[""t""] = t\n    return param_t, slot_vars\n\n\ndef sgdw_update_numpy(param, grad_t, slot_vars, learning_rate, momentum, weight_decay):\n    """"""Numpy update function for SGDW.""""""\n    m = slot_vars.get(""m"", 0)\n    lr, momentum, wd = (\n        v() if callable(v) else v for v in (learning_rate, momentum, weight_decay)\n    )\n    slot_vars[""m""] = momentum * m + grad_t\n    param_t = param * (1 - wd) - lr * slot_vars[""m""]\n    return param_t, slot_vars\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_sparse_adamw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.AdamW,\n        adamw_update_numpy,\n        do_sparse=True,\n        learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-8,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_sparse_repeated_indices_adamw(dtype):\n    do_test_sparse_repeated_indices(\n        dtype,\n        weight_decay_optimizers.AdamW,\n        learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-8,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_adamw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.AdamW,\n        adamw_update_numpy,\n        learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-8,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_callable_params_adamw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.AdamW,\n        adamw_update_numpy,\n        learning_rate=lambda: 0.001,\n        beta_1=lambda: 0.9,\n        beta_2=lambda: 0.999,\n        epsilon=1e-8,\n        weight_decay=lambda: WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_decay_var_list_adamw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.AdamW,\n        adamw_update_numpy,\n        do_decay_var_list=True,\n        learning_rate=0.001,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-8,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\ndef test_keras_fit():\n    """"""Check if calling model.fit works.""""""\n    model = tf.keras.models.Sequential([tf.keras.layers.Dense(2)])\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = weight_decay_optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4)\n    model.compile(optimizer=optimizer, loss=loss, metrics=[""accuracy""])\n    x, y = np.random.uniform(size=(2, 4, 1))\n    model.fit(x, y, epochs=1)\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_sparse_sgdw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.SGDW,\n        sgdw_update_numpy,\n        do_sparse=True,\n        learning_rate=0.001,\n        momentum=0.9,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_sparse_repeated_indices_sgdw(dtype):\n    do_test_sparse_repeated_indices(\n        dtype,\n        weight_decay_optimizers.SGDW,\n        learning_rate=0.001,\n        momentum=0.9,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_sgdw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.SGDW,\n        sgdw_update_numpy,\n        learning_rate=0.001,\n        momentum=0.9,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_callable_params_sgdw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.SGDW,\n        sgdw_update_numpy,\n        learning_rate=lambda: 0.001,\n        momentum=lambda: 0.9,\n        weight_decay=lambda: WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_basic_decay_var_list_sgdw(dtype):\n    do_test(\n        dtype,\n        weight_decay_optimizers.SGDW,\n        sgdw_update_numpy,\n        do_decay_var_list=True,\n        learning_rate=0.001,\n        momentum=0.9,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(\n    ""optimizer"",\n    [\n        weight_decay_optimizers.SGDW,\n        weight_decay_optimizers.extend_with_decoupled_weight_decay(\n            tf.keras.optimizers.SGD\n        ),\n    ],\n)\n@pytest.mark.parametrize(""dtype"", [(tf.half, 0), (tf.float32, 1), (tf.float64, 2)])\ndef test_optimizer_basic(dtype, optimizer):\n    do_test(\n        dtype,\n        optimizer,\n        sgdw_update_numpy,\n        learning_rate=0.001,\n        momentum=0.9,\n        weight_decay=WEIGHT_DECAY,\n    )\n\n\n@pytest.mark.parametrize(\n    ""optimizer"",\n    [\n        weight_decay_optimizers.SGDW,\n        weight_decay_optimizers.extend_with_decoupled_weight_decay(\n            tf.keras.optimizers.SGD\n        ),\n    ],\n)\n@pytest.mark.parametrize(""dtype"", [tf.half, tf.float32, tf.float64])\ndef test_optimizer_sparse(dtype, optimizer):\n    do_test_sparse_repeated_indices(\n        dtype, optimizer, learning_rate=0.001, momentum=0.9, weight_decay=WEIGHT_DECAY,\n    )\n\n\ndef test_serialization():\n    optimizer = weight_decay_optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-4)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/optimizers/tests/yogi_test.py,38,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Yogi optimizer.""""""\n\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.optimizers import yogi\nfrom tensorflow_addons.utils import test_utils\n\n\ndef yogi_update_numpy(\n    param,\n    g_t,\n    t,\n    m,\n    v,\n    alpha=0.01,\n    beta1=0.9,\n    beta2=0.999,\n    epsilon=1e-3,\n    l1reg=0.0,\n    l2reg=0.0,\n):\n    """"""Performs Yogi parameter update using numpy.\n\n    Args:\n      param: An numpy ndarray of the current parameter.\n      g_t: An numpy ndarray of the current gradients.\n      t: An numpy ndarray of the current time step.\n      m: An numpy ndarray of the 1st moment estimates.\n      v: An numpy ndarray of the 2nd moment estimates.\n      alpha: A float value of the learning rate.\n      beta1: A float value of the exponential decay rate for the 1st moment\n        estimates.\n      beta2: A float value of the exponential decay rate for the 2nd moment\n         estimates.\n      epsilon: A float of a small constant for numerical stability.\n      l1reg: A float value of L1 regularization\n      l2reg: A float value of L2 regularization\n    Returns:\n      A tuple of numpy ndarrays (param_t, m_t, v_t) representing the\n      updated parameters for `param`, `m`, and `v` respectively.\n    """"""\n    beta1 = np.array(beta1, dtype=param.dtype)\n    beta2 = np.array(beta2, dtype=param.dtype)\n\n    alpha_t = alpha * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n\n    m_t = beta1 * m + (1 - beta1) * g_t\n    g2_t = g_t * g_t\n    v_t = v - (1 - beta2) * np.sign(v - g2_t) * g2_t\n\n    per_coord_lr = alpha_t / (np.sqrt(v_t) + epsilon)\n    param_t = param - per_coord_lr * m_t\n\n    if l1reg > 0:\n        param_t = (param_t - l1reg * per_coord_lr * np.sign(param_t)) / (\n            1 + l2reg * per_coord_lr\n        )\n        print(param_t.dtype)\n        param_t[np.abs(param_t) < l1reg * per_coord_lr] = 0.0\n    elif l2reg > 0:\n        param_t = param_t / (1 + l2reg * per_coord_lr)\n    return param_t, m_t, v_t\n\n\ndef get_beta_accumulators(opt, dtype):\n    local_step = tf.cast(opt.iterations + 1, dtype)\n    beta_1_t = tf.cast(opt._get_hyper(""beta_1""), dtype)\n    beta_1_power = tf.math.pow(beta_1_t, local_step)\n    beta_2_t = tf.cast(opt._get_hyper(""beta_2""), dtype)\n    beta_2_power = tf.math.pow(beta_2_t, local_step)\n    return (beta_1_power, beta_2_power)\n\n\ndef _dtypes_to_test(use_gpu):\n    if use_gpu:\n        return [tf.dtypes.float32, tf.dtypes.float64]\n    else:\n        return [tf.dtypes.half, tf.dtypes.float32, tf.dtypes.float64]\n\n\ndef do_test_sparse(beta1=0.0, l1reg=0.0, l2reg=0.0):\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0_np_indices = np.array([0, 1], dtype=np.int32)\n        grads0 = tf.IndexedSlices(\n            tf.constant(grads0_np), tf.constant(grads0_np_indices), tf.constant([2])\n        )\n        grads1_np_indices = np.array([0, 1], dtype=np.int32)\n        grads1 = tf.IndexedSlices(\n            tf.constant(grads1_np), tf.constant(grads1_np_indices), tf.constant([2])\n        )\n        opt = yogi.Yogi(\n            beta1=beta1,\n            l1_regularization_strength=l1reg,\n            l2_regularization_strength=l2reg,\n            initial_accumulator_value=1.0,\n        )\n\n        # Fetch params to validate initial values.\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of Yogi.\n        for t in range(1, 4):\n            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(beta1 ** t, beta1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = yogi_update_numpy(\n                var0_np, grads0_np, t, m0, v0, beta1=beta1, l1reg=l1reg, l2reg=l2reg\n            )\n            var1_np, m1, v1 = yogi_update_numpy(\n                var1_np, grads1_np, t, m1, v1, beta1=beta1, l1reg=l1reg, l2reg=l2reg\n            )\n\n            # Validate updated params.\n            test_utils.assert_allclose_according_to_type(\n                var0_np, var0.numpy(),\n            )\n            test_utils.assert_allclose_according_to_type(\n                var1_np, var1.numpy(),\n            )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse():\n    do_test_sparse()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_regularization():\n    do_test_sparse(l1reg=0.1, l2reg=0.2)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_momentum():\n    do_test_sparse(beta1=0.9)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sparse_momentum_regularization():\n    do_test_sparse(beta1=0.9, l1reg=0.1, l2reg=0.2)\n\n\ndef test_sparse_repeated_indices():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        repeated_index_update_var = tf.Variable([[1.0], [2.0]], dtype=dtype)\n        aggregated_update_var = tf.Variable([[1.0], [2.0]], dtype=dtype)\n        grad_repeated_index = tf.IndexedSlices(\n            tf.constant([0.1, 0.1], shape=[2, 1], dtype=dtype),\n            tf.constant([1, 1]),\n            tf.constant([2, 1]),\n        )\n        grad_aggregated = tf.IndexedSlices(\n            tf.constant([0.2], shape=[1, 1], dtype=dtype),\n            tf.constant([1]),\n            tf.constant([2, 1]),\n        )\n        opt1 = yogi.Yogi()\n        opt2 = yogi.Yogi()\n\n        np.testing.assert_allclose(\n            aggregated_update_var.numpy(), repeated_index_update_var.numpy(),\n        )\n\n        for _ in range(3):\n            opt1.apply_gradients([(grad_repeated_index, repeated_index_update_var)])\n            opt2.apply_gradients([(grad_aggregated, aggregated_update_var)])\n\n        np.testing.assert_allclose(\n            aggregated_update_var.numpy(), repeated_index_update_var.numpy(),\n        )\n\n\ndef do_test_basic(beta1=0.0, l1reg=0.0, l2reg=0.0):\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n\n        opt = yogi.Yogi(\n            beta1=beta1,\n            l1_regularization_strength=l1reg,\n            l2_regularization_strength=l2reg,\n            initial_accumulator_value=1.0,\n        )\n\n        # Fetch params to validate initial values.\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of Yogi.\n        for t in range(1, 4):\n            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(beta1 ** t, beta1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)\n\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = yogi_update_numpy(\n                var0_np, grads0_np, t, m0, v0, beta1=beta1, l1reg=l1reg, l2reg=l2reg\n            )\n            var1_np, m1, v1 = yogi_update_numpy(\n                var1_np, grads1_np, t, m1, v1, beta1=beta1, l1reg=l1reg, l2reg=l2reg\n            )\n\n            # Validate updated params.\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic():\n    do_test_basic()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic_regularization():\n    do_test_basic(l1reg=0.1, l2reg=0.2)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic_momentum():\n    do_test_basic(beta1=0.9)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_basic_momentum_regularization():\n    do_test_basic(beta1=0.9, l1reg=0.1, l2reg=0.2)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_tensor_learning_rate():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n        opt = yogi.Yogi(tf.constant(0.01), initial_accumulator_value=1.0)\n\n        # Fetch params to validate initial values.\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of Yogi.\n        for t in range(1, 4):\n            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** t, beta1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)\n\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n\n            var0_np, m0, v0 = yogi_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = yogi_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params.\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_sharing():\n    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):\n        # Initialize variables for numpy implementation.\n        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0\n        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n\n        var0 = tf.Variable(var0_np)\n        var1 = tf.Variable(var1_np)\n        grads0 = tf.constant(grads0_np)\n        grads1 = tf.constant(grads1_np)\n        opt = yogi.Yogi(initial_accumulator_value=1.0)\n\n        # Fetch params to validate initial values.\n        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())\n        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())\n\n        # Run 3 steps of intertwined Yogi1 and Yogi2.\n        for t in range(1, 4):\n            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)\n            test_utils.assert_allclose_according_to_type(0.9 ** t, beta1_power)\n            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)\n            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))\n            var0_np, m0, v0 = yogi_update_numpy(var0_np, grads0_np, t, m0, v0)\n            var1_np, m1, v1 = yogi_update_numpy(var1_np, grads1_np, t, m1, v1)\n\n            # Validate updated params.\n            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())\n            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())\n\n\ndef test_get_config():\n    opt = yogi.Yogi(1e-4)\n    config = opt.get_config()\n    assert config[""learning_rate""] == 1e-4\n\n\ndef test_serialization():\n    optimizer = yogi.Yogi(1e-4)\n    config = tf.keras.optimizers.serialize(optimizer)\n    new_optimizer = tf.keras.optimizers.deserialize(config)\n    assert new_optimizer.get_config() == optimizer.get_config()\n'"
tensorflow_addons/rnn/tests/__init__.py,0,b''
tensorflow_addons/rnn/tests/cell_test.py,44,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for RNN cells.""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow_addons.rnn import cell as rnn_cell\nfrom tensorflow_addons.rnn import LayerNormSimpleRNNCell\n\n\ndef test_base():\n    units = 6\n    batch_size = 3\n    expected_output = np.array(\n        [\n            [0.576751, 0.576751, 0.576751, 0.576751, 0.576751, 0.576751],\n            [0.618936, 0.618936, 0.618936, 0.618936, 0.618936, 0.618936],\n            [0.627393, 0.627393, 0.627393, 0.627393, 0.627393, 0.627393],\n        ]\n    )\n    expected_state = np.array(\n        [\n            [\n                0.7157977,\n                0.7157977,\n                0.7157977,\n                0.7157977,\n                0.7157977,\n                0.7157977,\n                0.5767508,\n                0.5767508,\n                0.5767508,\n                0.5767508,\n                0.5767508,\n                0.5767508,\n            ],\n            [\n                0.7804162,\n                0.7804162,\n                0.7804162,\n                0.7804162,\n                0.7804162,\n                0.7804162,\n                0.6189357,\n                0.6189357,\n                0.6189357,\n                0.6189357,\n                0.6189357,\n                0.6189357,\n            ],\n            [\n                0.7945764,\n                0.7945764,\n                0.7945764,\n                0.7945764,\n                0.7945765,\n                0.7945765,\n                0.6273934,\n                0.6273934,\n                0.6273934,\n                0.6273934,\n                0.6273934,\n                0.6273934,\n            ],\n        ]\n    )\n    const_initializer = tf.constant_initializer(0.5)\n    cell = rnn_cell.NASCell(\n        units=units,\n        kernel_initializer=const_initializer,\n        recurrent_initializer=const_initializer,\n    )\n\n    inputs = tf.constant(\n        np.array(\n            [[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0]],\n            dtype=np.float32,\n        ),\n        dtype=tf.float32,\n    )\n    state_value = tf.constant(\n        0.1 * np.ones((batch_size, units), dtype=np.float32), dtype=tf.float32\n    )\n    init_state = [state_value, state_value]\n    output, state = cell(inputs, init_state)\n    res = [output, state]\n\n    # This is a smoke test: Only making sure expected values not change.\n    assert len(res) == 2\n    np.testing.assert_allclose(res[0], expected_output, rtol=1e-6, atol=1e-6)\n    # There should be 2 states in the list.\n    assert len(res[1]) == 2\n    # Checking the shape of each state to be batch_size * num_units\n    new_c, new_h = res[1]\n    assert new_c.shape[0] == batch_size\n    assert new_c.shape[1] == units\n    assert new_h.shape[0] == batch_size\n    assert new_h.shape[1] == units\n    np.testing.assert_allclose(\n        np.concatenate(res[1], axis=1), expected_state, rtol=1e-6, atol=1e-6\n    )\n\n\ndef test_projection():\n    units = 6\n    batch_size = 3\n    projection = 5\n    expected_output = np.array(\n        [\n            [1.697418, 1.697418, 1.697418, 1.697418, 1.697418],\n            [1.840037, 1.840037, 1.840037, 1.840037, 1.840037],\n            [1.873985, 1.873985, 1.873985, 1.873985, 1.873985],\n        ]\n    )\n\n    expected_state = np.array(\n        [\n            [\n                0.69855207,\n                0.69855207,\n                0.69855207,\n                0.69855207,\n                0.69855207,\n                0.69855207,\n                1.69741797,\n                1.69741797,\n                1.69741797,\n                1.69741797,\n                1.69741797,\n            ],\n            [\n                0.77073824,\n                0.77073824,\n                0.77073824,\n                0.77073824,\n                0.77073824,\n                0.77073824,\n                1.84003687,\n                1.84003687,\n                1.84003687,\n                1.84003687,\n                1.84003687,\n            ],\n            [\n                0.78973997,\n                0.78973997,\n                0.78973997,\n                0.78973997,\n                0.78973997,\n                0.78973997,\n                1.87398517,\n                1.87398517,\n                1.87398517,\n                1.87398517,\n                1.87398517,\n            ],\n        ]\n    )\n    const_initializer = tf.constant_initializer(0.5)\n    cell = rnn_cell.NASCell(\n        units=units,\n        projection=projection,\n        kernel_initializer=const_initializer,\n        recurrent_initializer=const_initializer,\n        projection_initializer=const_initializer,\n    )\n    inputs = tf.constant(\n        np.array(\n            [[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0]],\n            dtype=np.float32,\n        ),\n        dtype=tf.float32,\n    )\n    state_value_c = tf.constant(\n        0.1 * np.ones((batch_size, units), dtype=np.float32), dtype=tf.float32\n    )\n    state_value_h = tf.constant(\n        0.1 * np.ones((batch_size, projection), dtype=np.float32), dtype=tf.float32\n    )\n    init_state = [state_value_c, state_value_h]\n    output, state = cell(inputs, init_state)\n    res = [output, state]\n\n    # This is a smoke test: Only making sure expected values not change.\n    assert len(res) == 2\n    np.testing.assert_allclose(res[0], expected_output, rtol=1e-6, atol=1e-6)\n    # There should be 2 states in the tuple.\n    assert len(res[1]) == 2\n    # Checking the shape of each state to be batch_size * num_units\n    new_c, new_h = res[1]\n    assert new_c.shape[0] == batch_size\n    assert new_c.shape[1] == units\n    assert new_h.shape[0] == batch_size\n    assert new_h.shape[1] == projection\n    np.testing.assert_allclose(\n        np.concatenate(res[1], axis=1), expected_state, rtol=1e-6, atol=1e-6\n    )\n\n\ndef test_keras_rnn():\n    """"""Tests that NASCell works with keras RNN layer.""""""\n    cell = rnn_cell.NASCell(10)\n    seq_input = tf.convert_to_tensor(\n        np.random.rand(2, 3, 5), name=""seq_input"", dtype=tf.float32\n    )\n    rnn_layer = keras.layers.RNN(cell=cell)\n    rnn_outputs = rnn_layer(seq_input)\n    assert rnn_outputs.shape == (2, 10)\n\n\ndef test_config_nas():\n    cell = rnn_cell.NASCell(10, projection=5, use_bias=True, name=""nas_cell_3"")\n\n    expected_config = {\n        ""dtype"": ""float32"",\n        ""name"": ""nas_cell_3"",\n        ""trainable"": True,\n        ""units"": 10,\n        ""projection"": 5,\n        ""use_bias"": True,\n        ""kernel_initializer"": ""glorot_uniform"",\n        ""recurrent_initializer"": ""glorot_uniform"",\n        ""bias_initializer"": ""zeros"",\n        ""projection_initializer"": ""glorot_uniform"",\n    }\n    config = cell.get_config()\n    assert config == expected_config\n\n    restored_cell = rnn_cell.NASCell.from_config(config)\n    restored_config = restored_cell.get_config()\n    assert config == restored_config\n\n\ndef test_cell_output():\n    x = tf.ones([1, 2], dtype=tf.float32)\n    c0 = tf.constant(0.1 * np.asarray([[0, 1]]), dtype=tf.float32)\n    h0 = tf.constant(0.1 * np.asarray([[2, 3]]), dtype=tf.float32)\n    state0 = [h0, c0]\n    c1 = tf.constant(0.1 * np.asarray([[4, 5]]), dtype=tf.float32)\n    h1 = tf.constant(0.1 * np.asarray([[6, 7]]), dtype=tf.float32)\n    state1 = [h1, c1]\n    state = (state0, state1)\n    const_initializer = tf.constant_initializer(0.5)\n\n    def single_cell():\n        return rnn_cell.LayerNormLSTMCell(\n            units=2,\n            kernel_initializer=const_initializer,\n            recurrent_initializer=const_initializer,\n            bias_initializer=const_initializer,\n            norm_epsilon=1e-12,\n        )\n\n    cell = keras.layers.StackedRNNCells([single_cell() for _ in range(2)])\n    output_v, output_states_v = cell(x, state)\n\n    expected_output = np.array([[-0.47406167, 0.47406143]])\n    expected_state0_c = np.array([[-1.0, 1.0]])\n    expected_state0_h = np.array([[-0.47406167, 0.47406143]])\n    expected_state1_c = np.array([[-1.0, 1.0]])\n    expected_state1_h = np.array([[-0.47406167, 0.47406143]])\n\n    actual_state0_h = output_states_v[0][0]\n    actual_state0_c = output_states_v[0][1]\n    actual_state1_h = output_states_v[1][0]\n    actual_state1_c = output_states_v[1][1]\n\n    np.testing.assert_allclose(output_v, expected_output, 1e-5)\n    np.testing.assert_allclose(expected_state0_c, actual_state0_c, 1e-5)\n    np.testing.assert_allclose(expected_state0_h, actual_state0_h, 1e-5)\n    np.testing.assert_allclose(expected_state1_c, actual_state1_c, 1e-5)\n    np.testing.assert_allclose(expected_state1_h, actual_state1_h, 1e-5)\n\n    # Test BasicLSTMCell with input_size != num_units.\n    x = tf.ones([1, 3], dtype=tf.float32)\n    c = tf.constant(0.1 * np.asarray([[0, 1]]), dtype=tf.float32)\n    h = tf.constant(0.1 * np.asarray([[2, 3]]), dtype=tf.float32)\n    state = [h, c]\n    cell = rnn_cell.LayerNormLSTMCell(\n        units=2,\n        kernel_initializer=const_initializer,\n        recurrent_initializer=const_initializer,\n        bias_initializer=const_initializer,\n        norm_epsilon=1e-12,\n    )\n    output_v, output_states_v = cell(x, state)\n    expected_h = np.array([[-0.47406167, 0.47406143]])\n    expected_c = np.array([[-1.0, 1.0]])\n    np.testing.assert_allclose(output_v, expected_h, 1e-5)\n    np.testing.assert_allclose(output_states_v[0], expected_h, 1e-5)\n    np.testing.assert_allclose(output_states_v[1], expected_c, 1e-5)\n\n\ndef test_config_layer_norm():\n    cell = rnn_cell.LayerNormLSTMCell(10, name=""layer_norm_lstm_cell_3"")\n\n    expected_config = {\n        ""dtype"": ""float32"",\n        ""name"": ""layer_norm_lstm_cell_3"",\n        ""trainable"": True,\n        ""units"": 10,\n        ""activation"": ""tanh"",\n        ""recurrent_activation"": ""sigmoid"",\n        ""use_bias"": True,\n        ""kernel_initializer"": {\n            ""class_name"": ""GlorotUniform"",\n            ""config"": {""seed"": None},\n        },\n        ""recurrent_initializer"": {\n            ""class_name"": ""Orthogonal"",\n            ""config"": {""seed"": None, ""gain"": 1.0},\n        },\n        ""bias_initializer"": {""class_name"": ""Zeros"", ""config"": {}},\n        ""unit_forget_bias"": True,\n        ""kernel_regularizer"": None,\n        ""recurrent_regularizer"": None,\n        ""bias_regularizer"": None,\n        ""kernel_constraint"": None,\n        ""recurrent_constraint"": None,\n        ""bias_constraint"": None,\n        ""dropout"": 0.0,\n        ""recurrent_dropout"": 0.0,\n        ""implementation"": 2,\n        ""norm_gamma_initializer"": {""class_name"": ""Ones"", ""config"": {}},\n        ""norm_beta_initializer"": {""class_name"": ""Zeros"", ""config"": {}},\n        ""norm_epsilon"": 1e-3,\n    }\n    config = cell.get_config()\n    assert config == expected_config\n\n    restored_cell = rnn_cell.LayerNormLSTMCell.from_config(config)\n    restored_config = restored_cell.get_config()\n    assert config == restored_config\n\n\ndef test_constraints_layernorm_rnn():\n    embedding_dim = 4\n    k_constraint = keras.constraints.max_norm(0.01)\n    r_constraint = keras.constraints.max_norm(0.01)\n    b_constraint = keras.constraints.max_norm(0.01)\n    g_constraint = keras.constraints.max_norm(0.01)\n    layer = keras.layers.RNN(\n        LayerNormSimpleRNNCell(\n            units=5,\n            kernel_constraint=k_constraint,\n            recurrent_constraint=r_constraint,\n            bias_constraint=b_constraint,\n            gamma_constraint=g_constraint,\n        ),\n        input_shape=(None, embedding_dim),\n        return_sequences=False,\n    )\n    layer.build((None, None, embedding_dim))\n    assert layer.cell.kernel.constraint == k_constraint\n    assert layer.cell.recurrent_kernel.constraint == r_constraint\n    assert layer.cell.bias.constraint == b_constraint\n    assert layer.cell.layernorm.gamma.constraint == g_constraint\n\n\ndef test_with_masking_layer_layernorm_rnn():\n    inputs = np.random.random((2, 3, 4))\n    targets = np.abs(np.random.random((2, 3, 5)))\n    targets /= targets.sum(axis=-1, keepdims=True)\n    model = keras.models.Sequential()\n    model.add(keras.layers.Masking(input_shape=(3, 4)))\n    model.add(\n        keras.layers.RNN(\n            LayerNormSimpleRNNCell(units=5), return_sequences=True, unroll=False\n        )\n    )\n    model.compile(loss=""categorical_crossentropy"", optimizer=""rmsprop"")\n    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n\n\ndef test_regularizers_layernorm_rnn():\n    embedding_dim = 4\n    layer = keras.layers.RNN(\n        LayerNormSimpleRNNCell(\n            units=5,\n            kernel_regularizer=keras.regularizers.l1(0.01),\n            recurrent_regularizer=keras.regularizers.l1(0.01),\n            bias_regularizer=""l2"",\n            gamma_regularizer=""l2"",\n        ),\n        input_shape=(None, embedding_dim),\n        return_sequences=False,\n    )\n    layer.build((None, None, 2))\n    assert len(layer.losses) == 4\n\n\ndef test_configs_layernorm():\n    config = {""layernorm_epsilon"": 1e-6}\n    cell1 = LayerNormSimpleRNNCell(units=8, **config)\n    config1 = cell1.get_config()\n    cell2 = LayerNormSimpleRNNCell(**config1)\n    config2 = cell2.get_config()\n    assert config1 == config2\n\n\ndef test_base_esn():\n    units = 3\n    expected_output = np.array(\n        [[2.77, 2.77, 2.77], [4.77, 4.77, 4.77], [6.77, 6.77, 6.77],], dtype=np.float32,\n    )\n\n    const_initializer = tf.constant_initializer(0.5)\n    cell = rnn_cell.ESNCell(\n        units=units,\n        connectivity=1,\n        leaky=1,\n        spectral_radius=0.9,\n        use_norm2=True,\n        use_bias=True,\n        activation=None,\n        kernel_initializer=const_initializer,\n        recurrent_initializer=const_initializer,\n        bias_initializer=const_initializer,\n    )\n\n    inputs = tf.constant(\n        np.array(\n            [[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0]],\n            dtype=np.float32,\n        ),\n        dtype=tf.float32,\n    )\n    state_value = tf.constant(\n        0.3 * np.ones((units, units), dtype=np.float32), dtype=tf.float32\n    )\n    init_state = [state_value, state_value]\n    output, state = cell(inputs, init_state)\n\n    np.testing.assert_allclose(output, expected_output, 1e-5)\n    np.testing.assert_allclose(state, expected_output, 1e-5)\n\n\ndef test_esn_echo_state_property_eig():\n    use_norm2 = False\n    units = 3\n    cell = rnn_cell.ESNCell(\n        units=units,\n        use_norm2=use_norm2,\n        recurrent_initializer=""ones"",\n        connectivity=1.0,\n    )\n    cell.build((3, 3))\n    recurrent_weights = tf.constant(cell.get_weights()[0], dtype=tf.float32)\n    max_eig = tf.reduce_max(tf.abs(tf.linalg.eig(recurrent_weights)[0]))\n    assert max_eig < 1, ""max(eig(W)) < 1""\n\n\ndef test_esn_echo_state_property_norm2():\n    use_norm2 = True\n    units = 3\n    cell = rnn_cell.ESNCell(\n        units=units, use_norm2=use_norm2, recurrent_initializer=""ones"", connectivity=1.0\n    )\n    cell.build((3, 3))\n    recurrent_weights = tf.constant(cell.get_weights()[0])\n    max_eig = tf.reduce_max(tf.abs(tf.linalg.eig(recurrent_weights)[0]))\n    assert max_eig < 1, ""max(eig(W)) < 1""\n\n\ndef test_esn_connectivity():\n    units = 1000\n    connectivity = 0.5\n    cell = rnn_cell.ESNCell(\n        units=units,\n        connectivity=connectivity,\n        use_norm2=True,\n        recurrent_initializer=""ones"",\n    )\n    cell.build((3, 3))\n    recurrent_weights = tf.constant(cell.get_weights()[0])\n    num_non_zero = tf.math.count_nonzero(recurrent_weights)\n    actual_connectivity = tf.divide(num_non_zero, units ** 2)\n    np.testing.assert_allclose(\n        np.asarray([actual_connectivity]), np.asanyarray([connectivity]), 1e-2\n    )\n\n\ndef test_esn_keras_rnn():\n    cell = rnn_cell.ESNCell(10)\n    seq_input = tf.convert_to_tensor(\n        np.random.rand(2, 3, 5), name=""seq_input"", dtype=tf.float32\n    )\n    rnn_layer = keras.layers.RNN(cell=cell)\n    rnn_outputs = rnn_layer(seq_input)\n    assert rnn_outputs.shape == (2, 10)\n\n\ndef test_esn_keras_rnn_e2e():\n    inputs = np.random.random((2, 3, 4))\n    targets = np.abs(np.random.random((2, 5)))\n    targets /= targets.sum(axis=-1, keepdims=True)\n    cell = rnn_cell.ESNCell(5)\n    model = keras.models.Sequential()\n    model.add(keras.layers.Masking(input_shape=(3, 4)))\n    model.add(keras.layers.RNN(cell))\n    model.compile(loss=""categorical_crossentropy"", optimizer=""rmsprop"")\n    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n\n\ndef test_esn_config():\n    cell = rnn_cell.ESNCell(\n        units=3,\n        connectivity=1,\n        leaky=1,\n        spectral_radius=0.9,\n        use_norm2=False,\n        use_bias=True,\n        activation=""tanh"",\n        kernel_initializer=""glorot_uniform"",\n        recurrent_initializer=""glorot_uniform"",\n        bias_initializer=""glorot_uniform"",\n        name=""esn_cell_3"",\n    )\n\n    expected_config = {\n        ""name"": ""esn_cell_3"",\n        ""trainable"": True,\n        ""dtype"": ""float32"",\n        ""units"": 3,\n        ""connectivity"": 1,\n        ""leaky"": 1,\n        ""spectral_radius"": 0.9,\n        ""use_norm2"": False,\n        ""use_bias"": True,\n        ""activation"": tf.keras.activations.serialize(tf.keras.activations.get(""tanh"")),\n        ""kernel_initializer"": tf.keras.initializers.serialize(\n            tf.keras.initializers.get(""glorot_uniform"")\n        ),\n        ""recurrent_initializer"": tf.keras.initializers.serialize(\n            tf.keras.initializers.get(""glorot_uniform"")\n        ),\n        ""bias_initializer"": tf.keras.initializers.serialize(\n            tf.keras.initializers.get(""glorot_uniform"")\n        ),\n    }\n    config = cell.get_config()\n    assert config == expected_config\n\n    restored_cell = rnn_cell.ESNCell.from_config(config)\n    restored_config = restored_cell.get_config()\n    assert config == restored_config\n'"
tensorflow_addons/rnn/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/seq2seq/tests/__init__.py,0,b''
tensorflow_addons/seq2seq/tests/attention_wrapper_test.py,56,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tfa.seq2seq.attention_wrapper.""""""\n\nimport collections\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import attention_wrapper as wrapper\nfrom tensorflow_addons.seq2seq import basic_decoder\nfrom tensorflow_addons.seq2seq import sampler as sampler_py\n\n\nclass DummyData:\n    def __init__(self):\n        self.batch = 10\n        self.timestep = 5\n        self.memory_size = 6\n        self.units = 8\n\n        self.memory = np.random.randn(\n            self.batch, self.timestep, self.memory_size\n        ).astype(np.float32)\n        self.memory_length = np.random.randint(\n            low=1, high=self.timestep + 1, size=(self.batch,)\n        )\n        self.query = np.random.randn(self.batch, self.units).astype(np.float32)\n        self.state = np.random.randn(self.batch, self.timestep).astype(np.float32)\n\n\nattention_classes = [\n    wrapper.LuongAttention,\n    wrapper.LuongMonotonicAttention,\n    wrapper.BahdanauAttention,\n    wrapper.BahdanauMonotonicAttention,\n]\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_attention_shape_inference(attention_cls):\n    dummy_data = DummyData()\n    attention = attention_cls(dummy_data.units, dummy_data.memory)\n    attention_score = attention([dummy_data.query, dummy_data.state])\n    assert len(attention_score) == 2\n    assert attention_score[0].shape == (dummy_data.batch, dummy_data.timestep)\n    assert attention_score[1].shape == (dummy_data.batch, dummy_data.timestep)\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_get_config(attention_cls):\n    dummy_data = DummyData()\n    attention = attention_cls(dummy_data.units, dummy_data.memory)\n    config = attention.get_config()\n\n    attention_from_config = attention_cls.from_config(config)\n    config_from_clone = attention_from_config.get_config()\n\n    assert config == config_from_clone\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_layer_output(attention_cls):\n    dummy_data = DummyData()\n    attention = attention_cls(dummy_data.units, dummy_data.memory)\n    score = attention([dummy_data.query, dummy_data.state])\n\n    assert len(score) == 2\n    assert score[0].shape == (dummy_data.batch, dummy_data.timestep)\n    assert score[1].shape == (dummy_data.batch, dummy_data.timestep)\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_passing_memory_from_call(attention_cls):\n    dummy_data = DummyData()\n    attention = attention_cls(dummy_data.units, dummy_data.memory)\n    weights_before_query = attention.get_weights()\n    ref_score = attention([dummy_data.query, dummy_data.state])\n\n    all_weights = attention.get_weights()\n    config = attention.get_config()\n    # Simulate the twice invocation of calls here.\n    attention_from_config = attention_cls.from_config(config)\n    attention_from_config.build(dummy_data.memory.shape)\n    attention_from_config.set_weights(weights_before_query)\n    attention_from_config(dummy_data.memory, setup_memory=True)\n    attention_from_config.build([dummy_data.query.shape, dummy_data.state.shape])\n    attention_from_config.set_weights(all_weights)\n    score = attention_from_config([dummy_data.query, dummy_data.state])\n\n    np.testing.assert_allclose(ref_score, score)\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_save_load_layer(attention_cls):\n    dummy_data = DummyData()\n    vocab = 20\n    embedding_dim = 6\n    inputs = tf.keras.Input(shape=[dummy_data.timestep])\n    encoder_input = tf.keras.layers.Embedding(vocab, embedding_dim, mask_zero=True)(\n        inputs\n    )\n    encoder_output = tf.keras.layers.LSTM(\n        dummy_data.memory_size, return_sequences=True\n    )(encoder_input)\n\n    attention = attention_cls(dummy_data.units, encoder_output)\n    query = tf.keras.Input(shape=[dummy_data.units])\n    state = tf.keras.Input(shape=[dummy_data.timestep])\n\n    score = attention([query, state])\n\n    x_test = np.random.randint(vocab, size=(dummy_data.batch, dummy_data.timestep))\n    model = tf.keras.Model([inputs, query, state], score)\n    # Fall back to v1 style Keras training loop until issue with\n    # using outputs of a layer in another layer\'s constructor.\n    model.compile(""rmsprop"", ""mse"")\n    y_ref = model.predict_on_batch([x_test, dummy_data.query, dummy_data.state])\n\n    config = model.get_config()\n    weights = model.get_weights()\n    loaded_model = tf.keras.Model.from_config(\n        config, custom_objects={attention_cls.__name__: attention_cls}\n    )\n    loaded_model.set_weights(weights)\n\n    # Fall back to v1 style Keras training loop until issue with\n    # using outputs of a layer in another layer\'s constructor.\n    loaded_model.compile(""rmsprop"", ""mse"")\n\n    y = loaded_model.predict_on_batch([x_test, dummy_data.query, dummy_data.state])\n\n    np.testing.assert_allclose(y_ref, y)\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_manual_memory_reset(attention_cls):\n    dummy_data = DummyData()\n    attention = attention_cls(dummy_data.units)\n\n    def _compute_score(batch_size=None):\n        if batch_size is None:\n            batch_size = dummy_data.batch\n        memory = dummy_data.memory[:batch_size]\n        attention.setup_memory(\n            memory, memory_sequence_length=dummy_data.memory_length[:batch_size]\n        )\n        assert attention.values.shape.as_list() == list(memory.shape)\n        assert attention.keys.shape.as_list() == list(memory.shape)[:-1] + [\n            dummy_data.units\n        ]\n        return attention([dummy_data.query[:batch_size], dummy_data.state[:batch_size]])\n\n    _compute_score(batch_size=dummy_data.batch)\n    variables = list(attention.variables)\n    _compute_score(batch_size=dummy_data.batch - 1)\n\n    # No new variables were created.\n    for var_1, var_2 in zip(variables, list(attention.variables)):\n        assert var_1 is var_2\n\n\ndef test_masking():\n    memory = tf.ones([4, 4, 5], dtype=tf.float32)\n    memory_sequence_length = tf.constant([1, 2, 3, 4], dtype=tf.int32)\n    query = tf.ones([4, 5], dtype=tf.float32)\n    state = None\n    attention = wrapper.LuongAttention(5, memory, memory_sequence_length)\n    alignment, _ = attention([query, state])\n    assert np.sum(np.triu(alignment, k=1)) == 0\n\n\n@pytest.mark.parametrize(""attention_cls"", attention_classes)\ndef test_memory_re_setup(attention_cls):\n    class MyModel(tf.keras.models.Model):\n        def __init__(self, vocab, embedding_dim, memory_size, units):\n            super().__init__()\n            self.emb = tf.keras.layers.Embedding(vocab, embedding_dim, mask_zero=True)\n            self.encoder = tf.keras.layers.LSTM(memory_size, return_sequences=True)\n            self.attn_mch = attention_cls(units)\n\n        def call(self, inputs):\n            enc_input, query, state = inputs\n            mask = self.emb.compute_mask(enc_input)\n            enc_input = self.emb(enc_input)\n            enc_output = self.encoder(enc_input, mask=mask)\n            # To ensure manual resetting also works in the graph mode,\n            # we call the attention mechanism twice.\n            self.attn_mch(enc_output, mask=mask, setup_memory=True)\n            self.attn_mch(enc_output, mask=mask, setup_memory=True)\n            score = self.attn_mch([query, state])\n            return score\n\n    vocab = 20\n    embedding_dim = 6\n    num_batches = 5\n\n    dummy_data = DummyData()\n    model = MyModel(vocab, embedding_dim, dummy_data.memory_size, dummy_data.units)\n    model.compile(""rmsprop"", ""mse"")\n\n    x = np.random.randint(\n        vocab, size=(num_batches * dummy_data.batch, dummy_data.timestep)\n    )\n    x_test = np.random.randint(\n        vocab, size=(num_batches * dummy_data.batch, dummy_data.timestep)\n    )\n    y = np.random.randn(num_batches * dummy_data.batch, dummy_data.timestep)\n\n    query = np.tile(dummy_data.query, [num_batches, 1])\n    state = np.tile(dummy_data.state, [num_batches, 1])\n\n    model.fit([x, query, state], (y, y), batch_size=dummy_data.batch)\n    model.predict_on_batch([x_test, query, state])\n\n\nclass ResultSummary(\n    collections.namedtuple(""ResultSummary"", (""shape"", ""dtype"", ""mean""))\n):\n    pass\n\n\ndef get_result_summary(x):\n    if isinstance(x, np.ndarray):\n        return ResultSummary(x.shape, x.dtype, x.mean())\n    return x\n\n\ndef assert_allclose_or_equal(x, y, **kwargs):\n    if isinstance(x, np.ndarray) or isinstance(x, float):\n        np.testing.assert_allclose(x, y, atol=1e-3, **kwargs)\n    else:\n        assert x == y\n\n\nclass DummyData2:\n    def __init__(self):\n        self.batch = 64\n        self.units = 128\n        self.encoder_timestep = 10\n        self.encoder_dim = 256\n        self.decoder_timestep = 12\n        self.encoder_outputs = np.random.randn(\n            self.batch, self.encoder_timestep, self.encoder_dim\n        )\n        self.encoder_sequence_length = np.random.randint(\n            1, high=self.encoder_timestep, size=(self.batch,)\n        ).astype(np.int32)\n        self.decoder_inputs = np.random.randn(\n            self.batch, self.decoder_timestep, self.units\n        )\n        self.decoder_sequence_length = np.random.randint(\n            self.decoder_timestep, size=(self.batch,)\n        ).astype(np.int32)\n\n\ndef test_custom_attention_layer():\n    dummy_data = DummyData2()\n    attention_mechanism = wrapper.LuongAttention(dummy_data.units)\n    cell = tf.keras.layers.LSTMCell(dummy_data.units)\n    attention_layer = tf.keras.layers.Dense(\n        dummy_data.units * 2, use_bias=False, activation=tf.math.tanh\n    )\n    attention_wrapper = wrapper.AttentionWrapper(\n        cell, attention_mechanism, attention_layer=attention_layer\n    )\n    with pytest.raises(ValueError):\n        # Should fail because the attention mechanism has not been\n        # initialized.\n        attention_wrapper.get_initial_state(\n            batch_size=dummy_data.batch, dtype=tf.float32\n        )\n    attention_mechanism.setup_memory(\n        dummy_data.encoder_outputs.astype(np.float32),\n        memory_sequence_length=dummy_data.encoder_sequence_length,\n    )\n    initial_state = attention_wrapper.get_initial_state(\n        batch_size=dummy_data.batch, dtype=tf.float32\n    )\n    assert initial_state.attention.shape[-1] == dummy_data.units * 2\n    first_input = dummy_data.decoder_inputs[:, 0].astype(np.float32)\n    output, _ = attention_wrapper(first_input, initial_state)\n    assert output.shape[-1] == dummy_data.units * 2\n\n\ndef _test_with_attention(\n    create_attention_mechanism,\n    expected_final_output,\n    expected_final_state,\n    attention_mechanism_depth=3,\n    alignment_history=False,\n    expected_final_alignment_history=None,\n    attention_layer_size=6,\n    attention_layer=None,\n    create_query_layer=False,\n    create_memory_layer=True,\n    create_attention_kwargs=None,\n):\n    attention_layer_sizes = (\n        [attention_layer_size] if attention_layer_size is not None else None\n    )\n    attention_layers = [attention_layer] if attention_layer is not None else None\n    create_attention_mechanisms = [create_attention_mechanism]\n    attention_mechanism_depths = [attention_mechanism_depth]\n    assert len(create_attention_mechanisms) == 1\n    encoder_sequence_length = [3, 2, 3, 1, 1]\n    decoder_sequence_length = [2, 0, 1, 2, 3]\n    batch_size = 5\n    encoder_max_time = 8\n    decoder_max_time = 4\n    input_depth = 7\n    encoder_output_depth = 10\n    cell_depth = 9\n    create_attention_kwargs = create_attention_kwargs or {}\n\n    if attention_layer_sizes is not None:\n        # Compute sum of attention_layer_sizes. Use encoder_output_depth if\n        # None.\n        attention_depth = sum(\n            attention_layer_size or encoder_output_depth\n            for attention_layer_size in attention_layer_sizes\n        )\n    elif attention_layers is not None:\n        # Compute sum of attention_layers output depth.\n        attention_depth = sum(\n            attention_layer.compute_output_shape(\n                [batch_size, cell_depth + encoder_output_depth]\n            )[-1]\n            for attention_layer in attention_layers\n        )\n    else:\n        attention_depth = encoder_output_depth * len(create_attention_mechanisms)\n\n    decoder_inputs = np.random.randn(batch_size, decoder_max_time, input_depth).astype(\n        np.float32\n    )\n    encoder_outputs = np.random.randn(\n        batch_size, encoder_max_time, encoder_output_depth\n    ).astype(np.float32)\n\n    attention_mechanisms = []\n    for creator, depth in zip(create_attention_mechanisms, attention_mechanism_depths):\n        # Create a memory layer with deterministic initializer to avoid\n        # randomness in the test between graph and eager.\n        if create_query_layer:\n            create_attention_kwargs[""query_layer""] = tf.keras.layers.Dense(\n                depth, kernel_initializer=""ones"", use_bias=False\n            )\n        if create_memory_layer:\n            create_attention_kwargs[""memory_layer""] = tf.keras.layers.Dense(\n                depth, kernel_initializer=""ones"", use_bias=False\n            )\n\n        attention_mechanisms.append(\n            creator(\n                units=depth,\n                memory=encoder_outputs,\n                memory_sequence_length=encoder_sequence_length,\n                **create_attention_kwargs,\n            )\n        )\n\n    attention_layer_size = attention_layer_sizes\n    attention_layer = attention_layers\n    if attention_layer_size is not None:\n        attention_layer_size = attention_layer_size[0]\n    if attention_layer is not None:\n        attention_layer = attention_layer[0]\n    cell = tf.keras.layers.LSTMCell(\n        cell_depth,\n        recurrent_activation=""sigmoid"",\n        kernel_initializer=""ones"",\n        recurrent_initializer=""ones"",\n    )\n    cell = wrapper.AttentionWrapper(\n        cell,\n        attention_mechanisms[0],\n        attention_layer_size=attention_layer_size,\n        alignment_history=alignment_history,\n        attention_layer=attention_layer,\n    )\n    if cell._attention_layers is not None:\n        for layer in cell._attention_layers:\n            layer.kernel_initializer = tf.compat.v1.keras.initializers.glorot_uniform(\n                seed=1337\n            )\n\n    sampler = sampler_py.TrainingSampler()\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    initial_state = cell.get_initial_state(dtype=tf.float32, batch_size=batch_size)\n    final_outputs, final_state, _ = my_decoder(\n        decoder_inputs,\n        initial_state=initial_state,\n        sequence_length=decoder_sequence_length,\n    )\n\n    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)\n    assert isinstance(final_state, wrapper.AttentionWrapperState)\n\n    expected_time = max(decoder_sequence_length)\n    assert (batch_size, expected_time, attention_depth) == tuple(\n        final_outputs.rnn_output.shape.as_list()\n    )\n    assert (batch_size, expected_time) == tuple(final_outputs.sample_id.shape.as_list())\n\n    assert (batch_size, attention_depth) == tuple(final_state.attention.shape.as_list())\n    assert (batch_size, cell_depth) == tuple(final_state.cell_state[0].shape.as_list())\n    assert (batch_size, cell_depth) == tuple(final_state.cell_state[1].shape.as_list())\n\n    if alignment_history:\n        state_alignment_history = final_state.alignment_history.stack()\n        assert (expected_time, batch_size, encoder_max_time) == tuple(\n            state_alignment_history.shape.as_list()\n        )\n        tf.nest.assert_same_structure(\n            cell.state_size,\n            cell.get_initial_state(batch_size=batch_size, dtype=tf.float32),\n        )\n        # Remove the history from final_state for purposes of the\n        # remainder of the tests.\n        final_state = final_state._replace(\n            alignment_history=()\n        )  # pylint: disable=protected-access\n    else:\n        state_alignment_history = ()\n\n    final_outputs = tf.nest.map_structure(np.array, final_outputs)\n    final_state = tf.nest.map_structure(np.array, final_state)\n    state_alignment_history = tf.nest.map_structure(np.array, state_alignment_history)\n    final_output_info = tf.nest.map_structure(get_result_summary, final_outputs)\n\n    final_state_info = tf.nest.map_structure(get_result_summary, final_state)\n\n    tf.nest.map_structure(\n        assert_allclose_or_equal, expected_final_output, final_output_info\n    )\n    tf.nest.map_structure(\n        assert_allclose_or_equal, expected_final_state, final_state_info\n    )\n    # by default, the wrapper emits attention as output\n    if alignment_history:\n        final_alignment_history_info = tf.nest.map_structure(\n            get_result_summary, state_alignment_history\n        )\n        tf.nest.map_structure(\n            assert_allclose_or_equal,\n            # outputs are batch major but the stacked TensorArray is\n            # time major\n            expected_final_alignment_history,\n            final_alignment_history_info,\n        )\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_bahdanau_normalized_dtype(dtype):\n    dummy_data = DummyData2()\n    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)\n    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)\n    attention_mechanism = wrapper.BahdanauAttention(\n        units=dummy_data.units,\n        memory=encoder_outputs,\n        memory_sequence_length=dummy_data.encoder_sequence_length,\n        normalize=True,\n        dtype=dtype,\n    )\n    cell = tf.keras.layers.LSTMCell(\n        dummy_data.units, recurrent_activation=""sigmoid"", dtype=dtype\n    )\n    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)\n\n    sampler = sampler_py.TrainingSampler()\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)\n\n    final_outputs, final_state, _ = my_decoder(\n        decoder_inputs,\n        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),\n        sequence_length=dummy_data.decoder_sequence_length,\n    )\n    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)\n    assert final_outputs.rnn_output.dtype == dtype\n    assert isinstance(final_state, wrapper.AttentionWrapperState)\n\n\n@pytest.mark.parametrize(""dtype"", [np.float32, np.float64])\ndef test_luong_scaled_dtype(dtype):\n    dummy_data = DummyData2()\n    # Test case for GitHub issue 18099\n    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)\n    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)\n    attention_mechanism = wrapper.LuongAttention(\n        units=dummy_data.units,\n        memory=encoder_outputs,\n        memory_sequence_length=dummy_data.encoder_sequence_length,\n        scale=True,\n        dtype=dtype,\n    )\n    cell = tf.keras.layers.LSTMCell(\n        dummy_data.units, recurrent_activation=""sigmoid"", dtype=dtype\n    )\n    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)\n\n    sampler = sampler_py.TrainingSampler()\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)\n\n    final_outputs, final_state, _ = my_decoder(\n        decoder_inputs,\n        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),\n        sequence_length=dummy_data.decoder_sequence_length,\n    )\n    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)\n    assert final_outputs.rnn_output.dtype == dtype\n    assert isinstance(final_state, wrapper.AttentionWrapperState)\n\n\ndef set_random_state_for_tf_and_np():\n    """"""Since the results of the tests have been hardcoded, we need to make sure,\n    when we refactor code that the random state is the same. Meaning that all\n    random functions should be called in the same order.\n    """"""\n    tf.random.set_seed(87654321)\n    np.random.seed(87654321)\n    DummyData2()\n\n\ndef test_bahdanau_not_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.BahdanauAttention\n    create_attention_kwargs = {""kernel_initializer"": ""ones""}\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(np.float32), mean=-0.003204414\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(np.int32), mean=3.2),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(np.float32), mean=0.40868404),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(np.float32), mean=0.89017969),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(np.float32), mean=0.041453815\n        ),\n        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype(np.float32), mean=0.125),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(np.float32), mean=0.125\n        ),\n        alignment_history=(),\n    )\n    expected_final_alignment_history = ResultSummary(\n        shape=(3, 5, 8), dtype=np.dtype(np.float32), mean=0.125\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        alignment_history=True,\n        create_query_layer=True,\n        expected_final_alignment_history=expected_final_alignment_history,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_bahdanau_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.BahdanauAttention\n    create_attention_kwargs = {""kernel_initializer"": ""ones"", ""normalize"": True}\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=-0.008089137\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=2.8),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.49166861),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.01068615),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=0.042427111\n        ),\n        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125\n        ),\n        alignment_history=(),\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        create_query_layer=True,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_luong_not_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.LuongAttention\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=-0.06124732\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=2.73333333),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.52021580),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.0964939),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=-0.0318060\n        ),\n        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125\n        ),\n        alignment_history=(),\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        attention_mechanism_depth=9,\n    )\n\n\ndef test_luong_scaled():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.LuongAttention\n    create_attention_kwargs = {""scale"": True}\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=-0.06124732\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=2.73333333),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.52021580),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.0964939),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=-0.0318060\n        ),\n        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125\n        ),\n        alignment_history=(),\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        attention_mechanism_depth=9,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_not_use_attention_layer():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.BahdanauAttention\n    create_attention_kwargs = {""kernel_initializer"": ""ones""}\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 10), dtype=np.dtype(""float32""), mean=0.078317143\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=4.2),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.89382392),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.722382),\n        ],\n        attention=ResultSummary(\n            shape=(5, 10), dtype=np.dtype(""float32""), mean=0.026356646\n        ),\n        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.125\n        ),\n        alignment_history=(),\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        attention_layer_size=None,\n        create_query_layer=True,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_bahdanau_monotonic_not_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.BahdanauMonotonicAttention\n    create_attention_kwargs = {""kernel_initializer"": ""ones""}\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=-0.009921653\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=3.13333333),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.44612807),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.95786464),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=0.038682378\n        ),\n        alignments=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.09778417\n        ),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.09778417\n        ),\n        alignment_history=(),\n    )\n    expected_final_alignment_history = ResultSummary(\n        shape=(3, 5, 8), dtype=np.dtype(""float32""), mean=0.10261579603\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        alignment_history=True,\n        expected_final_alignment_history=expected_final_alignment_history,\n        create_query_layer=True,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_bahdanau_monotonic_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.BahdanauMonotonicAttention\n    create_attention_kwargs = {""kernel_initializer"": ""ones"", ""normalize"": True}\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=0.007140680\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=3.26666666),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.47012400),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.0249618),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=0.068432882\n        ),\n        alignments=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.0615656\n        ),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.0615656\n        ),\n        alignment_history=(),\n    )\n    expected_final_alignment_history = ResultSummary(\n        shape=(3, 5, 8), dtype=np.dtype(""float32""), mean=0.07909643\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        alignment_history=True,\n        expected_final_alignment_history=expected_final_alignment_history,\n        create_query_layer=True,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_luong_monotonic_not_normalized():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.LuongMonotonicAttention\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=0.003664831\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=3.06666666),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.54318606),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.12592840),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=0.059128221\n        ),\n        alignments=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.05112994\n        ),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.05112994\n        ),\n        alignment_history=(),\n    )\n    expected_final_alignment_history = ResultSummary(\n        shape=(3, 5, 8), dtype=np.dtype(""float32""), mean=0.06994973868\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        attention_mechanism_depth=9,\n        alignment_history=True,\n        expected_final_alignment_history=expected_final_alignment_history,\n    )\n\n\ndef test_luong_monotonic_scaled():\n    set_random_state_for_tf_and_np()\n    create_attention_mechanism = wrapper.LuongMonotonicAttention\n    create_attention_kwargs = {""scale"": True}\n\n    expected_final_output = basic_decoder.BasicDecoderOutput(\n        rnn_output=ResultSummary(\n            shape=(5, 3, 6), dtype=np.dtype(""float32""), mean=0.003664831\n        ),\n        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(""int32""), mean=3.06666666),\n    )\n    expected_final_state = wrapper.AttentionWrapperState(\n        cell_state=[\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=0.54318606),\n            ResultSummary(shape=(5, 9), dtype=np.dtype(""float32""), mean=1.12592840),\n        ],\n        attention=ResultSummary(\n            shape=(5, 6), dtype=np.dtype(""float32""), mean=0.059128221\n        ),\n        alignments=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.05112994\n        ),\n        attention_state=ResultSummary(\n            shape=(5, 8), dtype=np.dtype(""float32""), mean=0.05112994\n        ),\n        alignment_history=(),\n    )\n    expected_final_alignment_history = ResultSummary(\n        shape=(3, 5, 8), dtype=np.dtype(""float32""), mean=0.06994973868\n    )\n\n    _test_with_attention(\n        create_attention_mechanism,\n        expected_final_output,\n        expected_final_state,\n        attention_mechanism_depth=9,\n        alignment_history=True,\n        expected_final_alignment_history=expected_final_alignment_history,\n        create_attention_kwargs=create_attention_kwargs,\n    )\n\n\ndef test_attention_state_with_keras_rnn():\n    # See https://github.com/tensorflow/addons/issues/1095.\n    cell = tf.keras.layers.LSTMCell(8)\n\n    mechanism = wrapper.LuongAttention(units=8, memory=tf.ones((2, 4, 8)))\n\n    cell = wrapper.AttentionWrapper(cell=cell, attention_mechanism=mechanism)\n\n    layer = tf.keras.layers.RNN(cell)\n    _ = layer(inputs=tf.ones((2, 4, 8)))\n\n    # Make sure the explicit initial_state also works.\n    initial_state = cell.get_initial_state(batch_size=2, dtype=tf.float32)\n    _ = layer(inputs=tf.ones((2, 4, 8)), initial_state=initial_state)\n\n\ndef test_attention_state_with_variable_length_input():\n    cell = tf.keras.layers.LSTMCell(3)\n    mechanism = wrapper.LuongAttention(units=3)\n    cell = wrapper.AttentionWrapper(cell, mechanism)\n\n    var_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)\n    lengths = tf.random.uniform(\n        shape=(var_len,), minval=1, maxval=var_len + 1, dtype=tf.int32\n    )\n    data = tf.ones(shape=(var_len, var_len, 3))\n    mask = tf.sequence_mask(lengths, maxlen=var_len)\n\n    mechanism.setup_memory(data)\n    layer = tf.keras.layers.RNN(cell)\n\n    _ = layer(data, mask=mask)\n\n\ndef test_attention_wrapper_with_gru_cell():\n    mechanism = wrapper.LuongAttention(units=3)\n    cell = tf.keras.layers.GRUCell(3)\n    cell = wrapper.AttentionWrapper(cell, mechanism)\n    memory = tf.ones([2, 5, 3])\n    inputs = tf.ones([2, 3])\n    mechanism.setup_memory(memory)\n    initial_state = cell.get_initial_state(inputs=inputs)\n    _, state = cell(inputs, initial_state)\n    tf.nest.assert_same_structure(initial_state, state)\n\n\ndef test_attention_wrapper_with_multiple_attention_mechanisms():\n    cell = tf.keras.layers.LSTMCell(5)\n    mechanisms = [wrapper.LuongAttention(units=3), wrapper.LuongAttention(units=3)]\n    # We simply test that the wrapper creation makes no error.\n    wrapper.AttentionWrapper(cell, mechanisms, attention_layer_size=[4, 5])\n    wrapper.AttentionWrapper(\n        cell,\n        mechanisms,\n        attention_layer=[tf.keras.layers.Dense(4), tf.keras.layers.Dense(5)],\n    )\n'"
tensorflow_addons/seq2seq/tests/basic_decoder_test.py,66,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tfa.seq2seq.basic_decoder.""""""\n\nimport numpy as np\nimport pytest\n\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import attention_wrapper\nfrom tensorflow_addons.seq2seq import basic_decoder\nfrom tensorflow_addons.seq2seq import sampler as sampler_py\n\n\n@pytest.mark.parametrize(""use_output_layer"", [True, False])\n@pytest.mark.parametrize(\n    ""cell_class"", [tf.keras.layers.LSTMCell, tf.keras.layers.GRUCell]\n)\ndef test_step_with_training_helper_output_layer(cell_class, use_output_layer):\n    sequence_length = [3, 4, 3, 1, 0]\n    batch_size = 5\n    max_time = 8\n    input_depth = 7\n    cell_depth = 10\n    output_layer_depth = 3\n\n    inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    input_t = tf.constant(inputs)\n    cell = cell_class(cell_depth)\n    sampler = sampler_py.TrainingSampler(time_major=False)\n    if use_output_layer:\n        output_layer = tf.keras.layers.Dense(output_layer_depth, use_bias=False)\n        expected_output_depth = output_layer_depth\n    else:\n        output_layer = None\n        expected_output_depth = cell_depth\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(\n        cell=cell, sampler=sampler, output_layer=output_layer\n    )\n\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        input_t, initial_state=initial_state, sequence_length=sequence_length\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(expected_output_depth, tf.TensorShape([]))\n        == output_size\n    )\n\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    if isinstance(cell, tf.keras.layers.LSTMCell):\n        assert len(first_state) == 2\n        assert len(step_state) == 2\n        assert (batch_size, cell_depth) == first_state[0].shape\n        assert (batch_size, cell_depth) == first_state[1].shape\n        assert (batch_size, cell_depth) == step_state[0].shape\n        assert (batch_size, cell_depth) == step_state[1].shape\n    elif isinstance(cell, tf.keras.layers.GRUCell):\n        assert tf.is_tensor(first_state)\n        assert tf.is_tensor(step_state)\n        assert (batch_size, cell_depth) == first_state.shape\n        assert (batch_size, cell_depth) == step_state.shape\n    assert type(step_outputs) is basic_decoder.BasicDecoderOutput\n    assert (batch_size, expected_output_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n\n    if use_output_layer:\n        # The output layer was accessed\n        assert len(output_layer.variables) == 1\n\n    eval_result = {\n        ""batch_size"": batch_size_t,\n        ""first_finished"": first_finished,\n        ""first_inputs"": first_inputs,\n        ""first_state"": first_state,\n        ""step_outputs"": step_outputs,\n        ""step_state"": step_state,\n        ""step_next_inputs"": step_next_inputs,\n        ""step_finished"": step_finished,\n    }\n\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, False, True]),\n        eval_result[""first_finished""].numpy(),\n    )\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, True, True]),\n        eval_result[""step_finished""].numpy(),\n    )\n    assert output_dtype.sample_id == eval_result[""step_outputs""].sample_id.dtype\n    np.testing.assert_equal(\n        np.argmax(eval_result[""step_outputs""].rnn_output, -1),\n        eval_result[""step_outputs""].sample_id,\n    )\n\n\n@pytest.mark.parametrize(""use_mask"", [True, False, None])\ndef test_step_with_training_helper_masked_input(use_mask):\n    batch_size = 5\n    max_time = 8\n    sequence_length = [max_time] * batch_size if use_mask is None else [3, 4, 3, 1, 0]\n    sequence_length = np.array(sequence_length, dtype=np.int32)\n    mask = [[True] * l + [False] * (max_time - l) for l in sequence_length]\n    input_depth = 7\n    cell_depth = 10\n    output_layer_depth = 3\n\n    inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    input_t = tf.constant(inputs)\n    cell = tf.keras.layers.LSTMCell(cell_depth)\n    sampler = sampler_py.TrainingSampler(time_major=False)\n    output_layer = tf.keras.layers.Dense(output_layer_depth, use_bias=False)\n    expected_output_depth = output_layer_depth\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(\n        cell=cell, sampler=sampler, output_layer=output_layer\n    )\n\n    if use_mask is None:\n        (first_finished, first_inputs, first_state) = my_decoder.initialize(\n            input_t, initial_state=initial_state\n        )\n    elif use_mask:\n        (first_finished, first_inputs, first_state) = my_decoder.initialize(\n            input_t, initial_state=initial_state, mask=mask\n        )\n    else:\n        (first_finished, first_inputs, first_state) = my_decoder.initialize(\n            input_t, initial_state=initial_state, sequence_length=sequence_length,\n        )\n\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(expected_output_depth, tf.TensorShape([]))\n        == output_size\n    )\n\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert type(step_outputs) is basic_decoder.BasicDecoderOutput\n    assert (batch_size, expected_output_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    assert len(output_layer.variables) == 1\n\n    eval_result = {\n        ""batch_size"": batch_size_t,\n        ""first_finished"": first_finished,\n        ""first_inputs"": first_inputs,\n        ""first_state"": first_state,\n        ""step_outputs"": step_outputs,\n        ""step_state"": step_state,\n        ""step_next_inputs"": step_next_inputs,\n        ""step_finished"": step_finished,\n    }\n\n    np.testing.assert_equal(sequence_length == 0, eval_result[""first_finished""])\n    np.testing.assert_equal(\n        (np.maximum(sequence_length - 1, 0) == 0), eval_result[""step_finished""]\n    )\n    assert output_dtype.sample_id == eval_result[""step_outputs""].sample_id.dtype\n    np.testing.assert_equal(\n        np.argmax(eval_result[""step_outputs""].rnn_output, -1),\n        eval_result[""step_outputs""].sample_id,\n    )\n\n\ndef test_step_with_greedy_embedding_helper():\n    batch_size = 5\n    vocabulary_size = 7\n    cell_depth = vocabulary_size  # cell\'s logits must match vocabulary size\n    input_depth = 10\n    start_tokens = np.random.randint(0, vocabulary_size, size=batch_size)\n    end_token = 1\n\n    embeddings = np.random.randn(vocabulary_size, input_depth).astype(np.float32)\n    embeddings_t = tf.constant(embeddings)\n    cell = tf.keras.layers.LSTMCell(vocabulary_size)\n    sampler = sampler_py.GreedyEmbeddingSampler()\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        embeddings_t,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=initial_state,\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size\n    )\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, cell_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    eval_result = {\n        ""batch_size"": batch_size_t,\n        ""first_finished"": first_finished,\n        ""first_inputs"": first_inputs,\n        ""first_state"": first_state,\n        ""step_outputs"": step_outputs,\n        ""step_state"": step_state,\n        ""step_next_inputs"": step_next_inputs,\n        ""step_finished"": step_finished,\n    }\n\n    expected_sample_ids = np.argmax(eval_result[""step_outputs""].rnn_output, -1)\n    expected_step_finished = expected_sample_ids == end_token\n    expected_step_next_inputs = embeddings[expected_sample_ids]\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, False, False]),\n        eval_result[""first_finished""].numpy(),\n    )\n    np.testing.assert_equal(expected_step_finished, eval_result[""step_finished""])\n    assert output_dtype.sample_id == eval_result[""step_outputs""].sample_id.dtype\n    np.testing.assert_equal(expected_sample_ids, eval_result[""step_outputs""].sample_id)\n    np.testing.assert_equal(expected_step_next_inputs, eval_result[""step_next_inputs""])\n\n\ndef test_step_with_sample_embedding_helper():\n    batch_size = 5\n    vocabulary_size = 7\n    cell_depth = vocabulary_size  # cell\'s logits must match vocabulary size\n    input_depth = 10\n    np.random.seed(0)\n    start_tokens = np.random.randint(0, vocabulary_size, size=batch_size)\n    end_token = 1\n\n    embeddings = np.random.randn(vocabulary_size, input_depth).astype(np.float32)\n    embeddings_t = tf.constant(embeddings)\n    cell = tf.keras.layers.LSTMCell(vocabulary_size)\n    sampler = sampler_py.SampleEmbeddingSampler(seed=0)\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        embeddings_t,\n        start_tokens=start_tokens,\n        end_token=end_token,\n        initial_state=initial_state,\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size\n    )\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, cell_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    eval_result = {\n        ""batch_size"": batch_size_t,\n        ""first_finished"": first_finished,\n        ""first_inputs"": first_inputs,\n        ""first_state"": first_state,\n        ""step_outputs"": step_outputs,\n        ""step_state"": step_state,\n        ""step_next_inputs"": step_next_inputs,\n        ""step_finished"": step_finished,\n    }\n\n    sample_ids = eval_result[""step_outputs""].sample_id\n    assert output_dtype.sample_id == sample_ids.dtype\n    expected_step_finished = sample_ids == end_token\n    expected_step_next_inputs = embeddings[sample_ids, :]\n    np.testing.assert_equal(\n        np.asanyarray(expected_step_finished), eval_result[""step_finished""].numpy()\n    )\n    np.testing.assert_equal(expected_step_next_inputs, eval_result[""step_next_inputs""])\n\n\ndef test_step_with_scheduled_embedding_training_helper():\n    sequence_length = [3, 4, 3, 1, 0]\n    batch_size = 5\n    max_time = 8\n    input_depth = 7\n    vocabulary_size = 10\n\n    inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    input_t = tf.constant(inputs)\n    embeddings = np.random.randn(vocabulary_size, input_depth).astype(np.float32)\n    half = tf.constant(0.5)\n    cell = tf.keras.layers.LSTMCell(vocabulary_size)\n    sampler = sampler_py.ScheduledEmbeddingTrainingSampler(\n        sampling_probability=half, time_major=False\n    )\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        input_t,\n        sequence_length=sequence_length,\n        embedding=embeddings,\n        initial_state=initial_state,\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(vocabulary_size, tf.TensorShape([]))\n        == output_size\n    )\n\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, vocabulary_size) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, vocabulary_size) == first_state[0].shape\n    assert (batch_size, vocabulary_size) == first_state[1].shape\n    assert (batch_size, vocabulary_size) == step_state[0].shape\n    assert (batch_size, vocabulary_size) == step_state[1].shape\n    assert (batch_size, input_depth) == step_next_inputs.shape\n\n    eval_result = {\n        ""batch_size"": batch_size_t.numpy(),\n        ""first_finished"": first_finished.numpy(),\n        ""first_inputs"": first_inputs.numpy(),\n        ""first_state"": np.asanyarray(first_state),\n        ""step_outputs"": step_outputs,\n        ""step_state"": np.asanyarray(step_state),\n        ""step_next_inputs"": step_next_inputs.numpy(),\n        ""step_finished"": step_finished.numpy(),\n    }\n\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, False, True]),\n        eval_result[""first_finished""],\n    )\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, True, True]), eval_result[""step_finished""],\n    )\n    sample_ids = eval_result[""step_outputs""].sample_id.numpy()\n    assert output_dtype.sample_id == sample_ids.dtype\n    batch_where_not_sampling = np.where(sample_ids == -1)\n    batch_where_sampling = np.where(sample_ids > -1)\n\n    np.testing.assert_equal(\n        eval_result[""step_next_inputs""][batch_where_sampling],\n        embeddings[sample_ids[batch_where_sampling]],\n    )\n    np.testing.assert_equal(\n        eval_result[""step_next_inputs""][batch_where_not_sampling],\n        np.squeeze(inputs[batch_where_not_sampling, 1], axis=0),\n    )\n\n\n@pytest.mark.parametrize(""use_auxiliary_inputs"", [True, False])\n@pytest.mark.parametrize(""use_next_inputs_fn"", [True, False])\n@pytest.mark.parametrize(""sampling_probability"", [0.0, 0.5])\ndef test_step_with_scheduled_output_training_helper(\n    sampling_probability, use_next_inputs_fn, use_auxiliary_inputs\n):\n    sequence_length = [3, 4, 3, 1, 0]\n    batch_size = 5\n    max_time = 8\n    input_depth = 7\n    cell_depth = input_depth\n    if use_auxiliary_inputs:\n        auxiliary_input_depth = 4\n        auxiliary_inputs = np.random.randn(\n            batch_size, max_time, auxiliary_input_depth\n        ).astype(np.float32)\n    else:\n        auxiliary_inputs = None\n\n    inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    input_t = tf.constant(inputs)\n    cell = tf.keras.layers.LSTMCell(cell_depth)\n    sampling_probability = tf.constant(sampling_probability)\n\n    if use_next_inputs_fn:\n\n        def next_inputs_fn(outputs):\n            # Use deterministic function for test.\n            samples = tf.argmax(outputs, axis=1)\n            return tf.one_hot(samples, cell_depth, dtype=tf.float32)\n\n    else:\n        next_inputs_fn = None\n\n    sampler = sampler_py.ScheduledOutputTrainingSampler(\n        sampling_probability=sampling_probability,\n        time_major=False,\n        next_inputs_fn=next_inputs_fn,\n    )\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        input_t,\n        sequence_length=sequence_length,\n        initial_state=initial_state,\n        auxiliary_inputs=auxiliary_inputs,\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size\n    )\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n\n    if use_next_inputs_fn:\n        output_after_next_inputs_fn = next_inputs_fn(step_outputs.rnn_output)\n\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, cell_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    fetches = {\n        ""batch_size"": batch_size_t.numpy(),\n        ""first_finished"": first_finished.numpy(),\n        ""first_inputs"": first_inputs.numpy(),\n        ""first_state"": np.asanyarray(first_state),\n        ""step_outputs"": step_outputs,\n        ""step_state"": np.asanyarray(step_state),\n        ""step_next_inputs"": step_next_inputs.numpy(),\n        ""step_finished"": step_finished.numpy(),\n    }\n\n    if use_next_inputs_fn:\n        fetches[""output_after_next_inputs_fn""] = output_after_next_inputs_fn\n\n    eval_result = fetches\n\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, False, True]),\n        eval_result[""first_finished""],\n    )\n    np.testing.assert_equal(\n        np.asanyarray([False, False, False, True, True]), eval_result[""step_finished""],\n    )\n\n    sample_ids = eval_result[""step_outputs""].sample_id.numpy()\n    assert output_dtype.sample_id == sample_ids.dtype\n    batch_where_not_sampling = np.where(np.logical_not(sample_ids))\n    batch_where_sampling = np.where(sample_ids)\n\n    auxiliary_inputs_to_concat = (\n        auxiliary_inputs[:, 1]\n        if use_auxiliary_inputs\n        else np.array([]).reshape(batch_size, 0).astype(np.float32)\n    )\n\n    expected_next_sampling_inputs = np.concatenate(\n        (\n            eval_result[""output_after_next_inputs_fn""].numpy()[batch_where_sampling]\n            if use_next_inputs_fn\n            else eval_result[""step_outputs""].rnn_output.numpy()[batch_where_sampling],\n            auxiliary_inputs_to_concat[batch_where_sampling],\n        ),\n        axis=-1,\n    )\n\n    np.testing.assert_equal(\n        eval_result[""step_next_inputs""][batch_where_sampling],\n        expected_next_sampling_inputs,\n    )\n\n    np.testing.assert_equal(\n        eval_result[""step_next_inputs""][batch_where_not_sampling],\n        np.concatenate(\n            (\n                np.squeeze(inputs[batch_where_not_sampling, 1], axis=0),\n                auxiliary_inputs_to_concat[batch_where_not_sampling],\n            ),\n            axis=-1,\n        ),\n    )\n\n\ndef test_step_with_inference_helper_categorical():\n    batch_size = 5\n    vocabulary_size = 7\n    cell_depth = vocabulary_size\n    start_token = 0\n    end_token = 6\n\n    start_inputs = tf.one_hot(\n        np.ones(batch_size, dtype=np.int32) * start_token, vocabulary_size\n    )\n\n    # The sample function samples categorically from the logits.\n    def sample_fn(x):\n        return sampler_py.categorical_sample(logits=x)\n\n    # The next inputs are a one-hot encoding of the sampled labels.\n    def next_inputs_fn(x):\n        return tf.one_hot(x, vocabulary_size, dtype=tf.float32)\n\n    def end_fn(sample_ids):\n        return tf.equal(sample_ids, end_token)\n\n    cell = tf.keras.layers.LSTMCell(vocabulary_size)\n    sampler = sampler_py.InferenceSampler(\n        sample_fn,\n        sample_shape=(),\n        sample_dtype=tf.int32,\n        end_fn=end_fn,\n        next_inputs_fn=next_inputs_fn,\n    )\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        start_inputs, initial_state=initial_state\n    )\n\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert (\n        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size\n    )\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, cell_depth) == step_outputs[0].shape\n    assert (batch_size,) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    eval_result = {\n        ""batch_size"": batch_size_t.numpy(),\n        ""first_finished"": first_finished.numpy(),\n        ""first_inputs"": first_inputs.numpy(),\n        ""first_state"": np.asanyarray(first_state),\n        ""step_outputs"": step_outputs,\n        ""step_state"": np.asanyarray(step_state),\n        ""step_next_inputs"": step_next_inputs.numpy(),\n        ""step_finished"": step_finished.numpy(),\n    }\n\n    sample_ids = eval_result[""step_outputs""].sample_id.numpy()\n    assert output_dtype.sample_id == sample_ids.dtype\n    expected_step_finished = sample_ids == end_token\n    expected_step_next_inputs = np.zeros((batch_size, vocabulary_size))\n    expected_step_next_inputs[np.arange(batch_size), sample_ids] = 1.0\n    np.testing.assert_equal(expected_step_finished, eval_result[""step_finished""])\n    np.testing.assert_equal(expected_step_next_inputs, eval_result[""step_next_inputs""])\n\n\ndef test_step_with_inference_helper_multilabel():\n    batch_size = 5\n    vocabulary_size = 7\n    cell_depth = vocabulary_size\n    start_token = 0\n    end_token = 6\n\n    start_inputs = tf.one_hot(\n        np.ones(batch_size, dtype=np.int32) * start_token, vocabulary_size\n    )\n\n    # The sample function samples independent bernoullis from the logits.\n    def sample_fn(x):\n        return sampler_py.bernoulli_sample(logits=x, dtype=tf.bool)\n\n    # The next inputs are a one-hot encoding of the sampled labels.\n    def next_inputs_fn(x):\n        return tf.cast(x, tf.float32)\n\n    def end_fn(sample_ids):\n        return sample_ids[:, end_token]\n\n    cell = tf.keras.layers.LSTMCell(vocabulary_size)\n    sampler = sampler_py.InferenceSampler(\n        sample_fn,\n        sample_shape=[cell_depth],\n        sample_dtype=tf.bool,\n        end_fn=end_fn,\n        next_inputs_fn=next_inputs_fn,\n    )\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)\n    (first_finished, first_inputs, first_state) = my_decoder.initialize(\n        start_inputs, initial_state=initial_state\n    )\n    output_size = my_decoder.output_size\n    output_dtype = my_decoder.output_dtype\n    assert basic_decoder.BasicDecoderOutput(cell_depth, cell_depth) == output_size\n    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.bool) == output_dtype\n\n    (step_outputs, step_state, step_next_inputs, step_finished,) = my_decoder.step(\n        tf.constant(0), first_inputs, first_state\n    )\n    batch_size_t = my_decoder.batch_size\n\n    assert len(first_state) == 2\n    assert len(step_state) == 2\n    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)\n    assert (batch_size, cell_depth) == step_outputs[0].shape\n    assert (batch_size, cell_depth) == step_outputs[1].shape\n    assert (batch_size, cell_depth) == first_state[0].shape\n    assert (batch_size, cell_depth) == first_state[1].shape\n    assert (batch_size, cell_depth) == step_state[0].shape\n    assert (batch_size, cell_depth) == step_state[1].shape\n\n    eval_result = {\n        ""batch_size"": batch_size_t.numpy(),\n        ""first_finished"": first_finished.numpy(),\n        ""first_inputs"": first_inputs.numpy(),\n        ""first_state"": np.asanyarray(first_state),\n        ""step_outputs"": step_outputs,\n        ""step_state"": np.asanyarray(step_state),\n        ""step_next_inputs"": step_next_inputs.numpy(),\n        ""step_finished"": step_finished.numpy(),\n    }\n\n    sample_ids = eval_result[""step_outputs""].sample_id.numpy()\n    assert output_dtype.sample_id == sample_ids.dtype\n    expected_step_finished = sample_ids[:, end_token]\n    expected_step_next_inputs = sample_ids.astype(np.float32)\n    np.testing.assert_equal(expected_step_finished, eval_result[""step_finished""])\n    np.testing.assert_equal(expected_step_next_inputs, eval_result[""step_next_inputs""])\n\n\ndef test_basic_decoder_with_attention_wrapper():\n    units = 32\n    vocab_size = 1000\n    attention_mechanism = attention_wrapper.LuongAttention(units)\n    cell = tf.keras.layers.LSTMCell(units)\n    cell = attention_wrapper.AttentionWrapper(cell, attention_mechanism)\n    output_layer = tf.keras.layers.Dense(vocab_size)\n    sampler = sampler_py.TrainingSampler()\n    # BasicDecoder should accept a non initialized AttentionWrapper.\n    basic_decoder.BasicDecoder(cell, sampler, output_layer=output_layer)\n\n\ndef test_right_padded_sequence_assertion():\n    right_padded_sequence = [[True, True, False, False], [True, True, True, False]]\n    left_padded_sequence = [[False, False, True, True], [False, True, True, True]]\n\n    _ = sampler_py._check_sequence_is_right_padded(right_padded_sequence, False)\n\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        _ = sampler_py._check_sequence_is_right_padded(left_padded_sequence, False)\n'"
tensorflow_addons/seq2seq/tests/beam_search_decoder_test.py,66,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tfa.seq2seq.seq2seq.beam_search_decoder.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import attention_wrapper\nfrom tensorflow_addons.seq2seq import beam_search_decoder, gather_tree\n\n\ndef test_gather_tree():\n    # (max_time = 3, batch_size = 2, beam_width = 3)\n\n    # create (batch_size, max_time, beam_width) matrix and transpose it\n    predicted_ids = np.array(\n        [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[2, 3, 4], [5, 6, 7], [8, 9, 10]]],\n        dtype=np.int32,\n    ).transpose([1, 0, 2])\n    parent_ids = np.array(\n        [[[0, 0, 0], [0, 1, 1], [2, 1, 2]], [[0, 0, 0], [1, 2, 0], [2, 1, 1]]],\n        dtype=np.int32,\n    ).transpose([1, 0, 2])\n\n    # sequence_lengths is shaped (batch_size = 3)\n    max_sequence_lengths = [3, 3]\n\n    expected_result = np.array(\n        [[[2, 2, 2], [6, 5, 6], [7, 8, 9]], [[2, 4, 4], [7, 6, 6], [8, 9, 10]]]\n    ).transpose([1, 0, 2])\n\n    res = gather_tree(\n        predicted_ids,\n        parent_ids,\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=11,\n    )\n\n    np.testing.assert_equal(expected_result, res)\n\n\ndef _test_gather_tree_from_array(depth_ndims=0, merged_batch_beam=False):\n    array = np.array(\n        [\n            [[1, 2, 3], [4, 5, 6], [7, 8, 9], [0, 0, 0]],\n            [[2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 0]],\n        ]\n    ).transpose([1, 0, 2])\n    parent_ids = np.array(\n        [\n            [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]],\n            [[0, 0, 0], [1, 1, 0], [2, 0, 1], [0, 1, 0]],\n        ]\n    ).transpose([1, 0, 2])\n    expected_array = np.array(\n        [\n            [[2, 2, 2], [6, 5, 6], [7, 8, 9], [0, 0, 0]],\n            [[2, 3, 2], [7, 5, 7], [8, 9, 8], [11, 12, 0]],\n        ]\n    ).transpose([1, 0, 2])\n    sequence_length = [[3, 3, 3], [4, 4, 3]]\n\n    array = tf.convert_to_tensor(array, dtype=tf.float32)\n    parent_ids = tf.convert_to_tensor(parent_ids, dtype=tf.int32)\n    expected_array = tf.convert_to_tensor(expected_array, dtype=tf.float32)\n\n    max_time = tf.shape(array)[0]\n    batch_size = tf.shape(array)[1]\n    beam_width = tf.shape(array)[2]\n\n    def _tile_in_depth(tensor):\n        # Generate higher rank tensors by concatenating tensor and\n        # tensor + 1.\n        for _ in range(depth_ndims):\n            tensor = tf.stack([tensor, tensor + 1], -1)\n        return tensor\n\n    if merged_batch_beam:\n        array = tf.reshape(array, [max_time, batch_size * beam_width])\n        expected_array = tf.reshape(expected_array, [max_time, batch_size * beam_width])\n\n    if depth_ndims > 0:\n        array = _tile_in_depth(array)\n        expected_array = _tile_in_depth(expected_array)\n\n    sorted_array = beam_search_decoder.gather_tree_from_array(\n        array, parent_ids, sequence_length\n    )\n\n    np.testing.assert_equal(expected_array.numpy(), sorted_array.numpy())\n\n\ndef test_gather_tree_from_array_scalar():\n    _test_gather_tree_from_array()\n\n\ndef test_gather_tree_from_array_1d():\n    _test_gather_tree_from_array(depth_ndims=1)\n\n\ndef test_gather_tree_from_array_1d_with_merged_batch_beam():\n    _test_gather_tree_from_array(depth_ndims=1, merged_batch_beam=True)\n\n\ndef test_gather_tree_from_array_2d():\n    _test_gather_tree_from_array(depth_ndims=2)\n\n\ndef test_gather_tree_from_array_complex_trajectory():\n    # Max. time = 7, batch = 1, beam = 5.\n    array = np.expand_dims(\n        np.array(\n            [\n                [[25, 12, 114, 89, 97]],\n                [[9, 91, 64, 11, 162]],\n                [[34, 34, 34, 34, 34]],\n                [[2, 4, 2, 2, 4]],\n                [[2, 3, 6, 2, 2]],\n                [[2, 2, 2, 3, 2]],\n                [[2, 2, 2, 2, 2]],\n            ]\n        ),\n        -1,\n    )\n    parent_ids = np.array(\n        [\n            [[0, 0, 0, 0, 0]],\n            [[0, 0, 0, 0, 0]],\n            [[0, 1, 2, 3, 4]],\n            [[0, 0, 1, 2, 1]],\n            [[0, 1, 1, 2, 3]],\n            [[0, 1, 3, 1, 2]],\n            [[0, 1, 2, 3, 4]],\n        ]\n    )\n    expected_array = np.expand_dims(\n        np.array(\n            [\n                [[25, 25, 25, 25, 25]],\n                [[9, 9, 91, 9, 9]],\n                [[34, 34, 34, 34, 34]],\n                [[2, 4, 2, 4, 4]],\n                [[2, 3, 6, 3, 6]],\n                [[2, 2, 2, 3, 2]],\n                [[2, 2, 2, 2, 2]],\n            ]\n        ),\n        -1,\n    )\n    sequence_length = [[4, 6, 4, 7, 6]]\n\n    array = tf.convert_to_tensor(array, dtype=tf.float32)\n    parent_ids = tf.convert_to_tensor(parent_ids, dtype=tf.int32)\n    expected_array = tf.convert_to_tensor(expected_array, dtype=tf.float32)\n\n    sorted_array = beam_search_decoder.gather_tree_from_array(\n        array, parent_ids, sequence_length\n    )\n\n    np.testing.assert_equal(expected_array.numpy(), sorted_array.numpy())\n\n\ndef basic_test_array_shape_dynamic_checks(\n    static_shape, dynamic_shape, batch_size, beam_width, is_valid=True\n):\n    @tf.function(input_signature=(tf.TensorSpec(dynamic_shape, dtype=tf.float32),))\n    def _test_body(t):\n        beam_search_decoder._check_batch_beam(t, batch_size, beam_width)\n\n    t = tf.random.uniform(static_shape, dtype=tf.float32)\n    if is_valid:\n        _test_body(t)\n    else:\n        with pytest.raises(tf.errors.InvalidArgumentError):\n            _test_body(t)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_array_shape_dynamic_checks():\n    basic_test_array_shape_dynamic_checks(\n        (8, 4, 5, 10), (None, None, 5, 10), 4, 5, is_valid=True\n    )\n    basic_test_array_shape_dynamic_checks(\n        (8, 20, 10), (None, None, 10), 4, 5, is_valid=True\n    )\n    basic_test_array_shape_dynamic_checks(\n        (8, 21, 10), (None, None, 10), 4, 5, is_valid=False\n    )\n    basic_test_array_shape_dynamic_checks(\n        (8, 4, 6, 10), (None, None, None, 10), 4, 5, is_valid=False\n    )\n    basic_test_array_shape_dynamic_checks((8, 4), (None, None), 4, 5, is_valid=False)\n\n\ndef test_array_shape_static_checks():\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([None, None, None]), 3, 5\n        )\n        is True\n    )\n\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([15, None, None]), 3, 5\n        )\n        is True\n    )\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([16, None, None]), 3, 5\n        )\n        is False\n    )\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([3, 5, None]), 3, 5\n        )\n        is True\n    )\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([3, 6, None]), 3, 5\n        )\n        is False\n    )\n    assert (\n        beam_search_decoder._check_static_batch_beam_maybe(\n            tf.TensorShape([5, 3, None]), 3, 5\n        )\n        is False\n    )\n\n\ndef test_eos_masking():\n    probs = tf.constant(\n        [\n            [\n                [-0.2, -0.2, -0.2, -0.2, -0.2],\n                [-0.3, -0.3, -0.3, 3, 0],\n                [5, 6, 0, 0, 0],\n            ],\n            [[-0.2, -0.2, -0.2, -0.2, 0], [-0.3, -0.3, -0.1, 3, 0], [5, 6, 3, 0, 0],],\n        ]\n    )\n\n    eos_token = 0\n    previously_finished = np.array([[0, 1, 0], [0, 1, 1]], dtype=bool)\n    masked = beam_search_decoder._mask_probs(probs, eos_token, previously_finished)\n    masked = masked.numpy()\n\n    np.testing.assert_equal(probs[0][0], masked[0][0])\n    np.testing.assert_equal(probs[0][2], masked[0][2])\n    np.testing.assert_equal(probs[1][0], masked[1][0])\n\n    np.testing.assert_equal(masked[0][1][0], 0)\n    np.testing.assert_equal(masked[1][1][0], 0)\n    np.testing.assert_equal(masked[1][2][0], 0)\n\n    for i in range(1, 5):\n        np.testing.assert_allclose(masked[0][1][i], np.finfo(""float32"").min)\n        np.testing.assert_allclose(masked[1][1][i], np.finfo(""float32"").min)\n        np.testing.assert_allclose(masked[1][2][i], np.finfo(""float32"").min)\n\n\ndef test_missing_embedding_fn():\n    batch_size = 6\n    beam_width = 4\n    cell = tf.keras.layers.LSTMCell(5)\n    decoder = beam_search_decoder.BeamSearchDecoder(cell, beam_width=beam_width)\n    initial_state = cell.get_initial_state(\n        batch_size=batch_size * beam_width, dtype=tf.float32\n    )\n    start_tokens = tf.ones([batch_size], dtype=tf.int32)\n    end_token = tf.constant(2, dtype=tf.int32)\n    with pytest.raises(ValueError):\n        decoder(None, start_tokens, end_token, initial_state)\n\n\ndef test_beam_step():\n    batch_size = 2\n    beam_width = 3\n    vocab_size = 5\n    end_token = 0\n    length_penalty_weight = 0.6\n    coverage_penalty_weight = 0.0\n\n    dummy_cell_state = tf.zeros([batch_size, beam_width])\n    beam_state = beam_search_decoder.BeamSearchDecoderState(\n        cell_state=dummy_cell_state,\n        log_probs=tf.nn.log_softmax(tf.ones([batch_size, beam_width])),\n        lengths=tf.constant(2, shape=[batch_size, beam_width], dtype=tf.int64),\n        finished=tf.zeros([batch_size, beam_width], dtype=tf.bool),\n        accumulated_attention_probs=(),\n    )\n\n    logits_ = np.full([batch_size, beam_width, vocab_size], 0.0001)\n    logits_[0, 0, 2] = 1.9\n    logits_[0, 0, 3] = 2.1\n    logits_[0, 1, 3] = 3.1\n    logits_[0, 1, 4] = 0.9\n    logits_[1, 0, 1] = 0.5\n    logits_[1, 1, 2] = 2.7\n    logits_[1, 2, 2] = 10.0\n    logits_[1, 2, 3] = 0.2\n    logits = tf.convert_to_tensor(logits_, dtype=tf.float32)\n    log_probs = tf.nn.log_softmax(logits)\n\n    outputs, next_beam_state = beam_search_decoder._beam_search_step(\n        time=2,\n        logits=logits,\n        next_cell_state=dummy_cell_state,\n        beam_state=beam_state,\n        batch_size=tf.convert_to_tensor(batch_size),\n        beam_width=beam_width,\n        end_token=end_token,\n        length_penalty_weight=length_penalty_weight,\n        coverage_penalty_weight=coverage_penalty_weight,\n    )\n\n    outputs_, next_state_, state_, log_probs_ = [\n        outputs,\n        next_beam_state,\n        beam_state,\n        log_probs,\n    ]\n\n    np.testing.assert_equal(\n        outputs_.predicted_ids.numpy(), np.asanyarray([[3, 3, 2], [2, 2, 1]])\n    )\n    np.testing.assert_equal(\n        outputs_.parent_ids.numpy(), np.asanyarray([[1, 0, 0], [2, 1, 0]])\n    )\n    np.testing.assert_equal(\n        next_state_.lengths.numpy(), np.asanyarray([[3, 3, 3], [3, 3, 3]])\n    )\n    np.testing.assert_equal(\n        next_state_.finished.numpy(),\n        np.asanyarray([[False, False, False], [False, False, False]]),\n    )\n\n    expected_log_probs = []\n    expected_log_probs.append(state_.log_probs[0].numpy())\n    expected_log_probs.append(state_.log_probs[1].numpy())\n    expected_log_probs[0][0] += log_probs_[0, 1, 3]\n    expected_log_probs[0][1] += log_probs_[0, 0, 3]\n    expected_log_probs[0][2] += log_probs_[0, 0, 2]\n    expected_log_probs[1][0] += log_probs_[1, 2, 2]\n    expected_log_probs[1][1] += log_probs_[1, 1, 2]\n    expected_log_probs[1][2] += log_probs_[1, 0, 1]\n    np.testing.assert_equal(\n        next_state_.log_probs.numpy(), np.asanyarray(expected_log_probs)\n    )\n\n\ndef test_step_with_eos():\n    batch_size = 2\n    beam_width = 3\n    vocab_size = 5\n    end_token = 0\n    length_penalty_weight = 0.6\n    coverage_penalty_weight = 0.0\n\n    dummy_cell_state = tf.zeros([batch_size, beam_width])\n    beam_state = beam_search_decoder.BeamSearchDecoderState(\n        cell_state=dummy_cell_state,\n        log_probs=tf.nn.log_softmax(tf.ones([batch_size, beam_width])),\n        lengths=tf.convert_to_tensor([[2, 1, 2], [2, 2, 1]], dtype=tf.int64),\n        finished=tf.convert_to_tensor(\n            [[False, True, False], [False, False, True]], dtype=tf.bool\n        ),\n        accumulated_attention_probs=(),\n    )\n\n    logits_ = np.full([batch_size, beam_width, vocab_size], 0.0001)\n    logits_[0, 0, 2] = 1.9\n    logits_[0, 0, 3] = 2.1\n    logits_[0, 1, 3] = 3.1\n    logits_[0, 1, 4] = 0.9\n    logits_[1, 0, 1] = 0.5\n    logits_[1, 1, 2] = 5.7  # why does this not work when it\'s 2.7?\n    logits_[1, 2, 2] = 1.0\n    logits_[1, 2, 3] = 0.2\n    logits = tf.convert_to_tensor(logits_, dtype=tf.float32)\n    log_probs = tf.nn.log_softmax(logits)\n\n    outputs, next_beam_state = beam_search_decoder._beam_search_step(\n        time=2,\n        logits=logits,\n        next_cell_state=dummy_cell_state,\n        beam_state=beam_state,\n        batch_size=tf.convert_to_tensor(batch_size),\n        beam_width=beam_width,\n        end_token=end_token,\n        length_penalty_weight=length_penalty_weight,\n        coverage_penalty_weight=coverage_penalty_weight,\n    )\n\n    outputs_, next_state_, state_, log_probs_ = [\n        outputs,\n        next_beam_state,\n        beam_state,\n        log_probs,\n    ]\n\n    np.testing.assert_equal(\n        outputs_.parent_ids.numpy(), np.asanyarray([[1, 0, 0], [1, 2, 0]])\n    )\n    np.testing.assert_equal(\n        outputs_.predicted_ids.numpy(), np.asanyarray([[0, 3, 2], [2, 0, 1]])\n    )\n    np.testing.assert_equal(\n        next_state_.lengths.numpy(), np.asanyarray([[1, 3, 3], [3, 1, 3]])\n    )\n    np.testing.assert_equal(\n        next_state_.finished.numpy(),\n        np.asanyarray([[True, False, False], [False, True, False]]),\n    )\n\n    expected_log_probs = []\n    expected_log_probs.append(state_.log_probs[0].numpy())\n    expected_log_probs.append(state_.log_probs[1].numpy())\n    expected_log_probs[0][1] += log_probs_[0, 0, 3]\n    expected_log_probs[0][2] += log_probs_[0, 0, 2]\n    expected_log_probs[1][0] += log_probs_[1, 1, 2]\n    expected_log_probs[1][2] += log_probs_[1, 0, 1]\n    np.testing.assert_equal(\n        next_state_.log_probs.numpy(), np.asanyarray(expected_log_probs)\n    )\n\n\ndef test_large_beam_step():\n    batch_size = 2\n    beam_width = 8\n    vocab_size = 5\n    end_token = 0\n    length_penalty_weight = 0.6\n    coverage_penalty_weight = 0.0\n\n    def get_probs():\n        """"""this simulates the initialize method in BeamSearchDecoder.""""""\n        log_prob_mask = tf.one_hot(\n            tf.zeros([batch_size], dtype=tf.int32),\n            depth=beam_width,\n            on_value=True,\n            off_value=False,\n            dtype=tf.bool,\n        )\n\n        log_prob_zeros = tf.zeros([batch_size, beam_width], dtype=tf.float32)\n        log_prob_neg_inf = tf.ones([batch_size, beam_width], dtype=tf.float32) * -np.Inf\n\n        log_probs = tf.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)\n        return log_probs\n\n    log_probs = get_probs()\n    dummy_cell_state = tf.zeros([batch_size, beam_width])\n\n    _finished = tf.one_hot(\n        tf.zeros([batch_size], dtype=tf.int32),\n        depth=beam_width,\n        on_value=False,\n        off_value=True,\n        dtype=tf.bool,\n    )\n    _lengths = np.zeros([batch_size, beam_width], dtype=np.int64)\n    _lengths[:, 0] = 2\n    _lengths = tf.constant(_lengths, dtype=tf.int64)\n\n    beam_state = beam_search_decoder.BeamSearchDecoderState(\n        cell_state=dummy_cell_state,\n        log_probs=log_probs,\n        lengths=_lengths,\n        finished=_finished,\n        accumulated_attention_probs=(),\n    )\n\n    logits_ = np.full([batch_size, beam_width, vocab_size], 0.0001)\n    logits_[0, 0, 2] = 1.9\n    logits_[0, 0, 3] = 2.1\n    logits_[0, 1, 3] = 3.1\n    logits_[0, 1, 4] = 0.9\n    logits_[1, 0, 1] = 0.5\n    logits_[1, 1, 2] = 2.7\n    logits_[1, 2, 2] = 10.0\n    logits_[1, 2, 3] = 0.2\n    logits = tf.constant(logits_, dtype=tf.float32)\n    log_probs = tf.nn.log_softmax(logits)\n\n    outputs, next_beam_state = beam_search_decoder._beam_search_step(\n        time=2,\n        logits=logits,\n        next_cell_state=dummy_cell_state,\n        beam_state=beam_state,\n        batch_size=tf.convert_to_tensor(batch_size),\n        beam_width=beam_width,\n        end_token=end_token,\n        length_penalty_weight=length_penalty_weight,\n        coverage_penalty_weight=coverage_penalty_weight,\n    )\n\n    outputs_, next_state_ = [outputs, next_beam_state]\n\n    assert outputs_.predicted_ids[0, 0] == 3\n    assert outputs_.predicted_ids[0, 1] == 2\n    assert outputs_.predicted_ids[1, 0] == 1\n    neg_inf = -np.Inf\n    np.testing.assert_equal(\n        next_state_.log_probs[:, -3:].numpy(),\n        np.asanyarray([[neg_inf, neg_inf, neg_inf], [neg_inf, neg_inf, neg_inf]]),\n    )\n    np.testing.assert_equal(\n        np.asanyarray(next_state_.log_probs[:, :-3] > neg_inf), True\n    )\n    np.testing.assert_equal(np.asanyarray(next_state_.lengths[:, :-3] > 0), True)\n    np.testing.assert_equal(\n        next_state_.lengths[:, -3:].numpy(), np.asanyarray([[0, 0, 0], [0, 0, 0]])\n    )\n\n\n@pytest.mark.parametrize(""with_alignment_history"", [True, False])\n@pytest.mark.parametrize(""has_attention"", [True, False])\n@pytest.mark.parametrize(""time_major"", [True, False])\n@pytest.mark.parametrize(\n    ""cell_class"", [tf.keras.layers.LSTMCell, tf.keras.layers.GRUCell]\n)\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_beam_search_decoder(\n    cell_class, time_major, has_attention, with_alignment_history\n):\n    encoder_sequence_length = np.array([3, 2, 3, 1, 1])\n    batch_size = 5\n    decoder_max_time = 4\n    input_depth = 7\n    cell_depth = 9\n    attention_depth = 6\n    vocab_size = 20\n    end_token = vocab_size - 1\n    start_token = 0\n    embedding_dim = 50\n    maximum_iterations = 3\n    output_layer = tf.keras.layers.Dense(vocab_size, use_bias=True, activation=None)\n    beam_width = 3\n    embedding = tf.random.normal([vocab_size, embedding_dim])\n    cell = cell_class(cell_depth)\n\n    if has_attention:\n        attention_mechanism = attention_wrapper.BahdanauAttention(\n            units=attention_depth,\n        )\n        cell = attention_wrapper.AttentionWrapper(\n            cell=cell,\n            attention_mechanism=attention_mechanism,\n            attention_layer_size=attention_depth,\n            alignment_history=with_alignment_history,\n        )\n        coverage_penalty_weight = 0.2\n    else:\n        coverage_penalty_weight = 0.0\n\n    bsd = beam_search_decoder.BeamSearchDecoder(\n        cell=cell,\n        beam_width=beam_width,\n        output_layer=output_layer,\n        length_penalty_weight=0.0,\n        coverage_penalty_weight=coverage_penalty_weight,\n        output_time_major=time_major,\n        maximum_iterations=maximum_iterations,\n    )\n\n    @tf.function(\n        input_signature=(\n            tf.TensorSpec([None, None, input_depth], dtype=tf.float32),\n            tf.TensorSpec([None], dtype=tf.int32),\n        )\n    )\n    def _beam_decode_from(memory, memory_sequence_length):\n        batch_size_tensor = tf.shape(memory)[0]\n\n        if has_attention:\n            tiled_memory = beam_search_decoder.tile_batch(memory, multiplier=beam_width)\n            tiled_memory_sequence_length = beam_search_decoder.tile_batch(\n                memory_sequence_length, multiplier=beam_width\n            )\n            attention_mechanism.setup_memory(\n                tiled_memory, memory_sequence_length=tiled_memory_sequence_length\n            )\n\n        cell_state = cell.get_initial_state(\n            batch_size=batch_size_tensor * beam_width, dtype=tf.float32\n        )\n\n        return bsd(\n            embedding,\n            start_tokens=tf.fill([batch_size_tensor], start_token),\n            end_token=end_token,\n            initial_state=cell_state,\n        )\n\n    memory = tf.random.normal([batch_size, decoder_max_time, input_depth])\n    memory_sequence_length = tf.constant(encoder_sequence_length, dtype=tf.int32)\n    final_outputs, final_state, final_sequence_lengths = _beam_decode_from(\n        memory, memory_sequence_length\n    )\n\n    def _t(shape):\n        if time_major:\n            return (shape[1], shape[0]) + shape[2:]\n        return shape\n\n    assert isinstance(final_outputs, beam_search_decoder.FinalBeamSearchDecoderOutput)\n    assert isinstance(final_state, beam_search_decoder.BeamSearchDecoderState)\n\n    beam_search_decoder_output = final_outputs.beam_search_decoder_output\n    max_sequence_length = np.max(final_sequence_lengths.numpy())\n    assert _t((batch_size, max_sequence_length, beam_width)) == tuple(\n        beam_search_decoder_output.scores.shape.as_list()\n    )\n    assert _t((batch_size, max_sequence_length, beam_width)) == tuple(\n        final_outputs.predicted_ids.shape.as_list()\n    )\n'"
tensorflow_addons/seq2seq/tests/beam_search_ops_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tfa.seq2seq.beam_search_ops.""""""\n\nimport itertools\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import gather_tree\n\n\ndef _transpose_batch_time(x):\n    return np.transpose(x, [1, 0, 2]).astype(np.int32)\n\n\ndef test_gather_tree_one():\n    # (max_time = 4, batch_size = 1, beams = 3)\n    end_token = 10\n    step_ids = _transpose_batch_time([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n    parent_ids = _transpose_batch_time(\n        [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]]\n    )\n    max_sequence_lengths = [3]\n    expected_result = _transpose_batch_time(\n        [[[2, 2, 2], [6, 5, 6], [7, 8, 9], [10, 10, 10]]]\n    )\n    beams = gather_tree(\n        step_ids=step_ids,\n        parent_ids=parent_ids,\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=end_token,\n    )\n    np.testing.assert_equal(expected_result, beams.numpy())\n\n\ndef test_bad_parent_values_on_cpu():\n    # (batch_size = 1, max_time = 4, beams = 3)\n    # bad parent in beam 1 time 1\n    end_token = 10\n    step_ids = _transpose_batch_time([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n    parent_ids = _transpose_batch_time(\n        [[[0, 0, 0], [0, -1, 1], [2, 1, 2], [-1, -1, -1]]]\n    )\n    max_sequence_lengths = [3]\n\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        _ = gather_tree(\n            step_ids=step_ids,\n            parent_ids=parent_ids,\n            max_sequence_lengths=max_sequence_lengths,\n            end_token=end_token,\n        )\n\n\n@pytest.mark.with_device([""gpu""])\ndef test_bad_parent_values_on_gpu():\n    # (max_time = 4, batch_size = 1, beams = 3)\n    # bad parent in beam 1 time 1; appears as a negative index at time 0\n    end_token = 10\n    step_ids = _transpose_batch_time([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n    parent_ids = _transpose_batch_time(\n        [[[0, 0, 0], [0, -1, 1], [2, 1, 2], [-1, -1, -1]]]\n    )\n    max_sequence_lengths = [3]\n    expected_result = _transpose_batch_time(\n        [[[2, -1, 2], [6, 5, 6], [7, 8, 9], [10, 10, 10]]]\n    )\n    beams = gather_tree(\n        step_ids=step_ids,\n        parent_ids=parent_ids,\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=end_token,\n    )\n    np.testing.assert_equal(expected_result, beams.numpy())\n\n\ndef test_gather_tree_batch():\n    batch_size = 10\n    beam_width = 15\n    max_time = 8\n    max_sequence_lengths = [0, 1, 2, 4, 7, 8, 9, 10, 11, 0]\n    end_token = 5\n\n    step_ids = np.random.randint(\n        0, high=end_token + 1, size=(max_time, batch_size, beam_width)\n    )\n    parent_ids = np.random.randint(\n        0, high=beam_width - 1, size=(max_time, batch_size, beam_width)\n    )\n\n    beams = gather_tree(\n        step_ids=step_ids.astype(np.int32),\n        parent_ids=parent_ids.astype(np.int32),\n        max_sequence_lengths=max_sequence_lengths,\n        end_token=end_token,\n    )\n    beams = beams.numpy()\n\n    assert (max_time, batch_size, beam_width) == beams.shape\n    for b in range(batch_size):\n        # Past max_sequence_lengths[b], we emit all end tokens.\n        b_value = beams[max_sequence_lengths[b] :, b, :]\n        np.testing.assert_allclose(b_value, end_token * np.ones_like(b_value))\n    for batch, beam in itertools.product(range(batch_size), range(beam_width)):\n        v = np.squeeze(beams[:, batch, beam])\n        if end_token in v:\n            found_bad = np.where(v == -1)[0]\n            assert 0 == len(found_bad)\n            found = np.where(v == end_token)[0]\n            found = found[0]  # First occurrence of end_token.\n            # If an end_token is found, everything before it should be a\n            # valid id and everything after it should be -1.\n            if found > 0:\n                np.testing.assert_equal(\n                    v[: found - 1] >= 0, np.ones_like(v[: found - 1], dtype=bool),\n                )\n            np.testing.assert_allclose(\n                v[found + 1 :], end_token * np.ones_like(v[found + 1 :])\n            )\n'"
tensorflow_addons/seq2seq/tests/decoder_test.py,10,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tfa.seq2seq.decoder.""""""\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import basic_decoder\nfrom tensorflow_addons.seq2seq import sampler as sampler_py\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(\n    ""maximum_iterations"", [None, 0, 1, tf.constant(1, dtype=tf.int32)]\n)\n@pytest.mark.parametrize(""time_major"", [True, False])\ndef test_dynamic_decode_rnn(time_major, maximum_iterations):\n\n    sequence_length = [3, 4, 3, 1, 0]\n    batch_size = 5\n    max_time = 8\n    input_depth = 7\n    cell_depth = 10\n    max_out = max(sequence_length)\n\n    if time_major:\n        inputs = np.random.randn(max_time, batch_size, input_depth).astype(np.float32)\n    else:\n        inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    input_t = tf.constant(inputs)\n    cell = tf.keras.layers.LSTMCell(cell_depth)\n    sampler = sampler_py.TrainingSampler(time_major=time_major)\n    my_decoder = basic_decoder.BasicDecoder(\n        cell=cell,\n        sampler=sampler,\n        output_time_major=time_major,\n        maximum_iterations=maximum_iterations,\n    )\n\n    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    (final_outputs, unused_final_state, final_sequence_length,) = my_decoder(\n        input_t, initial_state=initial_state, sequence_length=sequence_length\n    )\n\n    def _t(shape):\n        if time_major:\n            return (shape[1], shape[0]) + shape[2:]\n        return shape\n\n    assert (batch_size,) == tuple(final_sequence_length.shape.as_list())\n    # Mostly a smoke test\n    time_steps = max_out\n    expected_length = sequence_length\n    if maximum_iterations is not None:\n        time_steps = min(max_out, maximum_iterations)\n        expected_length = [min(x, maximum_iterations) for x in expected_length]\n    if maximum_iterations != 0:\n        assert (\n            _t((batch_size, time_steps, cell_depth)) == final_outputs.rnn_output.shape\n        )\n        assert _t((batch_size, time_steps)) == final_outputs.sample_id.shape\n    np.testing.assert_array_equal(expected_length, final_sequence_length)\n\n\n@pytest.mark.parametrize(""use_sequence_length"", [True, False])\ndef test_dynamic_decode_rnn_with_training_helper_matches_dynamic_rnn(\n    use_sequence_length,\n):\n    sequence_length = [3, 4, 3, 1, 0]\n    batch_size = 5\n    max_time = 8\n    input_depth = 7\n    cell_depth = 10\n    max_out = max(sequence_length)\n\n    inputs = np.random.randn(batch_size, max_time, input_depth).astype(np.float32)\n    inputs = tf.constant(inputs)\n\n    cell = tf.keras.layers.LSTMCell(cell_depth)\n    zero_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n    sampler = sampler_py.TrainingSampler()\n    my_decoder = basic_decoder.BasicDecoder(\n        cell=cell, sampler=sampler, impute_finished=use_sequence_length\n    )\n\n    (final_decoder_outputs, final_decoder_state, _,) = my_decoder(\n        inputs, initial_state=zero_state, sequence_length=sequence_length\n    )\n\n    rnn = tf.keras.layers.RNN(cell, return_sequences=True, return_state=True)\n    mask = (\n        tf.sequence_mask(sequence_length, maxlen=max_time)\n        if use_sequence_length\n        else None\n    )\n    outputs = rnn(inputs, mask=mask, initial_state=zero_state)\n    final_rnn_outputs = outputs[0]\n    final_rnn_state = outputs[1:]\n    if use_sequence_length:\n        final_rnn_outputs *= tf.cast(tf.expand_dims(mask, -1), final_rnn_outputs.dtype)\n\n    eval_result = {\n        ""final_decoder_outputs"": final_decoder_outputs,\n        ""final_decoder_state"": final_decoder_state,\n        ""final_rnn_outputs"": final_rnn_outputs,\n        ""final_rnn_state"": final_rnn_state,\n    }\n\n    # Decoder only runs out to max_out; ensure values are identical\n    # to dynamic_rnn, which also zeros out outputs and passes along\n    # state.\n    np.testing.assert_allclose(\n        eval_result[""final_decoder_outputs""].rnn_output,\n        eval_result[""final_rnn_outputs""][:, 0:max_out, :],\n    )\n    if use_sequence_length:\n        np.testing.assert_allclose(\n            eval_result[""final_decoder_state""], eval_result[""final_rnn_state""]\n        )\n'"
tensorflow_addons/seq2seq/tests/loss_test.py,19,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tf.addons.seq2seq.python.loss_ops.""""""\n\nimport pytest\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons.seq2seq import loss\n\n\ndef get_test_data():\n    batch_size = 2\n    sequence_length = 3\n    number_of_classes = 5\n    logits = [\n        tf.constant(i + 0.5, shape=[batch_size, number_of_classes])\n        for i in range(sequence_length)\n    ]\n    logits = tf.stack(logits, axis=1)\n    targets = [\n        tf.constant(i, tf.int32, shape=[batch_size]) for i in range(sequence_length)\n    ]\n    targets = tf.stack(targets, axis=1)\n\n    weights = [tf.constant(1.0, shape=[batch_size]) for _ in range(sequence_length)]\n    weights = tf.stack(weights, axis=1)\n    # expected_loss = sparse_softmax_cross_entropy_with_logits(targets,\n    # logits) where targets = [0, 1, 2],\n    # and logits = [[0.5] * 5, [1.5] * 5, [2.5] * 5]\n    expected_loss = 1.60944\n    return (\n        batch_size,\n        sequence_length,\n        number_of_classes,\n        logits,\n        targets,\n        weights,\n        expected_loss,\n    )\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""average_across_timesteps"", [True, False])\n@pytest.mark.parametrize(""average_across_batch"", [True, False])\n@pytest.mark.parametrize(""zero_weights"", [True, False])\ndef test_sequence_loss(average_across_timesteps, average_across_batch, zero_weights):\n    (\n        batch_size,\n        sequence_length,\n        _,\n        logits,\n        targets,\n        weights,\n        expected_loss,\n    ) = get_test_data()\n\n    if zero_weights:\n        weights = [tf.constant(0.0, shape=[batch_size]) for _ in range(sequence_length)]\n        weights = tf.stack(weights, axis=1)\n\n    computed = loss.sequence_loss(\n        logits,\n        targets,\n        weights,\n        average_across_timesteps=average_across_timesteps,\n        average_across_batch=average_across_batch,\n    )\n    computed = computed.numpy()\n    if average_across_timesteps and average_across_batch and zero_weights:\n        expected = 0.0\n    elif not average_across_timesteps and average_across_batch and zero_weights:\n        expected = np.zeros(sequence_length)\n    elif average_across_timesteps and not average_across_batch and zero_weights:\n        expected = np.zeros(batch_size)\n    elif not average_across_timesteps and not average_across_batch and zero_weights:\n        expected = np.zeros((batch_size, sequence_length))\n    elif average_across_timesteps and average_across_batch and not zero_weights:\n        expected = expected_loss\n    elif not average_across_timesteps and average_across_batch and not zero_weights:\n        expected = np.full(sequence_length, expected_loss)\n    elif average_across_timesteps and not average_across_batch and not zero_weights:\n        expected = np.full(batch_size, expected_loss)\n    else:\n        expected = np.full((batch_size, sequence_length), expected_loss)\n\n    np.testing.assert_allclose(computed, expected, rtol=1e-6, atol=1e-6)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\n@pytest.mark.parametrize(""average_across_timesteps"", [True, False])\n@pytest.mark.parametrize(""average_across_batch"", [True, False])\ndef test_sequence_loss_class(average_across_timesteps, average_across_batch):\n\n    (\n        batch_size,\n        sequence_length,\n        _,\n        logits,\n        targets,\n        weights,\n        expected_loss,\n    ) = get_test_data()\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=average_across_timesteps,\n        average_across_batch=average_across_batch,\n        sum_over_timesteps=False,\n        sum_over_batch=False,\n    )\n    average_loss_per_example = seq_loss(targets, logits, weights)\n    res = average_loss_per_example.numpy()\n    if average_across_timesteps and average_across_batch:\n        expected = expected_loss\n    elif not average_across_timesteps and average_across_batch:\n        expected = np.full(sequence_length, expected_loss)\n    elif average_across_timesteps and not average_across_batch:\n        expected = np.full(batch_size, expected_loss)\n    elif not average_across_timesteps and not average_across_batch:\n        expected = np.full((batch_size, sequence_length), expected_loss)\n\n    np.testing.assert_allclose(res, expected, atol=1e-6, rtol=1e-6)\n\n\ndef test_sum_reduction():\n    (\n        batch_size,\n        sequence_length,\n        _,\n        logits,\n        targets,\n        weights,\n        expected_loss,\n    ) = get_test_data()\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=True,\n        sum_over_batch=True,\n    )\n    average_loss_per_example = seq_loss(targets, logits, weights)\n    res = average_loss_per_example.numpy()\n    np.testing.assert_allclose(expected_loss, res, atol=1e-6, rtol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=False,\n        sum_over_batch=True,\n    )\n    average_loss_per_sequence = seq_loss(targets, logits, weights)\n    res = average_loss_per_sequence.numpy()\n    compare_per_sequence = np.full((sequence_length), expected_loss)\n    np.testing.assert_allclose(compare_per_sequence, res, atol=1e-6, rtol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=True,\n        sum_over_batch=False,\n    )\n    average_loss_per_batch = seq_loss(targets, logits, weights)\n    res = average_loss_per_batch.numpy()\n    compare_per_batch = np.full((batch_size), expected_loss)\n    np.testing.assert_allclose(compare_per_batch, res, atol=1e-6, rtol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=False,\n        sum_over_batch=False,\n    )\n    total_loss = seq_loss(targets, logits, weights)\n    res = total_loss.numpy()\n    compare_total = np.full((batch_size, sequence_length), expected_loss)\n    np.testing.assert_allclose(compare_total, res, atol=1e-6, rtol=1e-6)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_weighted_sum_reduction():\n    (\n        batch_size,\n        sequence_length,\n        _,\n        logits,\n        targets,\n        _,\n        expected_loss,\n    ) = get_test_data()\n    weights = [tf.constant(1.0, shape=[batch_size]) for _ in range(sequence_length)]\n    # Make the last element in the sequence to have zero weights.\n    weights[-1] = tf.constant(0.0, shape=[batch_size])\n    weights = tf.stack(weights, axis=1)\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=True,\n        sum_over_batch=True,\n    )\n    average_loss_per_example = seq_loss(targets, logits, weights)\n    res = average_loss_per_example.numpy()\n    np.testing.assert_allclose(expected_loss, res, rtol=1e-6, atol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=False,\n        sum_over_batch=True,\n    )\n    average_loss_per_sequence = seq_loss(targets, logits, weights)\n    res = average_loss_per_sequence.numpy()\n    compare_per_sequence = np.full(sequence_length, expected_loss)\n    # The last element in every sequence are zeros, which will be\n    # filtered.\n    compare_per_sequence[-1] = 0.0\n    np.testing.assert_allclose(compare_per_sequence, res, rtol=1e-6, atol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=True,\n        sum_over_batch=False,\n    )\n    average_loss_per_batch = seq_loss(targets, logits, weights)\n    res = average_loss_per_batch.numpy()\n    compare_per_batch = np.full(batch_size, expected_loss)\n    np.testing.assert_allclose(compare_per_batch, res, rtol=1e-6, atol=1e-6)\n\n    seq_loss = loss.SequenceLoss(\n        average_across_timesteps=False,\n        average_across_batch=False,\n        sum_over_timesteps=False,\n        sum_over_batch=False,\n    )\n    total_loss = seq_loss(targets, logits, weights)\n    res = total_loss.numpy()\n    compare_total = np.full((batch_size, sequence_length), expected_loss)\n    # The last element in every sequence are zeros, which will be\n    # filtered.\n    compare_total[:, -1] = 0\n    np.testing.assert_allclose(compare_total, res, rtol=1e-6, atol=1e-6)\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_ambiguous_order():\n    with pytest.raises(ValueError, match=""because of ambiguous order""):\n        _, _, _, logits, targets, weights, _ = get_test_data()\n        seq_loss = loss.SequenceLoss(\n            average_across_timesteps=False,\n            average_across_batch=True,\n            sum_over_timesteps=True,\n            sum_over_batch=False,\n        )\n        seq_loss(targets, logits, weights).numpy()\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_keras_compatibility():\n    """"""To test the compatibility of SequenceLoss with Keras\'s built-in\n    training loops, we create a fake model which always outputs a pre-\n    defined set of logits.\n\n    Then we check the calculated loss to be equal to the expected\n    loss. Note that since the fake model doesn\'t have any trainable\n    parameters, no matter how many steps we train it, it always\n    outputs the same loss value.\n    """"""\n    (\n        batch_size,\n        sequence_length,\n        number_of_classes,\n        logits,\n        targets,\n        weights,\n        expected_loss,\n    ) = get_test_data()\n    targets = tf.one_hot(targets, depth=number_of_classes)\n\n    def return_logits(x):\n        logits_single_row = logits[0, :, :]\n        logits_batch = tf.tile(\n            tf.expand_dims(logits_single_row, 0), [tf.shape(x)[0], 1, 1]\n        )\n        return logits_batch\n\n    inp = tf.keras.layers.Input(shape=(sequence_length,))\n    out = tf.keras.layers.Lambda(\n        return_logits, output_shape=(sequence_length, number_of_classes),\n    )(inp)\n    model = tf.keras.models.Model(inp, out)\n\n    loss_obj = loss.SequenceLoss()\n    model.compile(optimizer=""adam"", loss=loss_obj, sample_weight_mode=""temporal"")\n\n    # This is a fake input.\n    x = tf.ones(shape=(batch_size, sequence_length))\n\n    h = model.fit(\n        x, targets, sample_weight=weights, batch_size=batch_size, steps_per_epoch=1,\n    )\n\n    calculated_loss = h.history[""loss""][0]\n    np.testing.assert_allclose(calculated_loss, expected_loss, rtol=1e-6, atol=1e-6)\n'"
tensorflow_addons/seq2seq/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/testing/tests/__init__.py,0,b''
tensorflow_addons/testing/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/testing/tests/serialization_test.py,1,"b'import numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow.keras.metrics import MeanAbsoluteError, TrueNegatives, Metric\nfrom tensorflow_addons.testing.serialization import check_metric_serialization\n\n\ndef test_check_metric_serialization_mae():\n    check_metric_serialization(MeanAbsoluteError(), (2, 2), (2, 2))\n    check_metric_serialization(MeanAbsoluteError(name=""hello""), (2, 2), (2, 2))\n    check_metric_serialization(MeanAbsoluteError(), (2, 2, 2), (2, 2, 2))\n    check_metric_serialization(MeanAbsoluteError(), (2, 2, 2), (2, 2, 2), (2, 2, 1))\n\n\ndef get_random_booleans():\n    return np.random.uniform(0, 2, size=(2, 2))\n\n\ndef test_check_metric_serialization_true_negative():\n    check_metric_serialization(\n        TrueNegatives(0.8),\n        np.random.uniform(0, 2, size=(2, 2)).astype(np.bool),\n        np.random.uniform(0, 1, size=(2, 2)).astype(np.float32),\n    )\n\n\nclass MyDummyMetric(Metric):\n    def __init__(self, stuff, name):\n        super().__init__(name)\n        self.stuff = stuff\n\n    def update_state(self, y_true, y_pred, sample_weights):\n        pass\n\n    def get_config(self):\n        return super().get_config()\n\n    def result(self):\n        return 3\n\n\ndef test_missing_arg():\n    with pytest.raises(KeyError) as exception_info:\n        check_metric_serialization(MyDummyMetric(""dodo"", ""dada""), (2,), (2,))\n\n    assert ""stuff"" in str(exception_info.value)\n\n\nclass MyOtherDummyMetric(Metric):\n    def __init__(self, to_add, name=None, dtype=None):\n        super().__init__(name, dtype)\n        self.to_add = to_add\n        self.sum_of_y_pred = self.add_weight(name=""my_sum"", initializer=""zeros"")\n\n    def update_state(self, y_true, y_pred, sample_weights=None):\n        self.sum_of_y_pred.assign_add(tf.math.reduce_sum(y_pred) + self.to_add)\n\n    def get_config(self):\n        config = {""to_add"": self.to_add + 1}\n        config.update(super().get_config())\n        return config\n\n    def result(self):\n        return self.sum_of_y_pred\n\n\ndef test_wrong_serialization():\n    with pytest.raises(AssertionError):\n        check_metric_serialization(MyOtherDummyMetric(5), (2,), (2,))\n'"
tensorflow_addons/text/tests/__init__.py,0,b''
tensorflow_addons/text/tests/crf_test.py,64,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for CRF.""""""\n\nimport itertools\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow_addons import text\n\n\ndef calculate_sequence_score(inputs, transition_params, tag_indices, sequence_lengths):\n    expected_unary_score = sum(\n        inputs[i][tag_indices[i]] for i in range(sequence_lengths)\n    )\n    expected_binary_score = sum(\n        transition_params[tag_indices[i], tag_indices[i + 1]]\n        for i in range(sequence_lengths - 1)\n    )\n    return expected_unary_score + expected_binary_score\n\n\ndef test_crf_sequence_score():\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    # Test both the length-1 and regular cases.\n    sequence_lengths_list = [\n        np.array(3, dtype=np.int32),\n        np.array(1, dtype=np.int32),\n    ]\n    inputs_list = [\n        np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32),\n        np.array([[4, 5, -3]], dtype=np.float32),\n    ]\n    tag_indices_list = [\n        np.array([1, 2, 1, 0], dtype=np.int32),\n        np.array([1], dtype=np.int32),\n    ]\n    for sequence_lengths, inputs, tag_indices in zip(\n        sequence_lengths_list, inputs_list, tag_indices_list\n    ):\n        sequence_score = text.crf_sequence_score(\n            inputs=tf.expand_dims(inputs, 0),\n            tag_indices=tf.expand_dims(tag_indices, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            transition_params=tf.constant(transition_params),\n        )\n        sequence_score = tf.squeeze(sequence_score, [0])\n\n        expected_sequence_score = calculate_sequence_score(\n            inputs, transition_params, tag_indices, sequence_lengths\n        )\n        np.testing.assert_allclose(sequence_score, expected_sequence_score)\n\n\ndef test_crf_multi_tag_sequence_score():\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    # Test both the length-1 and regular cases.\n    sequence_lengths_list = [\n        np.array(3, dtype=np.int32),\n        np.array(1, dtype=np.int32),\n    ]\n    inputs_list = [\n        np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32),\n        np.array([[4, 5, -3]], dtype=np.float32),\n    ]\n    tag_bitmap_list = [\n        np.array(\n            [\n                [True, True, False],\n                [True, False, True],\n                [False, True, True],\n                [True, False, True],\n            ],\n            dtype=np.bool,\n        ),\n        np.array([[True, True, False]], dtype=np.bool),\n    ]\n    for sequence_lengths, inputs, tag_bitmap in zip(\n        sequence_lengths_list, inputs_list, tag_bitmap_list\n    ):\n        sequence_score = text.crf_multitag_sequence_score(\n            inputs=tf.expand_dims(inputs, 0),\n            tag_bitmap=tf.expand_dims(tag_bitmap, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            transition_params=tf.constant(transition_params),\n        )\n        sequence_score = tf.squeeze(sequence_score, [0])\n        all_indices_list = [\n            single_index_bitmap.nonzero()[0]\n            for single_index_bitmap in tag_bitmap[:sequence_lengths]\n        ]\n        expected_sequence_scores = [\n            calculate_sequence_score(\n                inputs, transition_params, indices, sequence_lengths\n            )\n            for indices in itertools.product(*all_indices_list)\n        ]\n        expected_log_sum_exp_sequence_scores = np.logaddexp.reduce(\n            expected_sequence_scores\n        )\n        np.testing.assert_allclose(sequence_score, expected_log_sum_exp_sequence_scores)\n\n\ndef test_crf_unary_score():\n    inputs = np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32)\n    for dtype in (np.int32, np.int64):\n        tag_indices = np.array([1, 2, 1, 0], dtype=dtype)\n        sequence_lengths = np.array(3, dtype=np.int32)\n        unary_score = text.crf_unary_score(\n            tag_indices=tf.expand_dims(tag_indices, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            inputs=tf.expand_dims(inputs, 0),\n        )\n        unary_score = tf.squeeze(unary_score, [0])\n        expected_unary_score = sum(\n            inputs[i][tag_indices[i]] for i in range(sequence_lengths)\n        )\n        np.testing.assert_allclose(unary_score, expected_unary_score)\n\n\ndef test_crf_binary_score():\n    tag_indices = np.array([1, 2, 1, 0], dtype=np.int32)\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    sequence_lengths = np.array(3, dtype=np.int32)\n    binary_score = text.crf_binary_score(\n        tag_indices=tf.expand_dims(tag_indices, 0),\n        sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n        transition_params=tf.constant(transition_params),\n    )\n    binary_score = tf.squeeze(binary_score, [0])\n    expected_binary_score = sum(\n        transition_params[tag_indices[i], tag_indices[i + 1]]\n        for i in range(sequence_lengths - 1)\n    )\n    np.testing.assert_allclose(binary_score, expected_binary_score)\n\n\ndef test_crf_log_norm():\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    # Test both the length-1 and regular cases.\n    sequence_lengths_list = [\n        np.array(3, dtype=np.int32),\n        np.array(1, dtype=np.int64),\n    ]\n    inputs_list = [\n        np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32),\n        np.array([[3, -1, 3]], dtype=np.float32),\n    ]\n    tag_indices_list = [\n        np.array([1, 2, 1, 0], dtype=np.int32),\n        np.array([2], dtype=np.int32),\n    ]\n\n    for sequence_lengths, inputs, tag_indices in zip(\n        sequence_lengths_list, inputs_list, tag_indices_list\n    ):\n        num_words = inputs.shape[0]\n        num_tags = inputs.shape[1]\n        all_sequence_scores = []\n\n        # Compare the dynamic program with brute force computation.\n        for tag_indices in itertools.product(range(num_tags), repeat=sequence_lengths):\n            tag_indices = list(tag_indices)\n            tag_indices.extend([0] * (num_words - sequence_lengths))\n            all_sequence_scores.append(\n                text.crf_sequence_score(\n                    inputs=tf.expand_dims(inputs, 0),\n                    tag_indices=tf.expand_dims(tag_indices, 0),\n                    sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n                    transition_params=tf.constant(transition_params),\n                )\n            )\n\n        brute_force_log_norm = tf.reduce_logsumexp(all_sequence_scores)\n        log_norm = text.crf_log_norm(\n            inputs=tf.expand_dims(inputs, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            transition_params=tf.constant(transition_params),\n        )\n        log_norm = tf.squeeze(log_norm, [0])\n\n        np.testing.assert_allclose(log_norm, brute_force_log_norm)\n\n\ndef test_crf_log_norm_zero_seq_length():\n    """"""Test `crf_log_norm` when `sequence_lengths` contains one or more\n    zeros.""""""\n    inputs = tf.constant(np.ones([2, 10, 5], dtype=np.float32))\n    transition_params = tf.constant(np.ones([5, 5], dtype=np.float32))\n    sequence_lengths = tf.constant(np.zeros([2], dtype=np.int32))\n    expected_log_norm = np.zeros([2], dtype=np.float32)\n    log_norm = text.crf_log_norm(inputs, sequence_lengths, transition_params)\n    np.testing.assert_allclose(log_norm, expected_log_norm)\n\n\ndef test_crf_log_likelihood():\n    inputs = np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32)\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    sequence_lengths = np.array(3, dtype=np.int32)\n\n    num_words = inputs.shape[0]\n    num_tags = inputs.shape[1]\n    all_sequence_log_likelihoods = []\n\n    # Make sure all probabilities sum to 1.\n    for tag_indices in itertools.product(range(num_tags), repeat=sequence_lengths):\n        tag_indices = list(tag_indices)\n        tag_indices.extend([0] * (num_words - sequence_lengths))\n        sequence_log_likelihood, _ = text.crf_log_likelihood(\n            inputs=tf.expand_dims(inputs, 0),\n            tag_indices=tf.expand_dims(tag_indices, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            transition_params=tf.constant(transition_params),\n        )\n        all_sequence_log_likelihoods.append(sequence_log_likelihood)\n    total_log_likelihood = tf.reduce_logsumexp(all_sequence_log_likelihoods)\n    np.testing.assert_allclose(total_log_likelihood, 0.0, 1e-6, 1e-6)\n\n    # check if `transition_params = None` raises an error\n    text.crf_log_likelihood(\n        inputs=tf.expand_dims(inputs, 0),\n        tag_indices=tf.expand_dims(tag_indices, 0),\n        sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n    )\n\n\ndef test_viterbi_decode():\n    inputs = np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32)\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    sequence_lengths = np.array(3, dtype=np.int32)\n    num_words = inputs.shape[0]\n    num_tags = inputs.shape[1]\n\n    all_sequence_scores = []\n    all_sequences = []\n\n    # Compare the dynamic program with brute force computation.\n    for tag_indices in itertools.product(range(num_tags), repeat=sequence_lengths):\n        tag_indices = list(tag_indices)\n        tag_indices.extend([0] * (num_words - sequence_lengths))\n        all_sequences.append(tag_indices)\n        sequence_score = text.crf_sequence_score(\n            inputs=tf.expand_dims(inputs, 0),\n            tag_indices=tf.expand_dims(tag_indices, 0),\n            sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n            transition_params=tf.constant(transition_params),\n        )\n        sequence_score = tf.squeeze(sequence_score, [0])\n        all_sequence_scores.append(sequence_score)\n\n    expected_max_sequence_index = np.argmax(all_sequence_scores)\n    expected_max_sequence = all_sequences[expected_max_sequence_index]\n    expected_max_score = all_sequence_scores[expected_max_sequence_index]\n\n    actual_max_sequence, actual_max_score = text.viterbi_decode(\n        inputs[:sequence_lengths], transition_params\n    )\n\n    np.testing.assert_allclose(actual_max_score, expected_max_score)\n    assert actual_max_sequence == expected_max_sequence[:sequence_lengths]\n\n\ndef test_crf_decode():\n    transition_params = np.array([[-3, 5, -2], [3, 4, 1], [1, 2, 1]], dtype=np.float32)\n    # Test both the length-1 and regular cases.\n    sequence_lengths_list = [\n        np.array(3, dtype=np.int32),\n        np.array(1, dtype=np.int64),\n    ]\n    inputs_list = [\n        np.array([[4, 5, -3], [3, -1, 3], [-1, 2, 1], [0, 0, 0]], dtype=np.float32),\n        np.array([[-1, 2, 1]], dtype=np.float32),\n    ]\n    tag_indices_list = [\n        np.array([1, 2, 1, 0], dtype=np.int32),\n        np.array([2], dtype=np.int32),\n    ]\n\n    for sequence_lengths, inputs, tag_indices in zip(\n        sequence_lengths_list, inputs_list, tag_indices_list\n    ):\n        num_words = inputs.shape[0]\n        num_tags = inputs.shape[1]\n\n        all_sequence_scores = []\n        all_sequences = []\n\n        # Compare the dynamic program with brute force computation.\n        for tag_indices in itertools.product(range(num_tags), repeat=sequence_lengths):\n            tag_indices = list(tag_indices)\n            tag_indices.extend([0] * (num_words - sequence_lengths))\n            all_sequences.append(tag_indices)\n            sequence_score = text.crf_sequence_score(\n                inputs=tf.expand_dims(inputs, 0),\n                tag_indices=tf.expand_dims(tag_indices, 0),\n                sequence_lengths=tf.expand_dims(sequence_lengths, 0),\n                transition_params=tf.constant(transition_params),\n            )\n            sequence_score = tf.squeeze(sequence_score, [0])\n            all_sequence_scores.append(sequence_score)\n\n        expected_max_sequence_index = np.argmax(all_sequence_scores)\n        expected_max_sequence = all_sequences[expected_max_sequence_index]\n        expected_max_score = all_sequence_scores[expected_max_sequence_index]\n\n        actual_max_sequence, actual_max_score = text.crf_decode(\n            tf.expand_dims(inputs, 0),\n            tf.constant(transition_params),\n            tf.expand_dims(sequence_lengths, 0),\n        )\n        actual_max_sequence = tf.squeeze(actual_max_sequence, [0])\n        actual_max_score = tf.squeeze(actual_max_score, [0])\n\n        np.testing.assert_allclose(actual_max_score, expected_max_score, 1e-6, 1e-6)\n        assert (\n            list(actual_max_sequence[:sequence_lengths])\n            == expected_max_sequence[:sequence_lengths]\n        )\n\n\ndef test_crf_decode_zero_seq_length():\n    """"""Test that crf_decode works when sequence_length contains one or more\n    zeros.""""""\n    inputs = tf.constant(np.ones([2, 10, 5], dtype=np.float32))\n    transition_params = tf.constant(np.ones([5, 5], dtype=np.float32))\n    sequence_lengths = tf.constant(np.zeros([2], dtype=np.int32))\n    tags, scores = text.crf_decode(inputs, transition_params, sequence_lengths)\n    assert len(tags.shape) == 2\n    assert len(scores.shape) == 1\n\n\ndef test_different_dtype():\n    inputs = np.ones([16, 20, 5], dtype=np.float32)\n    tags = tf.convert_to_tensor(np.ones([16, 20], dtype=np.int64))\n    seq_lens = np.ones([16], dtype=np.int64) * 20\n\n    loss, _ = text.crf_log_likelihood(\n        inputs=inputs, tag_indices=tags, sequence_lengths=seq_lens\n    )\n\n\ndef test_tf_function():\n    batch_size = 4\n    num_tags = 10\n    input_signature = (\n        tf.TensorSpec([None, None, num_tags]),\n        tf.TensorSpec([num_tags, num_tags]),\n        tf.TensorSpec([None], dtype=tf.int32),\n    )\n    crf_decode = tf.function(input_signature=input_signature)(text.crf_decode)\n    potentials = tf.random.uniform([batch_size, 1, num_tags])\n    transition_params = tf.random.uniform([num_tags, num_tags])\n    sequence_length = tf.ones([batch_size], dtype=tf.int32)\n    crf_decode(potentials, transition_params, sequence_length)\n'"
tensorflow_addons/text/tests/parse_time_op_test.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Parse time op tests.""""""\nimport platform\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons import text\n\nIS_WINDOWS = platform.system() == ""Windows""\n\npytestmark = pytest.mark.skipif(\n    IS_WINDOWS,\n    reason=""Doesn\'t work on Windows, see https://github.com/tensorflow/addons/issues/782"",\n)\n\n\ndef test_parse_time():\n    time_format = ""%Y-%m-%dT%H:%M:%E*S%Ez""\n    items = [\n        (""2019-05-17T23:56:09.05Z"", time_format, ""NANOSECOND"", 1558137369050000000),\n        (""2019-05-17T23:56:09.05Z"", time_format, ""MICROSECOND"", 1558137369050000),\n        (""2019-05-17T23:56:09.05Z"", time_format, ""MILLISECOND"", 1558137369050),\n        (""2019-05-17T23:56:09.05Z"", time_format, ""SECOND"", 1558137369),\n        (\n            [\n                ""2019-05-17T23:56:09.05Z"",\n                ""2019-05-20T11:22:33.44Z"",\n                ""2019-05-30T22:33:44.55Z"",\n            ],\n            time_format,\n            ""MILLISECOND"",\n            [1558137369050, 1558351353440, 1559255624550],\n        ),\n    ]\n    for time_string, time_format, output_unit, expected in items:\n        result = text.parse_time(\n            time_string=time_string, time_format=time_format, output_unit=output_unit,\n        )\n        np.testing.assert_equal(expected, result.numpy())\n\n\ndef test_invalid_output_unit():\n    errors = (ValueError, tf.errors.InvalidArgumentError)\n    with pytest.raises(errors):\n        text.parse_time(\n            time_string=""2019-05-17T23:56:09.05Z"",\n            time_format=""%Y-%m-%dT%H:%M:%E*S%Ez"",\n            output_unit=""INVALID"",\n        )\n\n\ndef test_invalid_time_format():\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        text.parse_time(\n            time_string=""2019-05-17T23:56:09.05Z"",\n            time_format=""INVALID"",\n            output_unit=""SECOND"",\n        )\n\n\ndef test_invalid_time_string():\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        text.parse_time(\n            time_string=""INVALID"",\n            time_format=""%Y-%m-%dT%H:%M:%E*S%Ez"",\n            output_unit=""SECOND"",\n        )\n'"
tensorflow_addons/text/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/text/tests/skip_gram_ops_test.py,43,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Skip-gram sampling ops tests.""""""\n\nimport csv\nimport os\nimport tempfile\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons import text\nfrom tensorflow_addons.text import skip_gram_ops\n\n\ndef _split_tokens_labels(output):\n    tokens = [x[0] for x in output]\n    labels = [x[1] for x in output]\n    return tokens, labels\n\n\ndef test_skip_gram_sample_skips_2():\n    """"""Tests skip-gram with min_skips = max_skips = 2.""""""\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown"", b""fox"", b""jumps""])\n    tokens, labels = text.skip_gram_sample(input_tensor, min_skips=2, max_skips=2)\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""quick""),\n            (b""the"", b""brown""),\n            (b""quick"", b""the""),\n            (b""quick"", b""brown""),\n            (b""quick"", b""fox""),\n            (b""brown"", b""the""),\n            (b""brown"", b""quick""),\n            (b""brown"", b""fox""),\n            (b""brown"", b""jumps""),\n            (b""fox"", b""quick""),\n            (b""fox"", b""brown""),\n            (b""fox"", b""jumps""),\n            (b""jumps"", b""brown""),\n            (b""jumps"", b""fox""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_emit_self():\n    """"""Tests skip-gram with emit_self_as_target = True.""""""\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown"", b""fox"", b""jumps""])\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=2, max_skips=2, emit_self_as_target=True\n    )\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""the""),\n            (b""the"", b""quick""),\n            (b""the"", b""brown""),\n            (b""quick"", b""the""),\n            (b""quick"", b""quick""),\n            (b""quick"", b""brown""),\n            (b""quick"", b""fox""),\n            (b""brown"", b""the""),\n            (b""brown"", b""quick""),\n            (b""brown"", b""brown""),\n            (b""brown"", b""fox""),\n            (b""brown"", b""jumps""),\n            (b""fox"", b""quick""),\n            (b""fox"", b""brown""),\n            (b""fox"", b""fox""),\n            (b""fox"", b""jumps""),\n            (b""jumps"", b""brown""),\n            (b""jumps"", b""fox""),\n            (b""jumps"", b""jumps""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\n@pytest.mark.usefixtures(""maybe_run_functions_eagerly"")\ndef test_skip_gram_sample_skips_0():\n    """"""Tests skip-gram with min_skips = max_skips = 0.""""""\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown""])\n\n    # If emit_self_as_target is False (default), output will be empty.\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=0, max_skips=0, emit_self_as_target=False\n    )\n    assert 0 == len(tokens)\n    assert 0 == len(labels)\n\n    # If emit_self_as_target is True, each token will be its own label.\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=0, max_skips=0, emit_self_as_target=True\n    )\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [(b""the"", b""the""), (b""quick"", b""quick""), (b""brown"", b""brown""),]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_skips_exceed_length():\n    """"""Tests skip-gram when min/max_skips exceed length of input.""""""\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown""])\n    tokens, labels = text.skip_gram_sample(input_tensor, min_skips=100, max_skips=100)\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""quick""),\n            (b""the"", b""brown""),\n            (b""quick"", b""the""),\n            (b""quick"", b""brown""),\n            (b""brown"", b""the""),\n            (b""brown"", b""quick""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_start_limit():\n    """"""Tests skip-gram over a limited portion of the input.""""""\n    input_tensor = tf.constant([b""foo"", b""the"", b""quick"", b""brown"", b""bar""])\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=1, max_skips=1, start=1, limit=3\n    )\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""quick""),\n            (b""quick"", b""the""),\n            (b""quick"", b""brown""),\n            (b""brown"", b""quick""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_limit_exceeds():\n    """"""Tests skip-gram when limit exceeds the length of the input.""""""\n    input_tensor = tf.constant([b""foo"", b""the"", b""quick"", b""brown""])\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=1, max_skips=1, start=1, limit=100\n    )\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""quick""),\n            (b""quick"", b""the""),\n            (b""quick"", b""brown""),\n            (b""brown"", b""quick""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_random_skips():\n    """"""Tests skip-gram with min_skips != max_skips, with random output.""""""\n    # The number of outputs is non-deterministic in this case, so set random\n    # seed to help ensure the outputs remain constant for this test case.\n    tf.random.set_seed(42)\n\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown"", b""fox"", b""jumps"", b""over""])\n    tokens, labels = text.skip_gram_sample(\n        input_tensor, min_skips=1, max_skips=2, seed=9\n    )\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""quick""),\n            (b""the"", b""brown""),\n            (b""quick"", b""the""),\n            (b""quick"", b""brown""),\n            (b""quick"", b""fox""),\n            (b""brown"", b""the""),\n            (b""brown"", b""quick""),\n            (b""brown"", b""fox""),\n            (b""brown"", b""jumps""),\n            (b""fox"", b""brown""),\n            (b""fox"", b""jumps""),\n            (b""jumps"", b""fox""),\n            (b""jumps"", b""over""),\n            (b""over"", b""fox""),\n            (b""over"", b""jumps""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_random_skips_default_seed():\n    """"""Tests outputs are still random when no op-level seed is\n    specified.""""""\n\n    # This is needed since tests set a graph-level seed by default. We want\n    # to explicitly avoid setting both graph-level seed and op-level seed,\n    # to simulate behavior under non-test settings when the user doesn\'t\n    # provide a seed to us. This results in random_seed.get_seed() returning\n    # None for both seeds, forcing the C++ kernel to execute its default\n    # seed logic.\n    tf.random.set_seed(None)\n\n    # Uses an input tensor with 10 words, with possible skip ranges in\n    # [1, 5]. Thus, the probability that two random samplings would result\n    # in the same outputs is 1/5^10 ~ 1e-7 (aka the probability of this test\n    # being flaky).\n    input_tensor = tf.constant([str(x) for x in range(10)])\n\n    # Do not provide an op-level seed here!\n    tokens_1, labels_1 = text.skip_gram_sample(input_tensor, min_skips=1, max_skips=5)\n    tokens_2, labels_2 = text.skip_gram_sample(input_tensor, min_skips=1, max_skips=5)\n\n    if len(tokens_1) == len(tokens_2):\n        assert list(tokens_1) != list(tokens_2)\n    if len(labels_1) == len(labels_2):\n        assert list(labels_1) != list(labels_2)\n\n\ndef test_skip_gram_sample_non_string_input():\n    """"""Tests skip-gram with non-string input.""""""\n    input_tensor = tf.constant([1, 2, 3], dtype=tf.dtypes.int16)\n    tokens, labels = text.skip_gram_sample(input_tensor, min_skips=1, max_skips=1)\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [(1, 2), (2, 1), (2, 3), (3, 2),]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_errors_v1():\n    """"""Tests various errors raised by skip_gram_sample().""""""\n    # input_tensor must be of rank 1.\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        invalid_tensor = tf.constant([[b""the""], [b""quick""], [b""brown""]])\n        text.skip_gram_sample(invalid_tensor)\n\n\ndef test_skip_gram_sample_errors():\n    """"""Tests various errors raised by skip_gram_sample().""""""\n    input_tensor = tf.constant([b""the"", b""quick"", b""brown""])\n\n    invalid_skips = (\n        # min_skips and max_skips must be >= 0.\n        (-1, 2),\n        (1, -2),\n        # min_skips must be <= max_skips.\n        (2, 1),\n    )\n    for min_skips, max_skips in invalid_skips:\n        with pytest.raises(tf.errors.InvalidArgumentError):\n            text.skip_gram_sample(\n                input_tensor, min_skips=min_skips, max_skips=max_skips\n            )\n\n    # Eager tensor must be rank 1\n    with pytest.raises(tf.errors.InvalidArgumentError):\n        invalid_tensor = tf.constant([[b""the""], [b""quick""], [b""brown""]])\n        text.skip_gram_sample(invalid_tensor)\n\n    # vocab_freq_table must be provided if vocab_min_count,\n    # vocab_subsampling, or corpus_size is specified.\n    dummy_input = tf.constant([""""])\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(dummy_input, vocab_freq_table=None, vocab_min_count=1)\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(\n            dummy_input, vocab_freq_table=None, vocab_subsampling=1e-5\n        )\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(dummy_input, vocab_freq_table=None, corpus_size=100)\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(\n            dummy_input, vocab_freq_table=None, vocab_subsampling=1e-5, corpus_size=100,\n        )\n\n    # vocab_subsampling and corpus_size must both be present or absent.\n    dummy_table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer([b""foo""], [10]), -1\n    )\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(\n            dummy_input,\n            vocab_freq_table=dummy_table,\n            vocab_subsampling=None,\n            corpus_size=100,\n        )\n    with pytest.raises(ValueError):\n        text.skip_gram_sample(\n            dummy_input,\n            vocab_freq_table=dummy_table,\n            vocab_subsampling=1e-5,\n            corpus_size=None,\n        )\n\n\ndef test_filter_input_filter_vocab():\n    """"""Tests input filtering based on vocab frequency table and\n    thresholds.""""""\n    input_tensor = tf.constant([b""the"", b""answer"", b""to"", b""life"", b""and"", b""universe""])\n    keys = tf.constant([b""and"", b""life"", b""the"", b""to"", b""universe""])\n    values = tf.constant([0, 1, 2, 3, 4], tf.dtypes.int64)\n    vocab_freq_table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys, values), -1\n    )\n\n    # No vocab_freq_table specified - output should be the same as input\n    no_table_output = skip_gram_ops._filter_input(\n        input_tensor=input_tensor,\n        vocab_freq_table=None,\n        vocab_min_count=None,\n        vocab_subsampling=None,\n        corpus_size=None,\n        seed=None,\n    )\n    np.testing.assert_equal(input_tensor.numpy(), np.asanyarray(no_table_output))\n\n    # vocab_freq_table specified, but no vocab_min_count - output should\n    # have filtered out tokens not in the table (b""answer"").\n    table_output = skip_gram_ops._filter_input(\n        input_tensor=input_tensor,\n        vocab_freq_table=vocab_freq_table,\n        vocab_min_count=None,\n        vocab_subsampling=None,\n        corpus_size=None,\n        seed=None,\n    )\n    np.testing.assert_equal(\n        np.asanyarray([b""the"", b""to"", b""life"", b""and"", b""universe""]),\n        table_output.numpy(),\n    )\n\n    # vocab_freq_table and vocab_min_count specified - output should have\n    # filtered out tokens whose frequencies are below the threshold\n    # (b""and"": 0, b""life"": 1).\n    threshold_output = skip_gram_ops._filter_input(\n        input_tensor=input_tensor,\n        vocab_freq_table=vocab_freq_table,\n        vocab_min_count=2,\n        vocab_subsampling=None,\n        corpus_size=None,\n        seed=None,\n    )\n    np.testing.assert_equal(\n        np.asanyarray([b""the"", b""to"", b""universe""]), threshold_output.numpy()\n    )\n\n\ndef test_filter_input_subsample_vocab():\n    """"""Tests input filtering based on vocab subsampling.""""""\n    # The outputs are non-deterministic, so set random seed to help ensure\n    # that the outputs remain constant for testing.\n    tf.random.set_seed(42)\n\n    input_tensor = tf.constant(\n        [\n            # keep_prob = (sqrt(30/(0.05*100)) + 1) * (0.05*100/30) = 0.57.\n            b""the"",\n            b""answer"",  # Not in vocab. (Always discarded)\n            b""to"",  # keep_prob = 0.75.\n            b""life"",  # keep_prob > 1. (Always kept)\n            b""and"",  # keep_prob = 0.48.\n            b""universe"",  # Below vocab threshold of 3. (Always discarded)\n        ]\n    )\n    keys = tf.constant([b""and"", b""life"", b""the"", b""to"", b""universe""])\n    values = tf.constant([40, 8, 30, 20, 2], tf.dtypes.int64)\n    vocab_freq_table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys, values), -1\n    )\n\n    output = skip_gram_ops._filter_input(\n        input_tensor=input_tensor,\n        vocab_freq_table=vocab_freq_table,\n        vocab_min_count=3,\n        vocab_subsampling=0.05,\n        corpus_size=tf.math.reduce_sum(values),\n        seed=9,\n    )\n    np.testing.assert_equal(\n        np.asanyarray([b""the"", b""to"", b""life"", b""and""]), output.numpy()\n    )\n\n\ndef test_skip_gram_sample_with_text_vocab_filter_vocab():\n    """"""Tests skip-gram sampling with text vocab and freq threshold\n    filtering.""""""\n    input_tensor = tf.constant(\n        [\n            b""the"",\n            b""answer"",  # Will be filtered before candidate generation.\n            b""to"",\n            b""life"",\n            b""and"",\n            b""universe"",  # Will be filtered before candidate generation.\n        ]\n    )\n\n    # b""answer"" is not in vocab file, and b""universe""\'s frequency is below\n    # threshold of 3.\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        vocab_freq_file = _make_text_vocab_freq_file(tmp_dir)\n\n        tokens, labels = text.skip_gram_sample_with_text_vocab(\n            input_tensor=input_tensor,\n            vocab_freq_file=vocab_freq_file,\n            vocab_token_index=0,\n            vocab_freq_index=1,\n            vocab_min_count=3,\n            min_skips=1,\n            max_skips=1,\n        )\n\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [\n            (b""the"", b""to""),\n            (b""to"", b""the""),\n            (b""to"", b""life""),\n            (b""life"", b""to""),\n            (b""life"", b""and""),\n            (b""and"", b""life""),\n        ]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef _text_vocab_subsample_vocab_helper(\n    vocab_freq_file, vocab_min_count, vocab_freq_dtype, corpus_size=None\n):\n    # The outputs are non-deterministic, so set random seed to help ensure\n    # that the outputs remain constant for testing.\n    tf.random.set_seed(42)\n\n    input_tensor = tf.constant(\n        [\n            # keep_prob = (sqrt(30/(0.05*100)) + 1) * (0.05*100/30) = 0.57.\n            b""the"",\n            b""answer"",  # Not in vocab. (Always discarded)\n            b""to"",  # keep_prob = 0.75.\n            b""life"",  # keep_prob > 1. (Always kept)\n            b""and"",  # keep_prob = 0.48.\n            b""universe"",  # Below vocab threshold of 3. (Always discarded)\n        ]\n    )\n    # keep_prob calculated from vocab file with relative frequencies of:\n    # and: 40\n    # life: 8\n    # the: 30\n    # to: 20\n    # universe: 2\n\n    tokens, labels = text.skip_gram_sample_with_text_vocab(\n        input_tensor=input_tensor,\n        vocab_freq_file=vocab_freq_file,\n        vocab_token_index=0,\n        vocab_freq_index=1,\n        vocab_freq_dtype=tf.dtypes.float64,\n        vocab_min_count=vocab_min_count,\n        vocab_subsampling=0.05,\n        corpus_size=corpus_size,\n        min_skips=1,\n        max_skips=1,\n        seed=123,\n    )\n\n    expected_tokens, expected_labels = _split_tokens_labels(\n        [(b""the"", b""to""), (b""to"", b""the""), (b""to"", b""life""), (b""life"", b""to""),]\n    )\n    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())\n    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())\n\n\ndef test_skip_gram_sample_with_text_vocab_subsample_vocab():\n    """"""Tests skip-gram sampling with text vocab and vocab subsampling.""""""\n    # Vocab file frequencies\n    # and: 40\n    # life: 8\n    # the: 30\n    # to: 20\n    # universe: 2\n    #\n    # corpus_size for the above vocab is 40+8+30+20+2 = 100.\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        text_vocab_freq_file = _make_text_vocab_freq_file(tmp_dir)\n        _skip_gram_sample_with_text_vocab_subsample_vocab(text_vocab_freq_file)\n\n\ndef _skip_gram_sample_with_text_vocab_subsample_vocab(text_vocab_freq_file):\n    _text_vocab_subsample_vocab_helper(\n        vocab_freq_file=text_vocab_freq_file,\n        vocab_min_count=3,\n        vocab_freq_dtype=tf.dtypes.int64,\n    )\n    _text_vocab_subsample_vocab_helper(\n        vocab_freq_file=text_vocab_freq_file,\n        vocab_min_count=3,\n        vocab_freq_dtype=tf.dtypes.int64,\n        corpus_size=100,\n    )\n\n    # The user-supplied corpus_size should not be less than the sum of all\n    # the frequency counts of vocab_freq_file, which is 100.\n    with pytest.raises(ValueError):\n        _text_vocab_subsample_vocab_helper(\n            vocab_freq_file=text_vocab_freq_file,\n            vocab_min_count=3,\n            vocab_freq_dtype=tf.dtypes.int64,\n            corpus_size=99,\n        )\n\n\ndef test_skip_gram_sample_with_text_vocab_subsample_vocab_float():\n    """"""Tests skip-gram sampling with text vocab and subsampling with\n    floats.""""""\n    # Vocab file frequencies\n    # and: 0.4\n    # life: 0.08\n    # the: 0.3\n    # to: 0.2\n    # universe: 0.02\n    #\n    # corpus_size for the above vocab is 0.4+0.08+0.3+0.2+0.02 = 1.\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        text_vocab_float_file = _make_text_vocab_float_file(tmp_dir)\n        _skip_gram_sample_with_text_vocab_subsample_vocab_float(text_vocab_float_file)\n\n\ndef _skip_gram_sample_with_text_vocab_subsample_vocab_float(text_vocab_float_file):\n    _text_vocab_subsample_vocab_helper(\n        vocab_freq_file=text_vocab_float_file,\n        vocab_min_count=0.03,\n        vocab_freq_dtype=tf.dtypes.float32,\n    )\n    _text_vocab_subsample_vocab_helper(\n        vocab_freq_file=text_vocab_float_file,\n        vocab_min_count=0.03,\n        vocab_freq_dtype=tf.dtypes.float32,\n        corpus_size=1.0,\n    )\n\n    # The user-supplied corpus_size should not be less than the sum of all\n    # the frequency counts of vocab_freq_file, which is 1.\n    with pytest.raises(ValueError):\n        _text_vocab_subsample_vocab_helper(\n            vocab_freq_file=text_vocab_float_file,\n            vocab_min_count=0.03,\n            vocab_freq_dtype=tf.dtypes.float32,\n            corpus_size=0.99,\n        )\n\n\ndef test_skip_gram_sample_with_text_vocab_errors():\n    """"""Tests various errors raised by\n    skip_gram_sample_with_text_vocab().""""""\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        vocab_freq_file = _make_text_vocab_freq_file(tmp_dir)\n        _skip_gram_sample_with_text_vocab_errors(vocab_freq_file)\n\n\ndef _skip_gram_sample_with_text_vocab_errors(vocab_freq_file):\n    dummy_input = tf.constant([""""])\n    invalid_indices = (\n        # vocab_token_index can\'t be negative.\n        (-1, 0),\n        # vocab_freq_index can\'t be negative.\n        (0, -1),\n        # vocab_token_index can\'t be equal to vocab_freq_index.\n        (0, 0),\n        (1, 1),\n        # vocab_freq_file only has two columns.\n        (0, 2),\n        (2, 0),\n    )\n\n    for vocab_token_index, vocab_freq_index in invalid_indices:\n        with pytest.raises(ValueError):\n            text.skip_gram_sample_with_text_vocab(\n                input_tensor=dummy_input,\n                vocab_freq_file=vocab_freq_file,\n                vocab_token_index=vocab_token_index,\n                vocab_freq_index=vocab_freq_index,\n            )\n\n\ndef _make_text_vocab_freq_file(tmp_dir):\n    filepath = os.path.join(tmp_dir, ""vocab_freq.txt"")\n    with open(filepath, ""w"") as f:\n        writer = csv.writer(f)\n        writer.writerows(\n            [[""and"", 40], [""life"", 8], [""the"", 30], [""to"", 20], [""universe"", 2],]\n        )\n    return filepath\n\n\ndef _make_text_vocab_float_file(tmp_dir):\n    filepath = os.path.join(tmp_dir, ""vocab_freq_float.txt"")\n    with open(filepath, ""w"") as f:\n        writer = csv.writer(f)\n        writer.writerows(\n            [\n                [""and"", 0.4],\n                [""life"", 0.08],\n                [""the"", 0.3],\n                [""to"", 0.2],\n                [""universe"", 0.02],\n            ]\n        )\n    return filepath\n'"
tensorflow_addons/utils/tests/__init__.py,0,b''
tensorflow_addons/utils/tests/keras_utils_test.py,3,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Keras utils.""""""\n\nimport sys\n\nimport pytest\nimport tensorflow as tf\n\nfrom tensorflow_addons.utils import keras_utils\n\n\ndef test_normalize_data_format():\n    assert keras_utils.normalize_data_format(""Channels_Last"") == ""channels_last""\n    assert keras_utils.normalize_data_format(""CHANNELS_FIRST"") == ""channels_first""\n\n    with pytest.raises(ValueError, match=""The `data_format` argument must be one of""):\n        keras_utils.normalize_data_format(""invalid"")\n\n\ndef test_normalize_tuple():\n    assert (2, 2, 2) == keras_utils.normalize_tuple(2, n=3, name=""strides"")\n    assert (2, 1, 2) == keras_utils.normalize_tuple((2, 1, 2), n=3, name=""strides"")\n\n    with pytest.raises(ValueError):\n        keras_utils.normalize_tuple((2, 1), n=3, name=""strides"")\n\n    with pytest.raises(TypeError):\n        keras_utils.normalize_tuple(None, n=3, name=""strides"")\n\n\ndef test_standard_cell():\n    keras_utils.assert_like_rnncell(""cell"", tf.keras.layers.LSTMCell(10))\n\n\ndef test_non_cell():\n    with pytest.raises(TypeError):\n        keras_utils.assert_like_rnncell(""cell"", tf.keras.layers.Dense(10))\n\n\ndef test_custom_cell():\n    class CustomCell(tf.keras.layers.AbstractRNNCell):\n        @property\n        def output_size(self):\n            raise ValueError(""assert_like_rnncell should not run code"")\n\n    keras_utils.assert_like_rnncell(""cell"", CustomCell())\n\n\nif __name__ == ""__main__"":\n    sys.exit(pytest.main([__file__]))\n'"
tensorflow_addons/utils/tests/run_all_test.py,0,"b'from pathlib import Path\nimport sys\n\nimport pytest\n\nif __name__ == ""__main__"":\n    dirname = Path(__file__).absolute().parent\n    sys.exit(pytest.main([str(dirname)]))\n'"
tensorflow_addons/utils/tests/test_utils_test.py,10,"b'import random\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow_addons.utils import test_utils\n\n\ndef test_seed_is_set():\n    assert random.randint(0, 10000) == 6311\n    assert np.random.randint(0, 10000) == 2732\n    assert tf.random.uniform([], 0, 10000, dtype=tf.int64).numpy() == 9457\n\n\n@pytest.mark.with_device([""cpu"", ""gpu"", tf.distribute.MirroredStrategy])\ndef test_all_scopes(device):\n    assert isinstance(device, str) or isinstance(device, tf.distribute.Strategy)\n\n\ndef train_small_model():\n    model_input = tf.keras.layers.Input((3,))\n    model_output = tf.keras.layers.Dense(4)(model_input)\n    model = tf.keras.Model(model_input, model_output)\n    model.compile(loss=""mse"")\n\n    x = np.random.uniform(size=(5, 3))\n    y = np.random.uniform(size=(5, 4))\n    model.fit(x, y, epochs=1)\n\n\n@pytest.mark.with_device([tf.distribute.MirroredStrategy])\ndef test_distributed_strategy(device):\n    assert isinstance(device, tf.distribute.Strategy)\n    train_small_model()\n\n\n@pytest.mark.with_device([""no_device""])\n@pytest.mark.needs_gpu\ndef test_custom_device_placement():\n    with tf.device(test_utils.gpus_for_testing()[0]):\n        train_small_model()\n\n    strategy = tf.distribute.MirroredStrategy(test_utils.gpus_for_testing())\n    with strategy.scope():\n        train_small_model()\n'"
