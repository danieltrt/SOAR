file_path,api_count,code
build_docs.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Generate docs for TF-Agents.\n\n# How to run\n\n```\npython build_docs.py --output_dir=/path/to/output\n```\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom absl import app\nfrom absl import flags\n\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nimport tf_agents\n# pylint: disable=unused-import\nfrom tf_agents import agents\nfrom tf_agents import distributions\nfrom tf_agents import drivers\nfrom tf_agents import environments\nfrom tf_agents import metrics\nfrom tf_agents import networks\nfrom tf_agents import policies\nfrom tf_agents import replay_buffers\nfrom tf_agents import specs\nfrom tf_agents import trajectories\nfrom tf_agents import utils\n# pylint: enable=unused-import\n\nflags.DEFINE_string(\'output_dir\', \'/tmp/agents_api/\',\n                    \'The path to output the files to\')\n\nflags.DEFINE_string(\'code_url_prefix\',\n                    \'https://github.com/tensorflow/agents/blob/master/\',\n                    \'The url prefix for links to code.\')\n\nflags.DEFINE_bool(\'search_hints\', True,\n                  \'Include metadata search hints in the generated files\')\n\nflags.DEFINE_string(\'site_path\', \'agents/api_docs/python\',\n                    \'Path prefix in the _toc.yaml\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  doc_generator = generate_lib.DocGenerator(\n      root_title=\'TF-Agents\',\n      py_modules=[(\'tf_agents\', tf_agents)],\n      base_dir=os.path.dirname(tf_agents.__file__),\n      code_url_prefix=FLAGS.code_url_prefix,\n      search_hints=FLAGS.search_hints,\n      site_path=FLAGS.site_path,\n      private_map={},\n      callbacks=[public_api.local_definitions_filter])\n\n  sys.exit(doc_generator.build(output_dir=FLAGS.output_dir))\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
setup.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Install tf_agents.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport datetime\nimport fnmatch\nimport io\nimport os\nimport subprocess\nimport sys\nimport unittest\n\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommandBase\nfrom setuptools.dist import Distribution\n\n\nclass StderrWrapper(io.IOBase):\n\n  def write(self, *args, **kwargs):\n    return sys.stderr.write(*args, **kwargs)\n\n  def writeln(self, *args, **kwargs):\n    if args or kwargs:\n      sys.stderr.write(*args, **kwargs)\n    sys.stderr.write(\'\\n\')\n\n\nclass TestLoader(unittest.TestLoader):\n\n  def __init__(self, blacklist):\n    super(TestLoader, self).__init__()\n    self._blacklist = blacklist\n\n  def _match_path(self, path, full_path, pattern):\n    if not fnmatch.fnmatch(path, pattern):\n      return False\n    module_name = full_path.replace(\'/\', \'.\').rstrip(\'.py\')\n    if any(module_name.endswith(x) for x in self._blacklist):\n      return False\n    return True\n\n\ndef load_test_list(filename):\n  testcases = [\n      x.rstrip() for x in open(filename, \'r\').readlines()\n      if x]\n  # Remove comments and blanks after comments are removed.\n  testcases = [x.partition(\'#\')[0].strip() for x in testcases]\n  return [x for x in testcases if x]\n\n\nclass Test(TestCommandBase):\n\n  def run_tests(self):\n    # Import absl inside run, where dependencies have been loaded already.\n    from absl import app  # pylint: disable=g-import-not-at-top\n\n    def main(_):\n      # pybullet imports multiprocessing in their setup.py, which causes an\n      # issue when we import multiprocessing.pool.dummy down the line because\n      # the PYTHONPATH has changed.\n      for module in [\'multiprocessing\', \'multiprocessing.pool\',\n                     \'multiprocessing.dummy\', \'multiprocessing.pool.dummy\']:\n        if module in sys.modules:\n          del sys.modules[module]\n      # Reimport multiprocessing to avoid spurious error printouts. See\n      # https://bugs.python.org/issue15881.\n      import multiprocessing as _  # pylint: disable=g-import-not-at-top\n\n      run_separately = load_test_list(\'test_individually.txt\')\n      broken_tests = load_test_list(\'broken_tests.txt\')\n\n      test_loader = TestLoader(blacklist=run_separately + broken_tests)\n      test_suite = test_loader.discover(\'tf_agents\', pattern=\'*_test.py\')\n      stderr = StderrWrapper()\n      result = unittest.TextTestResult(stderr, descriptions=True, verbosity=2)\n      test_suite.run(result)\n\n      external_test_failures = []\n\n      for test in run_separately:\n        filename = \'tf_agents/%s.py\' % test.replace(\'.\', \'/\')\n        try:\n          subprocess.check_call([sys.executable, filename])\n        except subprocess.CalledProcessError as e:\n          external_test_failures.append(e)\n\n      result.printErrors()\n\n      for failure in external_test_failures:\n        stderr.writeln(str(failure))\n\n      final_output = (\n          \'Tests run: {} grouped and {} external.  \'.format(\n              result.testsRun, len(run_separately)) +\n          \'Errors: {}  Failures: {}  External failures: {}.\'.format(\n              len(result.errors),\n              len(result.failures),\n              len(external_test_failures)))\n\n      header = \'=\' * len(final_output)\n      stderr.writeln(header)\n      stderr.writeln(final_output)\n      stderr.writeln(header)\n\n      if result.wasSuccessful() and not external_test_failures:\n        return 0\n      else:\n        return 1\n\n    # Run inside absl.app.run to ensure flags parsing is done.\n    from tf_agents.system import system_multiprocessing as multiprocessing  # pylint: disable=g-import-not-at-top\n    return multiprocessing.handle_test_main(lambda: app.run(main))\n\n\nfrom tf_agents.version import __dev_version__  # pylint: disable=g-import-not-at-top\nfrom tf_agents.version import __rel_version__  # pylint: disable=g-import-not-at-top\n\nREQUIRED_PACKAGES = [\n    \'absl-py >= 0.6.1\',\n    \'cloudpickle == 1.3\',  # TODO(b/155109696): Unpin cloudpickle version.\n    \'gin-config >= 0.3.0\',\n    \'numpy >= 1.13.3\',\n    \'six >= 1.10.0\',\n    \'protobuf >= 3.11.3\',\n    \'wrapt >= 1.11.1\',\n    # tensorflow-probability added below\n]\n\n\nTEST_REQUIRED_PACKAGES = [\n    \'atari_py == 0.1.7\',\n    \'gym == 0.12.5\',\n    \'opencv-python >= 3.4.1.15\',\n    \'pybullet\',\n    \'scipy == 1.1.0\',\n]\n\nREQUIRED_TFP_VERSION = \'0.9.0\'\n\nif \'--release\' in sys.argv:\n  release = True\n  sys.argv.remove(\'--release\')\n  version = __rel_version__\nelse:\n  # Build a nightly package by default.\n  release = False\n  version = __dev_version__\n  version += datetime.datetime.now().strftime(\'%Y%m%d\')\n\nif release:\n  project_name = \'tf-agents\'\n  tfp_package_name = \'tensorflow-probability>={}\'.format(REQUIRED_TFP_VERSION)\nelse:\n  # Nightly releases use date-based versioning of the form\n  # \'0.0.1.dev20180305\'\n  project_name = \'tf-agents-nightly\'\n\n  try:\n    import tensorflow as tf  # pylint: disable=g-import-not-at-top\n  except:\n    raise ValueError(\'Tensorflow must be installed before installing TFAgents.\')\n\n  # Force tensorflow_probability at 0.8.0 for TF 1.x compatibility.\n  if tf.__version__.startswith(\'1\'):\n    tfp_package_name = \'tensorflow-probability==0.8.0\'\n  else:\n    tfp_package_name = \'tfp-nightly\'\n\nREQUIRED_PACKAGES.append(tfp_package_name)\n\nif sys.version_info.major == 2:\n  # mock comes with unittest.mock for python3, need to install for\n  # python2\n  REQUIRED_PACKAGES.append(\'mock >= 2.0.0\')\n\n\nclass BinaryDistribution(Distribution):\n  """"""This class is needed in order to create OS specific wheels.""""""\n\n  def has_ext_modules(self):\n    return False\n\nhere = os.path.abspath(os.path.dirname(__file__))\nwith codecs.open(os.path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n  long_description = f.read()\n\nsetup(\n    name=project_name,\n    version=version,\n    description=\'TF-Agents: A Reinforcement Learning Library for TensorFlow\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    author=\'Google LLC\',\n    author_email=\'no-reply@google.com\',\n    url=\'http://github.com/tensorflow/agents\',\n    license=\'Apache 2.0\',\n    packages=find_packages(),\n    install_requires=REQUIRED_PACKAGES,\n    tests_require=TEST_REQUIRED_PACKAGES,\n    extras_require={\'tests\': TEST_REQUIRED_PACKAGES},\n    # Supports Python 3 only.\n    python_requires=\'>=3\',\n    # Add in any packaged data.\n    zip_safe=False,\n    distclass=BinaryDistribution,\n    cmdclass={\n        \'test\': Test,\n    },\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n    keywords=\'tensorflow agents reinforcement learning machine learning\',\n)\n'"
tf_agents/__init__.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TF Agents.""""""\n\n# We need to put some imports inside a function call below, and the function\n# call needs to come before the *actual* imports that populate the\n# tf_agents namespace. Hence, we disable this lint check throughout\n# the file.\n#\n# pylint: disable=g-import-not-at-top\n\n\n# Ensure TensorFlow is importable and its version is sufficiently recent. This\n# needs to happen before anything else, since the imports below will try to\n# import tensorflow, too.\ndef _ensure_tf_install():  # pylint: disable=g-statement-before-imports\n  """"""Attempt to import tensorflow, and ensure its version is sufficient.\n\n  Raises:\n    ImportError: if either tensorflow is not importable or its version is\n    inadequate.\n  """"""\n  try:\n    import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n  except ImportError:\n    # Print more informative error message, then reraise.\n    print(""\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not ""\n          ""installed by default when you install TF Agents. This is so that ""\n          ""users can decide whether to install the GPU-enabled TensorFlow ""\n          ""package. To use TF Agents, please install the most recent version ""\n          ""of TensorFlow, by following instructions at ""\n          ""https://tensorflow.org/install.\\n\\n"")\n    raise\n\n  import distutils.version\n\n  #\n  # Update this whenever we need to depend on a newer TensorFlow release.\n  #\n  required_tensorflow_version = ""1.14""\n\n  version = tf.version.VERSION\n  if (distutils.version.LooseVersion(version) <\n      distutils.version.LooseVersion(required_tensorflow_version)):\n    raise ImportError(\n        ""This version of TF Agents requires TensorFlow ""\n        ""version >= {required}; Detected an installation of version {present}. ""\n        ""Please upgrade TensorFlow to proceed."".format(\n            required=required_tensorflow_version,\n            present=version))\n\n\n_ensure_tf_install()\n\nimport sys as _sys\nfrom tf_agents.version import __version__\n\n# Cleanup symbols to avoid polluting namespace.\nfor symbol in [""_ensure_tf_install"", ""_sys""]:\n  delattr(_sys.modules[__name__], symbol)\n'"
tf_agents/version.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Define TF Agents version information.""""""\n\n# We follow Semantic Versioning (https://semver.org/)\n_MAJOR_VERSION = \'0\'\n_MINOR_VERSION = \'6\'\n_PATCH_VERSION = \'0\'\n\n# When building releases, we can update this value on the release branch to\n# reflect the current release candidate (\'rc0\', \'rc1\') or, finally, the official\n# stable release (indicated by `_REL_SUFFIX = \'\'`). Outside the context of a\n# release branch, the current version is by default assumed to be a\n# \'development\' version, labeled \'dev\'.\n_DEV_SUFFIX = \'dev\'\n_REL_SUFFIX = \'rc0\'\n\n# Example, \'0.4.0rc0\'\n__version__ = \'.\'.join([\n    _MAJOR_VERSION,\n    _MINOR_VERSION,\n    _PATCH_VERSION,\n])\n__dev_version__ = \'{}.{}\'.format(__version__, _DEV_SUFFIX)\n__rel_version__ = \'{}{}\'.format(__version__, _REL_SUFFIX)\n'"
tools/test_colabs.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests colabs using Jupyter notebook.""""""\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom nbconvert.preprocessors import CellExecutionError\nfrom nbconvert.preprocessors import ExecutePreprocessor\nimport nbformat\n\nflags.DEFINE_string(\'output_dir\', \'/tmp/notebook_tests\',\n                    \'Full path for executed notebooks and artifacts.\')\nflags.DEFINE_boolean(\'debug\', True,\n                     \'Debug logging if true. Otherwise info only.\')\nflags.DEFINE_boolean(\'override_pip_install_agents\', True,\n                     \'If true a replace is done to prevent notebooks from \'\n                     \'installing tf-agents (often tf-agents-nightly)\')\nFLAGS = flags.FLAGS\n\n\ndef execute_test(file_path, result_path):\n  """"""Executes a single notebook.\n\n  Args:\n    file_path: Path to the notebook to execute.\n    result_path: Path to store the resulting notebook.\n\n  Returns:\n    bool: True if the notebook does not have any errors, False otherwise.\n\n  Raises:\n    Exception if an unexpected error occurs executing the notebook.\n  """"""\n  try:\n    with open(file_path, \'r\') as f:\n      filedata = f.read()\n      if FLAGS.override_pip_install_agents:\n        filedata = filedata.replace(\'pip install tf-agents\', \'pip --version\')\n      nb = nbformat.reads(filedata, as_version=4)\n\n      ep = ExecutePreprocessor(timeout=3600, kernel_name=\'python3\')\n      try:\n        ep.preprocess(nb, {\'metadata\': {\'path\': FLAGS.output_dir}})\n      except CellExecutionError as cex:\n        logging.error(\'ERROR executing:%s\', file_path)\n        logging.error(cex)\n        return False\n    with open(result_path, \'w\', encoding=\'utf-8\') as fo:\n      nbformat.write(nb, fo)\n    return True\n  except Exception as e:  # pylint: disable=W0703\n    logging.error(\'Unexpected ERROR: in %s\', file_path)\n    logging.error(e)\n\n\ndef run():\n  """"""Runs all notebooks and reports results.""""""\n  os.makedirs(FLAGS.output_dir)\n  colab_path = \'./docs/tutorials/\'\n  _, _, filenames = next(os.walk(colab_path))\n\n  passed = []\n  failed = []\n  filenames.sort()\n  for filename in filenames:\n    logging.info(\'Testing %s ...\', filename)\n    if \'ipynb\' not in filename:\n      logging.debug(\'Skipping non-notebook file:%s\', filename)\n      continue\n    if \'7_SAC_minitaur_tutorial.ipynb\' in filename:\n      logging.info(\'Skipping 7_SAC_minitaur_tutorial.ipynb. \'\n                   \'It takes 8 hours to run.\')\n      continue\n    file_path = os.path.join(colab_path, filename)\n    result_path = os.path.join(FLAGS.output_dir, \'executed_\' + filename)\n    if execute_test(file_path, result_path):\n      passed.append(filename)\n    else:\n      failed.append(filename)\n\n  logging.info(\'\\n\\n################# Report #################\')\n  logging.info(\'%d passed, %d failed\', len(passed), len(failed))\n  for p_result in passed:\n    logging.info(\'%s OK\', p_result)\n  for f_result in failed:\n    logging.info(\'%s FAILED\', f_result)\n\n  if failed:\n    sys.exit(1)\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  if FLAGS.debug:\n    logging.set_verbosity(logging.DEBUG)\n  run()\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
docs/tutorials/colab_kernel_init.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Initialization code for colab test.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport glob\nimport logging\nimport os\nimport time\n\n\ndef WaitForFilePath(path_pattern, timeout_sec):\n  start = time.time()\n  result = []\n  while not result:\n    if time.time() - start > timeout_sec:\n      return result\n    result = glob.glob(path_pattern)\n    time.sleep(0.1)\n  return result\n\n\ndef SetDisplayFromWebTest():\n  """"""Set up display from web test.\n\n  Colab test sets up display using xvfb for front end web test suite. We just\n  ensure that DISPLAY environment variable is properly set for colab kernel\n  (backend) which can be used for open gym environment rendering.\n  """"""\n\n  res = WaitForFilePath(""/tmp/.X11-unix"", 60)\n  assert res\n\n  pattern = ""/tmp/.X11-unix/X*""\n  res = WaitForFilePath(pattern, 60)\n  assert res\n\n  # If we find ""/tmp/.X11-unix/X1"", then we will set DISPLAY to be "":1"".\n  display = "":"" + res[0][len(pattern)-1:]\n  os.environ[""DISPLAY""] = display\n  logging.info(""Set DISPLAY=%s"", display)\n\n\nSetDisplayFromWebTest()\n'"
tf_agents/agents/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all agents.""""""\nfrom tf_agents.agents import tf_agent\n# TODO(b/130564501): Do not import classes directly, only expose modules.\nfrom tf_agents.agents.behavioral_cloning.behavioral_cloning_agent import BehavioralCloningAgent\nfrom tf_agents.agents.categorical_dqn.categorical_dqn_agent import CategoricalDqnAgent\nfrom tf_agents.agents.ddpg.ddpg_agent import DdpgAgent\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\nfrom tf_agents.agents.ppo.ppo_agent import PPOAgent\nfrom tf_agents.agents.reinforce.reinforce_agent import ReinforceAgent\nfrom tf_agents.agents.sac.sac_agent import SacAgent\nfrom tf_agents.agents.td3.td3_agent import Td3Agent\n'"
tf_agents/agents/tf_agent.py,19,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TensorFlow RL Agent API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nimport collections\nfrom typing import Dict, Optional, Text\n\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nLossInfo = collections.namedtuple(""LossInfo"", (""loss"", ""extra""))\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass TFAgent(tf.Module):\n  """"""Abstract base class for TF-based RL and Bandits agents.\n\n  The agent serves the following purposes:\n\n  * Training by reading minibatches of `experience`, and updating some set\n    of network weights (using the `train` method).\n\n  * Exposing `policy` objects which can be used to interact with an environment:\n    either to explore and collect new training data, or to maximize reward\n    in the given task.\n\n  The agents\' main training methods and properties are:\n\n  * `initialize`: Perform any self-initialization before training.\n\n  * `train`: This method reads minibatch experience from a replay buffer or\n    logs on disk, and updates some internal networks.\n\n  * `preprocess_sequence`: Some algorithms need to perform sequence\n    preprocessing on logs containing ""full episode"" or ""long subset"" sequences,\n    to create intermediate items that can then be used by `train`, even if\n    `train` does not see the full sequences.  In many cases this is just\n    the identity: it passes experience through untouched.  This function\n    is typically passed to the argument\n\n    `ReplayBuffer.as_dataset(..., sequence_preprocess_fn=...)`\n\n  * `training_data_spec`: Property that describes the structure expected of\n    the `experience` argument passed to `train`.\n\n  * `train_sequence_length`: Property that describes the **second** dimension\n    of all tensors in the `experience` argument passed `train`.  All tensors\n    passed to train must have the shape `[batch_size, sequence_length, ...]`,\n    and some Agents require this to be a fixed value.  For example, in regular\n    `DQN`, this second `sequence_length` dimension must be equal to `2` in all\n    `experience`.  In contrast, `n-step DQN` will have this equal to `n + 1` and\n    `DQN` agents constructed with `RNN` networks will have this equal to `None`,\n    meaning any length sequences are allowed.\n\n    This value may be `None`, to mean minibatches containing subsequences of any\n    length are allowed (so long as they\'re all the same length).  This is\n    typically the case with agents constructed with `RNN` networks.\n\n    This value is typically passed as a ReplayBuffer\'s\n    `as_dataset(..., num_steps=...)` argument.\n\n  * `train_argspec`: Property that contains a dict describing other arguments\n    that must be passed as `kwargs` to `train` (typically empty).\n\n  * `collect_data_spec`: Property that describes the structure expected of\n    experience collected by `agent.collect_policy`.  This is typically\n    identical to `training_data_spec`, but may be different if\n    `preprocess_sequence` method is not the identity.  In this case,\n    `preprocess_sequence` is expected to read sequences matching\n    `collect_data_spec` and emit sequences matching `training_data_spec`.\n\n  The agent exposes `TFPolicy` objects for interacting with environments:\n\n  * `policy`: Property that returns a policy meant for ""exploiting"" the\n    environment to its best ability.  This tends to mean the ""production"" policy\n    that doesn\'t collect additional info for training.  Works best when\n    the agent is fully trained.\n\n    TODO(b/154870654): Not all agents are properly exporting properly greedy\n    ""production"" policies yet.  We have to clean this up.  In particular,\n    we have to update PPO and SAC\'s `policy` objects.\n\n  * `collect_policy`: Property that returns a policy meant for ""exploring""\n    the environment to collect more data for training.  This tends to mean\n    a policy involves some level of randomized behavior and additional info\n    logging.\n\n  * `time_step_spec`: Property describing the observation and reward signatures\n    of the environment this agent\'s policies operate in.\n\n  * `action_spec`: Property describing the action signatures of the environment\n    this agent\'s policies operate in.\n\n\n  **NOTE**: For API consistency, subclasses are not allowed to override public\n  methods of `TFAgent` class. Instead, they may implement the protected methods\n  including `_initialize`, `_train`, and `_preprocess_sequence`. This\n  public-calls-private convention allowed this base class to do things like\n  properly add `spec` and shape checks, which provide users an easier experience\n  when debugging their environments and networks.\n\n  For researchers, and those developing new Agents and Policies, both the\n  `TFAgent` and `TFPolicy` base class constructors also accept a\n  `validate_args` parameter.  If `False`, this disables all spec structure,\n  dtype, and shape checks in the public methods of these classes.  It\n  allows algorithm developers to iterate and try different input and output\n  structures without worrying about overly restrictive requirements like\n  experience being a `Trajectory`, or input and output states being in a\n  certain format.  However, *disabling argument validation* can make it very\n  hard to identify structural input or algorithmic errors; and should not\n  be done for final, or production-ready, Agents.  In addition to having\n  implementations that may disagree with specs, this mean that the resulting\n  Agent will no longer interact well with other parts of TF-Agents.  Examples\n  include impedance mismatches with Actor/Learner APIs, replay buffers, and\n  the model export functionality in `PolicySaver`.\n  """"""\n\n  # TODO(b/127327645) Remove this attribute.\n  # This attribute allows subclasses to back out of automatic tf.function\n  # attribute inside TF1 (for autodeps).\n  _enable_functions = True\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      policy: tf_policy.TFPolicy,\n      collect_policy: tf_policy.TFPolicy,\n      train_sequence_length: Optional[int],\n      num_outer_dims: int = 2,\n      training_data_spec: Optional[types.NestedTensorSpec] = None,\n      train_argspec: Optional[Dict[Text, types.NestedTensorSpec]] = None,\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      enable_summaries: bool = True,\n      train_step_counter: Optional[tf.Variable] = None,\n      validate_args: bool = True):\n    """"""Meant to be called by subclass constructors.\n\n    Args:\n      time_step_spec: A nest of tf.TypeSpec representing the time_steps.\n        Provided by the user.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n        Provided by the user.\n      policy: An instance of `tf_policy.TFPolicy` representing the\n        Agent\'s current policy.\n      collect_policy: An instance of `tf_policy.TFPolicy` representing the\n        Agent\'s current data collection policy (used to set `self.step_spec`).\n      train_sequence_length: A python integer or `None`, signifying the number\n        of time steps required from tensors in `experience` as passed to\n        `train()`.  All tensors in `experience` will be shaped `[B, T, ...]` but\n        for certain agents, `T` should be fixed.  For example, DQN requires\n        transitions in the form of 2 time steps, so for a non-RNN DQN Agent, set\n        this value to 2.  For agents that don\'t care, or which can handle `T`\n        unknown at graph build time (i.e. most RNN-based agents), set this\n        argument to `None`.\n      num_outer_dims: The number of outer dimensions for the agent. Must be\n        either 1 or 2. If 2, training will require both a batch_size and time\n        dimension on every Tensor; if 1, training will require only a batch_size\n        outer dimension.\n      training_data_spec: A nest of TensorSpec specifying the structure of data\n        the train() function expects. If None, defaults to the trajectory_spec\n        of the collect_policy.\n      train_argspec: (Optional) Describes additional supported arguments\n        to the `train` call.  This must be a `dict` mapping strings to nests\n        of specs.  Overriding the `experience` arg is also supported.\n\n        Some algorithms require additional arguments to the `train()` call, and\n        while TF-Agents encourages most of these to be provided in the\n        `policy_info` / `info` field of `experience`, sometimes the extra\n        information doesn\'t fit well, i.e., when it doesn\'t come from the\n        policy.\n\n        **NOTE** kwargs will not have their outer dimensions validated.\n        In particular, `train_sequence_length` is ignored for these inputs,\n        and they may have any, or inconsistent, batch/time dimensions; only\n        their inner shape dimensions are checked against `train_argspec`.\n\n        Below is an example:\n\n        ```python\n        class MyAgent(TFAgent):\n          def __init__(self, counterfactual_training, ...):\n             collect_policy = ...\n             train_argspec = None\n             if counterfactual_training:\n               train_argspec = dict(\n                  counterfactual=collect_policy.trajectory_spec)\n             super(...).__init__(\n               ...\n               train_argspec=train_argspec)\n\n        my_agent = MyAgent(...)\n\n        for ...:\n          experience, counterfactual = next(experience_and_counterfactual_iter)\n          loss_info = my_agent.train(experience, counterfactual=counterfactual)\n        ```\n      debug_summaries: A bool; if true, subclasses should gather debug\n        summaries.\n      summarize_grads_and_vars: A bool; if true, subclasses should additionally\n        collect gradient and variable summaries.\n      enable_summaries: A bool; if false, subclasses should not gather any\n        summaries (debug or otherwise); subclasses should gate *all* summaries\n        using either `summaries_enabled`, `debug_summaries`, or\n        `summarize_grads_and_vars` properties.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      validate_args: Python bool.  Whether to verify inputs to, and outputs of,\n        functions like `train` and `preprocess_sequence` against spec\n        structures, dtypes, and shapes.\n\n        Research code may prefer to set this value to `False` to allow iterating\n        on input and output structures without being hamstrung by overly\n        rigid checking (at the cost of harder-to-debug errors).\n\n        See also `TFPolicy.validate_args`.\n\n    Raises:\n      TypeError: If `validate_args is True` and `train_argspec` is not a `dict`.\n      ValueError: If `validate_args is True` and `train_argspec` has the keys\n        `experience` or `weights`.\n      TypeError: If `validate_args is True` and any leaf nodes in\n        `train_argspec` values are not subclasses of `tf.TypeSpec`.\n      ValueError: If `validate_args is True` and `time_step_spec` is not an\n        instance of `ts.TimeStep`.\n      ValueError: If `num_outer_dims` is not in `[1, 2]`.\n    """"""\n    if validate_args:\n      def _each_isinstance(spec, spec_types):\n        """"""Checks if each element of `spec` is instance of `spec_types`.""""""\n        return all([isinstance(s, spec_types) for s in tf.nest.flatten(spec)])\n\n      if not _each_isinstance(time_step_spec, tf.TypeSpec):\n        raise TypeError(\n            ""time_step_spec has to contain TypeSpec (TensorSpec, ""\n            ""SparseTensorSpec, etc) objects, but received: {}""\n            .format(time_step_spec))\n\n      if not _each_isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n        raise TypeError(\n            ""action_spec has to contain BoundedTensorSpec objects, but received: ""\n            ""{}"".format(action_spec))\n\n    common.check_tf1_allowed()\n    common.tf_agents_gauge.get_cell(""TFAgent"").set(True)\n    common.assert_members_are_not_overridden(base_cls=TFAgent, instance=self)\n    if not isinstance(time_step_spec, ts.TimeStep):\n      raise TypeError(\n          ""The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.""\n          .format(type(time_step_spec)))\n\n    if num_outer_dims not in [1, 2]:\n      raise ValueError(""num_outer_dims must be in [1, 2]."")\n\n    self._time_step_spec = time_step_spec\n    self._action_spec = action_spec\n    self._policy = policy\n    self._collect_policy = collect_policy\n    self._train_sequence_length = train_sequence_length\n    self._num_outer_dims = num_outer_dims\n    self._debug_summaries = debug_summaries\n    self._summarize_grads_and_vars = summarize_grads_and_vars\n    self._enable_summaries = enable_summaries\n    self._training_data_spec = training_data_spec\n    self._validate_args = validate_args\n    if train_argspec is None:\n      train_argspec = {}\n    elif validate_args:\n      if not isinstance(train_argspec, dict):\n        raise TypeError(""train_argspec must be a dict, but saw: {}""\n                        .format(train_argspec))\n      if ""weights"" in train_argspec or ""experience"" in train_argspec:\n        raise ValueError(""train_argspec must not override \'weights\' or ""\n                         ""\'experience\' keys, but saw: {}"".format(train_argspec))\n      if not all(isinstance(x, tf.TypeSpec)\n                 for x in tf.nest.flatten(train_argspec)):\n        raise TypeError(""train_argspec contains non-TensorSpec objects: {}""\n                        .format(train_argspec))\n    train_argspec = dict(train_argspec)  # Create a local copy.\n    self._train_argspec = train_argspec\n    if train_step_counter is None:\n      train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n    self._train_step_counter = train_step_counter\n    self._train_fn = common.function_in_tf1()(self._train)\n    self._initialize_fn = common.function_in_tf1()(self._initialize)\n    self._preprocess_sequence_fn = common.function_in_tf1()(\n        self._preprocess_sequence)\n\n  def initialize(self) -> Optional[tf.Operation]:\n    """"""Initializes the agent.\n\n    Returns:\n      An operation that can be used to initialize the agent.\n\n    Raises:\n      RuntimeError: If the class was not initialized properly (`super.__init__`\n        was not called).\n    """"""\n    if self._enable_functions and getattr(self, ""_initialize_fn"", None) is None:\n      raise RuntimeError(\n          ""Cannot find _initialize_fn.  Did %s.__init__ call super?""\n          % type(self).__name__)\n    if self._enable_functions:\n      return self._initialize_fn()\n    else:\n      return self._initialize()\n\n  def preprocess_sequence(self,\n                          experience: types.NestedTensor) -> types.NestedTensor:\n    """"""Defines preprocess_sequence function to be fed into replay buffers.\n\n    This defines how we preprocess the collected data before training.\n    Defaults to pass through for most agents.\n    Structure of `experience` must match that of `self.collect_data_spec`.\n\n    Args:\n      experience: a `Trajectory` shaped [batch, time, ...] or [time, ...] which\n        represents the collected experience data.\n\n    Returns:\n      A post processed `Trajectory` with the same shape as the input.\n\n    Raises:\n      TypeError: If experience does not match `self.collect_data_spec` structure\n        types.\n    """"""\n    if self._validate_args:\n      nest_utils.assert_same_structure(\n          experience,\n          self.collect_data_spec,\n          message=""experience and collect_data_spec structures do not match"")\n\n    if self._enable_functions:\n      preprocessed_sequence = self._preprocess_sequence_fn(experience)\n    else:\n      preprocessed_sequence = self._preprocess_sequence(experience)\n\n    if self._validate_args:\n      nest_utils.assert_same_structure(\n          preprocessed_sequence,\n          self.training_data_spec,\n          message=(""output of preprocess_sequence and training_data_spec ""\n                   ""structures do not match""))\n\n    return preprocessed_sequence\n\n  def _check_trajectory_dimensions(self, experience):\n    """"""Checks the given Trajectory for batch and time outer dimensions.""""""\n    if not nest_utils.is_batched_nested_tensors(\n        experience, self.training_data_spec,\n        num_outer_dims=self._num_outer_dims,\n        allow_extra_fields=True,\n    ):\n      debug_str_1 = tf.nest.map_structure(lambda tp: tp.shape, experience)\n      debug_str_2 = tf.nest.map_structure(lambda spec: spec.shape,\n                                          self.training_data_spec)\n\n      if self._num_outer_dims == 2:\n        raise ValueError(\n            ""All of the Tensors in `experience` must have two outer ""\n            ""dimensions: batch size and time. Specifically, tensors should be ""\n            ""shaped as [B x T x ...].\\n""\n            ""Full shapes of experience tensors:\\n{}.\\n""\n            ""Full expected shapes (minus outer dimensions):\\n{}."".format(\n                debug_str_1, debug_str_2))\n      else:\n        # self._num_outer_dims must be 1.\n        raise ValueError(\n            ""All of the Tensors in `experience` must have a single outer ""\n            ""batch_size dimension. If you also want to include an outer time ""\n            ""dimension, set num_outer_dims=2 when initializing your agent.\\n""\n            ""Full shapes of experience tensors:\\n{}.\\n""\n            ""Full expected shapes (minus batch_size dimension):\\n{}."".format(\n                debug_str_1, debug_str_2))\n\n    # If we have a time dimension and a train_sequence_length, make sure they\n    # match.\n    if self._num_outer_dims == 2 and self.train_sequence_length is not None:\n\n      def check_shape(path, t):  # pylint: disable=invalid-name\n        if t.shape[1] != self.train_sequence_length:\n          debug_str = tf.nest.map_structure(lambda tp: tp.shape, experience)\n          raise ValueError(\n              ""The agent was configured to expect a `train_sequence_length` ""\n              ""of \'{seq_len}\'. Experience is expected to be shaped `[Batch x ""\n              ""Trajectory_sequence_length x spec.shape]` but at least one the ""\n              ""Tensors in `experience` has a time axis dim value \'{t_dim}\' vs ""\n              ""the expected \'{seq_len}\'.\\nFirst such tensor is:\\n\\t""\n              ""experience.{path}. \\nFull shape structure of ""\n              ""experience:\\n\\t{debug_str}"".format(\n                  seq_len=self.train_sequence_length,\n                  t_dim=t.shape[1],\n                  path=path,\n                  debug_str=debug_str))\n\n      nest_utils.map_structure_with_paths(check_shape, experience)\n\n  def _check_train_argspec(self, kwargs):\n    """"""Check that kwargs passed to train match `self.train_argspec`.\n\n    Args:\n      kwargs: The `kwargs` passed to `train()`.\n\n    Raises:\n      AttributeError: If `kwargs` keyset doesn\'t match `train_argspec`.\n      ValueError: If `kwargs` do not match the specs in `train_argspec`.\n    """"""\n    if not nest_utils.matching_dtypes_and_inner_shapes(\n        kwargs, self.train_argspec, allow_extra_fields=True):\n      get_dtypes = lambda v: tf.nest.map_structure(lambda x: x.dtype, v)\n      get_shapes = lambda v: tf.nest.map_structure(nest_utils.spec_shape, v)\n      raise ValueError(\n          ""Inconsistent dtypes or shapes between `kwargs` and `train_argspec`. ""\n          ""dtypes:\\n{}\\nvs.\\n{}.  shapes:\\n{}\\nvs.\\n{}""\n          .format(get_dtypes(kwargs), get_dtypes(self.train_argspec),\n                  get_shapes(kwargs), get_shapes(self.train_argspec)))\n\n  def train(self,\n            experience: types.NestedTensor,\n            weights: Optional[types.Tensor] = None,\n            **kwargs) -> LossInfo:\n    """"""Trains the agent.\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.training_data_spec`.\n        All tensors in `experience` must be shaped `[batch, time, ...]` where\n        `time` must be equal to `self.train_step_length` if that\n        property is not `None`.\n      weights: (optional).  A `Tensor`, either `0-D` or shaped `[batch]`,\n        containing weights to be used when calculating the total train loss.\n        Weights are typically multiplied elementwise against the per-batch loss,\n        but the implementation is up to the Agent.\n      **kwargs: Any additional data as declared by `self.train_argspec`.\n\n    Returns:\n        A `LossInfo` loss tuple containing loss and info tensors.\n        - In eager mode, the loss values are first calculated, then a train step\n          is performed before they are returned.\n        - In graph mode, executing any or all of the loss tensors\n          will first calculate the loss value(s), then perform a train step,\n          and return the pre-train-step `LossInfo`.\n\n    Raises:\n      TypeError: If `validate_args is True` and: Experience is not type\n        `Trajectory`; or if `experience`  does not match\n        `self.training_data_spec` structure types.\n      ValueError: If `validate_args is True` and: Experience tensors\' time axes\n        are not compatible with `self.train_sequence_length`; or if experience\n        does not match `self.training_data_spec` structure.\n      ValueError: If `validate_args is True` and the user does not pass\n        `**kwargs` matching `self.train_argspec`.\n      RuntimeError: If the class was not initialized properly (`super.__init__`\n        was not called).\n    """"""\n    if self._enable_functions and getattr(self, ""_train_fn"", None) is None:\n      raise RuntimeError(\n          ""Cannot find _train_fn.  Did %s.__init__ call super?""\n          % type(self).__name__)\n\n    if self._validate_args:\n      self._check_trajectory_dimensions(experience)\n      self._check_train_argspec(kwargs)\n\n      # Even though the checks above prune dict keys, we want them to see\n      # the non-pruned versions to provide clearer error messages.\n      # However, from here on out we want to remove dict entries that aren\'t\n      # requested in the spec.\n      experience = nest_utils.prune_extra_keys(\n          self.training_data_spec, experience)\n      kwargs = nest_utils.prune_extra_keys(self.train_argspec, kwargs)\n\n    if self._enable_functions:\n      loss_info = self._train_fn(\n          experience=experience, weights=weights, **kwargs)\n    else:\n      loss_info = self._train(experience=experience, weights=weights, **kwargs)\n\n    if not isinstance(loss_info, LossInfo):\n      raise TypeError(\n          ""loss_info is not a subclass of LossInfo: {}"".format(loss_info))\n    return loss_info\n\n  @property\n  def validate_args(self) -> bool:\n    """"""Whether `train` & `preprocess_sequence` validate input & output args.""""""\n    return self._validate_args\n\n  @property\n  def time_step_spec(self) -> ts.TimeStep:\n    """"""Describes the `TimeStep` tensors expected by the agent.\n\n    Returns:\n      A `TimeStep` namedtuple with `TensorSpec` objects instead of Tensors,\n      which describe the shape, dtype and name of each tensor.\n    """"""\n    return self._time_step_spec\n\n  @property\n  def action_spec(self) -> types.NestedTensorSpec:\n    """"""TensorSpec describing the action produced by the agent.\n\n    Returns:\n      An single BoundedTensorSpec, or a nested dict, list or tuple of\n      `BoundedTensorSpec` objects, which describe the shape and\n      dtype of each action Tensor.\n    """"""\n    return self._action_spec\n\n  @property\n  def train_argspec(self) -> Optional[Dict[Text, types.NestedTensorSpec]]:\n    """"""TensorSpec describing extra supported `kwargs` to `train()`.\n\n    Returns:\n       A `dict` mapping kwarg strings to nests of `tf.TypeSpec` objects (or\n       `None` if there is no `train_argspec`).\n    """"""\n    return self._train_argspec\n\n  @property\n  def policy(self) -> tf_policy.TFPolicy:\n    """"""Return the current policy held by the agent.\n\n    Returns:\n      A `tf_policy.TFPolicy` object.\n    """"""\n    return self._policy\n\n  @property\n  def collect_policy(self) -> tf_policy.TFPolicy:\n    """"""Return a policy that can be used to collect data from the environment.\n\n    Returns:\n      A `tf_policy.TFPolicy` object.\n    """"""\n    return self._collect_policy\n\n  @property\n  def collect_data_spec(self) -> types.NestedTensorSpec:\n    """"""Returns a `Trajectory` spec, as expected by the `collect_policy`.\n\n    Returns:\n      A `Trajectory` spec.\n    """"""\n    return self.collect_policy.trajectory_spec\n\n  @property\n  def training_data_spec(self) -> types.NestedTensorSpec:\n    """"""Returns a trajectory spec, as expected by the train() function.""""""\n    if self._training_data_spec is not None:\n      return self._training_data_spec\n    else:\n      return self.collect_data_spec\n\n  @property\n  def train_sequence_length(self) -> int:\n    """"""The number of time steps needed in experience tensors passed to `train`.\n\n    Train requires experience to be a `Trajectory` containing tensors shaped\n    `[B, T, ...]`.  This argument describes the value of `T` required.\n\n    For example, for non-RNN DQN training, `T=2` because DQN requires single\n    transitions.\n\n    If this value is `None`, then `train` can handle an unknown `T` (it can be\n    determined at runtime from the data).  Most RNN-based agents fall into\n    this category.\n\n    Returns:\n      The number of time steps needed in experience tensors passed to `train`.\n      May be `None` to mean no constraint.\n    """"""\n    return self._train_sequence_length\n\n  @property\n  def summaries_enabled(self) -> bool:\n    return self._enable_summaries\n\n  @property\n  def debug_summaries(self) -> bool:\n    return self._debug_summaries and self.summaries_enabled\n\n  @property\n  def summarize_grads_and_vars(self) -> bool:\n    return self._summarize_grads_and_vars and self.summaries_enabled\n\n  @property\n  def train_step_counter(self) -> tf.Variable:\n    return self._train_step_counter\n\n  def _initialize(self) -> Optional[tf.Operation]:\n    """"""Returns an op to initialize the agent.""""""\n    pass\n\n  def _preprocess_sequence(\n      self, experience: types.NestedTensor) -> types.NestedTensor:\n    """"""Defines preprocess_sequence function to be fed into replay buffers.\n\n    This defines how we preprocess the collected data before training.\n    Defaults to pass through for most agents. Subclasses may override this.\n\n    Args:\n      experience: a `Trajectory` shaped [batch, time, ...] or [time, ...] which\n        represents the collected experience data.\n\n    Returns:\n      A post processed `Trajectory` with the same shape as the input.\n    """"""\n    return experience\n\n  # Subclasses must implement these methods.\n  @abc.abstractmethod\n  def _train(self, experience: types.NestedTensor,\n             weights: types.Tensor) -> LossInfo:\n    """"""Returns an op to train the agent.\n\n    This method *must* increment self.train_step_counter exactly once.\n    TODO(b/126271669): Consider automatically incrementing this\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.training_data_spec`.\n        All tensors in `experience` must be shaped `[batch, time, ...]` where\n        `time` must be equal to `self.train_step_length` if that property is\n        not `None`.\n      weights: (optional).  A `Tensor`, either `0-D` or shaped `[batch]`,\n        containing weights to be used when calculating the total train loss.\n        Weights are typically multiplied elementwise against the per-batch loss,\n        but the implementation is up to the Agent.\n\n    Returns:\n        A `LossInfo` containing the loss *before* the training step is taken.\n        In most cases, if `weights` is provided, the entries of this tuple will\n        have been calculated with the weights.  Note that each Agent chooses\n        its own method of applying weights.\n    """"""\n'"
tf_agents/agents/tf_agent_test.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for agents.tf_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import test_utils\n\n\nclass LossInfoTest(tf.test.TestCase):\n\n  def testBaseLossInfo(self):\n    loss_info = tf_agent.LossInfo(0.0, ())\n    self.assertEqual(loss_info.loss, 0.0)\n    self.assertIsInstance(loss_info, tf_agent.LossInfo)\n\n\nclass MyAgent(tf_agent.TFAgent):\n\n  def __init__(self,\n               time_step_spec=None,\n               action_spec=None,\n               validate_args=True,\n               train_argspec=None,\n               train_sequence_length=None):\n    if time_step_spec is None:\n      obs_spec = {\'obs\': tf.TensorSpec([], tf.float32)}\n      time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = action_spec or ()\n    policy = random_tf_policy.RandomTFPolicy(time_step_spec, action_spec)\n    super(MyAgent, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy=policy,\n        collect_policy=policy,\n        train_sequence_length=train_sequence_length,\n        train_argspec=train_argspec,\n        validate_args=validate_args)\n\n  def _train(self, experience, weights=None, extra=None):\n    return tf_agent.LossInfo(loss=(), extra=(experience, extra))\n\n  def _initialize(self):\n    pass\n\n\nclass TFAgentTest(tf.test.TestCase):\n\n  def testChecksTrainSequenceLength(self):\n    agent = MyAgent(train_sequence_length=2)\n    experience = tensor_spec.sample_spec_nest(agent.collect_data_spec,\n                                              outer_dims=(2, 20,))\n    with self.assertRaisesRegex(\n        ValueError, \'The agent was configured\'):\n      agent.train(experience)\n\n  def testTrainArgspec(self):\n    train_argspec = {\'extra\': tf.TensorSpec(dtype=tf.float32, shape=[3, 4])}\n    agent = MyAgent(train_argspec=train_argspec)\n    extra = tf.ones(shape=[3, 4], dtype=tf.float32)\n    experience = tf.nest.map_structure(\n        lambda x: x[tf.newaxis, ...],\n        trajectory.from_episode(\n            observation={\'obs\': tf.constant([1.0])},\n            action=(),\n            policy_info=(),\n            reward=tf.constant([1.0])))\n    loss_info = agent.train(experience, extra=extra)\n    tf.nest.map_structure(\n        self.assertAllEqual, (experience, extra), loss_info.extra)\n    extra_newdim = tf.ones(shape=[2, 3, 4], dtype=tf.float32)\n    loss_info_newdim = agent.train(experience, extra=extra_newdim)\n    self.assertAllEqual(loss_info_newdim.extra[1], extra_newdim)\n    with self.assertRaisesRegex(\n        ValueError, \'Inconsistent dtypes or shapes between\'):\n      agent.train(experience, extra=tf.ones(shape=[3, 5], dtype=tf.float32))\n    with self.assertRaisesRegex(\n        ValueError, \'Inconsistent dtypes or shapes between\'):\n      agent.train(experience, extra=tf.ones(shape=[3, 4], dtype=tf.int32))\n\n  def testTrainIgnoresExtraFields(self):\n    train_argspec = {\'extra\': tf.TensorSpec(dtype=tf.float32, shape=[3, 4])}\n    agent = MyAgent(train_argspec=train_argspec)\n    extra = tf.ones(shape=[3, 4], dtype=tf.float32)\n    experience = tf.nest.map_structure(\n        lambda x: x[tf.newaxis, ...],\n        trajectory.from_episode(\n            observation={\n                \'obs\': tf.constant([1.0]), \'ignored\': tf.constant([2.0])},\n            action=(),\n            policy_info=(),\n            reward=tf.constant([1.0])))\n    loss_info = agent.train(experience, extra=extra)\n    reduced_experience = experience._replace(\n        observation=copy.copy(experience.observation))\n    del reduced_experience.observation[\'ignored\']\n    tf.nest.map_structure(\n        self.assertAllEqual, (reduced_experience, extra), loss_info.extra)\n\n  def testValidateArgsDisabled(self):\n    train_argspec = {\'extra\': tf.TensorSpec(dtype=tf.float32, shape=[3, 4])}\n    agent = MyAgent(validate_args=False, train_argspec=train_argspec)\n    loss_info = agent.train(experience=\'blah\', extra=3)\n    tf.nest.map_structure(\n        self.assertAllEqual, loss_info.extra, (\'blah\', 3))\n\n\nclass AgentSpecTest(test_utils.TestCase):\n\n  def testErrorOnWrongTimeStepSpecWhenCreatingAgent(self):\n    wrong_time_step_spec = ts.time_step_spec(\n        array_spec.ArraySpec([2], np.float32))\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n    with self.assertRaisesRegex(\n        TypeError, \'time_step_spec has to contain TypeSpec\'):\n      MyAgent(time_step_spec=wrong_time_step_spec, action_spec=action_spec)\n\n  def testErrorOnWrongActionSpecWhenCreatingAgent(self):\n    time_step_spec = ts.time_step_spec(tensor_spec.TensorSpec([2], tf.float32))\n    wrong_action_spec = array_spec.BoundedArraySpec([1], np.float32, -1, 1)\n    with self.assertRaisesRegex(\n        TypeError, \'action_spec has to contain BoundedTensorSpec\'):\n      MyAgent(time_step_spec=time_step_spec, action_spec=wrong_action_spec)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/bandits/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
tf_agents/benchmark/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
tf_agents/benchmark/distribution_strategy_utils.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Helper functions for running models in a distributed setting.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef get_distribution_strategy(distribution_strategy=""default"",\n                              num_gpus=0,\n                              num_packs=-1):\n  """"""Return a DistributionStrategy for running the model.\n\n  Args:\n    distribution_strategy: a string specifying which distribution strategy to\n      use. Accepted values are \'off\', \'default\', \'one_device\', and \'mirrored\'\n      case insensitive. \'off\' means not to use Distribution Strategy; \'default\'\n      means to choose from `MirroredStrategy`or `OneDeviceStrategy` according to\n      the number of GPUs.\n    num_gpus: Number of GPUs to run this model.\n    num_packs: Optional.  Sets the `num_packs` in `tf.distribute.NcclAllReduce`.\n\n  Returns:\n    tf.distribute.DistibutionStrategy object.\n  Raises:\n    ValueError: if `distribution_strategy` is \'off\' or \'one_device\' and\n      `num_gpus` is larger than 1; or `num_gpus` is negative.\n  """"""\n  if num_gpus < 0:\n    raise ValueError(""`num_gpus` can not be negative."")\n\n  distribution_strategy = distribution_strategy.lower()\n  if distribution_strategy == ""off"":\n    if num_gpus > 1:\n      raise ValueError(""When {} GPUs are specified, distribution_strategy ""\n                       ""cannot be set to \'off\'."".format(num_gpus))\n    return None\n\n  if (distribution_strategy == ""one_device"" or\n      (distribution_strategy == ""default"" and num_gpus <= 1)):\n    if num_gpus == 0:\n      return tf.distribute.OneDeviceStrategy(""device:CPU:0"")\n    else:\n      if num_gpus > 1:\n        raise ValueError(""`OneDeviceStrategy` can not be used for more than ""\n                         ""one device."")\n      return tf.distribute.OneDeviceStrategy(""device:GPU:0"")\n\n  if distribution_strategy in (""mirrored"", ""default""):\n    if num_gpus == 0:\n      assert distribution_strategy == ""mirrored""\n      devices = [""device:CPU:0""]\n    else:\n      devices = [""device:GPU:%d"" % i for i in range(num_gpus)]\n\n    cross_device_ops = None\n    if num_packs > -1:\n      cross_device_ops = tf.distribute.NcclAllReduce(num_packs=num_packs)\n    return tf.distribute.MirroredStrategy(devices=devices,\n                                          cross_device_ops=cross_device_ops)\n\n\ndef strategy_scope_context(strategy):\n  if strategy:\n    strategy_scope = strategy.scope()\n  else:\n    strategy_scope = DummyContextManager()\n\n  return strategy_scope\n\n\nclass DummyContextManager(object):\n\n  def __enter__(self):\n    pass\n\n  def __exit__(self, *args):\n    pass\n'"
tf_agents/benchmark/dqn_benchmark_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Benchmarks for DqnAgent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport unittest\n\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.benchmark import distribution_strategy_utils\nfrom tf_agents.benchmark import utils\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.specs import array_spec\nfrom tf_agents.utils import common\n\nfrom tensorflow.python import tf2  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\n\nclass DqnCartPoleAgentBenchmark(tf.test.Benchmark):\n  """"""Short benchmarks (~110 steps) for DQN CartPole environment.""""""\n\n  def _run(self,\n           strategy,\n           batch_size=64,\n           tf_function=True,\n           replay_buffer_max_length=1000,\n           train_steps=110,\n           log_steps=10):\n    """"""Runs Dqn CartPole environment.\n\n    Args:\n      strategy: Strategy to use, None is a valid value.\n      batch_size: Total batch size to use for the run.\n      tf_function: If True tf.function is used.\n      replay_buffer_max_length: Max length of the replay buffer.\n      train_steps: Number of steps to run.\n      log_steps: How often to log step statistics, e.g. step time.\n    """"""\n    obs_spec = array_spec.BoundedArraySpec([\n        4,\n    ], np.float32, -4., 4.)\n    action_spec = array_spec.BoundedArraySpec((), np.int64, 0, 1)\n\n    py_env = random_py_environment.RandomPyEnvironment(\n        obs_spec,\n        action_spec,\n        batch_size=1,\n        reward_fn=lambda *_: np.random.randint(1, 10, 1))\n    env = tf_py_environment.TFPyEnvironment(py_env)\n\n    policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n                                             env.action_spec())\n\n    with distribution_strategy_utils.strategy_scope_context(strategy):\n      q_net = q_network.QNetwork(\n          env.time_step_spec().observation,\n          env.action_spec(),\n          fc_layer_params=(100,))\n\n      tf_agent = dqn_agent.DqnAgent(\n          env.time_step_spec(),\n          env.action_spec(),\n          q_network=q_net,\n          optimizer=tf.keras.optimizers.Adam(),\n          td_errors_loss_fn=common.element_wise_squared_loss)\n      tf_agent.initialize()\n      print(q_net.summary())\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=tf_agent.collect_data_spec,\n        batch_size=1,\n        max_length=replay_buffer_max_length)\n\n    driver = dynamic_step_driver.DynamicStepDriver(env, policy,\n                                                   [replay_buffer.add_batch])\n    if tf_function:\n      driver.run = common.function(driver.run)\n\n    for _ in range(replay_buffer_max_length):\n      driver.run()\n\n    check_values = [\'QNetwork/EncodingNetwork/dense/bias:0\']\n    initial_values = utils.get_initial_values(tf_agent, check_values)\n\n    with distribution_strategy_utils.strategy_scope_context(strategy):\n      dataset = replay_buffer.as_dataset(\n          num_parallel_calls=tf.data.experimental.AUTOTUNE,\n          sample_batch_size=batch_size,\n          num_steps=2)\n      if strategy:\n        iterator = iter(strategy.experimental_distribute_dataset(dataset))\n      else:\n        iterator = iter(dataset)\n\n      def train_step():\n        experience, _ = next(iterator)\n        return tf_agent.train(experience)\n\n      if tf_function:\n        train_step = common.function(train_step)\n      self.run_and_report(\n          train_step,\n          strategy,\n          batch_size,\n          train_steps=train_steps,\n          log_steps=log_steps)\n\n    utils.check_values_changed(tf_agent, initial_values, check_values)\n\n  def run_and_report(self,\n                     train_step,\n                     strategy,\n                     batch_size,\n                     train_steps=110,\n                     log_steps=10):\n    """"""Run function provided and report results per `tf.test.Benchmark`.\n\n    Args:\n      train_step: Function to execute on each step.\n      strategy: Strategy to use, None is a valid value.\n      batch_size: Total batch_size.\n      train_steps: Number of steps to run.\n      log_steps: How often to log step statistics, e.g. step time.\n\n    Returns:\n      `TimeHistory` object with statistics about the throughput perforamnce.\n    """"""\n    history = utils.run_test(\n        train_step,\n        train_steps,\n        strategy,\n        batch_size=batch_size,\n        log_steps=log_steps)\n    print(\'Avg step time:{}\'.format(history.get_average_step_time()))\n    print(\'Avg exp/sec:{}\'.format(history.get_average_examples_per_second()))\n    metrics = []\n    metrics.append({\n        \'name\': \'exp_per_second\',\n        \'value\': history.get_average_examples_per_second()\n    })\n    metrics.append({\n        \'name\': \'steps_per_second\',\n        \'value\': 1 / history.get_average_step_time()\n    })\n    metrics.append({\n        \'name\': \'step_time\',\n        \'value\': history.get_average_step_time()\n    })\n    self.report_benchmark(\n        iters=-1, wall_time=history.get_average_step_time(), metrics=metrics)\n    return history\n\n  def benchmark_dqn_cpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=0)\n    self._run(strategy)\n\n  def benchmark_dqn_mirrored_cpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'mirrored\', num_gpus=0)\n    self._run(strategy)\n\n  def benchmark_dqn_eagerly_cpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=0)\n    self._run(strategy, tf_function=False)\n\n  def benchmark_dqn_no_dist_strat_cpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=0)\n    self._run(strategy)\n\n  def benchmark_dqn_no_dist_strat_eagerly_cpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=0)\n    self._run(strategy, tf_function=False)\n\n  def benchmark_dqn_no_dist_strat_1_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=1)\n    self._run(strategy)\n\n  def benchmark_dqn_no_dist_strat_eagerly_1_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=1)\n    self._run(strategy, tf_function=False)\n\n  def benchmark_dqn_no_dist_strat_1_gpu_xla(self):\n    utils.set_session_config(enable_xla=True)\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=1)\n    self._run(strategy)\n\n  def benchmark_dqn_1_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=1)\n    self._run(strategy)\n\n  def benchmark_dqn_2_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=2)\n    self._run(strategy, batch_size=64 * 2)\n\n  def benchmark_dqn_8_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=8)\n    self._run(strategy, batch_size=64 * 8)\n\n  def benchmark_dqn_mirrored_1_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'mirrored\', num_gpus=1)\n    self._run(strategy)\n\n  def benchmark_dqn_eagerly_1_gpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=1)\n    self._run(strategy, tf_function=False)\n\n  def benchmark_dqn_1_gpu_xla(self):\n    utils.set_session_config(enable_xla=True)\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=1)\n    self._run(strategy)\n\n  def benchmark_dqn_2_gpu_xla(self):\n    utils.set_session_config(enable_xla=True)\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=2)\n    self._run(strategy, batch_size=64 * 2)\n\n  def benchmark_dqn_8_gpu_xla(self):\n    utils.set_session_config(enable_xla=True)\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=8)\n    self._run(strategy, batch_size=64 * 8)\n\n\nclass DqnCartPoleAgentBenchmarkTest(tf.test.TestCase):\n  """"""Tests for DqnCartPoleAgentBenchmark.""""""\n\n  def _run(self, strategy, tf_function=True):\n\n    benchmark = DqnCartPoleAgentBenchmark()\n    benchmark._run(\n        strategy,\n        tf_function=tf_function,\n        replay_buffer_max_length=5,\n        train_steps=2,\n        log_steps=1)\n\n  @unittest.skipUnless(tf2.enabled(), \'TF 2.x only test.\')\n  def testCpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=0)\n    self._run(strategy)\n\n  @unittest.skipUnless(tf2.enabled(), \'TF 2.x only test.\')\n  def testEagerCpu(self):\n    print(\'TF 2.0 enable:{}\'.format(tf2.enabled()))\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'default\', num_gpus=0)\n    self._run(strategy, tf_function=False)\n\n  @unittest.skipUnless(tf2.enabled(), \'TF 2.x only test.\')\n  def testNoStrategyCpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=0)\n    self._run(strategy)\n\n  @unittest.skipUnless(tf2.enabled(), \'TF 2.x only test.\')\n  def testNoStrategyEagerCpu(self):\n    strategy = distribution_strategy_utils.get_distribution_strategy(\n        distribution_strategy=\'off\', num_gpus=0)\n    self._run(strategy, tf_function=False)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/benchmark/utils.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Utilities for running benchmarks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef run_test(target_call,\n             num_steps,\n             strategy,\n             batch_size=None,\n             log_steps=100,\n             num_steps_per_batch=1):\n  """"""Run benchmark and return TimeHistory object with stats.\n\n  Args:\n    target_call: Call to execute for each step.\n    num_steps: Number of steps to run.\n    strategy: None or tf.distribute.DistibutionStrategy object.\n    batch_size: Total batch size.\n    log_steps: Interval of steps between logging of stats.\n    num_steps_per_batch: Number of steps per batch. Used to account for total\n      number of transitions or examples processed per iteration.\n\n  Returns:\n    TimeHistory object containing step performance stats.\n  """"""\n  history = TimeHistory(batch_size, log_steps, num_steps_per_batch)\n\n  for _ in range(num_steps):\n    history.on_batch_begin()\n    if strategy:\n      strategy.run(target_call)\n    else:\n      target_call()\n    history.on_batch_end()\n\n  return history\n\n\nclass BatchTimestamp(object):\n  """"""A structure to store batch timestamp.""""""\n\n  def __init__(self, batch_index, timestamp):\n    self.batch_index = batch_index\n    self.timestamp = timestamp\n\n  def __repr__(self):\n    return ""\'BatchTimestamp<batch_index: {}, timestamp: {}>\'"".format(\n        self.batch_index, self.timestamp)\n\n\nclass TimeHistory(object):\n  """"""Track step performance statistics.""""""\n\n  def __init__(self, batch_size, log_steps, num_steps_per_batch=1):\n    """"""Callback for logging performance.\n\n    Args:\n      batch_size: Total batch size.\n      log_steps: Interval of steps between logging of stats.\n      num_steps_per_batch: Number of steps per batch.\n    """"""\n    self.batch_size = batch_size\n    super(TimeHistory, self).__init__()\n    self.log_steps = log_steps\n    self.global_steps = 0\n    self.num_steps_per_batch = num_steps_per_batch\n\n    # Logs start of step 1 then end of each step based on log_steps interval.\n    self.timestamp_log = []\n\n  def on_batch_begin(self):\n    self.global_steps += 1\n    if self.global_steps == 1:\n      self.start_time = time.time()\n      self.timestamp_log.append(\n          BatchTimestamp(self.global_steps, self.start_time))\n\n  def on_batch_end(self):\n    """"""Records elapse time of the batch and calculates examples per second.""""""\n    if self.global_steps % self.log_steps == 0:\n      timestamp = time.time()\n      elapsed_time = timestamp - self.start_time\n      steps_per_second = self.log_steps / elapsed_time\n      examples_per_second = steps_per_second * self.batch_size\n      step_time = elapsed_time / self.log_steps\n      self.timestamp_log.append(BatchTimestamp(self.global_steps, timestamp))\n      print(""BenchmarkMetric: \'{{global step\':{}, ""\n            ""\'steps_per_second\':{:.5g}, step_time:{:.5g}, ""\n            ""\'examples_per_second\':{:.3f}}}"".format(self.global_steps,\n                                                    steps_per_second, step_time,\n                                                    examples_per_second))\n      self.start_time = timestamp\n\n  def get_average_examples_per_second(self, warmup=True):\n    """"""Returns average examples per second so far.\n\n    Examples per second are defined by `batch_size` * `num_steps_per_batch`\n\n    Args:\n      warmup: If true ignore first set of steps executed as determined by\n        `log_steps`.\n\n    Returns:\n      Average examples per second.\n    """"""\n    return 1 / self.get_average_step_time(\n        warmup=warmup) * self.batch_size * self.num_steps_per_batch\n\n  def get_average_step_time(self, warmup=True):\n    """"""Returns average step time (seconds) so far.\n\n    Args:\n      warmup: If true ignore first set of steps executed as determined by\n        `log_steps`.\n\n    Returns:\n      Average step time in seconds.\n\n    """"""\n    if warmup:\n      if len(self.timestamp_log) < 3:\n        return -1\n      elapsed = self.timestamp_log[-1].timestamp - self.timestamp_log[\n          1].timestamp\n      return elapsed / (self.log_steps * (len(self.timestamp_log) - 2))\n    else:\n      if len(self.timestamp_log) < 2:\n        return -1\n      elapsed = self.timestamp_log[-1].timestamp - self.timestamp_log[\n          0].timestamp\n      return elapsed / (self.log_steps * (len(self.timestamp_log) - 1))\n\n\ndef set_session_config(enable_xla=False):\n  """"""Sets the session config.""""""\n  if enable_xla:\n    tf.config.optimizer.set_jit(True)\n    # Disable PinToHostOptimizer in grappler when enabling XLA because it\n    # causes OOM and performance regression.\n    tf.config.optimizer.set_experimental_options(\n        {\'pin_to_host_optimization\': False})\n\n\ndef get_variable_value(agent, name):\n  """"""Returns the value of the trainable variable with the given name.""""""\n  policy_vars = agent.policy.variables()\n  tf_vars = [v for v in policy_vars if name in v.name]\n  assert tf_vars, \'Variable ""{}"" does not exist. Found: {}\'.format(\n      name, policy_vars)\n  if tf.executing_eagerly() and len(tf_vars) > 1:\n    var = tf_vars[0]\n  else:\n    assert len(tf_vars) == 1, \'More than one variable with name {}. {}\'.format(\n        name, [(v.name, v.shape) for v in tf_vars])\n    var = tf_vars[0]\n  return var.numpy() if tf.executing_eagerly() else var.eval()\n\n\ndef get_initial_values(agent, check_values):\n  """"""Returns the initial values.""""""\n  return [get_variable_value(agent, var_name) for var_name in check_values]\n\n\ndef check_values_changed(agent, initial_values, check_value_changes, name=None):\n  """"""Checks that the initial values.""""""\n  final_values = [get_variable_value(agent, var_name) for var_name in \\\n                    check_value_changes]\n  for var_name, initial, final in zip(check_value_changes, initial_values,\n                                      final_values):\n    all_close = np.allclose(initial, final)\n    assert not all_close, (\'[{}] Variable ""{}"" did not change: {} -> {}\'.format(\n        name, var_name, initial, final))\n'"
tf_agents/distributions/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Distributions module.""""""\nfrom tf_agents.distributions import masked\nfrom tf_agents.distributions import shifted_categorical\nfrom tf_agents.distributions import tanh_bijector_stable\nfrom tf_agents.distributions import utils\n'"
tf_agents/distributions/gumbel_softmax.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Gumbel_Softmax distribution classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\n\nclass GumbelSoftmax(\n    tfp.distributions.relaxed_onehot_categorical.RelaxedOneHotCategorical):\n  """"""GumbelSoftmax distribution with temperature and logits.\n\n  The implementation is almost identical to tfp.distributions.\n  relaxed_onehot_categorical.RelaxedOneHotCategorical except for the following:\n\n  1. Add mode() function to return mode of the underlying categorical\n     distribution (There is no mode() defined in RelaxedOneHotCategorical)\n  2. Add a convert_to_integer() function to convert the sample from non-integer\n     to integer. Note that the sample function returns one_hot format of the\n     discrete action that is different from regular distributions.\n  3. log_prob() of RelaxedOneHotCategorical will return INF when the input is\n     at boundary. In this implementation, we add a small epsilon to avoid\n     getting NAN. In addition, when the input is discrete, we calculate log_prob\n     using the underlying categorical distribution.\n\n  """"""\n\n  def __init__(\n      self,\n      temperature,\n      logits=None,\n      probs=None,\n      dtype=tf.int32,\n      validate_args=False,\n      allow_nan_stats=True,\n      name=\'GumbelSoftmax\'):\n    """"""Initialize GumbelSoftmax using class log-probabilities.\n\n    Args:\n      temperature: A `Tensor`, representing the temperature of one or more\n        distributions. The temperature values must be positive, and the shape\n        must broadcast against `(logits or probs)[..., 0]`.\n      logits: An N-D `Tensor`, `N >= 1`, representing the log probabilities\n        of one or many distributions. The first `N - 1` dimensions index into a\n        batch of independent distributions and the last dimension represents a\n        vector of logits for each class. Only one of `logits` or `probs` should\n        be passed in.\n      probs: An N-D `Tensor`, `N >= 1`, representing the probabilities\n        of one or many distributions. The first `N - 1` dimensions index into a\n        batch of independent distributions and the last dimension represents a\n        vector of probabilities for each class. Only one of `logits` or `probs`\n        should be passed in.\n      dtype: The type of the event samples (default: int32).\n      validate_args: Python `bool`, default `False`. When `True` distribution\n        parameters are checked for validity despite possibly degrading runtime\n        performance. When `False` invalid inputs may silently render incorrect\n        outputs.\n      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n        (e.g., mean, mode, variance) use the value ""`NaN`"" to indicate the\n        result is undefined. When `False`, an exception is raised if one or\n        more of the statistic\'s batch members are undefined.\n      name: Python `str` name prefixed to Ops created by this class.\n    """"""\n    super(GumbelSoftmax, self).__init__(\n        temperature=temperature,\n        logits=logits,\n        probs=probs,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats)\n\n    self._output_dtype = dtype\n\n  def _log_prob(self, x):\n    if x.dtype != self.distribution.logits.dtype:\n      # Calculate log_prob using the underlying categorical distribution when\n      # the input is discrete.\n      x = tf.cast(x, self.distribution.logits.dtype)\n      return tf.reduce_sum(\n          x * tf.math.log_softmax(self.distribution.logits), axis=-1)\n    # Add an epsilon to prevent INF.\n    x += 1e-10\n    return super(GumbelSoftmax, self)._log_prob(x)\n\n  def convert_to_one_hot(self, samples):\n    return tf.one_hot(\n        tf.argmax(samples, axis=-1),\n        self.distribution.event_size, dtype=self._output_dtype)\n\n  def _mode(self):\n    return self.convert_to_one_hot(self.distribution.logits)\n\n\n'"
tf_agents/distributions/gumbel_softmax_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.distributions.gumbel_softmax.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.distributions import gumbel_softmax\n\n\nclass GumbelSoftmaxTest(tf.test.TestCase):\n\n  def testLogProb(self):\n    temperature = 0.8\n    logits = [.3, .1, .4]\n    dist = gumbel_softmax.GumbelSoftmax(\n        temperature, logits, validate_args=True)\n    x = tf.constant([0, 0, 1])\n    log_prob = self.evaluate(dist.log_prob(x))\n    expected_log_prob = -0.972918868065\n    self.assertAllClose(expected_log_prob, log_prob)\n\n  def testSample(self):\n    temperature = 0.8\n    logits = [.3, .1, .4]\n    dist = gumbel_softmax.GumbelSoftmax(\n        temperature, logits, dtype=tf.int64, validate_args=True)\n    actions = dist.convert_to_one_hot(dist.sample())\n    self.assertEqual(actions.dtype, tf.int64)\n    self.assertEqual(self.evaluate(tf.reduce_sum(actions, axis=-1)), 1)\n\n  def testMode(self):\n    temperature = 1.0\n    logits = [.3, .1, .4]\n    dist = gumbel_softmax.GumbelSoftmax(\n        temperature, logits, validate_args=True)\n    self.assertAllEqual(self.evaluate(dist.mode()),\n                        self.evaluate(tf.constant([0, 0, 1])))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/distributions/masked.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Define distributions for spaces where not all actions are valid.""""""\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\n\nclass MaskedCategorical(tfp.distributions.Categorical):\n  """"""A categorical distribution which supports masks per step.\n\n  Masked values are replaced with -inf inside the logits. This means the values\n  will never be sampled.\n\n  When computing the log probability of a set of actions, each action is\n  assigned a probability under each sample. _log_prob is modified to only return\n  the probability of a sample under the distribution for the same timestep.\n\n  TODO(ddohan): Integrate entropy calculation from cl/207017752\n  """"""\n\n  def __init__(self,\n               logits,\n               mask,\n               probs=None,\n               dtype=tf.int32,\n               validate_args=False,\n               allow_nan_stats=True,\n               neg_inf=-1e10,\n               name=\'MaskedCategorical\'):\n    """"""Initialize Categorical distributions using class log-probabilities.\n\n    Args:\n      logits: An N-D `Tensor`, `N >= 1`, representing the log probabilities of a\n        set of Categorical distributions. The first `N - 1` dimensions index\n        into a batch of independent distributions and the last dimension\n        represents a vector of logits for each class. Only one of `logits` or\n        `probs` should be passed in.\n      mask: A boolean mask. False/0 values mean a position should be masked out.\n      probs: Must be `None`. Required to conform with base\n        class `tfp.distributions.Categorical`.\n      dtype: The type of the event samples (default: int32).\n      validate_args: Python `bool`, default `False`. When `True` distribution\n        parameters are checked for validity despite possibly degrading runtime\n        performance. When `False` invalid inputs may silently render incorrect\n        outputs.\n      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n        (e.g., mean, mode, variance) use the value ""`NaN`"" to indicate the\n        result is undefined. When `False`, an exception is raised if one or more\n        of the statistic\'s batch members are undefined.\n      neg_inf: None or Float. Value used to mask out invalid positions. If None,\n        use logits.dtype.min to get a large negative number.\n        Otherwise use given value.\n      name: Python `str` name prefixed to Ops created by this class.\n    """"""\n    logits = tf.convert_to_tensor(value=logits)\n    mask = tf.convert_to_tensor(value=mask)\n    self._mask = tf.cast(mask, tf.bool)  # Nonzero values are True\n    if probs is not None:\n      raise ValueError(\'Must provide masked predictions as logits.\'\n                       \' Probs are accepted for API compatibility with \'\n                       \' Categorical distribution. Given `%s`.\' % probs)\n\n    if neg_inf is None:\n      neg_inf = logits.dtype.min\n    neg_inf = tf.cast(\n        tf.fill(dims=tf.shape(input=logits), value=neg_inf), logits.dtype)\n    logits = tf.compat.v2.where(self._mask, logits, neg_inf)\n\n    super(MaskedCategorical, self).__init__(\n        logits=logits,\n        probs=None,\n        dtype=dtype,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        name=name)\n\n  def _entropy(self):\n    entropy = tf.nn.log_softmax(self.logits) * self.probs_parameter()\n    # Replace the (potentially -inf) values with 0s before summing.\n    entropy = tf.compat.v1.where(self._mask, entropy, tf.zeros_like(entropy))\n    return -tf.reduce_sum(input_tensor=entropy, axis=-1)\n\n  @property\n  def mask(self):\n    return self._mask\n\n  @property\n  def parameters(self):\n    params = super(MaskedCategorical, self).parameters\n    params[\'mask\'] = self.mask\n    return params\n'"
tf_agents/distributions/masked_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests masked distributions.""""""\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.distributions import masked\n\n\nclass MaskedCategoricalTest(tf.test.TestCase):\n\n  def testCopy(self):\n    """"""Confirm we can copy the distribution.""""""\n    distribution = masked.MaskedCategorical([100.0, 100.0, 100.0],\n                                            mask=[True, False, True])\n    copy = distribution.copy()\n    with self.cached_session() as s:\n      probs_np = s.run(copy.probs_parameter())\n      logits_np = s.run(copy.logits_parameter())\n      ref_probs_np = s.run(distribution.probs_parameter())\n      ref_logits_np = s.run(distribution.logits_parameter())\n    self.assertAllEqual(ref_logits_np, logits_np)\n    self.assertAllEqual(ref_probs_np, probs_np)\n\n  def testMasking(self):\n    distribution = masked.MaskedCategorical([100.0, 100.0, 100.0],\n                                            mask=[True, False, True],\n                                            neg_inf=None)\n    sample = distribution.sample()\n    results = []\n\n    probs_tensor = distribution.probs_parameter()\n    logits_tensor = distribution.logits_parameter()\n\n    with self.cached_session() as s:\n      probs_np = s.run(probs_tensor)\n      logits_np = s.run(logits_tensor)\n\n      # Draw samples & confirm we never draw a masked sample\n      for _ in range(100):\n        results.append(s.run(sample))\n\n    self.assertAllEqual([0.5, 0, 0.5], probs_np)\n    self.assertAllEqual([100, logits_tensor.dtype.min, 100], logits_np)\n    self.assertNotIn(1, results)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/distributions/reparameterized_sampling.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Helper function to do reparameterized sampling if the distributions supports it.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import gumbel_softmax\n\n\ndef sample(distribution, reparam=False, **kwargs):\n  """"""Sample from distribution either with reparameterized sampling or regular sampling.\n\n  Args:\n    distribution: A `tfp.distributions.Distribution` instance.\n    reparam: Whether to use reparameterized sampling.\n    **kwargs: Parameters to be passed to distribution\'s sample() fucntion.\n\n  Returns:\n  """"""\n  if reparam:\n    if (distribution.reparameterization_type !=\n        tfp.distributions.FULLY_REPARAMETERIZED):\n      raise ValueError(\'This distribution cannot be reparameterized\'\n                       \': {}\'.format(distribution))\n    else:\n      return distribution.sample(**kwargs)\n  else:\n    if isinstance(distribution, gumbel_softmax.GumbelSoftmax):\n      samples = distribution.sample(**kwargs)\n      return distribution.convert_to_one_hot(samples)\n    else:\n      return distribution.sample(**kwargs)\n'"
tf_agents/distributions/shifted_categorical.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Categorical distribution where values are shifted to honor a range.""""""\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\n\nclass ShiftedCategorical(tfp.distributions.Categorical):\n  """"""Categorical distribution with support [shift, shift + K] instead of [0, K].\n\n  Simply a thin wrapper around Categorical which takes a user provided minimal\n  value and shifts the minimum value using the provided value. This distribution\n  allows policies where the user provides an action_spec range, e.g. QPolicy, to\n  honor it, by offsetting the sampled value.\n  """"""\n\n  def __init__(self,\n               logits=None,\n               probs=None,\n               dtype=tf.int32,\n               validate_args=False,\n               allow_nan_stats=True,\n               shift=None,\n               name=""ShiftedCategorical""):\n    """"""Initialize Categorical distributions using class log-probabilities.\n\n    Args:\n      logits: An N-D `Tensor`, `N >= 1`, representing the log probabilities of a\n        set of Categorical distributions. The first `N - 1` dimensions index\n        into a batch of independent distributions and the last dimension\n        represents a vector of logits for each class. Only one of `logits` or\n        `probs` should be passed in.\n      probs: An N-D `Tensor`, `N >= 1`, representing the probabilities\n        of a set of Categorical distributions. The first `N - 1` dimensions\n        index into a batch of independent distributions and the last dimension\n        represents a vector of probabilities for each class. Only one of\n        `logits` or `probs` should be passed in.\n      dtype: The type of the event samples (default: int32).\n      validate_args: Python `bool`, default `False`. When `True` distribution\n        parameters are checked for validity despite possibly degrading runtime\n        performance. When `False` invalid inputs may silently render incorrect\n        outputs.\n      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n        (e.g., mean, mode, variance) use the value ""`NaN`"" to indicate the\n        result is undefined. When `False`, an exception is raised if one or more\n        of the statistic\'s batch members are undefined.\n      shift: value to shift the interval such that the sampled values are\n        between [shift, shift + K] instead of [0, K].\n      name: Python `str` name prefixed to Ops created by this class.\n    """"""\n    if shift is None:\n      raise ValueError(""ShiftedCategorical expects a shift value."""""")\n\n    self._shift = shift\n    super(ShiftedCategorical, self).__init__(\n        logits=logits,\n        probs=probs,\n        dtype=dtype,\n        validate_args=validate_args,\n        allow_nan_stats=allow_nan_stats,\n        name=name)\n\n  def log_prob(self, value, name=""log_prob""):\n    """"""Log probability density/mass function.""""""\n    value -= self._shift\n    return super(ShiftedCategorical, self).log_prob(value, name)\n\n  def prob(self, value, name=""prob""):\n    """"""Probability density/mass function.""""""\n    value -= self._shift\n    return super(ShiftedCategorical, self).prob(value, name)\n\n  def cdf(self, value, name=""log_cdf""):\n    """"""Cumulative distribution function.""""""\n    value -= self._shift\n    return super(ShiftedCategorical, self).cdf(value, name)\n\n  def log_cdf(self, value, name=""log_cdf""):\n    """"""Log cumulative distribution function.""""""\n    value -= self._shift\n    return super(ShiftedCategorical, self).log_cdf(value, name)\n\n  def mode(self, name=""mode""):\n    """"""Mode of the distribution.""""""\n    mode = super(ShiftedCategorical, self).mode(name)\n    return mode + self._shift\n\n  def sample(self, sample_shape=(), seed=None, name=""sample"", **kwargs):\n    """"""Generate samples of the specified shape.""""""\n    sample = super(ShiftedCategorical, self).sample(\n        sample_shape=sample_shape, seed=seed, name=name, **kwargs)\n    return sample + self._shift\n\n  @property\n  def shift(self):\n    return self._shift\n\n  @property\n  def parameters(self):\n    params = super(ShiftedCategorical, self).parameters\n    params[""shift""] = self._shift\n    return params\n'"
tf_agents/distributions/shifted_categorical_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests shifted categorical distribution.""""""\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import shifted_categorical\n\n\nclass ShiftedCategoricalTest(tf.test.TestCase):\n\n  def testCopy(self):\n    """"""Confirm we can copy the distribution.""""""\n    distribution = shifted_categorical.ShiftedCategorical(\n        logits=[100.0, 100.0, 100.0], shift=2)\n    copy = distribution.copy()\n    with self.cached_session() as s:\n      probs_np = s.run(copy.probs_parameter())\n      logits_np = s.run(copy.logits_parameter())\n      ref_probs_np = s.run(distribution.probs_parameter())\n      ref_logits_np = s.run(distribution.logits_parameter())\n    self.assertAllEqual(ref_logits_np, logits_np)\n    self.assertAllEqual(ref_probs_np, probs_np)\n\n  def testShiftedSampling(self):\n    distribution = shifted_categorical.ShiftedCategorical(\n        probs=[0.1, 0.8, 0.1], shift=2)\n    sample = distribution.sample()\n    log_prob = distribution.log_prob(sample)\n    results = []\n\n    with self.cached_session() as s:\n      for _ in range(100):\n        value, _ = s.run([sample, log_prob])\n        results.append(value)\n\n    results = np.array(results, dtype=np.int32)\n    self.assertTrue(np.all(results >= 2))\n    self.assertTrue(np.all(results <= 4))\n\n  def testCompareToCategorical(self):\n    # Use the same probabilities for normal categorical and shifted one.\n    shift = 2\n    probabilities = [0.3, 0.3, 0.4]\n    distribution = tfp.distributions.Categorical(probs=probabilities)\n    shifted_distribution = shifted_categorical.ShiftedCategorical(\n        probs=probabilities, shift=shift)\n\n    # Compare outputs of basic methods, using the same starting seed.\n    tf.compat.v1.set_random_seed(1)  # required per b/131171329, only with TF2.\n    sample = distribution.sample(seed=1)\n    tf.compat.v1.set_random_seed(1)  # required per b/131171329, only with TF2.\n    shifted_sample = shifted_distribution.sample(seed=1)\n\n    mode = distribution.mode()\n    shifted_mode = shifted_distribution.mode()\n\n    sample, shifted_sample = self.evaluate([sample, shifted_sample])\n    mode, shifted_mode = self.evaluate([mode, shifted_mode])\n\n    self.assertEqual(shifted_sample, sample + shift)\n    self.assertEqual(shifted_mode, mode + shift)\n\n    # These functions should return the same values for shifted values.\n    fns = [\'cdf\', \'log_cdf\', \'prob\', \'log_prob\']\n    for fn_name in fns:\n      fn = getattr(distribution, fn_name)\n      shifted_fn = getattr(shifted_distribution, fn_name)\n      value, shifted_value = self.evaluate([fn(sample),\n                                            shifted_fn(shifted_sample)])\n      self.assertEqual(value, shifted_value)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/distributions/tanh_bijector_stable.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tanh bijector.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tensorflow_probability.python.bijectors import bijector\n\n\n__all__ = [\n    ""Tanh"",\n]\n\n\nclass Tanh(bijector.Bijector):\n  """"""Bijector that computes `Y = tanh(X)`, therefore `Y in (-1, 1)`.\n\n  This can be achieved by an affine transform of the Sigmoid bijector, i.e.,\n  it is equivalent to\n  ```\n  tfb.Chain([tfb.Affine(shift=-1, scale=2.),\n             tfb.Sigmoid(),\n             tfb.Affine(scale=2.)])\n  ```\n\n  However, using the `Tanh` bijector directly is slightly faster and more\n  numerically stable.\n  """"""\n\n  def __init__(self, validate_args=False, name=""tanh""):\n    super(Tanh, self).__init__(\n        forward_min_event_ndims=0,\n        validate_args=validate_args,\n        name=name)\n\n  def _forward(self, x):\n    return tf.nn.tanh(x)\n\n  def _inverse(self, y):\n    # 0.99999997 is the maximum value such that atanh(x) is valid for both\n    # tf.float32 and tf.float64\n    y = tf.where(tf.less_equal(tf.abs(y), 1.),\n                 tf.clip_by_value(y, -0.99999997, 0.99999997),\n                 y)\n    return tf.atanh(y)\n\n  def _forward_log_det_jacobian(self, x):\n    #  This formula is mathematically equivalent to\n    #  `tf.log1p(-tf.square(tf.tanh(x)))`, however this code is more numerically\n    #  stable.\n\n    #  Derivation:\n    #    log(1 - tanh(x)^2)\n    #    = log(sech(x)^2)\n    #    = 2 * log(sech(x))\n    #    = 2 * log(2e^-x / (e^-2x + 1))\n    #    = 2 * (log(2) - x - log(e^-2x + 1))\n    #    = 2 * (log(2) - x - softplus(-2x))\n    return 2.0 * (\n        tf.math.log(tf.constant(2.0, dtype=x.dtype)) - x - tf.nn.softplus(\n            -2.0 * x))\n'"
tf_agents/distributions/utils.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities related to distributions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.distributions import tanh_bijector_stable\nfrom tf_agents.utils import common\n\n\ndef scale_distribution_to_spec(distribution, spec):\n  """"""Scales the given distribution to the bounds of the given spec.""""""\n  return SquashToSpecNormal(distribution, spec)\n\n\nclass SquashToSpecNormal(tfp.distributions.Distribution):\n  """"""Scales an input normalized action distribution to match spec bounds.\n\n  Unlike the normal distribution computed when NormalProjectionNetwork\n  is called with scale_distribution=False, which merely squashes the mean\n  of the distribution to within the action spec, this distribution scales the\n  output distribution to ensure that the output action fits within the spec.\n\n  This distribution also maintains the input normal distribution, and uses this\n  distribution to compute the KL-divergence between two SquashToSpecNormal\n  distributions provided that they were scaled by the same action spec.\n  This is possible as KL divergence is invariant when both distributions are\n  transformed using the same invertible function.\n\n  Formally, let a be the action magnitude and b be the action mean. The\n  squashing operation performs the following change of variables to the\n  input distribution X:\n\n  Y = a * tanh(X) + b\n\n  Note that this is a change of variables as the function is invertible, with:\n\n  X = tan((Y - b) / a), where Y in (b - a, b + a)\n  """"""\n\n  def __init__(self,\n               distribution,\n               spec,\n               validate_args=False,\n               name=""SquashToSpecNormal""):\n    """"""Constructs a SquashToSpecNormal distribution.\n\n    Args:\n      distribution: input normal distribution with normalized mean and std dev\n      spec: bounded action spec from which to compute action ranges\n      validate_args: Python `bool`, default `False`. When `True` distribution\n        parameters are checked for validity despite possibly degrading runtime\n        performance. When `False` invalid inputs may silently render incorrect\n        outputs.\n      name: Python `str` name prefixed to Ops created by this class.\n    """"""\n\n    if not isinstance(\n        distribution,\n        (tfp.distributions.Normal, tfp.distributions.MultivariateNormalDiag)):\n      raise ValueError(""Input distribution must be a normal distribution, ""\n                       ""got {} instead"".format(distribution))\n    self.action_means, self.action_magnitudes = common.spec_means_and_magnitudes(\n        spec)\n    # Parameters here describe the actor network\'s output, which is a normalized\n    # distribution prior to squashing to the action spec.\n    # This is necessary (and sufficient) in order for policy info to compare an\n    # old policy to a new policy.\n    parameters = {""loc"": distribution.loc, ""scale"": distribution.scale}\n    # The raw action distribution\n    self.input_distribution = distribution\n\n    bijectors = [\n        tfp.bijectors.Shift(self.action_means)(\n            tfp.bijectors.Scale(self.action_magnitudes)),\n        tanh_bijector_stable.Tanh()\n    ]\n    bijector_chain = tfp.bijectors.Chain(bijectors)\n    self._squashed_distribution = tfp.distributions.TransformedDistribution(\n        distribution=distribution, bijector=bijector_chain)\n    super(SquashToSpecNormal, self).__init__(\n        dtype=distribution.dtype,\n        reparameterization_type=distribution.reparameterization_type,\n        validate_args=validate_args,\n        allow_nan_stats=distribution.allow_nan_stats,\n        parameters=parameters,\n        # We let TransformedDistribution access _graph_parents since this class\n        # is more like a baseclass than derived.\n        graph_parents=(\n            distribution._graph_parents +  # pylint: disable=protected-access\n            bijector_chain.graph_parents),\n        name=name)\n\n  def kl_divergence(self, other, name=""kl_divergence""):\n    """"""Computes the KL Divergence between two SquashToSpecNormal distributions.""""""\n    if not isinstance(other, SquashToSpecNormal):\n      raise ValueError(""other distribution should be of type ""\n                       ""SquashToSpecNormal, got {}"".format(other))\n    if (tf.reduce_any(tf.not_equal(self.action_means, other.action_means)) or\n        tf.reduce_any(\n            tf.not_equal(self.action_magnitudes, other.action_magnitudes))):\n      raise ValueError(""Other distribution does not have same action mean ""\n                       ""and magnitude. This mean {}, this magnitude {}, ""\n                       ""other mean {}, other magnitude {}."".format(\n                           self.action_means, self.action_magnitudes,\n                           other.action_means, other.action_magnitudes))\n    return self.input_distribution.kl_divergence(other.input_distribution, name)\n\n  def sample(self, sample_shape=(), seed=None, name=""sample""):\n    """"""Generates samples from the wrapped TransformedDistribution.""""""\n    return self._squashed_distribution.sample(sample_shape, seed, name)\n\n  def log_prob(self, value, name=""log_prob""):\n    """"""Computes log probability from the wrapped TransformedDistribution.""""""\n    return self._squashed_distribution.log_prob(value, name)\n\n  def prob(self, value, name=""prob""):\n    """"""Computes probability from the wrapped TransformedDistribution.""""""\n    return self._squashed_distribution.prob(value, name)\n\n  def stddev(self, name=""stddev""):\n    """"""Compute stddev of the SquashToSpecNormal distribution.""""""\n    stddev = self.action_magnitudes * tf.tanh(self.input_distribution.stddev())\n    return stddev\n\n  def mode(self, name=""mode""):\n    """"""Compute mean of the SquashToSpecNormal distribution.""""""\n    mean = self.action_magnitudes * tf.tanh(self.input_distribution.mode()) + \\\n        self.action_means\n    return mean\n\n  def mean(self, name=""mean"", **kwargs):\n    """"""Compute mean of the SquashToSpecNormal distribution.""""""\n    return self.mode(name)\n\n  def event_shape_tensor(self, name=""event_shape_tensor""):\n    """"""Compute event shape tensor of the SquashToSpecNormal distribution.""""""\n    return self._squashed_distribution.event_shape_tensor(name)\n\n  def batch_shape_tensor(self, name=""batch_shape_tensor""):\n    """"""Compute event shape tensor of the SquashToSpecNormal distribution.""""""\n    return self._squashed_distribution.batch_shape_tensor(name)\n'"
tf_agents/distributions/utils_test.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.distributions.utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.distributions import utils\nfrom tf_agents.specs import tensor_spec\n\n\nclass UtilsTest(tf.test.TestCase):\n\n  def testScaleDistribution(self):\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -2, 4)\n    distribution = tfp.distributions.Normal(0, 4)\n    scaled_distribution = utils.scale_distribution_to_spec(distribution,\n                                                           action_spec)\n    if tf.executing_eagerly():\n      sample = scaled_distribution.sample\n    else:\n      sample = scaled_distribution.sample()\n\n    for _ in range(1000):\n      sample_np = self.evaluate(sample)\n\n      self.assertGreater(sample_np, -2.00001)\n      self.assertLess(sample_np, 4.00001)\n\n  def testSquashToSpecNormalModeMethod(self):\n    input_dist = tfp.distributions.Normal(loc=1.0, scale=3.0)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -2.0, 4.0)\n    squash_to_spec_normal = utils.SquashToSpecNormal(input_dist, action_spec)\n    self.assertAlmostEqual(\n        self.evaluate(squash_to_spec_normal.mode()), 3.28478247, places=5)\n\n  def testSquashToSpecNormalStdMethod(self):\n    input_dist = tfp.distributions.Normal(loc=1.0, scale=3.0)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -2.0, 4.0)\n    squash_to_spec_normal = utils.SquashToSpecNormal(input_dist, action_spec)\n    self.assertAlmostEqual(\n        self.evaluate(squash_to_spec_normal.stddev()), 2.98516426, places=5)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/drivers/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Drivers for running a policy in an environment.""""""\n\nfrom tf_agents.drivers import driver\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.drivers import py_driver\n'"
tf_agents/drivers/driver.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for drivers that takes steps in an environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Driver(object):\n  """"""A driver that takes steps in an environment using a policy.""""""\n\n  def __init__(self, env, policy, observers=None, transition_observers=None):\n    """"""Creates a Driver.\n\n    Args:\n      env: An environment.Base environment.\n      policy: A policy.Base policy.\n      observers: A list of observers that are updated after the driver is run.\n        Each observer is a callable(Trajectory) that returns the input.\n        Trajectory.time_step is a stacked batch [N+1, batch_size, ...] of\n        timesteps and Trajectory.action is a stacked batch\n        [N, batch_size, ...] of actions in time major form.\n      transition_observers: A list of observers that are updated after every\n        step in the environment. Each observer is a callable((TimeStep,\n        PolicyStep, NextTimeStep)). The transition is shaped just as\n        trajectories are for regular observers.\n    """"""\n\n    self._env = env\n    self._policy = policy\n    self._observers = observers or []\n    self._transition_observers = transition_observers or []\n\n  @property\n  def env(self):\n    return self._env\n\n  @property\n  def policy(self):\n    return self._policy\n\n  @property\n  def transition_observers(self):\n    return self._transition_observers\n\n  @property\n  def observers(self):\n    return self._observers\n\n  @abc.abstractmethod\n  def run(self):\n    """"""Takes steps in the environment and updates observers.""""""\n'"
tf_agents/drivers/dynamic_episode_driver.py,19,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Driver that takes N episodes in the environment using a tf.while_loop.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.environments import bandit_tf_environment\nfrom tf_agents.drivers import driver\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ndef is_bandit_env(env):\n  actual_env = env\n  if isinstance(env, tf_py_environment.TFPyEnvironment):\n    actual_env = env.pyenv\n  is_bandit = (\n      isinstance(actual_env, bandit_py_environment.BanditPyEnvironment) or\n      isinstance(actual_env, bandit_tf_environment.BanditTFEnvironment))\n  return is_bandit\n\n\n@gin.configurable\nclass DynamicEpisodeDriver(driver.Driver):\n  """"""A driver that takes N episodes in an environment using a tf.while_loop.\n\n  The while loop will run num_episodes in the environment, counting transitions\n  that result in ending an episode.\n\n  As environments run batched time_episodes, the counters for all batch elements\n  are summed, and execution stops when the total exceeds num_episodes.\n\n  This termination condition can be overridden in subclasses by implementing the\n  self._loop_condition_fn() method.\n  """"""\n\n  def __init__(self,\n               env,\n               policy,\n               observers=None,\n               transition_observers=None,\n               num_episodes=1):\n    """"""Creates a DynamicEpisodeDriver.\n\n    Args:\n      env: A tf_environment.Base environment.\n      policy: A tf_policy.TFPolicy policy.\n      observers: A list of observers that are updated after every step in the\n        environment. Each observer is a callable(Trajectory).\n      transition_observers: A list of observers that are updated after every\n        step in the environment. Each observer is a callable((TimeStep,\n        PolicyStep, NextTimeStep)).\n      num_episodes: The number of episodes to take in the environment.\n\n    Raises:\n      ValueError:\n        If env is not a tf_environment.Base or policy is not an instance of\n        tf_policy.TFPolicy.\n    """"""\n    super(DynamicEpisodeDriver, self).__init__(env, policy, observers,\n                                               transition_observers)\n    self._num_episodes = num_episodes\n    self._run_fn = common.function_in_tf1()(self._run)\n    self._is_bandit_env = is_bandit_env(env)\n\n  def _loop_condition_fn(self, num_episodes):\n    """"""Returns a function with the condition needed for tf.while_loop.""""""\n\n    def loop_cond(counter, *_):\n      """"""Determines when to stop the loop, based on episode counter.\n\n      Args:\n        counter: Episode counters per batch index. Shape [batch_size] when\n          batch_size > 1, else shape [].\n\n      Returns:\n        tf.bool tensor, shape (), indicating whether while loop should continue.\n      """"""\n      return tf.less(tf.reduce_sum(input_tensor=counter), num_episodes)\n\n    return loop_cond\n\n  def _loop_body_fn(self):\n    """"""Returns a function with the driver\'s loop body ops.""""""\n\n    def loop_body(counter, time_step, policy_state):\n      """"""Runs a step in environment.\n\n      While loop will call multiple times.\n\n      Args:\n        counter: Episode counters per batch index. Shape [batch_size].\n        time_step: TimeStep tuple with elements shape [batch_size, ...].\n        policy_state: Poicy state tensor shape [batch_size, policy_state_dim].\n          Pass empty tuple for non-recurrent policies.\n\n      Returns:\n        loop_vars for next iteration of tf.while_loop.\n      """"""\n      action_step = self.policy.action(time_step, policy_state)\n\n      # TODO(b/134487572): TF2 while_loop seems to either ignore\n      # parallel_iterations or doesn\'t properly propagate control dependencies\n      # from one step to the next. Without this dep, self.env.step() is called\n      # in parallel.\n      with tf.control_dependencies(tf.nest.flatten([time_step])):\n        next_time_step = self.env.step(action_step.action)\n\n      policy_state = action_step.state\n\n      if self._is_bandit_env:\n        # For Bandits we create episodes of length 1.\n        # Since the `next_time_step` is always of type LAST we need to replace\n        # the step type of the current `time_step` to FIRST.\n        batch_size = tf.shape(input=time_step.discount)\n        time_step = time_step._replace(\n            step_type=tf.fill(batch_size, ts.StepType.FIRST))\n\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n      observer_ops = [observer(traj) for observer in self._observers]\n      transition_observer_ops = [\n          observer((time_step, action_step, next_time_step))\n          for observer in self._transition_observers\n      ]\n      with tf.control_dependencies(\n          [tf.group(observer_ops + transition_observer_ops)]):\n        time_step, next_time_step, policy_state = tf.nest.map_structure(\n            tf.identity, (time_step, next_time_step, policy_state))\n\n      # While loop counter is only incremented for episode reset episodes.\n      # For Bandits, this is every trajectory, for MDPs, this is at boundaries.\n      if self._is_bandit_env:\n        counter += tf.ones(batch_size, dtype=tf.int32)\n      else:\n        counter += tf.cast(traj.is_boundary(), dtype=tf.int32)\n\n      return [counter, next_time_step, policy_state]\n\n    return loop_body\n\n  def run(self,\n          time_step=None,\n          policy_state=None,\n          num_episodes=None,\n          maximum_iterations=None):\n    """"""Takes episodes in the environment using the policy and update observers.\n\n    If `time_step` and `policy_state` are not provided, `run` will reset the\n    environment and request an initial state from the policy.\n\n    Args:\n      time_step: optional initial time_step. If None, it will be obtained by\n        resetting the environment. Elements should be shape [batch_size, ...].\n      policy_state: optional initial state for the policy. If None, it will be\n        obtained from the policy.get_initial_state().\n      num_episodes: Optional number of episodes to take in the environment. If\n        None it would use initial num_episodes.\n      maximum_iterations: Optional maximum number of iterations of the while\n        loop to run. If provided, the cond output is AND-ed with an additional\n        condition ensuring the number of iterations executed is no greater than\n        maximum_iterations.\n\n    Returns:\n      time_step: TimeStep named tuple with final observation, reward, etc.\n      policy_state: Tensor with final step policy state.\n    """"""\n    return self._run_fn(\n        time_step=time_step,\n        policy_state=policy_state,\n        num_episodes=num_episodes,\n        maximum_iterations=maximum_iterations)\n\n  def _run(self,\n           time_step=None,\n           policy_state=None,\n           num_episodes=None,\n           maximum_iterations=None):\n    """"""See `run()` docstring for details.""""""\n    if time_step is None:\n      time_step = self.env.reset()\n\n    if policy_state is None:\n      policy_state = self.policy.get_initial_state(self.env.batch_size)\n\n    # Batch dim should be first index of tensors during data\n    # collection.\n    batch_dims = nest_utils.get_outer_shape(time_step,\n                                            self.env.time_step_spec())\n    counter = tf.zeros(batch_dims, tf.int32)\n\n    num_episodes = num_episodes or self._num_episodes\n    [_, time_step, policy_state] = tf.nest.map_structure(\n        tf.stop_gradient,\n        tf.while_loop(\n            cond=self._loop_condition_fn(num_episodes),\n            body=self._loop_body_fn(),\n            loop_vars=[counter, time_step, policy_state],\n            parallel_iterations=1,\n            maximum_iterations=maximum_iterations,\n            name=\'driver_loop\'))\n\n    return time_step, policy_state\n'"
tf_agents/drivers/dynamic_episode_driver_test.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.dynamic_episode_driver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import test_utils\n\n\nclass DynamicEpisodeDriverTest(test_utils.TestCase):\n\n  def testPolicyState(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n    num_steps_observer = driver_test_utils.NumStepsObserver()\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, observers=[num_episodes_observer, num_steps_observer])\n    run_driver = driver.run()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step, policy_state = self.evaluate(run_driver)\n\n    self.assertEqual(time_step.step_type, 0)\n    self.assertEqual(policy_state, [3])\n\n  def testContinuePreviusRun(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n    num_steps_observer = driver_test_utils.NumStepsObserver()\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, observers=[num_episodes_observer, num_steps_observer])\n    time_step, policy_state = driver.run()\n    time_step, policy_state = driver.run(time_step, policy_state)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step, policy_state = self.evaluate([time_step, policy_state])\n\n    self.assertEqual(time_step.step_type, 0)\n    self.assertEqual(policy_state, [3])\n\n  def testOneStepUpdatesObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n    num_steps_observer = driver_test_utils.NumStepsObserver()\n    num_steps_transition_observer = (\n        driver_test_utils.NumStepsTransitionObserver())\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env,\n        policy,\n        observers=[num_episodes_observer, num_steps_observer],\n        transition_observers=[num_steps_transition_observer])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for _ in range(5):\n      self.evaluate(driver.run())\n\n    self.assertEqual(self.evaluate(num_episodes_observer.num_episodes), 5)\n    # Two steps per episode.\n    self.assertEqual(self.evaluate(num_steps_observer.num_steps), 10)\n    self.assertEqual(self.evaluate(num_steps_transition_observer.num_steps), 10)\n\n  def testMultiStepUpdatesObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n    num_steps_observer = driver_test_utils.NumStepsObserver()\n    num_steps_transition_observer = (\n        driver_test_utils.NumStepsTransitionObserver())\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env,\n        policy,\n        observers=[num_episodes_observer, num_steps_observer],\n        transition_observers=[num_steps_transition_observer])\n\n    run_driver = driver.run(num_episodes=5)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    self.assertEqual(self.evaluate(num_episodes_observer.num_episodes), 5)\n    # Two steps per episode.\n    self.assertEqual(self.evaluate(num_steps_observer.num_steps), 10)\n    self.assertEqual(self.evaluate(num_steps_transition_observer.num_steps), 10)\n\n  def testTwoStepObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    num_episodes_observer0 = driver_test_utils.NumEpisodesObserver(\n        variable_scope=\'observer0\')\n    num_episodes_observer1 = driver_test_utils.NumEpisodesObserver(\n        variable_scope=\'observer1\')\n    num_steps_transition_observer = (\n        driver_test_utils.NumStepsTransitionObserver())\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env,\n        policy,\n        num_episodes=5,\n        observers=[num_episodes_observer0, num_episodes_observer1],\n        transition_observers=[num_steps_transition_observer])\n    run_driver = driver.run()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    self.assertEqual(self.evaluate(num_episodes_observer0.num_episodes), 5)\n    self.assertEqual(self.evaluate(num_episodes_observer1.num_episodes), 5)\n    self.assertEqual(self.evaluate(num_steps_transition_observer.num_steps), 10)\n\n  def testOneStepReplayBufferObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer = driver_test_utils.make_replay_buffer(policy)\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, num_episodes=1, observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run if tf.executing_eagerly() else driver.run()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    for _ in range(3):\n      self.evaluate(run_driver)\n\n    trajectories = self.evaluate(replay_buffer.gather_all())\n\n    self.assertAllEqual(trajectories.step_type, [[0, 1, 2, 0, 1, 2, 0, 1, 2]])\n    self.assertAllEqual(trajectories.action, [[1, 2, 1, 1, 2, 1, 1, 2, 1]])\n    self.assertAllEqual(trajectories.observation, [[0, 1, 3, 0, 1, 3, 0, 1, 3]])\n    self.assertAllEqual(trajectories.policy_info, [[2, 4, 2, 2, 4, 2, 2, 4, 2]])\n    self.assertAllEqual(trajectories.next_step_type,\n                        [[1, 2, 0, 1, 2, 0, 1, 2, 0]])\n    self.assertAllEqual(trajectories.reward,\n                        [[1., 1., 0., 1., 1., 0., 1., 1., 0.]])\n    self.assertAllEqual(trajectories.discount,\n                        [[1., 0., 1, 1, 0, 1., 1., 0., 1.]])\n\n  def testMultiStepReplayBufferObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer = driver_test_utils.make_replay_buffer(policy)\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, num_episodes=3, observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run()\n    rb_gather_all = replay_buffer.gather_all()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    trajectories = self.evaluate(rb_gather_all)\n\n    self.assertAllEqual(trajectories.step_type, [[0, 1, 2, 0, 1, 2, 0, 1, 2]])\n    self.assertAllEqual(trajectories.action, [[1, 2, 1, 1, 2, 1, 1, 2, 1]])\n    self.assertAllEqual(trajectories.observation, [[0, 1, 3, 0, 1, 3, 0, 1, 3]])\n    self.assertAllEqual(trajectories.policy_info, [[2, 4, 2, 2, 4, 2, 2, 4, 2]])\n    self.assertAllEqual(trajectories.next_step_type,\n                        [[1, 2, 0, 1, 2, 0, 1, 2, 0]])\n    self.assertAllEqual(trajectories.reward,\n                        [[1., 1., 0., 1., 1., 0., 1., 1., 0.]])\n    self.assertAllEqual(trajectories.discount,\n                        [[1., 0., 1., 1., 0., 1., 1., 0., 1.]])\n\n  def testBanditEnvironment(self):\n\n    def _context_sampling_fn():\n      return np.array([[5, -5], [2, -2]])\n\n    reward_fns = [\n        environment_utilities.LinearNormalReward(theta, sigma=0.0)\n        for theta in ([1, 0], [0, 1])\n    ]\n    batch_size = 2\n    py_env = sspe.StationaryStochasticPyEnvironment(\n        _context_sampling_fn, reward_fns, batch_size=batch_size)\n    env = tf_py_environment.TFPyEnvironment(py_env)\n    policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n                                             env.action_spec())\n\n    steps_per_loop = 4\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=policy.trajectory_spec,\n        batch_size=batch_size,\n        max_length=steps_per_loop)\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env,\n        policy,\n        num_episodes=steps_per_loop * batch_size,\n        observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run()\n    rb_gather_all = replay_buffer.gather_all()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    trajectories = self.evaluate(rb_gather_all)\n\n    self.assertAllEqual(trajectories.step_type, [[0, 0, 0, 0], [0, 0, 0, 0]])\n    self.assertAllEqual(trajectories.next_step_type,\n                        [[2, 2, 2, 2], [2, 2, 2, 2]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/drivers/dynamic_step_driver.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Driver that takes N steps in the environment using a tf.while_loop.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.environments import bandit_tf_environment\nfrom tf_agents.drivers import driver\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ndef is_bandit_env(env):\n  actual_env = env\n  if isinstance(env, tf_py_environment.TFPyEnvironment):\n    actual_env = env.pyenv\n  is_bandit = (\n      isinstance(actual_env, bandit_py_environment.BanditPyEnvironment) or\n      isinstance(actual_env, bandit_tf_environment.BanditTFEnvironment))\n  return is_bandit\n\n\n@gin.configurable\nclass DynamicStepDriver(driver.Driver):\n  """"""A driver that takes N steps in an environment using a tf.while_loop.\n\n  The while loop will run num_steps in the environment, only counting steps that\n  result in an environment transition, i.e. (time_step, action, next_time_step).\n  If a step results in environment resetting, i.e. time_step.is_last() and\n  next_time_step.is_first() (traj.is_boundary()), this is not counted toward the\n  num_steps.\n\n  As environments run batched time_steps, the counters for all batch elements\n  are summed, and execution stops when the total exceeds num_steps. When\n  batch_size > 1, there is no guarantee that exactly num_steps are taken -- it\n  may be more but never less.\n\n  This termination condition can be overridden in subclasses by implementing the\n  self._loop_condition_fn() method.\n  """"""\n\n  def __init__(\n      self,\n      env,\n      policy,\n      observers=None,\n      transition_observers=None,\n      num_steps=1,\n  ):\n    """"""Creates a DynamicStepDriver.\n\n    Args:\n      env: A tf_environment.Base environment.\n      policy: A tf_policy.TFPolicy policy.\n      observers: A list of observers that are updated after every step in the\n        environment. Each observer is a callable(time_step.Trajectory).\n      transition_observers: A list of observers that are updated after every\n        step in the environment. Each observer is a callable((TimeStep,\n        PolicyStep, NextTimeStep)).\n      num_steps: The number of steps to take in the environment.\n\n    Raises:\n      ValueError:\n        If env is not a tf_environment.Base or policy is not an instance of\n        tf_policy.TFPolicy.\n    """"""\n    super(DynamicStepDriver, self).__init__(env, policy, observers,\n                                            transition_observers)\n    self._num_steps = num_steps\n    self._run_fn = common.function_in_tf1()(self._run)\n    self._is_bandit_env = is_bandit_env(env)\n\n  def _loop_condition_fn(self):\n    """"""Returns a function with the condition needed for tf.while_loop.""""""\n\n    def loop_cond(counter, *_):\n      """"""Determines when to stop the loop, based on step counter.\n\n      Args:\n        counter: Step counters per batch index. Shape [batch_size] when\n          batch_size > 1, else shape [].\n\n      Returns:\n        tf.bool tensor, shape (), indicating whether while loop should continue.\n      """"""\n      return tf.less(tf.reduce_sum(input_tensor=counter), self._num_steps)\n\n    return loop_cond\n\n  def _loop_body_fn(self):\n    """"""Returns a function with the driver\'s loop body ops.""""""\n\n    def loop_body(counter, time_step, policy_state):\n      """"""Runs a step in environment.\n\n      While loop will call multiple times.\n\n      Args:\n        counter: Step counters per batch index. Shape [batch_size].\n        time_step: TimeStep tuple with elements shape [batch_size, ...].\n        policy_state: Policy state tensor shape [batch_size, policy_state_dim].\n          Pass empty tuple for non-recurrent policies.\n\n      Returns:\n        loop_vars for next iteration of tf.while_loop.\n      """"""\n      action_step = self.policy.action(time_step, policy_state)\n      policy_state = action_step.state\n      next_time_step = self.env.step(action_step.action)\n\n      if self._is_bandit_env:\n        # For Bandits we create episodes of length 1.\n        # Since the `next_time_step` is always of type LAST we need to replace\n        # the step type of the current `time_step` to FIRST.\n        batch_size = tf.shape(input=time_step.discount)\n        time_step = time_step._replace(\n            step_type=tf.fill(batch_size, ts.StepType.FIRST))\n\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n      observer_ops = [observer(traj) for observer in self._observers]\n      transition_observer_ops = [\n          observer((time_step, action_step, next_time_step))\n          for observer in self._transition_observers\n      ]\n      with tf.control_dependencies(\n          [tf.group(observer_ops + transition_observer_ops)]):\n        time_step, next_time_step, policy_state = tf.nest.map_structure(\n            tf.identity, (time_step, next_time_step, policy_state))\n\n      # While loop counter should not be incremented for episode reset steps.\n      counter += tf.cast(~traj.is_boundary(), dtype=tf.int32)\n\n      return [counter, next_time_step, policy_state]\n\n    return loop_body\n\n  def run(self, time_step=None, policy_state=None, maximum_iterations=None):\n    """"""Takes steps in the environment using the policy while updating observers.\n\n    Args:\n      time_step: optional initial time_step. If None, it will use the\n        current_time_step of the environment. Elements should be shape\n        [batch_size, ...].\n      policy_state: optional initial state for the policy.\n      maximum_iterations: Optional maximum number of iterations of the while\n        loop to run. If provided, the cond output is AND-ed with an additional\n        condition ensuring the number of iterations executed is no greater than\n        maximum_iterations.\n\n    Returns:\n      time_step: TimeStep named tuple with final observation, reward, etc.\n      policy_state: Tensor with final step policy state.\n    """"""\n    return self._run_fn(\n        time_step=time_step,\n        policy_state=policy_state,\n        maximum_iterations=maximum_iterations)\n\n  # TODO(b/113529538): Add tests for policy_state.\n  def _run(self, time_step=None, policy_state=None, maximum_iterations=None):\n    """"""See `run()` docstring for details.""""""\n    if time_step is None:\n      time_step = self.env.current_time_step()\n    if policy_state is None:\n      policy_state = self.policy.get_initial_state(self.env.batch_size)\n\n    # Batch dim should be first index of tensors during data collection.\n    batch_dims = nest_utils.get_outer_shape(time_step,\n                                            self.env.time_step_spec())\n    counter = tf.zeros(batch_dims, tf.int32)\n\n    [_, time_step, policy_state] = tf.while_loop(\n        cond=self._loop_condition_fn(),\n        body=self._loop_body_fn(),\n        loop_vars=[counter, time_step, policy_state],\n        back_prop=False,\n        parallel_iterations=1,\n        maximum_iterations=maximum_iterations,\n        name=\'driver_loop\')\n    return time_step, policy_state\n'"
tf_agents/drivers/dynamic_step_driver_test.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.dynamic_step_driver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import test_utils\n\n\nclass DynamicStepDriverTest(test_utils.TestCase):\n\n  def testOneStepUpdatesObservers(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123880556\')\n\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    policy_state_ph = tensor_spec.to_nest_placeholder(\n        policy.policy_state_spec,\n        default=0,\n        name_scope=\'policy_state_ph\',\n        outer_dims=(1,))\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env, policy, observers=[num_episodes_observer])\n    run_driver = driver.run(policy_state=policy_state_ph)\n\n    with self.session() as session:\n      session.run(tf.compat.v1.global_variables_initializer())\n      _, policy_state = session.run(run_driver)\n      for _ in range(4):\n        _, policy_state = session.run(\n            run_driver, feed_dict={policy_state_ph: policy_state})\n      self.assertEqual(self.evaluate(num_episodes_observer.num_episodes), 2)\n\n  def testMultiStepUpdatesObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    num_episodes_observer = driver_test_utils.NumEpisodesObserver()\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env, policy, num_steps=5, observers=[num_episodes_observer])\n\n    run_driver = driver.run()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    self.assertEqual(self.evaluate(num_episodes_observer.num_episodes), 2)\n\n  def testTwoObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    policy_state = policy.get_initial_state(1)\n    num_episodes_observer0 = driver_test_utils.NumEpisodesObserver(\n        variable_scope=\'observer0\')\n    num_episodes_observer1 = driver_test_utils.NumEpisodesObserver(\n        variable_scope=\'observer1\')\n    num_steps_transition_observer = (\n        driver_test_utils.NumStepsTransitionObserver())\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env,\n        policy,\n        num_steps=5,\n        observers=[num_episodes_observer0, num_episodes_observer1],\n        transition_observers=[num_steps_transition_observer],\n    )\n    run_driver = driver.run(policy_state=policy_state)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    self.assertEqual(self.evaluate(num_episodes_observer0.num_episodes), 2)\n    self.assertEqual(self.evaluate(num_episodes_observer1.num_episodes), 2)\n    self.assertEqual(self.evaluate(num_steps_transition_observer.num_steps), 5)\n\n  def testOneStepReplayBufferObservers(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123880556\')\n\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    policy_state_ph = tensor_spec.to_nest_placeholder(\n        policy.policy_state_spec,\n        default=0,\n        name_scope=\'policy_state_ph\',\n        outer_dims=(1,))\n    replay_buffer = driver_test_utils.make_replay_buffer(policy)\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env, policy, num_steps=1, observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run(policy_state=policy_state_ph)\n    rb_gather_all = replay_buffer.gather_all()\n\n    with self.session() as session:\n      session.run(tf.compat.v1.global_variables_initializer())\n      _, policy_state = session.run(run_driver)\n      for _ in range(5):\n        _, policy_state = session.run(\n            run_driver, feed_dict={policy_state_ph: policy_state})\n\n      trajectories = self.evaluate(rb_gather_all)\n\n    self.assertAllEqual(trajectories.step_type, [[0, 1, 2, 0, 1, 2, 0, 1]])\n    self.assertAllEqual(trajectories.observation, [[0, 1, 3, 0, 1, 3, 0, 1]])\n    self.assertAllEqual(trajectories.action, [[1, 2, 1, 1, 2, 1, 1, 2]])\n    self.assertAllEqual(trajectories.policy_info, [[2, 4, 2, 2, 4, 2, 2, 4]])\n    self.assertAllEqual(trajectories.next_step_type, [[1, 2, 0, 1, 2, 0, 1, 2]])\n    self.assertAllEqual(trajectories.reward, [[1., 1., 0., 1., 1., 0., 1., 1.]])\n    self.assertAllEqual(trajectories.discount, [[1., 0., 1, 1, 0, 1., 1., 0.]])\n\n  def testMultiStepReplayBufferObservers(self):\n    env = tf_py_environment.TFPyEnvironment(\n        driver_test_utils.PyEnvironmentMock())\n    policy = driver_test_utils.TFPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    policy_state = policy.get_initial_state(1)\n    replay_buffer = driver_test_utils.make_replay_buffer(policy)\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env, policy, num_steps=6, observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run(policy_state=policy_state)\n    rb_gather_all = replay_buffer.gather_all()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    trajectories = self.evaluate(rb_gather_all)\n\n    self.assertAllEqual(trajectories.step_type, [[0, 1, 2, 0, 1, 2, 0, 1]])\n    self.assertAllEqual(trajectories.observation, [[0, 1, 3, 0, 1, 3, 0, 1]])\n    self.assertAllEqual(trajectories.action, [[1, 2, 1, 1, 2, 1, 1, 2]])\n    self.assertAllEqual(trajectories.policy_info, [[2, 4, 2, 2, 4, 2, 2, 4]])\n    self.assertAllEqual(trajectories.next_step_type, [[1, 2, 0, 1, 2, 0, 1, 2]])\n    self.assertAllEqual(trajectories.reward, [[1., 1., 0., 1., 1., 0., 1., 1.]])\n    self.assertAllEqual(trajectories.discount, [[1., 0., 1, 1, 0, 1., 1., 0.]])\n\n  def testBanditEnvironment(self):\n\n    def _context_sampling_fn():\n      return np.array([[5, -5], [2, -2]])\n\n    reward_fns = [\n        environment_utilities.LinearNormalReward(theta, sigma=0.0)\n        for theta in ([1, 0], [0, 1])\n    ]\n    batch_size = 2\n    py_env = sspe.StationaryStochasticPyEnvironment(\n        _context_sampling_fn, reward_fns, batch_size=batch_size)\n    env = tf_py_environment.TFPyEnvironment(py_env)\n    policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n                                             env.action_spec())\n\n    steps_per_loop = 4\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=policy.trajectory_spec,\n        batch_size=batch_size,\n        max_length=steps_per_loop)\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env,\n        policy,\n        num_steps=steps_per_loop * batch_size,\n        observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run()\n    rb_gather_all = replay_buffer.gather_all()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    trajectories = self.evaluate(rb_gather_all)\n\n    self.assertAllEqual(trajectories.step_type, [[0, 0, 0, 0], [0, 0, 0, 0]])\n    self.assertAllEqual(trajectories.next_step_type,\n                        [[2, 2, 2, 2], [2, 2, 2, 2]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/drivers/py_driver.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Driver that steps a python environment using a python policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport numpy as np\nfrom tf_agents.drivers import driver\nfrom tf_agents.environments import py_environment\nfrom tf_agents.policies import py_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\n\nfrom tf_agents.typing import types\nfrom typing import Any, Callable, Optional, Sequence, Tuple\n\n\nclass PyDriver(driver.Driver):\n  """"""A driver that runs a python policy in a python environment.""""""\n\n  def __init__(\n      self,\n      env: py_environment.PyEnvironment,\n      policy: py_policy.PyPolicy,\n      observers: Sequence[Callable[[trajectory.Trajectory], Any]],\n      transition_observers: Optional[Sequence[Callable[[types.Transition],\n                                                       Any]]] = None,\n      max_steps: Optional[types.Int] = None,\n      max_episodes: Optional[types.Int] = None):\n    """"""A driver that runs a python policy in a python environment.\n\n    Args:\n      env: A py_environment.Base environment.\n      policy: A py_policy.PyPolicy policy.\n      observers: A list of observers that are notified after every step\n        in the environment. Each observer is a callable(trajectory.Trajectory).\n      transition_observers: A list of observers that are updated after every\n        step in the environment. Each observer is a callable((TimeStep,\n        PolicyStep, NextTimeStep)). The transition is shaped just as\n        trajectories are for regular observers.\n      max_steps: Optional maximum number of steps for each run() call.\n        Also see below.  Default: 0.\n      max_episodes: Optional maximum number of episodes for each run() call.\n        At least one of max_steps or max_episodes must be provided. If both\n        are set, run() terminates when at least one of the conditions is\n        satisfied.  Default: 0.\n\n    Raises:\n      ValueError: If both max_steps and max_episodes are None.\n    """"""\n    max_steps = max_steps or 0\n    max_episodes = max_episodes or 0\n    if max_steps < 1 and max_episodes < 1:\n      raise ValueError(\n          \'Either `max_steps` or `max_episodes` should be greater than 0.\')\n\n    super(PyDriver, self).__init__(env, policy, observers, transition_observers)\n    self._max_steps = max_steps or np.inf\n    self._max_episodes = max_episodes or np.inf\n\n  def run(\n      self,\n      time_step: ts.TimeStep,\n      policy_state: types.NestedArray = ()\n  ) -> Tuple[ts.TimeStep, types.NestedArray]:\n    """"""Run policy in environment given initial time_step and policy_state.\n\n    Args:\n      time_step: The initial time_step.\n      policy_state: The initial policy_state.\n\n    Returns:\n      A tuple (final time_step, final policy_state).\n    """"""\n    num_steps = 0\n    num_episodes = 0\n    while num_steps < self._max_steps and num_episodes < self._max_episodes:\n      action_step = self.policy.action(time_step, policy_state)\n      next_time_step = self.env.step(action_step.action)\n\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n      for observer in self._transition_observers:\n        observer((time_step, action_step, next_time_step))\n      for observer in self.observers:\n        observer(traj)\n\n      num_episodes += np.sum(traj.is_last())\n      num_steps += np.sum(~traj.is_boundary())\n\n      time_step = next_time_step\n      policy_state = action_step.state\n\n    return time_step, policy_state\n'"
tf_agents/drivers/py_driver_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.py_driver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.trajectories import trajectory\n\n\nclass MockReplayBufferObserver(object):\n\n  def __init__(self):\n    self._trajectories = []\n\n  def __call__(self, trajectory_):\n    self._trajectories.append(trajectory_)\n\n  def gather_all(self):\n    return self._trajectories\n\n\nclass PyDriverTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(PyDriverTest, self).setUp()\n    f0 = np.array(0., dtype=np.float32)\n    f1 = np.array(1., dtype=np.float32)\n    # Order of args for trajectory methods:\n    # (observation, action, policy_info, reward, discount)\n    self._trajectories = [\n        trajectory.first(0, 1, 2, f1, f1),\n        trajectory.last(1, 2, 4, f1, f0),\n        trajectory.boundary(3, 1, 2, f0, f1),\n        trajectory.first(0, 1, 2, f1, f1),\n        trajectory.last(1, 2, 4, f1, f0),\n        trajectory.boundary(3, 1, 2, f0, f1),\n        trajectory.first(0, 1, 2, f1, f1),\n    ]\n\n  @parameterized.named_parameters(\n      [(\'NoneStepsTwoEpisodes\', None, 2, 5),\n       (\'TwoStepsTwoEpisodes\', 2, 2, 2),\n       (\'FourStepsTwoEpisodes\', 4, 2, 5),\n       (\'FourStepsOneEpisodes\', 4, 1, 2),\n       (\'FourStepsNoneEpisodes\', 4, None, 5),\n      ])\n  def testRunOnce(self, max_steps, max_episodes, expected_steps):\n    env = driver_test_utils.PyEnvironmentMock()\n    policy = driver_test_utils.PyPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer_observer = MockReplayBufferObserver()\n    transition_replay_buffer_observer = MockReplayBufferObserver()\n    driver = py_driver.PyDriver(\n        env,\n        policy,\n        observers=[replay_buffer_observer],\n        transition_observers=[transition_replay_buffer_observer],\n        max_steps=max_steps,\n        max_episodes=max_episodes,\n    )\n\n    initial_time_step = env.reset()\n    initial_policy_state = policy.get_initial_state()\n    driver.run(initial_time_step, initial_policy_state)\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:expected_steps])\n\n    transitions = transition_replay_buffer_observer.gather_all()\n    self.assertLen(transitions, expected_steps)\n    # TimeStep, Action, NextTimeStep\n    self.assertLen(transitions[0], 3)\n\n  def testMultipleRunMaxSteps(self):\n\n    num_steps = 3\n    num_expected_steps = 4\n\n    env = driver_test_utils.PyEnvironmentMock()\n    policy = driver_test_utils.PyPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer_observer = MockReplayBufferObserver()\n    driver = py_driver.PyDriver(\n        env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=1,\n        max_episodes=None,\n    )\n\n    time_step = env.reset()\n    policy_state = policy.get_initial_state()\n    for _ in range(num_steps):\n      time_step, policy_state = driver.run(time_step, policy_state)\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:num_expected_steps])\n\n  def testMultipleRunMaxEpisodes(self):\n\n    num_episodes = 2\n    num_expected_steps = 5\n\n    env = driver_test_utils.PyEnvironmentMock()\n    policy = driver_test_utils.PyPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer_observer = MockReplayBufferObserver()\n    driver = py_driver.PyDriver(\n        env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=None,\n        max_episodes=1,\n    )\n\n    time_step = env.reset()\n    policy_state = policy.get_initial_state()\n    for _ in range(num_episodes):\n      time_step, policy_state = driver.run(time_step, policy_state)\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:num_expected_steps])\n\n  @parameterized.named_parameters([\n      (\'NoneStepsNoneEpisodes\', None, None),\n      (\'ZeroStepsNoneEpisodes\', 0, None),\n      (\'NoneStepsZeroEpisodes\', None, 0),\n      (\'ZeroStepsZeroEpisodes\', 0, 0),\n  ])\n  def testValueErrorOnInvalidArgs(self, max_steps, max_episodes):\n    env = driver_test_utils.PyEnvironmentMock()\n    policy = driver_test_utils.PyPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    replay_buffer_observer = MockReplayBufferObserver()\n    with self.assertRaises(ValueError):\n      py_driver.PyDriver(\n          env,\n          policy,\n          observers=[replay_buffer_observer],\n          max_steps=max_steps,\n          max_episodes=max_episodes,\n      )\n\n  @parameterized.named_parameters([\n      (\'FourStepsNoneEpisodesBoundaryNotCounted\', 4, None, 2),\n      (\'FiveStepsNoneEpisodesBoundaryNotCounted\', 5, None, 3),\n      (\'NoneStepsTwoEpisodesBoundaryNotCounted\', None, 2, 3),\n      (\'TwoStepsTwoEpisodesBoundaryNotCounted\', 2, 2, 1),\n      (\'FourStepsTwoEpisodesBoundaryNotCounted\', 4, 2, 2),\n  ])\n  def testBatchedEnvironment(self, max_steps, max_episodes, expected_length):\n\n    expected_trajectories = [\n        trajectory.Trajectory(\n            step_type=np.array([0, 0]),\n            observation=np.array([0, 0]),\n            action=np.array([2, 1]),\n            policy_info=np.array([4, 2]),\n            next_step_type=np.array([1, 1]),\n            reward=np.array([1., 1.]),\n            discount=np.array([1., 1.])),\n        trajectory.Trajectory(\n            step_type=np.array([1, 1]),\n            observation=np.array([2, 1]),\n            action=np.array([1, 2]),\n            policy_info=np.array([2, 4]),\n            next_step_type=np.array([2, 1]),\n            reward=np.array([1., 1.]),\n            discount=np.array([0., 1.])),\n        trajectory.Trajectory(\n            step_type=np.array([2, 1]),\n            observation=np.array([3, 3]),\n            action=np.array([2, 1]),\n            policy_info=np.array([4, 2]),\n            next_step_type=np.array([0, 2]),\n            reward=np.array([0., 1.]),\n            discount=np.array([1., 0.]))\n    ]\n\n    env1 = driver_test_utils.PyEnvironmentMock(final_state=3)\n    env2 = driver_test_utils.PyEnvironmentMock(final_state=4)\n    env = batched_py_environment.BatchedPyEnvironment([env1, env2])\n\n    policy = driver_test_utils.PyPolicyMock(\n        env.time_step_spec(),\n        env.action_spec(),\n        initial_policy_state=np.array([1, 2]))\n    replay_buffer_observer = MockReplayBufferObserver()\n\n    driver = py_driver.PyDriver(\n        env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=max_steps,\n        max_episodes=max_episodes,\n    )\n    initial_time_step = env.reset()\n    initial_policy_state = policy.get_initial_state()\n    driver.run(initial_time_step, initial_policy_state)\n    trajectories = replay_buffer_observer.gather_all()\n\n    self.assertEqual(\n        len(trajectories), len(expected_trajectories[:expected_length]))\n\n    for t1, t2 in zip(trajectories, expected_trajectories[:expected_length]):\n      for t1_field, t2_field in zip(t1, t2):\n        self.assertAllEqual(t1_field, t2_field)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/drivers/test_utils.py,33,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common mock env and policy for testing drivers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.environments import py_environment\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\n\ndef make_replay_buffer(policy):\n  """"""Default replay buffer factory.""""""\n  return tf_uniform_replay_buffer.TFUniformReplayBuffer(\n      policy.trajectory_spec, batch_size=1)\n\n\nclass PyEnvironmentMock(py_environment.PyEnvironment):\n  """"""Dummy Blackjack-like environment that increments `state` by `action`.\n\n  The environment resets when state becomes greater or equal than final_state.\n  Actions are 1 or 2.\n  A reward of 1 for all non restart states.\n  """"""\n\n  def __init__(self, final_state=3):\n    self._state = np.int32(0)\n    self._action_spec = specs.BoundedArraySpec([],\n                                               np.int32,\n                                               minimum=1,\n                                               maximum=2,\n                                               name=\'action\')\n    self._observation_spec = specs.ArraySpec([], np.int32, name=\'observation\')\n    self._final_state = final_state\n    super(PyEnvironmentMock, self).__init__()\n\n  @property\n  def batched(self):\n    return False\n\n  def _reset(self):\n    self._state = np.int32(0)\n    return ts.restart(self._state)\n\n  def _step(self, action):\n    if action < self._action_spec.minimum or action > self._action_spec.maximum:\n      raise ValueError(\'Action should be in [{0}, {1}], but saw: {2}\'.format(\n          self._action_spec.minimum, self._action_spec.maximum, action))\n    if action.shape != ():  # pylint: disable=g-explicit-bool-comparison\n      raise ValueError(\'Action should be a scalar.\')\n\n    if self._state >= self._final_state:\n      # Start a new episode. Ignore action\n      return self.reset()\n\n    self._state += action\n    self._state = np.int32(self._state)\n    if self._state < self._final_state:\n      return ts.transition(self._state, 1.)\n    else:\n      return ts.termination(self._state, 1.)\n\n  def action_spec(self):\n    return self._action_spec\n\n  def observation_spec(self):\n    return self._observation_spec\n\n\nclass TFPolicyMock(tf_policy.TFPolicy):\n  """"""Mock policy takes actions 1 and 2, alternating.""""""\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               batch_size=1,\n               policy_state_spec_name=\'policy_state_spec\',\n               policy_state_name=\'policy_state\',\n               initial_policy_state=None):\n    batch_shape = (batch_size,)\n    self._batch_shape = batch_shape\n    minimum = np.asarray(1, dtype=np.int32)\n    maximum = np.asarray(2, dtype=np.int32)\n    self._maximum = maximum\n    policy_state_spec = specs.BoundedTensorSpec((),\n                                                tf.int32,\n                                                minimum=minimum,\n                                                maximum=maximum,\n                                                name=policy_state_spec_name)\n    info_spec = action_spec\n    self._policy_state = common.create_variable(\n        name=policy_state_name,\n        initial_value=maximum,\n        shape=batch_shape,\n        dtype=tf.int32)\n    if initial_policy_state is None:\n      self._initial_policy_state = tf.fill([batch_size],\n                                           tf.constant(0, tf.int32))\n    else:\n      self._initial_policy_state = initial_policy_state\n\n    super(TFPolicyMock, self).__init__(time_step_spec, action_spec,\n                                       policy_state_spec, info_spec)\n\n  def _get_initial_state(self, batch_size):\n    return self._initial_policy_state\n\n  def _action(self, time_step, policy_state, seed):\n    del seed\n\n    # Reset the policy for batch indices that have restarted episode.\n    policy_state = tf.compat.v1.where(time_step.is_first(),\n                                      self._initial_policy_state, policy_state)\n\n    # Take actions 1 and 2 alternating.\n    action = tf.cast(tf.math.floormod(policy_state, 2) + 1, tf.int32)\n    new_policy_state = tf.cast(policy_state + tf.constant(\n        1, shape=self._batch_shape, dtype=tf.int32), tf.int32)\n    policy_info = tf.cast(action * 2, tf.int32)\n    return policy_step.PolicyStep(action, new_policy_state, policy_info)\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\'Not implemented.\')\n\n  def _variables(self):\n    return ()\n\n\nclass PyPolicyMock(py_policy.PyPolicy):\n  """"""Mock policy takes actions 1 and 2, alternating.""""""\n\n  # For batched environments, use a initial policy state of size [batch_size].\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               initial_policy_state=np.int32(2)):\n    policy_state_spec = specs.BoundedArraySpec((),\n                                               np.int32,\n                                               minimum=1,\n                                               maximum=2,\n                                               name=\'policy_state_spec\')\n    policy_info_spec = specs.BoundedArraySpec((),\n                                              np.int32,\n                                              minimum=1,\n                                              maximum=2,\n                                              name=\'policy_info_spec\')\n    self._initial_policy_state = initial_policy_state\n    super(PyPolicyMock, self).__init__(time_step_spec, action_spec,\n                                       policy_state_spec, policy_info_spec)\n\n  def _get_initial_state(self, batch_size=None):\n    return self._initial_policy_state\n\n  def _action(self, time_step, policy_state):\n    # Reset the policy when starting a new episode.\n    is_time_step_first = time_step.is_first()\n    if np.isscalar(is_time_step_first):\n      if is_time_step_first:\n        policy_state = self._initial_policy_state\n    else:\n      policy_state[is_time_step_first] = self._initial_policy_state[\n          is_time_step_first]\n\n    # Take actions 1 and 2 alternating.\n    action = (policy_state % 2) + 1\n    policy_info = np.int32(action * 2)\n    action = np.int32(action)\n    policy_state = np.int32(policy_state + 1)\n    return policy_step.PolicyStep(action, policy_state, policy_info)\n\n\nclass NumStepsObserver(object):\n  """"""Class to count number of steps run by an observer.""""""\n\n  def __init__(self, variable_scope=\'num_steps_step_observer\'):\n    with tf.compat.v1.variable_scope(variable_scope):\n      self._num_steps = common.create_variable(\n          \'num_steps\', 0, shape=[], dtype=tf.int32)\n\n  @property\n  def num_steps(self):\n    return self._num_steps\n\n  @num_steps.setter\n  def num_steps(self, num_steps):\n    self._num_steps.assign(num_steps)\n\n  def __call__(self, traj):\n    num_steps = tf.reduce_sum(\n        input_tensor=tf.cast(~traj.is_boundary(), dtype=tf.int32))\n    with tf.control_dependencies([self._num_steps.assign_add(num_steps)]):\n      return tf.nest.map_structure(tf.identity, traj)\n\n\nclass NumStepsTransitionObserver(object):\n  """"""Class to count number of steps run by an observer.""""""\n\n  def __init__(self, variable_scope=\'num_steps_step_observer\'):\n    with tf.compat.v1.variable_scope(variable_scope):\n      self._num_steps = common.create_variable(\n          \'num_steps\', 0, shape=[], dtype=tf.int32)\n\n  @property\n  def num_steps(self):\n    return self._num_steps\n\n  @num_steps.setter\n  def num_steps(self, num_steps):\n    self._num_steps.assign(num_steps)\n\n  def __call__(self, transition):\n    _, _, next_time_step = transition\n    num_steps = tf.reduce_sum(\n        input_tensor=tf.cast(~next_time_step.is_first(), dtype=tf.int32))\n    with tf.control_dependencies([self._num_steps.assign_add(num_steps)]):\n      return tf.nest.map_structure(tf.identity, transition)\n\n\nclass NumEpisodesObserver(object):\n  """"""Class to count number of episodes run by an observer.""""""\n\n  def __init__(self, variable_scope=\'num_episodes_step_observer\'):\n    with tf.compat.v1.variable_scope(variable_scope):\n      self._num_episodes = common.create_variable(\n          \'num_episodes\', 0, shape=[], dtype=tf.int32)\n\n  @property\n  def num_episodes(self):\n    return self._num_episodes\n\n  @num_episodes.setter\n  def num_episodes(self, num_episodes):\n    self._num_episodes.assign(num_episodes)\n\n  def __call__(self, traj):\n    num_episodes = tf.reduce_sum(\n        input_tensor=tf.cast(traj.is_last(), dtype=tf.int32))\n    with tf.control_dependencies([\n        self._num_episodes.assign_add(num_episodes)\n    ]):\n      return tf.nest.map_structure(tf.identity, traj)\n\n\ndef make_random_trajectory():\n  """"""Creates a random trajectory.\n\n  This trajectory contains Tensors shaped `[1, 6, ...]` where `1` is the batch\n  and `6` is the number of time steps.\n\n  Observations are unbounded but actions are bounded to take values within\n  `[1, 2]`.\n\n  Policy info is also provided, and is equal to the actions.  It can be removed\n  via:\n\n  ```python\n  traj = make_random_trajectory().clone(policy_info=())\n  ```\n\n  Returns:\n    A `Trajectory`.\n  """"""\n  time_step_spec = ts.time_step_spec(\n      tensor_spec.TensorSpec([], tf.int32, name=\'observation\'))\n  action_spec = tensor_spec.BoundedTensorSpec([],\n                                              tf.int32,\n                                              minimum=1,\n                                              maximum=2,\n                                              name=\'action\')\n  # info and policy state specs match that of TFPolicyMock.\n  outer_dims = [1, 6]  # (batch_size, time)\n  traj = trajectory.Trajectory(\n      observation=tensor_spec.sample_spec_nest(\n          time_step_spec.observation, outer_dims=outer_dims),\n      action=tensor_spec.sample_bounded_spec(\n          action_spec, outer_dims=outer_dims),\n      policy_info=tensor_spec.sample_bounded_spec(\n          action_spec, outer_dims=outer_dims),\n      reward=tf.fill(outer_dims, tf.constant(0, dtype=tf.float32)),\n      # step_type is F M L F M L.\n      step_type=tf.reshape(tf.range(0, 6) % 3, outer_dims),\n      # next_step_type is M L F M L F.\n      next_step_type=tf.reshape(tf.range(1, 7) % 3, outer_dims),\n      discount=tf.fill(outer_dims, tf.constant(1, dtype=tf.float32)),\n  )\n  return traj, time_step_spec, action_spec\n'"
tf_agents/drivers/test_utils_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.test_utils.""""""\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import test_utils\n\n\nclass TestUtilsTest(parameterized.TestCase, test_utils.TestCase):\n\n  @parameterized.named_parameters([\n      (\'BatchOfOneTrajectoryOfLengthThree\', 1, 3),\n      (\'BatchOfOneTrajectoryOfLengthSeven\', 1, 7),\n      (\'BatchOfOneTrajectoryOfLengthNine\', 1, 9),\n      (\'BatchOfTwoTrajectorieOfLengthThree\', 2, 3),\n      (\'BatchOfTwoTrajectorieOfLengthSeven\', 2, 7),\n      (\'BatchOfTwoTrajectorieOfLengthNine\', 2, 9),\n      (\'BatchOfFiveTrajectorieOfLengthThree\', 5, 3),\n      (\'BatchOfFiveTrajectorieOfLengthSeven\', 5, 7),\n      (\'BatchOfFiveTrajectorieOfLengthNine\', 5, 9)\n  ])\n  def testNumEpisodesObserverEpisodeTotal(self, batch_size, traj_len):\n    single_trajectory = np.concatenate([[ts.StepType.FIRST],\n                                        np.repeat(ts.StepType.MID,\n                                                  traj_len - 2),\n                                        [ts.StepType.LAST]])\n    next_step_type = np.tile(single_trajectory, (batch_size, 1))\n\n    traj = trajectory.Trajectory(\n        observation=np.random.rand(batch_size, traj_len),\n        action=np.random.rand(batch_size, traj_len),\n        policy_info=(),\n        reward=np.random.rand(batch_size, traj_len),\n        discount=np.ones((batch_size, traj_len)),\n        step_type=np.zeros((batch_size, traj_len)),\n        next_step_type=next_step_type)\n\n    observer = driver_test_utils.NumEpisodesObserver()\n    observer(traj)\n    self.assertEqual(observer.num_episodes, batch_size)\n'"
tf_agents/drivers/tf_driver.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Driver that steps a TF environment using a TF policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import driver\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\nfrom typing import Any, Callable, Optional, Sequence, Tuple\n\n\nclass TFDriver(driver.Driver):\n  """"""A driver that runs a TF policy in a TF environment.""""""\n\n  def __init__(\n      self,\n      env: tf_environment.TFEnvironment,\n      policy: tf_policy.TFPolicy,\n      observers: Sequence[Callable[[trajectory.Trajectory], Any]],\n      transition_observers: Optional[Sequence[Callable[[types.Transition],\n                                                       Any]]] = None,\n      max_steps: Optional[types.Int] = None,\n      max_episodes: Optional[types.Int] = None,\n      disable_tf_function: bool = False):\n    """"""A driver that runs a TF policy in a TF environment.\n\n    Args:\n      env: A tf_environment.Base environment.\n      policy: A tf_policy.TFPolicy policy.\n      observers: A list of observers that are notified after every step\n        in the environment. Each observer is a callable(trajectory.Trajectory).\n      transition_observers: A list of observers that are updated after every\n        step in the environment. Each observer is a callable((TimeStep,\n        PolicyStep, NextTimeStep)). The transition is shaped just as\n        trajectories are for regular observers.\n      max_steps: Optional maximum number of steps for each run() call.\n        Also see below.  Default: 0.\n      max_episodes: Optional maximum number of episodes for each run() call.\n        At least one of max_steps or max_episodes must be provided. If both\n        are set, run() terminates when at least one of the conditions is\n        satisfied.  Default: 0.\n      disable_tf_function: If True the use of tf.function for the run method is\n        disabled.\n\n    Raises:\n      ValueError: If both max_steps and max_episodes are None.\n    """"""\n    common.check_tf1_allowed()\n    max_steps = max_steps or 0\n    max_episodes = max_episodes or 0\n    if max_steps < 1 and max_episodes < 1:\n      raise ValueError(\n          \'Either `max_steps` or `max_episodes` should be greater than 0.\')\n\n    super(TFDriver, self).__init__(env, policy, observers, transition_observers)\n\n    self._max_steps = max_steps or np.inf\n    self._max_episodes = max_episodes or np.inf\n\n    if not disable_tf_function:\n      self.run = common.function(self.run, autograph=True)\n\n  def run(\n      self, time_step: ts.TimeStep,\n      policy_state: types.NestedTensor = ()\n  ) -> Tuple[ts.TimeStep, types.NestedTensor]:\n    """"""Run policy in environment given initial time_step and policy_state.\n\n    Args:\n      time_step: The initial time_step.\n      policy_state: The initial policy_state.\n\n    Returns:\n      A tuple (final time_step, final policy_state).\n    """"""\n    num_steps = tf.constant(0.0)\n    num_episodes = tf.constant(0.0)\n\n    while num_steps < self._max_steps and num_episodes < self._max_episodes:\n      action_step = self.policy.action(time_step, policy_state)\n      next_time_step = self.env.step(action_step.action)\n\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n      for observer in self._transition_observers:\n        observer((time_step, action_step, next_time_step))\n      for observer in self.observers:\n        observer(traj)\n\n      num_episodes += tf.math.reduce_sum(tf.cast(traj.is_last(), tf.float32))\n      num_steps += tf.math.reduce_sum(tf.cast(~traj.is_boundary(), tf.float32))\n\n      time_step = next_time_step\n      policy_state = action_step.state\n\n    return time_step, policy_state\n'"
tf_agents/drivers/tf_driver_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.tf_driver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.drivers import tf_driver\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import test_utils\n\n\nclass MockReplayBufferObserver(object):\n\n  def __init__(self):\n    self._trajectories = []\n\n  def __call__(self, trajectory_):\n\n    def _add_trajectory(*t):\n      self._trajectories.append(tf.nest.pack_sequence_as(trajectory_, t))\n\n    tf.numpy_function(_add_trajectory, tf.nest.flatten(trajectory_), [])\n\n  def gather_all(self):\n    return self._trajectories\n\n\nclass TFDriverTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(TFDriverTest, self).setUp()\n    f0 = np.array(0., dtype=np.float32)\n    f1 = np.array(1., dtype=np.float32)\n\n    # Order of args for trajectory methods:\n    # (observation, action, policy_info, reward, discount)\n    trajectories = [\n        trajectory.first(0, 1, 2, f1, f1),\n        trajectory.last(1, 2, 4, f1, f0),\n        trajectory.boundary(3, 1, 2, f0, f1),\n        trajectory.first(0, 1, 2, f1, f1),\n        trajectory.last(1, 2, 4, f1, f0),\n        trajectory.boundary(3, 1, 2, f0, f1),\n        trajectory.first(0, 1, 2, f1, f1),\n    ]\n    self._trajectories = nest_utils.batch_nested_array(trajectories)\n\n  @parameterized.named_parameters([\n      (\'NoneStepsTwoEpisodes\', None, 2, 5),\n      (\'TwoStepsTwoEpisodes\', 2, 2, 2),\n      (\'FourStepsTwoEpisodes\', 4, 2, 5),\n      (\'FourStepsOneEpisodes\', 4, 1, 2),\n      (\'FourStepsNoneEpisodes\', 4, None, 5),\n  ])\n  def testRunOnce(self, max_steps, max_episodes, expected_steps):\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    replay_buffer_observer = MockReplayBufferObserver()\n    transition_replay_buffer_observer = MockReplayBufferObserver()\n    driver = tf_driver.TFDriver(\n        tf_env,\n        policy,\n        observers=[replay_buffer_observer],\n        transition_observers=[transition_replay_buffer_observer],\n        max_steps=max_steps,\n        max_episodes=max_episodes)\n\n    initial_time_step = tf_env.reset()\n    initial_policy_state = policy.get_initial_state(batch_size=1)\n    self.evaluate(driver.run(initial_time_step, initial_policy_state))\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:expected_steps])\n\n    transitions = transition_replay_buffer_observer.gather_all()\n    self.assertLen(transitions, expected_steps)\n    # TimeStep, Action, NextTimeStep\n    self.assertLen(transitions[0], 3)\n\n  def testMultipleRunMaxSteps(self):\n    num_steps = 3\n    num_expected_steps = 4\n\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    replay_buffer_observer = MockReplayBufferObserver()\n    driver = tf_driver.TFDriver(\n        tf_env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=1,\n        max_episodes=None,\n    )\n\n    time_step = tf_env.reset()\n    policy_state = policy.get_initial_state(batch_size=1)\n    for _ in range(num_steps):\n      time_step, policy_state = self.evaluate(\n          driver.run(time_step, policy_state))\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:num_expected_steps])\n\n  def testMultipleRunMaxEpisodes(self):\n    num_episodes = 2\n    num_expected_steps = 5\n\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    replay_buffer_observer = MockReplayBufferObserver()\n    driver = tf_driver.TFDriver(\n        tf_env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=None,\n        max_episodes=1,\n    )\n\n    time_step = tf_env.reset()\n    policy_state = policy.get_initial_state(batch_size=1)\n    for _ in range(num_episodes):\n      time_step, policy_state = self.evaluate(\n          driver.run(time_step, policy_state))\n    trajectories = replay_buffer_observer.gather_all()\n    self.assertEqual(trajectories, self._trajectories[:num_expected_steps])\n\n  @parameterized.named_parameters([\n      (\'NoneStepsNoneEpisodes\', None, None),\n      (\'ZeroStepsNoneEpisodes\', 0, None),\n      (\'NoneStepsZeroEpisodes\', None, 0),\n      (\'ZeroStepsZeroEpisodes\', 0, 0),\n  ])\n  def testValueErrorOnInvalidArgs(self, max_steps, max_episodes):\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    replay_buffer_observer = MockReplayBufferObserver()\n    with self.assertRaises(ValueError):\n      tf_driver.TFDriver(\n          tf_env,\n          policy,\n          observers=[replay_buffer_observer],\n          max_steps=max_steps,\n          max_episodes=max_episodes,\n      )\n\n  @parameterized.named_parameters([\n      (\'FourStepsNoneEpisodesBoundaryNotCounted\', 4, None, 2),\n      (\'FiveStepsNoneEpisodesBoundaryNotCounted\', 5, None, 3),\n      (\'NoneStepsTwoEpisodesBoundaryNotCounted\', None, 2, 3),\n      (\'TwoStepsTwoEpisodesBoundaryNotCounted\', 2, 2, 1),\n      (\'FourStepsTwoEpisodesBoundaryNotCounted\', 4, 2, 2),\n  ])\n  def testBatchedEnvironment(self, max_steps, max_episodes, expected_length):\n\n    expected_trajectories = [\n        trajectory.Trajectory(\n            step_type=np.array([0, 0]),\n            observation=np.array([0, 0]),\n            action=np.array([2, 1]),\n            policy_info=np.array([4, 2]),\n            next_step_type=np.array([1, 1]),\n            reward=np.array([1., 1.]),\n            discount=np.array([1., 1.])),\n        trajectory.Trajectory(\n            step_type=np.array([1, 1]),\n            observation=np.array([2, 1]),\n            action=np.array([1, 2]),\n            policy_info=np.array([2, 4]),\n            next_step_type=np.array([2, 1]),\n            reward=np.array([1., 1.]),\n            discount=np.array([0., 1.])),\n        trajectory.Trajectory(\n            step_type=np.array([2, 1]),\n            observation=np.array([3, 3]),\n            action=np.array([2, 1]),\n            policy_info=np.array([4, 2]),\n            next_step_type=np.array([0, 2]),\n            reward=np.array([0., 1.]),\n            discount=np.array([1., 0.]))\n    ]\n\n    env1 = driver_test_utils.PyEnvironmentMock(final_state=3)\n    env2 = driver_test_utils.PyEnvironmentMock(final_state=4)\n    env = batched_py_environment.BatchedPyEnvironment([env1, env2])\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n\n    policy = driver_test_utils.TFPolicyMock(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        batch_size=2,\n        initial_policy_state=tf.constant([1, 2], dtype=tf.int32))\n\n    replay_buffer_observer = MockReplayBufferObserver()\n\n    driver = tf_driver.TFDriver(\n        tf_env,\n        policy,\n        observers=[replay_buffer_observer],\n        max_steps=max_steps,\n        max_episodes=max_episodes,\n    )\n    initial_time_step = tf_env.reset()\n    initial_policy_state = tf.constant([1, 2], dtype=tf.int32)\n    self.evaluate(driver.run(initial_time_step, initial_policy_state))\n    trajectories = replay_buffer_observer.gather_all()\n\n    self.assertEqual(\n        len(trajectories), len(expected_trajectories[:expected_length]))\n\n    for t1, t2 in zip(trajectories, expected_trajectories[:expected_length]):\n      for t1_field, t2_field in zip(t1, t2):\n        self.assertAllEqual(t1_field, t2_field)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Environments module.""""""\n\n# TODO(b/155801943): Bring parallel_py_environment here once we\'re py3-only.\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.environments import random_tf_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import trajectory_replay\nfrom tf_agents.environments import utils\nfrom tf_agents.environments import wrappers\n'"
tf_agents/environments/atari_preprocessing.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A class implementing minimal Atari 2600 preprocessing.\n\nAdapted from Dopamine.\n\nhttps://github.com/google/dopamine/blob/master/dopamine/discrete_domains/atari_lib.py\n\nThis includes:\n  . Emitting a terminal signal when losing a life (optional).\n  . Frame skipping and color pooling.\n  . Resizing the image before it is provided to the agent.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport gin\nimport gym\nfrom gym import core as gym_core\nfrom gym.spaces import box\nimport numpy as np\nimport cv2\n\n\n@gin.configurable\nclass AtariPreprocessing(gym_core.Wrapper):\n  """"""A class implementing image preprocessing for Atari 2600 agents.\n\n  Specifically, this provides the following subset from the JAIR paper\n  (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\n    * Frame skipping (defaults to 4).\n    * Terminal signal when a life is lost (off by default).\n    * Grayscale and max-pooling of the last two frames.\n    * Downsample the screen to a square image (defaults to 84x84).\n\n  More generally, this class follows the preprocessing guidelines set down in\n  Machado et al. (2018), ""Revisiting the Arcade Learning Environment:\n  Evaluation Protocols and Open Problems for General Agents"".\n  """"""\n\n  def __init__(self,\n               env: gym.Env,\n               frame_skip: int = 4,\n               terminal_on_life_loss: bool = False,\n               screen_size: int = 84):\n    """"""Constructor for an Atari 2600 preprocessor.\n\n    Args:\n      env: Gym environment whose observations are preprocessed.\n      frame_skip: int, the frequency at which the agent experiences the game.\n      terminal_on_life_loss: bool, If True, the step() method returns\n        is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n      screen_size: int, size of a resized Atari 2600 frame.\n\n    Raises:\n      ValueError: if frame_skip or screen_size are not strictly positive.\n    """"""\n    super(AtariPreprocessing, self).__init__(env)\n\n    # Return the observation space adjusted to match the shape of the processed\n    # observations.\n    self.observation_space = box.Box(\n        low=0,\n        high=255,\n        shape=(screen_size, screen_size, 1),\n        dtype=np.uint8)\n\n    if frame_skip <= 0:\n      raise ValueError(\n          \'Frame skip should be strictly positive, got {}\'.format(frame_skip))\n    if screen_size <= 0:\n      raise ValueError(\'Target screen size should be strictly positive, got {}\'\n                       .format(screen_size))\n\n    self.terminal_on_life_loss = terminal_on_life_loss\n    self.frame_skip = frame_skip\n    self.screen_size = screen_size\n\n    obs_dims = self.env.observation_space\n    # Stores temporary observations used for pooling over two successive\n    # frames.\n    self.screen_buffer = [\n        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n    ]\n\n    self.game_over = False\n    self.lives = 0  # Will need to be set by reset().\n\n  def reset(self) -> np.ndarray:\n    """"""Resets the environment.\n\n    Returns:\n      observation: numpy array, the initial observation emitted by the\n        environment.\n    """"""\n    self.env.reset()\n    self.lives = self.env.ale.lives()\n    self.game_over = False\n    self._fetch_grayscale_observation(self.screen_buffer[0])\n    self.screen_buffer[1].fill(0)\n    return self._pool_and_resize()\n\n  def step(self, action: np.ndarray) -> np.ndarray:\n    """"""Applies the given action in the environment.\n\n    Remarks:\n\n      * If a terminal state (from life loss or episode end) is reached, this may\n        execute fewer than self.frame_skip steps in the environment.\n      * Furthermore, in this case the returned observation may not contain valid\n        image data and should be ignored.\n\n    Args:\n      action: The action to be executed.\n\n    Returns:\n      observation: numpy array, the observation following the action.\n      reward: float, the reward following the action.\n      is_terminal: bool, whether the environment has reached a terminal state.\n        This is true when a life is lost and terminal_on_life_loss, or when the\n        episode is over.\n      info: Gym API\'s info data structure.\n    """"""\n    accumulated_reward = 0.\n\n    for time_step in range(self.frame_skip):\n      # We bypass the Gym observation altogether and directly fetch the\n      # grayscale image from the ALE. This is a little faster.\n      _, reward, game_over, info = self.env.step(action)\n      accumulated_reward += reward\n\n      if self.terminal_on_life_loss:\n        new_lives = self.env.ale.lives()\n        is_terminal = game_over or new_lives < self.lives\n        self.lives = new_lives\n      else:\n        is_terminal = game_over\n\n      if is_terminal:\n        break\n      # We max-pool over the last two frames, in grayscale.\n      elif time_step >= self.frame_skip - 2:\n        # When frame_skip==1, taking a max ensures that it\'s still\n        # screen_buffer[0] that holds the fetched observation\n        t = time_step - max(self.frame_skip - 2, 0)\n        self._fetch_grayscale_observation(self.screen_buffer[t])\n\n    # Pool the last two observations.\n    observation = self._pool_and_resize()\n\n    self.game_over = game_over\n    return observation, accumulated_reward, is_terminal, info\n\n  def _fetch_grayscale_observation(self, output):\n    """"""Returns the current observation in grayscale.\n\n    The returned observation is stored in \'output\'.\n\n    Args:\n      output: numpy array, screen buffer to hold the returned observation.\n\n    Returns:\n      observation: numpy array, the current observation in grayscale.\n    """"""\n    self.env.ale.getScreenGrayscale(output)\n    return output\n\n  def _pool_and_resize(self):\n    """"""Transforms two frames into a Nature DQN observation.\n\n    For efficiency, the transformation is done in-place in self.screen_buffer.\n\n    Returns:\n      transformed_screen: numpy array, pooled, resized screen.\n    """"""\n    # Pool if there are enough screens to do so.\n    if self.frame_skip > 1:\n      np.maximum(\n          self.screen_buffer[0],\n          self.screen_buffer[1],\n          out=self.screen_buffer[0])\n\n    transformed_image = cv2.resize(\n        self.screen_buffer[0], (self.screen_size, self.screen_size),\n        interpolation=cv2.INTER_AREA)\n    int_image = np.asarray(transformed_image, dtype=np.uint8)\n    return np.expand_dims(int_image, axis=2)\n'"
tf_agents/environments/atari_preprocessing_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for dopamine.atari.preprocessing.\n\nAdapted from Dopamine.\n\nhttps://github.com/google/dopamine/blob/master/tests/dopamine/discrete_domains/atari_lib_test.py\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom gym import core as gym_core\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.environments import atari_preprocessing as preprocessing\n\n\nclass MockALE(object):\n  """"""Mock internal ALE for testing.""""""\n\n  def __init__(self):\n    self.screen_value = 0\n\n  def lives(self):\n    return 1\n\n  def getScreenGrayscale(self, screen):  # pylint: disable=invalid-name\n    screen.fill(self.screen_value)\n\n\nclass MockEnvironment(gym_core.Env):\n  """"""Mock environment for testing.""""""\n\n  def __init__(self, screen_size=10, max_steps=10):\n    self.max_steps = max_steps\n    self.screen_size = screen_size\n    self.ale = MockALE()\n    self.observation_space = np.empty((screen_size, screen_size))\n    self.action_space = np.empty((5,))\n    self.game_over = False\n\n  def reset(self):\n    self.ale.screen_value = 10\n    self.num_steps = 0\n    return self.get_observation()\n\n  def get_observation(self):\n    observation = np.empty((self.screen_size, self.screen_size))\n    return self.ale.getScreenGrayscale(observation)\n\n  def step(self, action):\n    reward = -1. if action > 0 else 1.\n    self.num_steps += 1\n    is_terminal = self.num_steps >= self.max_steps\n\n    unused = 0\n    self.ale.screen_value -= 2\n    return (self.get_observation(), reward, is_terminal, unused)\n\n  def render(self, mode):\n    pass\n\n\nclass AtariPreprocessingTest(tf.test.TestCase):\n\n  def testResetPassesObservation(self):\n    env = MockEnvironment()\n    env = preprocessing.AtariPreprocessing(env, frame_skip=1, screen_size=16)\n    observation = env.reset()\n\n    self.assertEqual(observation.shape, (16, 16, 1))\n\n  def testTerminalPassedThrough(self):\n    max_steps = 10\n    env = MockEnvironment(max_steps=max_steps)\n    env = preprocessing.AtariPreprocessing(env, frame_skip=1)\n    env.reset()\n\n    # Make sure we get the right number of steps.\n    for _ in range(max_steps - 1):\n      _, _, is_terminal, _ = env.step(0)\n      self.assertFalse(is_terminal)\n\n    _, _, is_terminal, _ = env.step(0)\n    self.assertTrue(is_terminal)\n\n  def testFrameSkipAccumulatesReward(self):\n    frame_skip = 2\n    env = MockEnvironment()\n    env = preprocessing.AtariPreprocessing(env, frame_skip=frame_skip)\n    env.reset()\n\n    # Make sure we get the right number of steps. Reward is 1 when we\n    # pass in action 0.\n    _, reward, _, _ = env.step(0)\n    self.assertEqual(reward, frame_skip)\n\n  def testMaxFramePooling(self):\n    frame_skip = 2\n    env = MockEnvironment()\n    env = preprocessing.AtariPreprocessing(env, frame_skip=frame_skip)\n    env.reset()\n\n    # The first observation is 2, the second 0; max is 2.\n    observation, _, _, _ = env.step(0)\n    self.assertTrue((observation == 8).all())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/atari_wrappers.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wrappers for Atari Environments.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Any, Text\n\nimport gym\nimport numpy as np\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.trajectories import time_step as ts\n\n\nclass FrameStack4(gym.Wrapper):\n  """"""Stack previous four frames (must be applied to Gym env, not our envs).""""""\n\n  STACK_SIZE = 4\n\n  def __init__(self, env: gym.Env):\n    super(FrameStack4, self).__init__(env)\n    self._env = env\n    self._frames = collections.deque(maxlen=FrameStack4.STACK_SIZE)\n    space = self._env.observation_space\n    shape = space.shape[0:2] + (FrameStack4.STACK_SIZE,)\n    self.observation_space = gym.spaces.Box(\n        low=0, high=255, shape=shape, dtype=np.uint8)\n\n  def __getattr__(self, name: Text) -> Any:\n    """"""Forward all other calls to the base environment.""""""\n    return getattr(self._env, name)\n\n  def _generate_observation(self):\n    return np.concatenate(self._frames, axis=2)\n\n  def reset(self) -> np.ndarray:\n    observation = self._env.reset()\n    for _ in range(FrameStack4.STACK_SIZE):\n      self._frames.append(observation)\n    return self._generate_observation()\n\n  def step(self, action: np.ndarray) -> np.ndarray:\n    observation, reward, done, info = self._env.step(action)\n    self._frames.append(observation)\n    return self._generate_observation(), reward, done, info\n\n\n# TODO(sfishman): Add tests for this wrapper.\nclass AtariTimeLimit(wrappers.PyEnvironmentBaseWrapper):\n  """"""End episodes after specified number of steps and reset after game_over.\n\n  This differs from the default TimeLimit wrapper in that it looks at the\n  game_over property before resetting. We need this to properly handle life\n  loss terminations -- the default TimeLimit wrapper would .reset() the\n  environment and the step count after such a termination, but we want the\n  environment to keep going.\n  """"""\n\n  def __init__(self, env: py_environment.PyEnvironment, duration: int):\n    super(AtariTimeLimit, self).__init__(env)\n    self._duration = duration\n    self._num_steps = 0\n\n  def _reset(self):\n    self._num_steps = 0\n    return self._env.reset()\n\n  def _step(self, action):\n    if self.game_over:\n      return self.reset()\n\n    time_step = self._env.step(action)\n\n    self._num_steps += 1\n    if self._num_steps >= self._duration:\n      time_step = time_step._replace(step_type=ts.StepType.LAST)\n\n    return time_step\n\n  @property\n  def game_over(self) -> bool:\n    return self._num_steps >= self._duration or self.gym.game_over\n\n\nclass FireOnReset(gym.Wrapper):\n  """"""Start every episode with action 1 (FIRE) + another action (2).\n\n  In some environments (e.g., BeamRider, Breakout, Tennis) nothing\n  happens until the player presses the FIRE button. This wrapper can\n  be helpful in those environments, but it is not necessary.\n  """"""\n\n  def reset(self) -> np.ndarray:\n    observation = self.env.reset()\n    # The following code is from https://github.com/openai/gym/...\n    # ...blob/master/gym/wrappers/atari_preprocessing.py\n    action_meanings = self.env.unwrapped.get_action_meanings()\n    if action_meanings[1] == \'FIRE\' and len(action_meanings) >= 3:\n      self.env.step(1)\n      observation, _, _, _ = self.env.step(2)\n    return observation\n'"
tf_agents/environments/atari_wrappers_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for environments.atari_wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing.absltest import mock\n\nfrom tf_agents.environments import atari_wrappers\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass AtariTimeLimitTest(test_utils.TestCase):\n\n  def test_game_over_after_limit(self):\n    max_steps = 5\n    base_env = mock.MagicMock()\n    wrapped_env = atari_wrappers.AtariTimeLimit(base_env, max_steps)\n\n    base_env.gym.game_over = False\n    base_env.reset.return_value = ts.restart(1)\n    base_env.step.return_value = ts.transition(2, 0)\n    action = 1\n\n    self.assertFalse(wrapped_env.game_over)\n\n    for _ in range(max_steps):\n      time_step = wrapped_env.step(action)\n      self.assertFalse(time_step.is_last())\n      self.assertFalse(wrapped_env.game_over)\n\n    time_step = wrapped_env.step(action)\n    self.assertTrue(time_step.is_last())\n    self.assertTrue(wrapped_env.game_over)\n\n  def test_resets_after_limit(self):\n    max_steps = 5\n    base_env = mock.MagicMock()\n    wrapped_env = atari_wrappers.AtariTimeLimit(base_env, max_steps)\n\n    base_env.gym.game_over = False\n    base_env.reset.return_value = ts.restart(1)\n    base_env.step.return_value = ts.transition(2, 0)\n    action = 1\n\n    for _ in range(max_steps + 1):\n      wrapped_env.step(action)\n\n    self.assertTrue(wrapped_env.game_over)\n    self.assertEqual(1, base_env.reset.call_count)\n\n    wrapped_env.step(action)\n    self.assertFalse(wrapped_env.game_over)\n    self.assertEqual(2, base_env.reset.call_count)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/batched_py_environment.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Treat multiple non-batch environments as a single batch environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\n# pylint: disable=line-too-long\n# multiprocessing.dummy provides a pure *multithreaded* threadpool that works\n# in both python2 and python3 (concurrent.futures isn\'t available in python2).\n#   https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy\nfrom multiprocessing import dummy as mp_threads\nfrom multiprocessing import pool\n# pylint: enable=line-too-long\nfrom typing import Sequence, Optional\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\n@gin.configurable\nclass BatchedPyEnvironment(py_environment.PyEnvironment):\n  """"""Batch together multiple py environments and act as a single batch.\n\n  The environments should only access shared python variables using\n  shared mutex locks (from the threading module).\n  """"""\n  # These declarations are required because their types could not be inferred\n  # in Python 2.\n  _envs = ...  # type: Sequence[py_environment.PyEnvironment]\n  _num_envs = ...  # type: int\n  _parallel_execution = ...  # type: bool\n  _observation_spec = ...  # type: types.NestedArraySpec\n  _action_spec = ...  # type: types.NestedArraySpec\n  _time_step_spec = ...  # type: ts.TimeStep\n  _pool = ...  # type: pool.ThreadPool\n\n  def __init__(self,\n               envs: Sequence[py_environment.PyEnvironment],\n               multithreading: bool = True):\n    """"""Batch together multiple (non-batched) py environments.\n\n    The environments can be different but must use the same action and\n    observation specs.\n\n    Args:\n      envs: List python environments (must be non-batched).\n      multithreading: Python bool describing whether interactions with the\n        given environments should happen in their own threadpool.  If `False`,\n        then all interaction is performed serially in the current thread.\n\n        This may be combined with wrapper `TFPyEnvironment(..., isolation=True)`\n        to ensure that multiple environments are all run in the same thread.\n\n    Raises:\n      ValueError: If envs is not a list or tuple, or is zero length, or if\n        one of the envs is already batched.\n      ValueError: If the action or observation specs don\'t match.\n    """"""\n    if not isinstance(envs, (list, tuple)):\n      raise ValueError(""envs must be a list or tuple.  Got: %s"" % envs)\n    batched_envs = [(i, env) for i, env in enumerate(envs) if env.batched]\n    if batched_envs:\n      raise ValueError(\n          ""Some of the envs are already batched: %s"" % batched_envs)\n    self._parallel_execution = multithreading\n    self._envs = envs\n    self._num_envs = len(envs)\n    self._action_spec = self._envs[0].action_spec()\n    self._observation_spec = self._envs[0].observation_spec()\n    self._time_step_spec = self._envs[0].time_step_spec()\n    if any(env.action_spec() != self._action_spec for env in self._envs):\n      raise ValueError(\n          ""All environments must have the same action spec.  Saw: %s"" %\n          [env.action_spec() for env in self._envs])\n    if any(env.time_step_spec() != self._time_step_spec for env in self._envs):\n      raise ValueError(\n          ""All environments must have the same time_step_spec.  Saw: %s"" %\n          [env.time_step_spec() for env in self._envs])\n    # Create a multiprocessing threadpool for execution.\n    if multithreading:\n      self._pool = mp_threads.Pool(self._num_envs)\n    super(BatchedPyEnvironment, self).__init__()\n\n  def _execute(self, fn, iterable):\n    if self._parallel_execution:\n      return self._pool.map(fn, iterable)\n    else:\n      return [fn(x) for x in iterable]\n\n  @property\n  def batched(self) -> bool:\n    return True\n\n  @property\n  def batch_size(self) -> Optional[int]:\n    return len(self._envs)\n\n  @property\n  def envs(self) -> Sequence[py_environment.PyEnvironment]:\n    return self._envs\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._action_spec\n\n  def time_step_spec(self) -> ts.TimeStep:\n    return self._time_step_spec\n\n  def get_info(self) -> types.NestedArray:\n    if self._num_envs == 1:\n      return nest_utils.batch_nested_array(self._envs[0].get_info())\n    else:\n      infos = self._execute(lambda env: env.get_info(), self._envs)\n      return nest_utils.stack_nested_arrays(infos)\n\n  def _reset(self):\n    """"""Reset all environments and combine the resulting observation.\n\n    Returns:\n      Time step with batch dimension.\n    """"""\n    if self._num_envs == 1:\n      return nest_utils.batch_nested_array(self._envs[0].reset())\n    else:\n      time_steps = self._execute(lambda env: env.reset(), self._envs)\n      return nest_utils.stack_nested_arrays(time_steps)\n\n  def _step(self, actions):\n    """"""Forward a batch of actions to the wrapped environments.\n\n    Args:\n      actions: Batched action, possibly nested, to apply to the environment.\n\n    Raises:\n      ValueError: Invalid actions.\n\n    Returns:\n      Batch of observations, rewards, and done flags.\n    """"""\n\n    if self._num_envs == 1:\n      actions = nest_utils.unbatch_nested_array(actions)\n      time_steps = self._envs[0].step(actions)\n      return nest_utils.batch_nested_array(time_steps)\n    else:\n      unstacked_actions = unstack_actions(actions)\n      if len(unstacked_actions) != self.batch_size:\n        raise ValueError(\n            ""Primary dimension of action items does not match ""\n            ""batch size: %d vs. %d"" % (len(unstacked_actions), self.batch_size))\n      time_steps = self._execute(\n          lambda env_action: env_action[0].step(env_action[1]),\n          zip(self._envs, unstacked_actions))\n      return nest_utils.stack_nested_arrays(time_steps)\n\n  def render(self, mode=""rgb_array"") -> Optional[types.NestedArray]:\n    if self._num_envs == 1:\n      img = self._envs[0].render(mode)\n      return nest_utils.batch_nested_array(img)\n    else:\n      imgs = self._execute(lambda env: env.render(mode), self._envs)\n      return nest_utils.stack_nested_arrays(imgs)\n\n  def close(self) -> None:\n    """"""Send close messages to the external process and join them.""""""\n    self._execute(lambda env: env.close(), self._envs)\n    if self._parallel_execution:\n      self._pool.close()\n      self._pool.join()\n\n\ndef unstack_actions(batched_actions: types.NestedArray) -> types.NestedArray:\n  """"""Returns a list of actions from potentially nested batch of actions.""""""\n  flattened_actions = tf.nest.flatten(batched_actions)\n  unstacked_actions = [\n      tf.nest.pack_sequence_as(batched_actions, actions)\n      for actions in zip(*flattened_actions)\n  ]\n  return unstacked_actions\n'"
tf_agents/environments/batched_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the parallel environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\n\nCOMMON_PARAMETERS = (dict(multithreading=False), dict(multithreading=True))\n\n\nclass GymWrapperEnvironmentMock(random_py_environment.RandomPyEnvironment):\n\n  def __init__(self, *args, **kwargs):\n    super(GymWrapperEnvironmentMock, self).__init__(*args, **kwargs)\n    self._info = {}\n\n  def get_info(self):\n    return self._info\n\n  def _step(self, action):\n    self._info[\'last_action\'] = action\n    return super(GymWrapperEnvironmentMock, self)._step(action)\n\n\nclass BatchedPyEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  @property\n  def action_spec(self):\n    return array_spec.BoundedArraySpec(\n        [7], dtype=np.float32, minimum=-1.0, maximum=1.0)\n\n  @property\n  def observation_spec(self):\n    return array_spec.ArraySpec((3, 3), np.float32)\n\n  def _make_batched_py_environment(self, multithreading, num_envs=3):\n    self.time_step_spec = ts.time_step_spec(self.observation_spec)\n    constructor = functools.partial(random_py_environment.RandomPyEnvironment,\n                                    self.observation_spec, self.action_spec)\n    return batched_py_environment.BatchedPyEnvironment(\n        envs=[constructor() for _ in range(num_envs)],\n        multithreading=multithreading)\n\n  def _make_batched_mock_gym_py_environment(self, multithreading, num_envs=3):\n    self.time_step_spec = ts.time_step_spec(self.observation_spec)\n    constructor = functools.partial(GymWrapperEnvironmentMock,\n                                    self.observation_spec, self.action_spec)\n    return batched_py_environment.BatchedPyEnvironment(\n        envs=[constructor() for _ in range(num_envs)],\n        multithreading=multithreading)\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def test_close_no_hang_after_init(self, multithreading):\n    env = self._make_batched_py_environment(multithreading)\n    env.close()\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def test_get_specs(self, multithreading):\n    env = self._make_batched_py_environment(multithreading)\n    self.assertEqual(self.observation_spec, env.observation_spec())\n    self.assertEqual(self.time_step_spec, env.time_step_spec())\n    self.assertEqual(self.action_spec, env.action_spec())\n\n    env.close()\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def test_get_info_gym_env(self, multithreading):\n    num_envs = 5\n    rng = np.random.RandomState()\n    gym_env = self._make_batched_mock_gym_py_environment(\n        multithreading, num_envs=num_envs)\n    gym_env.reset()\n    info = gym_env.get_info()\n    self.assertEqual(info, {})\n    action = np.stack([\n        array_spec.sample_bounded_spec(self.action_spec, rng)\n        for _ in range(num_envs)\n    ])\n    gym_env.step(action)\n    info = gym_env.get_info()\n    self.assertAllEqual(info[\'last_action\'], action)\n    gym_env.close()\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def test_step(self, multithreading):\n    num_envs = 5\n    env = self._make_batched_py_environment(multithreading, num_envs=num_envs)\n    action_spec = env.action_spec()\n    observation_spec = env.observation_spec()\n    rng = np.random.RandomState()\n    action = np.stack([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n    env.reset()\n\n    # Take one step and assert observation is batched the right way.\n    time_step = env.step(action)\n    self.assertEqual(num_envs, time_step.observation.shape[0])\n    self.assertAllEqual(observation_spec.shape, time_step.observation.shape[1:])\n    self.assertEqual(num_envs, action.shape[0])\n    self.assertAllEqual(action_spec.shape, action.shape[1:])\n\n    # Take another step and assert that observations have the same shape.\n    time_step2 = env.step(action)\n    self.assertAllEqual(time_step.observation.shape,\n                        time_step2.observation.shape)\n    env.close()\n\n  def test_unstack_actions(self):\n    num_envs = 5\n    action_spec = self.action_spec\n    rng = np.random.RandomState()\n    batched_action = np.array([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n\n    # Test that actions are correctly unstacked when just batched in np.array.\n    unstacked_actions = batched_py_environment.unstack_actions(batched_action)\n    for action in unstacked_actions:\n      self.assertAllEqual(action_spec.shape, action.shape)\n\n  def test_unstack_nested_actions(self):\n    num_envs = 5\n    action_spec = self.action_spec\n    rng = np.random.RandomState()\n    batched_action = np.array([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n\n    # Test that actions are correctly unstacked when nested in namedtuple.\n    class NestedAction(\n        collections.namedtuple(\'NestedAction\', [\'action\', \'other_var\'])):\n      pass\n\n    nested_action = NestedAction(\n        action=batched_action, other_var=np.array([13.0] * num_envs))\n    unstacked_actions = batched_py_environment.unstack_actions(nested_action)\n    for nested_action in unstacked_actions:\n      self.assertAllEqual(action_spec.shape, nested_action.action.shape)\n      self.assertEqual(13.0, nested_action.other_var)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/dm_control_wrapper.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wrapper providing a PyEnvironmentBase adapter for Gym environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import wrappers\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\n\n_as_float32_array = functools.partial(np.asarray, dtype=np.float32)\n\n\ndef _maybe_float32(o):\n  if o.dtype == np.float64:\n    return _as_float32_array(o)\n  return o\n\n\ndef convert_time_step(time_step):\n  """"""Convert to agents time_step type as the __hash__ method is different.""""""\n  reward = time_step.reward\n  if reward is None:\n    reward = 0.0\n  discount = time_step.discount\n  if discount is None:\n    discount = 1.0\n\n  observation = tf.nest.map_structure(_maybe_float32, time_step.observation)\n  return ts.TimeStep(\n      ts.StepType(time_step.step_type),\n      _as_float32_array(reward),\n      _as_float32_array(discount),\n      observation,\n  )\n\n\ndef convert_spec(spec):\n  if hasattr(spec, \'minimum\') and hasattr(spec, \'maximum\'):\n    tfa_spec = array_spec.BoundedArraySpec.from_spec(spec)\n  else:\n    tfa_spec = array_spec.ArraySpec.from_spec(spec)\n\n  if tfa_spec.dtype == np.float64:\n    tfa_spec = tfa_spec.replace(dtype=np.float32)\n  return tfa_spec\n\n\nclass DmControlWrapper(wrappers.PyEnvironmentBaseWrapper):\n  """"""Base wrapper forwarding the DM control types into the tf_agents ones.""""""\n\n  def __init__(self, env, render_kwargs=None):\n    super(DmControlWrapper, self).__init__(env)\n    render_kwargs = render_kwargs or {}\n    self._render_kwargs = render_kwargs\n\n    self._observation_spec = tf.nest.map_structure(convert_spec,\n                                                   self._env.observation_spec())\n    self._action_spec = tf.nest.map_structure(convert_spec,\n                                              self._env.action_spec())\n\n  @property\n  def physics(self):\n    return self._env.physics\n\n  def _reset(self):\n    return convert_time_step(self._env.reset())\n\n  def _step(self, action):\n    action = tf.nest.map_structure(lambda a, s: np.asarray(a, dtype=s.dtype),\n                                   action, self._env.action_spec())\n    return convert_time_step(self._env.step(action))\n\n  def observation_spec(self):\n    return self._observation_spec\n\n  def action_spec(self):\n    return self._action_spec\n\n  def close(self):\n    self._env.close()\n\n  def render(self, mode=\'rgb_array\'):\n    if mode != \'rgb_array\':\n      raise ValueError(\'Only rgb_array rendering mode is supported. Got %s\' %\n                       mode)\n    return self._env.physics.render(**self._render_kwargs)\n'"
tf_agents/environments/dm_control_wrapper_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.google.environments.dm_control_wrapper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\n\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.utils import test_utils\n\n\nclass DmControlWrapperTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(DmControlWrapperTest, self).setUp()\n    if not suite_dm_control.is_available():\n      self.skipTest(\'dm_control is not available.\')\n\n  def test_wrapped_cartpole_specs(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n\n    action_spec = env.action_spec()\n    self.assertEqual((2,), action_spec.shape)\n    np.testing.assert_array_almost_equal([-1.0, -1.0], action_spec.minimum)\n    np.testing.assert_array_almost_equal([1.0, 1.0], action_spec.maximum)\n\n    observation_spec = env.observation_spec()\n    self.assertEqual((4,), observation_spec[\'position\'].shape)\n    self.assertEqual((4,), observation_spec[\'velocity\'].shape)\n\n  def test_reset(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n\n    first_time_step = env.reset()\n    self.assertTrue(first_time_step.is_first())\n    self.assertEqual(0.0, first_time_step.reward)\n    self.assertEqual(1.0, first_time_step.discount)\n\n  def test_transition(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    env.reset()\n    transition_time_step = env.step(np.array([0, 0]))\n\n    self.assertTrue(transition_time_step.is_mid())\n    self.assertNotEqual(None, transition_time_step.reward)\n    self.assertEqual(1.0, transition_time_step.discount)\n\n  def test_wrapped_cartpole_final(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    time_step = env.reset()\n\n    while not time_step.is_last():\n      time_step = env.step(np.array([1, 1]))\n\n    self.assertTrue(time_step.is_last())\n    self.assertNotEqual(None, time_step.reward)\n    # Discount is 1.0 as it\'s an infinite horizon task that DM is terminating\n    # early.\n    self.assertEqual(1.0, time_step.discount)\n\n  def test_automatic_reset_after_create(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n\n    first_time_step = env.step(np.array([0, 0]))\n    self.assertTrue(first_time_step.is_first())\n\n  def test_automatic_reset_after_done(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    time_step = env.reset()\n\n    while not time_step.is_last():\n      time_step = env.step(np.array([0, 0]))\n\n    self.assertTrue(time_step.is_last())\n    first_time_step = env.step(np.array([0, 0]))\n    self.assertTrue(first_time_step.is_first())\n\n  def test_automatic_reset_after_done_not_using_reset_directly(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    time_step = env.step(np.array([0, 0]))\n\n    while not time_step.is_last():\n      time_step = env.step(np.array([0, 0]))\n\n    self.assertTrue(time_step.is_last())\n    first_time_step = env.step(np.array([0, 0]))\n    self.assertTrue(first_time_step.is_first())\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/gym_wrapper.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wrapper providing a PyEnvironmentBase adapter for Gym environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Any, Dict, Optional, Text\n\nimport gym\nimport gym.spaces\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.environments import py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ndef spec_from_gym_space(space: gym.Space,\n                        dtype_map: Optional[Dict[gym.Space, np.dtype]] = None,\n                        simplify_box_bounds: bool = True,\n                        name: Optional[Text] = None) -> specs.BoundedArraySpec:\n  """"""Converts gym spaces into array specs.\n\n  Gym does not properly define dtypes for spaces. By default all spaces set\n  their type to float64 even though observations do not always return this type.\n  See:\n  https://github.com/openai/gym/issues/527\n\n  To handle this we allow a dtype_map for setting default types for mapping\n  spaces to specs.\n\n  TODO(oars): Support using different dtypes for different parts of the\n  observations. Not sure that we have a need for this yet.\n\n  Args:\n    space: gym.Space to turn into a spec.\n    dtype_map: A dict from spaces to dtypes to use as the default dtype.\n    simplify_box_bounds: Whether to replace bounds of Box space that are arrays\n      with identical values with one number and rely on broadcasting.\n    name: Name of the spec.\n\n  Returns:\n    A BoundedArraySpec nest mirroring the given space structure.\n  Raises:\n    ValueError: If there is an unknown space type.\n  """"""\n  if dtype_map is None:\n    dtype_map = {}\n\n  # We try to simplify redundant arrays to make logging and debugging less\n  # verbose and easier to read since the printed spec bounds may be large.\n  def try_simplify_array_to_value(np_array):\n    """"""If given numpy array has all the same values, returns that value.""""""\n    first_value = np_array.item(0)\n    if np.all(np_array == first_value):\n      return np.array(first_value, dtype=np_array.dtype)\n    else:\n      return np_array\n\n  def nested_spec(spec, child_name):\n    """"""Returns the nested spec with a unique name.""""""\n    nested_name = name + \'/\' + child_name if name else child_name\n    return spec_from_gym_space(spec, dtype_map, simplify_box_bounds,\n                               nested_name)\n\n  if isinstance(space, gym.spaces.Discrete):\n    # Discrete spaces span the set {0, 1, ... , n-1} while Bounded Array specs\n    # are inclusive on their bounds.\n    maximum = space.n - 1\n    # TODO(oars): change to use dtype in space once Gym is updated.\n    dtype = dtype_map.get(gym.spaces.Discrete, np.int64)\n    return specs.BoundedArraySpec(\n        shape=(), dtype=dtype, minimum=0, maximum=maximum, name=name)\n  elif isinstance(space, gym.spaces.MultiDiscrete):\n    dtype = dtype_map.get(gym.spaces.MultiDiscrete, np.int32)\n    maximum = try_simplify_array_to_value(\n        np.asarray(space.nvec - 1, dtype=dtype))\n    return specs.BoundedArraySpec(\n        shape=space.shape, dtype=dtype, minimum=0, maximum=maximum, name=name)\n  elif isinstance(space, gym.spaces.MultiBinary):\n    dtype = dtype_map.get(gym.spaces.MultiBinary, np.int32)\n    shape = (space.n,)\n    return specs.BoundedArraySpec(\n        shape=shape, dtype=dtype, minimum=0, maximum=1, name=name)\n  elif isinstance(space, gym.spaces.Box):\n    if hasattr(space, \'dtype\') and gym.spaces.Box not in dtype_map:\n      dtype = space.dtype\n    else:\n      dtype = dtype_map.get(gym.spaces.Box, np.float32)\n    minimum = np.asarray(space.low, dtype=dtype)\n    maximum = np.asarray(space.high, dtype=dtype)\n    if simplify_box_bounds:\n      minimum = try_simplify_array_to_value(minimum)\n      maximum = try_simplify_array_to_value(maximum)\n    return specs.BoundedArraySpec(\n        shape=space.shape,\n        dtype=dtype,\n        minimum=minimum,\n        maximum=maximum,\n        name=name)\n  elif isinstance(space, gym.spaces.Tuple):\n    return tuple(\n        [nested_spec(s, \'tuple_%d\' % i) for i, s in enumerate(space.spaces)])\n  elif isinstance(space, gym.spaces.Dict):\n    return collections.OrderedDict([\n        (key, nested_spec(s, key)) for key, s in space.spaces.items()\n    ])\n  else:\n    raise ValueError(\n        \'The gym space {} is currently not supported.\'.format(space))\n\n\nclass GymWrapper(py_environment.PyEnvironment):\n  """"""Base wrapper implementing PyEnvironmentBaseWrapper interface for Gym envs.\n\n  Action and observation specs are automatically generated from the action and\n  observation spaces. See base class for py_environment.Base details.\n  """"""\n\n  def __init__(self,\n               gym_env: gym.Env,\n               discount: types.Float = 1.0,\n               spec_dtype_map: Optional[Dict[gym.Space, np.dtype]] = None,\n               match_obs_space_dtype: bool = True,\n               auto_reset: bool = True,\n               simplify_box_bounds: bool = True):\n    super(GymWrapper, self).__init__()\n\n    self._gym_env = gym_env\n    self._discount = discount\n    self._action_is_discrete = isinstance(self._gym_env.action_space,\n                                          gym.spaces.Discrete)\n    self._match_obs_space_dtype = match_obs_space_dtype\n    # TODO(sfishman): Add test for auto_reset param.\n    self._auto_reset = auto_reset\n    self._observation_spec = spec_from_gym_space(\n        self._gym_env.observation_space, spec_dtype_map, simplify_box_bounds,\n        \'observation\')\n    self._action_spec = spec_from_gym_space(self._gym_env.action_space,\n                                            spec_dtype_map, simplify_box_bounds,\n                                            \'action\')\n    self._flat_obs_spec = tf.nest.flatten(self._observation_spec)\n    self._info = None\n    self._done = True\n\n  @property\n  def gym(self) -> gym.Env:\n    return self._gym_env\n\n  def __getattr__(self, name: Text) -> Any:\n    """"""Forward all other calls to the base environment.""""""\n    gym_env = super(GymWrapper, self).__getattribute__(\'_gym_env\')\n    return getattr(gym_env, name)\n\n  def get_info(self) -> Any:\n    """"""Returns the gym environment info returned on the last step.""""""\n    return self._info\n\n  def _reset(self):\n    # TODO(oars): Upcoming update on gym adds **kwargs on reset. Update this to\n    # support that.\n    observation = self._gym_env.reset()\n    self._info = None\n    self._done = False\n\n    if self._match_obs_space_dtype:\n      observation = self._to_obs_space_dtype(observation)\n    return ts.restart(observation)\n\n  @property\n  def done(self) -> bool:\n    return self._done\n\n  def _step(self, action):\n    # Automatically reset the environments on step if they need to be reset.\n    if self._auto_reset and self._done:\n      return self.reset()\n\n    # Some environments (e.g. FrozenLake) use the action as a key to the\n    # transition probability so it has to be hashable. In the case of discrete\n    # actions we have a numpy scalar (e.g array(2)) which is not hashable\n    # in this case, we simply pull out the scalar value which will be hashable.\n    action = action.item() if self._action_is_discrete else action\n\n    # TODO(oars): Figure out how tuple or dict actions will be generated by the\n    # agents and if we can pass them through directly to gym.\n    observation, reward, self._done, self._info = self._gym_env.step(action)\n\n    if self._match_obs_space_dtype:\n      observation = self._to_obs_space_dtype(observation)\n\n    if self._done:\n      return ts.termination(observation, reward)\n    else:\n      return ts.transition(observation, reward, self._discount)\n\n  def _to_obs_space_dtype(self, observation):\n    """"""Make sure observation matches the specified space.\n\n    Observation spaces in gym didn\'t have a dtype for a long time. Now that they\n    do there is a large number of environments that do not follow the dtype in\n    the space definition. Since we use the space definition to create the\n    tensorflow graph we need to make sure observations match the expected\n    dtypes.\n\n    Args:\n      observation: Observation to match the dtype on.\n\n    Returns:\n      The observation with a dtype matching the observation spec.\n    """"""\n    # Make sure we handle cases where observations are provided as a list.\n    flat_obs = nest.flatten_up_to(self._observation_spec, observation)\n\n    matched_observations = []\n    for spec, obs in zip(self._flat_obs_spec, flat_obs):\n      matched_observations.append(np.asarray(obs, dtype=spec.dtype))\n    return tf.nest.pack_sequence_as(self._observation_spec,\n                                    matched_observations)\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._action_spec\n\n  def close(self) -> None:\n    return self._gym_env.close()\n\n  def seed(self, seed: types.Seed) -> types.Seed:\n    return self._gym_env.seed(seed)\n\n  def render(self, mode: Text = \'rgb_array\') -> Any:\n    return self._gym_env.render(mode)\n\n  # pytype: disable=attribute-error\n  def set_state(self, state: Any) -> None:\n    return self._gym_env.set_state(state)\n\n  def get_state(self) -> Any:\n    return self._gym.get_state()\n  # pytype: enable=attribute-error\n'"
tf_agents/environments/gym_wrapper_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for environments.gym_wrapper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nfrom absl.testing.absltest import mock\nimport gym\nimport gym.spaces\nimport numpy as np\n\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.utils import test_utils\n\n\nclass GymWrapperSpecTest(test_utils.TestCase):\n\n  def test_spec_from_gym_space_discrete(self):\n    discrete_space = gym.spaces.Discrete(3)\n    spec = gym_wrapper.spec_from_gym_space(discrete_space)\n\n    self.assertEqual((), spec.shape)\n    self.assertEqual(np.int64, spec.dtype)\n    self.assertEqual(0, spec.minimum)\n    self.assertEqual(2, spec.maximum)\n\n  def test_spec_from_gym_space_multi_discrete(self):\n    multi_discrete_space = gym.spaces.MultiDiscrete([1, 2, 3, 4])\n    spec = gym_wrapper.spec_from_gym_space(multi_discrete_space)\n\n    self.assertEqual((4,), spec.shape)\n    self.assertEqual(np.int32, spec.dtype)\n    np.testing.assert_array_equal(np.array([0], dtype=np.int), spec.minimum)\n    np.testing.assert_array_equal(\n        np.array([0, 1, 2, 3], dtype=np.int), spec.maximum)\n\n  def test_spec_from_gym_space_multi_binary(self):\n    multi_binary_space = gym.spaces.MultiBinary(4)\n    spec = gym_wrapper.spec_from_gym_space(multi_binary_space)\n\n    self.assertEqual((4,), spec.shape)\n    self.assertEqual(np.int32, spec.dtype)\n    np.testing.assert_array_equal(np.array([0], dtype=np.int), spec.minimum)\n    np.testing.assert_array_equal(np.array([1], dtype=np.int), spec.maximum)\n\n  def test_spec_from_gym_space_box_scalars(self):\n    for dtype in (np.float32, np.float64):\n      box_space = gym.spaces.Box(-1.0, 1.0, (3, 4), dtype=dtype)\n      spec = gym_wrapper.spec_from_gym_space(box_space)\n\n      self.assertEqual((3, 4), spec.shape)\n      self.assertEqual(dtype, spec.dtype)\n      np.testing.assert_array_equal(-np.ones((3, 4)), spec.minimum)\n      np.testing.assert_array_equal(np.ones((3, 4)), spec.maximum)\n\n  def test_spec_from_gym_space_box_scalars_simplify_bounds(self):\n    box_space = gym.spaces.Box(-1.0, 1.0, (3, 4))\n    spec = gym_wrapper.spec_from_gym_space(box_space, simplify_box_bounds=True)\n\n    self.assertEqual((3, 4), spec.shape)\n    self.assertEqual(np.float32, spec.dtype)\n    np.testing.assert_array_equal(np.array([-1], dtype=np.int), spec.minimum)\n    np.testing.assert_array_equal(np.array([1], dtype=np.int), spec.maximum)\n\n  def test_spec_from_gym_space_when_simplify_box_bounds_false(self):\n    # testing on gym.spaces.Dict which makes recursive calls to\n    # _spec_from_gym_space\n    box_space = gym.spaces.Box(-1.0, 1.0, (2,))\n    dict_space = gym.spaces.Dict({\'box1\': box_space, \'box2\': box_space})\n    spec = gym_wrapper.spec_from_gym_space(\n        dict_space, simplify_box_bounds=False)\n\n    self.assertEqual((2,), spec[\'box1\'].shape)\n    self.assertEqual((2,), spec[\'box2\'].shape)\n    self.assertEqual(np.float32, spec[\'box1\'].dtype)\n    self.assertEqual(np.float32, spec[\'box2\'].dtype)\n    self.assertEqual(\'box1\', spec[\'box1\'].name)\n    self.assertEqual(\'box2\', spec[\'box2\'].name)\n    np.testing.assert_array_equal(np.array([-1, -1], dtype=np.int),\n                                  spec[\'box1\'].minimum)\n    np.testing.assert_array_equal(np.array([1, 1], dtype=np.int),\n                                  spec[\'box1\'].maximum)\n    np.testing.assert_array_equal(np.array([-1, -1], dtype=np.int),\n                                  spec[\'box2\'].minimum)\n    np.testing.assert_array_equal(np.array([1, 1], dtype=np.int),\n                                  spec[\'box2\'].maximum)\n\n  def test_spec_from_gym_space_box_array(self):\n    for dtype in (np.float32, np.float64):\n      box_space = gym.spaces.Box(np.array([-1.0, -2.0]), np.array([2.0, 4.0]),\n                                 dtype=dtype)\n      spec = gym_wrapper.spec_from_gym_space(box_space)\n\n      self.assertEqual((2,), spec.shape)\n      self.assertEqual(dtype, spec.dtype)\n      np.testing.assert_array_equal(np.array([-1.0, -2.0]), spec.minimum)\n      np.testing.assert_array_equal(np.array([2.0, 4.0]), spec.maximum)\n\n  def test_spec_from_gym_space_tuple(self):\n    tuple_space = gym.spaces.Tuple((gym.spaces.Discrete(2),\n                                    gym.spaces.Discrete(3)))\n    spec = gym_wrapper.spec_from_gym_space(tuple_space)\n\n    self.assertEqual(2, len(spec))\n    self.assertEqual((), spec[0].shape)\n    self.assertEqual(np.int64, spec[0].dtype)\n    self.assertEqual(0, spec[0].minimum)\n    self.assertEqual(1, spec[0].maximum)\n\n    self.assertEqual((), spec[1].shape)\n    self.assertEqual(np.int64, spec[1].dtype)\n    self.assertEqual(0, spec[1].minimum)\n    self.assertEqual(2, spec[1].maximum)\n\n  def test_spec_from_gym_space_tuple_mixed(self):\n    tuple_space = gym.spaces.Tuple((\n        gym.spaces.Discrete(2),\n        gym.spaces.Box(-1.0, 1.0, (3, 4)),\n        gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3))),\n        gym.spaces.Dict({\n            \'spec_1\':\n                gym.spaces.Discrete(2),\n            \'spec_2\':\n                gym.spaces.Tuple((gym.spaces.Discrete(2),\n                                  gym.spaces.Discrete(3))),\n        }),\n    ))\n    spec = gym_wrapper.spec_from_gym_space(tuple_space)\n\n    self.assertEqual(4, len(spec))\n    # Test Discrete\n    self.assertEqual((), spec[0].shape)\n    self.assertEqual(np.int64, spec[0].dtype)\n    self.assertEqual(0, spec[0].minimum)\n    self.assertEqual(1, spec[0].maximum)\n\n    # Test Box\n    self.assertEqual((3, 4), spec[1].shape)\n    self.assertEqual(np.float32, spec[1].dtype)\n    np.testing.assert_array_almost_equal(-np.ones((3, 4)), spec[1].minimum)\n    np.testing.assert_array_almost_equal(np.ones((3, 4)), spec[1].maximum)\n\n    # Test Tuple\n    self.assertEqual(2, len(spec[2]))\n    self.assertEqual((), spec[2][0].shape)\n    self.assertEqual(np.int64, spec[2][0].dtype)\n    self.assertEqual(0, spec[2][0].minimum)\n    self.assertEqual(1, spec[2][0].maximum)\n    self.assertEqual((), spec[2][1].shape)\n    self.assertEqual(np.int64, spec[2][1].dtype)\n    self.assertEqual(0, spec[2][1].minimum)\n    self.assertEqual(2, spec[2][1].maximum)\n\n    # Test Dict\n    # Test Discrete in Dict\n    discrete_in_dict = spec[3][\'spec_1\']\n    self.assertEqual((), discrete_in_dict.shape)\n    self.assertEqual(np.int64, discrete_in_dict.dtype)\n    self.assertEqual(0, discrete_in_dict.minimum)\n    self.assertEqual(1, discrete_in_dict.maximum)\n\n    # Test Tuple in Dict\n    tuple_in_dict = spec[3][\'spec_2\']\n    self.assertEqual(2, len(tuple_in_dict))\n    self.assertEqual((), tuple_in_dict[0].shape)\n    self.assertEqual(np.int64, tuple_in_dict[0].dtype)\n    self.assertEqual(0, tuple_in_dict[0].minimum)\n    self.assertEqual(1, tuple_in_dict[0].maximum)\n    self.assertEqual((), tuple_in_dict[1].shape)\n    self.assertEqual(np.int64, tuple_in_dict[1].dtype)\n    self.assertEqual(0, tuple_in_dict[1].minimum)\n    self.assertEqual(2, tuple_in_dict[1].maximum)\n\n  def test_spec_from_gym_space_dict(self):\n    dict_space = gym.spaces.Dict([\n        (\'spec_2\', gym.spaces.Box(-1.0, 1.0, (3, 4))),\n        (\'spec_1\', gym.spaces.Discrete(2)),\n    ])\n\n    spec = gym_wrapper.spec_from_gym_space(dict_space)\n\n    keys = list(spec.keys())\n    self.assertEqual(\'spec_1\', keys[1])\n    self.assertEqual(2, len(spec))\n    self.assertEqual((), spec[\'spec_1\'].shape)\n    self.assertEqual(np.int64, spec[\'spec_1\'].dtype)\n    self.assertEqual(0, spec[\'spec_1\'].minimum)\n    self.assertEqual(1, spec[\'spec_1\'].maximum)\n\n    self.assertEqual(\'spec_2\', keys[0])\n    self.assertEqual((3, 4), spec[\'spec_2\'].shape)\n    self.assertEqual(np.float32, spec[\'spec_2\'].dtype)\n    np.testing.assert_array_almost_equal(\n        -np.ones((3, 4)),\n        spec[\'spec_2\'].minimum,\n    )\n    np.testing.assert_array_almost_equal(\n        np.ones((3, 4)),\n        spec[\'spec_2\'].maximum,\n    )\n\n  def test_spec_from_gym_space_dtype_map(self):\n    class Box(gym.spaces.Box):\n      """"""Box space without the dtype property.""""""\n\n      def __init__(self, *args, **kwargs):\n        super(Box, self).__init__(*args, **kwargs)\n        del self.dtype\n\n    tuple_space = gym.spaces.Tuple((\n        gym.spaces.Discrete(2),\n        Box(0, 1, (3, 4)),\n        gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3))),\n        gym.spaces.Dict({\n            \'spec_1\':\n                gym.spaces.Discrete(2),\n            \'spec_2\':\n                gym.spaces.Tuple((\n                    gym.spaces.Discrete(2),\n                    Box(0, 1, (3, 4)),\n                )),\n        }),\n    ))\n\n    dtype_map = {gym.spaces.Discrete: np.uint8, gym.spaces.Box: np.uint16}\n    spec = gym_wrapper.spec_from_gym_space(tuple_space, dtype_map=dtype_map)\n    self.assertEqual(np.uint8, spec[0].dtype)\n    self.assertEqual(np.uint16, spec[1].dtype)\n    self.assertEqual(np.uint8, spec[2][0].dtype)\n    self.assertEqual(np.uint8, spec[2][1].dtype)\n    self.assertEqual(np.uint8, spec[3][\'spec_1\'].dtype)\n    self.assertEqual(np.uint8, spec[3][\'spec_2\'][0].dtype)\n    self.assertEqual(np.uint16, spec[3][\'spec_2\'][1].dtype)\n\n  def test_spec_name(self):\n    box_space = gym.spaces.Box(\n        np.array([-1.0, -2.0]), np.array([2.0, 4.0]), dtype=np.float32)\n    spec = gym_wrapper.spec_from_gym_space(box_space, name=\'observation\')\n    self.assertEqual(\'observation\', spec.name)\n\n  def test_spec_name_nested(self):\n    dict_space = gym.spaces.Tuple((gym.spaces.Dict({\n        \'spec_0\':\n            gym.spaces.Dict({\n                \'spec_1\': gym.spaces.Discrete(2),\n                \'spec_2\': gym.spaces.Discrete(2),\n            }),\n    }), gym.spaces.Discrete(2)))\n    spec = gym_wrapper.spec_from_gym_space(dict_space, name=\'observation\')\n    self.assertEqual(\'observation/tuple_0/spec_0/spec_1\',\n                     spec[0][\'spec_0\'][\'spec_1\'].name)\n    self.assertEqual(\'observation/tuple_0/spec_0/spec_2\',\n                     spec[0][\'spec_0\'][\'spec_2\'].name)\n    self.assertEqual(\'observation/tuple_1\', spec[1].name)\n\n\nclass GymWrapperOnCartpoleTest(test_utils.TestCase):\n\n  def test_wrapped_cartpole_specs(self):\n    # Note we use spec.make on gym envs to avoid getting a TimeLimit wrapper on\n    # the environment.\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n\n    action_spec = env.action_spec()\n    self.assertEqual((), action_spec.shape)\n    self.assertEqual(0, action_spec.minimum)\n    self.assertEqual(1, action_spec.maximum)\n\n    observation_spec = env.observation_spec()\n    self.assertEqual((4,), observation_spec.shape)\n    self.assertEqual(np.float32, observation_spec.dtype)\n    high = np.array([\n        4.8,\n        np.finfo(np.float32).max, 2 / 15.0 * math.pi,\n        np.finfo(np.float32).max\n    ])\n    np.testing.assert_array_almost_equal(-high, observation_spec.minimum)\n    np.testing.assert_array_almost_equal(high, observation_spec.maximum)\n\n  def test_wrapped_cartpole_reset(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n\n    first_time_step = env.reset()\n    self.assertTrue(first_time_step.is_first())\n    self.assertEqual(0.0, first_time_step.reward)\n    self.assertEqual(1.0, first_time_step.discount)\n    self.assertEqual((4,), first_time_step.observation.shape)\n    self.assertEqual(np.float32, first_time_step.observation.dtype)\n\n  def test_wrapped_cartpole_transition(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env.reset()\n    transition_time_step = env.step(np.array(0, dtype=np.int32))\n\n    self.assertTrue(transition_time_step.is_mid())\n    self.assertNotEqual(None, transition_time_step.reward)\n    self.assertEqual(1.0, transition_time_step.discount)\n    self.assertEqual((4,), transition_time_step.observation.shape)\n\n  def test_wrapped_cartpole_final(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    time_step = env.reset()\n\n    while not time_step.is_last():\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    self.assertNotEqual(None, time_step.reward)\n    self.assertEqual(0.0, time_step.discount)\n    self.assertEqual((4,), time_step.observation.shape)\n\n  def test_get_info(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    self.assertEqual(None, env.get_info())\n    env.reset()\n    self.assertEqual(None, env.get_info())\n    env.step(np.array(0, dtype=np.int32))\n    self.assertEqual({}, env.get_info())\n\n  def test_automatic_reset_after_create(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n\n    first_time_step = env.step(0)\n    self.assertTrue(first_time_step.is_first())\n\n  def test_automatic_reset_after_done(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    time_step = env.reset()\n\n    while not time_step.is_last():\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    first_time_step = env.step(0)\n    self.assertTrue(first_time_step.is_first())\n\n  def test_automatic_reset_after_done_not_using_reset_directly(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    time_step = env.step(1)\n\n    while not time_step.is_last():\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    first_time_step = env.step(0)\n    self.assertTrue(first_time_step.is_first())\n\n  def test_method_propagation(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    for method_name in (\'render\', \'seed\', \'close\'):\n      setattr(cartpole_env, method_name, mock.MagicMock())\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env.render()\n    self.assertEqual(1, cartpole_env.render.call_count)\n    env.seed(0)\n    self.assertEqual(1, cartpole_env.seed.call_count)\n    cartpole_env.seed.assert_called_with(0)\n    env.close()\n    self.assertEqual(1, cartpole_env.close.call_count)\n\n  def test_obs_dtype(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    time_step = env.reset()\n    self.assertEqual(env.observation_spec().dtype, time_step.observation.dtype)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/parallel_py_environment.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Runs multiple environments in parallel processes and steps them in batch.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport atexit\nimport sys\nimport traceback\nfrom typing import Any, Callable, Sequence, Text, Union\n\nfrom absl import logging\n\nimport cloudpickle\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\n# Worker polling period in seconds.\n_POLLING_PERIOD = 0.1\n\nEnvConstructor = Callable[[], py_environment.PyEnvironment]\nPromise = Callable[[], Any]\n\n\n@gin.configurable\nclass ParallelPyEnvironment(py_environment.PyEnvironment):\n  """"""Batch together environments and simulate them in external processes.\n\n  The environments are created in external processes by calling the provided\n  callables. This can be an environment class, or a function creating the\n  environment and potentially wrapping it. The returned environment should not\n  access global variables.\n  """"""\n\n  def __init__(self,\n               env_constructors: Sequence[EnvConstructor],\n               start_serially: bool = True,\n               blocking: bool = False,\n               flatten: bool = False):\n    """"""Batch together environments and simulate them in external processes.\n\n    The environments can be different but must use the same action and\n    observation specs.\n\n    Args:\n      env_constructors: List of callables that create environments.\n      start_serially: Whether to start environments serially or in parallel.\n      blocking: Whether to step environments one after another.\n      flatten: Boolean, whether to use flatten action and time_steps during\n        communication to reduce overhead.\n\n    Raises:\n      ValueError: If the action or observation specs don\'t match.\n    """"""\n    super(ParallelPyEnvironment, self).__init__()\n    self._envs = [ProcessPyEnvironment(ctor, flatten=flatten)\n                  for ctor in env_constructors]\n    self._num_envs = len(env_constructors)\n    self._blocking = blocking\n    self._start_serially = start_serially\n    self.start()\n    self._action_spec = self._envs[0].action_spec()\n    self._observation_spec = self._envs[0].observation_spec()\n    self._time_step_spec = self._envs[0].time_step_spec()\n    self._parallel_execution = True\n    if any(env.action_spec() != self._action_spec for env in self._envs):\n      raise ValueError(\'All environments must have the same action spec.\')\n    if any(env.time_step_spec() != self._time_step_spec for env in self._envs):\n      raise ValueError(\'All environments must have the same time_step_spec.\')\n    self._flatten = flatten\n\n  def start(self) -> None:\n    logging.info(\'Spawning all processes.\')\n    for env in self._envs:\n      env.start(wait_to_start=self._start_serially)\n    if not self._start_serially:\n      logging.info(\'Waiting for all processes to start.\')\n      for env in self._envs:\n        env.wait_start()\n    logging.info(\'All processes started.\')\n\n  @property\n  def batched(self) -> bool:\n    return True\n\n  @property\n  def batch_size(self) -> int:\n    return self._num_envs\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._action_spec\n\n  def time_step_spec(self)  -> ts.TimeStep:\n    return self._time_step_spec\n\n  def _reset(self):\n    """"""Reset all environments and combine the resulting observation.\n\n    Returns:\n      Time step with batch dimension.\n    """"""\n    time_steps = [env.reset(self._blocking) for env in self._envs]\n    if not self._blocking:\n      time_steps = [promise() for promise in time_steps]\n    return self._stack_time_steps(time_steps)\n\n  def _step(self, actions):\n    """"""Forward a batch of actions to the wrapped environments.\n\n    Args:\n      actions: Batched action, possibly nested, to apply to the environment.\n\n    Raises:\n      ValueError: Invalid actions.\n\n    Returns:\n      Batch of observations, rewards, and done flags.\n    """"""\n    time_steps = [\n        env.step(action, self._blocking)\n        for env, action in zip(self._envs, self._unstack_actions(actions))]\n    # When blocking is False we get promises that need to be called.\n    if not self._blocking:\n      time_steps = [promise() for promise in time_steps]\n    return self._stack_time_steps(time_steps)\n\n  def close(self) -> None:\n    """"""Close all external process.""""""\n    logging.info(\'Closing all processes.\')\n    for env in self._envs:\n      env.close()\n    logging.info(\'All processes closed.\')\n\n  def _stack_time_steps(self, time_steps):\n    """"""Given a list of TimeStep, combine to one with a batch dimension.""""""\n    if self._flatten:\n      return nest_utils.fast_map_structure_flatten(\n          lambda *arrays: np.stack(arrays), self._time_step_spec, *time_steps)\n    else:\n      return nest_utils.fast_map_structure(\n          lambda *arrays: np.stack(arrays), *time_steps)\n\n  def _unstack_actions(self, batched_actions):\n    """"""Returns a list of actions from potentially nested batch of actions.""""""\n    flattened_actions = tf.nest.flatten(batched_actions)\n    if self._flatten:\n      unstacked_actions = zip(*flattened_actions)\n    else:\n      unstacked_actions = [\n          tf.nest.pack_sequence_as(batched_actions, actions)\n          for actions in zip(*flattened_actions)\n      ]\n    return unstacked_actions\n\n  def seed(self, seeds: Sequence[types.Seed]) -> Sequence[Any]:\n    """"""Seeds the parallel environments.""""""\n    if len(seeds) != len(self._envs):\n      raise ValueError(\n          \'Number of seeds should match the number of parallel_envs.\')\n\n    promises = [env.call(\'seed\', seed) for seed, env in zip(seeds, self._envs)]\n    # Block until all envs are seeded.\n    return [promise() for promise in promises]\n\n\nclass ProcessPyEnvironment(object):\n  """"""Step a single env in a separate process for lock free paralellism.""""""\n\n  # Message types for communication via the pipe.\n  _READY = 1\n  _ACCESS = 2\n  _CALL = 3\n  _RESULT = 4\n  _EXCEPTION = 5\n  _CLOSE = 6\n\n  def __init__(self, env_constructor: EnvConstructor, flatten: bool = False):\n    """"""Step environment in a separate process for lock free paralellism.\n\n    The environment is created in an external process by calling the provided\n    callable. This can be an environment class, or a function creating the\n    environment and potentially wrapping it. The returned environment should\n    not access global variables.\n\n    Args:\n      env_constructor: Callable that creates and returns a Python environment.\n      flatten: Boolean, whether to assume flattened actions and time_steps\n        during communication to avoid overhead.\n\n    Attributes:\n      observation_spec: The cached observation spec of the environment.\n      action_spec: The cached action spec of the environment.\n      time_step_spec: The cached time step spec of the environment.\n    """"""\n    # NOTE(ebrevdo): multiprocessing uses the standard py3 pickler which does\n    # not support anonymous lambdas.  Folks usually pass anonymous lambdas as\n    # env constructors.  Here we work around this by manually pickling\n    # the constructor using cloudpickle; which supports these.  In the\n    # new process, we\'ll unpickle this constructor and run it.\n    self._pickled_env_constructor = cloudpickle.dumps(env_constructor)\n    self._flatten = flatten\n    self._observation_spec = None\n    self._action_spec = None\n    self._time_step_spec = None\n\n  def start(self, wait_to_start: bool = True) -> None:\n    """"""Start the process.\n\n    Args:\n      wait_to_start: Whether the call should wait for an env initialization.\n    """"""\n    mp_context = multiprocessing.get_context()\n    self._conn, conn = mp_context.Pipe()\n    self._process = mp_context.Process(target=self._worker, args=(conn,))\n    atexit.register(self.close)\n    self._process.start()\n    if wait_to_start:\n      self.wait_start()\n\n  def wait_start(self) -> None:\n    """"""Wait for the started process to finish initialization.""""""\n    result = self._conn.recv()\n    if isinstance(result, Exception):\n      self._conn.close()\n      self._process.join(5)\n      raise result\n    assert result == self._READY, result\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    if not self._observation_spec:\n      self._observation_spec = self.call(\'observation_spec\')()\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedArraySpec:\n    if not self._action_spec:\n      self._action_spec = self.call(\'action_spec\')()\n    return self._action_spec\n\n  def time_step_spec(self) -> ts.TimeStep:\n    if not self._time_step_spec:\n      self._time_step_spec = self.call(\'time_step_spec\')()\n    return self._time_step_spec\n\n  def __getattr__(self, name: Text) -> Any:\n    """"""Request an attribute from the environment.\n\n    Note that this involves communication with the external process, so it can\n    be slow.\n\n    This method is only called if the attribute is not found in the dictionary\n    of `ParallelPyEnvironment`\'s definition.\n\n    Args:\n      name: Attribute to access.\n\n    Returns:\n      Value of the attribute.\n    """"""\n    # Private properties are always accessed on this object, not in the\n    # wrapped object in another process.  This includes properties used\n    # for pickling (incl. __getstate__, __setstate__, _conn, _ACCESS, _receive),\n    # as well as private properties and methods created and used by subclasses\n    # of this class.  Allowing arbitrary private attributes to be requested\n    # from the other process can lead to deadlocks.\n    if name.startswith(\'_\'):\n      return super(ProcessPyEnvironment, self).__getattribute__(name)\n\n    # All other requests get sent to the worker.\n    self._conn.send((self._ACCESS, name))\n    return self._receive()\n\n  def call(self, name: Text, *args, **kwargs) -> Promise:\n    """"""Asynchronously call a method of the external environment.\n\n    Args:\n      name: Name of the method to call.\n      *args: Positional arguments to forward to the method.\n      **kwargs: Keyword arguments to forward to the method.\n\n    Returns:\n      The attribute.\n    """"""\n    payload = name, args, kwargs\n    self._conn.send((self._CALL, payload))\n    return self._receive\n\n  def access(self, name: Text) -> Any:\n    """"""Access an attribute of the external environment.\n\n    This method blocks.\n\n    Args:\n      name: Name of the attribute to access.\n\n    Returns:\n      The attribute value.\n    """"""\n    self._conn.send((self._ACCESS, name))\n    return self._receive()\n\n  def close(self) -> None:\n    """"""Send a close message to the external process and join it.""""""\n    try:\n      self._conn.send((self._CLOSE, None))\n      self._conn.close()\n    except IOError:\n      # The connection was already closed.\n      pass\n    if self._process.is_alive():\n      self._process.join(5)\n\n  def step(self,\n           action: types.NestedArray,\n           blocking: bool = True) -> Union[ts.TimeStep, Promise]:\n    """"""Step the environment.\n\n    Args:\n      action: The action to apply to the environment.\n      blocking: Whether to wait for the result.\n\n    Returns:\n      time step when blocking, otherwise callable that returns the time step.\n    """"""\n    promise = self.call(\'step\', action)\n    if blocking:\n      return promise()\n    else:\n      return promise\n\n  def reset(self, blocking: bool = True) -> Union[ts.TimeStep, Promise]:\n    """"""Reset the environment.\n\n    Args:\n      blocking: Whether to wait for the result.\n\n    Returns:\n      New observation when blocking, otherwise callable that returns the new\n      observation.\n    """"""\n    promise = self.call(\'reset\')\n    if blocking:\n      return promise()\n    else:\n      return promise\n\n  def _receive(self):\n    """"""Wait for a message from the worker process and return its payload.\n\n    Raises:\n      Exception: An exception was raised inside the worker process.\n      KeyError: The reveived message is of an unknown type.\n\n    Returns:\n      Payload object of the message.\n    """"""\n    message, payload = self._conn.recv()\n    # Re-raise exceptions in the main process.\n    if message == self._EXCEPTION:\n      stacktrace = payload\n      raise Exception(stacktrace)\n    if message == self._RESULT:\n      return payload\n    self.close()\n    raise KeyError(\'Received message of unexpected type {}\'.format(message))\n\n  def _worker(self, conn):\n    """"""The process waits for actions and sends back environment results.\n\n    Args:\n      conn: Connection for communication to the main process.\n\n    Raises:\n      KeyError: When receiving a message of unknown type.\n    """"""\n    try:\n      env = cloudpickle.loads(self._pickled_env_constructor)()\n      action_spec = env.action_spec()\n      conn.send(self._READY)  # Ready.\n      while True:\n        try:\n          # Only block for short times to have keyboard exceptions be raised.\n          if not conn.poll(_POLLING_PERIOD):\n            continue\n          message, payload = conn.recv()\n        except (EOFError, KeyboardInterrupt):\n          break\n        if message == self._ACCESS:\n          name = payload\n          result = getattr(env, name)\n          conn.send((self._RESULT, result))\n          continue\n        if message == self._CALL:\n          name, args, kwargs = payload\n          if self._flatten and name == \'step\':\n            args = [tf.nest.pack_sequence_as(action_spec, args[0])]\n          result = getattr(env, name)(*args, **kwargs)\n          if self._flatten and name in [\'step\', \'reset\']:\n            result = tf.nest.flatten(result)\n          conn.send((self._RESULT, result))\n          continue\n        if message == self._CLOSE:\n          assert payload is None\n          env.close()\n          break\n        raise KeyError(\'Received message of unknown type {}\'.format(message))\n    except Exception:  # pylint: disable=broad-except\n      etype, evalue, tb = sys.exc_info()\n      stacktrace = \'\'.join(traceback.format_exception(etype, evalue, tb))\n      message = \'Error in environment process: {}\'.format(stacktrace)\n      logging.error(message)\n      conn.send((self._EXCEPTION, stacktrace))\n    finally:\n      conn.close()\n'"
tf_agents/environments/parallel_py_environment_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the parallel_py_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport time\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nfrom tf_agents.trajectories import time_step as ts\n\n\nclass SlowStartingEnvironment(random_py_environment.RandomPyEnvironment):\n\n  def __init__(self, *args, **kwargs):\n    time_sleep = kwargs.pop(\'time_sleep\', 1.0)\n    time.sleep(time_sleep)\n    super(SlowStartingEnvironment, self).__init__(*args, **kwargs)\n\n\nclass ParallelPyEnvironmentTest(tf.test.TestCase):\n\n  def _set_default_specs(self):\n    self.observation_spec = array_spec.ArraySpec((3, 3), np.float32)\n    self.time_step_spec = ts.time_step_spec(self.observation_spec)\n    self.action_spec = array_spec.BoundedArraySpec([7],\n                                                   dtype=np.float32,\n                                                   minimum=-1.0,\n                                                   maximum=1.0)\n\n  def _make_parallel_py_environment(self,\n                                    constructor=None,\n                                    num_envs=2,\n                                    start_serially=True,\n                                    blocking=True):\n    self._set_default_specs()\n    constructor = constructor or functools.partial(\n        random_py_environment.RandomPyEnvironment, self.observation_spec,\n        self.action_spec)\n    return parallel_py_environment.ParallelPyEnvironment(\n        env_constructors=[constructor] * num_envs, blocking=blocking,\n        start_serially=start_serially)\n\n  def test_close_no_hang_after_init(self):\n    env = self._make_parallel_py_environment()\n    env.close()\n\n  def test_get_specs(self):\n    env = self._make_parallel_py_environment()\n    self.assertEqual(self.observation_spec, env.observation_spec())\n    self.assertEqual(self.time_step_spec, env.time_step_spec())\n    self.assertEqual(self.action_spec, env.action_spec())\n\n    env.close()\n\n  def test_step(self):\n    num_envs = 2\n    env = self._make_parallel_py_environment(num_envs=num_envs)\n    action_spec = env.action_spec()\n    observation_spec = env.observation_spec()\n    rng = np.random.RandomState()\n    action = np.array([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n    env.reset()\n\n    # Take one step and assert observation is batched the right way.\n    time_step = env.step(action)\n    self.assertEqual(num_envs, time_step.observation.shape[0])\n    self.assertAllEqual(observation_spec.shape, time_step.observation.shape[1:])\n    self.assertEqual(num_envs, action.shape[0])\n    self.assertAllEqual(action_spec.shape, action.shape[1:])\n\n    # Take another step and assert that observations have the same shape.\n    time_step2 = env.step(action)\n    self.assertAllEqual(time_step.observation.shape,\n                        time_step2.observation.shape)\n    env.close()\n\n  def test_non_blocking_start_processes_in_parallel(self):\n    self._set_default_specs()\n    constructor = functools.partial(\n        SlowStartingEnvironment,\n        self.observation_spec,\n        self.action_spec,\n        time_sleep=1.0)\n    start_time = time.time()\n    env = self._make_parallel_py_environment(\n        constructor=constructor, num_envs=10, start_serially=False,\n        blocking=False)\n    end_time = time.time()\n    self.assertLessEqual(\n        end_time - start_time,\n        5.0,\n        msg=(\'Expected all processes to start together, \'\n             \'got {} wait time\').format(end_time - start_time))\n    env.close()\n\n  def test_blocking_start_processes_one_after_another(self):\n    self._set_default_specs()\n    constructor = functools.partial(\n        SlowStartingEnvironment,\n        self.observation_spec,\n        self.action_spec,\n        time_sleep=1.0)\n    start_time = time.time()\n    env = self._make_parallel_py_environment(\n        constructor=constructor, num_envs=10, start_serially=True,\n        blocking=True)\n    end_time = time.time()\n    self.assertGreater(\n        end_time - start_time,\n        10,\n        msg=(\'Expected all processes to start one \'\n             \'after another, got {} wait time\').format(end_time - start_time))\n    env.close()\n\n  def test_unstack_actions(self):\n    num_envs = 2\n    env = self._make_parallel_py_environment(num_envs=num_envs)\n    action_spec = env.action_spec()\n    rng = np.random.RandomState()\n    batched_action = np.array([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n\n    # Test that actions are correctly unstacked when just batched in np.array.\n    unstacked_actions = env._unstack_actions(batched_action)\n    for action in unstacked_actions:\n      self.assertAllEqual(action_spec.shape, action.shape)\n    env.close()\n\n  def test_unstack_nested_actions(self):\n    num_envs = 2\n    env = self._make_parallel_py_environment(num_envs=num_envs)\n    action_spec = env.action_spec()\n    rng = np.random.RandomState()\n    batched_action = np.array([\n        array_spec.sample_bounded_spec(action_spec, rng)\n        for _ in range(num_envs)\n    ])\n\n    # Test that actions are correctly unstacked when nested in namedtuple.\n    class NestedAction(\n        collections.namedtuple(\'NestedAction\', [\'action\', \'other_var\'])):\n      pass\n\n    nested_action = NestedAction(\n        action=batched_action, other_var=np.array([13.0] * num_envs))\n    unstacked_actions = env._unstack_actions(nested_action)\n    for nested_action in unstacked_actions:\n      self.assertAllEqual(action_spec.shape, nested_action.action.shape)\n      self.assertEqual(13.0, nested_action.other_var)\n    env.close()\n\n  def test_seedable(self):\n    seeds = [0, 1]\n    env = self._make_parallel_py_environment()\n    env.seed(seeds)\n    self.assertEqual(\n        np.random.RandomState(0).get_state()[1][-1],\n        env._envs[0].access(\'_rng\').get_state()[1][-1])\n\n    self.assertEqual(\n        np.random.RandomState(1).get_state()[1][-1],\n        env._envs[1].access(\'_rng\').get_state()[1][-1])\n    env.close()\n\n\nclass ProcessPyEnvironmentTest(tf.test.TestCase):\n\n  def test_close_no_hang_after_init(self):\n    constructor = functools.partial(\n        random_py_environment.RandomPyEnvironment,\n        array_spec.ArraySpec((3, 3), np.float32),\n        array_spec.BoundedArraySpec([1], np.float32, minimum=-1.0, maximum=1.0),\n        episode_end_probability=0,\n        min_duration=2,\n        max_duration=2)\n    env = parallel_py_environment.ProcessPyEnvironment(constructor)\n    env.start()\n    env.close()\n\n  def test_close_no_hang_after_step(self):\n    constructor = functools.partial(\n        random_py_environment.RandomPyEnvironment,\n        array_spec.ArraySpec((3, 3), np.float32),\n        array_spec.BoundedArraySpec([1], np.float32, minimum=-1.0, maximum=1.0),\n        episode_end_probability=0,\n        min_duration=5,\n        max_duration=5)\n    rng = np.random.RandomState()\n    env = parallel_py_environment.ProcessPyEnvironment(constructor)\n    env.start()\n    action_spec = env.action_spec()\n    env.reset()\n    env.step(array_spec.sample_bounded_spec(action_spec, rng))\n    env.step(array_spec.sample_bounded_spec(action_spec, rng))\n    env.close()\n\n  def test_reraise_exception_in_init(self):\n    constructor = MockEnvironmentCrashInInit\n    env = parallel_py_environment.ProcessPyEnvironment(constructor)\n    with self.assertRaises(Exception):\n      env.start()\n\n  def test_reraise_exception_in_reset(self):\n    constructor = MockEnvironmentCrashInReset\n    env = parallel_py_environment.ProcessPyEnvironment(constructor)\n    env.start()\n    with self.assertRaises(Exception):\n      env.reset()\n\n  def test_reraise_exception_in_step(self):\n    constructor = functools.partial(MockEnvironmentCrashInStep, crash_at_step=3)\n    env = parallel_py_environment.ProcessPyEnvironment(constructor)\n    env.start()\n    env.reset()\n    action_spec = env.action_spec()\n    rng = np.random.RandomState()\n    env.step(array_spec.sample_bounded_spec(action_spec, rng))\n    env.step(array_spec.sample_bounded_spec(action_spec, rng))\n    with self.assertRaises(Exception):\n      env.step(array_spec.sample_bounded_spec(action_spec, rng))\n\n\nclass MockEnvironmentCrashInInit(py_environment.PyEnvironment):\n  """"""Raise an error when instantiated.""""""\n\n  def __init__(self, *unused_args, **unused_kwargs):\n    raise RuntimeError()\n\n  def observation_spec(self):\n    return []\n\n  def action_spec(self):\n    return []\n\n  def _reset(self):\n    return ()\n\n  def _step(self, action):\n    return ()\n\n\nclass MockEnvironmentCrashInReset(py_environment.PyEnvironment):\n  """"""Raise an error when instantiated.""""""\n\n  def __init__(self, *unused_args, **unused_kwargs):\n    pass\n\n  def observation_spec(self):\n    return []\n\n  def action_spec(self):\n    return []\n\n  def _reset(self):\n    raise RuntimeError()\n\n  def _step(self, action):\n    return ()\n\n\nclass MockEnvironmentCrashInStep(random_py_environment.RandomPyEnvironment):\n  """"""Raise an error after specified number of steps in an episode.""""""\n\n  def __init__(self, crash_at_step):\n    super(MockEnvironmentCrashInStep, self).__init__(\n        array_spec.ArraySpec((3, 3), np.float32),\n        array_spec.BoundedArraySpec([1], np.float32, minimum=-1.0, maximum=1.0),\n        episode_end_probability=0,\n        min_duration=crash_at_step + 1,\n        max_duration=crash_at_step + 1)\n    self._crash_at_step = crash_at_step\n    self._steps = 0\n\n  def _step(self, *args, **kwargs):\n    transition = super(MockEnvironmentCrashInStep, self)._step(*args, **kwargs)\n    self._steps += 1\n    if self._steps == self._crash_at_step:\n      raise RuntimeError()\n    return transition\n\n\nif __name__ == \'__main__\':\n  multiprocessing.handle_test_main(tf.test.main)\n'"
tf_agents/environments/py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Python RL Environment API.\n\nAdapted from the Deepmind\'s Environment API as seen in:\n  https://github.com/deepmind/dm_control\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nfrom typing import Any, Optional, Text\n\nimport numpy as np\nimport six\n\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass PyEnvironment(object):\n  """"""Abstract base class for Python RL environments.\n\n  Observations and valid actions are described with `ArraySpec`s, defined in\n  the `specs` module.\n\n  If the environment can run multiple steps at the same time and take a batched\n  set of actions and return a batched set of observations, it should overwrite\n  the property batched to True.\n  """"""\n\n  def __init__(self):\n    self._current_time_step = None\n    common.assert_members_are_not_overridden(\n        base_cls=PyEnvironment, instance=self, black_list=(\'reset\', \'step\'))\n\n  @property\n  def batched(self) -> bool:\n    """"""Whether the environment is batched or not.\n\n    If the environment supports batched observations and actions, then overwrite\n    this property to True.\n\n    A batched environment takes in a batched set of actions and returns a\n    batched set of observations. This means for all numpy arrays in the input\n    and output nested structures, the first dimension is the batch size.\n\n    When batched, the left-most dimension is not part of the action_spec\n    or the observation_spec and corresponds to the batch dimension.\n\n    Returns:\n      A boolean indicating whether the environment is batched or not.\n    """"""\n    return False\n\n  @property\n  def batch_size(self) -> Optional[int]:\n    """"""The batch size of the environment.\n\n    Returns:\n      The batch size of the environment, or `None` if the environment is not\n      batched.\n\n    Raises:\n      RuntimeError: If a subclass overrode batched to return True but did not\n        override the batch_size property.\n    """"""\n    if self.batched:\n      raise RuntimeError(\n          \'Environment %s marked itself as batched but did not override the \'\n          \'batch_size property\' % type(self))\n    return None\n\n  @abc.abstractmethod\n  def observation_spec(self) -> types.NestedArraySpec:\n    """"""Defines the observations provided by the environment.\n\n    May use a subclass of `ArraySpec` that specifies additional properties such\n    as min and max bounds on the values.\n\n    Returns:\n      An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n    """"""\n\n  @abc.abstractmethod\n  def action_spec(self) -> types.NestedArraySpec:\n    """"""Defines the actions that should be provided to `step()`.\n\n    May use a subclass of `ArraySpec` that specifies additional properties such\n    as min and max bounds on the values.\n\n    Returns:\n      An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n    """"""\n\n  def reward_spec(self) -> types.NestedArraySpec:\n    """"""Defines the rewards that are returned by `step()`.\n\n    Override this method to define an environment that uses non-standard reward\n    values, for example an environment with array-valued rewards.\n\n    Returns:\n      An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n    """"""\n    return array_spec.ArraySpec(shape=(), dtype=np.float32, name=\'reward\')\n\n  def discount_spec(self) -> types.NestedArraySpec:\n    """"""Defines the discount that are returned by `step()`.\n\n    Override this method to define an environment that uses non-standard\n    discount values, for example an environment with array-valued discounts.\n\n    Returns:\n      An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n    """"""\n    return array_spec.BoundedArraySpec(\n        shape=(), dtype=np.float32, minimum=0., maximum=1., name=\'discount\')\n\n  def time_step_spec(self) -> ts.TimeStep:\n    """"""Describes the `TimeStep` fields returned by `step()`.\n\n    Override this method to define an environment that uses non-standard values\n    for any of the items returned by `step()`. For example, an environment with\n    array-valued rewards.\n\n    Returns:\n      A `TimeStep` namedtuple containing (possibly nested) `ArraySpec`s defining\n      the step_type, reward, discount, and observation structure.\n    """"""\n    return ts.time_step_spec(self.observation_spec(), self.reward_spec())\n\n  def current_time_step(self) -> ts.TimeStep:\n    """"""Returns the current timestep.""""""\n    return self._current_time_step\n\n  def reset(self) -> ts.TimeStep:\n    """"""Starts a new sequence and returns the first `TimeStep` of this sequence.\n\n    Note: Subclasses cannot override this directly. Subclasses implement\n    _reset() which will be called by this method. The output of _reset() will\n    be cached and made available through current_time_step().\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` of `FIRST`.\n        reward: 0.0, indicating the reward.\n        discount: 1.0, indicating the discount.\n        observation: A NumPy array, or a nested dict, list or tuple of arrays\n          corresponding to `observation_spec()`.\n    """"""\n    self._current_time_step = self._reset()\n    return self._current_time_step\n\n  def step(self, action: types.NestedArray) -> ts.TimeStep:\n    """"""Updates the environment according to the action and returns a `TimeStep`.\n\n    If the environment returned a `TimeStep` with `StepType.LAST` at the\n    previous step the implementation of `_step` in the environment should call\n    `reset` to start a new sequence and ignore `action`.\n\n    This method will start a new sequence if called after the environment\n    has been constructed and `reset` has not been called. In this case\n    `action` will be ignored.\n\n    Note: Subclasses cannot override this directly. Subclasses implement\n    _step() which will be called by this method. The output of _step() will be\n    cached and made available through current_time_step().\n\n    Args:\n      action: A NumPy array, or a nested dict, list or tuple of arrays\n        corresponding to `action_spec()`.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` value.\n        reward: A NumPy array, reward value for this timestep.\n        discount: A NumPy array, discount in the range [0, 1].\n        observation: A NumPy array, or a nested dict, list or tuple of arrays\n          corresponding to `observation_spec()`.\n    """"""\n    if self._current_time_step is None:\n      return self.reset()\n\n    self._current_time_step = self._step(action)\n    return self._current_time_step\n\n  def close(self) -> None:\n    """"""Frees any resources used by the environment.\n\n    Implement this method for an environment backed by an external process.\n\n    This method be used directly\n\n    ```python\n    env = Env(...)\n    # Use env.\n    env.close()\n    ```\n\n    or via a context manager\n\n    ```python\n    with Env(...) as env:\n      # Use env.\n    ```\n    """"""\n    pass\n\n  def __enter__(self):\n    """"""Allows the environment to be used in a with-statement context.""""""\n    return self\n\n  def __exit__(self, unused_exception_type, unused_exc_value, unused_traceback):\n    """"""Allows the environment to be used in a with-statement context.""""""\n    self.close()\n\n  def render(self, mode: Text = \'rgb_array\') -> Optional[types.NestedArray]:\n    """"""Renders the environment.\n\n    Args:\n      mode: One of [\'rgb_array\', \'human\']. Renders to an numpy array, or brings\n        up a window where the environment can be visualized.\n\n    Returns:\n      An ndarray of shape [width, height, 3] denoting an RGB image if mode is\n      `rgb_array`. Otherwise return nothing and render directly to a display\n      window.\n    Raises:\n      NotImplementedError: If the environment does not support rendering.\n    """"""\n    del mode  # unused\n    raise NotImplementedError(\'No rendering support.\')\n\n  def seed(self, seed: types.Seed) -> Any:\n    """"""Seeds the environment.\n\n    Args:\n      seed: Value to use as seed for the environment.\n    """"""\n    del seed  # unused\n    raise NotImplementedError(\'No seed support for this environment.\')\n\n  def get_info(self) -> Any:\n    """"""Returns the environment info returned on the last step.\n\n    Returns:\n      Info returned by last call to step(). None by default.\n\n    Raises:\n      NotImplementedError: If the environment does not use info.\n    """"""\n    raise NotImplementedError(\'No support of get_info for this environment.\')\n\n  def get_state(self) -> Any:\n    """"""Returns the `state` of the environment.\n\n    The `state` contains everything required to restore the environment to the\n    current configuration. This can contain e.g.\n      - The current time_step.\n      - The number of steps taken in the environment (for finite horizon MDPs).\n      - Hidden state (for POMDPs).\n\n    Callers should not assume anything about the contents or format of the\n    returned `state`. It should be treated as a token that can be passed back to\n    `set_state()` later.\n\n    Note that the returned `state` handle should not be modified by the\n    environment later on, and ensuring this (e.g. using copy.deepcopy) is the\n    responsibility of the environment.\n\n    Returns:\n      state: The current state of the environment.\n    """"""\n    raise NotImplementedError(\'This environment has not implemented \'\n                              \'`get_state()`.\')\n\n  def set_state(self, state: Any) -> None:\n    """"""Restores the environment to a given `state`.\n\n    See definition of `state` in the documentation for get_state().\n\n    Args:\n      state: A state to restore the environment to.\n    """"""\n    raise NotImplementedError(\'This environment has not implemented \'\n                              \'`set_state()`.\')\n\n  #  These methods are to be implemented by subclasses:\n\n  @abc.abstractmethod\n  def _step(self, action: types.NestedArray) -> ts.TimeStep:\n    """"""Updates the environment according to action and returns a `TimeStep`.\n\n    See `step(self, action)` docstring for more details.\n\n    Args:\n      action: A NumPy array, or a nested dict, list or tuple of arrays\n        corresponding to `action_spec()`.\n    """"""\n\n  @abc.abstractmethod\n  def _reset(self) -> ts.TimeStep:\n    """"""Starts a new sequence, returns the first `TimeStep` of this sequence.\n\n    See `reset(self)` docstring for more details\n    """"""\n'"
tf_agents/environments/py_environment_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.py_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.specs import array_spec\n\n\nclass PyEnvironmentTest(tf.test.TestCase):\n\n  def testResetSavesCurrentTimeStep(self):\n    obs_spec = array_spec.BoundedArraySpec((1,), np.int32)\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32)\n\n    random_env = random_py_environment.RandomPyEnvironment(\n        observation_spec=obs_spec, action_spec=action_spec)\n\n    time_step = random_env.reset()\n    current_time_step = random_env.current_time_step()\n    tf.nest.map_structure(self.assertAllEqual, time_step, current_time_step)\n\n  def testStepSavesCurrentTimeStep(self):\n    obs_spec = array_spec.BoundedArraySpec((1,), np.int32)\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32)\n\n    random_env = random_py_environment.RandomPyEnvironment(\n        observation_spec=obs_spec, action_spec=action_spec)\n\n    random_env.reset()\n    time_step = random_env.step(action=np.ones((1,)))\n    current_time_step = random_env.current_time_step()\n    tf.nest.map_structure(self.assertAllEqual, time_step, current_time_step)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/random_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Environment implementation that generates random observations.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Callable, Optional, Sequence, Text\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\nRewardFn = Callable[[np.ndarray, types.NestedArray, types.NestedArray],\n                    types.NestedArray]\n\n\nclass RandomPyEnvironment(py_environment.PyEnvironment):\n  """"""Randomly generates observations following the given observation_spec.\n\n  If an action_spec is provided it validates that the actions used to step the\n  environment fall within the defined spec.\n  """"""\n\n  def __init__(self,\n               observation_spec: types.NestedArray,\n               action_spec: Optional[types.NestedArray] = None,\n               episode_end_probability: types.Float = 0.1,\n               discount: types.Float = 1.0,\n               reward_fn: Optional[RewardFn] = None,\n               batch_size: Optional[types.Int] = None,\n               seed: types.Seed = 42,\n               render_size: Sequence[int] = (2, 2, 3),\n               min_duration: types.Int = 0,\n               max_duration: Optional[types.Int] = None):\n    """"""Initializes the environment.\n\n    Args:\n      observation_spec: An `ArraySpec`, or a nested dict, list or tuple of\n        `ArraySpec`s.\n      action_spec: An `ArraySpec`, or a nested dict, list or tuple of\n        `ArraySpec`s.\n      episode_end_probability: Probability an episode will end when the\n        environment is stepped.\n      discount: Discount to set in time_steps.\n      reward_fn: Callable that takes in step_type, action, an observation(s),\n        and returns a numpy array of rewards.\n      batch_size: (Optional) Number of observations generated per call.\n        If this value is not `None`, then all actions are expected to\n        have an additional major axis of size `batch_size`, and all outputs\n        will have an additional major axis of size `batch_size`.\n      seed: Seed to use for rng used in observation generation.\n      render_size: Size of the random render image to return when calling\n        render.\n      min_duration: Number of steps at the beginning of the\n        episode during which the episode can not terminate.\n      max_duration: Optional number of steps after which the episode\n        terminates regarless of the termination probability.\n\n    Raises:\n      ValueError: If batch_size argument is not None and does not match the\n      shapes of discount or reward.\n    """"""\n    self._batch_size = batch_size\n    self._observation_spec = observation_spec\n    self._time_step_spec = ts.time_step_spec(self._observation_spec)\n    self._action_spec = action_spec or []\n    self._episode_end_probability = episode_end_probability\n    discount = np.asarray(discount, dtype=np.float32)\n\n    if self._batch_size:\n      if not discount.shape:\n        discount = np.tile(discount, self._batch_size)\n      if self._batch_size != len(discount):\n        raise ValueError(\'Size of discounts must equal the batch size.\')\n    self._discount = discount\n\n    if reward_fn is None:\n      # Return a reward whose size matches the batch size\n      if self._batch_size is None:\n        self._reward_fn = lambda *_: np.asarray(0.0, dtype=np.float32)\n      else:\n        self._reward_fn = (\n            lambda *_: np.zeros(self._batch_size, dtype=np.float32))\n    else:\n      self._reward_fn = reward_fn\n\n    self._done = True\n    self._num_steps = 0\n    self._min_duration = min_duration\n    self._max_duration = max_duration\n    self._rng = np.random.RandomState(seed)\n    self._render_size = render_size\n    super(RandomPyEnvironment, self).__init__()\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._action_spec\n\n  @property\n  def batch_size(self) -> Optional[types.Int]:\n    return self._batch_size\n\n  @property\n  def batched(self) -> bool:\n    return False if self._batch_size is None else True\n\n  def _get_observation(self):\n    batch_size = (self._batch_size,) if self._batch_size else ()\n    return array_spec.sample_spec_nest(self._observation_spec, self._rng,\n                                       batch_size)\n\n  def _reset(self):\n    self._done = False\n    return ts.restart(self._get_observation(), self._batch_size)\n\n  def _check_reward_shape(self, reward):\n    expected_shape = () if self._batch_size is None else (self._batch_size,)\n    if np.asarray(reward).shape != expected_shape:\n      raise ValueError(\'%r != %r. Size of reward must equal the batch size.\' %\n                       (np.asarray(reward).shape, self._batch_size))\n\n  def _step(self, action):\n    if self._done:\n      return self.reset()\n\n    if self._action_spec:\n      nest_utils.assert_same_structure(self._action_spec, action)\n\n    self._num_steps += 1\n\n    observation = self._get_observation()\n    if self._num_steps < self._min_duration:\n      self._done = False\n    elif self._max_duration and self._num_steps >= self._max_duration:\n      self._done = True\n    else:\n      self._done = self._rng.uniform() < self._episode_end_probability\n\n    if self._done:\n      reward = self._reward_fn(ts.StepType.LAST, action, observation)\n      self._check_reward_shape(reward)\n      time_step = ts.termination(observation, reward)\n      self._num_steps = 0\n    else:\n      reward = self._reward_fn(ts.StepType.MID, action, observation)\n      self._check_reward_shape(reward)\n      time_step = ts.transition(observation, reward, self._discount)\n\n    return time_step\n\n  def render(self, mode: Text = \'rgb_array\') -> np.ndarray:\n    if mode != \'rgb_array\':\n      raise ValueError(\n          ""Only rendering mode supported is \'rgb_array\', got {} instead."".\n          format(mode))\n\n    return self._rng.randint(0, 256, size=self._render_size, dtype=np.uint8)\n\n  def seed(self, seed: types.Seed) -> None:\n    self._rng.seed(seed)\n'"
tf_agents/environments/random_py_environment_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for utils.random_py_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.utils import test_utils\n\n\nclass RandomPyEnvironmentTest(parameterized.TestCase, test_utils.TestCase):\n\n  def testEnvResetAutomatically(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(obs_spec)\n\n    time_step = env.step([0])\n    self.assertTrue(np.all(time_step.observation >= -10))\n    self.assertTrue(np.all(time_step.observation <= 10))\n    self.assertTrue(time_step.is_first())\n\n    while not time_step.is_last():\n      time_step = env.step([0])\n      self.assertTrue(np.all(time_step.observation >= -10))\n      self.assertTrue(np.all(time_step.observation <= 10))\n\n    time_step = env.step([0])\n    self.assertTrue(np.all(time_step.observation >= -10))\n    self.assertTrue(np.all(time_step.observation <= 10))\n    self.assertTrue(time_step.is_first())\n\n  @parameterized.named_parameters([\n      (\'OneStep\', 1),\n      (\'FiveSteps\', 5),\n  ])\n  def testEnvMinDuration(self, min_duration):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, episode_end_probability=0.9, min_duration=min_duration)\n    num_episodes = 100\n\n    for _ in range(num_episodes):\n      time_step = env.step([0])\n      self.assertTrue(time_step.is_first())\n      num_steps = 0\n      while not time_step.is_last():\n        time_step = env.step([0])\n        num_steps += 1\n      self.assertGreaterEqual(num_steps, min_duration)\n\n  @parameterized.named_parameters([\n      (\'OneStep\', 1),\n      (\'FiveSteps\', 5),\n  ])\n  def testEnvMaxDuration(self, max_duration):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, episode_end_probability=0.1, max_duration=max_duration)\n    num_episodes = 100\n\n    for _ in range(num_episodes):\n      time_step = env.step([0])\n      self.assertTrue(time_step.is_first())\n      num_steps = 0\n      while not time_step.is_last():\n        time_step = env.step([0])\n        num_steps += 1\n      self.assertLessEqual(num_steps, max_duration)\n\n  def testEnvChecksActions(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n\n    env.step(np.array([[0, 0], [0, 0]]))\n\n    with self.assertRaises(ValueError):\n      env.step([0])\n\n  def testRewardFnCalled(self):\n\n    def reward_fn(unused_step_type, action, unused_observation):\n      return action\n\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    observation_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        observation_spec, action_spec, reward_fn=reward_fn)\n\n    time_step = env.step(1)  # No reward in first time_step\n    self.assertEqual(0.0, time_step.reward)\n    time_step = env.step(1)\n    self.assertEqual(1, time_step.reward)\n\n  def testRendersImage(self):\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    observation_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        observation_spec, action_spec, render_size=(4, 4, 3))\n\n    env.reset()\n    img = env.render()\n\n    self.assertTrue(np.all(img < 256))\n    self.assertTrue(np.all(img >= 0))\n    self.assertEqual((4, 4, 3), img.shape)\n    self.assertEqual(np.uint8, img.dtype)\n\n  def testBatchSize(self):\n    batch_size = 3\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(obs_spec,\n                                                    batch_size=batch_size)\n\n    time_step = env.step([0])\n    self.assertEqual(time_step.observation.shape, (3, 2, 3))\n    self.assertEqual(time_step.reward.shape[0], batch_size)\n    self.assertEqual(time_step.discount.shape[0], batch_size)\n\n  def testCustomRewardFn(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    batch_size = 3\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec,\n        reward_fn=lambda *_: np.ones(batch_size),\n        batch_size=batch_size)\n    env._done = False\n    env.reset()\n    time_step = env.step([0])\n    self.assertSequenceAlmostEqual([1.0] * 3, time_step.reward)\n\n  def testRewardCheckerBatchSizeOne(self):\n    # Ensure batch size 1 with scalar reward works\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec,\n        reward_fn=lambda *_: np.array([1.0]),\n        batch_size=1)\n    env._done = False\n    env.reset()\n    time_step = env.step([0])\n    self.assertEqual(time_step.reward, 1.0)\n\n  def testRewardCheckerSizeMismatch(self):\n    # Ensure custom scalar reward with batch_size greater than 1 raises\n    # ValueError\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec,\n        reward_fn=lambda *_: 1.0,\n        batch_size=5)\n    env.reset()\n    env._done = False\n    with self.assertRaises(ValueError):\n      env.step([0])\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/random_tf_environment.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utility environment that creates random observations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nclass RandomTFEnvironment(tf_environment.TFEnvironment):\n  """"""Randomly generates observations following the given observation_spec.\n\n  If an action_spec is provided, it validates that the actions used to step the\n  environment are compatible with the given spec.\n  """"""\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               batch_size=1,\n               episode_end_probability=0.1):\n    """"""Initializes the environment.\n\n    Args:\n      time_step_spec: A `TimeStep` namedtuple containing `TensorSpec`s defining\n        the Tensors returned by `step()` (step_type, reward, discount, and\n        observation).\n      action_spec: A nest of BoundedTensorSpec representing the actions of the\n        environment.\n      batch_size: The batch size expected for the actions and observations.\n      episode_end_probability: Probability an episode will end when the\n        environment is stepped.\n    """"""\n    super(RandomTFEnvironment, self).__init__(\n        time_step_spec, action_spec, batch_size=batch_size)\n    self._episode_end_probability = episode_end_probability\n\n    def _variable_from_spec(name, spec):\n      full_shape = [batch_size] + spec.shape.as_list()\n      if not name:\n        name = ""spec_var""\n      return common.create_variable(name, 0, shape=full_shape, dtype=spec.dtype)\n\n    paths_and_specs = nest_utils.flatten_with_joined_paths(time_step_spec)\n    variables = [\n        _variable_from_spec(path, spec) for path, spec in paths_and_specs\n    ]\n    self._time_step_variables = tf.nest.pack_sequence_as(\n        time_step_spec, variables)\n\n  def _current_time_step(self):\n    """"""Returns the current `TimeStep`.""""""\n    return tf.nest.map_structure(tf.identity, self._time_step_variables)\n\n  def _update_time_step(self, time_step):\n    tf.nest.map_structure(lambda var, value: var.assign(value),\n                          self._time_step_variables, time_step)\n\n  def _sample_obs_and_reward(self):\n    sampled_observation = tensor_spec.sample_spec_nest(\n        self._time_step_spec.observation, outer_dims=(self.batch_size,))\n    sampled_reward = tensor_spec.sample_spec_nest(\n        self._time_step_spec.reward, outer_dims=(self.batch_size,))\n    return sampled_observation, sampled_reward\n\n  @common.function\n  def _reset(self):\n    """"""Resets the environment and returns the current time_step.""""""\n    obs, _ = self._sample_obs_and_reward()\n    time_step = ts.restart(obs, self._batch_size)\n    self._update_time_step(time_step)\n    return self._current_time_step()\n\n  @common.function(autograph=True)\n  def _step(self, action):\n    """"""Steps the environment according to the action.""""""\n    # Make sure the given action is compatible with the spec. We compare it to\n    # t[0] as the spec doesn\'t have a batch dim.\n    tf.nest.map_structure(\n        lambda spec, t: tf.Assert(spec.is_compatible_with(t[0]), [t]),\n        self._action_spec, action)\n\n    # If we generalize the batched data to not terminate at the same time, we\n    # will need to only reset the correct batch_inidices.\n    if self._time_step_variables.is_last()[0]:\n      return self.reset()\n\n    obs, reward = self._sample_obs_and_reward()\n    # Note: everything in the batch terminates at the same time.\n    if tf.random.uniform(()) < self._episode_end_probability:\n      time_step = ts.termination(obs, reward)\n    else:\n      time_step = ts.transition(obs, reward)\n\n    self._update_time_step(time_step)\n    return time_step\n'"
tf_agents/environments/random_tf_environment_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.random_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import random_tf_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass RandomTFEnvironmentTest(test_utils.TestCase):\n\n  def setUp(self):\n    self.observation_spec = tensor_spec.TensorSpec((2, 3), tf.float32)\n    self.time_step_spec = ts.time_step_spec(self.observation_spec)\n    self.action_spec = tensor_spec.TensorSpec((2,), tf.float32)\n    self.random_env = random_tf_environment.RandomTFEnvironment(\n        self.time_step_spec, self.action_spec)\n\n  def test_state_saved_after_reset(self):\n    initial_time_step = self.evaluate(self.random_env.reset())\n    current_time_step = self.evaluate(self.random_env.current_time_step())\n\n    np.testing.assert_almost_equal(initial_time_step.step_type,\n                                   current_time_step.step_type)\n    np.testing.assert_almost_equal(initial_time_step.observation,\n                                   current_time_step.observation)\n    np.testing.assert_almost_equal(initial_time_step.discount,\n                                   current_time_step.discount)\n    np.testing.assert_almost_equal(initial_time_step.reward,\n                                   current_time_step.reward)\n\n  def test_state_saved_after_step(self):\n    self.evaluate(self.random_env.reset())\n    random_action = self.evaluate(\n        tensor_spec.sample_spec_nest(self.action_spec, outer_dims=(1,)))\n\n    expected_time_step = self.evaluate(self.random_env.step(random_action))\n    current_time_step = self.evaluate(self.random_env.current_time_step())\n\n    np.testing.assert_almost_equal(expected_time_step.step_type,\n                                   current_time_step.step_type)\n    np.testing.assert_almost_equal(expected_time_step.observation,\n                                   current_time_step.observation)\n    np.testing.assert_almost_equal(expected_time_step.discount,\n                                   current_time_step.discount)\n    np.testing.assert_almost_equal(expected_time_step.reward,\n                                   current_time_step.reward)\n\n  def test_auto_reset(self):\n    time_step = self.evaluate(self.random_env.reset())\n    random_action = self.evaluate(\n        tensor_spec.sample_spec_nest(self.action_spec, outer_dims=(1,)))\n\n    attempts = 0\n\n    # With a 1/10 chance of resetting on each step, the probability of failure\n    # after 500 attempts should be 0.9^500, roughly 1e-23. If we miss more than\n    # 500 attempts, we can safely assume the test is broken.\n    while not time_step.is_last() and attempts < 500:\n      time_step = self.evaluate(self.random_env.step(random_action))\n      attempts += 1\n\n    self.assertLess(attempts, 500)\n    self.assertTrue(time_step.is_last())\n\n    current_time_step = self.evaluate(self.random_env.current_time_step())\n    self.assertTrue(current_time_step.is_last())\n\n    first_time_step = self.evaluate(self.random_env.step(random_action))\n    self.assertTrue(first_time_step.is_first())\n\n  def test_step_batched_action(self):\n    self.evaluate(self.random_env.reset())\n    random_action = self.evaluate(\n        tensor_spec.sample_spec_nest(self.action_spec, outer_dims=(5,)))\n\n    self.evaluate(self.random_env.step(random_action))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/suite_atari.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Suite for loading Atari Gym environments.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Dict, Optional, Sequence, Text\n\nimport atari_py  # pylint: disable=unused-import\nimport gin\nimport gym\nimport numpy as np\n\nfrom tf_agents.environments import atari_preprocessing\nfrom tf_agents.environments import atari_wrappers\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_gym\n\nfrom tf_agents.typing import types\n\n\n# Typical Atari 2600 Gym environment with some basic preprocessing.\nDEFAULT_ATARI_GYM_WRAPPERS = (atari_preprocessing.AtariPreprocessing,)\n# The following is just AtariPreprocessing with frame stacking. Performance wise\n# it\'s much better to have stacking implemented as part of replay-buffer/agent.\n# As soon as this functionality in TF-Agents is ready and verified, this set of\n# wrappers will be removed.\nDEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING = DEFAULT_ATARI_GYM_WRAPPERS + (\n    atari_wrappers.FrameStack4,)\n\n\n@gin.configurable\ndef game(name: Text = \'Pong\',\n         obs_type: Text = \'image\',\n         mode: Text = \'NoFrameskip\',\n         version: Text = \'v0\') -> Text:\n  """"""Generates the full name for the game.\n\n  Args:\n    name: String. Ex. Pong, SpaceInvaders, ...\n    obs_type: String, type of observation. Ex. \'image\' or \'ram\'.\n    mode: String. Ex. \'\', \'NoFrameskip\' or \'Deterministic\'.\n    version: String. Ex. \'v0\' or \'v4\'.\n\n  Returns:\n    The full name for the game.\n  """"""\n  assert obs_type in [\'image\', \'ram\']\n  assert mode in [\'\', \'NoFrameskip\', \'Deterministic\']\n  assert version in [\'v0\', \'v4\']\n  if obs_type == \'ram\':\n    name = \'{}-ram\'.format(name)\n  return \'{}{}-{}\'.format(name, mode, version)\n\n\n@gin.configurable\ndef load(\n    environment_name: Text,\n    discount: types.Int = 1.0,\n    max_episode_steps: Optional[types.Int] = None,\n    gym_env_wrappers: Sequence[\n        types.GymEnvWrapper] = DEFAULT_ATARI_GYM_WRAPPERS,\n    env_wrappers: Sequence[types.PyEnvWrapper] = (),\n    spec_dtype_map: Optional[Dict[gym.Space, np.dtype]] = None\n) -> py_environment.PyEnvironment:\n  """"""Loads the selected environment and wraps it with the specified wrappers.""""""\n  if spec_dtype_map is None:\n    spec_dtype_map = {gym.spaces.Box: np.uint8}\n\n  gym_spec = gym.spec(environment_name)\n  gym_env = gym_spec.make()\n\n  if max_episode_steps is None and gym_spec.max_episode_steps is not None:\n    max_episode_steps = gym_spec.max_episode_steps\n\n  return suite_gym.wrap_env(\n      gym_env,\n      discount=discount,\n      max_episode_steps=max_episode_steps,\n      gym_env_wrappers=gym_env_wrappers,\n      time_limit_wrapper=atari_wrappers.AtariTimeLimit,\n      env_wrappers=env_wrappers,\n      spec_dtype_map=spec_dtype_map,\n      auto_reset=False)\n'"
tf_agents/environments/suite_atari_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for third_party.py.tf_agents.environments.suite_atari.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport numpy as np\n\nfrom tf_agents.environments import atari_wrappers\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.utils import test_utils\n\nFLAGS = flags.FLAGS\n\n# Atari ROMs are placed in atari_py.get_game_path(\'.\')\n\n\nclass SuiteAtariTest(test_utils.TestCase):\n\n  def testGameName(self):\n    name = suite_atari.game(\'Pong\')\n    self.assertEqual(name, \'PongNoFrameskip-v0\')\n\n  def testGameObsType(self):\n    name = suite_atari.game(\'Pong\', obs_type=\'ram\')\n    self.assertEqual(name, \'Pong-ramNoFrameskip-v0\')\n\n  def testGameMode(self):\n    name = suite_atari.game(\'Pong\', mode=\'Deterministic\')\n    self.assertEqual(name, \'PongDeterministic-v0\')\n\n  def testGameVersion(self):\n    name = suite_atari.game(\'Pong\', version=\'v4\')\n    self.assertEqual(name, \'PongNoFrameskip-v4\')\n\n  def testGameSetAll(self):\n    name = suite_atari.game(\'Pong\', \'ram\', \'Deterministic\', \'v4\')\n    self.assertEqual(name, \'Pong-ramDeterministic-v4\')\n\n  def testAtariEnvRegistered(self):\n    env = suite_atari.load(\'Pong-v0\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, atari_wrappers.AtariTimeLimit)\n\n  def testAtariObsSpec(self):\n    env = suite_atari.load(\'Pong-v0\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertEqual(np.uint8, env.observation_spec().dtype)\n    self.assertEqual((84, 84, 1), env.observation_spec().shape)\n\n  def testAtariActionSpec(self):\n    env = suite_atari.load(\'Pong-v0\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertEqual(np.int64, env.action_spec().dtype)\n    self.assertEqual((), env.action_spec().shape)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/suite_bsuite.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Suite for loading bsuite gym environments.\n\nOsband et al., Behaviour Suite for Reinforcement Learning, 2019.\nhttps://github.com/deepmind/bsuite/\n\nFollow https://github.com/deepmind/bsuite#getting-started to install bsuite\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\nimport gin\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_gym\n\n_TRY_IMPORT = True  # pylint: disable=g-statement-before-imports\n\n# pylint: disable=g-import-not-at-top\nif _TRY_IMPORT:\n  try:\n    from bsuite import bsuite\n    from bsuite.utils import gym_wrapper\n  except ImportError:\n    bsuite = None\nelse:\n  from bsuite import bsuite\n  from bsuite.utils import gym_wrapper\n# pylint: enable=g-import-not-at-top\n\n\ndef is_available() -> bool:\n  return bsuite is not None\n\n\n@gin.configurable\ndef load(bsuite_id: Text,\n         record: bool = True,\n         save_path: Optional[Text] = None,\n         logging_mode: Text = \'csv\',\n         overwrite: bool = False) -> py_environment.PyEnvironment:\n  """"""Loads the selected environment.\n\n  Args:\n    bsuite_id: a bsuite_id specifies a bsuite experiment. For an example\n      `bsuite_id` ""deep_sea/7"" will be 7th level of the ""deep_sea"" task.\n    record: whether to log bsuite results.\n    save_path: the directory to save bsuite results.\n    logging_mode: which form of logging to use for bsuite results\n      [\'csv\', \'sqlite\', \'terminal\'].\n    overwrite: overwrite csv logging if found.\n\n  Returns:\n    A PyEnvironment instance.\n  """"""\n  if record:\n    raw_env = bsuite.load_and_record(\n        bsuite_id=bsuite_id,\n        save_path=save_path,\n        logging_mode=logging_mode,\n        overwrite=overwrite)\n  else:\n    raw_env = bsuite.load_from_id(bsuite_id=bsuite_id)\n  gym_env = gym_wrapper.GymFromDMEnv(raw_env)\n  return suite_gym.wrap_env(gym_env)\n'"
tf_agents/environments/suite_bsuite_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.suite_bsuite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_bsuite\nfrom tf_agents.utils import test_utils\n\n\nclass SuiteBsuiteTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(SuiteBsuiteTest, self).setUp()\n    if not suite_bsuite.is_available():\n      self.skipTest(\'bsuite is not available.\')\n\n  def tearDown(self):\n    gin.clear_config()\n    super(SuiteBsuiteTest, self).tearDown()\n\n  def testBsuiteEnvRegisteredWithRecord(self):\n    env = suite_bsuite.load(\n        \'deep_sea/0\', record=True, save_path=None, logging_mode=\'terminal\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n\n  def testBsuiteEnvRegistered(self):\n    env = suite_bsuite.load(\n        \'deep_sea/0\', record=False)\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n\n  def testGinConfig(self):\n    gin.parse_config_file(\n        test_utils.test_src_dir_path(\'environments/configs/suite_bsuite.gin\')\n    )\n    env = suite_bsuite.load()\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/suite_dm_control.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Suite for loading DeepMind Control Suite environments.\n\nFollow these instructions to install it:\n\nhttps://github.com/deepmind/dm_control#installation-and-requirements\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Sequence, Text\n\nimport gin\nfrom tf_agents.environments import dm_control_wrapper\nfrom tf_agents.environments import py_environment\nfrom tf_agents.typing import types\n\n_TRY_IMPORT = True  # pylint: disable=g-statement-before-imports\n\nif _TRY_IMPORT:\n  try:\n    from dm_control import suite  # pylint: disable=g-import-not-at-top\n    from dm_control.suite.wrappers import pixels  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    suite = None\nelse:\n  from dm_control import suite  # pylint: disable=g-import-not-at-top\n  from dm_control.suite.wrappers import pixels  # pylint: disable=g-import-not-at-top\n\n\ndef is_available() -> bool:\n  return suite is not None\n\n\ndef _load_env(domain_name: Text,\n              task_name: Text,\n              task_kwargs=None,\n              environment_kwargs=None,\n              visualize_reward: bool = False):\n  """"""Loads a DM environment.\n\n  Args:\n    domain_name: A string containing the name of a domain.\n    task_name: A string containing the name of a task.\n    task_kwargs: Optional `dict` of keyword arguments for the task.\n    environment_kwargs: Optional `dict` specifying keyword arguments for the\n      environment.\n    visualize_reward: Optional `bool`. If `True`, object colours in rendered\n      frames are set to indicate the reward at each step. Default `False`.\n\n  Returns:\n    The requested environment.\n\n  Raises:\n    ImportError: if dm_control module was not available.\n  """"""\n\n  if not is_available():\n    raise ImportError(\'dm_control module is not available.\')\n  return suite.load(\n      domain_name,\n      task_name,\n      task_kwargs=task_kwargs,\n      environment_kwargs=environment_kwargs,\n      visualize_reward=visualize_reward)\n\n\n@gin.configurable\ndef load(\n    domain_name: Text,\n    task_name: Text,\n    task_kwargs=None,\n    environment_kwargs=None,\n    visualize_reward: bool = False,\n    render_kwargs=None,\n    env_wrappers: Sequence[types.PyEnvWrapper] = ()\n) -> py_environment.PyEnvironment:\n  """"""Returns an environment from a domain name, task name and optional settings.\n\n  Args:\n    domain_name: A string containing the name of a domain.\n    task_name: A string containing the name of a task.\n    task_kwargs: Optional `dict` of keyword arguments for the task.\n    environment_kwargs: Optional `dict` specifying keyword arguments for the\n      environment.\n    visualize_reward: Optional `bool`. If `True`, object colours in rendered\n      frames are set to indicate the reward at each step. Default `False`.\n    render_kwargs: Optional `dict` of keyword arguments for rendering.\n    env_wrappers: Iterable with references to wrapper classes to use on the\n      wrapped environment.\n\n  Returns:\n    The requested environment.\n\n  Raises:\n    ImportError: if dm_control module was not available.\n  """"""\n  dm_env = _load_env(\n      domain_name,\n      task_name,\n      task_kwargs=task_kwargs,\n      environment_kwargs=environment_kwargs,\n      visualize_reward=visualize_reward)\n\n  env = dm_control_wrapper.DmControlWrapper(dm_env, render_kwargs)\n\n  for wrapper in env_wrappers:\n    env = wrapper(env)\n\n  return env\n\n\n@gin.configurable\ndef load_pixels(\n    domain_name: Text,\n    task_name: Text,\n    observation_key: Text = \'pixels\',\n    pixels_only: bool = True,\n    task_kwargs=None,\n    environment_kwargs=None,\n    visualize_reward: bool = False,\n    render_kwargs=None,\n    env_wrappers: Sequence[types.PyEnvWrapper] = ()\n) -> py_environment.PyEnvironment:\n  """"""Returns an environment from a domain name, task name and optional settings.\n\n  Args:\n    domain_name: A string containing the name of a domain.\n    task_name: A string containing the name of a task.\n    observation_key: Optional custom string specifying the pixel observation\'s\n      key in the `OrderedDict` of observations. Defaults to \'pixels\'.\n    pixels_only: If True (default), the original set of \'state\' observations\n      returned by the wrapped environment will be discarded, and the\n      `OrderedDict` of observations will only contain pixels. If False, the\n      `OrderedDict` will contain the original observations as well as the pixel\n      observations.\n    task_kwargs: Optional `dict` of keyword arguments for the task.\n    environment_kwargs: Optional `dict` specifying keyword arguments for the\n      environment.\n    visualize_reward: Optional `bool`. If `True`, object colours in rendered\n      frames are set to indicate the reward at each step. Default `False`.\n    render_kwargs: Optional `dict` of keyword arguments for rendering.\n    env_wrappers: Iterable with references to wrapper classes to use on the\n      wrapped environment.\n\n  Returns:\n    The requested environment.\n\n  Raises:\n    ImportError: if dm_control module was not available.\n  """"""\n  dm_env = _load_env(\n      domain_name,\n      task_name,\n      task_kwargs=task_kwargs,\n      environment_kwargs=environment_kwargs,\n      visualize_reward=visualize_reward)\n\n  dm_env = pixels.Wrapper(\n      dm_env,\n      pixels_only=pixels_only,\n      render_kwargs=render_kwargs,\n      observation_key=observation_key)\n  env = dm_control_wrapper.DmControlWrapper(dm_env, render_kwargs)\n\n  for wrapper in env_wrappers:\n    env = wrapper(env)\n\n  return env\n'"
tf_agents/environments/suite_dm_control_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for dm_control_wrapper.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import utils\nfrom tf_agents.utils import test_utils\n\n\nclass SuiteDMControlTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(SuiteDMControlTest, self).setUp()\n    if not suite_dm_control.is_available():\n      self.skipTest(\'dm_control is not available.\')\n\n  def testEnvRegistered(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n\n    utils.validate_py_environment(env)\n\n  def testObservationSpec(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    obs_spec = env.observation_spec()\n    self.assertEqual(np.float32, obs_spec[\'position\'].dtype)\n    self.assertEqual((4,), obs_spec[\'position\'].shape)\n\n  def testActionSpec(self):\n    env = suite_dm_control.load(\'ball_in_cup\', \'catch\')\n    action_spec = env.action_spec()\n    self.assertEqual(np.float32, action_spec.dtype)\n    self.assertEqual((2,), action_spec.shape)\n\n  def testPixelObservationSpec(self):\n    render_kwargs = dict(width=100, height=50)\n    env = suite_dm_control.load_pixels(\'ball_in_cup\', \'catch\',\n                                       render_kwargs=render_kwargs)\n    obs_spec = env.observation_spec()\n\n    self.assertEqual(np.uint8, obs_spec[\'pixels\'].dtype)\n    self.assertEqual((50, 100, 3), obs_spec[\'pixels\'].shape)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/suite_gym.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Suite for loading Gym Environments.\n\nNote we use gym.spec(env_id).make() on gym envs to avoid getting a TimeLimit\nwrapper on the environment. OpenAI\'s TimeLimit wrappers terminate episodes\nwithout indicating if the failure is due to the time limit, or due to negative\nagent behaviour. This prevents us from setting the appropriate discount value\nfor the final step of an episode. To prevent that we extract the step limit\nfrom the environment specs and utilize our TimeLimit wrapper.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Callable, Dict, Optional, Sequence, Text\n\nimport gin\nimport gym\nimport numpy as np\n\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.typing import types\n\nTimeLimitWrapperType = Callable[[py_environment.PyEnvironment, int],\n                                py_environment.PyEnvironment]\n\n\n@gin.configurable\ndef load(\n    environment_name: Text,\n    discount: types.Float = 1.0,\n    max_episode_steps: Optional[types.Int] = None,\n    gym_env_wrappers: Sequence[types.GymEnvWrapper] = (),\n    env_wrappers: Sequence[types.PyEnvWrapper] = (),\n    spec_dtype_map: Optional[Dict[gym.Space, np.dtype]] = None,\n    gym_kwargs=None) -> py_environment.PyEnvironment:\n  """"""Loads the selected environment and wraps it with the specified wrappers.\n\n  Note that by default a TimeLimit wrapper is used to limit episode lengths\n  to the default benchmarks defined by the registered environments.\n\n  Args:\n    environment_name: Name for the environment to load.\n    discount: Discount to use for the environment.\n    max_episode_steps: If None the max_episode_steps will be set to the default\n      step limit defined in the environment\'s spec. No limit is applied if set\n      to 0 or if there is no max_episode_steps set in the environment\'s spec.\n    gym_env_wrappers: Iterable with references to wrapper classes to use\n      directly on the gym environment.\n    env_wrappers: Iterable with references to wrapper classes to use on the\n      gym_wrapped environment.\n    spec_dtype_map: A dict that maps gym spaces to np dtypes to use as the\n      default dtype for the arrays. An easy way how to configure a custom\n      mapping through Gin is to define a gin-configurable function that returns\n      desired mapping and call it in your Gin congif file, for example:\n      `suite_gym.load.spec_dtype_map = @get_custom_mapping()`.\n    gym_kwargs: The kwargs to pass to the Gym environment class.\n\n  Returns:\n    A PyEnvironment instance.\n  """"""\n  gym_kwargs = gym_kwargs if gym_kwargs else {}\n  gym_spec = gym.spec(environment_name)\n  gym_env = gym_spec.make(**gym_kwargs)\n\n  if max_episode_steps is None and gym_spec.max_episode_steps is not None:\n    max_episode_steps = gym_spec.max_episode_steps\n\n  return wrap_env(\n      gym_env,\n      discount=discount,\n      max_episode_steps=max_episode_steps,\n      gym_env_wrappers=gym_env_wrappers,\n      env_wrappers=env_wrappers,\n      spec_dtype_map=spec_dtype_map)\n\n\n@gin.configurable\ndef wrap_env(\n    gym_env: gym.Env,\n    discount: types.Float = 1.0,\n    max_episode_steps: Optional[types.Int] = None,\n    gym_env_wrappers: Sequence[types.GymEnvWrapper] = (),\n    time_limit_wrapper: TimeLimitWrapperType = wrappers.TimeLimit,\n    env_wrappers: Sequence[types.PyEnvWrapper] = (),\n    spec_dtype_map: Optional[Dict[gym.Space, np.dtype]] = None,\n    auto_reset: bool = True) -> py_environment.PyEnvironment:\n  """"""Wraps given gym environment with TF Agent\'s GymWrapper.\n\n  Note that by default a TimeLimit wrapper is used to limit episode lengths\n  to the default benchmarks defined by the registered environments.\n\n  Args:\n    gym_env: An instance of OpenAI gym environment.\n    discount: Discount to use for the environment.\n    max_episode_steps: Used to create a TimeLimitWrapper. No limit is applied\n      if set to None or 0. Usually set to `gym_spec.max_episode_steps` in `load.\n    gym_env_wrappers: Iterable with references to wrapper classes to use\n      directly on the gym environment.\n    time_limit_wrapper: Wrapper that accepts (env, max_episode_steps) params to\n      enforce a TimeLimit. Usuaully this should be left as the default,\n      wrappers.TimeLimit.\n    env_wrappers: Iterable with references to wrapper classes to use on the\n      gym_wrapped environment.\n    spec_dtype_map: A dict that maps gym specs to tf dtypes to use as the\n      default dtype for the tensors. An easy way how to configure a custom\n      mapping through Gin is to define a gin-configurable function that returns\n      desired mapping and call it in your Gin config file, for example:\n      `suite_gym.load.spec_dtype_map = @get_custom_mapping()`.\n    auto_reset: If True (default), reset the environment automatically after a\n      terminal state is reached.\n\n  Returns:\n    A PyEnvironment instance.\n  """"""\n\n  for wrapper in gym_env_wrappers:\n    gym_env = wrapper(gym_env)\n  env = gym_wrapper.GymWrapper(\n      gym_env,\n      discount=discount,\n      spec_dtype_map=spec_dtype_map,\n      auto_reset=auto_reset,\n  )\n\n  if max_episode_steps is not None and max_episode_steps > 0:\n    env = time_limit_wrapper(env, max_episode_steps)\n\n  for wrapper in env_wrappers:\n    env = wrapper(env)\n\n  return env\n'"
tf_agents/environments/suite_gym_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.environments.suite_gym.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport gin\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import wrappers\nfrom tf_agents.utils import test_utils\n\n\nclass SuiteGymTest(test_utils.TestCase):\n\n  def tearDown(self):\n    gin.clear_config()\n    super(SuiteGymTest, self).tearDown()\n\n  def test_load_adds_time_limit_steps(self):\n    env = suite_gym.load(\'CartPole-v1\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n  def test_load_disable_step_limit(self):\n    env = suite_gym.load(\'CartPole-v1\', max_episode_steps=0)\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertNotIsInstance(env, wrappers.TimeLimit)\n\n  def test_load_disable_wrappers_applied(self):\n    duration_wrapper = functools.partial(wrappers.TimeLimit, duration=10)\n    env = suite_gym.load(\n        \'CartPole-v1\', max_episode_steps=0, env_wrappers=(duration_wrapper,))\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n  def test_custom_max_steps(self):\n    env = suite_gym.load(\'CartPole-v1\', max_episode_steps=5)\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n    self.assertEqual(5, env._duration)\n\n  def testGinConfig(self):\n    gin.parse_config_file(\n        test_utils.test_src_dir_path(\'environments/configs/suite_gym.gin\')\n    )\n    env = suite_gym.load()\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n  def test_gym_kwargs_argument(self):\n    env = suite_gym.load(\'FrozenLake-v0\', gym_kwargs={\'map_name\': \'4x4\'})\n    self.assertTupleEqual(env.unwrapped.desc.shape, (4, 4))\n\n    env = suite_gym.load(\'FrozenLake-v0\', gym_kwargs={\'map_name\': \'8x8\'})\n    self.assertTupleEqual(env.unwrapped.desc.shape, (8, 8))\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/suite_mujoco.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Suite for loading MuJoCo Gym environments.\n\n**NOTE**: Mujoco requires separated installation.\n\nFollow the instructions at:\n\nhttps://github.com/openai/mujoco-py\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Dict, Optional, Sequence, Text\n\nimport gin\nimport gym\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.typing import types\n\n_TRY_IMPORT = True  # pylint: disable=g-statement-before-imports\n\nif _TRY_IMPORT:\n  try:\n    import mujoco_py  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    mujoco_py = None\nelse:\n  import mujoco_py  # pylint: disable=g-import-not-at-top\n\n\ndef is_available() -> bool:\n  return mujoco_py is not None\n\n\n@gin.configurable\ndef load(\n    environment_name: Text,\n    discount: types.Float = 1.0,\n    max_episode_steps: Optional[types.Int] = None,\n    gym_env_wrappers: Sequence[types.GymEnvWrapper] = (),\n    env_wrappers: Sequence[types.PyEnvWrapper] = (),\n    spec_dtype_map: Optional[Dict[gym.Space, np.dtype]] = None\n) -> py_environment.PyEnvironment:\n  """"""Loads the selected environment and wraps it with the specified wrappers.\n\n  Note that by default a TimeLimit wrapper is used to limit episode lengths\n  to the default benchmarks defined by the registered environments.\n\n  Args:\n    environment_name: Name for the environment to load.\n    discount: Discount to use for the environment.\n    max_episode_steps: If None the max_episode_steps will be set to the default\n      step limit defined in the environment\'s spec. No limit is applied if set\n      to 0 or if there is no timestep_limit set in the environment\'s spec.\n    gym_env_wrappers: Iterable with references to wrapper classes to use\n      directly on the gym environment.\n    env_wrappers: Iterable with references to wrapper classes to use on the\n      gym_wrapped environment.\n    spec_dtype_map: A dict that maps gym specs to tf dtypes to use as the\n      default dtype for the tensors. An easy way how to configure a custom\n      mapping through Gin is to define a gin-configurable function that returns\n      desired mapping and call it in your Gin config file, for example:\n      `suite_gym.load.spec_dtype_map = @get_custom_mapping()`.\n\n  Returns:\n    A PyEnvironmentBase instance.\n  """"""\n  return suite_gym.load(environment_name, discount, max_episode_steps,\n                        gym_env_wrappers, env_wrappers, spec_dtype_map)\n'"
tf_agents/environments/suite_mujoco_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.suite_mujoco.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import wrappers\nfrom tf_agents.utils import test_utils\n\n\nclass SuiteMujocoTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(SuiteMujocoTest, self).setUp()\n    if not suite_mujoco.is_available():\n      self.skipTest(\'suite_mujoco is not available.\')\n\n  def tearDown(self):\n    gin.clear_config()\n    super(SuiteMujocoTest, self).tearDown()\n\n  def testMujocoEnvRegistered(self):\n    env = suite_mujoco.load(\'HalfCheetah-v2\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n  def testObservationSpec(self):\n    env = suite_mujoco.load(\'HalfCheetah-v2\')\n    self.assertEqual(np.float32, env.observation_spec().dtype)\n    self.assertEqual((17,), env.observation_spec().shape)\n\n  def testActionSpec(self):\n    env = suite_mujoco.load(\'HalfCheetah-v2\')\n    self.assertEqual(np.float32, env.action_spec().dtype)\n    self.assertEqual((6,), env.action_spec().shape)\n\n  def testGinConfig(self):\n    gin.parse_config_file(\n        test_utils.test_src_dir_path(\'environments/configs/suite_mujoco.gin\')\n    )\n    env = suite_mujoco.load()\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/suite_pybullet.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Suite for loading pybullet Gym environments.\n\nImporting pybullet_envs registers the environments. Once this is done the\nregular gym loading mechanism used in suite_gym will generate pybullet envs.\n\nFor a list of registered pybullet environments take a look at:\n  pybullet_envs/__init__.py\n\nTo visualize a pybullet environment as it is being run you can launch the\nexample browser BEFORE you start the training.\n\n```bash\nExampleBrowser -- --start_demo_name=""PhysicsServer""\n```\n""""""\nimport gin\nfrom tf_agents.environments import suite_gym\n\n# pylint: disable=unused-import\nimport pybullet_envs\n# pylint: enable=unused-import\n\nload = gin.external_configurable(suite_gym.load, \'suite_pybullet.load\')\n'"
tf_agents/environments/suite_pybullet_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests tf_agents.environments.suite_pybullet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import suite_pybullet\nfrom tf_agents.environments import wrappers\nfrom tf_agents.utils import test_utils\n\n\nclass SuitePybulletTest(test_utils.TestCase):\n\n  def tearDown(self):\n    gin.clear_config()\n    super(SuitePybulletTest, self).tearDown()\n\n  def testPybulletEnvRegistered(self):\n    env = suite_pybullet.load(\'InvertedPendulumBulletEnv-v0\')\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n  def testGinConfig(self):\n    gin.parse_config_file(\n        test_utils.test_src_dir_path(\'environments/configs/suite_pybullet.gin\')\n    )\n    env = suite_pybullet.load()\n    self.assertIsInstance(env, py_environment.PyEnvironment)\n    self.assertIsInstance(env, wrappers.TimeLimit)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/test_envs.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Collection of simple environments useful for testing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\n\nfrom tf_agents import specs\nfrom tf_agents.environments import py_environment\nfrom tf_agents.trajectories import time_step as ts\n\nfrom tf_agents.typing import types\n\n\n# TODO(b/156832202) Replace with EpisodeCountingEnv\n@gin.configurable\nclass CountingEnv(py_environment.PyEnvironment):\n  """"""Counts up in the observation as steps are taken.\n\n  Step observation values are of the form (10 ** episodes + self._current_step)\n  if steps_per_episode is greater than 10 then on reset the value of the\n  observation count may go down.\n  """"""\n\n  def __init__(self, steps_per_episode: types.Int = 10):\n    self._steps_per_episode = steps_per_episode\n\n    self._episodes = 0\n    self._current_step = np.array(0, dtype=np.int32)\n    super(CountingEnv, self).__init__()\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return specs.BoundedArraySpec((), dtype=np.int32)\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return specs.BoundedArraySpec((), dtype=np.int32, minimum=0, maximum=1)\n\n  def _step(self, action):\n    del action  # Unused.\n    if self._current_time_step.is_last():\n      return self._reset()\n    self._current_step = np.array(1 + self._current_step, dtype=np.int32)\n    if self._current_step < self._steps_per_episode:\n      return ts.transition(self._get_observation(), 0)\n    return ts.termination(self._get_observation(), 1)\n\n  def _get_observation(self):\n    if self._episodes:\n      return np.array(10 * self._episodes + self._current_step, dtype=np.int32)\n    return self._current_step\n\n  def _reset(self):\n    if self._current_time_step and self._current_time_step.is_last():\n      self._episodes += 1\n    self._current_step = np.array(0, dtype=np.int32)\n    return ts.restart(self._get_observation())\n\n\n@gin.configurable\nclass EpisodeCountingEnv(py_environment.PyEnvironment):\n  """"""Counts up in the observation as steps are taken.\n\n  Step observation values are of the form (episodes, self._current_step)\n  """"""\n\n  def __init__(self, steps_per_episode=10):\n    self._steps_per_episode = steps_per_episode\n\n    self._episodes = 0\n    self._steps = 0\n    super(EpisodeCountingEnv, self).__init__()\n\n  def observation_spec(self):\n    return (specs.BoundedArraySpec((), dtype=np.int32),\n            specs.BoundedArraySpec((), dtype=np.int32))\n\n  def action_spec(self):\n    return specs.BoundedArraySpec((), dtype=np.int32, minimum=0, maximum=1)\n\n  def _step(self, action):\n    del action  # Unused.\n    if self._current_time_step.is_last():\n      return self._reset()\n    self._steps += 1\n    if self._steps < self._steps_per_episode:\n      return ts.transition(self._get_observation(), 0)\n    return ts.termination(self._get_observation(), 1)\n\n  def _get_observation(self):\n    return (np.array(self._episodes, dtype=np.int32),\n            np.array(self._steps, dtype=np.int32))\n\n  def _reset(self):\n    if self._current_time_step and self._current_time_step.is_last():\n      self._episodes += 1\n      self._steps = 0\n    return ts.restart(self._get_observation())\n'"
tf_agents/environments/test_envs_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.environments.test_envs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tf_agents.environments import test_envs\nfrom tf_agents.environments import utils as env_utils\nfrom tf_agents.utils import test_utils\n\n\nclass CountingEnvTest(test_utils.TestCase):\n\n  def test_sequential(self):\n    num_episodes = 3\n    steps_per_episode = 4\n    env = test_envs.CountingEnv(steps_per_episode)\n\n    for episode in range(num_episodes):\n      step = 0\n      time_step = env.reset()\n      self.assertEqual(episode * 10 + step, time_step.observation)\n      while not time_step.is_last():\n        time_step = env.step(0)\n        step += 1\n        self.assertEqual(episode * 10 + step, time_step.observation)\n      self.assertEqual(episode * 10 + steps_per_episode, time_step.observation)\n\n  def test_validate_specs(self):\n    env = test_envs.CountingEnv(steps_per_episode=15)\n    env_utils.validate_py_environment(env, episodes=10)\n\n\nclass EpisodeCountingEnvTest(test_utils.TestCase):\n\n  def test_sequential(self):\n    num_episodes = 3\n    steps_per_episode = 4\n    env = test_envs.EpisodeCountingEnv(steps_per_episode=steps_per_episode)\n\n    for episode in range(num_episodes):\n      step = 0\n      time_step = env.reset()\n      self.assertAllEqual((episode, step), time_step.observation)\n      while not time_step.is_last():\n        time_step = env.step(0)\n        step += 1\n        self.assertAllEqual((episode, step), time_step.observation)\n      self.assertAllEqual((episode, steps_per_episode), time_step.observation)\n\n  def test_validate_specs(self):\n    env = test_envs.EpisodeCountingEnv(steps_per_episode=15)\n    env_utils.validate_py_environment(env, episodes=10)\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/tf_environment.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TensorFlow RL Environment API.\n\nRepresents a task to be solved, an environment has to define three methods:\n`reset`, `current_time_step` and `step`.\n\n- The reset() method returns current time_step after resetting the environment.\n- The current_time_step() method returns current time_step initializing the\nenvironmet if needed. Only needed in graph mode.\n- The step(action) method applies the action and returns the new time_step.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass TFEnvironment(object):\n  """"""Abstract base class for TF RL environments.\n\n  The `current_time_step()` method returns current `time_step`, resetting the\n  environment if necessary.\n\n  The `step(action)` method applies the action and returns the new `time_step`.\n  This method will also reset the environment if needed and ignore the action in\n  that case.\n\n  The `reset()` method returns `time_step` that results from an environment\n  reset and is guaranteed to have step_type=ts.FIRST\n\n  The `reset()` method is only needed for explicit resets. In general, the\n  environment will reset automatically when needed, for example, when no\n  episode was started or when it reaches a step after the end of the episode\n  (i.e. step_type=ts.LAST).\n\n  Example for collecting an episode in eager mode:\n\n    tf_env = TFEnvironment()\n\n    # reset() creates the initial time_step and resets the environment.\n    time_step = tf_env.reset()\n    while not time_step.is_last():\n      action_step = policy.action(time_step)\n      time_step = tf_env.step(action_step.action)\n\n  Example of simple use in graph mode:\n\n    tf_env = TFEnvironment()\n\n    # current_time_step() creates the initial TimeStep.\n    time_step = tf_env.current_time_step()\n    action_step = policy.action(time_step)\n    # Apply the action and return the new TimeStep.\n    next_time_step = tf_env.step(action_step.action)\n\n    sess.run([time_step, action_step, next_time_step])\n\n  Example with explicit resets in graph mode:\n\n    reset_op = tf_env.reset()\n    time_step = tf_env.current_time_step()\n    action_step = policy.action(time_step)\n    # Apply the action and return the new TimeStep.\n    next_time_step = tf_env.step(action_step.action)\n\n    # The environment will initialize before starting.\n    sess.run([time_step, action_step, next_time_step])\n    # This will force reset the Environment.\n    sess.run(reset_op)\n    # This will apply a new action in the environment.\n    sess.run([time_step, action_step, next_time_step])\n\n  Example of random actions in graph mode:\n\n    tf_env = TFEnvironment()\n\n    # Action needs to depend on the time_step using control_dependencies.\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tensor_spec.sample_bounded_spec(tf_env.action_spec())\n    next_time_step = tf_env.step(action)\n\n    sess.run([time_step, action, next_time_step])\n\n  Example of collecting full episodes with a while_loop:\n\n    tf_env = TFEnvironment()\n\n    # reset() creates the initial time_step\n    time_step = tf_env.reset()\n    c = lambda t: tf.logical_not(t.is_last())\n    body = lambda t: [tf_env.step(t.observation)]\n\n    final_time_step = tf.while_loop(c, body, [time_step])\n\n    sess.run(final_time_step)\n\n  """"""\n\n  def __init__(self, time_step_spec=None, action_spec=None, batch_size=1):\n    """"""Initializes the environment.\n\n    Meant to be called by subclass constructors.\n\n    Args:\n      time_step_spec: A `TimeStep` namedtuple containing `TensorSpec`s\n        defining the Tensors returned by\n        `step()` (step_type, reward, discount, and observation).\n      action_spec: A nest of BoundedTensorSpec representing the actions of the\n        environment.\n      batch_size: The batch size expected for the actions and observations.\n    """"""\n\n    self._time_step_spec = time_step_spec\n    self._action_spec = action_spec\n    self._batch_size = batch_size\n\n  def time_step_spec(self):\n    """"""Describes the `TimeStep` specs of Tensors returned by `step()`.\n\n    Returns:\n      A `TimeStep` namedtuple containing `TensorSpec` objects defining the\n      Tensors returned by `step()`, i.e.\n      (step_type, reward, discount, observation).\n    """"""\n    return self._time_step_spec\n\n  def action_spec(self):\n    """"""Describes the specs of the Tensors expected by `step(action)`.\n\n    `action` can be a single Tensor, or a nested dict, list or tuple of\n    Tensors.\n\n    Returns:\n      An single `TensorSpec`, or a nested dict, list or tuple of\n      `TensorSpec` objects, which describe the shape and\n      dtype of each Tensor expected by `step()`.\n    """"""\n    return self._action_spec\n\n  def observation_spec(self):\n    """"""Defines the `TensorSpec` of observations provided by the environment.\n\n    Returns:\n      A `TensorSpec`, or a nested dict, list or tuple of\n      `TensorSpec` objects, which describe the observation.\n    """"""\n    return self.time_step_spec().observation\n\n  def reward_spec(self):\n    """"""Defines the `TensorSpec` of rewards provided by the environment.\n\n    Returns:\n      A `TensorSpec`, or a nested dict, list or tuple of\n      `TensorSpec` objects, which describe the observation.\n    """"""\n    return self.time_step_spec().reward\n\n  @property\n  def batched(self):\n    return True\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  def current_time_step(self):\n    """"""Returns the current `TimeStep`.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` value.\n        reward: Reward at this time_step.\n        discount: A discount in the range [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n    """"""\n    return self._current_time_step()\n\n  def reset(self):\n    """"""Resets the environment and returns the current time_step.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` value.\n        reward: Reward at this time_step.\n        discount: A discount in the range [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n    """"""\n    return self._reset()\n\n  def step(self, action):\n    """"""Steps the environment according to the action.\n\n    If the environment returned a `TimeStep` with `StepType.LAST` at the\n    previous step, this call to `step` should reset the environment (note that\n    it is expected that whoever defines this method, calls reset in this case),\n    start a new sequence and `action` will be ignored.\n\n    This method will also start a new sequence if called after the environment\n    has been constructed and `reset()` has not been called. In this case\n    `action` will be ignored.\n\n    Expected sequences look like:\n\n      time_step -> action -> next_time_step\n\n    The action should depend on the previous time_step for correctness.\n\n    Args:\n      action: A Tensor, or a nested dict, list or tuple of Tensors\n        corresponding to `action_spec()`.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` value.\n        reward: Reward at this time_step.\n        discount: A discount in the range [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n    """"""\n    return self._step(action)\n\n  def render(self):\n    """"""Renders a frame from the environment.\n\n    Raises:\n      NotImplementedError: If the environment does not support rendering.\n    """"""\n    raise NotImplementedError(\'No rendering support.\')\n\n  @abc.abstractmethod\n  def _current_time_step(self):\n    """"""Returns the current `TimeStep`.""""""\n\n  @abc.abstractmethod\n  def _reset(self):\n    """"""Resets the environment and returns the current time_step.""""""\n\n  @abc.abstractmethod\n  def _step(self, action):\n    """"""Steps the environment according to the action.""""""\n'"
tf_agents/environments/tf_environment_test.py,57,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for environments.tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\nFIRST = ts.StepType.FIRST\nMID = ts.StepType.MID\nLAST = ts.StepType.LAST\n\n\nclass TFEnvironmentMock(tf_environment.TFEnvironment):\n  """"""MockTFEnvironment.\n\n  Stores all actions taken in `actions_taken`. The returned values are:\n\n  step: FIRST, 1., 0., [0]\n  step: MID, 1., 0., [1]\n  step: LAST, 0., 1. [2]\n  ...repeated\n  """"""\n\n  def __init__(self, initial_state=0, dtype=tf.int64, scope=\'TFEnviroment\'):\n    self._dtype = dtype\n    self._scope = scope\n    self._initial_state = tf.cast(initial_state, dtype=self._dtype)\n    observation_spec = specs.TensorSpec([1], self._dtype, \'observation\')\n    action_spec = specs.BoundedTensorSpec([], tf.int32, minimum=0, maximum=10)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    super(TFEnvironmentMock, self).__init__(time_step_spec, action_spec)\n    self._state = common.create_variable(\'state\', initial_state,\n                                         dtype=self._dtype)\n    self.steps = common.create_variable(\'steps\', 0)\n    self.episodes = common.create_variable(\'episodes\', 0)\n    self.resets = common.create_variable(\'resets\', 0)\n\n  def _current_time_step(self):\n    def first():\n      return (tf.constant(FIRST, dtype=tf.int32),\n              tf.constant(0.0, dtype=tf.float32),\n              tf.constant(1.0, dtype=tf.float32))\n    def mid():\n      return (tf.constant(MID, dtype=tf.int32),\n              tf.constant(0.0, dtype=tf.float32),\n              tf.constant(1.0, dtype=tf.float32))\n    def last():\n      return (tf.constant(LAST, dtype=tf.int32),\n              tf.constant(1.0, dtype=tf.float32),\n              tf.constant(0.0, dtype=tf.float32))\n    state_value = tf.math.mod(self._state.value(), 3)\n    step_type, reward, discount = tf.case(\n        [(tf.equal(state_value, FIRST), first),\n         (tf.equal(state_value, MID), mid),\n         (tf.equal(state_value, LAST), last)],\n        exclusive=True, strict=True)\n    return ts.TimeStep(step_type, reward, discount, state_value)\n\n  def _reset(self):\n    increase_resets = self.resets.assign_add(1)\n    with tf.control_dependencies([increase_resets]):\n      reset_op = self._state.assign(self._initial_state)\n    with tf.control_dependencies([reset_op]):\n      time_step = self.current_time_step()\n    return time_step\n\n  def _step(self, action):\n    action = tf.convert_to_tensor(value=action)\n    with tf.control_dependencies(tf.nest.flatten(action)):\n      state_assign = self._state.assign_add(1)\n    with tf.control_dependencies([state_assign]):\n      state_value = self._state.value()\n      increase_steps = tf.cond(\n          pred=tf.equal(tf.math.mod(state_value, 3), FIRST),\n          true_fn=self.steps.value,\n          false_fn=lambda: self.steps.assign_add(1))\n      increase_episodes = tf.cond(\n          pred=tf.equal(tf.math.mod(state_value, 3), LAST),\n          true_fn=lambda: self.episodes.assign_add(1),\n          false_fn=self.episodes.value)\n    with tf.control_dependencies([increase_steps, increase_episodes]):\n      return self.current_time_step()\n\n\nclass TFEnvironmentTest(tf.test.TestCase):\n\n  def testResetOp(self):\n    tf_env = TFEnvironmentMock()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf_env.reset())\n    self.assertEqual(1, self.evaluate(tf_env.resets))\n    self.assertEqual(0, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testMultipleReset(self):\n    tf_env = TFEnvironmentMock()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf_env.reset())\n    self.assertEqual(1, self.evaluate(tf_env.resets))\n    self.evaluate(tf_env.reset())\n    self.assertEqual(2, self.evaluate(tf_env.resets))\n    self.evaluate(tf_env.reset())\n    self.assertEqual(3, self.evaluate(tf_env.resets))\n    self.assertEqual(0, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testFirstTimeStep(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    time_step = self.evaluate(time_step)\n    self.assertEqual(FIRST, time_step.step_type)\n    self.assertEqual(0.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n    self.assertEqual([0], time_step.observation)\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(0, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testFirstStepState(self):\n    tf_env = TFEnvironmentMock()\n    tf_env.current_time_step()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(0, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testOneStep(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant(1)\n    next_time_step = tf_env.step(action)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    time_step, next_time_step = self.evaluate([time_step, next_time_step])\n\n    self.assertEqual(FIRST, time_step.step_type)\n    self.assertEqual(0., time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n    self.assertEqual([0], time_step.observation)\n\n    self.assertEqual(MID, next_time_step.step_type)\n    self.assertEqual(0., next_time_step.reward)\n    self.assertEqual(1.0, next_time_step.discount)\n    self.assertEqual([1], next_time_step.observation)\n\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(1, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testCurrentStep(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881612\')\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant(1)\n    next_time_step = tf_env.step(action)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step_np, next_time_step_np = self.evaluate([time_step, next_time_step])\n    self.assertEqual(FIRST, time_step_np.step_type)\n    self.assertEqual(0., time_step_np.reward)\n    self.assertEqual(1.0, time_step_np.discount)\n    self.assertEqual([0], time_step_np.observation)\n\n    self.assertEqual(MID, next_time_step_np.step_type)\n    self.assertEqual(0., next_time_step_np.reward)\n    self.assertEqual(1.0, next_time_step_np.discount)\n    self.assertEqual([1], next_time_step_np.observation)\n\n    time_step_np, next_time_step_np = self.evaluate([time_step, next_time_step])\n    self.assertEqual(MID, time_step_np.step_type)\n    self.assertEqual(0., time_step_np.reward)\n    self.assertEqual(1.0, time_step_np.discount)\n    self.assertEqual([1], time_step_np.observation)\n\n    self.assertEqual(LAST, next_time_step_np.step_type)\n    self.assertEqual(1., next_time_step_np.reward)\n    self.assertEqual(0.0, next_time_step_np.discount)\n    self.assertEqual([2], next_time_step_np.observation)\n\n    time_step_np = self.evaluate(time_step)\n    self.assertEqual(LAST, time_step_np.step_type)\n    self.assertEqual(1., time_step_np.reward)\n    self.assertEqual(0.0, time_step_np.discount)\n    self.assertEqual([2], time_step_np.observation)\n\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(2, self.evaluate(tf_env.steps))\n    self.assertEqual(1, self.evaluate(tf_env.episodes))\n\n  def testTwoStepsDependenceOnTheFirst(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant(1)\n    time_step = tf_env.step(action)\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant(2)\n    time_step = self.evaluate(tf_env.step(action))\n    self.assertEqual(LAST, time_step.step_type)\n    self.assertEqual(1., time_step.reward)\n    self.assertEqual(0.0, time_step.discount)\n    self.assertEqual([2], time_step.observation)\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(2, self.evaluate(tf_env.steps))\n    self.assertEqual(1, self.evaluate(tf_env.episodes))\n\n  def testAutoReset(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    with tf.control_dependencies([time_step.step_type]):\n      time_step = tf_env.step(1)\n    with tf.control_dependencies([time_step.step_type]):\n      time_step = tf_env.step(2)\n    with tf.control_dependencies([time_step.step_type]):\n      time_step = self.evaluate(tf_env.step(3))\n    self.assertEqual(FIRST, time_step.step_type)\n    self.assertEqual(0.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n    self.assertEqual([0], time_step.observation)\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(2, self.evaluate(tf_env.steps))\n    self.assertEqual(1, self.evaluate(tf_env.episodes))\n\n  def testFirstObservationIsPreservedAfterTwoSteps(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    time_step_np = self.evaluate(time_step)\n    self.assertEqual([0], time_step_np.observation)\n    time_step = tf_env.step(1)\n    with tf.control_dependencies([time_step.step_type]):\n      next_time_step = tf_env.step(2)\n\n    observation_np, _ = self.evaluate([time_step.observation, next_time_step])\n\n    self.assertEqual([1], observation_np)\n\n  def testRandomAction(self):\n    tf_env = TFEnvironmentMock()\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.random.uniform([], minval=0, maxval=10, dtype=tf.int32)\n    next_time_step = tf_env.step(action)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    [time_step_np, next_time_step_np] = self.evaluate(\n        [time_step, next_time_step])\n    self.assertEqual([0], time_step_np.observation)\n    self.assertEqual([1], next_time_step_np.observation)\n    self.assertEqual(0, self.evaluate(tf_env.resets))\n    self.assertEqual(1, self.evaluate(tf_env.steps))\n    self.assertEqual(0, self.evaluate(tf_env.episodes))\n\n  def testRunEpisode(self):\n    tf_env = TFEnvironmentMock()\n    c = lambda t: tf.logical_not(t.is_last())\n    body = lambda t: [tf_env.step(t.observation)]\n\n    @common.function\n    def run_episode():\n      time_step = tf_env.reset()\n      return tf.while_loop(cond=c, body=body, loop_vars=[time_step])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    [final_time_step_np] = self.evaluate(run_episode())\n    self.assertEqual([2], final_time_step_np.step_type)\n    self.assertEqual([2], final_time_step_np.observation)\n    self.assertEqual(1, self.evaluate(tf_env.resets))\n    self.assertEqual(2, self.evaluate(tf_env.steps))\n    self.assertEqual(1, self.evaluate(tf_env.episodes))\n    # Run another episode.\n    [final_time_step_np] = self.evaluate(run_episode())\n    self.assertEqual([2], final_time_step_np.step_type)\n    self.assertEqual([2], final_time_step_np.observation)\n    self.assertEqual(2, self.evaluate(tf_env.resets))\n    self.assertEqual(4, self.evaluate(tf_env.steps))\n    self.assertEqual(2, self.evaluate(tf_env.episodes))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/tf_py_environment.py,26,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wrapper for PyEnvironments into TFEnvironments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nfrom multiprocessing import pool\nimport threading\n\nfrom absl import logging\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.specs import tensor_spec\n# TODO(b/123022201): Use tf.autograph instead.\nfrom tensorflow.python.autograph.impl import api as autograph  # pylint:disable=g-direct-tensorflow-import  # TF internal\nfrom tensorflow.python.framework import tensor_shape  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ndef _pack_named_sequence(flat_inputs, input_spec, batch_shape):\n  """"""Assembles back a nested structure that has been flattened.""""""\n  named_inputs = []\n  for flat_input, spec in zip(flat_inputs, tf.nest.flatten(input_spec)):\n    named_input = tf.identity(flat_input, name=spec.name)\n    if not tf.executing_eagerly():\n      named_input.set_shape(batch_shape.concatenate(spec.shape))\n    named_inputs.append(named_input)\n\n  nested_inputs = tf.nest.pack_sequence_as(input_spec, named_inputs)\n  return nested_inputs\n\n\n@contextlib.contextmanager\ndef _check_not_called_concurrently(lock):\n  """"""Checks the returned context is not executed concurrently with any other.""""""\n  if not lock.acquire(False):  # Non-blocking.\n    raise RuntimeError(\n        \'Detected concurrent execution of TFPyEnvironment ops. Make sure the \'\n        \'appropriate step_state is passed to step().\')\n  try:\n    yield\n  finally:\n    lock.release()\n\n\n@gin.configurable\nclass TFPyEnvironment(tf_environment.TFEnvironment):\n  """"""Exposes a Python environment as an in-graph TF environment.\n\n  This class supports Python environments that return nests of arrays as\n  observations and accept nests of arrays as actions. The nest structure is\n  reflected in the in-graph environment\'s observation and action structure.\n\n  Implementation notes:\n\n  * Since `tf.py_func` deals in lists of tensors, this class has some additional\n    `tf.nest.flatten` and `tf.nest.pack_structure_as` calls.\n\n  * This class currently cast rewards and discount to float32.\n  """"""\n\n  def __init__(self, environment, check_dims=False, isolation=False):\n    """"""Initializes a new `TFPyEnvironment`.\n\n    Args:\n      environment: Environment to interact with, implementing\n        `py_environment.PyEnvironment`.  Or a `callable` that returns\n        an environment of this form.  If a `callable` is provided and\n        `thread_isolation` is provided, the callable is executed in the\n        dedicated thread.\n      check_dims: Whether should check batch dimensions of actions in `step`.\n      isolation: If this value is `False` (default), interactions with\n        the environment will occur within whatever thread the methods of the\n        `TFPyEnvironment` are run from.  For example, in TF graph mode, methods\n        like `step` are called from multiple threads created by the TensorFlow\n        engine; calls to step the environment are guaranteed to be sequential,\n        but not from the same thread.  This creates problems for environments\n        that are not thread-safe.\n\n        Using isolation ensures not only that a dedicated thread (or\n        thread-pool) is used to interact with the environment, but also that\n        interaction with the environment happens in a serialized manner.\n\n        If `isolation == True`, a dedicated thread is created for\n        interactions with the environment.\n\n        If `isolation` is an instance of `multiprocessing.pool.Pool` (this\n        includes instances of `multiprocessing.pool.ThreadPool`, nee\n        `multiprocessing.dummy.Pool` and `multiprocessing.Pool`, then this\n        pool is used to interact with the environment.\n\n        **NOTE** If using `isolation` with a `BatchedPyEnvironment`, ensure\n        you create the `BatchedPyEnvironment` with `multithreading=False`, since\n        otherwise the multithreading in that wrapper reverses the effects of\n        this one.\n\n    Raises:\n      TypeError: If `environment` is not an instance of\n        `py_environment.PyEnvironment` or subclasses, or is a callable that does\n        not return an instance of `PyEnvironment`.\n      TypeError: If `isolation` is not `True`, `False`, or an instance of\n        `multiprocessing.pool.Pool`.\n    """"""\n    if not isolation:\n      self._pool = None\n    elif isinstance(isolation, pool.Pool):\n      self._pool = isolation\n    elif isolation:\n      self._pool = pool.ThreadPool(1)\n    else:\n      raise TypeError(\n          \'isolation should be True, False, or an instance of \'\n          \'a multiprocessing Pool or ThreadPool.  Saw: {}\'.format(isolation))\n\n    if callable(environment):\n      environment = self._execute(environment)\n    if not isinstance(environment, py_environment.PyEnvironment):\n      raise TypeError(\n          \'Environment should implement py_environment.PyEnvironment\')\n\n    if not environment.batched:\n      # If executing in an isolated thread, do not enable multiprocessing for\n      # this environment.\n      environment = batched_py_environment.BatchedPyEnvironment(\n          [environment], multithreading=not self._pool)\n    self._env = environment\n    self._check_dims = check_dims\n\n    if isolation and getattr(self._env, \'_parallel_execution\', None):\n      logging.warning(\n          \'Wrapped environment is executing in parallel.  \'\n          \'Perhaps it is a BatchedPyEnvironment with multithreading=True, \'\n          \'or it is a ParallelPyEnvironment.  This conflicts with the \'\n          \'`isolation` arg passed to TFPyEnvironment: interactions with the \'\n          \'wrapped environment are no longer guaranteed to happen in a common \'\n          \'thread.  Environment: %s\', (self._env,))\n\n    action_spec = tensor_spec.from_spec(self._env.action_spec())\n    time_step_spec = tensor_spec.from_spec(self._env.time_step_spec())\n    batch_size = self._env.batch_size if self._env.batch_size else 1\n    self._render_shape = None\n\n    super(TFPyEnvironment, self).__init__(time_step_spec,\n                                          action_spec,\n                                          batch_size)\n\n    # Gather all the dtypes and shapes of the elements in time_step.\n    self._time_step_dtypes = [\n        s.dtype for s in tf.nest.flatten(self.time_step_spec())\n    ]\n\n    self._time_step = None\n    self._lock = threading.Lock()\n\n  def __getattr__(self, name):\n    """"""Enables access attributes of the wrapped PyEnvironment.\n\n    Use with caution since methods of the PyEnvironment can be incompatible\n    with TF.\n\n    Args:\n      name: Name of the attribute.\n\n    Returns:\n      The attribute.\n    """"""\n    if name in self.__dict__:\n      return getattr(self, name)\n    return getattr(self._env, name)\n\n  def close(self):\n    """"""Send close to wrapped env & also to the isolation pool + join it.\n\n    Only closes pool when `isolation` was provided at init time.\n    """"""\n    self._env.close()\n    if self._pool:\n      self._pool.join()\n      self._pool.close()\n      self._pool = None\n\n  @property\n  def pyenv(self):\n    """"""Returns the underlying Python environment.""""""\n    return self._env\n\n  def _execute(self, fn, *args, **kwargs):\n    if not self._pool:\n      return fn(*args, **kwargs)\n    return self._pool.apply(fn, args=args, kwds=kwargs)\n\n  # TODO(b/123585179): Simplify this using py_environment.current_time_step().\n  # There currently is a bug causing py_function to resolve variables\n  # incorrectly when used inside autograph code. This decorator tells autograph\n  # to call it directly instead of converting it when called from other\n  # autograph-converted functions.\n  # TODO(b/123600776): Remove override.\n  @autograph.do_not_convert()\n  def _current_time_step(self):\n    """"""Returns the current ts.TimeStep.\n\n    Returns:\n      A `TimeStep` tuple of:\n        step_type: A scalar int32 tensor representing the `StepType` value.\n        reward: A float32 tensor representing the reward at this\n          timestep.\n        discount: A scalar float32 tensor representing the discount [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n    """"""\n\n    def _current_time_step_py():\n      with _check_not_called_concurrently(self._lock):\n        if self._time_step is None:\n          self._time_step = self._env.reset()\n        return tf.nest.flatten(self._time_step)\n\n    def _isolated_current_time_step_py():\n      return self._execute(_current_time_step_py)\n\n    with tf.name_scope(\'current_time_step\'):\n      outputs = tf.numpy_function(\n          _isolated_current_time_step_py,\n          [],  # No inputs.\n          self._time_step_dtypes,\n          name=\'current_time_step_py_func\')\n      return self._time_step_from_numpy_function_outputs(outputs)\n\n  # Make sure this is called without conversion from tf.function.\n  # TODO(b/123600776): Remove override.\n  @autograph.do_not_convert()\n  def _reset(self):\n    """"""Returns the current `TimeStep` after resetting the environment.\n\n    Returns:\n      A `TimeStep` tuple of:\n        step_type: A scalar int32 tensor representing the `StepType` value.\n        reward: A float32 tensor representing the reward at this\n          timestep.\n        discount: A scalar float32 tensor representing the discount [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n    """"""\n\n    def _reset_py():\n      with _check_not_called_concurrently(self._lock):\n        self._time_step = self._env.reset()\n\n    def _isolated_reset_py():\n      return self._execute(_reset_py)\n\n    with tf.name_scope(\'reset\'):\n      reset_op = tf.numpy_function(\n          _isolated_reset_py,\n          [],  # No inputs.\n          [],\n          name=\'reset_py_func\')\n      with tf.control_dependencies([reset_op]):\n        return self.current_time_step()\n\n  # Make sure this is called without conversion from tf.function.\n  # TODO(b/123600776): Remove override.\n  @autograph.do_not_convert()\n  def _step(self, actions):\n    """"""Returns a TensorFlow op to step the environment.\n\n    Args:\n      actions: A Tensor, or a nested dict, list or tuple of Tensors\n        corresponding to `action_spec()`.\n\n    Returns:\n      A `TimeStep` tuple of:\n        step_type: A scalar int32 tensor representing the `StepType` value.\n        reward: A float32 tensor representing the reward at this\n          time_step.\n        discount: A scalar float32 tensor representing the discount [0, 1].\n        observation: A Tensor, or a nested dict, list or tuple of Tensors\n          corresponding to `observation_spec()`.\n\n    Raises:\n      ValueError: If any of the actions are scalars or their major axis is known\n      and is not equal to `self.batch_size`.\n    """"""\n\n    def _step_py(*flattened_actions):\n      with _check_not_called_concurrently(self._lock):\n        packed = tf.nest.pack_sequence_as(\n            structure=self.action_spec(), flat_sequence=flattened_actions)\n        self._time_step = self._env.step(packed)\n        return tf.nest.flatten(self._time_step)\n\n    def _isolated_step_py(*flattened_actions):\n      return self._execute(_step_py, *flattened_actions)\n\n    with tf.name_scope(\'step\'):\n      flat_actions = [tf.identity(x) for x in tf.nest.flatten(actions)]\n      if self._check_dims:\n        for action in flat_actions:\n          dim_value = tensor_shape.dimension_value(action.shape[0])\n          if (action.shape.rank == 0 or\n              (dim_value is not None and dim_value != self.batch_size)):\n            raise ValueError(\n                \'Expected actions whose major dimension is batch_size (%d), \'\n                \'but saw action with shape %s:\\n   %s\' %\n                (self.batch_size, action.shape, action))\n      outputs = tf.numpy_function(\n          _isolated_step_py,\n          flat_actions,\n          self._time_step_dtypes,\n          name=\'step_py_func\')\n      return self._time_step_from_numpy_function_outputs(outputs)\n\n  def render(self, mode):\n    """"""Renders the environment.\n\n    Note for compatibility this will convert the image to uint8.\n\n    Args:\n      mode: One of [\'rgb_array\', \'human\']. Renders to an numpy array, or brings\n        up a window where the environment can be visualized.\n\n    Returns:\n      An ndarray of shape [width, height, 3] denoting an RGB image if mode is\n      `rgb_array`. Otherwise return nothing and render directly to a display\n      window.\n    Raises:\n      NotImplementedError: If the environment does not support rendering.\n    """"""\n\n    if not self._render_shape:\n      # Make sure the environment has been initialized.\n      self.current_time_step()\n      img = self._env.render(\'rgb_array\')\n      self._render_shape = img.shape\n\n    def _render(mode):\n      """"""Pywrapper fn to the environments render.""""""\n      # Mode might be passed down as bytes. If so convert to a str first.\n      if isinstance(mode, bytes):\n        mode = mode.decode(\'utf-8\')\n      if mode == \'rgb_array\':\n        img = self._env.render(mode)\n        img = img.astype(np.uint8, copy=False)\n        return img\n      elif mode == \'human\':\n        # Generate mock img to keep outputs the same.\n        self._env.render(mode)\n        return np.zeros(self._render_shape, dtype=np.uint8)\n\n    img = tf.numpy_function(\n        lambda mode: self._execute(_render, mode), [mode], [tf.uint8],\n        name=\'render_py_func\')\n\n    if not tf.executing_eagerly():\n      # Extract from list returned from np_function.\n      img = img[0]\n      img.set_shape(tf.TensorShape(self._render_shape))\n    return img\n\n  def _time_step_from_numpy_function_outputs(self, outputs):\n    """"""Forms a `TimeStep` from the output of the numpy_function outputs.""""""\n    batch_shape = () if not self.batched else (self.batch_size,)\n    batch_shape = tf.TensorShape(batch_shape)\n    time_step = _pack_named_sequence(outputs,\n                                     self.time_step_spec(),\n                                     batch_shape)\n    return time_step\n'"
tf_agents/environments/tf_py_environment_test.py,25,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for reinforment_learning.environment.tf_py_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport threading\n\nfrom absl.testing import parameterized\nfrom absl.testing.absltest import mock\nimport numpy as np\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import time_step as ts\n\nCOMMON_PARAMETERS = (\n    dict(batch_py_env=True, isolation=True),\n    dict(batch_py_env=False, isolation=True),\n    dict(batch_py_env=True, isolation=False),\n    dict(batch_py_env=False, isolation=False),\n)\n\n\ndef get(env, property_name):\n  if isinstance(env, batched_py_environment.BatchedPyEnvironment):\n    assert env.batch_size == 1\n    return getattr(env.envs[0], property_name)\n  return getattr(env, property_name)\n\n\nclass PYEnvironmentMock(py_environment.PyEnvironment):\n  """"""MockPyEnvironment.\n\n  Stores all actions taken in `actions_taken`. The returned values are:\n\n  step: step_type, discount, reward, observation\n\n  step: FIRST, 1., 0., [0]\n  step: MID, 1., 0., [1]\n  step: LAST, 0., 1. [2]\n  ...repeated\n  """"""\n\n  def __init__(self):\n    self.actions_taken = []\n    self.steps = 0\n    self.episodes = 0\n    self.resets = 0\n    self.last_call_thread_id = threading.current_thread().ident\n    self._state = 0\n\n  def _reset(self):\n    self._state = 0\n    self.resets += 1\n    self.last_call_thread_id = threading.current_thread().ident\n    return ts.restart([self._state])\n\n  def _step(self, action):\n    self._state = (self._state + 1) % 3\n    self.steps += 1\n    self.last_call_thread_id = threading.current_thread().ident\n    self.actions_taken.append(action)\n\n    observation = [self._state]\n    if self._state == 0:\n      return ts.restart(observation)\n    elif self._state == 2:\n      self.episodes += 1\n      return ts.termination(observation, reward=1.0)\n    return ts.transition(observation, reward=0.0)\n\n  def action_spec(self):\n    return specs.BoundedArraySpec(\n        [], np.int32, minimum=0, maximum=10, name=\'action\')\n\n  def observation_spec(self):\n    return specs.ArraySpec([], np.int64, name=\'observation\')\n\n  def render(self, mode):\n    if mode == \'rgb_array\':\n      return np.ones((4, 4, 3), dtype=np.uint8)\n\n\nclass PYEnvironmentMockNestedRewards(py_environment.PyEnvironment):\n  """"""Mock PyEnvironment with rewards that are nested dicts.""""""\n\n  def __init__(self):\n    self.last_call_thread_id = threading.current_thread().ident\n    self._state = 0\n    self._observation_spec = self.observation_spec()\n    self._action_spec = self.action_spec()\n    self._reward_spec = self.reward_spec()\n\n  def action_spec(self):\n    return specs.BoundedArraySpec(\n        [], np.int32, minimum=0, maximum=10, name=\'action\')\n\n  def observation_spec(self):\n    return specs.ArraySpec([], np.int64, name=\'observation\')\n\n  def reward_spec(self):\n    return {\n        \'reward\': specs.ArraySpec([], np.float32, name=\'reward\'),\n        \'constraint\': specs.ArraySpec([], np.float32, name=\'constraint\')\n    }\n\n  def _reset(self):\n    self._state = 0\n    self.last_call_thread_id = threading.current_thread().ident\n    return ts.restart(\n        [self._state], batch_size=1, reward_spec=self._reward_spec)\n\n  def _step(self, action):\n    self._state = (self._state + 1) % 3\n    self.last_call_thread_id = threading.current_thread().ident\n\n    observation = [self._state]\n    reward = {\n        \'constraint\': 2.,\n        \'reward\': 1.,\n    }\n    if self._state == 0:\n      return ts.restart(\n          observation, batch_size=1, reward_spec=self._reward_spec)\n    elif self._state == 2:\n      return ts.termination(observation, reward=reward)\n    return ts.transition(observation, reward=reward)\n\n\nclass TFPYEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testPyenv(self):\n    py_env = PYEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n    self.assertIsInstance(tf_env.pyenv,\n                          batched_py_environment.BatchedPyEnvironment)\n\n  def _get_py_env(self, batch_py_env, isolation, batch_size=None):\n    def _create_env():\n      if batch_size is None:\n        py_env = PYEnvironmentMock()\n      else:\n        py_env = [PYEnvironmentMock() for _ in range(batch_size)]\n      if batch_py_env:\n        py_env = batched_py_environment.BatchedPyEnvironment(\n            py_env if isinstance(py_env, list) else [py_env])\n      return py_env\n    # If using isolation, we\'ll pass a callable\n    return _create_env if isolation else _create_env()\n\n  def testMethodPropagation(self):\n    env = self._get_py_env(True, False, batch_size=1)\n    env.foo = mock.Mock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    tf_env.foo()\n    env.foo.assert_called_once()\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testActionSpec(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    self.assertTrue(tf_env.batched)\n    self.assertEqual(tf_env.batch_size, 1)\n    spec = tf_env.action_spec()\n    self.assertEqual(type(spec), specs.BoundedTensorSpec)\n    self.assertEqual(spec.dtype, tf.int32)\n    self.assertEqual(spec.shape, tf.TensorShape([]))\n    self.assertEqual(spec.name, \'action\')\n\n  def testObservationSpec(self):\n    py_env = PYEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n    spec = tf_env.observation_spec()\n    self.assertEqual(type(spec), specs.TensorSpec)\n    self.assertEqual(spec.dtype, tf.int64)\n    self.assertEqual(spec.shape, tf.TensorShape([]))\n    self.assertEqual(spec.name, \'observation\')\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testTimeStepSpec(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    spec = tf_env.time_step_spec()\n\n    # step_type\n    self.assertEqual(type(spec.step_type), specs.TensorSpec)\n    self.assertEqual(spec.step_type.dtype, tf.int32)\n    self.assertEqual(spec.step_type.shape, tf.TensorShape([]))\n\n    # reward\n    self.assertEqual(type(spec.reward), specs.TensorSpec)\n    self.assertEqual(spec.reward.dtype, tf.float32)\n    self.assertEqual(spec.reward.shape, tf.TensorShape([]))\n\n    # discount\n    self.assertEqual(type(spec.discount), specs.BoundedTensorSpec)\n    self.assertEqual(spec.discount.dtype, tf.float32)\n    self.assertEqual(spec.discount.shape, tf.TensorShape([]))\n    self.assertEqual(spec.discount.minimum, 0.0)\n    self.assertEqual(spec.discount.maximum, 1.0)\n\n    # observation\n    self.assertEqual(type(spec.observation), specs.TensorSpec)\n\n  @parameterized.parameters(\n      *COMMON_PARAMETERS)\n  def testResetOp(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    reset = tf_env.reset()\n    self.evaluate(reset)\n    self.assertEqual(1, get(tf_env.pyenv, \'resets\'))\n    self.assertEqual(0, get(tf_env.pyenv, \'steps\'))\n    self.assertEqual(0, get(tf_env.pyenv, \'episodes\'))\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testMultipleReset(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n\n    self.evaluate(tf_env.reset())\n    self.assertEqual(1, get(tf_env.pyenv, \'resets\'))\n    self.evaluate(tf_env.reset())\n    self.assertEqual(2, get(tf_env.pyenv, \'resets\'))\n    self.evaluate(tf_env.reset())\n    self.assertEqual(3, get(tf_env.pyenv, \'resets\'))\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testFirstTimeStep(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    time_step = tf_env.current_time_step()\n    time_step = self.evaluate(time_step)\n    self.assertAllEqual([ts.StepType.FIRST], time_step.step_type)\n    self.assertAllEqual([0.0], time_step.reward)\n    self.assertAllEqual([1.0], time_step.discount)\n    self.assertAllEqual([0], time_step.observation)\n    self.assertAllEqual([], get(tf_env.pyenv, \'actions_taken\'))\n    self.assertEqual(1, get(tf_env.pyenv, \'resets\'))\n    self.assertEqual(0, get(tf_env.pyenv, \'steps\'))\n    self.assertEqual(0, get(tf_env.pyenv, \'episodes\'))\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testOneStep(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant([1])\n    time_step = self.evaluate(tf_env.step(action))\n\n    self.assertAllEqual([ts.StepType.MID], time_step.step_type)\n    self.assertAllEqual([0.], time_step.reward)\n    self.assertAllEqual([1.0], time_step.discount)\n    self.assertAllEqual([1], time_step.observation)\n    self.assertAllEqual([1], get(tf_env.pyenv, \'actions_taken\'))\n    self.assertEqual(1, get(tf_env.pyenv, \'resets\'))\n    self.assertEqual(1, get(tf_env.pyenv, \'steps\'))\n    self.assertEqual(0, get(tf_env.pyenv, \'episodes\'))\n\n  @parameterized.parameters(dict(isolation=False), dict(isolation=True))\n  def testBatchedFirstTimeStepAndOneStep(self, isolation):\n    py_env = self._get_py_env(\n        batch_py_env=True, isolation=isolation, batch_size=3)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    self.assertEqual(tf_env.batch_size, 3)\n    time_step_0 = tf_env.current_time_step()\n    time_step_0_val = self.evaluate(time_step_0)\n\n    self.assertAllEqual([ts.StepType.FIRST] * 3, time_step_0_val.step_type)\n    self.assertAllEqual([0.0] * 3, time_step_0_val.reward)\n    self.assertAllEqual([1.0] * 3, time_step_0_val.discount)\n    self.assertAllEqual(np.array([0, 0, 0]), time_step_0_val.observation)\n    for py_env in tf_env.pyenv.envs:\n      self.assertEqual([], py_env.actions_taken)\n      self.assertEqual(1, py_env.resets)\n      self.assertEqual(0, py_env.steps)\n      self.assertEqual(0, py_env.episodes)\n\n    time_step_1 = tf_env.step(np.array([1, 1, 1]))\n\n    time_step_1_val = self.evaluate(time_step_1)\n\n    self.assertAllEqual([ts.StepType.MID] * 3, time_step_1_val.step_type)\n    self.assertAllEqual([0.] * 3, time_step_1_val.reward)\n    self.assertAllEqual([1.0] * 3, time_step_1_val.discount)\n    self.assertAllEqual(np.array([1, 1, 1]), time_step_1_val.observation)\n    for py_env in tf_env.pyenv.envs:\n      self.assertEqual([1], py_env.actions_taken)\n      self.assertEqual(1, py_env.resets)\n      self.assertEqual(1, py_env.steps)\n      self.assertEqual(0, py_env.episodes)\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testTwoStepsDependenceOnTheFirst(self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant([1])\n    time_step = tf_env.step(action)\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant([2])\n    time_step = self.evaluate(tf_env.step(action))\n\n    self.assertEqual(ts.StepType.LAST, time_step.step_type)\n    self.assertEqual([2], time_step.observation)\n    self.assertEqual(1., time_step.reward)\n    self.assertEqual(0., time_step.discount)\n    self.assertEqual([1, 2], get(tf_env.pyenv, \'actions_taken\'))\n\n  @parameterized.parameters(*COMMON_PARAMETERS)\n  def testFirstObservationIsPreservedAfterTwoSteps(\n      self, batch_py_env, isolation):\n    py_env = self._get_py_env(batch_py_env, isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant([1])\n    next_time_step = tf_env.step(action)\n    with tf.control_dependencies([next_time_step.step_type]):\n      action = tf.constant([2])\n    _, observation = self.evaluate([tf_env.step(action), time_step.observation])\n\n    self.assertEqual(np.array([0]), observation)\n\n  @parameterized.parameters(dict(isolation=False), dict(isolation=True))\n  def testIsolation(self, isolation):\n    py_env = self._get_py_env(batch_py_env=False, isolation=isolation)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env, isolation=isolation)\n    last_env_thread = lambda: get(tf_env.pyenv, \'last_call_thread_id\')\n    local_thread = threading.current_thread().ident\n    if isolation:\n      self.assertNotEqual(local_thread, last_env_thread())\n    else:\n      self.assertEqual(local_thread, last_env_thread())\n\n    # The remaining tests apply only to isolation == True\n    if not isolation:\n      return\n\n    init_env_thread = last_env_thread()\n    # Ensure that parallel computation does run in a thread different from the\n    # one the pyenv was initialized in: that isolation forced execution on a\n    # single dedicated threadpool.\n    for _ in range(30):\n      self.evaluate([tf_env.reset() for _ in range(16)])\n      self.assertEqual(init_env_thread, last_env_thread())\n      self.evaluate([tf_env.current_time_step() for _ in range(16)])\n      self.assertEqual(init_env_thread, last_env_thread())\n      self.evaluate([tf_env.step(tf.constant([1])) for _ in range(16)])\n      self.assertEqual(init_env_thread, last_env_thread())\n\n  def testRender(self):\n    py_env = self._get_py_env(False, False, batch_size=None)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n\n    img = self.evaluate(tf_env.render(\'rgb_array\'))\n    self.assertEqual(img.shape, (1, 4, 4, 3))\n    self.assertEqual(img.dtype, np.uint8)\n    img = self.evaluate(tf_env.render(\'human\'))\n    self.assertEqual(img.shape, (1, 4, 4, 3))\n    self.assertEqual(img.dtype, np.uint8)\n\n  def testRenderBatched(self):\n    py_env = self._get_py_env(True, False, batch_size=3)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n\n    img = self.evaluate(tf_env.render(\'rgb_array\'))\n    self.assertEqual(img.shape, (3, 4, 4, 3))\n    self.assertEqual(img.dtype, np.uint8)\n    img = self.evaluate(tf_env.render(\'human\'))\n    self.assertEqual(img.shape, (3, 4, 4, 3))\n    self.assertEqual(img.dtype, np.uint8)\n\n  def testOneStepNestedRewards(self):\n    py_env = PYEnvironmentMockNestedRewards()\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n    time_step = tf_env.current_time_step()\n    with tf.control_dependencies([time_step.step_type]):\n      action = tf.constant([1])\n    time_step = self.evaluate(tf_env.step(action))\n\n    self.assertAllEqual([ts.StepType.MID], time_step.step_type)\n    self.assertAllEqual([1.], time_step.reward[\'reward\'])\n    self.assertAllEqual([2.], time_step.reward[\'constraint\'])\n    self.assertAllEqual([1.0], time_step.discount)\n    self.assertAllEqual([1], time_step.observation)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/tf_wrappers.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wrappers for TF environments.\n\nUse tf_agents.environments.wrapper for PyEnvironments.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.specs import tensor_spec\n\n\nclass TFEnvironmentBaseWrapper(tf_environment.TFEnvironment):\n  """"""Base class for TFEnvrionment wrappers.""""""\n\n  def __init__(self, env):\n    super(TFEnvironmentBaseWrapper, self).__init__()\n    self._env = env\n\n  def __getattr__(self, name):\n    if name in self.__dict__:\n      return getattr(self, name)\n    return getattr(self._env, name)\n\n  def time_step_spec(self):\n    return self._env.time_step_spec()\n\n  def action_spec(self):\n    return self._env.action_spec()\n\n  def observation_spec(self):\n    return self._env.observation_spec()\n\n  @property\n  def batched(self):\n    return self._env.batched\n\n  @property\n  def batch_size(self):\n    return self._env.batch_size\n\n  def _current_time_step(self):\n    return self._env.current_time_step()\n\n  def _reset(self):\n    return self._env.reset()\n\n  def _step(self, action):\n    return self._env.step(action)\n\n  def render(self):\n    return self._env.render()\n\n\nclass OneHotActionWrapper(TFEnvironmentBaseWrapper):\n  """"""Converts discrete action to one_hot format.""""""\n\n  def __init__(self, env):\n    super(OneHotActionWrapper, self).__init__(env)\n    self._validate_action_spec()\n\n  def _validate_action_spec(self):\n\n    def _validate(action_spec):\n      if action_spec.dtype.is_integer and len(action_spec.shape.as_list()) > 1:\n        raise ValueError(\n            \'OneHotActionWrapper only supports actions with at most one \'\n            \'dimension! action_spec: {}\'.format(action_spec))\n\n    tf.nest.map_structure(_validate, self._env.action_spec())\n\n  def action_spec(self):\n\n    def convert_to_one_hot(action_spec):\n      """"""Convert action_spec to one_hot format.""""""\n      if action_spec.dtype.is_integer:\n        num_actions = action_spec.maximum - action_spec.minimum + 1\n        output_shape = action_spec.shape + (num_actions,)\n\n        return tensor_spec.BoundedTensorSpec(\n            shape=output_shape,\n            dtype=action_spec.dtype,\n            minimum=0,\n            maximum=1,\n            name=\'one_hot_action_spec\')\n      else:\n        return action_spec\n\n    return tf.nest.map_structure(convert_to_one_hot, self._env.action_spec())\n\n  def _step(self, action):\n    action = tf.argmax(action, axis=-1, output_type=action.dtype)\n    return self._env.step(action)\n'"
tf_agents/environments/tf_wrappers_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.environments.tf_wrappers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing.absltest import mock\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import random_tf_environment\nfrom tf_agents.environments import tf_wrappers\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass TFEnvironmentBaseWrapperTest(tf.test.TestCase):\n\n  def test_wrapped_method_propagation(self):\n    mock_env = mock.MagicMock()\n    env = tf_wrappers.TFEnvironmentBaseWrapper(mock_env)\n\n    env.time_step_spec()\n    self.assertEqual(1, mock_env.time_step_spec.call_count)\n\n    env.action_spec()\n    self.assertEqual(1, mock_env.action_spec.call_count)\n\n    env.observation_spec()\n    self.assertEqual(1, mock_env.observation_spec.call_count)\n\n    env.batched()\n    self.assertEqual(1, mock_env.batched.call_count)\n\n    env.batch_size()\n    self.assertEqual(1, mock_env.batch_size.call_count)\n\n    env.current_time_step()\n    self.assertEqual(1, mock_env.current_time_step.call_count)\n\n    env.reset()\n    self.assertEqual(1, mock_env.reset.call_count)\n\n    env.step(0)\n    self.assertEqual(1, mock_env.step.call_count)\n    mock_env.step.assert_called_with(0)\n\n    env.render()\n    self.assertEqual(1, mock_env.render.call_count)\n\n\ndef _build_test_env(obs_spec=None, action_spec=None, batch_size=2):\n  if obs_spec is None:\n    obs_spec = tensor_spec.BoundedTensorSpec((2, 3), tf.int32, -10, 10)\n  if action_spec is None:\n    action_spec = tensor_spec.BoundedTensorSpec((1,), tf.int32, 0, 4)\n  time_step_spec = ts.time_step_spec(obs_spec)\n  return random_tf_environment.RandomTFEnvironment(\n      time_step_spec, action_spec, batch_size=batch_size)\n\n\nclass OneHotActionWrapperTest(tf.test.TestCase):\n\n  def test_action_spec(self):\n    action_spec = tensor_spec.BoundedTensorSpec((1,), tf.int32, 0, 4)\n    env = _build_test_env(action_spec=action_spec)\n    wrapper = tf_wrappers.OneHotActionWrapper(env)\n    expected_action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(1, 5),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=1,\n        name=\'one_hot_action_spec\')\n    self.assertEqual(expected_action_spec, wrapper.action_spec())\n\n  def test_action_spec_nested(self):\n    action_spec = (tensor_spec.BoundedTensorSpec((1,), tf.int32, 0, 4),\n                   tensor_spec.TensorSpec((2, 2), tf.float32))\n    env = _build_test_env(action_spec=action_spec)\n    wrapper = tf_wrappers.OneHotActionWrapper(env)\n    expected_action_spec = (\n        tensor_spec.BoundedTensorSpec(\n            shape=(1, 5),\n            dtype=tf.int32,\n            minimum=0,\n            maximum=1,\n            name=\'one_hot_action_spec\'),\n        action_spec[1])\n    self.assertEqual(expected_action_spec, wrapper.action_spec())\n\n  def test_raises_invalid_action_spec(self):\n    action_spec = tensor_spec.BoundedTensorSpec((1, 1), tf.int32, 0, 4)\n    with self.assertRaisesRegexp(ValueError, \'at most one dimension\'):\n      tf_wrappers.OneHotActionWrapper(_build_test_env(action_spec=action_spec))\n\n  def test_step(self):\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 4)\n    env = _build_test_env(action_spec=action_spec)\n    mock_env = mock.Mock(wraps=env)\n    wrapper = tf_wrappers.OneHotActionWrapper(mock_env)\n    wrapper.reset()\n\n    wrapper.step(tf.constant([[0, 1, 0, 0, 0],\n                              [0, 0, 0, 1, 0]], tf.int32))\n    self.assertTrue(mock_env.step.called)\n    self.assertAllEqual([1, 3], mock_env.step.call_args[0][0])\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/environments/trajectory_replay.py,28,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Driver-like object that replays Trajectories.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\n@gin.configurable\nclass TrajectoryReplay(object):\n  """"""A helper that replays a policy against given `Trajectory` observations.\n\n  """"""\n\n  def __init__(\n      self,\n      policy,\n      time_major=False):\n    """"""Creates a TrajectoryReplay object.\n\n    TrajectoryReplay.run returns the actions and policy info of the new policy\n    assuming it saw the observations from the given trajectory.\n\n    Args:\n      policy: A tf_policy.TFPolicy policy.\n      time_major: If `True`, the tensors in `trajectory` passed to method `run`\n        are assumed to have shape `[time, batch, ...]`.  Otherwise (default)\n        they are assumed to have shape `[batch, time, ...]`.\n\n    Raises:\n      ValueError:\n        If policy is not an instance of tf_policy.TFPolicy.\n    """"""\n    self._policy = policy\n    self._time_major = time_major\n\n  def run(self, trajectory, policy_state=None):\n    """"""Apply the policy to trajectory steps and store actions/info.\n\n    If `self.time_major == True`, the tensors in `trajectory` are assumed to\n    have shape `[time, batch, ...]`.  Otherwise they are assumed to\n    have shape `[batch, time, ...]`.\n\n    Args:\n      trajectory: The `Trajectory` to run against.\n        If the replay class was created with `time_major=True`, then\n        the tensors in trajectory must be shaped `[time, batch, ...]`.\n        Otherwise they must be shaped `[batch, time, ...]`.\n      policy_state: (optional) A nest Tensor with initial step policy state.\n\n    Returns:\n      output_actions: A nest of the actions that the policy took.\n        If the replay class was created with `time_major=True`, then\n        the tensors here will be shaped `[time, batch, ...]`.  Otherwise\n        they\'ll be shaped `[batch, time, ...]`.\n      output_policy_info: A nest of the policy info that the policy emitted.\n        If the replay class was created with `time_major=True`, then\n        the tensors here will be shaped `[time, batch, ...]`.  Otherwise\n        they\'ll be shaped `[batch, time, ...]`.\n      policy_state: A nest Tensor with final step policy state.\n\n    Raises:\n      TypeError: If `policy_state` structure doesn\'t match\n        `self.policy.policy_state_spec`, or `trajectory` structure doesn\'t\n        match `self.policy.trajectory_spec`.\n      ValueError: If `policy_state` doesn\'t match\n        `self.policy.policy_state_spec`, or `trajectory` structure doesn\'t\n        match `self.policy.trajectory_spec`.\n      ValueError: If `trajectory` lacks two outer dims.\n    """"""\n    trajectory_spec = self._policy.trajectory_spec\n    outer_dims = nest_utils.get_outer_shape(trajectory, trajectory_spec)\n\n    if tf.compat.dimension_value(outer_dims.shape[0]) != 2:\n      raise ValueError(\n          ""Expected two outer dimensions, but saw \'{}\' dimensions.\\n""\n          ""Trajectory:\\n{}.\\nTrajectory spec from policy:\\n{}."".format(\n              tf.compat.dimension_value(outer_dims.shape[0]), trajectory,\n              trajectory_spec))\n    if self._time_major:\n      sequence_length = outer_dims[0]\n      batch_size = outer_dims[1]\n      static_batch_size = tf.compat.dimension_value(\n          trajectory.discount.shape[1])\n    else:\n      batch_size = outer_dims[0]\n      sequence_length = outer_dims[1]\n      static_batch_size = tf.compat.dimension_value(\n          trajectory.discount.shape[0])\n\n    if policy_state is None:\n      policy_state = self._policy.get_initial_state(batch_size)\n    else:\n      nest_utils.assert_same_structure(policy_state,\n                                       self._policy.policy_state_spec)\n\n    if not self._time_major:\n      # Make trajectory time-major.\n      trajectory = tf.nest.map_structure(common.transpose_batch_time,\n                                         trajectory)\n\n    trajectory_tas = tf.nest.map_structure(\n        lambda t: tf.TensorArray(t.dtype, size=sequence_length).unstack(t),\n        trajectory)\n\n    def create_output_ta(spec):\n      return tf.TensorArray(\n          spec.dtype, size=sequence_length,\n          element_shape=(tf.TensorShape([static_batch_size])\n                         .concatenate(spec.shape)))\n\n    output_action_tas = tf.nest.map_structure(create_output_ta,\n                                              trajectory_spec.action)\n    output_policy_info_tas = tf.nest.map_structure(create_output_ta,\n                                                   trajectory_spec.policy_info)\n\n    read0 = lambda ta: ta.read(0)\n    zeros_like0 = lambda t: tf.zeros_like(t[0])\n    ones_like0 = lambda t: tf.ones_like(t[0])\n    time_step = ts.TimeStep(\n        step_type=read0(trajectory_tas.step_type),\n        reward=tf.nest.map_structure(zeros_like0, trajectory.reward),\n        discount=ones_like0(trajectory.discount),\n        observation=tf.nest.map_structure(read0, trajectory_tas.observation))\n\n    def process_step(time, time_step, policy_state,\n                     output_action_tas, output_policy_info_tas):\n      """"""Take an action on the given step, and update output TensorArrays.\n\n      Args:\n        time: Step time.  Describes which row to read from the trajectory\n          TensorArrays and which location to write into in the output\n          TensorArrays.\n        time_step: Previous step\'s `TimeStep`.\n        policy_state: Policy state tensor or nested structure of tensors.\n        output_action_tas: Nest of `tf.TensorArray` containing new actions.\n        output_policy_info_tas: Nest of `tf.TensorArray` containing new\n          policy info.\n\n      Returns:\n        policy_state: The next policy state.\n        next_output_action_tas: Updated `output_action_tas`.\n        next_output_policy_info_tas: Updated `output_policy_info_tas`.\n      """"""\n      action_step = self._policy.action(time_step, policy_state)\n      policy_state = action_step.state\n      write_ta = lambda ta, t: ta.write(time - 1, t)\n      next_output_action_tas = tf.nest.map_structure(\n          write_ta, output_action_tas, action_step.action)\n      next_output_policy_info_tas = tf.nest.map_structure(\n          write_ta, output_policy_info_tas, action_step.info)\n\n      return (action_step.state,\n              next_output_action_tas,\n              next_output_policy_info_tas)\n\n    def loop_body(time, time_step, policy_state,\n                  output_action_tas, output_policy_info_tas):\n      """"""Runs a step in environment.\n\n      While loop will call multiple times.\n\n      Args:\n        time: Step time.\n        time_step: Previous step\'s `TimeStep`.\n        policy_state: Policy state tensor or nested structure of tensors.\n        output_action_tas: Updated nest of `tf.TensorArray`, the new actions.\n        output_policy_info_tas: Updated nest of `tf.TensorArray`, the new\n          policy info.\n\n      Returns:\n        loop_vars for next iteration of tf.while_loop.\n      """"""\n      policy_state, next_output_action_tas, next_output_policy_info_tas = (\n          process_step(time, time_step, policy_state,\n                       output_action_tas,\n                       output_policy_info_tas))\n\n      ta_read = lambda ta: ta.read(time)\n      ta_read_prev = lambda ta: ta.read(time - 1)\n      time_step = ts.TimeStep(\n          step_type=ta_read(trajectory_tas.step_type),\n          observation=tf.nest.map_structure(ta_read,\n                                            trajectory_tas.observation),\n          reward=tf.nest.map_structure(ta_read_prev, trajectory_tas.reward),\n          discount=ta_read_prev(trajectory_tas.discount))\n\n      return (time + 1, time_step, policy_state,\n              next_output_action_tas, next_output_policy_info_tas)\n\n    time = tf.constant(1)\n    time, time_step, policy_state, output_action_tas, output_policy_info_tas = (\n        tf.while_loop(\n            cond=lambda time, *_: time < sequence_length,\n            body=loop_body,\n            loop_vars=[time, time_step, policy_state,\n                       output_action_tas, output_policy_info_tas],\n            back_prop=False,\n            name=""trajectory_replay_loop""))\n\n    # Run the last time step\n    last_policy_state, output_action_tas, output_policy_info_tas = (\n        process_step(time, time_step, policy_state,\n                     output_action_tas, output_policy_info_tas))\n\n    def stack_ta(ta):\n      t = ta.stack()\n      if not self._time_major:\n        t = common.transpose_batch_time(t)\n      return t\n\n    stacked_output_actions = tf.nest.map_structure(stack_ta, output_action_tas)\n    stacked_output_policy_info = tf.nest.map_structure(stack_ta,\n                                                       output_policy_info_tas)\n\n    return (stacked_output_actions,\n            stacked_output_policy_info,\n            last_policy_state)\n'"
tf_agents/environments/trajectory_replay_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.drivers.trajectory_replay.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.environments import trajectory_replay\nfrom tf_agents.utils import test_utils\n\n\nclass TrajectoryReplayTest(test_utils.TestCase):\n\n  def _compare_to_original(self,\n                           output_actions,\n                           output_policy_info,\n                           traj):\n    # policy_info & action between collected & original is different because the\n    # policy will be emitting different outputs.\n    self.assertFalse(\n        np.all(np.isclose(output_policy_info,\n                          traj.policy_info)))\n    self.assertFalse(\n        np.all(np.isclose(output_actions,\n                          traj.action)))\n\n  def testReplayBufferObservers(self):\n    traj, time_step_spec, action_spec = (\n        driver_test_utils.make_random_trajectory())\n    policy = driver_test_utils.TFPolicyMock(time_step_spec, action_spec)\n    replay = trajectory_replay.TrajectoryReplay(policy)\n    output_actions, output_policy_info, _ = replay.run(traj)\n    new_traj = traj._replace(\n        action=output_actions,\n        policy_info=output_policy_info)\n    repeat_output_actions, repeat_output_policy_info, _ = replay.run(new_traj)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    (output_actions, output_policy_info, traj,\n     repeat_output_actions, repeat_output_policy_info) = self.evaluate(\n         (output_actions, output_policy_info, traj,\n          repeat_output_actions, repeat_output_policy_info))\n\n    # Ensure output actions & policy info don\'t match original trajectory.\n    self._compare_to_original(output_actions, output_policy_info, traj)\n\n    # Ensure repeated run with the same deterministic policy recreates the same\n    # actions & policy info.\n    tf.nest.map_structure(self.assertAllEqual, output_actions,\n                          repeat_output_actions)\n    tf.nest.map_structure(self.assertAllEqual, output_policy_info,\n                          repeat_output_policy_info)\n\n  def testReplayBufferObserversWithInitialState(self):\n    traj, time_step_spec, action_spec = (\n        driver_test_utils.make_random_trajectory())\n    policy = driver_test_utils.TFPolicyMock(time_step_spec, action_spec)\n    policy_state = policy.get_initial_state(1)\n    replay = trajectory_replay.TrajectoryReplay(policy)\n    output_actions, output_policy_info, _ = replay.run(\n        traj, policy_state=policy_state)\n    new_traj = traj._replace(\n        action=output_actions,\n        policy_info=output_policy_info)\n    repeat_output_actions, repeat_output_policy_info, _ = replay.run(\n        new_traj, policy_state=policy_state)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    (output_actions, output_policy_info, traj,\n     repeat_output_actions, repeat_output_policy_info) = self.evaluate(\n         (output_actions, output_policy_info, traj,\n          repeat_output_actions, repeat_output_policy_info))\n\n    # Ensure output actions & policy info don\'t match original trajectory.\n    self._compare_to_original(output_actions, output_policy_info, traj)\n\n    # Ensure repeated run with the same deterministic policy recreates the same\n    # actions & policy info.\n    tf.nest.map_structure(self.assertAllEqual, output_actions,\n                          repeat_output_actions)\n    tf.nest.map_structure(self.assertAllEqual, output_policy_info,\n                          repeat_output_policy_info)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/environments/utils.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utilities for TF-Agents Environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Union\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\n\n\ndef get_tf_env(\n    environment: Union[py_environment.PyEnvironment,\n                       tf_environment.TFEnvironment]\n) -> tf_environment.TFEnvironment:\n  """"""Ensures output is a tf_environment, wrapping py_environments if needed.""""""\n  if environment is None:\n    raise ValueError(\'`environment` cannot be None\')\n  if isinstance(environment, py_environment.PyEnvironment):\n    tf_env = tf_py_environment.TFPyEnvironment(environment)\n  elif isinstance(environment, tf_environment.TFEnvironment):\n    tf_env = environment\n  else:\n    raise ValueError(\n        \'`environment` %s must be an instance of \'\n        \'`tf_environment.TFEnvironment` or `py_environment.PyEnvironment`.\' %\n        environment)\n  return tf_env\n\n\ndef validate_py_environment(environment: py_environment.PyEnvironment,\n                            episodes: int = 5):\n  """"""Validates the environment follows the defined specs.""""""\n  time_step_spec = environment.time_step_spec()\n  action_spec = environment.action_spec()\n\n  random_policy = random_py_policy.RandomPyPolicy(\n      time_step_spec=time_step_spec, action_spec=action_spec)\n\n  episode_count = 0\n  time_step = environment.reset()\n\n  while episode_count < episodes:\n    if not array_spec.check_arrays_nest(time_step, time_step_spec):\n      raise ValueError(\n          \'Given `time_step`: %r does not match expected `time_step_spec`: %r\' %\n          (time_step, time_step_spec))\n\n    action = random_policy.action(time_step).action\n    time_step = environment.step(action)\n\n    if time_step.is_last():\n      episode_count += 1\n      time_step = environment.reset()\n'"
tf_agents/environments/utils_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing.absltest import mock\nimport numpy as np\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\ndef get_mock_env(action_spec, observation_spec, step_return):\n  env = mock.MagicMock()\n\n  env.observation_spec = lambda: observation_spec\n  time_step_spec = ts.time_step_spec(observation_spec)\n  env.time_step_spec = lambda: time_step_spec\n  env.action_spec = lambda: action_spec\n  env.step = lambda: step_return\n  env.step.reset = lambda: step_return\n  return env\n\n\nclass UtilsTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(UtilsTest, self).setUp()\n    self._action_spec = [\n        array_spec.BoundedArraySpec((1,), np.int32, -10, 10),\n    ]\n\n    self._observation_spec = array_spec.BoundedArraySpec((1,), np.int32, -10,\n                                                         10)\n\n  def testValidateOk(self):\n    env = get_mock_env(self._action_spec, self._observation_spec, None)\n    rng = np.random.RandomState()\n\n    sample_fn = lambda: array_spec.sample_spec_nest(env.observation_spec(), rng)\n\n    def step(unused_time_step):\n      if rng.rand() < 0.10:\n        return ts.termination(sample_fn(), 0.0)\n      else:\n        return ts.transition(sample_fn(), 1.0)\n\n    env.step = step\n    env.reset = lambda: ts.restart(sample_fn())\n\n    utils.validate_py_environment(env, episodes=2)\n\n  def testValidateNotATimeStep(self):\n    env = get_mock_env(self._action_spec, self._observation_spec, None)\n\n    with self.assertRaises(ValueError):\n      utils.validate_py_environment(env, episodes=1)\n\n  def testValidateWrongDType(self):\n    env = get_mock_env(self._action_spec, self._observation_spec,\n                       ts.restart(np.array([0], dtype=np.int64)))\n\n    with self.assertRaisesRegexp(ValueError, ""does not match expected""):\n      utils.validate_py_environment(env, episodes=1)\n\n  def testValidateWrongShape(self):\n    env = get_mock_env(self._action_spec, self._observation_spec,\n                       ts.restart(np.array([0, 1], dtype=np.int32)))\n\n    with self.assertRaisesRegexp(ValueError, ""does not match expected""):\n      utils.validate_py_environment(env, episodes=1)\n\n  def testValidateWrongDTypeAndShape(self):\n    env = get_mock_env(self._action_spec, self._observation_spec,\n                       ts.restart(np.array([0, 1], dtype=np.int64)))\n\n    with self.assertRaisesRegexp(ValueError, ""does not match expected""):\n      utils.validate_py_environment(env, episodes=1)\n\n  def testValidateOutOfBounds(self):\n    env = get_mock_env(self._action_spec, self._observation_spec,\n                       ts.restart(np.array([-11], dtype=np.int32)))\n\n    with self.assertRaisesRegexp(ValueError, ""does not match expected""):\n      utils.validate_py_environment(env, episodes=1)\n\n  def testValidateBoundedSpecDistinctBounds(self):\n    observation_spec = array_spec.BoundedArraySpec((3,), np.int32,\n                                                   [-10, -5, -2], [10, 5, 2])\n    env = get_mock_env(self._action_spec, observation_spec, None)\n    rng = np.random.RandomState()\n    sample_fn = lambda: array_spec.sample_spec_nest(env.observation_spec(), rng)\n\n    def step(unused_time_step):\n      if rng.rand() < 0.10:\n        return ts.termination(sample_fn(), 0.0)\n      else:\n        return ts.transition(sample_fn(), 1.0)\n\n    env.step = step\n    env.reset = lambda: ts.restart(sample_fn())\n    utils.validate_py_environment(env, episodes=1)\n\n\nif __name__ == ""__main__"":\n  test_utils.main()\n'"
tf_agents/environments/wrappers.py,11,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Environment wrappers.\n\nWrappers in this module can be chained to change the overall behaviour of an\nenvironment in common ways.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport cProfile\nfrom typing import Any, Callable, Optional, Sequence, Text, Union\n\nimport gin\nimport numpy as np\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass PyEnvironmentBaseWrapper(py_environment.PyEnvironment):\n  """"""PyEnvironment wrapper forwards calls to the given environment.""""""\n\n  def __init__(self, env: Any):\n    super(PyEnvironmentBaseWrapper, self).__init__()\n    self._env = env\n\n  def __getattr__(self, name: Text):\n    """"""Forward all other calls to the base environment.""""""\n    return getattr(self._env, name)\n\n  @property\n  def batched(self) -> bool:\n    return getattr(self._env, \'batched\', False)\n\n  @property\n  def batch_size(self) -> Optional[types.Int]:\n    return getattr(self._env, \'batch_size\', None)\n\n  def _reset(self):\n    return self._env.reset()\n\n  def _step(self, action):\n    return self._env.step(action)\n\n  def get_info(self) -> Any:\n    return self._env.get_info()\n\n  def observation_spec(self) -> types.NestedArray:\n    return self._env.observation_spec()\n\n  def action_spec(self) -> types.NestedArray:\n    return self._env.action_spec()\n\n  def close(self) -> None:\n    return self._env.close()\n\n  def render(self, mode: Text = \'rgb_array\') -> types.NestedArray:\n    return self._env.render(mode)\n\n  def seed(self, seed: types.Seed) -> types.Seed:\n    return self._env.seed(seed)\n\n  def wrapped_env(self) -> Any:\n    return self._env\n\n  def set_state(self, state: Any) -> None:\n    self._env.set_state(state)\n\n  def get_state(self) -> Any:\n    return self._env.get_state()\n\n\n@gin.configurable\nclass TimeLimit(PyEnvironmentBaseWrapper):\n  """"""End episodes after specified number of steps.""""""\n\n  def __init__(self, env: py_environment.PyEnvironment, duration: types.Int):\n    super(TimeLimit, self).__init__(env)\n    self._duration = duration\n    self._num_steps = None\n\n  def _reset(self):\n    self._num_steps = 0\n    return self._env.reset()\n\n  def _step(self, action):\n    if self._num_steps is None:\n      return self.reset()\n\n    time_step = self._env.step(action)\n\n    self._num_steps += 1\n    if self._num_steps >= self._duration:\n      time_step = time_step._replace(step_type=ts.StepType.LAST)\n\n    if time_step.is_last():\n      self._num_steps = None\n\n    return time_step\n\n  @property\n  def duration(self) -> types.Int:\n    return self._duration\n\n\n@gin.configurable\nclass PerformanceProfiler(PyEnvironmentBaseWrapper):\n  """"""End episodes after specified number of steps.""""""\n\n  def __init__(self, env: py_environment.PyEnvironment,\n               process_profile_fn: Callable[[cProfile.Profile], Any],\n               process_steps: int):\n    """"""Create a PerformanceProfiler that uses cProfile to profile env execution.\n\n    Args:\n      env: Environment to wrap.\n      process_profile_fn: A callback that accepts a `Profile` object.\n        After `process_profile_fn` is called, profile information is reset.\n      process_steps: The frequency with which `process_profile_fn` is\n        called.  The counter is incremented each time `step` is called\n        (not `reset`); every `process_steps` steps, `process_profile_fn`\n        is called and the profiler is reset.\n    """"""\n    super(PerformanceProfiler, self).__init__(env)\n    self._started = False\n    self._num_steps = 0\n    self._process_steps = process_steps\n    self._process_profile_fn = process_profile_fn\n    self._profile = cProfile.Profile()\n\n  def _reset(self):\n    self._profile.enable()\n    try:\n      return self._env.reset()\n    finally:\n      self._profile.disable()\n\n  def _step(self, action):\n    if not self._started:\n      self._started = True\n      self._num_steps += 1\n      return self.reset()\n\n    self._profile.enable()\n    try:\n      time_step = self._env.step(action)\n    finally:\n      self._profile.disable()\n\n    self._num_steps += 1\n    if self._num_steps >= self._process_steps:\n      self._process_profile_fn(self._profile)\n      self._profile = cProfile.Profile()\n      self._num_steps = 0\n\n    if time_step.is_last():\n      self._started = False\n\n    return time_step\n\n\n@gin.configurable\nclass ActionRepeat(PyEnvironmentBaseWrapper):\n  """"""Repeates actions over n-steps while acummulating the received reward.""""""\n\n  def __init__(self, env: py_environment.PyEnvironment, times: types.Int):\n    """"""Creates an action repeat wrapper.\n\n    Args:\n      env: Environment to wrap.\n      times: Number of times the action should be repeated.\n\n    Raises:\n      ValueError: If the times parameter is not greater than 1.\n    """"""\n    super(ActionRepeat, self).__init__(env)\n    if times <= 1:\n      raise ValueError(\n          \'Times parameter ({}) should be greater than 1\'.format(times))\n    self._times = times\n\n  def _step(self, action):\n    total_reward = 0\n\n    for _ in range(self._times):\n      time_step = self._env.step(action)\n      total_reward += time_step.reward\n      if time_step.is_first() or time_step.is_last():\n        break\n\n    total_reward = np.asarray(total_reward,\n                              dtype=np.asarray(time_step.reward).dtype)\n    return ts.TimeStep(time_step.step_type, total_reward, time_step.discount,\n                       time_step.observation)\n\n\n@gin.configurable\nclass ObservationFilterWrapper(PyEnvironmentBaseWrapper):\n  """"""Filters observations based on an array of indexes.\n\n  Note that this wrapper only supports single-dimensional observations.\n  """"""\n\n  def __init__(self,\n               env: py_environment.PyEnvironment,\n               idx: Union[Sequence[int], np.ndarray]):\n    """"""Creates an observation filter wrapper.\n\n    Args:\n      env: Environment to wrap.\n      idx: Array of indexes pointing to elements to include in output.\n\n    Raises:\n      ValueError: If observation spec is nested.\n      ValueError: If indexes are not single-dimensional.\n      ValueError: If no index is provided.\n      ValueError: If one of the indexes is out of bounds.\n    """"""\n    super(ObservationFilterWrapper, self).__init__(env)\n    idx = np.array(idx)\n    if tf.nest.is_nested(env.observation_spec()):\n      raise ValueError(\'ObservationFilterWrapper only works with single-array \'\n                       \'observations (not nested).\')\n    if len(idx.shape) != 1:\n      raise ValueError(\'ObservationFilterWrapper only works with \'\n                       \'single-dimensional indexes for filtering.\')\n    if idx.shape[0] < 1:\n      raise ValueError(\'At least one index needs to be provided for filtering.\')\n    if not np.all(idx < env.observation_spec().shape[0]):\n      raise ValueError(\'One of the indexes is out of bounds.\')\n\n    self._idx = idx\n    self._observation_spec = env.observation_spec().replace(shape=idx.shape)\n\n  def _step(self, action):\n    time_step = self._env.step(action)\n    return time_step._replace(observation=\n                              np.array(time_step.observation)[self._idx])\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def _reset(self):\n    time_step = self._env.reset()\n    return time_step._replace(observation=\n                              np.array(time_step.observation)[self._idx])\n\n\n@gin.configurable\nclass RunStats(PyEnvironmentBaseWrapper):\n  """"""Wrapper that accumulates run statistics as the environment iterates.\n\n  Note the episodes are only counted if the environment is stepped until the\n  last timestep. This will be triggered correctly when using TimeLimit wrappers.\n\n  In summary:\n   * episodes == number of LAST timesteps,\n   * resets   == number of FIRST timesteps,\n  """"""\n\n  def __init__(self, env: py_environment.PyEnvironment):\n    super(RunStats, self).__init__(env)\n    self._episodes = 0\n    self._resets = 0\n    self._episode_steps = 0\n    self._total_steps = 0\n\n  @property\n  def episodes(self) -> int:\n    return self._episodes\n\n  @property\n  def episode_steps(self) -> int:\n    return self._episode_steps\n\n  @property\n  def total_steps(self) -> int:\n    return self._total_steps\n\n  @property\n  def resets(self) -> int:\n    return self._resets\n\n  def _reset(self):\n    self._resets += 1\n    self._episode_steps = 0\n    return self._env.reset()\n\n  def _step(self, action):\n    time_step = self._env.step(action)\n\n    if time_step.is_first():\n      self._resets += 1\n      self._episode_steps = 0\n    else:\n      self._total_steps += 1\n      self._episode_steps += 1\n\n    if time_step.is_last():\n      self._episodes += 1\n\n    return time_step\n\n\n@gin.configurable\nclass ActionDiscretizeWrapper(PyEnvironmentBaseWrapper):\n  """"""Wraps an environment with continuous actions and discretizes them.""""""\n\n  def __init__(self,\n               env: py_environment.PyEnvironment,\n               num_actions: np.ndarray):\n    """"""Constructs a wrapper for discretizing the action space.\n\n    **Note:** Only environments with a single BoundedArraySpec are supported.\n\n    Args:\n      env: Environment to wrap.\n      num_actions: A np.array of the same shape as the environment\'s\n        action_spec. Elements in the array specify the number of actions to\n        discretize to for each dimension.\n\n    Raises:\n      ValueError: IF the action_spec shape and the limits shape are not equal.\n    """"""\n    super(ActionDiscretizeWrapper, self).__init__(env)\n\n    action_spec = tf.nest.flatten(env.action_spec())\n    if len(action_spec) != 1:\n      raise ValueError(\n          \'ActionDiscretizeWrapper only supports environments with a single \'\n          \'action spec. Got {}\'.format(env.action_spec()))\n\n    action_spec = action_spec[0]\n    self._original_spec = action_spec\n    self._num_actions = np.broadcast_to(num_actions, action_spec.shape)\n\n    if action_spec.shape != self._num_actions.shape:\n      raise ValueError(\'Spec {} and limit shape do not match. Got {}\'.format(\n          action_spec, self._num_actions.shape))\n\n    self._discrete_spec, self._action_map = self._discretize_spec(\n        action_spec, self._num_actions)\n\n  def _discretize_spec(self, spec, limits):\n    """"""Generates a discrete bounded spec and a linspace for the given limits.\n\n    Args:\n      spec: An array_spec to discretize.\n      limits: A np.array with limits for the given spec.\n\n    Returns:\n      Tuple with the discrete_spec along with a list of lists mapping actions.\n    Raises:\n      ValueError: If not all limits value are >=2.\n    """"""\n    if not np.all(limits >= 2):\n      raise ValueError(\'num_actions should all be at least size 2.\')\n\n    limits = np.asarray(limits)\n    # Simplify shape of bounds if they are all equal.\n    if np.all(limits == limits.flat[0]):\n      limits = limits.flat[0]\n    # Workaround for b/148086610. Makes the discretized wrapper generate a\n    # scalar spec when possible.\n    shape = () if spec.shape == (1,) else spec.shape\n    discrete_spec = array_spec.BoundedArraySpec(\n        shape=shape,\n        dtype=np.int32,\n        minimum=0,\n        maximum=limits - 1,\n        name=spec.name)\n\n    minimum = np.broadcast_to(spec.minimum, shape)\n    maximum = np.broadcast_to(spec.maximum, shape)\n\n    action_map = [\n        np.linspace(spec_min, spec_max, num=n_actions)\n        for spec_min, spec_max, n_actions in zip(\n            np.nditer(minimum), np.nditer(maximum), np.nditer(limits))\n    ]\n\n    return discrete_spec, action_map\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._discrete_spec\n\n  def _map_actions(self, action, action_map):\n    """"""Maps the given discrete action to the corresponding continuous action.\n\n    Args:\n      action: Discrete action to map.\n      action_map: Array with the continuous linspaces for the action.\n\n    Returns:\n      Numpy array with the mapped continuous actions.\n    Raises:\n      ValueError: If the given action\'s shpe does not match the action_spec\n      shape.\n    """"""\n    action = np.asarray(action)\n    if action.shape != self._discrete_spec.shape:\n      raise ValueError(\n          \'Received action with incorrect shape. Got {}, expected {}\'.format(\n              action.shape, self._discrete_spec.shape))\n\n    mapped_action = [action_map[i][a] for i, a in enumerate(action.flatten())]\n    return np.reshape(mapped_action, newshape=self._original_spec.shape)\n\n  def _step(self, action):\n    """"""Steps the environment while remapping the actions.\n\n    Args:\n      action: Action to take.\n\n    Returns:\n      The next time_step from the environment.\n    """"""\n    continuous_actions = self._map_actions(action, self._action_map)\n    env_action_spec = self._env.action_spec()\n\n    if tf.nest.is_nested(env_action_spec):\n      continuous_actions = tf.nest.pack_sequence_as(env_action_spec,\n                                                    [continuous_actions])\n    return self._env.step(continuous_actions)\n\n\n@gin.configurable\nclass ActionClipWrapper(PyEnvironmentBaseWrapper):\n  """"""Wraps an environment and clips actions to spec before applying.""""""\n\n  def _step(self, action):\n    """"""Steps the environment after clipping the actions.\n\n    Args:\n      action: Action to take.\n\n    Returns:\n      The next time_step from the environment.\n    """"""\n    env_action_spec = self._env.action_spec()\n\n    def _clip_to_spec(act_spec, act):\n      # NumPy does not allow both min and max to be None\n      if act_spec.minimum is None and act_spec.maximum is None:\n        return act\n      return np.clip(act, act_spec.minimum, act_spec.maximum)\n\n    clipped_actions = nest.map_structure_up_to(env_action_spec, _clip_to_spec,\n                                               env_action_spec, action)\n\n    return self._env.step(clipped_actions)\n\n\n# TODO(b/119321125): Remove this once index_with_actions supports negative\n# actions.\nclass ActionOffsetWrapper(PyEnvironmentBaseWrapper):\n  """"""Offsets actions to be zero-based.\n\n  This is useful for the DQN agent, which currently doesn\'t support\n  negative-valued actions.\n  """"""\n\n  def __init__(self, env: py_environment.PyEnvironment):\n    super(ActionOffsetWrapper, self).__init__(env)\n    if tf.nest.is_nested(self._env.action_spec()):\n      raise ValueError(\'ActionOffsetWrapper only works with single-array \'\n                       \'action specs (not nested specs).\')\n    if not tensor_spec.is_bounded(self._env.action_spec()):\n      raise ValueError(\'ActionOffsetWrapper only works with bounded \'\n                       \'action specs.\')\n    if not tensor_spec.is_discrete(self._env.action_spec()):\n      raise ValueError(\'ActionOffsetWrapper only works with discrete \'\n                       \'action specs.\')\n\n  def action_spec(self) -> types.NestedArraySpec:\n    spec = self._env.action_spec()\n    minimum = np.zeros(shape=spec.shape, dtype=spec.dtype)\n    maximum = spec.maximum - spec.minimum\n    return array_spec.BoundedArraySpec(spec.shape, spec.dtype, minimum=minimum,\n                                       maximum=maximum)\n\n  def _step(self, action):\n    return self._env.step(action + self._env.action_spec().minimum)\n\n\n@gin.configurable\nclass FlattenObservationsWrapper(PyEnvironmentBaseWrapper):\n  """"""Wraps an environment and flattens nested multi-dimensional observations.\n\n  Example:\n    The observation returned by the environment is a multi-dimensional sequence\n    of items of varying lengths.\n\n    timestep.observation_spec =\n      {\'position\': ArraySpec(shape=(4,), dtype=float32),\n       \'target\': ArraySpec(shape=(5,), dtype=float32)}\n\n    timestep.observation =\n      {\'position\':  [1,2,3,4], target\': [5,6,7,8,9]}\n\n    By packing the observation, we reduce the dimensions into a single dimension\n    and concatenate the values of all the observations into one array.\n\n    timestep.observation_spec = (\n      \'packed_observations\': ArraySpec(shape=(9,), dtype=float32)\n\n    timestep.observation = [1,2,3,4,5,6,7,8,9] # Array of len-9.\n\n\n  Note: By packing observations into a single dimension, the specific ArraySpec\n  structure of each observation (such as if min or max bounds are set) are lost.\n  """"""\n\n  def __init__(self,\n               env: py_environment.PyEnvironment,\n               observations_whitelist: Optional[Sequence[Text]] = None):\n    """"""Initializes a wrapper to flatten environment observations.\n\n    Args:\n      env: A `py_environment.PyEnvironment` environment to wrap.\n      observations_whitelist: A list of observation keys that want to be\n        observed from the environment.  All other observations returned are\n        filtered out.  If not provided, all observations will be kept.\n        Additionally, if this is provided, the environment is expected to return\n        a dictionary of observations.\n\n    Raises:\n      ValueError: If the current environment does not return a dictionary of\n        observations and observations whitelist is provided.\n      ValueError: If the observation whitelist keys are not found in the\n        environment.\n    """"""\n    super(FlattenObservationsWrapper, self).__init__(env)\n\n    # If observations whitelist is provided:\n    #  Check that the environment returns a dictionary of observations.\n    #  Check that the set of whitelist keys is a found in the environment keys.\n    if observations_whitelist is not None:\n      if not isinstance(env.observation_spec(), dict):\n        raise ValueError(\n            \'If you provide an observations whitelist, the current environment \'\n            \'must return a dictionary of observations! The returned observation\'\n            \' spec is type %s.\' % (type(env.observation_spec())))\n\n      # Check that observation whitelist keys are valid observation keys.\n      if not (set(observations_whitelist).issubset(\n          env.observation_spec().keys())):\n        raise ValueError(\n            \'The observation whitelist contains keys not found in the \'\n            \'environment! Unknown keys: %s\' % list(\n                set(observations_whitelist).difference(\n                    env.observation_spec().keys())))\n\n    # Check that all observations have the same dtype. This dtype will be used\n    # to create the flattened ArraySpec.\n    env_dtypes = list(\n        set([obs.dtype for obs in env.observation_spec().values()]))\n    if len(env_dtypes) != 1:\n      raise ValueError(\'The observation spec must all have the same dtypes! \'\n                       \'Currently found dtypes: %s\' % (env_dtypes))\n    inferred_spec_dtype = env_dtypes[0]\n\n    self._observation_spec_dtype = inferred_spec_dtype\n    self._observations_whitelist = observations_whitelist\n    # Update the observation spec in the environment.\n    observations_spec = env.observation_spec()\n    if self._observations_whitelist is not None:\n      observations_spec = self._filter_observations(observations_spec)\n\n    # Compute the observation length after flattening the observation items and\n    # nested structure. Observation specs are not batched.\n    observation_total_len = sum(\n        int(np.prod(observation.shape))\n        for observation in self._flatten_nested_observations(\n            observations_spec, is_batched=False))\n\n    # Update the observation spec as an array of one-dimension.\n    self._flattened_observation_spec = array_spec.ArraySpec(\n        shape=(observation_total_len,),\n        dtype=self._observation_spec_dtype,\n        name=\'packed_observations\')\n\n  def _filter_observations(self, observations):\n    """"""Filters out unwanted observations from the environment.\n\n    Args:\n      observations: A nested dictionary of arrays corresponding to\n      `observation_spec()`. This is the observation attribute in the\n      TimeStep object returned by the environment.\n\n    Returns:\n      A nested dict of arrays corresponding to `observation_spec()` with only\n        observation keys in the observation whitelist.\n    """"""\n    filter_out = set(observations.keys()).difference(\n        self._observations_whitelist)\n    # Remove unwanted keys from the observation list.\n    for filter_key in filter_out:\n      del observations[filter_key]\n    return observations\n\n  def _pack_and_filter_timestep_observation(self, timestep):\n    """"""Pack and filter observations into a single dimension.\n\n    Args:\n      timestep: A `TimeStep` namedtuple containing:\n        - step_type: A `StepType` value.\n        - reward: Reward at this timestep.\n        - discount: A discount in the range [0, 1].\n        - observation: A NumPy array, or a nested dict, list or tuple of arrays\n          corresponding to `observation_spec()`.\n\n    Returns:\n      A new `TimeStep` namedtuple that has filtered observations and packed into\n        a single dimenison.\n    """"""\n    # We can\'t set attribute to the TimeStep tuple, so we make a copy of the\n    # observations.\n    observations = timestep.observation\n    if self._observations_whitelist is not None:\n      observations = self._filter_observations(observations)\n\n    return ts.TimeStep(\n        timestep.step_type, timestep.reward, timestep.discount,\n        self._flatten_nested_observations(\n            observations, is_batched=self._env.batched))\n\n  def _flatten_nested_observations(self, observations, is_batched):\n    """"""Flatten individual observations and then flatten the nested structure.\n\n    Args:\n      observations: A flattened NumPy array of shape corresponding to\n        `observation_spec()` or an `observation_spec()`.\n      is_batched: Whether or not the provided observation is batched.\n\n    Returns:\n      A concatenated and flattened NumPy array of observations.\n    """"""\n\n    def np_flatten(x):\n      # Check if observations are batch, and if so keep the batch dimension and\n      # flatten the all other dimensions into one.\n      if is_batched:\n        return np.reshape(x, [x.shape[0], -1])\n      else:\n        return np.reshape(x, [-1])\n\n    # Flatten the individual observations if they are multi-dimensional and then\n    # flatten the nested structure.\n    flat_observations = [np_flatten(x) for x in tf.nest.flatten(observations)]\n    axis = 1 if is_batched else 0\n    return np.concatenate(flat_observations, axis=axis)\n\n  def _step(self, action):\n    """"""Steps the environment while packing the observations returned.\n\n    Args:\n      action: A NumPy array, or a nested dict, list or tuple of arrays\n        corresponding to `action_spec()`.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` value.\n        reward: Reward at this timestep.\n        discount: A discount in the range [0, 1].\n        observation: A flattened NumPy array of shape corresponding to\n         `observation_spec()`.\n    """"""\n    return self._pack_and_filter_timestep_observation(self._env.step(action))\n\n  def _reset(self):\n    """"""Starts a new sequence and returns the first `TimeStep` of this sequence.\n\n    Returns:\n      A `TimeStep` namedtuple containing:\n        step_type: A `StepType` of `FIRST`.\n        reward: `None`, indicating the reward is undefined.\n        discount: `None`, indicating the discount is undefined.\n        observation: A flattened NumPy array of shape corresponding to\n         `observation_spec()`.\n    """"""\n    return self._pack_and_filter_timestep_observation(self._env.reset())\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    """"""Defines the observations provided by the environment.\n\n    Returns:\n      An `ArraySpec` with a shape of the total length of observations kept.\n    """"""\n    return self._flattened_observation_spec\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass GoalReplayEnvWrapper(PyEnvironmentBaseWrapper):\n  """"""Adds a goal to the observation, used for HER (Hindsight Experience Replay).\n\n  Sources:\n    [1] Hindsight Experience Replay. https://arxiv.org/abs/1707.01495.\n\n  To use this wrapper, create an environment-specific version by inheriting this\n  class.\n  """"""\n\n  def __init__(self, env: py_environment.PyEnvironment):\n    """"""Initializes a wrapper to add a goal to the observation.\n\n    Args:\n      env: A `py_environment.PyEnvironment` environment to wrap.\n\n    Raises:\n      ValueError: If environment observation is not a dict\n    """"""\n    super(GoalReplayEnvWrapper, self).__init__(env)\n    self._env = env\n    self._goal = None\n\n  @abc.abstractmethod\n  def get_trajectory_with_goal(self,\n                               trajectory: ts.TimeStep,\n                               goal: types.NestedArray) -> ts.TimeStep:\n    """"""Generates a new trajectory assuming the given goal was the actual target.\n\n    One example is updating a ""distance-to-goal"" field in the observation. Note\n    that relevant state information must be recovered or re-calculated from the\n    given trajectory.\n\n    Args:\n      trajectory: An instance of `TimeStep`.\n      goal: Environment specific goal\n\n    Returns:\n      Updated instance of `TimeStep`\n\n    Raises:\n      NotImplementedError: function should be implemented in child class.\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def get_goal_from_trajectory(self,\n                               trajectory: ts.TimeStep) -> types.NestedArray:\n    """"""Extracts the goal from a given trajectory.\n\n    Args:\n      trajectory: An instance of `TimeStep`.\n\n    Returns:\n      Environment specific goal\n\n    Raises:\n      NotImplementedError: function should be implemented in child class.\n    """"""\n    pass\n\n  def _reset(self, *args, **kwargs):\n    """"""Resets the environment, updating the trajectory with goal.""""""\n    trajectory = self._env.reset(*args, **kwargs)\n    self._goal = self.get_goal_from_trajectory(trajectory)\n    return self.get_trajectory_with_goal(trajectory, self._goal)\n\n  def _step(self, *args, **kwargs):\n    """"""Execute a step in the environment, updating the trajectory with goal.""""""\n    trajectory = self._env.step(*args, **kwargs)\n    return self.get_trajectory_with_goal(trajectory, self._goal)\n\n\n@gin.configurable\nclass HistoryWrapper(PyEnvironmentBaseWrapper):\n  """"""Adds observation and action history to the environment\'s observations.""""""\n\n  def __init__(self,\n               env: py_environment.PyEnvironment,\n               history_length: int = 3,\n               include_actions: bool = False):\n    """"""Initializes a HistoryWrapper.\n\n    Args:\n      env: Environment to wrap.\n      history_length: Length of the history to attach.\n      include_actions: Whether actions should be included in the history.\n    """"""\n    super(HistoryWrapper, self).__init__(env)\n    self._history_length = history_length\n    self._include_actions = include_actions\n\n    self._zero_observation = self._zeros_from_spec(env.observation_spec())\n    self._zero_action = self._zeros_from_spec(env.action_spec())\n\n    self._observation_history = collections.deque(maxlen=history_length)\n    self._action_history = collections.deque(maxlen=history_length)\n\n    self._observation_spec = self._get_observation_spec()\n\n  def _get_observation_spec(self):\n\n    def _update_shape(spec):\n      return spec.replace(shape=(self._history_length,) + spec.shape)\n\n    observation_spec = tf.nest.map_structure(_update_shape,\n                                             self._env.observation_spec())\n\n    if self._include_actions:\n      action_spec = tf.nest.map_structure(_update_shape,\n                                          self._env.action_spec())\n      return {\'observation\': observation_spec, \'action\': action_spec}\n    else:\n      return observation_spec\n\n  def observation_spec(self) -> types.NestedArraySpec:\n    return self._observation_spec\n\n  def _zeros_from_spec(self, spec):\n\n    def _zeros(spec):\n      return np.zeros(spec.shape, dtype=spec.dtype)\n\n    return tf.nest.map_structure(_zeros, spec)\n\n  def _add_history(self, time_step, action):\n    self._observation_history.append(time_step.observation)\n    self._action_history.append(action)\n\n    if self._include_actions:\n      observation = {\n          \'observation\': np.stack(self._observation_history),\n          \'action\': np.stack(self._action_history)\n      }\n    else:\n      observation = np.stack(self._observation_history)\n    return time_step._replace(observation=observation)\n\n  def _reset(self):\n    self._observation_history.extend([self._zero_observation] *\n                                     (self._history_length - 1))\n    self._action_history.extend([self._zero_action] *\n                                (self._history_length - 1))\n\n    time_step = self._env.reset()\n    return self._add_history(time_step, self._zero_action)\n\n  def _step(self, action):\n    if self.current_time_step() is None or self.current_time_step().is_last():\n      return self._reset()\n\n    time_step = self._env.step(action)\n    return self._add_history(time_step, action)\n\n\n@gin.configurable\nclass OneHotActionWrapper(PyEnvironmentBaseWrapper):\n  """"""Converts discrete action to one_hot format.""""""\n\n  def __init__(self, env: py_environment.PyEnvironment):\n    super(OneHotActionWrapper, self).__init__(env)\n\n    def convert_to_one_hot(spec):\n      """"""Convert spec to one_hot format.""""""\n      if np.issubdtype(spec.dtype, np.integer):\n        if len(spec.shape) > 1:\n          raise ValueError(\'OneHotActionWrapper only supports single action!\'\n                           \'action_spec: {}\'.format(spec))\n\n        num_actions = spec.maximum - spec.minimum + 1\n        output_shape = spec.shape + (num_actions,)\n\n        return array_spec.BoundedArraySpec(\n            shape=output_shape,\n            dtype=spec.dtype,\n            minimum=0,\n            maximum=1,\n            name=\'one_hot_action_spec\')\n      else:\n        return spec\n\n    self._one_hot_action_spec = tf.nest.map_structure(\n        convert_to_one_hot, self._env.action_spec())\n\n  def action_spec(self) -> types.NestedArraySpec:\n    return self._one_hot_action_spec\n\n  def _step(self, action):\n\n    def convert_back(action, inner_spec, spec):\n      if action.shape != inner_spec.shape or action.dtype != inner_spec.dtype:\n        raise ValueError(\'Action shape/dtype different from its definition in \'\n                         \'the inner_spec. Action: {action}. Inner_spec: \'\n                         \'{spec}.\'.format(action=action, spec=spec))\n      if np.issubdtype(action.dtype, np.integer):\n        action = spec.minimum + np.argmax(action, axis=-1)\n      return action\n\n    action = tf.nest.map_structure(\n        convert_back, action, self._one_hot_action_spec,\n        self._env.action_spec())\n    return self._env.step(action)\n'"
tf_agents/environments/wrappers_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.environments.wrappers.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nimport cProfile\nimport math\nimport pstats\n\nfrom absl.testing import parameterized\nfrom absl.testing.absltest import mock\n\nimport gym\nimport gym.spaces\nimport numpy as np\n\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.environments import test_envs\nfrom tf_agents.environments import wrappers\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass PyEnvironmentBaseWrapperTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      {\n          \'testcase_name\': \'scalar\',\n          \'batch_size\': None\n      },\n      {\n          \'testcase_name\': \'batched\',\n          \'batch_size\': 2\n      },\n  )\n  def test_batch_properties(self, batch_size):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec,\n        action_spec,\n        reward_fn=lambda *_: np.array([1.0]),\n        batch_size=batch_size)\n    wrap_env = wrappers.PyEnvironmentBaseWrapper(env)\n    self.assertEqual(wrap_env.batched, env.batched)\n    self.assertEqual(wrap_env.batch_size, env.batch_size)\n\n  def test_default_batch_properties(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    self.assertFalse(env.batched)\n    self.assertEqual(env.batch_size, None)\n    wrap_env = wrappers.PyEnvironmentBaseWrapper(env)\n    self.assertEqual(wrap_env.batched, env.batched)\n    self.assertEqual(wrap_env.batch_size, env.batch_size)\n\n  def test_wrapped_method_propagation(self):\n    mock_env = mock.MagicMock()\n    env = wrappers.PyEnvironmentBaseWrapper(mock_env)\n    env.reset()\n    self.assertEqual(1, mock_env.reset.call_count)\n    env.step(0)\n    self.assertEqual(1, mock_env.step.call_count)\n    mock_env.step.assert_called_with(0)\n    env.seed(0)\n    self.assertEqual(1, mock_env.seed.call_count)\n    mock_env.seed.assert_called_with(0)\n    env.render()\n    self.assertEqual(1, mock_env.render.call_count)\n    env.close()\n    self.assertEqual(1, mock_env.close.call_count)\n\n\nclass TimeLimitWrapperTest(test_utils.TestCase):\n\n  def test_limit_duration_wrapped_env_forwards_calls(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 10)\n\n    action_spec = env.action_spec()\n    self.assertEqual((), action_spec.shape)\n    self.assertEqual(0, action_spec.minimum)\n    self.assertEqual(1, action_spec.maximum)\n\n    observation_spec = env.observation_spec()\n    self.assertEqual((4,), observation_spec.shape)\n    high = np.array([\n        4.8,\n        np.finfo(np.float32).max, 2 / 15.0 * math.pi,\n        np.finfo(np.float32).max\n    ])\n    np.testing.assert_array_almost_equal(-high, observation_spec.minimum)\n    np.testing.assert_array_almost_equal(high, observation_spec.maximum)\n\n  def test_limit_duration_stops_after_duration(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 2)\n\n    env.reset()\n    env.step(np.array(0, dtype=np.int32))\n    time_step = env.step(np.array(0, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    self.assertNotEqual(None, time_step.discount)\n    self.assertNotEqual(0.0, time_step.discount)\n\n  def test_extra_env_methods_work(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 2)\n\n    self.assertEqual(None, env.get_info())\n    env.reset()\n    env.step(np.array(0, dtype=np.int32))\n    self.assertEqual({}, env.get_info())\n\n  def test_automatic_reset(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 2)\n\n    # Episode 1\n    first_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(first_time_step.is_first())\n    mid_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(mid_time_step.is_mid())\n    last_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(last_time_step.is_last())\n\n    # Episode 2\n    first_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(first_time_step.is_first())\n    mid_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(mid_time_step.is_mid())\n    last_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(last_time_step.is_last())\n\n  def test_duration_applied_after_episode_terminates_early(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 10000)\n\n    # Episode 1 stepped until termination occurs.\n    time_step = env.step(np.array(1, dtype=np.int32))\n    while not time_step.is_last():\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    env._duration = 2\n\n    # Episode 2 short duration hits step limit.\n    first_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(first_time_step.is_first())\n    mid_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(mid_time_step.is_mid())\n    last_time_step = env.step(np.array(0, dtype=np.int32))\n    self.assertTrue(last_time_step.is_last())\n\n\nclass ActionRepeatWrapperTest(test_utils.TestCase):\n\n  def _get_mock_env_episode(self):\n    mock_env = mock.MagicMock()\n    mock_env.step.side_effect = [\n        # In practice, the first reward would be 0, but test with a reward of 1.\n        ts.TimeStep(ts.StepType.FIRST, 1, 1, [0]),\n        ts.TimeStep(ts.StepType.MID, 2, 1, [1]),\n        ts.TimeStep(ts.StepType.MID, 3, 1, [2]),\n        ts.TimeStep(ts.StepType.MID, 5, 1, [3]),\n        ts.TimeStep(ts.StepType.LAST, 7, 1, [4]),\n    ]\n    return mock_env\n\n  def test_action_stops_on_first(self):\n    mock_env = self._get_mock_env_episode()\n    env = wrappers.ActionRepeat(mock_env, 3)\n    env.reset()\n\n    time_step = env.step([2])\n    mock_env.step.assert_has_calls([mock.call([2])])\n\n    self.assertEqual(1, time_step.reward)\n    self.assertEqual([0], time_step.observation)\n\n  def test_action_repeated(self):\n    mock_env = self._get_mock_env_episode()\n    env = wrappers.ActionRepeat(mock_env, 3)\n    env.reset()\n\n    env.step([2])\n    env.step([3])\n    mock_env.step.assert_has_calls([mock.call([2])] +\n                                   [mock.call([3])] * 3)\n\n  def test_action_stops_on_last(self):\n    mock_env = self._get_mock_env_episode()\n    env = wrappers.ActionRepeat(mock_env, 3)\n    env.reset()\n\n    env.step([2])\n    env.step([3])\n    time_step = env.step([4])\n    mock_env.step.assert_has_calls([mock.call([2])] +\n                                   [mock.call([3])] * 3 +\n                                   [mock.call([4])])\n\n    self.assertEqual(7, time_step.reward)\n    self.assertEqual([4], time_step.observation)\n\n  def test_checks_times_param(self):\n    mock_env = mock.MagicMock()\n    with self.assertRaises(ValueError):\n      wrappers.ActionRepeat(mock_env, 1)\n\n  def test_accumulates_reward(self):\n    mock_env = self._get_mock_env_episode()\n    env = wrappers.ActionRepeat(mock_env, 3)\n    env.reset()\n\n    env.step(0)\n    time_step = env.step(0)\n\n    mock_env.step.assert_called_with(0)\n    self.assertEqual(10, time_step.reward)\n    self.assertEqual([3], time_step.observation)\n\n\nclass ObservationFilterWrapperTest(test_utils.TestCase):\n\n  def _get_mock_env_step(self):\n    mock_env = mock.MagicMock()\n    mock_env.observation_spec.side_effect = [\n        array_spec.BoundedArraySpec((3,), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((3,), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((3,), np.int32, -10, 10),\n    ]\n    mock_env.reset.side_effect = [ts.TimeStep(ts.StepType.MID, 5, 1, [3, 5, 2])]\n    mock_env.step.side_effect = [ts.TimeStep(ts.StepType.MID, 5, 1, [1, 2, 3])]\n    return mock_env\n\n  def test_filtered_obs_spec(self):\n    mock_env = self._get_mock_env_step()\n    env = wrappers.ObservationFilterWrapper(mock_env, [1])\n\n    self.assertEqual((1,), env.observation_spec().shape)\n\n  def test_obs_filtered_reset(self):\n    mock_env = self._get_mock_env_step()\n    env = wrappers.ObservationFilterWrapper(mock_env, [0])\n    time_step = env.reset()\n\n    self.assertLen(time_step.observation, 1)\n    self.assertEqual([3], time_step.observation)\n\n  def test_obs_filtered_step(self):\n    mock_env = self._get_mock_env_step()\n    env = wrappers.ObservationFilterWrapper(mock_env, [0, 2])\n    env.reset()\n    time_step = env.step(0)\n\n    self.assertLen(time_step.observation, 2)\n    self.assertAllEqual([1, 3], time_step.observation)\n\n  def test_checks_nested_obs(self):\n    mock_env = self._get_mock_env_step()\n    mock_env.observation_spec.side_effect = [\n        [array_spec.BoundedArraySpec((2,), np.int32, -10, 10),\n         array_spec.BoundedArraySpec((2,), np.int32, -10, 10)]\n    ]\n    with self.assertRaises(ValueError):\n      _ = wrappers.ObservationFilterWrapper(mock_env, [0])\n\n  def test_checks_multidim_idx(self):\n    mock_env = self._get_mock_env_step()\n    with self.assertRaises(ValueError):\n      _ = wrappers.ObservationFilterWrapper(mock_env, [[0]])\n\n  def test_checks_idx_provided(self):\n    mock_env = self._get_mock_env_step()\n    with self.assertRaises(ValueError):\n      _ = wrappers.ObservationFilterWrapper(mock_env, [])\n\n  def test_checks_idx_outofbounds(self):\n    mock_env = self._get_mock_env_step()\n    with self.assertRaises(ValueError):\n      _ = wrappers.ObservationFilterWrapper(mock_env, [5])\n\n\nclass RunStatsWrapperTest(test_utils.TestCase):\n\n  def test_episode_count(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.RunStats(env)\n\n    self.assertEqual(0, env.episodes)\n    time_step = env.reset()\n    self.assertEqual(0, env.episodes)\n\n    for episode_num in range(1, 4):\n      while not time_step.is_last():\n        time_step = env.step(np.array(1, dtype=np.int32))\n      self.assertEqual(episode_num, env.episodes)\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n  def test_episode_count_with_time_limit(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.TimeLimit(env, 2)\n    env = wrappers.RunStats(env)\n\n    env.reset()\n    self.assertEqual(0, env.episodes)\n\n    env.step(np.array(0, dtype=np.int32))\n    time_step = env.step(np.array(0, dtype=np.int32))\n\n    self.assertTrue(time_step.is_last())\n    self.assertEqual(1, env.episodes)\n\n  def test_step_count(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.RunStats(env)\n\n    self.assertEqual(0, env.episodes)\n    time_step = env.reset()\n    self.assertEqual(0, env.episodes)\n\n    steps = 0\n    for _ in range(0, 4):\n      while not time_step.is_last():\n        self.assertEqual(steps, env.total_steps)\n        time_step = env.step(np.array(1, dtype=np.int32))\n        steps += 1\n      time_step = env.step(np.array(1, dtype=np.int32))\n\n  def test_resets_count(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    env = wrappers.RunStats(env)\n\n    self.assertEqual(0, env.resets)\n    time_step = env.reset()\n    self.assertEqual(1, env.resets)\n\n    resets = 1\n    for _ in range(0, 4):\n      while not time_step.is_last():\n        self.assertEqual(resets, env.resets)\n        time_step = env.step(np.array(1, dtype=np.int32))\n      time_step = env.step(np.array(1, dtype=np.int32))\n      resets += 1\n\n\nclass ActionDiscretizeWrapper(test_utils.TestCase):\n\n  def test_discrete_spec_scalar_limit(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((), np.float32, -10, 10)\n    limits = 3\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    env = wrappers.ActionDiscretizeWrapper(env, limits)\n\n    expected_spec = array_spec.BoundedArraySpec((), np.int32, 0,\n                                                np.asarray(limits) - 1)\n    self.assertEqual(expected_spec, env.action_spec())\n\n  def test_discrete_spec_1d(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2,), np.float32, -10, 10)\n    limits = [5, 3]\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    env = wrappers.ActionDiscretizeWrapper(env, limits)\n\n    expected_spec = array_spec.BoundedArraySpec((2,), np.int32, 0,\n                                                np.asarray(limits) - 1)\n    self.assertEqual(expected_spec, env.action_spec())\n\n  def test_discrete_spec_nd(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    limits = np.array([[2, 4], [3, 2]])\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    env = wrappers.ActionDiscretizeWrapper(env, limits)\n\n    expected_spec = array_spec.BoundedArraySpec((2, 2), np.int32, 0, limits - 1)\n    self.assertEqual(expected_spec, env.action_spec())\n\n  def test_action_mapping_1d(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((), np.float32, -10, 10)\n    limits = np.array(5)\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n\n      action = env.step(2)\n      np.testing.assert_array_almost_equal(0.0, action)\n      action = env.step(4)\n      np.testing.assert_array_almost_equal(10.0, action)\n\n  def test_action_mapping_nd(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    limits = np.array([[2, 5], [3, 2]])\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n\n      action = env.step([[0, 2], [1, 1]])\n      np.testing.assert_array_almost_equal([[-10.0, 0.0], [0.0, 10.0]], action)\n\n  def test_shapes_broadcast(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    limits = np.array([[2, 5]])\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n\n      action = env.step([[0, 2], [1, 4]])\n      np.testing.assert_array_almost_equal([[-10.0, 0.0], [10.0, 10.0]], action)\n\n  def test_check_limits(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    limits = np.array([[1, 5], [2, 2]])\n\n    with self.assertRaisesRegexp(ValueError, \'.*size 2.\'):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n\n  def test_check_action_shape(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    limits = np.array([[2, 5], [2, 2]])\n\n    with self.assertRaisesRegexp(ValueError, \'.*incorrect shape.*\'):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n      env.step([0, 0])\n\n  def test_check_array_bounds(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2,), np.float32, [-10, 0], 10)\n    limits = np.array([2, 5])\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n\n      action = env.step([0, 0])\n      np.testing.assert_array_almost_equal([-10.0, 0.0], action)\n\n      action = env.step([1, 4])\n      np.testing.assert_array_almost_equal([10.0, 10.0], action)\n\n      action = env.step([0, 2])\n      np.testing.assert_array_almost_equal([-10.0, 5.0], action)\n\n  def test_action_nest(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = {\n        \'action1\': array_spec.BoundedArraySpec((2, 2), np.float32, -10, 10)\n    }\n    limits = np.array([[2, 5]])\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionDiscretizeWrapper(env, limits)\n      env.reset()\n\n      action = env.step(np.array([[0, 2], [1, 4]]))\n      np.testing.assert_array_almost_equal([[-10.0, 0.0], [10.0, 10.0]],\n                                           action[\'action1\'])\n\n\nclass ActionClipWrapper(test_utils.TestCase):\n\n  def test_clip(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2,), np.float32, [-1, 0], 1)\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionClipWrapper(env)\n      env.reset()\n\n      # actions within bounds, use NumPy action\n      action = env.step(np.array([0, 0]))\n      np.testing.assert_array_almost_equal([0.0, 0.0], action)\n\n      # action 1 outside bounds, use list action\n      action = env.step([-4, 0])\n      np.testing.assert_array_almost_equal([-1.0, 0.0], action)\n\n      # action 2 outside bounds, use NumPy action\n      action = env.step(np.array([0, -4]))\n      np.testing.assert_array_almost_equal([0.0, 0.0], action)\n\n      # actions outside bounds, use list action\n      action = env.step([4, 4])\n      action = env.step(np.array([4, 4]))\n      np.testing.assert_array_almost_equal([1.0, 1.0], action)\n\n  def test_nested(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = [\n        array_spec.BoundedArraySpec((2,), np.float32, -1, 1), [\n            array_spec.BoundedArraySpec((2,), np.float32, -2, 2),\n            array_spec.BoundedArraySpec((2,), np.float32, -3, 3)\n        ]\n    ]\n\n    def mock_step(_, action):\n      return action\n\n    with mock.patch.object(\n        random_py_environment.RandomPyEnvironment,\n        \'_step\',\n        side_effect=mock_step,\n        autospec=True,\n    ):\n      env = random_py_environment.RandomPyEnvironment(\n          obs_spec, action_spec=action_spec)\n      env = wrappers.ActionClipWrapper(env)\n      env.reset()\n\n      # use NumPy action\n      action = [np.array([10, -10]), [np.array([10, -10]), np.array([10, -10])]]\n      action = env.step(action)\n      np.testing.assert_array_almost_equal([1, -1], action[0])\n      np.testing.assert_array_almost_equal([2, -2], action[1][0])\n      np.testing.assert_array_almost_equal([3, -3], action[1][1])\n\n      # use list action\n      action = [[10, -10], [[10, -10], [10, -10]]]\n      action = env.step(action)\n      np.testing.assert_array_almost_equal([1, -1], action[0])\n      np.testing.assert_array_almost_equal([2, -2], action[1][0])\n      np.testing.assert_array_almost_equal([3, -3], action[1][1])\n\n\nclass ActionOffsetWrapperTest(test_utils.TestCase):\n\n  def test_nested(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = [\n        array_spec.BoundedArraySpec((2,), np.int32, -1, 1), [\n            array_spec.BoundedArraySpec((2,), np.int32, -2, 2),\n            array_spec.BoundedArraySpec((2,), np.int32, -3, 3)\n        ]\n    ]\n    with self.assertRaisesRegexp(ValueError, \'single-array action specs\'):\n      env = random_py_environment.RandomPyEnvironment(obs_spec, action_spec)\n      env = wrappers.ActionOffsetWrapper(env)\n\n  def test_unbounded(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.ArraySpec((2,), np.int32)\n    with self.assertRaisesRegexp(ValueError, \'bounded action specs\'):\n      env = random_py_environment.RandomPyEnvironment(obs_spec, action_spec)\n      env = wrappers.ActionOffsetWrapper(env)\n\n  def test_continuous(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((2,), np.float32, -1, 1)\n    with self.assertRaisesRegexp(ValueError, \'discrete action specs\'):\n      env = random_py_environment.RandomPyEnvironment(obs_spec, action_spec)\n      env = wrappers.ActionOffsetWrapper(env)\n\n  def test_action_spec(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((3,), np.int32, -1, 1)\n    env = random_py_environment.RandomPyEnvironment(obs_spec, action_spec)\n    env = wrappers.ActionOffsetWrapper(env)\n    self.assertEqual(array_spec.BoundedArraySpec((3,), np.int32, 0, 2),\n                     env.action_spec())\n\n  def test_step(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((3,), np.int32, -1, 1)\n    mock_env = mock.Mock(\n        wraps=random_py_environment.RandomPyEnvironment(obs_spec, action_spec))\n    env = wrappers.ActionOffsetWrapper(mock_env)\n    env.reset()\n\n    env.step(np.array([0, 1, 2]))\n    self.assertTrue(mock_env.step.called)\n    np.testing.assert_array_equal(np.array([-1, 0, 1]),\n                                  mock_env.step.call_args[0][0])\n\n\nclass FlattenObservationsWrapper(parameterized.TestCase):\n\n  @parameterized.parameters(([\'obs1\', \'obs2\'], [(4,), (5,)], np.int32),\n                            ([\'obs1\', \'obs2\', \'obs3\'], [(1,), (1,),\n                                                        (4,)], np.float32),\n                            (([\'obs1\', \'obs2\'], [(5, 2), (3, 3)], np.float32)))\n  def test_with_varying_observation_specs(\n      self, observation_keys, observation_shapes, observation_dtypes):\n    """"""Vary the observation spec and step the environment.""""""\n    obs_spec = collections.OrderedDict()\n    for idx, key in enumerate(observation_keys):\n      obs_spec[key] = array_spec.ArraySpec(observation_shapes[idx],\n                                           observation_dtypes)\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    env = wrappers.FlattenObservationsWrapper(env)\n    time_step = env.step(\n        array_spec.sample_bounded_spec(action_spec, np.random.RandomState()))\n    # Check that all observations returned from environment is packed into one\n    # dimension.\n    expected_shape = self._get_expected_shape(obs_spec, obs_spec.keys())\n    self.assertEqual(time_step.observation.shape, expected_shape)\n    self.assertEqual(\n        env.observation_spec(),\n        array_spec.ArraySpec(\n            shape=expected_shape,\n            dtype=observation_dtypes,\n            name=\'packed_observations\'))\n\n  @parameterized.parameters(((\'obs1\'),), ((\'obs1\', \'obs3\'),))\n  def test_with_varying_observation_filters(self, observations_to_keep):\n    """"""Vary the observations to save from the environment.""""""\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.int32),\n        \'obs3\': array_spec.ArraySpec((3,), np.int32)\n    })\n\n    observations_to_keep = np.array([observations_to_keep]).flatten()\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    # Create the wrapper with list of observations to keep before packing it\n    # into one dimension.\n    env = wrappers.FlattenObservationsWrapper(\n        env, observations_whitelist=observations_to_keep)\n    time_step = env.step(\n        array_spec.sample_bounded_spec(action_spec, np.random.RandomState()))\n    # The expected shape is the sum of observation lengths in the observation\n    # spec that has been filtered by the observations_to_keep list.\n    expected_shape = self._get_expected_shape(obs_spec, observations_to_keep)\n    # Test the expected shape of observations returned from stepping the\n    # environment and additionally, check the environment spec.\n    self.assertEqual(time_step.observation.shape, expected_shape)\n    self.assertEqual(\n        env.observation_spec(),\n        array_spec.ArraySpec(\n            shape=expected_shape, dtype=np.int32, name=\'packed_observations\'))\n\n  def test_env_reset(self):\n    """"""Test the observations returned after an environment reset.""""""\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.int32),\n        \'obs3\': array_spec.ArraySpec((3,), np.int32)\n    })\n\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    # Create the wrapper with list of observations to keep before packing it\n    # into one dimension.\n    env = wrappers.FlattenObservationsWrapper(env)\n    time_step = env.reset()\n    expected_shape = self._get_expected_shape(obs_spec, obs_spec.keys())\n    self.assertEqual(time_step.observation.shape, expected_shape)\n    self.assertEqual(\n        env.observation_spec(),\n        array_spec.ArraySpec(\n            shape=expected_shape, dtype=np.int32, name=\'packed_observations\'))\n\n  @parameterized.parameters(([array_spec.ArraySpec((1,), np.int32)],),\n                            array_spec.ArraySpec((1,), np.int32))\n  def test_observations_wrong_spec_for_whitelist(self, observation_spec):\n    """"""Test the Wrapper has ValueError if the observation spec is invalid.""""""\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    env = random_py_environment.RandomPyEnvironment(\n        observation_spec, action_spec=action_spec)\n    # Create the wrapper with list of observations to keep before packing it\n    # into one dimension.\n    with self.assertRaises(ValueError):\n      env = wrappers.FlattenObservationsWrapper(\n          env, observations_whitelist=[\'obs1\'])\n\n  def test_observations_unknown_whitelist(self):\n    """"""Test the Wrapper has ValueError if given unknown keys.""""""\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.int32),\n        \'obs3\': array_spec.ArraySpec((3,), np.int32)\n    })\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n\n    whitelist_unknown_keys = [\'obs1\', \'obs4\']\n\n    with self.assertRaises(ValueError):\n      env = wrappers.FlattenObservationsWrapper(\n          env, observations_whitelist=whitelist_unknown_keys)\n\n  def test_observations_multiple_dtypes(self):\n    """"""Test the Wrapper has ValueError if given unknown keys.""""""\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.float32),\n    })\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n\n    with self.assertRaises(ValueError):\n      env = wrappers.FlattenObservationsWrapper(env)\n\n  def test_batch_env(self):\n    """"""Vary the observation spec and step the environment.""""""\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.int32),\n    })\n\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    # Generate a randomy py environment with batch size.\n    batch_size = 4\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec, batch_size=batch_size)\n\n    env = wrappers.FlattenObservationsWrapper(env)\n    time_step = env.step(\n        array_spec.sample_bounded_spec(action_spec, np.random.RandomState()))\n\n    expected_shape = self._get_expected_shape(obs_spec, obs_spec.keys())\n    self.assertEqual(time_step.observation.shape,\n                     (batch_size, expected_shape[0]))\n    self.assertEqual(\n        env.observation_spec(),\n        array_spec.ArraySpec(\n            shape=expected_shape, dtype=np.int32, name=\'packed_observations\'))\n\n  def _get_expected_shape(self, observation, observations_to_keep):\n    """"""Gets the expected shape of a flattened observation nest.""""""\n    # The expected shape is the sum of observation lengths in the observation\n    # spec.  For a multi-dimensional observation, it is flattened, thus the\n    # length is the product of its shape, i.e. Two arrays ([3, 3], [2, 3])\n    # result in a len-9 and len-6 observation, with total length of 15.\n    expected_shape = 0\n    for obs in observations_to_keep:\n      expected_shape += np.prod(observation[obs].shape)\n    return (expected_shape,)\n\n\nclass MockGoalReplayEnvWrapper(wrappers.GoalReplayEnvWrapper):\n  """"""Mock environment specific implementation of GoalReplayEnvWrapper.""""""\n\n  def get_trajectory_with_goal(self, trajectory, goal):\n    # In this mock environment, \'obs1\' is the goal\n    trajectory.observation.update({\'obs1\': goal})\n    return trajectory\n\n  def get_goal_from_trajectory(self, trajectory):\n    return trajectory.observation[\'obs1\']\n\n\nclass GoalReplayEnvWrapperTest(parameterized.TestCase):\n\n  @parameterized.parameters(([\'obs1\', \'obs2\'], [(4,), (5,)], np.int32),\n                            ([\'obs1\', \'obs2\', \'obs3\'], [(1,), (1,),\n                                                        (4,)], np.float32),\n                            (([\'obs1\', \'obs2\'], [(5, 2), (3, 3)], np.float32)))\n  def test_with_varying_observation_specs(\n      self, observation_keys, observation_shapes, observation_dtypes):\n    """"""Vary the observation spec and step the environment.""""""\n    obs_spec = collections.OrderedDict()\n    for idx, key in enumerate(observation_keys):\n      obs_spec[key] = array_spec.ArraySpec(observation_shapes[idx],\n                                           observation_dtypes)\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec)\n    env = MockGoalReplayEnvWrapper(env)\n    random_action = array_spec.sample_bounded_spec(action_spec,\n                                                   np.random.RandomState())\n    time_step = env.step(random_action)\n    self.assertIsInstance(time_step.observation, dict)\n    self.assertEqual(time_step.observation.keys(),\n                     env.observation_spec().keys())\n    time_step = env.reset()\n    self.assertIsInstance(time_step.observation, dict)\n    self.assertEqual(time_step.observation.keys(),\n                     env.observation_spec().keys())\n\n  def test_batch_env(self):\n    """"""Test batched version of the environment.""""""\n    obs_spec = collections.OrderedDict({\n        \'obs1\': array_spec.ArraySpec((1,), np.int32),\n        \'obs2\': array_spec.ArraySpec((2,), np.int32),\n    })\n    action_spec = array_spec.BoundedArraySpec((), np.int32, -10, 10)\n\n    # Generate a randomy py environment with batch size.\n    batch_size = 4\n    env = random_py_environment.RandomPyEnvironment(\n        obs_spec, action_spec=action_spec, batch_size=batch_size)\n    env = MockGoalReplayEnvWrapper(env)\n    random_action = array_spec.sample_bounded_spec(action_spec,\n                                                   np.random.RandomState())\n\n    time_step = env.step(random_action)\n    self.assertIsInstance(time_step.observation, dict)\n    self.assertEqual(time_step.observation.keys(),\n                     env.observation_spec().keys())\n    time_step = env.reset()\n    self.assertIsInstance(time_step.observation, dict)\n    self.assertEqual(time_step.observation.keys(),\n                     env.observation_spec().keys())\n\n\nclass HistoryWrapperTest(test_utils.TestCase):\n\n  def test_observation_spec_changed(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    obs_shape = env.observation_spec().shape\n\n    history_env = wrappers.HistoryWrapper(env, 3)\n    self.assertEqual((3,) + obs_shape, history_env.observation_spec().shape)\n\n  def test_observation_spec_changed_with_action(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    obs_shape = env.observation_spec().shape\n    action_shape = env.action_spec().shape\n\n    history_env = wrappers.HistoryWrapper(env, 3, include_actions=True)\n    self.assertEqual((3,) + obs_shape,\n                     history_env.observation_spec()[\'observation\'].shape)\n    self.assertEqual((3,) + action_shape,\n                     history_env.observation_spec()[\'action\'].shape)\n\n  def test_observation_stacked(self):\n    env = test_envs.CountingEnv()\n    history_env = wrappers.HistoryWrapper(env, 3)\n    time_step = history_env.reset()\n    self.assertEqual([0, 0, 0], time_step.observation.tolist())\n\n    time_step = history_env.step(0)\n    self.assertEqual([0, 0, 1], time_step.observation.tolist())\n\n    time_step = history_env.step(0)\n    self.assertEqual([0, 1, 2], time_step.observation.tolist())\n\n    time_step = history_env.step(0)\n    self.assertEqual([1, 2, 3], time_step.observation.tolist())\n\n  def test_observation_and_action_stacked(self):\n    env = test_envs.CountingEnv()\n    history_env = wrappers.HistoryWrapper(env, 3, include_actions=True)\n    time_step = history_env.reset()\n    self.assertEqual([0, 0, 0], time_step.observation[\'observation\'].tolist())\n    self.assertEqual([0, 0, 0], time_step.observation[\'action\'].tolist())\n\n    time_step = history_env.step(5)\n    self.assertEqual([0, 0, 1], time_step.observation[\'observation\'].tolist())\n    self.assertEqual([0, 0, 5], time_step.observation[\'action\'].tolist())\n\n    time_step = history_env.step(6)\n    self.assertEqual([0, 1, 2], time_step.observation[\'observation\'].tolist())\n    self.assertEqual([0, 5, 6], time_step.observation[\'action\'].tolist())\n\n    time_step = history_env.step(7)\n    self.assertEqual([1, 2, 3], time_step.observation[\'observation\'].tolist())\n    self.assertEqual([5, 6, 7], time_step.observation[\'action\'].tolist())\n\n\nclass PerformanceProfilerWrapperTest(test_utils.TestCase):\n\n  def test_profiling(self):\n    cartpole_env = gym.make(\'CartPole-v1\')\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    profile = [None]\n    def profile_fn(p):\n      self.assertIsInstance(p, cProfile.Profile)\n      profile[0] = p\n\n    env = wrappers.PerformanceProfiler(\n        env, process_profile_fn=profile_fn,\n        process_steps=2)\n\n    env.reset()\n\n    # Resets are also profiled.\n    s = pstats.Stats(env._profile)\n    self.assertGreater(s.total_calls, 0)  # pytype: disable=attribute-error\n\n    for _ in range(2):\n      env.step(np.array(1, dtype=np.int32))\n\n    self.assertIsNotNone(profile[0])\n    previous_profile = profile[0]\n\n    updated_s = pstats.Stats(profile[0])\n    self.assertGreater(updated_s.total_calls, s.total_calls)  # pytype: disable=attribute-error\n\n    for _ in range(2):\n      env.step(np.array(1, dtype=np.int32))\n\n    self.assertIsNotNone(profile[0])\n    # We saw a new profile.\n    self.assertNotEqual(profile[0], previous_profile)\n\n\nclass OneHotActionWrapperTest(test_utils.TestCase):\n\n  def testActionSpec(self):\n    cartpole_env = gym.spec(\'CartPole-v1\').make()\n    env = gym_wrapper.GymWrapper(cartpole_env)\n    one_hot_action_wrapper = wrappers.OneHotActionWrapper(env)\n    expected_spec = array_spec.BoundedArraySpec(\n        shape=(2,),\n        dtype=np.int64,\n        minimum=0,\n        maximum=1,\n        name=\'one_hot_action_spec\')\n    self.assertEqual(one_hot_action_wrapper.action_spec(), expected_spec)\n\n  def testStepDiscrete(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32, 1, 3)\n    mock_env = mock.Mock(\n        wraps=random_py_environment.RandomPyEnvironment(obs_spec, action_spec))\n    one_hot_action_wrapper = wrappers.OneHotActionWrapper(mock_env)\n    one_hot_action_wrapper.reset()\n\n    one_hot_action_wrapper.step(np.array([[0, 1, 0]]).astype(np.int32))\n    self.assertTrue(mock_env.step.called)\n    np.testing.assert_array_equal(\n        np.array([2]).astype(np.int32), mock_env.step.call_args[0][0])\n\n  def testStepContinuous(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = array_spec.ArraySpec((2,), np.float32)\n    mock_env = mock.Mock(\n        wraps=random_py_environment.RandomPyEnvironment(obs_spec, action_spec))\n    one_hot_action_wrapper = wrappers.OneHotActionWrapper(mock_env)\n    one_hot_action_wrapper.reset()\n\n    one_hot_action_wrapper.step(np.array([0.5, 0.3]).astype(np.float32))\n    self.assertTrue(mock_env.step.called)\n    np.testing.assert_array_equal(np.array([0.5, 0.3]).astype(np.float32),\n                                  mock_env.step.call_args[0][0])\n\n  def testStepHybrid(self):\n    obs_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n    action_spec = {\n        \'discrete\':\n            array_spec.BoundedArraySpec((1,), np.int32, 1, 3),\n        \'continuous\':\n            array_spec.ArraySpec((2,), np.float32)\n    }\n    mock_env = mock.Mock(\n        wraps=random_py_environment.RandomPyEnvironment(obs_spec, action_spec))\n    one_hot_action_wrapper = wrappers.OneHotActionWrapper(mock_env)\n    one_hot_action_wrapper.reset()\n\n    action = {\n        \'discrete\':\n            np.array([[0, 1, 0]]).astype(np.int32),\n        \'continuous\':\n            np.array([0.5, 0.3]).astype(np.float32)\n    }\n\n    one_hot_action_wrapper.step(action)\n    self.assertTrue(mock_env.step.called)\n\n    expected_action = {\n        \'discrete\':\n            np.array([2]),\n        \'continuous\':\n            np.array([0.5, 0.3])\n    }\n    np.testing.assert_array_almost_equal(\n        expected_action[\'discrete\'], mock_env.step.call_args[0][0][\'discrete\'])\n    np.testing.assert_array_almost_equal(\n        expected_action[\'continuous\'],\n        mock_env.step.call_args[0][0][\'continuous\'])\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/eval/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Eval module.""""""\n\nfrom tf_agents.eval import metric_utils\n'"
tf_agents/eval/metric_utils.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utils for Metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom absl import logging\nimport gin\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.utils import common\n\n\nclass MetricsGroup(tf.Module):\n  """"""Group a list of Metrics into a container.""""""\n\n  def __init__(self, metrics, name=None):\n    super(MetricsGroup, self).__init__(name=name)\n    self.metrics = metrics\n\n  def results(self):\n    results = [(metric.name, metric.result()) for metric in self.metrics]\n    return collections.OrderedDict(results)\n\n\ndef log_metrics(metrics, prefix=\'\'):\n  log = [\'{0} = {1}\'.format(m.name, m.result()) for m in metrics]\n  logging.info(\'%s \\n\\t\\t %s\', prefix, \'\\n\\t\\t \'.join(log))\n\n\n@gin.configurable\ndef compute(metrics,\n            environment,\n            policy,\n            num_episodes=1):\n  """"""Compute metrics using `policy` on the `environment`.\n\n  Args:\n    metrics: List of metrics to compute.\n    environment: py_environment instance.\n    policy: py_policy instance used to step the environment. A tf_policy can be\n      used in_eager_mode.\n    num_episodes: Number of episodes to compute the metrics over.\n\n  Returns:\n    A dictionary of results {metric_name: metric_value}\n  """"""\n  for metric in metrics:\n    metric.reset()\n\n  time_step = environment.reset()\n  policy_state = policy.get_initial_state(environment.batch_size)\n\n  driver = py_driver.PyDriver(\n      environment,\n      policy,\n      observers=metrics,\n      max_steps=None,\n      max_episodes=num_episodes)\n  driver.run(time_step, policy_state)\n\n  results = [(metric.name, metric.result()) for metric in metrics]\n  return collections.OrderedDict(results)\n\n\n@gin.configurable\ndef compute_summaries(metrics,\n                      environment,\n                      policy,\n                      num_episodes=1,\n                      global_step=None,\n                      tf_summaries=True,\n                      log=False,\n                      callback=None):\n  """"""Compute metrics using `policy` on the `environment` and logs summaries.\n\n  Args:\n    metrics: List of metrics to compute.\n    environment: py_environment instance.\n    policy: py_policy instance used to step the environment. A tf_policy can be\n      used in_eager_mode.\n    num_episodes: Number of episodes to compute the metrics over.\n    global_step: An optional global step for summaries.\n    tf_summaries: If True, write TF summaries for each computed metric.\n    log: If True, log computed metrics.\n    callback: If provided, this function is called with (computed_metrics,\n      global_step).\n\n  Returns:\n    A dictionary of results {metric_name: metric_value}\n  """"""\n  results = compute(metrics, environment, policy, num_episodes)\n  if tf_summaries:\n    py_metric.run_summaries(metrics)\n  if log:\n    log_metrics(metrics, prefix=\'Step = {}\'.format(global_step))\n  if callback is not None:\n    callback(results, global_step)\n  return results\n\n\n# TODO(b/130250285): Match compute and compute_summaries signatures.\n@gin.configurable\ndef eager_compute(metrics,\n                  environment,\n                  policy,\n                  num_episodes=1,\n                  train_step=None,\n                  summary_writer=None,\n                  summary_prefix=\'\',\n                  use_function=True):\n  """"""Compute metrics using `policy` on the `environment`.\n\n  *NOTE*: Because placeholders are not compatible with Eager mode we can not use\n  python policies. Because we use tf_policies we need the environment time_steps\n  to be tensors making it easier to use a tf_env for evaluations. Otherwise this\n  method mirrors `compute` directly.\n\n  Args:\n    metrics: List of metrics to compute.\n    environment: tf_environment instance.\n    policy: tf_policy instance used to step the environment.\n    num_episodes: Number of episodes to compute the metrics over.\n    train_step: An optional step to write summaries against.\n    summary_writer: An optional writer for generating metric summaries.\n    summary_prefix: An optional prefix scope for metric summaries.\n    use_function: Option to enable use of `tf.function` when collecting the\n      metrics.\n  Returns:\n    A dictionary of results {metric_name: metric_value}\n  """"""\n  for metric in metrics:\n    metric.reset()\n\n  time_step = environment.reset()\n  policy_state = policy.get_initial_state(environment.batch_size)\n\n  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n      environment,\n      policy,\n      observers=metrics,\n      num_episodes=num_episodes)\n  if use_function:\n    common.function(driver.run)(time_step, policy_state)\n  else:\n    driver.run(time_step, policy_state)\n\n  results = [(metric.name, metric.result()) for metric in metrics]\n  # TODO(b/120301678) remove the summaries and merge with compute\n  if train_step is not None and summary_writer:\n    with summary_writer.as_default():\n      for m in metrics:\n        tag = common.join_scope(summary_prefix, m.name)\n        tf.compat.v2.summary.scalar(name=tag, data=m.result(), step=train_step)\n  # TODO(b/130249101): Add an option to log metrics.\n  return collections.OrderedDict(results)\n'"
tf_agents/eval/metric_utils_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.eval.metric_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\n\n\nclass MetricUtilsTest(tf.test.TestCase):\n\n  def testMetricIsComputedCorrectly(self):\n\n    def reward_fn(*unused_args):\n      reward = np.random.uniform()\n      reward_fn.total_reward += reward\n      return reward\n\n    reward_fn.total_reward = 0\n\n    action_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    observation_spec = array_spec.BoundedArraySpec((1,), np.int32, -10, 10)\n    env = random_py_environment.RandomPyEnvironment(\n        observation_spec, action_spec, reward_fn=reward_fn)\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=None, action_spec=action_spec)\n\n    average_return = py_metrics.AverageReturnMetric()\n\n    num_episodes = 10\n    results = metric_utils.compute([average_return], env, policy, num_episodes)\n    self.assertAlmostEqual(reward_fn.total_reward / num_episodes,\n                           results[average_return.name], places=5)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/keras_layers/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keras Layers Module.""""""\n\nfrom tf_agents.keras_layers import bias_layer\nfrom tf_agents.keras_layers import dynamic_unroll_layer\nfrom tf_agents.keras_layers import sequential_layer\n'"
tf_agents/keras_layers/bias_layer.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keras layer mirroring tf.contrib.layers.bias_add.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\nclass BiasLayer(tf.keras.layers.Layer):\n  """"""Keras layer that only adds a bias to the input.\n\n  `BiasLayer` implements the operation:\n  `output = input + bias`\n\n  Arguments:\n      bias_initializer: Initializer for the bias vector.\n  Input shape:\n      nD tensor with shape: `(batch_size, ..., input_dim)`. The most common\n        situation would be a 2D input with shape `(batch_size, input_dim)`. Note\n        a rank of at least 2 is required.\n  Output shape:\n      nD tensor with shape: `(batch_size, ..., input_dim)`. For instance, for a\n        2D input with shape `(batch_size, input_dim)`, the output would have\n        shape `(batch_size, input_dim)`.\n  """"""\n\n  def __init__(self, bias_initializer=\'zeros\', **kwargs):\n    if \'input_shape\' not in kwargs and \'input_dim\' in kwargs:\n      kwargs[\'input_shape\'] = (kwargs.pop(\'input_dim\'),)\n\n    super(BiasLayer, self).__init__(**kwargs)\n    self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n\n    self.supports_masking = True\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if input_shape.rank == 1:\n      shape = (1,)\n    else:\n      shape = (tf.compat.dimension_value(input_shape[-1]),)\n\n    self.bias = self.add_weight(\n        \'bias\',\n        shape=shape,\n        initializer=self.bias_initializer,\n        dtype=self.dtype,\n        trainable=True)\n    self.built = True\n\n  def call(self, inputs):\n    if inputs.shape.rank == 1:\n      expanded_inputs = tf.expand_dims(inputs, -1)\n      with_bias = tf.nn.bias_add(expanded_inputs, self.bias)\n      return with_bias[..., 0]\n    return tf.nn.bias_add(inputs, self.bias)\n\n  def compute_output_shape(self, input_shape):\n    return input_shape\n\n  def get_config(self):\n    config = {\n        \'bias_initializer\':\n            tf.keras.initializers.serialize(self.bias_initializer),\n    }\n    base_config = super(BiasLayer, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n'"
tf_agents/keras_layers/bias_layer_test.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.keras_layers.bias_layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.keras_layers import bias_layer\n\n\nclass BiasLayerTest(tf.test.TestCase):\n\n  def testBuild(self):\n    bias = bias_layer.BiasLayer()\n    states = tf.ones((2, 3))\n    out = bias(states)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    np.testing.assert_almost_equal([[1.0] * 3] * 2, self.evaluate(out))\n\n  def testBuildScalar(self):\n    bias = bias_layer.BiasLayer()\n    states = tf.ones((2,))\n    out = bias(states)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    np.testing.assert_almost_equal([1.0] * 2, self.evaluate(out))\n\n  def testTrainableVariables(self):\n    bias = bias_layer.BiasLayer(\n        bias_initializer=tf.keras.initializers.Constant(value=1.0))\n    states = tf.zeros((2, 3))\n    _ = bias(states)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    variables = bias.trainable_variables\n    np.testing.assert_almost_equal([[1.0] * 3], self.evaluate(variables))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/keras_layers/dynamic_unroll_layer.py,46,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tensorflow RL Agent RNN utilities.\n\nThis module provides helper functions that Agents can use to train\nRNN-based policies.\n\n`DynamicUnroll`\n\nThe layer `DynamicUnroll` allows an Agent to train an RNN-based policy\nby running an RNN over a batch of episode chunks from a replay buffer.\n\nThe agent creates a subclass of `tf.contrib.rnn.LayerRNNCell` or a Keras RNN\ncell, such as `tf.keras.layers.LSTMCell`, instances of which\nwhich can themselves be wrappers of `RNNCell`.  Training this instance\ninvoles passing it to `DynamicUnroll` constructor; and then pass a set of\nepisode tensors in the form of `inputs`.\n\nSee the unit tests in `rnn_utils_test.py` for more details.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.framework import tensor_shape  # TF internal\nfrom tensorflow.python.keras import layers  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\n__all__ = [""DynamicUnroll""]\n\n\ndef _maybe_tensor_shape_from_tensor(shape):\n  if isinstance(shape, tf.Tensor):\n    return tensor_shape.as_shape(tf.get_static_value(shape))\n  else:\n    return shape\n\n\ndef _infer_state_dtype(explicit_dtype, state):\n  """"""Infer the dtype of an RNN state.\n\n  Args:\n    explicit_dtype: explicitly declared dtype or None.\n    state: RNN\'s hidden state. Must be a Tensor or a nested iterable containing\n      Tensors.\n\n  Returns:\n    dtype: inferred dtype of hidden state.\n\n  Raises:\n    ValueError: if `state` has heterogeneous dtypes or is empty.\n  """"""\n  if explicit_dtype is not None:\n    return explicit_dtype\n  elif tf.nest.is_nested(state):\n    inferred_dtypes = [element.dtype for element in tf.nest.flatten(state)]\n    if not inferred_dtypes:\n      raise ValueError(""Unable to infer dtype from empty state."")\n    all_same = all([x == inferred_dtypes[0] for x in inferred_dtypes])\n    if not all_same:\n      raise ValueError(\n          ""State has tensors of different inferred_dtypes. Unable to infer a ""\n          ""single representative dtype."")\n    return inferred_dtypes[0]\n  else:\n    return state.dtype\n\n\ndef _best_effort_input_batch_size(flat_input):\n  """"""Get static input batch size if available, with fallback to the dynamic one.\n\n  Args:\n    flat_input: An iterable of time major input Tensors of shape\n      `[max_time, batch_size, ...]`.\n    All inputs should have compatible batch sizes.\n\n  Returns:\n    The batch size in Python integer if available, or a scalar Tensor otherwise.\n\n  Raises:\n    ValueError: if there is any input with an invalid shape.\n  """"""\n  for input_ in flat_input:\n    shape = input_.shape\n    if shape.rank is None:\n      continue\n    if shape.rank < 2:\n      raise ValueError(\n          ""Expected input tensor %s to have rank at least 2"" % input_)\n    batch_size = shape.dims[1].value\n    if batch_size is not None:\n      return batch_size\n  # Fallback to the dynamic batch size of the first input.\n  return tf.shape(input=flat_input[0])[1]\n\n\nclass DynamicUnroll(tf.keras.layers.Layer):\n  """"""Process a history of sequences that are concatenated without padding.\n\n  Given batched, batch-major `inputs`, `DynamicUnroll` unrolls\n  an RNN using `cell`; at each time step it feeds a frame of `inputs` as input\n  to `cell.call()`.\n\n  If at least one tensor in `inputs` has rank 3 or above (shaped\n  `[batch_size, n, ...]` where `n` is the number of time steps),\n  the RNN will run for exactly `n` steps.\n\n  If `n == 1` is known statically, then only a single step is executed.\n  This is done via a static unroll without using a `tf.while_loop`.\n\n  If all of the tensors in `inputs` have rank at most `2` (i.e., shaped\n  `[batch_size]` or `[batch_size, d]`, then it is assumed that a single step\n  is being taken (i.e. `n = 1`) and the outputs will also not have a time\n  dimension in their output.\n\n  **NOTE** The `call` method optionally accepts `reset_mask` argument, which\n  allows for state resets partway through a batch, at the cost of more\n  computation.\n  """"""\n\n  def __init__(self, cell, parallel_iterations=20, swap_memory=None,\n               **kwargs):\n    """"""Create a `DynamicUnroll` layer.\n\n    Args:\n      cell: A `tf.nn.rnn_cell.RNNCell` or Keras `RNNCell` (e.g. `LSTMCell`)\n        whose `call()` method has the signature `call(input, state, ...)`.\n        Each tensor in the tuple is shaped `[batch_size, ...]`.\n      parallel_iterations: Parallel iterations to pass to `tf.while_loop`.\n        The default value is a good trades off between memory use and\n        performance.  See documentation of `tf.while_loop` for more details.\n      swap_memory: Python bool.  Whether to swap memory from GPU to CPU when\n        storing activations for backprop.  This may sometimes have a negligible\n        performance impact, but can improve memory usage.  See documentation\n        of `tf.while_loop` for more details.\n      **kwargs: Additional layer arguments, such as `dtype` and `name`.\n\n    Raises:\n      TypeError: if `cell` lacks `get_initial_state`, `output_size`, or\n        `state_size` property.\n    """"""\n    if getattr(cell, ""get_initial_state"", None) is None:\n      raise TypeError(""cell lacks get_initial_state method: %s"" % cell)\n    if getattr(cell, ""output_size"", None) is None:\n      raise TypeError(""cell lacks output_size property: %s"" % cell)\n    if getattr(cell, ""state_size"", None) is None:\n      raise TypeError(""cell lacks state_size property: %s"" % cell)\n    self.cell = cell\n    self.parallel_iterations = parallel_iterations\n    self.swap_memory = swap_memory\n    super(DynamicUnroll, self).__init__(**kwargs)\n\n  def get_config(self):\n    config = {\n        ""parallel_iterations"": self.parallel_iterations,\n        ""swap_memory"": self.swap_memory,\n        ""cell"": {\n            ""class_name"": self.cell.__class__.__name__,\n            ""config"": self.cell.get_config()\n        }\n    }\n    base_config = dict(super(DynamicUnroll, self).get_config())\n    base_config.update(config)\n    return base_config\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    cell = layers.deserialize(config.pop(""cell""), custom_objects=custom_objects)\n    layer = cls(cell, **config)\n    return layer\n\n  def compute_output_shape(self, input_shape):\n    return self.cell.compute_output_shape(input_shape)\n\n  @property\n  def trainable_weights(self):\n    if not self.trainable:\n      return []\n    return self.cell.trainable_weights\n\n  @property\n  def non_trainable_weights(self):\n    if not self.trainable:\n      return self.cell.weights\n    return self.cell.non_trainable_weights\n\n  @property\n  def losses(self):\n    layer_losses = super(DynamicUnroll, self).losses\n    return self.cell.losses + layer_losses\n\n  @property\n  def updates(self):\n    updates = self.cell.updates\n    return updates + self._updates\n\n  def build(self, input_shape):\n    self.cell.build(input_shape)\n    self.built = True\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    if inputs is not None:\n      return self.cell.get_initial_state(inputs)\n    else:\n      return self.cell.get_initial_state(\n          batch_size=batch_size, dtype=dtype or self.dtype)\n\n  def call(self, inputs, initial_state=None, reset_mask=None, training=False):\n    """"""Perform the computation.\n\n    Args:\n      inputs: A tuple containing tensors in batch-major format,\n        each shaped `[batch_size, n, ...]`.\n\n        If none of the inputs has rank greater than 2 (i.e., all inputs\n        are shaped `[batch_size, d]` or `[batch_size]`) then it is assumed that\n        a single frame is being calculated and that no time dimension\n        was provided.  In this case, a single step is taken and the outputs\n        will also not have a singleton time dimension either.\n      initial_state: (Optional) An initial state for `cell`.  If not provided,\n        `dtype` must be set and `cell.get_initial_state()` is used instead.\n      reset_mask (Optional): A `bool` matrix shaped `[batch_size, n]`,\n        describing the locations for which the state will be reset to zeros.\n        Typically this is the value `time_steps.is_first()` where `time_steps`\n        is a `TimeStep` containing tensors of the shape `[batch_size, n, ...]`.\n        The `zero_state` of the cell will be used whenever `reset` is `True`,\n        instead of either the current state or the `initial_state`.\n\n        If this argument is not provided, state resetting is not performed\n        (this tends to speed up the computation by a non-negligible amount).\n      training: Whether the output is being used for training.\n\n    Returns:\n      A 2-tuple `(outputs, final_state)` where:\n\n       - `outputs` contains the outputs for all states of the unroll; this is\n         either a tensor or nested tuple with tensors all shaped\n         `[n, batch_size, ...]` (if at least one input had rank `3` or above),\n         or `[batch_size, ...]` (if all of the inputs were at most rank `2`).\n         with structure and shape matching `cell.output_size`.\n       - `final_state` contains the final state of the unroll; with structure\n         and shape matching `cell.state_size`.\n\n    Raises:\n      ValueError: if static batch sizes within input tensors don\'t match.\n      ValueError: if `initial_state` is `None` and `self.dtype` is `None`.\n    """"""\n    if not initial_state and self.dtype is None:\n      raise ValueError(""Must provide either dtype or initial_state"")\n\n    inputs_flat = [\n        tf.convert_to_tensor(x, name=""input"") for x in tf.nest.flatten(inputs)]\n    has_time_axis = all(\n        [x.shape.ndims is None or x.shape.ndims > 2 for x in inputs_flat])\n\n    if not has_time_axis:\n      # No time axis; and we\'re converting to time major anyway; add a time axis\n      # at the front.\n      inputs_flat = [tf.expand_dims(x, 0) for x in inputs_flat]\n    else:\n      # Assume all inputs are batch major.  Convert to time major.\n      inputs_flat = [common.transpose_batch_time(x) for x in inputs_flat]\n\n    inputs_static_shapes = tuple(x.shape for x in inputs_flat)\n    batch_size = _best_effort_input_batch_size(inputs_flat)\n    const_batch_size = tensor_shape.dimension_value(inputs_static_shapes[0][1])\n\n    inputs = tf.nest.pack_sequence_as(inputs, inputs_flat)\n\n    # reset_mask is batch major.  Convert to time major.\n    if reset_mask is not None:\n      reset_mask = tf.transpose(a=reset_mask)\n\n    for shape in inputs_static_shapes:\n      got_batch_size = tensor_shape.dimension_value(shape[1])\n      if const_batch_size is None:\n        const_batch_size = got_batch_size\n      if got_batch_size is not None and const_batch_size != got_batch_size:\n        raise ValueError(\n            ""batch_size is not the same for all the elements in the input. ""\n            ""Saw values %s and %s"" % (const_batch_size, got_batch_size))\n\n    if not initial_state:\n      dtype = self.dtype\n      initial_state = zero_state = self.cell.get_initial_state(\n          batch_size=batch_size, dtype=self.dtype)\n    else:\n      dtype = _infer_state_dtype(self.dtype, initial_state)\n      zero_state = self.cell.get_initial_state(\n          batch_size=batch_size, dtype=dtype)\n\n    # Try to get the iteration count statically; if that\'s not possible,\n    # access it dynamically at runtime.\n    iterations = tensor_shape.dimension_value(inputs_flat[0].shape[0])\n    iterations = iterations or tf.shape(input=inputs_flat[0])[0]\n\n    if not tf.is_tensor(iterations) and iterations == 1:\n      # Take exactly one time step\n      outputs, new_state = _static_unroll_single_step(\n          self.cell,\n          inputs,\n          reset_mask,\n          state=initial_state,\n          zero_state=zero_state,\n          training=training)\n    else:\n      outputs, new_state = _dynamic_unroll_multi_step(\n          self.cell,\n          inputs,\n          reset_mask,\n          initial_state=initial_state,\n          zero_state=zero_state,\n          dtype=dtype,\n          parallel_iterations=self.parallel_iterations,\n          swap_memory=self.swap_memory,\n          iterations=iterations,\n          const_batch_size=const_batch_size,\n          training=training)\n\n    if not has_time_axis:\n      # Remove the time axis.\n      outputs = tf.nest.map_structure(\n          lambda o: tf.squeeze(o, axis=1), outputs)\n\n    return outputs, new_state\n\n\ndef _maybe_reset_state(reset, s_zero, s):\n  if not isinstance(s, tf.TensorArray) and s.shape.rank > 0:\n    return tf.compat.v1.where(reset, s_zero, s)\n  else:\n    return s\n\n\ndef _static_unroll_single_step(cell,\n                               inputs,\n                               reset_mask,\n                               state,\n                               zero_state,\n                               training):\n  """"""Helper for dynamic_unroll which runs a single step.""""""\n  def _squeeze(t):\n    if not isinstance(t, tf.TensorArray) and t.shape.rank > 0:\n      return tf.squeeze(t, [0])\n    else:\n      return t\n\n  # Remove time dimension.\n  inputs = tf.nest.map_structure(_squeeze, inputs)\n  if reset_mask is not None:\n    reset_mask = _squeeze(reset_mask)\n    state = tf.nest.map_structure(\n        lambda s, s_zero: _maybe_reset_state(reset_mask, s_zero, s), state,\n        zero_state)\n\n  outputs, final_state = cell(inputs, state, training=training)\n  outputs = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), outputs)\n\n  return (outputs, final_state)\n\n\ndef _dynamic_unroll_multi_step(cell,\n                               inputs,\n                               reset_mask,\n                               initial_state,\n                               zero_state,\n                               dtype,\n                               parallel_iterations,\n                               swap_memory,\n                               iterations,\n                               const_batch_size,\n                               training):\n  """"""Helper for dynamic_unroll which uses a tf.while_loop.""""""\n\n  # Convert all inputs to TensorArrays\n  def ta_and_unstack(x):\n    return (tf.TensorArray(dtype=x.dtype,\n                           size=iterations,\n                           element_shape=x.shape[1:])\n            .unstack(x))\n\n  inputs_tas = tf.nest.map_structure(ta_and_unstack, inputs)\n  if reset_mask is None:\n    reset_mask_ta = None\n  else:\n    reset_mask_ta = ta_and_unstack(reset_mask)\n\n  # Create a TensorArray for each output\n  def create_output_ta(s):\n    return tf.TensorArray(\n        dtype=_infer_state_dtype(dtype, initial_state),\n        size=iterations,\n        element_shape=(tf.TensorShape([const_batch_size])\n                       .concatenate(_maybe_tensor_shape_from_tensor(s))))\n\n  output_tas = tf.nest.map_structure(create_output_ta, cell.output_size)\n\n  def pred(time, *unused_args):\n    return time < iterations\n\n  def body(time, state, output_tas):\n    """"""Internal while_loop body.\n\n    Args:\n      time: time\n      state: rnn state @ time\n      output_tas: output tensorarrays\n\n    Returns:\n      - time + 1\n      - state: rnn state @ time + 1\n      - output_tas: output tensorarrays with values written @ time\n      - masks_ta: optional mask tensorarray with mask written @ time\n    """"""\n    input_ = tf.nest.map_structure(lambda ta: ta.read(time), inputs_tas)\n    if reset_mask_ta is not None:\n      is_reset = reset_mask_ta.read(time)\n      state = tf.nest.map_structure(\n          lambda s_zero, s: _maybe_reset_state(is_reset, s_zero, s), zero_state,\n          state)\n\n    outputs, next_state = cell(input_, state, training=training)\n\n    output_tas = tf.nest.map_structure(lambda ta, x: ta.write(time, x),\n                                       output_tas, outputs)\n\n    return (time + 1, next_state, output_tas)\n\n  # Create a new scope in which the caching device is either\n  # determined by the parent scope, or is set to place the cached\n  # Variable using the same placement as for the rest of the RNN.\n  with tf.compat.v1.variable_scope(\n      tf.compat.v1.get_variable_scope()) as varscope:\n    if (not tf.executing_eagerly() and varscope.caching_device is None):\n      varscope.set_caching_device(lambda op: op.device)\n\n    _, final_state, output_tas = (\n        tf.while_loop(\n            cond=pred,\n            body=body,\n            loop_vars=(tf.constant(0, name=""time""), initial_state, output_tas),\n            parallel_iterations=parallel_iterations,\n            swap_memory=swap_memory,\n            maximum_iterations=iterations))\n\n  outputs = tf.nest.map_structure(lambda ta: ta.stack(), output_tas)\n\n  if isinstance(iterations, int):\n    # TensorArray.stack() doesn\'t set a static value for dimension 0,\n    # even if the size is known. Set the shapes here.\n    iterations_shape = tf.TensorShape([iterations])\n    tf.nest.map_structure(\n        lambda t: t.set_shape(iterations_shape.concatenate(t.shape[1:])),\n        outputs)\n\n  # Convert everything back to batch major\n  outputs = tf.nest.map_structure(common.transpose_batch_time, outputs)\n\n  return (outputs, final_state)\n'"
tf_agents/keras_layers/dynamic_unroll_layer_test.py,34,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.keras_layers.dynamic_unroll_layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.keras_layers import dynamic_unroll_layer\n\n\nclass AddInputAndStateKerasRNNCell(tf.keras.layers.Layer):\n\n  def __init__(self):\n    super(AddInputAndStateKerasRNNCell, self).__init__()\n    self.output_size = 1\n    self.state_size = 1\n\n  def call(self, input_, state):\n    s = input_ + state\n    return s, s\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    if inputs is not None:\n      return tf.zeros_like(inputs)\n    return tf.zeros([batch_size, 1], dtype)\n\n\nclass DynamicUnrollTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testFromConfigLSTM(self):\n    l1 = dynamic_unroll_layer.DynamicUnroll(\n        tf.keras.layers.LSTMCell(units=3), parallel_iterations=10)\n    l2 = dynamic_unroll_layer.DynamicUnroll.from_config(l1.get_config())\n    self.assertEqual(l1.get_config(), l2.get_config())\n\n  @parameterized.named_parameters(\n      (\'WithMask\', True,),\n      (\'NoMask\', False))\n  def testDynamicUnrollMatchesDynamicRNNWhenNoReset(self, with_mask):\n    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(3)\n    batch_size = 4\n    max_time = 7\n    inputs = tf.random.uniform((batch_size, max_time, 2), dtype=tf.float32)\n    layer = dynamic_unroll_layer.DynamicUnroll(cell, dtype=tf.float32)\n    if with_mask:\n      reset_mask = tf.zeros((batch_size, max_time), dtype=tf.bool)\n    else:\n      reset_mask = None\n    outputs_dun, final_state_dun = layer(inputs, reset_mask=reset_mask)\n    outputs_drnn, final_state_drnn = tf.compat.v1.nn.dynamic_rnn(\n        cell, inputs, dtype=tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    outputs_dun, final_state_dun, outputs_drnn, final_state_drnn = (\n        self.evaluate(\n            (outputs_dun, final_state_dun, outputs_drnn, final_state_drnn)))\n    self.assertAllClose(outputs_dun, outputs_drnn)\n    self.assertAllClose(final_state_dun, final_state_drnn)\n\n  @parameterized.named_parameters(\n      (\'WithMask\', True,),\n      (\'NoMask\', False))\n  def testDynamicUnrollMatchesDynamicRNNWhenNoResetSingleTimeStep(\n      self, with_mask):\n    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(3)\n    batch_size = 4\n    max_time = 1\n    inputs = tf.random.uniform((batch_size, max_time, 2), dtype=tf.float32)\n    layer = dynamic_unroll_layer.DynamicUnroll(cell, dtype=tf.float32)\n    if with_mask:\n      reset_mask = tf.zeros((batch_size, max_time), dtype=tf.bool)\n    else:\n      reset_mask = None\n    outputs_dun, final_state_dun = layer(inputs, reset_mask=reset_mask)\n    outputs_drnn, final_state_drnn = tf.compat.v1.nn.dynamic_rnn(\n        cell, inputs, dtype=tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    outputs_dun, final_state_dun, outputs_drnn, final_state_drnn = (\n        self.evaluate(\n            (outputs_dun, final_state_dun, outputs_drnn, final_state_drnn)))\n    self.assertAllClose(outputs_dun, outputs_drnn)\n    self.assertAllClose(final_state_dun, final_state_drnn)\n\n  def testNoTimeDimensionMatchesSingleStep(self):\n    cell = tf.keras.layers.LSTMCell(3)\n    batch_size = 4\n    max_time = 1\n    inputs = tf.random.uniform((batch_size, max_time, 2), dtype=tf.float32)\n    inputs_no_time = tf.squeeze(inputs, axis=1)\n    layer = dynamic_unroll_layer.DynamicUnroll(cell, dtype=tf.float32)\n    outputs, next_state = layer(inputs)\n    outputs_squeezed_time = tf.squeeze(outputs, axis=1)\n    outputs_no_time, next_state_no_time = layer(inputs_no_time)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    outputs_squeezed_time, next_state, outputs_no_time, next_state_no_time = (\n        self.evaluate((outputs_squeezed_time, next_state,\n                       outputs_no_time, next_state_no_time)))\n    self.assertAllEqual(outputs_squeezed_time, outputs_no_time)\n    self.assertAllEqual(next_state, next_state_no_time)\n\n  def testDynamicUnrollResetsStateOnReset(self):\n    if hasattr(tf, \'contrib\'):\n      class AddInputAndStateRNNCell(tf.contrib.rnn.LayerRNNCell):\n\n        @property\n        def state_size(self):\n          return tf.TensorShape([1])\n\n        @property\n        def output_size(self):\n          return tf.TensorShape([1])\n\n        def call(self, input_, state):\n          s = input_ + state\n          return s, s\n\n      self._testDynamicUnrollResetsStateOnReset(\n          AddInputAndStateRNNCell)\n\n    self._testDynamicUnrollResetsStateOnReset(\n        AddInputAndStateKerasRNNCell)\n\n  def _testDynamicUnrollResetsStateOnReset(self, cell_type):\n    cell = cell_type()\n    batch_size = 4\n    max_time = 7\n    inputs = tf.random.uniform((batch_size, max_time, 1))\n    reset_mask = (tf.random.normal((batch_size, max_time)) > 0)\n\n    layer = dynamic_unroll_layer.DynamicUnroll(cell, dtype=tf.float32)\n    outputs, final_state = layer(inputs, reset_mask=reset_mask)\n\n    tf.nest.assert_same_structure(outputs, cell.output_size)\n    tf.nest.assert_same_structure(final_state, cell.state_size)\n\n    reset_mask, inputs, outputs, final_state = self.evaluate(\n        (reset_mask, inputs, outputs, final_state))\n\n    self.assertAllClose(outputs[:, -1, :], final_state)\n\n    # outputs will contain cumulative sums up until a reset\n    expected_outputs = []\n    state = np.zeros_like(final_state)\n    for i, frame in enumerate(np.transpose(inputs, [1, 0, 2])):\n      state = state * np.reshape(~reset_mask[:, i], state.shape) + frame\n      expected_outputs.append(np.array(state))\n    expected_outputs = np.transpose(expected_outputs, [1, 0, 2])\n    self.assertAllClose(outputs, expected_outputs)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/keras_layers/sequential_layer.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keras layer to replace the Sequential Model object.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\nclass SequentialLayer(tf.keras.layers.Layer):\n  """"""The SequentialLayer represents a sequence of Keras layers.\n\n  It is a Keras Layer that can be used instead of tf.keras.layers.Sequential,\n  which is actually a Keras Model.  In contrast to keras Sequential, this\n  layer can be used as a pure Layer in tf.functions and when exporting\n  SavedModels, without having to pre-declare input and output shapes.  In turn,\n  this layer is usable as a preprocessing layer for TF Agents Networks, and\n  can be exported via PolicySaver.\n\n  Usage:\n  ```python\n  c = SequentialLayer([layer1, layer2, layer3])\n  output = c(inputs)    # Equivalent to: output = layer3(layer2(layer1(inputs)))\n  ```\n  """"""\n\n  def __init__(self, layers, **kwargs):\n    """"""Create a composition.\n\n    Args:\n      layers: A list or tuple of layers to compose.\n      **kwargs: Arguments to pass to `Keras` layer initializer, including\n        `name`.\n\n    Raises:\n      TypeError: If any of the layers are not instances of keras `Layer`.\n    """"""\n    for layer in layers:\n      if not isinstance(layer, tf.keras.layers.Layer):\n        raise TypeError(\n            ""Expected all layers to be instances of keras Layer, but saw: \'{}\'""\n            .format(layer))\n\n    super(SequentialLayer, self).__init__(**kwargs)\n    self.layers = copy.copy(layers)\n\n  def compute_output_shape(self, input_shape):\n    output_shape = tf.TensorShape(input_shape)\n    for l in self.layers:\n      output_shape = l.compute_output_shape(output_shape)\n    return tf.TensorShape(output_shape)\n\n  def compute_output_signature(self, input_signature):\n    output_signature = input_signature\n    for l in self.layers:\n      output_signature = l.compute_output_signature(output_signature)\n    return output_signature\n\n  def build(self, input_shape=None):\n    for l in self.layers:\n      l.build(input_shape)\n      input_shape = l.compute_output_shape(input_shape)\n    self.built = True\n\n  @property\n  def trainable_weights(self):\n    if not self.trainable:\n      return []\n    weights = {}\n    for l in self.layers:\n      for v in l.trainable_weights:\n        weights[id(v)] = v\n    return list(weights.values())\n\n  @property\n  def non_trainable_weights(self):\n    weights = {}\n    for l in self.layers:\n      for v in l.non_trainable_weights:\n        weights[id(v)] = v\n    return list(weights.values())\n\n  @property\n  def trainable(self):\n    return all([l.trainable for l in self.layers])\n\n  @trainable.setter\n  def trainable(self, value):\n    for l in self.layers:\n      l.trainable = value\n\n  @property\n  def losses(self):\n    values = set()\n    for l in self.layers:\n      values.update(l.losses)\n    return list(values)\n\n  @property\n  def regularizers(self):\n    values = set()\n    for l in self.layers:\n      values.update(l.regularizers)\n    return list(values)\n\n  def call(self, inputs, training=False):\n    outputs = inputs\n    for l in self.layers:\n      outputs = l(outputs, training=training)\n    return outputs\n\n  def get_config(self):\n    config = {}\n    for i, layer in enumerate(self.layers):\n      config[i] = {\n          \'class_name\': layer.__class__.__name__,\n          \'config\': copy.deepcopy(layer.get_config())\n      }\n    return config\n\n  @classmethod\n  def from_config(cls, config, custom_objects=None):\n    layers = [\n        tf.keras.layers.deserialize(conf, custom_objects=custom_objects)\n        for conf in config.values()\n    ]\n    return cls(layers)\n\n# Register with Keras so we can do type(layer).from_config(layer.get_config())\ntf.keras.utils.get_custom_objects()[\'SequentialLayer\'] = SequentialLayer\n'"
tf_agents/keras_layers/sequential_layer_test.py,17,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.keras_layers.sequential_layer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.keras_layers import sequential_layer\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\nFLAGS = flags.FLAGS\n\n\nclass ActorNetwork(network.Network):\n\n  def __init__(self, input_tensor_spec, output_tensor_spec):\n    super(ActorNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=\'TestActorNetwork\')\n    num_actions = output_tensor_spec.shape.num_elements()\n    self._sequential_layer = sequential_layer.SequentialLayer([\n        tf.keras.layers.Dense(50),\n        tf.keras.layers.Dense(10),\n        tf.keras.layers.Dense(num_actions)\n    ])\n\n  def call(self, observations, step_type=(), network_state=(), training=False):\n    return self._sequential_layer(observations), network_state\n\n\nclass SequentialLayerTest(test_utils.TestCase):\n\n  def testBuild(self):\n    sequential = sequential_layer.SequentialLayer(\n        [tf.keras.layers.Dense(4, use_bias=False),\n         tf.keras.layers.ReLU()])\n    inputs = np.ones((2, 3))\n    out = sequential(inputs)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    out = self.evaluate(out)\n    weights = self.evaluate(sequential.layers[0].weights[0])\n    expected = np.dot(inputs, weights)\n    expected[expected < 0] = 0\n    self.assertAllClose(expected, out)\n\n  def testTrainableVariables(self):\n    sequential = sequential_layer.SequentialLayer(\n        [tf.keras.layers.Dense(3),\n         tf.keras.layers.Dense(4)])\n    sequential.build((3, 2))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    variables = self.evaluate(sequential.trainable_variables)\n    self.assertLen(variables, 4)\n    self.assertLen(sequential.variables, 4)\n    self.assertTrue(sequential.trainable)\n    sequential.trainable = False\n    self.assertFalse(sequential.trainable)\n    self.assertEmpty(sequential.trainable_variables)\n    self.assertLen(sequential.variables, 4)\n\n  def testTrainableVariablesNestedNetwork(self):\n    sequential_inner = sequential_layer.SequentialLayer(\n        [tf.keras.layers.Dense(3),\n         tf.keras.layers.Dense(4)])\n    sequential = sequential_layer.SequentialLayer(\n        [tf.keras.layers.Dense(3),\n         sequential_inner])\n    sequential.build((3, 2))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    variables = self.evaluate(sequential.trainable_variables)\n\n    self.assertLen(variables, 6)\n    self.assertLen(sequential.variables, 6)\n    self.assertLen(sequential_inner.variables, 4)\n    self.assertTrue(sequential.trainable)\n    sequential.trainable = False\n    self.assertFalse(sequential.trainable)\n    self.assertEmpty(sequential.trainable_variables)\n    self.assertLen(sequential.variables, 6)\n\n  def testCopy(self):\n    sequential = sequential_layer.SequentialLayer(\n        [tf.keras.layers.Dense(3),\n         tf.keras.layers.Dense(4, use_bias=False)])\n    clone = type(sequential).from_config(sequential.get_config())\n    self.assertLen(clone.layers, 2)\n    for l1, l2 in zip(sequential.layers, clone.layers):\n      self.assertEqual(l1.dtype, l2.dtype)\n      self.assertEqual(l1.units, l2.units)\n      self.assertEqual(l1.use_bias, l2.use_bias)\n\n  def testPolicySaverCompatibility(self):\n    observation_spec = tensor_spec.TensorSpec(shape=(100,), dtype=tf.float32)\n    action_spec = tensor_spec.TensorSpec(shape=(5,), dtype=tf.float32)\n    time_step_tensor_spec = ts.time_step_spec(observation_spec)\n    net = ActorNetwork(observation_spec, action_spec)\n    net.create_variables()\n    policy = actor_policy.ActorPolicy(time_step_tensor_spec, action_spec, net)\n\n    sample = tensor_spec.sample_spec_nest(\n        time_step_tensor_spec, outer_dims=(5,))\n\n    policy.action(sample)\n\n    train_step = common.create_variable(\'train_step\')\n    saver = policy_saver.PolicySaver(policy, train_step=train_step)\n    self.initialize_v1_variables()\n\n    with self.cached_session():\n      saver.save(os.path.join(FLAGS.test_tmpdir, \'sequential_layer_model\'))\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/metrics/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Metrics module.""""""\n\nfrom tf_agents.metrics import batched_py_metric\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metric\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.metrics import tf_py_metric\n'"
tf_agents/metrics/batched_py_metric.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A python metric that can be called with batches of trajectories.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport numpy as np\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.trajectories import trajectory as traj\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\nfrom typing import Any, Optional, Text\n\n\nclass BatchedPyMetric(py_metric.PyStepMetric):\n  """"""Wrapper for batching metrics.\n\n  This can be used to wrap any python metric that takes a single trajectory to\n  produce a batched version of the metric that takes a batch of trajectories.\n  """"""\n\n  def __init__(self,\n               metric_class: py_metric.PyMetric.__class__,\n               metric_args: Optional[Any] = None,\n               name: Optional[Text] = None,\n               batch_size: Optional[types.Int] = None,\n               dtype: np.dtype = np.float32):\n    """"""Creates a BatchedPyMetric metric.""""""\n    self._metric_class = metric_class\n    if metric_args is None:\n      self._metric_args = {}\n    else:\n      self._metric_args = metric_args\n\n    if not name:\n      name = self._metric_class(**self._metric_args).name\n    super(BatchedPyMetric, self).__init__(name)\n\n    self._built = False\n    self._dtype = dtype\n    if batch_size is not None:\n      self.build(batch_size)\n\n  def build(self, batch_size: types.Int):\n    self._metrics = [self._metric_class(**self._metric_args)\n                     for _ in range(batch_size)]\n    for metric in self._metrics:\n      metric.reset()\n    self._built = True\n\n  def call(self, batched_trajectory: traj.Trajectory):\n    """"""Processes the batched_trajectory to update the metric.\n\n    Args:\n      batched_trajectory: A Trajectory containing batches of experience.\n\n    Raises:\n      ValueError: If the batch size is an unexpected value.\n    """"""\n    trajectories = nest_utils.unstack_nested_arrays(batched_trajectory)\n    batch_size = len(trajectories)\n    if not self._built:\n      self.build(batch_size)\n    if batch_size != len(self._metrics):\n      raise ValueError(\'Batch size {} does not match previously set batch \'\n                       \'size {}. Make sure your batch size is set correctly \'\n                       \'in BatchedPyMetric initialization and that the batch \'\n                       \'size remains constant.\'.format(batch_size,\n                                                       len(self._metrics)))\n\n    for metric, trajectory in zip(self._metrics, trajectories):\n      metric(trajectory)\n\n  def reset(self):\n    """"""Resets internal stat gathering variables used to compute the metric.""""""\n    if self._built:\n      for metric in self._metrics:\n        metric.reset()\n\n  def result(self) -> Any:\n    """"""Evaluates the current value of the metric.""""""\n    if self._built:\n      return self._metric_class.aggregate(self._metrics)\n    else:\n      return np.array(0.0, dtype=self._dtype)\n\n  @staticmethod\n  def aggregate(metrics):\n    raise NotImplementedError(\n        \'aggregate() is not implemented for BatchedPyMetric.\')\n'"
tf_agents/metrics/batched_py_metric_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.metrics.batched_py_metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.metrics import batched_py_metric\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import nest_utils\n\n\nclass BatchedPyMetricTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(BatchedPyMetricTest, self).setUp()\n    # Order of args for trajectory methods:\n    # (observation, action, policy_info, reward, discount)\n    self._ts0 = nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)\n    ])\n    self._ts1 = nest_utils.stack_nested_arrays([\n        trajectory.first((), (), (), 1., 1.),\n        trajectory.first((), (), (), 2., 1.)\n    ])\n    self._ts2 = nest_utils.stack_nested_arrays([\n        trajectory.last((), (), (), 3., 1.),\n        trajectory.last((), (), (), 4., 1.)\n    ])\n    self._ts3 = nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)\n    ])\n    self._ts4 = nest_utils.stack_nested_arrays([\n        trajectory.first((), (), (), 5., 1.),\n        trajectory.first((), (), (), 6., 1.)\n    ])\n    self._ts5 = nest_utils.stack_nested_arrays([\n        trajectory.last((), (), (), 7., 1.),\n        trajectory.last((), (), (), 8., 1.)\n    ])\n\n  def testMetricIsComputedCorrectlyNoSteps(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    self.assertEqual(batched_avg_return_metric.result(), 0)\n\n  def testMetricIsComputedCorrectlyPartialEpisode(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n\n    batched_avg_return_metric(self._ts0)\n    batched_avg_return_metric(self._ts1)\n    self.assertEqual(batched_avg_return_metric.result(), 0)\n\n  def testMetricIsComputedCorrectlyOneEpisode(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n\n    batched_avg_return_metric(self._ts0)\n    batched_avg_return_metric(self._ts1)\n    batched_avg_return_metric(self._ts2)\n\n    self.assertEqual(batched_avg_return_metric.result(), 5)\n\n  def testMetricIsComputedCorrectlyOneAndPartialEpisode(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    batched_avg_return_metric(self._ts0)\n    batched_avg_return_metric(self._ts1)\n    batched_avg_return_metric(self._ts2)\n    batched_avg_return_metric(self._ts3)\n    batched_avg_return_metric(self._ts4)\n\n    self.assertEqual(batched_avg_return_metric.result(), 5)\n\n  def testMetricIsComputedCorrectlyTwoEpisodes(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    batched_avg_return_metric(self._ts0)\n    batched_avg_return_metric(self._ts1)\n    batched_avg_return_metric(self._ts2)\n    batched_avg_return_metric(self._ts3)\n    batched_avg_return_metric(self._ts4)\n    batched_avg_return_metric(self._ts5)\n    self.assertEqual(batched_avg_return_metric.result(), 9)\n\n  def testReset(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    batched_avg_return_metric(self._ts0)\n    batched_avg_return_metric(self._ts1)\n    batched_avg_return_metric(self._ts2)\n    batched_avg_return_metric.reset()\n    batched_avg_return_metric(self._ts3)\n    batched_avg_return_metric(self._ts4)\n    batched_avg_return_metric(self._ts5)\n    self.assertEqual(batched_avg_return_metric.result(), 13)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/metrics/metric_equality_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.metrics.metric_equality.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.metrics import tf_py_metric\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass MetricEqualityTest(test_utils.TestCase):\n\n  def _build_metrics(self, buffer_size=10, batch_size=None):\n    python_metrics = [\n        tf_py_metric.TFPyMetric(\n            py_metrics.AverageReturnMetric(\n                buffer_size=buffer_size, batch_size=batch_size)),\n        tf_py_metric.TFPyMetric(\n            py_metrics.AverageEpisodeLengthMetric(\n                buffer_size=buffer_size, batch_size=batch_size)),\n    ]\n    if batch_size is None:\n      batch_size = 1\n    tensorflow_metrics = [\n        tf_metrics.AverageReturnMetric(\n            buffer_size=buffer_size, batch_size=batch_size),\n        tf_metrics.AverageEpisodeLengthMetric(\n            buffer_size=buffer_size, batch_size=batch_size),\n    ]\n\n    return python_metrics, tensorflow_metrics\n\n  def setUp(self):\n    super(MetricEqualityTest, self).setUp()\n    observation_spec = array_spec.BoundedArraySpec((1,),\n                                                   dtype=np.float32,\n                                                   minimum=0,\n                                                   maximum=10)\n    self._action_spec = array_spec.BoundedArraySpec((1,),\n                                                    dtype=np.float32,\n                                                    minimum=0,\n                                                    maximum=10)\n    reward_spec = array_spec.BoundedArraySpec((),\n                                              dtype=np.float32,\n                                              minimum=0,\n                                              maximum=10)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    self._time_step_spec = time_step_spec._replace(reward=reward_spec)\n\n    self._tensor_action_spec = tensor_spec.from_spec(self._action_spec)\n    self._tensor_time_step_spec = tensor_spec.from_spec(self._time_step_spec)\n\n    self._env = random_py_environment.RandomPyEnvironment(\n        observation_spec, self._action_spec)\n    self._tf_env = tf_py_environment.TFPyEnvironment(self._env)\n    self._policy = random_tf_policy.RandomTFPolicy(self._tensor_time_step_spec,\n                                                   self._tensor_action_spec)\n\n  def test_metric_results_equal(self):\n    python_metrics, tensorflow_metrics = self._build_metrics()\n    observers = python_metrics + tensorflow_metrics\n    driver = dynamic_step_driver.DynamicStepDriver(\n        self._tf_env, self._policy, observers=observers, num_steps=1000)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(driver.run())\n\n    for python_metric, tensorflow_metric in zip(python_metrics,\n                                                tensorflow_metrics):\n      python_result = self.evaluate(python_metric.result())\n      tensorflow_result = self.evaluate(tensorflow_metric.result())\n      self.assertEqual(python_result, tensorflow_result)\n\n  def test_metric_results_equal_with_batched_env(self):\n    env_ctor = lambda: random_py_environment.RandomPyEnvironment(  # pylint: disable=g-long-lambda\n        self._time_step_spec.observation, self._action_spec)\n    batch_size = 5\n    env = batched_py_environment.BatchedPyEnvironment(\n        [env_ctor() for _ in range(batch_size)])\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n\n    python_metrics, tensorflow_metrics = self._build_metrics(\n        batch_size=batch_size)\n    observers = python_metrics + tensorflow_metrics\n    driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env, self._policy, observers=observers, num_steps=1000)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(driver.run())\n\n    for python_metric, tensorflow_metric in zip(python_metrics,\n                                                tensorflow_metrics):\n      python_result = self.evaluate(python_metric.result())\n      tensorflow_result = self.evaluate(tensorflow_metric.result())\n      self.assertEqual(python_result, tensorflow_result)\n\n\nif __name__ == \'__main__\':\n  tf.compat.v1.enable_resource_variables()\n  test_utils.main()\n'"
tf_agents/metrics/py_metric.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for Python metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nfrom typing import Any, Optional, Sequence, Text, Union\n\nfrom absl import logging\nimport numpy as np\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.metrics import tf_metric\nfrom tf_agents.trajectories import trajectory as traj\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\n\nPyMetricType = types.ForwardRef(\'PyMetric\')  # pylint: disable=invalid-name\nMetricType = Union[tf_metric.TFStepMetric, PyMetricType]\n\n\ndef run_summaries(metrics: Sequence[PyMetricType],\n                  session: Optional[tf.compat.v1.Session] = None):\n  """"""Execute summary ops for py_metrics.\n\n  Args:\n    metrics: A list of py_metric.Base objects.\n    session: A TensorFlow session-like object. If it is not provided, it will\n      use the current TensorFlow session context manager.\n\n  Raises:\n    RuntimeError: If .tf_summaries() was not previously called on any of the\n      `metrics`.\n    AttributeError: If session is not provided and there is no default session\n      provided by a context manager.\n  """"""\n  if session is None:\n    default_session = tf.compat.v1.get_default_session()\n    if default_session is None:\n      raise AttributeError(\n          \'No TensorFlow session-like object was provided, and none \'\n          \'could be retrieved using \\\'tf.get_default_session()\\\'.\')\n    session = default_session\n\n  for metric in metrics:\n    if metric.summary_op is None:\n      raise RuntimeError(\'metric.tf_summaries() must be called on py_metric \'\n                         \'{} before attempting to run \'\n                         \'summaries.\'.format(metric.name))\n  summary_ops = [metric.summary_op for metric in metrics]\n  feed_dict = dict(\n      (metric.summary_placeholder, metric.result()) for metric in metrics)\n  session.run(summary_ops, feed_dict=feed_dict)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass PyMetric(tf.Module):\n  """"""Defines the interface for metrics.""""""\n\n  def __init__(self, name: Text, prefix: Text = \'Metrics\'):\n    """"""Creates a metric.""""""\n    super(PyMetric, self).__init__(name)\n    self._prefix = prefix\n    self._summary_placeholder = None\n    self._summary_op = None\n\n  @property\n  def prefix(self) -> Text:\n    """"""Prefix for the metric.""""""\n    return self._prefix\n\n  @abc.abstractmethod\n  def reset(self):\n    """"""Resets internal stat gathering variables used to compute the metric.""""""\n\n  @abc.abstractmethod\n  def result(self) -> Any:\n    """"""Evaluates the current value of the metric.""""""\n\n  def log(self):\n    tag = common.join_scope(self.prefix, self.name)\n    logging.info(\'%s\', \'{0} = {1}\'.format(tag, self.result()))\n\n  def tf_summaries(self,\n                   train_step: types.Int = None,\n                   step_metrics: Sequence[MetricType] = ()) -> tf.Operation:\n    """"""Build TF summary op and placeholder for this metric.\n\n    To execute the op, call py_metric.run_summaries.\n\n    Args:\n      train_step: Step counter for training iterations. If None, no metric is\n        generated against the global step.\n      step_metrics: Step values to plot as X axis in addition to global_step.\n\n    Returns:\n      The summary op.\n\n    Raises:\n      RuntimeError: If this method has already been called (it can only be\n        called once).\n      ValueError: If any item in step_metrics is not of type PyMetric or\n        tf_metric.TFStepMetric.\n    """"""\n    if self.summary_op is not None:\n      raise RuntimeError(\'metric.tf_summaries() can only be called once.\')\n\n    tag = common.join_scope(self.prefix, self.name)\n    summaries = []\n    summaries.append(tf.compat.v2.summary.scalar(\n        name=tag, data=self.summary_placeholder, step=train_step))\n    prefix = self.prefix\n    if prefix:\n      prefix += \'_\'\n    for step_metric in step_metrics:\n      # Skip plotting the metrics against itself.\n      if self.name == step_metric.name:\n        continue\n      step_tag = \'{}vs_{}/{}\'.format(prefix, step_metric.name, self.name)\n      if isinstance(step_metric, PyMetric):\n        step_tensor = step_metric.summary_placeholder\n      elif isinstance(step_metric, tf_metric.TFStepMetric):\n        step_tensor = step_metric.result()\n      else:\n        raise ValueError(\'step_metric is not PyMetric or TFStepMetric: \'\n                         \'{}\'.format(step_metric))\n      summaries.append(tf.compat.v2.summary.scalar(\n          name=step_tag,\n          data=self.summary_placeholder,\n          step=step_tensor))\n\n    self._summary_op = tf.group(*summaries)\n    return self._summary_op\n\n  @property\n  def summary_placeholder(self) -> tf.compat.v1.placeholder:\n    """"""TF placeholder to be used for the result of this metric.""""""\n    if self._summary_placeholder is None:\n      result = self.result()\n      if not isinstance(result, (np.ndarray, np.generic)):\n        result = np.array(result)\n      dtype = tf.as_dtype(result.dtype)\n      shape = result.shape\n      self._summary_placeholder = tf.compat.v1.placeholder(\n          dtype, shape=shape, name=\'{}_ph\'.format(self.name))\n    return self._summary_placeholder\n\n  @property\n  def summary_op(self) -> tf.Operation:\n    """"""TF summary op for this metric.""""""\n    return self._summary_op\n\n  @staticmethod\n  def aggregate(metrics: Sequence[PyMetricType]) -> types.Float:\n    """"""Aggregates a list of metrics.\n\n    The default behaviour is to return the average of the metrics.\n\n    Args:\n      metrics: a list of metrics, of the same class.\n    Returns:\n      The result of aggregating this metric.\n    """"""\n    return np.mean([metric.result() for metric in metrics])\n\n  def __call__(self, *args):\n    """"""Method to update the metric contents.\n\n    To change the behavior of this function, override the call method.\n\n    Different subclasses might use this differently. For instance, the\n    PyStepMetric takes in a trajectory, while the CounterMetric takes no\n    parameters.\n\n    Args:\n      *args: See call method of subclass for specific arguments.\n    """"""\n    self.call(*args)\n\n\nclass PyStepMetric(PyMetric):\n  """"""Defines the interface for metrics that operate on trajectories.""""""\n\n  @abc.abstractmethod\n  def call(self, trajectory: traj.Trajectory):\n    """"""Processes a trajectory to update the metric.\n\n    Args:\n      trajectory: A trajectory.Trajectory.\n    """"""\n'"
tf_agents/metrics/py_metric_test.py,23,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.metrics.py_metric.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.metrics import py_metric\n\n\nclass DummyMetric(py_metric.PyStepMetric):\n\n  def __init__(self, name=\'Metric\'):\n    super(DummyMetric, self).__init__(name)\n    self.reset()\n\n  def reset(self):\n    self.value = 0\n\n  def result(self):\n    return self.value\n\n  def call(self, trajectory):\n    pass\n\n\nclass PyMetricSummariesTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(PyMetricSummariesTest, self).setUp()\n    self.summary_dir = tempfile.mkdtemp(dir=os.getenv(\'TEST_TMPDIR\'))\n    self.writer = tf.compat.v2.summary.create_file_writer(self.summary_dir)\n    self.writer.set_as_default()\n    self.metric1 = DummyMetric(\'Metric1\')\n    self.metric2 = DummyMetric(\'Metric2\')\n    self.metric3 = DummyMetric(\'Metric3\')\n    self.global_step = tf.compat.v1.train.get_or_create_global_step()\n    self.incr_global_step = tf.compat.v1.assign_add(self.global_step, 1)\n\n  def testBuildsSummary(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    metric = DummyMetric()\n    self.assertIsNone(metric.summary_op)\n    metric.tf_summaries(train_step=self.global_step)\n    self.assertIsNotNone(metric.summary_op)\n\n  def assert_summary_equals(self, records, tag, step, value):\n    for record in records[1:]:\n      if record.summary.value[0].tag != tag:\n        continue\n      if record.step != step:\n        continue\n      self.assertEqual(value, tf.make_ndarray(record.summary.value[0].tensor))\n      return\n    self.fail(\n        \'Could not find record for tag {} and step {}\'.format(tag, step))\n\n  def get_records(self):\n    files = os.listdir(self.summary_dir)\n    self.assertEqual(1, len(files))\n    file_path = os.path.join(self.summary_dir, files[0])\n    return list(tf.compat.v1.train.summary_iterator(file_path))\n\n  def testSummarySimple(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    with tf.compat.v2.summary.record_if(True):\n      self.metric1.tf_summaries(train_step=self.global_step)\n      self.metric2.tf_summaries(train_step=self.global_step)\n\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(self.writer.init())\n      self.metric1.value = 3\n      py_metric.run_summaries([self.metric1, self.metric2])\n      sess.run(self.writer.flush())\n\n    records = self.get_records()\n\n    # 2 summaries + 1 file header\n    self.assertEqual(3, len(records))\n\n    self.assert_summary_equals(records, \'Metrics/Metric1\', 0, 3)\n    self.assert_summary_equals(records, \'Metrics/Metric2\', 0, 0)\n\n  def testSummaryUpdates(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    with tf.compat.v2.summary.record_if(True):\n      self.metric1.tf_summaries(train_step=self.global_step)\n      self.metric2.tf_summaries(train_step=self.global_step)\n\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(self.writer.init())\n      self.metric1.value = 3\n      self.metric2.value = 0\n      py_metric.run_summaries([self.metric1, self.metric2])\n      sess.run(self.incr_global_step)\n      self.metric1.value = 5\n      self.metric2.value = 2\n      py_metric.run_summaries([self.metric1, self.metric2])\n      sess.run(self.writer.flush())\n\n    records = self.get_records()\n\n    # 4 summaries + 1 file header\n    self.assertEqual(5, len(records))\n\n    self.assert_summary_equals(records, \'Metrics/Metric1\', 0, 3)\n    self.assert_summary_equals(records, \'Metrics/Metric2\', 0, 0)\n    self.assert_summary_equals(records, \'Metrics/Metric1\', 1, 5)\n    self.assert_summary_equals(records, \'Metrics/Metric2\', 1, 2)\n\n  def testSummaryStepMetrics(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    with tf.compat.v2.summary.record_if(True):\n      self.metric1.tf_summaries(\n          train_step=self.global_step, step_metrics=(self.metric2,))\n      self.metric2.tf_summaries(\n          train_step=self.global_step, step_metrics=(self.metric2,))\n\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(self.writer.init())\n      self.metric1.value = 3\n      self.metric2.value = 2\n      py_metric.run_summaries([self.metric1, self.metric2])\n      sess.run(self.writer.flush())\n\n    records = self.get_records()\n\n    # (2 records for metric1, 1 for metric2) + 1 file header\n    self.assertEqual(4, len(records))\n\n    self.assert_summary_equals(records, \'Metrics/Metric1\', 0, 3)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric2/Metric1\', 2, 3)\n\n    self.assert_summary_equals(records, \'Metrics/Metric2\', 0, 2)\n\n  def testSummaryStepMetricsUpdate(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    with tf.compat.v2.summary.record_if(True):\n      self.metric1.tf_summaries(\n          train_step=self.global_step, step_metrics=(self.metric2,))\n      self.metric2.tf_summaries(\n          train_step=self.global_step, step_metrics=(self.metric2,))\n\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(self.writer.init())\n      self.metric1.value = 3\n      self.metric2.value = 2\n      py_metric.run_summaries([self.metric1, self.metric2])\n      self.metric1.value = 4\n      self.metric2.value = 3\n      py_metric.run_summaries([self.metric1, self.metric2])\n      sess.run(self.writer.flush())\n\n    records = self.get_records()\n\n    # (2 records for metric1, 1 for metric2) * 2 + 1 file header\n    self.assertEqual(7, len(records))\n\n    self.assert_summary_equals(records, \'Metrics_vs_Metric2/Metric1\', 2, 3)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric2/Metric1\', 3, 4)\n\n  def testSummaryMultipleStepMetrics(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123881100\')\n    with tf.compat.v2.summary.record_if(True):\n      self.metric1.tf_summaries(\n          train_step=self.global_step,\n          step_metrics=(self.metric2, self.metric3))\n      self.metric2.tf_summaries(\n          train_step=self.global_step,\n          step_metrics=(self.metric2, self.metric3))\n      self.metric3.tf_summaries(\n          train_step=self.global_step,\n          step_metrics=(self.metric2, self.metric3))\n\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(self.writer.init())\n      self.metric1.value = 1\n      self.metric2.value = 2\n      self.metric3.value = 3\n      py_metric.run_summaries([self.metric1, self.metric2, self.metric3])\n      sess.run(self.writer.flush())\n\n    records = self.get_records()\n\n    # (3 records for metric1, 2 for metric2, 2 for metric3) + 1 file header\n    self.assertEqual(8, len(records))\n\n    self.assert_summary_equals(records, \'Metrics/Metric1\', 0, 1)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric2/Metric1\', 2, 1)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric3/Metric1\', 3, 1)\n\n    self.assert_summary_equals(records, \'Metrics/Metric2\', 0, 2)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric3/Metric2\', 3, 2)\n\n    self.assert_summary_equals(records, \'Metrics/Metric3\', 0, 3)\n    self.assert_summary_equals(records, \'Metrics_vs_Metric2/Metric3\', 2, 3)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/metrics/py_metrics.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implementation of various python metrics.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\n\nimport gin\nimport numpy as np\nimport six\n\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.trajectories import trajectory as traj\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import numpy_storage\n\nfrom typing import Any, Iterable, Optional, Text\n\n\nclass NumpyDeque(numpy_storage.NumpyState):\n  """"""Deque implementation using a numpy array as a circular buffer.""""""\n\n  def __init__(self, maxlen: types.Int, dtype: np.dtype):\n    """"""Deque using a numpy array as a circular buffer, with FIFO evictions.\n\n    Args:\n      maxlen: Maximum length of the deque before beginning to evict the oldest\n        entries. If np.inf, deque size is unlimited and the array will grow\n        automatically.\n      dtype: Data type of deque elements.\n    """"""\n    self._start_index = np.int64(0)\n    self._len = np.int64(0)\n    self._maxlen = np.array(maxlen)\n    initial_len = 10 if np.isinf(self._maxlen) else self._maxlen\n    self._buffer = np.zeros(shape=(initial_len,), dtype=dtype)\n\n  def clear(self):\n    self._start_index = np.int64(0)\n    self._len = np.int64(0)\n\n  def add(self, value: Any):\n    insert_idx = int((self._start_index + self._len) % self._maxlen)\n\n    # Increase buffer size if necessary.\n    if np.isinf(self._maxlen) and insert_idx >= self._buffer.shape[0]:\n      self._buffer.resize((self._buffer.shape[0] * 2,))\n\n    self._buffer[insert_idx] = value\n    if self._len < self._maxlen:\n      self._len += 1\n    else:\n      self._start_index = np.mod(self._start_index + 1, self._maxlen)\n\n  def extend(self, values: Iterable[Any]):\n    for value in values:\n      self.add(value)\n\n  def __len__(self) -> types.Int:\n    return self._len\n\n  def mean(self, dtype: Optional[np.dtype] = None):\n    if self._len == self._buffer.shape[0]:\n      return np.mean(self._buffer, dtype=dtype)\n\n    assert self._start_index == 0\n    return np.mean(self._buffer[:self._len], dtype=dtype)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass StreamingMetric(py_metric.PyStepMetric):\n  """"""Abstract base class for streaming metrics.\n\n  Streaming metrics keep track of the last (upto) K values of the metric in a\n  Deque buffer of size K. Calling result() will return the average value of the\n  items in the buffer.\n  """"""\n\n  def __init__(self,\n               name: Text = \'StreamingMetric\',\n               buffer_size: types.Int = 10,\n               batch_size: Optional[types.Int] = None):\n    super(StreamingMetric, self).__init__(name)\n    self._buffer = NumpyDeque(maxlen=buffer_size, dtype=np.float64)\n    self._batch_size = batch_size\n    self.reset()\n\n  def reset(self):\n    self._buffer.clear()\n    if self._batch_size:\n      self._reset(self._batch_size)\n\n  @abc.abstractmethod\n  def _reset(self, batch_size: types.Int):\n    """"""Reset stat gathering variables in child classes.""""""\n\n  def add_to_buffer(self, values: Iterable[Any]):\n    """"""Appends new values to the buffer.""""""\n    self._buffer.extend(values)\n\n  def result(self) -> np.float32:\n    """"""Returns the value of this metric.""""""\n    if self._buffer:\n      return self._buffer.mean(dtype=np.float32)\n    return np.array(0.0, dtype=np.float32)\n\n  @abc.abstractmethod\n  def _batched_call(self, trajectory: traj.Trajectory):\n    """"""Call with trajectory always batched.""""""\n\n  def call(self, trajectory: traj.Trajectory):\n    if not self._batch_size:\n      if trajectory.step_type.ndim == 0:\n        self._batch_size = 1\n      else:\n        assert trajectory.step_type.ndim == 1\n        self._batch_size = trajectory.step_type.shape[0]\n      self.reset()\n    if trajectory.step_type.ndim == 0:\n      trajectory = nest_utils.batch_nested_array(trajectory)\n    self._batched_call(trajectory)\n\n\n@gin.configurable\nclass AverageReturnMetric(StreamingMetric):\n  """"""Computes the average undiscounted reward.""""""\n\n  def __init__(self,\n               name: Text = \'AverageReturn\',\n               buffer_size: types.Int = 10,\n               batch_size: Optional[types.Int] = None):\n    """"""Creates an AverageReturnMetric.""""""\n    self._np_state = numpy_storage.NumpyState()\n    # Set a dummy value on self._np_state.episode_return so it gets included in\n    # the first checkpoint (before metric is first called).\n    self._np_state.episode_return = np.float64(0)\n    super(AverageReturnMetric, self).__init__(name, buffer_size=buffer_size,\n                                              batch_size=batch_size)\n\n  def _reset(self, batch_size):\n    """"""Resets stat gathering variables.""""""\n    self._np_state.episode_return = np.zeros(\n        shape=(batch_size,), dtype=np.float64)\n\n  def _batched_call(self, trajectory):\n    """"""Processes the trajectory to update the metric.\n\n    Args:\n      trajectory: a tf_agents.trajectory.Trajectory.\n    """"""\n    episode_return = self._np_state.episode_return\n\n    is_first = np.where(trajectory.is_first())\n    episode_return[is_first] = 0\n\n    episode_return += trajectory.reward\n\n    is_last = np.where(trajectory.is_last())\n    self.add_to_buffer(episode_return[is_last])\n\n\n@gin.configurable\nclass AverageEpisodeLengthMetric(StreamingMetric):\n  """"""Computes the average episode length.""""""\n\n  def __init__(self,\n               name: Text = \'AverageEpisodeLength\',\n               buffer_size: types.Int = 10,\n               batch_size: Optional[types.Int] = None):\n    """"""Creates an AverageEpisodeLengthMetric.""""""\n    self._np_state = numpy_storage.NumpyState()\n    # Set a dummy value on self._np_state.episode_return so it gets included in\n    # the first checkpoint (before metric is first called).\n    self._np_state.episode_steps = np.float64(0)\n    super(AverageEpisodeLengthMetric, self).__init__(\n        name, buffer_size=buffer_size, batch_size=batch_size)\n\n  def _reset(self, batch_size):\n    """"""Resets stat gathering variables.""""""\n    self._np_state.episode_steps = np.zeros(\n        shape=(batch_size,), dtype=np.float64)\n\n  def _batched_call(self, trajectory):\n    """"""Processes the trajectory to update the metric.\n\n    Args:\n      trajectory: a tf_agents.trajectory.Trajectory.\n    """"""\n    episode_steps = self._np_state.episode_steps\n\n    # Each non-boundary trajectory (first, mid or last) represents a step.\n    episode_steps[np.where(~trajectory.is_boundary())] += 1\n    self.add_to_buffer(episode_steps[np.where(trajectory.is_last())])\n    episode_steps[np.where(trajectory.is_last())] = 0\n\n\n@gin.configurable\nclass EnvironmentSteps(py_metric.PyStepMetric):\n  """"""Counts the number of steps taken in the environment.""""""\n\n  def __init__(self, name: Text = \'EnvironmentSteps\'):\n    super(EnvironmentSteps, self).__init__(name)\n    self._np_state = numpy_storage.NumpyState()\n    self.reset()\n\n  def reset(self):\n    self._np_state.environment_steps = np.int64(0)\n\n  def result(self) -> np.int64:\n    return self._np_state.environment_steps\n\n  def call(self, trajectory: traj.Trajectory):\n    if trajectory.step_type.ndim == 0:\n      trajectory = nest_utils.batch_nested_array(trajectory)\n\n    new_steps = np.sum((~trajectory.is_boundary()).astype(np.int64))\n    self._np_state.environment_steps += new_steps\n\n\n@gin.configurable\nclass NumberOfEpisodes(py_metric.PyStepMetric):\n  """"""Counts the number of episodes in the environment.""""""\n\n  def __init__(self, name: Text = \'NumberOfEpisodes\'):\n    super(NumberOfEpisodes, self).__init__(name)\n    self._np_state = numpy_storage.NumpyState()\n    self.reset()\n\n  def reset(self):\n    self._np_state.number_episodes = np.int64(0)\n\n  def result(self) -> np.int64:\n    return self._np_state.number_episodes\n\n  def call(self, trajectory: traj.Trajectory):\n    if trajectory.step_type.ndim == 0:\n      trajectory = nest_utils.batch_nested_array(trajectory)\n\n    completed_episodes = np.sum(trajectory.is_last().astype(np.int64))\n    self._np_state.number_episodes += completed_episodes\n\n\n@gin.configurable\nclass CounterMetric(py_metric.PyMetric):\n  """"""Metric to track an arbitrary counter.\n\n  This is useful for, e.g., tracking the current train/eval iteration number.\n\n  To increment the counter, you can __call__ it (e.g. metric_obj()).\n  """"""\n\n  def __init__(self, name: Text = \'Counter\'):\n    super(CounterMetric, self).__init__(name)\n    self._np_state = numpy_storage.NumpyState()\n    self.reset()\n\n  def reset(self):\n    self._np_state.count = np.int64(0)\n\n  def call(self):\n    self._np_state.count += 1\n\n  def result(self) -> np.int64:\n    return self._np_state.count\n'"
tf_agents/metrics/py_metrics_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.metrics.py_metrics.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import nest_utils\n\n\nclass PyMetricsTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, \'AverageReturn\'),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       \'AverageEpisodeLength\'),\n      (\'EnvironmentSteps\', py_metrics.EnvironmentSteps, \'EnvironmentSteps\'),\n      (\'NumberOfEpisodes\', py_metrics.NumberOfEpisodes, \'NumberOfEpisodes\'),\n      (\'CounterMetric\', py_metrics.CounterMetric, \'Counter\'))\n  def testName(self, metric_class, expected_name):\n    metric = metric_class()\n    self.assertEqual(expected_name, metric.name)\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric),\n      (\'EnvironmentSteps\', py_metrics.EnvironmentSteps),\n      (\'NumberOfEpisodes\', py_metrics.NumberOfEpisodes),\n      (\'CounterMetric\', py_metrics.NumberOfEpisodes))\n  def testChangeName(self, metric_class):\n    name = \'SomeMetric\'\n    metric = metric_class(name)\n    self.assertEqual(metric.name, name)\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, 0.0),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       0.0),\n      (\'EnvironmentSteps\', py_metrics.EnvironmentSteps, 1.0),\n      (\'NumberOfEpisodes\', py_metrics.NumberOfEpisodes, 0.0))\n  def testZeroEpisodes(self, metric_class, expected_result):\n    metric = metric_class()\n    # Order of args for trajectory methods:\n    # observation, action, policy_info, reward, discount\n    metric(trajectory.boundary((), (), (), 0., 1.))\n    metric(trajectory.first((), (), (), 1., 1.))\n    self.assertEqual(expected_result, metric.result())\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, 6.0),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       3.0),\n      (\'EnvironmentSteps\', py_metrics.EnvironmentSteps, 3.0),\n      (\'NumberOfEpisodes\', py_metrics.NumberOfEpisodes, 1.0))\n  def testAverageOneEpisode(self, metric_class, expected_result):\n    metric = metric_class()\n\n    metric(trajectory.boundary((), (), (), 0., 1.))\n    metric(trajectory.mid((), (), (), 1., 1.))\n    metric(trajectory.mid((), (), (), 2., 1.))\n    metric(trajectory.last((), (), (), 3., 0.))\n    self.assertEqual(expected_result, metric.result())\n\n  @parameterized.named_parameters((\'AverageReturnMetric\',\n                                   py_metrics.AverageReturnMetric, 7.0))\n  def testAverageOneEpisodeWithReset(self, metric_class, expected_result):\n    metric = metric_class()\n\n    metric(trajectory.first((), (), (), 0., 1.))\n    metric(trajectory.mid((), (), (), 1., 1.))\n    metric(trajectory.mid((), (), (), 2., 1.))\n    # The episode is reset.\n    #\n    # This could happen when using the dynamic_episode_driver with\n    # parallel_py_environment. When the parallel episodes are of different\n    # lengths and num_episodes is reached, some episodes would be left in ""MID"".\n    # When the driver runs again, all environments are reset at the beginning\n    # of the tf.while_loop and the unfinished episodes would get ""FIRST"" without\n    # seeing ""LAST"".\n    metric(trajectory.first((), (), (), 3., 1.))\n    metric(trajectory.last((), (), (), 4., 1.))\n    self.assertEqual(expected_result, metric.result())\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, 0.0),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       2.0),\n      (\'EnvironmentSteps\', py_metrics.EnvironmentSteps, 4.0),\n      (\'NumberOfEpisodes\', py_metrics.NumberOfEpisodes, 2.0))\n  def testAverageTwoEpisode(self, metric_class, expected_result):\n    metric = metric_class()\n\n    metric(trajectory.boundary((), (), (), 0., 1.))\n    metric(trajectory.first((), (), (), 1., 1.))\n    metric(trajectory.mid((), (), (), 2., 1.))\n    metric(trajectory.last((), (), (), 3., 0.))\n    metric(trajectory.boundary((), (), (), 0., 1.))\n\n    # TODO(kbanoop): Add optional next_step_type arg to trajectory.first. Or\n    # implement trajectory.first_last().\n    metric(\n        trajectory.Trajectory(ts.StepType.FIRST, (), (), (), ts.StepType.LAST,\n                              -6., 1.))\n\n    self.assertEqual(expected_result, metric.result())\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, 5.0),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       2.5))\n  def testBatch(self, metric_class, expected_result):\n    metric = metric_class()\n\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.first((), (), (), 1., 1.),\n        trajectory.first((), (), (), 1., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.mid((), (), (), 2., 1.),\n        trajectory.last((), (), (), 3., 0.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.last((), (), (), 3., 0.),\n        trajectory.boundary((), (), (), 0., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.first((), (), (), 1., 1.)]))\n    self.assertEqual(expected_result, metric.result(), 5.0)\n\n  @parameterized.named_parameters(\n      (\'AverageReturnMetric\', py_metrics.AverageReturnMetric, 5.0),\n      (\'AverageEpisodeLengthMetric\', py_metrics.AverageEpisodeLengthMetric,\n       2.5))\n  def testBatchSizeProvided(self, metric_class, expected_result):\n    metric = metric_class(batch_size=2)\n\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.first((), (), (), 1., 1.),\n        trajectory.first((), (), (), 1., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.mid((), (), (), 2., 1.),\n        trajectory.last((), (), (), 3., 0.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.last((), (), (), 3., 0.),\n        trajectory.boundary((), (), (), 0., 1.)]))\n    metric(nest_utils.stack_nested_arrays([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.first((), (), (), 1., 1.)]))\n    self.assertEqual(metric.result(), expected_result)\n\n  def testCounterMetricIncrements(self):\n    counter = py_metrics.CounterMetric()\n\n    self.assertEqual(0, counter.result())\n    counter()\n    self.assertEqual(1, counter.result())\n    counter()\n    self.assertEqual(2, counter.result())\n    counter.reset()\n    self.assertEqual(0, counter.result())\n    counter()\n    self.assertEqual(1, counter.result())\n\n  def testSaveRestore(self):\n    metrics = [\n        py_metrics.AverageReturnMetric(),\n        py_metrics.AverageEpisodeLengthMetric(),\n        py_metrics.EnvironmentSteps(),\n        py_metrics.NumberOfEpisodes()\n    ]\n\n    for metric in metrics:\n      metric(trajectory.boundary((), (), (), 0., 1.))\n      metric(trajectory.mid((), (), (), 1., 1.))\n      metric(trajectory.mid((), (), (), 2., 1.))\n      metric(trajectory.last((), (), (), 3., 0.))\n\n    checkpoint = tf.train.Checkpoint(**{m.name: m for m in metrics})\n    prefix = self.get_temp_dir() + \'/ckpt\'\n    save_path = checkpoint.save(prefix)\n    for metric in metrics:\n      metric.reset()\n      self.assertEqual(0, metric.result())\n    checkpoint.restore(save_path).assert_consumed()\n    for metric in metrics:\n      self.assertGreater(metric.result(), 0)\n\n\nclass NumpyDequeTest(tf.test.TestCase):\n\n  def testSimple(self):\n    buf = py_metrics.NumpyDeque(maxlen=10, dtype=np.float64)\n    buf.add(2)\n    buf.add(3)\n    buf.add(5)\n    buf.add(6)\n    self.assertEqual(4, buf.mean())\n\n  def testFullLength(self):\n    buf = py_metrics.NumpyDeque(maxlen=4, dtype=np.float64)\n    buf.add(2)\n    buf.add(3)\n    buf.add(5)\n    buf.add(6)\n    self.assertEqual(4, buf.mean())\n\n  def testPastMaxLen(self):\n    buf = py_metrics.NumpyDeque(maxlen=4, dtype=np.float64)\n    buf.add(2)\n    buf.add(3)\n    buf.add(5)\n    buf.add(6)\n    buf.add(8)\n    buf.add(9)\n    self.assertEqual(7, buf.mean())\n\n  def testClear(self):\n    buf = py_metrics.NumpyDeque(maxlen=4, dtype=np.float64)\n    buf.add(2)\n    buf.add(3)\n    buf.clear()\n    buf.add(5)\n    self.assertEqual(5, buf.mean())\n\n  def testUnbounded(self):\n    buf = py_metrics.NumpyDeque(maxlen=np.inf, dtype=np.float64)\n    for i in range(101):\n      buf.add(i)\n    self.assertEqual(50, buf.mean())\n\n  def testUnboundedClear(self):\n    buf = py_metrics.NumpyDeque(maxlen=np.inf, dtype=np.float64)\n    for i in range(101):\n      buf.add(i)\n    buf.clear()\n    buf.add(4)\n    buf.add(6)\n    self.assertEqual(5, buf.mean())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/metrics/tf_metric.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for TensorFlow metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import common\n\n\nclass TFStepMetric(tf.Module):\n  """"""Defines the interface for TF metrics.""""""\n\n  def __init__(self, name, prefix=\'Metrics\'):\n    super(TFStepMetric, self).__init__(name)\n    common.check_tf1_allowed()\n    self._prefix = prefix\n\n  def call(self, *args, **kwargs):\n    """"""Accumulates statistics for the metric. Users should use __call__ instead.\n\n    Note: This function is executed as a graph function in graph mode.\n    This means:\n    a) Operations on the same resource are executed in textual order.\n       This should make it easier to do things like add the updated\n       value of a variable to another, for example.\n    b) You don\'t need to worry about collecting the update ops to execute.\n       All update ops added to the graph by this function will be executed.\n    As a result, code should generally work the same way with graph or\n    eager execution.\n\n    Args:\n      *args:\n      **kwargs: A mini-batch of inputs to the Metric, as passed to\n        `__call__()`.\n    """"""\n    raise NotImplementedError(\'Metrics must define a call() member function\')\n\n  def reset(self):\n    """"""Resets the values being tracked by the metric.""""""\n    raise NotImplementedError(\'Metrics must define a reset() member function\')\n\n  def result(self):\n    """"""Computes and returns a final value for the metric.""""""\n    raise NotImplementedError(\'Metrics must define a result() member function\')\n\n  def init_variables(self):\n    """"""Initializes this Metric\'s variables.\n\n    Should be called after variables are created in the first execution\n    of `__call__()`. If using graph execution, the return value should be\n    `run()` in a session before running the op returned by `__call__()`.\n    (See example above.)\n\n    Returns:\n      If using graph execution, this returns an op to perform the\n      initialization. Under eager execution, the variables are reset to their\n      initial values as a side effect and this function returns None.\n    """"""\n    if not tf.executing_eagerly():\n      return tf.compat.v1.group([v.initializer for v in self.variables])\n\n  @common.function\n  def _update_state(self, *arg, **kwargs):\n    """"""A function wrapping the implementor-defined call method.""""""\n    return self.call(*arg, **kwargs)\n\n  def __call__(self, *args, **kwargs):\n    """"""Returns op to execute to update this metric for these inputs.\n\n    Returns None if eager execution is enabled.\n    Returns a graph-mode function if graph execution is enabled.\n\n    Args:\n      *args:\n      **kwargs: A mini-batch of inputs to the Metric, passed on to `call()`.\n    """"""\n    return self._update_state(*args, **kwargs)\n\n  def tf_summaries(self, train_step=None, step_metrics=()):\n    """"""Generates summaries against train_step and all step_metrics.\n\n    Args:\n      train_step: (Optional) Step counter for training iterations. If None, no\n        metric is generated against the global step.\n      step_metrics: (Optional) Iterable of step metrics to generate summaries\n        against.\n\n    Returns:\n      A list of summaries.\n    """"""\n    summaries = []\n    prefix = self._prefix\n    tag = common.join_scope(prefix, self.name)\n    result = self.result()\n    if train_step is not None:\n      summaries.append(\n          tf.compat.v2.summary.scalar(name=tag, data=result, step=train_step))\n    if prefix:\n      prefix += \'_\'\n    for step_metric in step_metrics:\n      # Skip plotting the metrics against itself.\n      if self.name == step_metric.name:\n        continue\n      step_tag = \'{}vs_{}/{}\'.format(prefix, step_metric.name, self.name)\n      # Summaries expect the step value to be an int64.\n      step = tf.cast(step_metric.result(), tf.int64)\n      summaries.append(tf.compat.v2.summary.scalar(\n          name=step_tag,\n          data=result,\n          step=step))\n    return summaries\n\n\nclass TFHistogramStepMetric(TFStepMetric):\n  """"""A metric class for metrics that emit multiple values.\n\n  The only difference between `TFSTepMetric` and `TFHistogramStepMetric` is that\n  the latter uses histogram summaries instead of scalar summaries.\n  """"""\n\n  def tf_summaries(self, train_step=None, step_metrics=()):\n    """"""Generates histogram summaries against train_step and all step_metrics.\n\n    Args:\n      train_step: (Optional) Step counter for training iterations. If None, no\n        metric is generated against the global step.\n      step_metrics: (Optional) Iterable of step metrics to generate summaries\n        against.\n\n    Returns:\n      A list of histogram summaries.\n    """"""\n    summaries = []\n    prefix = self._prefix\n    tag = common.join_scope(prefix, self.name)\n    result = self.result()\n    if train_step is not None:\n      summaries.append(\n          tf.compat.v2.summary.histogram(\n              name=tag, data=result, step=train_step))\n    if prefix:\n      prefix += \'_\'\n    for step_metric in step_metrics:\n      # Skip plotting the metrics against itself.\n      if self.name == step_metric.name:\n        continue\n      step_tag = \'{}vs_{}/{}\'.format(prefix, step_metric.name, self.name)\n      # Summaries expect the step value to be an int64.\n      step = tf.cast(step_metric.result(), tf.int64)\n      summaries.append(\n          tf.compat.v2.summary.histogram(\n              name=step_tag, data=result, step=step))\n    return summaries\n\n\nclass TFMultiMetricStepMetric(TFStepMetric):\n  """"""A TF step metric that emits multiple values per step.\n\n  The only difference between `TFSTepMetric` and `TFMultiMetricStepMetric` is\n  that the latter creates at each step many scalar summaries, one per metric.\n  """"""\n\n  def __init__(self, name, prefix=\'Metrics\', metric_names=()):\n    super(TFMultiMetricStepMetric, self).__init__(name, prefix)\n    self._metric_names = metric_names\n\n  @property\n  def metric_names(self):\n    return self._metric_names\n\n  def tf_summaries(self, train_step=None, step_metrics=()):\n    """"""Generates per-metric summaries against `train_step` and `step_metrics`.\n\n    Args:\n      train_step: (Optional) Step counter for training iterations. If None, no\n        metric is generated against the global step.\n      step_metrics: (Optional) Iterable of step metrics to generate summaries\n        against.\n\n    Returns:\n      A list of scalar summaries.\n    """"""\n    result_list = self.result()\n    prefix = self._prefix\n    single_metric_name = \'Metric\'\n    # In case there is a single name (e.g., `Reward`) for all metrics, store it\n    # in `single_metric_name`.\n    if len(self.metric_names) == 1:\n      single_metric_name = self.metric_names[0]\n    summaries = []\n    for metric_index, result in enumerate(result_list):\n      # Common name for all metrics.\n      tag = common.join_scope(prefix, self.name)\n      # The default metric name is the `single_metric_name` followed by the\n      # index.\n      metric_name = single_metric_name + str(metric_index)\n      # In case there is a valid individual name for each metric, use it.\n      if (metric_index < len(self.metric_names) and\n          len(result_list) == len(self.metric_names) and\n          self.metric_names[metric_index] is not None):\n        metric_name = self.metric_names[metric_index]\n      tag = common.join_scope(tag, metric_name)\n      if train_step is not None:\n        summaries.append(\n            tf.compat.v2.summary.scalar(name=tag, data=result, step=train_step))\n    if prefix:\n      prefix += \'_\'\n    for metric_index, result in enumerate(result_list):\n      for step_metric in step_metrics:\n        # Skip plotting the metrics against itself.\n        if self.name == step_metric.name:\n          continue\n\n        # The default metric name is the `single_metric_name` followed by the\n        # index.\n        metric_name = single_metric_name + str(metric_index)\n        # In case there is a valid individual name for each metric, use it.\n        if (metric_index < len(self.metric_names) and\n            len(result_list) == len(self.metric_names) and\n            self.metric_names[metric_index] is not None):\n          metric_name = self.metric_names[metric_index]\n        step_tag = \'{}vs_{}/{}/{}\'.format(prefix, step_metric.name,\n                                          self.name, metric_name)\n        # Summaries expect the step value to be an int64.\n        step = tf.cast(step_metric.result(), tf.int64)\n        summaries.append(tf.compat.v2.summary.scalar(\n            name=step_tag,\n            data=result,\n            step=step))\n\n    return summaries\n'"
tf_agents/metrics/tf_metrics.py,47,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TF metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.metrics import tf_metric\nfrom tf_agents.replay_buffers import table\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nclass TFDeque(object):\n  """"""Deque backed by tf.Variable storage.""""""\n\n  def __init__(self, max_len, dtype, shape=(), name=\'TFDeque\'):\n    self._max_len = tf.convert_to_tensor(max_len, dtype=tf.int32)\n    self._spec = tf.TensorSpec(shape, dtype, name=\'Buffer\')\n    self._buffer = table.Table(self._spec, capacity=max_len)\n\n    self._head = common.create_variable(\n        initial_value=0, dtype=tf.int32, shape=(), name=name + \'Head\')\n\n  @property\n  def data(self):\n    return self._buffer.read(tf.range(self.length))\n\n  @common.function(autograph=True)\n  def extend(self, value):\n    for v in value:\n      self.add(v)\n\n  @common.function(autograph=True)\n  def add(self, value):\n    position = tf.math.mod(self._head, self._max_len)\n    self._buffer.write(position, value)\n    self._head.assign_add(1)\n\n  @property\n  def length(self):\n    return tf.minimum(self._head, self._max_len)\n\n  @common.function\n  def clear(self):\n    self._head.assign(0)\n\n  @common.function(autograph=True)\n  def mean(self):\n    if tf.equal(self._head, 0):\n      return tf.zeros(self._spec.shape, self._spec.dtype)\n    return tf.math.reduce_mean(self.data, axis=0)\n\n\n@gin.configurable(module=\'tf_agents\')\nclass EnvironmentSteps(tf_metric.TFStepMetric):\n  """"""Counts the number of steps taken in the environment.""""""\n\n  def __init__(self, name=\'EnvironmentSteps\', prefix=\'Metrics\', dtype=tf.int64):\n    super(EnvironmentSteps, self).__init__(name=name, prefix=prefix)\n    self.dtype = dtype\n    self.environment_steps = common.create_variable(\n        initial_value=0, dtype=self.dtype, shape=(), name=\'environment_steps\')\n\n  def call(self, trajectory):\n    """"""Increase the number of environment_steps according to trajectory.\n\n    Step count is not increased on trajectory.boundary() since that step\n    is not part of any episode.\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    # The __call__ will execute this.\n    num_steps = tf.cast(~trajectory.is_boundary(), self.dtype)\n    num_steps = tf.reduce_sum(input_tensor=num_steps)\n    self.environment_steps.assign_add(num_steps)\n    return trajectory\n\n  def result(self):\n    return tf.identity(self.environment_steps, name=self.name)\n\n  @common.function\n  def reset(self):\n    self.environment_steps.assign(0)\n\n\n@gin.configurable(module=\'tf_agents\')\nclass NumberOfEpisodes(tf_metric.TFStepMetric):\n  """"""Counts the number of episodes in the environment.""""""\n\n  def __init__(self, name=\'NumberOfEpisodes\', prefix=\'Metrics\', dtype=tf.int64):\n    super(NumberOfEpisodes, self).__init__(name=name, prefix=prefix)\n    self.dtype = dtype\n    self.number_episodes = common.create_variable(\n        initial_value=0, dtype=self.dtype, shape=(), name=\'number_episodes\')\n\n  def call(self, trajectory):\n    """"""Increase the number of number_episodes according to trajectory.\n\n    It would increase for all trajectory.is_last().\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    # The __call__ will execute this.\n    num_episodes = tf.cast(trajectory.is_last(), self.dtype)\n    num_episodes = tf.reduce_sum(input_tensor=num_episodes)\n    self.number_episodes.assign_add(num_episodes)\n    return trajectory\n\n  def result(self):\n    return tf.identity(self.number_episodes, name=self.name)\n\n  @common.function\n  def reset(self):\n    self.number_episodes.assign(0)\n\n\n@gin.configurable(module=\'tf_agents\')\nclass AverageReturnMetric(tf_metric.TFStepMetric):\n  """"""Metric to compute the average return.""""""\n\n  def __init__(self,\n               name=\'AverageReturn\',\n               prefix=\'Metrics\',\n               dtype=tf.float32,\n               batch_size=1,\n               buffer_size=10):\n    super(AverageReturnMetric, self).__init__(name=name, prefix=prefix)\n    self._buffer = TFDeque(buffer_size, dtype)\n    self._dtype = dtype\n    self._return_accumulator = common.create_variable(\n        initial_value=0, dtype=dtype, shape=(batch_size,), name=\'Accumulator\')\n\n  @common.function(autograph=True)\n  def call(self, trajectory):\n    # Zero out batch indices where a new episode is starting.\n    self._return_accumulator.assign(\n        tf.where(trajectory.is_first(), tf.zeros_like(self._return_accumulator),\n                 self._return_accumulator))\n\n    # Update accumulator with received rewards.\n    self._return_accumulator.assign_add(trajectory.reward)\n\n    # Add final returns to buffer.\n    last_episode_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n    for indx in last_episode_indices:\n      self._buffer.add(self._return_accumulator[indx])\n\n    return trajectory\n\n  def result(self):\n    return self._buffer.mean()\n\n  @common.function\n  def reset(self):\n    self._buffer.clear()\n    self._return_accumulator.assign(tf.zeros_like(self._return_accumulator))\n\n\n@gin.configurable(module=\'tf_agents\')\nclass AverageEpisodeLengthMetric(tf_metric.TFStepMetric):\n  """"""Metric to compute the average episode length.""""""\n\n  def __init__(self,\n               name=\'AverageEpisodeLength\',\n               prefix=\'Metrics\',\n               dtype=tf.float32,\n               batch_size=1,\n               buffer_size=10):\n    super(AverageEpisodeLengthMetric, self).__init__(name=name, prefix=prefix)\n    self._buffer = TFDeque(buffer_size, dtype)\n    self._dtype = dtype\n    self._length_accumulator = common.create_variable(\n        initial_value=0, dtype=dtype, shape=(batch_size,), name=\'Accumulator\')\n\n  @common.function(autograph=True)\n  def call(self, trajectory):\n    # Each non-boundary trajectory (first, mid or last) represents a step.\n    non_boundary_indices = tf.squeeze(\n        tf.where(tf.logical_not(trajectory.is_boundary())), axis=-1)\n    self._length_accumulator.scatter_add(\n        tf.IndexedSlices(\n            tf.ones_like(\n                non_boundary_indices, dtype=self._length_accumulator.dtype),\n            non_boundary_indices))\n\n    # Add lengths to buffer when we hit end of episode\n    last_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n    for indx in last_indices:\n      self._buffer.add(self._length_accumulator[indx])\n\n    # Clear length accumulator at the end of episodes.\n    self._length_accumulator.scatter_update(\n        tf.IndexedSlices(\n            tf.zeros_like(last_indices, dtype=self._dtype), last_indices))\n\n    return trajectory\n\n  def result(self):\n    return self._buffer.mean()\n\n  @common.function\n  def reset(self):\n    self._buffer.clear()\n    self._length_accumulator.assign(tf.zeros_like(self._length_accumulator))\n\n\n@gin.configurable(module=\'tf_agents\')\nclass ChosenActionHistogram(tf_metric.TFHistogramStepMetric):\n  """"""Metric to compute the frequency of each action chosen.""""""\n\n  def __init__(self,\n               name=\'ChosenActionHistogram\',\n               dtype=tf.int32,\n               buffer_size=100):\n    super(ChosenActionHistogram, self).__init__(name=name)\n    self._buffer = TFDeque(buffer_size, dtype)\n    self._dtype = dtype\n\n  @common.function\n  def call(self, trajectory):\n    self._buffer.extend(trajectory.action)\n    return trajectory\n\n  @common.function\n  def result(self):\n    return self._buffer.data\n\n  @common.function\n  def reset(self):\n    self._buffer.clear()\n\n\n@gin.configurable(module=\'tf_agents\')\nclass AverageReturnMultiMetric(tf_metric.TFMultiMetricStepMetric):\n  """"""Metric to compute the average return for multiple metrics.""""""\n\n  def __init__(self,\n               reward_spec,\n               name=\'AverageReturnMultiMetric\',\n               prefix=\'Metrics\',\n               dtype=tf.float32,\n               batch_size=1,\n               buffer_size=10):\n    self._batch_size = batch_size\n    self._buffer = tf.nest.map_structure(\n        lambda r: TFDeque(buffer_size, r.dtype, r.shape), reward_spec)\n    metric_names = _get_metric_names_from_spec(reward_spec)\n    self._dtype = dtype\n    def create_acc(spec):\n      return common.create_variable(\n          initial_value=np.zeros((batch_size,) + spec.shape),\n          shape=(batch_size,) + spec.shape,\n          dtype=spec.dtype,\n          name=\'Accumulator/\' + spec.name)\n    self._return_accumulator = tf.nest.map_structure(create_acc, reward_spec)\n    self._reward_spec = reward_spec\n    super(AverageReturnMultiMetric, self).__init__(\n        name=name, prefix=prefix, metric_names=metric_names)\n\n  @common.function(autograph=True)\n  def call(self, trajectory):\n    nest_utils.assert_same_structure(trajectory.reward, self._reward_spec)\n    for buf, return_acc, reward in zip(\n        tf.nest.flatten(self._buffer),\n        tf.nest.flatten(self._return_accumulator),\n        tf.nest.flatten(trajectory.reward)):\n      # Zero out batch indices where a new episode is starting.\n      is_start = trajectory.is_first()\n      if reward.shape.rank > 1:\n        is_start = tf.broadcast_to(tf.reshape(trajectory.is_first(), [-1, 1]),\n                                   tf.shape(return_acc))\n      return_acc.assign(\n          tf.where(is_start, tf.zeros_like(return_acc),\n                   return_acc))\n\n      # Update accumulator with received rewards.\n      return_acc.assign_add(reward)\n\n      # Add final returns to buffer.\n      last_episode_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n      for indx in last_episode_indices:\n        buf.add(return_acc[indx])\n\n    return trajectory\n\n  def result(self):\n    return tf.nest.map_structure(lambda b: b.mean(), self._buffer)\n\n  @common.function\n  def reset(self):\n    tf.nest.map_structure(lambda b: b.clear(), self._buffer)\n    tf.nest.map_structure(lambda acc: acc.assign(tf.zeros_like(acc)),\n                          self._return_accumulator)\n\n\ndef log_metrics(metrics, prefix=\'\'):\n  log = [\'{0} = {1}\'.format(m.name, m.log().numpy()) for m in metrics]\n  logging.info(\'%s\', \'{0} \\n\\t\\t {1}\'.format(prefix, \'\\n\\t\\t \'.join(log)))\n\n\ndef _get_metric_names_from_spec(reward_spec):\n  reward_spec_flat = tf.nest.flatten(reward_spec)\n  metric_names_list = tf.nest.map_structure(lambda r: r.name, reward_spec_flat)\n  return metric_names_list\n'"
tf_agents/metrics/tf_metrics_test.py,98,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.train.tf_metrics.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import trajectory\n\nfrom tensorflow.python.eager import context  # TF internal\n\n\nclass TFDequeTest(tf.test.TestCase):\n\n  def test_data_is_zero(self):\n    d = tf_metrics.TFDeque(3, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual([], self.evaluate(d.data))\n\n  def test_rolls_over(self):\n    d = tf_metrics.TFDeque(3, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add(1))\n    self.evaluate(d.add(2))\n    self.evaluate(d.add(3))\n    self.assertAllEqual([1, 2, 3], self.evaluate(d.data))\n\n    self.evaluate(d.add(4))\n    self.assertAllEqual([4, 2, 3], self.evaluate(d.data))\n\n  def test_clear(self):\n    d = tf_metrics.TFDeque(3, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add(1))\n    self.evaluate(d.add(2))\n    self.evaluate(d.add(3))\n    self.assertAllEqual([1, 2, 3], self.evaluate(d.data))\n\n    self.evaluate(d.clear())\n    self.assertAllEqual([], self.evaluate(d.data))\n\n    self.evaluate(d.add(4))\n    self.evaluate(d.add(5))\n    self.assertAllEqual([4, 5], self.evaluate(d.data))\n\n  def test_mean_not_full(self):\n    d = tf_metrics.TFDeque(3, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add(2))\n    self.evaluate(d.add(4))\n    self.assertEqual(3, self.evaluate(d.mean()))\n    self.assertEqual(tf.int32, d.mean().dtype)\n\n  def test_mean_empty(self):\n    d = tf_metrics.TFDeque(3, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.assertEqual(0, self.evaluate(d.mean()))\n    self.assertEqual(tf.int32, d.mean().dtype)\n\n  def test_mean_roll_over(self):\n    d = tf_metrics.TFDeque(3, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add(1))\n    self.evaluate(d.add(2))\n    self.evaluate(d.add(3))\n    self.evaluate(d.add(4))\n    self.assertEqual(3.0, self.evaluate(d.mean()))\n    self.assertEqual(tf.float32, d.mean().dtype)\n\n  def test_extend(self):\n    d = tf_metrics.TFDeque(3, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.extend([1, 2, 3, 4]))\n    self.assertEqual(3.0, self.evaluate(d.mean()))\n    self.assertEqual(tf.float32, d.mean().dtype)\n\n\nclass TFShapedDequeTest(tf.test.TestCase):\n\n  def test_data_is_zero(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    values = self.evaluate(d.data)\n    self.assertAllEqual((0, 2), values.shape)\n\n  def test_rolls_over(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add([1, 1]))\n    self.evaluate(d.add([2, 2]))\n    self.evaluate(d.add([3, 3]))\n    self.assertAllEqual([[1, 1], [2, 2], [3, 3]], self.evaluate(d.data))\n\n    self.evaluate(d.add([4, 4]))\n    self.assertAllEqual([[4, 4], [2, 2], [3, 3]], self.evaluate(d.data))\n\n  def test_clear(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add([1, 1]))\n\n    self.evaluate(d.clear())\n    self.assertAllEqual((0, 2), self.evaluate(d.data).shape)\n\n    self.evaluate(d.add([2, 2]))\n    self.evaluate(d.add([3, 3]))\n    self.assertAllEqual([[2, 2], [3, 3]], self.evaluate(d.data))\n\n  def test_mean_full(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add([1, 1]))\n    self.evaluate(d.add([2, 2]))\n    self.evaluate(d.add([3, 3]))\n    self.assertAllEqual([2, 2], self.evaluate(d.mean()))\n    self.assertEqual(tf.int32, d.mean().dtype)\n\n  def test_mean_not_full(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add([1, 1]))\n    self.evaluate(d.add([3, 3]))\n    self.assertAllEqual([2, 2], self.evaluate(d.mean()))\n    self.assertEqual(tf.int32, d.mean().dtype)\n\n  def test_mean_empty(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.assertAllEqual([0, 0], self.evaluate(d.mean()))\n    self.assertEqual(tf.int32, d.mean().dtype)\n\n  def test_mean_roll_over(self):\n    d = tf_metrics.TFDeque(3, tf.float32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.add([1, 1]))\n    self.evaluate(d.add([2, 2]))\n    self.evaluate(d.add([3, 3]))\n    self.evaluate(d.add([4, 4]))\n    self.assertAllEqual([3.0, 3.0], self.evaluate(d.mean()))\n    self.assertEqual(tf.float32, d.mean().dtype)\n\n  def test_extend(self):\n    d = tf_metrics.TFDeque(3, tf.int32, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.evaluate(d.extend([[1, 1], [2, 2], [3, 3], [4, 4]]))\n    self.assertAllEqual([[4, 4], [2, 2], [3, 3]], self.evaluate(d.data))\n    self.assertAllEqual([3, 3], self.evaluate(d.mean()))\n\n\nclass TFMetricsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _create_trajectories(self):\n\n    def _concat_nested_tensors(nest1, nest2):\n      return tf.nest.map_structure(lambda t1, t2: tf.concat([t1, t2], axis=0),\n                                   nest1, nest2)\n\n    # Order of args for trajectory methods:\n    # observation, action, policy_info, reward, discount\n    ts0 = _concat_nested_tensors(\n        trajectory.boundary((), tf.constant([1]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]),\n        trajectory.boundary((), tf.constant([2]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]))\n    ts1 = _concat_nested_tensors(\n        trajectory.first((), tf.constant([2]), (),\n                         tf.constant([1.], dtype=tf.float32), [1.]),\n        trajectory.first((), tf.constant([1]), (),\n                         tf.constant([2.], dtype=tf.float32), [1.]))\n    ts2 = _concat_nested_tensors(\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([3.], dtype=tf.float32), [1.]),\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([4.], dtype=tf.float32), [1.]))\n    ts3 = _concat_nested_tensors(\n        trajectory.boundary((), tf.constant([2]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]),\n        trajectory.boundary((), tf.constant([0]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]))\n    ts4 = _concat_nested_tensors(\n        trajectory.first((), tf.constant([1]), (),\n                         tf.constant([5.], dtype=tf.float32), [1.]),\n        trajectory.first((), tf.constant([1]), (),\n                         tf.constant([6.], dtype=tf.float32), [1.]))\n    ts5 = _concat_nested_tensors(\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([7.], dtype=tf.float32), [1.]),\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([8.], dtype=tf.float32), [1.]))\n\n    return [ts0, ts1, ts2, ts3, ts4, ts5]\n\n  def _create_misaligned_trajectories(self):\n\n    def _concat_nested_tensors(nest1, nest2):\n      return tf.nest.map_structure(lambda t1, t2: tf.concat([t1, t2], axis=0),\n                                   nest1, nest2)\n\n    # Order of args for trajectory methods:\n    # observation, action, policy_info, reward, discount\n    ts1 = _concat_nested_tensors(\n        trajectory.first((), tf.constant([2]), (),\n                         tf.constant([1.], dtype=tf.float32), [1.]),\n        trajectory.boundary((), tf.constant([1]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]))\n    ts2 = _concat_nested_tensors(\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([3.], dtype=tf.float32), [1.]),\n        trajectory.first((), tf.constant([1]), (),\n                         tf.constant([2.], dtype=tf.float32), [1.]))\n    ts3 = _concat_nested_tensors(\n        trajectory.boundary((), tf.constant([2]), (),\n                            tf.constant([0.], dtype=tf.float32), [1.]),\n        trajectory.last((), tf.constant([1]), (),\n                        tf.constant([4.], dtype=tf.float32), [1.]))\n\n    return [ts1, ts2, ts3]\n\n  @parameterized.named_parameters([\n      (\'testEnvironmentStepsGraph\', context.graph_mode,\n       tf_metrics.EnvironmentSteps, 5, 6),\n      (\'testNumberOfEpisodesGraph\', context.graph_mode,\n       tf_metrics.NumberOfEpisodes, 4, 2),\n      (\'testAverageReturnGraph\', context.graph_mode,\n       tf_metrics.AverageReturnMetric, 6, 9.0),\n      (\'testAverageEpisodeLengthGraph\', context.graph_mode,\n       tf_metrics.AverageEpisodeLengthMetric, 6, 2.0),\n      (\'testEnvironmentStepsEager\', context.eager_mode,\n       tf_metrics.EnvironmentSteps, 5, 6),\n      (\'testNumberOfEpisodesEager\', context.eager_mode,\n       tf_metrics.NumberOfEpisodes, 4, 2),\n      (\'testAverageReturnEager\', context.eager_mode,\n       tf_metrics.AverageReturnMetric, 6, 9.0),\n      (\'testAverageEpisodeLengthEager\', context.eager_mode,\n       tf_metrics.AverageEpisodeLengthMetric, 6, 2.0),\n  ])\n  def testMetric(self, run_mode, metric_class, num_trajectories,\n                 expected_result):\n    with run_mode():\n      trajectories = self._create_trajectories()\n      if metric_class in [tf_metrics.AverageReturnMetric,\n                          tf_metrics.AverageEpisodeLengthMetric]:\n        metric = metric_class(batch_size=2)\n      else:\n        metric = metric_class()\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      self.evaluate(metric.init_variables())\n      for i in range(num_trajectories):\n        self.evaluate(metric(trajectories[i]))\n\n      self.assertEqual(expected_result, self.evaluate(metric.result()))\n      self.evaluate(metric.reset())\n      self.assertEqual(0.0, self.evaluate(metric.result()))\n\n  @parameterized.named_parameters([\n      (\'testActionRelativeFreqGraph\', context.graph_mode),\n      (\'testActionRelativeFreqEager\', context.eager_mode),\n  ])\n  def testChosenActionHistogram(self, run_mode):\n    with run_mode():\n      trajectories = self._create_trajectories()\n      num_trajectories = 5\n      expected_result = [1, 2, 2, 1, 1, 1, 2, 0, 1, 1]\n      metric = tf_metrics.ChosenActionHistogram(buffer_size=10)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      self.evaluate(metric.init_variables())\n      for i in range(num_trajectories):\n        self.evaluate(metric(trajectories[i]))\n\n      self.assertAllEqual(expected_result, self.evaluate(metric.result()))\n      self.evaluate(metric.reset())\n      self.assertEmpty(self.evaluate(metric.result()))\n\n  @parameterized.named_parameters([\n      (\'testAverageReturnMultiMetricGraph\', context.graph_mode, 6,\n       tensor_spec.TensorSpec((2,), tf.float32, \'r\'), [9.0, 9.0]),\n      (\'testAverageReturnMultiMetricEager\', context.eager_mode, 6,\n       tensor_spec.TensorSpec((2,), tf.float32, \'r\'), [9.0, 9.0]),\n      (\'testAverageReturnMultiMetricRewardSpecListGraph\', context.graph_mode, 6,\n       [tensor_spec.TensorSpec((), tf.float32, \'r1\'),\n        tensor_spec.TensorSpec((), tf.float32, \'r2\')], [9.0, 9.0]),\n      (\'testAverageReturnMultiMetricRewardSpecListEager\', context.eager_mode, 6,\n       [tensor_spec.TensorSpec((), tf.float32, \'r1\'),\n        tensor_spec.TensorSpec((), tf.float32, \'r2\')], [9.0, 9.0])\n  ])\n  def testAverageReturnMultiMetric(self, run_mode, num_trajectories,\n                                   reward_spec, expected_result):\n    with run_mode():\n      trajectories = self._create_trajectories()\n      multi_trajectories = []\n      for traj in trajectories:\n        if isinstance(reward_spec, list):\n          new_reward = [traj.reward, traj.reward]\n        else:\n          new_reward = tf.stack([traj.reward, traj.reward], axis=1)\n        new_traj = trajectory.Trajectory(\n            step_type=traj.step_type,\n            observation=traj.observation,\n            action=traj.action,\n            policy_info=traj.policy_info,\n            next_step_type=traj.next_step_type,\n            reward=new_reward,\n            discount=traj.discount)\n        multi_trajectories.append(new_traj)\n\n      metric = tf_metrics.AverageReturnMultiMetric(reward_spec, batch_size=2)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      self.evaluate(metric.init_variables())\n      for i in range(num_trajectories):\n        self.evaluate(metric(multi_trajectories[i]))\n\n      self.assertAllEqual(expected_result, self.evaluate(metric.result()))\n      self.evaluate(metric.reset())\n      self.assertAllEqual([0.0, 0.0], self.evaluate(metric.result()))\n\n  @parameterized.named_parameters([\n      (\'testAverageReturnMultiMetricTimeMisalignedGraph\', context.graph_mode, 3,\n       tensor_spec.TensorSpec((2,), tf.float32, \'r\'), [5.0, 5.0]),\n      (\'testAverageReturnMultiMetricTimeMisalignedEager\', context.eager_mode, 3,\n       tensor_spec.TensorSpec((2,), tf.float32, \'r\'), [5.0, 5.0]),\n      (\'testAverageReturnMultiMetricRewardSpecListTimeMisalignedGraph\',\n       context.graph_mode, 3,\n       [tensor_spec.TensorSpec((), tf.float32, \'r1\'),\n        tensor_spec.TensorSpec((), tf.float32, \'r2\')], [5.0, 5.0]),\n      (\'testAverageReturnMultiMetricRewardSpecListTimeMisalignedEager\',\n       context.eager_mode, 3,\n       [tensor_spec.TensorSpec((), tf.float32, \'r1\'),\n        tensor_spec.TensorSpec((), tf.float32, \'r2\')], [5.0, 5.0])\n  ])\n  def testAverageReturnMultiMetricTimeMisalignment(\n      self, run_mode, num_trajectories, reward_spec, expected_result):\n    with run_mode():\n      trajectories = self._create_misaligned_trajectories()\n      multi_trajectories = []\n      for traj in trajectories:\n        if isinstance(reward_spec, list):\n          new_reward = [traj.reward, traj.reward]\n        else:\n          new_reward = tf.stack([traj.reward, traj.reward], axis=1)\n        new_traj = trajectory.Trajectory(\n            step_type=traj.step_type,\n            observation=traj.observation,\n            action=traj.action,\n            policy_info=traj.policy_info,\n            next_step_type=traj.next_step_type,\n            reward=new_reward,\n            discount=traj.discount)\n        multi_trajectories.append(new_traj)\n\n      metric = tf_metrics.AverageReturnMultiMetric(reward_spec, batch_size=2)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      self.evaluate(metric.init_variables())\n      for i in range(num_trajectories):\n        self.evaluate(metric(multi_trajectories[i]))\n\n      self.assertAllEqual(expected_result, self.evaluate(metric.result()))\n      self.evaluate(metric.reset())\n      self.assertAllEqual([0.0, 0.0], self.evaluate(metric.result()))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/metrics/tf_py_metric.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Wraps a python metric as a TF metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport threading\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.metrics import tf_metric\n\n\n@contextlib.contextmanager\ndef _check_not_called_concurrently(lock):\n  """"""Checks the returned context is not executed concurrently with any other.""""""\n  if not lock.acquire(False):  # Non-blocking.\n    raise RuntimeError(\n        \'Detected concurrent execution of TFPyMetric ops.\')\n  try:\n    yield\n  finally:\n    lock.release()\n\n\nclass TFPyMetric(tf_metric.TFStepMetric):\n  """"""Wraps a python metric as a TF metric.""""""\n\n  def __init__(self, py_metric, name=None, dtype=tf.float32):\n    """"""Creates a TF metric given a py metric to wrap.\n\n    Args:\n      py_metric: A batched python metric to wrap.\n      name: Name of the metric.\n      dtype: Data type of the metric.\n    """"""\n    name = name or py_metric.name\n    super(TFPyMetric, self).__init__(name=name)\n    self._py_metric = py_metric\n    self._dtype = dtype\n    self._lock = threading.Lock()\n\n  def call(self, trajectory):\n    """"""Update the value of the metric using trajectory.\n\n    The trajectory can be either batched or un-batched depending on\n    the expected inputs for the py_metric being wrapped.\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory.\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    def _call(*flattened_trajectories):\n      with _check_not_called_concurrently(self._lock):\n        flat_sequence = [x.numpy() for x in flattened_trajectories]\n        packed_trajectories = tf.nest.pack_sequence_as(\n            structure=(trajectory), flat_sequence=flat_sequence)\n        return self._py_metric(packed_trajectories)\n\n    flattened_trajectories = tf.nest.flatten(trajectory)\n    metric_op = tf.py_function(\n        _call,\n        flattened_trajectories,\n        [],\n        name=\'metric_call_py_func\')\n\n    with tf.control_dependencies([metric_op]):\n      return tf.nest.map_structure(tf.identity, trajectory)\n\n  def result(self):\n    def _result():\n      with _check_not_called_concurrently(self._lock):\n        return self._py_metric.result()\n\n    result_value = tf.py_function(\n        _result,\n        [],\n        self._dtype,\n        name=\'metric_result_py_func\')\n    if not tf.executing_eagerly():\n      result_value.set_shape(())\n    return result_value\n\n  def reset(self):\n    def _reset():\n      with _check_not_called_concurrently(self._lock):\n        return self._py_metric.reset()\n\n    return tf.py_function(\n        _reset, [], [],\n        name=\'metric_reset_py_func\')\n'"
tf_agents/metrics/tf_py_metric_test.py,11,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.metrics.tf_py_metric.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.metrics import batched_py_metric\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_py_metric\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import nest_utils\n\n\nclass BatchedPyMetricTest(parameterized.TestCase, tf.test.TestCase):\n\n  def setUp(self):\n    super(BatchedPyMetricTest, self).setUp()\n    # Order of args for trajectory methods:\n    # observation, action, policy_info, reward, discount\n    ts0 = nest_utils.stack_nested_tensors([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)\n    ])\n    ts1 = nest_utils.stack_nested_tensors([\n        trajectory.first((), (), (), 1., 1.),\n        trajectory.first((), (), (), 2., 1.)\n    ])\n    ts2 = nest_utils.stack_nested_tensors([\n        trajectory.last((), (), (), 3., 1.),\n        trajectory.last((), (), (), 4., 1.)\n    ])\n    ts3 = nest_utils.stack_nested_tensors([\n        trajectory.boundary((), (), (), 0., 1.),\n        trajectory.boundary((), (), (), 0., 1.)\n    ])\n    ts4 = nest_utils.stack_nested_tensors([\n        trajectory.first((), (), (), 5., 1.),\n        trajectory.first((), (), (), 6., 1.)\n    ])\n    ts5 = nest_utils.stack_nested_tensors([\n        trajectory.last((), (), (), 7., 1.),\n        trajectory.last((), (), (), 8., 1.)\n    ])\n\n    self._ts = [ts0, ts1, ts2, ts3, ts4, ts5]\n\n  @parameterized.named_parameters(\n      [(\'testMetricIsComputedCorrectlyNoSteps\', 0, 0),\n       (\'testMetricIsComputedCorrectlyPartialEpisode\', 2, 0),\n       (\'testMetricIsComputedCorrectlyOneEpisode\', 3, 5),\n       (\'testMetricIsComputedCorrectlyOneAndPartialEpisode\', 5, 5),\n       (\'testMetricIsComputedCorrectlyTwoEpisodes\', 6, 9),\n      ])\n  def testMetricIsComputedCorrectly(self, num_time_steps, expected_reward):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    tf_avg_return_metric = tf_py_metric.TFPyMetric(batched_avg_return_metric)\n    deps = []\n    for i in range(num_time_steps):\n      with tf.control_dependencies(deps):\n        traj = tf_avg_return_metric(self._ts[i])\n        deps = tf.nest.flatten(traj)\n    with tf.control_dependencies(deps):\n      result = tf_avg_return_metric.result()\n    result_ = self.evaluate(result)\n    self.assertEqual(result_, expected_reward)\n\n  def testReset(self):\n    batched_avg_return_metric = batched_py_metric.BatchedPyMetric(\n        py_metrics.AverageReturnMetric)\n    tf_avg_return_metric = tf_py_metric.TFPyMetric(batched_avg_return_metric)\n\n    deps = []\n    # run one episode\n    for i in range(3):\n      with tf.control_dependencies(deps):\n        traj = tf_avg_return_metric(self._ts[i])\n        deps = tf.nest.flatten(traj)\n\n    # reset\n    with tf.control_dependencies(deps):\n      reset_op = tf_avg_return_metric.reset()\n      deps = [reset_op]\n\n    # run second episode\n    for i in range(3, 6):\n      with tf.control_dependencies(deps):\n        traj = tf_avg_return_metric(self._ts[i])\n        deps = tf.nest.flatten(traj)\n\n    # Test result is the reward for the second episode.\n    with tf.control_dependencies(deps):\n      result = tf_avg_return_metric.result()\n\n    result_ = self.evaluate(result)\n    self.assertEqual(result_, 13)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Networks Module.""""""\n\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import categorical_projection_network\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import expand_dims_layer\nfrom tf_agents.networks import lstm_encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import normal_projection_network\nfrom tf_agents.networks import q_network\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.networks import utils\nfrom tf_agents.networks import value_network\nfrom tf_agents.networks import value_rnn_network\n'"
tf_agents/networks/actor_distribution_network.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Keras actor network that generates distributions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import categorical_projection_network\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import normal_projection_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import nest_utils\n\n\ndef _categorical_projection_net(action_spec, logits_init_output_factor=0.1):\n  return categorical_projection_network.CategoricalProjectionNetwork(\n      action_spec, logits_init_output_factor=logits_init_output_factor)\n\n\ndef _normal_projection_net(action_spec,\n                           init_action_stddev=0.35,\n                           init_means_output_factor=0.1):\n  std_bias_initializer_value = np.log(np.exp(init_action_stddev) - 1)\n\n  return normal_projection_network.NormalProjectionNetwork(\n      action_spec,\n      init_means_output_factor=init_means_output_factor,\n      std_bias_initializer_value=std_bias_initializer_value,\n      scale_distribution=False)\n\n\n@gin.configurable\nclass ActorDistributionNetwork(network.DistributionNetwork):\n  """"""Creates an actor producing either Normal or Categorical distribution.\n\n  Note: By default, this network uses `NormalProjectionNetwork` for continuous\n  projection which by default uses `tanh_squash_to_spec` to normalize its\n  output. Due to the nature of the `tanh` function, values near the spec bounds\n  cannot be returned.\n  """"""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=(200, 100),\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               batch_squash=True,\n               dtype=tf.float32,\n               discrete_projection_net=_categorical_projection_net,\n               continuous_projection_net=_normal_projection_net,\n               name=\'ActorDistributionNetwork\'):\n    """"""Creates an instance of `ActorDistributionNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input.\n      output_tensor_spec: A nest of `tensor_spec.BoundedTensorSpec` representing\n        the output.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, each item\n        is the fraction of input units to drop or a dictionary of parameters\n        according to the keras.Dropout documentation. The additional parameter\n        `permanent\', if set to True, allows to apply dropout at inference for\n        approximated Bayesian inference. The dropout layers are interleaved with\n        the fully connected layers; there is a dropout layer after each fully\n        connected layer, except if the entry in the list is None. This list must\n        have the same length of fc_layer_params, or be None.\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      kernel_initializer: Initializer to use for the kernels of the conv and\n        dense layers. If none is provided a default glorot_uniform\n      batch_squash: If True the outer_ranks of the observation are squashed into\n        the batch dimension. This allow encoding networks to be used with\n        observations with shape [BxTx...].\n      dtype: The dtype to use by the convolution and fully connected layers.\n      discrete_projection_net: Callable that generates a discrete projection\n        network to be called with some hidden state and the outer_rank of the\n        state.\n      continuous_projection_net: Callable that generates a continuous projection\n        network to be called with some hidden state and the outer_rank of the\n        state.\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If `input_tensor_spec` contains more than one observation.\n    """"""\n\n    if not kernel_initializer:\n      kernel_initializer = tf.compat.v1.keras.initializers.glorot_uniform()\n\n    encoder = encoding_network.EncodingNetwork(\n        input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=fc_layer_params,\n        dropout_layer_params=dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        batch_squash=batch_squash,\n        dtype=dtype)\n\n    def map_proj(spec):\n      if tensor_spec.is_discrete(spec):\n        return discrete_projection_net(spec)\n      else:\n        return continuous_projection_net(spec)\n\n    projection_networks = tf.nest.map_structure(map_proj, output_tensor_spec)\n    output_spec = tf.nest.map_structure(lambda proj_net: proj_net.output_spec,\n                                        projection_networks)\n\n    super(ActorDistributionNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        output_spec=output_spec,\n        name=name)\n\n    self._encoder = encoder\n    self._projection_networks = projection_networks\n    self._output_tensor_spec = output_tensor_spec\n\n  @property\n  def output_tensor_spec(self):\n    return self._output_tensor_spec\n\n  def call(self,\n           observations,\n           step_type,\n           network_state,\n           training=False,\n           mask=None):\n    state, network_state = self._encoder(\n        observations,\n        step_type=step_type,\n        network_state=network_state,\n        training=training)\n    outer_rank = nest_utils.get_outer_rank(observations, self.input_tensor_spec)\n\n    def call_projection_net(proj_net):\n      distribution, _ = proj_net(\n          state, outer_rank, training=training, mask=mask)\n      return distribution\n\n    output_actions = tf.nest.map_structure(\n        call_projection_net, self._projection_networks)\n    return output_actions, network_state\n'"
tf_agents/networks/actor_distribution_network_test.py,26,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.actor_distribution_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\nclass ActorDistributionNetworkTest(tf.test.TestCase, parameterized.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testBuilds(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        fc_layer_params=(5,))\n\n    action_distributions, _ = net(time_step.observation, time_step.step_type,\n                                  ())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1, 2], action_distributions[0].mode().shape.as_list())\n    self.assertEqual([1, 3], action_distributions[1].mode().shape.as_list())\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testBuildsScalarContinuousActionSpace(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.float32, 2, 3)\n\n    net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec\n    )\n\n    action_distributions, _ = net(time_step.observation, time_step.step_type,\n                                  ())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1], action_distributions.mode().shape.as_list())\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testHandlesExtraOuterDims(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(\n        time_step_spec, outer_dims=(3, 2, 2))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        fc_layer_params=(5,))\n\n    action_distributions, _ = net(time_step.observation, time_step.step_type,\n                                  ())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([3, 2, 2, 2],\n                     action_distributions[0].mode().shape.as_list())\n    self.assertEqual([3, 2, 2, 3],\n                     action_distributions[1].mode().shape.as_list())\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testHandlePreprocessingLayers(self):\n    observation_spec = (tensor_spec.TensorSpec([1], tf.float32),\n                        tensor_spec.TensorSpec([], tf.float32))\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(3,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    preprocessing_layers = (tf.keras.layers.Dense(4),\n                            tf.keras.Sequential([\n                                tf.keras.layers.Reshape((1,)),\n                                tf.keras.layers.Dense(4)\n                            ]))\n\n    net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add())\n\n    action_distributions, _ = net(time_step.observation, time_step.step_type,\n                                  ())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([3, 2], action_distributions[0].mode().shape.as_list())\n    self.assertEqual([3, 3], action_distributions[1].mode().shape.as_list())\n    self.assertGreater(len(net.trainable_variables), 4)\n\n  @parameterized.named_parameters(\n      (\'TrainingTrue\', True,),\n      (\'TrainingFalse\', False))\n  def testDropoutFCLayersWithConv(self, training):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n    action_spec = tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3)\n\n    net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        fc_layer_params=[5],\n        dropout_layer_params=[0.5])\n\n    modes = []\n    num_modes = 10\n    for _ in range(num_modes):\n      action_distributions, _ = net(\n          time_step.observation, time_step.step_type, (), training=training)\n      modes.append(action_distributions.mode())\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    modes = self.evaluate(modes)\n\n    # Verify that the modes from action_distributions are not all the same.\n    any_modes_differ = False\n    for i in range(num_modes):\n      for j in range(i+1, num_modes):\n        any_modes_differ = np.linalg.norm(modes[i] - modes[j]) > 1e-6\n        if any_modes_differ:\n          self.assertEqual(training, any_modes_differ)\n          return\n\n    self.assertEqual(training, any_modes_differ)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/actor_distribution_rnn_network.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Keras actor network  with LSTM cells that generates distributions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import categorical_projection_network\nfrom tf_agents.networks import lstm_encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import normal_projection_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import nest_utils\n\n\ndef _categorical_projection_net(action_spec, logits_init_output_factor=0.1):\n  return categorical_projection_network.CategoricalProjectionNetwork(\n      action_spec, logits_init_output_factor=logits_init_output_factor)\n\n\ndef _normal_projection_net(action_spec,\n                           init_action_stddev=0.35,\n                           init_means_output_factor=0.1):\n  std_bias_initializer_value = np.log(np.exp(init_action_stddev) - 1)\n\n  return normal_projection_network.NormalProjectionNetwork(\n      action_spec,\n      init_means_output_factor=init_means_output_factor,\n      std_bias_initializer_value=std_bias_initializer_value)\n\n\n@gin.configurable\nclass ActorDistributionRnnNetwork(network.DistributionNetwork):\n  """"""Creates an actor producing either Normal or Categorical distribution.\n\n  Note: By default, this network uses `NormalProjectionNetwork` for continuous\n  projection which by default uses `tanh_squash_to_spec` to normalize its\n  output. Due to the nature of the `tanh` function, values near the spec bounds\n  cannot be returned.\n  """"""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               input_fc_layer_params=(200, 100),\n               input_dropout_layer_params=None,\n               lstm_size=None,\n               output_fc_layer_params=(200, 100),\n               activation_fn=tf.keras.activations.relu,\n               dtype=tf.float32,\n               discrete_projection_net=_categorical_projection_net,\n               continuous_projection_net=_normal_projection_net,\n               rnn_construction_fn=None,\n               rnn_construction_kwargs={},\n               name=\'ActorDistributionRnnNetwork\'):\n    """"""Creates an instance of `ActorDistributionRnnNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input.\n      output_tensor_spec: A nest of `tensor_spec.BoundedTensorSpec` representing\n        the output.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      input_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied before\n        the LSTM cell.\n      input_dropout_layer_params: Optional list of dropout layer parameters,\n        each item is the fraction of input units to drop or a dictionary of\n        parameters according to the keras.Dropout documentation. The additional\n        parameter `permanent\', if set to True, allows to apply dropout at\n        inference for approximated Bayesian inference. The dropout layers are\n        interleaved with the fully connected layers; there is a dropout layer\n        after each fully connected layer, except if the entry in the list is\n        None. This list must have the same length of input_fc_layer_params, or\n        be None.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied after the\n        LSTM cell.\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      dtype: The dtype to use by the convolution and fully connected layers.\n      discrete_projection_net: Callable that generates a discrete projection\n        network to be called with some hidden state and the outer_rank of the\n        state.\n      continuous_projection_net: Callable that generates a continuous projection\n        network to be called with some hidden state and the outer_rank of the\n        state.\n      rnn_construction_fn: (Optional.) Alternate RNN construction function, e.g.\n        tf.keras.layers.LSTM, tf.keras.layers.CuDNNLSTM. It is invalid to\n        provide both rnn_construction_fn and lstm_size.\n      rnn_construction_kwargs: (Optional.) Dictionary or arguments to pass to\n        rnn_construction_fn.\n\n        The RNN will be constructed via:\n\n        ```\n        rnn_layer = rnn_construction_fn(**rnn_construction_kwargs)\n        ```\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If \'input_dropout_layer_params\' is not None.\n    """"""\n    if input_dropout_layer_params:\n      raise ValueError(\'Dropout layer is not supported.\')\n\n    lstm_encoder = lstm_encoding_network.LSTMEncodingNetwork(\n        input_tensor_spec=input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        input_fc_layer_params=input_fc_layer_params,\n        lstm_size=lstm_size,\n        output_fc_layer_params=output_fc_layer_params,\n        activation_fn=activation_fn,\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs=rnn_construction_kwargs,\n        dtype=dtype,\n        name=name)\n\n    def map_proj(spec):\n      if tensor_spec.is_discrete(spec):\n        return discrete_projection_net(spec)\n      else:\n        return continuous_projection_net(spec)\n\n    projection_networks = tf.nest.map_structure(map_proj, output_tensor_spec)\n    output_spec = tf.nest.map_structure(lambda proj_net: proj_net.output_spec,\n                                        projection_networks)\n\n    super(ActorDistributionRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=lstm_encoder.state_spec,\n        output_spec=output_spec,\n        name=name)\n\n    self._lstm_encoder = lstm_encoder\n    self._projection_networks = projection_networks\n    self._output_tensor_spec = output_tensor_spec\n\n  @property\n  def output_tensor_spec(self):\n    return self._output_tensor_spec\n\n  def call(self, observation, step_type, network_state=(), training=False):\n    state, network_state = self._lstm_encoder(\n        observation, step_type=step_type, network_state=network_state,\n        training=training)\n    outer_rank = nest_utils.get_outer_rank(observation, self.input_tensor_spec)\n    output_actions = tf.nest.map_structure(\n        lambda proj_net: proj_net(state, outer_rank, training=training)[0],\n        self._projection_networks)\n    return output_actions, network_state\n'"
tf_agents/networks/actor_distribution_rnn_network_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.actor_distribution_rnn_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.keras_layers import sequential_layer\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\n\n\ndef lstm_keras_fn(lstm_size):\n  return tf.keras.layers.LSTM(lstm_size, return_state=True,\n                              return_sequences=True)\n\n\ndef rnn_keras_fn(lstm_size):\n  cell = tf.keras.layers.SimpleRNNCell(lstm_size)\n  return tf.keras.layers.RNN(cell, return_state=True,\n                             return_sequences=True)\n\n\nclass ActorDistributionNetworkTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      (\'RNNKerasUnroll\', None, rnn_keras_fn),\n  )\n  def testBuildsRnn(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        output_fc_layer_params=(5,),\n        lstm_size=lstm_size,\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    action_distributions, network_state = net(\n        time_step.observation, time_step.step_type,\n        net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1, 2], action_distributions[0].mode().shape.as_list())\n    self.assertEqual([1, 3], action_distributions[1].mode().shape.as_list())\n\n    self.assertLen(net.variables, 14)\n    # Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Fc Kernel\n    self.assertEqual((64, 5), net.variables[2].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[3].shape)\n    # RNN Cell Kernel\n    self.assertEqual((5, 3), net.variables[4].shape)\n    # RNN Cell Recurrent Kernel\n    self.assertEqual((3, 3), net.variables[5].shape)\n    # RNN Cell Bias\n    self.assertEqual((3,), net.variables[6].shape)\n    # Fc Kernel\n    self.assertEqual((3, 5), net.variables[7].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[8].shape)\n    # Normal Projection Kernel\n    self.assertEqual((5, 2), net.variables[9].shape)\n    # Normal Projection Bias\n    self.assertEqual((2,), net.variables[10].shape)\n    # Normal Projection STD Bias layer\n    self.assertEqual((2,), net.variables[11].shape)\n    # Categorical Projection Kernel\n    self.assertEqual((5, 12), net.variables[12].shape)\n    # Categorical Projection Bias\n    self.assertEqual((12,), net.variables[13].shape)\n\n    # Assert RNN cell is created.\n    self.assertEqual((3,), network_state[0].shape)\n\n  @parameterized.named_parameters(\n      (\'DynamicUnroll\', (3,), None),\n      (\'LSTMKerasUnroll\', None, lstm_keras_fn),\n  )\n  def testBuilds(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        output_fc_layer_params=(5,),\n        lstm_size=lstm_size,\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    action_distributions, network_state = net(\n        time_step.observation, time_step.step_type,\n        net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1, 2], action_distributions[0].mode().shape.as_list())\n    self.assertEqual([1, 3], action_distributions[1].mode().shape.as_list())\n\n    self.assertLen(net.variables, 14)\n    # Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Fc Kernel\n    self.assertEqual((64, 5), net.variables[2].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[3].shape)\n    # LSTM Cell Kernel\n    self.assertEqual((5, 12), net.variables[4].shape)\n    # LSTM Cell Recurrent Kernel\n    self.assertEqual((3, 12), net.variables[5].shape)\n    # LSTM Cell Bias\n    self.assertEqual((12,), net.variables[6].shape)\n    # Fc Kernel\n    self.assertEqual((3, 5), net.variables[7].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[8].shape)\n    # Normal Projection Kernel\n    self.assertEqual((5, 2), net.variables[9].shape)\n    # Normal Projection Bias\n    self.assertEqual((2,), net.variables[10].shape)\n    # Normal Projection STD Bias layer\n    self.assertEqual((2,), net.variables[11].shape)\n    # Categorical Projection Kernel\n    self.assertEqual((5, 12), net.variables[12].shape)\n    # Categorical Projection Bias\n    self.assertEqual((12,), net.variables[13].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 3), network_state[0].shape)\n    self.assertEqual((1, 3), network_state[1].shape)\n\n  @parameterized.named_parameters(\n      (\'DynamicUnroll\', (3,), None),\n      (\'KerasUnroll\', None, rnn_keras_fn),\n  )\n  def testRunsWithLstmStack(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1, 5))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        output_fc_layer_params=(5,),\n        lstm_size=lstm_size,\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    initial_state = actor_policy.ActorPolicy(time_step_spec, action_spec,\n                                             net).get_initial_state(1)\n    net_call = net(time_step.observation, time_step.step_type, initial_state)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.nest.map_structure(lambda d: d.sample(), net_call[0]))\n\n  @parameterized.named_parameters(\n      (\'DynamicUnroll\', (3,), None),\n      (\'KerasUnroll\', None, rnn_keras_fn),\n  )\n  def testHandlePreprocessingLayers(self, lstm_size, rnn_construction_fn):\n    observation_spec = (tensor_spec.TensorSpec([1], tf.float32),\n                        tensor_spec.TensorSpec([], tf.float32))\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(3, 4))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.int32, 0, 3)\n    ]\n\n    preprocessing_layers = (tf.keras.layers.Dense(4),\n                            sequential_layer.SequentialLayer([\n                                tf.keras.layers.Reshape((1,)),\n                                tf.keras.layers.Dense(4)\n                            ]))\n\n    net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        observation_spec,\n        action_spec,\n        preprocessing_layers=preprocessing_layers,\n        lstm_size=lstm_size,\n        preprocessing_combiner=tf.keras.layers.Add(),\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    initial_state = actor_policy.ActorPolicy(time_step_spec, action_spec,\n                                             net).get_initial_state(3)\n    action_distributions, _ = net(time_step.observation, time_step.step_type,\n                                  initial_state)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([3, 4, 2], action_distributions[0].mode().shape.as_list())\n    self.assertEqual([3, 4, 3], action_distributions[1].mode().shape.as_list())\n    self.assertGreater(len(net.trainable_variables), 4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/categorical_projection_network.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Project inputs to a categorical distribution object.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\n\n\n@gin.configurable\nclass CategoricalProjectionNetwork(network.DistributionNetwork):\n  """"""Generates a tfp.distribution.Categorical by predicting logits.""""""\n\n  def __init__(self,\n               sample_spec,\n               logits_init_output_factor=0.1,\n               name=\'CategoricalProjectionNetwork\'):\n    """"""Creates an instance of CategoricalProjectionNetwork.\n\n    Args:\n      sample_spec: A `tensor_spec.BoundedTensorSpec` detailing the shape and\n        dtypes of samples pulled from the output distribution.\n      logits_init_output_factor: Output factor for initializing kernel logits\n        weights.\n      name: A string representing name of the network.\n    """"""\n    unique_num_actions = np.unique(sample_spec.maximum - sample_spec.minimum +\n                                   1)\n    if len(unique_num_actions) > 1 or np.any(unique_num_actions <= 0):\n      raise ValueError(\'Bounds on discrete actions must be the same for all \'\n                       \'dimensions and have at least 1 action. Projection \'\n                       \'Network requires num_actions to be equal across \'\n                       \'action dimensions. Implement a more general \'\n                       \'categorical projection if you need more flexibility.\')\n\n    output_shape = sample_spec.shape.concatenate([int(unique_num_actions)])\n    output_spec = self._output_distribution_spec(output_shape, sample_spec,\n                                                 name)\n\n    super(CategoricalProjectionNetwork, self).__init__(\n        # We don\'t need these, but base class requires them.\n        input_tensor_spec=None,\n        state_spec=(),\n        output_spec=output_spec,\n        name=name)\n\n    if not tensor_spec.is_bounded(sample_spec):\n      raise ValueError(\n          \'sample_spec must be bounded. Got: %s.\' % type(sample_spec))\n\n    if not tensor_spec.is_discrete(sample_spec):\n      raise ValueError(\'sample_spec must be discrete. Got: %s.\' % sample_spec)\n\n    self._sample_spec = sample_spec\n    self._output_shape = output_shape\n\n    self._projection_layer = tf.keras.layers.Dense(\n        self._output_shape.num_elements(),\n        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n            scale=logits_init_output_factor),\n        bias_initializer=tf.keras.initializers.Zeros(),\n        name=\'logits\')\n\n  def _output_distribution_spec(self, output_shape, sample_spec, network_name):\n    input_param_spec = {\n        \'logits\':\n            tensor_spec.TensorSpec(\n                shape=output_shape,\n                dtype=tf.float32,\n                name=network_name + \'_logits\')\n    }\n\n    return distribution_spec.DistributionSpec(\n        tfp.distributions.Categorical,\n        input_param_spec,\n        sample_spec=sample_spec,\n        dtype=sample_spec.dtype)\n\n  def call(self, inputs, outer_rank, training=False, mask=None):\n    # outer_rank is needed because the projection is not done on the raw\n    # observations so getting the outer rank is hard as there is no spec to\n    # compare to.\n    batch_squash = utils.BatchSquash(outer_rank)\n    inputs = batch_squash.flatten(inputs)\n    inputs = tf.cast(inputs, tf.float32)\n\n    logits = self._projection_layer(inputs, training=training)\n    logits = tf.reshape(logits, [-1] + self._output_shape.as_list())\n    logits = batch_squash.unflatten(logits)\n\n    if mask is not None:\n      # If the action spec says each action should be shaped (1,), add another\n      # dimension so the final shape is (B, 1, A), where A is the number of\n      # actions. This will make Categorical emit events shaped (B, 1) rather\n      # than (B,). Using axis -2 to allow for (B, T, 1, A) shaped q_values.\n      if mask.shape.rank < logits.shape.rank:\n        mask = tf.expand_dims(mask, -2)\n\n      # Overwrite the logits for invalid actions to -inf.\n      neg_inf = tf.constant(-np.inf, dtype=logits.dtype)\n      logits = tf.compat.v2.where(tf.cast(mask, tf.bool), logits, neg_inf)\n\n    return self.output_spec.build_distribution(logits=logits), ()\n'"
tf_agents/networks/categorical_projection_network_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.categorical_projection_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import categorical_projection_network\nfrom tf_agents.specs import tensor_spec\n\n\ndef _get_inputs(batch_size, num_input_dims):\n  return tf.random.uniform([batch_size, num_input_dims])\n\n\nclass CategoricalProjectionNetworkTest(tf.test.TestCase):\n\n  def testBuild(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2, 3], tf.int32, 0, 1)\n    network = categorical_projection_network.CategoricalProjectionNetwork(\n        output_spec)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    distribution, _ = network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    sample = self.evaluate(distribution.sample())\n\n    self.assertEqual(tfp.distributions.Categorical, type(distribution))\n    # Batch = 3; 2x3 action choices, 2x actions per choise.\n    self.assertEqual((3, 2, 3, 2), distribution.logits.shape)\n    self.assertAllEqual((3, 2, 3), sample.shape)\n\n  def testTrainableVariables(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.int32, 0, 1)\n    network = categorical_projection_network.CategoricalProjectionNetwork(\n        output_spec)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Dense kernel, dense bias.\n    self.assertEqual(2, len(network.trainable_variables))\n    self.assertEqual((5, 4), network.trainable_variables[0].shape)\n    self.assertEqual((4,), network.trainable_variables[1].shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/categorical_q_network.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Q-network for categorical DQN.\n\nSee ""A Distributional Perspective on Reinforcement Learning"" by Bellemare,\nDabney, and Munos (2017). https://arxiv.org/abs/1707.06887\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\nfrom tf_agents.specs import tensor_spec\n\n\n@gin.configurable\nclass CategoricalQNetwork(network.Network):\n  """"""Creates a categorical Q-network.\n\n  It can be used to take an input of batched observations and outputs\n  ([batch_size, num_actions, num_atoms], network\'s state).\n\n  The first element of the output is a batch of logits based on the distribution\n  called C51 from Bellemare et al., 2017 (https://arxiv.org/abs/1707.06887). The\n  logits are used to compute approximate probability distributions for Q-values\n  for each potential action, by computing the probabilities at the 51 points\n  (called atoms) in np.linspace(-10.0, 10.0, 51).\n  """"""\n\n  def __init__(self,\n               input_tensor_spec,\n               action_spec,\n               num_atoms=51,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=None,\n               activation_fn=tf.nn.relu,\n               name=\'CategoricalQNetwork\'):\n    """"""Creates an instance of `CategoricalQNetwork`.\n\n    The logits output by __call__ will ultimately have a shape of\n    `[batch_size, num_actions, num_atoms]`, where `num_actions` is computed as\n    `action_spec.maximum - action_spec.minimum + 1`. Each value is a logit for\n    a particular action at a particular atom (see above).\n\n    As an example, if\n    `action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 4)` and\n    `num_atoms = 51`, the logits will have a shape of `[batch_size, 5, 51]`.\n\n    Args:\n      input_tensor_spec: A `tensor_spec.TensorSpec` specifying the observation\n        spec.\n      action_spec: A `tensor_spec.BoundedTensorSpec` representing the actions.\n      num_atoms: The number of atoms to use in our approximate probability\n        distributions. Defaults to 51 to produce C51.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layer parameters for\n        observations, where each item is a length-three tuple indicating\n        (num_units, kernel_size, stride).\n      fc_layer_params: Optional list of fully connected parameters for\n        observations, where each item is the number of units in the layer.\n      activation_fn: Activation function, e.g. tf.nn.relu or tf.nn.leaky_relu.\n      name: A string representing the name of the network.\n\n    Raises:\n      TypeError: `action_spec` is not a `BoundedTensorSpec`.\n    """"""\n    super(CategoricalQNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n      raise TypeError(\'action_spec must be a BoundedTensorSpec. Got: %s\' % (\n          action_spec,))\n\n    self._num_actions = action_spec.maximum - action_spec.minimum + 1\n    self._num_atoms = num_atoms\n\n    q_network_action_spec = tensor_spec.BoundedTensorSpec(\n        (), tf.int32, minimum=0, maximum=self._num_actions * num_atoms - 1)\n\n    self._q_network = q_network.QNetwork(\n        input_tensor_spec=input_tensor_spec,\n        action_spec=q_network_action_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=fc_layer_params,\n        activation_fn=activation_fn,\n        name=name)\n\n  @property\n  def num_atoms(self):\n    return self._num_atoms\n\n  def call(self, observation, step_type=None, network_state=(), training=False):\n    """"""Runs the given observation through the network.\n\n    Args:\n      observation: The observation to provide to the network.\n      step_type: The step type for the given observation. See `StepType` in\n        time_step.py.\n      network_state: A state tuple to pass to the network, mainly used by RNNs.\n      training: Whether the output will be used for training.\n\n    Returns:\n      A tuple `(logits, network_state)`.\n    """"""\n    logits, network_state = self._q_network(\n        observation, step_type, network_state, training=training)\n    logits = tf.reshape(logits, [-1, self._num_actions, self._num_atoms])\n    return logits, network_state\n'"
tf_agents/networks/categorical_q_network_test.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.categorical_q_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import categorical_q_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass CategoricalQNetworkTest(test_utils.TestCase):\n\n  def tearDown(self):\n    gin.clear_config()\n    super(CategoricalQNetworkTest, self).tearDown()\n\n  def testBuild(self):\n    batch_size = 3\n    num_state_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    self.assertEqual(num_actions, 2)\n\n    observations_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_steps = ts.restart(observations, batch_size)\n\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=observations_spec,\n        action_spec=action_spec,\n        fc_layer_params=[3])\n\n    logits, _ = q_network(time_steps.observation)\n    self.assertAllEqual(logits.shape.as_list(),\n                        [batch_size, num_actions, q_network._num_atoms])\n\n    # There are two trainable layers here: the specified fc_layer and the final\n    # logits layer. Each layer has two trainable_variables (kernel and bias),\n    # for a total of 4.\n    self.assertLen(q_network.trainable_variables, 4)\n\n  def testChangeHiddenLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    self.assertEqual(num_actions, 2)\n\n    observations_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_steps = ts.restart(observations, batch_size)\n\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=observations_spec,\n        action_spec=action_spec,\n        fc_layer_params=[3, 3])\n\n    logits, _ = q_network(time_steps.observation)\n    self.assertAllEqual(logits.shape.as_list(),\n                        [batch_size, num_actions, q_network._num_atoms])\n\n    # This time there is an extra fc layer, for a total of 6\n    # trainable_variables.\n    self.assertLen(q_network.trainable_variables, 6)\n\n  def testAddConvLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    self.assertEqual(num_actions, 2)\n\n    observations_spec = tensor_spec.TensorSpec(\n        [3, 3, num_state_dims], tf.float32)\n    observations = tf.random.uniform([batch_size, 3, 3, num_state_dims])\n    time_steps = ts.restart(observations, batch_size)\n\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=observations_spec,\n        action_spec=action_spec,\n        conv_layer_params=[(16, 2, 1), (15, 2, 1)])\n\n    logits, _ = q_network(time_steps.observation)\n    self.assertAllEqual(logits.shape.as_list(),\n                        [batch_size, num_actions, q_network._num_atoms])\n\n    # This time there are two conv layers and one final logits layer, for a\n    # total of 6 trainable_variables.\n    self.assertLen(q_network.trainable_variables, 6)\n\n  def testCorrectOutputShape(self):\n    batch_size = 3\n    num_state_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    self.assertEqual(num_actions, 2)\n\n    observations_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_steps = ts.restart(observations, batch_size)\n\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=observations_spec,\n        action_spec=action_spec,\n        fc_layer_params=[3])\n\n    logits, _ = q_network(time_steps.observation)\n    self.assertAllEqual(logits.shape.as_list(),\n                        [batch_size, num_actions, q_network._num_atoms])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    eval_logits = self.evaluate(logits)\n    self.assertAllEqual(\n        eval_logits.shape, [batch_size, num_actions, q_network._num_atoms])\n\n  def testGinConfig(self):\n    batch_size = 3\n    num_state_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    self.assertEqual(num_actions, 2)\n\n    observations_spec = tensor_spec.TensorSpec(\n        [3, 3, num_state_dims], tf.float32)\n    observations = tf.random.uniform([batch_size, 3, 3, num_state_dims])\n    next_observations = tf.random.uniform([batch_size, 3, 3, num_state_dims])\n    time_steps = ts.restart(observations, batch_size)\n    next_time_steps = ts.restart(next_observations, batch_size)\n\n    # Note: this is cleared in tearDown().\n    gin.parse_config(""""""\n        CategoricalQNetwork.conv_layer_params = [(16, 2, 1), (15, 2, 1)]\n        CategoricalQNetwork.fc_layer_params = [4, 3, 5]\n    """""")\n\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=observations_spec,\n        action_spec=action_spec)\n\n    logits, _ = q_network(time_steps.observation)\n    next_logits, _ = q_network(next_time_steps.observation)\n    self.assertAllEqual(logits.shape.as_list(),\n                        [batch_size, num_actions, q_network.num_atoms])\n    self.assertAllEqual(next_logits.shape.as_list(),\n                        [batch_size, num_actions, q_network.num_atoms])\n\n    # This time there are six layers: two conv layers, three fc layers, and one\n    # final logits layer, for 12 trainable_variables in total.\n    self.assertLen(q_network.trainable_variables, 12)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/encoding_network.py,20,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Keras Encoding Network.\n\nImplements a network that will generate the following layers:\n\n  [optional]: preprocessing_layers  # preprocessing_layers\n  [optional]: (Add | Concat(axis=-1) | ...)  # preprocessing_combiner\n  [optional]: Conv2D # conv_layer_params\n  Flatten\n  [optional]: Dense  # fc_layer_params\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\nimport gin\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.utils import nest_utils\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\nCONV_TYPE_2D = \'2d\'\nCONV_TYPE_1D = \'1d\'\n\n\ndef _copy_layer(layer):\n  """"""Create a copy of a Keras layer with identical parameters.\n\n  The new layer will not share weights with the old one.\n\n  Args:\n    layer: An instance of `tf.keras.layers.Layer`.\n\n  Returns:\n    A new keras layer.\n\n  Raises:\n    TypeError: If `layer` is not a keras layer.\n    ValueError: If `layer` cannot be correctly cloned.\n  """"""\n  if not isinstance(layer, tf.keras.layers.Layer):\n    raise TypeError(\'layer is not a keras layer: %s\' % str(layer))\n\n  # pylint:disable=unidiomatic-typecheck\n  if type(layer) == tf.compat.v1.keras.layers.DenseFeatures:\n    raise ValueError(\'DenseFeatures V1 is not supported. \'\n                     \'Use tf.compat.v2.keras.layers.DenseFeatures instead.\')\n  if layer.built:\n    logging.warning(\n        \'Beware: Copying a layer that has already been built: \\\'%s\\\'.  \'\n        \'This can lead to subtle bugs because the original layer\\\'s weights \'\n        \'will not be used in the copy.\', layer.name)\n  # Get a fresh copy so we don\'t modify an incoming layer in place.  Weights\n  # will not be shared.\n  return type(layer).from_config(layer.get_config())\n\n\n@gin.configurable\nclass EncodingNetwork(network.Network):\n  """"""Feed Forward network with CNN and FNN layers.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=None,\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               weight_decay_params=None,\n               kernel_initializer=None,\n               batch_squash=True,\n               dtype=tf.float32,\n               name=\'EncodingNetwork\',\n               conv_type=CONV_TYPE_2D):\n    """"""Creates an instance of `EncodingNetwork`.\n\n    Network supports calls with shape outer_rank + input_tensor_spec.shape. Note\n    outer_rank must be at least 1.\n\n    For example an input tensor spec with shape `(2, 3)` will require\n    inputs with at least a batch size, the input shape is `(?, 2, 3)`.\n\n    Input preprocessing is possible via `preprocessing_layers` and\n    `preprocessing_combiner` Layers.  If the `preprocessing_layers` nest is\n    shallower than `input_tensor_spec`, then the layers will get the subnests.\n    For example, if:\n\n    ```python\n    input_tensor_spec = ([TensorSpec(3)] * 2, [TensorSpec(3)] * 5)\n    preprocessing_layers = (Layer1(), Layer2())\n    ```\n\n    then preprocessing will call:\n\n    ```python\n    preprocessed = [preprocessing_layers[0](observations[0]),\n                    preprocessing_layers[1](obsrevations[1])]\n    ```\n\n    However if\n\n    ```python\n    preprocessing_layers = ([Layer1() for _ in range(2)],\n                            [Layer2() for _ in range(5)])\n    ```\n\n    then preprocessing will call:\n    ```python\n    preprocessed = [\n      layer(obs) for layer, obs in zip(flatten(preprocessing_layers),\n                                       flatten(observations))\n    ]\n    ```\n\n    **NOTE** `preprocessing_layers` and `preprocessing_combiner` are not allowed\n    to have already been built.  This ensures calls to `network.copy()` in the\n    future always have an unbuilt, fresh set of parameters.  Furtheremore,\n    a shallow copy of the layers is always created by the Network, so the\n    layer objects passed to the network are never modified.  For more details\n    of the semantics of `copy`, see the docstring of\n    `tf_agents.networks.Network.copy`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations. All of these\n        layers must not be already built.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them.  Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`. This\n        layer must not be already built.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is either a length-three tuple indicating\n        `(filters, kernel_size, stride)` or a length-four tuple indicating\n        `(filters, kernel_size, stride, dilation_rate)`.\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, each item\n        is the fraction of input units to drop or a dictionary of parameters\n        according to the keras.Dropout documentation. The additional parameter\n        `permanent\', if set to True, allows to apply dropout at inference for\n        approximated Bayesian inference. The dropout layers are interleaved with\n        the fully connected layers; there is a dropout layer after each fully\n        connected layer, except if the entry in the list is None. This list must\n        have the same length of fc_layer_params, or be None.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu.\n      weight_decay_params: Optional list of weight decay parameters for the\n        fully connected layers.\n      kernel_initializer: Initializer to use for the kernels of the conv and\n        dense layers. If none is provided a default variance_scaling_initializer\n      batch_squash: If True the outer_ranks of the observation are squashed into\n        the batch dimension. This allow encoding networks to be used with\n        observations with shape [BxTx...].\n      dtype: The dtype to use by the convolution and fully connected layers.\n      name: A string representing name of the network.\n      conv_type: string, \'1d\' or \'2d\'. Convolution layers will be 1d or 2D\n        respectively\n\n    Raises:\n      ValueError: If any of `preprocessing_layers` is already built.\n      ValueError: If `preprocessing_combiner` is already built.\n      ValueError: If the number of dropout layer parameters does not match the\n        number of fully connected layer parameters.\n      ValueError: If conv_layer_params tuples do not have 3 or 4 elements each.\n    """"""\n    if preprocessing_layers is None:\n      flat_preprocessing_layers = None\n    else:\n      flat_preprocessing_layers = [\n          _copy_layer(layer) for layer in tf.nest.flatten(preprocessing_layers)\n      ]\n      # Assert shallow structure is the same. This verifies preprocessing\n      # layers can be applied on expected input nests.\n      input_nest = input_tensor_spec\n      # Given the flatten on preprocessing_layers above we need to make sure\n      # input_tensor_spec is a sequence for the shallow_structure check below\n      # to work.\n      if not nest.is_sequence(input_tensor_spec):\n        input_nest = [input_tensor_spec]\n      nest.assert_shallow_structure(preprocessing_layers, input_nest)\n\n    if (len(tf.nest.flatten(input_tensor_spec)) > 1 and\n        preprocessing_combiner is None):\n      raise ValueError(\n          \'preprocessing_combiner layer is required when more than 1 \'\n          \'input_tensor_spec is provided.\')\n\n    if preprocessing_combiner is not None:\n      preprocessing_combiner = _copy_layer(preprocessing_combiner)\n\n    if not kernel_initializer:\n      kernel_initializer = tf.compat.v1.variance_scaling_initializer(\n          scale=2.0, mode=\'fan_in\', distribution=\'truncated_normal\')\n\n    layers = []\n\n    if conv_layer_params:\n      if conv_type == \'2d\':\n        conv_layer_type = tf.keras.layers.Conv2D\n      elif conv_type == \'1d\':\n        conv_layer_type = tf.keras.layers.Conv1D\n      else:\n        raise ValueError(\'unsupported conv type of %s. Use 1d or 2d\' % (\n            conv_type))\n\n      for config in conv_layer_params:\n        if len(config) == 4:\n          (filters, kernel_size, strides, dilation_rate) = config\n        elif len(config) == 3:\n          (filters, kernel_size, strides) = config\n          dilation_rate = (1, 1) if conv_type == \'2d\' else (1,)\n        else:\n          raise ValueError(\n              \'only 3 or 4 elements permitted in conv_layer_params tuples\')\n        layers.append(\n            conv_layer_type(\n                filters=filters,\n                kernel_size=kernel_size,\n                strides=strides,\n                dilation_rate=dilation_rate,\n                activation=activation_fn,\n                kernel_initializer=kernel_initializer,\n                dtype=dtype))\n\n    layers.append(tf.keras.layers.Flatten())\n\n    if fc_layer_params:\n      if dropout_layer_params is None:\n        dropout_layer_params = [None] * len(fc_layer_params)\n      else:\n        if len(dropout_layer_params) != len(fc_layer_params):\n          raise ValueError(\'Dropout and fully connected layer parameter lists\'\n                           \'have different lengths (%d vs. %d.)\' %\n                           (len(dropout_layer_params), len(fc_layer_params)))\n      if weight_decay_params is None:\n        weight_decay_params = [None] * len(fc_layer_params)\n      else:\n        if len(weight_decay_params) != len(fc_layer_params):\n          raise ValueError(\'Weight decay and fully connected layer parameter \'\n                           \'lists have different lengths (%d vs. %d.)\' %\n                           (len(weight_decay_params), len(fc_layer_params)))\n\n      for num_units, dropout_params, weight_decay in zip(\n          fc_layer_params, dropout_layer_params, weight_decay_params):\n        kernal_regularizer = None\n        if weight_decay is not None:\n          kernal_regularizer = tf.keras.regularizers.l2(weight_decay)\n        layers.append(\n            tf.keras.layers.Dense(\n                num_units,\n                activation=activation_fn,\n                kernel_initializer=kernel_initializer,\n                kernel_regularizer=kernal_regularizer,\n                dtype=dtype))\n        if not isinstance(dropout_params, dict):\n          dropout_params = {\'rate\': dropout_params} if dropout_params else None\n\n        if dropout_params is not None:\n          layers.append(utils.maybe_permanent_dropout(**dropout_params))\n\n    super(EncodingNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec, state_spec=(), name=name)\n\n    # Pull out the nest structure of the preprocessing layers. This avoids\n    # saving the original kwarg layers as a class attribute which Keras would\n    # then track.\n    self._preprocessing_nest = tf.nest.map_structure(lambda l: None,\n                                                     preprocessing_layers)\n    self._flat_preprocessing_layers = flat_preprocessing_layers\n    self._preprocessing_combiner = preprocessing_combiner\n    self._postprocessing_layers = layers\n    self._batch_squash = batch_squash\n\n  def call(self, observation, step_type=None, network_state=(), training=False):\n    del step_type  # unused.\n\n    if self._batch_squash:\n      outer_rank = nest_utils.get_outer_rank(\n          observation, self.input_tensor_spec)\n      batch_squash = utils.BatchSquash(outer_rank)\n      observation = tf.nest.map_structure(batch_squash.flatten, observation)\n\n    if self._flat_preprocessing_layers is None:\n      processed = observation\n    else:\n      processed = []\n      for obs, layer in zip(\n          nest.flatten_up_to(self._preprocessing_nest, observation),\n          self._flat_preprocessing_layers):\n        processed.append(layer(obs, training=training))\n      if len(processed) == 1 and self._preprocessing_combiner is None:\n        # If only one observation is passed and the preprocessing_combiner\n        # is unspecified, use the preprocessed version of this observation.\n        processed = processed[0]\n\n    states = processed\n\n    if self._preprocessing_combiner is not None:\n      states = self._preprocessing_combiner(states)\n\n    for layer in self._postprocessing_layers:\n      states = layer(states, training=training)\n\n    if self._batch_squash:\n      states = tf.nest.map_structure(batch_squash.unflatten, states)\n\n    return states, network_state\n'"
tf_agents/networks/encoding_network_test.py,70,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for tf_agents.networks.encoding_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.keras_layers import sequential_layer\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import test_utils\n\n\nclass EncodingNetworkTest(test_utils.TestCase, parameterized.TestCase):\n\n  def test_empty_layers(self):\n    input_spec = tensor_spec.TensorSpec((2, 3), tf.float32)\n    network = encoding_network.EncodingNetwork(input_spec,)\n\n    with self.assertRaises(ValueError):\n      network.variables  # pylint: disable=pointless-statement\n\n    # Only one layer to flatten input.\n    self.assertLen(network.layers, 1)\n    config = network.layers[0].get_config()\n    self.assertEqual(\'flatten\', config[\'name\'])\n\n    out, _ = network(tf.ones((1, 2, 3)))\n    self.assertAllEqual(out, [[1, 1, 1, 1, 1, 1]])\n    self.assertEmpty(network.variables)\n\n  def test_non_preprocessing_layers_2d(self):\n    input_spec = tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n    network = encoding_network.EncodingNetwork(\n        input_spec,\n        conv_layer_params=((16, 2, 1), (15, 2, 1)),\n        fc_layer_params=(10, 5, 2),\n        activation_fn=tf.keras.activations.tanh,\n    )\n\n    network.create_variables()\n\n    variables = network.variables\n    self.assertLen(variables, 10)\n    self.assertLen(network.layers, 6)\n\n    # Validate first conv layer.\n    config = network.layers[0].get_config()\n    self.assertEqual(\'tanh\', config[\'activation\'])\n    self.assertEqual((2, 2), config[\'kernel_size\'])\n    self.assertEqual(16, config[\'filters\'])\n    self.assertEqual((1, 1), config[\'strides\'])\n    self.assertTrue(config[\'trainable\'])\n\n    # Validate second conv layer.\n    config = network.layers[1].get_config()\n    self.assertEqual(\'tanh\', config[\'activation\'])\n    self.assertEqual((2, 2), config[\'kernel_size\'])\n    self.assertEqual(15, config[\'filters\'])\n    self.assertEqual((1, 1), config[\'strides\'])\n    self.assertTrue(config[\'trainable\'])\n\n    # Validate flatten layer.\n    config = network.layers[2].get_config()\n    self.assertEqual(\'flatten\', config[\'name\'])\n\n    # Validate dense layers.\n    self.assertEqual(10, network.layers[3].get_config()[\'units\'])\n    self.assertEqual(5, network.layers[4].get_config()[\'units\'])\n    self.assertEqual(2, network.layers[5].get_config()[\'units\'])\n\n  def test_non_preprocessing_layers_1d(self):\n    input_spec = tensor_spec.TensorSpec((32, 3), tf.float32)\n    network = encoding_network.EncodingNetwork(\n        input_spec,\n        conv_layer_params=((16, 2, 1), (15, 2, 1)),\n        fc_layer_params=(10, 5, 2),\n        activation_fn=tf.keras.activations.tanh,\n        conv_type=\'1d\',\n    )\n\n    network.create_variables()\n\n    variables = network.variables\n    self.assertLen(variables, 10)\n    self.assertLen(network.layers, 6)\n\n    # Validate first conv layer.\n    config = network.layers[0].get_config()\n    self.assertEqual(\'tanh\', config[\'activation\'])\n    self.assertEqual((2,), config[\'kernel_size\'])\n    self.assertEqual(16, config[\'filters\'])\n    self.assertEqual((1,), config[\'strides\'])\n    self.assertTrue(config[\'trainable\'])\n\n    # Validate second conv layer.\n    config = network.layers[1].get_config()\n    self.assertEqual(\'tanh\', config[\'activation\'])\n    self.assertEqual((2,), config[\'kernel_size\'])\n    self.assertEqual(15, config[\'filters\'])\n    self.assertEqual((1,), config[\'strides\'])\n    self.assertTrue(config[\'trainable\'])\n\n  def test_conv_raise_error(self):\n    input_spec = tensor_spec.TensorSpec((32, 3), tf.float32)\n    with self.assertRaises(ValueError):\n      _ = encoding_network.EncodingNetwork(\n          input_spec,\n          conv_layer_params=((16, 2, 1), (15, 2, 1)),\n          fc_layer_params=(10, 5, 2),\n          activation_fn=tf.keras.activations.tanh,\n          conv_type=\'3d\')\n\n  def test_conv_dilation_params(self):\n    with self.subTest(name=\'no dilations\'):\n      input_spec = tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n      network = encoding_network.EncodingNetwork(\n          input_spec,\n          conv_layer_params=((16, 2, 1), (15, 2, 1)),\n      )\n\n      network.create_variables()\n      variables = network.variables\n\n      self.assertLen(variables, 4)\n      self.assertLen(network.layers, 3)\n\n      # Validate dilation rates\n      config = network.layers[0].get_config()\n      self.assertEqual((1, 1), config[\'dilation_rate\'])\n      config = network.layers[1].get_config()\n      self.assertEqual((1, 1), config[\'dilation_rate\'])\n\n    with self.subTest(name=\'dilations\'):\n      input_spec = tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n      network = encoding_network.EncodingNetwork(\n          input_spec,\n          conv_layer_params=((16, 2, 1, 2), (15, 2, 1, (2, 4))),\n      )\n\n      network.create_variables()\n      variables = network.variables\n\n      self.assertLen(variables, 4)\n      self.assertLen(network.layers, 3)\n\n      # Validate dilation rates\n      config = network.layers[0].get_config()\n      self.assertEqual((2, 2), config[\'dilation_rate\'])\n      config = network.layers[1].get_config()\n      self.assertEqual((2, 4), config[\'dilation_rate\'])\n\n    with self.subTest(name=\'failing conv spec\'):\n      input_spec = tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n      with self.assertRaises(ValueError):\n        network = encoding_network.EncodingNetwork(\n            input_spec,\n            conv_layer_params=((16, 2, 1, 2, 4), (15, 2, 1)),\n            )\n      with self.assertRaises(ValueError):\n        network = encoding_network.EncodingNetwork(\n            input_spec,\n            conv_layer_params=((16, 2, 1), (15, 2)),\n            )\n\n  def test_preprocessing_layer_no_combiner(self):\n    network = encoding_network.EncodingNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([5], tf.float32),\n        preprocessing_layers=tf.keras.layers.Lambda(lambda x: x),\n        preprocessing_combiner=None,\n        fc_layer_params=(2,))\n    out, _ = network(tf.ones((3, 5)))\n    self.assertAllEqual(out.shape.as_list(), [3, 2])\n\n  def test_preprocessing_layers_no_combiner_error(self):\n    with self.assertRaisesRegex(ValueError, \'required\'):\n      encoding_network.EncodingNetwork(\n          input_tensor_spec=[\n              tensor_spec.TensorSpec([5], tf.float32),\n              tensor_spec.TensorSpec([5], tf.float32)\n          ],\n          preprocessing_layers=[\n              tf.keras.layers.Lambda(lambda x: x),\n              tf.keras.layers.Lambda(lambda x: x)\n          ],\n          preprocessing_combiner=None,\n          fc_layer_params=(2,))\n\n  def test_error_raised_if_missing_preprocessing_layer(self):\n    with self.assertRaisesRegex(ValueError, \'sequence length\'):\n      encoding_network.EncodingNetwork(\n          input_tensor_spec=[\n              tensor_spec.TensorSpec([5], tf.float32),\n              tensor_spec.TensorSpec([5], tf.float32)\n          ],\n          preprocessing_layers=[\n              tf.keras.layers.Lambda(lambda x: x),\n          ],\n          preprocessing_combiner=None,\n          fc_layer_params=(2,))\n\n  def test_error_raised_extra_preprocessing_layer(self):\n    with self.assertRaisesRegex(ValueError, \'sequence length\'):\n      encoding_network.EncodingNetwork(\n          input_tensor_spec=tensor_spec.TensorSpec([5], tf.float32),\n          preprocessing_layers=[\n              tf.keras.layers.Lambda(lambda x: x),\n              tf.keras.layers.Lambda(lambda x: x)\n          ],\n          preprocessing_combiner=None,\n          fc_layer_params=(2,))\n\n  def test_dict_spec_and_pre_processing(self):\n    input_spec = {\n        \'a\': tensor_spec.TensorSpec((32, 32, 3), tf.float32),\n        \'b\': tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n    }\n    network = encoding_network.EncodingNetwork(\n        input_spec,\n        preprocessing_layers={\n            \'a\':\n                sequential_layer.SequentialLayer([\n                    tf.keras.layers.Dense(4, activation=\'tanh\'),\n                    tf.keras.layers.Flatten()\n                ]),\n            \'b\':\n                tf.keras.layers.Flatten()\n        },\n        fc_layer_params=(),\n        preprocessing_combiner=tf.keras.layers.Concatenate(axis=-1),\n        activation_fn=tf.keras.activations.tanh,\n    )\n\n    sample_input = tensor_spec.sample_spec_nest(input_spec)\n    output, _ = network(sample_input)\n    # 6144 is the shape from a concat of flat (32, 32, 3) x2.\n    self.assertEqual((7168,), output.shape)\n\n  def test_layers_buildable(self):\n    input_spec = {\n        \'a\': tensor_spec.TensorSpec((32, 32, 3), tf.float32),\n        \'b\': tensor_spec.TensorSpec((32, 32, 3), tf.float32)\n    }\n    network = encoding_network.EncodingNetwork(\n        input_spec,\n        preprocessing_layers={\n            \'a\':\n                sequential_layer.SequentialLayer([\n                    tf.keras.layers.Dense(4, activation=\'tanh\'),\n                    tf.keras.layers.Flatten()\n                ]),\n            \'b\':\n                tf.keras.layers.Flatten()\n        },\n        fc_layer_params=(),\n        preprocessing_combiner=tf.keras.layers.Concatenate(axis=-1),\n        activation_fn=tf.keras.activations.tanh,\n    )\n    network.create_variables()\n    self.assertNotEmpty(network.variables)\n\n  def testDenseFeaturesV1RaisesError(self):\n    key = \'feature_key\'\n    state_dims = 5\n    column = tf.feature_column.numeric_column(key, [state_dims])\n    input_spec = {key: tensor_spec.TensorSpec([state_dims], tf.int32)}\n    dense_features = tf.compat.v1.keras.layers.DenseFeatures([column])\n    with self.assertRaisesRegex(ValueError, \'DenseFeatures\'):\n      encoding_network.EncodingNetwork(\n          input_spec, preprocessing_combiner=dense_features)\n\n  def testNumericFeatureColumnInput(self):\n    key = \'feature_key\'\n    batch_size = 3\n    state_dims = 5\n    input_shape = (batch_size, state_dims)\n    column = tf.feature_column.numeric_column(key, [state_dims])\n    state = {key: tf.ones(input_shape, tf.int32)}\n    input_spec = {key: tensor_spec.TensorSpec([state_dims], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    network = encoding_network.EncodingNetwork(\n        input_spec, preprocessing_combiner=dense_features)\n\n    output, _ = network(state)\n    self.assertEqual(input_shape, output.shape)\n\n  def testIndicatorFeatureColumnInput(self):\n    key = \'feature_key\'\n    vocab_list = [2, 3, 4]\n    column = tf.feature_column.categorical_column_with_vocabulary_list(\n        key, vocab_list)\n    column = tf.feature_column.indicator_column(column)\n\n    state_input = [3, 2, 2, 4, 3]\n    state = {key: tf.expand_dims(state_input, -1)}\n    input_spec = {key: tensor_spec.TensorSpec([1], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    network = encoding_network.EncodingNetwork(\n        input_spec, preprocessing_combiner=dense_features)\n\n    output, _ = network(state)\n    expected_shape = (len(state_input), len(vocab_list))\n    self.assertEqual(expected_shape, output.shape)\n\n  def testCombinedFeatureColumnInput(self):\n    columns = {}\n    tensors = {}\n    specs = {}\n    expected_dim = 0\n\n    indicator_key = \'indicator_key\'\n    vocab_list = [2, 3, 4]\n    column1 = tf.feature_column.categorical_column_with_vocabulary_list(\n        indicator_key, vocab_list)\n    columns[indicator_key] = tf.feature_column.indicator_column(column1)\n    state_input = [3, 2, 2, 4, 3]\n    tensors[indicator_key] = tf.expand_dims(state_input, -1)\n    specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32)\n    expected_dim += len(vocab_list)\n\n    # TODO(b/134950354): Test embedding column for non-eager mode only for now.\n    if not tf.executing_eagerly():\n      embedding_key = \'embedding_key\'\n      embedding_dim = 3\n      vocab_list = [2, 3, 4]\n      column2 = tf.feature_column.categorical_column_with_vocabulary_list(\n          embedding_key, vocab_list)\n      columns[embedding_key] = tf.feature_column.embedding_column(\n          column2, embedding_dim)\n      state_input = [3, 2, 2, 4, 3]\n      tensors[embedding_key] = tf.expand_dims(state_input, -1)\n      specs[embedding_key] = tensor_spec.TensorSpec([1], tf.int32)\n      expected_dim += embedding_dim\n\n    numeric_key = \'numeric_key\'\n    batch_size = 5\n    state_dims = 3\n    input_shape = (batch_size, state_dims)\n    columns[numeric_key] = tf.feature_column.numeric_column(\n        numeric_key, [state_dims])\n    tensors[numeric_key] = tf.ones(input_shape, tf.int32)\n    specs[numeric_key] = tensor_spec.TensorSpec([state_dims], tf.int32)\n    expected_dim += state_dims\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures(\n        list(columns.values()))\n    network = encoding_network.EncodingNetwork(\n        specs, preprocessing_combiner=dense_features)\n\n    output, _ = network(tensors)\n    expected_shape = (batch_size, expected_dim)\n    self.assertEqual(expected_shape, output.shape)\n\n  @parameterized.named_parameters(\n      (\'TrainingTrue\', True,),\n      (\'TrainingFalse\', False))\n  def testDropoutFCLayers(self, training):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    network = encoding_network.EncodingNetwork(\n        obs_spec,\n        fc_layer_params=[20],\n        dropout_layer_params=[0.5])\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    output1, _ = network(obs, training=training)\n    output2, _ = network(obs, training=training)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    output1, output2 = self.evaluate([output1, output2])\n    if training:\n      self.assertGreater(np.linalg.norm(output1 - output2), 0)\n    else:\n      self.assertAllEqual(output1, output2)\n\n  def testWeightDecay(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    network = encoding_network.EncodingNetwork(\n        obs_spec,\n        fc_layer_params=[20],\n        weight_decay_params=[0.5])\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    network(obs)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    regularization_loss = self.evaluate(network.losses[0])\n    self.assertGreater(regularization_loss, 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/expand_dims_layer.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keras layer performing the equivalent of tf.expand_dims.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\nclass ExpandDims(tf.keras.layers.Layer):\n  """"""Expands dims along a particular axis.\n\n  Arguments:\n      axis: Axis to expand.  A new dim is added before this axis.\n         May be a negative value.  Must not be a tensor.\n\n  Input shape:\n      `(batch_size,) + shape`\n\n  Output shape:\n      `(batch_size,) + shape + [1]`, if `axis == -1`.\n\n      `(batch_size,) + shape[:axis + 1] + [1] + shape[axis + 1:]`,\n      if `axis < -1`.\n\n      `(batch_size,) + shape[:axis] + [1] + shape[axis:]`, if `axis >= 0`.\n  """"""\n\n  def __init__(self, axis, **kwargs):\n    super(ExpandDims, self).__init__(**kwargs)\n    self.axis = axis\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    if input_shape.rank is None:\n      return input_shape\n    input_shape = input_shape.as_list()\n    if self.axis == -1:\n      output_shape = input_shape + [1]\n    elif self.axis < 0:\n      output_shape = (\n          input_shape[:self.axis + 1] + [1] + input_shape[self.axis + 1:])\n    else:\n      output_shape = input_shape[:self.axis] + [1] + input_shape[self.axis:]\n    return tf.TensorShape(output_shape)\n\n  def call(self, inputs):\n    if self.axis < 0:\n      # Negative axis, so expand starting from the right\n      return tf.expand_dims(inputs, self.axis)\n    else:\n      # Perform the expansion from the left, but skip the batch dimension.\n      return tf.expand_dims(inputs, self.axis + 1)\n\n  def get_config(self):\n    config = {\'axis\': self.axis}\n    base_config = super(ExpandDims, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n\n# Register with Keras so we can do type(layer).from_config(layer.get_config())\ntf.keras.utils.get_custom_objects()[\'ExpandDims\'] = ExpandDims\n'"
tf_agents/networks/lstm_encoding_network.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Keras LSTM Encoding Network.\n\nImplements a network that will generate the following layers:\n\n  [optional]: preprocessing_layers  # preprocessing_layers\n  [optional]: (Add | Concat(axis=-1) | ...)  # preprocessing_combiner\n  [optional]: Conv2D # input_conv_layer_params\n  Flatten\n  [optional]: Dense  # input_fc_layer_params\n  [optional]: LSTM cell\n  [optional]: Dense  # output_fc_layer_params\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.keras_layers import dynamic_unroll_layer\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import nest_utils\n\nKERAS_LSTM_FUSED = 2\n\n\n@gin.configurable\nclass LSTMEncodingNetwork(network.Network):\n  """"""Recurrent network.""""""\n\n  def __init__(\n      self,\n      input_tensor_spec,\n      preprocessing_layers=None,\n      preprocessing_combiner=None,\n      conv_layer_params=None,\n      input_fc_layer_params=(75, 40),\n      lstm_size=None,\n      output_fc_layer_params=(75, 40),\n      activation_fn=tf.keras.activations.relu,\n      rnn_construction_fn=None,\n      rnn_construction_kwargs=None,\n      dtype=tf.float32,\n      name=\'LSTMEncodingNetwork\',\n  ):\n    """"""Creates an instance of `LSTMEncodingNetwork`.\n\n    Input preprocessing is possible via `preprocessing_layers` and\n    `preprocessing_combiner` Layers.  If the `preprocessing_layers` nest is\n    shallower than `input_tensor_spec`, then the layers will get the subnests.\n    For example, if:\n\n    ```python\n    input_tensor_spec = ([TensorSpec(3)] * 2, [TensorSpec(3)] * 5)\n    preprocessing_layers = (Layer1(), Layer2())\n    ```\n\n    then preprocessing will call:\n\n    ```python\n    preprocessed = [preprocessing_layers[0](observations[0]),\n                    preprocessing_layers[1](obsrevations[1])]\n    ```\n\n    However if\n\n    ```python\n    preprocessing_layers = ([Layer1() for _ in range(2)],\n                            [Layer2() for _ in range(5)])\n    ```\n\n    then preprocessing will call:\n    ```python\n    preprocessed = [\n      layer(obs) for layer, obs in zip(flatten(preprocessing_layers),\n                                       flatten(observations))\n    ]\n    ```\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        observations.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations. All of these\n        layers must not be already built.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them.  Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`. This\n        layer must not be already built.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      input_fc_layer_params: Optional list of fully connected parameters, where\n        each item is the number of units in the layer. These feed into the\n        recurrent layer.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully connected parameters, where\n        each item is the number of units in the layer. These are applied on top\n        of the recurrent layer.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu,.\n      rnn_construction_fn: (Optional.) Alternate RNN construction function, e.g.\n        tf.keras.layers.LSTM, tf.keras.layers.CuDNNLSTM. It is invalid to\n        provide both rnn_construction_fn and lstm_size.\n      rnn_construction_kwargs: (Optional.) Dictionary or arguments to pass to\n        rnn_construction_fn.\n\n        The RNN will be constructed via:\n\n        ```\n        rnn_layer = rnn_construction_fn(**rnn_construction_kwargs)\n        ```\n      dtype: The dtype to use by the convolution, LSTM, and fully connected\n        layers.\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If any of `preprocessing_layers` is already built.\n      ValueError: If `preprocessing_combiner` is already built.\n      ValueError: If neither `lstm_size` nor `rnn_construction_fn` are provided.\n      ValueError: If both `lstm_size` and `rnn_construction_fn` are provided.\n    """"""\n    if lstm_size is None and rnn_construction_fn is None:\n      raise ValueError(\'Need to provide either custom rnn_construction_fn or \'\n                       \'lstm_size.\')\n    if lstm_size and rnn_construction_fn:\n      raise ValueError(\'Cannot provide both custom rnn_construction_fn and \'\n                       \'lstm_size.\')\n\n    kernel_initializer = tf.compat.v1.variance_scaling_initializer(\n        scale=2.0, mode=\'fan_in\', distribution=\'truncated_normal\')\n\n    input_encoder = encoding_network.EncodingNetwork(\n        input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=input_fc_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        dtype=dtype)\n\n    # Create RNN cell\n    if rnn_construction_fn:\n      rnn_construction_kwargs = rnn_construction_kwargs or {}\n      lstm_network = rnn_construction_fn(**rnn_construction_kwargs)\n    else:\n      if len(lstm_size) == 1:\n        cell = tf.keras.layers.LSTMCell(\n            lstm_size[0],\n            dtype=dtype,\n            implementation=KERAS_LSTM_FUSED)\n      else:\n        cell = tf.keras.layers.StackedRNNCells(\n            [tf.keras.layers.LSTMCell(size, dtype=dtype,\n                                      implementation=KERAS_LSTM_FUSED)\n             for size in lstm_size])\n      lstm_network = dynamic_unroll_layer.DynamicUnroll(cell)\n\n    output_encoder = []\n    if output_fc_layer_params:\n      output_encoder = [\n          tf.keras.layers.Dense(\n              num_units,\n              activation=activation_fn,\n              kernel_initializer=kernel_initializer,\n              dtype=dtype) for num_units in output_fc_layer_params\n      ]\n\n    counter = [-1]\n\n    def create_spec(size):\n      counter[0] += 1\n      return tensor_spec.TensorSpec(\n          size, dtype=dtype, name=\'network_state_%d\' % counter[0])\n\n    state_spec = tf.nest.map_structure(create_spec,\n                                       lstm_network.cell.state_size)\n\n    super(LSTMEncodingNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec, state_spec=state_spec, name=name)\n\n    self._conv_layer_params = conv_layer_params\n    self._input_encoder = input_encoder\n    self._lstm_network = lstm_network\n    self._output_encoder = output_encoder\n\n  def call(self,\n           observation,\n           step_type,\n           network_state=(),\n           training=False):\n    """"""Apply the network.\n\n    Args:\n      observation: A tuple of tensors matching `input_tensor_spec`.\n      step_type: A tensor of `StepType.\n      network_state: (optional.) The network state.\n      training: Whether the output is being used for training.\n\n    Returns:\n      `(outputs, network_state)` - the network output and next network state.\n\n    Raises:\n      ValueError: If observation tensors lack outer `(batch,)` or\n        `(batch, time)` axes.\n    """"""\n    num_outer_dims = nest_utils.get_outer_rank(observation,\n                                               self.input_tensor_spec)\n    if num_outer_dims not in (1, 2):\n      raise ValueError(\n          \'Input observation must have a batch or batch x time outer shape.\')\n\n    has_time_dim = num_outer_dims == 2\n    if not has_time_dim:\n      # Add a time dimension to the inputs.\n      observation = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                          observation)\n      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                        step_type)\n\n    state, _ = self._input_encoder(\n        observation, step_type=step_type, network_state=(), training=training)\n\n    network_kwargs = {}\n    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n      network_kwargs[\'reset_mask\'] = tf.equal(step_type,\n                                              time_step.StepType.FIRST,\n                                              name=\'mask\')\n\n    # Unroll over the time sequence.\n    output = self._lstm_network(\n        inputs=state,\n        initial_state=network_state,\n        training=training,\n        **network_kwargs)\n\n    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n      state, network_state = output\n    else:\n      state = output[0]\n      network_state = tf.nest.pack_sequence_as(\n          self._lstm_network.cell.state_size, tf.nest.flatten(output[1:]))\n\n    for layer in self._output_encoder:\n      state = layer(state, training=training)\n\n    if not has_time_dim:\n      # Remove time dimension from the state.\n      state = tf.squeeze(state, [1])\n\n    return state, network_state\n'"
tf_agents/networks/network.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base extension to Keras network to simplify copy operations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nimport typing\n\nimport six\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tensorflow.keras import layers  # pylint: disable=unused-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.keras_layers import sequential_layer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import object_identity\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.keras.utils import layer_utils  # TF internal\nfrom tensorflow.python.training.tracking import base  # TF internal\nfrom tensorflow.python.util import tf_decorator  # TF internal\nfrom tensorflow.python.util import tf_inspect  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\n\nclass _NetworkMeta(abc.ABCMeta):\n  """"""Meta class for Network object.\n\n  We mainly use this class to capture all args to `__init__` of all `Network`\n  instances, and store them in `instance._saved_kwargs`.  This in turn is\n  used by the `instance.copy` method.\n  """"""\n\n  def __new__(mcs, classname, baseclasses, attrs):\n    """"""Control the creation of subclasses of the Network class.\n\n    Args:\n      classname: The name of the subclass being created.\n      baseclasses: A tuple of parent classes.\n      attrs: A dict mapping new attributes to their values.\n\n    Returns:\n      The class object.\n\n    Raises:\n      RuntimeError: if the class __init__ has *args in its signature.\n    """"""\n    if baseclasses[0] == tf.keras.layers.Layer:\n      # This is just Network below.  Return early.\n      return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n\n    init = attrs.get(""__init__"", None)\n\n    if not init:\n      # This wrapper class does not define an __init__.  When someone creates\n      # the object, the __init__ of its parent class will be called.  We will\n      # call that __init__ instead separately since the parent class is also a\n      # subclass of Network.  Here just create the class and return.\n      return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n\n    arg_spec = tf_inspect.getargspec(init)\n    if arg_spec.varargs is not None:\n      raise RuntimeError(\n          ""%s.__init__ function accepts *args.  This is not allowed."" %\n          classname)\n\n    def _capture_init(self, *args, **kwargs):\n      """"""Captures init args and kwargs and stores them into `_saved_kwargs`.""""""\n      if len(args) > len(arg_spec.args) + 1:\n        # Error case: more inputs than args.  Call init so that the appropriate\n        # error can be raised to the user.\n        init(self, *args, **kwargs)\n      # Convert to a canonical kwarg format.\n      kwargs = tf_inspect.getcallargs(init, self, *args, **kwargs)\n      kwargs.pop(""self"")\n      init(self, **kwargs)\n      # Avoid auto tracking which prevents keras from tracking layers that are\n      # passed as kwargs to the Network.\n      with base.no_automatic_dependency_tracking_scope(self):\n        setattr(self, ""_saved_kwargs"", kwargs)\n\n    attrs[""__init__""] = tf_decorator.make_decorator(init, _capture_init)\n    return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n\n\n@six.add_metaclass(_NetworkMeta)\nclass Network(tf.keras.layers.Layer):\n  """"""A class used to represent networks used by TF-Agents policies and agents.\n\n  The main differences between a TF-Agents Network and a Keras Layer include:\n  networks keep track of their underlying layers, explicitly represent RNN-like\n  state in inputs and outputs, and simplify variable creation and clone\n  operations.\n\n  When calling a network `net`, typically one passes data through it via:\n\n  ```python\n  outputs, next_state = net(observation, network_state=...)\n  outputs, next_state = net(observation, step_type=..., network_state=...)\n  outputs, next_state = net(observation)  # net.call must fill an empty state\n  outputs, next_state = net(observation, step_type=...)\n  outputs, next_state = net(\n      observation, step_type=..., network_state=..., learning=...)\n  ```\n\n  etc.\n\n  To force construction of a network\'s variables:\n  ```python\n  net.create_variables()\n  net.create_variables(input_tensor_spec=...)  # To provide an input spec\n  net.create_variables(training=True)  # Provide extra kwargs\n  net.create_variables(input_tensor_spec, training=True)\n  ```\n\n  To create a copy of the network:\n  ```python\n  cloned_net = net.copy()\n  cloned_net.variables  # Raises ValueError: cloned net does not share weights.\n  cloned_net.create_variables(...)\n  cloned_net.variables  # Now new variables have been created.\n  ```\n  """"""\n\n  # TODO(b/156314975): Rename input_tensor_spec to input_spec.\n  def __init__(self, input_tensor_spec=None, state_spec=(), name=None):\n    """"""Creates an instance of `Network`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.  Optional.  If not provided, `create_variables()`\n        will fail unless a spec is provided.\n      state_spec: A nest of `tensor_spec.TensorSpec` representing the state\n        needed by the network. Default is `()`, which means no state.\n      name: (Optional.) A string representing the name of the network.\n    """"""\n    super(Network, self).__init__(name=name)\n    common.check_tf1_allowed()\n\n    # Required for summary() to work.\n    self._is_graph_network = False\n\n    self._input_tensor_spec = input_tensor_spec\n    # NOTE(ebrevdo): Would have preferred to call this output_tensor_spec, but\n    # looks like keras.Layer already reserves that one.\n    self._network_output_spec = None\n    self._state_spec = state_spec\n\n  @property\n  def state_spec(self):\n    return self._state_spec\n\n  @property\n  def input_tensor_spec(self):\n    """"""Returns the spec of the input to the network of type InputSpec.""""""\n    return self._input_tensor_spec\n\n  def create_variables(self, input_tensor_spec=None, **kwargs):\n    """"""Force creation of the network\'s variables.\n\n    Return output specs.\n\n    Args:\n      input_tensor_spec: (Optional).  Override or provide an input tensor spec\n        when creating variables.\n      **kwargs: Other arguments to `network.call()`, e.g. `training=True`.\n\n    Returns:\n      Output specs - a nested spec calculated from the outputs (excluding any\n      batch dimensions).  If any of the output elements is a tfp `Distribution`,\n      the associated spec entry returned is `None`.\n\n    Raises:\n      ValueError: If no `input_tensor_spec` is provided, and the network did\n        not provide one during construction.\n    """"""\n    if self._network_output_spec is not None:\n      return self._network_output_spec\n    if self._input_tensor_spec is None:\n      self._input_tensor_spec = input_tensor_spec\n    input_tensor_spec = self._input_tensor_spec\n    if input_tensor_spec is None:\n      raise ValueError(\n          ""Unable to create_variables: no input_tensor_spec provided, and ""\n          ""Network did not define one."")\n\n    random_input = tensor_spec.sample_spec_nest(\n        input_tensor_spec, outer_dims=(1,))\n    initial_state = self.get_initial_state(batch_size=1)\n    step_type = tf.fill((1,), time_step.StepType.FIRST)\n    outputs = self.__call__(\n        random_input,\n        step_type=step_type,\n        network_state=initial_state,\n        **kwargs)\n\n    def _calc_unbatched_spec(x):\n      if isinstance(x, tfp.distributions.Distribution):\n        return None\n      else:\n        return nest_utils.remove_singleton_batch_spec_dim(\n            tf.type_spec_from_value(x), outer_ndim=1)\n\n    self._network_output_spec = tf.nest.map_structure(\n        _calc_unbatched_spec, outputs[0])\n    return self._network_output_spec\n\n  @property\n  def variables(self):\n    if not self.built:\n      raise ValueError(\n          ""Network has not been built, unable to access variables.  ""\n          ""Please call `create_variables` or apply the network first."")\n    return super(Network, self).variables\n\n  @property\n  def trainable_variables(self):\n    if not self.built:\n      raise ValueError(\n          ""Network has not been built, unable to access variables.  ""\n          ""Please call `create_variables` or apply the network first."")\n    return super(Network, self).trainable_variables\n\n  @property\n  def layers(self):\n    """"""Get the list of all (nested) sub-layers used in this Network.""""""\n    return list(_filter_empty_layer_containers(self._layers))\n\n  def get_layer(self, name=None, index=None):\n    """"""Retrieves a layer based on either its name (unique) or index.\n\n    If `name` and `index` are both provided, `index` will take precedence.\n    Indices are based on order of horizontal graph traversal (bottom-up).\n\n    Arguments:\n        name: String, name of layer.\n        index: Integer, index of layer.\n\n    Returns:\n        A layer instance.\n\n    Raises:\n        ValueError: In case of invalid layer name or index.\n    """"""\n    if index is not None and name is not None:\n      raise ValueError(""Provide only a layer name or a layer index."")\n\n    if index is not None:\n      if len(self.layers) <= index:\n        raise ValueError(""Was asked to retrieve layer at index "" + str(index) +\n                         "" but model only has "" + str(len(self.layers)) +\n                         "" layers."")\n      else:\n        return self.layers[index]\n\n    if name is not None:\n      for layer in self.layers:\n        if layer.name == name:\n          return layer\n      raise ValueError(""No such layer: "" + name + ""."")\n\n  def summary(self, line_length=None, positions=None, print_fn=None):\n    """"""Prints a string summary of the network.\n\n    Args:\n        line_length: Total length of printed lines\n            (e.g. set this to adapt the display to different\n            terminal window sizes).\n        positions: Relative or absolute positions of log elements\n            in each line. If not provided,\n            defaults to `[.33, .55, .67, 1.]`.\n        print_fn: Print function to use. Defaults to `print`.\n            It will be called on each line of the summary.\n            You can set it to a custom function\n            in order to capture the string summary.\n\n    Raises:\n        ValueError: if `summary()` is called before the model is built.\n    """"""\n    if not self.built:\n      raise ValueError(""This model has not yet been built. ""\n                       ""Build the model first by calling `build()` or ""\n                       ""`__call__()` with some data, or `create_variables()`."")\n    layer_utils.print_summary(self,\n                              line_length=line_length,\n                              positions=positions,\n                              print_fn=print_fn)\n\n  def copy(self, **kwargs):\n    """"""Create a shallow copy of this network.\n\n    **NOTE** Network layer weights are *never* copied.  This method recreates\n    the `Network` instance with the same arguments it was initialized with\n    (excepting any new kwargs).\n\n    Args:\n      **kwargs: Args to override when recreating this network.  Commonly\n        overridden args include \'name\'.\n\n    Returns:\n      A shallow copy of this network.\n    """"""\n    return type(self)(**dict(self._saved_kwargs, **kwargs))\n\n  def __call__(self, inputs, *args, **kwargs):\n    """"""A wrapper around `Network.call`.\n\n    A typical `call` method in a class subclassing `Network` will have a\n    signature that accepts `inputs`, as well as other `*args` and `**kwargs`.\n    `call` can optionally also accept `step_type` and `network_state`\n    (if `state_spec != ()` is not trivial).  e.g.:\n\n    ```python\n    def call(self,\n             inputs,\n             step_type=None,\n             network_state=(),\n             training=False):\n        ...\n        return outputs, new_network_state\n    ```\n\n    We will validate the first argument (`inputs`)\n    against `self.input_tensor_spec` if one is available.\n\n    If a `network_state` kwarg is given it is also validated against\n    `self.state_spec`.  Similarly, the return value of the `call` method is\n    expected to be a tuple/list with 2 values:  `(output, new_state)`.\n    We validate `new_state` against `self.state_spec`.\n\n    If no `network_state` kwarg is given (or if empty `network_state = ()` is\n    given, it is up to `call` to assume a proper ""empty"" state, and to\n    emit an appropriate `output_state`.\n\n    Args:\n      inputs: The input to `self.call`, matching `self.input_tensor_spec`.\n      *args: Additional arguments to `self.call`.\n      **kwargs: Additional keyword arguments to `self.call`.\n        These can include `network_state` and `step_type`.  `step_type` is\n        required if the network\'s `call` requires it. `network_state` is\n        required if the underlying network\'s `call` requires it.\n\n    Returns:\n      A tuple `(outputs, new_network_state)`.\n    """"""\n    if self.input_tensor_spec is not None:\n      nest_utils.assert_same_structure(\n          inputs,\n          self.input_tensor_spec,\n          message=""inputs and input_tensor_spec structures do not match"")\n    call_argspec = tf_inspect.getargspec(self.call)\n\n    # Convert *args, **kwargs to a canonical kwarg representation.\n    normalized_kwargs = tf_inspect.getcallargs(\n        self.call, inputs, *args, **kwargs)\n    # TODO(b/156315434): Rename network_state to just state.\n    network_state = normalized_kwargs.get(""network_state"", None)\n    normalized_kwargs.pop(""self"", None)\n\n    if network_state not in (None, ()):\n      nest_utils.assert_same_structure(\n          network_state,\n          self.state_spec,\n          message=""network_state and state_spec structures do not match"")\n\n    if ""step_type"" not in call_argspec.args and not call_argspec.keywords:\n      normalized_kwargs.pop(""step_type"", None)\n\n    if (network_state in (None, ())\n        and ""network_state"" not in call_argspec.args\n        and not call_argspec.keywords):\n      normalized_kwargs.pop(""network_state"", None)\n\n    outputs, new_state = super(Network, self).__call__(**normalized_kwargs)\n    nest_utils.assert_same_structure(\n        new_state,\n        self.state_spec,\n        message=""network output state and state_spec structures do not match"")\n\n    return outputs, new_state\n\n  def _check_trainable_weights_consistency(self):\n    """"""Check trainable weights count consistency.\n\n    This method makes up for the missing method (b/143631010) of the same name\n    in `keras.Network`, which is needed when calling `Network.summary()`. This\n    method is a no op. If a Network wants to check the consistency of trainable\n    weights, see `keras.Model._check_trainable_weights_consistency` as a\n    reference.\n    """"""\n    # TODO(b/143631010): If recognized and fixed, remove this entire method.\n    return\n\n  def get_initial_state(self, batch_size=None):\n    """"""Returns an initial state usable by the network.\n\n    Args:\n      batch_size: Tensor or constant: size of the batch dimension. Can be None\n        in which case not dimensions gets added.\n\n    Returns:\n      A nested object of type `self.state_spec` containing properly\n      initialized Tensors.\n    """"""\n    return self._get_initial_state(batch_size)\n\n  def _get_initial_state(self, batch_size):\n    """"""Returns the initial state of the policy network.\n\n    Args:\n      batch_size: A constant or Tensor holding the batch size. Can be None, in\n        which case the state will not have a batch dimension added.\n\n    Returns:\n      A nest of zero tensors matching the spec of the policy network state.\n    """"""\n    return tensor_spec.zero_spec_nest(\n        self._state_spec,\n        outer_dims=None if batch_size is None else [batch_size])\n\n\nclass DistributionNetwork(Network):\n  """"""Base class for networks which generate Distributions as their output.""""""\n\n  def __init__(self, input_tensor_spec, state_spec, output_spec, name):\n    super(DistributionNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec, state_spec=state_spec, name=name)\n    self._output_spec = output_spec\n\n  @property\n  def output_spec(self):\n    return self._output_spec\n\n\ndef _is_layer(obj):\n  """"""Implicit check for Layer-like objects.""""""\n  # TODO(b/110718070): Replace with isinstance(obj, tf.keras.layers.Layer).\n  return hasattr(obj, ""_is_layer"") and not isinstance(obj, type)\n\n\ndef _filter_empty_layer_containers(layer_list):\n  """"""Remove empty layer containers.""""""\n  existing = object_identity.ObjectIdentitySet()\n  to_visit = layer_list[::-1]\n  while to_visit:\n    obj = to_visit.pop()\n    if obj in existing:\n      continue\n    existing.add(obj)\n    if _is_layer(obj):\n      yield obj\n    else:\n      sub_layers = getattr(obj, ""layers"", None) or []\n\n      # Trackable data structures will not show up in "".layers"" lists, but\n      # the layers they contain will.\n      to_visit.extend(sub_layers[::-1])\n\n\ndef create_variables(module: typing.Union[Network, tf.keras.layers.Layer],\n                     input_spec: typing.Optional[types.NestedTensorSpec] = None,\n                     **kwargs: typing.Any) -> types.NestedTensorSpec:\n  """"""Create variables in `module` given `input_spec`; return `output_spec`.\n\n  Here `module` can be a `Network`, and we will soon also support Keras\n  layers (and possibly Sonnet layers).\n\n  Args:\n    module: The instance we would like to create layers on.\n    input_spec: The input spec (excluding batch dimensions).\n    **kwargs: Extra arguments to `module.__call__`, e.g. `training=True`.\n\n  Returns:\n    Output specs, a nested `tf.TypeSpec` describing the output signature.\n  """"""\n  if isinstance(module, Network):\n    return module.create_variables(input_spec, **kwargs)\n\n  # Keras layer\n  if input_spec is None:\n    raise ValueError(\n        ""Module is a Keras layer; an input_spec is required but saw ""\n        ""None: {}"".format(module))\n\n  maybe_spec = getattr(module, ""_network_output_spec"", None)\n  if maybe_spec is not None:\n    return maybe_spec\n\n  # Has state outputs - so expect that a state input is required,\n  # and output[1:] are output states.\n  recurrent_layer = getattr(module, ""get_initial_state"", None) is not None\n\n  # Required input rank\n  outer_ndim = _get_input_outer_ndim(module, input_spec)\n\n  random_input = tensor_spec.sample_spec_nest(\n      input_spec, outer_dims=(1,) * outer_ndim)\n\n  if recurrent_layer:\n    state = module.get_initial_state(random_input)\n    outputs = module(random_input, state, **kwargs)\n  else:\n    outputs = module(random_input, **kwargs)\n\n  if isinstance(outputs, (list, tuple)):\n    output = outputs[0]\n  else:\n    output = outputs\n\n  def _calc_unbatched_spec(x):\n    if isinstance(x, tfp.distributions.Distribution):\n      return None\n    else:\n      return nest_utils.remove_singleton_batch_spec_dim(\n          tf.type_spec_from_value(x), outer_ndim=outer_ndim)\n\n  # pylint: disable=protected-access\n  module._network_output_spec = tf.nest.map_structure(_calc_unbatched_spec,\n                                                      output)\n\n  return module._network_output_spec\n  # pylint: disable=protected-access\n\n\ndef _get_input_outer_ndim(layer: tf.keras.layers.Layer,\n                          input_spec: types.NestedTensorSpec) -> int:\n  """"""Calculate or guess the number of batch (outer) ndims in `layer`.""""""\n  if isinstance(layer, tf.keras.layers.TimeDistributed):\n    return 1 + _get_input_outer_ndim(layer.layer, input_spec)\n  if isinstance(layer, tf.keras.layers.RNN):\n    return 1 + _get_input_outer_ndim(layer.cell, input_spec)\n  if isinstance(layer, (sequential_layer.SequentialLayer, tf.keras.Sequential)):\n    # We don\'t trust Sequential to give us the right thing if the first layer\n    # is e.g. a TimeDistributed.\n    return _get_input_outer_ndim(layer.layers[0], input_spec)\n\n  layer_input_spec = layer.input_spec\n\n  if layer_input_spec is None:\n    return 1\n\n  outer_ndim = layer_input_spec.ndim\n  if outer_ndim is None:\n    outer_ndim = layer_input_spec.min_ndim\n\n  if outer_ndim is None:\n    return 1\n\n  if input_spec:\n    input_spec = tf.nest.flatten(input_spec)[0]\n    if outer_ndim >= input_spec.shape.ndims:\n      # We can capture the ""outer batch size"" as the diff between the\n      # expected input rank and the rank of the non-batched spec passed in the\n      # input_spec.\n      return outer_ndim - input_spec.shape.ndims\n\n  # Empty input_spec.\n  return 1\n'"
tf_agents/networks/network_test.py,44,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.networks import network\nfrom tf_agents.utils import common\n\n\nclass BaseNetwork(network.Network):\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, v1, **kwargs):\n    super(BaseNetwork, self).__init__(v1, **kwargs)\n  # pylint: enable=useless-super-delegation\n\n\nclass NetworkNoExtraKeywordsInCallSignature(network.Network):\n\n  def call(self, inputs):\n    return inputs, ()\n\n\nclass MockNetwork(BaseNetwork):\n\n  def __init__(self, param1, param2, kwarg1=2, kwarg2=3):\n    self.param1 = param1\n    self.param2 = param2\n    self.kwarg1 = kwarg1\n    self.kwarg2 = kwarg2\n    super(MockNetwork, self).__init__(param1,\n                                      state_spec=(),\n                                      name=\'mock\')\n\n  def build(self, *args, **kwargs):\n    self.var1 = common.create_variable(\n        \'variable\', dtype=tf.float32, trainable=False)\n    self.var2 = common.create_variable(\n        \'trainable_variable\', dtype=tf.float32, trainable=True)\n\n  def call(self, observations, step_type, network_state=None):\n    return self.var1 + self.var2 + observations, ()\n\n\nclass NoInitNetwork(MockNetwork):\n  pass\n\n\nclass GnarlyNetwork(network.Network):\n\n  def __init__(self):\n    k1 = tf.keras.Sequential([\n        tf.keras.layers.Dense(\n            32,\n            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n            bias_regularizer=tf.keras.regularizers.l2(1e-4),\n        ),\n        tf.keras.layers.Dense(64),\n        tf.keras.layers.BatchNormalization()\n    ], name=\'a\')\n    k2 = tf.keras.layers.Dense(12, name=\'b\')\n    super(GnarlyNetwork, self).__init__(\n        input_tensor_spec=tf.TensorSpec(dtype=tf.float32, shape=(2,)),\n        state_spec=(), name=None)\n    self._k1 = k1\n    self._k2 = k2\n\n  def call(self, observations, step_type, network_state=None):\n    return self._k2(self._k1(observations)), network_state\n\n\nclass NetworkTest(tf.test.TestCase):\n\n  def test_copy_works(self):\n    network1 = MockNetwork(0, 1)\n    network2 = network1.copy()\n\n    self.assertNotEqual(network1, network2)\n    self.assertEqual(0, network2.param1)\n    self.assertEqual(1, network2.param2)\n    self.assertEqual(2, network2.kwarg1)\n    self.assertEqual(3, network2.kwarg2)\n\n  def test_noinit_copy_works(self):\n    network1 = NoInitNetwork(0, 1)\n    network2 = network1.copy()\n\n    self.assertNotEqual(network1, network2)\n    self.assertEqual(0, network2.param1)\n    self.assertEqual(1, network2.param2)\n    self.assertEqual(2, network2.kwarg1)\n    self.assertEqual(3, network2.kwarg2)\n\n  def test_too_many_args_raises_appropriate_error(self):\n    with self.assertRaisesRegexp(TypeError, \'__init__.*given\'):\n      # pylint: disable=too-many-function-args\n      MockNetwork(0, 1, 2, 3, 4, 5, 6)\n\n  def test_assert_input_spec(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    net = MockNetwork(spec, 1)\n    with self.assertRaises(ValueError):\n      net((1, 2), 2)\n\n  def test_create_variables(self):\n    observation_spec = specs.TensorSpec([1], tf.float32, \'observation\')\n    action_spec = specs.TensorSpec([2], tf.float32, \'action\')\n    net = MockNetwork(observation_spec, action_spec)\n    self.assertFalse(net.built)\n    with self.assertRaises(ValueError):\n      net.variables  # pylint: disable=pointless-statement\n    output_spec = net.create_variables()\n    # MockNetwork adds some variables to observation, which has shape [bs, 1]\n    self.assertEqual(output_spec, tf.TensorSpec([1], dtype=tf.float32))\n    self.assertTrue(net.built)\n    self.assertLen(net.variables, 2)\n    self.assertLen(net.trainable_variables, 1)\n\n  def test_summary_no_exception(self):\n    """"""Tests that Network.summary() does not throw an exception.""""""\n    observation_spec = specs.TensorSpec([1], tf.float32, \'observation\')\n    action_spec = specs.TensorSpec([2], tf.float32, \'action\')\n    net = MockNetwork(observation_spec, action_spec)\n    net.create_variables()\n    net.summary()\n\n  def test_access_deep_layers_weights_and_losses(self):\n    net = GnarlyNetwork()\n    net.create_variables(training=True)\n    layer_names = sorted([l.name for l in net.layers])\n    losses = net.losses\n    trainable_weight_names = sorted([w.name for w in net.trainable_weights])\n    non_trainable_weight_names = sorted(\n        [w.name for w in net.non_trainable_weights])\n    self.assertEqual(layer_names, [\'a\', \'b\'])\n    self.assertLen(losses, 2)\n    for loss in losses:\n      self.assertEqual(loss.dtype, tf.float32)\n      self.assertEqual(loss.shape, ())\n    self.assertEqual(\n        [x.lstrip(\'gnarly_network/\') for x in trainable_weight_names],\n        [\'batch_normalization/beta:0\',\n         \'batch_normalization/gamma:0\',\n         \'dense/bias:0\',\n         \'dense/kernel:0\',\n         \'dense_1/bias:0\',\n         \'dense_1/kernel:0\',\n         \'b/bias:0\',\n         \'b/kernel:0\'])\n    self.assertEqual(\n        [x.lstrip(\'gnarly_network/\') for x in non_trainable_weight_names],\n        [\'batch_normalization/moving_mean:0\',\n         \'batch_normalization/moving_variance:0\'])\n\n  def test_dont_complain_if_no_network_state_in_call_signature(self):\n    net = NetworkNoExtraKeywordsInCallSignature()\n    out, _ = net(1, network_state=None)  # This shouldn\'t complain.\n    self.assertAllEqual(out, 1)\n    out, _ = net(1, step_type=3, network_state=None)  # This shouldn\'t complain.\n    self.assertAllEqual(out, 1)\n\n\nclass CreateVariablesTest(parameterized.TestCase, tf.test.TestCase):\n\n  def testNetworkCreate(self):\n    observation_spec = specs.TensorSpec([1], tf.float32, \'observation\')\n    action_spec = specs.TensorSpec([2], tf.float32, \'action\')\n    net = MockNetwork(observation_spec, action_spec)\n    self.assertFalse(net.built)\n    with self.assertRaises(ValueError):\n      net.variables  # pylint: disable=pointless-statement\n    output_spec = network.create_variables(net)\n    # MockNetwork adds some variables to observation, which has shape [bs, 1]\n    self.assertEqual(output_spec, tf.TensorSpec([1], dtype=tf.float32))\n    self.assertTrue(net.built)\n    self.assertLen(net.variables, 2)\n    self.assertLen(net.trainable_variables, 1)\n\n  @parameterized.named_parameters(\n      (\'Dense\',\n       lambda: tf.keras.layers.Dense(3),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n      (\'LSTMCell\',\n       lambda: tf.keras.layers.LSTMCell(3),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n      (\'LSTMCellInRNN\',\n       lambda: tf.keras.layers.RNN(tf.keras.layers.LSTMCell(3)),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n      (\'LSTM\',\n       lambda: tf.keras.layers.LSTM(3),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n      (\'TimeDistributed\',\n       lambda: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3)),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n      (\'Conv2D\',\n       lambda: tf.keras.layers.Conv2D(2, 3),\n       tf.TensorSpec((28, 28, 5), tf.float32),\n       tf.TensorSpec((26, 26, 2), tf.float32)),\n      (\'SequentialOfDense\',\n       lambda: tf.keras.Sequential([tf.keras.layers.Dense(3)] * 2),\n       tf.TensorSpec((5,), tf.float32),\n       tf.TensorSpec((3,), tf.float32)),\n  )\n  def testKerasLayerCreate(self, layer_fn, input_spec, expected_output_spec):\n    layer = layer_fn()\n    with self.assertRaisesRegex(ValueError, \'an input_spec is required\'):\n      network.create_variables(layer)\n    output_spec = network.create_variables(layer, input_spec)\n    self.assertTrue(layer.built)\n    self.assertEqual(output_spec, expected_output_spec)\n    output_spec_2 = network.create_variables(layer, input_spec)\n    self.assertEqual(output_spec_2, expected_output_spec)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/normal_projection_network.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Project inputs to a normal distribution object.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import utils as distribution_utils\nfrom tf_agents.keras_layers import bias_layer\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils as network_utils\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\n\n\ndef tanh_squash_to_spec(inputs, spec):\n  """"""Maps inputs with arbitrary range to range defined by spec using `tanh`.""""""\n  means = (spec.maximum + spec.minimum) / 2.0\n  magnitudes = (spec.maximum - spec.minimum) / 2.0\n\n  return means + magnitudes * tf.tanh(inputs)\n\n\n@gin.configurable\nclass NormalProjectionNetwork(network.DistributionNetwork):\n  """"""Generates a tfp.distribution.Normal by predicting a mean and std.\n\n  Note: By default this network uses `tanh_squash_to_spec` to normalize its\n  output. Due to the nature of the `tanh` function, values near the spec bounds\n  cannot be returned.\n\n  Note: The standard deviations are independent of the input.\n  """"""\n\n  def __init__(self,\n               sample_spec,\n               activation_fn=None,\n               init_means_output_factor=0.1,\n               std_bias_initializer_value=0.0,\n               mean_transform=tanh_squash_to_spec,\n               std_transform=tf.nn.softplus,\n               state_dependent_std=False,\n               scale_distribution=False,\n               name=\'NormalProjectionNetwork\'):\n    """"""Creates an instance of NormalProjectionNetwork.\n\n    Args:\n      sample_spec: A `tensor_spec.BoundedTensorSpec` detailing the shape and\n        dtypes of samples pulled from the output distribution.\n      activation_fn: Activation function to use in dense layer.\n      init_means_output_factor: Output factor for initializing action means\n        weights.\n      std_bias_initializer_value: Initial value for the bias of the\n        stddev_projection_layer or the direct bias_layer depending on the\n        state_dependent_std flag.\n      mean_transform: Transform to apply to the calculated means. Uses\n        `tanh_squash_to_spec` by default.\n      std_transform: Transform to apply to the stddevs.\n      state_dependent_std: If true, stddevs will be produced by MLP from state.\n        else, stddevs will be an independent variable.\n      scale_distribution: Whether or not to use a bijector chain to scale\n        distributions to match the sample spec. Note the TransformedDistribution\n        does not support certain operations required by some agents or policies\n        such as KL divergence calculations or Mode.\n      name: A string representing name of the network.\n    """"""\n    if len(tf.nest.flatten(sample_spec)) != 1:\n      raise ValueError(\'Normal Projection network only supports single spec \'\n                       \'samples.\')\n    self._scale_distribution = scale_distribution\n    output_spec = self._output_distribution_spec(sample_spec, name)\n    super(NormalProjectionNetwork, self).__init__(\n        # We don\'t need these, but base class requires them.\n        input_tensor_spec=None,\n        state_spec=(),\n        output_spec=output_spec,\n        name=name)\n\n    self._sample_spec = sample_spec\n    self._mean_transform = mean_transform\n    self._std_transform = std_transform\n    self._state_dependent_std = state_dependent_std\n\n    self._means_projection_layer = tf.keras.layers.Dense(\n        sample_spec.shape.num_elements(),\n        activation=activation_fn,\n        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n            scale=init_means_output_factor),\n        bias_initializer=tf.keras.initializers.Zeros(),\n        name=\'means_projection_layer\')\n\n    self._stddev_projection_layer = None\n    if self._state_dependent_std:\n      self._stddev_projection_layer = tf.keras.layers.Dense(\n          sample_spec.shape.num_elements(),\n          activation=activation_fn,\n          kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n              scale=init_means_output_factor),\n          bias_initializer=tf.keras.initializers.Constant(\n              value=std_bias_initializer_value),\n          name=\'stddev_projection_layer\')\n    else:\n      self._bias = bias_layer.BiasLayer(\n          bias_initializer=tf.keras.initializers.Constant(\n              value=std_bias_initializer_value))\n\n  def _output_distribution_spec(self, sample_spec, network_name):\n    input_param_shapes = tfp.distributions.Normal.param_static_shapes(\n        sample_spec.shape)\n\n    input_param_spec = {\n        name: tensor_spec.TensorSpec(  # pylint: disable=g-complex-comprehension\n            shape=shape,\n            dtype=sample_spec.dtype,\n            name=network_name + \'_\' + name)\n        for name, shape in input_param_shapes.items()\n    }\n\n    def distribution_builder(*args, **kwargs):\n      distribution = tfp.distributions.Normal(*args, **kwargs)\n      if self._scale_distribution:\n        return distribution_utils.scale_distribution_to_spec(\n            distribution, sample_spec)\n      return distribution\n\n    return distribution_spec.DistributionSpec(\n        distribution_builder, input_param_spec, sample_spec=sample_spec)\n\n  def call(self, inputs, outer_rank, training=False, mask=None):\n    if inputs.dtype != self._sample_spec.dtype:\n      raise ValueError(\n          \'Inputs to NormalProjectionNetwork must match the sample_spec.dtype.\')\n\n    if mask is not None:\n      raise NotImplementedError(\n          \'NormalProjectionNetwork does not yet implement action masking; got \'\n          \'mask={}\'.format(mask))\n\n    # outer_rank is needed because the projection is not done on the raw\n    # observations so getting the outer rank is hard as there is no spec to\n    # compare to.\n    batch_squash = network_utils.BatchSquash(outer_rank)\n    inputs = batch_squash.flatten(inputs)\n\n    means = self._means_projection_layer(inputs, training=training)\n    means = tf.reshape(means, [-1] + self._sample_spec.shape.as_list())\n\n    # If scaling the distribution later, use a normalized mean.\n    if not self._scale_distribution and self._mean_transform is not None:\n      means = self._mean_transform(means, self._sample_spec)\n    means = tf.cast(means, self._sample_spec.dtype)\n\n    if self._state_dependent_std:\n      stds = self._stddev_projection_layer(inputs, training=training)\n    else:\n      stds = self._bias(tf.zeros_like(means), training=training)\n      stds = tf.reshape(stds, [-1] + self._sample_spec.shape.as_list())\n\n    if self._std_transform is not None:\n      stds = self._std_transform(stds)\n    stds = tf.cast(stds, self._sample_spec.dtype)\n\n    means = batch_squash.unflatten(means)\n    stds = batch_squash.unflatten(stds)\n\n    return self.output_spec.build_distribution(loc=means, scale=stds), ()\n'"
tf_agents/networks/normal_projection_network_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.normal_projection_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import normal_projection_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\n\ndef _get_inputs(batch_size, num_input_dims):\n  return tf.random.uniform([batch_size, num_input_dims])\n\n\nclass NormalProjectionNetworkTest(tf.test.TestCase):\n\n  def testBuild(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = normal_projection_network.NormalProjectionNetwork(\n        output_spec, scale_distribution=False)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    distribution, _ = network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(tfp.distributions.Normal, type(distribution))\n\n    means, stds = distribution.loc, distribution.scale\n\n    self.assertAllEqual(means.shape.as_list(),\n                        [3] + output_spec.shape.as_list())\n    self.assertAllEqual(stds.shape.as_list(), [3] + output_spec.shape.as_list())\n\n  def testBuildStateDepStddev(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = normal_projection_network.NormalProjectionNetwork(\n        output_spec, state_dependent_std=True, scale_distribution=False)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    distribution, _ = network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(tfp.distributions.Normal, type(distribution))\n\n    means, stds = distribution.loc, distribution.scale\n\n    self.assertAllEqual(means.shape.as_list(),\n                        [3] + output_spec.shape.as_list())\n    self.assertAllEqual(stds.shape.as_list(), [3] + output_spec.shape.as_list())\n\n  def testTrainableVariables(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = normal_projection_network.NormalProjectionNetwork(output_spec)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Dense kernel, dense bias, std bias.\n    self.assertEqual(3, len(network.trainable_variables))\n    self.assertEqual((5, 2), network.trainable_variables[0].shape)\n    self.assertEqual((2,), network.trainable_variables[1].shape)\n    self.assertEqual((2,), network.trainable_variables[2].shape)\n\n  def testTrainableVariablesStateDepStddev(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = normal_projection_network.NormalProjectionNetwork(\n        output_spec, state_dependent_std=True)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Dense kernel, dense bias, std bias.\n    self.assertEqual(4, len(network.trainable_variables))\n    self.assertEqual((5, 2), network.trainable_variables[0].shape)\n    self.assertEqual((2,), network.trainable_variables[1].shape)\n    self.assertEqual((5, 2), network.trainable_variables[2].shape)\n    self.assertEqual((2,), network.trainable_variables[3].shape)\n\n  def testScaledDistribution(self):\n    output_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -2, 4)\n    network = normal_projection_network.NormalProjectionNetwork(\n        output_spec, init_means_output_factor=10, state_dependent_std=True,\n        scale_distribution=True)\n\n    inputs = _get_inputs(batch_size=100, num_input_dims=5)\n\n    distributions, _ = network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    sample = self.evaluate(distributions.sample())\n    clipped = self.evaluate(common.clip_to_spec(sample, output_spec))\n    np.testing.assert_almost_equal(clipped, sample)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/q_network.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Keras networks for DQN.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\n\n\ndef validate_specs(action_spec, observation_spec):\n  """"""Validates the spec contains a single action.""""""\n  del observation_spec  # not currently validated\n\n  flat_action_spec = tf.nest.flatten(action_spec)\n  if len(flat_action_spec) > 1:\n    raise ValueError(\'Network only supports action_specs with a single action.\')\n\n  if flat_action_spec[0].shape not in [(), (1,)]:\n    raise ValueError(\n        \'Network only supports action_specs with shape in [(), (1,)])\')\n\n\n@gin.configurable\nclass QNetwork(network.Network):\n  """"""Feed Forward network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               action_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=(75, 40),\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               batch_squash=True,\n               dtype=tf.float32,\n               name=\'QNetwork\'):\n    """"""Creates an instance of `QNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      action_spec: A nest of `tensor_spec.BoundedTensorSpec` representing the\n        actions.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, where\n        each item is the fraction of input units to drop. The dropout layers are\n        interleaved with the fully connected layers; there is a dropout layer\n        after each fully connected layer, except if the entry in the list is\n        None. This list must have the same length of fc_layer_params, or be\n        None.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu.\n      kernel_initializer: Initializer to use for the kernels of the conv and\n        dense layers. If none is provided a default variance_scaling_initializer\n      batch_squash: If True the outer_ranks of the observation are squashed into\n        the batch dimension. This allow encoding networks to be used with\n        observations with shape [BxTx...].\n      dtype: The dtype to use by the convolution and fully connected layers.\n      name: A string representing the name of the network.\n\n    Raises:\n      ValueError: If `input_tensor_spec` contains more than one observation. Or\n        if `action_spec` contains more than one action.\n    """"""\n    validate_specs(action_spec, input_tensor_spec)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    encoder_input_tensor_spec = input_tensor_spec\n\n    encoder = encoding_network.EncodingNetwork(\n        encoder_input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=fc_layer_params,\n        dropout_layer_params=dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        batch_squash=batch_squash,\n        dtype=dtype)\n\n    q_value_layer = tf.keras.layers.Dense(\n        num_actions,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03),\n        bias_initializer=tf.compat.v1.initializers.constant(-0.2),\n        dtype=dtype)\n\n    super(QNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    self._encoder = encoder\n    self._q_value_layer = q_value_layer\n\n  def call(self, observation, step_type=None, network_state=(), training=False):\n    """"""Runs the given observation through the network.\n\n    Args:\n      observation: The observation to provide to the network.\n      step_type: The step type for the given observation. See `StepType` in\n        time_step.py.\n      network_state: A state tuple to pass to the network, mainly used by RNNs.\n      training: Whether the output is being used for training.\n\n    Returns:\n      A tuple `(logits, network_state)`.\n    """"""\n    state, network_state = self._encoder(\n        observation, step_type=step_type, network_state=network_state,\n        training=training)\n    q_value = self._q_value_layer(state, training=training)\n    return q_value, network_state\n'"
tf_agents/networks/q_network_test.py,72,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.network.q_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import q_network\nfrom tf_agents.specs import tensor_spec\n\n\nclass SingleObservationSingleActionTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(SingleObservationSingleActionTest, self).setUp()\n    gin.clear_config()\n\n  def testBuild(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    q_values, _ = network(states)\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_weights), 6)\n\n  def testChangeHiddenLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        fc_layer_params=(40,))\n    q_values, _ = network(states)\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_variables), 4)\n\n  def testAddConvLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, 5, 5, num_state_dims])\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([5, 5, num_state_dims],\n                                                 tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        conv_layer_params=((16, 3, 2),))\n    q_values, _ = network(states)\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_variables), 8)\n\n  def testAddPreprocessingLayers(self):\n    batch_size = 3\n    num_actions = 2\n    states = (tf.random.uniform([batch_size, 1]),\n              tf.random.uniform([batch_size]))\n    preprocessing_layers = (\n        tf.keras.layers.Dense(4),\n        tf.keras.Sequential([\n            tf.keras.layers.Reshape((1,)),\n            tf.keras.layers.Dense(4)]))\n    network = q_network.QNetwork(\n        input_tensor_spec=(\n            tensor_spec.TensorSpec([1], tf.float32),\n            tensor_spec.TensorSpec([], tf.float32)),\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add(),\n        action_spec=tensor_spec.BoundedTensorSpec(\n            [1], tf.int32, 0, num_actions - 1))\n    q_values, _ = network(states)\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    # At least 2 variables each for the preprocessing layers.\n    self.assertGreater(len(network.trainable_variables), 4)\n\n  def testCorrectOutputShape(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    q_values, _ = network(states)\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n\n  def testNetworkVariablesAreReused(self):\n    batch_size = 3\n    num_state_dims = 5\n    states = tf.ones([batch_size, num_state_dims])\n    next_states = tf.ones([batch_size, num_state_dims])\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    q_values, _ = network(states)\n    next_q_values, _ = network(next_states)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(q_values, next_q_values)\n\n  def testNumericFeatureColumnInput(self):\n    key = \'feature_key\'\n    batch_size = 3\n    state_dims = 5\n    column = tf.feature_column.numeric_column(key, [state_dims])\n    state = {key: tf.ones([batch_size, state_dims], tf.int32)}\n    state_spec = {key: tensor_spec.TensorSpec([state_dims], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = q_network.QNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online, _ = online_network(state)\n    q_target, _ = target_network(state)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testIndicatorFeatureColumnInput(self):\n    key = \'feature_key\'\n    vocab_list = [2, 3, 4]\n    column = tf.feature_column.categorical_column_with_vocabulary_list(\n        key, vocab_list)\n    column = tf.feature_column.indicator_column(column)\n    feature_tensor = tf.convert_to_tensor([3, 2, 2, 4, 3])\n    state = {key: tf.expand_dims(feature_tensor, -1)}\n    state_spec = {key: tensor_spec.TensorSpec([1], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = q_network.QNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online, _ = online_network(state)\n    q_target, _ = target_network(state)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testEmbeddingFeatureColumnInput(self):\n    key = \'feature_key\'\n    vocab_list = [\'a\', \'b\']\n    column = tf.feature_column.categorical_column_with_vocabulary_list(\n        key, vocab_list)\n    column = tf.feature_column.embedding_column(column, 3)\n    feature_tensor = tf.convert_to_tensor([\'a\', \'b\', \'c\', \'a\', \'c\'])\n    state = {key: tf.expand_dims(feature_tensor, -1)}\n    state_spec = {key: tensor_spec.TensorSpec([1], tf.string)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = q_network.QNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online, _ = online_network(state)\n    q_target, _ = target_network(state)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testCombinedFeatureColumnInput(self):\n    columns = {}\n    state_tensors = {}\n    state_specs = {}\n    expected_dim = 0\n\n    indicator_key = \'indicator_key\'\n    vocab_list = [2, 3, 4]\n    column1 = tf.feature_column.categorical_column_with_vocabulary_list(\n        indicator_key, vocab_list)\n    columns[indicator_key] = tf.feature_column.indicator_column(column1)\n    state_tensors[indicator_key] = tf.expand_dims([3, 2, 2, 4, 3], -1)\n    state_specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32)\n    expected_dim += len(vocab_list)\n\n    embedding_key = \'embedding_key\'\n    embedding_dim = 3\n    vocab_list = [2, 3, 4]\n    column2 = tf.feature_column.categorical_column_with_vocabulary_list(\n        embedding_key, vocab_list)\n    columns[embedding_key] = tf.feature_column.embedding_column(\n        column2, embedding_dim)\n    state_tensors[embedding_key] = tf.expand_dims([3, 2, 2, 4, 3], -1)\n    state_specs[embedding_key] = tensor_spec.TensorSpec([1], tf.int32)\n    expected_dim += embedding_dim\n\n    numeric_key = \'numeric_key\'\n    batch_size = 5\n    state_dims = 3\n    input_shape = (batch_size, state_dims)\n    columns[numeric_key] = tf.feature_column.numeric_column(\n        numeric_key, [state_dims])\n    state_tensors[numeric_key] = tf.ones(input_shape, tf.int32)\n    state_specs[numeric_key] = tensor_spec.TensorSpec([state_dims], tf.int32)\n    expected_dim += state_dims\n\n    num_actions = 4\n    action_spec = tensor_spec.BoundedTensorSpec(\n        [1], tf.int32, 0, num_actions - 1)\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures(columns.values())\n    online_network = q_network.QNetwork(\n        state_specs, action_spec, preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online, _ = online_network(state_tensors)\n    q_target, _ = target_network(state_tensors)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n\n    expected_shape = (batch_size, num_actions)\n    self.assertEqual(expected_shape, q_online.shape)\n    self.assertEqual(expected_shape, q_target.shape)\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testPreprocessingLayersSingleObservations(self):\n    """"""Tests using preprocessing_layers without preprocessing_combiner.""""""\n    num_state_dims = 5\n    network = q_network.QNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_layers=tf.keras.layers.Lambda(lambda x: x),\n        preprocessing_combiner=None)\n    q_logits, _ = network(tf.ones((3, num_state_dims)))\n    self.assertAllEqual(q_logits.shape.as_list(), [3, 2])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/q_rnn_network.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample recurrent Keras network for DQN.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tensorflow.keras import layers\n\nfrom tf_agents.networks import lstm_encoding_network\nfrom tf_agents.networks import q_network\n\n\n@gin.configurable\nclass QRnnNetwork(lstm_encoding_network.LSTMEncodingNetwork):\n  """"""Recurrent network.""""""\n\n  def __init__(\n      self,\n      input_tensor_spec,\n      action_spec,\n      preprocessing_layers=None,\n      preprocessing_combiner=None,\n      conv_layer_params=None,\n      input_fc_layer_params=(75, 40),\n      lstm_size=(40,),\n      output_fc_layer_params=(75, 40),\n      activation_fn=tf.keras.activations.relu,\n      dtype=tf.float32,\n      name=\'QRnnNetwork\',\n  ):\n    """"""Creates an instance of `QRnnNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      action_spec: A nest of `tensor_spec.BoundedTensorSpec` representing the\n        actions.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them.  Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      input_fc_layer_params: Optional list of fully connected parameters, where\n        each item is the number of units in the layer. These feed into the\n        recurrent layer.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully connected parameters, where\n        each item is the number of units in the layer. These are applied on top\n        of the recurrent layer.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu,.\n      dtype: The dtype to use by the convolution, LSTM, and fully connected\n        layers.\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If any of `preprocessing_layers` is already built.\n      ValueError: If `preprocessing_combiner` is already built.\n      ValueError: If `action_spec` contains more than one action.\n    """"""\n    q_network.validate_specs(action_spec, input_tensor_spec)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    q_projection = layers.Dense(\n        num_actions,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03),\n        bias_initializer=tf.compat.v1.initializers.constant(-0.2),\n        dtype=dtype,\n        name=\'num_action_project/dense\')\n\n    super(QRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        input_fc_layer_params=input_fc_layer_params,\n        lstm_size=lstm_size,\n        output_fc_layer_params=output_fc_layer_params,\n        activation_fn=activation_fn,\n        dtype=dtype,\n        name=name)\n\n    self._output_encoder.append(q_projection)\n'"
tf_agents/networks/q_rnn_network_test.py,23,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.q_rnn_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import expand_dims_layer\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\n\n\nclass QRnnNetworkTest(tf.test.TestCase):\n\n  def test_network_builds(self):\n    env = suite_gym.load(\'CartPole-v0\')\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    rnn_network = q_rnn_network.QRnnNetwork(tf_env.observation_spec(),\n                                            tf_env.action_spec())\n\n    first_time_step = tf_env.current_time_step()\n    q_values, state = rnn_network(\n        first_time_step.observation, first_time_step.step_type,\n        network_state=rnn_network.get_initial_state(batch_size=1)\n    )\n    self.assertEqual((1, 2), q_values.shape)\n    self.assertEqual((1, 40), state[0].shape)\n    self.assertEqual((1, 40), state[1].shape)\n\n  def test_network_can_preprocess_and_combine(self):\n    batch_size = 3\n    frames = 5\n    num_actions = 2\n    lstm_size = 6\n    states = (tf.random.uniform([batch_size, frames, 1]),\n              tf.random.uniform([batch_size, frames]))\n    preprocessing_layers = (\n        tf.keras.layers.Dense(4),\n        tf.keras.Sequential([\n            expand_dims_layer.ExpandDims(-1),  # Convert to vec size (1,).\n            tf.keras.layers.Dense(4)]))\n    network = q_rnn_network.QRnnNetwork(\n        input_tensor_spec=(\n            tensor_spec.TensorSpec([1], tf.float32),\n            tensor_spec.TensorSpec([], tf.float32)),\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add(),\n        lstm_size=(lstm_size,),\n        action_spec=tensor_spec.BoundedTensorSpec(\n            [1], tf.int32, 0, num_actions - 1))\n    empty_step_type = tf.constant(\n        [[time_step.StepType.FIRST] * frames] * batch_size)\n    q_values, _ = network(states, empty_step_type,\n                          network_state=network.get_initial_state(batch_size))\n    self.assertAllEqual(\n        q_values.shape.as_list(), [batch_size, frames, num_actions])\n    # At least 2 variables each for the preprocessing layers.\n    self.assertGreater(len(network.trainable_variables), 4)\n\n  def test_network_can_preprocess_and_combine_no_time_dim(self):\n    batch_size = 3\n    num_actions = 2\n    lstm_size = 5\n    states = (tf.random.uniform([batch_size, 1]),\n              tf.random.uniform([batch_size]))\n    preprocessing_layers = (\n        tf.keras.layers.Dense(4),\n        tf.keras.Sequential([\n            expand_dims_layer.ExpandDims(-1),  # Convert to vec size (1,).\n            tf.keras.layers.Dense(4)]))\n    network = q_rnn_network.QRnnNetwork(\n        input_tensor_spec=(\n            tensor_spec.TensorSpec([1], tf.float32),\n            tensor_spec.TensorSpec([], tf.float32)),\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add(),\n        lstm_size=(lstm_size,),\n        action_spec=tensor_spec.BoundedTensorSpec(\n            [1], tf.int32, 0, num_actions - 1))\n    empty_step_type = tf.constant([time_step.StepType.FIRST] * batch_size)\n    q_values, _ = network(\n        states, empty_step_type,\n        network_state=network.get_initial_state(batch_size=batch_size))\n\n    # Processed 1 time step and the time axis was squeezed back.\n    self.assertAllEqual(\n        q_values.shape.as_list(), [batch_size, num_actions])\n\n    # At least 2 variables each for the preprocessing layers.\n    self.assertGreater(len(network.trainable_variables), 4)\n\n  def test_network_builds_stacked_cells(self):\n    env = suite_gym.load(\'CartPole-v0\')\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    rnn_network = q_rnn_network.QRnnNetwork(\n        tf_env.observation_spec(), tf_env.action_spec(), lstm_size=(10, 5))\n\n    first_time_step = tf_env.current_time_step()\n    q_values, state = rnn_network(\n        first_time_step.observation, first_time_step.step_type,\n        network_state=rnn_network.get_initial_state(batch_size=1)\n    )\n    tf.nest.assert_same_structure(rnn_network.state_spec, state)\n    self.assertEqual(2, len(state))\n\n    self.assertEqual((1, 2), q_values.shape)\n    self.assertEqual((1, 10), state[0][0].shape)\n    self.assertEqual((1, 10), state[0][1].shape)\n    self.assertEqual((1, 5), state[1][0].shape)\n    self.assertEqual((1, 5), state[1][1].shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/test_utils.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utility functions for testing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tf_agents.networks import network\n\n\nclass KerasLayersNet(network.Network):\n\n  def __init__(self, observation_spec, action_spec, layer, name=None):\n    super(KerasLayersNet, self).__init__(\n        observation_spec, state_spec=(), name=name)\n    self._layer = layer\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    return self._layer(inputs), network_state\n'"
tf_agents/networks/utils.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Network utilities.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import composite\n\n\ndef maybe_permanent_dropout(rate, noise_shape=None, seed=None, permanent=False):\n  """"""Adds a Keras dropout layer with the option of applying it at inference.\n\n  Args:\n    rate: the probability of dropping an input.\n    noise_shape: 1D integer tensor representing the dropout mask multiplied to\n      the input.\n    seed: A Python integer to use as random seed.\n    permanent: If set, applies dropout during inference and not only during\n      training. This flag is used for approximated Bayesian inference.\n  Returns:\n    A function adding a dropout layer according to the parameters for the given\n      input.\n  """"""\n  if permanent:\n    def _keras_dropout(x):\n      return tf.nn.dropout(\n          x, rate=rate, noise_shape=noise_shape, seed=seed)\n    return tf.keras.layers.Lambda(_keras_dropout)\n  return tf.keras.layers.Dropout(rate, noise_shape, seed)\n\n\nclass BatchSquash(object):\n  """"""Facilitates flattening and unflattening batch dims of a tensor.\n\n  Exposes a pair of matched faltten and unflatten methods. After flattening only\n  1 batch dimension will be left. This facilitates evaluating networks that\n  expect inputs to have only 1 batch dimension.\n  """"""\n\n  def __init__(self, batch_dims):\n    """"""Create two tied ops to flatten and unflatten the front dimensions.\n\n    Args:\n      batch_dims: Number of batch dimensions the flatten/unflatten ops should\n        handle.\n\n    Raises:\n      ValueError: if batch dims is negative.\n    """"""\n    if batch_dims < 0:\n      raise ValueError(\'Batch dims must be non-negative.\')\n    self._batch_dims = batch_dims\n    self._original_tensor_shape = None\n\n  def flatten(self, tensor):\n    """"""Flattens and caches the tensor\'s batch_dims.""""""\n    with tf.name_scope(\'batch_flatten\'):\n      if self._batch_dims == 1:\n        return tensor\n\n      self._original_tensor_shape = composite.shape(tensor)\n\n      if tensor.shape[self._batch_dims:].is_fully_defined():\n        return composite.reshape(\n            tensor, [-1] + tensor.shape[self._batch_dims:].as_list())\n\n      return tf.reshape(\n          tensor,\n          tf.concat([[-1], composite.shape(tensor)[self._batch_dims:]], axis=0),\n      )\n\n  def unflatten(self, tensor):\n    """"""Unflattens the tensor\'s batch_dims using the cached shape.""""""\n    with tf.name_scope(\'batch_unflatten\'):\n      if self._batch_dims == 1:\n        return tensor\n\n      if self._original_tensor_shape is None:\n        raise ValueError(\'Please call flatten before unflatten.\')\n\n      # pyformat: disable\n      return composite.reshape(\n          tensor,\n          tf.concat([\n              self._original_tensor_shape[:self._batch_dims],\n              composite.shape(tensor)[1:]], axis=0)\n      )\n      # pyformat: enable\n\n\ndef mlp_layers(conv_layer_params=None,\n               fc_layer_params=None,\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               weight_decay_params=None,\n               name=None):\n  """"""Generates conv and fc layers to encode into a hidden state.\n\n  Args:\n    conv_layer_params: Optional list of convolution layers parameters, where\n      each item is a length-three tuple indicating (filters, kernel_size,\n      stride).\n    fc_layer_params: Optional list of fully_connected parameters, where each\n      item is the number of units in the layer.\n    dropout_layer_params: Optional list of dropout layer parameters, each item\n      is the fraction of input units to drop or a dictionary of parameters\n      according to the keras.Dropout documentation. The additional parameter\n      `permanent\', if set to True, allows to apply dropout at inference for\n      approximated Bayesian inference. The dropout layers are interleaved with\n      the fully connected layers; there is a dropout layer after each fully\n      connected layer, except if the entry in the list is None. This list must\n      have the same length of fc_layer_params, or be None.\n    activation_fn: Activation function, e.g. tf.keras.activations.relu,.\n    kernel_initializer: Initializer to use for the kernels of the conv and\n      dense layers. If none is provided a default variance_scaling_initializer\n      is used.\n    weight_decay_params: Optional list of weight decay params for the fully\n      connected layer.\n    name: Name for the mlp layers.\n\n  Returns:\n    List of mlp layers.\n\n  Raises:\n    ValueError: If the number of dropout layer parameters does not match the\n      number of fully connected layer parameters.\n  """"""\n  if kernel_initializer is None:\n    kernel_initializer = tf.compat.v1.variance_scaling_initializer(\n        scale=2.0, mode=\'fan_in\', distribution=\'truncated_normal\')\n\n  layers = []\n\n  if conv_layer_params is not None:\n    layers.extend([\n        tf.keras.layers.Conv2D(\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            activation=activation_fn,\n            kernel_initializer=kernel_initializer,\n            name=\'/\'.join([name, \'conv2d\']) if name else None)\n        for (filters, kernel_size, strides) in conv_layer_params\n    ])\n  layers.append(tf.keras.layers.Flatten())\n\n  if fc_layer_params is not None:\n    if dropout_layer_params is None:\n      dropout_layer_params = [None] * len(fc_layer_params)\n    else:\n      if len(dropout_layer_params) != len(fc_layer_params):\n        raise ValueError(\'Dropout and full connected layer parameter lists have\'\n                         \' different lengths (%d vs. %d.)\' %\n                         (len(dropout_layer_params), len(fc_layer_params)))\n\n    if weight_decay_params is None:\n      weight_decay_params = [None] * len(fc_layer_params)\n    else:\n      if len(weight_decay_params) != len(fc_layer_params):\n        raise ValueError(\'Weight decay and fully connected layer parameter \'\n                         \'lists have different lengths (%d vs. %d.)\' %\n                         (len(weight_decay_params), len(fc_layer_params)))\n\n    for num_units, dropout_params, weight_decay in zip(\n        fc_layer_params, dropout_layer_params, weight_decay_params):\n      kernel_regularizer = None\n      if weight_decay is not None:\n        kernel_regularizer = tf.keras.regularizers.l2(weight_decay)\n      layers.append(tf.keras.layers.Dense(\n          num_units,\n          activation=activation_fn,\n          kernel_initializer=kernel_initializer,\n          kernel_regularizer=kernel_regularizer,\n          name=\'/\'.join([name, \'dense\']) if name else None))\n      if not isinstance(dropout_params, dict):\n        dropout_params = {\'rate\': dropout_params} if dropout_params else None\n\n      if dropout_params is not None:\n        layers.append(maybe_permanent_dropout(**dropout_params))\n\n  return layers\n'"
tf_agents/networks/utils_test.py,25,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests tf_agents.utils.network_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import utils\n\n\ndef _to_dense(st):\n  return tf.scatter_nd(st.indices, st.values, st.dense_shape)\n\n\nclass NetworkUtilsTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(NetworkUtilsTest, self).setUp()\n    self._tensors = [\n        tf.constant(0, shape=(5, 4, 3, 2, 1)),\n        tf.SparseTensor(\n            indices=tf.zeros((0, 5), dtype=tf.int64),\n            values=tf.zeros((0,), dtype=tf.float32),\n            dense_shape=tf.constant([5, 4, 3, 2, 1], dtype=tf.int64))\n    ]\n\n  def test_flatten_and_unflatten_ops(self):\n    batch_squash = utils.BatchSquash(2)\n    for tensor in self._tensors:\n      flat = batch_squash.flatten(tensor)\n      unflat = batch_squash.unflatten(flat)\n      self.assertAllEqual((20, 3, 2, 1), flat.shape)\n      self.assertAllEqual((5, 4, 3, 2, 1), unflat.shape)\n\n  def test_flatten_and_unflatten_ops_no_batch_dims(self):\n    batch_squash = utils.BatchSquash(0)\n\n    for tensor in self._tensors:\n      flat = batch_squash.flatten(tensor)\n      unflat = batch_squash.unflatten(flat)\n\n      self.assertAllEqual((1, 5, 4, 3, 2, 1), flat.shape)\n      self.assertAllEqual((5, 4, 3, 2, 1), unflat.shape)\n\n  def test_flatten_and_unflatten_ops_one_batch_dims(self):\n    batch_squash = utils.BatchSquash(1)\n\n    for tensor in self._tensors:\n      flat = batch_squash.flatten(tensor)\n      unflat = batch_squash.unflatten(flat)\n\n      self.assertAllEqual((5, 4, 3, 2, 1), flat.shape)\n      self.assertAllEqual((5, 4, 3, 2, 1), unflat.shape)\n\n  def test_mlp_layers(self):\n    layers = utils.mlp_layers(conv_layer_params=[(3, 4, 5), (4, 6, 8)],\n                              fc_layer_params=[10, 20],\n                              activation_fn=tf.keras.activations.tanh,\n                              name=\'testnet\')\n    self.assertEqual(5, len(layers))\n\n    self.assertAllEqual([tf.keras.layers.Conv2D, tf.keras.layers.Conv2D,\n                         tf.keras.layers.Flatten, tf.keras.layers.Dense,\n                         tf.keras.layers.Dense],\n                        [type(layer) for layer in layers])\n\n    layers = utils.mlp_layers(conv_layer_params=[(3, 4, 5), (4, 6, 8)],\n                              fc_layer_params=[10, 20],\n                              activation_fn=tf.keras.activations.tanh,\n                              dropout_layer_params=[0.5, 0.3],\n                              name=\'testnet\')\n    self.assertEqual(7, len(layers))\n\n    self.assertAllEqual([tf.keras.layers.Conv2D, tf.keras.layers.Conv2D,\n                         tf.keras.layers.Flatten, tf.keras.layers.Dense,\n                         tf.keras.layers.Dropout, tf.keras.layers.Dense,\n                         tf.keras.layers.Dropout],\n                        [type(layer) for layer in layers])\n\n    layers = utils.mlp_layers(conv_layer_params=[(3, 4, 5), (4, 6, 8)],\n                              fc_layer_params=[10, 20],\n                              activation_fn=tf.keras.activations.tanh,\n                              dropout_layer_params=[None, 0.3],\n                              name=\'testnet\')\n    self.assertEqual(6, len(layers))\n\n    self.assertAllEqual([tf.keras.layers.Conv2D, tf.keras.layers.Conv2D,\n                         tf.keras.layers.Flatten, tf.keras.layers.Dense,\n                         tf.keras.layers.Dense, tf.keras.layers.Dropout],\n                        [type(layer) for layer in layers])\n\n    layers = utils.mlp_layers(conv_layer_params=[(3, 4, 5), (4, 6, 8)],\n                              fc_layer_params=[10, 20],\n                              activation_fn=tf.keras.activations.tanh,\n                              dropout_layer_params=[\n                                  dict(rate=0.5, permanent=True), None],\n                              name=\'testnet\')\n    self.assertEqual(6, len(layers))\n\n    self.assertAllEqual([tf.keras.layers.Conv2D, tf.keras.layers.Conv2D,\n                         tf.keras.layers.Flatten, tf.keras.layers.Dense,\n                         tf.keras.layers.Lambda, tf.keras.layers.Dense],\n                        [type(layer) for layer in layers])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/value_network.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Keras Value Network.\n\nImplements a network that will generate the following layers:\n\n  [optional]: preprocessing_layers  # preprocessing_layers\n  [optional]: (Add | Concat(axis=-1) | ...)  # preprocessing_combiner\n  [optional]: Conv2D # conv_layer_params\n  Flatten\n  [optional]: Dense  # fc_layer_params\n  Dense -> 1         # Value output\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\n\n\n@gin.configurable\nclass ValueNetwork(network.Network):\n  """"""Feed Forward value network. Reduces to 1 value output per batch item.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=(75, 40),\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               batch_squash=True,\n               dtype=tf.float32,\n               name=\'ValueNetwork\'):\n    """"""Creates an instance of `ValueNetwork`.\n\n    Network supports calls with shape outer_rank + observation_spec.shape. Note\n    outer_rank must be at least 1.\n\n    Args:\n      input_tensor_spec: A `tensor_spec.TensorSpec` or a tuple of specs\n        representing the input observations.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, each item\n        is the fraction of input units to drop or a dictionary of parameters\n        according to the keras.Dropout documentation. The additional parameter\n        `permanent\', if set to True, allows to apply dropout at inference for\n        approximated Bayesian inference. The dropout layers are interleaved with\n        the fully connected layers; there is a dropout layer after each fully\n        connected layer, except if the entry in the list is None. This list must\n        have the same length of fc_layer_params, or be None.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu,.\n      kernel_initializer: Initializer to use for the kernels of the conv and\n        dense layers. If none is provided a default variance_scaling_initializer\n      batch_squash: If True the outer_ranks of the observation are squashed into\n        the batch dimension. This allow encoding networks to be used with\n        observations with shape [BxTx...].\n      dtype: The dtype to use by the convolution and fully connected layers.\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If input_tensor_spec is not an instance of network.InputSpec.\n    """"""\n    super(ValueNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    if not kernel_initializer:\n      kernel_initializer = tf.compat.v1.keras.initializers.glorot_uniform()\n\n    self._encoder = encoding_network.EncodingNetwork(\n        input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=fc_layer_params,\n        dropout_layer_params=dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        batch_squash=batch_squash,\n        dtype=dtype)\n\n    self._postprocessing_layers = tf.keras.layers.Dense(\n        1,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03))\n\n  def call(self, observation, step_type=None, network_state=(), training=False):\n    state, network_state = self._encoder(\n        observation, step_type=step_type, network_state=network_state,\n        training=training)\n    value = self._postprocessing_layers(state, training=training)\n    return tf.squeeze(value, -1), network_state\n'"
tf_agents/networks/value_network_test.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.network.value_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import value_network\nfrom tf_agents.specs import tensor_spec\n\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\nclass ValueNetworkTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testBuilds(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    observation = tensor_spec.sample_spec_nest(\n        observation_spec, outer_dims=(1,))\n\n    net = value_network.ValueNetwork(\n        observation_spec, conv_layer_params=[(4, 2, 2)], fc_layer_params=(5,))\n\n    value, _ = net(observation)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.assertEqual([1], value.shape.as_list())\n\n    self.assertEqual(6, len(net.variables))\n    # Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Fc Kernel\n    self.assertEqual((64, 5), net.variables[2].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[3].shape)\n    # Value Shrink Kernel\n    self.assertEqual((5, 1), net.variables[4].shape)\n    # Value Shrink bias\n    self.assertEqual((1,), net.variables[5].shape)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testHandlesExtraOuterDims(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    observation = tensor_spec.sample_spec_nest(\n        observation_spec, outer_dims=(3, 3, 2))\n\n    net = value_network.ValueNetwork(\n        observation_spec, conv_layer_params=[(4, 2, 2)], fc_layer_params=(5,))\n\n    value, _ = net(observation)\n    self.assertEqual([3, 3, 2], value.shape.as_list())\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testHandlePreprocessingLayers(self):\n    observation_spec = (tensor_spec.TensorSpec([1], tf.float32),\n                        tensor_spec.TensorSpec([], tf.float32))\n    observation = tensor_spec.sample_spec_nest(\n        observation_spec, outer_dims=(3,))\n\n    preprocessing_layers = (tf.keras.layers.Dense(4),\n                            tf.keras.Sequential([\n                                tf.keras.layers.Reshape((1,)),\n                                tf.keras.layers.Dense(4)\n                            ]))\n\n    net = value_network.ValueNetwork(\n        observation_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add())\n\n    value, _ = net(observation)\n    self.assertEqual([3], value.shape.as_list())\n    self.assertGreater(len(net.trainable_variables), 4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/networks/value_rnn_network.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Keras Value Network with LSTM cells .\n\nImplements a network that will generate the following layers:\n\n  [optional]: preprocessing_layers  # preprocessing_layers\n  [optional]: (Add | Concat(axis=-1) | ...)  # preprocessing_combiner\n  [optional]: Conv2D # conv_layer_params\n  Flatten\n  [optional]: Dense  # input_fc_layer_params\n  [optional]: LSTM   # lstm_cell_params\n  [optional]: Dense  # output_fc_layer_params\n  Dense -> 1         # Value output\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import lstm_encoding_network\nfrom tf_agents.networks import network\n\n\n@gin.configurable\nclass ValueRnnNetwork(network.Network):\n  """"""Recurrent value network. Reduces to 1 value output per batch item.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               input_fc_layer_params=(75, 40),\n               input_dropout_layer_params=None,\n               lstm_size=(40,),\n               output_fc_layer_params=(75, 40),\n               activation_fn=tf.keras.activations.relu,\n               dtype=tf.float32,\n               name=\'ValueRnnNetwork\'):\n    """"""Creates an instance of `ValueRnnNetwork`.\n\n    Network supports calls with shape outer_rank + input_tensor_shape.shape.\n    Note outer_rank must be at least 1.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations.\n        All of these layers must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them.  Good options include\n        `tf.keras.layers.Add` and `tf.keras.layers.Concatenate(axis=-1)`.\n        This layer must not be already built. For more details see\n        the documentation of `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      input_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied before\n        the LSTM cell.\n      input_dropout_layer_params: Optional list of dropout layer parameters,\n        where each item is the fraction of input units to drop. The dropout\n        layers are interleaved with the fully connected layers; there is a\n        dropout layer after each fully connected layer, except if the entry in\n        the list is None. This list must have the same length of\n        input_fc_layer_params, or be None.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied after the\n        LSTM cell.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu,.\n      dtype: The dtype to use by the convolution, LSTM, and fully connected\n        layers.\n      name: A string representing name of the network.\n    """"""\n    del input_dropout_layer_params\n\n    lstm_encoder = lstm_encoding_network.LSTMEncodingNetwork(\n        input_tensor_spec=input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        input_fc_layer_params=input_fc_layer_params,\n        lstm_size=lstm_size,\n        output_fc_layer_params=output_fc_layer_params,\n        activation_fn=activation_fn,\n        dtype=dtype,\n        name=name)\n\n    postprocessing_layers = tf.keras.layers.Dense(\n        1,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03))\n\n    super(ValueRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=lstm_encoder.state_spec,\n        name=name)\n\n    self._lstm_encoder = lstm_encoder\n    self._postprocessing_layers = postprocessing_layers\n\n  def call(self,\n           observation,\n           step_type=None,\n           network_state=(),\n           training=False):\n    state, network_state = self._lstm_encoder(\n        observation, step_type=step_type, network_state=network_state,\n        training=training)\n    value = self._postprocessing_layers(state, training=training)\n    return tf.squeeze(value, -1), network_state\n'"
tf_agents/networks/value_rnn_network_test.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.network.value_rnn_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import value_rnn_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\n\n\nclass ValueRnnNetworkTest(tf.test.TestCase):\n\n  def testBuilds(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1, 3))\n\n    net = value_rnn_network.ValueRnnNetwork(\n        observation_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        lstm_size=(7,),\n        output_fc_layer_params=(3,))\n\n    value, state = net(time_step.observation, step_type=time_step.step_type,\n                       network_state=net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    self.assertEqual((1, 3), value.shape)\n\n    self.assertEqual(11, len(net.variables))\n    # Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Fc Kernel\n    self.assertEqual((64, 5), net.variables[2].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[3].shape)\n    # LSTM Cell Kernel\n    self.assertEqual((5, 28), net.variables[4].shape)\n    # LSTM Cell Recurrent Kernel\n    self.assertEqual((7, 28), net.variables[5].shape)\n    # LSTM Cell Bias\n    self.assertEqual((28,), net.variables[6].shape)\n    # Fc Kernel\n    self.assertEqual((7, 3), net.variables[7].shape)\n    # Fc Bias\n    self.assertEqual((3,), net.variables[8].shape)\n    # Value Shrink Kernel\n    self.assertEqual((3, 1), net.variables[9].shape)\n    # Value Shrink bias\n    self.assertEqual((1,), net.variables[10].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 7), state[0].shape)\n    self.assertEqual((1, 7), state[1].shape)\n\n  def testBuildsStackedLstm(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1, 3))\n\n    net = value_rnn_network.ValueRnnNetwork(\n        observation_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        lstm_size=(7, 5),\n        output_fc_layer_params=(3,))\n\n    _, state = net(time_step.observation,\n                   step_type=time_step.step_type,\n                   network_state=net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 7), state[0][0].shape)\n    self.assertEqual((1, 7), state[0][1].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 5), state[1][0].shape)\n    self.assertEqual((1, 5), state[1][1].shape)\n\n  def testHandleBatchOnlyObservation(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(3,))\n\n    net = value_rnn_network.ValueRnnNetwork(\n        observation_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        lstm_size=(7, 5),\n        output_fc_layer_params=(3,))\n\n    value, _ = net(time_step.observation,\n                   step_type=time_step.step_type,\n                   network_state=net.get_initial_state(batch_size=3))\n    self.assertEqual([3], value.shape.as_list())\n\n  def testHandlePreprocessingLayers(self):\n    observation_spec = (tensor_spec.TensorSpec([1], tf.float32),\n                        tensor_spec.TensorSpec([], tf.float32))\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(2, 3))\n\n    preprocessing_layers = (tf.keras.layers.Dense(4),\n                            tf.keras.Sequential([\n                                tf.keras.layers.Reshape((1,)),\n                                tf.keras.layers.Dense(4)\n                            ]))\n\n    net = value_rnn_network.ValueRnnNetwork(\n        observation_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add())\n\n    value, _ = net(time_step.observation,\n                   step_type=time_step.step_type,\n                   network_state=net.get_initial_state(batch_size=2))\n    self.assertEqual([2, 3], value.shape.as_list())\n    self.assertGreater(len(net.trainable_variables), 4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policies Module.""""""\n\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import boltzmann_policy\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import fixed_policy\nfrom tf_agents.policies import gaussian_policy\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import ou_noise_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import q_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.policies import scripted_py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.policies import tf_py_policy\n\n'"
tf_agents/policies/actor_policy.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Actor Policy based on an actor network.\n\nThis is used in e.g. actor-critic algorithms like DDPG.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import network\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import tensor_normalizer\n\n\n@gin.configurable\nclass ActorPolicy(tf_policy.TFPolicy):\n  """"""Class to build Actor Policies.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               actor_network: network.Network,\n               policy_state_spec: types.NestedTensorSpec = (),\n               info_spec: types.NestedTensorSpec = (),\n               observation_normalizer: Optional[\n                   tensor_normalizer.TensorNormalizer] = None,\n               clip: bool = True,\n               training: bool = False,\n               observation_and_action_constraint_splitter: Optional[\n                   types.Splitter] = None,\n               name: Optional[Text] = None):\n    """"""Builds an Actor Policy given an actor network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      actor_network: An instance of a `tf_agents.networks.network.Network` to be\n        used by the policy. The network will be called with `call(observation,\n        step_type, policy_state)` and should return `(actions_or_distributions,\n        new_state)`.\n      policy_state_spec: A nest of TensorSpec representing the policy_state.\n        If not set, defaults to actor_network.state_spec.\n      info_spec: A nest of `TensorSpec` representing the policy info.\n      observation_normalizer: An object to use for observation normalization.\n      clip: Whether to clip actions to spec before returning them. Default True.\n        Most policy-based algorithms (PCL, PPO, REINFORCE) use unclipped\n        continuous actions for training.\n      training: Whether the network should be called in training mode.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `actor_network` is compatible with the\n        network-specific half of the output of the\n        `observation_and_action_constraint_splitter`. In particular,\n        `observation_and_action_constraint_splitter` will be called on the\n        observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: if `actor_network` is not of type `network.Network`.\n      NotImplementedError: if `observation_and_action_constraint_splitter` is\n        not None but `action_spec` is not discrete.\n    """"""\n    if not isinstance(actor_network, network.Network):\n      raise ValueError(\'actor_network must be a network.Network. Found \'\n                       \'{}.\'.format(type(actor_network)))\n    actor_network.create_variables()\n    self._actor_network = actor_network\n    self._observation_normalizer = observation_normalizer\n    self._training = training\n\n    if observation_and_action_constraint_splitter is not None:\n      if len(tf.nest.flatten(action_spec)) > 1 or (\n          not tensor_spec.is_discrete(action_spec)):\n        raise NotImplementedError(\n            \'Action constraints for ActorPolicy are currently only supported \'\n            \'for a single spec of discrete actions. Got action_spec {}\'.format(\n                action_spec))\n\n    if not policy_state_spec:\n      policy_state_spec = actor_network.state_spec\n\n    super(ActorPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy_state_spec=policy_state_spec,\n        info_spec=info_spec,\n        clip=clip,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n\n  def _apply_actor_network(self, observation, step_type, policy_state,\n                           mask=None):\n    if self._observation_normalizer:\n      observation = self._observation_normalizer.normalize(observation)\n    if mask is None:\n      return self._actor_network(\n          observation, step_type, policy_state, training=self._training)\n    else:\n      return self._actor_network(\n          observation, step_type, policy_state, training=self._training,\n          mask=mask)\n\n  @property\n  def observation_normalizer(\n      self) -> Optional[tensor_normalizer.TensorNormalizer]:\n    return self._observation_normalizer\n\n  def _variables(self):\n    return self._actor_network.variables\n\n  def _distribution(self, time_step, policy_state):\n    observation_and_action_constraint_splitter = (\n        self.observation_and_action_constraint_splitter)\n    network_observation = time_step.observation\n    mask = None\n\n    if observation_and_action_constraint_splitter is not None:\n      network_observation, mask = observation_and_action_constraint_splitter(\n          network_observation)\n\n    # Actor network outputs nested structure of distributions or actions.\n    actions_or_distributions, policy_state = self._apply_actor_network(\n        network_observation, time_step.step_type, policy_state, mask=mask)\n\n    def _to_distribution(action_or_distribution):\n      if isinstance(action_or_distribution, tf.Tensor):\n        # This is an action tensor, so wrap it in a deterministic distribution.\n        return tfp.distributions.Deterministic(loc=action_or_distribution)\n      return action_or_distribution\n\n    distributions = tf.nest.map_structure(_to_distribution,\n                                          actions_or_distributions)\n    return policy_step.PolicyStep(distributions, policy_state)\n'"
tf_agents/policies/actor_policy_test.py,40,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.policies.actor_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActionNet(network.Network):\n\n  def __init__(self, input_tensor_spec, output_tensor_spec):\n    super(DummyActionNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=\'DummyActionNet\')\n    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    self._output_tensor_spec = output_tensor_spec\n    self._sub_layers = [\n        tf.keras.layers.Dense(\n            single_action_spec.shape.num_elements(),\n            activation=tf.nn.tanh,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]),\n        ),\n    ]\n\n  def call(self, observations, step_type, network_state):\n    del step_type\n\n    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    for layer in self._sub_layers:\n      states = layer(states)\n\n    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n    means = tf.reshape(states, [-1] + single_action_spec.shape.as_list())\n    spec_means = (single_action_spec.maximum + single_action_spec.minimum) / 2.0\n    spec_ranges = (\n        single_action_spec.maximum - single_action_spec.minimum) / 2.0\n    action_means = spec_means + spec_ranges * means\n\n    return (tf.nest.pack_sequence_as(self._output_tensor_spec, [action_means]),\n            network_state)\n\n\nclass DummyActionDistributionNet(DummyActionNet):\n\n  def call(self, observations, step_type, network_state):\n    action_means, network_state = super(DummyActionDistributionNet, self).call(\n        observations, step_type, network_state)\n\n    def _action_distribution(action_mean):\n      action_std = tf.ones_like(action_mean)\n      return tfp.distributions.Normal(action_mean, action_std)\n\n    return tf.nest.map_structure(_action_distribution,\n                                 action_means), network_state\n\n\ndef test_cases():\n  return parameterized.named_parameters({\n      \'testcase_name\': \'SimpleNet\',\n      \'network_ctor\': DummyActionNet,\n  }, {\n      \'testcase_name\': \'DistributionNet\',\n      \'network_ctor\': DummyActionDistributionNet,\n  })\n\n\nclass ActorPolicyTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(ActorPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2, 3)\n\n  @property\n  def _time_step(self):\n    return ts.restart(tf.constant([1, 2], dtype=tf.float32))\n\n  @property\n  def _time_step_batch(self):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[2], name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\'observation\'))\n\n  @test_cases()\n  def testBuild(self, network_ctor):\n    actor_network = network_ctor(self._obs_spec, self._action_spec)\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n    self.assertLen(policy.variables(), 2)\n\n  @test_cases()\n  def testActionBatch(self, network_ctor):\n    actor_network = network_ctor(self._obs_spec, self._action_spec)\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  def testUpdate(self):\n    tf.compat.v1.set_random_seed(1)\n    actor_network = DummyActionNet(self._obs_spec, self._action_spec)\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n    self.assertLen(policy.variables(), 2)\n    new_policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    action_step = policy.action(self._time_step_batch)\n    self.assertLen(policy.variables(), 2)\n    new_action_step = new_policy.action(self._time_step_batch)\n    self.assertLen(new_policy.variables(), 2)\n\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(new_policy.update(policy))\n    actions_, new_actions_ = self.evaluate(\n        [action_step.action, new_action_step.action])\n    self.assertAllEqual(actions_, new_actions_)\n\n  def testDeterministicDistribution(self):\n    actor_network = DummyActionNet(self._obs_spec, self._action_spec)\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    action_step = policy.action(self._time_step_batch)\n    distribution_step = policy.distribution(self._time_step_batch)\n    self.assertIsInstance(distribution_step.action,\n                          tfp.distributions.Deterministic)\n    distribution_mean = distribution_step.action.mean()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    distribution_mean_ = self.evaluate(distribution_mean)\n    self.assertNear(actions_[0], distribution_mean_[0], 1e-6)\n\n  def testGaussianDistribution(self):\n    actor_network = DummyActionDistributionNet(self._obs_spec,\n                                               self._action_spec)\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    distribution_step = policy.distribution(self._time_step_batch)\n    self.assertIsInstance(distribution_step.action, tfp.distributions.Normal)\n\n\nclass ActorPolicyDiscreteActionsTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(ActorPolicyDiscreteActionsTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 7)\n\n  @property\n  def _time_step(self):\n    return ts.restart(tf.constant([1, 2], dtype=tf.float32))\n\n  @property\n  def _time_step_batch(self):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[2], name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\'observation\'))\n\n  def testBuild(self):\n    actor_network = actor_distribution_network.ActorDistributionNetwork(\n        self._obs_spec, self._action_spec, fc_layer_params=(2, 1))\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n  def testActionBatch(self):\n    actor_network = actor_distribution_network.ActorDistributionNetwork(\n        self._obs_spec, self._action_spec, fc_layer_params=(2, 1))\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, self._action_spec.dtype)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  def testActionDistribution(self):\n    actor_network = actor_distribution_network.ActorDistributionNetwork(\n        self._obs_spec, self._action_spec, fc_layer_params=(2, 1))\n    policy = actor_policy.ActorPolicy(\n        self._time_step_spec, self._action_spec, actor_network=actor_network)\n\n    # Force creation of variables before global_variables_initializer.\n    policy.variables()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    distribution = policy.distribution(self._time_step_batch)\n    actions_ = self.evaluate(distribution.action.sample())\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  def testMasking(self):\n    batch_size = 1000\n    num_state_dims = 5\n    num_actions = 8\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_step = ts.restart(observations, batch_size=batch_size)\n    input_tensor_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    time_step_spec = ts.time_step_spec(input_tensor_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        [1], tf.int32, 0, num_actions - 1)\n\n    # We create a fixed mask here for testing purposes. Normally the mask would\n    # be part of the observation.\n    mask = [0, 1, 0, 1, 0, 0, 1, 0]\n    np_mask = np.array(mask)\n    tf_mask = tf.constant([mask for _ in range(batch_size)])\n    actor_network = actor_distribution_network.ActorDistributionNetwork(\n        input_tensor_spec, action_spec, fc_layer_params=(2, 1))\n    policy = actor_policy.ActorPolicy(\n        time_step_spec, action_spec, actor_network=actor_network,\n        observation_and_action_constraint_splitter=(\n            lambda observation: (observation, tf_mask)))\n\n    # Force creation of variables before global_variables_initializer.\n    policy.variables()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Sample from the policy 1000 times, and ensure that actions considered\n    # invalid according to the mask are never chosen.\n    action_step = policy.action(time_step)\n    action = self.evaluate(action_step.action)\n    self.assertEqual(action.shape, (batch_size, 1))\n    self.assertAllEqual(np_mask[action], np.ones([batch_size, 1]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/async_policy_saver.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Async helper for the policy saver.""""""\n\nimport threading\nfrom typing import Text\nfrom absl import logging\n\nfrom tf_agents.policies import policy_saver as policy_saver_module\n\n\nclass AsyncPolicySaver(object):\n  """"""Triggers `policy_saver` save calls in a separate thread asynchronously.""""""\n\n  def __init__(self, policy_saver: policy_saver_module.PolicySaver):\n    """"""Initialize an AsyncPolicySaver.\n\n    Args:\n      policy_saver: An instance of a `policy_saver.PolicySaver`.\n    """"""\n    self._policy_saver = policy_saver\n    self._save_condition_variable = threading.Condition()\n\n    # These vars should only be accessed if the lock in save_condition is held.\n    # export_dir is set to None whenever there is no pending save. Otherwise it\n    # is used to communicate across threads.\n    self._export_dir = None\n    self._saving_checkpoint = False\n    self._join_save_thread = False\n\n    self._save_thread = threading.Thread(target=self._save_loop)\n    self._save_thread.start()\n\n  def _save_loop(self):\n    """"""Helper method for the saving thread to wait and execute save requests.""""""\n    while True:\n      with self._save_condition_variable:\n        while not self._export_dir:\n          self._save_condition_variable.wait()\n          if self._join_save_thread:\n            return\n        if self._saving_checkpoint:\n          logging.info(""Saving checkpoint to %s"", self._export_dir)\n          self._policy_saver.save_checkpoint(self._export_dir)\n        else:\n          logging.info(""Saving policy to %s"", self._export_dir)\n          self._policy_saver.save(self._export_dir)\n        self._export_dir = None\n        self._save_condition_variable.notify()\n\n  def _assert_save_thread_is_alive(self):\n    if self._join_save_thread or not self._save_thread.is_alive():\n      raise ValueError(""Saving thread in AsyncPolicySaver is not alive. Either ""\n                       ""an exception has occured while saving, or the saver ""\n                       ""was closed."")\n\n  def save(self, export_dir: Text, blocking: bool = False):\n    """"""Triggers an async save of the policy to the given `export_dir`.\n\n    Only one save can be triggered at a time. If `save` or `save_checkpoint`\n    are called while another save of either kind is still ongoing the saving is\n    skipped.\n\n    If blocking is set then the call will block until any ongoing saves finish,\n    and then a new save will be made before returning.\n\n    Args:\n      export_dir: Directory path for the `saved_model` of the policy.\n      blocking: If True the call to save will block until a save can be\n        performed and finished. If a save was ongoing it will wait for that to\n        finish, and then do a blocking save before returning.\n    """"""\n    self._save(export_dir, saving_checkpoint=False, blocking=blocking)\n\n  def save_checkpoint(self, export_dir: Text, blocking: bool = False):\n    """"""Triggers an async save of the policy checkpoint.\n\n    Only one save can be triggered at a time. If `save` or `save_checkpoint`\n    are called while another save of either kind is still ongoing the saving is\n    skipped.\n\n    If blocking is set then the call will block until any ongoing saves finish,\n    and then a new save will be made before returning.\n\n    Args:\n      export_dir: Directory path for the checkpoint of the policy.\n      blocking: If True the call to save will block until a save can be\n        performed and finished. If a save was ongoing it will wait for that to\n        finish, and then do a blocking save before returning.\n    """"""\n    self._save(export_dir, saving_checkpoint=True, blocking=blocking)\n\n  def _save(self, export_dir, saving_checkpoint, blocking):\n    """"""Helper save method, generalizes over save and save_checkpoint.""""""\n    self._assert_save_thread_is_alive()\n\n    if blocking:\n      with self._save_condition_variable:\n        while self._export_dir:\n          logging.info(""Waiting for AsyncPolicySaver to finish."")\n          self._save_condition_variable.wait()\n        if saving_checkpoint:\n          self._policy_saver.save_checkpoint(export_dir)\n        else:\n          self._policy_saver.save(export_dir)\n      return\n\n    if not self._save_condition_variable.acquire(blocking=False):\n      logging.info(""AsyncPolicySaver save is still in progress skipping save."")\n      return\n    try:\n      self._saving_checkpoint = saving_checkpoint\n      self._export_dir = export_dir\n      self._save_condition_variable.notify()\n    finally:\n      self._save_condition_variable.release()\n\n  def flush(self):\n    """"""Blocks until there is no saving happening.""""""\n    with self._save_condition_variable:\n      while self._export_dir:\n        logging.info(""Waiting for AsyncPolicySaver to finish."")\n        self._save_condition_variable.wait()\n\n  def close(self):\n    """"""Blocks until there is no saving happening and kills the save_thread.""""""\n    with self._save_condition_variable:\n      while self._export_dir:\n        logging.info(""Waiting for AsyncPolicySaver to finish."")\n        self._save_condition_variable.wait()\n      self._join_save_thread = True\n      self._save_condition_variable.notify()\n    self._save_thread.join()\n'"
tf_agents/policies/async_policy_saver_test.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.policies.async_policy_saver.""""""\n\nimport os\n\nfrom absl.testing.absltest import mock\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import async_policy_saver\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.utils import test_utils\n\n\nclass AsyncPolicySaverTest(test_utils.TestCase):\n\n  def testSave(self):\n    saver = mock.create_autospec(policy_saver.PolicySaver, instance=True)\n    async_saver = async_policy_saver.AsyncPolicySaver(saver)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    save_path = os.path.join(self.get_temp_dir(), \'policy\')\n    async_saver.save(save_path)\n    async_saver.flush()\n\n    saver.save.assert_called_once_with(save_path)\n    # Have to close the saver to avoid hanging threads that will prevent OSS\n    # tests from finishing.\n    async_saver.close()\n\n  def testCheckpointSave(self):\n    saver = mock.create_autospec(policy_saver.PolicySaver, instance=True)\n    async_saver = async_policy_saver.AsyncPolicySaver(saver)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    async_saver.save(path)\n    async_saver.flush()\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    async_saver.save_checkpoint(checkpoint_path)\n    async_saver.flush()\n\n    saver.save_checkpoint.assert_called_once_with(checkpoint_path)\n    # Have to close the saver to avoid hanging threads that will prevent OSS\n    # tests from finishing.\n    async_saver.close()\n\n  def testBlockingSave(self):\n    saver = mock.create_autospec(policy_saver.PolicySaver, instance=True)\n    async_saver = async_policy_saver.AsyncPolicySaver(saver)\n    path1 = os.path.join(self.get_temp_dir(), \'save_model\')\n    path2 = os.path.join(self.get_temp_dir(), \'save_model2\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    async_saver.save(path1)\n    async_saver.save(path2, blocking=True)\n\n    saver.save.assert_has_calls([mock.call(path1), mock.call(path2)])\n    # Have to close the saver to avoid hanging threads that will prevent OSS\n    # tests from finishing.\n    async_saver.close()\n\n  def testBlockingCheckpointSave(self):\n    saver = mock.create_autospec(policy_saver.PolicySaver, instance=True)\n    async_saver = async_policy_saver.AsyncPolicySaver(saver)\n    path1 = os.path.join(self.get_temp_dir(), \'save_model\')\n    path2 = os.path.join(self.get_temp_dir(), \'save_model2\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    async_saver.save_checkpoint(path1)\n    async_saver.save_checkpoint(path2, blocking=True)\n\n    saver.save_checkpoint.assert_has_calls([mock.call(path1), mock.call(path2)])\n    # Have to close the saver to avoid hanging threads that will prevent OSS\n    # tests from finishing.\n    async_saver.close()\n\n  def testClose(self):\n    saver = mock.create_autospec(policy_saver.PolicySaver, instance=True)\n    async_saver = async_policy_saver.AsyncPolicySaver(saver)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    async_saver.save(path)\n    self.assertTrue(async_saver._save_thread.is_alive())\n\n    async_saver.close()\n    saver.save.assert_called_once()\n\n    self.assertFalse(async_saver._save_thread.is_alive())\n\n    with self.assertRaises(ValueError):\n      async_saver.save(path)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/batched_py_policy.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Treat multiple non-batch policies as a single batch policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\n# pylint: disable=line-too-long\n# multiprocessing.dummy provides a pure *multithreaded* threadpool that works\n# in both python2 and python3 (concurrent.futures isn\'t available in python2).\n#   https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy\nfrom multiprocessing import dummy as mp_threads\n# pylint: enable=line-too-long\n\nfrom typing import Sequence\n\nimport gin\n\nfrom tf_agents.policies import py_policy\nfrom tf_agents.trajectories import policy_step as ps\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\n@gin.configurable\nclass BatchedPyPolicy(py_policy.PyPolicy):\n  """"""Batch together multiple py policies and act as a single batch.\n\n  The policies should only access shared python variables using\n  shared mutex locks (from the threading module).\n  """"""\n\n  def __init__(self,\n               policies: Sequence[py_policy.PyPolicy],\n               multithreading: bool = True):\n    """"""Batch together multiple (non-batched) py policies.\n\n    The policies can be different but must use the same action and\n    observation specs.\n\n    Args:\n      policies: List python policies (must be non-batched).\n      multithreading: Python bool describing whether interactions with the\n        given policies should happen in their own threadpool.  If `False`,\n        then all interaction is performed serially in the current thread.\n\n        This may be combined with `TFPyPolicy(..., py_policy_is_batched=True)`\n        to ensure that multiple policies are all run in the same thread.\n\n    Raises:\n      ValueError: If policies is not a list or tuple, or is zero length, or if\n        one of the policies is already batched.\n      ValueError: If the action or observation specs don\'t match.\n    """"""\n    if not isinstance(policies, (list, tuple)):\n      raise ValueError(""policies must be a list or tuple.  Got: %s"" % policies)\n\n    self._parallel_execution = multithreading\n    self._policies = policies\n    self._num_policies = len(policies)\n    self._time_step_spec = self._policies[0].time_step_spec\n    self._action_spec = self._policies[0].action_spec\n    self._policy_state_spec = self._policies[0].policy_state_spec\n    self._info_spec = self._policies[0].info_spec\n    self._policy_step_spec = self._policies[0].policy_step_spec\n    self._trajectory_spec = self._policies[0].trajectory_spec\n    self._collect_data_spec = self._policies[0].collect_data_spec\n    self._observation_and_action_constraint_splitter = \\\n        self._policies[0].observation_and_action_constraint_splitter\n\n    self._validate_spec(py_policy.PyPolicy.time_step_spec,\n                        self._time_step_spec)\n    self._validate_spec(py_policy.PyPolicy.action_spec,\n                        self._action_spec)\n    self._validate_spec(py_policy.PyPolicy.policy_state_spec,\n                        self._policy_state_spec)\n    self._validate_spec(py_policy.PyPolicy.info_spec,\n                        self._info_spec)\n    self._validate_spec(py_policy.PyPolicy.policy_step_spec,\n                        self._policy_step_spec)\n    self._validate_spec(py_policy.PyPolicy.trajectory_spec,\n                        self._trajectory_spec)\n    self._validate_spec(py_policy.PyPolicy.collect_data_spec,\n                        self._collect_data_spec)\n    self._validate_spec(\n        py_policy.PyPolicy.observation_and_action_constraint_splitter,\n        self._observation_and_action_constraint_splitter)\n\n    # Create a multiprocessing threadpool for execution.\n    if multithreading:\n      self._pool = mp_threads.Pool(self._num_policies)\n\n    super(BatchedPyPolicy, self).__init__(\n        self._time_step_spec,\n        self._action_spec,\n        self._policy_state_spec,\n        self._info_spec,\n        self._observation_and_action_constraint_splitter)\n\n  def __del__(self):\n    """"""Join external processes, if necessary.""""""\n    if self._parallel_execution:\n      self._pool.close()\n      self._pool.join()\n\n  def _validate_spec(self, policy_spec_method, spec_to_match):\n    # pytype: disable=attribute-error\n    if any(policy_spec_method.__get__(p) != spec_to_match\n           for p in self._policies):\n      raise ValueError(\n          ""All policies must have the same specs.  Saw: %s"" % self._policies)\n    # pytype: enable=attribute-error\n\n  def _execute(self, fn, iterable):\n    if self._parallel_execution:\n      return self._pool.map(fn, iterable)\n    else:\n      return [fn(x) for x in iterable]\n\n  def _get_initial_state(self, batch_size: int) -> types.NestedArray:\n    if self._num_policies == 1:\n      return nest_utils.batch_nested_array(\n          self._policies[0].get_initial_state())\n    else:\n      infos = self._execute(_execute_get_initial_state, self._policies)\n      infos = nest_utils.unbatch_nested_array(infos)\n      return nest_utils.stack_nested_arrays(infos)\n\n  def _action(self, time_step: ts.TimeStep,\n              policy_state: types.NestedArray) -> ps.PolicyStep:\n    """"""Forward a batch of time_step and policy_states to the wrapped policies.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: An Array, or a nested dict, list or tuple of\n        Arrays representing the previous policy_state.\n\n    Returns:\n      A batch of `PolicyStep` named tuples, each one containing:\n        `action`: A nest of action Arrays matching the `action_spec()`.\n        `state`: A nest of policy states to be fed into the next call to action.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n\n    if self._num_policies == 1:\n      time_step = nest_utils.unbatch_nested_array(time_step)\n      policy_state = nest_utils.unbatch_nested_array(policy_state)\n      policy_steps = self._policies[0].action(time_step, policy_state)\n      return nest_utils.batch_nested_array(policy_steps)\n    else:\n      unstacked_time_steps = nest_utils.unstack_nested_arrays(time_step)\n      if len(unstacked_time_steps) != len(self._policies):\n        raise ValueError(\n            ""Primary dimension of time_step items does not match ""\n            ""batch size: %d vs. %d"" % (len(unstacked_time_steps),\n                                       len(self._policies)))\n      unstacked_policy_states = [()] * len(unstacked_time_steps)\n      if policy_state:\n        unstacked_policy_states = nest_utils.unstack_nested_arrays(policy_state)\n        if len(unstacked_policy_states) != len(self._policies):\n          raise ValueError(\n              ""Primary dimension of policy_state items does not match ""\n              ""batch size: %d vs. %d"" % (len(unstacked_policy_states),\n                                         len(self._policies)))\n      policy_steps = self._execute(_execute_policy,\n                                   zip(self._policies,\n                                       unstacked_time_steps,\n                                       unstacked_policy_states))\n      return nest_utils.stack_nested_arrays(policy_steps)\n\n\ndef _execute_policy(zip_results_element) -> ps.PolicyStep:\n  """"""Called on each element of zip return value, in _action method.""""""\n  (policy, time_step, policy_state) = zip_results_element\n  return policy.action(time_step, policy_state)\n\n\ndef _execute_get_initial_state(policy) -> types.NestedArray:\n  """"""Called on each policy in _get_initial_state method.""""""\n  return policy.get_initial_state(batch_size=1)\n'"
tf_agents/policies/batched_py_policy_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.policies.batched_py_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import batched_py_policy\nfrom tf_agents.policies import py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import policy_step as ps\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\nMT_PARAMETERS = ({\'multithreading\': False},\n                 {\'multithreading\': True})\nNP_PARAMETERS = ({\'multithreading\': False, \'num_policies\': 1},\n                 {\'multithreading\': True, \'num_policies\': 1},\n                 {\'multithreading\': False, \'num_policies\': 5},\n                 {\'multithreading\': True, \'num_policies\': 5},\n                )\n\n\nclass MockPyPolicy(py_policy.PyPolicy):\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               policy_state_spec: types.NestedArraySpec = (),\n               info_spec: types.NestedArraySpec = ()):\n    seed = 987654321\n    self._rng = np.random.RandomState(seed)\n    super(MockPyPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy_state_spec=policy_state_spec)\n\n  def _action(self,\n              time_step: ts.TimeStep,\n              policy_state: types.NestedArray) -> ps.PolicyStep:\n    random_action = array_spec.sample_spec_nest(\n        self._action_spec, self._rng)\n\n    return ps.PolicyStep(random_action, policy_state)\n\n\nclass BatchedPyPolicyTest(tf.test.TestCase, parameterized.TestCase):\n\n  @property\n  def time_step_spec(self):\n    return ts.time_step_spec(\n        observation_spec=array_spec.ArraySpec((1,), np.int32))\n\n  @property\n  def action_spec(self):\n    return array_spec.BoundedArraySpec(\n        [7], dtype=np.float32, minimum=-1.0, maximum=1.0)\n\n  @property\n  def policy_state_spec(self):\n    return array_spec.BoundedArraySpec(\n        [3], dtype=np.int16, minimum=-7.0, maximum=7.0)\n\n  def _make_batched_py_policy(self, multithreading, num_policies=3):\n    policies = []\n    for _ in range(num_policies):\n      policies.append(MockPyPolicy(self.time_step_spec,\n                                   self.action_spec,\n                                   self.policy_state_spec))\n    return batched_py_policy.BatchedPyPolicy(\n        policies=policies, multithreading=multithreading)\n\n  @parameterized.parameters(*MT_PARAMETERS)\n  def test_close_no_hang_after_init(self, multithreading):\n    self._make_batched_py_policy(multithreading)\n\n  @parameterized.parameters(*MT_PARAMETERS)\n  def test_get_specs(self, multithreading):\n    policy = self._make_batched_py_policy(multithreading)\n    self.assertEqual(self.time_step_spec, policy.time_step_spec)\n    self.assertEqual(self.action_spec, policy.action_spec)\n    self.assertEqual(self.policy_state_spec, policy.policy_state_spec)\n\n  @parameterized.parameters(*NP_PARAMETERS)\n  def test_get_initial_state(self, multithreading, num_policies):\n    policy = self._make_batched_py_policy(multithreading,\n                                          num_policies=num_policies)\n    policy_state = policy.get_initial_state()\n    # Expect policy_state.shape[0] to be batch_size aka num_policies.\n    # The remaining dimensions should match the policy_state_spec.\n    correct_shape = (num_policies,) + self.policy_state_spec.shape\n    self.assertEqual(correct_shape, policy_state.shape)\n\n  @parameterized.parameters(*NP_PARAMETERS)\n  def test_action(self, multithreading, num_policies):\n    policy = self._make_batched_py_policy(multithreading,\n                                          num_policies=num_policies)\n    time_steps = np.array([\n        ts.restart(observation=np.array([1]))\n        for _ in range(num_policies)])\n\n    # Call policy.action() and assert PolicySteps are batched correctly.\n    policy_step = policy.action(time_steps)\n    self.assertEqual(num_policies, policy_step.action.shape[0])\n\n    # Take another step and assert that actions have the same shape.\n    policy_step2 = policy.action(time_steps)\n    self.assertAllEqual(policy_step.action.shape[0],\n                        policy_step2.action.shape[0])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/boltzmann_policy.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that applies temperature to a distribution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.typing import types\n\n\n@gin.configurable\nclass BoltzmannPolicy(tf_policy.TFPolicy):\n  """"""Returns boltzmann samples of a given policy.\n\n  The wrapped policy must expose a distribution parameterized by logits.\n  """"""\n\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               temperature: types.FloatOrReturningFloat = 1.0,\n               name: Optional[Text] = None):\n    """"""Builds a BoltzmannPolicy wrapping the given policy.\n\n    Args:\n      policy: A policy implementing the tf_policy.TFPolicy interface, using\n        a distribution parameterized by logits.\n      temperature: Tensor or function that returns the temperature for sampling\n        when `action` is called. This parameter applies when the action spec is\n        discrete. If the temperature is close to 0.0 this is equivalent to\n        calling `tf.argmax` on the output of the network.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    super(BoltzmannPolicy, self).__init__(\n        policy.time_step_spec,\n        policy.action_spec,\n        policy.policy_state_spec,\n        policy.info_spec,\n        emit_log_probability=policy.emit_log_probability,\n        clip=False,\n        name=name)\n    self._temperature = temperature\n    self._wrapped_policy = policy\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  def _get_temperature_value(self):\n    if callable(self._temperature):\n      return self._temperature()\n    return self._temperature\n\n  def _apply_temperature(self, dist):\n    """"""Change the action distribution to incorporate the temperature.""""""\n    logits = dist.logits / self._get_temperature_value()\n    return dist.copy(logits=logits)\n\n  def _distribution(self, time_step, policy_state):\n    distribution_step = self._wrapped_policy.distribution(\n        time_step, policy_state)\n    if self._temperature is None:\n      return distribution_step\n\n    action_dist = tf.nest.map_structure(self._apply_temperature,\n                                        distribution_step.action)\n    return distribution_step._replace(action=action_dist)\n'"
tf_agents/policies/boltzmann_policy_test.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.policies.boltzmann_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import network\nfrom tf_agents.policies import boltzmann_policy\nfrom tf_agents.policies import q_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, name=None, num_actions=2):\n    super(DummyNet, self).__init__(\n        tensor_spec.TensorSpec([2], tf.float32), (), \'DummyNet\')\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant([[1, 1.5],\n                                                                   [1, 1.5]]),\n            bias_initializer=tf.compat.v1.initializers.constant([[1], [1]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass BoltzmannPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(BoltzmannPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n\n  def testBuild(self):\n    wrapped = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    policy = boltzmann_policy.BoltzmannPolicy(wrapped, temperature=0.9)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n  def testAction(self):\n    tf.compat.v1.set_random_seed(1)\n    wrapped = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    policy = boltzmann_policy.BoltzmannPolicy(wrapped, temperature=0.9)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(action_step.action)\n\n  def testDistribution(self):\n    tf.compat.v1.set_random_seed(1)\n    wrapped = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    policy = boltzmann_policy.BoltzmannPolicy(wrapped, temperature=0.9)\n\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=1)\n    distribution_step = policy.distribution(time_step)\n    mode = distribution_step.action.mode()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # The weights of index 0 are all 1 and the weights of index 1 are all 1.5,\n    # so the Q values of index 1 will be higher.\n    self.assertAllEqual([[1]], self.evaluate(mode))\n\n  def testLogits(self):\n    tf.compat.v1.set_random_seed(1)\n    wrapped = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    policy = boltzmann_policy.BoltzmannPolicy(wrapped, temperature=0.5)\n\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=1)\n    distribution_step = policy.distribution(time_step)\n    logits = distribution_step.action.logits\n    original_logits = wrapped.distribution(time_step).action.logits\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # The un-temperature\'d logits would be 4 and 5.5, because it is (1 2) . (1\n    # 1) + 1 and (1 2) . (1.5 1.5) + 1. The temperature\'d logits will be double\n    # that.\n    self.assertAllEqual([[[4., 5.5]]], self.evaluate(original_logits))\n    self.assertAllEqual([[[8., 11.]]], self.evaluate(logits))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/categorical_q_policy.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Simple Categorical Q-Policy for Q-Learning with Categorical DQN.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import network\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\n\n@gin.configurable()\nclass CategoricalQPolicy(tf_policy.TFPolicy):\n  """"""Class to build categorical Q-policies.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               q_network: network.Network,\n               min_q_value: float,\n               max_q_value: float,\n               observation_and_action_constraint_splitter: Optional[\n                   types.Splitter] = None,\n               temperature: types.Float = 1.0):\n    """"""Builds a categorical Q-policy given a categorical Q-network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A `BoundedTensorSpec` representing the actions.\n      q_network: A network.Network to use for our policy.\n      min_q_value: A float specifying the minimum Q-value, used for setting up\n        the support.\n      max_q_value: A float specifying the maximum Q-value, used for setting up\n        the support.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `q_network` is compatible with the network-specific\n        half of the output of the `observation_and_action_constraint_splitter`.\n        In particular, `observation_and_action_constraint_splitter` will be\n        called on the observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n      temperature: temperature for sampling, when close to 0.0 is arg_max.\n\n    Raises:\n      ValueError: if `q_network` does not have property `num_atoms`.\n      TypeError: if `action_spec` is not a `BoundedTensorSpec`.\n    """"""\n    network_action_spec = getattr(q_network, \'action_spec\', None)\n\n    if network_action_spec is not None:\n      if not action_spec.is_compatible_with(network_action_spec):\n        raise ValueError(\n            \'action_spec must be compatible with q_network.action_spec; \'\n            \'instead got action_spec=%s, q_network.action_spec=%s\' % (\n                action_spec, network_action_spec))\n\n    if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n      raise TypeError(\'action_spec must be a BoundedTensorSpec. Got: %s\' % (\n          action_spec,))\n\n    num_atoms = getattr(q_network, \'num_atoms\', None)\n    if num_atoms is None:\n      raise ValueError(\'Expected q_network to have property `num_atoms`, but \'\n                       \'it doesn\\\'t. (Note: you likely want to use a \'\n                       \'CategoricalQNetwork.) Network is: %s\' % q_network)\n\n    super(CategoricalQPolicy, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy_state_spec=q_network.state_spec,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter))\n\n    self._temperature = tf.convert_to_tensor(temperature, dtype=tf.float32)\n    self._num_atoms = q_network.num_atoms\n    q_network.create_variables()\n    self._q_network = q_network\n\n    # Generate support in numpy so that we can assign it to a constant and avoid\n    # having a tensor property.\n    support = np.linspace(min_q_value, max_q_value, self._num_atoms,\n                          dtype=np.float32)\n    self._support = tf.constant(support, dtype=tf.float32)\n    self._action_dtype = action_spec.dtype\n\n  def _variables(self):\n    return self._q_network.variables\n\n  def _distribution(self, time_step, policy_state):\n    """"""Generates the distribution over next actions given the time_step.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: A Tensor, or a nested dict, list or tuple of\n        Tensors representing the previous policy_state.\n\n    Returns:\n      A tfp.distributions.Categorical capturing the distribution of next\n        actions.\n      A policy_state Tensor, or a nested dict, list or tuple of Tensors,\n        representing the new policy state.\n    """"""\n    network_observation = time_step.observation\n    observation_and_action_constraint_splitter = (\n        self.observation_and_action_constraint_splitter)\n\n    if observation_and_action_constraint_splitter is not None:\n      network_observation, mask = (\n          observation_and_action_constraint_splitter(network_observation))\n\n    q_logits, policy_state = self._q_network(\n        network_observation, time_step.step_type, policy_state)\n    q_logits.shape.assert_has_rank(3)\n    q_values = common.convert_q_logits_to_values(q_logits, self._support)\n\n    logits = q_values\n\n    if observation_and_action_constraint_splitter is not None:\n      # Overwrite the logits for invalid actions to -inf.\n      neg_inf = tf.constant(-np.inf, dtype=logits.dtype)\n      logits = tf.compat.v2.where(tf.cast(mask, tf.bool), logits, neg_inf)\n\n    dist = tfp.distributions.Categorical(\n        logits=logits, dtype=self.action_spec.dtype)\n    return policy_step.PolicyStep(dist, policy_state)\n'"
tf_agents/policies/categorical_q_policy_test.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for learning.reinforcement_learning.policies.categorical_q_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import flags\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import categorical_q_network\nfrom tf_agents.networks import network\nfrom tf_agents.policies import categorical_q_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyCategoricalNet(network.Network):\n\n  def __init__(self,\n               input_tensor_spec,\n               num_atoms=51,\n               num_actions=2,\n               name=None):\n    self._num_atoms = num_atoms\n    self._num_actions = num_actions\n    super(DummyCategoricalNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    # In CategoricalDQN we are dealing with a distribution over Q-values, which\n    # are represented as num_atoms bins, ranging from min_q_value to\n    # max_q_value. In order to replicate the setup in the non-categorical\n    # network (namely, [[2, 1], [1, 1]]), we use the following ""logits"":\n    # [[0, 1, ..., num_atoms-1, num_atoms, 1, ..., 1],\n    #  [1, ......................................, 1]]\n    # The important bit is that the first half of the first list (which\n    # corresponds to the logits for the first action) place more weight on the\n    # higher q_values than on the lower ones, thereby resulting in a higher\n    # value for the first action.\n    weights_initializer = np.array([\n        np.concatenate((np.arange(num_atoms), np.ones(num_atoms))),\n        np.concatenate((np.ones(num_atoms), np.ones(num_atoms)))])\n    kernel_initializer = tf.compat.v1.initializers.constant(\n        weights_initializer, verify_shape=True)\n    bias_initializer = tf.compat.v1.initializers.ones()\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = []\n    self._dummy_layers.append(\n        tf.keras.layers.Dense(\n            num_actions * num_atoms,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer))\n\n  @property\n  def num_atoms(self):\n    return self._num_atoms\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    logits = tf.reshape(inputs, [-1, self._num_actions, self._num_atoms])\n    return logits, network_state\n\n\nclass CategoricalQPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(CategoricalQPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    self._min_q_value = -10\n    self._max_q_value = 10\n    self._q_network = DummyCategoricalNet(\n        input_tensor_spec=self._obs_spec,\n        num_atoms=3,\n        num_actions=2)\n\n  def testBuild(self):\n    policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, self._action_spec, self._q_network,\n        self._min_q_value, self._max_q_value)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n    # There should be two variables in our network for the fc_layer we specified\n    # (one kernel and one bias).\n    self.assertLen(policy.variables(), 2)\n\n  def testMultipleActionsRaiseError(self):\n    with self.assertRaisesRegexp(\n        TypeError, \'.*action_spec must be a BoundedTensorSpec.*\'):\n      # Replace the action_spec for this test.\n      action_spec = [tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)] * 2\n      q_network = categorical_q_network.CategoricalQNetwork(\n          input_tensor_spec=self._obs_spec,\n          action_spec=action_spec,\n          num_atoms=3,\n          fc_layer_params=[4])\n      categorical_q_policy.CategoricalQPolicy(\n          self._time_step_spec, action_spec, q_network,\n          self._min_q_value, self._max_q_value)\n\n  def testAction(self):\n    policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, self._action_spec, self._q_network,\n        self._min_q_value, self._max_q_value)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations)\n    actions, _, _ = policy.action(time_step)\n    self.assertEqual(actions.shape.as_list(), [2])\n    self.assertEqual(actions.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(actions)\n\n    # actions should be a list of two elements; e.g., [0, 1]\n    self.assertLen(actions, 2)\n\n    for action in actions:\n      self.assertGreaterEqual(action, self._action_spec.minimum)\n      self.assertLessEqual(action, self._action_spec.maximum)\n\n  def testSample(self):\n    policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, self._action_spec, self._q_network,\n        self._min_q_value, self._max_q_value)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations)\n    actions = policy.action(time_step).action\n    self.assertEqual(actions.shape.as_list(), [2])\n    self.assertEqual(actions.dtype, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(actions)\n\n    # actions should be a list of two elements; e.g., [0, 1]\n    self.assertLen(actions, 2)\n\n    for action in actions:\n      self.assertGreaterEqual(action, self._action_spec.minimum)\n      self.assertLessEqual(action, self._action_spec.maximum)\n\n  def testUpdate(self):\n    policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, self._action_spec, self._q_network,\n        self._min_q_value, self._max_q_value)\n\n    new_policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, self._action_spec, self._q_network,\n        self._min_q_value, self._max_q_value)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations)\n\n    # There should be two variables in our networks for the fc_layer we\n    # specified (one kernel and one bias).\n    self.assertLen(policy.variables(), 2)\n    self.assertLen(new_policy.variables(), 2)\n\n    actions, _, _ = policy.action(time_step)\n    new_actions, _, _ = new_policy.action(time_step)\n\n    self.assertEqual(actions.shape, new_actions.shape)\n    self.assertEqual(actions.dtype, new_actions.dtype)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(actions)\n\n    # actions should be a list of two elements; e.g., [0, 1]\n    self.assertLen(actions, 2)\n\n    for action in actions:\n      self.assertGreaterEqual(action, self._action_spec.minimum)\n      self.assertLessEqual(action, self._action_spec.maximum)\n\n    self.assertEqual(self.evaluate(new_policy.update(policy)), None)\n    new_actions = self.evaluate(new_actions)\n\n    # new_actions should also be a list of two elements; e.g., [0, 1]\n    self.assertLen(new_actions, 2)\n\n    for action in new_actions:\n      self.assertGreaterEqual(action, self._action_spec.minimum)\n      self.assertLessEqual(action, self._action_spec.maximum)\n\n  def testMasking(self):\n    batch_size = 1000\n    num_state_dims = 5\n    num_actions = 8\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_step = ts.restart(observations, batch_size=batch_size)\n    input_tensor_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        [1], tf.int32, 0, num_actions - 1)\n\n    # We create a fixed mask here for testing purposes. Normally the mask would\n    # be part of the observation.\n    mask = [0, 1, 0, 1, 0, 0, 1, 0]\n    np_mask = np.array(mask)\n    tf_mask = tf.constant([mask for _ in range(batch_size)])\n    q_network = categorical_q_network.CategoricalQNetwork(\n        input_tensor_spec=input_tensor_spec,\n        action_spec=action_spec,\n        num_atoms=3,\n        fc_layer_params=[4])\n    policy = categorical_q_policy.CategoricalQPolicy(\n        self._time_step_spec, action_spec, q_network,\n        self._min_q_value, self._max_q_value,\n        observation_and_action_constraint_splitter=(\n            lambda observation: (observation, tf_mask)))\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Sample from the policy 1000 times, and ensure that actions considered\n    # invalid according to the mask are never chosen.\n    action_step = policy.action(time_step)\n    action = self.evaluate(action_step.action)\n    self.assertEqual(action.shape, (batch_size,))\n    self.assertAllEqual(np_mask[action], np.ones([batch_size]))\n\n  def testSaver(self):\n    policy = categorical_q_policy.CategoricalQPolicy(self._time_step_spec,\n                                                     self._action_spec,\n                                                     self._q_network,\n                                                     self._min_q_value,\n                                                     self._max_q_value)\n\n    train_step = tf.compat.v1.train.get_or_create_global_step()\n    saver = policy_saver.PolicySaver(policy, train_step=train_step)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    save_path = os.path.join(flags.FLAGS.test_tmpdir,\n                             \'saved_categorical_q_policy\')\n\n    # For TF1 Compatibility we set the cached session as default. This is a\n    # no-op in TF2.\n    with self.cached_session():\n      saver.save(save_path)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/policies/epsilon_greedy_policy.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that generates epsilon-greedy actions from a policy.\n\nTODO(kbanoop): Make policy state optional in the action method.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\ntfd = tfp.distributions\n\n\n@gin.configurable(module=\'tf_agents\', blacklist=[\'policy\'])\nclass EpsilonGreedyPolicy(tf_policy.TFPolicy):\n  """"""Returns epsilon-greedy samples of a given policy.""""""\n\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               epsilon: types.FloatOrReturningFloat,\n               name: Optional[Text] = None):\n    """"""Builds an epsilon-greedy MixturePolicy wrapping the given policy.\n\n    Args:\n      policy: A policy implementing the tf_policy.TFPolicy interface.\n      epsilon: The probability of taking the random action represented as a\n        float scalar, a scalar Tensor of shape=(), or a callable that returns a\n        float scalar or Tensor.\n      name: The name of this policy.\n\n    Raises:\n      ValueError: If epsilon is invalid.\n    """"""\n    observation_and_action_constraint_splitter = getattr(\n        policy, \'observation_and_action_constraint_splitter\', None)\n    accepts_per_arm_features = getattr(policy, \'accepts_per_arm_features\',\n                                       False)\n    self._greedy_policy = greedy_policy.GreedyPolicy(policy)\n    self._epsilon = epsilon\n    self._random_policy = random_tf_policy.RandomTFPolicy(\n        policy.time_step_spec,\n        policy.action_spec,\n        emit_log_probability=policy.emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=accepts_per_arm_features,\n        info_spec=policy.info_spec)\n    super(EpsilonGreedyPolicy, self).__init__(\n        policy.time_step_spec,\n        policy.action_spec,\n        policy.policy_state_spec,\n        policy.info_spec,\n        emit_log_probability=policy.emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n\n  @property\n  def wrapped_policy(self) -> tf_policy.TFPolicy:\n    return self._greedy_policy.wrapped_policy\n\n  def _variables(self):\n    return self._greedy_policy.variables()\n\n  def _get_epsilon(self):\n    if callable(self._epsilon):\n      return self._epsilon()\n    else:\n      return self._epsilon\n\n  def _action(self, time_step, policy_state, seed):\n    seed_stream = tfp.util.SeedStream(seed=seed, salt=\'epsilon_greedy\')\n    greedy_action = self._greedy_policy.action(time_step, policy_state)\n    random_action = self._random_policy.action(time_step, (), seed_stream())\n\n    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n    rng = tf.random.uniform(\n        outer_shape, maxval=1.0, seed=seed_stream(), name=\'epsilon_rng\')\n    cond = tf.greater(rng, self._get_epsilon())\n\n    # Selects the action/info from the random policy with probability epsilon.\n    # TODO(b/133175894): tf.compat.v1.where only supports a condition which is\n    # either a scalar or a vector. Use tf.compat.v2 so that it can support any\n    # condition whose leading dimensions are the same as the other operands of\n    # tf.where.\n    outer_ndims = int(outer_shape.shape[0])\n    if outer_ndims >= 2:\n      raise ValueError(\n          \'Only supports batched time steps with a single batch dimension\')\n    action = tf.nest.map_structure(lambda g, r: tf.compat.v1.where(cond, g, r),\n                                   greedy_action.action, random_action.action)\n\n    if greedy_action.info:\n      if not random_action.info:\n        raise ValueError(\'Incompatible info field\')\n      info = nest_utils.where(cond, greedy_action.info, random_action.info)\n      # Overwrite bandit policy info type.\n      if policy_utilities.has_bandit_policy_type(info, check_for_tensor=True):\n        # Generate mask of the same shape as bandit_policy_type (batch_size, 1).\n        # This is the opposite of `cond`, which is 1-D bool tensor (batch_size,)\n        # that is true when greedy policy was used, otherwise `cond` is false.\n        random_policy_mask = tf.reshape(tf.logical_not(cond),\n                                        tf.shape(info.bandit_policy_type))\n        bandit_policy_type = policy_utilities.bandit_policy_uniform_mask(\n            info.bandit_policy_type, mask=random_policy_mask)\n        info = policy_utilities.set_bandit_policy_type(\n            info, bandit_policy_type)\n    else:\n      if random_action.info:\n        raise ValueError(\'Incompatible info field\')\n      info = ()\n\n    # The state of the epsilon greedy policy is the state of the underlying\n    # greedy policy (the random policy carries no state).\n    # It is commonly assumed that the new policy state only depends only\n    # on the previous state and ""time_step"", the action (be it the greedy one\n    # or the random one) does not influence the new policy state.\n    state = greedy_action.state\n\n    return policy_step.PolicyStep(action, state, info)\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'EpsilonGreedyPolicy does not support distributions yet.\')\n'"
tf_agents/policies/epsilon_greedy_policy_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.policies.epsilon_greedy_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.policies import policy_utilities as policy_util\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import fixed_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass EpsilonGreedyPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(EpsilonGreedyPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._num_actions = 3\n    self._greedy_action = 1\n    self._action_spec = tensor_spec.BoundedTensorSpec((1,), tf.int32, 0,\n                                                      self._num_actions-1)\n    self._policy = fixed_policy.FixedPolicy(\n        np.asarray([self._greedy_action], dtype=np.int32),\n        self._time_step_spec,\n        self._action_spec)\n    self._bandit_policy_type = tf.constant([[1], [1]])\n    self._bandit_policy_type_spec = (\n        policy_util.create_bandit_policy_type_tensor_spec(shape=(1,)))\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    self._time_step = ts.restart(observations, batch_size=2)\n\n  def checkActionDistribution(self, actions, epsilon, num_steps):\n    # Check that the distribution of sampled actions is aligned with the epsilon\n    # values.\n    action_counts = np.bincount(np.hstack(actions), minlength=self._num_actions)\n    greedy_prob = 1.0-epsilon\n    expected_counts = [(epsilon*num_steps)/self._num_actions\n                       for _ in range(self._num_actions)]\n    expected_counts[self._greedy_action] += greedy_prob*num_steps\n    delta = num_steps*0.1\n    # Check that action_counts[i] \\in [expected-delta, expected+delta]\n    for i in range(self._num_actions):\n      self.assertLessEqual(action_counts[i], expected_counts[i]+delta)\n      self.assertGreaterEqual(action_counts[i], expected_counts[i]-delta)\n\n  @parameterized.named_parameters(\n      (\'Tensor0.0\', 0.0, True), (\'Tensor0.2\', 0.2, True),\n      (\'Tensor0.7\', 0.7, True), (\'Tensor1.0\', 1.0, True),\n      (\'Fixed0.0\', 0.0, False), (\'Fixed0.2\', 0.2, False),\n      (\'Fixed0.7\', 0.7, False), (\'Fixed1.0\', 1.0, False))\n  def testEpsilon(self, float_epsilon, is_tensor):\n    epsilon = tf.constant(float_epsilon) if is_tensor else float_epsilon\n    policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self._policy, epsilon=epsilon)\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n    policy_state = policy.get_initial_state(batch_size=2)\n    time_step = tf.nest.map_structure(tf.convert_to_tensor, self._time_step)\n\n    @common.function\n    def action_step_fn(time_step=time_step):\n      return policy.action(time_step, policy_state, seed=54)\n\n    tf.nest.assert_same_structure(\n        self._action_spec,\n        self.evaluate(action_step_fn(time_step)).action)\n\n    if tf.executing_eagerly():\n      action_step = action_step_fn\n    else:\n      action_step = action_step_fn()\n\n    actions = []\n\n    num_steps = 1000\n    for _ in range(num_steps):\n      action_ = self.evaluate(action_step).action[0]\n      self.assertIn(action_, [0, 1, 2])\n      actions.append(action_)\n\n    # Verify that action distribution changes as we vary epsilon.\n    self.checkActionDistribution(actions, float_epsilon, num_steps)\n\n  def checkBanditPolicyTypeShape(self, bandit_policy_type, batch_size):\n    self.assertAllEqual(bandit_policy_type.shape, [batch_size, 1])\n\n  def testInfoSpec(self):\n    PolicyInfo = collections.namedtuple(  # pylint: disable=invalid-name\n        \'PolicyInfo\',\n        (\'log_probability\', \'predicted_rewards\', \'bandit_policy_type\'))\n    # Set default empty tuple for all fields.\n    PolicyInfo.__new__.__defaults__ = ((),) * len(PolicyInfo._fields)\n\n    info_spec = PolicyInfo(bandit_policy_type=self._bandit_policy_type_spec)\n\n    policy_with_info_spec = fixed_policy.FixedPolicy(\n        np.asarray([self._greedy_action], dtype=np.int32),\n        self._time_step_spec,\n        self._action_spec,\n        policy_info=PolicyInfo(bandit_policy_type=self._bandit_policy_type),\n        info_spec=info_spec)\n\n    epsilon = 0.2\n    policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n        policy_with_info_spec, epsilon=epsilon)\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n    time_step = tf.nest.map_structure(tf.convert_to_tensor, self._time_step)\n\n    @common.function\n    def action_step_fn(time_step=time_step):\n      return policy.action(time_step, policy_state=(), seed=54)\n\n    tf.nest.assert_same_structure(\n        self._action_spec,\n        self.evaluate(action_step_fn(time_step)).action)\n\n    if tf.executing_eagerly():\n      action_step = action_step_fn\n    else:\n      action_step = action_step_fn()\n\n    step = self.evaluate(action_step)\n    tf.nest.assert_same_structure(\n        info_spec,\n        step.info)\n\n    self.checkBanditPolicyTypeShape(step.info.bandit_policy_type, batch_size=2)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/fixed_policy.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A policy which always returns a fixed action.\n\nMainly used for unit tests.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nclass FixedPolicy(tf_policy.TFPolicy):\n  """"""A policy which always returns a fixed action.""""""\n\n  def __init__(self,\n               actions: types.NestedTensor,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               policy_info: types.NestedTensorSpec = (),\n               info_spec: types.NestedTensorSpec = (),\n               name: Optional[Text] = None):\n    """"""A policy which always returns a fixed action.\n\n    Args:\n      actions: A Tensor, or a nested dict, list or tuple of Tensors\n        corresponding to `action_spec()`.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      policy_info: A policy info to be returned in PolicyStep.\n      info_spec: A policy info spec.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    super(FixedPolicy, self).__init__(time_step_spec, action_spec, clip=False,\n                                      info_spec=info_spec,\n                                      name=name)\n    nest_utils.assert_same_structure(self._action_spec, actions)\n\n    def convert(action, spec):\n      return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n\n    self._action_value = tf.nest.map_structure(convert, actions,\n                                               self._action_spec)\n    self._policy_info = policy_info\n\n  def _variables(self):\n    return []\n\n  def _action(self, time_step, policy_state, seed):\n    del seed\n    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n    action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n                                   self._action_value)\n    return policy_step.PolicyStep(action, policy_state, self._policy_info)\n\n  def _distribution(self, time_step, policy_state):\n    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n    action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n                                   self._action_value)\n\n    def dist_fn(action):\n      """"""Return a categorical distribution with all density on fixed action.""""""\n      return tfp.distributions.Deterministic(loc=action)\n\n    return policy_step.PolicyStep(\n        tf.nest.map_structure(dist_fn, action), policy_state, self._policy_info)\n'"
tf_agents/policies/fixed_policy_test.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.policies.fixed_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.policies import fixed_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass FixedPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(FixedPolicyTest, self).setUp()\n    # Creates an MDP with:\n    # - dim(observation) = 2\n    # - number of actions = 4\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._num_actions = 4\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(1,), dtype=tf.int32,\n        minimum=0, maximum=self._num_actions - 1)\n\n    # The policy always outputs the same action.\n    self._fixed_action = 1\n    self._policy = fixed_policy.FixedPolicy(\n        np.asarray([self._fixed_action], dtype=np.int32),\n        self._time_step_spec,\n        self._action_spec)\n\n  def testFixedPolicySingle(self):\n    observations = tf.constant([1, 2], dtype=tf.float32)\n    time_step = ts.restart(observations)\n    action_step = self._policy.action(time_step)\n    distribution_step = self._policy.distribution(time_step)\n    mode = distribution_step.action.mode()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action),\n                        [self._fixed_action])\n    self.assertAllEqual(self.evaluate(mode), [self._fixed_action])\n\n  def testFixedPolicyBatched(self):\n    batch_size = 2\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=batch_size)\n    action_step = self._policy.action(time_step)\n    distribution_step = self._policy.distribution(time_step)\n    mode = distribution_step.action.mode()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(\n        self.evaluate(action_step.action), [[self._fixed_action]] * batch_size)\n    self.assertAllEqual(\n        self.evaluate(mode), [[self._fixed_action]] * batch_size)\n\n  def testFixedPolicyBatchedOnNestedObservations(self):\n    batch_size = 2\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=batch_size)\n    action_spec = (tensor_spec.TensorSpec(shape=(2,), dtype=tf.float32),\n                   (tensor_spec.TensorSpec(shape=(1,), dtype=tf.int64), {\n                       \'dict\': tensor_spec.TensorSpec(shape=(), dtype=tf.int32)\n                   }))\n    fixed_action = (np.array([100, 200],\n                             dtype=np.float32), (np.array([300],\n                                                          dtype=np.int64), {\n                                                              \'dict\': 400\n                                                          }))\n    policy = fixed_policy.FixedPolicy(fixed_action, self._time_step_spec,\n                                      action_spec)\n    action = policy.action(time_step).action\n    distribution_mode = tf.nest.map_structure(\n        lambda t: t.mode(),\n        policy.distribution(time_step).action)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    expected = (tf.constant([[100, 200]] * batch_size, dtype=tf.float32),\n                (tf.constant([[300]] * batch_size, dtype=tf.int64), {\n                    \'dict\': tf.constant([400] * batch_size, dtype=tf.int32)\n                }))\n    tf.nest.map_structure(self.assertAllEqual, action, expected)\n    tf.nest.map_structure(self.assertAllEqual, distribution_mode, expected)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/gaussian_policy.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A policy that wraps a given policy and adds Gaussian noise.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.typing import types\n\ntfd = tfp.distributions\n\n\nclass GaussianPolicy(tf_policy.TFPolicy):\n  """"""Actor Policy with Gaussian exploration noise.""""""\n\n  def __init__(self,\n               wrapped_policy: tf_policy.TFPolicy,\n               scale: types.Float = 1.,\n               clip: bool = True,\n               name: Optional[Text] = None):\n    """"""Builds an GaussianPolicy wrapping wrapped_policy.\n\n    Args:\n      wrapped_policy: A policy to wrap and add OU noise to.\n      scale: Stddev of the Gaussian distribution from which noise is drawn.\n      clip: Whether to clip actions to spec. Default True.\n      name: The name of this policy.\n    """"""\n\n    def _validate_action_spec(action_spec):\n      if not tensor_spec.is_continuous(action_spec):\n        raise ValueError(\n            \'Gaussian Noise is applicable only to continuous actions.\')\n\n    tf.nest.map_structure(_validate_action_spec, wrapped_policy.action_spec)\n\n    super(GaussianPolicy, self).__init__(\n        wrapped_policy.time_step_spec,\n        wrapped_policy.action_spec,\n        wrapped_policy.policy_state_spec,\n        wrapped_policy.info_spec,\n        clip=clip,\n        name=name)\n    self._wrapped_policy = wrapped_policy\n\n    def _create_normal_distribution(action_spec):\n      return tfd.Normal(\n          loc=tf.zeros(action_spec.shape, dtype=action_spec.dtype),\n          scale=tf.ones(action_spec.shape, dtype=action_spec.dtype) * scale)\n\n    self._noise_distribution = tf.nest.map_structure(\n        _create_normal_distribution, self._action_spec)\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  def _action(self, time_step, policy_state, seed):\n    seed_stream = tfp.util.SeedStream(seed=seed, salt=\'gaussian_noise\')\n\n    action_step = self._wrapped_policy.action(time_step, policy_state,\n                                              seed_stream())\n\n    def _add_noise(action, distribution):\n      return action + distribution.sample(seed=seed_stream())\n\n    actions = tf.nest.map_structure(_add_noise, action_step.action,\n                                    self._noise_distribution)\n    return policy_step.PolicyStep(actions, action_step.state, action_step.info)\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\'Distributions are not implemented yet.\')\n'"
tf_agents/policies/gaussian_policy_test.py,26,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for third_party.py.tf_agents.policies.gaussian_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import gaussian_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActionNet(network.Network):\n\n  def __init__(self, input_tensor_spec, output_tensor_spec):\n    super(DummyActionNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=\'DummyActionNet\')\n    self._output_tensor_spec = output_tensor_spec\n    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    self._sub_layers = [\n        tf.keras.layers.Dense(\n            single_action_spec.shape.num_elements(),\n            activation=tf.nn.tanh,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]),\n        ),\n    ]\n\n  def call(self, observations, step_type, network_state):\n    del step_type\n\n    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    for layer in self._sub_layers:\n      states = layer(states)\n\n    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n    means = tf.reshape(states, [-1] + single_action_spec.shape.as_list())\n    spec_means = (single_action_spec.maximum + single_action_spec.minimum) / 2.0\n    spec_ranges = (\n        single_action_spec.maximum - single_action_spec.minimum) / 2.0\n    action_means = spec_means + spec_ranges * means\n\n    return (tf.nest.pack_sequence_as(self._output_tensor_spec, [action_means]),\n            network_state)\n\n\nclass GaussianPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(GaussianPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2, 3)\n    actor_network = DummyActionNet(self._obs_spec, self._action_spec)\n    self._wrapped_policy = actor_policy.ActorPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        actor_network=actor_network,\n        clip=False)\n\n  @property\n  def _time_step(self):\n    return ts.restart(tf.constant([1, 2], dtype=tf.float32))\n\n  @property\n  def _time_step_batch(self):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[2], name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\'observation\'))\n\n  def testBuild(self):\n    policy = gaussian_policy.GaussianPolicy(self._wrapped_policy)\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n    self.assertEqual(len(policy.variables()), 2)\n\n  def testActionIsInRange(self):\n    policy = gaussian_policy.GaussianPolicy(self._wrapped_policy)\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  def testActionAddsGaussianNoise(self):\n    policy = gaussian_policy.GaussianPolicy(self._wrapped_policy, clip=False)\n    action_step = policy.action(self._time_step_batch)\n    wrapped_action_step = self._wrapped_policy.action(self._time_step_batch)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    wrapped_policy_actions_ = self.evaluate(wrapped_action_step.action)\n\n    self.assertTrue(np.linalg.norm(actions_ - wrapped_policy_actions_) > 0)\n\n  def testActionList(self):\n    action_spec = [self._action_spec]\n    actor_network = DummyActionNet(self._obs_spec, action_spec)\n    self._wrapped_policy = actor_policy.ActorPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        actor_network=actor_network,\n        clip=False)\n\n    policy = gaussian_policy.GaussianPolicy(self._wrapped_policy)\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action[0].shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action[0].dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_[0] >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_[0] <= self._action_spec.maximum))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/greedy_policy.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that generates greedy actions from another policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\n\n\n# TODO(b/131405384): Remove this once Deterministic does casting internally.\n@tfp.experimental.register_composite\nclass DeterministicWithLogProb(tfp.distributions.Deterministic):\n  """"""Thin wrapper around Deterministic that supports taking log_prob.""""""\n\n  def _log_prob(self, x):\n    """"""Takes log-probs by casting to tf.float32 instead of self.dtype.""""""\n    return tf.math.log(tf.cast(self.prob(x), dtype=tf.float32))\n\n\n@gin.configurable(module=\'tf_agents\', blacklist=[\'policy\'])\nclass GreedyPolicy(tf_policy.TFPolicy):\n  """"""Returns greedy samples of a given policy.""""""\n\n  def __init__(self, policy: tf_policy.TFPolicy, name: Optional[Text] = None):\n    """"""Builds a greedy TFPolicy wrapping the given policy.\n\n    Args:\n      policy: A policy implementing the tf_policy.TFPolicy interface.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    super(GreedyPolicy, self).__init__(\n        policy.time_step_spec,\n        policy.action_spec,\n        policy.policy_state_spec,\n        policy.info_spec,\n        emit_log_probability=policy.emit_log_probability,\n        name=name)\n    self._wrapped_policy = policy\n\n  @property\n  def wrapped_policy(self) -> tf_policy.TFPolicy:\n    return self._wrapped_policy\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  def _distribution(self, time_step, policy_state):\n    def dist_fn(dist):\n      try:\n        greedy_action = dist.mode()\n      except NotImplementedError:\n        raise ValueError(""Your network\'s distribution does not implement mode ""\n                         ""making it incompatible with a greedy policy."")\n\n      return DeterministicWithLogProb(loc=greedy_action)\n\n    distribution_step = self._wrapped_policy.distribution(\n        time_step, policy_state)\n    return policy_step.PolicyStep(\n        tf.nest.map_structure(dist_fn, distribution_step.action),\n        distribution_step.state, distribution_step.info)\n'"
tf_agents/policies/greedy_policy_test.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.policies.greedy_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DistributionPolicy(tf_policy.TFPolicy):\n  """"""A policy which always returns the configured distribution.""""""\n\n  def __init__(self, distribution, time_step_spec, action_spec, name=None):\n    self._distribution_value = distribution\n    super(DistributionPolicy, self).__init__(\n        time_step_spec, action_spec, name=name)\n\n  def _action(self, time_step, policy_state, seed):\n    raise NotImplementedError(\'Not implemented.\')\n\n  def _distribution(self, time_step, policy_state):\n    return policy_step.PolicyStep(self._distribution_value, policy_state)\n\n  def _variables(self):\n    return []\n\n\nclass GreedyPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(GreedyPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n\n  @parameterized.parameters(\n      {\'action_probs\': [0.5, 0.2, 0.3]},\n      {\'action_probs\': [0.1, 0.1, 0.6, 0.2]}\n  )\n  def testCategoricalActions(self, action_probs):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((1,), tf.int32, 0, len(action_probs)-1),\n        tensor_spec.BoundedTensorSpec((), tf.int32, 0, len(action_probs)-1)]\n    wrapped_policy = DistributionPolicy([\n        tfp.distributions.Categorical(probs=[action_probs]),\n        tfp.distributions.Categorical(probs=action_probs)\n    ], self._time_step_spec, action_spec)\n    policy = greedy_policy.GreedyPolicy(wrapped_policy)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, action_spec)\n\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=1)\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    action_ = self.evaluate(action_step.action)\n    self.assertEqual(action_[0][0], np.argmax(action_probs))\n    self.assertEqual(action_[1], np.argmax(action_probs))\n\n  @parameterized.parameters(\n      {\'loc\': 1.0, \'scale\': 0.2},\n      {\'loc\': -2.0, \'scale\': 1.0},\n      {\'loc\': 0.0, \'scale\': 0.5}\n  )\n  def testNormalActions(self, loc, scale):\n    action_spec = tensor_spec.BoundedTensorSpec(\n        [1], tf.float32, tf.float32.min, tf.float32.max)\n    wrapped_policy = DistributionPolicy(\n        tfp.distributions.Normal([loc], [scale]), self._time_step_spec,\n        action_spec)\n    policy = greedy_policy.GreedyPolicy(wrapped_policy)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, action_spec)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    action_ = self.evaluate(action_step.action)\n    self.assertAlmostEqual(action_[0], loc)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/ou_noise_policy.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A policy that wraps a given policy and adds Ornstein Uhlenbeck (OU) noise.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\ntfd = tfp.distributions\n\n\nclass OUNoisePolicy(tf_policy.TFPolicy):\n  """"""Actor Policy with Ornstein Uhlenbeck (OU) exploration noise.""""""\n\n  def __init__(self,\n               wrapped_policy: tf_policy.TFPolicy,\n               ou_stddev: types.Float = 1.0,\n               ou_damping: types.Float = 1.0,\n               clip: bool = True,\n               name: Optional[Text] = None):\n    """"""Builds an OUNoisePolicy wrapping wrapped_policy.\n\n    Args:\n      wrapped_policy: A policy to wrap and add OU noise to.\n      ou_stddev:  stddev for the Ornstein-Uhlenbeck noise.\n      ou_damping: damping factor for the Ornstein-Uhlenbeck noise.\n      clip: Whether to clip actions to spec. Default True.\n      name: The name of this policy.\n    """"""\n\n    def _validate_action_spec(action_spec):\n      if not tensor_spec.is_continuous(action_spec):\n        raise ValueError(\'OU Noise is applicable only to continuous actions.\')\n\n    tf.nest.map_structure(_validate_action_spec, wrapped_policy.action_spec)\n\n    super(OUNoisePolicy, self).__init__(\n        wrapped_policy.time_step_spec,\n        wrapped_policy.action_spec,\n        wrapped_policy.policy_state_spec,\n        wrapped_policy.info_spec,\n        clip=clip,\n        name=name)\n    self._ou_stddev = ou_stddev\n    self._ou_damping = ou_damping\n    self._ou_process = None\n    self._wrapped_policy = wrapped_policy\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  def _action(self, time_step, policy_state, seed):\n    seed_stream = tfp.util.SeedStream(seed=seed, salt=\'ou_noise\')\n\n    def _create_ou_process(action_spec):\n      return common.OUProcess(\n          lambda: tf.zeros(action_spec.shape, dtype=action_spec.dtype),\n          self._ou_damping,\n          self._ou_stddev,\n          seed=seed_stream())\n\n    if self._ou_process is None:\n      self._ou_process = tf.nest.map_structure(_create_ou_process,\n                                               self._action_spec)\n\n    action_step = self._wrapped_policy.action(time_step, policy_state,\n                                              seed_stream())\n\n    def _add_ou_noise(action, ou_process):\n      return action + ou_process()\n\n    actions = tf.nest.map_structure(_add_ou_noise, action_step.action,\n                                    self._ou_process)\n    return policy_step.PolicyStep(actions, action_step.state, action_step.info)\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\'Distributions are not implemented yet.\')\n'"
tf_agents/policies/ou_noise_policy_test.py,26,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for third_party.py.tf_agents.policies.ou_noise_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import network\n\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import ou_noise_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActionNet(network.Network):\n\n  def __init__(self, input_tensor_spec, output_tensor_spec):\n    super(DummyActionNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=\'DummyActionNet\')\n    self._output_tensor_spec = output_tensor_spec\n    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    self._sub_layers = [\n        tf.keras.layers.Dense(\n            single_action_spec.shape.num_elements(),\n            activation=tf.nn.tanh,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]),\n        ),\n    ]\n\n  def call(self, observations, step_type, network_state):\n    del step_type\n\n    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    for layer in self._sub_layers:\n      states = layer(states)\n\n    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n    means = tf.reshape(states, [-1] + single_action_spec.shape.as_list())\n    spec_means = (single_action_spec.maximum + single_action_spec.minimum) / 2.0\n    spec_ranges = (\n        single_action_spec.maximum - single_action_spec.minimum) / 2.0\n    action_means = spec_means + spec_ranges * means\n\n    return (tf.nest.pack_sequence_as(self._output_tensor_spec, [action_means]),\n            network_state)\n\n\nclass OuNoisePolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(OuNoisePolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2, 3)\n    actor_network = DummyActionNet(self._obs_spec, self._action_spec)\n    self._wrapped_policy = actor_policy.ActorPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        actor_network=actor_network,\n        clip=False)\n\n  @property\n  def _time_step(self):\n    return ts.restart(tf.constant([1, 2], dtype=tf.float32))\n\n  @property\n  def _time_step_batch(self):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[2], name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\'observation\'))\n\n  def testBuild(self):\n    policy = ou_noise_policy.OUNoisePolicy(self._wrapped_policy)\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n    self.assertEqual(len(policy.variables()), 2)\n\n  def testActionIsInRange(self):\n    policy = ou_noise_policy.OUNoisePolicy(self._wrapped_policy)\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  def testActionAddsOUNoise(self):\n    policy = ou_noise_policy.OUNoisePolicy(self._wrapped_policy, clip=False)\n    action_step = policy.action(self._time_step_batch)\n    wrapped_action_step = self._wrapped_policy.action(self._time_step_batch)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    wrapped_policy_actions_ = self.evaluate(wrapped_action_step.action)\n\n    self.assertTrue(np.linalg.norm(actions_ - wrapped_policy_actions_) > 0)\n\n  def testActionList(self):\n    action_spec = [self._action_spec]\n    actor_network = DummyActionNet(self._obs_spec, action_spec)\n    self._wrapped_policy = actor_policy.ActorPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        actor_network=actor_network,\n        clip=False)\n\n    policy = ou_noise_policy.OUNoisePolicy(self._wrapped_policy)\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action[0].shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action[0].dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_[0] >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_[0] <= self._action_spec.maximum))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/policy_info_updater_wrapper.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Policy wrapper that updates `policy_info` from wrapped policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom typing import Callable, Text, Dict, Union, Sequence, Optional\n\nimport tensorflow.compat.v2 as tf\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.typing import types\n\n# A callable that receives a `PolicyStep` and returns a dictionary of a\n# tf.Tensor or a sequence of tf.Tensor`s used to update the policy_info.\nUpdaterFnType = Callable[[policy_step.PolicyStep],\n                         Dict[Text, Union[tf.Tensor, Sequence[tf.Tensor]]]]\n\n\nclass PolicyInfoUpdaterWrapper(tf_policy.TFPolicy):\n  """"""Returns samples with updated `policy_info` (a dictionary).\n  """"""\n\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               info_spec: types.NestedTensorSpec,\n               updater_fn: UpdaterFnType,\n               name: Optional[Text] = None):\n    """"""Builds a TFPolicy wrapping the given policy.\n\n    PolicyInfoUpdaterWrapper class updates `policy_info` using a user-defined\n    updater function. The main use case of this policy wrapper is to annotate\n    `policy_info` with some auxiliary information. For example, appending\n    an identifier to specify which model is used for current rollout.\n\n    Args:\n      policy: A policy implementing the tf_policy.TFPolicy interface.\n      info_spec: User-defined `info_spec` which specifies the policy info after\n        applying the updater function.\n      updater_fn: An updater function that updates the `policy_info`. This is a\n        callable that receives a `PolicyStep` and will return a dictionary of a\n        tf.Tensor or sequence of tf.Tensor`s.\n\n        **NOTE** If `policy.distribution` is called, the `PolicyStep.action`\n        object may contain a `tfp.distributions.Distribution` object instead\n        of a `Tensor`.  The `updater_fn` must be able to handle both cases\n        to be compatible with `PolicySaver`.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    super(PolicyInfoUpdaterWrapper, self).__init__(\n        time_step_spec=policy.time_step_spec,\n        action_spec=policy.action_spec,\n        policy_state_spec=policy.policy_state_spec,\n        info_spec=info_spec,\n        emit_log_probability=policy.emit_log_probability,\n        name=name)\n    self._wrapped_policy = policy\n    self._info_spec = info_spec\n    self._updater_fn = updater_fn\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  # Helper function to verify the compatibility between `current_info` and\n  # `_info_spec`.\n  def _check_value(self, tensor: tf.Tensor, tensorspec: tf.TensorSpec):\n    if not tf.TensorShape(tf.squeeze(tensor.get_shape())).is_compatible_with(\n        tensorspec.shape):\n      raise ValueError(\n          \'Tensor {} is not compatible with specification {}.\'.format(\n              tensor, tensorspec))\n\n  def apply_value_network(self, *args, **kwargs):\n    return self._wrapped_policy.apply_value_network(*args, **kwargs)\n\n  def _update_info(self, step):\n    if not isinstance(step.info, dict):\n      raise ValueError(\'`step.info` must be a dictionary.\')\n    current_info = step.info\n    current_info.update(self._updater_fn(step))\n    return policy_step.PolicyStep(step.action, step.state, current_info)\n\n  def _action(self, time_step, policy_state, seed):\n    action_step = self._wrapped_policy.action(time_step, policy_state, seed)\n    return self._update_info(action_step)\n\n  def _distribution(self, time_step, policy_state):\n    distribution_step = self._wrapped_policy.distribution(\n        time_step, policy_state)\n    return self._update_info(distribution_step)\n'"
tf_agents/policies/policy_info_updater_wrapper_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\nr""""""Tests for tf_agents.policies.policy_info_updater_wrapper.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import policy_info_updater_wrapper\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DistributionPolicy(tf_policy.TFPolicy):\n  """"""A policy which always returns the configured distribution.""""""\n\n  def __init__(self,\n               distribution,\n               time_step_spec,\n               action_spec,\n               info_spec,\n               name=None):\n    self._distribution_value = distribution\n    super(DistributionPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=info_spec,\n        name=name)\n\n  def _action(self, time_step, policy_state, seed):\n    return policy_step.PolicyStep(tf.constant(1., shape=(1,)), policy_state,\n                                  {\'test_info\': tf.constant(2, shape=(1,))})\n\n  def _distribution(self, time_step, policy_state):\n    return policy_step.PolicyStep(self._distribution_value, policy_state,\n                                  {\'test_info\': tf.constant(2, shape=(1,))})\n\n  def _variables(self):\n    return []\n\n\nclass ModelIdUpdater(object):\n\n  def __call__(self, step):\n    del step\n    return {\'model_id\': tf.expand_dims(2, axis=0)}\n\n\nclass PolicyInfoUpdaterWrapperTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(PolicyInfoUpdaterWrapperTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n\n  def test_model_id_updater(self):\n    loc = 0.0\n    scale = 0.5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, tf.float32.min,\n                                                tf.float32.max)\n    wrapped_policy = DistributionPolicy(\n        distribution=tfp.distributions.Normal([loc], [scale]),\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        info_spec={\n            \'test_info\':\n                tf.TensorSpec(shape=(1,), dtype=tf.int32, name=\'test_info\')\n        })\n    updater_info_spec = {\n        \'model_id\': tf.TensorSpec(shape=(1,), dtype=tf.int32, name=\'model_id\')\n    }\n    updater_info_spec.update(wrapped_policy.info_spec)\n    policy = policy_info_updater_wrapper.PolicyInfoUpdaterWrapper(\n        policy=wrapped_policy,\n        info_spec=updater_info_spec,\n        updater_fn=ModelIdUpdater(),\n        name=\'model_id_updater\')\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, action_spec)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step)\n    distribution_step = policy.distribution(time_step)\n\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n    tf.nest.assert_same_structure(action_spec, distribution_step.action)\n\n    self.assertListEqual(list(self.evaluate(action_step.info[\'model_id\'])), [2])\n    self.assertListEqual(\n        list(self.evaluate(distribution_step.info[\'model_id\'])), [2])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/policy_loader.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Helper function to load policies from disk.""""""\nimport os\nfrom typing import Optional, Text\n\nimport tensorflow.compat.v2 as tf\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import py_tf_eager_policy\n\n\ndef load(saved_model_path: Text, checkpoint_path: Optional[Text] = None):\n  """"""Loads a policy.\n\n  The argument `saved_model_path` is the path of a directory containing a full\n  saved model for the policy. The path typically looks like\n  \'/root_dir/policies/policy\', it may contain trailing numbers for the\n  train_step.\n\n  `saved_model_path` is expected to contain the following files. (There can be\n  additional shards for the `variables.data` files.)\n     * `saved_model.pb`\n     * `policy_specs.pbtxt`\n     * `variables/variables.index`\n     * `variables/variables.data-00000-of-00001`\n\n  The optional argument `checkpoint_path` is the path to a directory that\n  contains variable checkpoints (as opposed to full saved models) for the\n  policy. The path also typically ends-up with the checkpoint number,\n  for example: \'/my/save/dir/checkpoint/000022100\'.\n\n  If specified, `checkpoint_path` is expected to contain the following\n  files. (There can be additional shards for the `variables.data` files.)\n     * `variables/variables.index`\n     * `variables/variables.data-00000-of-00001`\n\n  `load()` recreates a policy from the saved model, and if it was specified\n  updates the policy from the checkpoint.  It returns the policy.\n\n  Args:\n    saved_model_path: string. Path to a directory containing a full saved model.\n    checkpoint_path: string. Optional path to a directory containing a\n      checkpoint of the model variables.\n\n  Returns:\n    A `tf_agents.policies.SavedModelPyTFEagerPolicy`.\n  """"""\n  policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n      saved_model_path, load_specs_from_pbtxt=True)\n  if checkpoint_path:\n    policy.update_from_checkpoint(checkpoint_path)\n  return policy\n\n\ndef _copy_file(from_dir, name, to_dir):\n  tf.io.gfile.copy(os.path.join(from_dir, name), os.path.join(to_dir, name))\n\n\ndef _copy_dir(from_dir, name, to_dir):\n  from_dir_name = os.path.join(from_dir, name)\n  to_dir_name = os.path.join(to_dir, name)\n  tf.io.gfile.mkdir(to_dir_name)\n  for file_name in tf.io.gfile.listdir(from_dir_name):\n    _copy_file(from_dir_name, file_name, to_dir_name)\n\n\ndef materialize_saved_model(saved_model_path: Text, checkpoint_path: Text,\n                            output_path: Text):\n  """"""Materializes a full saved model for a policy.\n\n  Some training processes generate a full saved model only at step 0, and then\n  generate checkpoints for the model variables at different train steps. In this\n  case there are no full saved models available for these train steps and you\n  must pass both the path to initial full saved model and the path to the\n  checkpoint to load a model at a given train step.\n\n  This function allows you to assemble a full saved model by combining the full\n  saved model at step 0 with a checkpoint at a further train step. The new saved\n  model is all that is needed to deploy the model for testing or production.\n\n  The arguments `saved_model_path` and `checkpoint_path` are exactly as for\n  `load()`:\n\n     * `saved_model_path` is the path to the full saved model at step 0.\n     * `checkpoint_path` is the path to the variable checkpoint at a further\n       step.\n\n  `output_path` must be a non-existent path on disk. It will be created as a new\n  directory. After `materialize_saved_model()` runs `output_path` will contain\n  the following files (There can be additional shards for the `variables.data`\n  files.)\n\n     * `saved_model.pb`\n     * `policy_specs.pbtxt`\n     * `variables/variables.index`\n     * `variables/variables.data-00000-of-00001`:\n\n  After running this function you can pass `output_path` to\n  `policy_loader.load()` to load the policy.\n\n  Example usage:\n\n  ```python\n  # The training process generated a saved model at\n  # `/path/policies/collect_policy` and checkpoints at\n  # `/path/policies/checkpoint/policy_checkpoint_NNNNNNNN`\n  #\n  # Assemble in \'/path/policies/collect_policy/prod\' a full saved model\n  # with the checkpoint at step 13400:\n  policy_loader.materialize_saved_model(\n      \'/path/policies/collect_policy\',\n      \'/path/policies/checkpoint/policy_checkpoint_0001340\',\n      \'/path/policies/collect_policy/prod\')\n  ...\n  # Later, load a model from the assembled model\n  collect_policy = policy_loader.load(\'/path/policies/collect_policy/prod\')\n  ```\n\n  Args:\n    saved_model_path: string. Path to a directory containing a full saved model.\n    checkpoint_path: string. Path to a directory containing a checkpoint of the\n      model variables.\n    output_path: string. Path where to save the materialized full saved model.\n  """"""\n  if tf.io.gfile.exists(output_path):\n    raise ValueError(\'Output path already exists: %s\' % output_path)\n  tf.io.gfile.makedirs(output_path)\n  _copy_dir(checkpoint_path, tf.saved_model.VARIABLES_DIRECTORY, output_path)\n  _copy_dir(saved_model_path, tf.saved_model.ASSETS_DIRECTORY, output_path)\n  _copy_file(saved_model_path, tf.saved_model.SAVED_MODEL_FILENAME_PB,\n             output_path)\n  _copy_file(saved_model_path, policy_saver.POLICY_SPECS_PBTXT, output_path)\n'"
tf_agents/policies/policy_loader_test.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for model_service_quick_saver.""""""\nimport os\n\nimport numpy as np\n\nimport tensorflow.compat.v2 as tf\nfrom tf_agents.networks import network\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import policy_loader\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import q_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass AddNet(network.Network):\n  """"""Small model used for tests.""""""\n\n  def __init__(self):\n    super(AddNet,\n          self).__init__(tensor_spec.TensorSpec((), tf.float32), (), \'add_net\')\n    self.var = tf.Variable(0.0, dtype=tf.float32)\n\n  def call(self, observation, step_type=None, network_state=(), training=False):\n    del step_type, network_state, training\n    return observation + self.var, ()\n\n\nclass PolicyLoaderTest(test_utils.TestCase):\n  """"""Tests for policy loader.""""""\n\n  def setUp(self):\n    super(PolicyLoaderTest, self).setUp()\n    self.root_dir = self.get_temp_dir()\n    tf_observation_spec = tensor_spec.TensorSpec((), np.float32)\n    tf_time_step_spec = ts.time_step_spec(tf_observation_spec)\n    tf_action_spec = tensor_spec.BoundedTensorSpec((), np.float32, 0.0, 3.0)\n    self.net = AddNet()\n    self.policy = greedy_policy.GreedyPolicy(\n        q_policy.QPolicy(tf_time_step_spec, tf_action_spec, self.net))\n    self.train_step = common.create_variable(\'train_step\', initial_value=0)\n    self.saver = policy_saver.PolicySaver(\n        self.policy, train_step=self.train_step)\n\n  def _createModelsOnDisk(self):\n    saved_model_dir = os.path.join(self.root_dir, \'policy\')\n    ckpt_dir = os.path.join(self.root_dir, \'checkpoint\')\n    self.train_step.assign(0)\n    self.net.var.assign(0)\n    saved_at_0_path = os.path.join(saved_model_dir, \'000\')\n    self.saver.save(saved_at_0_path)\n    self.train_step.assign(1)\n    self.net.var.assign(10)\n    ckpt_at_1_path = os.path.join(ckpt_dir, \'001\')\n    self.saver.save_checkpoint(ckpt_at_1_path)\n    return saved_at_0_path, ckpt_at_1_path\n\n  def testLoad(self):\n    saved_path, ckpt_at_path_1 = self._createModelsOnDisk()\n    policy_at_0 = policy_loader.load(saved_path)\n    self.assertEqual(0, policy_at_0.get_train_step())\n    self.assertEqual(0, policy_at_0.variables()[0].numpy())\n    policy_at_1 = policy_loader.load(saved_path, ckpt_at_path_1)\n    self.assertEqual(1, policy_at_1.get_train_step())\n    self.assertEqual(10, policy_at_1.variables()[0].numpy())\n\n  def testMaterialize(self):\n    saved_path, ckpt_at_path_1 = self._createModelsOnDisk()\n    materialized_path = os.path.join(self.root_dir, \'material/001\')\n    policy_loader.materialize_saved_model(saved_path, ckpt_at_path_1,\n                                          materialized_path)\n    policy_at_1 = policy_loader.load(materialized_path)\n    self.assertEqual(1, policy_at_1.get_train_step())\n    self.assertEqual(10, policy_at_1.variables()[0].numpy())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/policy_saver.py,57,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TF-Agents SavedModel API.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport copy\nimport functools\nimport os\nfrom typing import Callable, Dict, Tuple, Optional, Text\n\nfrom absl import logging\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nPOLICY_SPECS_PBTXT = \'policy_specs.pbtxt\'\n\n\ndef _true_if_missing_or_collision(spec, spec_names):\n  if not spec.name or spec.name in spec_names:\n    return True\n  spec_names.add(spec.name)\n  return False\n\n\ndef _rename_spec_with_nest_paths(spec):\n  renamed_spec = [\n      tf.TensorSpec(shape=s.shape, name=path, dtype=s.dtype)\n      for path, s in nest_utils.flatten_with_joined_paths(spec)\n  ]\n  return tf.nest.pack_sequence_as(spec, renamed_spec)\n\n\ndef _check_spec(spec):\n  """"""Checks for missing or colliding names in specs.""""""\n  spec_names = set()\n  checked = [\n      _true_if_missing_or_collision(s, spec_names)\n      for s in tf.nest.flatten(spec)\n  ]\n  if any(checked):\n    raise ValueError(\n        \'Specs contain either a missing name or a name collision.\\n  \'\n        \'Spec names: %s\\n\' %\n        (tf.nest.map_structure(lambda s: s.name or \'<MISSING>\', spec),))\n\n\nInputFnType = Callable[[types.NestedTensor], Tuple[types.NestedTensor,\n                                                   types.NestedTensor]]\nInputFnAndSpecType = Tuple[InputFnType, types.NestedTensorSpec]\n\n\nclass PolicySaver(object):\n  """"""A `PolicySaver` allows you to save a `tf_policy.Policy` to `SavedModel`.\n\n  The `save()` method exports a saved model to the requested export location.\n  The SavedModel that is exported can be loaded via\n  `tf.compat.v2.saved_model.load` (or `tf.saved_model.load` in TF2).  It\n  will have available signatures (concrete functions): `action`,\n  `get_initial_state`, `get_train_step.\n\n  The attribute `model_variables` is also available when the saved_model is\n  loaded which gives access to model variables in order to update them if\n  needed.\n\n  Usage:\n  ```python\n\n  my_policy = agent.collect_policy\n  saver = PolicySaver(my_policy, batch_size=None)\n\n  for i in range(...):\n    agent.train(...)\n    if i % 100 == 0:\n      saver.save(\'policy_%d\' % global_step)\n  ```\n\n  To load and use the saved policy directly:\n\n  ```python\n  saved_policy = tf.compat.v2.saved_model.load(\'policy_0\')\n  policy_state = saved_policy.get_initial_state(batch_size=3)\n  time_step = ...\n  while True:\n    policy_step = saved_policy.action(time_step, policy_state)\n    policy_state = policy_step.state\n    time_step = f(policy_step.action)\n    ...\n  ```\n\n  or to use the distributional form, e.g.:\n\n  ```python\n  batch_size = 3\n  saved_policy = tf.compat.v2.saved_model.load(\'policy_0\')\n  policy_state = saved_policy.get_initial_state(batch_size=batch_size)\n  time_step = ...\n  while True:\n    policy_step = saved_policy.distribution(time_step, policy_state)\n    policy_state = policy_step.state\n    time_step = f(policy_step.action.sample(batch_size))\n    ...\n  ```\n\n  If using the flattened (signature) version, you will be limited to using\n  dicts keyed by the specs\' name fields.\n\n  ```python\n  saved_policy = tf.compat.v2.saved_model.load(\'policy_0\')\n  get_initial_state_fn = saved_policy.signatures[\'get_initial_state\']\n  action_fn = saved_policy.signatures[\'action\']\n\n  policy_state_dict = get_initial_state_fn(batch_size=3)\n  time_step_dict = ...\n  while True:\n    time_step_state = dict(time_step_dict)\n    time_step_state.update(policy_state_dict)\n    policy_step_dict = action_fn(time_step_state)\n    policy_state_dict = extract_policy_state_fields(policy_step_dict)\n    action_dict = extract_action_fields(policy_step_dict)\n    time_step_dict = f(action_dict)\n    ...\n  ```\n  """"""\n\n  def __init__(\n      self,\n      policy: tf_policy.TFPolicy,\n      batch_size: Optional[int] = None,\n      use_nest_path_signatures: bool = True,\n      seed: Optional[types.Seed] = None,\n      train_step: Optional[tf.Variable] = None,\n      input_fn_and_spec: Optional[InputFnAndSpecType] = None,\n      metadata: Optional[Dict[Text, tf.Variable]] = None\n      ):\n    """"""Initialize PolicySaver for  TF policy `policy`.\n\n    Args:\n      policy: A TF Policy.\n      batch_size: The number of batch entries the policy will process at a time.\n        This must be either `None` (unknown batch size) or a python integer.\n      use_nest_path_signatures: SavedModel spec signatures will be created based\n        on the sructure of the specs. Otherwise all specs must have unique\n        names.\n      seed: Random seed for the `policy.action` call, if any (this should\n        usually be `None`, except for testing).\n      train_step: Variable holding the train step for the policy. The value\n        saved will be set at the time `saver.save` is called. If not provided,\n        train_step defaults to -1. Note since the train step must be a variable\n        it is not safe to create it directly in TF1 so in that case this is a\n        required parameter.\n      input_fn_and_spec: A `(input_fn, tensor_spec)` tuple where input_fn is a\n        function that takes inputs according to tensor_spec and converts them to\n        the `(time_step, policy_state)` tuple that is used as the input to the\n        action_fn. When `input_fn_and_spec` is set, `tensor_spec` is the input\n        for the action signature. When `input_fn_and_spec is None`, the action\n        signature takes as input `(time_step, policy_state)`.\n      metadata: A dictionary of `tf.Variables` to be saved along with the\n        policy.\n\n    Raises:\n      TypeError: If `policy` is not an instance of TFPolicy.\n      TypeError: If `metadata` is not a dictionary of tf.Variables.\n      ValueError: If use_nest_path_signatures is not used and any of the\n        following `policy` specs are missing names, or the names collide:\n        `policy.time_step_spec`, `policy.action_spec`,\n        `policy.policy_state_spec`, `policy.info_spec`.\n      ValueError: If `batch_size` is not either `None` or a python integer > 0.\n    """"""\n    if not isinstance(policy, tf_policy.TFPolicy):\n      raise TypeError(\'policy is not a TFPolicy.  Saw: %s\' % type(policy))\n    if (batch_size is not None and\n        (not isinstance(batch_size, int) or batch_size < 1)):\n      raise ValueError(\n          \'Expected batch_size == None or python int > 0, saw: %s\' %\n          (batch_size,))\n\n    action_fn_input_spec = (policy.time_step_spec, policy.policy_state_spec)\n    if use_nest_path_signatures:\n      action_fn_input_spec = _rename_spec_with_nest_paths(action_fn_input_spec)\n    else:\n      _check_spec(action_fn_input_spec)\n\n    # Make a shallow copy as we\'ll be making some changes in-place.\n    saved_policy = tf.Module()\n    saved_policy.collect_data_spec = copy.copy(policy.collect_data_spec)\n    saved_policy.policy_state_spec = copy.copy(policy.policy_state_spec)\n\n    if train_step is None:\n      if not common.has_eager_been_enabled():\n        raise ValueError(\'train_step is required in TF1 and must be a \'\n                         \'`tf.Variable`: %s\' % train_step)\n      train_step = tf.Variable(\n          -1,\n          trainable=False,\n          dtype=tf.int64,\n          aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,\n          shape=())\n    elif not isinstance(train_step, tf.Variable):\n      raise ValueError(\'train_step must be a TensorFlow variable: %s\' %\n                       train_step)\n\n    # We will need the train step for the Checkpoint object.\n    self._train_step = train_step\n    saved_policy.train_step = self._train_step\n\n    self._metadata = metadata or {}\n    for key, value in self._metadata.items():\n      if not isinstance(key, str):\n        raise TypeError(\'Keys of metadata must be strings: %s\' % key)\n      if not isinstance(value, tf.Variable):\n        raise TypeError(\'Values of metadata must be tf.Variable: %s\' % value)\n    saved_policy.metadata = self._metadata\n\n    if batch_size is None:\n      get_initial_state_fn = policy.get_initial_state\n      get_initial_state_input_specs = (tf.TensorSpec(\n          dtype=tf.int32, shape=(), name=\'batch_size\'),)\n    else:\n      get_initial_state_fn = functools.partial(\n          policy.get_initial_state, batch_size=batch_size)\n      get_initial_state_input_specs = ()\n\n    get_initial_state_fn = common.function()(get_initial_state_fn)\n\n    original_action_fn = policy.action\n\n    if seed is not None:\n\n      def action_fn(time_step, policy_state):\n        return original_action_fn(time_step, policy_state, seed=seed)\n    else:\n      action_fn = original_action_fn\n\n    def distribution_fn(time_step, policy_state):\n      """"""Wrapper for policy.distribution() in the SavedModel.""""""\n      try:\n        outs = policy.distribution(\n            time_step=time_step, policy_state=policy_state)\n        return tf.nest.map_structure(_composite_distribution, outs)\n      except (TypeError, NotImplementedError) as e:\n        # TODO(b/156526399): Move this to just the policy.distribution() call\n        # once tfp.experimental.as_composite() properly handles LinearOperator*\n        # components as well as TransformedDistributions.\n        logging.error(\n            \'Could not serialize policy.distribution() for policy ""%s"". \'\n            \'Calling saved_model.distribution() will raise the \'\n            \'assertion error: %s\', policy, e)\n        @common.function()\n        def _raise():\n          tf.Assert(False, [str(e)])\n          return ()\n        outs = _raise()\n\n    # We call get_concrete_function() for its side effect: to ensure the proper\n    # ConcreteFunction is stored in the SavedModel.\n    get_initial_state_fn.get_concrete_function(*get_initial_state_input_specs)\n\n    train_step_fn = common.function(\n        lambda: saved_policy.train_step).get_concrete_function()\n    get_metadata_fn = common.function(\n        lambda: saved_policy.metadata).get_concrete_function()\n\n    def add_batch_dim(spec):\n      return tf.TensorSpec(\n          shape=tf.TensorShape([batch_size]).concatenate(spec.shape),\n          name=spec.name,\n          dtype=spec.dtype)\n\n    batched_time_step_spec = tf.nest.map_structure(add_batch_dim,\n                                                   policy.time_step_spec)\n    batched_policy_state_spec = tf.nest.map_structure(add_batch_dim,\n                                                      policy.policy_state_spec)\n\n    policy_step_spec = policy.policy_step_spec\n    policy_state_spec = policy.policy_state_spec\n\n    if use_nest_path_signatures:\n      batched_time_step_spec = _rename_spec_with_nest_paths(\n          batched_time_step_spec)\n      batched_policy_state_spec = _rename_spec_with_nest_paths(\n          batched_policy_state_spec)\n      policy_step_spec = _rename_spec_with_nest_paths(policy_step_spec)\n      policy_state_spec = _rename_spec_with_nest_paths(policy_state_spec)\n    else:\n      _check_spec(batched_time_step_spec)\n      _check_spec(batched_policy_state_spec)\n      _check_spec(policy_step_spec)\n      _check_spec(policy_state_spec)\n\n    if input_fn_and_spec is not None:\n      # Store a signature based on input_fn_and_spec\n      @common.function()\n      def polymorphic_action_fn(example):\n        action_inputs = input_fn_and_spec[0](example)\n        tf.nest.map_structure(\n            lambda spec, t: tf.Assert(spec.is_compatible_with(t[0]), [t]),\n            action_fn_input_spec, action_inputs)\n        return action_fn(*action_inputs)\n\n      @common.function()\n      def polymorphic_distribution_fn(example):\n        action_inputs = input_fn_and_spec[0](example)\n        tf.nest.map_structure(\n            lambda spec, t: tf.Assert(spec.is_compatible_with(t[0]), [t]),\n            action_fn_input_spec, action_inputs)\n        return distribution_fn(*action_inputs)\n\n      batched_input_spec = tf.nest.map_structure(add_batch_dim,\n                                                 input_fn_and_spec[1])\n      # We call get_concrete_function() for its side effect: to ensure the\n      # proper ConcreteFunction is stored in the SavedModel.\n      polymorphic_action_fn.get_concrete_function(example=batched_input_spec)\n      polymorphic_distribution_fn.get_concrete_function(\n          example=batched_input_spec)\n\n      action_input_spec = (input_fn_and_spec[1],)\n\n    else:\n      action_input_spec = action_fn_input_spec\n      if batched_policy_state_spec:\n        # Store the signature with a required policy state spec\n        polymorphic_action_fn = common.function()(action_fn)\n        polymorphic_action_fn.get_concrete_function(\n            time_step=batched_time_step_spec,\n            policy_state=batched_policy_state_spec)\n\n        polymorphic_distribution_fn = common.function()(distribution_fn)\n        polymorphic_distribution_fn.get_concrete_function(\n            time_step=batched_time_step_spec,\n            policy_state=batched_policy_state_spec)\n      else:\n        # Create a polymorphic action_fn which you can call as\n        #  restored.action(time_step)\n        # or\n        #  restored.action(time_step, ())\n        # (without retracing the inner action twice)\n        @common.function()\n        def polymorphic_action_fn(time_step,\n                                  policy_state=batched_policy_state_spec):\n          return action_fn(time_step, policy_state)\n\n        polymorphic_action_fn.get_concrete_function(\n            time_step=batched_time_step_spec,\n            policy_state=batched_policy_state_spec)\n        polymorphic_action_fn.get_concrete_function(\n            time_step=batched_time_step_spec)\n\n        @common.function()\n        def polymorphic_distribution_fn(time_step,\n                                        policy_state=batched_policy_state_spec):\n          return distribution_fn(time_step, policy_state)\n\n        polymorphic_distribution_fn.get_concrete_function(\n            time_step=batched_time_step_spec,\n            policy_state=batched_policy_state_spec)\n        polymorphic_distribution_fn.get_concrete_function(\n            time_step=batched_time_step_spec)\n\n    signatures = {\n        # CompositeTensors aren\'t well supported by old-style signature\n        # mechanisms, so we do not have a signature for policy.distribution.\n        \'action\':\n            _function_with_flat_signature(\n                polymorphic_action_fn,\n                input_specs=action_input_spec,\n                output_spec=policy_step_spec,\n                include_batch_dimension=True,\n                batch_size=batch_size),\n        \'get_initial_state\':\n            _function_with_flat_signature(\n                get_initial_state_fn,\n                input_specs=get_initial_state_input_specs,\n                output_spec=policy_state_spec,\n                include_batch_dimension=False),\n        \'get_train_step\':\n            _function_with_flat_signature(\n                train_step_fn,\n                input_specs=(),\n                output_spec=train_step.dtype,\n                include_batch_dimension=False),\n        \'get_metadata\':\n            _function_with_flat_signature(\n                get_metadata_fn,\n                input_specs=(),\n                output_spec=tf.nest.map_structure(lambda v: v.dtype,\n                                                  self._metadata),\n                include_batch_dimension=False),\n    }\n\n    saved_policy.action = polymorphic_action_fn\n    saved_policy.distribution = polymorphic_distribution_fn\n    saved_policy.get_initial_state = get_initial_state_fn\n    saved_policy.get_train_step = train_step_fn\n    saved_policy.get_metadata = get_metadata_fn\n    # Adding variables as an attribute to facilitate updating them.\n    saved_policy.model_variables = policy.variables()\n\n    # TODO(b/156779400): Move to a public API for accessing all trackable leaf\n    # objects (once it\'s available).  For now, we have no other way of tracking\n    # objects like Tables, Vocabulary files, etc.\n    try:\n      saved_policy._all_assets = policy._unconditional_checkpoint_dependencies  # pylint: disable=protected-access\n    except AttributeError as e:\n      if \'_self_unconditional\' in str(e):\n        logging.warn(\n            \'Unable to capture all trackable objects in policy ""%s"".  This \'\n            \'may be okay.  Error: %s\', policy, e)\n      else:\n        raise e\n\n    self._policy = saved_policy\n    self._signatures = signatures\n    self._action_input_spec = action_input_spec\n    self._policy_step_spec = policy_step_spec\n    self._policy_state_spec = policy_state_spec\n\n  @property\n  def action_input_spec(self) -> types.NestedTensorSpec:\n    """"""Tuple `(time_step_spec, policy_state_spec)` for feeding `action`.\n\n    This describes the input of `action` in the SavedModel.\n\n    This may differ from the original policy if `use_nest_path_signatures` was\n    enabled.\n\n    Returns:\n      A nest of specs.\n    """"""\n    return self._action_input_spec\n\n  @property\n  def policy_step_spec(self) -> types.NestedTensorSpec:\n    """"""Spec that describes the output of `action` in the SavedModel.\n\n    This may differ from the original policy if `use_nest_path_signatures` was\n    enabled.\n\n    Returns:\n      A nest of specs.\n    """"""\n    return self._policy_step_spec\n\n  @property\n  def policy_state_spec(self) -> types.NestedTensorSpec:\n    """"""Spec that describes the output of `get_initial_state` in the SavedModel.\n\n    This may differ from the original policy if `use_nest_path_signatures` was\n    enabled.\n\n    Returns:\n      A nest of specs.\n    """"""\n    return self._policy_state_spec\n\n  @property\n  def signatures(self) -> Dict[Text, Callable]:  # pylint: disable=g-bare-generic\n    """"""Get the (flat) signatures used when exporting the `SavedModel`.\n\n    Returns:\n      A `dict` mapping each of ""action"", ""get_initial_state"",  ""get_train_step""\n      and ""get_metadata"" to their respective flat signatures.\n    """"""\n    return self._signatures\n\n  def get_train_step(self) -> types.Int:\n    """"""Returns the train step of the policy.\n\n    Returns:\n      An integer.\n    """"""\n    if tf.executing_eagerly():\n      return self._train_step.numpy()\n    else:\n      return tf.identity(self._train_step)\n\n  def get_metadata(self) -> Dict[Text, tf.Variable]:\n    """"""Returns the metadata of the policy.\n\n    Returns:\n      An a dictionary of tf.Variable.\n    """"""\n    if tf.executing_eagerly():\n      return {k: self._metadata[k].numpy() for k in self._metadata}\n    else:\n      return self._metadata\n\n  def save(self,\n           export_dir: Text,\n           options: Optional[tf.saved_model.SaveOptions] = None):\n    """"""Save the policy to the given `export_dir`.\n\n    Args:\n      export_dir: Directory to save the policy to.\n      options: Optional `tf.saved_model.SaveOptions` object.\n    """"""\n    tf.compat.v2.saved_model.save(\n        self._policy, export_dir, signatures=self._signatures, options=options)\n\n    temp_spec_file_name = \'{}_temp\'.format(POLICY_SPECS_PBTXT)\n    temp_spec_output_path = os.path.join(export_dir, temp_spec_file_name)\n    specs = {\n        \'collect_data_spec\': self._policy.collect_data_spec,\n        \'policy_state_spec\': self._policy.policy_state_spec\n    }\n    tensor_spec.to_pbtxt_file(temp_spec_output_path, specs)\n    spec_output_path = os.path.join(export_dir, POLICY_SPECS_PBTXT)\n    # By moving the file to its final location makes it safer to wait for the\n    # file (e.g. from a separate binary). The parameter `overwrite=True`\n    # reproduces the exact previous behavior.\n    tf.io.gfile.rename(temp_spec_output_path, spec_output_path, overwrite=True)\n\n  def save_checkpoint(self,\n                      export_dir: Text,\n                      options: Optional[tf.train.CheckpointOptions] = None):\n    """"""Saves the policy as a checkpoint to the given `export_dir`.\n\n    This will only work with checkpoints generated in TF2.x.\n\n    For the checkpoint to be useful users should first call `save` to generate a\n    saved_model of the policy. Checkpoints can then be used to update the policy\n    without having to reload the saved_model, or saving multiple copies of the\n    `saved_model.pb` file.\n\n    The checkpoint is always created in the sub-directory \'variables/\' and the\n    checkpoint file prefix used is \'variables\'. The checkpoint files are as\n    follows:\n       * export_dir/variables/variables.index\n       * export_dir/variables/variables-xxxxx-of-xxxxx\n\n    This makes the files compatible with the checkpoint part of full saved\n    models, which enables you to load a saved model made up from the graph part\n    of a full saved model and the variables part of a checkpoint.\n\n    Args:\n      export_dir: Directory to save the checkpoint to.\n      options: Optional `tf.train.CheckpointOptions` object.\n    """"""\n    # In addition to the policy, also list dependencies on model_variables and\n    # train_step so the checkpoint can be combined with a saved graph from a\n    # full saved model.\n    checkpoint = tf.compat.v2.train.Checkpoint(\n        policy=self._policy,\n        model_variables=self._policy.model_variables,\n        train_step=self._train_step)\n    # Use write() to make sure that the file prefix is not modified by appending\n    # a save counter value.\n    file_prefix = os.path.join(export_dir, tf.saved_model.VARIABLES_DIRECTORY,\n                               tf.saved_model.VARIABLES_FILENAME)\n    checkpoint.write(file_prefix, options=options)\n\n\ndef _function_with_flat_signature(function,\n                                  input_specs,\n                                  output_spec,\n                                  include_batch_dimension,\n                                  batch_size=None):\n  """"""Create a tf.function with a given signature for export.\n\n  Args:\n    function: A callable that can be wrapped in tf.function.\n    input_specs: A tuple nested specs declaring ordered arguments to function.\n    output_spec: The nested spec describing the output of the function.\n    include_batch_dimension: Python bool, whether to prepend a batch dimension\n      to inputs and outputs.\n    batch_size: Known batch size, or `None` for unknown.  Ignored if\n      `include_batch_dimension == False`.\n\n  Returns:\n    A `tf.function` with the given input spec that returns a `dict` mapping\n    output spec keys to corresponding output values.\n  """"""\n\n  def _with_batch(spec):\n    if include_batch_dimension:\n      return tf.TensorSpec(\n          shape=tf.TensorShape([batch_size]).concatenate(spec.shape),\n          name=spec.name,\n          dtype=spec.dtype)\n    else:\n      return spec\n\n  flat_input_spec = [_with_batch(spec) for spec in tf.nest.flatten(input_specs)]\n\n  def as_dict(outputs, output_spec):\n    nest_utils.assert_same_structure(outputs, output_spec)\n    flat_outputs = tf.nest.flatten(outputs)\n    flat_names = [s.name for s in tf.nest.flatten(output_spec)]\n    return dict(zip(flat_names, flat_outputs))\n\n  @common.function(input_signature=flat_input_spec)\n  def function_with_signature(*input_list):\n    inputs_ = tf.nest.pack_sequence_as(input_specs, input_list)\n    outputs_ = function(*inputs_)\n    dict_outputs_ = as_dict(outputs_, output_spec)\n    return dict_outputs_\n\n  return function_with_signature\n\n\ndef specs_from_collect_data_spec(\n    loaded_policy_specs: types.NestedTensorSpec\n) -> Dict[types.NestedSpec, types.NestedSpec]:\n  """"""Creates policy specs from specs loaded from disk.\n\n  The PolicySaver saves policy specs next to the saved model as\n  a `struct.StructuredValue` proto. This recreates the\n  original specs from the proto.\n\n  Pass the proto loaded from the file with `tensor_spec.from_pbtxt_file()`\n  to this function.\n\n  Args:\n     loaded_policy_specs: `struct.StructuredValue` proto that had been\n       previously created by PolicySaver as a pbtxt.\n\n  Returns:\n    A dict with specs extracted from the proto. The dict contains the following\n    keys and values. Except `time_step_spec` all the specs are nests of\n    `ArraySpecs`.\n       * `collect_data_spec`: Collect data spec for the policy.\n       * `time_step_spec`: `TimeStepSpec` for the policy.\n       * `action_spec`:  Action spec for the policy\n       * `policy_state_spec`: State spec for the policy.\n       * `info_spec`: Info spec for the policy.\n  """"""\n  policy_specs = tensor_spec.to_nest_array_spec(loaded_policy_specs)\n  collect_data_spec = policy_specs[\'collect_data_spec\']\n  policy_state_spec = policy_specs[\'policy_state_spec\']\n  time_step_spec = ts.TimeStep(\n      step_type=collect_data_spec.step_type,\n      reward=collect_data_spec.reward,\n      discount=collect_data_spec.discount,\n      observation=collect_data_spec.observation)\n  action_spec = collect_data_spec.action\n  info_spec = collect_data_spec.policy_info\n  return dict(\n      collect_data_spec=collect_data_spec,\n      time_step_spec=time_step_spec,\n      action_spec=action_spec,\n      policy_state_spec=policy_state_spec,\n      info_spec=info_spec)\n\n\ndef _composite_distribution(d):\n  """"""Converts tfp Distributions to CompositeTensors.""""""\n  return (tfp.experimental.as_composite(d)\n          if isinstance(d, tfp.distributions.Distribution)\n          else d)\n'"
tf_agents/policies/policy_saver_test.py,119,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for PolicySaver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport shutil\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import q_network\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import q_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass PolicyNoDistribution(tf_policy.TFPolicy):\n\n  def __init__(self):\n    super(PolicyNoDistribution, self).__init__(\n        time_step_spec=ts.TimeStep(\n            step_type=(), reward=(), discount=(), observation=()),\n        action_spec=())\n\n  def _action(self, **kwargs):\n    return policy_step.PolicyStep((), ())\n\n  def _distribution(self, **kwargs):\n    raise NotImplementedError(\'_distribution has not been implemented.\')\n\n\nclass PolicySaverTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(PolicySaverTest, self).setUp()\n    self._time_step_spec = ts.TimeStep(\n        step_type=tensor_spec.BoundedTensorSpec(\n            dtype=tf.int32, shape=(), name=\'st\', minimum=0, maximum=2),\n        reward=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32, shape=(), name=\'reward\', minimum=0.0,\n            maximum=5.0),\n        discount=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32,\n            shape=(),\n            name=\'discount\',\n            minimum=0.0,\n            maximum=1.0),\n        observation=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32,\n            shape=(4,),\n            name=\'obs\',\n            minimum=-10.0,\n            maximum=10.0))\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=10, name=\'act_0\')\n    self._global_seed = 12345\n    tf.compat.v1.set_random_seed(self._global_seed)\n\n  def testUniqueSignatures(self):\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    action_signature_names = [\n        s.name for s in saver._signatures[\'action\'].input_signature\n    ]\n    self.assertAllEqual(\n        [\'0/step_type\', \'0/reward\', \'0/discount\', \'0/observation\'],\n        action_signature_names)\n    initial_state_signature_names = [\n        s.name for s in saver._signatures[\'get_initial_state\'].input_signature\n    ]\n    self.assertAllEqual([\'batch_size\'], initial_state_signature_names)\n\n  def testRenamedSignatures(self):\n    time_step_spec = self._time_step_spec._replace(\n        observation=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32, shape=(4,), minimum=-10.0, maximum=10.0))\n\n    network = q_network.QNetwork(\n        input_tensor_spec=time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=7)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    action_signature_names = [\n        s.name for s in saver._signatures[\'action\'].input_signature\n    ]\n    self.assertAllEqual(\n        [\'0/step_type\', \'0/reward\', \'0/discount\', \'0/observation\'],\n        action_signature_names)\n    initial_state_signature_names = [\n        s.name for s in saver._signatures[\'get_initial_state\'].input_signature\n    ]\n    self.assertAllEqual([\'batch_size\'], initial_state_signature_names)\n\n  def _convert_action_input_to_string_vector(self, action_input_tensors):\n    action_input_tensors_strings = tf.nest.map_structure(\n        tf.strings.as_string, action_input_tensors)\n\n    return tf.concat([\n        tf.expand_dims(action_input_tensors_strings[0].step_type, 1),\n        tf.expand_dims(action_input_tensors_strings[0].reward, 1),\n        tf.expand_dims(action_input_tensors_strings[0].discount, 1),\n        action_input_tensors_strings[0].observation\n    ], 1)\n\n  def _convert_string_vector_to_action_input(self, example):\n    return (ts.TimeStep(\n        step_type=tf.cast(\n            tf.strings.to_number(example[:, 0], tf.float32), tf.int32),\n        reward=tf.strings.to_number(example[:, 1], tf.float32),\n        discount=tf.strings.to_number(example[:, 2], tf.float32),\n        observation=tf.strings.to_number(example[:, 3:7], tf.float32)), ())\n\n  @parameterized.named_parameters(\n      (\'NotSeededNoStateNoInputFn\', False, False, False, False),\n      (\'NotSeededWithStateNoInputFn\', False, True, False, False),\n      (\'NotSeededDistributionNetworkNoInputFn\', False, False, True, False),\n      (\'SeededNoStateNoInputFn\', True, False, False, False),\n      (\'SeededWithStateNoInputFn\', True, True, False, False),\n      (\'SeededDistributionNetworkNoInputFn\', True, False, True, False),\n      (\'NotSeededNoStateInputFn\', False, False, False, True),\n      (\'SeededNoStateInputFn\', True, False, False, True),\n  )\n  def testSaveAction(self, seeded, has_state, distribution_net,\n                     has_input_fn_and_spec):\n    with tf.compat.v1.Graph().as_default():\n      tf.compat.v1.set_random_seed(self._global_seed)\n      with tf.compat.v1.Session().as_default():\n        global_step = common.create_variable(\'train_step\', initial_value=0)\n        if distribution_net:\n          network = actor_distribution_network.ActorDistributionNetwork(\n              self._time_step_spec.observation, self._action_spec)\n          policy = actor_policy.ActorPolicy(\n              time_step_spec=self._time_step_spec,\n              action_spec=self._action_spec,\n              actor_network=network)\n        else:\n          if has_state:\n            network = q_rnn_network.QRnnNetwork(\n                input_tensor_spec=self._time_step_spec.observation,\n                action_spec=self._action_spec)\n          else:\n            network = q_network.QNetwork(\n                input_tensor_spec=self._time_step_spec.observation,\n                action_spec=self._action_spec)\n\n          policy = q_policy.QPolicy(\n              time_step_spec=self._time_step_spec,\n              action_spec=self._action_spec,\n              q_network=network)\n\n        action_seed = 98723\n\n        batch_size = 3\n        action_inputs = tensor_spec.sample_spec_nest(\n            (self._time_step_spec, policy.policy_state_spec),\n            outer_dims=(batch_size,),\n            seed=4)\n        action_input_values = self.evaluate(action_inputs)\n        action_input_tensors = tf.nest.map_structure(tf.convert_to_tensor,\n                                                     action_input_values)\n\n        action_output = policy.action(*action_input_tensors, seed=action_seed)\n        distribution_output = policy.distribution(*action_input_tensors)\n        self.assertIsInstance(\n            distribution_output.action, tfp.distributions.Distribution)\n\n        self.evaluate(tf.compat.v1.global_variables_initializer())\n\n        action_output_dict = dict(((spec.name, value) for (spec, value) in zip(\n            tf.nest.flatten(policy.policy_step_spec),\n            tf.nest.flatten(action_output))))\n\n        # Check output of the flattened signature call.\n        (action_output_value, action_output_dict) = self.evaluate(\n            (action_output, action_output_dict))\n\n        distribution_output_value = self.evaluate(_sample_from_distributions(\n            distribution_output))\n\n        input_fn_and_spec = None\n        if has_input_fn_and_spec:\n          input_fn_and_spec = (self._convert_string_vector_to_action_input,\n                               tf.TensorSpec((7,), tf.string, name=\'example\'))\n\n        saver = policy_saver.PolicySaver(\n            policy,\n            batch_size=None,\n            use_nest_path_signatures=False,\n            seed=action_seed,\n            input_fn_and_spec=input_fn_and_spec,\n            train_step=global_step)\n        path = os.path.join(self.get_temp_dir(), \'save_model_action\')\n        saver.save(path)\n\n    with tf.compat.v1.Graph().as_default():\n      tf.compat.v1.set_random_seed(self._global_seed)\n      with tf.compat.v1.Session().as_default():\n        reloaded = tf.compat.v2.saved_model.load(path)\n\n        self.assertIn(\'action\', reloaded.signatures)\n        reloaded_action = reloaded.signatures[\'action\']\n        if has_input_fn_and_spec:\n          self._compare_input_output_specs(\n              reloaded_action,\n              expected_input_specs=input_fn_and_spec[1],\n              expected_output_spec=policy.policy_step_spec,\n              batch_input=True)\n\n        else:\n          self._compare_input_output_specs(\n              reloaded_action,\n              expected_input_specs=(self._time_step_spec,\n                                    policy.policy_state_spec),\n              expected_output_spec=policy.policy_step_spec,\n              batch_input=True)\n\n        # Reload action_input_values as tensors in the new graph.\n        action_input_tensors = tf.nest.map_structure(tf.convert_to_tensor,\n                                                     action_input_values)\n\n        action_input_spec = (self._time_step_spec, policy.policy_state_spec)\n        function_action_input_dict = dict(\n            (spec.name, value) for (spec, value) in zip(\n                tf.nest.flatten(action_input_spec),\n                tf.nest.flatten(action_input_tensors)))\n\n        # NOTE(ebrevdo): The graph-level seeds for the policy and the reloaded\n        # model are equal, which in addition to seeding the call to action() and\n        # PolicySaver helps ensure equality of the output of action() in both\n        # cases.\n        self.assertEqual(reloaded_action.graph.seed, self._global_seed)\n\n        def match_dtype_shape(x, y, msg=None):\n          self.assertEqual(x.shape, y.shape, msg=msg)\n          self.assertEqual(x.dtype, y.dtype, msg=msg)\n\n        # The seed= argument for the SavedModel action call was given at\n        # creation of the PolicySaver.\n        if has_input_fn_and_spec:\n          action_string_vector = self._convert_action_input_to_string_vector(\n              action_input_tensors)\n          reloaded_action_output_dict = reloaded_action(action_string_vector)\n          reloaded_action_output = reloaded.action(action_string_vector)\n          reloaded_distribution_output = reloaded.distribution(\n              action_string_vector)\n          self.assertIsInstance(reloaded_distribution_output.action,\n                                tfp.distributions.Distribution)\n\n        else:\n          # This is the flat-signature function.\n          reloaded_action_output_dict = reloaded_action(\n              **function_action_input_dict)\n          # This is the non-flat function.\n          reloaded_action_output = reloaded.action(*action_input_tensors)\n          reloaded_distribution_output = reloaded.distribution(\n              *action_input_tensors)\n          self.assertIsInstance(reloaded_distribution_output.action,\n                                tfp.distributions.Distribution)\n\n          if not has_state:\n            # Try both cases: one with an empty policy_state and one with no\n            # policy_state.  Compare them.\n\n            # NOTE(ebrevdo): The first call to .action() must be stored in\n            # reloaded_action_output because this is the version being compared\n            # later against the true action_output and the values will change\n            # after the first call due to randomness.\n            reloaded_action_output_no_input_state = reloaded.action(\n                action_input_tensors[0])\n            reloaded_distribution_output_no_input_state = reloaded.distribution(\n                action_input_tensors[0])\n            # Even with a seed, multiple calls to action will get different\n            # values, so here we just check the signature matches.\n            self.assertIsInstance(\n                reloaded_distribution_output_no_input_state.action,\n                tfp.distributions.Distribution)\n            tf.nest.map_structure(match_dtype_shape,\n                                  reloaded_action_output_no_input_state,\n                                  reloaded_action_output)\n\n            tf.nest.map_structure(\n                match_dtype_shape,\n                _sample_from_distributions(\n                    reloaded_distribution_output_no_input_state),\n                _sample_from_distributions(reloaded_distribution_output))\n\n        self.evaluate(tf.compat.v1.global_variables_initializer())\n        (reloaded_action_output_dict,\n         reloaded_action_output_value) = self.evaluate(\n             (reloaded_action_output_dict, reloaded_action_output))\n\n        reloaded_distribution_output_value = self.evaluate(\n            _sample_from_distributions(reloaded_distribution_output))\n\n        self.assertAllEqual(action_output_dict.keys(),\n                            reloaded_action_output_dict.keys())\n\n        for k in action_output_dict:\n          if seeded:\n            self.assertAllClose(\n                action_output_dict[k],\n                reloaded_action_output_dict[k],\n                msg=\'\\nMismatched dict key: %s.\' % k)\n          else:\n            match_dtype_shape(\n                action_output_dict[k],\n                reloaded_action_output_dict[k],\n                msg=\'\\nMismatch dict key: %s.\' % k)\n\n        # With non-signature functions, we can check that passing a seed does\n        # the right thing the second time.\n        if seeded:\n          tf.nest.map_structure(self.assertAllClose, action_output_value,\n                                reloaded_action_output_value)\n        else:\n          tf.nest.map_structure(match_dtype_shape, action_output_value,\n                                reloaded_action_output_value)\n\n        tf.nest.map_structure(self.assertAllClose,\n                              distribution_output_value,\n                              reloaded_distribution_output_value)\n\n  def testSaveGetInitialState(self):\n    network = q_rnn_network.QRnnNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver_nobatch = policy_saver.PolicySaver(\n        policy,\n        train_step=train_step,\n        batch_size=None,\n        use_nest_path_signatures=False)\n    path = os.path.join(self.get_temp_dir(), \'save_model_initial_state_nobatch\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    with self.cached_session():\n      saver_nobatch.save(path)\n      reloaded_nobatch = tf.compat.v2.saved_model.load(path)\n      self.evaluate(\n          tf.compat.v1.initializers.variables(reloaded_nobatch.model_variables))\n\n    self.assertIn(\'get_initial_state\', reloaded_nobatch.signatures)\n    reloaded_get_initial_state = (\n        reloaded_nobatch.signatures[\'get_initial_state\'])\n    self._compare_input_output_specs(\n        reloaded_get_initial_state,\n        expected_input_specs=(tf.TensorSpec(\n            dtype=tf.int32, shape=(), name=\'batch_size\'),),\n        expected_output_spec=policy.policy_state_spec,\n        batch_input=False,\n        batch_size=None)\n\n    initial_state = policy.get_initial_state(batch_size=3)\n    initial_state = self.evaluate(initial_state)\n\n    reloaded_nobatch_initial_state = reloaded_nobatch.get_initial_state(\n        batch_size=3)\n    reloaded_nobatch_initial_state = self.evaluate(\n        reloaded_nobatch_initial_state)\n    tf.nest.map_structure(self.assertAllClose, initial_state,\n                          reloaded_nobatch_initial_state)\n\n    saver_batch = policy_saver.PolicySaver(\n        policy,\n        train_step=train_step,\n        batch_size=3,\n        use_nest_path_signatures=False)\n    path = os.path.join(self.get_temp_dir(), \'save_model_initial_state_batch\')\n    with self.cached_session():\n      saver_batch.save(path)\n      reloaded_batch = tf.compat.v2.saved_model.load(path)\n      self.evaluate(\n          tf.compat.v1.initializers.variables(reloaded_batch.model_variables))\n    self.assertIn(\'get_initial_state\', reloaded_batch.signatures)\n    reloaded_get_initial_state = reloaded_batch.signatures[\'get_initial_state\']\n    self._compare_input_output_specs(\n        reloaded_get_initial_state,\n        expected_input_specs=(),\n        expected_output_spec=policy.policy_state_spec,\n        batch_input=False,\n        batch_size=3)\n\n    reloaded_batch_initial_state = reloaded_batch.get_initial_state()\n    reloaded_batch_initial_state = self.evaluate(reloaded_batch_initial_state)\n    tf.nest.map_structure(self.assertAllClose, initial_state,\n                          reloaded_batch_initial_state)\n\n  def testNoSpecMissingOrColliding(self):\n    spec_names = set()\n    flat_spec = tf.nest.flatten(self._time_step_spec)\n    missing_or_colliding = [\n        policy_saver._true_if_missing_or_collision(s, spec_names)\n        for s in flat_spec\n    ]\n\n    self.assertFalse(any(missing_or_colliding))\n\n  def testTrueIfMissing(self):\n    time_step_spec = self._time_step_spec._replace(\n        observation=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32, shape=(4,), minimum=-10.0, maximum=10.0))\n    spec_names = set()\n    flat_spec = tf.nest.flatten(time_step_spec)\n    missing_or_colliding = [\n        policy_saver._true_if_missing_or_collision(s, spec_names)\n        for s in flat_spec\n    ]\n\n    self.assertTrue(any(missing_or_colliding))\n\n  def testTrueIfCollision(self):\n    time_step_spec = self._time_step_spec._replace(\n        observation=tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32,\n            shape=(4,),\n            name=\'st\',\n            minimum=-10.0,\n            maximum=10.0))\n    spec_names = set()\n    flat_spec = tf.nest.flatten(time_step_spec)\n    missing_or_colliding = [\n        policy_saver._true_if_missing_or_collision(s, spec_names)\n        for s in flat_spec\n    ]\n\n    self.assertTrue(any(missing_or_colliding))\n\n  def testRenameSpecWithNestPaths(self):\n    time_step_spec = self._time_step_spec._replace(observation=[\n        tensor_spec.TensorSpec(\n            dtype=tf.float32,\n            shape=(4,),\n            name=\'obs1\',\n        ),\n        tensor_spec.TensorSpec(\n            dtype=tf.float32,\n            shape=(4,),\n            name=\'obs1\',\n        )\n    ])\n\n    renamed_spec = policy_saver._rename_spec_with_nest_paths(time_step_spec)\n\n    new_names = [s.name for s in tf.nest.flatten(renamed_spec)]\n    self.assertAllEqual(\n        [\'step_type\', \'reward\', \'discount\', \'observation/0\', \'observation/1\'],\n        new_names)\n\n  def testTrainStepSaved(self):\n    # We need to use one default session so that self.evaluate and the\n    # SavedModel loader share the same session.\n    with tf.compat.v1.Session().as_default():\n      network = q_network.QNetwork(\n          input_tensor_spec=self._time_step_spec.observation,\n          action_spec=self._action_spec)\n\n      policy = q_policy.QPolicy(\n          time_step_spec=self._time_step_spec,\n          action_spec=self._action_spec,\n          q_network=network)\n      self.evaluate(tf.compat.v1.initializers.variables(policy.variables()))\n\n      train_step = common.create_variable(\'train_step\', initial_value=7)\n      self.evaluate(tf.compat.v1.initializers.variables([train_step]))\n\n      saver = policy_saver.PolicySaver(\n          policy, batch_size=None, train_step=train_step)\n      if tf.executing_eagerly():\n        step = saver.get_train_step()\n      else:\n        step = self.evaluate(saver.get_train_step())\n      self.assertEqual(7, step)\n      path = os.path.join(self.get_temp_dir(), \'save_model\')\n      saver.save(path)\n\n      reloaded = tf.compat.v2.saved_model.load(path)\n      self.assertIn(\'get_train_step\', reloaded.signatures)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      train_step_value = self.evaluate(reloaded.get_train_step())\n      self.assertEqual(7, train_step_value)\n      train_step = train_step.assign_add(3)\n      self.evaluate(train_step)\n      saver.save(path)\n\n      reloaded = tf.compat.v2.saved_model.load(path)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      train_step_value = self.evaluate(reloaded.get_train_step())\n      self.assertEqual(10, train_step_value)\n\n      # Also test passing SaveOptions.\n      train_step = train_step.assign_add(3)\n      self.evaluate(train_step)\n      path2 = os.path.join(self.get_temp_dir(), \'save_model2\')\n      saver.save(\n          path2,\n          options=tf.saved_model.SaveOptions(\n              experimental_io_device=\'/job:localhost\'))\n      reloaded = tf.compat.v2.saved_model.load(path2)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      train_step_value = self.evaluate(reloaded.get_train_step())\n      self.assertEqual(13, train_step_value)\n\n  def testTrainStepNotSaved(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in TF2.x. Step is required in TF1.x\')\n\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    saver = policy_saver.PolicySaver(policy, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    saver.save(path)\n    reloaded = tf.compat.v2.saved_model.load(path)\n\n    self.assertIn(\'get_train_step\', reloaded.signatures)\n    train_step_value = self.evaluate(reloaded.get_train_step())\n    self.assertEqual(-1, train_step_value)\n\n  def testMetadataSaved(self):\n    # We need to use one default session so that self.evaluate and the\n    # SavedModel loader share the same session.\n    with tf.compat.v1.Session().as_default():\n      network = q_network.QNetwork(\n          input_tensor_spec=self._time_step_spec.observation,\n          action_spec=self._action_spec)\n\n      policy = q_policy.QPolicy(\n          time_step_spec=self._time_step_spec,\n          action_spec=self._action_spec,\n          q_network=network)\n      self.evaluate(tf.compat.v1.initializers.variables(policy.variables()))\n\n      train_step = common.create_variable(\'train_step\', initial_value=1)\n      env_step = common.create_variable(\'env_step\', initial_value=7)\n      metadata = {\'env_step\': env_step}\n      self.evaluate(tf.compat.v1.initializers.variables([train_step, env_step]))\n\n      saver = policy_saver.PolicySaver(\n          policy, batch_size=None, train_step=train_step, metadata=metadata)\n      if tf.executing_eagerly():\n        loaded_metadata = saver.get_metadata()\n      else:\n        loaded_metadata = self.evaluate(saver.get_metadata())\n      self.assertEqual(self.evaluate(metadata), loaded_metadata)\n\n      path = os.path.join(self.get_temp_dir(), \'save_model\')\n      saver.save(path)\n\n      reloaded = tf.compat.v2.saved_model.load(path)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      self.assertIn(\'get_metadata\', reloaded.signatures)\n      env_step_value = self.evaluate(reloaded.get_metadata())[\'env_step\']\n      self.assertEqual(7, env_step_value)\n\n  def testVariablesAccessible(self):\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    with self.cached_session():\n      saver.save(path)\n    reloaded = tf.compat.v2.saved_model.load(path)\n    self.evaluate(tf.compat.v1.initializers.variables(reloaded.model_variables))\n\n    model_variables = self.evaluate(policy.variables())\n    reloaded_model_variables = self.evaluate(reloaded.model_variables)\n\n    assert_np_all_equal = lambda a, b: self.assertTrue(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_all_equal, model_variables,\n                          reloaded_model_variables, check_types=False)\n\n  def testDistributionNotImplemented(self):\n    policy = PolicyNoDistribution()\n\n    with self.assertRaisesRegex(\n        NotImplementedError, \'_distribution has not been implemented\'):\n      policy.distribution(\n          ts.TimeStep(step_type=(), reward=(), discount=(), observation=()))\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    with self.cached_session():\n      saver.save(path)\n\n    reloaded = tf.compat.v2.saved_model.load(path)\n    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                 \'_distribution has not been implemented\'):\n      self.evaluate(\n          reloaded.distribution(\n              ts.TimeStep(step_type=(), reward=(), discount=(), observation=()))\n      )\n\n  def testCheckpointSave(self):\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    with self.cached_session():\n      saver.save(path)\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    with self.cached_session():\n      saver.save_checkpoint(checkpoint_path)\n    self.assertTrue(tf.compat.v2.io.gfile.exists(checkpoint_path))\n\n    # Also test CheckpointOptions\n    checkpoint2_path = os.path.join(self.get_temp_dir(), \'checkpoint2\')\n    options = tf.train.CheckpointOptions(\n        experimental_io_device=\'/job:localhost\')\n    with self.cached_session():\n      saver.save_checkpoint(checkpoint2_path, options=options)\n    self.assertTrue(tf.compat.v2.io.gfile.exists(checkpoint2_path))\n\n  def testUpdateWithCheckpoint(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in TF2.x.\')\n\n    # Create and saved_model for a q_policy.\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    saver = policy_saver.PolicySaver(policy, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    saver.save(path)\n\n    # Assign -1 to all variables in the policy. Making checkpoint different than\n    # the initial saved_model.\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + -1),\n                              policy.variables()))\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    saver.save_checkpoint(checkpoint_path)\n\n    # Get an instance of the saved_model.\n    reloaded_policy = tf.compat.v2.saved_model.load(path)\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n\n    # Verify loaded saved_model variables are different than the current policy.\n    model_variables = self.evaluate(policy.variables())\n    reloaded_model_variables = self.evaluate(reloaded_policy.model_variables)\n\n    any_not_equal = lambda a, b: np.not_equal(a, b).any()\n    self.assertTrue(\n        any(any_not_equal(a, b)\n            for a, b in zip(tf.nest.flatten(model_variables),\n                            tf.nest.flatten(reloaded_model_variables))))\n\n    # Update from checkpoint.\n    checkpoint = tf.train.Checkpoint(policy=reloaded_policy)\n    checkpoint_file_prefix = os.path.join(checkpoint_path, \'variables\',\n                                          \'variables\')\n    checkpoint.read(checkpoint_file_prefix).assert_existing_objects_matched()\n\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n\n    # Verify variables are now equal.\n    model_variables = self.evaluate(policy.variables())\n    reloaded_model_variables = self.evaluate(reloaded_policy.model_variables)\n\n    assert_np_all_equal = lambda a, b: self.assertTrue(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_all_equal,\n                          model_variables,\n                          reloaded_model_variables,\n                          check_types=False)\n\n  def testInferenceWithCheckpoint(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in TF2.x.\')\n\n    # Create and saved_model for a q_policy.\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n    sample_input = self.evaluate(\n        tensor_spec.sample_spec_nest(self._time_step_spec, outer_dims=(3,)))\n\n    saver = policy_saver.PolicySaver(policy, batch_size=None)\n    path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    original_eval = self.evaluate(policy.action(sample_input))\n    saver.save(path)\n    # Asign -1 to all variables in the policy. Making checkpoint different than\n    # the initial saved_model.\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + -1),\n                              policy.variables()))\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    saver.save_checkpoint(checkpoint_path)\n\n    # Get an instance of the saved_model.\n    reloaded_policy = tf.compat.v2.saved_model.load(path)\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n\n    # Verify loaded saved_model variables are different than the current policy.\n    model_variables = self.evaluate(policy.variables())\n    reloaded_model_variables = self.evaluate(reloaded_policy.model_variables)\n\n    any_not_equal = lambda a, b: np.not_equal(a, b).any()\n    self.assertTrue(\n        any([any_not_equal(a, b)\n             for a, b in zip(tf.nest.flatten(model_variables),\n                             tf.nest.flatten(reloaded_model_variables))]))\n\n    # Update from checkpoint.\n    checkpoint = tf.train.Checkpoint(policy=reloaded_policy)\n    checkpoint_file_prefix = os.path.join(checkpoint_path, \'variables\',\n                                          \'variables\')\n    checkpoint.read(checkpoint_file_prefix).assert_existing_objects_matched()\n\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n\n    # Verify variables are now equal.\n    model_variables = self.evaluate(policy.variables())\n    reloaded_model_variables = self.evaluate(reloaded_policy.model_variables)\n\n    all_equal = lambda a, b: np.equal(a, b).all()\n    self.assertTrue(\n        all([all_equal(a, b)\n             for a, b in zip(tf.nest.flatten(model_variables),\n                             tf.nest.flatten(reloaded_model_variables))]))\n\n    # Verify variable update affects inference.\n    assert_np_not_equal = lambda a, b: self.assertFalse(np.equal(a, b).any())\n    reloaded_eval = self.evaluate(reloaded_policy.action(sample_input))\n    tf.nest.map_structure(assert_np_not_equal, original_eval, reloaded_eval)\n    current_eval = self.evaluate(policy.action(sample_input))\n    tf.nest.map_structure(assert_np_not_equal, current_eval, reloaded_eval)\n\n  def copy_tree(self, src_dir, dst_dir, skip_variables=False):\n    for src_root, _, files in os.walk(src_dir):\n      if src_root != src_dir:\n        rel_root = os.path.relpath(src_root, src_dir)\n      else:\n        rel_root = \'\'\n      if skip_variables and rel_root.startswith(\'variables\'):\n        continue\n      dst_root = os.path.join(dst_dir, rel_root)\n      if not os.path.exists(dst_root):\n        os.makedirs(dst_root)\n      for f in files:\n        shutil.copy(os.path.join(src_root, f), os.path.join(dst_root, f))\n\n  def testUpdateWithCompositeSavedModelAndCheckpoint(self):\n    # Create and saved_model for a q_policy.\n    network = q_network.QNetwork(\n        input_tensor_spec=self._time_step_spec.observation,\n        action_spec=self._action_spec)\n\n    policy = q_policy.QPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=self._action_spec,\n        q_network=network)\n\n    train_step = common.create_variable(\'train_step\', initial_value=0)\n    saver = policy_saver.PolicySaver(\n        policy, train_step=train_step, batch_size=None)\n    full_model_path = os.path.join(self.get_temp_dir(), \'save_model\')\n\n    def assert_val_equal_var(val, var):\n      self.assertTrue(np.array_equal(np.full_like(var, val), var))\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # Set all variables in the saved model to 1\n    variables = policy.variables()\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + 1), variables))\n    for v in self.evaluate(variables):\n      assert_val_equal_var(1, v)\n    with self.cached_session():\n      saver.save(full_model_path)\n\n    # Assign 2 to all variables in the policy. Making checkpoint different than\n    # the initial saved_model.\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + 2), variables))\n    for v in self.evaluate(variables):\n      assert_val_equal_var(2, v)\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    with self.cached_session():\n      saver.save_checkpoint(checkpoint_path)\n\n    # Reload the full model and check all variables are 1\n    reloaded_policy = tf.compat.v2.saved_model.load(full_model_path)\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n    for v in self.evaluate(reloaded_policy.model_variables):\n      assert_val_equal_var(1, v)\n\n    # Compose a new full saved model from the original saved model files\n    # and variables from the checkpoint.\n    composite_path = os.path.join(self.get_temp_dir(), \'composite_model\')\n    self.copy_tree(full_model_path, composite_path, skip_variables=True)\n    self.copy_tree(checkpoint_path, os.path.join(composite_path))\n\n    # Reload the composite model and check all variables are 2\n    reloaded_policy = tf.compat.v2.saved_model.load(composite_path)\n    self.evaluate(\n        tf.compat.v1.initializers.variables(reloaded_policy.model_variables))\n    for v in self.evaluate(reloaded_policy.model_variables):\n      assert_val_equal_var(2, v)\n\n  def _compare_input_output_specs(self,\n                                  function,\n                                  expected_input_specs,\n                                  expected_output_spec,\n                                  batch_input,\n                                  batch_size=None):\n    args, kwargs = function.structured_input_signature\n    self.assertFalse(args)\n\n    def expected_spec(spec, include_batch_dimension):\n      if include_batch_dimension:\n        return tf.TensorSpec(\n            dtype=spec.dtype,\n            shape=tf.TensorShape([batch_size]).concatenate(spec.shape),\n            name=spec.name)\n      else:\n        return spec\n\n    expected_input_spec_dict = dict(\n        (spec.name, expected_spec(spec, include_batch_dimension=batch_input))\n        for spec in tf.nest.flatten(expected_input_specs))\n    expected_output_spec_dict = dict(\n        (spec.name, expected_spec(spec, include_batch_dimension=True))\n        for spec in tf.nest.flatten(expected_output_spec))\n\n    self.assertEqual(kwargs, expected_input_spec_dict)\n    self.assertEqual(function.structured_outputs, expected_output_spec_dict)\n\n\ndef _sample_from_distributions(x):\n  def _convert(d):\n    return (d.sample((), seed=1234)\n            if isinstance(d, tfp.distributions.Distribution)\n            else d)\n\n  return tf.nest.map_structure(_convert, x)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/py_epsilon_greedy_policy.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""epsilon-greedy policy in python.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional\n\nimport numpy as np\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.typing import types\n\n\nclass EpsilonGreedyPolicy(py_policy.PyPolicy):\n  """"""Implementation of the epsilon-greedy policy.""""""\n\n  def __init__(self,\n               greedy_policy: py_policy.PyPolicy,\n               epsilon: types.Float,\n               random_policy: Optional[random_py_policy.RandomPyPolicy] = None,\n               epsilon_decay_end_count: Optional[types.Float] = None,\n               epsilon_decay_end_value: Optional[types.Float] = None,\n               random_seed: Optional[types.Seed] = None):\n    """"""Initializes the epsilon-greedy policy.\n\n    Args:\n      greedy_policy: An instance of py_policy.PyPolicy to use as the greedy\n        policy.\n      epsilon: The probability 0.0 <= epsilon <= 1.0 with which an\n        action will be selected at random.\n      random_policy: An instance of random_py_policy.RandomPyPolicy to\n        use as the random policy, if None is provided, a\n        RandomPyPolicy will be automatically created with the\n        greedy_policy\'s action_spec and observation_spec and\n        random_seed.\n      epsilon_decay_end_count: if set, anneal the epislon every time\n        this policy is used, until it hits the epsilon_decay_end_value.\n      epsilon_decay_end_value: the value of epislon to use when the\n        policy usage count hits epsilon_decay_end_count.\n      random_seed: seed used to create numpy.random.RandomState.\n        /dev/urandom will be used if it\'s None.\n\n    Raises:\n      ValueError: If epsilon is not between 0.0 and 1.0. Or if\n      epsilon_decay_end_value is invalid when epsilon_decay_end_count is\n      set.\n    """"""\n    if not 0 <= epsilon <= 1.0:\n      raise ValueError(\'epsilon should be in [0.0, 1.0]\')\n\n    self._greedy_policy = greedy_policy\n    if random_policy is None:\n      self._random_policy = random_py_policy.RandomPyPolicy(\n          time_step_spec=greedy_policy.time_step_spec,\n          action_spec=greedy_policy.action_spec,\n          seed=random_seed)\n    else:\n      self._random_policy = random_policy\n    # TODO(b/110841809) consider making epsilon be provided by a function.\n    self._epsilon = epsilon\n    self._epsilon_decay_end_count = epsilon_decay_end_count\n    if epsilon_decay_end_count is not None:\n      if epsilon_decay_end_value is None or epsilon_decay_end_value >= epsilon:\n        raise ValueError(\'Invalid value for epsilon_decay_end_value {}\'.format(\n            epsilon_decay_end_value))\n      self._epsilon_decay_step_factor = float(\n          epsilon - epsilon_decay_end_value) / epsilon_decay_end_count\n    self._epsilon_decay_end_value = epsilon_decay_end_value\n\n    self._random_seed = random_seed  # Keep it for copy method.\n    self._rng = np.random.RandomState(random_seed)\n\n    # Total times action method has been called.\n    self._count = 0\n\n    super(EpsilonGreedyPolicy, self).__init__(greedy_policy.time_step_spec,\n                                              greedy_policy.action_spec,\n                                              greedy_policy.policy_state_spec,\n                                              greedy_policy.info_spec)\n\n  def _get_initial_state(self, batch_size):\n    self._random_policy.get_initial_state(batch_size=batch_size)\n    return self._greedy_policy.get_initial_state(batch_size=batch_size)\n\n  def _get_epsilon(self):\n    if self._epsilon_decay_end_count is not None:\n      if self._count >= self._epsilon_decay_end_count:\n        return self._epsilon_decay_end_value\n      else:\n        return (self._epsilon - (self._count - 1) *\n                self._epsilon_decay_step_factor)\n    else:\n      return self._epsilon\n\n  def _random_function(self):\n    return self._rng.rand()\n\n  def _action(self, time_step, policy_state=()):\n    self._count += 1\n    # _random_function()\'s range should be [0, 1), so if epsilon is 1,\n    # we should always use random policy, and if epislon is 0, it\n    # should always use greedy_policy since the if condition won\'t be\n    # met.\n    if self._random_function() < self._get_epsilon():\n      # Avoid mixing policy_state from greedy_policy and random_policy,\n      # always return policy_state from greedy_policy.\n      action_step = self._random_policy.action(time_step)\n      return policy_step.PolicyStep(action_step.action, policy_state)\n    else:\n      return self._greedy_policy.action(time_step, policy_state=policy_state)\n'"
tf_agents/policies/py_epsilon_greedy_policy_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for py_epsilon_greedy_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing.absltest import mock\nfrom tf_agents.policies import py_epsilon_greedy_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.utils import test_utils\n\n\nclass EpsilonGreedyPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(EpsilonGreedyPolicyTest, self).setUp()\n    self.greedy_policy = mock.MagicMock()\n    self.random_policy = mock.MagicMock()\n    self.random_policy.action.return_value = policy_step.PolicyStep(0, ())\n\n  def testCtorAutoRandomPolicy(self):\n    self.greedy_policy.action_spec = mock.MagicMock()\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 0.5)\n    self.assertEqual(self.greedy_policy.action_spec,\n                     policy._random_policy.action_spec)\n\n  def testCtorValueErrorNegativeEpsilon(self):\n    with self.assertRaises(ValueError):\n      py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n          self.greedy_policy, -0.00001, random_policy=self.random_policy)\n\n  def testCtorValueErrorEpsilonMorThanOne(self):\n    with self.assertRaises(ValueError):\n      py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n          self.greedy_policy, 1.00001, random_policy=self.random_policy)\n\n  def testCtorValueErrorMissingEpsilonEndValue(self):\n    with self.assertRaises(ValueError):\n      py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n          self.greedy_policy, 0.99,\n          random_policy=self.random_policy,\n          epsilon_decay_end_count=100)\n\n  def testZeroState(self):\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 0.5, random_policy=self.random_policy)\n    policy.get_initial_state()\n    self.greedy_policy.get_initial_state.assert_called_once_with(\n        batch_size=None)\n    self.random_policy.get_initial_state.assert_called_once_with(\n        batch_size=None)\n\n  def testActionAlwaysRandom(self):\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 1, random_policy=self.random_policy)\n    time_step = mock.MagicMock()\n    for _ in range(5):\n      policy.action(time_step)\n    self.random_policy.action.assert_called_with(time_step)\n    self.assertEqual(5, self.random_policy.action.call_count)\n    self.assertEqual(0, self.greedy_policy.action.call_count)\n\n  def testActionAlwaysGreedy(self):\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 0, random_policy=self.random_policy)\n    time_step = mock.MagicMock()\n    for _ in range(5):\n      policy.action(time_step)\n    self.greedy_policy.action.assert_called_with(time_step, policy_state=())\n    self.assertEqual(0, self.random_policy.action.call_count)\n    self.assertEqual(5, self.greedy_policy.action.call_count)\n\n  def testActionSelection(self):\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 0.9, random_policy=self.random_policy)\n    time_step = mock.MagicMock()\n    # Replace the random generator with fixed behaviour\n    random = mock.MagicMock()\n    policy._rng = random\n\n    # 0.8 < 0.9, so random policy should be used.\n    policy._rng.rand.return_value = 0.8\n    policy.action(time_step)\n    self.random_policy.action.assert_called_with(time_step)\n    self.assertEqual(1, self.random_policy.action.call_count)\n    self.assertEqual(0, self.greedy_policy.action.call_count)\n\n    # 0.91 > 0.9, so greedy policy should be used.\n    policy._rng.rand.return_value = 0.91\n    policy.action(time_step)\n    self.greedy_policy.action.assert_called_with(time_step, policy_state=())\n    self.assertEqual(1, self.random_policy.action.call_count)\n    self.assertEqual(1, self.greedy_policy.action.call_count)\n\n  def testActionSelectionWithEpsilonDecay(self):\n    policy = py_epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self.greedy_policy, 0.9, random_policy=self.random_policy,\n        epsilon_decay_end_count=10,\n        epsilon_decay_end_value=0.4)\n    time_step = mock.MagicMock()\n    # Replace the random generator with fixed behaviour\n    random = mock.MagicMock()\n    policy._rng = random\n\n    # 0.8 < 0.9 and 0.8 < 0.85, so random policy should be used.\n    policy._rng.rand.return_value = 0.8\n    for _ in range(2):\n      policy.action(time_step)\n      self.random_policy.action.assert_called_with(time_step)\n    self.assertEqual(2, self.random_policy.action.call_count)\n    self.assertEqual(0, self.greedy_policy.action.call_count)\n\n    # epislon will change from [0.8 to 0.4], and greedy policy should be used\n    for _ in range(8):\n      policy.action(time_step)\n      self.greedy_policy.action.assert_called_with(time_step, policy_state=())\n    self.assertEqual(2, self.random_policy.action.call_count)\n    self.assertEqual(8, self.greedy_policy.action.call_count)\n\n    # 0.399 < 0.4, random policy should be used.\n    policy._rng.rand.return_value = 0.399\n    self.random_policy.reset_mock()\n    for _ in range(5):\n      policy.action(time_step)\n      self.random_policy.action.assert_called_with(time_step)\n    self.assertEqual(5, self.random_policy.action.call_count)\n    # greedy policy should not be called any more\n    self.assertEqual(8, self.greedy_policy.action.call_count)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/policies/py_policy.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Python Policies API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\n\nimport abc\nfrom typing import Optional\n\nimport numpy as np\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass PyPolicy(object):\n  """"""Abstract base class for Python Policies.\n\n  The `action(time_step, policy_state)` method returns a PolicyStep named tuple\n  containing the following nested arrays:\n    `action`: The action to be applied on the environment.\n    `state`: The state of the policy (E.g. RNN state) to be fed into the next\n      call to action.\n    `info`: Optional side information such as action log probabilities.\n\n  For stateful policies, e.g. those containing RNNs, an initial policy state can\n  be obtained through a call to `get_initial_state()`.\n\n  Example of simple use in Python:\n\n    py_env = PyEnvironment()\n    policy = PyPolicy()\n\n    time_step = py_env.reset()\n    policy_state = policy.get_initial_state()\n\n    acc_reward = 0\n    while not time_step.is_last():\n      action_step = policy.action(time_step, policy_state)\n      policy_state = action_step.state\n      time_step = py_env.step(action_step.action)\n      acc_reward += time_step.reward\n  """"""\n\n  # TODO(kbanoop): Expose a batched/batch_size property.\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               policy_state_spec: types.NestedArraySpec = (),\n               info_spec: types.NestedArraySpec = (),\n               observation_and_action_constraint_splitter: Optional[\n                   types.Splitter] = None):\n    """"""Initialization of PyPolicy class.\n\n    Args:\n      time_step_spec: A `TimeStep` ArraySpec of the expected time_steps. Usually\n        provided by the user to the subclass.\n      action_spec: A nest of BoundedArraySpec representing the actions. Usually\n        provided by the user to the subclass.\n      policy_state_spec: A nest of ArraySpec representing the policy state.\n        Provided by the subclass, not directly by the user.\n      info_spec: A nest of ArraySpec representing the policy info. Provided by\n        the subclass, not directly by the user.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment. The function takes in a full observation and returns a\n        tuple consisting of 1) the part of the observation intended as input to\n        the network and 2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as: ```\n        def observation_and_action_constraint_splitter(observation): return\n          observation[\'network_input\'], observation[\'constraint\'] ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n          sure the provided `q_network` is compatible with the network-specific\n          half of the output of the\n          `observation_and_action_constraint_splitter`. In particular,\n          `observation_and_action_constraint_splitter` will be called on the\n          observation before passing to the network. If\n          `observation_and_action_constraint_splitter` is None, action\n          constraints are not applied.\n    """"""\n    common.tf_agents_gauge.get_cell(\'TFAPolicy\').set(True)\n    common.assert_members_are_not_overridden(base_cls=PyPolicy, instance=self)\n    self._time_step_spec = time_step_spec\n    self._action_spec = action_spec\n    # TODO(kbanoop): rename policy_state to state.\n    self._policy_state_spec = policy_state_spec\n    self._info_spec = info_spec\n    self._setup_specs()\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n\n  def _setup_specs(self):\n    self._policy_step_spec = policy_step.PolicyStep(\n        action=self._action_spec, state=self._policy_state_spec,\n        info=self._info_spec)\n    self._trajectory_spec = trajectory.from_transition(\n        self._time_step_spec, self._policy_step_spec, self._time_step_spec)\n    self._collect_data_spec = self._trajectory_spec\n\n  @property\n  def observation_and_action_constraint_splitter(\n      self) -> Optional[types.Splitter]:\n    return self._observation_and_action_constraint_splitter\n\n  def get_initial_state(self,\n                        batch_size: Optional[int] = None) -> types.NestedArray:\n    """"""Returns an initial state usable by the policy.\n\n    Args:\n      batch_size: An optional batch size.\n\n    Returns:\n      An initial policy state.\n    """"""\n    return self._get_initial_state(batch_size)\n\n  def action(\n      self, time_step: ts.TimeStep, policy_state: types.NestedArray = ()\n  ) -> policy_step.PolicyStep:\n    """"""Generates next action given the time_step and policy_state.\n\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: An optional previous policy_state.\n\n    Returns:\n      A PolicyStep named tuple containing:\n        `action`: A nest of action Arrays matching the `action_spec()`.\n        `state`: A nest of policy states to be fed into the next call to action.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n    return self._action(time_step, policy_state)\n\n  @property\n  def time_step_spec(self) -> ts.TimeStep:\n    """"""Describes the `TimeStep` np.Arrays expected by `action(time_step)`.\n\n    Returns:\n      A `TimeStep` namedtuple with `ArraySpec` objects instead of np.Array,\n      which describe the shape, dtype and name of each array expected by\n      `action()`.\n    """"""\n    return self._time_step_spec\n\n  @property\n  def action_spec(self) -> types.NestedArraySpec:\n    """"""Describes the ArraySpecs of the np.Array returned by `action()`.\n\n    `action` can be a single np.Array, or a nested dict, list or tuple of\n    np.Array.\n\n    Returns:\n      A single BoundedArraySpec, or a nested dict, list or tuple of\n      `BoundedArraySpec` objects, which describe the shape and\n      dtype of each np.Array returned by `action()`.\n    """"""\n    return self._action_spec\n\n  @property\n  def policy_state_spec(self) -> types.NestedArraySpec:\n    """"""Describes the arrays expected by functions with `policy_state` as input.\n\n    Returns:\n      A single BoundedArraySpec, or a nested dict, list or tuple of\n      `BoundedArraySpec` objects, which describe the shape and\n      dtype of each np.Array returned by `action()`.\n    """"""\n    return self._policy_state_spec\n\n  @property\n  def info_spec(self) -> types.NestedArraySpec:\n    """"""Describes the Arrays emitted as info by `action()`.\n\n    Returns:\n      A nest of ArraySpec which describe the shape and dtype of each Array\n      emitted as `info` by `action()`.\n    """"""\n    return self._info_spec\n\n  @property\n  def policy_step_spec(self) -> policy_step.PolicyStep:\n    """"""Describes the output of `action()`.\n\n    Returns:\n      A nest of ArraySpec which describe the shape and dtype of each Array\n      emitted by `action()`.\n    """"""\n    return self._policy_step_spec\n\n  @property\n  def trajectory_spec(self) -> trajectory.Trajectory:\n    """"""Describes the data collected when using this policy with an environment.\n\n    Returns:\n      A `Trajectory` containing all array specs associated with the\n      time_step_spec and policy_step_spec of this policy.\n    """"""\n    return self._trajectory_spec\n\n  @property\n  def collect_data_spec(self) -> trajectory.Trajectory:\n    """"""Describes the data collected when using this policy with an environment.\n\n    Returns:\n      A nest of ArraySpecs which describe the shape and dtype of each array\n      required to train the agent which generated this policy.\n    """"""\n    return self._collect_data_spec\n\n  @abc.abstractmethod\n  def _action(self, time_step: ts.TimeStep,\n              policy_state: types.NestedArray) -> policy_step.PolicyStep:\n    """"""Implementation of `action`.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: An Array, or a nested dict, list or tuple of\n        Arrays representing the previous policy_state.\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n        `action`: A nest of action Arrays matching the `action_spec()`.\n        `state`: A nest of policy states to be fed into the next call to action.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n\n  def _get_initial_state(self, batch_size: int) -> types.NestedArray:\n    """"""Default implementation of `get_initial_state`.\n\n    This implementation returns arrays of all zeros matching `batch_size` and\n    spec `self.policy_state_spec`.\n\n    Args:\n      batch_size: The batch shape.\n\n    Returns:\n      A nested object of type `policy_state` containing properly\n      initialized Arrays.\n    """"""\n    def _zero_array(spec):\n      if batch_size is None:\n        shape = spec.shape\n      else:\n        shape = (batch_size,) + spec.shape\n      return np.zeros(shape, spec.dtype)\n\n    return tf.nest.map_structure(_zero_array, self._policy_state_spec)\n'"
tf_agents/policies/py_tf_eager_policy.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Converts tf_policies when working in eager mode to py_policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport os\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\n@gin.configurable\nclass PyTFEagerPolicyBase(py_policy.PyPolicy):\n  """"""Base class for py_policy instances of TF policies in Eager mode.\n\n  Handles adding and removing batch dimensions from the actions and time_steps.\n  Note if you have a tf_policy you should directly use the PyTFEagerPolicy class\n  instead of this Base.\n  """"""\n\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               policy_state_spec: types.NestedArraySpec,\n               info_spec: types.NestedArraySpec,\n               use_tf_function: bool = False):\n    self._policy = policy\n    if use_tf_function:\n      self._policy_action_fn = common.function(policy.action)\n    else:\n      self._policy_action_fn = policy.action\n    super(PyTFEagerPolicyBase, self).__init__(time_step_spec, action_spec,\n                                              policy_state_spec, info_spec)\n\n  def variables(self):\n    return tf.nest.map_structure(lambda t: t.numpy(), self._policy.variables())\n\n  def _get_initial_state(self, batch_size):\n    return self._policy.get_initial_state(batch_size=batch_size)\n\n  def _action(self, time_step, policy_state):\n    time_step = nest_utils.batch_nested_array(time_step)\n    # Avoid passing numpy arrays to avoid retracing of the tf.function.\n    time_step = tf.nest.map_structure(tf.convert_to_tensor, time_step)\n    policy_step = self._policy_action_fn(time_step, policy_state)\n    return policy_step._replace(\n        action=nest_utils.unbatch_nested_tensors_to_arrays(policy_step.action),\n        # We intentionally do not convert the `state` so it is outputted as the\n        # underlying policy generated it (i.e. in the form of a Tensor) which is\n        # not necessarily compatible with a py-policy. However, we do so since\n        # the `state` is fed back to the policy. So if it was converted, it\'d be\n        # required to convert back to the original form before calling the\n        # method `action` of the policy again in the next step. If one wants to\n        # store the `state` e.g. in replay buffer, then we suggest placing it\n        # into the `info` field.\n        info=nest_utils.unbatch_nested_tensors_to_arrays(policy_step.info))\n\n\n@gin.configurable\nclass PyTFEagerPolicy(PyTFEagerPolicyBase):\n  """"""Exposes a numpy API for TF policies in Eager mode.""""""\n\n  def __init__(self, policy: tf_policy.TFPolicy, use_tf_function: bool = False):\n    time_step_spec = tensor_spec.to_nest_array_spec(policy.time_step_spec)\n    action_spec = tensor_spec.to_nest_array_spec(policy.action_spec)\n    policy_state_spec = tensor_spec.to_nest_array_spec(policy.policy_state_spec)\n    info_spec = tensor_spec.to_nest_array_spec(policy.info_spec)\n    super(PyTFEagerPolicy,\n          self).__init__(policy, time_step_spec, action_spec, policy_state_spec,\n                         info_spec, use_tf_function)\n\n\n@gin.configurable\nclass SavedModelPyTFEagerPolicy(PyTFEagerPolicyBase):\n  """"""Exposes a numpy API for saved_model policies in Eager mode.""""""\n\n  def __init__(self,\n               model_path: Text,\n               time_step_spec: Optional[ts.TimeStep] = None,\n               action_spec: Optional[types.NestedTensorSpec] = None,\n               policy_state_spec: types.NestedTensorSpec = (),\n               info_spec: types.NestedTensorSpec = (),\n               load_specs_from_pbtxt: bool = False):\n    """"""Initializes a PyPolicy from a saved_model.\n\n    *Note* (b/151318119): BoundedSpecs are converted to regular specs when saved\n    into a proto as the `nested_structure_coder` from TF currently doesn\'t\n    handle BoundedSpecs. Shape and dtypes will still match the original specs.\n\n    Args:\n      model_path: Path to a saved_model generated by the `policy_saver`.\n      time_step_spec: Optional nested structure of ArraySpecs describing the\n        policy\'s `time_step_spec`. This is not used by the\n        SavedModelPyTFEagerPolicy, but may be accessed by other objects as it is\n        part of the public policy API.\n      action_spec: Optional nested structure of `ArraySpecs` describing the\n        policy\'s `action_spec`. This is not used by the\n        SavedModelPyTFEagerPolicy, but may be accessed by other objects as it is\n        part of the public policy API.\n      policy_state_spec: Optional nested structure of `ArraySpecs` describing\n        the policy\'s `policy_state_spec`. This is not used by the\n        SavedModelPyTFEagerPolicy, but may be accessed by other objects as it is\n        part of the public policy API.\n      info_spec: Optional nested structure of `ArraySpecs` describing the\n        policy\'s `info_spec`. This is not used by the SavedModelPyTFEagerPolicy,\n        but may be accessed by other objects as it is part of the public policy\n        API.\n      load_specs_from_pbtxt: If True the specs will be loaded from the proto\n        file generated by the `policy_saver`.\n    """"""\n    policy = tf.compat.v2.saved_model.load(model_path)\n    self._checkpoint = tf.train.Checkpoint(policy=policy)\n    if not (time_step_spec or load_specs_from_pbtxt):\n      raise ValueError(\n          \'To load a SavedModel policy you have to provide the specs, or\'\n          \'enable loading from proto.\')\n    policy_specs = None\n    if not time_step_spec and load_specs_from_pbtxt:\n      spec_path = os.path.join(model_path, policy_saver.POLICY_SPECS_PBTXT)\n      policy_specs = policy_saver.specs_from_collect_data_spec(\n          tensor_spec.from_pbtxt_file(spec_path))\n      time_step_spec = policy_specs[\'time_step_spec\']\n      action_spec = policy_specs[\'action_spec\']\n      policy_state_spec = policy_specs[\'policy_state_spec\']\n      info_spec = policy_specs[\'info_spec\']\n    super(SavedModelPyTFEagerPolicy,\n          self).__init__(policy, time_step_spec, action_spec, policy_state_spec,\n                         info_spec)\n    # Override collect data_spec with whatever was loaded instead of relying\n    # on trajectory_data_spec.\n    if policy_specs:\n      self._collect_data_spec = policy_specs[\'collect_data_spec\']\n\n  def get_train_step(self) -> types.Int:\n    """"""Returns the training global step of the saved model.""""""\n    return self._policy.get_train_step().numpy()\n\n  def get_metadata(self):\n    """"""Returns the metadata of the saved model.""""""\n    return self._policy.get_metadata()\n\n  def variables(self):\n    return self._policy.model_variables\n\n  def update_from_checkpoint(self, checkpoint_path: Text):\n    """"""Allows users to update saved_model variables directly from a checkpoint.\n\n    `checkpoint_path` is a path that was passed to either `PolicySaver.save()`\n    or `PolicySaver.save_checkpoint()`. The policy looks for set of checkpoint\n    files with the file prefix `<checkpoint_path>/variables/variables\'\n\n    Args:\n      checkpoint_path: Path to the checkpoint to restore and use to udpate this\n        policy.\n    """"""\n    file_prefix = os.path.join(checkpoint_path,\n                               tf.saved_model.VARIABLES_DIRECTORY,\n                               tf.saved_model.VARIABLES_FILENAME)\n    status = self._checkpoint.read(file_prefix)\n    # Check that all the variables in the policy were updated, but allow the\n    # checkpoint to have additional variables. This helps sharing checkpoints\n    # across policies.\n    status.assert_existing_objects_matched().expect_partial()\n'"
tf_agents/policies/py_tf_eager_policy_test.py,11,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.policies.eager_tf_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import test_utils\n\n\nclass PyTFEagerPolicyTest(test_utils.TestCase):\n\n  def testPyEnvCompatible(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in eager.\')\n\n    observation_spec = array_spec.ArraySpec([2], np.float32)\n    action_spec = array_spec.BoundedArraySpec([1], np.float32, 2, 3)\n\n    observation_tensor_spec = tensor_spec.from_spec(observation_spec)\n    action_tensor_spec = tensor_spec.from_spec(action_spec)\n    time_step_tensor_spec = ts.time_step_spec(observation_tensor_spec)\n\n    actor_net = actor_network.ActorNetwork(\n        observation_tensor_spec,\n        action_tensor_spec,\n        fc_layer_params=(10,),\n    )\n\n    tf_policy = actor_policy.ActorPolicy(\n        time_step_tensor_spec, action_tensor_spec, actor_network=actor_net)\n\n    py_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_policy)\n    # Env will validate action types automaticall since we provided the\n    # action_spec.\n    env = random_py_environment.RandomPyEnvironment(observation_spec,\n                                                    action_spec)\n\n    time_step = env.reset()\n\n    for _ in range(100):\n      action_step = py_policy.action(time_step)\n      time_step = env.step(action_step.action)\n\n  def testRandomTFPolicyCompatibility(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in eager.\')\n\n    observation_spec = array_spec.ArraySpec([2], np.float32)\n    action_spec = array_spec.BoundedArraySpec([1], np.float32, 2, 3)\n    info_spec = {\n        \'a\': array_spec.BoundedArraySpec([1], np.float32, 0, 1),\n        \'b\': array_spec.BoundedArraySpec([1], np.float32, 100, 101)\n    }\n\n    observation_tensor_spec = tensor_spec.from_spec(observation_spec)\n    action_tensor_spec = tensor_spec.from_spec(action_spec)\n    info_tensor_spec = tensor_spec.from_spec(info_spec)\n    time_step_tensor_spec = ts.time_step_spec(observation_tensor_spec)\n\n    tf_policy = random_tf_policy.RandomTFPolicy(\n        time_step_tensor_spec, action_tensor_spec, info_spec=info_tensor_spec)\n\n    py_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_policy)\n    env = random_py_environment.RandomPyEnvironment(observation_spec,\n                                                    action_spec)\n    time_step = env.reset()\n\n    def _check_action_step(action_step):\n      self.assertIsInstance(action_step.action, np.ndarray)\n      self.assertEqual(action_step.action.shape, (1,))\n      self.assertBetween(action_step.action[0], 2.0, 3.0)\n\n      self.assertIsInstance(action_step.info[\'a\'], np.ndarray)\n      self.assertEqual(action_step.info[\'a\'].shape, (1,))\n      self.assertBetween(action_step.info[\'a\'][0], 0.0, 1.0)\n\n      self.assertIsInstance(action_step.info[\'b\'], np.ndarray)\n      self.assertEqual(action_step.info[\'b\'].shape, (1,))\n      self.assertBetween(action_step.info[\'b\'][0], 100.0, 101.0)\n\n    for _ in range(100):\n      action_step = py_policy.action(time_step)\n      _check_action_step(action_step)\n      time_step = env.step(action_step.action)\n\n\nclass SavedModelPYTFEagerPolicyTest(test_utils.TestCase,\n                                    parameterized.TestCase):\n\n  def setUp(self):\n    super(SavedModelPYTFEagerPolicyTest, self).setUp()\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in eager.\')\n\n    observation_spec = array_spec.ArraySpec([2], np.float32)\n    self.action_spec = array_spec.BoundedArraySpec([1], np.float32, 2, 3)\n    self.time_step_spec = ts.time_step_spec(observation_spec)\n\n    observation_tensor_spec = tensor_spec.from_spec(observation_spec)\n    action_tensor_spec = tensor_spec.from_spec(self.action_spec)\n    time_step_tensor_spec = tensor_spec.from_spec(self.time_step_spec)\n\n    actor_net = actor_network.ActorNetwork(\n        observation_tensor_spec,\n        action_tensor_spec,\n        fc_layer_params=(10,),\n    )\n\n    self.tf_policy = actor_policy.ActorPolicy(\n        time_step_tensor_spec, action_tensor_spec, actor_network=actor_net)\n\n  def testSavedModel(self):\n    path = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    saver = policy_saver.PolicySaver(self.tf_policy)\n    saver.save(path)\n\n    eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n        path, self.time_step_spec, self.action_spec)\n    rng = np.random.RandomState()\n    sample_time_step = array_spec.sample_spec_nest(self.time_step_spec, rng)\n    batched_sample_time_step = nest_utils.batch_nested_array(sample_time_step)\n\n    original_action = self.tf_policy.action(batched_sample_time_step)\n    unbatched_original_action = nest_utils.unbatch_nested_tensors(\n        original_action)\n    original_action_np = tf.nest.map_structure(lambda t: t.numpy(),\n                                               unbatched_original_action)\n    saved_policy_action = eager_py_policy.action(sample_time_step)\n\n    tf.nest.assert_same_structure(saved_policy_action.action, self.action_spec)\n\n    np.testing.assert_array_almost_equal(original_action_np.action,\n                                         saved_policy_action.action)\n\n  def testSavedModelLoadingSpecs(self):\n    path = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    saver = policy_saver.PolicySaver(self.tf_policy)\n    saver.save(path)\n\n    eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n        path, load_specs_from_pbtxt=True)\n\n    # Bounded specs get converted to regular specs when saved into a proto.\n    def assert_specs_mostly_equal(loaded_spec, expected_spec):\n      self.assertEqual(loaded_spec.shape, expected_spec.shape)\n      self.assertEqual(loaded_spec.dtype, expected_spec.dtype)\n\n    tf.nest.map_structure(assert_specs_mostly_equal,\n                          eager_py_policy.time_step_spec, self.time_step_spec)\n    tf.nest.map_structure(assert_specs_mostly_equal,\n                          eager_py_policy.action_spec, self.action_spec)\n\n  @parameterized.parameters(None, 0, 100, 200000)\n  def testGetTrainStep(self, train_step):\n    path = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    if train_step is None:\n      # Use the default argument, which should set the train step to be -1.\n      saver = policy_saver.PolicySaver(self.tf_policy)\n      expected_train_step = -1\n    else:\n      saver = policy_saver.PolicySaver(\n          self.tf_policy,\n          train_step=common.create_variable(\n              \'train_step\', initial_value=train_step))\n      expected_train_step = train_step\n    saver.save(path)\n\n    eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n        path, self.time_step_spec, self.action_spec)\n\n    self.assertEqual(expected_train_step, eager_py_policy.get_train_step())\n\n  def testUpdateFromCheckpoint(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in TF2.x.\')\n\n    path = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    saver = policy_saver.PolicySaver(self.tf_policy)\n    saver.save(path)\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + -1),\n                              self.tf_policy.variables()))\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    saver.save_checkpoint(checkpoint_path)\n\n    eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n        path, self.time_step_spec, self.action_spec)\n\n    # Use evaluate to force a copy.\n    saved_model_variables = self.evaluate(eager_py_policy.variables())\n\n    eager_py_policy.update_from_checkpoint(checkpoint_path)\n\n    assert_np_not_equal = lambda a, b: self.assertFalse(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_not_equal, saved_model_variables,\n                          self.evaluate(eager_py_policy.variables()))\n\n    assert_np_all_equal = lambda a, b: self.assertTrue(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_all_equal,\n                          self.evaluate(self.tf_policy.variables()),\n                          self.evaluate(eager_py_policy.variables()),\n                          check_types=False)\n\n  def testInferenceFromCheckpoint(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(\'Only supported in TF2.x.\')\n\n    path = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    saver = policy_saver.PolicySaver(self.tf_policy)\n    saver.save(path)\n\n    rng = np.random.RandomState()\n    sample_time_step = array_spec.sample_spec_nest(self.time_step_spec, rng)\n    batched_sample_time_step = nest_utils.batch_nested_array(sample_time_step)\n\n    self.evaluate(\n        tf.nest.map_structure(lambda v: v.assign(v * 0 + -1),\n                              self.tf_policy.variables()))\n    checkpoint_path = os.path.join(self.get_temp_dir(), \'checkpoint\')\n    saver.save_checkpoint(checkpoint_path)\n\n    eager_py_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n        path, self.time_step_spec, self.action_spec)\n\n    # Use evaluate to force a copy.\n    saved_model_variables = self.evaluate(eager_py_policy.variables())\n\n    eager_py_policy.update_from_checkpoint(checkpoint_path)\n\n    assert_np_not_equal = lambda a, b: self.assertFalse(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_not_equal, saved_model_variables,\n                          self.evaluate(eager_py_policy.variables()))\n\n    assert_np_all_equal = lambda a, b: self.assertTrue(np.equal(a, b).all())\n    tf.nest.map_structure(assert_np_all_equal,\n                          self.evaluate(self.tf_policy.variables()),\n                          self.evaluate(eager_py_policy.variables()),\n                          check_types=False)\n\n    # Can\'t check if the action is different as in some cases depending on\n    # variable initialization it will be the same. Checking that they are at\n    # least always the same.\n    checkpoint_action = eager_py_policy.action(sample_time_step)\n\n    current_policy_action = self.tf_policy.action(batched_sample_time_step)\n    current_policy_action = self.evaluate(\n        nest_utils.unbatch_nested_tensors(current_policy_action))\n    tf.nest.map_structure(assert_np_all_equal, current_policy_action,\n                          checkpoint_action)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/policies/py_tf_policy.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Converts TensorFlow Policies into Python Policies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\nfrom absl import logging\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import session_utils\n\n\nclass PyTFPolicy(py_policy.PyPolicy, session_utils.SessionUser):\n  """"""Exposes a Python policy as wrapper over a TF Policy.""""""\n\n  _time_step = ...  # type: ts.TimeStep\n  _policy_state = ...  # type: types.NestedPlaceHolder\n  _action_step = ...  # type: policy_step.PolicyStep\n\n  # TODO(damienv): currently, the initial policy state must be batched\n  # if batch_size is given. Without losing too much generality, the initial\n  # policy state could be the same for every element in the batch.\n  # In that case, the initial policy state could be given with no batch\n  # dimension.\n  # TODO(sfishman): Remove batch_size param entirely.\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               batch_size: Optional[int] = None,\n               seed: Optional[types.Seed] = None):\n    """"""Initializes a new `PyTFPolicy`.\n\n    Args:\n      policy: A TF Policy implementing `tf_policy.TFPolicy`.\n      batch_size: (deprecated)\n      seed: Seed to use if policy performs random actions (optional).\n    """"""\n    if not isinstance(policy, tf_policy.TFPolicy):\n      logging.warning(\'Policy should implement tf_policy.TFPolicy\')\n\n    if batch_size is not None:\n      logging.warning(\'In PyTFPolicy constructor, `batch_size` is deprecated, \'\n                      \'this parameter has no effect. This argument will be \'\n                      \'removed on 2019-05-01\')\n\n    time_step_spec = tensor_spec.to_nest_array_spec(policy.time_step_spec)\n    action_spec = tensor_spec.to_nest_array_spec(policy.action_spec)\n    super(PyTFPolicy, self).__init__(\n        time_step_spec, action_spec, policy_state_spec=(), info_spec=())\n\n    self._tf_policy = policy\n    self.session = None\n\n    self._policy_state_spec = tensor_spec.to_nest_array_spec(\n        self._tf_policy.policy_state_spec)\n\n    self._batch_size = None\n    self._batched = None\n    self._seed = seed\n    self._built = False\n\n  def _construct(self, batch_size, graph):\n    """"""Construct the agent graph through placeholders.""""""\n\n    self._batch_size = batch_size\n    self._batched = batch_size is not None\n\n    outer_dims = [self._batch_size] if self._batched else [1]\n    with graph.as_default():\n      self._time_step = tensor_spec.to_nest_placeholder(\n          self._tf_policy.time_step_spec, outer_dims=outer_dims)\n      self._tf_initial_state = self._tf_policy.get_initial_state(\n          batch_size=self._batch_size or 1)\n\n      self._policy_state = tf.nest.map_structure(\n          lambda ps: tf.compat.v1.placeholder(  # pylint: disable=g-long-lambda\n              ps.dtype,\n              ps.shape,\n              name=\'policy_state\'),\n          self._tf_initial_state)\n      self._action_step = self._tf_policy.action(\n          self._time_step, self._policy_state, seed=self._seed)\n\n  def initialize(self,\n                 batch_size: Optional[int],\n                 graph: Optional[tf.Graph] = None):\n    if self._built:\n      raise RuntimeError(\'PyTFPolicy can only be initialized once.\')\n\n    if not graph:\n      graph = tf.compat.v1.get_default_graph()\n\n    self._construct(batch_size, graph)\n    var_list = tf.nest.flatten(self._tf_policy.variables())\n    common.initialize_uninitialized_variables(self.session, var_list)\n    self._built = True\n\n  def save(self,\n           policy_dir: Optional[Text] = None,\n           graph: Optional[tf.Graph] = None):\n    if not self._built:\n      raise RuntimeError(\'PyTFPolicy has not been initialized yet.\')\n\n    if not graph:\n      graph = tf.compat.v1.get_default_graph()\n\n    with graph.as_default():\n      global_step = tf.compat.v1.train.get_or_create_global_step()\n      policy_checkpointer = common.Checkpointer(\n          ckpt_dir=policy_dir, policy=self._tf_policy, global_step=global_step)\n      policy_checkpointer.initialize_or_restore(self.session)\n      with self.session.as_default():\n        policy_checkpointer.save(global_step)\n\n  def restore(self,\n              policy_dir: Text,\n              graph: Optional[tf.Graph] = None,\n              assert_consumed: bool = True):\n    """"""Restores the policy from the checkpoint.\n\n    Args:\n      policy_dir: Directory with the checkpoint.\n      graph: A graph, inside which policy the is restored (optional).\n      assert_consumed: If true, contents of the checkpoint will be checked\n        for a match against graph variables.\n\n    Returns:\n      step: Global step associated with the restored policy checkpoint.\n\n    Raises:\n      RuntimeError: if the policy is not initialized.\n      AssertionError: if the checkpoint contains variables which do not have\n        matching names in the graph, and assert_consumed is set to True.\n\n    """"""\n\n    if not self._built:\n      raise RuntimeError(\n          \'PyTFPolicy must be initialized before being restored.\')\n    if not graph:\n      graph = tf.compat.v1.get_default_graph()\n\n    with graph.as_default():\n      global_step = tf.compat.v1.train.get_or_create_global_step()\n      policy_checkpointer = common.Checkpointer(\n          ckpt_dir=policy_dir, policy=self._tf_policy, global_step=global_step)\n      status = policy_checkpointer.initialize_or_restore(self.session)\n      with self.session.as_default():\n        if assert_consumed:\n          status.assert_consumed()\n        status.run_restore_ops()\n      return self.session.run(global_step)\n\n  def _build_from_time_step(self, time_step):\n    outer_shape = nest_utils.get_outer_array_shape(time_step,\n                                                   self._time_step_spec)\n    if len(outer_shape) == 1:\n      self.initialize(outer_shape[0])\n    elif not outer_shape:\n      self.initialize(None)\n    else:\n      raise ValueError(\n          \'Cannot handle more than one outer dimension. Saw {} outer \'\n          \'dimensions: {}\'.format(len(outer_shape), outer_shape))\n\n  def _get_initial_state(self, batch_size):\n    if not self._built:\n      self.initialize(batch_size)\n    if batch_size != self._batch_size:\n      raise ValueError(\n          \'`batch_size` argument is different from the batch size provided \'\n          \'previously. Expected {}, but saw {}.\'.format(self._batch_size,\n                                                        batch_size))\n    return self.session.run(self._tf_initial_state)\n\n  def _action(self, time_step, policy_state):\n    if not self._built:\n      self._build_from_time_step(time_step)\n\n    batch_size = None\n    if time_step.step_type.shape:\n      batch_size = time_step.step_type.shape[0]\n    if self._batch_size != batch_size:\n      raise ValueError(\n          \'The batch size of time_step is different from the batch size \'\n          \'provided previously. Expected {}, but saw {}.\'.format(\n              self._batch_size, batch_size))\n\n    if not self._batched:\n      # Since policy_state is given in a batched form from the policy and we\n      # simply have to send it back we do not need to worry about it. Only\n      # update time_step.\n      time_step = nest_utils.batch_nested_array(time_step)\n\n    nest_utils.assert_same_structure(self._time_step, time_step)\n    feed_dict = {self._time_step: time_step}\n    if policy_state is not None:\n      # Flatten policy_state to handle specs that are not hashable due to lists.\n      for state_ph, state in zip(\n          tf.nest.flatten(self._policy_state), tf.nest.flatten(policy_state)):\n        feed_dict[state_ph] = state\n\n    action_step = self.session.run(self._action_step, feed_dict)\n    action, state, info = action_step\n\n    if not self._batched:\n      action, info = nest_utils.unbatch_nested_array([action, info])\n\n    return policy_step.PolicyStep(action, state, info)\n'"
tf_agents/policies/py_tf_policy_test.py,41,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.py_tf_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import ou_noise_policy\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import q_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self,\n               name=None,\n               num_actions=2,\n               stateful=True,\n               use_constant_initializer=True):\n    if stateful:\n      state_spec = tensor_spec.TensorSpec(shape=(1,), dtype=tf.float32)\n    else:\n      state_spec = ()\n    super(DummyNet, self).__init__(\n        input_tensor_spec=tensor_spec.TensorSpec([2], tf.float32, \'obs\'),\n        state_spec=state_spec,\n        name=name)\n\n    kernel_initializer = None\n    bias_initializer = None\n    if use_constant_initializer:\n      kernel_initializer = tf.compat.v1.initializers.constant(\n          [[1, 200], [3, 4]], verify_shape=True)\n      bias_initializer = tf.compat.v1.initializers.constant([1, 1],\n                                                            verify_shape=True)\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer)\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass DummyActionNet(network.Network):\n\n  def __init__(self, input_tensor_spec, output_tensor_spec):\n    super(DummyActionNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=\'DummyActionNet\')\n    self._forward = tf.keras.layers.Dense(\n        output_tensor_spec.shape.num_elements(),\n        activation=tf.nn.tanh,\n        kernel_initializer=None,\n        bias_initializer=None)\n\n  def call(self, observations, step_type, network_state):\n    del step_type\n    return self._forward(observations), network_state\n\n\n# TODO(damienv): This function should belong to nest_utils\ndef fast_map_structure(func, *structure):\n  flat_structure = [tf.nest.flatten(s) for s in structure]\n  entries = zip(*flat_structure)\n\n  return tf.nest.pack_sequence_as(structure[0], [func(*x) for x in entries])\n\n\nclass PyTFPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(PyTFPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32, \'obs\')\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1,\n                                                      \'action\')\n    self._float_action_spec = tensor_spec.BoundedTensorSpec([], tf.float32,\n                                                            0, 1, \'action\')\n    self._tf_policy = q_policy.QPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=DummyNet())\n\n  def testBuild(self):\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n    expected_time_step_spec = ts.time_step_spec(\n        tensor_spec.to_nest_array_spec(self._obs_spec))\n    expected_action_spec = tensor_spec.to_nest_array_spec(self._action_spec)\n    self.assertEqual(expected_time_step_spec, policy.time_step_spec)\n    self.assertEqual(expected_action_spec, policy.action_spec)\n\n  def testRaiseValueErrorWithoutSession(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n    with self.assertRaisesRegexp(\n        AttributeError,\n        ""No TensorFlow session-like object was set on this \'PyTFPolicy\'.*""):\n      policy.get_initial_state()\n\n  @parameterized.parameters([{\'batch_size\': None}, {\'batch_size\': 5}])\n  def testAssignSession(self, batch_size):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n    policy.session = tf.compat.v1.Session()\n    expected_initial_state = np.zeros([batch_size or 1, 1], dtype=np.float32)\n    self.assertTrue(\n        np.array_equal(\n            policy.get_initial_state(batch_size), expected_initial_state))\n\n  @parameterized.parameters([{\'batch_size\': None}, {\'batch_size\': 5}])\n  def testZeroState(self, batch_size):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n    expected_initial_state = np.zeros([batch_size or 1, 1], dtype=np.float32)\n    with self.cached_session():\n      self.assertTrue(\n          np.array_equal(\n              policy.get_initial_state(batch_size), expected_initial_state))\n\n  @parameterized.parameters([{\'batch_size\': None}, {\'batch_size\': 5}])\n  def testAction(self, batch_size):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n\n    single_observation = np.array([1, 2], dtype=np.float32)\n    time_steps = ts.restart(single_observation)\n    if batch_size is not None:\n      time_steps = [time_steps] * batch_size\n      time_steps = fast_map_structure(lambda *arrays: np.stack(arrays),\n                                      *time_steps)\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n\n    with self.cached_session():\n      policy_state = policy.get_initial_state(batch_size)\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      action_steps = policy.action(time_steps, policy_state)\n      self.assertEqual(action_steps.action.dtype, np.int32)\n      if batch_size is None:\n        self.assertEqual(action_steps.action.shape, ())\n        self.assertIn(action_steps.action, (0, 1))\n        self.assertEqual(action_steps.state, np.zeros([1, 1]))\n      else:\n        self.assertEqual(action_steps.action.shape, (batch_size,))\n        for a in action_steps.action:\n          self.assertIn(a, (0, 1))\n        self.assertAllEqual(action_steps.state, np.zeros([batch_size, 1]))\n\n  @parameterized.parameters([{\'batch_size\': None}, {\'batch_size\': 5}])\n  def testSaveRestore(self, batch_size):\n    policy_save_path = os.path.join(flags.FLAGS.test_tmpdir, \'policy\',\n                                    str(batch_size))\n\n    # Construct a policy to be saved under a tf.Graph instance.\n    policy_saved_graph = tf.Graph()\n    with policy_saved_graph.as_default():\n      tf_policy = q_policy.QPolicy(self._time_step_spec, self._action_spec,\n                                   DummyNet(use_constant_initializer=False))\n\n      # Parameterized tests reuse temp directories, make no save exists.\n      try:\n        tf.io.gfile.listdir(policy_save_path)\n        tf.io.gfile.rmtree(policy_save_path)\n      except tf.errors.NotFoundError:\n        pass\n      policy_saved = py_tf_policy.PyTFPolicy(tf_policy)\n      policy_saved.session = tf.compat.v1.Session(graph=policy_saved_graph)\n      policy_saved.initialize(batch_size)\n      policy_saved.save(policy_dir=policy_save_path, graph=policy_saved_graph)\n      # Verify that index files were written. There will also be some number of\n      # data files, but this depends on the number of devices.\n      self.assertContainsSubset(\n          set([\'checkpoint\', \'ckpt-0.index\']),\n          set(tf.io.gfile.listdir(policy_save_path)))\n\n    # Construct a policy to be restored under another tf.Graph instance.\n    policy_restore_graph = tf.Graph()\n    with policy_restore_graph.as_default():\n      tf_policy = q_policy.QPolicy(self._time_step_spec, self._action_spec,\n                                   DummyNet(use_constant_initializer=False))\n      policy_restored = py_tf_policy.PyTFPolicy(tf_policy)\n      policy_restored.session = tf.compat.v1.Session(graph=policy_restore_graph)\n      policy_restored.initialize(batch_size)\n      random_init_vals = policy_restored.session.run(tf_policy.variables())\n      policy_restored.restore(\n          policy_dir=policy_save_path, graph=policy_restore_graph)\n      restored_vals = policy_restored.session.run(tf_policy.variables())\n      for random_init_var, restored_var in zip(random_init_vals, restored_vals):\n        self.assertFalse(np.array_equal(random_init_var, restored_var))\n\n    # Check that variables in the two policies have identical values.\n    with policy_restore_graph.as_default():\n      restored_values = policy_restored.session.run(\n          tf.compat.v1.global_variables())\n    with policy_saved_graph.as_default():\n      initial_values = policy_saved.session.run(tf.compat.v1.global_variables())\n\n    # Networks have two fully connected layers.\n    self.assertLen(initial_values, 4)\n    self.assertLen(restored_values, 4)\n\n    for initial_var, restored_var in zip(initial_values, restored_values):\n      np.testing.assert_array_equal(initial_var, restored_var)\n\n  def testDeferredBatchingAction(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n\n    # Construct policy without providing batch_size.\n    tf_policy = q_policy.QPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=DummyNet(stateful=False))\n    policy = py_tf_policy.PyTFPolicy(tf_policy)\n\n    # But time_steps have batch_size of 5\n    batch_size = 5\n    single_observation = np.array([1, 2], dtype=np.float32)\n    time_steps = [ts.restart(single_observation)] * batch_size\n    time_steps = fast_map_structure(lambda *arrays: np.stack(arrays),\n                                    *time_steps)\n\n    with self.cached_session():\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      action_steps = policy.action(time_steps)\n      self.assertEqual(action_steps.action.shape, (batch_size,))\n      for a in action_steps.action:\n        self.assertIn(a, (0, 1))\n      self.assertAllEqual(action_steps.state, ())\n\n  def testDeferredBatchingStateful(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770140: Handling sessions with eager mode is buggy\')\n\n    # Construct policy without providing batch_size.\n    policy = py_tf_policy.PyTFPolicy(self._tf_policy)\n\n    # But time_steps have batch_size of 5\n    batch_size = 5\n    single_observation = np.array([1, 2], dtype=np.float32)\n    time_steps = [ts.restart(single_observation)] * batch_size\n    time_steps = fast_map_structure(lambda *arrays: np.stack(arrays),\n                                    *time_steps)\n\n    with self.cached_session():\n      initial_state = policy.get_initial_state(batch_size=batch_size)\n      self.assertAllEqual(initial_state, np.zeros([5, 1]))\n      action_steps = policy.action(time_steps, initial_state)\n      self.assertEqual(action_steps.action.shape, (batch_size,))\n      for a in action_steps.action:\n        self.assertIn(a, (0, 1))\n      self.assertAllEqual(action_steps.state, np.zeros([5, 1]))\n\n  def testSaveWrappedPolicyRestoreOuterCheckAssertConsumed(self, batch_size=5):\n\n    actor_policy_save_path = os.path.join(self.get_temp_dir(),\n                                          \'actor_policy\', str(batch_size))\n    noise_policy_save_path = os.path.join(self.get_temp_dir(),\n                                          \'noise_policy\', str(batch_size))\n\n    # Construct a policy to be saved under a tf.Graph instance.\n    policy_saved_graph = tf.Graph()\n    with policy_saved_graph.as_default():\n      actor_network = DummyActionNet(self._obs_spec, self._float_action_spec)\n      wrapped_policy = actor_policy.ActorPolicy(\n          time_step_spec=self._time_step_spec,\n          action_spec=self._float_action_spec,\n          actor_network=actor_network,\n          clip=False)\n      tf_policy = ou_noise_policy.OUNoisePolicy(wrapped_policy)\n\n      # Save the exploration policy and the wrapped actor policy.\n      actor_policy_saved = py_tf_policy.PyTFPolicy(wrapped_policy)\n      noise_policy_saved = py_tf_policy.PyTFPolicy(tf_policy)\n      for policy_saved, policy_save_path in zip(\n          [actor_policy_saved, noise_policy_saved],\n          [actor_policy_save_path, noise_policy_save_path]):\n        policy_saved.session = tf.compat.v1.Session(graph=policy_saved_graph)\n        policy_saved.initialize(batch_size)\n        policy_saved.save(policy_dir=policy_save_path, graph=policy_saved_graph)\n\n    # Construct a policy to be restored under another tf.Graph instance.\n    policy_restore_graph = tf.Graph()\n    with policy_restore_graph.as_default():\n      actor_network = DummyActionNet(self._obs_spec, self._float_action_spec)\n      wrapped_policy = actor_policy.ActorPolicy(\n          time_step_spec=self._time_step_spec,\n          action_spec=self._float_action_spec,\n          actor_network=actor_network,\n          clip=False)\n      tf_policy = ou_noise_policy.OUNoisePolicy(wrapped_policy)\n\n      policy_restored = py_tf_policy.PyTFPolicy(tf_policy)\n      policy_restored.session = tf.compat.v1.Session(graph=policy_restore_graph)\n      policy_restored.initialize(batch_size)\n      # 1). Restoring the same noise policy as was saved.\n      policy_restored.restore(\n          policy_dir=noise_policy_save_path, graph=policy_restore_graph)\n      # 2). Restoring the actor policy inside of the noise policy. While the\n      # graph for policy restore contains additional local variable for the\n      # OUNoise, if there is no checking that checkpoint was consumed, this\n      # also works.\n      policy_restored.restore(\n          policy_dir=actor_policy_save_path, graph=policy_restore_graph,\n          assert_consumed=False)\n      # 3). Restoring the actor policy while checking that all variables in\n      # the checkpoint were found in the graph should fail.\n      with self.assertRaisesRegexp(\n          AssertionError,\n          \'Some Python objects were not bound to checkpointed values*\'):\n        policy_restored.restore(\n            policy_dir=actor_policy_save_path,\n            graph=policy_restore_graph)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/q_policy.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Simple Policy for DQN.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import shifted_categorical\nfrom tf_agents.networks import network\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\n\n@gin.configurable\nclass QPolicy(tf_policy.TFPolicy):\n  """"""Class to build Q-Policies.""""""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      q_network: network.Network,\n      emit_log_probability: bool = False,\n      observation_and_action_constraint_splitter: Optional[\n          types.Splitter] = None,\n      name: Optional[Text] = None):\n    """"""Builds a Q-Policy given a q_network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      q_network: An instance of a `tf_agents.network.Network`,\n        callable via `network(observation, step_type) -> (output, final_state)`.\n      emit_log_probability: Whether to emit log-probs in info of `PolicyStep`.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `q_network` is compatible with the network-specific\n        half of the output of the `observation_and_action_constraint_splitter`.\n        In particular, `observation_and_action_constraint_splitter` will be\n        called on the observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If `q_network.action_spec` exists and is not compatible with\n        `action_spec`.\n      NotImplementedError: If `action_spec` contains more than one\n        `BoundedTensorSpec`.\n    """"""\n    network_action_spec = getattr(q_network, \'action_spec\', None)\n\n    if network_action_spec is not None:\n      if not action_spec.is_compatible_with(network_action_spec):\n        raise ValueError(\n            \'action_spec must be compatible with q_network.action_spec; \'\n            \'instead got action_spec=%s, q_network.action_spec=%s\' % (\n                action_spec, network_action_spec))\n\n    flat_action_spec = tf.nest.flatten(action_spec)\n    if len(flat_action_spec) > 1:\n      raise NotImplementedError(\n          \'action_spec can only contain a single BoundedTensorSpec.\')\n    # We need to maintain the flat action spec for dtype, shape and range.\n    self._flat_action_spec = flat_action_spec[0]\n    q_network.create_variables()\n    self._q_network = q_network\n    super(QPolicy, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy_state_spec=q_network.state_spec,\n        clip=False,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n\n  def _variables(self):\n    return self._q_network.variables\n\n  def _distribution(self, time_step, policy_state):\n    # In DQN, we always either take a uniformly random action, or the action\n    # with the highest Q-value. However, to support more complicated policies,\n    # we expose all Q-values as a categorical distribution with Q-values as\n    # logits, and apply the GreedyPolicy wrapper in dqn_agent.py to select the\n    # action with the highest Q-value.\n    observation_and_action_constraint_splitter = (\n        self.observation_and_action_constraint_splitter)\n    network_observation = time_step.observation\n\n    if observation_and_action_constraint_splitter is not None:\n      network_observation, mask = observation_and_action_constraint_splitter(\n          network_observation)\n\n    q_values, policy_state = self._q_network(\n        network_observation, time_step.step_type, policy_state)\n\n    # TODO(b/122314058): Validate and enforce that sampling distributions\n    # created with the q_network logits generate the right action shapes. This\n    # is curretly patching the problem.\n\n    # If the action spec says each action should be shaped (1,), add another\n    # dimension so the final shape is (B, 1, A), where A is the number of\n    # actions. This will make Categorical emit events shaped (B, 1) rather than\n    # (B,). Using axis -2 to allow for (B, T, 1, A) shaped q_values.\n    if self._flat_action_spec.shape.rank == 1:\n      q_values = tf.expand_dims(q_values, -2)\n\n    logits = q_values\n\n    if observation_and_action_constraint_splitter is not None:\n      # Expand the mask as needed in the same way as q_values above.\n      if self._flat_action_spec.shape.rank == 1:\n        mask = tf.expand_dims(mask, -2)\n\n      # Overwrite the logits for invalid actions to -inf.\n      neg_inf = tf.constant(-np.inf, dtype=logits.dtype)\n      logits = tf.compat.v2.where(tf.cast(mask, tf.bool), logits, neg_inf)\n\n    # TODO(kbanoop): Handle distributions over nests.\n    if self._flat_action_spec.minimum != 0:\n      distribution = shifted_categorical.ShiftedCategorical(\n          logits=logits,\n          dtype=self._flat_action_spec.dtype,\n          shift=self._flat_action_spec.minimum)\n    else:\n      distribution = tfp.distributions.Categorical(\n          logits=logits,\n          dtype=self._flat_action_spec.dtype)\n    distribution = tf.nest.pack_sequence_as(self._action_spec, [distribution])\n    return policy_step.PolicyStep(distribution, policy_state)\n'"
tf_agents/policies/q_policy_test.py,33,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.policies.q_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import q_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, name=None, num_actions=2):\n    super(DummyNet, self).__init__(\n        tensor_spec.TensorSpec([2], tf.float32), (), \'DummyNet\')\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant([[1, 1.5],\n                                                                   [1, 1.5]]),\n            bias_initializer=tf.compat.v1.initializers.constant([[1], [1]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass DummyNetWithActionSpec(DummyNet):\n\n  def __init__(self, action_spec, name=None, num_actions=2):\n    super(DummyNetWithActionSpec, self).__init__(name, num_actions)\n    self._action_spec = action_spec\n\n  @property\n  def action_spec(self):\n    return self._action_spec\n\n\nclass QPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(QPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n\n  def testBuild(self):\n    policy = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n  def testMultipleActionsRaiseError(self):\n    action_spec = [tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)] * 2\n    with self.assertRaisesRegexp(\n        NotImplementedError,\n        \'action_spec can only contain a single BoundedTensorSpec\'):\n      q_policy.QPolicy(\n          self._time_step_spec, action_spec, q_network=DummyNet())\n\n  def testAction(self):\n    policy = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action >= 0) and np.all(action <= 1))\n\n  def testActionWithinBounds(self):\n    bounded_action_spec = tensor_spec.BoundedTensorSpec([1],\n                                                        tf.int32,\n                                                        minimum=-6,\n                                                        maximum=-5)\n    policy = q_policy.QPolicy(\n        self._time_step_spec, bounded_action_spec, q_network=DummyNet())\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action <= -5) and np.all(action >= -6))\n\n  def testActionScalarSpec(self):\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 1)\n    policy = q_policy.QPolicy(\n        self._time_step_spec, action_spec, q_network=DummyNet())\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action >= 0) and np.all(action <= 1))\n\n  def testActionList(self):\n    action_spec = [tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)]\n    policy = q_policy.QPolicy(\n        self._time_step_spec, action_spec, q_network=DummyNet())\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertIsInstance(action_step.action, list)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action = self.evaluate(action_step.action)\n    self.assertLen(action, 1)\n    # Extract contents from the outer list.\n    action = action[0]\n    self.assertTrue(np.all(action >= 0) and np.all(action <= 1))\n\n  def testDistribution(self):\n    policy = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=1)\n    distribution_step = policy.distribution(time_step)\n    mode = distribution_step.action.mode()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # The weights of index 0 are all 1 and the weights of index 1 are all 1.5,\n    # so the Q values of index 1 will be higher.\n    self.assertAllEqual([[1]], self.evaluate(mode))\n\n  def testUpdate(self):\n    policy = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    new_policy = q_policy.QPolicy(\n        self._time_step_spec, self._action_spec, q_network=DummyNet())\n    self.assertEqual(len(policy.variables()), 2)\n    self.assertEqual(len(new_policy.variables()), 2)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(new_policy.update(policy)), None)\n\n    distribution = policy.distribution(time_step).action.parameters\n    new_distribution = new_policy.distribution(time_step).action.parameters\n    self.assertAllEqual(\n        self.evaluate(distribution[\'logits\']),\n        self.evaluate(new_distribution[\'logits\']))\n\n  def testActionSpecsCompatible(self):\n    q_net = DummyNetWithActionSpec(self._action_spec)\n    q_policy.QPolicy(self._time_step_spec, self._action_spec, q_net)\n\n  def testActionSpecsIncompatible(self):\n    network_action_spec = tensor_spec.BoundedTensorSpec([2], tf.int32, 0, 1)\n    q_net = DummyNetWithActionSpec(network_action_spec)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'action_spec must be compatible with q_network.action_spec\'):\n      q_policy.QPolicy(self._time_step_spec, self._action_spec, q_net)\n\n  def testMasking(self):\n    batch_size = 1000\n    num_state_dims = 5\n    num_actions = 8\n    observations = tf.random.uniform([batch_size, num_state_dims])\n    time_step = ts.restart(observations, batch_size=batch_size)\n    input_tensor_spec = tensor_spec.TensorSpec([num_state_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        [1], tf.int32, 0, num_actions - 1)\n\n    # We create a fixed mask here for testing purposes. Normally the mask would\n    # be part of the observation.\n    mask = [0, 1, 0, 1, 0, 0, 1, 0]\n    np_mask = np.array(mask)\n    tf_mask = tf.constant([mask for _ in range(batch_size)])\n    q_net = q_network.QNetwork(input_tensor_spec, action_spec)\n    policy = q_policy.QPolicy(\n        ts.time_step_spec(input_tensor_spec), action_spec, q_net,\n        observation_and_action_constraint_splitter=(\n            lambda observation: (observation, tf_mask)))\n\n    # Force creation of variables before global_variables_initializer.\n    policy.variables()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Sample from the policy 1000 times, and ensure that actions considered\n    # invalid according to the mask are never chosen.\n    action_step = policy.action(time_step)\n    action = self.evaluate(action_step.action)\n    self.assertEqual(action.shape, (batch_size, 1))\n    self.assertAllEqual(np_mask[action], np.ones([batch_size, 1]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/random_py_policy.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that generates random actions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Sequence\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.distributions import masked\nfrom tf_agents.policies import py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\nclass RandomPyPolicy(py_policy.PyPolicy):\n  """"""Returns random samples of the given action_spec.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               seed: Optional[types.Seed] = None,\n               outer_dims: Optional[Sequence[int]] = None,\n               observation_and_action_constraint_splitter: Optional[\n                   types.Splitter] = None):\n    """"""Initializes the RandomPyPolicy.\n\n    Args:\n      time_step_spec: Reference `time_step_spec`. If not None and outer_dims\n        is not provided this is used to infer the outer_dims required for the\n        given time_step when action is called.\n      action_spec: A nest of BoundedArraySpec representing the actions to sample\n        from.\n      seed: Optional seed used to instantiate a random number generator.\n      outer_dims: An optional list/tuple specifying outer dimensions to add to\n        the spec shape before sampling. If unspecified the outer_dims are\n        derived from the outer_dims in the given observation when `action` is\n        called.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `q_network` is compatible with the network-specific\n        half of the output of the `observation_and_action_constraint_splitter`.\n        In particular, `observation_and_action_constraint_splitter` will be\n        called on the observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n    """"""\n    self._seed = seed\n    self._outer_dims = outer_dims\n\n    if observation_and_action_constraint_splitter is not None:\n      if not isinstance(action_spec, array_spec.BoundedArraySpec):\n        raise NotImplementedError(\n            \'RandomPyPolicy only supports action constraints for \'\n            \'BoundedArraySpec action specs.\')\n\n      scalar_shape = not action_spec.shape\n      single_dim_shape = action_spec.shape == (1,) or action_spec.shape == [1]\n\n      if not scalar_shape and not single_dim_shape:\n        raise NotImplementedError(\n            \'RandomPyPolicy only supports action constraints for action specs \'\n            \'shaped as () or (1,) or their equivalent list forms.\')\n\n    self._rng = np.random.RandomState(seed)\n    if time_step_spec is None:\n      time_step_spec = ts.time_step_spec()\n\n    super(RandomPyPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter))\n\n  def _action(self, time_step, policy_state):\n    outer_dims = self._outer_dims\n    if outer_dims is None:\n      if self.time_step_spec.observation:\n        outer_dims = nest_utils.get_outer_array_shape(\n            time_step.observation, self.time_step_spec.observation)\n      else:\n        outer_dims = ()\n\n    observation_and_action_constraint_splitter = (\n        self.observation_and_action_constraint_splitter)\n\n    if observation_and_action_constraint_splitter is not None:\n      _, mask = observation_and_action_constraint_splitter(\n          time_step.observation)\n\n      zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n      masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n      random_action = tf.cast(\n          masked_categorical.sample() + self.action_spec.minimum,\n          self.action_spec.dtype)\n\n      # If the action spec says each action should be shaped (1,), add another\n      # dimension so the final shape is (B, 1) rather than (B,).\n      if len(self.action_spec.shape) == 1:\n        random_action = tf.expand_dims(random_action, axis=-1)\n    else:\n      random_action = array_spec.sample_spec_nest(\n          self._action_spec, self._rng, outer_dims=outer_dims)\n\n    return policy_step.PolicyStep(random_action, policy_state)\n'"
tf_agents/policies/random_py_policy_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.random_py_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import test_utils\n\n\nclass RandomPyPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(RandomPyPolicyTest, self).setUp()\n    self._time_step_spec = time_step.time_step_spec(\n        observation_spec=array_spec.ArraySpec((1,), np.int32))\n    self._time_step = time_step.restart(observation=np.array([1]))\n\n  def testGeneratesActions(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=self._time_step_spec, action_spec=action_spec)\n\n    action_step = policy.action(self._time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    self.assertTrue(np.all(action_step.action[0] >= -10))\n    self.assertTrue(np.all(action_step.action[0] <= 10))\n    self.assertTrue(np.all(action_step.action[1] >= -10))\n    self.assertTrue(np.all(action_step.action[1] <= 10))\n\n  def testGeneratesBatchedActions(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        outer_dims=(3,))\n\n    action_step = policy.action(self._time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n    self.assertEqual((3, 2, 3), action_step.action[0].shape)\n    self.assertEqual((3, 1, 2), action_step.action[1].shape)\n\n    self.assertTrue(np.all(action_step.action[0] >= -10))\n    self.assertTrue(np.all(action_step.action[0] <= 10))\n    self.assertTrue(np.all(action_step.action[1] >= -10))\n    self.assertTrue(np.all(action_step.action[1] <= 10))\n\n  def testGeneratesBatchedActionsWithoutSpecifyingOuterDims(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n    time_step_spec = time_step.time_step_spec(\n        observation_spec=array_spec.ArraySpec((1,), np.int32))\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    action_step = policy.action(\n        time_step.restart(np.array([[1], [2], [3]], dtype=np.int32)))\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n    self.assertEqual((3, 2, 3), action_step.action[0].shape)\n    self.assertEqual((3, 1, 2), action_step.action[1].shape)\n\n    self.assertTrue(np.all(action_step.action[0] >= -10))\n    self.assertTrue(np.all(action_step.action[0] <= 10))\n    self.assertTrue(np.all(action_step.action[1] >= -10))\n    self.assertTrue(np.all(action_step.action[1] <= 10))\n\n  def testPolicyStateSpecIsEmpty(self):\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=self._time_step_spec, action_spec=[])\n    self.assertEqual(policy.policy_state_spec, ())\n\n  def testMasking(self):\n    batch_size = 1000\n\n    time_step_spec = time_step.time_step_spec(\n        observation_spec=array_spec.ArraySpec((1,), np.int32))\n    action_spec = array_spec.BoundedArraySpec((), np.int64, -5, 5)\n\n    # We create a fixed mask here for testing purposes. Normally the mask would\n    # be part of the observation.\n    mask = [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n    np_mask = np.array(mask)\n    batched_mask = np.array([mask for _ in range(batch_size)])\n\n    policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        observation_and_action_constraint_splitter=(\n            lambda obs: (obs, batched_mask)))\n\n    my_time_step = time_step.restart(time_step_spec, batch_size)\n    action_step = policy.action(my_time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    # Sample from the policy 1000 times, and ensure that actions considered\n    # invalid according to the mask are never chosen.\n    action_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action_ >= -5))\n    self.assertTrue(np.all(action_ <= 5))\n    self.assertAllEqual(np_mask[action_ - action_spec.minimum],\n                        np.ones([batch_size]))\n\n    # Ensure that all valid actions occur somewhere within the batch. Because we\n    # sample 1000 times, the chance of this failing for any particular action is\n    # (2/3)^1000, roughly 1e-176.\n    for index in range(action_spec.minimum, action_spec.maximum + 1):\n      if np_mask[index - action_spec.minimum]:\n        self.assertIn(index, action_)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/policies/random_tf_policy.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that generates random actions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.distributions import masked\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\ndef _calculate_log_probability(outer_dims, action_spec):\n  """"""Helper function for calculating log prob of a uniform distribution.\n\n  Each item in the returned tensor will be equal to:\n  |action_spec.shape| * log_prob_of_each_component_of_action_spec.\n\n  Note that this method expects the same value for all outer_dims because\n  we\'re sampling uniformly from the same distribution for each batch row.\n\n  Args:\n    outer_dims: TensorShape.\n    action_spec: BoundedTensorSpec.\n\n  Returns:\n    A tensor of type float32 with shape outer_dims.\n  """"""\n  # Equivalent of what a tfp.distribution.Categorical would return.\n  if action_spec.dtype.is_integer:\n    log_prob = -tf.math.log(action_spec.maximum - action_spec.minimum + 1.0)\n  # Equivalent of what a tfp.distribution.Uniform would return.\n  else:\n    log_prob = -tf.math.log(action_spec.maximum - action_spec.minimum)\n\n  # Note that log_prob may be a vector. We first reduce it to a scalar, and then\n  # adjust by the number of times that vector is repeated in action_spec.\n  log_prob = tf.reduce_sum(log_prob) * (\n      action_spec.shape.num_elements() / log_prob.shape.num_elements())\n  return tf.fill(outer_dims, log_prob)\n\n\nclass RandomTFPolicy(tf_policy.TFPolicy):\n  """"""Returns random samples of the given action_spec.\n\n  Note: the values in the info_spec (except for the log_probability) are random\n    values that have nothing to do with the emitted actions.\n\n  Note: The returned info.log_probabiliy will be an object matching the\n  structure of action_spec, where each value is a tensor of size [batch_size].\n  """"""\n\n  def __init__(self, time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec, *args, **kwargs):\n    observation_and_action_constraint_splitter = (\n        kwargs.get(\'observation_and_action_constraint_splitter\', None))\n    self._accepts_per_arm_features = (\n        kwargs.pop(\'accepts_per_arm_features\', False))\n\n    if observation_and_action_constraint_splitter is not None:\n      if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n        raise NotImplementedError(\n            \'RandomTFPolicy only supports action constraints for \'\n            \'BoundedTensorSpec action specs.\')\n\n      scalar_shape = action_spec.shape.rank == 0\n      single_dim_shape = (\n          action_spec.shape.rank == 1 and action_spec.shape.dims == [1])\n\n      if not scalar_shape and not single_dim_shape:\n        raise NotImplementedError(\n            \'RandomTFPolicy only supports action constraints for action specs \'\n            \'shaped as () or (1,) or their equivalent list forms.\')\n\n    super(RandomTFPolicy, self).__init__(time_step_spec, action_spec, *args,\n                                         **kwargs)\n\n  def _variables(self):\n    return []\n\n  def _action(self, time_step, policy_state, seed):\n    observation_and_action_constraint_splitter = (\n        self.observation_and_action_constraint_splitter)\n\n    outer_dims = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n    if observation_and_action_constraint_splitter is not None:\n      observation, mask = observation_and_action_constraint_splitter(\n          time_step.observation)\n\n      zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n      masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n      action_ = tf.cast(masked_categorical.sample() + self.action_spec.minimum,\n                        self.action_spec.dtype)\n\n      # If the action spec says each action should be shaped (1,), add another\n      # dimension so the final shape is (B, 1) rather than (B,).\n      if self.action_spec.shape.rank == 1:\n        action_ = tf.expand_dims(action_, axis=-1)\n      policy_info = tensor_spec.sample_spec_nest(\n          self._info_spec, outer_dims=outer_dims)\n    else:\n      observation = time_step.observation\n\n      action_ = tensor_spec.sample_spec_nest(\n          self._action_spec, seed=seed, outer_dims=outer_dims)\n      policy_info = tensor_spec.sample_spec_nest(\n          self._info_spec, outer_dims=outer_dims)\n    if self._accepts_per_arm_features:\n      def _gather_fn(t):\n        return tf.gather(params=t, indices=action_, batch_dims=1)\n\n      chosen_arm_features = tf.nest.map_structure(_gather_fn,\n                                                  observation[\'per_arm\'])\n      policy_info = policy_info._replace(\n          chosen_arm_features=chosen_arm_features)\n\n    # TODO(b/78181147): Investigate why this control dependency is required.\n    if time_step is not None:\n      with tf.control_dependencies(tf.nest.flatten(time_step)):\n        action_ = tf.nest.map_structure(tf.identity, action_)\n\n    if self.emit_log_probability:\n      if observation_and_action_constraint_splitter is not None:\n        log_probability = masked_categorical.log_prob(action_ -\n                                                      self.action_spec.minimum)\n      else:\n        log_probability = tf.nest.map_structure(\n            lambda s: _calculate_log_probability(outer_dims, s),\n            self._action_spec)\n      policy_info = policy_step.set_log_probability(policy_info,\n                                                    log_probability)\n\n    step = policy_step.PolicyStep(action_, policy_state, policy_info)\n    return step\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'RandomTFPolicy does not support distributions yet.\')\n'"
tf_agents/policies/random_tf_policy_test.py,20,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.random_tf_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import test_utils\n\n\n@parameterized.named_parameters(\n    (\'_int32\', tf.int32),\n    (\'_int64\', tf.int64),\n    (\'_float32\', tf.float32),\n    (\'_float64\', tf.float64),\n)\nclass RandomTFPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  def create_batch(self, single_time_step, batch_size):\n    batch_time_step = nest_utils.stack_nested_tensors([single_time_step] *\n                                                      batch_size)\n    return batch_time_step\n\n  def create_time_step(self):\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observation)\n\n    observation_spec = tensor_spec.TensorSpec(observation.shape.as_list(),\n                                              tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    return time_step_spec, time_step\n\n  def testGeneratesBoundedActions(self, dtype):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2, 3), dtype, -10, 10),\n        tensor_spec.BoundedTensorSpec((1, 2), dtype, -10, 10)\n    ]\n    time_step_spec, time_step = self.create_time_step()\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    action_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action_[0] >= -10))\n    self.assertTrue(np.all(action_[0] <= 10))\n    self.assertTrue(np.all(action_[1] >= -10))\n    self.assertTrue(np.all(action_[1] <= 10))\n\n  def testGeneratesUnBoundedActions(self, dtype):\n    action_spec = [\n        tensor_spec.TensorSpec((2, 3), dtype),\n        tensor_spec.TensorSpec((1, 2), dtype)\n    ]\n    bounded = tensor_spec.BoundedTensorSpec.from_spec(action_spec[0])\n    time_step_spec, time_step = self.create_time_step()\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    action_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action_[0] >= bounded.minimum))\n    self.assertTrue(np.all(action_[0] <= bounded.maximum))\n    self.assertTrue(np.all(action_[1] >= bounded.minimum))\n    self.assertTrue(np.all(action_[1] <= bounded.maximum))\n\n  def testGeneratesBatchedActionsImplicitBatchSize(self, dtype):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2, 3), dtype, -10, 10),\n        tensor_spec.BoundedTensorSpec((1, 2), dtype, -10, 10)\n    ]\n    time_step_spec, time_step = self.create_time_step()\n    time_step = self.create_batch(time_step, 2)\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    action_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(action_[0] >= -10))\n    self.assertTrue(np.all(action_[0] <= 10))\n    self.assertTrue(np.all(action_[1] >= -10))\n    self.assertTrue(np.all(action_[1] <= 10))\n\n    self.assertEqual((2, 2, 3), action_[0].shape)\n    self.assertEqual((2, 1, 2), action_[1].shape)\n\n  def testEmitLogProbability(self, dtype):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2, 3), dtype, -10, 10),\n        tensor_spec.BoundedTensorSpec((1, 2), dtype, -10, 10)\n    ]\n    time_step_spec, time_step = self.create_time_step()\n    batch_size = 3\n    time_step = self.create_batch(time_step, batch_size)\n\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        emit_log_probability=True)\n\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    step = self.evaluate(action_step)\n    action_ = step.action\n    # For integer specs, boundaries are inclusive.\n    p = 1. / 21 if dtype.is_integer else 1. / 20\n    np.testing.assert_allclose(\n        np.array(step.info.log_probability, dtype=np.float32),\n        np.array(\n            np.log([[math.pow(p, 6) for _ in range(3)],\n                    [math.pow(p, 2) for _ in range(3)]]),\n            dtype=np.float32),\n        rtol=1e-5)\n    self.assertTrue(np.all(action_[0] >= -10))\n    self.assertTrue(np.all(action_[0] <= 10))\n    self.assertTrue(np.all(action_[1] >= -10))\n    self.assertTrue(np.all(action_[1] <= 10))\n\n  def testMasking(self, dtype):\n    if not dtype.is_integer:\n      self.skipTest(\'testMasking only applies to integer dtypes\')\n\n    batch_size = 1000\n\n    action_spec = tensor_spec.BoundedTensorSpec((), dtype, -5, 5)\n    time_step_spec, time_step = self.create_time_step()\n    time_step = self.create_batch(time_step, batch_size)\n\n    # We create a fixed mask here for testing purposes. Normally the mask would\n    # be part of the observation.\n    mask = [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n    np_mask = np.array(mask)\n    tf_mask = tf.constant([mask for _ in range(batch_size)])\n\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        emit_log_probability=True,\n        observation_and_action_constraint_splitter=lambda obs: (obs, tf_mask))\n\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n\n    # Sample from the policy 1000 times, and ensure that actions considered\n    # invalid according to the mask are never chosen.\n    step = self.evaluate(action_step)\n    action_ = step.action\n    self.assertTrue(np.all(action_ >= -5))\n    self.assertTrue(np.all(action_ <= 5))\n    self.assertAllEqual(np_mask[action_ - action_spec.minimum],\n                        np.ones([batch_size]))\n\n    # Ensure that all valid actions occur somewhere within the batch. Because we\n    # sample 1000 times, the chance of this failing for any particular action is\n    # (2/3)^1000, roughly 1e-176.\n    for index in range(action_spec.minimum, action_spec.maximum + 1):\n      if np_mask[index - action_spec.minimum]:\n        self.assertIn(index, action_)\n\n    # With only three valid actions, all of the probabilities should be 1/3.\n    self.assertAllClose(step.info.log_probability,\n                        tf.constant(np.log(1. / 3), shape=[batch_size]))\n\n  def testInfoSpec(self, dtype):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2, 3), dtype, -10, 10),\n        tensor_spec.BoundedTensorSpec((1, 2), dtype, -10, 10)\n    ]\n    info_spec = [\n        tensor_spec.TensorSpec([1], dtype=tf.float32, name=\'loc\'),\n        tensor_spec.TensorSpec([1], dtype=tf.float32, name=\'scale\')\n    ]\n    time_step_spec, time_step = self.create_time_step()\n    policy = random_tf_policy.RandomTFPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=info_spec)\n\n    # Test without batch\n    action_step = policy.action(time_step)\n    tf.nest.assert_same_structure(action_spec, action_step.action)\n    self.assertEqual((2, 3,), action_step.action[0].shape)\n    self.assertEqual((1, 2,), action_step.action[1].shape)\n    tf.nest.assert_same_structure(info_spec, action_step.info)\n    self.assertEqual((1,), action_step.info[0].shape)\n    self.assertEqual((1,), action_step.info[1].shape)\n\n    # Test with batch, we should see the additional outer batch dim for both\n    # `action` and `info`.\n    batch_size = 2\n    batched_time_step = self.create_batch(time_step, batch_size)\n    batched_action_step = policy.action(batched_time_step)\n    tf.nest.assert_same_structure(action_spec, batched_action_step.action)\n    self.assertEqual((batch_size, 2, 3,), batched_action_step.action[0].shape)\n    self.assertEqual((batch_size, 1, 2,), batched_action_step.action[1].shape)\n    tf.nest.assert_same_structure(info_spec, batched_action_step.info)\n    self.assertEqual((batch_size, 1,), batched_action_step.info[0].shape)\n    self.assertEqual((batch_size, 1,), batched_action_step.info[1].shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/scripted_py_policy.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy implementation that steps over a given configuration.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Sequence, Tuple\nfrom absl import logging\n\nimport numpy as np\nfrom tf_agents.policies import py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass ScriptedPyPolicy(py_policy.PyPolicy):\n  """"""Returns actions from the given configuration.""""""\n\n  def __init__(self, time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               action_script: Sequence[Tuple[int, types.NestedArray]]):\n    """"""Instantiates the scripted policy.\n\n    The Action  script can be configured through gin. e.g:\n\n    ScriptedPyPolicy.action_script = [\n        (1, {  ""action1"": [[5, 2], [1, 3]],\n               ""action2"": [[4, 6]]},),\n        (0, {  ""action1"": [[8, 1], [9, 2]],\n               ""action2"": [[1, 2]]},),\n        (2, {  ""action1"": [[1, 1], [3, 2]],\n               ""action2"": [[8, 2]]},),\n    ]\n\n    In this case the first action is executed once, the second scripted action\n    is disabled and skipped. Then the third listed action is executed for two\n    steps.\n\n    Args:\n      time_step_spec: A time_step_spec for the policy will interact\n        with.\n      action_spec: An action_spec for the environment the policy will interact\n        with.\n      action_script: A list of 2-tuples of the form (n, nest) where the nest of\n        actions follow the action_spec. Each action will be executed for n\n        steps.\n    """"""\n    if time_step_spec is None:\n      time_step_spec = ts.time_step_spec()\n    super(ScriptedPyPolicy, self).__init__(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    self._action_script = action_script\n\n  def _get_initial_state(self, batch_size):\n    del batch_size\n    # We use the state to keep track of the action index to execute and to count\n    # how many times it has been performed.\n    return [0, 0]\n\n  def _action(self, time_step, policy_state):\n    del time_step  # Unused.\n    if policy_state is None:\n      policy_state = [0, 0]\n\n    action_index, num_repeats = policy_state  #  pylint: disable=unpacking-non-sequence\n\n    def _check_episode_length():\n      if action_index >= len(self._action_script):\n        raise ValueError(\n            ""Episode is longer than the provided scripted policy. Consider ""\n            ""setting a TimeLimit wrapper that stops episodes within the length""\n            "" of your scripted policy."")\n\n    _check_episode_length()\n    n, current_action = self._action_script[action_index]\n\n    # If the policy has been executed n times get the next scripted action.\n    # Allow users to disable entries in the scripted policy by setting n <= 0.\n    while num_repeats >= n:\n      action_index += 1\n      num_repeats = 0\n      _check_episode_length()\n      n, current_action = self._action_script[action_index]\n\n    num_repeats += 1\n\n    # To make it easier for the user we allow the actions in the script to be\n    # lists instead of numpy arrays. Checking the arrays_nest requires us to\n    # have the leaves be objects and not lists so we lift them into numpy\n    # arrays.\n    def actions_as_array(action_spec, action):\n      return np.asarray(action, dtype=action_spec.dtype)\n\n    current_action = nest.map_structure_up_to(\n        self._action_spec, actions_as_array, self._action_spec, current_action)\n\n    if not array_spec.check_arrays_nest(current_action, self._action_spec):\n      raise ValueError(\n          ""Action at index {} does not match the environment\'s action_spec. ""\n          ""Got: {}. Expected {}."".format(action_index, current_action,\n                                         self._action_spec))\n\n    logging.info(""Policy_state: %r"", policy_state)\n    return policy_step.PolicyStep(current_action, [action_index, num_repeats])\n'"
tf_agents/policies/scripted_py_policy_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.policies.scripted_py_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\n\nfrom tf_agents.policies import scripted_py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass ScriptedPyPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(ScriptedPyPolicyTest, self).setUp()\n    self._obs_spec = array_spec.ArraySpec((), np.int32, \'obs\')\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._time_step = ts.restart(observation=1)\n\n  def testFollowsScript(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n\n    action_script = [\n        (1, [\n            np.array([[5, 2], [1, 3]], dtype=np.int32),\n            np.array([[4, 6]], dtype=np.int32)\n        ]),\n        (0, [\n            np.array([[0, 0], [0, 0]], dtype=np.int32),\n            np.array([[0, 0]], dtype=np.int32)\n        ]),\n        (2, [\n            np.array([[1, 2], [3, 4]], dtype=np.int32),\n            np.array([[5, 6]], dtype=np.int32)\n        ]),\n    ]\n\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        action_script=action_script)\n    policy_state = policy.get_initial_state()\n\n    action_step = policy.action(self._time_step, policy_state)\n    self.assertEqual(action_script[0][1], action_step.action)\n    action_step = policy.action(self._time_step, action_step.state)\n    self.assertEqual(action_script[2][1], action_step.action)\n    action_step = policy.action(self._time_step, action_step.state)\n    self.assertEqual(action_script[2][1], action_step.action)\n\n  def testFollowsScriptWithListInsteadOfNpArrays(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n\n    action_script = [\n        (1, [\n            [[5, 2], [1, 3]],\n            [[4, 6]],\n        ]),\n        (2, [[[1, 2], [3, 4]], [[5, 6]]]),\n    ]\n\n    expected = [\n        [\n            np.array([[5, 2], [1, 3]], dtype=np.int32),\n            np.array([[4, 6]], dtype=np.int32)\n        ],\n        [\n            np.array([[1, 2], [3, 4]], dtype=np.int32),\n            np.array([[5, 6]], dtype=np.int32)\n        ],\n    ]\n\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        action_script=action_script)\n    policy_state = policy.get_initial_state()\n\n    action_step = policy.action(self._time_step, policy_state)\n    np.testing.assert_array_equal(expected[0][0], action_step.action[0])\n    np.testing.assert_array_equal(expected[0][1], action_step.action[1])\n    action_step = policy.action(self._time_step, action_step.state)\n    np.testing.assert_array_equal(expected[1][0], action_step.action[0])\n    np.testing.assert_array_equal(expected[1][1], action_step.action[1])\n    action_step = policy.action(self._time_step, action_step.state)\n    np.testing.assert_array_equal(expected[1][0], action_step.action[0])\n    np.testing.assert_array_equal(expected[1][1], action_step.action[1])\n\n  def testChecksSpecBounds(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n\n    action_script = [\n        (1, [\n            np.array([[15, 2], [1, 3]], dtype=np.int32),\n            np.array([[4, 6]], dtype=np.int32)\n        ]),\n        (2, [\n            np.array([[1, 2], [3, 4]], dtype=np.int32),\n            np.array([[5, 6]], dtype=np.int32)\n        ]),\n    ]\n\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        action_script=action_script)\n    policy_state = policy.get_initial_state()\n\n    with self.assertRaises(ValueError):\n      policy.action(self._time_step, policy_state)\n\n  def testChecksSpecNest(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n\n    action_script = [\n        (1, [np.array([[5, 2], [1, 3]], dtype=np.int32)]),\n        (2, [\n            np.array([[1, 2], [3, 4]], dtype=np.int32),\n            np.array([[5, 6]], dtype=np.int32)\n        ]),\n    ]\n\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        action_script=action_script)\n    policy_state = policy.get_initial_state()\n\n    with self.assertRaises(ValueError):\n      policy.action(self._time_step, policy_state)\n\n  def testEpisodeLength(self):\n    action_spec = [\n        array_spec.BoundedArraySpec((2, 2), np.int32, -10, 10),\n        array_spec.BoundedArraySpec((1, 2), np.int32, -10, 10)\n    ]\n\n    action_script = [\n        (1, [\n            np.array([[5, 2], [1, 3]], dtype=np.int32),\n            np.array([[4, 6]], dtype=np.int32)\n        ]),\n        (2, [\n            np.array([[1, 2], [3, 4]], dtype=np.int32),\n            np.array([[5, 6]], dtype=np.int32)\n        ]),\n    ]\n\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec,\n        action_spec=action_spec,\n        action_script=action_script)\n    policy_state = policy.get_initial_state()\n\n    action_step = policy.action(self._time_step, policy_state)\n    self.assertEqual(action_script[0][1], action_step.action)\n    action_step = policy.action(self._time_step, action_step.state)\n    self.assertEqual(action_script[1][1], action_step.action)\n    action_step = policy.action(self._time_step, action_step.state)\n    self.assertEqual(action_script[1][1], action_step.action)\n    with self.assertRaisesRegexp(ValueError, \'.*Episode is longer than.*\'):\n      policy.action(self._time_step, action_step.state)\n\n  def testPolicyStateSpecIsEmpty(self):\n    policy = scripted_py_policy.ScriptedPyPolicy(\n        time_step_spec=self._time_step_spec, action_spec=[], action_script=[])\n    self.assertEqual(policy.policy_state_spec, ())\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/policies/temporal_action_smoothing.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A TFPolicy wrapper that applies exponential moving averaging to actions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\n\n\nclass TemporalActionSmoothing(tf_policy.TFPolicy):\n  """"""A wrapper that applies exponential moving averaging to action outputs.""""""\n\n  def __init__(self,\n               policy: tf_policy.TFPolicy,\n               smoothing_coefficient: float,\n               name: Optional[Text] = None):\n    """"""Adds TemporalActionSmoothing to the given policy.\n\n    smoothed_action = previous_action * smoothing_coefficient +\n                      action * (1.0 - smoothing_coefficient))\n\n    Args:\n      policy: A policy implementing the tf_policy.TFPolicy interface.\n      smoothing_coefficient: Coefficient used for smoothing actions.\n      name: The name of this policy. Defaults to the class name.\n    """"""\n    policy_state_spec = (policy.policy_state_spec, policy.action_spec)\n    super(TemporalActionSmoothing, self).__init__(\n        policy.time_step_spec, policy.action_spec, policy_state_spec, name=name)\n    self._wrapped_policy = policy\n    self._smoothing_coefficient = smoothing_coefficient\n\n  def _get_initial_state(self, batch_size):\n    """"""Creates zero state tuple with wrapped initial state and smoothing vars.\n\n    Args:\n      batch_size: The batch shape.\n\n    Returns:\n      A tuple of (wrapped_policy_initial_state, initial_smoothing_state)\n    """"""\n    wrapped_initial_state = self._wrapped_policy.get_initial_state(batch_size)\n    initial_smoothing_state = super(TemporalActionSmoothing,\n                                    self)._get_initial_state(batch_size)[1]\n    return (wrapped_initial_state, initial_smoothing_state)\n\n  def _variables(self):\n    return self._wrapped_policy.variables()\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'`distribution` not implemented for TemporalActionSmoothingWrapper.\')\n\n  def _action(self, time_step, policy_state, seed):\n    # Get action from the wrapped policy.\n    wrapped_policy_state, moving_average = policy_state\n    wrapped_policy_step = self._wrapped_policy.action(time_step,\n                                                      wrapped_policy_state,\n                                                      seed)\n\n    # Compute smoothed action & updated action tensor.\n    def _smooth_action_tensor(smoothing_state_tensor, action_tensor):\n      return (smoothing_state_tensor * self._smoothing_coefficient +\n              action_tensor * (1.0 - self._smoothing_coefficient))\n\n    smoothed_action = tf.nest.map_structure(_smooth_action_tensor,\n                                            moving_average,\n                                            wrapped_policy_step.action)\n\n    # Package results in PolicyStep.\n    return policy_step.PolicyStep(smoothed_action,\n                                  (wrapped_policy_step.state, smoothed_action),\n                                  wrapped_policy_step.info)\n'"
tf_agents/policies/temporal_action_smoothing_test.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for tf_agents.policies.temporal_action_smoothing.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import temporal_action_smoothing\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass StateIncrementPolicy(tf_policy.TFPolicy):\n\n  def __init__(self, time_step_spec, action_spec):\n    super(StateIncrementPolicy, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy_state_spec=action_spec,\n    )\n\n  def _action(self, time_step, policy_state, seed):\n    actions = tf.nest.map_structure(lambda t: t + 1, policy_state)\n    return policy_step.PolicyStep(actions, actions, ())\n\n  def _distribution(self):\n    return policy_step.PolicyStep(())\n\n\nclass TemporalActionSmoothingTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(TemporalActionSmoothingTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 0, 10)\n\n  @property\n  def _time_step(self):\n    return ts.transition(tf.constant([[1, 2]], dtype=tf.float32),\n                         reward=tf.constant([1.]))\n\n  def testStateIncrementPolicy(self):\n    policy = StateIncrementPolicy(self._time_step_spec, self._action_spec)\n    policy_state = policy.get_initial_state(1)\n    step = policy.action(self._time_step, policy_state)\n    self.assertEqual(1, self.evaluate(step.action))\n    step = policy.action(self._time_step, step.state)\n    self.assertEqual(2, self.evaluate(step.action))\n\n  @parameterized.named_parameters(\n      (\'0p0\', 0.0, [1., 2., 3., 4., 5.]),\n      (\'0p5\', 0.5, [0.5, 1.25, 2.125, 3.0625, 4.03125]),\n      (\'1p0\', 1.0, [0., 0., 0., 0., 0.]),\n  )\n  def testSmoothedActions(self, smoothing_coefficient, expected_actions):\n    # Set up the smoothing policy.\n    policy = StateIncrementPolicy(self._time_step_spec, self._action_spec)\n    smoothed_policy = temporal_action_smoothing.TemporalActionSmoothing(\n        policy, smoothing_coefficient)\n\n    # Create actions sampled in time order.\n    policy_state = smoothed_policy.get_initial_state(batch_size=1)\n    smoothed_actions = []\n    for _ in range(5):\n      action, policy_state, unused_policy_info = smoothed_policy.action(\n          self._time_step, policy_state=policy_state)\n      smoothed_actions.append(action)\n\n    # Make sure smoothed actions are as expected.\n    smoothed_actions_ = self.evaluate(smoothed_actions)\n    self.assertAllClose(np.squeeze(smoothed_actions_), expected_actions)\n\n  def testDistributionRaisesError(self):\n    # Set up the smoothing policy.\n    policy = StateIncrementPolicy(self._time_step_spec, self._action_spec)\n    smoothed_policy = temporal_action_smoothing.TemporalActionSmoothing(\n        policy, smoothing_coefficient=0.5)\n\n    # Create actions sampled in time order.\n    policy_state = smoothed_policy.get_initial_state(batch_size=1)\n    with self.assertRaises(NotImplementedError):\n      smoothed_policy.distribution(self._time_step, policy_state)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/policies/tf_policy.py,21,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TensorFlow Policies API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport abc\nfrom typing import Optional, Text, Sequence\n\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import reparameterized_sampling\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ntfd = tfp.distributions\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass TFPolicy(tf.Module):\n  """"""Abstract base class for TF Policies.\n\n  The Policy represents a mapping from `time_steps` recieved from the\n  environment to `actions` that can be applied to the environment.\n\n  Agents expose two policies. A `policy` meant for deployment and evaluation,\n  and a `collect_policy` for collecting data from the environment. The\n  `collect_policy` is usually stochastic for exploring the environment better\n  and may log auxilliary information such as log probabilities required for\n  training as well. `Policy` objects can also be created directly by the users\n  without using an `Agent`.\n\n  The main methods of TFPolicy are:\n\n  * `action`: Maps a `time_step` from the environment to an action.\n  * `distribution`: Maps a `time_step` to a distribution over actions.\n  * `get_initial_state`: Generates the initial state for stateful policies, e.g.\n      RNN/LSTM policies.\n\n  Example usage:\n\n  ```\n  env = SomeTFEnvironment()\n  policy = TFRandomPolicy(env.time_step_spec(), env.action_spec())\n  # Or policy = agent.policy or agent.collect_policy\n\n  policy_state = policy.get_initial_state(env.batch_size)\n  time_step = env.reset()\n\n  while not time_step.is_last():\n    policy_step = policy.action(time_step, policy_state)\n    time_step = env.step(policy_step.action)\n\n    policy_state = policy_step.state\n    # policy_step.info may contain side info for logging, such as action log\n    # probabilities.\n  ```\n\n  Policies can be saved to disk as SavedModels (see policy_saver.py and\n  policy_loader.py) or as TF Checkpoints.\n\n  A `PyTFEagerPolicy` can be used to wrap a `TFPolicy` so that it works with\n  `PyEnvironment`s.\n\n\n  **NOTE**: For API consistency, subclasses are not allowed to override public\n  methods of `TFPolicy` class. Instead, they may implement the protected methods\n  including `_get_initial_state`, `_action`, and `_distribution`. This\n  public-calls-private convention allowed this base class to do things like\n  properly add `spec` and shape checks, which provide users an easier experience\n  when debugging their environments and networks.\n\n  For researchers, and those developing new Policies, the `TFPolicy` base class\n  constructor also accept a `validate_args` parameter.  If `False`, this\n  disables all spec structure, dtype, and shape checks in the public methods of\n  these classes.  It allows algorithm developers to iterate and try different\n  input and output structures without worrying about overly restrictive\n  requirements, or input and output states being in a certain format.  However,\n  *disabling argument validation* can make it very hard to identify structural\n  input or algorithmic errors; and should not be done for final, or\n  production-ready, Policies.  In addition to having implementations that may\n  disagree with specs, this mean that the resulting Policy may no longer\n  interact well with other parts of TF-Agents.  Examples include impedance\n  mismatches with Actor/Learner APIs, replay buffers, and the model export\n  functionality in `PolicySaver.\n  """"""\n\n  # TODO(b/127327645) Remove this attribute.\n  # This attribute allows subclasses to back out of automatic tf.function\n  # attribute inside TF1 (for autodeps).\n  _enable_functions = True\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      policy_state_spec: types.NestedTensorSpec = (),\n      info_spec: types.NestedTensorSpec = (),\n      clip: bool = True,\n      emit_log_probability: bool = False,\n      automatic_state_reset: bool = True,\n      observation_and_action_constraint_splitter: Optional[\n          types.Splitter] = None,\n      validate_args: bool = True,\n      name: Optional[Text] = None):\n    """"""Initialization of TFPolicy class.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps. Usually\n        provided by the user to the subclass.\n      action_spec: A nest of BoundedTensorSpec representing the actions. Usually\n        provided by the user to the subclass.\n      policy_state_spec: A nest of TensorSpec representing the policy_state.\n        Provided by the subclass, not directly by the user.\n      info_spec: A nest of TensorSpec representing the policy info. Provided by\n        the subclass, not directly by the user.\n      clip: Whether to clip actions to spec before returning them.  Default\n        True. Most policy-based algorithms (PCL, PPO, REINFORCE) use unclipped\n        continuous actions for training.\n      emit_log_probability: Emit log-probabilities of actions, if supported. If\n        True, policy_step.info will have CommonFields.LOG_PROBABILITY set.\n        Please consult utility methods provided in policy_step for setting and\n        retrieving these. When working with custom policies, either provide a\n        dictionary info_spec or a namedtuple with the field \'log_probability\'.\n      automatic_state_reset:  If `True`, then `get_initial_policy_state` is used\n        to clear state in `action()` and `distribution()` for for time steps\n        where `time_step.is_first()`.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment. The function takes in a full observation and returns a\n        tuple consisting of 1) the part of the observation intended as input to\n        the network and 2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as: ```\n        def observation_and_action_constraint_splitter(observation): return\n          observation[\'network_input\'], observation[\'constraint\'] ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n          sure the provided `q_network` is compatible with the network-specific\n          half of the output of the\n          `observation_and_action_constraint_splitter`. In particular,\n          `observation_and_action_constraint_splitter` will be called on the\n          observation before passing to the network. If\n          `observation_and_action_constraint_splitter` is None, action\n          constraints are not applied.\n      validate_args: Python bool.  Whether to verify inputs to, and outputs of,\n        functions like `action` and `distribution` against spec structures,\n        dtypes, and shapes.\n\n        Research code may prefer to set this value to `False` to allow iterating\n        on input and output structures without being hamstrung by overly\n        rigid checking (at the cost of harder-to-debug errors).\n\n        See also `TFAgent.validate_args`.\n      name: A name for this module. Defaults to the class name.\n    """"""\n    super(TFPolicy, self).__init__(name=name)\n    common.check_tf1_allowed()\n    common.tf_agents_gauge.get_cell(\'TFAPolicy\').set(True)\n    common.assert_members_are_not_overridden(base_cls=TFPolicy, instance=self)\n    if not isinstance(time_step_spec, ts.TimeStep):\n      raise ValueError(\n          \'The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.\'\n          .format(type(time_step_spec)))\n\n    self._time_step_spec = time_step_spec\n    self._action_spec = action_spec\n    self._policy_state_spec = policy_state_spec\n    self._emit_log_probability = emit_log_probability\n    self._validate_args = validate_args\n\n    if emit_log_probability:\n      log_probability_spec = tensor_spec.BoundedTensorSpec(\n          shape=(),\n          dtype=tf.float32,\n          maximum=0,\n          minimum=-float(\'inf\'),\n          name=\'log_probability\')\n      log_probability_spec = tf.nest.map_structure(\n          lambda _: log_probability_spec, action_spec)\n      info_spec = policy_step.set_log_probability(info_spec,\n                                                  log_probability_spec)\n\n    self._info_spec = info_spec\n    self._setup_specs()\n    self._clip = clip\n    self._action_fn = common.function_in_tf1()(self._action)\n    self._automatic_state_reset = automatic_state_reset\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n\n  def _setup_specs(self):\n    self._policy_step_spec = policy_step.PolicyStep(\n        action=self._action_spec,\n        state=self._policy_state_spec,\n        info=self._info_spec)\n    self._trajectory_spec = trajectory.from_transition(self._time_step_spec,\n                                                       self._policy_step_spec,\n                                                       self._time_step_spec)\n\n  def variables(self) -> Sequence[tf.Variable]:\n    """"""Returns the list of Variables that belong to the policy.""""""\n    # Ignore self._variables() in favor of using tf.Module\'s tracking.\n    return super(TFPolicy, self).variables\n\n  @property\n  def observation_and_action_constraint_splitter(self) -> types.Splitter:\n    return self._observation_and_action_constraint_splitter\n\n  @property\n  def validate_args(self) -> bool:\n    """"""Whether `action` & `distribution` validate input and output args.""""""\n    return self._validate_args\n\n  def get_initial_state(self,\n                        batch_size: Optional[types.Int]) -> types.NestedTensor:\n    """"""Returns an initial state usable by the policy.\n\n    Args:\n      batch_size: Tensor or constant: size of the batch dimension. Can be None\n        in which case no dimensions gets added.\n\n    Returns:\n      A nested object of type `policy_state` containing properly\n      initialized Tensors.\n    """"""\n    return self._get_initial_state(batch_size)\n\n  def _maybe_reset_state(self, time_step, policy_state):\n    if policy_state is ():  # pylint: disable=literal-comparison\n      return policy_state\n\n    batch_size = tf.compat.dimension_value(time_step.discount.shape[0])\n    if batch_size is None:\n      batch_size = tf.shape(time_step.discount)[0]\n\n    # Make sure we call this with a kwarg as it may be wrapped in tf.function\n    # which would expect a tensor if it was not a kwarg.\n    zero_state = self.get_initial_state(batch_size=batch_size)\n    condition = time_step.is_first()\n    # When experience is a sequence we only reset automatically for the first\n    # time_step in the sequence as we can\'t easily generalize how the policy is\n    # unrolled over the sequence.\n    if nest_utils.get_outer_rank(time_step, self._time_step_spec) > 1:\n      condition = time_step.is_first()[:, 0, ...]\n    return nest_utils.where(condition, zero_state, policy_state)\n\n  def action(self,\n             time_step: ts.TimeStep,\n             policy_state: types.NestedTensor = (),\n             seed: Optional[types.Seed] = None) -> policy_step.PolicyStep:\n    """"""Generates next action given the time_step and policy_state.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: A Tensor, or a nested dict, list or tuple of Tensors\n        representing the previous policy_state.\n      seed: Seed to use if action performs sampling (optional).\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n        `action`: An action Tensor matching the `action_spec`.\n        `state`: A policy state tensor to be fed into the next call to action.\n        `info`: Optional side information such as action log probabilities.\n\n    Raises:\n      RuntimeError: If subclass __init__ didn\'t call super().__init__.\n      ValueError or TypeError: If `validate_args is True` and inputs or\n        outputs do not match `time_step_spec`, `policy_state_spec`,\n        or `policy_step_spec`.\n    """"""\n    if self._enable_functions and getattr(self, \'_action_fn\', None) is None:\n      raise RuntimeError(\n          \'Cannot find _action_fn.  Did %s.__init__ call super?\' %\n          type(self).__name__)\n    if self._enable_functions:\n      action_fn = self._action_fn\n    else:\n      action_fn = self._action\n\n    if self._validate_args:\n      time_step = nest_utils.prune_extra_keys(self._time_step_spec, time_step)\n      policy_state = nest_utils.prune_extra_keys(\n          self._policy_state_spec, policy_state)\n      nest_utils.assert_same_structure(\n          time_step,\n          self._time_step_spec,\n          message=\'time_step and time_step_spec structures do not match\')\n      nest_utils.assert_same_structure(\n          policy_state,\n          self._policy_state_spec,\n          message=\'policy_state and policy_state_spec structures do not match\')\n\n    if self._automatic_state_reset:\n      policy_state = self._maybe_reset_state(time_step, policy_state)\n    step = action_fn(time_step=time_step, policy_state=policy_state, seed=seed)\n\n    def clip_action(action, action_spec):\n      if isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n        return common.clip_to_spec(action, action_spec)\n      return action\n\n    if self._validate_args:\n      nest_utils.assert_same_structure(\n          step.action, self._action_spec,\n          message=\'action and action_spec structures do not match\')\n\n    if self._clip:\n      clipped_actions = tf.nest.map_structure(clip_action,\n                                              step.action,\n                                              self._action_spec)\n      step = step._replace(action=clipped_actions)\n\n    if self._validate_args:\n      nest_utils.assert_same_structure(\n          step,\n          self._policy_step_spec,\n          message=\'action output and policy_step_spec structures do not match\')\n\n      def compare_to_spec(value, spec):\n        return value.dtype.is_compatible_with(spec.dtype)\n\n      compatibility = [\n          compare_to_spec(v, s) for (v, s)\n          in zip(tf.nest.flatten(step.action),\n                 tf.nest.flatten(self.action_spec))]\n\n      if not all(compatibility):\n        get_dtype = lambda x: x.dtype\n        action_dtypes = tf.nest.map_structure(get_dtype, step.action)\n        spec_dtypes = tf.nest.map_structure(get_dtype, self.action_spec)\n\n        raise TypeError(\'Policy produced an action with a dtype that doesn\\\'t \'\n                        \'match its action_spec. Got action:\\n  %s\\n with \'\n                        \'action_spec:\\n  %s\' % (action_dtypes, spec_dtypes))\n\n    return step\n\n  def distribution(\n      self, time_step: ts.TimeStep, policy_state: types.NestedTensor = ()\n  ) -> policy_step.PolicyStep:\n    """"""Generates the distribution over next actions given the time_step.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: A Tensor, or a nested dict, list or tuple of Tensors\n        representing the previous policy_state.\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n\n        `action`: A tf.distribution capturing the distribution of next actions.\n        `state`: A policy state tensor for the next call to distribution.\n        `info`: Optional side information such as action log probabilities.\n\n    Raises:\n      ValueError or TypeError: If `validate_args is True` and inputs or\n        outputs do not match `time_step_spec`, `policy_state_spec`,\n        or `policy_step_spec`.\n    """"""\n    if self._validate_args:\n      time_step = nest_utils.prune_extra_keys(self._time_step_spec, time_step)\n      policy_state = nest_utils.prune_extra_keys(\n          self._policy_state_spec, policy_state)\n      nest_utils.assert_same_structure(\n          time_step,\n          self._time_step_spec,\n          message=\'time_step and time_step_spec structures do not match\')\n      nest_utils.assert_same_structure(\n          policy_state,\n          self._policy_state_spec,\n          message=\'policy_state and policy_state_spec structures do not match\')\n    if self._automatic_state_reset:\n      policy_state = self._maybe_reset_state(time_step, policy_state)\n    step = self._distribution(time_step=time_step, policy_state=policy_state)\n    if self.emit_log_probability:\n      # This here is set only for compatibility with info_spec in constructor.\n      info = policy_step.set_log_probability(\n          step.info,\n          tf.nest.map_structure(\n              lambda _: tf.constant(0., dtype=tf.float32),\n              policy_step.get_log_probability(self._info_spec)))\n      step = step._replace(info=info)\n    if self._validate_args:\n      nest_utils.assert_same_structure(\n          step,\n          self._policy_step_spec,\n          message=(\'distribution output and policy_step_spec structures \'\n                   \'do not match\'))\n    return step\n\n  def update(self,\n             policy,\n             tau: float = 1.0,\n             tau_non_trainable: Optional[float] = None,\n             sort_variables_by_name: bool = False) -> tf.Operation:\n    """"""Update the current policy with another policy.\n\n    This would include copying the variables from the other policy.\n\n    Args:\n      policy: Another policy it can update from.\n      tau: A float scalar in [0, 1]. When tau is 1.0 (the default), we do a hard\n        update. This is used for trainable variables.\n      tau_non_trainable: A float scalar in [0, 1] for non_trainable variables.\n        If None, will copy from tau.\n      sort_variables_by_name: A bool, when True would sort the variables by name\n        before doing the update.\n\n    Returns:\n      An TF op to do the update.\n    """"""\n    if self.variables():\n      return common.soft_variables_update(\n          policy.variables(),\n          self.variables(),\n          tau=tau,\n          tau_non_trainable=tau_non_trainable,\n          sort_variables_by_name=sort_variables_by_name)\n    else:\n      return tf.no_op()\n\n  @property\n  def emit_log_probability(self) -> bool:\n    """"""Whether this policy instance emits log probabilities or not.""""""\n    return self._emit_log_probability\n\n  @property\n  def time_step_spec(self) -> ts.TimeStep:\n    """"""Describes the `TimeStep` tensors returned by `step()`.\n\n    Returns:\n      A `TimeStep` namedtuple with `TensorSpec` objects instead of Tensors,\n      which describe the shape, dtype and name of each tensor returned by\n      `step()`.\n    """"""\n    return self._time_step_spec\n\n  @property\n  def action_spec(self) -> types.NestedTensorSpec:\n    """"""Describes the TensorSpecs of the Tensors expected by `step(action)`.\n\n    `action` can be a single Tensor, or a nested dict, list or tuple of\n    Tensors.\n\n    Returns:\n      An single BoundedTensorSpec, or a nested dict, list or tuple of\n      `BoundedTensorSpec` objects, which describe the shape and\n      dtype of each Tensor expected by `step()`.\n    """"""\n    return self._action_spec\n\n  @property\n  def policy_state_spec(self) -> types.NestedTensorSpec:\n    """"""Describes the Tensors expected by `step(_, policy_state)`.\n\n    `policy_state` can be an empty tuple, a single Tensor, or a nested dict,\n    list or tuple of Tensors.\n\n    Returns:\n      An single TensorSpec, or a nested dict, list or tuple of\n      `TensorSpec` objects, which describe the shape and\n      dtype of each Tensor expected by `step(_, policy_state)`.\n    """"""\n    return self._policy_state_spec\n\n  @property\n  def info_spec(self) -> types.NestedTensorSpec:\n    """"""Describes the Tensors emitted as info by `action` and `distribution`.\n\n    `info` can be an empty tuple, a single Tensor, or a nested dict,\n    list or tuple of Tensors.\n\n    Returns:\n      An single TensorSpec, or a nested dict, list or tuple of\n      `TensorSpec` objects, which describe the shape and\n      dtype of each Tensor expected by `step(_, policy_state)`.\n    """"""\n    return self._info_spec\n\n  @property\n  def policy_step_spec(self) -> policy_step.PolicyStep:\n    """"""Describes the output of `action()`.\n\n    Returns:\n      A nest of TensorSpec which describe the shape and dtype of each Tensor\n      emitted by `action()`.\n    """"""\n    return self._policy_step_spec\n\n  # TODO(kbanoop, ebrevdo): Should this be collect_data_spec to mirror agents?\n  @property\n  def trajectory_spec(self) -> trajectory.Trajectory:\n    """"""Describes the Tensors written when using this policy with an environment.\n\n    Returns:\n      A `Trajectory` containing all tensor specs associated with the\n      observation_spec, action_spec, policy_state_spec, and info_spec of\n      this policy.\n    """"""\n    return self._trajectory_spec\n\n  @property\n  def collect_data_spec(self) -> trajectory.Trajectory:\n    """"""Describes the Tensors written when using this policy with an environment.\n\n    Returns:\n      A nest of TensorSpec which describe the shape and dtype of each Tensor\n      required to train the agent which generated this policy.\n    """"""\n    return self._trajectory_spec\n\n  # Subclasses MAY optionally override _action.\n  def _action(self, time_step: ts.TimeStep,\n              policy_state: types.NestedTensor,\n              seed: Optional[types.Seed]) -> policy_step.PolicyStep:\n    """"""Implementation of `action`.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: A Tensor, or a nested dict, list or tuple of Tensors\n        representing the previous policy_state.\n      seed: Seed to use if action performs sampling (optional).\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n        `action`: An action Tensor matching the `action_spec`.\n        `state`: A policy state tensor to be fed into the next call to action.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n    seed_stream = tfp.util.SeedStream(seed=seed, salt=\'tf_agents_tf_policy\')\n    distribution_step = self._distribution(time_step, policy_state)\n    actions = tf.nest.map_structure(\n        lambda d: reparameterized_sampling.sample(d, seed=seed_stream()),\n        distribution_step.action)\n    info = distribution_step.info\n    if self.emit_log_probability:\n      try:\n        log_probability = tf.nest.map_structure(lambda a, d: d.log_prob(a),\n                                                actions,\n                                                distribution_step.action)\n        info = policy_step.set_log_probability(info, log_probability)\n      except:\n        raise TypeError(\'%s does not support emitting log-probabilities.\' %\n                        type(self).__name__)\n\n    return distribution_step._replace(action=actions, info=info)\n\n  ## Subclasses MUST implement these.\n  def _distribution(\n      self, time_step: ts.TimeStep,\n      policy_state: types.NestedTensorSpec) -> policy_step.PolicyStep:\n    """"""Implementation of `distribution`.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: A Tensor, or a nested dict, list or tuple of Tensors\n        representing the previous policy_state.\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n        `action`: A (optionally nested) of tfp.distribution.Distribution\n          capturing the distribution of next actions.\n        `state`: A policy state tensor for the next call to distribution.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n    raise NotImplementedError()\n\n  # Subclasses MAY optionally overwrite _get_initial_state.\n  def _get_initial_state(self, batch_size: int) -> types.NestedTensor:\n    """"""Returns the initial state of the policy network.\n\n    Args:\n      batch_size: A constant or Tensor holding the batch size. Can be None, in\n        which case the state will not have a batch dimension added.\n\n    Returns:\n      A nest of zero tensors matching the spec of the policy network state.\n    """"""\n    return tensor_spec.zero_spec_nest(\n        self._policy_state_spec,\n        outer_dims=None if batch_size is None else [batch_size])\n'"
tf_agents/policies/tf_policy_test.py,60,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass TfPolicyHoldsVariables(tf_policy.TFPolicy):\n  """"""Test tf_policy which contains only trainable variables.""""""\n\n  def __init__(self, init_var_value, var_scope, name=None):\n    """"""Initializes policy containing variables with specified value.\n\n    Args:\n      init_var_value: A scalar specifies the initial value of all variables.\n      var_scope: A String defines variable scope.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n    with tf.compat.v1.variable_scope(var_scope):\n      self._variables_list = [\n          common.create_variable(\n              ""var_1"", init_var_value, [3, 3], dtype=tf.float32),\n          common.create_variable(\n              ""var_2"", init_var_value, [5, 5], dtype=tf.float32)\n      ]\n\n  def _variables(self):\n    return self._variables_list\n\n  def _action(self, time_step, policy_state, seed):\n    return policy_step.PolicyStep(())\n\n  def _distribution(self, time_step, policy_state):\n    return policy_step.PolicyStep(())\n\n\nclass TFPolicyMismatchedDtypes(tf_policy.TFPolicy):\n  """"""Dummy tf_policy with mismatched dtypes.""""""\n\n  def __init__(self):\n    observation_spec = tensor_spec.TensorSpec([2, 2], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    super(TFPolicyMismatchedDtypes, self).__init__(time_step_spec, action_spec)\n\n  def _action(self, time_step, policy_state, seed):\n    # This action\'s dtype intentionally doesn\'t match action_spec\'s dtype.\n    return policy_step.PolicyStep(action=tf.constant([0], dtype=tf.int64))\n\n  def _distribution(self, time_step, policy_state):\n    return policy_step.PolicyStep(())\n\n\nclass TFPolicyMismatchedDtypesListAction(tf_policy.TFPolicy):\n  """"""Dummy tf_policy with mismatched dtypes and a list action_spec.""""""\n\n  def __init__(self):\n    observation_spec = tensor_spec.TensorSpec([2, 2], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([1], tf.int64, 0, 1),\n        tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    ]\n    super(TFPolicyMismatchedDtypesListAction,\n          self).__init__(time_step_spec, action_spec)\n\n  def _action(self, time_step, policy_state, seed):\n    # This time, the action is a list where only the second dtype doesn\'t match.\n    return policy_step.PolicyStep(action=[\n        tf.constant([0], dtype=tf.int64),\n        tf.constant([0], dtype=tf.int64)\n    ])\n\n  def _distribution(self, time_step, policy_state):\n    return policy_step.PolicyStep(())\n\n\nclass TfPassThroughPolicy(tf_policy.TFPolicy):\n\n  def _action(self, time_step, policy_state, seed):\n    distributions = self._distribution(time_step, policy_state)\n    actions = tf.nest.map_structure(lambda d: d.sample(), distributions.action)\n    return policy_step.PolicyStep(actions, policy_state, ())\n\n  def _distribution(self, time_step, policy_state):\n    action_distribution = tf.nest.map_structure(\n        lambda loc: tfp.distributions.Deterministic(loc=loc),\n        time_step.observation)\n    return policy_step.PolicyStep(action_distribution, policy_state, ())\n\n\nclass TfEmitLogProbsPolicy(tf_policy.TFPolicy):\n  """"""Dummy policy with constant probability distribution.""""""\n\n  def __init__(self, info_spec=()):\n    observation_spec = tensor_spec.TensorSpec([2, 2], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 5)\n    super(TfEmitLogProbsPolicy, self).__init__(\n        time_step_spec,\n        action_spec,\n        info_spec=info_spec,\n        emit_log_probability=True)\n\n  def _distribution(self, time_step, policy_state):\n    probs = tf.constant(\n        0.2, shape=[self.action_spec.maximum - self.action_spec.minimum])\n    action_distribution = tf.nest.map_structure(\n        lambda obs: tfp.distributions.Categorical(probs=probs),\n        time_step.observation)\n    step = policy_step.PolicyStep(action_distribution)\n    return step\n\n\nclass TfDictInfoAndLogProbs(TfEmitLogProbsPolicy):\n  """"""Same dummy policy as above except it stores more things in info.""""""\n\n  def __init__(self):\n    info_spec = {""test"": tensor_spec.BoundedTensorSpec([1], tf.int64, 0, 1)}\n    super(TfDictInfoAndLogProbs, self).__init__(info_spec=info_spec)\n\n  def _distribution(self, time_step, policy_state):\n    distribution_step = super(TfDictInfoAndLogProbs, self)._distribution(\n        time_step=time_step, policy_state=policy_state)\n    return distribution_step._replace(\n        info={""test"": tf.constant(1, dtype=tf.int64)})\n\n\nclass TfPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (""SoftUpdate"", 0.5, False),\n      (""SyncVariables"", 1.0, True),\n  )\n  def testUpdate(self, tau, sort_variables_by_name):\n    source_policy = TfPolicyHoldsVariables(\n        init_var_value=1., var_scope=""source"")\n    target_policy = TfPolicyHoldsVariables(\n        init_var_value=0., var_scope=""target"")\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for var in self.evaluate(target_policy.variables()):\n      self.assertAllEqual(var, np.zeros(var.shape))\n\n    update_op = target_policy.update(\n        source_policy, tau=tau, sort_variables_by_name=sort_variables_by_name)\n    self.evaluate(update_op)\n    for var in self.evaluate(target_policy.variables()):\n      self.assertAllEqual(var, np.ones(var.shape) * tau)\n\n  def testClipping(self):\n    action_spec = (tensor_spec.BoundedTensorSpec([1], tf.float32, 2, 3),\n                   tensor_spec.TensorSpec([1], tf.float32),\n                   tensor_spec.BoundedTensorSpec([1], tf.int32, 2, 3),\n                   tensor_spec.TensorSpec([1], tf.int32))\n    time_step_spec = ts.time_step_spec(action_spec)\n\n    policy = TfPassThroughPolicy(time_step_spec, action_spec, clip=True)\n\n    observation = (tf.constant(1, shape=(1,), dtype=tf.float32),\n                   tf.constant(1, shape=(1,), dtype=tf.float32),\n                   tf.constant(1, shape=(1,), dtype=tf.int32),\n                   tf.constant(1, shape=(1,), dtype=tf.int32))\n    time_step = ts.restart(observation)\n\n    clipped_action = self.evaluate(policy.action(time_step).action)\n    self.assertEqual(2, clipped_action[0])\n    self.assertEqual(1, clipped_action[1])\n    self.assertEqual(2, clipped_action[2])\n    self.assertEqual(1, clipped_action[3])\n\n  def testObservationsContainExtraFields(self):\n    action_spec = {\n        ""inp"": tensor_spec.TensorSpec([1], tf.float32)\n    }\n    time_step_spec = ts.time_step_spec(observation_spec=action_spec)\n\n    policy = TfPassThroughPolicy(time_step_spec, action_spec, clip=True)\n\n    observation = {""inp"": tf.constant(1, shape=(1,), dtype=tf.float32),\n                   ""extra"": tf.constant(1, shape=(1,), dtype=tf.int32)}\n\n    time_step = ts.restart(observation)\n\n    action = policy.action(time_step).action\n    distribution = policy.distribution(time_step).action\n    tf.nest.assert_same_structure(action, action_spec)\n    tf.nest.assert_same_structure(distribution, action_spec)\n    self.assertEqual(1, self.evaluate(action[""inp""]))\n    self.assertEqual(1, self.evaluate(distribution[""inp""].sample()))\n\n  def testValidateArgsDisabled(self):\n    action_spec = ""blah""\n    time_step_spec = ts.time_step_spec(observation_spec=None)\n    policy = TfPassThroughPolicy(\n        time_step_spec, action_spec, validate_args=False, clip=False)\n    observation = (tf.constant(1, shape=(1,), dtype=tf.float32),\n                   tf.constant(1, shape=(1,), dtype=tf.float32),\n                   tf.constant(1, shape=(1,), dtype=tf.int32),\n                   tf.constant(1, shape=(1,), dtype=tf.int32))\n    time_step = ts.restart(observation)\n\n    action = self.evaluate(policy.action(time_step).action)\n    self.assertAllEqual([[1], [1], [1], [1]], action)\n\n  def testMismatchedDtypes(self):\n    with self.assertRaisesRegexp(TypeError, "".*dtype that doesn\'t match.*""):\n      policy = TFPolicyMismatchedDtypes()\n      observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n      time_step = ts.restart(observation)\n      policy.action(time_step)\n\n  def testMatchedDtypes(self):\n    policy = TFPolicyMismatchedDtypes()\n\n    # Overwrite the action_spec to match the dtype of _action.\n    policy._action_spec = tensor_spec.BoundedTensorSpec([1], tf.int64, 0, 1)\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observation)\n    policy.action(time_step)\n\n  def testMismatchedDtypesListAction(self):\n    with self.assertRaisesRegexp(TypeError, "".*dtype that doesn\'t match.*""):\n      policy = TFPolicyMismatchedDtypesListAction()\n      observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n      time_step = ts.restart(observation)\n      policy.action(time_step)\n\n  def testMatchedDtypesListAction(self):\n    policy = TFPolicyMismatchedDtypesListAction()\n\n    # Overwrite the action_spec to match the dtype of _action.\n    policy._action_spec = [\n        tensor_spec.BoundedTensorSpec([1], tf.int64, 0, 1),\n        tensor_spec.BoundedTensorSpec([1], tf.int64, 0, 1)\n    ]\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observation)\n    policy.action(time_step)\n\n  def testEmitLogProbability(self):\n    policy = TfEmitLogProbsPolicy()\n    observation = tf.constant(2., shape=(2, 2), dtype=tf.float32)\n    time_step = ts.restart(observation)\n\n    step = self.evaluate(policy.action(time_step))\n    self.assertAlmostEqual(step.info.log_probability, np.log(0.2))\n\n  def testKeepInfoAndEmitLogProbability(self):\n    policy = TfDictInfoAndLogProbs()\n    observation = tf.constant(2., shape=(2, 2), dtype=tf.float32)\n    time_step = ts.restart(observation)\n\n    step = self.evaluate(policy.action(time_step))\n    self.assertEqual(step.info.get(""test"", None), 1)\n    self.assertAlmostEqual(step.info[""log_probability""], np.log(0.2))\n\n  def testAutomaticReset(self):\n    observation_spec = tensor_spec.TensorSpec([1], tf.float32)\n    action_spec = tensor_spec.TensorSpec([1], tf.float32)\n    policy_state_spec = tensor_spec.TensorSpec([1], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    policy = TfPassThroughPolicy(\n        time_step_spec,\n        action_spec,\n        policy_state_spec=policy_state_spec,\n        automatic_state_reset=True)\n\n    observation = tf.constant(1, dtype=tf.float32, shape=(1, 1))\n    reward = tf.constant(1, dtype=tf.float32, shape=(1,))\n    time_step = tf.nest.map_structure(lambda *t: tf.concat(t, axis=0),\n                                      ts.restart(observation, batch_size=1),\n                                      ts.transition(observation, reward),\n                                      ts.termination(observation, reward))\n\n    state = self.evaluate(\n        policy.action(time_step,\n                      policy_state=policy.get_initial_state(3) + 1).state)\n\n    self.assertEqual(0, state[0])\n    self.assertEqual(1, state[1])\n    self.assertEqual(1, state[2])\n\n    state = self.evaluate(\n        policy.distribution(\n            time_step, policy_state=policy.get_initial_state(3) + 1).state)\n\n    self.assertEqual(0, state[0])\n    self.assertEqual(1, state[1])\n    self.assertEqual(1, state[2])\n\n  def testStateShape(self):\n    time_step_spec = ts.time_step_spec(tensor_spec.TensorSpec([1], tf.float32))\n    action_spec = tensor_spec.TensorSpec([1], tf.float32)\n    policy_state_spec = {""foo"": tensor_spec.TensorSpec([1], tf.float32),\n                         ""bar"": tensor_spec.TensorSpec([2, 2], tf.int8)}\n\n    policy = TfPassThroughPolicy(\n        time_step_spec,\n        action_spec,\n        policy_state_spec=policy_state_spec)\n\n    # Test state shape with explicit batch_size\n    initial_state = policy.get_initial_state(3)\n    tf.nest.assert_same_structure(policy_state_spec, initial_state)\n    self.assertEqual([3, 1], initial_state[""foo""].shape.as_list())\n    self.assertEqual([3, 2, 2], initial_state[""bar""].shape.as_list())\n\n    # Test state shape with batch_size None\n    initial_state = policy.get_initial_state(None)\n    tf.nest.assert_same_structure(policy_state_spec, initial_state)\n    self.assertEqual([1], initial_state[""foo""].shape.as_list())\n    self.assertEqual([2, 2], initial_state[""bar""].shape.as_list())\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/policies/tf_py_policy.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Exposes a python policy as an in-graph TensorFlow policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ndef map_tensor_spec_to_dtypes_list(t_spec):\n  return [spec.dtype for spec in tf.nest.flatten(t_spec)]\n\n\nclass TFPyPolicy(tf_policy.TFPolicy):\n  """"""Exposes a Python policy as an in-graph TensorFlow policy.\n\n  # TODO(kbanoop): This class does not seem to handle batching/unbatching when\n  # converting between TF and Py policies.\n  """"""\n\n  def __init__(self,\n               policy: py_policy.PyPolicy,\n               py_policy_is_batched: bool = False,\n               name: Optional[Text] = None):\n    """"""Initializes a new `TFPyPolicy` instance with an Pyton policy .\n\n    Args:\n      policy: Python policy implementing `py_policy.PyPolicy`.\n      py_policy_is_batched: If False, time_steps will be unbatched before\n        passing to py_policy.action(), and a batch dimension will be added to\n        the returned action. This will only work with time_steps that have a\n        batch dimension of 1. If True, the time_step (input) and action (output)\n        are passed exactly as is from/to the py_policy.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      TypeError: if a non python policy is passed to constructor.\n    """"""\n    if not isinstance(policy, py_policy.PyPolicy):\n      raise TypeError(\n          \'Input policy should implement py_policy.PyPolicy, but saw %s.\' %\n          type(policy).__name__)\n\n    self._py_policy = policy\n    self._py_policy_is_batched = py_policy_is_batched\n\n    (time_step_spec, action_spec,\n     policy_state_spec, info_spec) = tf.nest.map_structure(\n         tensor_spec.from_spec,\n         (policy.time_step_spec, policy.action_spec, policy.policy_state_spec,\n          policy.info_spec))\n\n    super(TFPyPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy_state_spec=policy_state_spec,\n        info_spec=info_spec,\n        clip=False,\n        name=name,\n        automatic_state_reset=False)\n\n    # Output types of py_funcs.\n    self._policy_state_dtypes = map_tensor_spec_to_dtypes_list(\n        self.policy_state_spec)\n    self._policy_step_dtypes = map_tensor_spec_to_dtypes_list(\n        self.policy_step_spec)\n\n  # Wrapped in common.function to avoid failures in eager mode. This happens\n  # when the policy_state is empty and it gets dropped by tf.nest.flatten\n  # # in the numpy_function\n  @common.function\n  def _get_initial_state(self, batch_size):\n    """"""Invokes  python policy reset through numpy_function.\n\n    Args:\n      batch_size: Batch size for the get_initial_state tensor(s).\n\n    Returns:\n      A tuple of (policy_state, reset_op).\n      policy_state: Tensor, or a nested dict, list or tuple of Tensors,\n      representing the new policy state.\n      reset_op: a list of Tensors representing the results of py_policy.reset().\n    """"""\n\n    def _get_initial_state_fn(*batch_size):\n      return tf.nest.flatten(\n          self._py_policy.get_initial_state(batch_size=batch_size))\n\n    with tf.name_scope(\'get_initial_state\'):\n      flat_policy_state = tf.numpy_function(\n          _get_initial_state_fn, [batch_size],\n          self._policy_state_dtypes,\n          name=\'get_initial_state_numpy_function\')\n      return tf.nest.pack_sequence_as(\n          structure=self.policy_state_spec, flat_sequence=flat_policy_state)\n\n  # Wrapped in common.function to avoid failures in eager mode. This happens\n  # when empty fields in the policy_step get dropped by tf.nest.flatten\n  # in the numpy_function.\n  @common.function\n  def _action(self, time_step, policy_state, seed):\n    if seed is not None:\n      raise NotImplementedError(\n          \'seed is not supported; but saw seed: {}\'.format(seed))\n\n    def _action_fn(*flattened_time_step_and_policy_state):\n      packed_py_time_step, packed_py_policy_state = tf.nest.pack_sequence_as(\n          structure=(self._py_policy.time_step_spec,\n                     self._py_policy.policy_state_spec),\n          flat_sequence=flattened_time_step_and_policy_state)\n      py_action_step = self._py_policy.action(\n          time_step=packed_py_time_step, policy_state=packed_py_policy_state)\n      return tf.nest.flatten(py_action_step)\n\n    with tf.name_scope(\'action\'):\n      if not self._py_policy_is_batched:\n        time_step = nest_utils.unbatch_nested_tensors(time_step)\n      flattened_input_tensors = tf.nest.flatten((time_step, policy_state))\n\n      flat_action_step = tf.numpy_function(\n          _action_fn,\n          flattened_input_tensors,\n          self._policy_step_dtypes,\n          name=\'action_numpy_function\')\n      action_step = tf.nest.pack_sequence_as(\n          structure=self.policy_step_spec, flat_sequence=flat_action_step)\n      if not self._py_policy_is_batched:\n        action_step = action_step._replace(\n            action=nest_utils.batch_nested_tensors(action_step.action))\n      return action_step\n\n  def _variables(self):\n    """"""Returns default [] representing a policy that has no variables.""""""\n    return []\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\'%s does not support distribution yet.\' %\n                              self.__class__.__name__)\n'"
tf_agents/policies/tf_py_policy_test.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.policies.tf_py_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing.absltest import mock\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.policies import tf_py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import test_utils\n\n\nclass TFPyPolicyTest(test_utils.TestCase):\n\n  def testRandomPyPolicyGeneratesActionTensors(self):\n    array_action_spec = array_spec.BoundedArraySpec((7,), np.int32, -10, 10)\n    observation = tf.ones([3], tf.float32)\n    time_step = ts.restart(observation)\n\n    observation_spec = tensor_spec.TensorSpec.from_tensor(observation)\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    tf_py_random_policy = tf_py_policy.TFPyPolicy(\n        random_py_policy.RandomPyPolicy(time_step_spec=time_step_spec,\n                                        action_spec=array_action_spec))\n\n    batched_time_step = nest_utils.batch_nested_tensors(time_step)\n    action_step = tf_py_random_policy.action(time_step=batched_time_step)\n    action, new_policy_state = self.evaluate(\n        [action_step.action, action_step.state])\n\n    self.assertEqual((1,) + array_action_spec.shape, action.shape)\n    self.assertTrue(np.all(action >= array_action_spec.minimum))\n    self.assertTrue(np.all(action <= array_action_spec.maximum))\n    self.assertEqual(new_policy_state, ())\n\n  def testAction(self):\n    py_observation_spec = array_spec.BoundedArraySpec((3,), np.int32, 1, 1)\n    py_time_step_spec = ts.time_step_spec(py_observation_spec)\n    py_action_spec = array_spec.BoundedArraySpec((7,), np.int32, 1, 1)\n    py_policy_state_spec = array_spec.BoundedArraySpec((5,), np.int32, 0, 1)\n    py_policy_info_spec = array_spec.BoundedArraySpec((3,), np.int32, 0, 1)\n\n    mock_py_policy = mock.create_autospec(py_policy.PyPolicy)\n    mock_py_policy.time_step_spec = py_time_step_spec\n    mock_py_policy.action_spec = py_action_spec\n    mock_py_policy.policy_state_spec = py_policy_state_spec\n    mock_py_policy.info_spec = py_policy_info_spec\n\n    expected_py_policy_state = np.ones(py_policy_state_spec.shape,\n                                       py_policy_state_spec.dtype)\n    expected_py_time_step = tf.nest.map_structure(\n        lambda arr_spec: np.ones((1,) + arr_spec.shape, arr_spec.dtype),\n        py_time_step_spec)\n    expected_py_action = np.ones((1,) + py_action_spec.shape,\n                                 py_action_spec.dtype)\n    expected_new_py_policy_state = np.zeros(py_policy_state_spec.shape,\n                                            py_policy_state_spec.dtype)\n    expected_py_info = np.zeros(py_policy_info_spec.shape,\n                                py_policy_info_spec.dtype)\n\n    mock_py_policy.action.return_value = policy_step.PolicyStep(\n        nest_utils.unbatch_nested_array(expected_py_action),\n        expected_new_py_policy_state, expected_py_info)\n\n    tf_mock_py_policy = tf_py_policy.TFPyPolicy(mock_py_policy)\n    time_step = tf.nest.map_structure(\n        lambda arr_spec: tf.ones((1,) + arr_spec.shape, arr_spec.dtype),\n        py_time_step_spec)\n    action_step = tf_mock_py_policy.action(\n        time_step, tf.ones(py_policy_state_spec.shape, tf.int32))\n    py_action_step = self.evaluate(action_step)\n\n    self.assertEqual(1, mock_py_policy.action.call_count)\n    np.testing.assert_equal(\n        mock_py_policy.action.call_args[1][\'time_step\'],\n        nest_utils.unbatch_nested_array(expected_py_time_step))\n    np.testing.assert_equal(mock_py_policy.action.call_args[1][\'policy_state\'],\n                            expected_py_policy_state)\n    np.testing.assert_equal(py_action_step.action, expected_py_action)\n    np.testing.assert_equal(py_action_step.state, expected_new_py_policy_state)\n    np.testing.assert_equal(py_action_step.info, expected_py_info)\n\n  def testZeroState(self):\n    policy_state_length = 5\n    batch_size = 3\n    mock_py_policy = mock.create_autospec(py_policy.PyPolicy)\n    observation_spec = array_spec.ArraySpec((3,), np.float32)\n    mock_py_policy.time_step_spec = ts.time_step_spec(observation_spec)\n    mock_py_policy.action_spec = array_spec.BoundedArraySpec(\n        (7,), np.int32, 1, 1)\n    py_policy_state_spec = array_spec.BoundedArraySpec((policy_state_length,),\n                                                       np.int32, 1, 1)\n    # Make the mock policy and reset return value.\n    mock_py_policy.policy_state_spec = py_policy_state_spec\n    mock_py_policy.info_spec = ()\n\n    expected_py_policy_state = np.zeros(\n        [batch_size] + list(py_policy_state_spec.shape),\n        py_policy_state_spec.dtype)\n    mock_py_policy.get_initial_state.return_value = expected_py_policy_state\n\n    tf_mock_py_policy = tf_py_policy.TFPyPolicy(mock_py_policy)\n    initial_state = tf_mock_py_policy.get_initial_state(batch_size=batch_size)\n    initial_state_ = self.evaluate(initial_state)\n\n    self.assertEqual(1, mock_py_policy.get_initial_state.call_count)\n    np.testing.assert_equal(initial_state_, expected_py_policy_state)\n\n  def testDistributionRaisesNotImplementedError(self):\n    mock_tf_py_policy = tf_py_policy.TFPyPolicy(\n        self._get_mock_py_policy())\n    observation = tf.ones([5], tf.float32)\n    time_step = ts.restart(observation)\n    with self.assertRaises(NotImplementedError):\n      mock_tf_py_policy.distribution(time_step=time_step)\n\n  def testVariables(self):\n    mock_tf_py_policy = tf_py_policy.TFPyPolicy(\n        self._get_mock_py_policy())\n    np.testing.assert_equal(mock_tf_py_policy.variables(), [])\n\n  def testPyPolicyIsBatchedTrue(self):\n    action_dims = 5\n    observation_dims = 3\n    batch_size = 2\n    array_action_spec = array_spec.BoundedArraySpec((action_dims,), np.int32,\n                                                    -10, 10)\n    observation_spec = array_spec.ArraySpec((observation_dims,), np.float32)\n    array_time_step_spec = ts.time_step_spec(observation_spec)\n\n    observation = tf.ones([batch_size, observation_dims], tf.float32)\n    time_step = ts.restart(observation, batch_size=batch_size)\n\n    tf_py_random_policy = tf_py_policy.TFPyPolicy(\n        random_py_policy.RandomPyPolicy(time_step_spec=array_time_step_spec,\n                                        action_spec=array_action_spec),\n        py_policy_is_batched=True)\n\n    action_step = tf_py_random_policy.action(time_step=time_step)\n    action = self.evaluate(action_step.action)\n\n    self.assertEqual(action.shape, (batch_size, action_dims))\n\n  def _get_mock_py_policy(self):\n    mock_py_policy = mock.create_autospec(py_policy.PyPolicy)\n    observation_spec = tensor_spec.TensorSpec([5], dtype=tf.float32)\n    mock_py_policy.time_step_spec = ts.time_step_spec(observation_spec)\n    mock_py_policy.action_spec = tensor_spec.BoundedTensorSpec(\n        [3], tf.float32, -1.0, 1.0)\n    mock_py_policy.policy_state_spec = ()\n    mock_py_policy.info_spec = ()\n    return mock_py_policy\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Replay Buffers Module.""""""\n\nfrom tf_agents.replay_buffers import py_hashed_replay_buffer\nfrom tf_agents.replay_buffers import py_uniform_replay_buffer\nfrom tf_agents.replay_buffers import replay_buffer\nfrom tf_agents.replay_buffers import table\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\n'"
tf_agents/replay_buffers/episodic_replay_buffer.py,200,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An episodic replay buffer of nests of Tensors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import episodic_table\nfrom tf_agents.replay_buffers import replay_buffer as replay_buffer_base\nfrom tf_agents.replay_buffers import table\nfrom tf_agents.utils import common\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.data.util import nest as data_nest  # TF internal\nfrom tensorflow.python.layers import utils  # TF internal\nfrom tensorflow.python.ops import list_ops  # TF internal\nfrom tensorflow.python.ops.distributions import util as distributions_util  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\n\n# The ID value for episode id holders that do not point to an actual episode.\n_INVALID_EPISODE_ID = -1\n\n\nEpisodes = collections.namedtuple(\'Episodes\',\n                                  [\'length\', \'completed\', \'tensor_lists\'])\n\nBufferInfo = collections.namedtuple(\'BufferInfo\', [\'ids\'])\n\n\n@gin.configurable\nclass EpisodicReplayBuffer(replay_buffer_base.ReplayBuffer):\n  """"""An episodic ReplayBuffer with Uniform sampling.""""""\n\n  # Internal details of the EpisodicReplayBuffer:\n  #\n  #  The EpisodicReplayBuffer is composed of the following objects:\n  #     _data_table: EpisodicTable structure data_spec, capacity _capacity.\n  #     _id_table: Table containing single int64 id, capacity _capacity.\n  #     _episodes_loc_to_id_map: counter variable, capacity _capacity.\n  #     _episode_lengths: counter variable, capacity _capacity.\n  #     _episode_completed: uint8(bool) variable, capacity _capacity.\n  #     _last_episode: scalar int variable.\n  #     _add_episode_critical_section: CriticalSection for adding new episodes.\n  #\n  #  Users call create_episode_ids() to get a scalar or vector int64 Tensor\n  #  of size num_episodes (or scalar, if num_episodes is None).\n  #\n  #  All operations map values from an input Tensor ""episode_id[s]"" to\n  #  an episode_location.  The calculation is:\n  #     episode_location = episode_id % capacity.\n  #\n  #  New episodes are emitted by incrementing _last_episode inside the critical\n  #  section, and returning the new value.  The values of new episode ids\n  #  are logical values: they always go up.\n  #\n  #  To get the current logical id for a given concrete location in the replay\n  #  buffer, look it up via _episodes_loc_to_id_map[episode_id].\n  #\n\n  def __init__(self,\n               data_spec,\n               capacity=1000,\n               completed_only=False,\n               buffer_size=8,\n               name_prefix=\'EpisodicReplayBuffer\',\n               device=\'cpu:*\',\n               seed=None,\n               begin_episode_fn=None,\n               end_episode_fn=None,\n               dataset_drop_remainder=False,\n               dataset_window_shift=None):\n    """"""Creates an EpisodicReplayBuffer.\n\n    This class receives a dataspec and capacity and creates a replay buffer\n    supporting read/write operations, organized into episodes.\n    This uses an underlying EpisodicTable with capacity equal to\n    capacity.  Each row in the table can have an episode of unbounded\n    length.\n\n    Args:\n      data_spec: A TensorSpec or a list/tuple/nest of TensorSpecs describing\n        a single item that can be stored in this buffer.\n      capacity: An integer, the maximum number of episodes.\n      completed_only: Scalar bool.  Whether to sample full episodes\n        (in as_dataset if `num_steps = None`), and whether to sample subsets\n        from full episodes (in as_dataset `num_steps != None`).\n      buffer_size: How many episode IDs to precalculate in a buffer when using\n        `as_dataset(..., single_deterministic_pass=False)`.\n\n        This parameter controls how often episode IDs are sampled\n        according to their lengths in the `tf.data.Dataset` returned by\n        `as_dataset`.  Choosing a small number means episodes are sampled\n        more frequently, which is expensive, but new data in the replay buffer\n        is seen more quickly by the resulting dataset.  Choosing a larger number\n        means the sampling is less frequent and less costly, but new episodes\n        and updated episode lengths may not be respected for up to\n        `buffer_size` requests to the `Dataset` iterator.\n\n        For example, if `buffer_size > 1` then a new episode may be added to\n        the replay buffer, but this episode won\'t be ""seen"" by the\n        `Dataset` for up to `buffer_size - 1` more accesses.\n      name_prefix: A prefix for variable and op names created by this class.\n      device: A TensorFlow device to place the Variables and ops.\n      seed: optional random seed for sampling operations.\n      begin_episode_fn: A function that maps batched tensors respecting\n        `data_spec` to a boolean scalar or vector Tensor, indicating whether the\n        given entries are the start of a new episode.\n\n        Used by `add_batch`, `add_sequence`, and `extend_episodes` to indicate\n        whether the `episode_id` should be incremented.\n\n        Default value:\n        ```python\n        begin_episode_fn = lambda traj: traj.is_first()\n        ```\n      end_episode_fn: A function that maps batched tensors respecting\n        `data_spec` to a boolean scalar or vector Tensor, indicating whether the\n        given entries are the end of an episode.\n\n        **NOTE** The current default behavior is to mark an episode as\n        completed once it receives its final reward.  However,\n        additional frames may be added to the episode after this.\n        Typically, exactly one boundary frame (LAST -> FIRST), is likely\n        to be added to the episode after it is marked as completed).\n\n        Used by `add_batch`, `add_sequence`, and `extend_episodes` to\n        indicate whether to end the episode at `episode_id`.\n\n        Default value:\n        ```python\n        begin_episode_fn = lambda traj: traj.is_last()\n        ```\n      dataset_drop_remainder: If `True`, then when calling `as_dataset` with\n        arguments `sample_batch_size is not None`, the final batch will be\n        dropped if it does not contain exactly `sample_batch_size` items.  This\n        is helpful for static shape inference as the resulting tensors will\n        always have leading dimension `sample_batch_size` instead of `None`.\n      dataset_window_shift: Window shift used when calling\n        `as_dataset` with arguments `single_deterministic_pass=True` and\n        `num_steps is not None`.  This determines how the resulting frames are\n        windowed.  If `None`, then there is no overlap created between frames\n        and each frame is seen exactly once.  For example, if `max_length=5`,\n        `num_steps=2`, `sample_batch_size=None`, and\n        `dataset_window_shift=None`, then the datasets returned will have\n        frames `{[0, 1], [2, 3], [4]}`.\n\n        If `num_steps is not None`, then windows are created\n        with a window overlap of `dataset_window_shift` and you will see each\n        frame up to `num_steps` times.  For example, if `max_length=5`,\n        `num_steps=2`, `sample_batch_size=None`, and `dataset_window_shift=1`,\n        then the datasets returned will have windows of shifted repeated frames:\n        `{[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]}`.\n\n        For more details, see the documentation of `tf.data.Dataset.window`,\n        specifically for the `shift` argument.\n\n        The default behavior is to not overlap frames\n        (`dataset_window_shift=None`) but users often want to see all\n        combinations of frame sequences, in which case `dataset_window_shift=1`\n        is the appropriate value.\n    """"""\n    super(EpisodicReplayBuffer, self).__init__(\n        data_spec,\n        capacity,\n        stateful_dataset=True)\n    # Create tables `table_fn(data_spec, capacity)` that\n    # can read/write nested tensors.\n    table_fn = table.Table\n\n    # Create tables `episode_table_fn(data_spec, capacity, name_prefix)`\n    # that can read/write nested variable-length episodes\n    # (in practice this means using TensorLists).\n    episode_table_fn = episodic_table.EpisodicTable\n\n    if begin_episode_fn is None:\n      def _begin_episode_fn(t):\n        is_first = getattr(t, \'is_first\', None)\n        if not callable(is_first):\n          raise TypeError(\n              \'Argument t is not a Trajectory; did you forget to pass the \'\n              \'proper begin_episode_fn to EpisodicReplayBuffer?  Saw: \\\'{}\\\'\'\n              .format(t))\n        return is_first()\n      begin_episode_fn = _begin_episode_fn\n    if end_episode_fn is None:\n      def _end_episode_fn(t):\n        is_last = getattr(t, \'is_last\', None)\n        if not callable(is_last):\n          raise TypeError(\n              \'Argument t is not a Trajectory; did you forget to pass the \'\n              \'proper end_episode_fn to EpisodicReplayBuffer?  Saw: \\\'{}\\\'\'\n              .format(t))\n        return is_last()\n      end_episode_fn = _end_episode_fn\n    if not callable(begin_episode_fn):\n      raise TypeError(\n          \'begin_episode_fn is not callable: {}\'.format(begin_episode_fn))\n    if not callable(end_episode_fn):\n      raise TypeError(\n          \'end_episode_fn is not callable: {}\'.format(end_episode_fn))\n    self._begin_episode_fn = begin_episode_fn\n    self._end_episode_fn = end_episode_fn\n    self._id_spec = specs.TensorSpec([], dtype=tf.int64, name=\'id\')\n    self._name_prefix = name_prefix\n    self._device = device\n    self._seed = seed\n    self._completed_only = completed_only\n    self._buffer_size = buffer_size\n    self._dataset_window_shift = dataset_window_shift\n    self._dataset_drop_remainder = dataset_drop_remainder\n    self._num_writes = common.create_variable(\'num_writes_counter\')\n\n    with tf.device(self._device):\n      self._data_table = episode_table_fn(\n          self._data_spec, self._capacity, self._name_prefix)\n      # The episode ids\n      self._id_table = table_fn(self._id_spec, self._capacity)\n      self._episodes_loc_to_id_map = common.create_variable(\n          \'episodes_loc_to_id_map\', shape=[self._capacity],\n          initial_value=_INVALID_EPISODE_ID)\n      self._episode_lengths = common.create_variable(\n          \'episode_lengths\', shape=[self._capacity], initial_value=0)\n      # Marks episodes as completed or not.\n      # TODO(b/80430723) Add a way for users to mark episodes completed.\n      # TODO(b/76154485) Change to tf.bool.\n      self._episode_completed = common.create_variable(\n          \'episode_completed\',\n          shape=[self._capacity],\n          dtype=tf.uint8,\n          initial_value=0)\n      # The last episode id so far in the table.\n      self._last_episode = common.create_variable(\n          \'last_episode\', initial_value=_INVALID_EPISODE_ID)\n      self._add_episode_critical_section = tf.CriticalSection(\n          name=\'add_episode\')\n\n  @property\n  def num_writes(self):\n    return self._num_writes\n\n  @property\n  def name_prefix(self):\n    return self._name_prefix\n\n  def _num_frames(self):\n    """"""Returns the number of frames in the buffer.""""""\n    return tf.math.reduce_sum(self._episode_lengths)\n\n  def create_episode_ids(self, num_episodes=None):\n    """"""Returns a new tensor containing initial invalid episode ID(s).\n\n    This tensor is meant to be passed to methods like `add_batch` and\n    `extend_episodes`; those methods will return an updated set of episode id\n    values in their output.  To keep track of updated episode IDs across\n    multiple TF1 session run calls, the `episode_ids` may be read out and passed\n    back in by the user, or stored in a `tf.Variable`.  A helper class which\n    does this for you is available in this module, it is called\n    `StatefulEpisodicReplayBuffer`.\n\n    A simple non-`Variable` way to do this (in TF1) is:\n\n    ```python\n    data = collect_data_tf_op()\n    episode_ids = tf.placeholder_with_default(rb.create_episode_ids(3), [3])\n    new_episode_ids = rb.add_batch(data, episode_ids)\n\n    ids = session.run(episode_ids)\n    while True:\n      ...\n      ids = session.run(new_episode_ids, feed_dict=dict(episode_ids=ids))\n    ```\n\n    The initial value of these ids is subject to change, but currently set\n    to `-1`.  When methods like `add_batch` see entries like this, they\n    reserve a new (valid) id for this entry in the buffer and return the\n    associated id in this location.\n\n    Args:\n      num_episodes: (Optional) int32, number of episode IDs to create.\n        This may be a tensor.  If `None`, a scalar ID tensor is returned.\n\n    Returns:\n      An int64 Tensor containing initial episode(s) ID(s).\n\n    Raises:\n      ValueError: If `num_episodes` is bigger than capacity, or non-scalar.\n    """"""\n    if tf.is_tensor(num_episodes):\n      if num_episodes.shape.rank != 0:\n        raise ValueError(\'num_episodes must be a scalar, but saw shape: {}\'\n                         .format(num_episodes.shape))\n      return tf.fill(\n          [num_episodes],\n          tf.convert_to_tensor(_INVALID_EPISODE_ID, dtype=tf.int64),\n          name=\'episode_id\')\n\n    shape = ()\n    if num_episodes is not None and num_episodes > 0:\n      if num_episodes > self._capacity:\n        raise ValueError(\'Buffer cannot create episode_ids when \'\n                         \'num_episodes {} > capacity {}.\'.format(\n                             num_episodes, self._capacity))\n      shape = (num_episodes,)\n    return tf.constant(\n        _INVALID_EPISODE_ID, shape=shape, dtype=tf.int64, name=\'episode_id\')\n\n  def add_sequence(self, items, episode_id):\n    """"""Adds a sequence of items to the replay buffer for the selected episode.\n\n    Args:\n      items: A sequence of items to be added to the buffer. Items will have the\n        same structure as the data_spec of this class, but the tensors in items\n        will have an outer sequence dimension in addition to the corresponding\n        spec in data_spec.\n      episode_id: A Tensor containing the current episode_id.\n\n    Returns:\n      An updated episode id Tensor.  Accessing this episode id value will,\n      as a side effect, start or end the current episode in the buffer.\n    """"""\n    episode_id.shape.assert_has_rank(0)\n    with tf.device(self._device):\n      with tf.name_scope(\'add_steps\'):\n        # If users pass in, e.g., a python list [2, 3, 4] of type int32\n        # but the data_spec requires an int64, then the user will get a very\n        # confusing error much deeper in the TensorList code.  Doing the\n        # conversion here either converts when necessary, or raises an error\n        # on incompatible types earlier in the run.\n        items = tf.nest.map_structure(\n            lambda x, spec: tf.convert_to_tensor(value=x, dtype=spec.dtype),\n            items, self._data_spec)\n        item_0 = tf.nest.flatten(items)[0]\n        num_steps = tf.cast(\n            tf.compat.dimension_value(item_0.shape[0]) or\n            tf.shape(input=item_0)[0], tf.int64)\n        # If begin_episode is True, then the increment of the episode_id happens\n        # before trying to add anything to the buffer, regardless of whether the\n        # item will actually be added.\n        begin_episode = self._begin_episode_fn(items)\n        end_episode = self._end_episode_fn(items)\n        new_episode_id = self._get_episode_id(\n            episode_id, begin_episode, end_episode)\n        episode_location = self._get_episode_id_location(new_episode_id)\n\n        def _add_steps():\n          """"""Add sequence of items to the buffer.""""""\n          inc_episode_length = self._increment_episode_length_locked(\n              episode_location, num_steps)\n          write_data_op = self._data_table.append(episode_location, items)\n          with tf.control_dependencies([inc_episode_length, write_data_op]):\n            return tf.identity(new_episode_id)\n\n        # Accessing episode_id may modify\n        # self._episodes_loc_to_id_map, so ensure it is executed\n        # before the tf.equal.\n        with tf.control_dependencies([new_episode_id]):\n          episode_valid = tf.equal(\n              self._episodes_loc_to_id_map[episode_location], new_episode_id)\n        def _maybe_add_steps():\n          return self._add_episode_critical_section.execute(_add_steps)\n        return utils.smart_cond(\n            episode_valid,\n            _maybe_add_steps,\n            lambda: tf.identity(new_episode_id),\n            name=\'conditioned_add_steps\')\n\n  def add_batch(self, items, episode_ids):\n    """"""Adds a batch of single steps for the corresponding episodes IDs.\n\n    Args:\n      items: A batch of items to be added to the buffer. Items will have the\n        same structure as the data_spec of this class, but the tensors in items\n        will have an extra outer dimension `(num_episodes, ...)` in addition to\n        the corresponding spec in data_spec.\n      episode_ids: A int64 vector `Tensor` containing the ids of the\n        episodes the items are being added to. Shaped `(num_episodes,)`.\n\n    Returns:\n      A `Tensor` containing the updated episode ids.  Accessing or executing\n      this tensor also adds `items` to the replay buffer.\n    """"""\n    episode_ids.shape.assert_has_rank(1)\n    with tf.device(self._device):\n      with tf.name_scope(\'add_batch\'):\n        # If begin_episode is True, then the increment of the episode_id happens\n        # before trying to add anything to the buffer, regardless of whether the\n        # item will actually be added.\n\n        begin_episode = self._begin_episode_fn(items)\n        end_episode = self._end_episode_fn(items)\n        batch_episode_ids = self._get_batch_episode_ids(episode_ids,\n                                                        begin_episode,\n                                                        end_episode)\n        episodes_locations = tf.math.mod(batch_episode_ids, self._capacity)\n        # Accessing episode_id may modify self._episodes_loc_to_id_map, so\n        # ensure it is executed before\n        with tf.control_dependencies([episodes_locations]):\n          episode_valid = tf.equal(\n              self._episodes_loc_to_id_map.sparse_read(episodes_locations),\n              batch_episode_ids)\n\n        def _add_batch():\n          """"""Add elements to the appropiate episode_locations.""""""\n          ids_to_update = tf.reshape(tf.compat.v1.where(episode_valid), [-1])\n          episodes_locations_ = tf.gather(episodes_locations, ids_to_update)\n          filter_items = lambda item: tf.gather(item, ids_to_update)\n          items_ = tf.nest.map_structure(filter_items, items)\n          write_data_op = self._data_table.add(episodes_locations_, items_)\n          inc_episode_lengths = self._increment_episode_length_locked(\n              episodes_locations_)\n          inc_write_counter_op = self._num_writes.assign_add(1)\n          with tf.control_dependencies([\n              write_data_op, inc_episode_lengths, inc_write_counter_op]):\n            return tf.identity(batch_episode_ids)\n\n        num_adds = tf.reduce_sum(input_tensor=tf.cast(episode_valid, tf.int64))\n\n        def _maybe_add_batch():\n          return self._add_episode_critical_section.execute(_add_batch)\n\n        return tf.cond(\n            pred=num_adds > 0,\n            true_fn=_maybe_add_batch,\n            false_fn=lambda: episode_ids)\n\n  def gather_all(self):\n    """"""Returns all the items in buffer.\n\n    Returns:\n      Returns all the items currently in the buffer. Returns a tensor\n      of shape [1, SUM(T_i), ...]. Since episodes can be of different lengths,\n      all steps of all episodes are grouped into one batch. Thus the first\n      dimension is batch size = 1, the second dimension is of size equal to\n      the sum of all timesteps for all episodes (SUM(T_i) for i in episode_ids).\n      The remaining dimensions are the shape of the spec of items in the buffer.\n    """"""\n    items, _ = self._gather_all()\n    return items\n\n  # Defining abstract methods from replay_buffers.ReplayBuffer\n\n  def _add_batch(self, items):\n    raise NotImplementedError(""""""add_batch(items) is not implemented in\n      EpisodicReplayBuffer. Use add_batch(items, episode_ids) instead"""""")\n\n  def _get_next(self,\n                sample_batch_size=None,\n                num_steps=None,\n                time_stacked=None):\n    """"""Returns an episode sampled uniformly from the buffer.\n\n    Args:\n      sample_batch_size: Not used\n      num_steps: Not used\n      time_stacked: Not used\n\n    Returns:\n      A 2-tuple containing:\n\n        - An episode sampled uniformly from the buffer.\n        - BufferInfo NamedTuple, containing the episode id.\n    """"""\n    with tf.device(self._device):\n      with tf.name_scope(\'get_next\'):\n        episode_id = self._sample_episode_ids(shape=[], seed=self._seed)\n        row = self._get_episode_id_location(episode_id)\n        data = self._data_table.get_episode_values(row)\n        id_ = self._id_table.read(row)\n    return data, BufferInfo(ids=id_)\n\n  def _as_dataset(self,\n                  sample_batch_size=None,\n                  num_steps=None,\n                  sequence_preprocess_fn=None,\n                  num_parallel_calls=tf.data.experimental.AUTOTUNE):\n    """"""Creates a dataset that returns episodes entries from the buffer.\n\n    The dataset behaves differently depending on if `num_steps` is provided or\n    not.  If `num_steps = None`, then entire episodes are sampled uniformly at\n    random from the buffer.  If `num_steps != None`, then we attempt to sample\n    uniformly across frames of all the episodes, and return subsets of length\n    `num_steps`.  The algorithm for this is roughly:\n\n    1. Sample an episode with a probability proportional to its length.\n    2. If the length of the episode is less than `num_steps`, drop it.\n    3. Sample a starting location `start` in `[0, len(episode) - num_steps]`\n    4. Take a slice `[start, start + num_steps]`.\n\n    The larger `num_steps` is, the higher the likelihood of edge effects (e.g.,\n    certain frames not being visited often because they are near the start\n    or end of an episode).  In the worst case, if `num_steps` is greater than\n    most episode lengths, those episodes will never be visited.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. See as_dataset() documentation.\n      num_steps: (Optional.) Scalar int.  How many contiguous frames to get\n        per entry. Default is `None`: return full-length episodes.\n      sequence_preprocess_fn: (Optional.) Preprocessing function for sequences\n        before they are sharded into subsequences of length `num_steps` and\n        batched.\n      num_parallel_calls: Number of parallel calls to use in the\n        dataset pipeline when extracting episodes.  Default is to have\n        tensorflow determine the optimal number of calls.\n\n    Returns:\n      A dataset of type tf.data.Dataset, elements of which are 2-tuples of:\n\n        - An item or sequence of items sampled uniformly from the buffer.\n        - BufferInfo NamedTuple, containing the episode id.\n\n    Raises:\n      ValueError: If the data spec contains lists that must be converted to\n        tuples.\n      NotImplementedError: If `sequence_preprocess_fn != None` is passed in.\n    """"""\n    if sequence_preprocess_fn is not None:\n      raise NotImplementedError(\'sequence_preprocess_fn is not supported.\')\n\n    # data_tf.nest.flatten does not flatten python lists, tf.nest.flatten does.\n    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n      raise ValueError(\n          \'Cannot perform gather; data spec contains lists and this conflicts \'\n          \'with gathering operator.  Convert any lists to tuples.  \'\n          \'For example, if your spec looks like [a, b, c], \'\n          \'change it to (a, b, c).  Spec structure is:\\n  {}\'.format(\n              tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec)))\n\n    seed_per_episode = distributions_util.gen_new_seed(\n        self._seed,\n        salt=\'per_episode\')\n\n    episode_id_buffer_size = self._buffer_size * (sample_batch_size or 1)\n\n    def _get_episode_locations(_):\n      """"""Sample episode ids according to value of num_steps.""""""\n      if num_steps is None:\n        # Just want to get a uniform sampling of episodes.\n        episode_ids = self._sample_episode_ids(\n            shape=[episode_id_buffer_size], seed=self._seed)\n      else:\n        # Want to try to sample uniformly from frames, which means\n        # sampling episodes by length.\n        episode_ids = self._sample_episode_ids(\n            shape=[episode_id_buffer_size],\n            weigh_by_episode_length=True,\n            seed=self._seed)\n      episode_locations = self._get_episode_id_location(episode_ids)\n\n      if self._completed_only:\n        return tf.boolean_mask(\n            tensor=episode_locations,\n            mask=self._episode_completed.sparse_read(episode_locations))\n      else:\n        return episode_locations\n\n    ds = tf.data.experimental.Counter().map(_get_episode_locations).unbatch()\n\n    if num_steps is None:\n      @tf.autograph.experimental.do_not_convert\n      def _read_data_and_id(row):\n        return (\n            self._data_table.get_episode_values(row),\n            self._id_table.read(row))\n      ds = ds.map(_read_data_and_id, num_parallel_calls=num_parallel_calls)\n    else:\n      @tf.autograph.experimental.do_not_convert\n      def _read_tensor_list_and_id(row):\n        """"""Read the TensorLists out of the table row, get id and num_frames.""""""\n        # Return a flattened tensor list\n        flat_tensor_lists = tuple(\n            tf.nest.flatten(self._data_table.get_episode_lists(row)))\n        # Due to race conditions, not all entries may have been written for the\n        # given episode.  Use the minimum list length to identify the full valid\n        # available length.\n        num_frames = tf.reduce_min(\n            [list_ops.tensor_list_length(l) for l in flat_tensor_lists])\n        return flat_tensor_lists, self._id_table.read(row), num_frames\n\n      ds = ds.map(\n          _read_tensor_list_and_id, num_parallel_calls=num_parallel_calls)\n\n      def _filter_by_length(unused_1, unused_2, num_frames):\n        # Remove episodes that are too short.\n        return num_frames >= num_steps\n\n      ds = ds.filter(_filter_by_length)\n\n      @tf.autograph.experimental.do_not_convert\n      def _random_slice(flat_tensor_lists, id_, num_frames):\n        """"""Take a random slice from the episode, of length num_steps.""""""\n        # Sample uniformly between [0, num_frames - num_steps]\n        start_slice = tf.random.uniform((),\n                                        minval=0,\n                                        maxval=num_frames - num_steps + 1,\n                                        dtype=tf.int32,\n                                        seed=seed_per_episode)\n        end_slice = start_slice + num_steps\n\n        flat_spec = tf.nest.flatten(self._data_spec)\n\n        # Pull out frames in [start_slice, start_slice + num_steps]\n        flat = tuple(\n            list_ops.tensor_list_gather(  # pylint: disable=g-complex-comprehension\n                t, indices=tf.range(start_slice, end_slice),\n                element_dtype=spec.dtype, element_shape=spec.shape)\n            for t, spec in zip(flat_tensor_lists, flat_spec))\n        return flat, id_\n\n      ds = ds.map(_random_slice, num_parallel_calls=num_parallel_calls)\n\n      def set_shape_and_restore_structure(flat_data, id_):\n        def restore_shape(t_sliced):\n          if t_sliced.shape.rank is not None:\n            t_sliced.set_shape([num_steps] + [None] * (t_sliced.shape.rank - 1))\n            return t_sliced\n        shaped_flat = [restore_shape(x) for x in flat_data]\n        return tf.nest.pack_sequence_as(self._data_spec, shaped_flat), id_\n\n      ds = ds.map(set_shape_and_restore_structure)\n\n    if sample_batch_size:\n      if num_steps is None:\n        raise ValueError(""""""`num_steps` must be set if `sample_batch_size` is\n                         set in EpisodicReplayBuffer as_dataset."""""")\n      # We set drop_remainder on this batch since the dataset never ends,\n      # therefore setting this will not cause any lost data and allows the\n      # output tensors to have a definite leading dimension of\n      # `sample_batch_size`.\n      ds = ds.batch(sample_batch_size, drop_remainder=True)\n\n    return ds\n\n  def _single_deterministic_pass_dataset(\n      self,\n      sample_batch_size=None,\n      num_steps=None,\n      sequence_preprocess_fn=None,\n      num_parallel_calls=tf.data.experimental.AUTOTUNE):\n    """"""Creates a dataset that returns entries from the buffer in fixed order.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. See as_dataset() documentation.\n        **NOTE** This argument may only be provided when\n        `num_steps is not None`.  Otherwise the episodes may be different\n        lengths and cannot be batched.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. See as_dataset() documentation.  Required if\n        `sample_batch_size` is provided.\n      sequence_preprocess_fn: (Optional.) Preprocessing function for sequences\n        before they are sharded into subsequences of length `num_steps` and\n        batched.\n      num_parallel_calls: (Optional.) Number elements to process in parallel.\n        See as_dataset() documentation.  Note, that the parallelism here is\n        not ""sloppy"", in that setting this value does not affect the order\n        in which frames are returned.\n\n    Returns:\n      A dataset of type tf.data.Dataset, elements of which are 2-tuples of:\n\n        - An item or sequence of items or batch thereof\n        - Auxiliary info for the items (i.e. ids, probs).\n\n    Raises:\n      ValueError: If `sample_batch_size is not None` but `num_steps is None`.\n        When `num_steps is None`, the episodes returned may have different\n        lengths, and there is no unique way to batch them.\n      NotImplementedError: If `sequence_preprocess_fn != None` is passed in.\n    """"""\n    if sequence_preprocess_fn is not None:\n      raise NotImplementedError(\'sequence_preprocess_fn is not supported.\')\n    if sample_batch_size is not None and num_steps is None:\n      raise ValueError(\n          \'When requesting a batched dataset from EpisodicReplayBuffer, \'\n          \'num_steps must be provided (but saw num_steps=None).\')\n\n    drop_remainder = self._dataset_drop_remainder\n    window_shift = self._dataset_window_shift\n\n    def get_episode_ids(_):\n      min_frame_offset, max_frame_offset = _valid_range_ids(\n          self._get_last_episode_id(), self._capacity)\n      return tf.data.Dataset.range(min_frame_offset, max_frame_offset)\n\n    # Instead of calling get_episode_ids and creating a dataset from this,\n    # we instead build a dataset whose iterator recalculates the available\n    # episode_ids in the dataset whenever it is reinitialized.  We\n    # want to do this because the RB valid episodes can change over time;\n    # specifically the RB may be empty when this dataset is first created.\n    episode_ids_ds = tf.data.Dataset.range(1).flat_map(get_episode_ids)\n\n    def read_episode(episode_id):\n      row = self._get_episode_id_location(episode_id)\n      return self._data_table.get_episode_values(row)\n\n    ds = (episode_ids_ds\n          .map(read_episode, num_parallel_calls=num_parallel_calls))\n    if sample_batch_size is None:\n      if num_steps is not None:\n        # Disable autograph to make debugging errors easier.\n        @tf.autograph.experimental.do_not_convert\n        def group_windows(windowed):\n          return tf.data.Dataset.zip(\n              tf.nest.map_structure(\n                  lambda d: d.batch(num_steps, drop_remainder=drop_remainder),\n                  windowed))\n        ds = (ds.unbatch()\n              .window(num_steps, shift=window_shift)\n              .flat_map(group_windows))\n    else:\n      # sample_batch_size is not None, which also implies num_steps is not None\n      # per the check at the top of this function.\n      assert num_steps is not None\n\n      # Split up the replay buffer into sample_batch_size parallel datasets.\n      ds_shards = (tf.data.Dataset.range(sample_batch_size)\n                   .map(lambda i: ds.shard(sample_batch_size, i)))\n      # In each dataset, convert different-length episodes to blocks of size\n      # num_steps.  The very final blocks may be dropped if their size is not a\n      # multiple of num_steps.\n      # Disable autograph to make debugging errors easier.\n      @tf.autograph.experimental.do_not_convert\n      def rebatch(ds_):\n        def batch_nest(window):\n          return tf.data.Dataset.zip(\n              tf.nest.map_structure(\n                  lambda d: d.batch(num_steps, drop_remainder=True),\n                  window))\n        return (ds_\n                .unbatch()\n                .window(num_steps, shift=window_shift)\n                .flat_map(batch_nest))\n      ds_shards = ds_shards.map(rebatch)\n      ds = ds_shards.interleave(lambda ds_: ds_)\n      # Batch by sample_batch_size from the interleaved stream.\n      ds = ds.batch(sample_batch_size, drop_remainder=drop_remainder)\n\n    return ds\n\n  def _gather_all(self):\n    """"""Returns all the items currently in the buffer.\n\n    Returns:\n      A tuple containing two entries:\n        - All the items currently in the buffer (nested).\n        - The items ids.\n\n    Raises:\n      ValueError: If the data spec contains lists that must be converted to\n        tuples.\n    """"""\n    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n      raise ValueError(\n          \'Cannot perform gather; data spec contains lists and this conflicts \'\n          \'with gathering operator.  Convert any lists to tuples.  \'\n          \'For example, if your spec looks like [a, b, c], \'\n          \'change it to (a, b, c).  Spec structure is:\\n  %s\' %\n          tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec))\n\n    min_val, max_val = _valid_range_ids(self._get_last_episode_id(),\n                                        self._capacity)\n\n    def get_episode_and_id(id_):\n      row = self._get_episode_id_location(id_)\n      data = self._data_table.get_episode_values(row)\n      n = tf.shape(tf.nest.flatten(data)[0])[0]\n      id_repeated = tf.fill([n], id_)\n      return (tuple(tf.nest.flatten(data)), id_repeated)\n\n    episode_lengths = self._episode_lengths.read_value()\n    if self._completed_only:\n      episode_lengths *= tf.cast(self._episode_completed, dtype=tf.int64)\n    total_length = tf.reduce_sum(input_tensor=episode_lengths)\n\n    def via_iterator():\n      """"""If total_length > 0, create a dataset iterator to concat episodes.""""""\n      valid_episodes = tf.range(min_val, max_val)\n      ds = tf.data.Dataset.from_tensor_slices(valid_episodes)\n\n      if self._completed_only:\n        # Filter out incomplete episodes.\n        def check_completed(id_):\n          return tf.cast(self._episode_completed.sparse_read(id_), tf.bool)\n        ds = ds.filter(check_completed)\n\n      def _unflatten(flat_data, id_):\n        return tf.nest.pack_sequence_as(self._data_spec, flat_data), id_\n\n      ds = (\n          ds.map(get_episode_and_id).unbatch()\n          # Batch all the frames in the buffer.  Request a larger amount in\n          # case the buffer grows between the construction of total_length and\n          # the call to .map().\n          .batch(10 + 2 * total_length).map(_unflatten)).batch(1)\n\n      # Use ds.take(1) in case we haven\'t requested a large enough batch (the\n      # replay buffer has grown too quickly), since get_single_element requires\n      # that the dataset only has a single entry; and we don\'t consider this\n      # case to be an error.\n      return tf.data.experimental.get_single_element(ds.take(1))\n\n    def empty():\n\n      def _empty_from_spec(spec):\n        return tf.zeros([0] + spec.shape.as_list(), spec.dtype, name=\'empty\')\n\n      empty_data = tf.nest.map_structure(_empty_from_spec, self._data_spec)\n      empty_id = tf.zeros([], dtype=tf.int64)\n      return empty_data, empty_id\n\n    return tf.cond(pred=total_length > 0, true_fn=via_iterator, false_fn=empty)\n\n  def _clear(self, clear_all_variables=False):\n    """"""Clears the replay buffer.\n\n    Args:\n      clear_all_variables: Boolean to indicate whether to clear all variables or\n      just the data table and episode lengths (i.e. keep the current episode ids\n      that are in flight in the buffer).\n    Returns:\n      An op to clear the buffer.\n    """"""\n    assignments = [\n        self._episode_lengths.assign(tf.zeros_like(self._episode_lengths))]\n    assignments += [self._num_writes.assign(tf.zeros_like(self._num_writes))]\n\n    if clear_all_variables:\n      zero_vars = self._id_table.variables() + [self._episode_completed]\n      assignments += [var.assign(tf.zeros_like(var)) for var in zero_vars]\n      neg_one_vars = [self._episodes_loc_to_id_map, self._last_episode]\n      assignments += [var.assign(_INVALID_EPISODE_ID * tf.ones_like(var))\n                      for var in neg_one_vars]\n\n    return tf.group(self._data_table.clear(), assignments, name=\'clear\')\n\n  # Other private methods.\n\n  def _completed_episodes(self):\n    """"""Get a list of completed episode ids in the replay buffer.\n\n    Returns:\n      An int64 vector of length at most `capacity`.\n    """"""\n    def _completed_episodes():\n      completed_mask = tf.equal(self._episode_completed, 1)\n      return tf.boolean_mask(\n          tensor=self._episodes_loc_to_id_map, mask=completed_mask)\n    return self._add_episode_critical_section.execute(_completed_episodes)\n\n  def _get_episode(self, episode_id):\n    """"""Gets the current steps of the episode_id.\n\n    Args:\n      episode_id: A Tensor with the episode_id.\n\n    Returns:\n      A nested tuple/list of Tensors with all the items/steps of the episode.\n      Each Tensor has shape `(episode_length,) + TensorSpec.shape`.\n\n    Raises:\n      InvalidArgumentException: (at runtime) if episode_id is not valid.\n    """"""\n    with tf.device(self._device), tf.name_scope(\'get_episode\'):\n      episode_id = tf.convert_to_tensor(\n          episode_id, dtype=tf.int64, name=\'episode_id\')\n      episode_location = self._get_episode_id_location(episode_id)\n      # Accessing episode_id may modify _episodes_loc_to_id_map upstream, so\n      # ensure that we\'ve performed that modification *first*.\n      with tf.control_dependencies([episode_id]):\n        episode_at_location = self._episodes_loc_to_id_map[episode_location]\n      episode_valid = tf.equal(episode_at_location, episode_id)\n      assert_valid = tf.Assert(episode_valid, [\n          \'Episode id\', episode_id, \'is not valid.  It points to location\',\n          episode_location, \'but the episode at that location is currently id\',\n          episode_at_location\n      ])\n      with tf.control_dependencies([assert_valid, episode_location]):\n        return self._data_table.get_episode_values(episode_location)\n\n  def _maybe_end_episode(self, episode_id, end_episode=False):\n    """"""Mark episode ID as complete when end_episode is True.\n\n    Args:\n      episode_id: A Tensor containing the current episode_id.\n      end_episode: A Boolean Tensor whether should end the episode_id.\n\n    Returns:\n      A Boolean Tensor whether the episode was marked as complete.\n    """"""\n    episode_location = self._get_episode_id_location(episode_id)\n\n    def _maybe_end_episode_id():\n      """"""Maybe end episode ID.""""""\n      def _end_episode_id():\n        return tf.group(\n            tf.compat.v1.scatter_update(self._episode_completed,\n                                        [episode_location], 1))\n\n      episode_valid = tf.equal(\n          self._episodes_loc_to_id_map.sparse_read(episode_location),\n          episode_id)\n\n      pred_value = end_episode & (episode_id >= 0) & episode_valid\n      if pred_value.shape.rank != 0:\n        raise ValueError(\'Invalid condition shape: {} (should be scalar).\'\n                         .format(pred_value.shape))\n      maybe_end = tf.cond(\n          pred=pred_value,\n          true_fn=_end_episode_id,\n          false_fn=tf.no_op,\n          name=\'maybe_end_episode_id\')\n      with tf.control_dependencies([maybe_end]):\n        return self._episode_completed.sparse_read(episode_location) > 0\n\n    return self._add_episode_critical_section.execute(_maybe_end_episode_id)\n\n  def _maybe_end_batch_episodes(self, batch_episode_ids, end_episode=False):\n    """"""Mark episode ID as complete when end_episode is True.\n\n    Args:\n      batch_episode_ids: A Tensor int64 with a batch of episode_ids\n        with shape `(batch_size,)`.\n      end_episode: A Boolean Tensor whether should end all batch_episode_ids,\n        or Tensor with shape `(batch_size,)` to mark which ones to end.\n    Returns:\n      A `bool` Tensor `(batch_size,)` with the episode_ids marked as complete.\n    """"""\n    episodes_location = self._get_episode_id_location(batch_episode_ids)\n\n    def _execute():\n      """"""Maybe end episode ID.""""""\n      valid_episodes = tf.equal(\n          batch_episode_ids,\n          self._episodes_loc_to_id_map.sparse_read(episodes_location))\n      maybe_end_mask = end_episode & (batch_episode_ids >= 0) & valid_episodes\n      episodes_location_ = tf.boolean_mask(\n          tensor=episodes_location, mask=maybe_end_mask)\n      update_completed = tf.compat.v1.scatter_update(self._episode_completed,\n                                                     episodes_location_, 1)\n      with tf.control_dependencies([update_completed]):\n        return self._episode_completed.sparse_read(episodes_location) > 0\n\n    return self._add_episode_critical_section.execute(_execute)\n\n  def _get_episode_id(self,\n                      episode_id,\n                      begin_episode=False,\n                      end_episode=False):\n    """"""Increments the episode_id when begin_episode is True.\n\n    Args:\n      episode_id: A Tensor containing the current episode_id.\n      begin_episode: A Boolean Tensor whether should increment the episode_id.\n      end_episode: A Boolean Tensor whether should end the episode_id.\n\n    Returns:\n      An updated episode id value.  Accessing this episode id value will,\n      as a side effect, start or end the current episode in the var.\n    """"""\n    def _assign_new_episode_id():\n      """"""Increment the episode_id inside a critical section.""""""\n      new_episode_id = self._last_episode.assign_add(1)\n      episode_location = self._get_episode_id_location(new_episode_id)\n      update_mapping = tf.compat.v1.scatter_update(\n          self._episodes_loc_to_id_map, episode_location, new_episode_id)\n      update_completed = tf.compat.v1.scatter_update(self._episode_completed,\n                                                     episode_location, 0)\n      reset_data = self._data_table.clear_rows(\n          tf.expand_dims(episode_location, 0))\n      reset_length = tf.compat.v1.scatter_update(self._episode_lengths,\n                                                 episode_location, 0)\n      with tf.control_dependencies([\n          update_mapping, update_completed, reset_data, reset_length]):\n        return tf.identity(new_episode_id)\n\n    def _get_new_episode_id():\n      return self._add_episode_critical_section.execute(_assign_new_episode_id)\n\n    begin_episode = tf.convert_to_tensor(\n        value=begin_episode, name=\'begin_episode\')\n    end_episode = tf.convert_to_tensor(value=end_episode, name=\'end_episode\')\n    # If episode_id value is still -1 we need to assign a proper value.\n    pred_value = begin_episode | (episode_id < 0)\n    if pred_value.shape.rank != 0:\n      raise ValueError(\'Invalid condition predicate shape: {} \'\n                       \'(should be scalar).\'\n                       .format(pred_value.shape))\n    updated_episode_id = tf.cond(\n        pred=pred_value,\n        true_fn=_get_new_episode_id,\n        false_fn=lambda: tf.identity(episode_id),\n        name=\'get_episode_id\')\n\n    # _maybe_end_episode acquires the critical section.\n    mark_completed = self._maybe_end_episode(updated_episode_id, end_episode)\n    with tf.control_dependencies([mark_completed]):\n      updated_episode_id = tf.identity(updated_episode_id)\n    return updated_episode_id\n\n  def _get_batch_episode_ids(self,\n                             batch_episode_ids,\n                             begin_episode=False,\n                             end_episode=False,\n                             mask=None):\n    """"""Increments the episode_id of the elements that have begin_episode True.\n\n    Mark as completed those that have end_episode True.\n\n    Args:\n      batch_episode_ids: A tf.int64 tensor with shape `(num_episodes,)`\n        containing a one or more episodes IDs.\n      begin_episode: A Boolean Tensor whether should increment each episode ID.\n         It can be a scalar or a vector with the same dimensions of\n         batch_episode_ids.\n      end_episode: A Boolean Tensor whether should end each episode ID.\n         It can be a scalar or a vector with the same dimensions of\n         batch_episode_ids.\n      mask: An optional Boolean Tensor to select which IDs are updated.\n    Returns:\n      An Tensor shaped `(num_episodes,)` with the updated episode IDs.\n    Raises:\n      ValueError: If the shape of `begin_episode` is not compatible with\n        `batch_episode_ids`.\n\n    """"""\n    if batch_episode_ids.shape.rank != 1:\n      raise ValueError(\n          \'batch_episode_ids must be a vector with 1 dimension\')\n    # If batch_episode_ids value is still -1 we need to assign a\n    # proper value.  Find which IDs need to be updated.\n    begin_episode = tf.convert_to_tensor(value=begin_episode)\n    end_episode = tf.convert_to_tensor(value=end_episode)\n    ids_to_update_mask = ((batch_episode_ids < 0) | begin_episode)\n    if mask is not None:\n      ids_to_update_mask &= mask\n\n    def _update_batch_episode_ids():\n      """"""Increment the episode_id inside a critical section.""""""\n      num_ids = tf.reduce_sum(\n          input_tensor=tf.cast(ids_to_update_mask, tf.int64))\n      end_id = self._last_episode.assign_add(num_ids).value() + 1\n      start_id = end_id - num_ids\n      new_batch_episode_ids = tf.range(start_id, end_id)\n      # Update when b/74385543 is fixed.\n      ids_to_update = tf.compat.v1.where(ids_to_update_mask)\n      scattered_updated_episode_ids = tf.scatter_nd(\n          ids_to_update, new_batch_episode_ids,\n          shape=tf.shape(batch_episode_ids, out_type=tf.int64))\n      updated_batch_episode_ids = tf.compat.v1.where(\n          ids_to_update_mask,\n          scattered_updated_episode_ids,\n          batch_episode_ids)\n      episode_locations = tf.math.mod(new_batch_episode_ids, self._capacity)\n      update_mapping = tf.compat.v1.scatter_update(self._episodes_loc_to_id_map,\n                                                   [episode_locations],\n                                                   [new_batch_episode_ids])\n      reset_completed = tf.compat.v1.scatter_update(self._episode_completed,\n                                                    [episode_locations], 0)\n      reset_data = self._data_table.clear_rows(episode_locations)\n      reset_length = tf.compat.v1.scatter_update(self._episode_lengths,\n                                                 episode_locations, 0)\n      with tf.control_dependencies([\n          update_mapping, reset_completed, reset_data, reset_length]):\n        return tf.identity(updated_batch_episode_ids)\n\n    episode_ids = self._add_episode_critical_section.execute(\n        _update_batch_episode_ids)\n\n    # _maybe_end_batch_episodes acquires the critical section.\n    mark_completed = self._maybe_end_batch_episodes(episode_ids, end_episode)\n\n    with tf.control_dependencies([mark_completed]):\n      episode_ids = tf.identity(episode_ids)\n    return episode_ids\n\n  def _get_episode_id_location(self, episode_id):\n    return tf.math.mod(episode_id, self._capacity)\n\n  def _sample_episode_ids(self, shape, weigh_by_episode_length=False,\n                          seed=None):\n    """"""Samples episode ids from the replay buffer.""""""\n    last_id = self._get_last_episode_id()\n    assert_nonempty = tf.compat.v1.assert_non_negative(\n        last_id,\n        message=\'EpisodicReplayBuffer is empty. Make sure to add items \'\n        \'before sampling the buffer.\')\n    if weigh_by_episode_length:\n      # Sample episodes proportional to length.\n      with tf.control_dependencies([assert_nonempty]):\n        num_episodes = tf.minimum(self._last_episode + 1, self._capacity)\n        episode_lengths = self._episode_lengths[:num_episodes]\n        logits = tf.math.log(tf.cast(episode_lengths, tf.float32))\n        return tf.reshape(\n            tf.random.categorical(\n                [logits],  # shape is: [1, num_episodes]\n                num_samples=tf.reduce_prod(shape),\n                seed=seed,\n                dtype=tf.int64),\n            shape)\n    else:\n      min_val, max_val = _valid_range_ids(self._get_last_episode_id(),\n                                          self._capacity)\n      with tf.control_dependencies([assert_nonempty]):\n        return tf.random.uniform(\n            shape, minval=min_val, maxval=max_val, dtype=tf.int64, seed=seed)\n\n  def _get_last_episode_id(self):\n    def last_episode():\n      return self._last_episode.value()\n\n    return self._add_episode_critical_section.execute(last_episode)\n\n  def _increment_episode_length_locked(self, episode_id, increment=1):\n    """"""Increments the length of episode_id in a thread safe manner.\n\n    NOTE: This method should only be called inside a critical section.\n\n    Args:\n      episode_id: int64 scalar of vector. ID(s) of the episode(s) for which we\n        will increase the length.\n      increment: Amount to increment episode_length by.\n    Returns:\n      An op that increments the last_id.\n    Raises:\n      ValueError: If `len(episode_id.shape) > 1`.\n    """"""\n    episode_location = self._get_episode_id_location(episode_id)\n\n    def _assign_add():\n      new_length = self._episode_lengths[episode_location] + increment\n      update_length = tf.compat.v1.scatter_update(\n          self._episode_lengths, [episode_location], new_length)\n      with tf.control_dependencies([update_length]):\n        return tf.identity(new_length)\n\n    def _assign_add_multiple():\n      new_length = tf.gather(self._episode_lengths, episode_location)\n      new_length += increment\n      update_length = tf.compat.v1.scatter_update(self._episode_lengths,\n                                                  episode_location, new_length)\n      with tf.control_dependencies([update_length]):\n        return tf.identity(new_length)\n\n    if episode_location.shape.rank == 0:\n      return _assign_add()\n    elif episode_location.shape.rank == 1:\n      return _assign_add_multiple()\n    else:\n      raise ValueError(\'episode_id must have rank <= 1\')\n\n  def _get_valid_ids_mask_locked(self, episode_ids):\n    """"""Returns a mask of whether the given IDs are valid. Caller must lock.""""""\n    episode_locations = self._get_episode_id_location(episode_ids)\n    location_matches_id = tf.equal(\n        episode_ids,\n        self._episodes_loc_to_id_map.sparse_read(episode_locations))\n    # Note that the above map is initialized with -1s.\n    return (episode_ids >= 0) & location_matches_id\n\n  def get_valid_ids_mask(self, episode_ids):\n    """"""Returns a mask of whether the given IDs are valid.""""""\n    with tf.device(self._device):\n      with tf.name_scope(\'get_valid_ids_mask\'):\n        return self._add_episode_critical_section.execute(\n            lambda: self._get_valid_ids_mask_locked(episode_ids))\n\n  def extract(self, locations, clear_data=False):\n    """"""Extracts Episodes with the given IDs.\n\n    Args:\n      locations: A `1-D` Tensor of locations to extract from (note, this is\n        NOT the same as an episode ids variable).  It\'s up to the user to\n        ensure that only valid episode locations are requested (i.e.,\n        values should be between `0` and `self.capacity`).\n        Passing locations that are out of bounds will lead to runtime errors.\n      clear_data: If `True`, clears the extracted data from this buffer.\n\n    Returns:\n      episodes: An Episodes object with an outer dimension of the same size as\n        locations.\n    """"""\n    locations = tf.cast(locations, dtype=tf.int64, name=\'locations\')\n    locations.shape.assert_has_rank(1)\n\n    def _extract_locked():\n      """"""Does the above within the buffer\'s critical section.""""""\n      episodes = Episodes(\n          length=self._episode_lengths.sparse_read(locations),\n          completed=self._episode_completed.sparse_read(locations),\n          tensor_lists=self._data_table.get_episode_lists(locations))\n      if clear_data:\n        with tf.control_dependencies(tf.nest.flatten(episodes)):\n          clear_rows = self._data_table.clear_rows(locations)\n          clear_lengths = tf.compat.v1.scatter_update(self._episode_lengths,\n                                                      locations, 0)\n          clear_completed = tf.compat.v1.scatter_update(self._episode_completed,\n                                                        locations, 0)\n        with tf.control_dependencies(\n            [clear_rows, clear_lengths, clear_completed]):\n          episodes = tf.nest.map_structure(tf.identity, episodes)\n      return episodes\n\n    with tf.device(self._device):\n      with tf.name_scope(\'extract\'):\n        return self._add_episode_critical_section.execute(_extract_locked)\n\n  def extend_episodes(self,\n                      episode_ids,\n                      episode_ids_indices,\n                      episodes):\n    """"""Extends a batch of episodes in this buffer.\n\n    Args:\n      episode_ids: A int64 vector containing the ids of the\n        episodes the items are being added to.  Shaped `(max_num_episodes,)`.\n      episode_ids_indices: An int64 vector containing the locations in\n        `episode_ids` that are being extended.  Shaped `(num_episodes,)`,\n        where `num_episodes <= max_num_episodes`.  Rows in `episodes`\n        correspond to locations in `episode_ids_indices`.\n      episodes: An `Episodes` tuple containing the extension data. Tensors must\n        have outer dimension of size `num_episodes`.\n\n    Returns:\n      A `Tensor` containing the updated episode ids.  Accessing or executing\n      this tensor also extends episodes in the replay buffer.\n    """"""\n    episode_ids.shape.assert_has_rank(1)\n    episode_ids_indices = tf.convert_to_tensor(\n        value=episode_ids_indices, name=\'episode_ids_indices\')\n    episode_ids_indices.shape.assert_has_rank(1)\n\n    def _extend_locked(episode_ids, expanded_episode_ids):\n      """"""Does the above within the buffer\'s critical section.""""""\n      episode_locations = self._get_episode_id_location(episode_ids)\n      episode_valid = tf.equal(\n          self._episodes_loc_to_id_map.sparse_read(episode_locations),\n          episode_ids)\n      episode_valid_idx = tf.reshape(tf.compat.v1.where(episode_valid), [-1])\n      episode_locations = tf.gather(episode_locations, episode_valid_idx)\n      increment_lengths = self._increment_episode_length_locked(\n          episode_locations,\n          tf.gather(episodes.length, episode_valid_idx))\n      set_completed = tf.compat.v1.scatter_update(\n          self._episode_completed, episode_locations,\n          tf.gather(episodes.completed, episode_valid_idx))\n      extend = self._data_table.extend(\n          episode_locations,\n          tf.nest.map_structure(lambda tl: tf.gather(tl, episode_valid_idx),\n                                episodes.tensor_lists))\n      with tf.control_dependencies([increment_lengths, set_completed, extend]):\n        return tf.identity(expanded_episode_ids)\n\n    with tf.device(self._device):\n      with tf.name_scope(\'extend\'):\n        episode_ids_indices_shape = tf.shape(episode_ids_indices)\n        begin_episode = self._begin_episode_fn(episodes)\n        begin_episode = tf.broadcast_to(\n            begin_episode, episode_ids_indices_shape, name=\'begin_episode\')\n        column_indices = tf.reshape(episode_ids_indices, [-1, 1])\n        episode_ids_shape = tf.shape(input=episode_ids)\n        # We expand the tensors below from size `num_episodes` (the size of\n        # episode_ids_indices) to tensors of  size `max_num_episodes` (the size\n        # of episode_ids).\n        expanded_begin_episode = tf.scatter_nd(column_indices, begin_episode,\n                                               episode_ids_shape)\n        expanded_mask = tf.scatter_nd(column_indices,\n                                      tf.fill(episode_ids_indices_shape, True),\n                                      episode_ids_shape)\n        expanded_episode_ids = self._get_batch_episode_ids(\n            episode_ids,\n            begin_episode=expanded_begin_episode,\n            mask=expanded_mask)\n        episode_ids = tf.gather(expanded_episode_ids, episode_ids_indices)\n        return self._add_episode_critical_section.execute(\n            lambda: _extend_locked(episode_ids, expanded_episode_ids))\n\n\nclass StatefulEpisodicReplayBuffer(replay_buffer_base.ReplayBuffer):\n  """"""Wrapper enabling use of `EpisodicReplayBuffer` with a `Driver`.\n\n  This wrapper keeps track of episode ids in a `tf.Variable`.\n\n  Use:\n  ```python\n  tf_env = ...\n  rb = EpisodicReplayBuffer(...)\n  stateful_rb = StatefulEpisodicReplayBuffer(rb, num_episodes=tf_env.batch_size)\n  driver = DynamicEpisodeDriver(\n      tf_env, policy, ...,\n      observers=[\n        lambda traj: stateful_rb.add_batch(traj),\n      ])\n  driver.run()\n  ```\n  """"""\n\n  def __init__(self, replay_buffer, num_episodes=None):\n    """"""Create a `StatefulEpisodicReplayBuffer` for `num_episodes` batches.\n\n    Args:\n      replay_buffer: An instance of `EpisodicReplayBuffer`.\n      num_episodes: (Optional) integer, number of episode IDs to create.\n        If `None`, a scalar ID variable is returned.\n\n    Raises:\n      TypeError: If `replay_buffer` is not an `EpisodicReplayBuffer`.\n    """"""\n    super(StatefulEpisodicReplayBuffer, self).__init__(\n        replay_buffer.data_spec, replay_buffer.capacity)\n\n    if not isinstance(replay_buffer, EpisodicReplayBuffer):\n      raise TypeError(\n          \'Expected an EpisodicReplayBuffer, saw {}\'.format(replay_buffer))\n    shape = ()\n    if num_episodes and num_episodes > 0:\n      if num_episodes > replay_buffer.capacity:\n        raise ValueError(\'Buffer cannot create episode_ids when \'\n                         \'num_episodes {} > capacity {}.\'.format(\n                             num_episodes, replay_buffer.capacity))\n      shape = (num_episodes,)\n    self._replay_buffer = replay_buffer\n    self._episode_ids_var = common.create_variable(\n        \'episode_id\', initial_value=_INVALID_EPISODE_ID,\n        shape=shape, use_local_variable=True)\n\n  @property\n  def episode_ids(self):\n    """"""Returns the `tf.Variable` tracking the episode ids.""""""\n    return self._episode_ids_var\n\n  @common.function_in_tf1()\n  def add_batch(self, items):\n    """"""Adds a batch of single steps for the corresponding episodes IDs.\n\n    Args:\n      items: A batch of items to be added to the buffer. Items will have the\n        same structure as the data_spec of this class, but the tensors in items\n        will have an extra outer dimension `(num_episodes, ...)` in addition to\n        the corresponding spec in data_spec.\n\n    Returns:\n      A `Tensor` containing the updated episode ids.  Accessing or executing\n      this varaible also adds `items` to the replay buffer.\n    """"""\n    new_episode_ids = self._replay_buffer.add_batch(\n        items=items, episode_ids=self._episode_ids_var)\n    self._episode_ids_var.assign(new_episode_ids)\n    return new_episode_ids\n\n  @common.function_in_tf1()\n  def add_sequence(self, items):\n    """"""Adds a sequence of items to the replay buffer for the selected episode.\n\n    Args:\n      items: A sequence of items to be added to the buffer. Items will have the\n        same structure as the data_spec of this class, but the tensors in items\n        will have an outer sequence dimension in addition to the corresponding\n        spec in data_spec.\n\n    Returns:\n      An updated episode id value.  Accessing this episode id value will,\n      as a side effect, start or end the current episode.\n    """"""\n    new_episode_id = self._replay_buffer.add_sequence(\n        items=items, episode_id=self._episode_ids_var)\n    self._episode_ids_var.assign(new_episode_id)\n    return new_episode_id\n\n  @common.function_in_tf1()\n  def extend_episodes(self,\n                      episode_ids_indices,\n                      episodes):\n    """"""Extends a batch of episodes in this buffer.\n\n    Args:\n      episode_ids_indices: An int64 vector containing the locations in\n        `self.episode_ids` that are being extended.  Shaped `(num_episodes,)`,\n        where `num_episodes <= max_num_episodes`.  Rows in `episodes` and\n        `begin_episode` correspond to locations in `episode_ids_indices`.\n      episodes: An `Episodes` tuple containing the extension data. Tensors must\n        have outer dimension of size `num_episodes`.\n\n    Returns:\n      A `Tensor` containing the updated episode ids.  Accessing or executing\n      this tensor also extends the replay buffer.\n    """"""\n    new_episode_ids = self._replay_buffer.extend_episodes(\n        episode_ids=self._episode_ids_var,\n        episode_ids_indices=episode_ids_indices,\n        episodes=episodes)\n    self._episode_ids_var.assign(new_episode_ids)\n    return new_episode_ids\n\n  def _get_next(\n      self, sample_batch_size=None, num_steps=None, time_stacked=None):\n    return self._replay_buffer.get_next(\n        sample_batch_size, num_steps, time_stacked)\n\n  def _as_dataset(\n      self, sample_batch_size=None, num_steps=None,\n      sequence_preprocess_fn=None, num_parallel_calls=None):\n    return self._replay_buffer.as_dataset(\n        sample_batch_size,\n        num_steps,\n        sequence_preprocess_fn=sequence_preprocess_fn,\n        num_parallel_calls=num_parallel_calls)\n\n\ndef _valid_range_ids(last_id, capacity):\n  """"""Returns the [min_val, max_val) range of ids.\n\n  Args:\n    last_id: A tensor that indicates the last id stored in the replay buffer.\n    capacity: The maximum number of elements that the replay buffer can hold.\n\n  Returns:\n    A tuple (min_id, max_id) for the range [min_id, max_id) of valid ids.\n  """"""\n  min_id_non_full = tf.constant(0, dtype=tf.int64)\n  max_id_non_full = tf.maximum(last_id + 1, 0)\n\n  min_id_full = tf.cast(last_id + 1 - capacity, dtype=tf.int64)\n  max_id_full = tf.cast(last_id + 1, dtype=tf.int64)\n\n  return (\n      tf.where(last_id < capacity, min_id_non_full, min_id_full),\n      tf.where(last_id < capacity, max_id_non_full, max_id_full))\n'"
tf_agents/replay_buffers/episodic_replay_buffer_driver_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for episodic_replay_buffer using driver.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.replay_buffers import episodic_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import test_utils\n\n\nclass EpisodicReplayBufferDriverTest(test_utils.TestCase):\n\n  # Creates a test EpisodicReplayBuffer.\n  def _make_replay_buffer(self, tf_env):\n    """"""Default replay buffer factory.""""""\n\n    time_step_spec = tf_env.time_step_spec()\n    action_spec = tf_env.action_spec()\n    action_step_spec = policy_step.PolicyStep(\n        action_spec, (), tensor_spec.TensorSpec((), tf.int32))\n    trajectory_spec = trajectory.from_transition(time_step_spec,\n                                                 action_step_spec,\n                                                 time_step_spec)\n    return episodic_replay_buffer.EpisodicReplayBuffer(\n        trajectory_spec, end_episode_fn=lambda _: False)\n\n  def testMultiStepEpisodicReplayBuffer(self):\n    num_episodes = 5\n    num_driver_episodes = 5\n\n    # Create mock environment.\n    py_env = batched_py_environment.BatchedPyEnvironment([\n        driver_test_utils.PyEnvironmentMock(final_state=i+1)\n        for i in range(num_episodes)\n    ])\n    env = tf_py_environment.TFPyEnvironment(py_env)\n\n    # Creat mock policy.\n    policy = driver_test_utils.TFPolicyMock(\n        env.time_step_spec(), env.action_spec(), batch_size=num_episodes)\n\n    # Create replay buffer and driver.\n    replay_buffer = self._make_replay_buffer(env)\n    stateful_buffer = episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n        replay_buffer, num_episodes)\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, num_episodes=num_driver_episodes,\n        observers=[stateful_buffer.add_batch])\n\n    run_driver = driver.run()\n\n    end_episodes = replay_buffer._maybe_end_batch_episodes(\n        stateful_buffer.episode_ids, end_episode=True)\n\n    completed_episodes = replay_buffer._completed_episodes()\n\n    self.evaluate([\n        tf.compat.v1.local_variables_initializer(),\n        tf.compat.v1.global_variables_initializer()\n    ])\n\n    self.evaluate(run_driver)\n\n    self.evaluate(end_episodes)\n    completed_episodes = self.evaluate(completed_episodes)\n    eps = [replay_buffer._get_episode(ep) for ep in completed_episodes]\n    eps = self.evaluate(eps)\n\n    episodes_length = [tf.nest.flatten(ep)[0].shape[0] for ep in eps]\n\n    # Compare with expected output.\n    self.assertAllEqual(completed_episodes, [3, 4, 5, 6, 7])\n    self.assertAllEqual(episodes_length, [4, 4, 2, 1, 1])\n\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n\n    step_types = [ep.step_type for ep in eps]\n    observations = [ep.observation for ep in eps]\n    rewards = [ep.reward for ep in eps]\n    actions = [ep.action for ep in eps]\n\n    self.assertAllClose([[first, mid, mid, last], [first, mid, mid, mid],\n                         [first, last], [first], [first]], step_types)\n\n    self.assertAllClose([\n        [0, 1, 3, 4],\n        [0, 1, 3, 4],\n        [0, 1],\n        [0],\n        [0],\n    ], observations)\n\n    self.assertAllClose([\n        [1, 2, 1, 2],\n        [1, 2, 1, 2],\n        [1, 2],\n        [1],\n        [1],\n    ], actions)\n\n    self.assertAllClose([\n        [1, 1, 1, 0],\n        [1, 1, 1, 1],\n        [1, 0],\n        [1],\n        [1],\n    ], rewards)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/episodic_replay_buffer_test.py,228,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for episodic_replay_buffer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import episodic_replay_buffer\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.ops import list_ops  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\n\n# Shorthand for converting arrays to np.arrays.\n_a = np.asarray\n\n\ndef sample_as_dataset(replay_buffer,\n                      num_steps=None,\n                      batch_size=None):\n  ds = replay_buffer.as_dataset(\n      num_steps=num_steps,\n      sample_batch_size=batch_size)\n  # Note, we don\'t use tf.contrib.data.get_single_element because the dataset\n  # contains more than one element.\n  if tf.executing_eagerly():\n    itr = iter(ds)\n    return next(itr)\n  else:\n    itr = tf.compat.v1.data.make_initializable_iterator(ds)\n    with tf.control_dependencies([itr.initializer]):\n      return itr.get_next()\n\n\ndef iterator_from_dataset(\n    replay_buffer,\n    num_steps=None,\n    batch_size=None,\n    single_deterministic_pass=None,\n    session=None):\n  ds = replay_buffer.as_dataset(\n      num_steps=num_steps,\n      sample_batch_size=batch_size,\n      single_deterministic_pass=single_deterministic_pass)\n\n  if tf.executing_eagerly():\n    for value in iter(ds):\n      yield tf.nest.map_structure(lambda v: v.numpy(), value)\n  else:\n    itr = tf.compat.v1.data.make_initializable_iterator(ds)\n    gn = itr.get_next()\n    initialized = [False]\n    while True:\n      if not initialized[0]:\n        session.run(itr.initializer)\n        initialized[0] = True\n      yield session.run(gn)\n\n\n# We access a lot of protected methods on replay_buffer to test them.\n# pylint: disable=protected-access\n\n\nclass EpisodicReplayBufferTest(test_utils.TestCase, parameterized.TestCase):\n\n  def _assertContains(self, list1, list2):\n    self.assertTrue(test_utils.contains(list1, list2))\n\n  def _assertNestedCloseness(self, closeness_fn, expected, actual):\n    tf.nest.map_structure(closeness_fn, expected, actual)\n\n  def testCreateEpisodeId(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n\n    episode_ids = replay_buffer.create_episode_ids()\n\n    self.assertEqual(self.evaluate(episode_ids), -1)\n\n  def testCreateBatchEpisodeIds(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=5)\n\n    episodes_0 = replay_buffer.create_episode_ids(2)\n    episodes_1 = replay_buffer.create_episode_ids(3)\n\n    self.assertAllEqual([-1] * 2, self.evaluate(episodes_0))\n    self.assertAllEqual([-1] * 3, self.evaluate(episodes_1))\n\n  def testCreateTooManyBatchEpisodeIdsRaisesError(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'Buffer cannot create episode_ids when \'\n        \'num_episodes 3 > capacity 2.\'):\n      replay_buffer.create_episode_ids(num_episodes=3)\n\n  def testGetEpisodeId(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n    episode_id = replay_buffer.create_episode_ids()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertEqual(self.evaluate(episode_id), -1)\n\n    # episode_id should be updated when begin_episode is True.\n    new_episode_id_0 = replay_buffer._get_episode_id(\n        episode_id, begin_episode=True)\n\n    # episode_id should not be updated when begin_episode is False and the input\n    # episode_id is not negative.\n    new_episode_id_1 = replay_buffer._get_episode_id(\n        new_episode_id_0, begin_episode=False)\n\n    # episode_id should be updated when begin_episode is True even when the\n    # input episode_id is not negative.\n    new_episode_id_2 = replay_buffer._get_episode_id(\n        new_episode_id_1, begin_episode=True)\n\n    (new_episode_id_0_value,\n     new_episode_id_1_value,\n     new_episode_id_2_value) = self.evaluate(\n         (new_episode_id_0, new_episode_id_1, new_episode_id_2))\n\n    self.assertEqual(new_episode_id_0_value, 0)\n    self.assertEqual(new_episode_id_1_value, 0)\n    self.assertEqual(new_episode_id_2_value, 1)\n\n  def testGetBatchEpisodeIds(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=5)\n    episode_ids = [replay_buffer.create_episode_ids(num_episodes=3)]\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    expected_ids = [[-1, -1, -1]]\n\n    # batch_episode_ids should be updated the first time regardless of\n    # begin_episode.\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=False))\n    expected_ids.append([0, 1, 2])\n\n    # batch_episode_ids should not be updated when begin_episode is False.\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=False))\n    expected_ids.append([0, 1, 2])\n\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=[False, False, False]))\n    expected_ids.append([0, 1, 2])\n\n    # batch_episode_ids should be updated only when begin_episode is True.\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=[True, False, False]))\n    expected_ids.append([3, 1, 2])\n\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=[False, True, False]))\n    expected_ids.append([3, 4, 2])\n\n    episode_ids.append(replay_buffer._get_batch_episode_ids(\n        episode_ids[-1], begin_episode=[False, True, True]))\n    expected_ids.append([3, 5, 6])\n\n    self.assertAllEqual(expected_ids, self.evaluate(episode_ids))\n\n  def testGetEpisodeIdBeginEpisodeFalse(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n    episode_id = replay_buffer.create_episode_ids()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertEqual(self.evaluate(episode_id), -1)\n\n    # episode_id should be updated regardless begin_episode being False since\n    # the initial value of the episode_id_var is not valid (id >= 0).\n    episode_id_0 = replay_buffer._get_episode_id(\n        episode_id, begin_episode=False)\n    expected_id_0 = 0\n\n    # episode_id should be updated because begin_episode is True.\n    episode_id_1 = replay_buffer._get_episode_id(\n        episode_id_0, begin_episode=True)\n    expected_id_1 = 1\n\n    # episode_id should not be updated because begin_episode is False.\n    episode_id_2 = replay_buffer._get_episode_id(\n        episode_id_1, begin_episode=False)\n    expected_id_2 = 1\n\n    self.assertAllEqual(\n        (expected_id_0, expected_id_1, expected_id_2),\n        self.evaluate(\n            (episode_id_0, episode_id_1, episode_id_2)))\n\n  def testGetTwoEpisodeId(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n    episode_id_0 = replay_buffer.create_episode_ids()\n    episode_id_1 = replay_buffer.create_episode_ids()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertEqual(self.evaluate(episode_id_0), -1)\n    self.assertEqual(self.evaluate(episode_id_1), -1)\n\n    # episode_id will be updated regardless of begin_episode being False since\n    # it doesn\'t have a valid value >= 0.\n    episode_id_0 = replay_buffer._get_episode_id(\n        replay_buffer.create_episode_ids())\n    with tf.control_dependencies([episode_id_0]):\n      episode_id_1 = replay_buffer._get_episode_id(\n          replay_buffer.create_episode_ids())\n    expected_id_0 = 0\n    expected_id_1 = 1\n\n    with tf.control_dependencies([episode_id_0, episode_id_1]):\n      # Now episode_id should be updated only when begin_episode is True.\n      episode_id_0_next = replay_buffer._get_episode_id(\n          episode_id_0, begin_episode=False)\n      with tf.control_dependencies([episode_id_0_next]):\n        episode_id_1_next = replay_buffer._get_episode_id(\n            episode_id_1, begin_episode=True)\n      expected_id_0_next = 0\n      expected_id_1_next = 2\n\n    self.assertEqual(\n        (expected_id_0, expected_id_1,\n         expected_id_0_next, expected_id_1_next),\n        self.evaluate((\n            episode_id_0, episode_id_1,\n            episode_id_0_next, episode_id_1_next)))\n\n  def testMaybeEndEpisode(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n    episode_id = replay_buffer.create_episode_ids()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    episode_id = replay_buffer._get_episode_id(episode_id)\n    completed = replay_buffer._maybe_end_episode(episode_id)\n    with tf.control_dependencies([completed]):\n      completed_episodes = replay_buffer._completed_episodes()\n\n    episode_id_value, completed_value, completed_episodes_value = self.evaluate(\n        (episode_id, completed, completed_episodes))\n    self.assertEqual(0, episode_id_value)\n    self.assertEqual(False, completed_value)\n    self.assertEmpty(completed_episodes_value)\n\n    # Mark episode as completed.\n    completed = replay_buffer._maybe_end_episode(0, end_episode=True)\n    with tf.control_dependencies([completed]):\n      completed_episodes = replay_buffer._completed_episodes()\n    completed_value, completed_episodes_value = self.evaluate(\n        (completed, completed_episodes))\n    self.assertEqual(True, completed_value)\n    self.assertEqual([0], completed_episodes_value)\n\n    # Invalid episode cannot be completed.\n    completed = replay_buffer._maybe_end_episode(1, end_episode=True)\n    with tf.control_dependencies([completed]):\n      completed_episodes = replay_buffer._completed_episodes()\n    completed_value, completed_episodes_value = self.evaluate(\n        (completed, completed_episodes))\n    self.assertEqual(False, completed_value)\n    self.assertEqual([0], completed_episodes_value)\n\n  def testGetBatchEpisodeIdsEndEpisode(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3)\n    batch_episode_ids = [replay_buffer.create_episode_ids(num_episodes=3)]\n    expected_episode_ids = [[-1, -1, -1]]\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    batch_episode_ids.append(\n        replay_buffer._get_batch_episode_ids(batch_episode_ids[-1]))\n    with tf.control_dependencies([batch_episode_ids[-1]]):\n      completed = [\n          replay_buffer._maybe_end_batch_episodes(batch_episode_ids[-1])]\n    with tf.control_dependencies(completed):\n      completed_episodes = [replay_buffer._completed_episodes()]\n    expected_episode_ids.append([0, 1, 2])\n    expected_completed = [[False, False, False]]\n    expected_completed_episodes = [[]]\n\n    # Mark all episodes as completed.\n    with tf.control_dependencies([completed_episodes[-1]]):\n      batch_episode_ids.append(\n          replay_buffer._get_batch_episode_ids(\n              batch_episode_ids[-1], end_episode=True))\n    with tf.control_dependencies([batch_episode_ids[-1]]):\n      completed_episodes.append(replay_buffer._completed_episodes())\n    expected_episode_ids.append([0, 1, 2])\n    expected_completed_episodes.append([0, 1, 2])\n\n    # Begin new episodes, it would overwrite the previous ones.\n    with tf.control_dependencies([completed_episodes[-1]]):\n      batch_episode_ids.append(\n          replay_buffer._get_batch_episode_ids(\n              batch_episode_ids[-1], begin_episode=True))\n    with tf.control_dependencies([batch_episode_ids[-1]]):\n      completed_episodes.append(replay_buffer._completed_episodes())\n    expected_episode_ids.append([3, 4, 5])\n    expected_completed_episodes.append([])\n\n    # Mark one of the new episodes as completed.\n    with tf.control_dependencies([completed_episodes[-1]]):\n      batch_episode_ids.append(\n          replay_buffer._get_batch_episode_ids(\n              batch_episode_ids[-1], end_episode=[False, True, False]))\n    with tf.control_dependencies([batch_episode_ids[-1]]):\n      completed_episodes.append(replay_buffer._completed_episodes())\n    expected_episode_ids.append([3, 4, 5])\n    expected_completed_episodes.append([4])\n\n    (batch_episode_ids_values,\n     completed_episodes_values,\n     completed_values) = self.evaluate(\n         (batch_episode_ids, completed_episodes, completed))\n\n    self.assertAllEqual(expected_episode_ids, batch_episode_ids_values)\n    tf.nest.map_structure(\n        self.assertAllEqual,\n        [np.array(x) for x in expected_completed_episodes],\n        completed_episodes_values)\n    self.assertAllEqual(expected_completed, completed_values)\n\n  def testAddSingleSample(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n    episode_ids = replay_buffer.create_episode_ids(num_episodes=1)\n\n    action = 1 * np.ones(spec[0].shape.as_list(), dtype=np.float32)\n    lidar = 2 * np.ones(spec[1][0].shape.as_list(), dtype=np.float32)\n    camera = 3 * np.ones(spec[1][1].shape.as_list(), dtype=np.float32)\n    values = [action, [lidar, camera]]\n    values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * 1), values)\n\n    episode_len_1_values = tf.nest.map_structure(\n        lambda arr: np.expand_dims(arr, 0), values)\n\n    new_episode_ids = replay_buffer.add_batch(values_batched, episode_ids)\n    sample, _ = replay_buffer.get_next()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    new_episode_ids_value = self.evaluate(new_episode_ids)\n    self.assertEqual(new_episode_ids_value, 0)\n    sample_ = self.evaluate(sample)\n    self._assertNestedCloseness(self.assertAllClose, episode_len_1_values,\n                                sample_)\n\n  def testNumFrames(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=5,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    episode_ids_var = common.create_variable(\n        \'episode_id\', initial_value=-1,\n        shape=(2,), use_local_variable=True)\n\n    action = 1 * np.ones(spec[0].shape.as_list(), dtype=np.float32)\n    lidar = 2 * np.ones(spec[1][0].shape.as_list(), dtype=np.float32)\n    camera = 3 * np.ones(spec[1][1].shape.as_list(), dtype=np.float32)\n    values = [action, [lidar, camera]]\n    values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * 2), values)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    @common.function_in_tf1()\n    def add_to_buffer(values_batched):\n      new_episode_ids = replay_buffer.add_batch(values_batched, episode_ids_var)\n      episode_ids_var.assign(new_episode_ids)\n      return new_episode_ids\n\n    for _ in range(4):\n      self.evaluate(add_to_buffer(values_batched))\n\n    num_frames = replay_buffer.num_frames()\n    num_frames_value = self.evaluate(num_frames)\n    self.assertEqual(num_frames_value, 8)\n\n  def testGetNextEmpty(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'),\n        [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, \'EpisodicReplayBuffer is empty. Make \'\n        \'sure to add items before sampling the buffer.\'):\n      sample, _ = replay_buffer.get_next()\n      self.evaluate(sample)\n\n  def testIterateEmpty(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, \'EpisodicReplayBuffer is empty. Make \'\n        \'sure to add items before sampling the buffer.\'):\n      sample_two_steps = sample_as_dataset(\n          replay_buffer, num_steps=2, batch_size=1)[0]\n      self.evaluate(sample_two_steps)\n\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, \'EpisodicReplayBuffer is empty. Make \'\n        \'sure to add items before sampling the buffer.\'):\n      sample_episode = sample_as_dataset(replay_buffer, num_steps=2,\n                                         batch_size=1)[0]\n      self.evaluate(sample_episode)\n\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, \'EpisodicReplayBuffer is empty. Make \'\n        \'sure to add items before sampling the buffer.\'):\n      sample_episode = sample_as_dataset(\n          replay_buffer, num_steps=2, batch_size=1)[0]\n      self.evaluate(sample_episode)\n\n  def testAsDatasetBatchSizeFullEpisodeRaisesError(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=10)\n    with self.assertRaises(ValueError):\n      sample_as_dataset(replay_buffer, num_steps=None, batch_size=10)\n\n  def testAddSteps(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n    episode_id = replay_buffer.create_episode_ids()\n\n    values = np.ones(spec.shape.as_list())\n    values = np.stack([values, 10 * values, 100 * values])\n    episode_id = replay_buffer.add_sequence(values, episode_id)\n    sample = lambda: sample_as_dataset(replay_buffer, 3, 10)[0]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.evaluate(episode_id)\n    sample_ = self.evaluate(sample())\n    self._assertContains([values], list(sample_))\n\n  def testAddStepsGetEpisode(self):\n    spec = specs.TensorSpec([5], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    episode_id = replay_buffer.create_episode_ids()\n\n    values = np.ones(spec.shape.as_list())\n    # Stack 3 steps.\n    num_steps = 3\n    values = np.stack([values * step for step in range(num_steps)])\n    episode_id = replay_buffer.add_sequence(values, episode_id)\n    episode = replay_buffer._get_episode(episode_id)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertAllEqual(values, self.evaluate(episode))\n\n  def testAddStepsGetEpisodes(self):\n    spec = specs.TensorSpec([5], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n    episode_id_0 = replay_buffer.create_episode_ids()\n    episode_id_1 = replay_buffer.create_episode_ids()\n\n    values = np.ones(spec.shape.as_list())\n    # Add episode_0 with 3 steps\n    values_0 = np.stack((values, 10 * values, 100 * values))\n    episode_id_0 = replay_buffer.add_sequence(values_0, episode_id_0)\n    episode_0 = replay_buffer._get_episode(episode_id_0)\n    # Add episode_1 with 2 steps\n    values_1 = np.stack((2 * values, 20 * values))\n    episode_id_1 = replay_buffer.add_sequence(values_1, episode_id_1)\n    episode_1 = replay_buffer._get_episode(episode_id_1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    episode_0_value = self.evaluate(episode_0)\n    episode_1_value = self.evaluate(episode_1)\n\n    self.assertEqual(1, self.evaluate(replay_buffer._get_last_episode_id()))\n    self.assertAllEqual(values_0, episode_0_value)\n    self.assertAllEqual(values_1, episode_1_value)\n\n  def testAddStepsUnknownBatchDims(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770194\')\n\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n    episode_ids = replay_buffer.create_episode_ids()\n\n    values = np.ones(spec.shape.as_list())\n    values = np.stack([values, 10 * values, 100 * values])\n    batch = tf.compat.v1.placeholder(shape=[None, 3], dtype=tf.int32)\n    sample, _ = sample_as_dataset(replay_buffer, num_steps=3, batch_size=10)\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(replay_buffer.add_sequence(batch, episode_ids),\n               {batch: values})\n      sample_ = sess.run(sample)\n      self._assertContains([values], list(sample_))\n\n  def testMultipleAddBatch(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    values = np.ones(spec.shape.as_list(), dtype=np.int32)\n    values = np.stack([values, 10 * values, 100 * values])\n    episode_ids = replay_buffer.create_episode_ids(num_episodes=3)\n    # In this case all episodes are valid, so it will add all the values.\n    # Add 1 to ep0, 10 to ep1, 100 to ep2\n    new_episode_ids = replay_buffer.add_batch(values, episode_ids)\n    # In this case the second episode will be invalid because we\'re starting\n    # two new episodes with a capacity of 3, so ep1 with location 1 is\n    # now going to be replaced by ep4 (in location 1).  As a result we won\'t\n    # add its values.\n    #\n    # Add 1 to ep3 (was ep0), ., 100 to ep4 (was ep1).\n    replay_buffer._begin_episode_fn = lambda _: [True, False, True]\n    new_episode_ids_2 = replay_buffer.add_batch(values, new_episode_ids)\n    # In this case all episodes are valid, so it will add all the values.\n    # Add 1 to ep3, 10 to ep5 (was ep2), 100 to ep4.\n    replay_buffer._begin_episode_fn = lambda _: [False, True, False]\n    new_episode_ids_3 = replay_buffer.add_batch(values, new_episode_ids_2)\n\n    # End result: ep3 with [1, 1], ep4 with [100, 100], ep5 with [10].\n    items = [replay_buffer._get_episode(i) for i in [3, 4, 5]]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertAllEqual(\n        ([0, 1, 2],\n         [3, 1, 4],\n         [3, 5, 4]),\n        self.evaluate((new_episode_ids,\n                       new_episode_ids_2,\n                       new_episode_ids_3)))\n    self.assertEqual(5, self.evaluate(replay_buffer._get_last_episode_id()))\n    items_ = self.evaluate(items)\n    self.assertAllEqual(items_[0], [values[0], values[0]])\n    self.assertAllEqual(items_[1], [values[2], values[2]])\n    self.assertAllEqual(items_[2], [values[1]])\n\n  def testAddBatchGetEpisodes(self):\n    num_episodes = 3\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=num_episodes, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    episode_ids = replay_buffer.create_episode_ids(num_episodes)\n\n    values = np.ones(spec.shape.as_list(), dtype=np.int32)\n    values = np.stack([values, 10 * values, 100 * values])\n    episode_ids = replay_buffer.add_batch(values, episode_ids)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    episode_ids = self.evaluate(episode_ids)\n    for episode_id in range(num_episodes):\n      episode = replay_buffer._get_episode(episode_ids[episode_id])\n      self.assertAllEqual([values[episode_id]], self.evaluate(episode))\n\n  def testAddBatchUnknownBatchDims(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'b/123770194\')\n\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    episode_ids = replay_buffer.create_episode_ids(3)\n\n    values = np.ones(spec.shape.as_list(), dtype=np.int32)\n    values = np.stack([values, 10 * values, 100 * values])\n    batch = tf.compat.v1.placeholder(shape=[None, 3], dtype=tf.int32)\n    items = replay_buffer.gather_all()\n    with self.cached_session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      sess.run(tf.compat.v1.local_variables_initializer())\n      sess.run(\n          replay_buffer.add_batch(batch, episode_ids), {batch: values})\n      self.assertEqual(2, sess.run(replay_buffer._get_last_episode_id()))\n      self.assertAllEqual(values, sess.run(items)[0])\n      # Make sure it\'s safe to run the tf.data pipeline for gather_all twice!\n      self.assertAllEqual(values, sess.run(items)[0])\n\n  def testGatherAllEmpty(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(spec)\n\n    items = replay_buffer.gather_all()\n    expected = []\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    items_ = self.evaluate(items)\n    self.assertAllClose(expected, items_)\n\n  def testParallelAdds(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    num_adds = 10\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=num_adds, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    expected_items = range(num_adds)\n    items_batched = tf.nest.map_structure(lambda t: tf.stack([t] * 1),\n                                          expected_items)\n    add_ops = []\n    for item in items_batched:\n      episode_ids = replay_buffer.create_episode_ids(1)\n      add_ops.append(replay_buffer.add_batch(item, episode_ids))\n    items = replay_buffer.gather_all()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.evaluate(add_ops)\n    self.assertEqual(self.evaluate(replay_buffer._get_last_episode_id()), 9)\n    items_ = self.evaluate(items)[0]\n    self.assertSameElements(expected_items, items_)\n\n  def testExtractNoClear(self):\n    num_episodes = 5\n    episode_length = 3\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=num_episodes - 1,\n        begin_episode_fn=lambda _: True, end_episode_fn=lambda _: False)\n    episode_id = replay_buffer.create_episode_ids()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    for i in range(num_episodes):\n      episode_id = replay_buffer.add_sequence(\n          i * tf.ones([episode_length], dtype=tf.int32),\n          episode_id)\n    self.evaluate(episode_id)\n    episodes = replay_buffer.extract([1, 0], clear_data=False)\n    [\n        episodes_length_, episodes_completed_, extracted_first_,\n        extracted_second_\n    ] = self.evaluate([\n        episodes.length,\n        episodes.completed,\n        list_ops.tensor_list_stack(episodes.tensor_lists[0], spec.dtype),\n        list_ops.tensor_list_stack(episodes.tensor_lists[1], spec.dtype),\n    ])\n    self.assertAllEqual(episodes_length_, [episode_length, episode_length])\n    self.assertAllEqual(episodes_completed_, [False, False])\n    self.assertAllClose(extracted_first_, [1] * episode_length)\n    self.assertAllClose(extracted_second_, [4] * episode_length)\n\n    # The location associated with episode ID 0 is not cleared.\n    self.assertAllClose(\n        self.evaluate(replay_buffer._get_episode(num_episodes - 1)),\n        [num_episodes - 1] * episode_length)\n    # The episode ID 1 (extracted) was not cleared.\n    self.assertAllClose(\n        self.evaluate(replay_buffer._get_episode(1)), [1.0] * episode_length)\n\n  def testExtractAndClear(self):\n    num_episodes = 5\n    episode_length = 3\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=num_episodes - 1,\n        begin_episode_fn=lambda _: True, end_episode_fn=lambda _: False)\n    episode_id = replay_buffer.create_episode_ids()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    for i in range(num_episodes):\n      episode_id = replay_buffer.add_sequence(\n          i * tf.ones([episode_length], dtype=tf.int32),\n          episode_id)\n    self.evaluate(episode_id)  # Run the insertions.\n\n    episodes = replay_buffer.extract([1, 0], clear_data=True)\n    [\n        episodes_length_, episodes_completed_, extracted_first_,\n        extracted_second_\n    ] = self.evaluate([\n        episodes.length,\n        episodes.completed,\n        list_ops.tensor_list_stack(episodes.tensor_lists[0], spec.dtype),\n        list_ops.tensor_list_stack(episodes.tensor_lists[1], spec.dtype),\n    ])\n    self.assertAllEqual(episodes_length_, [episode_length, episode_length])\n    self.assertAllEqual(episodes_completed_, [False, False])\n    self.assertAllClose(extracted_first_, [1] * episode_length)\n    self.assertAllClose(extracted_second_, [4] * episode_length)\n\n    # The location associated with episode ID 0 (extracted) was cleared.\n    self.assertAllEqual(\n        self.evaluate(\n            tf.size(input=replay_buffer._get_episode(num_episodes - 1))), 0)\n    # The episode ID 1 (extracted) was cleared.\n    self.assertEqual(\n        self.evaluate(tf.size(input=replay_buffer._get_episode(1))), 0)\n\n  @parameterized.parameters([dict(stateless=False), dict(stateless=True)])\n  def testExtend(self, stateless):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n\n    if stateless:\n      extend_ids = replay_buffer.create_episode_ids(3)\n    else:\n      stateful_replay_buffer = (\n          episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n              replay_buffer, num_episodes=3))\n      extend_ids = stateful_replay_buffer.episode_ids\n\n    episodes1 = episodic_replay_buffer.Episodes(\n        length=tf.constant([2, 1, 3], dtype=tf.int64),\n        completed=tf.constant([0, 0, 1], dtype=tf.uint8),\n        tensor_lists=tf.stack([\n            list_ops.tensor_list_from_tensor(\n                tf.constant([100, 200], dtype=spec.dtype),\n                element_shape=spec.shape),\n            list_ops.tensor_list_from_tensor(\n                tf.constant([999], dtype=spec.dtype), element_shape=spec.shape),\n            list_ops.tensor_list_from_tensor(\n                tf.constant([1, 2, 3], dtype=spec.dtype),\n                element_shape=spec.shape),\n        ]))\n\n    episodes2 = episodic_replay_buffer.Episodes(\n        length=tf.constant([1, 3], dtype=tf.int64),\n        completed=tf.constant([0, 1], dtype=tf.uint8),\n        tensor_lists=tf.stack([\n            list_ops.tensor_list_from_tensor(\n                tf.constant([888], dtype=spec.dtype), element_shape=spec.shape),\n            list_ops.tensor_list_from_tensor(\n                tf.constant([4, 5, 6], dtype=spec.dtype),\n                element_shape=spec.shape),\n        ]))\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    if stateless:\n      extended_ids_1 = replay_buffer.extend_episodes(\n          extend_ids, [0, 1, 2], episodes1)\n      extend_ids = extended_ids_1\n      replay_buffer._begin_episode_fn = lambda _: [False, True]\n      extended_ids_2 = replay_buffer.extend_episodes(\n          extend_ids, [1, 2], episodes2)\n    else:\n      replay_buffer._begin_episode_fn = lambda _: False\n      extended_ids_1 = stateful_replay_buffer.extend_episodes(\n          [0, 1, 2], episodes1)\n      replay_buffer._begin_episode_fn = lambda _: [False, True]\n      extended_ids_2 = stateful_replay_buffer.extend_episodes(\n          [1, 2], episodes2)\n\n    if stateless:\n      extended_ids_1_value, extended_ids_2_value = (\n          self.evaluate((extended_ids_1, extended_ids_2)))\n    else:\n      extended_ids_1_value = self.evaluate(extended_ids_1)\n      extended_ids_2_value = self.evaluate(extended_ids_2)\n\n    self.assertAllEqual(extended_ids_1_value, [0, 1, 2])\n    self.assertAllEqual(extended_ids_2_value, [0, 1, 3])\n\n    if not stateless:\n      self.assertAllEqual(self.evaluate(extend_ids), [0, 1, 3])\n\n    self.assertAllEqual(self.evaluate(\n        replay_buffer._get_episode(1)), [999, 888])\n    self.assertAllEqual(self.evaluate(replay_buffer._get_episode(2)), [1, 2, 3])\n    self.assertAllEqual(self.evaluate(replay_buffer._get_episode(3)), [4, 5, 6])\n\n  def testClearAll(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    values = tf.expand_dims(np.ones(spec.shape.as_list(), dtype=np.int32), 0)\n\n    empty_values = np.empty((0, 3), dtype=np.int32)\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    episode_id_1 = replay_buffer.create_episode_ids(1)\n    episode_id_2 = replay_buffer.create_episode_ids(1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    self.assertAllEqual(self.evaluate(replay_buffer.gather_all()), empty_values)\n    self.assertEqual(self.evaluate(replay_buffer._last_episode), -1)\n    self.assertAllEqual(self.evaluate(replay_buffer._episode_lengths), [0, 0])\n\n    self.evaluate(replay_buffer.add_batch(values, episode_id_1))\n    self.assertAllEqual(self.evaluate(replay_buffer.gather_all())[0], values)\n    self.assertEqual(self.evaluate(replay_buffer._last_episode), 0)\n    self.assertAllEqual(self.evaluate(replay_buffer._episode_lengths), [1, 0])\n\n    self.evaluate(replay_buffer.clear())\n    self.assertAllEqual(self.evaluate(replay_buffer.gather_all()), empty_values)\n    self.assertEqual(self.evaluate(replay_buffer._last_episode), 0)\n    self.assertAllEqual(self.evaluate(replay_buffer._episode_lengths), [0, 0])\n\n    self.evaluate(replay_buffer.add_batch(values, episode_id_2))\n    self.assertAllEqual(self.evaluate(replay_buffer.gather_all())[0], values)\n    self.evaluate(replay_buffer._clear(clear_all_variables=True))\n    self.assertAllEqual(self.evaluate(replay_buffer.gather_all()), empty_values)\n    self.assertEqual(self.evaluate(replay_buffer._last_episode), -1)\n    self.assertAllEqual(self.evaluate(replay_buffer._episode_lengths), [0, 0])\n\n  def _create_rb_and_add_3N_episodes(\n      self, drop_remainder=False, window_shift=None, repeat=1):\n    spec = {\'a\': specs.TensorSpec([], tf.int32, \'spec\')}\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec,\n        capacity=10,\n        dataset_drop_remainder=drop_remainder,\n        dataset_window_shift=window_shift,\n        begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    episode_id = replay_buffer.create_episode_ids()\n\n    values = np.ones(spec[\'a\'].shape.as_list())\n    values = np.stack([values, 10 * values, 100 * values])\n\n    original_episode_id = self.evaluate(episode_id)\n\n    for _ in range(repeat):\n      # Add an episode with frames [1, 10, 100]\n      replay_buffer._begin_episode_fn = lambda _: True\n      episode_id = replay_buffer.add_sequence({\'a\': values}, episode_id)\n      # Add an episode with frames [2, 20, 200, 3, 30, 300]\n      episode_id = replay_buffer.add_sequence({\'a\': 2 * values}, episode_id)\n      replay_buffer._begin_episode_fn = lambda _: False\n      episode_id = replay_buffer.add_sequence({\'a\': 3 * values}, episode_id)\n      # Add an episode with frames [-1, -10, -100]\n      replay_buffer._begin_episode_fn = lambda _: True\n      episode_id = replay_buffer.add_sequence({\'a\': -values}, episode_id)\n    new_episode_id = self.evaluate(episode_id)\n\n    assert new_episode_id >= original_episode_id + repeat * 3, (\n        \'{} vs. {}\'.format(new_episode_id, original_episode_id + repeat * 3))\n\n    return replay_buffer\n\n  def testSingleDeterministicPassAsDataset(self):\n    replay_buffer = self._create_rb_and_add_3N_episodes()\n    with self.cached_session() as session:\n      pass\n    itr = iterator_from_dataset(\n        replay_buffer,\n        single_deterministic_pass=True,\n        session=session)\n\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([1, 10, 100])}, next(itr))\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([2, 20, 200, 3, 30, 300])}, next(itr))\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([-1, -10, -100])}, next(itr))\n    with self.assertRaises((tf.errors.OutOfRangeError, StopIteration)):\n      next(itr)\n\n  @parameterized.parameters(\n      [dict(drop_remainder=False), dict(drop_remainder=True)])\n  def testSingleDeterministicPassAsDatasetWithNumSteps(\n      self, drop_remainder):\n    replay_buffer = self._create_rb_and_add_3N_episodes(\n        drop_remainder=drop_remainder)\n    with self.cached_session() as session:\n      pass\n    itr = iterator_from_dataset(\n        replay_buffer,\n        num_steps=5,\n        single_deterministic_pass=True,\n        session=session)\n\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([1, 10, 100, 2, 20])}, next(itr))\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([200, 3, 30, 300, -1])}, next(itr))\n    if not drop_remainder:\n      tf.nest.map_structure(\n          self.assertAllEqual, {\'a\': _a([-10, -100])}, next(itr))\n    with self.assertRaises((tf.errors.OutOfRangeError, StopIteration)):\n      next(itr)\n\n  def testSingleDeterministicPassAsDatasetWithNumStepsBatchSize(self):\n    # Add 6 episodes, repeating 3 episodes twice.\n    replay_buffer = self._create_rb_and_add_3N_episodes(\n        window_shift=None, drop_remainder=False, repeat=2)\n    with self.cached_session() as session:\n      pass\n    itr = iterator_from_dataset(\n        replay_buffer,\n        batch_size=3,\n        num_steps=5,\n        single_deterministic_pass=True,\n        session=session)\n\n    # NOTE(ebrevdo): Here, the final steps of the final episodes get cut off.\n    tf.nest.map_structure(self.assertAllEqual,\n                          {\'a\': _a([[1, 10, 100, 1, 10],\n                                    [2, 20, 200, 3, 30],\n                                    [-1, -10, -100, -1, -10]])},\n                          next(itr))\n    # Note here we are missing the final [100] from episode 4, the final [30,\n    # 300] from episode 5, and the final [-100] from episode 6.\n    tf.nest.map_structure(\n        self.assertAllEqual, {\'a\': _a([[300, 2, 20, 200, 3]])}, next(itr))\n    with self.assertRaises((tf.errors.OutOfRangeError, StopIteration)):\n      next(itr)\n\n  def testSingleDeterministicPassAsDatasetWithNumStepsBatchSizeAndShift(self):\n    # Add 6 episodes, repeating 3 episodes twice.\n    replay_buffer = self._create_rb_and_add_3N_episodes(\n        window_shift=1, drop_remainder=False, repeat=2)\n    with self.cached_session() as session:\n      pass\n    itr = iterator_from_dataset(\n        replay_buffer,\n        batch_size=3,\n        num_steps=5,\n        single_deterministic_pass=True,\n        session=session)\n\n    # Due to window_shift == 1, we see two instances of each block: the\n    # original, and one shifted left by 1.  For example, if we have two episodes\n    # in a batch entry, [1, 10, 100] and [1, 10, 100] again, then with num_steps\n    # == 5 we\'ll see one block that\'s [1, 10, 100, 1, 10] and a second one\n    # that\'s shifted: [10, 100, 1, 10, 100].\n    tf.nest.map_structure(self.assertAllEqual,\n                          {\'a\': _a([[1, 10, 100, 1, 10],\n                                    [2, 20, 200, 3, 30],\n                                    [-1, -10, -100, -1, -10]])},\n                          next(itr))\n    tf.nest.map_structure(self.assertAllEqual,\n                          {\'a\': _a([[10, 100, 1, 10, 100],\n                                    [20, 200, 3, 30, 300],\n                                    [-10, -100, -1, -10, -100]])},\n                          next(itr))\n    tf.nest.map_structure(self.assertAllEqual,\n                          {\'a\': _a([[200, 3, 30, 300, 2],\n                                    [3, 30, 300, 2, 20],\n                                    [30, 300, 2, 20, 200]])},\n                          next(itr))\n    tf.nest.map_structure(self.assertAllEqual,\n                          {\'a\': _a([[300, 2, 20, 200, 3],\n                                    [2, 20, 200, 3, 30],\n                                    [20, 200, 3, 30, 300]])},\n                          next(itr))\n    with self.assertRaises((tf.errors.OutOfRangeError, StopIteration)):\n      next(itr)\n\n\nclass StatefulEpisodicReplayBufferTest(test_utils.TestCase):\n\n  def _assertContains(self, list1, list2):\n    self.assertTrue(test_utils.contains(list1, list2))\n\n  def _assertCircularOrdering(self, expected_order, given_order):\n    for i in range(len(given_order)):\n      self.assertIn(given_order[i], expected_order)\n      if i > 0:\n        prev_idx = expected_order.index(given_order[i - 1])\n        cur_idx = expected_order.index(given_order[i])\n        self.assertEqual(cur_idx, (prev_idx + 1) % len(expected_order))\n\n  def testCreateEpisodeId(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n    replay_buffer_stateful_0 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(replay_buffer))\n    replay_buffer_stateful_1 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(replay_buffer))\n\n    episode_0 = replay_buffer_stateful_0.episode_ids\n    episode_1 = replay_buffer_stateful_1.episode_ids\n\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertIsNot(episode_0, episode_1)\n    self.assertEqual(self.evaluate(episode_0), -1)\n    self.assertEqual(self.evaluate(episode_1), -1)\n\n  def testCreateBatchEpisodeIds(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=5)\n\n    replay_buffer_stateful_0 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=2))\n    replay_buffer_stateful_1 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=3))\n    episodes_0 = replay_buffer_stateful_0.episode_ids\n    episodes_1 = replay_buffer_stateful_1.episode_ids\n\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertIsNot(episodes_0, episodes_1)\n    self.assertAllEqual([-1] * 2, self.evaluate(episodes_0))\n    self.assertAllEqual([-1] * 3, self.evaluate(episodes_1))\n\n  def testCreateTooManyBatchEpisodeIdsRaisesError(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2)\n\n    with self.assertRaisesRegexp(\n        ValueError, \'Buffer cannot create episode_ids when \'\n        \'num_episodes 3 > capacity 2.\'):\n      episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n          replay_buffer, num_episodes=3)\n\n  def testAddSingleMultipleTimesSampleAsDatasetBatched(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=10, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    replay_buffer_stateful = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    values = np.expand_dims(np.ones(spec.shape.as_list(), dtype=np.int32), 0)\n    values = [values, 10 * values, 100 * values]\n    simple_values = [1, 10, 100]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.evaluate(replay_buffer_stateful.add_batch(values[0]))\n    self.evaluate(replay_buffer_stateful.add_batch(values[1]))\n    self.evaluate(replay_buffer_stateful.add_batch(values[2]))\n    sample_next = (sample_as_dataset(replay_buffer, num_steps=3,\n                                     batch_size=100)[0])\n\n    self.assertEqual(tf.compat.dimension_value(sample_next.shape[2]), 3)\n    sample_ = self.evaluate(sample_next)\n\n    # Shape should be batch_size x episode_length x tensor spec.\n    # In this case, all episodes have length 3, which is why batching works.\n    self.assertEqual(sample_.shape, (100, 3, 3))\n\n    for multi_item in sample_:\n      self._assertCircularOrdering(simple_values,\n                                   [item[0] for item in multi_item])\n\n    self._assertContains(\n        list(values), [multi_item[0] for multi_item in sample_])\n\n  def testAddSingleMultipleTimesSampleAsDatasetBatchedMultiStep(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=2, begin_episode_fn=lambda _: False,\n        end_episode_fn=lambda _: False)\n    replay_buffer_stateful = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    values = np.expand_dims(np.ones(spec.shape.as_list(), dtype=np.int32), 0)\n    values = [values, 10 * values, 100 * values]\n    simple_values = [1, 10, 100]\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.evaluate(replay_buffer_stateful.add_batch(values[0]))\n    self.evaluate(replay_buffer_stateful.add_batch(values[1]))\n    self.evaluate(replay_buffer_stateful.add_batch(values[2]))\n    sample_next = (\n        sample_as_dataset(replay_buffer, num_steps=2, batch_size=100)[0])\n    self.assertEqual(sample_next.shape[1:].as_list(), [2, 3])\n    sample_ = self.evaluate(sample_next)\n\n    # Shape should be batch_size x num_steps x tensor spec.\n    self.assertEqual(sample_.shape, (100, 2, 3))\n\n    for multi_item in sample_:\n      self._assertCircularOrdering(simple_values,\n                                   [item[0] for item in multi_item])\n\n    self._assertContains(\n        list(values), [multi_item[0] for multi_item in sample_])\n\n  def testAddBatch(self):\n    spec = specs.TensorSpec([3], tf.int32, \'lidar\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    stateful_replay_buffer = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=3))\n    values = np.ones(spec.shape.as_list(), dtype=np.int32)\n    values = np.stack([values, 10 * values, 100 * values])\n    new_episode_ids = stateful_replay_buffer.add_batch(values)\n    item_0 = replay_buffer._get_episode(0)\n    item_1 = replay_buffer._get_episode(1)\n    item_2 = replay_buffer._get_episode(2)\n    # We appended one time step to each of 3 episodes.  If we tf.stack\n    # the line below, we would end up with shape\n    #  (batch_size, time_steps, depth) == (3, 1, 3).\n    #\n    # Instead we concat here to make testing easier below.\n    items = tf.nest.map_structure(lambda *x: tf.concat(x, 0), item_0, item_1,\n                                  item_2)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    self.assertAllEqual([0, 1, 2], self.evaluate(new_episode_ids))\n    self.assertEqual(2, self.evaluate(replay_buffer._get_last_episode_id()))\n    self.assertAllEqual(values, self.evaluate(items))\n\n  def testMultipleAddBatch(self):\n    spec = (specs.TensorSpec([3], tf.int32, \'lidar\'),\n            specs.TensorSpec([], tf.bool, \'begin_episode\'))\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda value_and_begin: value_and_begin[1],\n        end_episode_fn=lambda _: False)\n    stateful_replay_buffer = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=3))\n    episode_ids_var = stateful_replay_buffer.episode_ids\n\n    values = np.ones(spec[0].shape.as_list(), dtype=np.int32)\n    values = np.stack([values, 10 * values, 100 * values])\n    # In this case all episodes are valid, so it will add all the values.\n    # Add 1 to ep0, 10 to ep1, 100 to ep2\n    new_episode_ids = stateful_replay_buffer.add_batch(\n        (values, tf.constant([False, False, False])))\n    with tf.control_dependencies([new_episode_ids]):\n      items = [[replay_buffer._get_episode(i) for i in [0, 1, 2]]]\n    # In this case the second episode is invalid, so it won\'t add its values.\n    # Add 1 to ep3 (was ep0), ., 100 to ep4 (was ep1).\n    with tf.control_dependencies(items[-1][0]):\n      new_episode_ids_2 = stateful_replay_buffer.add_batch(\n          (values, tf.constant([True, False, True])))\n    with tf.control_dependencies([new_episode_ids_2]):\n      items.append([replay_buffer._get_episode(i) for i in [2, 3, 4]])\n    # In this case all episodes are valid, so it will add all the values.\n    # Add 1 to ep3, 10 to ep5 (was ep2), 100 to ep4.\n    with tf.control_dependencies(items[-1][0]):\n      new_episode_ids_3 = stateful_replay_buffer.add_batch(\n          (values, tf.constant([False, True, False])))\n    with tf.control_dependencies([new_episode_ids_3]):\n      # End result: ep3 with [1, 1], ep4 with [100, 100], ep5 with [10].\n      items.append([replay_buffer._get_episode(i) for i in [3, 4, 5]])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    (new_episode_ids,\n     new_episode_ids_2,\n     new_episode_ids_3,\n     items) = self.evaluate(\n         (new_episode_ids,\n          new_episode_ids_2,\n          new_episode_ids_3,\n          items))\n    self.assertAllEqual([0, 1, 2], new_episode_ids)\n    self.assertAllEqual([3, 1, 4], new_episode_ids_2)\n    self.assertAllEqual([3, 5, 4], new_episode_ids_3)\n    episode_ids_value = self.evaluate(episode_ids_var)\n    self.assertAllEqual([3, 5, 4], episode_ids_value)\n    self.assertEqual(5, self.evaluate(replay_buffer._get_last_episode_id()))\n\n    # First add_batch: ep0, ep1, ep2.\n    self.assertAllEqual(items[0][0][0], [values[0]])\n    self.assertAllEqual(items[0][1][0], [values[1]])\n    self.assertAllEqual(items[0][2][0], [values[2]])\n    # Second add_batch: ep2, ep3, ep4.\n    self.assertAllEqual(items[1][0][0], [values[2]])\n    self.assertAllEqual(items[1][1][0], [values[0]])\n    self.assertAllEqual(items[1][2][0], [values[2]])\n    # Third add_batch: ep3, ep4, ep5.\n    self.assertAllEqual(items[2][0][0], [values[0], values[0]])\n    self.assertAllEqual(items[2][1][0], [values[2], values[2]])\n    self.assertAllEqual(items[2][2][0], [values[1]])\n\n  def testGatherAll(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n    replay_buffer_stateful = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n\n    action_state = common.create_variable(\'counter\', -1, dtype=spec.dtype)\n    action = lambda: tf.expand_dims(action_state.assign_add(1), 0)\n\n    # pylint: disable=unnecessary-lambda\n    items = lambda: replay_buffer.gather_all()\n    expected = [list(range(10))]\n    if tf.executing_eagerly():\n      add_op = lambda: replay_buffer_stateful.add_batch(action())\n    else:\n      add_op = replay_buffer_stateful.add_batch(action())\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    for _ in range(10):\n      self.evaluate(add_op)\n\n    items_ = self.evaluate(items())\n    self.assertAllClose(expected, items_)\n\n  def testAddOverCapacity(self):\n    num_adds = 5\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda _: True, end_episode_fn=lambda _: False)\n\n    a_state = common.create_variable(\'counter\', -1, dtype=spec.dtype)\n    a = lambda: tf.expand_dims(a_state.assign_add(1), 0)\n    replay_buffer_stateful = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n\n    if tf.executing_eagerly():\n      # pylint: disable=g-long-lambda\n      # pylint: disable=unnecessary-lambda\n      add = lambda: replay_buffer_stateful.add_batch(a())\n      items = replay_buffer.gather_all\n    else:\n      add = replay_buffer_stateful.add_batch(a())\n      items = replay_buffer.gather_all()\n\n    expected = [2, 3, 4]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    for _ in range(num_adds):\n      self.evaluate(add)\n    items_ = self.evaluate(items)[0]\n    self.assertAllClose(expected, items_)\n\n  def testOverwriteOldEpisodes(self):\n    num_adds = 5\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=3,\n        begin_episode_fn=lambda _: True, end_episode_fn=lambda _: False)\n\n    a_state = common.create_variable(\'counter\', -1, dtype=spec.dtype)\n    a = lambda: tf.expand_dims(a_state.assign_add(1), 0)\n\n    replay_buffer_stateful = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    episode_id_var = replay_buffer_stateful.episode_ids\n    if tf.executing_eagerly():\n      def add():\n        return replay_buffer_stateful.add_batch(a())\n      items = replay_buffer.gather_all\n    else:\n      add = replay_buffer_stateful.add_batch(a())\n      items = replay_buffer.gather_all()\n    expected = [2, 3, 4]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    for i in range(num_adds):\n      self.evaluate(add)\n      self.assertEqual(self.evaluate(episode_id_var), i)\n\n    items_ = self.evaluate(items)[0]\n    self.assertAllClose(items_, expected)\n\n  def testAddToStaleEpisodeIDIsAvoided(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=1,\n        begin_episode_fn=lambda _: False, end_episode_fn=lambda _: False)\n\n    a_state = common.create_variable(\'counter\', -1, dtype=spec.dtype)\n    a = lambda: tf.expand_dims(a_state.assign_add(1), 0)\n\n    replay_buffer_stateful_0 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    replay_buffer_stateful_1 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    episode_id_var_0 = replay_buffer_stateful_0.episode_ids\n    episode_id_var_1 = replay_buffer_stateful_1.episode_ids\n\n    items_fn = replay_buffer.gather_all\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n\n    self.evaluate(replay_buffer_stateful_0.add_batch(a()))\n    self.assertEqual(self.evaluate(episode_id_var_0), 0)\n    self.assertAllClose(self.evaluate(items_fn())[0], [0])\n\n    self.evaluate(replay_buffer_stateful_1.add_batch(a()))\n    self.assertEqual(self.evaluate(episode_id_var_1), 1)\n    self.assertAllClose(self.evaluate(items_fn())[0], [1])\n\n    # Adding to episode 0 will be avoided as it is no longer there.\n    self.evaluate(replay_buffer_stateful_0.add_batch(a()))\n    self.assertEqual(self.evaluate(episode_id_var_0), 0)\n    self.assertAllClose(self.evaluate(items_fn())[0], [1])\n\n  def testParallelAddOverCapacity(self):\n    num_adds = 3\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n        spec, capacity=4,\n        begin_episode_fn=lambda _: True, end_episode_fn=lambda _: False)\n\n    a = common.create_variable(\'a\', 0, dtype=spec.dtype)\n    b = common.create_variable(\'b\', 10, dtype=spec.dtype)\n\n    replay_buffer_stateful_0 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    replay_buffer_stateful_1 = (\n        episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n            replay_buffer, num_episodes=1))\n    episode_id_var_0 = replay_buffer_stateful_0.episode_ids\n    episode_id_var_1 = replay_buffer_stateful_1.episode_ids\n\n    expected = [2, 12, 1, 11]\n\n    @common.function\n    def add_elem_0(variable):\n      elem = tf.expand_dims(variable, 0)\n      replay_buffer_stateful_0.add_batch(elem)\n\n    @common.function\n    def add_elem_1(variable):\n      elem = tf.expand_dims(variable, 0)\n      replay_buffer_stateful_1.add_batch(elem)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.local_variables_initializer())\n    for _ in range(num_adds):\n      self.evaluate([add_elem_0(a),\n                     add_elem_1(b)])\n      self.evaluate([a.assign_add(1), b.assign_add(1)])\n    items = replay_buffer.gather_all()\n    items_ = self.evaluate(items)[0]\n    episode_ids = self.evaluate([episode_id_var_0, episode_id_var_1])\n    self.assertSameElements(episode_ids, [4, 5])\n    self.assertSameElements(items_, expected)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/episodic_table.py,32,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A tensorflow table stored in tf.Variables.\n\nThe row is the index or location at which the value is saved, and the value is\na nest of Tensors.\n\nThis class is not threadsafe.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import nest_utils\n\nfrom tensorflow.python.ops import list_ops  # TF internal\n\n\ndef _empty_slot(spec):\n  shape = [s if s is not None else -1 for s in spec.shape.as_list()]\n  shape = tf.convert_to_tensor(value=shape, dtype=tf.int64, name=\'shape\')\n  return list_ops.empty_tensor_list(shape, spec.dtype)\n\n\nclass EpisodicTable(tf.Module):\n  """"""A table that can store Episodes of variable length.""""""\n\n  def __init__(self, tensor_spec, capacity, name_prefix=\'EpisodicTable\'):\n    """"""Creates a table.\n\n    Args:\n      tensor_spec: A nest of TensorSpec representing each value that can be\n        stored in the table.\n      capacity: Maximum number of values the table can store.\n      name_prefix: optional name prefix for variable names.\n\n    Raises:\n      ValueError: If the names in tensor_spec are empty or not unique.\n    """"""\n    self._tensor_spec = tensor_spec\n    self._capacity = capacity\n    self._spec_names = []\n\n    def _create_unique_slot_name(spec, count=0):\n      name = spec.name or \'slot\'\n      name = name + \'_\' + str(count)\n      if name not in self._spec_names:\n        self._spec_names.append(name)\n        return name_prefix + \'.\' + name\n      else:\n        return _create_unique_slot_name(spec, count + 1)\n\n    self._slots = tf.nest.map_structure(_create_unique_slot_name,\n                                        self._tensor_spec)\n    self._flattened_slots = tf.nest.flatten(self._slots)\n    self._flattened_specs = tf.nest.flatten(self._tensor_spec)\n\n    def _create_storage(spec, slot_name):\n      return tf.lookup.experimental.DenseHashTable(\n          key_dtype=tf.int64,\n          value_dtype=tf.variant,\n          empty_key=-1,\n          deleted_key=-2,\n          name=slot_name,\n          default_value=_empty_slot(spec))\n\n    self._storage = tf.nest.map_structure(_create_storage, self._tensor_spec,\n                                          self._slots)\n    self._variables = tf.nest.flatten(self._storage)\n    self._slot2variable_map = dict(zip(self._flattened_slots, self._variables))\n    self._slot2spec_map = dict(\n        zip(self._flattened_slots, self._flattened_specs))\n\n  @property\n  def slots(self):\n    return self._slots\n\n  def variables(self):\n    return self._variables\n\n  def _stack_tensor_list(self, slot, tensor_list):\n    """"""Stacks a slot list, restoring dtype and shape information.\n\n    Going through the Variable loses all TensorList dtype and\n    static element shape info.  Setting dtype, shape, and adding batch\n    dimension for the length of the list.\n\n    Args:\n      slot: The slot corresponding to tensor_list\n      tensor_list: TensorList of values stored in a scalar Tensor.\n\n    Returns:\n      A Tensor with first dimension equal to the length of tensor_list.\n    """"""\n    tensor_list.shape.assert_has_rank(0)\n    value = list_ops.tensor_list_stack(tensor_list,\n                                       self._slot2spec_map[slot].dtype)\n    value.set_shape([None] + self._slot2spec_map[slot].shape.as_list())\n    return value\n\n  def get_episode_lists(self, rows=None):\n    """"""Returns episodes as TensorLists.\n\n    Args:\n      rows: A list/tensor of location(s) to retrieve. If not specified, all\n        episodes are returned.\n\n    Returns:\n      Episodes as TensorLists, stored in nested Tensors.\n    """"""\n    if rows is None:\n      rows = tf.range(self._capacity, dtype=tf.int64)\n    else:\n      rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n    values = [\n        self._slot2variable_map[slot].lookup(rows)\n        for slot in self._flattened_slots\n    ]\n    return tf.nest.pack_sequence_as(self.slots, values)\n\n  def get_episode_values(self, row):\n    """"""Returns all values for the given row.\n\n    Args:\n      row: A scalar tensor of location to read values from. A batch of values\n        will be returned with each Tensor having an extra first dimension equal\n        to the length of rows.\n\n    Returns:\n      Stacked values at given row.\n    """"""\n    row = tf.convert_to_tensor(value=row, dtype=tf.int64)\n    row.shape.assert_has_rank(0)\n    return tf.nest.map_structure(self._stack_tensor_list, self.slots,\n                                 self.get_episode_lists(row))\n\n  def append(self, row, values):\n    """"""Returns ops for appending multiple time values at the given row.\n\n    Args:\n      row: A scalar location at which to append values.\n      values: A nest of Tensors to append.  The outermost dimension of each\n        tensor is treated as a time axis, and these must all be equal.\n\n    Returns:\n      Ops for appending values at the given row.\n    """"""\n    row = tf.convert_to_tensor(value=row, dtype=tf.int64)\n    flattened_values = tf.nest.flatten(values)\n    append_ops = []\n    for spec, slot, value in zip(self._flattened_specs, self._flattened_slots,\n                                 flattened_values):\n      var_slot = self._slot2variable_map[slot].lookup(row)\n      value_as_tl = list_ops.tensor_list_from_tensor(\n          value, element_shape=tf.cast(spec.shape.as_list(), dtype=tf.int64))\n      new_value = list_ops.tensor_list_concat_lists(\n          var_slot, value_as_tl, element_dtype=spec.dtype)\n      append_ops.append(\n          self._slot2variable_map[slot].insert_or_assign(row, new_value))\n    return tf.group(*append_ops)\n\n  def add(self, rows, values):\n    """"""Returns ops for appending a single frame value to the given rows.\n\n    This operation is batch-aware.\n\n    Args:\n      rows: A list/tensor of location(s) to write values at.\n      values: A nest of Tensors to write. If rows has more than one element,\n        values can have an extra first dimension representing the batch size.\n        Values must have the same structure as the tensor_spec of this class\n        Must have batch dimension matching the number of rows.\n\n    Returns:\n      Ops for appending values at rows.\n    """"""\n    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n    flattened_values = tf.nest.flatten(values)\n    write_ops = []\n    for slot, value in zip(self._flattened_slots, flattened_values):\n      var_slots = self._slot2variable_map[slot].lookup(rows)\n      new_value = list_ops.tensor_list_push_back_batch(var_slots, value)\n      write_ops.append(\n          self._slot2variable_map[slot].insert_or_assign(rows, new_value))\n    return tf.group(*write_ops)\n\n  def extend(self, rows, episode_lists):\n    """"""Returns ops for extending a set of rows by the given TensorLists.\n\n    Args:\n      rows: A batch of row locations to extend.\n      episode_lists: Nested batch of TensorLists, must have the same batch\n        dimension as rows.\n\n    Returns:\n      Ops for extending the table.\n    """"""\n    nest_utils.assert_same_structure(self.slots, episode_lists)\n\n    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n    existing_lists = self.get_episode_lists(rows)\n    flat_existing_lists = tf.nest.flatten(existing_lists)\n    flat_episode_lists = tf.nest.flatten(episode_lists)\n\n    write_ops = []\n    for spec, slot, existing_list, episode_list in zip(\n        self._flattened_specs, self._flattened_slots, flat_existing_lists,\n        flat_episode_lists):\n      extended_list = list_ops.tensor_list_concat_lists(\n          existing_list, episode_list, element_dtype=spec.dtype)\n      slot_variable = self._slot2variable_map[slot]\n      write_ops.append(\n          slot_variable.insert_or_assign(rows, extended_list))\n    return tf.group(*write_ops)\n\n  def clear(self):\n    """"""Returns op for clearing the table and removing all the episodes.\n\n    Returns:\n      Op for clearing the table.\n    """"""\n    clear_ops = []\n    for slot in self._flattened_slots:\n      clear_ops.append(\n          self._slot2variable_map[slot].erase(\n              tf.range(self._capacity, dtype=tf.int64)))\n    return tf.group(*clear_ops)\n\n  def clear_rows(self, rows):\n    """"""Returns ops for clearing all the values at the given rows.\n\n    Args:\n      rows: A list/tensor of location(s) to clear values.\n    Returns:\n      Ops for clearing the values at rows.\n    """"""\n    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n    clear_ops = []\n    for spec, slot in zip(self._flattened_specs, self._flattened_slots):\n      new_value = tf.fill([tf.size(input=rows)], _empty_slot(spec))\n      clear_ops.append(\n          self._slot2variable_map[slot].insert_or_assign(rows, new_value))\n    return tf.group(*clear_ops)\n'"
tf_agents/replay_buffers/episodic_table_test.py,62,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.replay_buffers.table.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import episodic_table\n\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\nclass EpisodicTableTest(tf.test.TestCase):\n\n  def default_specs(self):\n    return {\'action\': specs.TensorSpec([3], tf.float32, \'action\'),\n            \'camera\': specs.TensorSpec([5], tf.float32, \'camera\'),\n            \'lidar\': specs.TensorSpec([3, 2], tf.float32, \'lidar\')}\n\n  def np_values(self, spec, batch_size=1):\n    def ones(sp):\n      return np.ones(\n          [batch_size] + sp.shape.as_list(), dtype=sp.dtype.as_numpy_dtype)\n\n    return tf.nest.map_structure(ones, spec)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetAddSingle(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n\n    expected_values = self.np_values(spec)\n\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32),\n        expected_values)\n\n    add_op = replay_table.add([0], tensors)\n    values = replay_table.get_episode_values(0)\n    # Check static shape\n    assert_same_shape = lambda s, v: self.assertEqual(s.shape[1:], v.shape)\n    tf.nest.map_structure(assert_same_shape, values, spec)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_op)\n    values_ = self.evaluate(values)\n    tf.nest.map_structure(self.assertAllClose, expected_values, values_)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetEmpty(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    empty_values = self.np_values(spec, 0)\n    values = replay_table.get_episode_values(0)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    values_ = self.evaluate(values)\n    tf.nest.map_structure(self.assertAllClose, empty_values, values_)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetAddMultiple(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=4)\n\n    batch_size = 2\n    input_values = self.np_values(spec, batch_size)\n    expected_values = self.np_values(spec)\n    empty_values = self.np_values(spec, 0)\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32), input_values)\n\n    write_op = replay_table.add([0, 1], tensors)\n    values_0 = replay_table.get_episode_values(0)\n    values_1 = replay_table.get_episode_values(1)\n    # This should be empty\n    values_2 = replay_table.get_episode_values(2)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_0))\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_1))\n    tf.nest.map_structure(self.assertAllClose, empty_values,\n                          self.evaluate(values_2))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetAddAppendMultiple(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=4)\n\n    batch_size = 2\n    input_values = self.np_values(spec, batch_size)\n    expected_values = self.np_values(spec)\n    empty_values = self.np_values(spec, 0)\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32), input_values)\n\n    # Pull out the first entry in the batch, and add an outer\n    # dimension to represent a single time step that we\'ll append.\n    tensors_batch0 = tf.nest.map_structure(\n        lambda x: tf.expand_dims(x[0, ...], 0), tensors)\n\n    # We will append tensors_batch0 to row 0, which contains x[0].\n    expected_appended_values = tf.nest.map_structure(\n        lambda x: np.stack((x[0], x[0])), input_values)\n\n    # batch_size == 2, so add [0, 1]\n    write_op = replay_table.add([0, 1], tensors)\n    append_op_0 = lambda: replay_table.append(0, tensors_batch0)\n\n    values_0 = lambda: replay_table.get_episode_values(0)\n    values_1 = lambda: replay_table.get_episode_values(1)\n    values_2 = lambda: replay_table.get_episode_values(2)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_0()))\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_1()))\n    tf.nest.map_structure(self.assertAllClose, empty_values,\n                          self.evaluate(values_2()))\n\n    self.evaluate(append_op_0())\n    tf.nest.map_structure(self.assertAllClose, expected_appended_values,\n                          self.evaluate(values_0()))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testExtend(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    test_values = self.np_values(spec, 5)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_table.append(1, test_values))\n    # Extend row 2 by the contents of row 1.\n    self.evaluate(replay_table.extend(2, replay_table.get_episode_lists(1)))\n    # Extend rows 0 and 1 by the contents of rows 1 and 2.\n    self.evaluate(\n        replay_table.extend([0, 1], replay_table.get_episode_lists([1, 2])))\n    episode_0, episode_1, episode_2 = self.evaluate(\n        [replay_table.get_episode_values(r) for r in range(3)])\n    self.assertAllClose(episode_0, self.np_values(spec, 5))\n    self.assertAllClose(episode_1, self.np_values(spec, 10))\n    self.assertAllClose(episode_2, self.np_values(spec, 5))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testClear(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=4)\n\n    batch_size = 2\n    input_values = self.np_values(spec, batch_size)\n    expected_values = self.np_values(spec)\n    empty_values = self.np_values(spec, 0)\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32), input_values)\n\n    write_op = replay_table.add([0, 1], tensors)\n    values_0 = replay_table.get_episode_values(0)\n    values_1 = replay_table.get_episode_values(1)\n    # This should be empty\n    clear_op = replay_table.clear()\n    values_0_after_clear = replay_table.get_episode_values(0)\n    values_1_after_clear = replay_table.get_episode_values(1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_0))\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_1))\n    self.evaluate(clear_op)\n    tf.nest.map_structure(self.assertAllClose, empty_values,\n                          self.evaluate(values_0_after_clear))\n    tf.nest.map_structure(self.assertAllClose, empty_values,\n                          self.evaluate(values_1_after_clear))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testClearRows(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=4)\n\n    batch_size = 2\n    input_values = self.np_values(spec, batch_size)\n    expected_values = self.np_values(spec)\n    empty_values = self.np_values(spec, 0)\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32), input_values)\n\n    write_op = replay_table.add([0, 1], tensors)\n    values_0 = replay_table.get_episode_values(0)\n    values_1 = replay_table.get_episode_values(1)\n    # This should be empty\n    clear_0 = replay_table.clear_rows([0])\n    values_0_after_clear = replay_table.get_episode_values(0)\n    values_1_after_clear = replay_table.get_episode_values(1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_0))\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_1))\n    self.evaluate(clear_0)\n    tf.nest.map_structure(self.assertAllClose, empty_values,\n                          self.evaluate(values_0_after_clear))\n    tf.nest.map_structure(self.assertAllClose, expected_values,\n                          self.evaluate(values_1_after_clear))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetEpisodeListsOneRow(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    test_values = self.np_values(spec, 5)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_table.append(1, test_values))\n    # Get the episode lists we just added. The returned value should look like:\n    # {\'action\': <tensorlist>, \'camera\': <tensorlist>, \'lidar\': <tensorlist>}\n    episode_lists = replay_table.get_episode_lists(1)\n    for episode_list_slot in tf.nest.flatten(episode_lists):\n      self.assertEqual(episode_list_slot.shape.rank, 0)\n    episode_tensors = tf.nest.map_structure(replay_table._stack_tensor_list,\n                                            replay_table.slots, episode_lists)\n    self.assertAllClose(self.evaluate(episode_tensors), test_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetEpisodeListsSomeRows(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    empty_values = self.np_values(spec, 0)\n    test_values = self.np_values(spec, 5)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_table.append(1, test_values))\n    # Get episode lists for rows 1 and 2. The returned values should look like:\n    # {\'action\': [<tensorlist for row 1>, <tensorlist for row 2>],\n    #  \'camera\': [<tensorlist for row 1>, <tensorlist for row 2>],\n    #  \'lidar\': [<tensorlist for row 1>, <tensorlist for row 2>]}\n    episode_lists = replay_table.get_episode_lists([1, 2])\n    for episode_list_slot in tf.nest.flatten(episode_lists):\n      self.assertEqual(episode_list_slot.shape.rank, 1)\n      self.assertEqual(self.evaluate(tf.size(input=episode_list_slot)), 2)\n    # Stack episode tensors for row 1, i.e.:\n    # {\'action\': <five items>, \'camera\': <five items>, \'lidar\': <five items>}\n    episode_tensors_1 = tf.nest.map_structure(\n        lambda slot, lists: replay_table._stack_tensor_list(slot, lists[0]),\n        replay_table.slots, episode_lists)\n    # Should be equivalent to\n    # Stack episode tensors for row 2, which is empty.\n    episode_tensors_2 = tf.nest.map_structure(\n        lambda slot, lists: replay_table._stack_tensor_list(slot, lists[1]),\n        replay_table.slots, episode_lists)\n    self.assertAllClose(self.evaluate(episode_tensors_1), test_values)\n    self.assertAllClose(self.evaluate(episode_tensors_2), empty_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetEpisodeListsAllRows(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    empty_values = self.np_values(spec, 0)\n    test_values = self.np_values(spec, 5)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_table.append(1, test_values))\n    # Get episode lists for all rows. The returned values should look like:\n    # {\'action\': [<tensorlist for row 0>, <tensorlist for row 1>,\n    #             <tensorlist for row 2>],\n    #  ... }\n    self._test_episode_lists(replay_table, empty_values, test_values)\n\n  def _test_episode_lists(self, replay_table, empty_values, test_values):\n    episode_lists = replay_table.get_episode_lists()\n    for episode_list_slot in tf.nest.flatten(episode_lists):\n      self.assertEqual(episode_list_slot.shape.rank, 1)\n      self.assertEqual(self.evaluate(tf.size(input=episode_list_slot)), 3)\n    # Stack each row individually.\n    episode_tensors_0 = tf.nest.map_structure(\n        lambda slot, lists: replay_table._stack_tensor_list(slot, lists[0]),\n        replay_table.slots, episode_lists)\n    episode_tensors_1 = tf.nest.map_structure(\n        lambda slot, lists: replay_table._stack_tensor_list(slot, lists[1]),\n        replay_table.slots, episode_lists)\n    episode_tensors_2 = tf.nest.map_structure(\n        lambda slot, lists: replay_table._stack_tensor_list(slot, lists[2]),\n        replay_table.slots, episode_lists)\n    # Only row 1 should have non-empty values.\n    self.assertAllClose(self.evaluate(episode_tensors_0), empty_values)\n    self.assertAllClose(self.evaluate(episode_tensors_1), test_values)\n    self.assertAllClose(self.evaluate(episode_tensors_2), empty_values)\n\n  def testCheckpoint(self):\n    spec = self.default_specs()\n    replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    empty_values = self.np_values(spec, 0)\n    test_values = self.np_values(spec, 5)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_table.append(1, test_values))\n\n    checkpoint = tf.train.Checkpoint(table=replay_table)\n    tmpdir = self.get_temp_dir()\n    if tf.executing_eagerly():\n      location = checkpoint.save(os.path.join(tmpdir, \'ckpt\'))\n    else:\n      with self.cached_session() as sess:\n        location = checkpoint.save(os.path.join(tmpdir, \'ckpt\'), session=sess)\n    reload_replay_table = episodic_table.EpisodicTable(spec, capacity=3)\n    reload_checkpoint = tf.train.Checkpoint(table=reload_replay_table)\n    status = reload_checkpoint.restore(location)\n    status.assert_consumed()\n    with self.cached_session() as sess:\n      status.initialize_or_restore(session=sess)\n    self._test_episode_lists(replay_table, empty_values, test_values)\n    self._test_episode_lists(reload_replay_table, empty_values, test_values)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/py_hashed_replay_buffer.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Uniform replay buffer in Python with compressed storage.\n\nPyHashedReplayBuffer is a flavor of the base class which\ncompresses the observations when the observations have some partial overlap\n(e.g. when using frame stacking).\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\nimport threading\n\nfrom absl import logging\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.replay_buffers import py_uniform_replay_buffer\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import trajectory\n\n\nclass FrameBuffer(tf.train.experimental.PythonState):\n  """"""Saves some frames in a memory efficient way.\n\n  Thread safety: cannot add multiple frames in parallel.\n  """"""\n\n  def __init__(self):\n    self._frames = {}\n\n  def add_frame(self, frame):\n    """"""Add a frame to the buffer.\n\n    Args:\n      frame: Numpy array.\n\n    Returns:\n      A deduplicated frame.\n    """"""\n    h = hash(frame.tostring())\n    if h in self._frames:\n      _, refcount = self._frames[h]\n      self._frames[h] = (frame, refcount + 1)\n      return h\n    self._frames[h] = (frame, 1)\n    return h\n\n  def __len__(self):\n    return len(self._frames)\n\n  def serialize(self):\n    """"""Callback for `PythonStateWrapper` to serialize the dictionary.""""""\n    return pickle.dumps(self._frames)\n\n  def deserialize(self, string_value):\n    """"""Callback for `PythonStateWrapper` to deserialize the array.""""""\n    self._frames = pickle.loads(string_value)\n\n  def compress(self, observation, split_axis=-1):\n    # e.g. When split_axis is -1, turns an array of size 84x84x4\n    # into a list of arrays of size 84x84x1.\n    frame_list = np.split(observation, observation.shape[split_axis],\n                          split_axis)\n    return np.array([self.add_frame(f) for f in frame_list])\n\n  def decompress(self, observation, split_axis=-1):\n    frames = [self._frames[h][0] for h in observation]\n    return np.concatenate(frames, axis=split_axis)\n\n  def on_delete(self, observation, split_axis=-1):\n    for h in observation:\n      frame, refcount = self._frames[h]\n      if refcount > 1:\n        self._frames[h] = (frame, refcount - 1)\n      else:\n        del self._frames[h]\n\n  def clear(self):\n    self._frames = {}\n\n\nclass PyHashedReplayBuffer(py_uniform_replay_buffer.PyUniformReplayBuffer):\n  """"""A Python-based replay buffer with optimized underlying storage.\n\n  This replay buffer deduplicates data in the stored trajectories along the\n  last axis of the observation, which is useful, e.g., if you are performing\n  something like frame stacking. For example, if each observation is 4 stacked\n  84x84 grayscale images forming a shape [84, 84, 4], then the replay buffer\n  will separate out each of the images and depuplicate across each trajectory\n  in case an image is repeated.\n\n  Note: This replay buffer assumes that the items being stored are\n  trajectory.Trajectory instances.\n  """"""\n\n  def __init__(self, data_spec, capacity, log_interval=None):\n    if not isinstance(data_spec, trajectory.Trajectory):\n      raise ValueError(\n          \'data_spec must be the spec of a trajectory: {}\'.format(data_spec))\n    super(PyHashedReplayBuffer, self).__init__(\n        data_spec, capacity)\n\n    self._frame_buffer = FrameBuffer()\n    self._lock_frame_buffer = threading.Lock()\n    self._log_interval = log_interval\n\n  def _encoded_data_spec(self):\n    observation = self._data_spec.observation\n    observation = array_spec.ArraySpec(\n        shape=(observation.shape[-1],), dtype=np.int64)\n    return self._data_spec._replace(observation=observation)\n\n  def _encode(self, traj):\n    """"""Encodes a trajectory for efficient storage.\n\n    The observations in this trajectory are replaced by a compressed\n    version of the observations: each frame is only stored exactly once.\n\n    Args:\n      traj: The original trajectory.\n\n    Returns:\n      The same trajectory where frames in the observation have been\n      de-duplicated.\n    """"""\n    with self._lock_frame_buffer:\n      observation = self._frame_buffer.compress(traj.observation)\n\n    if (self._log_interval and\n        self._np_state.item_count % self._log_interval == 0):\n      logging.info(\'%s\', \'Effective Replay buffer frame count: {}\'.format(\n          len(self._frame_buffer)))\n\n    return traj._replace(observation=observation)\n\n  def _decode(self, encoded_trajectory):\n    """"""Decodes a trajectory.\n\n    The observation in the trajectory has been compressed so that no frame\n    is present more than once in the replay buffer. Uncompress the observations\n    in this trajectory.\n\n    Args:\n      encoded_trajectory: The compressed version of the trajectory.\n\n    Returns:\n      The original trajectory (uncompressed).\n    """"""\n    observation = self._frame_buffer.decompress(encoded_trajectory.observation)\n    return encoded_trajectory._replace(observation=observation)\n\n  def _on_delete(self, encoded_trajectory):\n    with self._lock_frame_buffer:\n      self._frame_buffer.on_delete(encoded_trajectory.observation)\n\n  def _clear(self):\n    super(PyHashedReplayBuffer, self)._clear()\n    self._frame_buffer.clear()\n'"
tf_agents/replay_buffers/py_replay_buffers_test.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Unit tests for PyUniformReplayBuffer and PyHashedReplayBuffer.""""""\n\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.replay_buffers import py_hashed_replay_buffer\nfrom tf_agents.replay_buffers import py_uniform_replay_buffer\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import nest_utils\n\n\ndef next_dataset_element(test_case, dataset):\n  """"""Utility function to iterate over tf.data.Datasets in both TF 1.x and 2.x.\n\n  TensorFlow 1.x and 2.x have different mechanisms for iterating over elements\n  of a tf.data.Dataset. TensorFlow 1.x would require something like:\n\n  itr = tf.data.Dataset.range(10).make_one_shot_iterator()\n  get_next = itr.get_next()\n  with tf.Session() as sess:\n    for _ in range(10):\n      item = sess.run(get_next)\n      process(item)\n\n  While TensorFlow 2.x enables something simpler like:\n\n  for item in tf.data.Dataset.range(10):\n    process(item)\n\n  That simpler latter form is also available in TensorFlow 1.x when running\n  with eager execution enabled.\n\n  This function accomodates for the differing styles using:\n\n  next_element = next_dataset_element(self, tf.data.Dataset.range(10))\n  for _ in range 10:\n    process(next_element())\n\n  Args:\n    test_case: The tf.test.TestCase object of the test calling this function.\n    dataset: A tf.data.Dataset object.\n\n  Returns:\n    A Python function that returns successive elements from dataset on each call\n    (using test_case.evaluate() in TensorFlow 1.x).\n  """"""\n  if tf.executing_eagerly():\n    itr = iter(dataset)\n    return lambda: next(itr)\n  get_next = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n  return lambda: test_case.evaluate(get_next)\n\n\nclass FrameBufferTest(tf.test.TestCase):\n\n  def testFrameBuffer(self):\n    fb = py_hashed_replay_buffer.FrameBuffer()\n    a = np.random.randint(low=0, high=256, size=[84, 84, 1], dtype=np.uint8)\n    h = fb.add_frame(a)\n    fb.on_delete([h])\n    self.assertEqual(0, len(fb))\n\n    a = np.random.randint(low=0, high=256, size=[84, 84, 1], dtype=np.uint8)\n    b = np.random.randint(low=0, high=256, size=[84, 84, 1], dtype=np.uint8)\n    fb.add_frame(a)\n    h = fb.add_frame(b)\n    fb.on_delete([h])\n    self.assertEqual(1, len(fb))\n\n\nclass PyUniformReplayBufferTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _create_replay_buffer(self, rb_cls):\n    self._stack_count = 4\n    self._single_shape = (15, 15, 1)\n    shape = (15, 15, self._stack_count)\n    observation_spec = array_spec.ArraySpec(shape, np.int32, \'obs\')\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = policy_step.PolicyStep(array_spec.BoundedArraySpec(\n        shape=(), dtype=np.int32, minimum=0, maximum=1, name=\'action\'))\n    self._trajectory_spec = trajectory.from_transition(\n        time_step_spec, action_spec, time_step_spec)\n\n    self._capacity = 32\n    self._replay_buffer = rb_cls(\n        data_spec=self._trajectory_spec, capacity=self._capacity)\n\n  def _fill_replay_buffer(self):\n    # Generate N frames: the value of pixels is the frame index.\n    # The observations will be generated by stacking K frames out of those N,\n    # generating some redundancies between the observations.\n    single_frames = []\n    frame_count = 100\n    for k in range(frame_count):\n      single_frames.append(np.full(self._single_shape, k, dtype=np.int32))\n\n    # Add stack of frames to the replay buffer.\n    time_steps = []\n    for k in range(len(single_frames) - self._stack_count + 1):\n      observation = np.concatenate(single_frames[k:k + self._stack_count],\n                                   axis=-1)\n      time_steps.append(ts.transition(observation, reward=0.0))\n\n    self._transition_count = len(time_steps) - 1\n    dummy_action = policy_step.PolicyStep(np.int32(0))\n    for k in range(self._transition_count):\n      self._replay_buffer.add_batch(nest_utils.batch_nested_array(\n          trajectory.from_transition(\n              time_steps[k], dummy_action, time_steps[k + 1])))\n\n  def _generate_replay_buffer(self, rb_cls):\n    self._create_replay_buffer(rb_cls)\n    self._fill_replay_buffer()\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testEmptyBuffer(self, rb_cls):\n    self._create_replay_buffer(rb_cls=rb_cls)\n    ds = self._replay_buffer.as_dataset()\n    if tf.executing_eagerly():\n      next(iter(ds))\n    else:\n      get_next = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n      self.evaluate(get_next)\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testEmptyBufferBatchSize(self, rb_cls):\n    self._create_replay_buffer(rb_cls=rb_cls)\n    ds = self._replay_buffer.as_dataset(sample_batch_size=2)\n    if tf.executing_eagerly():\n      next(iter(ds))\n    else:\n      get_next = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n      self.evaluate(get_next)\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testEmptyBufferNumSteps(self, rb_cls):\n    self._create_replay_buffer(rb_cls=rb_cls)\n    ds = self._replay_buffer.as_dataset(num_steps=2)\n    if tf.executing_eagerly():\n      next(iter(ds))\n    else:\n      get_next = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n      self.evaluate(get_next)\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testReplayBufferCircular(self, rb_cls):\n    self._generate_replay_buffer(rb_cls=rb_cls)\n\n    # Since data is added in a circular way, we know that frames sampled from\n    # the replay buffer should not have values below a given threshold.\n    ds = self._replay_buffer.as_dataset()\n    next_trajectory = next_dataset_element(self, ds)\n    min_value = self._transition_count - self._capacity\n    for _ in range(200):\n      traj = next_trajectory()\n      self.assertLessEqual(min_value, traj.observation[0, 0, 0])\n      self.assertAllEqual(traj.observation[:, :, 0] + 1,\n                          traj.observation[:, :, 1])\n      self.assertAllEqual(traj.observation[:, :, 0] + 2,\n                          traj.observation[:, :, 2])\n      self.assertAllEqual(traj.observation[:, :, 0] + 3,\n                          traj.observation[:, :, 3])\n\n  def testSampleDoesNotCrossHead(self):\n    np.random.seed(12345)\n\n    data_spec = array_spec.ArraySpec((), np.int32)\n    replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n        data_spec=data_spec, capacity=10)\n\n    # Seed RB with 5 elements to move head to position 5.\n    for _ in range(5):\n      replay_buffer.add_batch(np.array([0]))\n\n    # Fill RB with elements 0-9.\n    for i in range(10):\n      replay_buffer.add_batch(np.array([i]))\n\n    # Sample with num_steps = 2. We should never sample (9, 0) since this is an\n    # invalid transition. With 1000 samples, the probability of sampling (9, 0)\n    # if it were not protected against would be (1 - (9/10)^10000) ~= 1.\n    sample_frequency = [0 for _ in range(10)]\n    for _ in range(10000):\n      (first, second) = replay_buffer.get_next(num_steps=2, time_stacked=False)\n      self.assertNotEqual(np.array(9), first)\n      self.assertNotEqual(np.array(0), second)\n      sample_frequency[first] += 1\n\n    # 0-9 should all have been sampled about 10000/9 ~= 1111. We allow a delta\n    # of 150 off of 1111 -- the chance each sample frequency is within this\n    # range is 99.9998% (computed using the pmf of the binomial distribution).\n    # And since we fix the random seed, this test is repeatable.\n    for i in range(9):\n      self.assertAlmostEqual(10000 / 9, sample_frequency[i], delta=150)\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testSampleBatches(self, rb_cls):\n    self._generate_replay_buffer(rb_cls=rb_cls)\n\n    ds = self._replay_buffer.as_dataset(sample_batch_size=5)\n    next_trajectory = next_dataset_element(self, ds)\n    ds_structure = tf.data.experimental.get_structure(ds)\n    self.assertEqual(list(ds_structure.observation.shape), [5, 15, 15, 4])\n    self.assertEqual(list(ds_structure.action.shape), [5])\n    traj = next_trajectory()\n    self.assertEqual(traj.observation.shape, (5, 15, 15, 4))\n    self.assertEqual(traj.step_type.shape, (5,))\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testSampleBatchesWithNumSteps(self, rb_cls):\n    self._generate_replay_buffer(rb_cls=rb_cls)\n\n    ds = self._replay_buffer.as_dataset(sample_batch_size=5, num_steps=3)\n    ds_structure = tf.data.experimental.get_structure(ds)\n    self.assertEqual(list(ds_structure.observation.shape), [5, 3, 15, 15, 4])\n    self.assertEqual(list(ds_structure.action.shape), [5, 3])\n    next_trajectory = next_dataset_element(self, ds)\n\n    traj = next_trajectory()\n    self.assertEqual(traj.observation.shape, (5, 3, 15, 15, 4))\n    self.assertEqual(traj.action.shape, (5, 3))\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testNumStepsNoBatching(self, rb_cls):\n    self._generate_replay_buffer(rb_cls=rb_cls)\n\n    ds = self._replay_buffer.as_dataset(num_steps=3)\n    ds_structure = tf.data.experimental.get_structure(ds)\n    self.assertEqual(list(ds_structure.observation.shape), [3, 15, 15, 4])\n    self.assertEqual(list(ds_structure.action.shape), [3])\n    next_trajectory = next_dataset_element(self, ds)\n\n    traj = next_trajectory()\n    self.assertEqual(traj.observation.shape, (3, 15, 15, 4))\n    self.assertEqual(traj.action.shape, (3,))\n\n  @parameterized.named_parameters(\n      [(\'WithoutHashing\', py_uniform_replay_buffer.PyUniformReplayBuffer),\n       (\'WithHashing\', py_hashed_replay_buffer.PyHashedReplayBuffer)])\n  def testCheckpointable(self, rb_cls):\n    self._generate_replay_buffer(rb_cls=rb_cls)\n    self.assertEqual(32, self._replay_buffer.size)\n\n    with self.cached_session():\n      directory = self.get_temp_dir()\n      prefix = os.path.join(directory, \'ckpt\')\n      saver = tf.train.Checkpoint(rb=self._replay_buffer)\n      save_path = saver.save(prefix)\n\n      loaded_rb = (\n          rb_cls(data_spec=self._trajectory_spec, capacity=self._capacity))\n      loader = tf.train.Checkpoint(rb=loaded_rb)\n      loader.restore(save_path).initialize_or_restore()\n      self.assertEqual(32, loaded_rb.size)\n\n      # Check that replay buffer contains the same items as before\n      ds = loaded_rb.as_dataset()\n      next_trajectory = next_dataset_element(self, ds)\n      min_value = self._transition_count - self._capacity\n      for _ in range(200):\n        traj = next_trajectory()\n        self.assertLessEqual(min_value, traj.observation[0, 0, 0])\n        self.assertAllEqual(traj.observation[:, :, 0] + 1,\n                            traj.observation[:, :, 1])\n        self.assertAllEqual(traj.observation[:, :, 0] + 2,\n                            traj.observation[:, :, 2])\n        self.assertAllEqual(traj.observation[:, :, 0] + 3,\n                            traj.observation[:, :, 3])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/py_uniform_replay_buffer.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Uniform replay buffer in Python.\n\nThe base class provides all the functionalities of a uniform replay buffer:\n  - add samples in a First In First Out way.\n  - read samples uniformly.\n\nPyHashedReplayBuffer is a flavor of the base class which\ncompresses the observations when the observations have some partial overlap\n(e.g. when using frame stacking).\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport threading\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.replay_buffers import replay_buffer\nfrom tf_agents.specs import array_spec\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import numpy_storage\n\n\nclass PyUniformReplayBuffer(replay_buffer.ReplayBuffer):\n  """"""A Python-based replay buffer that supports uniform sampling.\n\n  Writing and reading to this replay buffer is thread safe.\n\n  This replay buffer can be subclassed to change the encoding used for the\n  underlying storage by overriding _encoded_data_spec, _encode, _decode, and\n  _on_delete.\n  """"""\n\n  def __init__(self, data_spec, capacity):\n    """"""Creates a PyUniformReplayBuffer.\n\n    Args:\n      data_spec: An ArraySpec or a list/tuple/nest of ArraySpecs describing a\n        single item that can be stored in this buffer.\n      capacity: The maximum number of items that can be stored in the buffer.\n    """"""\n    super(PyUniformReplayBuffer, self).__init__(data_spec, capacity)\n\n    self._storage = numpy_storage.NumpyStorage(self._encoded_data_spec(),\n                                               capacity)\n    self._lock = threading.Lock()\n    self._np_state = numpy_storage.NumpyState()\n\n    # Adding elements to the replay buffer is done in a circular way.\n    # Keeps track of the actual size of the replay buffer and the location\n    # where to add new elements.\n    self._np_state.size = np.int64(0)\n    self._np_state.cur_id = np.int64(0)\n\n    # Total number of items that went through the replay buffer.\n    self._np_state.item_count = np.int64(0)\n\n  def _encoded_data_spec(self):\n    """"""Spec of data items after encoding using _encode.""""""\n    return self._data_spec\n\n  def _encode(self, item):\n    """"""Encodes an item (before adding it to the buffer).""""""\n    return item\n\n  def _decode(self, item):\n    """"""Decodes an item.""""""\n    return item\n\n  def _on_delete(self, encoded_item):\n    """"""Do any necessary cleanup.""""""\n    pass\n\n  @property\n  def size(self):\n    return self._np_state.size\n\n  def _num_frames(self):\n    raise NotImplementedError(\n        \'num_frames is not yet implemented in PyUniformReplayBuffer\')\n\n  def _add_batch(self, items):\n    outer_shape = nest_utils.get_outer_array_shape(items, self._data_spec)\n    if outer_shape[0] != 1:\n      raise NotImplementedError(\'PyUniformReplayBuffer only supports a batch \'\n                                \'size of 1, but received `items` with batch \'\n                                \'size {}.\'.format(outer_shape[0]))\n\n    item = nest_utils.unbatch_nested_array(items)\n    with self._lock:\n      if self._np_state.size == self._capacity:\n        # If we are at capacity, we are deleting element cur_id.\n        self._on_delete(self._storage.get(self._np_state.cur_id))\n      self._storage.set(self._np_state.cur_id, self._encode(item))\n      self._np_state.size = np.minimum(self._np_state.size + 1,\n                                       self._capacity)\n      self._np_state.cur_id = (self._np_state.cur_id + 1) % self._capacity\n      self._np_state.item_count += 1\n\n  def _get_next(self,\n                sample_batch_size=None,\n                num_steps=None,\n                time_stacked=True):\n    num_steps_value = num_steps if num_steps is not None else 1\n    def get_single():\n      """"""Gets a single item from the replay buffer.""""""\n      with self._lock:\n        if self._np_state.size <= 0:\n          def empty_item(spec):\n            return np.empty(spec.shape, dtype=spec.dtype)\n          if num_steps is not None:\n            item = [tf.nest.map_structure(empty_item, self.data_spec)\n                    for n in range(num_steps)]\n            if time_stacked:\n              item = nest_utils.stack_nested_arrays(item)\n          else:\n            item = tf.nest.map_structure(empty_item, self.data_spec)\n          return item\n        idx = np.random.randint(self._np_state.size - num_steps_value + 1)\n        if self._np_state.size == self._capacity:\n          # If the buffer is full, add cur_id (head of circular buffer) so that\n          # we sample from the range [cur_id, cur_id + size - num_steps_value].\n          # We will modulo the size below.\n          idx += self._np_state.cur_id\n\n        if num_steps is not None:\n          # TODO(b/120242830): Try getting data from numpy in one shot rather\n          # than num_steps_value.\n          item = [self._decode(self._storage.get((idx + n) % self._capacity))\n                  for n in range(num_steps)]\n        else:\n          item = self._decode(self._storage.get(idx % self._capacity))\n\n      if num_steps is not None and time_stacked:\n        item = nest_utils.stack_nested_arrays(item)\n      return item\n\n    if sample_batch_size is None:\n      return get_single()\n    else:\n      samples = [get_single() for _ in range(sample_batch_size)]\n      return nest_utils.stack_nested_arrays(samples)\n\n  def _as_dataset(self, sample_batch_size=None, num_steps=None,\n                  sequence_preprocess_fn=None, num_parallel_calls=None):\n    if sequence_preprocess_fn is not None:\n      raise NotImplementedError(\'sequence_preprocess_fn is not supported.\')\n    if num_parallel_calls is not None:\n      raise NotImplementedError(\'PyUniformReplayBuffer does not support \'\n                                \'num_parallel_calls (must be None).\')\n\n    data_spec = self._data_spec\n    if sample_batch_size is not None:\n      data_spec = array_spec.add_outer_dims_nest(\n          data_spec, (sample_batch_size,))\n    if num_steps is not None:\n      data_spec = (data_spec,) * num_steps\n    shapes = tuple(s.shape for s in tf.nest.flatten(data_spec))\n    dtypes = tuple(s.dtype for s in tf.nest.flatten(data_spec))\n\n    def generator_fn():\n      while True:\n        if sample_batch_size is not None:\n          batch = [self._get_next(num_steps=num_steps, time_stacked=False)\n                   for _ in range(sample_batch_size)]\n          item = nest_utils.stack_nested_arrays(batch)\n        else:\n          item = self._get_next(num_steps=num_steps, time_stacked=False)\n        yield tuple(tf.nest.flatten(item))\n\n    def time_stack(*structures):\n      time_axis = 0 if sample_batch_size is None else 1\n      return tf.nest.map_structure(\n          lambda *elements: tf.stack(elements, axis=time_axis), *structures)\n\n    ds = tf.data.Dataset.from_generator(\n        generator_fn, dtypes,\n        shapes).map(lambda *items: tf.nest.pack_sequence_as(data_spec, items))\n    if num_steps is not None:\n      return ds.map(time_stack)\n    else:\n      return ds\n\n  def _gather_all(self):\n    data = [self._decode(self._storage.get(idx))\n            for idx in range(self._capacity)]\n    stacked = nest_utils.stack_nested_arrays(data)\n    batched = tf.nest.map_structure(lambda t: np.expand_dims(t, 0), stacked)\n    return batched\n\n  def _clear(self):\n    self._np_state.size = np.int64(0)\n    self._np_state.cur_id = np.int64(0)\n'"
tf_agents/replay_buffers/replay_buffer.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TF-Agents Replay Buffer API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\n\nfrom tensorflow.python.data.util import nest as data_nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\nfrom tensorflow.python.util import deprecation   # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass ReplayBuffer(tf.Module):\n  """"""Abstract base class for TF-Agents replay buffer.\n\n  In eager mode, methods modify the buffer or return values directly. In graph\n  mode, methods return ops that do so when executed.\n  """"""\n\n  def __init__(self, data_spec, capacity, stateful_dataset=False):\n    """"""Initializes the replay buffer.\n\n    Args:\n      data_spec: A spec or a list/tuple/nest of specs describing a single item\n        that can be stored in this buffer\n      capacity: number of elements that the replay buffer can hold.\n      stateful_dataset: whether the dataset contains stateful ops or not.\n    """"""\n    super(ReplayBuffer, self).__init__()\n    common.check_tf1_allowed()\n    self._data_spec = data_spec\n    self._capacity = capacity\n    self._stateful_dataset = stateful_dataset\n\n  @property\n  def data_spec(self):\n    """"""Returns the spec for items in the replay buffer.""""""\n    return self._data_spec\n\n  @property\n  def capacity(self):\n    """"""Returns the capacity of the replay buffer.""""""\n    return self._capacity\n\n  @property\n  def stateful_dataset(self):\n    """"""Returns whether the dataset of the replay buffer has stateful ops.""""""\n    return self._stateful_dataset\n\n  def num_frames(self):\n    """"""Returns the number of frames in the replay buffer.""""""\n    return self._num_frames()\n\n  def add_batch(self, items):\n    """"""Adds a batch of items to the replay buffer.\n\n    Args:\n      items: An item or list/tuple/nest of items to be added to the replay\n        buffer. `items` must match the data_spec of this class, with a\n        batch_size dimension added to the beginning of each tensor/array.\n\n    Returns:\n      Adds `items` to the replay buffer.\n    """"""\n    return self._add_batch(items)\n\n  @deprecation.deprecated(\n      date=None,\n      instructions=(\n          \'Use `as_dataset(..., single_deterministic_pass=False) instead.\'\n      ))\n  def get_next(self, sample_batch_size=None, num_steps=None, time_stacked=True):\n    """"""Returns an item or batch of items from the buffer.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. If None (default), a single item is returned\n        which matches the data_spec of this class (without a batch dimension).\n        Otherwise, a batch of sample_batch_size items is returned, where each\n        tensor in items will have its first dimension equal to sample_batch_size\n        and the rest of the dimensions match the corresponding data_spec. See\n        examples below.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. If None (default), in non-episodic replay buffers, a batch of\n        single items is returned. In episodic buffers, full episodes are\n        returned (note that sample_batch_size must be None in that case).\n        Otherwise, a batch of sub-episodes is returned, where a sub-episode is a\n        sequence of consecutive items in the replay_buffer. The returned tensors\n        will have first dimension equal to sample_batch_size (if\n        sample_batch_size is not None), subsequent dimension equal to num_steps,\n        if time_stacked=True and remaining dimensions which match the data_spec\n        of this class. See examples below.\n      time_stacked: (Optional.) Boolean, when true and num_steps > 1 it returns\n        the items stacked on the time dimension. See examples below for details.\n      Examples of tensor shapes returned: (B = batch size, T = timestep, D =\n        data spec)  get_next(sample_batch_size=None, num_steps=None,\n        time_stacked=True)\n          return shape (non-episodic): [D]\n          return shape (episodic): [T, D] (T = full length of the episode)\n            get_next(sample_batch_size=B, num_steps=None, time_stacked=True)\n          return shape (non-episodic): [B, D]\n          return shape (episodic): Not supported get_next(sample_batch_size=B,\n            num_steps=T, time_stacked=True)\n          return shape: [B, T, D] get_next(sample_batch_size=None, num_steps=T,\n            time_stacked=False)\n          return shape: ([D], [D], ..) T tensors in the tuple\n            get_next(sample_batch_size=B, num_steps=T, time_stacked=False)\n          return shape: ([B, D], [B, D], ..) T tensors in the tuple\n\n    Returns:\n      A 2-tuple containing:\n        - An item or sequence of (optionally batched and stacked) items.\n        - Auxiliary info for the items (i.e. ids, probs).\n    """"""\n    return self._get_next(sample_batch_size, num_steps, time_stacked)\n\n  def as_dataset(self,\n                 sample_batch_size=None,\n                 num_steps=None,\n                 num_parallel_calls=None,\n                 sequence_preprocess_fn=None,\n                 single_deterministic_pass=False):\n    """"""Creates and returns a dataset that returns entries from the buffer.\n\n    A single entry from the dataset is the result of the following pipeline:\n\n      * Sample sequences from the underlying data store\n      * (optionally) Process them with `sequence_preprocess_fn`,\n      * (optionally) Split them into subsequences of length `num_steps`\n      * (optionally) Batch them into batches of size `sample_batch_size`.\n\n    In practice, this pipeline is executed in parallel as much as possible\n    if `num_parallel_calls != 1`.\n\n    Some additional notes:\n\n    If `num_steps is None`, different replay buffers will behave differently.\n    For example, `TFUniformReplayBuffer` will return single time steps without\n    a time dimension.  In contrast, e.g., `EpisodicReplayBuffer` will return\n    full sequences (since each sequence may be an episode of unknown length,\n    the outermost shape dimension will be `None`).\n\n    If `sample_batch_size is None`, no batching is performed; and there is no\n    outer batch dimension in the returned Dataset entries.  This setting\n    is useful with variable episode lengths using e.g. `EpisodicReplayBuffer`,\n    because it allows the user to get full episodes back, and use `tf.data`\n    to build padded or truncated batches themselves.\n\n    If `single_determinsitic_pass == True`, the replay buffer will make\n    every attempt to ensure every time step is visited once and exactly once\n    in a deterministic manner (though true determinism depends on the\n    underlying data store).  Additional work may be done to ensure minibatches\n    do not have multiple rows from the same episode.  In some cases, this\n    may mean arguments like `num_parallel_calls` are ignored.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. If None (default), a single item is returned\n        which matches the data_spec of this class (without a batch dimension).\n        Otherwise, a batch of sample_batch_size items is returned, where each\n        tensor in items will have its first dimension equal to sample_batch_size\n        and the rest of the dimensions match the corresponding data_spec.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. If None (default), a batch of single items is returned.\n        Otherwise, a batch of sub-episodes is returned, where a sub-episode is a\n        sequence of consecutive items in the replay_buffer. The returned tensors\n        will have first dimension equal to sample_batch_size (if\n        sample_batch_size is not None), subsequent dimension equal to num_steps,\n        and remaining dimensions which match the data_spec of this class.\n      num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n        representing the number elements to process in parallel. If not\n        specified, elements will be processed sequentially.\n      sequence_preprocess_fn: (Optional) fn for preprocessing the collected\n        data before it is split into subsequences of length `num_steps`.\n        Defined in `TFAgent.preprocess_sequence`.  Defaults to pass through.\n      single_deterministic_pass: Python boolean.  If `True`, the dataset will\n        return a single deterministic pass through its underlying data.\n\n        **NOTE**: If the buffer is modified while a Dataset iterator is\n        iterating over this data, the iterator may miss any new data or\n        otherwise have subtly invalid data.\n\n    Returns:\n      A dataset of type tf.data.Dataset, elements of which are 2-tuples of:\n\n        - An item or sequence of items or batch thereof\n        - Auxiliary info for the items (i.e. ids, probs).\n\n    Raises:\n      NotImplementedError: If a non-default argument value is not supported.\n      ValueError: If the data spec contains lists that must be converted to\n        tuples.\n    """"""\n    # data_tf.nest.flatten does not flatten python lists, nest.flatten does.\n    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n      raise ValueError(\n          \'Cannot perform gather; data spec contains lists and this conflicts \'\n          \'with gathering operator.  Convert any lists to tuples.  \'\n          \'For example, if your spec looks like [a, b, c], \'\n          \'change it to (a, b, c).  Spec structure is:\\n  {}\'.format(\n              tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec)))\n\n    if single_deterministic_pass:\n      ds = self._single_deterministic_pass_dataset(\n          sample_batch_size=sample_batch_size,\n          num_steps=num_steps,\n          sequence_preprocess_fn=sequence_preprocess_fn,\n          num_parallel_calls=num_parallel_calls)\n    else:\n      ds = self._as_dataset(\n          sample_batch_size=sample_batch_size,\n          num_steps=num_steps,\n          sequence_preprocess_fn=sequence_preprocess_fn,\n          num_parallel_calls=num_parallel_calls)\n\n    if self._stateful_dataset:\n      options = tf.data.Options()\n      if hasattr(options, \'experimental_allow_stateful\'):\n        options.experimental_allow_stateful = True\n        ds = ds.with_options(options)\n    return ds\n\n  @deprecation.deprecated(\n      date=None,\n      instructions=(\n          \'Use `as_dataset(..., single_deterministic_pass=True)` instead.\'\n      ))\n  def gather_all(self):\n    """"""Returns all the items in buffer.\n\n    Returns:\n      Returns all the items currently in the buffer. Returns a tensor\n      of shape [B, T, ...] where B = batch size, T = timesteps,\n      and the remaining shape is the shape spec of the items in the buffer.\n    """"""\n    return self._gather_all()\n\n  def clear(self):\n    """"""Resets the contents of replay buffer.\n\n    Returns:\n      Clears the replay buffer contents.\n    """"""\n    return self._clear()\n\n  # Subclasses must implement these methods.\n  @abc.abstractmethod\n  def _num_frames(self):\n    """"""Returns the number of frames in the replay buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _add_batch(self, items):\n    """"""Adds a batch of items to the replay buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _get_next(self, sample_batch_size, num_steps, time_stacked):\n    """"""Returns an item or batch of items from the buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _as_dataset(self,\n                  sample_batch_size,\n                  num_steps,\n                  sequence_preprocess_fn,\n                  num_parallel_calls):\n    """"""Creates and returns a dataset that returns entries from the buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _single_deterministic_pass_dataset(self,\n                                         sample_batch_size,\n                                         num_steps,\n                                         sequence_preprocess_fn,\n                                         num_parallel_calls):\n    """"""Creates and returns a dataset that returns entries from the buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _gather_all(self):\n    """"""Returns all the items in buffer.""""""\n    raise NotImplementedError\n\n  @abc.abstractmethod\n  def _clear(self):\n    """"""Clears the replay buffer.""""""\n    raise NotImplementedError\n'"
tf_agents/replay_buffers/replay_buffer_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.replay_buffers.replay_buffer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import replay_buffer\n\n\nclass ReplayBufferTestClass(replay_buffer.ReplayBuffer):\n  """"""Basic test for ReplayBuffer subclass.""""""\n\n  pass\n\n\nclass ReplayBufferInitTest(tf.test.TestCase):\n\n  def _data_spec(self):\n    return (\n        specs.TensorSpec([3], tf.float32, \'action\'),\n        (\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        )\n    )\n\n  def testReplayBufferInit(self):\n    spec = self._data_spec()\n    capacity = 10\n    rb = ReplayBufferTestClass(spec, capacity)\n    self.assertEqual(rb.data_spec, spec)\n    self.assertEqual(rb.capacity, capacity)\n\n  def testReplayBufferInitWithStatefulDataset(self):\n    spec = self._data_spec()\n    capacity = 10\n    rb = ReplayBufferTestClass(spec, capacity, stateful_dataset=True)\n    self.assertEqual(rb.data_spec, spec)\n    self.assertEqual(rb.capacity, capacity)\n    self.assertEqual(rb.stateful_dataset, True)\n\n  def testMethods(self):\n    spec = self._data_spec()\n    capacity = 10\n    rb = ReplayBufferTestClass(spec, capacity)\n    with self.assertRaises(NotImplementedError):\n      rb.as_dataset()\n    with self.assertRaises(NotImplementedError):\n      rb.as_dataset(single_deterministic_pass=True)\n    with self.assertRaises(NotImplementedError):\n      rb.get_next()\n    with self.assertRaises(NotImplementedError):\n      rb.add_batch(items=None)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/table.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A tensorflow table stored in tf.Variables.\n\nThe row is the index or location at which the value is saved, and the value is\na nest of Tensors.\n\nThis class is not threadsafe.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\n\n\nclass Table(tf.Module):\n  """"""A table that can store Tensors or nested Tensors.""""""\n\n  def __init__(self, tensor_spec, capacity, scope=\'Table\'):\n    """"""Creates a table.\n\n    Args:\n      tensor_spec: A nest of TensorSpec representing each value that can be\n        stored in the table.\n      capacity: Maximum number of values the table can store.\n      scope: Variable scope for the Table.\n    Raises:\n      ValueError: If the names in tensor_spec are empty or not unique.\n    """"""\n    super(Table, self).__init__(name=scope)\n    self._tensor_spec = tensor_spec\n    self._capacity = capacity\n\n    def _create_unique_slot_name(spec):\n      return tf.compat.v1.get_default_graph().unique_name(spec.name or \'slot\')\n\n    self._slots = tf.nest.map_structure(_create_unique_slot_name,\n                                        self._tensor_spec)\n\n    def _create_storage(spec, slot_name):\n      """"""Create storage for a slot, track it.""""""\n      shape = [self._capacity] + spec.shape.as_list()\n      new_storage = common.create_variable(\n          name=slot_name,\n          initializer=tf.zeros(shape, dtype=spec.dtype),\n          shape=None,\n          dtype=spec.dtype,\n          unique_name=False)\n      return new_storage\n\n    with tf.compat.v1.variable_scope(scope):\n      self._storage = tf.nest.map_structure(_create_storage, self._tensor_spec,\n                                            self._slots)\n\n    self._slot2storage_map = dict(\n        zip(tf.nest.flatten(self._slots), tf.nest.flatten(self._storage)))\n\n  @property\n  def slots(self):\n    return self._slots\n\n  def variables(self):\n    return tf.nest.flatten(self._storage)\n\n  def read(self, rows, slots=None):\n    """"""Returns values for the given rows.\n\n    Args:\n      rows: A scalar/list/tensor of location(s) to read values from. If rows is\n        a scalar, a single value is returned without a batch dimension. If rows\n        is a list of integers or a rank-1 int Tensor a batch of values will be\n        returned with each Tensor having an extra first dimension equal to the\n        length of rows.\n      slots: Optional list/tuple/nest of slots to read from. If None, all\n        tensors at the given rows are retrieved and the return value has the\n        same structure as the tensor_spec. Otherwise, only tensors with names\n        matching the slots are retrieved, and the return value has the same\n        structure as slots.\n\n    Returns:\n      Values at given rows.\n    """"""\n    slots = slots or self._slots\n    flattened_slots = tf.nest.flatten(slots)\n    values = [\n        self._slot2storage_map[slot].sparse_read(rows)\n        for slot in flattened_slots\n    ]\n    return tf.nest.pack_sequence_as(slots, values)\n\n  def write(self, rows, values, slots=None):\n    """"""Returns ops for writing values at the given rows.\n\n    Args:\n      rows: A scalar/list/tensor of location(s) to write values at.\n      values: A nest of Tensors to write. If rows has more than one element,\n        values can have an extra first dimension representing the batch size.\n        Values must have the same structure as the tensor_spec of this class\n        if `slots` is None, otherwise it must have the same structure as\n        `slots`.\n      slots: Optional list/tuple/nest of slots to write. If None, all tensors\n        in the table are updated. Otherwise, only tensors with names matching\n        the slots are updated.\n\n    Returns:\n      Ops for writing values at rows.\n    """"""\n    slots = slots or self._slots\n    flattened_slots = tf.nest.flatten(slots)\n    flattened_values = tf.nest.flatten(values)\n    write_ops = [\n        tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n                                    value).op\n        for (slot, value) in zip(flattened_slots, flattened_values)\n    ]\n    return tf.group(*write_ops)\n'"
tf_agents/replay_buffers/table_test.py,62,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.replay_buffers.table.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import table\n\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass TableTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadWriteSingle(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'camera\'),\n            specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n        ]\n    ]\n    replay_table = table.Table(spec, capacity=3)\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/action:0\', \'Table/camera:0\', \'Table/lidar:0\'],\n                        [v.name for v in variables])\n\n    expected_values = [\n        1 * np.ones(spec[0].shape.as_list()),\n        [2 * np.ones(spec[1][0].shape.as_list()),\n         3 * np.ones(spec[1][1].shape.as_list())]\n    ]\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32),\n        expected_values)\n\n    write_op = replay_table.write(0, tensors)\n    read_op = replay_table.read(0)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadWriteBatch(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'camera\'),\n            specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n        ]\n    ]\n    replay_table = table.Table(spec, capacity=4)\n\n    batch_size = 2\n    expected_values = [\n        1 * np.ones([batch_size] + spec[0].shape.as_list()),\n        [2 * np.ones([batch_size] + spec[1][0].shape.as_list()),\n         3 * np.ones([batch_size] + spec[1][1].shape.as_list())]\n    ]\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32),\n        expected_values)\n\n    write_op = replay_table.write(list(range(batch_size)), tensors)\n    read_op = replay_table.read(list(range(batch_size)))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadPartialSlots(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'camera\'),\n            specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n        ]\n    ]\n    replay_table = table.Table(spec, capacity=4)\n\n    batch_size = 2\n    action = 1 * np.ones([batch_size] + spec[0].shape.as_list())\n    camera = 2 * np.ones([batch_size] + spec[1][0].shape.as_list())\n    lidar = 3 * np.ones([batch_size] + spec[1][1].shape.as_list())\n\n    values = [action, [camera, lidar]]\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32), values)\n\n    write_op = replay_table.write(list(range(batch_size)), tensors)\n    read_op = replay_table.read(\n        list(range(batch_size)), slots=[\'lidar\', [\'action\']])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    expected_values = [lidar, [action]]\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testWritePartialSlots(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([5], tf.float32, \'camera\'),\n            specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n        ]\n    ]\n    replay_table = table.Table(spec, capacity=4)\n\n    batch_size = 2\n\n    action1 = 1 * np.ones([batch_size] + spec[0].shape.as_list())\n    camera1 = 2 * np.ones([batch_size] + spec[1][0].shape.as_list())\n    lidar1 = 3 * np.ones([batch_size] + spec[1][1].shape.as_list())\n    write_op1 = replay_table.write(\n        list(range(batch_size)), [action1, [camera1, lidar1]])\n\n    lidar2 = 10 * np.ones([batch_size] + spec[1][1].shape.as_list())\n    action2 = 20 * np.ones([batch_size] + spec[0].shape.as_list())\n    write_op2 = replay_table.write(\n        list(range(batch_size)), [lidar2, [action2]], [\'lidar\', [\'action\']])\n    read_op = replay_table.read(list(range(batch_size)))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op1)\n    self.evaluate(write_op2)\n    read_value_ = self.evaluate(read_op)\n    expected_values = [action2, [camera1, lidar2]]\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadWriteDict(self):\n    spec = {\n        \'action\': specs.TensorSpec([3], tf.float32, \'action\'),\n        \'camera\': specs.TensorSpec([5], tf.float32, \'camera\'),\n        \'lidar\': specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n    }\n    replay_table = table.Table(spec, capacity=3)\n\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/action:0\', \'Table/camera:0\', \'Table/lidar:0\'],\n                        [v.name for v in variables])\n\n    expected_values = {\n        \'action\': 1 * np.ones(spec[\'action\'].shape.as_list()),\n        \'camera\': 2 * np.ones(spec[\'camera\'].shape.as_list()),\n        \'lidar\': 3 * np.ones(spec[\'lidar\'].shape.as_list())\n    }\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32),\n        expected_values)\n\n    write_op = replay_table.write(0, tensors)\n    read_op = replay_table.read(0)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadWriteNamedTuple(self):\n    # pylint: disable=invalid-name\n    Observation = collections.namedtuple(\'Observation\',\n                                         [\'action\', \'camera\', \'lidar\'])\n    # pylint: enable=invalid-name\n    spec = Observation(\n        action=specs.TensorSpec([3], tf.float32, \'action\'),\n        camera=specs.TensorSpec([5], tf.float32, \'camera\'),\n        lidar=specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n    )\n    replay_table = table.Table(spec, capacity=3)\n\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/action:0\', \'Table/camera:0\', \'Table/lidar:0\'],\n                        [v.name for v in variables])\n\n    expected_values = Observation(\n        action=1 * np.ones(spec.action.shape.as_list()),\n        camera=2 * np.ones(spec.camera.shape.as_list()),\n        lidar=3 * np.ones(spec.lidar.shape.as_list())\n    )\n    tensors = tf.nest.map_structure(\n        lambda x: tf.convert_to_tensor(value=x, dtype=tf.float32),\n        expected_values)\n\n    write_op = replay_table.write(0, tensors)\n    read_op = replay_table.read(0)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    tf.nest.map_structure(self.assertAllClose, read_value_, expected_values)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testEmptySpecNames(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32),\n        specs.TensorSpec([5], tf.float32, \'\'),\n        specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n    ]\n    replay_table = table.Table(spec, capacity=3)\n\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/slot:0\', \'Table/slot_1:0\', \'Table/lidar:0\'],\n                        [v.name for v in variables])\n\n    expected_slots = [\'slot\', \'slot_1\', \'lidar\']\n    self.assertAllEqual(replay_table.slots, expected_slots)\n    tensors = replay_table.read(0, expected_slots)\n    tf.nest.map_structure(lambda x, y: self.assertEqual(x.shape, y.shape), spec,\n                          tensors)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testDuplicateSpecNames(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'lidar\'),\n        specs.TensorSpec([5], tf.float32, \'lidar\'),\n        specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n    ]\n    replay_table = table.Table(spec, capacity=3)\n\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/lidar:0\', \'Table/lidar_1:0\', \'Table/lidar_2:0\'],\n                        [v.name for v in variables])\n\n    expected_slots = [\'lidar\', \'lidar_1\', \'lidar_2\']\n    self.assertAllEqual(replay_table.slots, expected_slots)\n    tensors = replay_table.read(0, expected_slots)\n    tf.nest.map_structure(lambda x, y: self.assertEqual(x.shape, y.shape), spec,\n                          tensors)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testReadWriteString(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32, \'action\'), [\n            specs.TensorSpec([], tf.string, \'camera\'),\n            specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n        ]\n    ]\n    replay_table = table.Table(spec, capacity=3)\n    variables = replay_table.variables()\n    self.assertEqual(3, len(variables))\n    self.assertAllEqual([\'Table/action:0\', \'Table/camera:0\', \'Table/lidar:0\'],\n                        [v.name for v in variables])\n\n    expected_values = [\n        1 * np.ones(spec[0].shape.as_list()),\n        [b\'foo\',\n         3 * np.ones(spec[1][1].shape.as_list())]\n    ]\n    tensors = tf.nest.map_structure(\n        lambda x, dtype: tf.convert_to_tensor(value=x, dtype=dtype),\n        expected_values, [tf.float32, [tf.string, tf.float32]])\n\n    write_op = replay_table.write(0, tensors)\n    read_op = replay_table.read(0)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(write_op)\n    read_value_ = self.evaluate(read_op)\n    self.assertAllClose(read_value_[0], expected_values[0])\n    self.assertEqual(read_value_[1][0], expected_values[1][0])\n    self.assertAllClose(read_value_[1][1], expected_values[1][1])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testSaveRestore(self):\n    spec = [\n        specs.TensorSpec([3], tf.float32),\n        specs.TensorSpec([5], tf.float32, \'lidar\'),\n        specs.TensorSpec([3, 2], tf.float32, \'lidar\')\n    ]\n    replay_table = table.Table(spec, capacity=3)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    directory = self.get_temp_dir()\n    prefix = os.path.join(directory, \'table\')\n    root = tf.train.Checkpoint(table=replay_table)\n    save_path = root.save(prefix)\n    root.restore(save_path).assert_consumed().run_restore_ops()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/replay_buffers/tf_uniform_replay_buffer.py,65,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A batched replay buffer of nests of Tensors which can be sampled uniformly.\n\n- Each add assumes tensors have batch_size as first dimension, and will store\neach element of the batch in an offset segment, so that each batch dimension has\nits own contiguous memory. Within batch segments, behaves as a circular buffer.\n\nThe get_next function returns \'ids\' in addition to the data. This is not really\nneeded for the batched replay buffer, but is returned to be consistent with\nthe API for a priority replay buffer, which needs the ids to update priorities.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.replay_buffers import replay_buffer\nfrom tf_agents.replay_buffers import table\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nBufferInfo = collections.namedtuple(\'BufferInfo\',\n                                    [\'ids\', \'probabilities\'])\n\n\n@gin.configurable\nclass TFUniformReplayBuffer(replay_buffer.ReplayBuffer):\n  """"""A TFUniformReplayBuffer with batched adds and uniform sampling.""""""\n\n  def __init__(self,\n               data_spec,\n               batch_size,\n               max_length=1000,\n               scope=\'TFUniformReplayBuffer\',\n               device=\'cpu:*\',\n               table_fn=table.Table,\n               dataset_drop_remainder=False,\n               dataset_window_shift=None,\n               stateful_dataset=False):\n    """"""Creates a TFUniformReplayBuffer.\n\n    The TFUniformReplayBuffer stores episodes in `B == batch_size` blocks of\n    size `L == max_length`, with total frame capacity\n    `C == L * B`.  Storage looks like:\n\n    ```\n    block1 ep1 frame1\n               frame2\n           ...\n           ep2 frame1\n               frame2\n           ...\n           <L frames total>\n    block2 ep1 frame1\n               frame2\n           ...\n           ep2 frame1\n               frame2\n           ...\n           <L frames total>\n    ...\n    blockB ep1 frame1\n               frame2\n           ...\n           ep2 frame1\n               frame2\n           ...\n           <L frames total>\n    ```\n    Multiple episodes may be stored within a given block, up to `max_length`\n    frames total.  In practice, new episodes will overwrite old ones as the\n    block rolls over its `max_length`.\n\n    Args:\n      data_spec: A TensorSpec or a list/tuple/nest of TensorSpecs describing a\n        single item that can be stored in this buffer.\n      batch_size: Batch dimension of tensors when adding to buffer.\n      max_length: The maximum number of items that can be stored in a single\n        batch segment of the buffer.\n      scope: Scope prefix for variables and ops created by this class.\n      device: A TensorFlow device to place the Variables and ops.\n      table_fn: Function to create tables `table_fn(data_spec, capacity)` that\n        can read/write nested tensors.\n      dataset_drop_remainder: If `True`, then when calling\n        `as_dataset` with arguments `single_deterministic_pass=True` and\n        `sample_batch_size is not None`, the final batch will be dropped if it\n        does not contain exactly `sample_batch_size` items.  This is helpful for\n        static shape inference as the resulting tensors will always have\n        leading dimension `sample_batch_size` instead of `None`.\n      dataset_window_shift: Window shift used when calling `as_dataset` with\n        arguments `single_deterministic_pass=True` and `num_steps is not None`.\n        This determines how the resulting frames are windowed.  If `None`, then\n        there is no overlap created between frames and each frame is seen\n        exactly once.  For example, if `max_length=5`, `num_steps=2`,\n        `sample_batch_size=None`, and `dataset_window_shift=None`, then the\n        datasets returned will have frames `{[0, 1], [2, 3], [4]}`.\n\n        If `dataset_window_shift is not None`, then windows are created with a\n        window overlap of `dataset_window_shift` and you will see each frame up\n        to `num_steps` times.  For example, if `max_length=5`, `num_steps=2`,\n        `sample_batch_size=None`, and `dataset_window_shift=1`, then the\n        datasets returned will have windows of shifted repeated frames:\n        `{[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]}`.\n\n        For more details, see the documentation of `tf.data.Dataset.window`,\n        specifically for the `shift` argument.\n\n        The default behavior is to not overlap frames\n        (`dataset_window_shift=None`) but users often want to see all\n        combinations of frame sequences, in which case `dataset_window_shift=1`\n        is the appropriate value.\n      stateful_dataset: whether the dataset contains stateful ops or not.\n    """"""\n    self._batch_size = batch_size\n    self._max_length = max_length\n    capacity = self._batch_size * self._max_length\n    super(TFUniformReplayBuffer, self).__init__(\n        data_spec, capacity, stateful_dataset)\n\n    self._id_spec = tensor_spec.TensorSpec([], dtype=tf.int64, name=\'id\')\n    self._capacity_value = np.int64(self._capacity)\n    self._batch_offsets = (\n        tf.range(self._batch_size, dtype=tf.int64) * self._max_length)\n    self._scope = scope\n    self._device = device\n    self._table_fn = table_fn\n    self._dataset_drop_remainder = dataset_drop_remainder\n    self._dataset_window_shift = dataset_window_shift\n    with tf.device(self._device), tf.compat.v1.variable_scope(self._scope):\n      self._capacity = tf.constant(capacity, dtype=tf.int64)\n      self._data_table = table_fn(self._data_spec, self._capacity_value)\n      self._id_table = table_fn(self._id_spec, self._capacity_value)\n      self._last_id = common.create_variable(\'last_id\', -1)\n      self._last_id_cs = tf.CriticalSection(name=\'last_id\')\n\n  def variables(self):\n    return (self._data_table.variables() +\n            self._id_table.variables() +\n            [self._last_id])\n\n  @property\n  def device(self):\n    return self._device\n\n  @property\n  def table_fn(self):\n    return self._table_fn\n\n  @property\n  def scope(self):\n    return self._scope\n\n  # Methods defined in ReplayBuffer base class\n\n  def _num_frames(self):\n    num_items_single_batch_segment = self._get_last_id() + 1\n    total_frames = num_items_single_batch_segment * self._batch_size\n    return tf.minimum(total_frames, self._capacity)\n\n  def _add_batch(self, items):\n    """"""Adds a batch of items to the replay buffer.\n\n    Args:\n      items: A tensor or list/tuple/nest of tensors representing a batch of\n      items to be added to the replay buffer. Each element of `items` must match\n      the data_spec of this class. Should be shape [batch_size, data_spec, ...]\n    Returns:\n      An op that adds `items` to the replay buffer.\n    Raises:\n      ValueError: If called more than once.\n    """"""\n    nest_utils.assert_same_structure(items, self._data_spec)\n\n    with tf.device(self._device), tf.name_scope(self._scope):\n      id_ = self._increment_last_id()\n      write_rows = self._get_rows_for_id(id_)\n      write_id_op = self._id_table.write(write_rows, id_)\n      write_data_op = self._data_table.write(write_rows, items)\n      return tf.group(write_id_op, write_data_op)\n\n  def _get_next(self,\n                sample_batch_size=None,\n                num_steps=None,\n                time_stacked=True):\n    """"""Returns an item or batch of items sampled uniformly from the buffer.\n\n    Sample transitions uniformly from replay buffer. When sub-episodes are\n    desired, specify num_steps, although note that for the returned items to\n    truly be sub-episodes also requires that experience collection be\n    single-threaded.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. See get_next() documentation.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. See get_next() documentation.\n      time_stacked: Bool, when true and num_steps > 1 get_next on the buffer\n        would return the items stack on the time dimension. The outputs would be\n        [B, T, ..] if sample_batch_size is given or [T, ..] otherwise.\n    Returns:\n      A 2 tuple, containing:\n        - An item, sequence of items, or batch thereof sampled uniformly\n          from the buffer.\n        - BufferInfo NamedTuple, containing:\n          - The items\' ids.\n          - The sampling probability of each item.\n    Raises:\n      ValueError: if num_steps is bigger than the capacity.\n    """"""\n    with tf.device(self._device), tf.name_scope(self._scope):\n      with tf.name_scope(\'get_next\'):\n        min_val, max_val = _valid_range_ids(\n            self._get_last_id(), self._max_length, num_steps)\n        rows_shape = () if sample_batch_size is None else (sample_batch_size,)\n        assert_nonempty = tf.compat.v1.assert_greater(\n            max_val,\n            min_val,\n            message=\'TFUniformReplayBuffer is empty. Make sure to add items \'\n            \'before sampling the buffer.\')\n        with tf.control_dependencies([assert_nonempty]):\n          num_ids = max_val - min_val\n          probability = tf.cond(\n              pred=tf.equal(num_ids, 0),\n              true_fn=lambda: 0.,\n              false_fn=lambda: 1. / tf.cast(num_ids * self._batch_size,  # pylint: disable=g-long-lambda\n                                            tf.float32))\n          ids = tf.random.uniform(\n              rows_shape, minval=min_val, maxval=max_val, dtype=tf.int64)\n\n        # Move each id sample to a random batch.\n        batch_offsets = tf.random.uniform(\n            rows_shape, minval=0, maxval=self._batch_size, dtype=tf.int64)\n        batch_offsets *= self._max_length\n        ids += batch_offsets\n\n        if num_steps is None:\n          rows_to_get = tf.math.mod(ids, self._capacity)\n          data = self._data_table.read(rows_to_get)\n          data_ids = self._id_table.read(rows_to_get)\n        else:\n          if time_stacked:\n            step_range = tf.range(num_steps, dtype=tf.int64)\n            if sample_batch_size:\n              step_range = tf.reshape(step_range, [1, num_steps])\n              step_range = tf.tile(step_range, [sample_batch_size, 1])\n              ids = tf.tile(tf.expand_dims(ids, -1), [1, num_steps])\n            else:\n              step_range = tf.reshape(step_range, [num_steps])\n\n            rows_to_get = tf.math.mod(step_range + ids, self._capacity)\n            data = self._data_table.read(rows_to_get)\n            data_ids = self._id_table.read(rows_to_get)\n          else:\n            data = []\n            data_ids = []\n            for step in range(num_steps):\n              steps_to_get = tf.math.mod(ids + step, self._capacity)\n              items = self._data_table.read(steps_to_get)\n              data.append(items)\n              data_ids.append(self._id_table.read(steps_to_get))\n            data = tuple(data)\n            data_ids = tuple(data_ids)\n        probabilities = tf.fill(rows_shape, probability)\n\n        buffer_info = BufferInfo(ids=data_ids,\n                                 probabilities=probabilities)\n    return data, buffer_info\n\n  @gin.configurable(\n      \'tf_agents.tf_uniform_replay_buffer.TFUniformReplayBuffer.as_dataset\')\n  def as_dataset(self,\n                 sample_batch_size=None,\n                 num_steps=None,\n                 num_parallel_calls=None,\n                 single_deterministic_pass=False):\n    return super(TFUniformReplayBuffer, self).as_dataset(\n        sample_batch_size, num_steps, num_parallel_calls,\n        single_deterministic_pass=single_deterministic_pass)\n\n  def _as_dataset(self,\n                  sample_batch_size=None,\n                  num_steps=None,\n                  sequence_preprocess_fn=None,\n                  num_parallel_calls=None):\n    """"""Creates a dataset that returns entries from the buffer in shuffled order.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. See as_dataset() documentation.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. See as_dataset() documentation.\n      sequence_preprocess_fn: (Optional.) Preprocessing function for sequences\n        before they are sharded into subsequences of length `num_steps` and\n        batched.\n      num_parallel_calls: (Optional.) Number elements to process in parallel.\n        See as_dataset() documentation.\n\n    Returns:\n      A dataset of type tf.data.Dataset, elements of which are 2-tuples of:\n\n        - An item or sequence of items or batch thereof\n        - Auxiliary info for the items (i.e. ids, probs).\n\n    Raises:\n      NotImplementedError: If `sequence_preprocess_fn != None` is passed in.\n    """"""\n    if sequence_preprocess_fn is not None:\n      raise NotImplementedError(\'sequence_preprocess_fn is not supported.\')\n\n    def get_next(_):\n      return self.get_next(sample_batch_size, num_steps, time_stacked=True)\n\n    dataset = tf.data.experimental.Counter().map(\n        get_next, num_parallel_calls=num_parallel_calls)\n    return dataset\n\n  def _single_deterministic_pass_dataset(self,\n                                         sample_batch_size=None,\n                                         num_steps=None,\n                                         sequence_preprocess_fn=None,\n                                         num_parallel_calls=None):\n    """"""Creates a dataset that returns entries from the buffer in fixed order.\n\n    Args:\n      sample_batch_size: (Optional.) An optional batch_size to specify the\n        number of items to return. See as_dataset() documentation.\n      num_steps: (Optional.)  Optional way to specify that sub-episodes are\n        desired. See as_dataset() documentation.\n      sequence_preprocess_fn: (Optional.) Preprocessing function for sequences\n        before they are sharded into subsequences of length `num_steps` and\n        batched.\n      num_parallel_calls: (Optional.) Number elements to process in parallel.\n        See as_dataset() documentation.\n\n    Returns:\n      A dataset of type tf.data.Dataset, elements of which are 2-tuples of:\n\n        - An item or sequence of items or batch thereof\n        - Auxiliary info for the items (i.e. ids, probs).\n\n    Raises:\n      ValueError: If `dataset_drop_remainder` is set, and\n        `sample_batch_size > self.batch_size`.  In this case all data will\n        be dropped.\n      NotImplementedError: If `sequence_preprocess_fn != None` is passed in.\n    """"""\n    if sequence_preprocess_fn is not None:\n      raise NotImplementedError(\'sequence_preprocess_fn is not supported.\')\n    static_size = tf.get_static_value(sample_batch_size)\n    static_num_steps = tf.get_static_value(num_steps)\n    static_self_batch_size = tf.get_static_value(self._batch_size)\n    static_self_max_length = tf.get_static_value(self._max_length)\n    if (self._dataset_drop_remainder\n        and static_size is not None\n        and static_self_batch_size is not None\n        and static_size > static_self_batch_size):\n      raise ValueError(\n          \'sample_batch_size ({}) > self.batch_size ({}) and \'\n          \'dataset_drop_remainder is True.  In \'\n          \'this case, ALL data will be dropped by the deterministic dataset.\'\n          .format(static_size, static_self_batch_size))\n    if (self._dataset_drop_remainder\n        and static_num_steps is not None\n        and static_self_max_length is not None\n        and static_num_steps > static_self_max_length):\n      raise ValueError(\n          \'num_steps_size ({}) > self.max_length ({}) and \'\n          \'dataset_drop_remainder is True.  In \'\n          \'this case, ALL data will be dropped by the deterministic dataset.\'\n          .format(static_num_steps, static_self_max_length))\n\n    def get_row_ids(_):\n      """"""Passed to Dataset.range(self._batch_size).flat_map(.), gets row ids.""""""\n      with tf.device(self._device), tf.name_scope(self._scope):\n        with tf.name_scope(\'single_deterministic_pass_dataset\'):\n          # Here we pass num_steps=None because _valid_range_ids uses\n          # num_steps to determine a hard stop when sampling num_steps starting\n          # from the returned indices.  But in our case, we want all the indices\n          # and we\'ll use TF dataset\'s window() mechanism to get\n          # num_steps-length blocks.  The window mechanism handles this stuff\n          # for us.\n          min_frame_offset, max_frame_offset = _valid_range_ids(\n              self._get_last_id(), self._max_length, num_steps=None)\n          tf.compat.v1.assert_less(\n              min_frame_offset,\n              max_frame_offset,\n              message=\'TFUniformReplayBuffer is empty. Make sure to add items \'\n              \'before asking the buffer for data.\')\n\n          min_max_frame_range = tf.range(min_frame_offset, max_frame_offset)\n\n          window_shift = self._dataset_window_shift\n          def group_windows(ds_, drop_remainder=self._dataset_drop_remainder):\n            return ds_.batch(num_steps, drop_remainder=drop_remainder)\n\n          if sample_batch_size is None:\n            def row_ids(b):\n              # Create a vector of shape [num_frames] and slice it along each\n              # frame.\n              ids = tf.data.Dataset.from_tensor_slices(\n                  b * self._max_length + min_max_frame_range)\n              if num_steps is not None:\n                ids = (ids.window(num_steps, shift=window_shift)\n                       .flat_map(group_windows))\n              return ids\n            return tf.data.Dataset.range(self._batch_size).flat_map(row_ids)\n          else:\n            def batched_row_ids(batch):\n              # Create a matrix of indices shaped [num_frames, batch_size]\n              # and slice it along each frame row to get groups of batches\n              # for frame 0, frame 1, ...\n              return tf.data.Dataset.from_tensor_slices(\n                  (min_max_frame_range[:, tf.newaxis]\n                   + batch * self._max_length))\n\n            indices_ds = (\n                tf.data.Dataset.range(self._batch_size)\n                .batch(sample_batch_size,\n                       drop_remainder=self._dataset_drop_remainder)\n                .flat_map(batched_row_ids))\n\n            if num_steps is not None:\n              # We have sequences of num_frames rows shaped [sample_batch_size].\n              # Window and group these to rows of shape\n              # [num_steps, sample_batch_size], then\n              # transpose them to get index tensors of shape\n              # [sample_batch_size, num_steps].\n              def group_windows_drop_remainder(d):\n                return group_windows(d, drop_remainder=True)\n\n              indices_ds = (indices_ds.window(num_steps, shift=window_shift)\n                            .flat_map(group_windows_drop_remainder)\n                            .map(tf.transpose))\n\n            return indices_ds\n\n    # Get our indices as a dataset; each time we reinitialize the iterator we\n    # update our min/max id bounds from the state of the replay buffer.\n    ds = tf.data.Dataset.range(1).flat_map(get_row_ids)\n\n    def get_data(id_):\n      with tf.device(self._device), tf.name_scope(self._scope):\n        with tf.name_scope(\'single_deterministic_pass_dataset\'):\n          data = self._data_table.read(id_ % self._capacity)\n      buffer_info = BufferInfo(ids=id_, probabilities=())\n      return (data, buffer_info)\n\n    # Deterministic even though num_parallel_calls > 1.  Operations are\n    # run in parallel but then the results are returned in original stream\n    # order.\n    ds = ds.map(get_data, num_parallel_calls=num_parallel_calls)\n\n    return ds\n\n  def _gather_all(self):\n    """"""Returns all the items in buffer, shape [batch_size, timestep, ...].\n\n    Returns:\n      All the items currently in the buffer.\n    """"""\n    with tf.device(self._device), tf.name_scope(self._scope):\n      with tf.name_scope(\'gather_all\'):\n        # Make ids, repeated over batch_size. Shape [batch_size, num_ids, ...].\n        min_val, max_val = _valid_range_ids(\n            self._get_last_id(), self._max_length)\n        ids = tf.range(min_val, max_val)\n        ids = tf.stack([ids] * self._batch_size)\n        rows = tf.math.mod(ids, self._max_length)\n\n        # Make batch_offsets, shape [batch_size, 1], then add to rows.\n        batch_offsets = tf.expand_dims(\n            tf.range(self._batch_size, dtype=tf.int64) * self._max_length,\n            1)\n        rows += batch_offsets\n\n        # Expected shape is [batch_size, max_length, ...].\n        data = self._data_table.read(rows)\n    return data\n\n  def _clear(self, clear_all_variables=False):\n    """"""Return op that resets the contents of replay buffer.\n\n    Args:\n      clear_all_variables: boolean indicating if all variables should be\n        cleared. By default, table contents will be unlinked from\n        replay buffer, but values are unmodified for efficiency. Set\n        `clear_all_variables=True` to reset all variables including Table\n        contents.\n\n    Returns:\n      op that clears or unlinks the replay buffer contents.\n    """"""\n    table_vars = self._data_table.variables() + self._id_table.variables()\n    def _init_vars():\n      assignments = [self._last_id.assign(-1)]\n      if clear_all_variables:\n        assignments += [v.assign(tf.zeros_like(v)) for v in table_vars]\n      return tf.group(*assignments, name=\'clear\')\n    return self._last_id_cs.execute(_init_vars)\n\n  #  Helper functions.\n  def _increment_last_id(self, increment=1):\n    """"""Increments the last_id in a thread safe manner.\n\n    Args:\n      increment: amount to increment last_id by.\n    Returns:\n      An op that increments the last_id.\n    """"""\n    def _assign_add():\n      return self._last_id.assign_add(increment).value()\n    return self._last_id_cs.execute(_assign_add)\n\n  def _get_last_id(self):\n\n    def last_id():\n      return self._last_id.value()\n\n    return self._last_id_cs.execute(last_id)\n\n  def _get_rows_for_id(self, id_):\n    """"""Make a batch_size length list of tensors, with row ids for write.""""""\n    id_mod = tf.math.mod(id_, self._max_length)\n    rows = self._batch_offsets + id_mod\n    return rows\n\n\ndef _valid_range_ids(last_id, max_length, num_steps=None):\n  """"""Returns the [min_val, max_val) range of ids.\n\n  When num_steps is provided, [min_val, max_val+num_steps) are also valid ids.\n\n  Args:\n    last_id: The last id added to the buffer.\n    max_length: The max length of each batch segment in the buffer.\n    num_steps: Optional way to specify that how many ids need to be valid.\n  Returns:\n    A tuple (min_id, max_id) for the range [min_id, max_id) of valid ids.\n  """"""\n  if num_steps is None:\n    num_steps = tf.constant(1, tf.int64)\n\n  min_id_not_full = tf.constant(0, dtype=tf.int64)\n  max_id_not_full = tf.maximum(last_id + 1 - num_steps + 1, 0)\n\n  min_id_full = last_id + 1 - max_length\n  max_id_full = last_id + 1 - num_steps + 1\n\n  return (tf.where(last_id < max_length, min_id_not_full, min_id_full),\n          tf.where(last_id < max_length, max_id_not_full, max_id_full))\n'"
tf_agents/replay_buffers/tf_uniform_replay_buffer_test.py,77,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_uniform_replay_buffer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents import specs\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\ndef _get_add_op(spec, replay_buffer, batch_size):\n  # TODO(b/68398658) Remove dtypes once scatter_update is fixed.\n  action = tf.constant(1 * np.ones(spec[0].shape.as_list(), dtype=np.float32))\n  lidar = tf.constant(2 * np.ones(spec[1][0].shape.as_list(), dtype=np.float32))\n  camera = tf.constant(\n      3 * np.ones(spec[1][1].shape.as_list(), dtype=np.float32))\n  values = [action, [lidar, camera]]\n  values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n                                         values)\n\n  return values, replay_buffer.add_batch(values_batched)\n\n\nclass TFUniformReplayBufferTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _assertContains(self, list1, list2):\n    self.assertTrue(\n        test_utils.contains(list1, list2), \'%s vs. %s\' % (list1, list2))\n\n  def _assertCircularOrdering(self, expected_order, given_order):\n    for i in range(len(given_order)):\n      self.assertIn(given_order[i], expected_order)\n      if i > 0:\n        prev_idx = expected_order.index(given_order[i - 1])\n        cur_idx = expected_order.index(given_order[i])\n        self.assertEqual(cur_idx, (prev_idx + 1) % len(expected_order))\n\n  def _data_spec(self):\n    return [\n        specs.TensorSpec([3], tf.float32, \'action\'),\n        [\n            specs.TensorSpec([5], tf.float32, \'lidar\'),\n            specs.TensorSpec([3, 2], tf.float32, \'camera\')\n        ]\n    ]\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testAdd(self, batch_size):\n    spec = self._data_spec()\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec,\n        batch_size=batch_size,\n        max_length=10,\n        scope=\'rb{}\'.format(batch_size))\n\n    values, add_op = _get_add_op(spec, replay_buffer, batch_size)\n    sample, _ = replay_buffer.get_next()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_op)\n    sample_ = self.evaluate(sample)\n    values_ = self.evaluate(values)\n    tf.nest.map_structure(self.assertAllClose, values_, sample_)\n\n  def testGetNextEmpty(self):\n    spec = self._data_spec()\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=1, max_length=10)\n\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, \'TFUniformReplayBuffer is empty. Make \'\n        \'sure to add items before sampling the buffer.\'):\n      self.evaluate(tf.compat.v1.global_variables_initializer())\n      sample, _ = replay_buffer.get_next()\n      self.evaluate(sample)\n\n  def testAddSingleSampleBatch(self):\n    batch_size = 1\n    spec = self._data_spec()\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size, max_length=10)\n\n    values, add_op = _get_add_op(spec, replay_buffer, batch_size)\n    sample, _ = replay_buffer.get_next(sample_batch_size=3)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_op)\n    values_ = self.evaluate(values)\n    sample_ = self.evaluate(sample)\n    tf.nest.map_structure(lambda x, y: self._assertContains([x], list(y)),\n                          values_, sample_)\n\n  def testClear(self):\n    batch_size = 1\n    spec = self._data_spec()\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size, max_length=10)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    initial_id = self.evaluate(replay_buffer._get_last_id())\n    empty_items = self.evaluate(replay_buffer.gather_all())\n\n    values, _ = self.evaluate(_get_add_op(spec, replay_buffer, batch_size))\n    sample, _ = self.evaluate(replay_buffer.get_next(sample_batch_size=3))\n    tf.nest.map_structure(lambda x, y: self._assertContains([x], list(y)),\n                          values, sample)\n    self.assertNotEqual(initial_id, self.evaluate(replay_buffer._get_last_id()))\n\n    self.evaluate(replay_buffer.clear())\n    self.assertEqual(initial_id, self.evaluate(replay_buffer._get_last_id()))\n\n    def check_np_arrays_everything_equal(x, y):\n      np.testing.assert_equal(x, y)\n      self.assertEqual(x.dtype, y.dtype)\n\n    tf.nest.map_structure(check_np_arrays_everything_equal, empty_items,\n                          self.evaluate(replay_buffer.gather_all()))\n\n  def testClearAllVariables(self):\n    batch_size = 1\n    spec = self._data_spec()\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size, max_length=10)\n\n    action = tf.constant(1 * np.ones(spec[0].shape.as_list(), dtype=np.float32))\n    lidar = tf.constant(\n        2 * np.ones(spec[1][0].shape.as_list(), dtype=np.float32))\n    camera = tf.constant(\n        3 * np.ones(spec[1][1].shape.as_list(), dtype=np.float32))\n    values = [action, [lidar, camera]]\n    values_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\n                                           values)\n\n    if tf.executing_eagerly():\n      add_op = lambda: replay_buffer.add_batch(values_batched)\n    else:\n      add_op = replay_buffer.add_batch(values_batched)\n\n    def get_table_vars():\n      return [var for var in replay_buffer.variables() if \'Table\' in var.name]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(replay_buffer._clear(clear_all_variables=True))\n    empty_table_vars = self.evaluate(get_table_vars())\n    initial_id = self.evaluate(replay_buffer._get_last_id())\n    empty_items = self.evaluate(replay_buffer.gather_all())\n    self.evaluate(add_op)\n    self.evaluate(add_op)\n    self.evaluate(add_op)\n    self.evaluate(add_op)\n    values_ = self.evaluate(values)\n    sample, _ = self.evaluate(replay_buffer.get_next(sample_batch_size=3))\n    tf.nest.map_structure(lambda x, y: self._assertContains([x], list(y)),\n                          values_, sample)\n    self.assertNotEqual(initial_id, self.evaluate(replay_buffer._get_last_id()))\n\n    tf.nest.map_structure(lambda x, y: self.assertFalse(np.all(x == y)),\n                          empty_table_vars, self.evaluate(get_table_vars()))\n\n    self.evaluate(replay_buffer._clear(clear_all_variables=True))\n    self.assertEqual(initial_id, self.evaluate(replay_buffer._get_last_id()))\n\n    def check_np_arrays_everything_equal(x, y):\n      np.testing.assert_equal(x, y)\n      self.assertEqual(x.dtype, y.dtype)\n\n    tf.nest.map_structure(check_np_arrays_everything_equal, empty_items,\n                          self.evaluate(replay_buffer.gather_all()))\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testMultiStepSampling(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size)\n\n    @common.function(autograph=True)\n    def add_data():\n      for i in tf.range(10, dtype=tf.int64):\n        replay_buffer.add_batch(tf.ones((batch_size,), dtype=tf.int64) * i)\n\n    if tf.executing_eagerly():\n      sample = lambda: replay_buffer.get_next(num_steps=2, time_stacked=False)\n    else:\n      sample = replay_buffer.get_next(\n          num_steps=2, time_stacked=False)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n\n    for _ in range(100):\n      (step_, next_step_), _ = self.evaluate(sample)\n      self.assertEqual((step_ + 1) % 10, next_step_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testMultiStepStackedSampling(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size)\n\n    @common.function(autograph=True)\n    def add_data():\n      for i in tf.range(10, dtype=tf.int64):\n        replay_buffer.add_batch(tf.ones((batch_size,), dtype=tf.int64) * i)\n\n    if tf.executing_eagerly():\n      steps = lambda: replay_buffer.get_next(num_steps=2)[0]\n    else:\n      steps, _ = replay_buffer.get_next(num_steps=2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n    for _ in range(100):\n      steps_ = self.evaluate(steps)\n      self.assertEqual((steps_[0] + 1) % 10, steps_[1])\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testMultiStepStackedBatchedSampling(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size)\n\n    @common.function(autograph=True)\n    def add_data():\n      for i in tf.range(10, dtype=tf.int64):\n        replay_buffer.add_batch(tf.ones((batch_size,), dtype=tf.int64) * i)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n\n    if tf.executing_eagerly():\n      steps = lambda: replay_buffer._get_next(3,  # pylint: disable=g-long-lambda\n                                              num_steps=2,\n                                              time_stacked=True)[0]\n    else:\n      steps, _ = replay_buffer._get_next(3, num_steps=2, time_stacked=True)\n    self.assertEqual(self.evaluate(steps).shape, (3, 2))\n\n    for _ in range(100):\n      steps_ = self.evaluate(steps)\n      self.assertAllEqual((steps_[:, 0] + 1) % 10, steps_[:, 1])\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testGatherAll(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size)\n\n    @common.function(autograph=True)\n    def add_data():\n      for i in tf.range(10, dtype=tf.int64):\n        batch = tf.range(i, i + batch_size, 1, dtype=tf.int64)\n        replay_buffer.add_batch(batch)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n\n    items = replay_buffer.gather_all()\n    expected = [list(range(i, i + 10)) for i in range(0, batch_size)]\n\n    items_ = self.evaluate(items)\n    self.assertAllClose(expected, items_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testGatherAllOverCapacity(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size, max_length=10)\n\n    @common.function(autograph=True)\n    def add_data():\n      # Each element has its batch index in the 100s place.\n      for i in tf.range(15, dtype=tf.int64):\n        batch = tf.range(0, batch_size * 100, 100, dtype=tf.int64) + i\n        replay_buffer.add_batch(batch)\n\n    expected = [\n        list(range(5 + x * 100, 15 + x * 100)) for x in range(batch_size)\n    ]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n    items = replay_buffer.gather_all()\n    items_ = self.evaluate(items)\n    self.assertAllClose(expected, items_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testGatherAllEmpty(self, batch_size):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size)\n\n    items = replay_buffer.gather_all()\n    expected = [[]] * batch_size\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    items_ = self.evaluate(items)\n    self.assertAllClose(expected, items_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testSampleBatchCorrectProbabilities(self, buffer_batch_size):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=buffer_batch_size, max_length=4)\n\n    actions = tf.stack([tf.Variable(0).count_up_to(9)] * buffer_batch_size)\n    sample_batch_size = 2\n\n    @common.function\n    def add(actions):\n      replay_buffer.add_batch(actions)\n\n    @common.function\n    def probabilities():\n      _, buffer_info = replay_buffer.get_next(\n          sample_batch_size=sample_batch_size)\n      return buffer_info.probabilities\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    num_adds = 3\n    for i in range(1, num_adds):\n      self.evaluate(add(actions))\n      expected_probabilities = [1. /\n                                (i * buffer_batch_size)] * sample_batch_size\n      probabilities_ = self.evaluate(probabilities())\n      self.assertAllClose(expected_probabilities, probabilities_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testSampleSingleCorrectProbability(self, buffer_batch_size):\n    max_length = 3\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=buffer_batch_size, max_length=max_length)\n\n    actions = tf.stack([tf.Variable(0).count_up_to(9)] * buffer_batch_size)\n\n    @common.function\n    def add(actions):\n      replay_buffer.add_batch(actions)\n\n    @common.function\n    def probabilities():\n      _, buffer_info = replay_buffer.get_next()\n      return buffer_info.probabilities\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    num_adds = 5\n    for i in range(1, num_adds):\n      self.evaluate(add(actions))\n      probabilities_ = self.evaluate(probabilities())\n      expected_probability = (\n          1. / min(i * buffer_batch_size, max_length * buffer_batch_size))\n      self.assertAllClose(expected_probability, probabilities_)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testSampleSingleCorrectProbabilityAsDataset(self, buffer_batch_size):\n    max_length = 3\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=buffer_batch_size, max_length=max_length)\n\n    actions = tf.stack([tf.Variable(0).count_up_to(9)] * buffer_batch_size)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    ds = replay_buffer.as_dataset()\n    if tf.executing_eagerly():\n      add_op = lambda: replay_buffer.add_batch(actions)\n      itr = iter(ds)\n      sample = lambda: next(itr)\n    else:\n      add_op = replay_buffer.add_batch(actions)\n      itr = tf.compat.v1.data.make_initializable_iterator(ds)\n      self.evaluate(itr.initializer)\n      sample = itr.get_next()\n\n    num_adds = 5\n    for i in range(1, num_adds):\n      self.evaluate(add_op)\n      probabilities_ = self.evaluate(sample)[1].probabilities\n      expected_probability = (\n          1. / min(i * buffer_batch_size, max_length * buffer_batch_size))\n      self.assertAllClose(expected_probability, probabilities_)\n\n  def _create_collect_rb_dataset(\n      self, max_length, buffer_batch_size, num_adds,\n      sample_batch_size, num_steps=None):\n    """"""Create a replay buffer, add items to it, and collect from its dataset.""""""\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=buffer_batch_size, max_length=max_length)\n\n    ds = replay_buffer.as_dataset(\n        single_deterministic_pass=True, sample_batch_size=sample_batch_size,\n        num_steps=num_steps)\n    if tf.executing_eagerly():\n      ix = [0]\n      def add_op():\n        replay_buffer.add_batch(10 * tf.range(buffer_batch_size) + ix[0])\n        ix[0] += 1\n      itr = iter(ds)\n      get_next = lambda: next(itr)\n    else:\n      actions = 10 * tf.range(buffer_batch_size) + tf.Variable(0).count_up_to(9)\n      add_op = replay_buffer.add_batch(actions)\n      itr = tf.compat.v1.data.make_initializable_iterator(ds)\n      get_next = itr.get_next()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    for _ in range(num_adds):\n      # Add 10*range(buffer_batch_size) then 1 + 10*range(buffer_batch_size), ..\n      # The actual episodes are:\n      #   [0, 1, 2, ...],\n      #   [10, 11, 12, ...],\n      #   [20, 21, 22, ...]\n      #   ... (buffer_batch_size of these)\n      self.evaluate(add_op)\n\n    rb_values = []\n    if not tf.executing_eagerly():\n      self.evaluate(itr.initializer)\n    try:\n      while True:\n        rb_values.append(self.evaluate(get_next)[0].tolist())\n    except (tf.errors.OutOfRangeError, StopIteration):\n      pass\n\n    return replay_buffer, rb_values\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testDeterministicAsDataset(self, buffer_batch_size):\n    max_length = 3\n    num_adds = 3\n    unused_rb, rb_values = self._create_collect_rb_dataset(\n        max_length, buffer_batch_size, num_adds, sample_batch_size=None)\n\n    expected = np.hstack(\n        [np.arange(max_length) + 10*i for i in range(buffer_batch_size)])\n    self.assertAllEqual(expected, rb_values)\n\n  def testDeterministicAsDatasetWithNumSteps(self):\n    max_length = 4\n    buffer_batch_size = 5\n    unused_rb, rb_values = self._create_collect_rb_dataset(\n        max_length, buffer_batch_size, num_adds=4,\n        sample_batch_size=None, num_steps=2)\n\n    # Expect to get each episode 2 frames at a time when\n    # num_steps=2 and max_length=4.  Once an episode is finished, move on\n    # to the next episode.\n    expected = np.asarray([\n        # First 2 batches are ep0 frames 0..3.\n        [0, 1],\n        [2, 3],\n        # Next 2 batches are ep1 frames 0..3.\n        [10, 11],\n        [12, 13],\n        # ...\n        [20, 21],\n        [22, 23],\n        [30, 31],\n        [32, 33],\n        [40, 41],\n        [42, 43]])\n    self.assertAllEqual(expected, rb_values)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testDeterministicAsDatasetWithSampleBatch(self, buffer_batch_size):\n    max_length = 3\n    unused_rb, rb_values = self._create_collect_rb_dataset(\n        max_length, buffer_batch_size, num_adds=3,\n        sample_batch_size=buffer_batch_size)\n\n    # Expect to see batches of data in the form:\n    #  [0, 10, 20, ..., 10 * (sample_batch_size - 1)]  # frames 0\n    #  [1, 11, 21, ..., 1 + 10 * (sample_batch_size - 1)]  # frames 1\n    #  [2, 12, 22, ..., 2 + 10 * (sample_batch_size - 1)]  # frames 2\n    # because here, sample_batch_size == buffer_batch_size\n    expected = np.vstack(\n        [10 * np.arange(buffer_batch_size) + i for i in range(max_length)])\n    self.assertAllEqual(expected, rb_values)\n\n  def testDeterministicAsDatasetWithNumStepsAndSampleBatch(self):\n    max_length = 4\n    buffer_batch_size = 6\n    sample_batch_size = 3\n    num_steps = 2\n    unused_rb, rb_values = self._create_collect_rb_dataset(\n        max_length,\n        buffer_batch_size,\n        num_adds=4,\n        sample_batch_size=sample_batch_size,\n        num_steps=num_steps)\n\n    # Expect to get 5 episodes per batch, 2 frames at a time when\n    # num_steps=2, max_length=4, and sample_batch_size=5.\n    # Once an episode batch row is finished, move on to the next episode in that\n    # batch row.\n    expected = np.asarray([\n        # First minibatch out, time steps t=0,1 for eps 0..2.\n        [[0, 1],\n         [10, 11],\n         [20, 21]],\n        # Second minibatch out, time steps t=2,3 for eps 0..2.\n        [[2, 3],\n         [12, 13],\n         [22, 23]],\n        # Third minibatch, time steps t=0,1 for eps 3..5\n        [[30, 31],\n         [40, 41],\n         [50, 51]],\n        # Fourth minibatch, time steps t=2,3 for eps 3..5\n        [[32, 33],\n         [42, 43],\n         [52, 53]]])\n    self.assertAllEqual(expected, rb_values)\n\n  def testDeterministicAsDatasetSampleBatchGreaterThanBufferBatchFails(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=2, max_length=3,\n        # If this isn\'t turned on, then the batching works fine.\n        dataset_drop_remainder=True)\n    with self.assertRaisesRegexp(ValueError, \'ALL data will be dropped\'):\n      replay_buffer.as_dataset(\n          single_deterministic_pass=True, sample_batch_size=3)\n\n  def testDeterministicAsDatasetNumStepsGreaterThanMaxLengthFails(self):\n    spec = specs.TensorSpec([], tf.int32, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=2, max_length=3,\n        # If this isn\'t turned on, then the batching works fine.\n        dataset_drop_remainder=True)\n    with self.assertRaisesRegexp(ValueError, \'ALL data will be dropped\'):\n      replay_buffer.as_dataset(\n          single_deterministic_pass=True, num_steps=4)\n\n  @parameterized.named_parameters(\n      (\'BatchSizeOne\', 1),\n      (\'BatchSizeFive\', 5),\n  )\n  def testNumFrames(self, batch_size):\n    spec = specs.TensorSpec([], tf.int64, \'action\')\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        spec, batch_size=batch_size, max_length=12)\n\n    @common.function(autograph=True)\n    def add_data():\n      for i in tf.range(10, dtype=tf.int64):\n        batch = tf.range(i, i + batch_size, 1, dtype=tf.int64)\n        replay_buffer.add_batch(batch)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(add_data())\n\n    num_frames = replay_buffer.num_frames()\n    num_frames_value = self.evaluate(num_frames)\n    expected = 10 * batch_size\n    self.assertEqual(expected, num_frames_value)\n\n    self.evaluate(add_data())\n    num_frames = replay_buffer.num_frames()\n    num_frames_value = self.evaluate(num_frames)\n    capacity = self.evaluate(replay_buffer._capacity)\n    self.assertEqual(capacity, num_frames_value)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/specs/__init__.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module for numpy array and `tf.Tensor` shape and dtype specifications.""""""\n# TODO(b/130564501): Do not import classes directly, only expose modules.\nfrom tf_agents.specs.array_spec import ArraySpec\nfrom tf_agents.specs.array_spec import BoundedArraySpec\nfrom tf_agents.specs.distribution_spec import DistributionSpec\nfrom tf_agents.specs.tensor_spec import BoundedTensorSpec\nfrom tf_agents.specs.tensor_spec import TensorSpec\n'"
tf_agents/specs/array_spec.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A class to describe the shape and dtype of numpy arrays.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numbers\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef sample_bounded_spec(spec, rng):\n  """"""Samples the given bounded spec.\n\n  Args:\n    spec: A BoundedSpec to sample.\n    rng: A numpy RandomState to use for the sampling.\n\n  Returns:\n    An np.array sample of the requested space.\n  """"""\n  tf_dtype = tf.as_dtype(spec.dtype)\n  low = spec.minimum\n  high = spec.maximum\n\n  if tf_dtype.is_floating:\n    if spec.dtype == np.float64 and np.any(np.isinf(high - low)):\n      # The min-max interval cannot be represented by the np.float64. This is a\n      # problem only for np.float64, np.float32 works as expected.\n      # Spec bounds are set to read only so we can\'t use argumented assignment.\n      low = low / 2  # pylint: disable=g-no-augmented-assignment\n      high = high / 2  # pylint: disable=g-no-augmented-assignment\n    return rng.uniform(\n        low,\n        high,\n        size=spec.shape,\n    ).astype(spec.dtype)\n\n  else:\n    if spec.dtype == np.int64 and np.any(high - low < 0):\n      # The min-max interval cannot be represented by the tf_dtype. This is a\n      # problem only for int64.\n      low = low / 2  # pylint: disable=g-no-augmented-assignment\n      high = high / 2  # pylint: disable=g-no-augmented-assignment\n\n    if np.any(high < tf_dtype.max):\n      high = np.where(high < tf_dtype.max, high + 1, high)  # pylint: disable=g-no-augmented-assignment\n    elif spec.dtype != np.int64 or spec.dtype != np.uint64:\n      # We can still +1 the high if we cast it to the larger dtype.\n      high = high.astype(np.int64) + 1\n\n    if low.size == 1 and high.size == 1:\n      return rng.randint(\n          low,\n          high,\n          size=spec.shape,\n          dtype=spec.dtype,\n      )\n    else:\n      return np.reshape(\n          np.array([\n              rng.randint(low, high, size=1, dtype=spec.dtype)\n              for low, high in zip(low.flatten(), high.flatten())\n          ]), spec.shape)\n\n\ndef sample_spec_nest(structure, rng, outer_dims=()):\n  """"""Samples the given nest of specs.\n\n  Args:\n    structure: An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n    rng: A numpy RandomState to use for the sampling.\n    outer_dims: An optional list/tuple specifying outer dimensions to add to the\n      spec shape before sampling.\n\n  Returns:\n    A nest of sampled values following the ArraySpec definition.\n  """"""\n\n  def sample_fn(spec):\n    spec = BoundedArraySpec.from_spec(spec)\n    spec = BoundedArraySpec(\n        tuple(outer_dims) + tuple(spec.shape), spec.dtype, spec.minimum,\n        spec.maximum, spec.name)\n    return sample_bounded_spec(spec, rng)\n\n  return tf.nest.map_structure(sample_fn, structure)\n\n\ndef check_arrays_nest(arrays, spec):\n  """"""Check that the arrays conform to the spec.\n\n  Args:\n    arrays: A NumPy array, or a nested dict, list or tuple of arrays.\n    spec: An `ArraySpec`, or a nested dict, list or tuple of `ArraySpec`s.\n\n  Returns:\n    True if the arrays conforms to the spec, False otherwise.\n  """"""\n  # Check that arrays and spec has the same structure.\n  try:\n    tf.nest.assert_same_structure(arrays, spec)\n  except (TypeError, ValueError):\n    return False\n\n  def check_array(spec, array):\n    if not isinstance(spec, ArraySpec):\n      return False\n    return spec.check_array(array)\n\n  # Check all the elements in arrays match to their spec\n  checks = tf.nest.map_structure(check_array, spec, arrays)\n  # Only return True if all the checks pass.\n  return all(tf.nest.flatten(checks))\n\n\ndef add_outer_dims_nest(structure, outer_dims):\n  def add_outer_dims(spec):\n    name = spec.name\n    shape = outer_dims + spec.shape\n    if hasattr(spec, \'minimum\') and hasattr(spec, \'maximum\'):\n      return BoundedArraySpec(shape, spec.dtype, spec.minimum,\n                              spec.maximum, name)\n    return ArraySpec(shape, spec.dtype, name=name)\n\n  return tf.nest.map_structure(add_outer_dims, structure)\n\n\n@gin.configurable\nclass ArraySpec(object):\n  """"""Describes a numpy array or scalar shape and dtype.\n\n  An `ArraySpec` allows an API to describe the arrays that it accepts or\n  returns, before that array exists.\n  The equivalent version describing a `tf.Tensor` is `TensorSpec`.\n  """"""\n\n  __hash__ = None\n  __slots__ = (\'_shape\', \'_dtype\', \'_name\')\n\n  def __init__(self, shape, dtype, name=None):\n    """"""Initializes a new `ArraySpec`.\n\n    Args:\n      shape: An iterable specifying the array shape.\n      dtype: numpy dtype or string specifying the array dtype.\n      name: Optional string containing a semantic name for the corresponding\n        array. Defaults to `None`.\n\n    Raises:\n      TypeError: If the shape is not an iterable or if the `dtype` is an invalid\n        numpy dtype.\n    """"""\n    self._shape = tuple(shape)\n    self._dtype = np.dtype(dtype)\n    self._name = name\n\n  @property\n  def shape(self):\n    """"""Returns a `tuple` specifying the array shape.""""""\n    return self._shape\n\n  @property\n  def dtype(self):\n    """"""Returns a numpy dtype specifying the array dtype.""""""\n    return self._dtype\n\n  @property\n  def name(self):\n    """"""Returns the name of the ArraySpec.""""""\n    return self._name\n\n  def __repr__(self):\n    return \'ArraySpec(shape={}, dtype={}, name={})\'.format(\n        self.shape, repr(self.dtype), repr(self.name))\n\n  def __eq__(self, other):\n    """"""Checks if the shape and dtype of two specs are equal.""""""\n    if not isinstance(other, ArraySpec):\n      return False\n    return self.shape == other.shape and self.dtype == other.dtype\n\n  def __ne__(self, other):\n    return not self == other\n\n  def check_array(self, array):\n    """"""Return whether the given NumPy array conforms to the spec.\n\n    Args:\n      array: A NumPy array or a scalar. Tuples and lists will not be converted\n        to a NumPy array automatically; they will cause this function to return\n        false, even if a conversion to a conforming array is trivial.\n\n    Returns:\n      True if the array conforms to the spec, False otherwise.\n    """"""\n    if isinstance(array, np.ndarray):\n      return self.shape == array.shape and self.dtype == array.dtype\n    elif isinstance(array, numbers.Number):\n      return self.shape == tuple() and self.dtype == np.dtype(type(array))\n    else:\n      return False\n\n  @staticmethod\n  def from_array(array, name=None):\n    """"""Construct a spec from the given array or number.""""""\n    if isinstance(array, np.ndarray):\n      return ArraySpec(array.shape, array.dtype, name)\n    elif isinstance(array, numbers.Number):\n      return ArraySpec(tuple(), type(array), name)\n    else:\n      raise ValueError(\'Array must be a np.ndarray or number. Got %r.\' % array)\n\n  @staticmethod\n  def from_spec(spec):\n    """"""Construct a spec from the given spec.""""""\n    return ArraySpec(spec.shape, spec.dtype, spec.name)\n\n  def replace(self, shape=None, dtype=None, name=None):\n    shape = self.shape if shape is None else shape\n    dtype = self.dtype if dtype is None else dtype\n    name = self.name if name is None else name\n    return ArraySpec(shape, dtype, name)\n\n\n@gin.configurable\nclass BoundedArraySpec(ArraySpec):\n  """"""An `ArraySpec` that specifies minimum and maximum values.\n\n  Example usage:\n  ```python\n  # Specifying the same minimum and maximum for every element.\n  spec = BoundedArraySpec((3, 4), np.float64, minimum=0.0, maximum=1.0)\n\n  # Specifying a different minimum and maximum for each element.\n  spec = BoundedArraySpec(\n      (2,), np.float64, minimum=[0.1, 0.2], maximum=[0.9, 0.9])\n\n  # Specifying the same minimum and a different maximum for each element.\n  spec = BoundedArraySpec(\n      (3,), np.float64, minimum=-10.0, maximum=[4.0, 5.0, 3.0])\n  ```\n\n  Bounds are meant to be inclusive. This is especially important for\n  integer types. The following spec will be satisfied by arrays\n  with values in the set {0, 1, 2}:\n  ```python\n  spec = BoundedArraySpec((3, 4), np.int, minimum=0, maximum=2)\n  ```\n  """"""\n\n  __hash__ = None\n  __slots__ = (\'_minimum\', \'_maximum\')\n\n  def __init__(self, shape, dtype, minimum=None, maximum=None, name=None):\n    """"""Initializes a new `BoundedArraySpec`.\n\n    Args:\n      shape: An iterable specifying the array shape.\n      dtype: numpy dtype or string specifying the array dtype.\n      minimum: Number or sequence specifying the maximum element bounds\n        (inclusive). Must be broadcastable to `shape`.\n      maximum: Number or sequence specifying the maximum element bounds\n        (inclusive). Must be broadcastable to `shape`.\n      name: Optional string containing a semantic name for the corresponding\n        array. Defaults to `None`.\n\n    Raises:\n      ValueError: If `minimum` or `maximum` are not broadcastable to `shape` or\n        if the limits are outside of the range of the specified dtype.\n      TypeError: If the shape is not an iterable or if the `dtype` is an invalid\n        numpy dtype.\n    """"""\n    super(BoundedArraySpec, self).__init__(shape, dtype, name)\n\n    try:\n      np.broadcast_to(minimum, shape=shape)\n    except ValueError as numpy_exception:\n      raise ValueError(\'minimum is not compatible with shape. \'\n                       \'Message: {!r}.\'.format(numpy_exception))\n\n    try:\n      np.broadcast_to(maximum, shape=shape)\n    except ValueError as numpy_exception:\n      raise ValueError(\'maximum is not compatible with shape. \'\n                       \'Message: {!r}.\'.format(numpy_exception))\n\n    tf_dtype = tf.as_dtype(self._dtype)\n    low = tf_dtype.min\n    high = tf_dtype.max\n\n    if minimum is None:\n      minimum = low\n    if maximum is None:\n      maximum = high\n\n    self._minimum = np.array(minimum)\n    self._maximum = np.array(maximum)\n\n    if tf_dtype.is_floating:\n      # Replacing infinities with extreme finite float values.\n      self._minimum[self._minimum == -np.inf] = low\n      self._minimum[self._minimum == np.inf] = high\n\n      self._maximum[self._maximum == -np.inf] = low\n      self._maximum[self._maximum == np.inf] = high\n\n    if np.any(self._minimum > self._maximum):\n      raise ValueError(\n          \'Spec bounds min has values greater than max: [{},{}]\'.format(\n              self._minimum, self._maximum))\n    if (np.any(self._minimum < low) or np.any(self._minimum > high) or\n        np.any(self._maximum < low) or np.any(self._maximum > high)):\n      raise ValueError(\n          \'Spec bounds [{},{}] not within the range [{}, {}] of the given \'\n          \'dtype ({})\'.format(self._minimum, self._maximum, low, high,\n                              self._dtype))\n\n    self._minimum = self._minimum.astype(self._dtype)\n    self._minimum.setflags(write=False)\n\n    self._maximum = self._maximum.astype(self._dtype)\n    self._maximum.setflags(write=False)\n\n  @classmethod\n  def from_spec(cls, spec, name=None):\n    if name is None:\n      name = spec.name\n\n    if hasattr(spec, \'minimum\') and hasattr(spec, \'maximum\'):\n      return BoundedArraySpec(spec.shape, spec.dtype, spec.minimum,\n                              spec.maximum, name)\n\n    return BoundedArraySpec(spec.shape, spec.dtype, name=name)\n\n  @property\n  def minimum(self):\n    """"""Returns a NumPy array specifying the minimum bounds (inclusive).""""""\n    return self._minimum\n\n  @property\n  def maximum(self):\n    """"""Returns a NumPy array specifying the maximum bounds (inclusive).""""""\n    return self._maximum\n\n  @property\n  def num_values(self):\n    """"""Returns the number of values for discrete BoundedArraySpec.""""""\n    if is_discrete(self):\n      return (np.broadcast_to(self.maximum, shape=self.shape) -\n              np.broadcast_to(self.minimum, shape=self.shape) + 1)\n\n  def __repr__(self):\n    template = (\'BoundedArraySpec(shape={}, dtype={}, name={}, \'\n                \'minimum={}, maximum={})\')\n    return template.format(self.shape, repr(self.dtype), repr(self.name),\n                           self._minimum, self._maximum)\n\n  def __eq__(self, other):\n    if not isinstance(other, BoundedArraySpec):\n      return False\n    return (super(BoundedArraySpec, self).__eq__(other) and\n            (self.minimum == other.minimum).all() and\n            (self.maximum == other.maximum).all())\n\n  def check_array(self, array):\n    """"""Return true if the given array conforms to the spec.""""""\n    return (super(BoundedArraySpec, self).check_array(array) and\n            np.all(array >= self.minimum) and np.all(array <= self.maximum))\n\n  def replace(self, shape=None, dtype=None,\n              minimum=None, maximum=None,\n              name=None):\n    shape = self.shape if shape is None else shape\n    dtype = self.dtype if dtype is None else dtype\n    minimum = self.minimum if minimum is None else minimum\n    maximum = self.maximum if maximum is None else maximum\n    name = self.name if name is None else name\n    return BoundedArraySpec(shape, dtype, minimum, maximum, name)\n\n\ndef is_bounded(spec):\n  return isinstance(spec, BoundedArraySpec)\n\n\ndef is_discrete(spec):\n  return np.issubdtype(spec.dtype, np.integer)\n\n\ndef is_continuous(spec):\n  return np.issubdtype(spec.dtype, np.float)\n'"
tf_agents/specs/array_spec_test.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.environments.specs.array_spec.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\n\n\nTYPE_PARAMETERS = (\n    (""np.int8"", np.int8),\n    (""np.int16"", np.int16),\n    (""np.int32"", np.int32),\n    (""np.int64"", np.int64),\n    (""np.float16"", np.float16),\n    (""np.float32"", np.float32),\n    (""np.float64"", np.float64),\n    (""python float"", float),\n    (""python int"", int),\n)\n\n\ndef example_basic_spec():\n  return array_spec.ArraySpec((1,), np.int64)\n\n\ndef example_nested_spec(dtype):\n  """"""Return an example nested array spec.""""""\n  return {\n      ""array_spec_1"":\n          array_spec.ArraySpec((2, 3), dtype),\n      ""bounded_spec_1"":\n          array_spec.BoundedArraySpec((2, 3), dtype, -10, 10),\n      ""dict_spec"": {\n          ""array_spec_2"": array_spec.ArraySpec((2, 3), dtype),\n          ""bounded_spec_2"": array_spec.BoundedArraySpec((2, 3), dtype, -10, 10)\n      },\n      ""tuple_spec"": (\n          array_spec.ArraySpec((2, 3), dtype),\n          array_spec.BoundedArraySpec((2, 3), dtype, -10, 10),\n      ),\n      ""list_spec"": [\n          array_spec.ArraySpec((2, 3), dtype),\n          (array_spec.ArraySpec((2, 3), dtype),\n           array_spec.BoundedArraySpec((2, 3), dtype, -10, 10)),\n      ],\n  }\n\n\n# These parameters will be used with every test* method in this class.\n@parameterized.named_parameters(*TYPE_PARAMETERS)\nclass ArraySpecNestSampleTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    self.rng = np.random.RandomState()\n    return super(ArraySpecNestSampleTest, self).setUp()\n\n  def testArraySpecSample(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype)\n    sample = array_spec.sample_spec_nest(spec, self.rng)\n\n    bounded = array_spec.BoundedArraySpec.from_spec(spec)\n    self.assertTrue(np.all(sample >= bounded.minimum))\n    self.assertTrue(np.all(sample <= bounded.maximum))\n\n  def testArraySpecSampleWithName(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype, name=""test_spec"")\n    sample = array_spec.sample_spec_nest(spec, self.rng)\n\n    bounded = array_spec.BoundedArraySpec.from_spec(spec)\n    self.assertTrue(np.all(sample >= bounded.minimum))\n    self.assertTrue(np.all(sample <= bounded.maximum))\n    self.assertEqual(""test_spec"", bounded.name)\n\n  def testBoundedArraySpecSample(self, dtype):\n    spec = array_spec.BoundedArraySpec((2, 3), dtype, -10, 10)\n    sample = array_spec.sample_spec_nest(spec, self.rng)\n    self.assertTrue(np.all(sample >= -10))\n    self.assertTrue(np.all(sample <= 10))\n\n  def testBoundedArraySpecSampleMultipleBounds(self, dtype):\n    spec = array_spec.BoundedArraySpec((2,), dtype, [-10, 1], [10, 3])\n    sample = array_spec.sample_spec_nest(spec, self.rng)\n    self.assertGreaterEqual(sample[0], -10)\n    self.assertLessEqual(sample[0], 10)\n    self.assertGreaterEqual(sample[1], 1)\n    self.assertLessEqual(sample[1], 3)\n\n  def testBoundedArraySpecNoBounds(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype)\n    bounded_spec = array_spec.BoundedArraySpec.from_spec(spec)\n    sample = array_spec.sample_spec_nest(bounded_spec, self.rng)\n    tf_dtype = tf.as_dtype(spec.dtype)\n    self.assertTrue(np.all(sample >= tf_dtype.min))\n    self.assertTrue(np.all(sample <= tf_dtype.max))\n\n  def testSampleTensorBoundedSpecFromArraySpecNoBounds(self, dtype):\n    if dtype in [int, float]:\n      return\n\n    tf_dtype = tf.as_dtype(dtype)\n\n    # Skip unsupported random_ops dtypes.\n    # TODO(b/68706911): Add tf.float16 once bug is fixed.\n    if tf_dtype not in (tf.bfloat16, tf.float32, tf.float64, tf.int32,\n                        tf.int64):\n      return\n\n    spec = array_spec.ArraySpec((2, 3), dtype)\n    bounded_spec = array_spec.BoundedArraySpec.from_spec(spec)\n\n    t_spec = tensor_spec.BoundedTensorSpec.from_spec(bounded_spec)\n    sample = tensor_spec.sample_spec_nest(t_spec)\n    bounded = tensor_spec.BoundedTensorSpec.from_spec(t_spec)\n\n    sample_ = self.evaluate(sample)\n    self.assertTrue(\n        np.all(sample_ >= bounded.minimum), (sample_.min(), sample_.max()))\n    self.assertTrue(\n        np.all(sample_ <= bounded.maximum), (sample_.min(), sample_.max()))\n\n  def testNestSample(self, dtype):\n    spec = example_nested_spec(dtype)\n    sample = array_spec.sample_spec_nest(spec, self.rng)\n\n    bounded = array_spec.BoundedArraySpec.from_spec(spec[""array_spec_1""])\n    self.assertTrue(np.all(sample[""array_spec_1""] >= bounded.minimum))\n    self.assertTrue(np.all(sample[""array_spec_1""] <= bounded.maximum))\n\n    self.assertTrue(np.all(sample[""bounded_spec_1""] >= -10))\n    self.assertTrue(np.all(sample[""bounded_spec_1""] <= 10))\n\n    self.assertIn(""array_spec_2"", sample[""dict_spec""])\n    self.assertIn(""bounded_spec_2"", sample[""dict_spec""])\n\n    self.assertIn(""tuple_spec"", sample)\n\n    self.assertIn(""list_spec"", sample)\n    self.assertTrue(np.all(sample[""list_spec""][1][1] >= -10))\n    self.assertTrue(np.all(sample[""list_spec""][1][1] <= 10))\n\n  def testNestSampleOuterDims(self, dtype):\n    spec = example_nested_spec(dtype)\n    outer_dims = [2, 3]\n    sample = array_spec.sample_spec_nest(\n        spec, self.rng, outer_dims=outer_dims)\n\n    bounded = array_spec.BoundedArraySpec.from_spec(spec[""array_spec_1""])\n    self.assertTrue(np.all(sample[""array_spec_1""] >= bounded.minimum))\n    self.assertTrue(np.all(sample[""array_spec_1""] <= bounded.maximum))\n\n    self.assertTrue(np.all(sample[""bounded_spec_1""] >= -10))\n    self.assertTrue(np.all(sample[""bounded_spec_1""] <= 10))\n\n    self.assertIn(""array_spec_2"", sample[""dict_spec""])\n    self.assertIn(""bounded_spec_2"", sample[""dict_spec""])\n\n    self.assertIn(""tuple_spec"", sample)\n\n    self.assertIn(""list_spec"", sample)\n    self.assertTrue(np.all(sample[""list_spec""][1][1] >= -10))\n    self.assertTrue(np.all(sample[""list_spec""][1][1] <= 10))\n\n    def _test_batched_shape(sample_, spec_):\n      self.assertSequenceEqual(sample_.shape, outer_dims + list(spec_.shape))\n\n    tf.nest.map_structure(_test_batched_shape, sample, spec)\n\n\nclass CheckArraysNestTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def testMatch(self, dtype):\n    spec = example_nested_spec(dtype)\n    sample = array_spec.sample_spec_nest(spec, np.random.RandomState())\n    self.assertTrue(array_spec.check_arrays_nest(sample, spec))\n\n  @parameterized.named_parameters(\n      (""different keys"", {""foo"": np.array([1])}, {""bar"": example_basic_spec()}),\n      (""different types 1"", {""foo"": np.array([1])}, [example_basic_spec()]),\n      (""different types 2"", [np.array([1])], {""foo"": example_basic_spec()}),\n      (""different lengths"", [np.array([1])], [example_basic_spec(),\n                                              example_basic_spec()]),\n      (""array mismatch 1"",\n       {""foo"": np.array([1, 2])}, {""foo"": example_basic_spec()}),\n      (""array mismatch 2"", [np.array([1, 2])], [example_basic_spec()]),\n      (""not an array"", ""a string"", example_basic_spec()),\n      (""not a spec"", np.array([1]), ""a string""),\n  )\n  def testNoMatch(self, arrays, spec):\n    self.assertFalse(array_spec.check_arrays_nest(arrays, spec))\n\n\nclass ArraySpecTest(parameterized.TestCase):\n\n  def testShapeTypeError(self):\n    with self.assertRaises(TypeError):\n      array_spec.ArraySpec(32, np.int32)\n\n  def testDtypeTypeError(self):\n    with self.assertRaises(TypeError):\n      array_spec.ArraySpec((1, 2, 3), ""32"")\n\n  def testStringDtype(self):\n    array_spec.ArraySpec((1, 2, 3), ""int32"")\n\n  def testNumpyDtype(self):\n    array_spec.ArraySpec((1, 2, 3), np.int32)\n\n  def testDtype(self):\n    spec = array_spec.ArraySpec((1, 2, 3), np.int32)\n    self.assertEqual(np.int32, spec.dtype)\n\n  def testShape(self):\n    spec = array_spec.ArraySpec([1, 2, 3], np.int32)\n    self.assertEqual((1, 2, 3), spec.shape)\n\n  def testEqual(self):\n    spec_1 = array_spec.ArraySpec((1, 2, 3), np.int32)\n    spec_2 = array_spec.ArraySpec((1, 2, 3), np.int32)\n    self.assertEqual(spec_1, spec_2)\n\n  def testNotEqualDifferentShape(self):\n    spec_1 = array_spec.ArraySpec((1, 2, 3), np.int32)\n    spec_2 = array_spec.ArraySpec((1, 3, 3), np.int32)\n    self.assertNotEqual(spec_1, spec_2)\n\n  def testNotEqualDifferentDtype(self):\n    spec_1 = array_spec.ArraySpec((1, 2, 3), np.int64)\n    spec_2 = array_spec.ArraySpec((1, 2, 3), np.int32)\n    self.assertNotEqual(spec_1, spec_2)\n\n  def testNotEqualOtherClass(self):\n    spec_1 = array_spec.ArraySpec((1, 2, 3), np.int32)\n    spec_2 = None\n    self.assertNotEqual(spec_1, spec_2)\n    self.assertNotEqual(spec_2, spec_1)\n\n    spec_2 = ()\n    self.assertNotEqual(spec_1, spec_2)\n    self.assertNotEqual(spec_2, spec_1)\n\n  def testFromArray(self):\n    spec = array_spec.ArraySpec.from_array(np.array([1, 2]), ""test"")\n    self.assertEqual(spec.shape, (2,))\n    self.assertEqual(spec.dtype, np.int64)\n    self.assertEqual(spec.name, ""test"")\n\n  def testFromArrayWithScalar(self):\n    spec = array_spec.ArraySpec.from_array(5, ""test"")\n    self.assertEqual(spec.shape, tuple())\n    self.assertEqual(spec.dtype, np.int64)\n    self.assertEqual(spec.name, ""test"")\n\n  def testFromArrayWithNonNumeric(self):\n    self.assertRaises(ValueError, array_spec.ArraySpec.from_array, ""a string"")\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def testCheckArrayMatch(self, dtype):\n    spec = array_spec.ArraySpec((2,), dtype)\n    self.assertTrue(spec.check_array(np.array([1, 2], dtype)))\n\n  def testCheckArrayMatchWithScalar(self):\n    spec = array_spec.ArraySpec(tuple(), np.double)\n    self.assertTrue(spec.check_array(5.0))\n\n  @parameterized.named_parameters(\n      (""wrong shape"", np.array([1])),\n      (""wrong dtype"", np.array([1, 2], dtype=np.double)),\n      (""not an array"", ""a string""))\n  def testCheckArrayNoMatch(self, array):\n    spec = array_spec.ArraySpec((2,), np.int64)\n    self.assertFalse(spec.check_array(array))\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def testReplaceDtype(self, dtype):\n    spec = array_spec.ArraySpec(tuple(), np.double).replace(dtype=dtype)\n    self.assertEqual(spec.dtype, dtype)\n\n  def testReplace(self):\n    spec = array_spec.ArraySpec(tuple(), np.double)\n    new_spec = spec.replace(shape=(2,))\n    self.assertEqual(new_spec.shape, (2,))\n    new_spec = new_spec.replace(dtype=np.int8)\n    self.assertEqual(new_spec.dtype, np.int8)\n    new_spec = new_spec.replace(name=""name"")\n    self.assertEqual(new_spec.name, ""name"")\n    exp_spec = array_spec.ArraySpec((2,), np.int8, name=""name"")\n    self.assertEqual(exp_spec, new_spec)\n\n\nclass BoundedArraySpecTest(parameterized.TestCase):\n\n  def testInvalidMinimum(self):\n    with self.assertRaisesRegexp(ValueError, ""not compatible""):\n      array_spec.BoundedArraySpec((3, 5), np.uint8, (0, 0, 0), (1, 1))\n\n  def testInvalidMaximum(self):\n    with self.assertRaisesRegexp(ValueError, ""not compatible""):\n      array_spec.BoundedArraySpec((3, 5), np.uint8, 0, (1, 1, 1))\n\n  def testMinLargerThanMax(self):\n    with self.assertRaisesRegexp(ValueError, ""min has values greater than max""):\n      array_spec.BoundedArraySpec((3,), np.uint8, (1, 2, 3), (3, 2, 1))\n\n  def testHandleInfLimits(self):\n    spec = array_spec.BoundedArraySpec(\n        (1, 2, 3),\n        np.float32,\n        (-np.inf, 5, -np.inf),\n        (np.inf, 5, np.inf),\n    )\n    self.assertNotIn(np.inf, spec.minimum)\n    self.assertNotIn(-np.inf, spec.minimum)\n\n    self.assertNotIn(np.inf, spec.maximum)\n    self.assertNotIn(-np.inf, spec.maximum)\n\n    self.assertEqual(5, spec.minimum[1])\n    self.assertEqual(5, spec.maximum[1])\n\n  def testMinMaxAttributes(self):\n    spec = array_spec.BoundedArraySpec((1, 2, 3), np.float32, 0, (5, 5, 5))\n    self.assertEqual(type(spec.minimum), np.ndarray)\n    self.assertEqual(type(spec.maximum), np.ndarray)\n\n  def testNotWriteable(self):\n    spec = array_spec.BoundedArraySpec((1, 2, 3), np.float32, 0, (5, 5, 5))\n    with self.assertRaisesRegexp(ValueError, ""read-only""):\n      spec.minimum[0] = -1\n    with self.assertRaisesRegexp(ValueError, ""read-only""):\n      spec.maximum[0] = 100\n\n  def testEqualBroadcastingBounds(self):\n    spec_1 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=0.0, maximum=1.0)\n    spec_2 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=[0.0, 0.0], maximum=[1.0, 1.0])\n    self.assertEqual(spec_1, spec_2)\n\n  def testNotEqualDifferentMinimum(self):\n    spec_1 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=[0.0, -1.6], maximum=[1.0, 1.0])\n    spec_2 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=[0.0, 0.0], maximum=[1.0, 1.0])\n    self.assertNotEqual(spec_1, spec_2)\n\n  def testReuseSpec(self):\n    spec_1 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=0.0, maximum=1.0)\n    spec_2 = array_spec.BoundedArraySpec(spec_1.shape, spec_1.dtype,\n                                         spec_1.minimum, spec_1.maximum)\n    self.assertEqual(spec_1, spec_2)\n\n  def testNotEqualOtherClass(self):\n    spec_1 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=[0.0, -0.6], maximum=[1.0, 1.0])\n    spec_2 = array_spec.ArraySpec((1, 2), np.int32)\n    self.assertNotEqual(spec_1, spec_2)\n    self.assertNotEqual(spec_2, spec_1)\n\n    spec_2 = None\n    self.assertNotEqual(spec_1, spec_2)\n    self.assertNotEqual(spec_2, spec_1)\n\n    spec_2 = ()\n    self.assertNotEqual(spec_1, spec_2)\n    self.assertNotEqual(spec_2, spec_1)\n\n  def testNotEqualDifferentMaximum(self):\n    spec_1 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=0.0, maximum=2.0)\n    spec_2 = array_spec.BoundedArraySpec(\n        (1, 2), np.int32, minimum=[0.0, 0.0], maximum=[1.0, 1.0])\n    self.assertNotEqual(spec_1, spec_2)\n\n  def testRepr(self):\n    as_string = repr(\n        array_spec.BoundedArraySpec(\n            (1, 2), np.int32, minimum=73.0, maximum=101.0))\n    self.assertIn(""101"", as_string)\n    self.assertIn(""73"", as_string)\n\n  def testFromArraySpec(self):\n    spec = array_spec.ArraySpec((2, 3), np.int32)\n    bounded_spec = array_spec.BoundedArraySpec.from_spec(spec)\n    self.assertEqual(np.int32, bounded_spec.dtype)\n\n    i64_info = np.iinfo(np.int32)\n\n    self.assertEqual(i64_info.min, bounded_spec.minimum)\n    self.assertEqual(i64_info.max, bounded_spec.maximum)\n\n  def testFromBoundedArraySpec(self):\n    bounded_spec = array_spec.BoundedArraySpec(\n        (2, 3), np.int32, minimum=5, maximum=15, name=""test_spec"")\n    new_spec = array_spec.BoundedArraySpec.from_spec(bounded_spec)\n\n    self.assertEqual(bounded_spec.minimum, new_spec.minimum)\n    self.assertEqual(bounded_spec.maximum, new_spec.maximum)\n    self.assertEqual(bounded_spec.dtype, new_spec.dtype)\n    self.assertEqual(bounded_spec.shape, new_spec.shape)\n    self.assertEqual(bounded_spec.name, new_spec.name)\n\n  def testFromArraySpecRename(self):\n    bounded_spec = array_spec.BoundedArraySpec(\n        (2, 3), np.int32, minimum=5, maximum=15, name=""test_spec"")\n    new_spec = array_spec.BoundedArraySpec.from_spec(\n        bounded_spec, name=""rename"")\n\n    self.assertEqual(bounded_spec.minimum, new_spec.minimum)\n    self.assertEqual(bounded_spec.maximum, new_spec.maximum)\n    self.assertEqual(bounded_spec.dtype, new_spec.dtype)\n    self.assertEqual(bounded_spec.shape, new_spec.shape)\n    self.assertEqual(""rename"", new_spec.name)\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def testCheckArrayMatch(self, dtype):\n    spec = array_spec.BoundedArraySpec((2,), dtype, minimum=5, maximum=15)\n    self.assertTrue(spec.check_array(np.array([6, 7], dtype)))\n    # Bounds should be inclusive.\n    self.assertTrue(spec.check_array(np.array([5, 15], dtype)))\n\n  @parameterized.named_parameters(\n      (""wrong shape"", np.array([1])),\n      (""wrong dtype"", np.array([1, 2], dtype=np.double)),\n      (""not an array"", ""a string""),\n      (""out of bounds 1"", np.array([1, 10])),\n      (""out of bounds 2"", np.array([5, 20])))\n  def testCheckArrayNoMatch(self, array):\n    spec = array_spec.BoundedArraySpec((2,), np.int64, minimum=5, maximum=15)\n    self.assertFalse(spec.check_array(array))\n\n  # Tests that random sample of a complete uint8 range contains all values.\n  def testSampleUint8(self):\n    self.skipTest(""TODO(oars): Fix this test."")\n    rng = np.random.RandomState()\n    spec = array_spec.BoundedArraySpec(\n        (100, 10, 10), np.uint8, minimum=0, maximum=255)\n    sample = array_spec.sample_bounded_spec(spec, rng)\n    self.assertTupleEqual((100, 10, 10), sample.shape)\n    hist, _ = np.histogram(sample, bins=256, range=(0, 255))\n    self.assertTrue(np.all(hist > 0))\n\n  # Tests that random sample of a complete int8 range contains all values. The\n  # caveat is that difference of max - min is not int8.\n  # TODO(oars): Fix these tests: perhaps by chance not every bin is filled?\n  # Need a lot more samples (e.g. shape (100, 100, 100) to ensure they are?\n  def testSampleInt8(self):\n    self.skipTest(""TODO(oars): Fix this test."")\n    rng = np.random.RandomState()\n    spec = array_spec.BoundedArraySpec(\n        (100, 10, 10), np.int8, minimum=-128, maximum=127)\n    sample = array_spec.sample_bounded_spec(spec, rng)\n    self.assertTupleEqual((100, 10, 10), sample.shape)\n    hist, _ = np.histogram(sample, bins=256, range=(-128, 127))\n    self.assertTrue(np.all(hist > 0))\n\n  # Tests that random sample from uint64 does have all values requested.\n  def testSampleUint64SmallRange(self):\n    self.skipTest(""TODO(oars): Fix this test."")\n    rng = np.random.RandomState()\n    spec = array_spec.BoundedArraySpec(\n        (100, 10, 10), np.uint64, minimum=0, maximum=100)\n    sample = array_spec.sample_bounded_spec(spec, rng)\n    self.assertTupleEqual((100, 10, 10), sample.shape)\n    hist, _ = np.histogram(sample, bins=100, range=(0, 100))\n    self.assertTrue(np.all(hist > 0))\n\n  # Tests that random sample from full int64 works well. The caveat is that the\n  # full range min-max cannot be represented as an int64.\n  def testSampleInt64FullRange(self):\n    rng = np.random.RandomState()\n    spec = array_spec.BoundedArraySpec(\n        (100, 10, 10),\n        np.int64,\n        minimum=np.iinfo(np.int64).min,\n        maximum=np.iinfo(np.int64).max)\n    sample = array_spec.sample_bounded_spec(spec, rng)\n    self.assertTupleEqual((100, 10, 10), sample.shape)\n    hist, _ = np.histogram(sample, bins=100, range=(np.iinfo(np.int64).min / 2,\n                                                    np.iinfo(np.int64).max / 2))\n    self.assertTrue(np.all(hist > 0))\n\n  # Tests that random sample from full float64 does have no infs.\n  def testSampleFloat64FullRange(self):\n    rng = np.random.RandomState()\n    spec = array_spec.BoundedArraySpec(\n        (100, 10, 10), np.float64, minimum=0, maximum=100)\n    sample = array_spec.sample_bounded_spec(spec, rng)\n    self.assertTupleEqual((100, 10, 10), sample.shape)\n    self.assertFalse(np.any(np.isinf(sample)))\n    hist, _ = np.histogram(sample, bins=100, range=(0, 100))\n    self.assertTrue(np.all(hist > 0))\n\n  def testReplace(self):\n    spec = array_spec.BoundedArraySpec(tuple(), np.int8, minimum=0, maximum=1)\n    new_spec = spec.replace(shape=(2,))\n    self.assertEqual(new_spec.shape, (2,))\n    new_spec = new_spec.replace(dtype=np.int32)\n    self.assertEqual(new_spec.dtype, np.int32)\n    new_spec = new_spec.replace(name=""name"")\n    self.assertEqual(new_spec.name, ""name"")\n    new_spec = new_spec.replace(minimum=-1)\n    self.assertEqual(new_spec.minimum, -1)\n    new_spec = new_spec.replace(maximum=0)\n    self.assertEqual(new_spec.maximum, 0)\n    exp_spec = array_spec.BoundedArraySpec((2,), np.int32,\n                                           minimum=-1, maximum=0, name=""name"")\n    self.assertEqual(exp_spec, new_spec)\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def testNumValues(self, dtype):\n    spec = array_spec.BoundedArraySpec(tuple(), dtype, minimum=0, maximum=9)\n    num_values = spec.num_values\n    if array_spec.is_discrete(spec):\n      self.assertEqual(10, num_values)\n    else:\n      self.assertEqual(None, num_values)\n\n  def testNumValuesVector(self):\n    spec = array_spec.BoundedArraySpec((2,), np.int32, [0, 0], [1, 1])\n    self.assertTrue(np.all([2, 2] == spec.num_values))\n    spec = spec.replace(minimum=1)\n    self.assertTrue(np.all([1, 1] == spec.num_values))\n    spec = spec.replace(maximum=2)\n    self.assertTrue(np.all([2, 2] == spec.num_values))\n\n\n@parameterized.named_parameters(*TYPE_PARAMETERS)\nclass ArraySpecTypeTest(parameterized.TestCase):\n\n  def testIsDiscrete(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype=dtype)\n    self.assertIs(tensor_spec.is_discrete(spec),\n                  issubclass(np.dtype(dtype).type, np.integer))\n\n  def testIsContinuous(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype=dtype)\n    self.assertIs(tensor_spec.is_continuous(spec),\n                  issubclass(np.dtype(dtype).type, np.floating))\n\n  def testExclusive(self, dtype):\n    spec = array_spec.ArraySpec((2, 3), dtype=dtype)\n    self.assertIs(\n        tensor_spec.is_discrete(spec) ^ tensor_spec.is_continuous(spec), True)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/specs/distribution_spec.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Spec definition for tensorflow_probability.Distribution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_probability as tfp\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\nclass DistributionSpec(object):\n  """"""Describes a tfp.distribution.Distribution.""""""\n\n  __slots__ = [\n      ""_builder"", ""_input_params_spec"", ""_sample_spec"",\n      ""_distribution_parameters""\n  ]\n\n  def __init__(self, builder, input_params_spec, sample_spec,\n               **distribution_parameters):\n    """"""Creates a DistributionSpec.\n\n    Args:\n      builder: Callable function(**params) which returns a Distribution\n        following the spec.\n      input_params_spec: Nest of tensor_specs describing the tensor parameters\n        required for building the described distribution.\n      sample_spec: Data type of the output samples of the described\n        distribution.\n      **distribution_parameters: Extra parameters for building the distribution.\n    """"""\n    self._builder = builder\n    self._input_params_spec = input_params_spec\n    self._sample_spec = sample_spec\n    self._distribution_parameters = distribution_parameters\n\n  @property\n  def builder(self):\n    """"""Returns the `distribution_builder` of the spec.""""""\n    return self._builder\n\n  @property\n  def input_params_spec(self):\n    """"""Returns the `input_params_spec` of the spec.""""""\n    return self._input_params_spec\n\n  @property\n  def sample_spec(self):\n    """"""Returns the `sample_spec` of the spec.""""""\n    return self._sample_spec\n\n  @property\n  def distribution_parameters(self):\n    """"""Returns the `distribution_parameters` of the spec.""""""\n    return self._distribution_parameters\n\n  def build_distribution(self, **distribution_parameters):\n    """"""Creates an instance of the described distribution.\n\n    The spec\'s paramers are updated with the given ones.\n    Args:\n      **distribution_parameters: Kwargs update the spec\'s distribution\n        parameters.\n\n    Returns:\n      Distribution instance.\n    """"""\n    kwargs = self._distribution_parameters.copy()\n    kwargs.update(distribution_parameters)\n    return self._builder(**kwargs)\n\n  def __repr__(self):\n    return (""DistributionSpec(builder={}, input_params_spec={}, ""\n            ""sample_spec={})"").format(self.builder,\n                                      repr(self.input_params_spec),\n                                      repr(self.sample_spec))\n\n\ndef deterministic_distribution_from_spec(spec):\n  """"""Creates a Deterministic distribution_spec from a tensor_spec.""""""\n  return DistributionSpec(tfd.Deterministic, {""loc"": spec}, sample_spec=spec)\n\n\ndef nested_distributions_from_specs(specs, parameters):\n  """"""Builds a nest of distributions from a nest of specs.\n\n  Args:\n    specs: A nest of distribution specs.\n    parameters: A nest of distribution kwargs.\n\n  Returns:\n    Nest of distribution instances with the same structure as the given specs.\n  """"""\n  return nest.map_structure_up_to(\n      specs, lambda spec, parameters: spec.build_distribution(**parameters),\n      specs, parameters)\n'"
tf_agents/specs/distribution_spec_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.specs.distribution_spec.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\n\ntfd = tfp.distributions\n\n\nclass DistributionSpecTest(tf.test.TestCase):\n\n  def testBuildsDistribution(self):\n    expected_distribution = tfd.Categorical([0.2, 0.3, 0.5], validate_args=True)\n    input_param_spec = tensor_spec.TensorSpec((3,), dtype=tf.float32)\n    sample_spec = tensor_spec.TensorSpec((1,), dtype=tf.int32)\n\n    spec = distribution_spec.DistributionSpec(\n        tfd.Categorical,\n        input_param_spec,\n        sample_spec=sample_spec,\n        **expected_distribution.parameters)\n\n    self.assertEqual(expected_distribution.parameters[\'logits\'],\n                     spec.distribution_parameters[\'logits\'])\n\n    distribution = spec.build_distribution(logits=[0.1, 0.4, 0.5])\n\n    self.assertTrue(isinstance(distribution, tfd.Categorical))\n    self.assertTrue(distribution.parameters[\'validate_args\'])\n    self.assertEqual([0.1, 0.4, 0.5], distribution.parameters[\'logits\'])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/specs/specs_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests reinforcement_learning.environments.specs without tensorflow.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\n\nfrom tf_agents import specs\nfrom tf_agents.utils import test_utils\n\n\nclass SpecWithTensorflowTest(test_utils.TestCase):\n\n  def testLoad(self):\n    specs.ArraySpec([1, 2, 3], np.int32)\n    specs.BoundedArraySpec([1, 2, 3], np.int32, 0, 1)\n    specs.TensorSpec([1, 2, 3], np.int32)\n    specs.BoundedTensorSpec([1, 2, 3], np.int32, 0, 1)\n\n  def testFromArraySpecToTensorSpec(self):\n    array_spec = specs.ArraySpec([1, 2, 3], np.int32)\n    tensor_spec = specs.TensorSpec.from_spec(array_spec)\n    self.assertEqual(array_spec.shape, tensor_spec.shape)\n    self.assertEqual(array_spec.dtype, tensor_spec.dtype.as_numpy_dtype(0))\n    self.assertEqual(array_spec.name, tensor_spec.name)\n    self.assertEqual(type(tensor_spec), specs.tensor_spec.TensorSpec)\n\n  def testFromArraySpecToBoundedTensorSpec(self):\n    array_spec = specs.ArraySpec([1, 2, 3], np.int32)\n    tensor_spec = specs.BoundedTensorSpec.from_spec(array_spec)\n    self.assertEqual(array_spec.shape, tensor_spec.shape)\n    self.assertEqual(array_spec.dtype, tensor_spec.dtype.as_numpy_dtype(0))\n    self.assertEqual(array_spec.name, tensor_spec.name)\n    self.assertEqual(tensor_spec.dtype.min, tensor_spec.minimum)\n    self.assertEqual(tensor_spec.dtype.max, tensor_spec.maximum)\n    self.assertEqual(type(tensor_spec), specs.tensor_spec.BoundedTensorSpec)\n\n  def testFromBoundedArraySpecToBoundedTensorSpec(self):\n    array_spec = specs.BoundedArraySpec([1, 2, 3], np.int32, 0, 1)\n    tensor_spec = specs.BoundedTensorSpec.from_spec(array_spec)\n    self.assertEqual(array_spec.shape, tensor_spec.shape)\n    self.assertEqual(array_spec.dtype, tensor_spec.dtype.as_numpy_dtype(0))\n    self.assertEqual(array_spec.minimum, tensor_spec.minimum)\n    self.assertEqual(array_spec.maximum, tensor_spec.maximum)\n    self.assertEqual(array_spec.name, tensor_spec.name)\n    self.assertEqual(type(tensor_spec), specs.tensor_spec.BoundedTensorSpec)\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/specs/tensor_spec.py,58,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities related to TensorSpec class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.specs import array_spec\n\nfrom google.protobuf import text_format\nfrom tensorflow.core.protobuf import struct_pb2  # pylint:disable=g-direct-tensorflow-import  # TF internal\nfrom tensorflow.python.framework import tensor_spec as ts  # TF internal\nfrom tensorflow.python.saved_model import nested_structure_coder  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\nTensorSpec = tf.TensorSpec\nBoundedTensorSpec = ts.BoundedTensorSpec\n\n\ndef is_bounded(spec):\n  return isinstance(spec, (array_spec.BoundedArraySpec, BoundedTensorSpec))\n\n\ndef is_discrete(spec):\n  if isinstance(spec, TensorSpec):\n    return spec.dtype.is_integer\n  else:\n    return array_spec.is_discrete(spec)\n\n\ndef is_continuous(spec):\n  if isinstance(spec, TensorSpec):\n    return spec.dtype.is_floating\n  else:\n    return array_spec.is_continuous(spec)\n\n\ndef from_spec(spec):\n  """"""Maps the given spec into corresponding TensorSpecs keeping bounds.""""""\n\n  def _convert_to_tensor_spec(s):\n    # Need to check bounded first as non bounded specs are base class.\n    if isinstance(s, (array_spec.BoundedArraySpec, BoundedTensorSpec)):\n      return BoundedTensorSpec.from_spec(s)\n    elif isinstance(s, (array_spec.ArraySpec, TensorSpec)):\n      return TensorSpec.from_spec(s)\n    else:\n      raise ValueError(""No known conversion from type `%s` to a TensorSpec"" %\n                       type(s))\n\n  return tf.nest.map_structure(_convert_to_tensor_spec, spec)\n\n\ndef to_array_spec(tensor_spec):\n  """"""Converts TensorSpec into ArraySpec.""""""\n  if hasattr(tensor_spec, ""minimum"") and hasattr(tensor_spec, ""maximum""):\n    return array_spec.BoundedArraySpec(\n        tensor_spec.shape.as_list(),\n        tensor_spec.dtype.as_numpy_dtype,\n        minimum=tensor_spec.minimum,\n        maximum=tensor_spec.maximum,\n        name=tensor_spec.name)\n  else:\n    return array_spec.ArraySpec(tensor_spec.shape.as_list(),\n                                tensor_spec.dtype.as_numpy_dtype,\n                                tensor_spec.name)\n\n\ndef to_nest_array_spec(nest_array_spec):\n  """"""Converted a nest of TensorSpecs to a nest of matching ArraySpecs.""""""\n  return tf.nest.map_structure(to_array_spec, nest_array_spec)\n\n\ndef to_placeholder(spec, outer_dims=()):\n  """"""Creates a placeholder from TensorSpec.\n\n  Args:\n    spec: instance of TensorSpec\n    outer_dims: optional leading dimensions of the placeholder.\n\n  Returns:\n    An instance of tf.placeholder.\n  """"""\n  ph_shape = list(outer_dims) + spec.shape.as_list()\n  return tf.compat.v1.placeholder(spec.dtype, ph_shape, spec.name)\n\n\ndef to_placeholder_with_default(default, spec, outer_dims=()):\n  """"""Creates a placeholder from TensorSpec.\n\n  Args:\n    default: A constant value of output type dtype.\n    spec: Instance of TensorSpec\n    outer_dims: Optional leading dimensions of the placeholder.\n\n  Returns:\n    An instance of tf.placeholder.\n  """"""\n  ph_shape = list(outer_dims) + spec.shape.as_list()\n  return tf.compat.v1.placeholder_with_default(default, ph_shape, spec.name)\n\n\ndef to_nest_placeholder(nested_tensor_specs,\n                        default=None,\n                        name_scope="""",\n                        outer_dims=()):\n  """"""Converts a nest of TensorSpecs to a nest of matching placeholders.\n\n  Args:\n    nested_tensor_specs: A nest of tensor specs.\n    default: Optional constant value to set as a default for the placeholder.\n    name_scope: String name for the scope to create the placeholders in.\n    outer_dims: Optional leading dimensions for the placeholder.\n\n  Returns:\n    A nest of placeholders matching the given tensor spec.\n\n  Raises:\n    ValueError: If a default is provided outside of the allowed types, or if\n      default is a np.array that does not match the spec shape.\n  """"""\n  if default is None:\n    to_ph = lambda spec: to_placeholder(spec, outer_dims=outer_dims)\n  else:\n    if not isinstance(default, (int, float, np.ndarray)):\n      raise ValueError(""to_nest_placeholder default value must be an int, ""\n                       ""float, or np.ndarray"")\n\n    def to_ph(spec):\n      shape = list(outer_dims) + spec.shape.as_list()\n      if isinstance(default, np.ndarray) and list(default.shape) != shape:\n        raise ValueError(""Shape mismatch between default value and spec. ""\n                         ""Got {}, expected {}"".format(default.shape, shape))\n      const = tf.constant(default, shape=shape, dtype=spec.dtype)\n      return to_placeholder_with_default(const, spec, outer_dims=outer_dims)\n\n  with tf.name_scope(name_scope):\n    return tf.nest.map_structure(to_ph, nested_tensor_specs)\n\n\ndef _random_uniform_int(shape, outer_dims, minval, maxval, dtype, seed=None):\n  """"""Iterates over n-d tensor minval, maxval limits to sample uniformly.""""""\n  # maxval in BoundedTensorSpec is bound inclusive.\n  # tf.random_uniform is upper bound exclusive, +1 to fix the sampling\n  # behavior.\n  # However +1 could cause overflow, in such cases we use the original maxval.\n  maxval = np.broadcast_to(maxval, minval.shape).astype(dtype.as_numpy_dtype)\n  minval = np.broadcast_to(minval, maxval.shape).astype(dtype.as_numpy_dtype)\n\n  sampling_maxval = maxval\n  if dtype.is_integer:\n    sampling_maxval = np.where(maxval < dtype.max, maxval + 1, maxval)\n\n  if not np.all(shape[-len(minval.shape):] == minval.shape):\n    raise ValueError(\n        ""%s == shape[-%d:] != minval.shape == %s.  shape == %s."" %\n        (shape[len(minval.shape):], len(minval.shape), minval.shape, shape))\n\n  # Example:\n  #  minval = [1.0, 2.0]\n  #  shape = [3, 2]\n  #  outer_dims = [5]\n  # Sampling becomes:\n  #  sample [5, 3] for minval 1.0\n  #  sample [5, 3] for minval 2.0\n  #  stack on innermost axis to get [5, 3, 2]\n  #  reshape to get [5, 3, 2]\n  samples = []\n  shape = tf.convert_to_tensor(shape, dtype=tf.int32)\n  sample_shape = tf.concat((outer_dims, shape[:-len(minval.shape)]), axis=0)\n  full_shape = tf.concat((outer_dims, shape), axis=0)\n  for (single_min, single_max) in zip(minval.flat, sampling_maxval.flat):\n    samples.append(\n        tf.random.uniform(\n            shape=sample_shape,\n            minval=single_min,\n            maxval=single_max,\n            dtype=dtype,\n            seed=seed))\n  samples = tf.stack(samples, axis=-1)\n  samples = tf.reshape(samples, full_shape)\n  return samples\n\n\ndef sample_bounded_spec(spec, seed=None, outer_dims=None):\n  """"""Samples uniformily the given bounded spec.\n\n  Args:\n    spec: A BoundedSpec to sample.\n    seed: A seed used for sampling ops\n    outer_dims: An optional `Tensor` specifying outer dimensions to add to the\n      spec shape before sampling.\n\n  Returns:\n    A Tensor sample of the requested spec.\n  """"""\n  minval = spec.minimum\n  maxval = spec.maximum\n  dtype = tf.as_dtype(spec.dtype)\n\n  # To sample uint8 we will use int32 and cast later. This is needed for two\n  # reasons:\n  #  - tf.random_uniform does not currently support uint8\n  #  - if you want to sample [0, 255] range, there\'s no way to do this since\n  #    tf.random_uniform has exclusive upper bound and 255 + 1 would overflow.\n  is_uint8 = dtype == tf.uint8\n  sampling_dtype = tf.int32 if is_uint8 else dtype\n\n  if dtype in [tf.float64, tf.float32]:\n    # Avoid under/over-flow as random_uniform can\'t sample over the full range\n    # for these types.\n    minval = np.maximum(dtype.min / 8, minval)\n    maxval = np.minimum(dtype.max / 8, maxval)\n\n  if outer_dims is None:\n    outer_dims = tf.constant([], dtype=tf.int32)\n  else:\n    outer_dims = tf.convert_to_tensor(outer_dims, dtype=tf.int32)\n\n  def _unique_vals(vals):\n    if vals.size > 0:\n      if vals.ndim > 0:\n        return np.all(vals == vals[0])\n    return True\n\n  if (minval.ndim != 0 or\n      maxval.ndim != 0) and not (_unique_vals(minval) and _unique_vals(maxval)):\n    # tf.random_uniform can only handle minval/maxval 0-d tensors.\n    res = _random_uniform_int(\n        shape=spec.shape,\n        outer_dims=outer_dims,\n        minval=minval,\n        maxval=maxval,\n        dtype=sampling_dtype,\n        seed=seed)\n  else:\n    minval = minval.item(0) if minval.ndim != 0 else minval\n    maxval = maxval.item(0) if maxval.ndim != 0 else maxval\n    # BoundedTensorSpec are bounds inclusive.\n    # tf.random_uniform is upper bound exclusive, +1 to fix the sampling\n    # behavior.\n    # However +1 will cause overflow, in such cases we use the original maxval.\n    if sampling_dtype.is_integer and maxval < sampling_dtype.max:\n      maxval = maxval + 1\n\n    shape = tf.convert_to_tensor(spec.shape, dtype=tf.int32)\n    full_shape = tf.concat((outer_dims, shape), axis=0)\n    res = tf.random.uniform(\n        full_shape,\n        minval=minval,\n        maxval=maxval,\n        dtype=sampling_dtype,\n        seed=seed)\n\n  if is_uint8:\n    res = tf.cast(res, dtype=dtype)\n\n  return res\n\n\ndef sample_spec_nest(structure, seed=None, outer_dims=()):\n  """"""Samples the given nest of specs.\n\n  Args:\n    structure: A nest of `TensorSpec`.\n    seed: A seed used for sampling ops\n    outer_dims: An optional `Tensor` specifying outer dimensions to add to the\n      spec shape before sampling.\n\n  Returns:\n    A nest of sampled values following the ArraySpec definition.\n\n  Raises:\n    TypeError: If `spec` is an unknown type.\n    NotImplementedError: If `outer_dims` is not statically known but nest\n      contains a `SparseTensorSpec`.\n  """"""\n  seed_stream = tfp.util.SeedStream(seed=seed, salt=""sample_spec_nest"")\n\n  def sample_fn(spec):\n    """"""Return a composite tensor sample given `spec`.\n\n    Args:\n      spec: A TensorSpec, SparseTensorSpec, etc.\n\n    Returns:\n      A tensor or SparseTensor.\n\n    Raises:\n      NotImplementedError: If `outer_dims` is not statically known and a\n        SparseTensor is requested.\n    """"""\n    if isinstance(spec, tf.SparseTensorSpec):\n      outer_shape = tf.get_static_value(outer_dims)\n      if outer_dims is not None and outer_shape is None:\n        raise NotImplementedError(\n            ""outer_dims must be statically known, got: {}"".format(outer_dims))\n      shape = tf.TensorShape(outer_shape or []).concatenate(spec.shape)\n\n      if shape.num_elements() == 0 or tf.compat.dimension_value(shape[0]) == 0:\n        return tf.SparseTensor(\n            indices=tf.zeros([0, shape.rank], dtype=tf.int64),\n            values=tf.zeros([0], dtype=spec.dtype),\n            dense_shape=shape)\n\n      indices_spec = BoundedTensorSpec(\n          dtype=tf.int64,\n          shape=[7, shape.rank],\n          minimum=[0] * shape.rank,\n          maximum=[x - 1 for x in shape.as_list()])\n      values_dtype = tf.int32 if spec.dtype == tf.string else spec.dtype\n      values_spec = BoundedTensorSpec(\n          dtype=values_dtype,\n          shape=[7],\n          minimum=0,\n          maximum=shape.as_list()[-1] - 1)\n      values_sample = sample_bounded_spec(values_spec, seed=seed_stream())\n      if spec.dtype == tf.string:\n        values_sample = tf.as_string(values_sample)\n      return tf.sparse.reorder(\n          tf.SparseTensor(\n              indices=sample_bounded_spec(indices_spec, seed=seed_stream()),\n              values=values_sample,\n              dense_shape=shape))\n    elif isinstance(spec, (TensorSpec, BoundedTensorSpec)):\n      if spec.dtype == tf.string:\n        sample_spec = BoundedTensorSpec(\n            spec.shape, tf.int32, minimum=0, maximum=10)\n        return tf.as_string(\n            sample_bounded_spec(\n                sample_spec, outer_dims=outer_dims, seed=seed_stream()))\n      else:\n        return sample_bounded_spec(\n            BoundedTensorSpec.from_spec(spec),\n            outer_dims=outer_dims,\n            seed=seed_stream())\n    else:\n      raise TypeError(""Spec type not supported: \'{}\'"".format(spec))\n\n  return tf.nest.map_structure(sample_fn, structure)\n\n\ndef zero_spec_nest(specs, outer_dims=None):\n  """"""Create zero tensors for a given spec.\n\n  Args:\n    specs: A nest of `TensorSpec`.\n    outer_dims: An optional list of constants or `Tensor` specifying outer\n      dimensions to add to the spec shape before sampling.\n\n  Returns:\n    A nest of zero tensors matching `specs`, with the optional outer\n    dimensions added.\n\n  Raises:\n    TypeError: If `specs` is an unknown type.\n    NotImplementedError: If `specs` contains non-dense tensor specs.\n  """"""\n\n  def make_zero(spec):\n    if not isinstance(spec, TensorSpec):\n      raise NotImplementedError(""Spec type not supported: \'{}\'"".format(spec))\n    if outer_dims is None:\n      shape = spec.shape\n    else:\n      spec_shape = tf.convert_to_tensor(value=spec.shape, dtype=tf.int32)\n      shape = tf.concat((outer_dims, spec_shape), axis=0)\n    return tf.zeros(shape, spec.dtype)\n\n  if specs:\n    if outer_dims is None:\n      outer_dims = tf.constant([], dtype=tf.int32)\n    else:\n      outer_dims = tf.convert_to_tensor(outer_dims, dtype=tf.int32)\n\n  return tf.nest.map_structure(make_zero, specs)\n\n\ndef add_outer_dims_nest(specs, outer_dims):\n  """"""Adds outer dimensions to the shape of input specs.\n\n  Args:\n    specs: Nested list/tuple/dict of TensorSpecs/ArraySpecs, describing the\n      shape of tensors.\n    outer_dims: a list or tuple, representing the outer shape to be added to the\n      TensorSpecs in specs.\n\n  Returns:\n    Nested TensorSpecs with outer dimensions added to the shape of input specs.\n\n  Raises:\n    ValueError: if any outer_dims is neither a list nor tuple.\n  """"""\n  if not isinstance(outer_dims, (tuple, list)):\n    raise ValueError(""outer_dims must be a tuple or list of dimensions"")\n\n  def add_outer_dims(spec):\n    name = spec.name\n    shape = outer_dims + spec.shape\n    if hasattr(spec, ""minimum"") and hasattr(spec, ""maximum""):\n      return BoundedTensorSpec(shape, spec.dtype, spec.minimum, spec.maximum,\n                               name)\n    return TensorSpec(shape, spec.dtype, name=name)\n\n  return tf.nest.map_structure(add_outer_dims, specs)\n\n\ndef remove_outer_dims_nest(specs, outer_dims):\n  """"""Removes the specified number of outer dimensions from the input spec nest.\n\n  Args:\n    specs: Nested list/tuple/dict of TensorSpecs/ArraySpecs, describing the\n      shape of tensors.\n    outer_dims: (int) Number of outer dimensions to remove.\n\n  Returns:\n    Nested TensorSpecs with outer dimensions removed from the input specs.\n\n  Raises:\n    Value error if a spec in the nest has shape rank less than `outer_dims`.\n  """"""\n\n  def remove_outer_dims(spec):\n    """"""Removes the outer_dims of a tensor spec.""""""\n    name = spec.name\n    if len(spec.shape) < outer_dims:\n      raise ValueError(""The shape of spec {} has rank lower than the specified ""\n                       ""outer_dims {}"".format(spec, outer_dims))\n    shape = list(spec.shape)[outer_dims:]\n    if hasattr(spec, ""minimum"") and hasattr(spec, ""maximum""):\n      if isinstance(spec.minimum,\n                    (tuple, list)) and len(spec.minimum) == len(spec.shape):\n        minimum = spec.minimum[outer_dims:]\n      else:\n        minimum = spec.minimum\n      if isinstance(spec.maximum,\n                    (tuple, list)) and len(spec.maximum) == len(spec.shape):\n        maximum = spec.maximum[outer_dims:]\n      else:\n        maximum = spec.maximum\n\n      return BoundedTensorSpec(shape, spec.dtype, minimum, maximum, name)\n    return TensorSpec(shape, spec.dtype, name=name)\n\n  return tf.nest.map_structure(remove_outer_dims, specs)\n\n\ndef to_proto(spec):\n  """"""Encodes a nested spec into a struct_pb2.StructuredValue proto.\n\n  Args:\n    spec: Nested list/tuple or dict of TensorSpecs, describing the\n      shape of the non-batched Tensors.\n  Returns:\n    A `struct_pb2.StructuredValue` proto.\n  """"""\n  # Make sure spec is a tensor_spec.\n  spec = from_spec(spec)\n  signature_encoder = nested_structure_coder.StructureCoder()\n  return signature_encoder.encode_structure(spec)\n\n\ndef from_proto(spec_proto):\n  """"""Decodes a struct_pb2.StructuredValue proto into a nested spec.""""""\n  signature_encoder = nested_structure_coder.StructureCoder()\n  return signature_encoder.decode_proto(spec_proto)\n\n\ndef from_packed_proto(spec_packed_proto):\n  """"""Decodes a packed Any proto containing the structured value for the spec.""""""\n  spec_proto = struct_pb2.StructuredValue()\n  spec_packed_proto.Unpack(spec_proto)\n  return from_proto(spec_proto)\n\n\ndef to_pbtxt_file(output_path, spec):\n  """"""Saves a spec encoded as a struct_pb2.StructuredValue in a pbtxt file.""""""\n  spec_proto = to_proto(spec)\n  with tf.io.gfile.GFile(output_path, ""wb"") as f:\n    f.write(text_format.MessageToString(spec_proto))\n\n\ndef from_pbtxt_file(spec_path):\n  """"""Loads a spec encoded as a struct_pb2.StructuredValue from a pbtxt file.""""""\n  spec_proto = struct_pb2.StructuredValue()\n  with tf.io.gfile.GFile(spec_path, ""rb"") as f:\n    text_format.MergeLines(f, spec_proto)\n  return from_proto(spec_proto)\n'"
tf_agents/specs/tensor_spec_test.py,50,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for specs.tensor_spec.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\n\nTYPE_PARAMETERS = (\n    (""tf.int32"", tf.int32),\n    (""tf.int64"", tf.int64),\n    (""tf.float32"", tf.float32),\n    (""tf.float64"", tf.float64),\n    (""tf.uint8"", tf.uint8),\n    (""tf.string"", tf.string),\n)\n\n\ndef example_nested_array_spec(dtype):\n  return {\n      ""spec_1"":\n          array_spec.ArraySpec((2, 3), dtype),\n      ""bounded_spec_1"":\n          array_spec.BoundedArraySpec((2, 3), dtype, -10, 10),\n      ""bounded_spec_2"":\n          array_spec.BoundedArraySpec((2, 3), dtype, -10, -10),\n      ""bounded_array_spec_3"":\n          array_spec.BoundedArraySpec((2,), dtype, [-10, -10], [10, 10]),\n      ""bounded_array_spec_4"":\n          array_spec.BoundedArraySpec((2,), dtype, [-10, -9], [10, 9]),\n      ""dict_spec"": {\n          ""spec_2"": array_spec.ArraySpec((2, 3), dtype),\n          ""bounded_spec_2"": array_spec.BoundedArraySpec((2, 3), dtype, -10, 10)\n      },\n      ""tuple_spec"": (\n          array_spec.ArraySpec((2, 3), dtype),\n          array_spec.BoundedArraySpec((2, 3), dtype, -10, 10),\n      ),\n      ""list_spec"": [\n          array_spec.ArraySpec((2, 3), dtype),\n          (array_spec.ArraySpec(\n              (2, 3), dtype), array_spec.BoundedArraySpec((2, 3), dtype, -10,\n                                                          10)),\n      ],\n  }\n\n\ndef example_nested_tensor_spec(dtype, outer_dims=()):\n  minval = 0 if dtype == tf.uint8 else -10\n  maxval = 255 if dtype == tf.uint8 else 10\n  return {\n      ""spec_1"":\n          tensor_spec.TensorSpec(outer_dims + (2, 3), dtype),\n      ""bounded_spec_1"":\n          tensor_spec.BoundedTensorSpec(outer_dims + (2, 3), dtype, minval,\n                                        maxval),\n      ""bounded_spec_2"":\n          tensor_spec.BoundedTensorSpec(outer_dims + (2, 3), dtype, minval,\n                                        minval),\n      ""bounded_array_spec_3"":\n          tensor_spec.BoundedTensorSpec(outer_dims + (2,), dtype,\n                                        [minval, minval], [maxval, maxval]),\n      ""bounded_array_spec_4"":\n          tensor_spec.BoundedTensorSpec(outer_dims + (2,), dtype,\n                                        [minval, minval + 1],\n                                        [maxval, maxval - 1]),\n      ""dict_spec"": {\n          ""spec_2"":\n              tensor_spec.TensorSpec(outer_dims + (2, 3), dtype),\n          ""bounded_spec_2"":\n              tensor_spec.BoundedTensorSpec(outer_dims + (2, 3), dtype, minval,\n                                            maxval)\n      },\n      ""tuple_spec"": (\n          tensor_spec.TensorSpec(outer_dims + (2, 3), dtype),\n          tensor_spec.BoundedTensorSpec(outer_dims + (2, 3), dtype, minval,\n                                        maxval),\n      ),\n      ""list_spec"": [\n          tensor_spec.TensorSpec(outer_dims + (2, 3), dtype),\n          (tensor_spec.TensorSpec(outer_dims + (2, 3), dtype),\n           tensor_spec.BoundedTensorSpec(outer_dims + (2, 3), dtype, minval,\n                                         maxval)),\n      ],\n  }\n\n\n@parameterized.named_parameters(*TYPE_PARAMETERS)\nclass BoundedTensorSpecSampleTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testIntegerSamplesIncludeUpperBound(self, dtype):\n    if not dtype.is_integer:  # Only test on integer dtypes.\n      return\n    spec = tensor_spec.BoundedTensorSpec((2, 3), dtype, 3, 3)\n    sample = tensor_spec.sample_spec_nest(spec)\n    sample_ = self.evaluate(sample)\n    self.assertEqual(sample_.shape, (2, 3))\n    self.assertTrue(np.all(sample_ == 3))\n\n  def testIntegerSamplesExcludeMaxOfDtype(self, dtype):\n    # Exclude non integer types and uint8 (has special sampling logic).\n    if not dtype.is_integer or dtype == tf.uint8:\n      return\n    spec = tensor_spec.BoundedTensorSpec((2, 3), dtype, dtype.max - 1,\n                                         dtype.max)\n    sample = tensor_spec.sample_spec_nest(spec)\n    sample_ = self.evaluate(sample)\n    self.assertEqual(sample_.shape, (2, 3))\n    self.assertTrue(np.all(sample_ == dtype.max - 1))\n\n  def testSampleWithArrayInBounds(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    spec = tensor_spec.BoundedTensorSpec((2, 3), dtype, (0, 0, 0), 3)\n    sample = tensor_spec.sample_spec_nest(spec)\n    self.assertEqual((2, 3), sample.shape)\n    sample_ = self.evaluate(sample)\n    self.assertEqual((2, 3), sample_.shape)\n    self.assertTrue(np.all(sample_ <= 3))\n    self.assertTrue(np.all(0 <= sample_))\n\n  def testTensorSpecSample(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    spec = tensor_spec.TensorSpec((2, 3), dtype)\n    sample = tensor_spec.sample_spec_nest(spec)\n    bounded = tensor_spec.BoundedTensorSpec.from_spec(spec)\n\n    sample_ = self.evaluate(sample)\n    self.assertTrue(\n        np.all(sample_ >= bounded.minimum), (sample_.min(), sample_.max()))\n    self.assertTrue(\n        np.all(sample_ <= bounded.maximum), (sample_.min(), sample_.max()))\n\n  def testBoundedTensorSpecSample(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    spec = tensor_spec.BoundedTensorSpec((2, 3), dtype, 2, 7)\n    sample = tensor_spec.sample_spec_nest(spec)\n    sample_ = self.evaluate(sample)\n    self.assertTrue(np.all(sample_ >= 2))\n    self.assertTrue(np.all(sample_ <= 7))\n\n  def testOuterDimsNestAddsDimensionsToSpecs(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    outer_dims = (4, 3)\n    self.assertEqual(\n        tensor_spec.add_outer_dims_nest(nested_spec, outer_dims),\n        example_nested_tensor_spec(dtype, outer_dims))\n\n  def testAddOuterShapeWhenNotTupleOrListThrows(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    with self.assertRaises(ValueError):\n      tensor_spec.add_outer_dims_nest(1, example_nested_tensor_spec(dtype))\n\n  def testOuterDimsNestRemovesDimensionsFromSpecs(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    larger_spec = tensor_spec.add_outer_dims_nest(nested_spec, (3, 4))\n    removed_spec = tensor_spec.remove_outer_dims_nest(larger_spec, 2)\n    self.assertEqual(nested_spec, removed_spec)\n\n  def testOuterDimsNestRemovesDimensionsFromSpecsThrows(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    with self.assertRaises(ValueError):\n      tensor_spec.remove_outer_dims_nest(nested_spec, 10)\n\n  def testNestSample(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    sample = tensor_spec.sample_spec_nest(nested_spec)\n    spec_1 = tensor_spec.BoundedTensorSpec.from_spec(nested_spec[""spec_1""])\n    bounded_spec_1 = nested_spec[""bounded_spec_1""]\n    sample_ = self.evaluate(sample)\n    self.assertTrue(np.all(sample_[""spec_1""] >= spec_1.minimum))\n    self.assertTrue(np.all(sample_[""spec_1""] <= spec_1.maximum))\n\n    self.assertTrue(np.all(sample_[""bounded_spec_1""] >= bounded_spec_1.minimum))\n    self.assertTrue(np.all(sample_[""bounded_spec_1""] <= bounded_spec_1.maximum))\n\n    self.assertIn(""spec_2"", sample_[""dict_spec""])\n    tensor_spec_2 = sample_[""dict_spec""][""spec_2""]\n    self.assertTrue(np.all(tensor_spec_2 >= spec_1.minimum))\n    self.assertTrue(np.all(tensor_spec_2 <= spec_1.maximum))\n    self.assertIn(""bounded_spec_2"", sample_[""dict_spec""])\n    sampled_bounded_spec_2 = sample_[""dict_spec""][""bounded_spec_2""]\n    self.assertTrue(np.all(sampled_bounded_spec_2 >= spec_1.minimum))\n    self.assertTrue(np.all(sampled_bounded_spec_2 <= spec_1.maximum))\n\n    self.assertIn(""tuple_spec"", sample_)\n    self.assertTrue(np.all(sample_[""tuple_spec""][0] >= spec_1.minimum))\n    self.assertTrue(np.all(sample_[""tuple_spec""][0] <= spec_1.maximum))\n    self.assertTrue(np.all(sample_[""tuple_spec""][1] >= bounded_spec_1.minimum))\n    self.assertTrue(np.all(sample_[""tuple_spec""][1] <= bounded_spec_1.maximum))\n\n    self.assertIn(""list_spec"", sample_)\n    self.assertTrue(np.all(sample_[""list_spec""][0] >= spec_1.minimum))\n    self.assertTrue(np.all(sample_[""list_spec""][0] <= spec_1.maximum))\n    self.assertTrue(np.all(sample_[""list_spec""][1][0] >= spec_1.minimum))\n    self.assertTrue(np.all(sample_[""list_spec""][1][0] <= spec_1.maximum))\n    self.assertTrue(\n        np.all(sample_[""list_spec""][1][1] >= bounded_spec_1.minimum))\n    self.assertTrue(\n        np.all(sample_[""list_spec""][1][1] <= bounded_spec_1.maximum))\n\n  def testNestSampleOuterDims(self, dtype):\n    # Can\'t add another level of parameterized args because the test class is\n    # already parameterized on dtype.\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    self._testNestSampleOuterDims(dtype, use_tensor=False)\n    self._testNestSampleOuterDims(dtype, use_tensor=True)\n\n  def _testNestSampleOuterDims(self, dtype, use_tensor):\n    nested_spec = example_nested_tensor_spec(dtype)\n    if use_tensor:\n      outer_dims = tf.constant([2, 3], dtype=tf.int32)\n    else:\n      outer_dims = (2, 3)\n    sample = tensor_spec.sample_spec_nest(nested_spec, outer_dims=outer_dims)\n    bounded = tensor_spec.BoundedTensorSpec.from_spec(nested_spec[""spec_1""])\n    sample_ = self.evaluate(sample)\n    self.assertEqual((2, 3) + tuple(nested_spec[""spec_1""].shape.as_list()),\n                     sample_[""spec_1""].shape)\n    self.assertTrue(np.all(sample_[""spec_1""] >= bounded.minimum))\n    self.assertTrue(np.all(sample_[""spec_1""] <= bounded.maximum))\n\n    bounded_spec_1 = nested_spec[""bounded_spec_1""]\n    self.assertEqual((2, 3) + tuple(bounded_spec_1.shape.as_list()),\n                     sample_[""bounded_spec_1""].shape)\n    self.assertTrue(np.all(sample_[""bounded_spec_1""] >= bounded_spec_1.minimum))\n    self.assertTrue(np.all(sample_[""bounded_spec_1""] <= bounded_spec_1.maximum))\n\n    self.assertIn(""spec_2"", sample_[""dict_spec""])\n    tensor_spec_2 = sample_[""dict_spec""][""spec_2""]\n    self.assertEqual(\n        (2, 3) + tuple(nested_spec[""dict_spec""][""spec_2""].shape.as_list()),\n        tensor_spec_2.shape)\n    self.assertTrue(np.all(tensor_spec_2 >= bounded.minimum))\n    self.assertTrue(np.all(tensor_spec_2 <= bounded.maximum))\n    self.assertIn(""bounded_spec_2"", sample_[""dict_spec""])\n    sampled_bounded_spec_2 = sample_[""dict_spec""][""bounded_spec_2""]\n    self.assertEqual(\n        (2, 3) +\n        tuple(nested_spec[""dict_spec""][""bounded_spec_2""].shape.as_list()),\n        sampled_bounded_spec_2.shape)\n    self.assertTrue(np.all(sampled_bounded_spec_2 >= bounded.minimum))\n    self.assertTrue(np.all(sampled_bounded_spec_2 <= bounded.maximum))\n\n    self.assertIn(""tuple_spec"", sample_)\n    self.assertEqual(\n        (2, 3) + tuple(nested_spec[""tuple_spec""][0].shape.as_list()),\n        sample_[""tuple_spec""][0].shape)\n    self.assertTrue(np.all(sample_[""tuple_spec""][0] >= bounded.minimum))\n    self.assertTrue(np.all(sample_[""tuple_spec""][0] <= bounded.maximum))\n    tuple_bounded_spec = nested_spec[""tuple_spec""][1]\n    self.assertEqual((2, 3) + tuple(tuple_bounded_spec.shape.as_list()),\n                     sample_[""tuple_spec""][1].shape)\n    self.assertTrue(\n        np.all(sample_[""tuple_spec""][1] >= tuple_bounded_spec.minimum))\n    self.assertTrue(\n        np.all(sample_[""tuple_spec""][1] <= tuple_bounded_spec.maximum))\n\n    self.assertIn(""list_spec"", sample_)\n    self.assertEqual(\n        (2, 3) + tuple(nested_spec[""list_spec""][0].shape.as_list()),\n        sample_[""list_spec""][0].shape)\n    self.assertTrue(np.all(sample_[""list_spec""][0] >= bounded.minimum))\n    self.assertTrue(np.all(sample_[""list_spec""][0] <= bounded.maximum))\n    self.assertEqual(\n        (2, 3) + tuple(nested_spec[""list_spec""][1][0].shape.as_list()),\n        sample_[""list_spec""][1][0].shape)\n    self.assertTrue(np.all(sample_[""list_spec""][1][0] >= bounded.minimum))\n    self.assertTrue(np.all(sample_[""list_spec""][1][0] <= bounded.maximum))\n    list_bounded_spec = nested_spec[""list_spec""][1][1]\n    self.assertTrue(\n        np.all(sample_[""list_spec""][1][1] >= list_bounded_spec.minimum))\n    self.assertTrue(\n        np.all(sample_[""list_spec""][1][1] <= list_bounded_spec.maximum))\n\n    def _test_batched_shape(sample_, spec_):\n      self.assertSequenceEqual(sample_.shape, outer_dims + tuple(spec_.shape))\n      tf.nest.map_structure(_test_batched_shape, sample, nested_spec)\n\n\n@parameterized.named_parameters(*TYPE_PARAMETERS)\nclass TensorSpecZeroTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testNestZero(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    zeros = tensor_spec.zero_spec_nest(nested_spec)\n    zeros_ = self.evaluate(zeros)\n\n    def check_shape_and_zero(spec, value):\n      self.assertEqual(spec.shape, value.shape)\n      self.assertTrue(np.all(value == 0))\n\n    tf.nest.map_structure(check_shape_and_zero, nested_spec, zeros_)\n\n  def testNestZeroWithOuterDims(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    zeros = tensor_spec.zero_spec_nest(nested_spec, outer_dims=[4])\n    zeros_ = self.evaluate(zeros)\n\n    def check_shape_and_zero(spec, value):\n      self.assertEqual([4] + spec.shape, value.shape)\n      self.assertTrue(np.all(value == 0))\n\n    tf.nest.map_structure(check_shape_and_zero, nested_spec, zeros_)\n\n  def testNestZeroWithOuterDimsTensor(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    nested_spec = example_nested_tensor_spec(dtype)\n    zeros = tensor_spec.zero_spec_nest(\n        nested_spec, outer_dims=[tf.constant(8, dtype=tf.int32)])\n    zeros_ = self.evaluate(zeros)\n\n    def check_shape_and_zero(spec, value):\n      self.assertEqual([8] + spec.shape, value.shape)\n      self.assertTrue(np.all(value == 0))\n\n    tf.nest.map_structure(check_shape_and_zero, nested_spec, zeros_)\n\n  def testOnlyTensorSpecIsSupported(self, dtype):\n    sparse_spec = tf.SparseTensorSpec([1], tf.int32)\n    with self.assertRaisesRegexp(NotImplementedError, ""not supported.*Sparse""):\n      _ = tensor_spec.zero_spec_nest(sparse_spec)\n    ragged_spec = tf.RaggedTensorSpec(ragged_rank=0, shape=[3, 5])\n    with self.assertRaisesRegexp(NotImplementedError, ""not supported.*Ragged""):\n      _ = tensor_spec.zero_spec_nest(ragged_spec)\n\n  def testEmptySpec(self, dtype):\n    self.assertEqual((), tensor_spec.zero_spec_nest(()))\n    self.assertEqual([], tensor_spec.zero_spec_nest([]))\n\n\n@parameterized.named_parameters(*TYPE_PARAMETERS)\nclass TensorSpecTypeTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testIsDiscrete(self, dtype):\n    spec = tensor_spec.TensorSpec((2, 3), dtype=dtype)\n    self.assertIs(tensor_spec.is_discrete(spec), dtype.is_integer)\n\n  def testIsContinuous(self, dtype):\n    spec = tensor_spec.TensorSpec((2, 3), dtype=dtype)\n    self.assertIs(tensor_spec.is_continuous(spec), dtype.is_floating)\n\n  def testExclusive(self, dtype):\n    if dtype == tf.string:\n      self.skipTest(""Not compatible with string type."")\n    spec = tensor_spec.TensorSpec((2, 3), dtype=dtype)\n    self.assertIs(\n        tensor_spec.is_discrete(spec) ^ tensor_spec.is_continuous(spec), True)\n\n\nclass FromSpecTest(tf.test.TestCase):\n\n  def testFromSpec(self):\n    example_array_spec = example_nested_array_spec(np.int32)\n    example_tensor_spec = tensor_spec.from_spec(example_array_spec)\n\n    flat_tensor_spec = tf.nest.flatten(example_tensor_spec)\n    expected_tensor_spec = tf.nest.flatten(example_nested_tensor_spec(tf.int32))\n\n    for spec, expected_spec in zip(flat_tensor_spec, expected_tensor_spec):\n      self.assertEqual(type(expected_spec), type(spec))\n      self.assertEqual(expected_spec, spec)\n\n  def testFromStringSpec(self):\n    spec = tensor_spec.from_spec(array_spec.ArraySpec([1], np.string_))\n    self.assertEqual(tf.string, spec.dtype)\n\n\nclass ToPlaceholderTest(tf.test.TestCase):\n\n  def skipIfExecutingEagerly(self):\n    # With TF 2.0 (or when executing eagerly), these tests do not make sense.\n    if tf.executing_eagerly():\n      self.skipTest(""Placeholders do not make sense when executing eagerly"")\n\n  def testCreatePlaceholderFromTuple(self):\n    self.skipIfExecutingEagerly()\n    specs = (\n        tensor_spec.TensorSpec(shape=(), dtype=tf.float32, name=""act_prob""),\n        tensor_spec.TensorSpec(shape=(), dtype=tf.float32, name=""value_pred""),\n    )\n    ph = tensor_spec.to_nest_placeholder(specs)\n    self.assertEqual(2, len(ph))\n    self.assertEqual(ph[0].name, ""act_prob:0"")\n    self.assertEqual(ph[0].dtype, tf.float32)\n    self.assertEqual(ph[0].shape, tf.TensorShape([]))\n    self.assertEqual(ph[1].name, ""value_pred:0"")\n    self.assertEqual(ph[1].dtype, tf.float32)\n    self.assertEqual(ph[1].shape, tf.TensorShape([]))\n\n  def testCreatePlaceholderFromTimeStepSpec(self):\n    self.skipIfExecutingEagerly()\n    obs_spec = tensor_spec.TensorSpec([2], tf.float32, ""obs"")\n    time_step_spec = ts.time_step_spec(obs_spec)\n    ph = tensor_spec.to_nest_placeholder(time_step_spec)\n    self.assertIsInstance(ph, ts.TimeStep)\n    self.assertEqual(ph.observation.name, ""obs:0"")\n    self.assertEqual(ph.observation.dtype, tf.float32)\n    self.assertEqual(ph.observation.shape, tf.TensorShape([2]))\n\n  def testCreatePlaceholderWithNameScope(self):\n    self.skipIfExecutingEagerly()\n    obs_spec = tensor_spec.TensorSpec([2], tf.float32, ""obs"")\n    time_step_spec = ts.time_step_spec(obs_spec)\n    ph = tensor_spec.to_nest_placeholder(time_step_spec, name_scope=""action"")\n    self.assertEqual(ph.observation.name, ""action/obs:0"")\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/system/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all system utilities.""""""\nfrom tf_agents.system import system_multiprocessing as multiprocessing\n'"
tf_agents/system/multiprocessing_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.system.multiprocessing.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\n\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nfrom tf_agents.utils import test_utils\n\n_XVAL = 1\n\n\ndef get_xval(queue):\n  global _XVAL\n  queue.put(_XVAL)\n\n\nclass XValStateSaver(multiprocessing.StateSaver):\n\n  def collect_state(self):\n    global _XVAL\n    return _XVAL\n\n  def restore_state(self, state):\n    global _XVAL\n    _XVAL = state\n\n\ndef execute_pickled_fn(ra, queue):\n  pickle.loads(ra)(queue)\n\n\nclass MultiprocessingTest(test_utils.TestCase):\n\n  def testGinBindingsInOtherProcess(self):\n    # Serialize a function that we will call in subprocesses\n    serialized_get_xval = pickle.dumps(get_xval)\n\n    # get_xval accesses _XVAL, we set the state to 2 and will check that\n    # subprocesses will see this value.\n    global _XVAL\n    _XVAL = 2\n\n    ctx = multiprocessing.get_context()\n\n    # Local function should easily access _XVAL\n    local_queue = ctx.SimpleQueue()\n    execute_pickled_fn(serialized_get_xval, local_queue)\n    self.assertFalse(local_queue.empty())\n    self.assertEqual(local_queue.get(), 2)\n\n    # Remote function can access new _XVAL since part of running it\n    # is serializing the state via XValStateSaver (passed to handle_test_main\n    # below).\n    remote_queue = ctx.SimpleQueue()\n    p = ctx.Process(\n        target=execute_pickled_fn, args=(serialized_get_xval, remote_queue))\n    p.start()\n    p.join()\n    self.assertFalse(remote_queue.empty())\n    self.assertEqual(remote_queue.get(), 2)\n\n  def testPool(self):\n    ctx = multiprocessing.get_context()\n    p = ctx.Pool(3)\n    x = 1\n    values = p.map(x.__add__, [3, 4, 5, 6, 6])\n    self.assertEqual(values, [4, 5, 6, 7, 7])\n\n\nif __name__ == \'__main__\':\n  multiprocessing.handle_test_main(\n      test_utils.main, extra_state_savers=[XValStateSaver()])\n'"
tf_agents/system/system_multiprocessing.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Multiprocessing library for TF-Agents.""""""\n\nimport multiprocessing as _multiprocessing\nfrom typing import Text\n\nimport cloudpickle\nimport gin\nimport gym\n\n\nfrom tf_agents.system.default import multiprocessing_core\nfrom tf_agents.system.default.multiprocessing_core import *  # pylint: disable=wildcard-import\n\n\n_STATE_SAVERS = multiprocessing_core._STATE_SAVERS  # pylint: disable=protected-access\n\n\n_NOT_INITIALIZED_ERROR = """"""Unable to load multiprocessing context.\n\nPlease ensure that you properly initialize your program by wrapping your main()\ncall:\n\ndef main(argv):\n  ...\n\nif __name__ == \'__main__\':\n  tf_agents.system.multiprocessing.handle_main(main, extra_state_savers=...)\n\nor, if using absl.app:\n\nif __name__ == \'__main__\':\n  tf_agents.system.multiprocessing.handle_main(\n      lambda _: absl.app.run(main), extra_state_savers=...)\n\n\nFor unit tests, this also means wrapping your test.main using handle_test_main:\n\nif __name__ == \'__main__\':\n  tf_agents.system.multiprocessing.handle_test_main(\n      tf.test.main, extra_state_savers=...)\n\nor\n\nif __name__ == \'__main__\':\n  tf_agents.system.multiprocessing.handle_test_main(\n      tf_agents.utils.test_utils.main, extra_state_savers=...)\n\nIf you are in interactive mode (e.g. python console, ipython, jupyter notebook)\nuse:\n\ntf_agents.system.multiprocessing.enable_interactive_mode(\n    extra_state_savers=...)\n\nFor more details on state savers, see the docstrings for\n`tf_agents.multiprocessing.handle_*` and:\n\nhttps://pythonspeed.com/articles/python-multiprocessing/\n""""""\n\n\ndef get_context(method: Text = None) -> _multiprocessing.context.BaseContext:\n  """"""Get a context: an object with the same API as multiprocessing module.\n\n  Args:\n    method: (Optional.) The method name; a Google-safe default is provided.\n\n  Returns:\n    A multiprocessing context.\n\n  Raises:\n    RuntimeError: If main() was not executed via handle_main().\n  """"""\n  if not multiprocessing_core.initialized():\n    raise RuntimeError(_NOT_INITIALIZED_ERROR)\n  return _rewrite_target_with_state(multiprocessing_core.get_context(method))\n\n\nclass _WrappedTargetWithState:\n  """"""Wraps a function to reload global state before it executes in a subprocess.\n\n  The `__call__` method calls `target()` after reloading global state;\n  it also logs any errors to stderr.  After creation, this object\n  will be pickled by the multiprocessing module; and its __call__ method\n  is executed in the subprocess.  We take great care to safely pickle\n  all global state returned from the state savers (see the\n  `extra_state_savers` arguments to handle_main, handle_test_main, etc).\n\n  The `__call__` also captures exceptions and logs them to stderr of the\n  main process.\n  """"""\n\n  def __init__(self, context, target):\n    """"""Store target function and global state.\n\n    This function runs on the process that\'s creating subprocesses.\n\n    Args:\n      context: An instance of a multiprocessing BaseContext.\n      target: A callable that will be run in a subprocess.\n    """"""\n    self._context = context\n    # Use cloudpickle to serialize target as this allows much more flexible\n    # target functions, e.g., lambdas, to be passed to Process()/Pool().\n    self._target = cloudpickle.dumps(target)\n    self._global_state = []\n    for saver in _STATE_SAVERS:\n      try:\n        self._global_state.append(cloudpickle.dumps(saver.collect_state()))\n      except TypeError as e:\n        context.get_logger().error(\n            \'Error while pickling global state from saver %s: %s.  Skipping.\',\n            saver, e)\n        self._global_state.append(None)\n\n  def __call__(self, *args, **kwargs):\n    """"""Load global state and run target function.\n\n    This function runs on the subprocess.\n\n    Args:\n      *args: Arguments to target.\n      **kwargs: Keyword arguments to target.\n\n    Returns:\n      Return value of target.\n\n    Raises:\n      Reraises any exceptions by target.\n    """"""\n    try:\n      if len(_STATE_SAVERS) != len(self._global_state):\n        raise RuntimeError(\n            \'Expected number of state savers to match count of state values, \'\n            \'but saw {} vs. {}\'.format(len(_STATE_SAVERS), self._global_state))\n\n      # Deserialize and restore global state\n      for saver, state in zip(_STATE_SAVERS, self._global_state):\n        if state is not None:\n          saver.restore_state(cloudpickle.loads(state))\n\n      # Perform the actual computation\n      target = cloudpickle.loads(self._target)\n      return target(*args, **kwargs)\n    except Exception as e:\n      logger = self._context.log_to_stderr()\n      logger.error(e)\n      raise e\n\n\ndef _rewrite_target_with_state(context):\n  """"""Replaces context.Process.__init__ with a fn that stores global state.""""""\n  wrapped_context = _ContextWrapper(context)\n  return wrapped_context\n\n\n# pylint: disable=invalid-name\nclass _PoolWrapper:\n  """"""Wrapper for multiprocessing Pool that wraps function.""""""\n\n  def __init__(self, context, pool):\n    self._context = context\n    self._pool = pool\n\n  def apply(self, func, args=None, kwds=None):\n    args = args or ()\n    kwds = kwds or {}\n    if func is not None:\n      func = _WrappedTargetWithState(self._context, func)\n    return self._pool.apply(func, args=args, kwds=kwds)\n\n  def apply_async(self, func, args=None, kwds=None, callback=None,\n                  error_callback=None):\n    args = args or ()\n    kwds = kwds or {}\n    if func is not None:\n      func = _WrappedTargetWithState(self._context, func)\n    return self._pool.apply_async(func,\n                                  args=args,\n                                  kwds=kwds,\n                                  callback=callback,\n                                  error_callback=error_callback)\n\n  def map(self, func, iterable, chunksize=None):\n    if func is not None:\n      func = _WrappedTargetWithState(self._context, func)\n    return self._pool.map(func, iterable=iterable, chunksize=chunksize)\n\n  def map_async(self, func, iterable, chunksize=None, callback=None,\n                error_callback=None):\n    if func is not None:\n      func = _WrappedTargetWithState(self._context, func)\n    return self._pool.map_async(func, iterable=iterable, chunksize=chunksize,\n                                callback=callback,\n                                error_callback=error_callback)\n\n  def imap(self, *args, **kwargs):\n    raise NotImplementedError(\'imap not implemented; try map\')\n\n  def imap_unordered(self, *args, **kwargs):\n    raise NotImplementedError(\'imap_unordered not implemented; try map\')\n\n  def __getattr__(self, k):\n    return getattr(self._pool, k)\n\n\nclass _ContextWrapper:\n  """"""Wrapper for a multiprocessing Context that overrides Process and Pool.""""""\n\n  def __init__(self, context):\n    self._context = context\n\n  def Process(self, group=None, target=None, name=None, args=None, kwargs=None,\n              *, daemon=None):\n    args = args or ()\n    kwargs = kwargs or {}\n    if target is not None:\n      target = _WrappedTargetWithState(self._context, target)\n    return self._context.Process(group=group, target=target, name=name,\n                                 args=args, kwargs=kwargs, daemon=daemon)\n\n  def Pool(self, processes=None, initializer=None, initargs=(),\n           maxtasksperchild=None):\n    return _PoolWrapper(\n        self._context,\n        self._context.Pool(\n            processes=processes,\n            initializer=initializer,\n            initargs=initargs,\n            maxtasksperchild=maxtasksperchild)\n    )\n\n  def __getattr__(self, k):\n    return getattr(self._context, k)\n# pylint: enable=invalid-name\n\n\nclass GinStateSaver(multiprocessing_core.StateSaver):\n  """"""Sets and restores internal gin state.""""""\n\n  def collect_state(self):\n    return gin.config.config_str()\n\n  def restore_state(self, state):\n    gin.config.parse_config(state)\n\n\nclass OpenAIGymStateSaver(multiprocessing_core.StateSaver):\n  """"""Sets and restores OpenAI gym registry.""""""\n\n  def collect_state(self):\n    return gym.envs.registration.registry\n\n  def restore_state(self, state):\n    if not isinstance(state, gym.envs.registration.EnvRegistry):\n      raise RuntimeError(\n          \'Expected gym registry object of type {}, but saw state {}\'\n          .format(gym.envs.registration.EnvRegistry, state))\n    gym.envs.registration.registry = state\n\n\n_STATE_SAVERS.extend([\n    GinStateSaver(),\n    OpenAIGymStateSaver(),\n])\n'"
tf_agents/trajectories/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Trajectories module.""""""\n\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.trajectories import trajectory\n'"
tf_agents/trajectories/policy_step.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy Step.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import NamedTuple, Optional, Union\nfrom tf_agents.typing import types\n\n\nActionType = Union[types.NestedSpecTensorOrArray, types.NestedDistribution]\n\n# Returned with every call to policy.action() and policy.distribution().\n#\n# Attributes:\n#   action: An action tensor or action distribution for `TFPolicy`, or numpy\n#     array for `PyPolicy`.\n#   state: State of the policy to be fed back into the next call to\n#     policy.action() or policy.distribution(), e.g. an RNN state. For stateless\n#     policies, this will be an empty tuple.\n#   info: Auxiliary information emitted by the policy, e.g. log probabilities of\n#     the actions. For policies without info this will be an empty tuple.\nPolicyStep = NamedTuple(\'PolicyStep\',\n                        [(\'action\', ActionType),\n                         (\'state\', types.NestedSpecTensorOrArray),\n                         (\'info\', types.NestedSpecTensorOrArray)])\n\n# Set default empty tuple for PolicyStep.state and PolicyStep.info.\nPolicyStep.__new__.__defaults__ = ((),) * len(PolicyStep._fields)\n\n\nclass CommonFields(object):\n  """"""Strings which can be used for querying returned PolicyStep.info field.\n\n  For example, use getattr(info, CommonFields.LOG_PROBABILITY, None) to check if\n  log probabilities are returned in the step or not.\n  """"""\n  LOG_PROBABILITY = \'log_probability\'\n\n\n# Generic PolicyInfo object which is recommended to be subclassed when requiring\n# that log-probabilities are returned, but having a custom namedtuple instead.\nPolicyInfo = collections.namedtuple(\'PolicyInfo\',\n                                    (CommonFields.LOG_PROBABILITY,))\n\n\ndef set_log_probability(\n    info: types.NestedTensorOrArray,\n    log_probability: types.Float) -> types.NestedTensorOrArray:\n  """"""Sets the CommonFields.LOG_PROBABILITY on info to be log_probability.""""""\n  if info in ((), None):\n    return PolicyInfo(log_probability=log_probability)\n  fields = getattr(info, \'_fields\', None)\n  if fields is not None and CommonFields.LOG_PROBABILITY in fields:\n    return info._replace(log_probability=log_probability)\n  try:\n    info[CommonFields.LOG_PROBABILITY] = log_probability\n  except TypeError:\n    pass\n  return info\n\n\ndef get_log_probability(\n    info: types.NestedTensorOrArray,\n    default_log_probability: Optional[types.Float] = None) -> types.Float:\n  """"""Gets the CommonFields.LOG_PROBABILITY from info depending on type.""""""\n  if isinstance(info, PolicyInfo):\n    return getattr(info, CommonFields.LOG_PROBABILITY, default_log_probability)\n  if hasattr(info, \'update\'):\n    return info.get(CommonFields.LOG_PROBABILITY, default_log_probability)\n\n  return default_log_probability\n'"
tf_agents/trajectories/policy_step_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.trajectories.policy_step.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.trajectories import policy_step\n\n\nclass PolicyStepTest(tf.test.TestCase):\n\n  def testCreate(self):\n    action = 1\n    state = 2\n    info = 3\n    step = policy_step.PolicyStep(action=action, state=state, info=info)\n    self.assertEqual(step.action, action)\n    self.assertEqual(step.state, state)\n    self.assertEqual(step.info, info)\n\n  def testCreateWithAllDefaults(self):\n    action = 1\n    state = ()\n    info = ()\n    step = policy_step.PolicyStep(action)\n    self.assertEqual(step.action, action)\n    self.assertEqual(step.state, state)\n    self.assertEqual(step.info, info)\n\n  def testCreateWithDefaultInfo(self):\n    action = 1\n    state = 2\n    info = ()\n    step = policy_step.PolicyStep(action, state)\n    self.assertEqual(step.action, action)\n    self.assertEqual(step.state, state)\n    self.assertEqual(step.info, info)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/trajectories/test_utils.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utility functions for testing with trajectories.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.trajectories import trajectory\n\n\ndef stacked_trajectory_from_transition(time_step, action_step, next_time_step):\n  """"""Given transitions, returns a time stacked `Trajectory`.\n\n  The tensors of the produced `Trajectory` will have a time dimension added\n  (i.e., a shape of `[B, T, ...]` where T = 2 in this case). The `Trajectory`\n  can be used when calling `agent.train()` or passed directly to `to_transition`\n  without the need for a `next_trajectory` argument.\n\n  Args:\n    time_step: A `time_step.TimeStep` representing the first step in a\n      transition.\n    action_step: A `policy_step.PolicyStep` representing actions corresponding\n      to observations from time_step.\n    next_time_step: A `time_step.TimeStep` representing the second step in a\n      transition.\n\n  Returns:\n    A time stacked `Trajectory`.\n  """"""\n  # Note that we reuse action_step and next_time_step in experience2 in order to\n  # ensure the action, policy_info, next_step_type, reward, and discount match\n  # for both values of the time dimension.\n  experience1 = trajectory.from_transition(\n      time_step, action_step, next_time_step)\n  experience2 = trajectory.from_transition(\n      next_time_step, action_step, next_time_step)\n  return tf.nest.map_structure(lambda x, y: tf.stack([x, y], axis=1),\n                               experience1, experience2)\n\n'"
tf_agents/trajectories/time_step.py,65,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TimeStep representing a step in the environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.typing import types\nfrom typing import NamedTuple, Optional\n\n\ndef _as_float32_array(a):\n  r = np.asarray(a, dtype=np.float32)\n  if np.isnan(np.sum(r)):\n    raise ValueError(\'Received a time_step input that converted to a nan array.\'\n                     \' Did you accidentally set some input value to None?.\\n\'\n                     \'Got:\\n{}\'.format(a))\n  return r\n\n\nclass TimeStep(\n    NamedTuple(\'TimeStep\', [(\'step_type\', types.SpecTensorOrArray),\n                            (\'reward\', types.NestedSpecTensorOrArray),\n                            (\'discount\', types.SpecTensorOrArray),\n                            (\'observation\', types.NestedSpecTensorOrArray)])):\n  """"""Returned with every call to `step` and `reset` on an environment.\n\n  A `TimeStep` contains the data emitted by an environment at each step of\n  interaction. A `TimeStep` holds a `step_type`, an `observation` (typically a\n  NumPy array or a dict or list of arrays), and an associated `reward` and\n  `discount`.\n\n  The first `TimeStep` in a sequence will equal `StepType.FIRST`. The final\n  `TimeStep` will equal `StepType.LAST`. All other `TimeStep`s in a sequence\n  will equal `StepType.MID.\n\n  Attributes:\n    step_type: a `Tensor` or array of `StepType` enum values.\n    reward: a `Tensor` or array of reward values.\n    discount: A discount value in the range `[0, 1]`.\n    observation: A NumPy array, or a nested dict, list or tuple of arrays.\n  """"""\n  __slots__ = ()\n\n  def is_first(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.equal(self.step_type, StepType.FIRST)\n    return np.equal(self.step_type, StepType.FIRST)\n\n  def is_mid(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.equal(self.step_type, StepType.MID)\n    return np.equal(self.step_type, StepType.MID)\n\n  def is_last(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.equal(self.step_type, StepType.LAST)\n    return np.equal(self.step_type, StepType.LAST)\n\n  def __hash__(self):\n    # TODO(b/130243327): Explore performance impact and consider converting\n    # dicts in the observation into ordered dicts in __new__ call.\n    return hash(tuple(tf.nest.flatten(self)))\n\n\nclass StepType(object):\n  """"""Defines the status of a `TimeStep` within a sequence.""""""\n  # Denotes the first `TimeStep` in a sequence.\n  FIRST = np.asarray(0, dtype=np.int32)\n  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.\n  MID = np.asarray(1, dtype=np.int32)\n  # Denotes the last `TimeStep` in a sequence.\n  LAST = np.asarray(2, dtype=np.int32)\n\n  def __new__(cls, value):\n    """"""Add ability to create StepType constants from a value.""""""\n    if value == cls.FIRST:\n      return cls.FIRST\n    if value == cls.MID:\n      return cls.MID\n    if value == cls.LAST:\n      return cls.LAST\n\n    raise ValueError(\'No known conversion for `%r` into a StepType\' % value)\n\n\ndef restart(observation: types.NestedTensorOrArray,\n            batch_size: Optional[types.Int] = None,\n            reward_spec: Optional[types.NestedSpec] = None) -> TimeStep:\n  """"""Returns a `TimeStep` with `step_type` set equal to `StepType.FIRST`.\n\n  Args:\n    observation: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    batch_size: (Optional) A python or tensorflow integer scalar.\n    reward_spec: If provided, the reward in the returned `TimeStep` will be\n      compatible with the provided `reward_spec`.\n\n  Returns:\n    A `TimeStep`.\n  """"""\n  first_observation = tf.nest.flatten(observation)[0]\n  if not tf.is_tensor(first_observation):\n    if batch_size is not None:\n      if reward_spec is None:\n        reward = np.zeros(batch_size, dtype=np.float32)\n      else:\n        reward = tf.nest.map_structure(\n            lambda r: np.zeros([batch_size] + list(r.shape), dtype=r.dtype),\n            reward_spec)\n      discount = np.ones(batch_size, dtype=np.float32)\n      step_type = np.tile(StepType.FIRST, batch_size)\n      return TimeStep(step_type, reward, discount, observation)\n    else:\n      if reward_spec is None:\n        return TimeStep(\n            StepType.FIRST,\n            _as_float32_array(0.0),\n            _as_float32_array(1.0),\n            observation)\n      else:\n        reward = tf.nest.map_structure(\n            lambda r: np.zeros(r.shape, dtype=r.dtype), reward_spec)\n        return TimeStep(\n            StepType.FIRST,\n            reward,\n            _as_float32_array(1.0),\n            observation)\n\n  # TODO(b/130244501): Check leading dimension of first_observation\n  # against batch_size if all are known statically.\n  shape = _as_multi_dim(batch_size)\n  step_type = tf.fill(shape, StepType.FIRST, name=\'step_type\')\n  if reward_spec is None:\n    reward = tf.fill(shape, _as_float32_array(0.0), name=\'reward\')\n  else:\n    reward = tf.nest.map_structure(\n        # pylint: disable=g-long-lambda\n        lambda r: tf.fill(shape + r.shape, _as_float32_array(0.0),\n                          name=\'reward\'), reward_spec)\n  discount = tf.fill(shape, _as_float32_array(1.0), name=\'discount\')\n  return TimeStep(step_type, reward, discount, observation)\n\n\ndef _as_multi_dim(maybe_scalar):\n  if maybe_scalar is None:\n    shape = ()\n  elif tf.is_tensor(maybe_scalar) and maybe_scalar.shape.rank > 0:\n    shape = maybe_scalar\n  elif np.asarray(maybe_scalar).ndim > 0:\n    shape = maybe_scalar\n  else:\n    shape = (maybe_scalar,)\n  return shape\n\n\ndef transition(observation: types.NestedTensorOrArray,\n               reward: types.NestedTensorOrArray,\n               discount: types.Float = 1.0,\n               outer_dims: Optional[types.Shape] = None) -> TimeStep:\n  """"""Returns a `TimeStep` with `step_type` set equal to `StepType.MID`.\n\n  For TF transitions, the batch size is inferred from the shape of `reward`.\n\n  If `discount` is a scalar, and `observation` contains Tensors,\n  then `discount` will be broadcasted to match `reward.shape`.\n\n  Args:\n    observation: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    reward: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    discount: (optional) A scalar, or 1D NumPy array, or tensor.\n    outer_dims: (optional) If provided, it will be used to determine the\n      batch dimensions.\n\n  Returns:\n    A `TimeStep`.\n\n  Raises:\n    ValueError: If observations are tensors but reward\'s statically known rank\n      is not `0` or `1`.\n  """"""\n  first_observation = tf.nest.flatten(observation)[0]\n  if not tf.is_tensor(first_observation):\n    if outer_dims is not None:\n      step_type = np.tile(StepType.MID, outer_dims)\n      discount = _as_float32_array(discount)\n      return TimeStep(step_type, reward, discount, observation)\n    # Infer the batch size.\n    reward = tf.nest.map_structure(_as_float32_array, reward)\n    first_reward = tf.nest.flatten(reward)[0]\n    discount = _as_float32_array(discount)\n    if first_reward.shape:\n      step_type = np.tile(StepType.MID, first_reward.shape)\n    else:\n      step_type = StepType.MID\n    return TimeStep(step_type, reward, discount, observation)\n\n  # TODO(b/130245199): If reward.shape.rank == 2, and static\n  # batch sizes are available for both first_observation and reward,\n  # check that these match.\n  reward = tf.nest.map_structure(\n      # pylint: disable=g-long-lambda\n      lambda r: tf.convert_to_tensor(\n          value=r, dtype=tf.float32, name=\'reward\'), reward)\n  if outer_dims is not None:\n    shape = outer_dims\n  else:\n    first_reward = tf.nest.flatten(reward)[0]\n    if first_reward.shape.rank == 0:\n      shape = []\n    else:\n      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n               tf.shape(input=first_reward)[0]]\n  step_type = tf.fill(shape, StepType.MID, name=\'step_type\')\n  discount = tf.convert_to_tensor(\n      value=discount, dtype=tf.float32, name=\'discount\')\n  if discount.shape.rank == 0:\n    discount = tf.fill(shape, discount, name=\'discount_fill\')\n  return TimeStep(step_type, reward, discount, observation)\n\n\ndef termination(observation: types.NestedTensorOrArray,\n                reward: types.NestedTensorOrArray,\n                outer_dims: Optional[types.Shape] = None) -> TimeStep:\n  """"""Returns a `TimeStep` with `step_type` set to `StepType.LAST`.\n\n  Args:\n    observation: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    reward: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    outer_dims: (optional) If provided, it will be used to determine the\n      batch dimensions.\n  Returns:\n    A `TimeStep`.\n\n  """"""\n  first_observation = tf.nest.flatten(observation)[0]\n  if not tf.is_tensor(first_observation):\n    if outer_dims is not None:\n      step_type = np.tile(StepType.LAST, outer_dims)\n      discount = np.zeros(outer_dims, dtype=np.float32)\n      return TimeStep(step_type, reward, discount, observation)\n\n    # Infer the batch size based on reward\n    reward = tf.nest.map_structure(_as_float32_array, reward)\n    first_reward = tf.nest.flatten(reward)[0]\n    if first_reward.shape:\n      batch_size = first_reward.shape[0]\n      step_type = np.tile(StepType.LAST, batch_size)\n      discount = np.zeros(batch_size, dtype=np.float32)\n    else:\n      step_type = StepType.LAST\n      discount = _as_float32_array(0.0)\n    return TimeStep(step_type, reward, discount, observation)\n\n  # TODO(b/130245199): If reward.shape.rank == 2, and static\n  # batch sizes are available for both first_observation and reward,\n  # check that these match.\n  reward = tf.nest.map_structure(\n      lambda r: tf.convert_to_tensor(r, dtype=tf.float32, name=\'reward\'),\n      reward)\n\n  if outer_dims is not None:\n    shape = outer_dims\n  else:\n    first_reward = tf.nest.flatten(reward)[0]\n    if first_reward.shape.rank == 0:\n      shape = []\n    else:\n      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n               tf.shape(input=first_reward)[0]]\n  step_type = tf.fill(shape, StepType.LAST, name=\'step_type\')\n  discount = tf.fill(shape, _as_float32_array(0.0), name=\'discount\')\n  return TimeStep(step_type, reward, discount, observation)\n\n\n# TODO(b/152907905): Update this function.\ndef truncation(observation: types.NestedTensorOrArray,\n               reward: types.NestedTensorOrArray,\n               discount: types.Float = 1.0,\n               outer_dims: Optional[types.Shape] = None)  -> TimeStep:\n  """"""Returns a `TimeStep` with `step_type` set to `StepType.LAST`.\n\n  If `discount` is a scalar, and `observation` contains Tensors,\n  then `discount` will be broadcasted to match the outer dimensions.\n\n  Args:\n    observation: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    reward: A NumPy array, tensor, or a nested dict, list or tuple of\n      arrays or tensors.\n    discount: (optional) A scalar, or 1D NumPy array, or tensor.\n    outer_dims: (optional) If provided, it will be used to determine the\n      batch dimensions.\n\n  Returns:\n    A `TimeStep`.\n  """"""\n  first_observation = tf.nest.flatten(observation)[0]\n  if not tf.is_tensor(first_observation):\n    if outer_dims is not None:\n      step_type = np.tile(StepType.LAST, outer_dims)\n      discount = _as_float32_array(discount)\n      return TimeStep(step_type, reward, discount, observation)\n    # Infer the batch size.\n    reward = tf.nest.map_structure(_as_float32_array, reward)\n    first_reward = tf.nest.flatten(reward)[0]\n    discount = _as_float32_array(discount)\n    if first_reward.shape:\n      step_type = np.tile(StepType.LAST, first_reward.shape)\n    else:\n      step_type = StepType.LAST\n    return TimeStep(step_type, reward, discount, observation)\n\n  reward = tf.nest.map_structure(\n      lambda r: tf.convert_to_tensor(value=r, name=\'reward\'),\n      reward)\n  if outer_dims is not None:\n    shape = outer_dims\n  else:\n    first_reward = tf.nest.flatten(reward)[0]\n    if first_reward.shape.rank == 0:\n      shape = []\n    else:\n      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n               tf.shape(input=first_reward)[0]]\n  step_type = tf.fill(shape, StepType.LAST, name=\'step_type\')\n  discount = tf.convert_to_tensor(\n      value=discount, dtype=tf.float32, name=\'discount\')\n  if discount.shape.rank == 0:\n    discount = tf.fill(shape, discount, name=\'discount_fill\')\n  return TimeStep(step_type, reward, discount, observation)\n\n\ndef time_step_spec(\n    observation_spec: Optional[types.NestedSpec] = None,\n    reward_spec: Optional[types.NestedSpec] = None) -> TimeStep:\n  """"""Returns a `TimeStep` spec given the observation_spec.\n\n  Args:\n    observation_spec: A nest of `tf.TypeSpec` or `ArraySpec` objects.\n    reward_spec: (Optional) A nest of `tf.TypeSpec` or `ArraySpec` objects.\n      Default - a scalar float32 of the same type (Tensor or Array) as\n      `observation_spec`.\n\n  Returns:\n    A `TimeStep` with the same types (`TypeSpec` or `ArraySpec`) as\n    the first entry in `observation_spec`.\n\n  Raises:\n    TypeError: If observation and reward specs aren\'t both either tensor type\n      specs or array type specs.\n  """"""\n  if observation_spec is None:\n    return TimeStep(step_type=(), reward=(), discount=(), observation=())\n\n  first_observation_spec = tf.nest.flatten(observation_spec)[0]\n  if reward_spec is not None:\n    first_reward_spec = tf.nest.flatten(reward_spec)[0]\n    if (isinstance(first_reward_spec, tf.TypeSpec)\n        != isinstance(first_observation_spec, tf.TypeSpec)):\n      raise TypeError(\n          \'Expected observation and reward specs to both be either tensor or \'\n          \'array specs, but saw spec values {} vs. {}\'\n          .format(first_observation_spec, first_reward_spec))\n  if isinstance(first_observation_spec, tf.TypeSpec):\n    return TimeStep(\n        step_type=tensor_spec.TensorSpec([], tf.int32, name=\'step_type\'),\n        reward=reward_spec or tf.TensorSpec([], tf.float32, name=\'reward\'),\n        discount=tensor_spec.BoundedTensorSpec(\n            [], tf.float32, minimum=0.0, maximum=1.0, name=\'discount\'),\n        observation=observation_spec)\n  return TimeStep(\n      step_type=array_spec.ArraySpec([], np.int32, name=\'step_type\'),\n      reward=reward_spec or array_spec.ArraySpec([], np.float32, name=\'reward\'),\n      discount=array_spec.BoundedArraySpec(\n          [], np.float32, minimum=0.0, maximum=1.0, name=\'discount\'),\n      observation=observation_spec)\n'"
tf_agents/trajectories/time_step_test.py,40,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for trajectories.time_step.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import nest_utils\n\n\nclass TimeStepTest(tf.test.TestCase):\n\n  def testRestart(self):\n    observation = -1\n    time_step = ts.restart(observation)\n\n    self.assertEqual(ts.StepType.FIRST, time_step.step_type)\n    self.assertEqual(-1, time_step.observation)\n    self.assertEqual(0.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n\n  def testTransition(self):\n    observation = -1\n    reward = 2.0\n    discount = 1.0\n    time_step = ts.transition(observation, reward, discount)\n\n    self.assertEqual(ts.StepType.MID, time_step.step_type)\n    self.assertEqual(-1, time_step.observation)\n    self.assertEqual(2.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n\n  def testTermination(self):\n    observation = -1\n    reward = 2.0\n    time_step = ts.termination(observation, reward)\n\n    self.assertEqual(ts.StepType.LAST, time_step.step_type)\n    self.assertEqual(-1, time_step.observation)\n    self.assertEqual(2.0, time_step.reward)\n    self.assertEqual(0.0, time_step.discount)\n\n  def testTruncation(self):\n    observation = -1\n    reward = 2.0\n    discount = 1.0\n    time_step = ts.truncation(observation, reward, discount)\n\n    self.assertEqual(ts.StepType.LAST, time_step.step_type)\n    self.assertEqual(-1, time_step.observation)\n    self.assertEqual(2.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n\n  def testRestartIsFirst(self):\n    observation = -1\n    time_step = ts.restart(observation)\n    self.assertTrue(time_step.is_first())\n\n  def testTransitionIsMid(self):\n    observation = -1\n    reward = 2.0\n    time_step = ts.transition(observation, reward)\n    self.assertTrue(time_step.is_mid())\n\n  def testTerminationIsLast(self):\n    observation = -1\n    reward = 2.0\n    time_step = ts.termination(observation, reward)\n    self.assertTrue(time_step.is_last())\n\n  def testLastNumpy(self):\n    observation = -1\n    reward = 2.0\n    discount = 1.0\n    time_step = ts.TimeStep(np.asarray(ts.StepType.LAST),\n                            np.asarray(reward),\n                            np.asarray(discount),\n                            np.asarray(observation))\n    self.assertTrue(time_step.is_last())\n    self.assertEqual(ts.StepType.LAST, time_step.step_type)\n    self.assertEqual(-1, time_step.observation)\n    self.assertEqual(2.0, time_step.reward)\n    self.assertEqual(1.0, time_step.discount)\n\n  def testRestartBatched(self):\n    observation = np.array([[-1], [-1]])\n    time_step = ts.restart(observation, batch_size=2)\n\n    self.assertItemsEqual([ts.StepType.FIRST] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual([0.0, 0.0], time_step.reward)\n    self.assertItemsEqual([1.0, 1.0], time_step.discount)\n\n  def testRestartMultiRewards(self):\n    observation = np.array([[-1], [-1]])\n    reward_spec = [array_spec.ArraySpec((1,), np.float32, \'r1\'),\n                   array_spec.ArraySpec((2,), np.float32, \'r2\')]\n    time_step = ts.restart(observation, batch_size=2, reward_spec=reward_spec)\n\n    expected_reward = [np.array([[0.], [0.]], dtype=np.float32),\n                       np.array([[0., 0.], [0., 0.]], dtype=np.float32)]\n\n    self.assertItemsEqual([ts.StepType.FIRST] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertAllEqual(expected_reward[0], time_step.reward[0])\n    self.assertAllEqual(expected_reward[1], time_step.reward[1])\n    self.assertItemsEqual([1.0, 1.0], time_step.discount)\n\n  def testTransitionBatched(self):\n    observation = np.array([[-1], [-1]])\n    reward = np.array([2., 2.])\n    discount = np.array([1., 1.])\n    time_step = ts.transition(observation, reward, discount)\n\n    self.assertItemsEqual([ts.StepType.MID] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual(reward, time_step.reward)\n    self.assertItemsEqual(discount, time_step.discount)\n\n  def testTransitionMultiRewards(self):\n    observation = np.array([[-1], [-1]])\n    reward = [np.array([[2.], [2.]]),\n              np.array([[3., 3.], [4., 4.]])]\n    discount = np.array([1., 1.])\n    time_step = ts.transition(observation, reward, discount)\n\n    time_step_with_outerdims = ts.transition(\n        observation, reward, discount, outer_dims=[2])\n\n    self.assertItemsEqual([ts.StepType.MID] * 2, time_step.step_type)\n    self.assertItemsEqual(\n        [ts.StepType.MID] * 2, time_step_with_outerdims.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual(observation, time_step_with_outerdims.observation)\n    self.assertAllEqual(reward[0], time_step.reward[0])\n    self.assertAllEqual(reward[1], time_step.reward[1])\n    self.assertAllEqual(reward[0], time_step_with_outerdims.reward[0])\n    self.assertAllEqual(reward[1], time_step_with_outerdims.reward[1])\n    self.assertItemsEqual(discount, time_step.discount)\n    self.assertItemsEqual(discount, time_step_with_outerdims.discount)\n\n  def testTerminationBatched(self):\n    observation = np.array([[-1], [-1]])\n    reward = np.array([2., 2.])\n    time_step = ts.termination(observation, reward)\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual(reward, time_step.reward)\n    self.assertItemsEqual([0., 0.], time_step.discount)\n\n  def testTerminationMultiRewards(self):\n    observation = np.array([[-1], [-1]])\n    reward = [np.array([[2.], [2.]]),\n              np.array([[3., 3.], [4., 4.]])]\n    time_step = ts.termination(observation, reward)\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertAllEqual(reward[0], time_step.reward[0])\n    self.assertAllEqual(reward[1], time_step.reward[1])\n    self.assertItemsEqual([0., 0.], time_step.discount)\n\n    reward = np.array([[2., 2., 2.], [3., 3., 3.]])\n    reward_spec = [array_spec.ArraySpec((3,), np.float32, \'multi_r\')]\n    outer_dims = nest_utils.get_outer_array_shape(reward, reward_spec)\n    time_step_batch = ts.termination(observation, reward, outer_dims)\n\n    # Check that passing outer_dims works\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step_batch.step_type)\n    self.assertItemsEqual(observation, time_step_batch.observation)\n    self.assertAllEqual(reward[0], time_step_batch.reward[0])\n    self.assertAllEqual(reward[1], time_step_batch.reward[1])\n    self.assertItemsEqual([0., 0.], time_step_batch.discount)\n\n    # Check that it gets a different result with no outer_dims\n    time_step_no_batch = ts.termination(observation, reward, outer_dims=[])\n    self.assertEqual(ts.StepType.LAST, time_step_no_batch.step_type)\n    self.assertItemsEqual(observation, time_step_no_batch.observation)\n    self.assertAllEqual(reward[0], time_step_no_batch.reward[0])\n    self.assertAllEqual(reward[1], time_step_no_batch.reward[1])\n    self.assertEqual(0., time_step_no_batch.discount)\n\n  def testTruncationBatched(self):\n    observation = np.array([[-1], [-1]])\n    reward = np.array([2., 2.])\n    discount = np.array([1., 1.])\n    time_step = ts.truncation(observation, reward, discount)\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual(reward, time_step.reward)\n    self.assertItemsEqual(discount, time_step.discount)\n\n  def testTruncationMultiRewards(self):\n    observation = np.array([[-1], [-1]])\n    reward = [np.array([[2.], [2.]]),\n              np.array([[3., 3.], [4., 4.]])]\n    discount = np.array([1., 1.])\n    time_step = ts.truncation(observation, reward, discount)\n\n    time_step_with_outerdims = ts.truncation(\n        observation, reward, discount, outer_dims=[2])\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step.step_type)\n    self.assertItemsEqual(\n        [ts.StepType.LAST] * 2, time_step_with_outerdims.step_type)\n    self.assertItemsEqual(observation, time_step.observation)\n    self.assertItemsEqual(observation, time_step_with_outerdims.observation)\n    self.assertAllEqual(reward[0], time_step.reward[0])\n    self.assertAllEqual(reward[1], time_step.reward[1])\n    self.assertAllEqual(reward[0], time_step_with_outerdims.reward[0])\n    self.assertAllEqual(reward[1], time_step_with_outerdims.reward[1])\n    self.assertItemsEqual(discount, time_step.discount)\n    self.assertItemsEqual(discount, time_step_with_outerdims.discount)\n\n\nclass TimeStepSpecTest(tf.test.TestCase):\n\n  def testObservationSpec(self):\n    observation_spec = [array_spec.ArraySpec((1, 2, 3), np.int32, \'obs1\'),\n                        array_spec.ArraySpec((1, 2, 3), np.float32, \'obs2\')]\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    self.assertEqual(time_step_spec.observation, observation_spec)\n    self.assertEqual(time_step_spec.step_type,\n                     array_spec.ArraySpec([], np.int32, name=\'step_type\'))\n    self.assertEqual(time_step_spec.reward,\n                     array_spec.ArraySpec([], np.float32, name=\'reward\'))\n    self.assertEqual(time_step_spec.discount,\n                     array_spec.BoundedArraySpec([], np.float32,\n                                                 minimum=0.0, maximum=1.0,\n                                                 name=\'discount\'))\n\n\nclass TFTimeStepTest(tf.test.TestCase):\n\n  def testRestart(self):\n    observation = tf.constant(-1)\n    time_step = ts.restart(observation)\n    time_step_ = self.evaluate(time_step)\n    self.assertEqual(ts.StepType.FIRST, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertEqual(0.0, time_step_.reward)\n    self.assertEqual(1.0, time_step_.discount)\n\n  def testBatchRestart(self):\n    obs_spec = [tensor_spec.TensorSpec([2], tf.float32)]\n    time_step_spec = ts.time_step_spec(obs_spec)\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, 2)\n    time_step_batched = nest_utils.is_batched_nested_tensors(\n        time_steps, time_step_spec)\n    self.assertTrue(time_step_batched)\n\n  def testRestartMultiRewards(self):\n    observation = tf.constant(-1)\n    reward_spec = [tensor_spec.TensorSpec((1,), tf.float32, \'r1\'),\n                   tensor_spec.TensorSpec((2,), tf.float32, \'r2\')]\n    time_step = ts.restart(observation, batch_size=2, reward_spec=reward_spec)\n    time_step_ = self.evaluate(time_step)\n\n    expected_reward = [np.array([[0.], [0.]], dtype=np.float32),\n                       np.array([[0., 0.], [0., 0.]], dtype=np.float32)]\n\n    self.assertItemsEqual([ts.StepType.FIRST] * 2, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertAllEqual(expected_reward[0], time_step_.reward[0])\n    self.assertAllEqual(expected_reward[1], time_step_.reward[1])\n    self.assertItemsEqual([1.0, 1.0], time_step_.discount)\n\n  def testTransition(self):\n    observation = tf.constant(-1)\n    reward = tf.constant(2.0)\n    discount = tf.constant(1.0)\n    time_step = ts.transition(observation, reward, discount)\n    time_step_ = self.evaluate(time_step)\n    self.assertEqual(ts.StepType.MID, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertEqual(2.0, time_step_.reward)\n    self.assertEqual(1.0, time_step_.discount)\n\n  def testTransitionMultiRewards(self):\n    observation = tf.constant([[-1], [-1]])\n    reward = [tf.constant([[2.], [2.]]),\n              tf.constant([[3., 3.], [4., 4.]])]\n    discount = tf.constant(0.5)\n    time_step = ts.transition(observation, reward, discount)\n    time_step_ = self.evaluate(time_step)\n\n    time_step_with_outerdims = ts.transition(\n        observation, reward, discount, outer_dims=[2])\n    time_step_with_outerdims_ = self.evaluate(time_step_with_outerdims)\n\n    self.assertItemsEqual([ts.StepType.MID] * 2, time_step_.step_type)\n    self.assertItemsEqual(\n        [ts.StepType.MID] * 2, time_step_with_outerdims_.step_type)\n    self.assertItemsEqual([-1, -1], time_step_.observation)\n    self.assertItemsEqual([-1, -1], time_step_with_outerdims_.observation)\n    self.assertAllEqual(reward[0], time_step_.reward[0])\n    self.assertAllEqual(reward[1], time_step_.reward[1])\n    self.assertAllEqual(reward[0], time_step_with_outerdims_.reward[0])\n    self.assertAllEqual(reward[1], time_step_with_outerdims_.reward[1])\n    self.assertItemsEqual([0.5, 0.5], time_step_.discount)\n    self.assertItemsEqual([0.5, 0.5], time_step_with_outerdims_.discount)\n\n  def testTermination(self):\n    observation = tf.constant(-1)\n    reward = tf.constant(2.0)\n    time_step = ts.termination(observation, reward)\n    time_step_ = self.evaluate(time_step)\n    self.assertEqual(ts.StepType.LAST, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertEqual(2.0, time_step_.reward)\n    self.assertEqual(0.0, time_step_.discount)\n\n  def testTruncation(self):\n    observation = tf.constant(-1)\n    reward = tf.constant(2.0)\n    discount = tf.constant(1.0)\n    time_step = ts.truncation(observation, reward, discount)\n    time_step_ = self.evaluate(time_step)\n    self.assertEqual(ts.StepType.LAST, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertEqual(2.0, time_step_.reward)\n    self.assertEqual(1.0, time_step_.discount)\n\n  def testRestartIsFirst(self):\n    observation = tf.constant(-1)\n    time_step = ts.restart(observation)\n    is_first = time_step.is_first()\n    self.assertEqual(True, self.evaluate(is_first))\n\n  def testTransitionIsMid(self):\n    observation = tf.constant(-1)\n    reward = tf.constant(2.0)\n    time_step = ts.transition(observation, reward)\n    is_mid = time_step.is_mid()\n    self.assertEqual(True, self.evaluate(is_mid))\n\n  def testTerminationIsLast(self):\n    observation = tf.constant(-1)\n    reward = tf.constant(2.0)\n    time_step = ts.termination(observation, reward)\n    is_last = time_step.is_last()\n    self.assertEqual(True, self.evaluate(is_last))\n\n  def testTerminationMultiRewards(self):\n    observation = tf.constant(-1)\n    reward = [tf.constant([[2.], [2.]]),\n              tf.constant([[3., 3.], [4., 4.]])]\n    time_step = ts.termination(observation, reward)\n    time_step_ = self.evaluate(time_step)\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step_.step_type)\n    self.assertEqual(-1, time_step_.observation)\n    self.assertAllEqual(reward[0], time_step_.reward[0])\n    self.assertAllEqual(reward[1], time_step_.reward[1])\n    self.assertItemsEqual([0.0, 0.0], time_step_.discount)\n\n  def testTruncationMultiRewards(self):\n    observation = tf.constant([[-1], [-1]])\n    reward = [tf.constant([[2.], [2.]]),\n              tf.constant([[3., 3.], [4., 4.]])]\n    discount = tf.constant(0.5)\n    time_step = ts.truncation(observation, reward, discount)\n    time_step_ = self.evaluate(time_step)\n\n    time_step_with_outerdims = ts.truncation(\n        observation, reward, discount, outer_dims=[2])\n    time_step_with_outerdims_ = self.evaluate(time_step_with_outerdims)\n\n    self.assertItemsEqual([ts.StepType.LAST] * 2, time_step_.step_type)\n    self.assertItemsEqual([ts.StepType.LAST] * 2,\n                          time_step_with_outerdims_.step_type)\n    self.assertItemsEqual([-1, -1], time_step_.observation)\n    self.assertItemsEqual([-1, -1], time_step_with_outerdims_.observation)\n    self.assertAllEqual(reward[0], time_step_.reward[0])\n    self.assertAllEqual(reward[1], time_step_.reward[1])\n    self.assertAllEqual(reward[0], time_step_with_outerdims_.reward[0])\n    self.assertAllEqual(reward[1], time_step_with_outerdims_.reward[1])\n    self.assertItemsEqual([0.5, 0.5], time_step_.discount)\n\n  def testNoneValuesCaught(self):\n    observation = (1, 2)\n    reward = (None, None)\n    with self.assertRaises(ValueError):\n      ts.transition(observation, reward)\n\n\nclass TFTimeStepSpecTest(tf.test.TestCase):\n\n  def testObservationSpec(self):\n    observation_spec = [tensor_spec.TensorSpec((1, 2, 3), tf.int32, \'obs1\'),\n                        tensor_spec.TensorSpec((1, 2, 3), tf.float32, \'obs2\')]\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    self.assertEqual(time_step_spec.observation, observation_spec)\n    self.assertEqual(time_step_spec.step_type,\n                     tensor_spec.TensorSpec([], tf.int32, name=\'step_type\'))\n    self.assertEqual(time_step_spec.reward,\n                     tensor_spec.TensorSpec([], tf.float32, name=\'reward\'))\n    self.assertEqual(time_step_spec.discount,\n                     tensor_spec.BoundedTensorSpec([], tf.float32,\n                                                   minimum=0.0, maximum=1.0,\n                                                   name=\'discount\'))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/trajectories/trajectory.py,46,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Trajectory containing time_step transition information.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\n\nimport functools\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import composite\nfrom tf_agents.utils import nest_utils\n\nfrom typing import NamedTuple, Optional, Tuple\n\n\nclass Trajectory(\n    NamedTuple(\'Trajectory\', [\n        (\'step_type\', types.SpecTensorOrArray),\n        (\'observation\', types.NestedSpecTensorOrArray),\n        (\'action\', types.NestedSpecTensorOrArray),\n        (\'policy_info\', types.NestedSpecTensorOrArray),\n        (\'next_step_type\', types.SpecTensorOrArray),\n        (\'reward\', types.NestedSpecTensorOrArray),\n        (\'discount\', types.SpecTensorOrArray),\n    ])):\n  """"""A tuple that represents a trajectory.\n\n  A `Trajectory` is a sequence of aligned time steps. It captures the\n  observation, step_type from current time step with the computed action\n  and policy_info. Discount, reward and next_step_type come from the next\n  time step.\n\n  Attributes:\n    step_type: A `StepType`.\n    observation: An array (tensor), or a nested dict, list or tuple of arrays\n      (tensors) that represents the observation.\n    action: An array/a tensor, or a nested dict, list or tuple of actions. This\n      represents action generated according to the observation.\n    policy_info: An arbitrary nest that contains auxiliary information related\n      to the action. Note that this does not include the policy/RNN state which\n      was used to generate the action.\n    next_step_type: The `StepType` of the next time step.\n    reward: An array/a tensor, or a nested dict, list, or tuple of rewards.\n      This represents the rewards and/or constraint satisfiability after\n      performing the action in an environment.\n    discount: A scalar that representing the discount factor to multiply with\n      future rewards.\n  """"""\n  __slots__ = ()\n\n  def is_first(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.equal(self.step_type, ts.StepType.FIRST)\n    return self.step_type == ts.StepType.FIRST\n\n  def is_mid(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.logical_and(\n          tf.equal(self.step_type, ts.StepType.MID),\n          tf.equal(self.next_step_type, ts.StepType.MID))\n    return (self.step_type == ts.StepType.MID) & (\n        self.next_step_type == ts.StepType.MID)\n\n  def is_last(self) -> types.Bool:\n    if tf.is_tensor(self.next_step_type):\n      return tf.equal(self.next_step_type, ts.StepType.LAST)\n    return self.next_step_type == ts.StepType.LAST\n\n  def is_boundary(self) -> types.Bool:\n    if tf.is_tensor(self.step_type):\n      return tf.equal(self.step_type, ts.StepType.LAST)\n    return self.step_type == ts.StepType.LAST\n\n  def replace(self, **kwargs):\n    """"""Exposes as namedtuple._replace.\n\n    Usage:\n    ```\n      new_trajectory = trajectory.replace(policy_info=())\n    ```\n\n    This returns a new trajectory with an empty policy_info.\n\n    Args:\n      **kwargs: key/value pairs of fields in the trajectory.\n\n    Returns:\n      A new `Trajectory`.\n    """"""\n    return self._replace(**kwargs)\n\n\ndef _create_trajectory(\n    observation,\n    action,\n    policy_info,\n    reward,\n    discount,\n    step_type,\n    next_step_type,\n    name_scope):\n  """"""Create a Trajectory composed of either Tensors or numpy arrays.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`;\n      shaped `[B]`, `[T]`, or `[B, T]` (optional).\n    step_type: `Tensor` or `np.ndarray` of `ts.StepType`,\n      shaped `[B]`, `[T]`, or `[B, T]`.\n    next_step_type: `Tensor` or `np.ndarray` of `ts.StepType`,\n      shaped `[B]`, `[T]`, or `[B, T]`.\n    name_scope: Python string, name to use when creating tensors.\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  if nest_utils.has_tensors(\n      observation, action, policy_info, reward, discount):\n    with tf.name_scope(name_scope):\n      discount = tf.identity(discount)\n      shape = tf.shape(input=discount)\n      make_tensors = lambda struct: tf.nest.map_structure(tf.identity, struct)\n      return Trajectory(\n          step_type=tf.fill(shape, step_type),\n          observation=make_tensors(observation),\n          action=make_tensors(action),\n          policy_info=make_tensors(policy_info),\n          next_step_type=tf.fill(shape, next_step_type),\n          reward=make_tensors(reward),\n          discount=discount)\n  else:\n    discount = np.asarray(discount)\n    shape = discount.shape\n    make_arrays = lambda struct: tf.nest.map_structure(np.asarray, struct)\n    return Trajectory(\n        step_type=np.full(shape, step_type),\n        observation=make_arrays(observation),\n        action=make_arrays(action),\n        policy_info=make_arrays(policy_info),\n        next_step_type=np.full(shape, next_step_type),\n        reward=make_arrays(reward),\n        discount=discount)\n\n\ndef first(observation: types.NestedSpecTensorOrArray,\n          action: types.NestedSpecTensorOrArray,\n          policy_info: types.NestedSpecTensorOrArray,\n          reward: types.NestedSpecTensorOrArray,\n          discount: types.SpecTensorOrArray) -> Trajectory:\n  """"""Create a Trajectory transitioning between StepTypes `FIRST` and `MID`.\n\n  All inputs may be batched.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`;\n      shaped `[B]`, `[T]`, or `[B, T]` (optional).\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  return _create_trajectory(observation,\n                            action,\n                            policy_info,\n                            reward,\n                            discount,\n                            name_scope=\'trajectory_first\',\n                            step_type=ts.StepType.FIRST,\n                            next_step_type=ts.StepType.MID)\n\n\ndef mid(observation: types.NestedSpecTensorOrArray,\n        action: types.NestedSpecTensorOrArray,\n        policy_info: types.NestedSpecTensorOrArray,\n        reward: types.NestedSpecTensorOrArray,\n        discount: types.SpecTensorOrArray) -> Trajectory:\n  """"""Create a Trajectory transitioning between StepTypes `MID` and `MID`.\n\n  All inputs may be batched.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`;\n      shaped `[B]`, `[T]`, or `[B, T]` (optional).\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  return _create_trajectory(observation,\n                            action,\n                            policy_info,\n                            reward,\n                            discount,\n                            name_scope=\'trajectory_mid\',\n                            step_type=ts.StepType.MID,\n                            next_step_type=ts.StepType.MID)\n\n\ndef last(observation: types.NestedSpecTensorOrArray,\n         action: types.NestedSpecTensorOrArray,\n         policy_info: types.NestedSpecTensorOrArray,\n         reward: types.NestedSpecTensorOrArray,\n         discount: types.SpecTensorOrArray) -> Trajectory:\n  """"""Create a Trajectory transitioning between StepTypes `MID` and `LAST`.\n\n  All inputs may be batched.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`;\n      shaped `[B]`, `[T]`, or `[B, T]` (optional).\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  return _create_trajectory(observation,\n                            action,\n                            policy_info,\n                            reward,\n                            discount,\n                            name_scope=\'trajectory_last\',\n                            step_type=ts.StepType.MID,\n                            next_step_type=ts.StepType.LAST)\n\n\ndef single_step(observation: types.NestedSpecTensorOrArray,\n                action: types.NestedSpecTensorOrArray,\n                policy_info: types.NestedSpecTensorOrArray,\n                reward: types.NestedSpecTensorOrArray,\n                discount: types.SpecTensorOrArray) -> Trajectory:\n  """"""Create a Trajectory transitioning between StepTypes `FIRST` and `LAST`.\n\n  All inputs may be batched.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped `[B,\n      ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped `[B,\n      ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`; shaped `[B]`,\n      `[T]`, or `[B, T]` (optional).\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  return _create_trajectory(\n      observation,\n      action,\n      policy_info,\n      reward,\n      discount,\n      name_scope=\'trajectory_single_step\',\n      step_type=ts.StepType.FIRST,\n      next_step_type=ts.StepType.LAST)\n\n\ndef boundary(observation: types.NestedSpecTensorOrArray,\n             action: types.NestedSpecTensorOrArray,\n             policy_info: types.NestedSpecTensorOrArray,\n             reward: types.NestedSpecTensorOrArray,\n             discount: types.SpecTensorOrArray) -> Trajectory:\n  """"""Create a Trajectory transitioning between StepTypes `LAST` and `FIRST`.\n\n  All inputs may be batched.\n\n  The input `discount` is used to infer the outer shape of the inputs,\n  as it is always expected to be a singleton array with scalar inner shape.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`;\n      all shaped `[B, ...]`, `[T, ...]`, or `[B, T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`;\n      shaped `[B]`, `[T]`, or `[B, T]` (optional).\n\n  Returns:\n    A `Trajectory` instance.\n  """"""\n  return _create_trajectory(observation,\n                            action,\n                            policy_info,\n                            reward,\n                            discount,\n                            name_scope=\'trajectory_boundary\',\n                            step_type=ts.StepType.LAST,\n                            next_step_type=ts.StepType.FIRST)\n\n\ndef _maybe_static_outer_dim(t):\n  """"""Return the left-most dense shape dimension of `t`.\n\n  Args:\n    t: A `Tensor` or `CompositeTensor`.\n\n  Returns:\n    A python integer or `0-D` scalar tensor with type `int64`.\n  """"""\n  assert tf.is_tensor(t), t\n  if isinstance(t, tf.SparseTensor):\n    static_shape = tf.get_static_value(t.dense_shape)\n    if static_shape is not None:\n      return static_shape[0]\n    else:\n      return t.dense_shape[0]\n  elif isinstance(t, tf.RaggedTensor):\n    outer_dim = tf.compat.dimension_value(t.shape[0])\n    return outer_dim if outer_dim is not None else t.nrows()\n  else:\n    outer_dim = tf.compat.dimension_value(t.shape[0])\n    return outer_dim if outer_dim is not None else tf.shape(t)[0]\n\n\ndef from_episode(\n    observation: types.NestedSpecTensorOrArray,\n    action: types.NestedSpecTensorOrArray,\n    policy_info: types.NestedSpecTensorOrArray,\n    reward: types.NestedSpecTensorOrArray,\n    discount: Optional[types.SpecTensorOrArray] = None) -> Trajectory:\n  """"""Create a Trajectory from tensors representing a single episode.\n\n  If none of the inputs are tensors, then numpy arrays are generated instead.\n\n  If `discount` is not provided, the first entry in `reward` is used to estimate\n  `T`:\n\n  ```\n  reward_0 = tf.nest.flatten(reward)[0]\n  T = shape(reward_0)[0]\n  ```\n\n  In this case, a `discount` of all ones having dtype `float32` is generated.\n\n  **NOTE**: all tensors/numpy arrays passed to this function have the same time\n  dimension `T`. When the generated trajectory passes through `to_transition`,\n  it will only return a `(time_steps, next_time_steps)` pair with `T - 1` in the\n  time dimension, which means the reward at step T is dropped. So if the reward\n  at step `T` is important, please make sure the episode passed to this function\n  contains an additional step.\n\n  Args:\n    observation: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[T, ...]`.\n    action: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[T, ...]`.\n    policy_info: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[T, ...]`.\n    reward: (possibly nested tuple of) `Tensor` or `np.ndarray`; all shaped\n      `[T, ...]`.\n    discount: A floating point vector `Tensor` or `np.ndarray`; shaped `[T]`\n      (optional).\n\n  Returns:\n    An instance of `Trajectory`.\n  """"""\n  use_tensors = nest_utils.has_tensors(\n      observation, action, policy_info, reward, discount)\n  map_structure = functools.partial(\n      tf.nest.map_structure, expand_composites=True)\n  if use_tensors:\n    ones_fn = tf.ones\n    float_dtype = tf.float32\n    convert_fn = tf.convert_to_tensor\n    concat_fn = tf.concat\n    maximum_fn = tf.maximum\n    fill_fn = tf.fill\n    identity_map = lambda struct: map_structure(tf.identity, struct)\n  else:\n    ones_fn = np.ones\n    float_dtype = np.float32\n    convert_fn = np.asarray\n    concat_fn = np.concatenate\n    maximum_fn = np.maximum\n    fill_fn = np.full\n    identity_map = lambda struct: map_structure(np.asarray, struct)\n\n  def _from_episode(observation, action, policy_info, reward, discount):\n    """"""Implementation of from_episode.""""""\n    if discount is not None:\n      time_source = discount\n    else:\n      time_source = tf.nest.flatten(reward)[0]\n    if tf.is_tensor(time_source):\n      num_frames = _maybe_static_outer_dim(time_source)\n    else:\n      num_frames = np.shape(time_source)[0]\n    if discount is None:\n      discount = ones_fn([num_frames], dtype=float_dtype)\n\n    if not tf.is_tensor(num_frames):\n\n      def check_num_frames(t):\n        if tf.is_tensor(t):\n          outer_dim = _maybe_static_outer_dim(t)\n        else:\n          outer_dim = t.shape[0]\n        if not tf.is_tensor(outer_dim) and outer_dim != num_frames:\n          raise ValueError(\'Expected first dimension to be {}, \'\n                           \'but saw outer dim: {}\'.format(num_frames,\n                                                          outer_dim))\n\n      tf.nest.map_structure(\n          check_num_frames,\n          (observation, action, policy_info, reward, discount),\n          expand_composites=False)\n\n    ts_first = convert_fn(ts.StepType.FIRST)\n    ts_last = convert_fn(ts.StepType.LAST)\n    mid_size = maximum_fn(0, num_frames - 1)\n    ts_mid = fill_fn([mid_size], ts.StepType.MID)\n    step_type = concat_fn(([ts_first], ts_mid), axis=0)\n    next_step_type = concat_fn((ts_mid, [ts_last]), axis=0)\n\n    return Trajectory(\n        step_type=step_type,\n        observation=identity_map(observation),\n        action=identity_map(action),\n        policy_info=identity_map(policy_info),\n        next_step_type=next_step_type,\n        reward=identity_map(reward),\n        discount=identity_map(discount))\n\n  if use_tensors:\n    with tf.name_scope(\'from_episode\'):\n      return _from_episode(observation, action, policy_info, reward, discount)\n  else:\n    return _from_episode(observation, action, policy_info, reward, discount)\n\n\ndef from_transition(time_step: ts.TimeStep,\n                    action_step: policy_step.PolicyStep,\n                    next_time_step: ts.TimeStep) -> Trajectory:\n  """"""Returns a `Trajectory` given transitions.\n\n  `from_transition` is used by a driver to convert sequence of transitions into\n  a `Trajectory` for efficient storage. Then an agent (e.g.\n  `ppo_agent.PPOAgent`) converts it back to transitions by invoking\n  `to_transition`.\n\n  Note that this method does not add a time dimension to the Tensors in the\n  resulting `Trajectory`. This means that if your transitions don\'t already\n  include a time dimension, the `Trajectory` cannot be passed to\n  `agent.train()`.\n\n  Args:\n    time_step: A `time_step.TimeStep` representing the first step in a\n      transition.\n    action_step: A `policy_step.PolicyStep` representing actions corresponding\n      to observations from time_step.\n    next_time_step: A `time_step.TimeStep` representing the second step in a\n      transition.\n  """"""\n  return Trajectory(\n      step_type=time_step.step_type,\n      observation=time_step.observation,\n      action=action_step.action,\n      policy_info=action_step.info,\n      next_step_type=next_time_step.step_type,\n      reward=next_time_step.reward,\n      discount=next_time_step.discount)\n\n\ndef to_transition(\n    trajectory: Trajectory,\n    next_trajectory: Optional[Trajectory] = None\n) -> Tuple[ts.TimeStep, policy_step.PolicyStep, ts.TimeStep]:\n  """"""Create a transition from a trajectory or two adjacent trajectories.\n\n  **NOTE** If `next_trajectory` is not provided, tensors of `trajectory` are\n  sliced along their *second* (`time`) dimension; for example:\n\n  ```\n  time_steps.step_type = trajectory.step_type[:,:-1]\n  time_steps.observation = trajectory.observation[:,:-1]\n  next_time_steps.observation = trajectory.observation[:,1:]\n  next_time_steps. step_type = trajectory. next_step_type[:,:-1]\n  next_time_steps.reward = trajectory.reward[:,:-1]\n  next_time_steps. discount = trajectory. discount[:,:-1]\n\n  ```\n  Notice that reward and discount for time_steps are undefined, therefore filled\n  with zero.\n\n  TODO(b/155302755): Update the rank validation check for discount.\n\n  Args:\n    trajectory: An instance of `Trajectory`. The tensors in Trajectory must have\n      shape `[B, T, ...]` when next_trajectory is `None`.  `discount` is assumed\n      to be a scalar float; hence the shape of `trajectory.discount` must\n      be `[B, T]`.\n    next_trajectory: (optional) An instance of `Trajectory`.\n\n  Returns:\n    A tuple `(time_steps, policy_steps, next_time_steps)`.  The `reward` and\n    `discount` fields of `time_steps` are filled with zeros because these\n    cannot be deduced (please do not use them).\n\n  Raises:\n    ValueError: if `discount` rank is not within the range [1, 2].\n  """"""\n  _validate_rank(trajectory.discount, min_rank=1, max_rank=2)\n\n  if next_trajectory is not None:\n    _validate_rank(next_trajectory.discount, min_rank=1, max_rank=2)\n\n  if next_trajectory is None:\n    next_trajectory = tf.nest.map_structure(\n        lambda t: composite.slice_from(t, axis=1, start=1), trajectory)\n    trajectory = tf.nest.map_structure(\n        lambda t: composite.slice_to(t, axis=1, end=-1), trajectory)\n  policy_steps = policy_step.PolicyStep(\n      action=trajectory.action, state=(), info=trajectory.policy_info)\n  # TODO(b/130244652): Consider replacing 0 rewards & discounts with ().\n  time_steps = ts.TimeStep(\n      trajectory.step_type,\n      reward=tf.nest.map_structure(tf.zeros_like, trajectory.reward),  # unknown\n      discount=tf.zeros_like(trajectory.discount),  # unknown\n      observation=trajectory.observation)\n  next_time_steps = ts.TimeStep(\n      step_type=trajectory.next_step_type,\n      reward=trajectory.reward,\n      discount=trajectory.discount,\n      observation=next_trajectory.observation)\n  return (time_steps, policy_steps, next_time_steps)\n\n\ndef to_transition_spec(\n    trajectory_spec: Trajectory\n) -> Tuple[ts.TimeStep, policy_step.PolicyStep, ts.TimeStep]:\n  """"""Create a transition spec from a trajectory spec.\n\n  Args:\n    trajectory_spec: An instance of `Trajectory` representing trajectory specs.\n\n  Returns:\n    A tuple `(time_steps, policy_steps, next_time_steps)` specs.\n  """"""\n  policy_step_spec = policy_step.PolicyStep(\n      action=trajectory_spec.action, state=(), info=trajectory_spec.policy_info)\n  time_step_spec = ts.TimeStep(\n      trajectory_spec.step_type,\n      reward=trajectory_spec.reward,\n      discount=trajectory_spec.discount,\n      observation=trajectory_spec.observation)\n  return (time_step_spec, policy_step_spec, time_step_spec)\n\n\ndef _validate_rank(variable, min_rank, max_rank=None):\n  """"""Validates if a variable has the correct rank.\n\n  Args:\n    variable: A `tf.Tensor` or `numpy.array`.\n    min_rank: An int representing the min expected rank of the variable.\n    max_rank: An int representing the max expected rank of the variable.\n\n  Raises:\n    ValueError: if variable doesn\'t have expected rank.\n  """"""\n  rank = len(variable.shape)\n  if rank < min_rank or rank > max_rank:\n    raise ValueError(\n        \'Expect variable within rank [{},{}], but got rank {}.\'.format(\n            min_rank, max_rank, rank))\n\n\ndef experience_to_transitions(\n    experience: Trajectory, squeeze_time_dim: bool\n) -> Tuple[ts.TimeStep, policy_step.PolicyStep, ts.TimeStep]:\n  """"""Break experience to transitions.""""""\n  transitions = to_transition(experience)\n\n  if squeeze_time_dim:\n    transitions = tf.nest.map_structure(lambda x: composite.squeeze(x, 1),\n                                        transitions)\n  time_steps, policy_steps, next_time_steps = transitions\n  return time_steps, policy_steps, next_time_steps\n'"
tf_agents/trajectories/trajectory_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for trajectory.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.drivers import test_utils as drivers_test_utils\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import test_utils\n\n\nclass TrajectoryTest(test_utils.TestCase):\n\n  def testFirstTensors(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = tf.constant([1.0, 1.0, 2.0])\n    discount = tf.constant([1.0, 1.0, 1.0])\n    traj = trajectory.first(observation, action, policy_info, reward, discount)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val = self.evaluate(traj)\n    self.assertAllEqual(traj_val.step_type, [ts.StepType.FIRST] * 3)\n    self.assertAllEqual(traj_val.next_step_type, [ts.StepType.MID] * 3)\n\n  def testFirstArrays(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = np.array([1.0, 1.0, 2.0])\n    discount = np.array([1.0, 1.0, 1.0])\n    traj = trajectory.first(observation, action, policy_info, reward, discount)\n    self.assertFalse(tf.is_tensor(traj.step_type))\n    self.assertAllEqual(traj.step_type, [ts.StepType.FIRST] * 3)\n    self.assertAllEqual(traj.next_step_type, [ts.StepType.MID] * 3)\n\n  def testMidTensors(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = tf.constant([1.0, 1.0, 2.0])\n    discount = tf.constant([1.0, 1.0, 1.0])\n    traj = trajectory.mid(observation, action, policy_info, reward, discount)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val = self.evaluate(traj)\n    self.assertAllEqual(traj_val.step_type, [ts.StepType.MID] * 3)\n    self.assertAllEqual(traj_val.next_step_type, [ts.StepType.MID] * 3)\n\n  def testMidArrays(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = np.array([1.0, 1.0, 2.0])\n    discount = np.array([1.0, 1.0, 1.0])\n    traj = trajectory.mid(observation, action, policy_info, reward, discount)\n    self.assertFalse(tf.is_tensor(traj.step_type))\n    self.assertAllEqual(traj.step_type, [ts.StepType.MID] * 3)\n    self.assertAllEqual(traj.next_step_type, [ts.StepType.MID] * 3)\n\n  def testLastTensors(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = tf.constant([1.0, 1.0, 2.0])\n    discount = tf.constant([1.0, 1.0, 1.0])\n    traj = trajectory.last(observation, action, policy_info, reward, discount)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val = self.evaluate(traj)\n    self.assertAllEqual(traj_val.step_type, [ts.StepType.MID] * 3)\n    self.assertAllEqual(traj_val.next_step_type, [ts.StepType.LAST] * 3)\n\n  def testLastArrays(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = np.array([1.0, 1.0, 2.0])\n    discount = np.array([1.0, 1.0, 1.0])\n    traj = trajectory.last(observation, action, policy_info, reward, discount)\n    self.assertFalse(tf.is_tensor(traj.step_type))\n    self.assertAllEqual(traj.step_type, [ts.StepType.MID] * 3)\n    self.assertAllEqual(traj.next_step_type, [ts.StepType.LAST] * 3)\n\n  def testSingleStepTensors(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = tf.constant([1.0, 1.0, 2.0])\n    discount = tf.constant([1.0, 1.0, 1.0])\n    traj = trajectory.single_step(observation, action, policy_info, reward,\n                                  discount)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val = self.evaluate(traj)\n    self.assertAllEqual(traj_val.step_type, [ts.StepType.FIRST] * 3)\n    self.assertAllEqual(traj_val.next_step_type, [ts.StepType.LAST] * 3)\n\n  def testSingleStepArrays(self):\n    observation = ()\n    action = ()\n    policy_info = ()\n    reward = np.array([1.0, 1.0, 2.0])\n    discount = np.array([1.0, 1.0, 1.0])\n    traj = trajectory.single_step(observation, action, policy_info, reward,\n                                  discount)\n    self.assertFalse(tf.is_tensor(traj.step_type))\n    self.assertAllEqual(traj.step_type, [ts.StepType.FIRST] * 3)\n    self.assertAllEqual(traj.next_step_type, [ts.StepType.LAST] * 3)\n\n  def testFromEpisodeTensor(self):\n    observation = tf.random.uniform((4, 5))\n    action = ()\n    policy_info = ()\n    reward = tf.random.uniform((4,))\n    traj = trajectory.from_episode(\n        observation, action, policy_info, reward, discount=None)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val, obs_val, reward_val = self.evaluate((traj, observation, reward))\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n    self.assertAllEqual(\n        traj_val.step_type, [first, mid, mid, mid])\n    self.assertAllEqual(\n        traj_val.next_step_type, [mid, mid, mid, last])\n    self.assertAllClose(traj_val.observation, obs_val)\n    self.assertAllEqual(traj_val.reward, reward_val)\n    self.assertAllEqual(traj_val.discount, [1.0, 1.0, 1.0, 1.0])\n\n  def testFromEpisodeWithCompositeTensorOfTensors(self):\n    observation = tf.SparseTensor(\n        indices=tf.random.uniform((7, 2), maxval=9, dtype=tf.int64),\n        values=tf.random.uniform((7,)),\n        dense_shape=[4, 10])  # The 4 is important, it must match reward length.\n    action = ()\n    policy_info = ()\n    reward = tf.random.uniform((4,))\n    traj = trajectory.from_episode(\n        observation, action, policy_info, reward, discount=None)\n    self.assertTrue(tf.is_tensor(traj.step_type))\n    traj_val, obs_val, reward_val = self.evaluate((traj, observation, reward))\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n    self.assertAllEqual(\n        traj_val.step_type, [first, mid, mid, mid])\n    self.assertAllEqual(\n        traj_val.next_step_type, [mid, mid, mid, last])\n    self.assertAllClose(traj_val.observation, obs_val)\n    self.assertAllEqual(traj_val.reward, reward_val)\n    self.assertAllEqual(traj_val.discount, [1.0, 1.0, 1.0, 1.0])\n\n  def testFromEpisodeArray(self):\n    observation = np.random.rand(4, 5)\n    action = ()\n    policy_info = ()\n    reward = np.random.rand(4)\n    traj = trajectory.from_episode(\n        observation, action, policy_info, reward, discount=None)\n    self.assertFalse(tf.is_tensor(traj.step_type))\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n    self.assertAllEqual(\n        traj.step_type, [first, mid, mid, mid])\n    self.assertAllEqual(\n        traj.next_step_type, [mid, mid, mid, last])\n    self.assertAllEqual(traj.observation, observation)\n    self.assertAllEqual(traj.reward, reward)\n    self.assertAllEqual(traj.discount, [1.0, 1.0, 1.0, 1.0])\n\n  def testToTransition(self):\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n\n    # Define a batch size 1, 3-step trajectory.\n    traj = trajectory.Trajectory(\n        step_type=np.array([[first, mid, last]]),\n        next_step_type=np.array([[mid, last, first]]),\n        observation=np.array([[10.0, 20.0, 30.0]]),\n        action=np.array([[11.0, 22.0, 33.0]]),\n        # reward at step 0 is an invalid dummy reward.\n        reward=np.array([[0.0, 1.0, 2.0]]),\n        discount=np.array([[1.0, 1.0, 0.0]]),\n        policy_info=np.array([[1.0, 2.0, 3.0]]))\n\n    time_steps, policy_steps, next_time_steps = trajectory.to_transition(traj)\n\n    self.assertAllEqual(time_steps.step_type, np.array([[first, mid]]))\n    self.assertAllEqual(time_steps.observation, np.array([[10.0, 20.0]]))\n\n    self.assertAllEqual(next_time_steps.step_type, np.array([[mid, last]]))\n    self.assertAllEqual(next_time_steps.observation, np.array([[20.0, 30.0]]))\n    self.assertAllEqual(next_time_steps.reward, np.array([[0.0, 1.0]]))\n    self.assertAllEqual(next_time_steps.discount, np.array([[1.0, 1.0]]))\n\n    self.assertAllEqual(policy_steps.action, np.array([[11.0, 22.0]]))\n    self.assertAllEqual(policy_steps.info, np.array([[1.0, 2.0]]))\n\n  def testToTransitionHandlesTrajectoryFromDriverCorrectly(self):\n    env = tf_py_environment.TFPyEnvironment(\n        drivers_test_utils.PyEnvironmentMock())\n    policy = drivers_test_utils.TFPolicyMock(\n        env.time_step_spec(), env.action_spec())\n    replay_buffer = drivers_test_utils.make_replay_buffer(policy)\n\n    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        env, policy, num_episodes=3, observers=[replay_buffer.add_batch])\n\n    run_driver = driver.run()\n    rb_gather_all = replay_buffer.gather_all()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(run_driver)\n    trajectories = self.evaluate(rb_gather_all)\n\n    time_steps, policy_step, next_time_steps = trajectory.to_transition(\n        trajectories)\n\n    self.assertAllEqual(time_steps.observation,\n                        trajectories.observation[:, :-1])\n    self.assertAllEqual(time_steps.step_type, trajectories.step_type[:, :-1])\n    self.assertAllEqual(next_time_steps.observation,\n                        trajectories.observation[:, 1:])\n    self.assertAllEqual(next_time_steps.step_type,\n                        trajectories.step_type[:, 1:])\n    self.assertAllEqual(next_time_steps.reward, trajectories.reward[:, :-1])\n    self.assertAllEqual(next_time_steps.discount, trajectories.discount[:, :-1])\n\n    self.assertAllEqual(policy_step.action, trajectories.action[:, :-1])\n    self.assertAllEqual(policy_step.info, trajectories.policy_info[:, :-1])\n\n  def testToTransitionSpec(self):\n    env = tf_py_environment.TFPyEnvironment(\n        drivers_test_utils.PyEnvironmentMock())\n    policy = drivers_test_utils.TFPolicyMock(\n        env.time_step_spec(), env.action_spec())\n    trajectory_spec = policy.trajectory_spec\n    ts_spec, ps_spec, nts_spec = trajectory.to_transition_spec(trajectory_spec)\n\n    self.assertAllEqual(ts_spec, env.time_step_spec())\n    self.assertAllEqual(ps_spec.action, env.action_spec())\n    self.assertAllEqual(nts_spec, env.time_step_spec())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/typing/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Typing module.""""""\n'"
tf_agents/typing/types.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common types used in TF-Agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport sys\nimport typing\nfrom typing import Callable, Iterable, Mapping, Optional, Sequence, Text, Tuple, Union\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom tf_agents.specs import array_spec\n\nif sys.version_info < (3, 7):\n  ForwardRef = typing._ForwardRef  # pylint: disable=protected-access\nelse:\n  ForwardRef = typing.ForwardRef\n\nTensor = Union[tf.Tensor, tf.SparseTensor, tf.RaggedTensor]\nArray = np.ndarray   # pylint: disable=invalid-name\nTensorOrArray = Union[Tensor, Array]\nDistribution = tfp.distributions.Distribution\n\nTensorSpec = tf.TypeSpec\nArraySpec = array_spec.ArraySpec\nSpec = Union[TensorSpec, ArraySpec]\n\nSpecTensorOrArray = Union[Spec, Tensor, Array]\n\n# Note that this is effectively treated as `Any`; see b/109648354.\n\n# pytype: disable=not-supported-yet\nNestedTensor = Union[Tensor, Iterable[\'NestedTensor\'],\n                     Mapping[str, \'NestedTensor\']]\nNestedVariable = Union[tf.Variable, Iterable[\'NestedVariable\'],\n                       Mapping[str, \'NestedVariable\']]\nNestedArray = Union[Array, Iterable[\'NestedArray\'],\n                    Mapping[str, \'NestedArray\']]\nNestedDistribution = Union[tfp.distributions.Distribution,\n                           Iterable[\'NestedDistribution\'],\n                           Mapping[str, \'NestedDistribution\']]\nNestedPlaceHolder = Union[tf.compat.v1.placeholder,\n                          Iterable[\'NestedPlaceHolder\'],\n                          Mapping[Text, \'NestedPlaceHolder\']]\n\nNestedTensorSpec = Union[TensorSpec, Iterable[\'NestedTensorSpec\'],\n                         Mapping[str, \'NestedTensorSpec\']]\nNestedArraySpec = Union[array_spec.ArraySpec, Iterable[\'NestedArraySpec\'],\n                        Mapping[str, \'NestedArraySpec\']]\n# pytype: enable=not-supported-yet\n\nNestedSpec = Union[NestedTensorSpec, NestedArraySpec]\nNestedTensorOrArray = Union[NestedTensor, NestedArray]\nNestedSpecTensorOrArray = Union[NestedSpec, NestedTensor, NestedArray]\n\nInt = Union[int, np.int16, np.int32, np.int64, Tensor, Array]\nBool = Union[bool, np.bool, Tensor, Array]\n\nFloat = Union[float, np.float16, np.float32, np.float64, Tensor, Array]\nFloatOrReturningFloat = Union[Float, Callable[[], Float]]\n\nShape = Union[TensorOrArray, Sequence[int], tf.TensorShape]\n\nSplitter = Optional[Callable[\n    [NestedSpecTensorOrArray], Iterable[NestedSpecTensorOrArray]]]\nSeed = Union[int, Sequence[int], Tensor, Array]\n\nTimeStep = ForwardRef(\'tf_agents.trajectories.TimeStep\')  # pylint: disable=invalid-name\nPolicyStep = ForwardRef(\'tf_agents.trajectories.PolicyStep\')  # pylint: disable=invalid-name\nTransition = Tuple[TimeStep, PolicyStep, TimeStep]\n\nGymEnv = ForwardRef(\'gym.Env\')  # pylint: disable=invalid-name\nGymEnvWrapper = Callable[[GymEnv], GymEnv]\n\nPyEnv = ForwardRef(\'tf_agents.environments.PyEnvironment\')  # pylint: disable=invalid-name\nPyEnvWrapper = Callable[[PyEnv], PyEnv]\n\nLossFn = Callable[[Tensor, Tensor], Tensor]\n\nOptimizer = Union[tf.keras.optimizers.Optimizer, tf.compat.v1.train.Optimizer]\n'"
tf_agents/utils/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utils module.""""""\n\nfrom tf_agents.utils import common\nfrom tf_agents.utils import composite\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import example_encoding\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import numpy_storage\nfrom tf_agents.utils import session_utils\nfrom tf_agents.utils import tensor_normalizer\nfrom tf_agents.utils import test_utils\nfrom tf_agents.utils import timer\nfrom tf_agents.utils import value_ops\n'"
tf_agents/utils/common.py,156,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utilities for TF-Agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections as cs\nimport contextlib\nimport functools\nimport importlib\nimport os\n\nfrom absl import logging\nimport distutils.version\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import object_identity\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.core.protobuf import struct_pb2  # TF internal\nfrom tensorflow.python import tf2 as tf2_checker  # TF internal\nfrom tensorflow.python.eager import monitoring  # TF internal\nfrom tensorflow.python.saved_model import nested_structure_coder  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\ntry:\n  importlib.import_module(\'tf_agents.utils.allow_tf1\')\nexcept ImportError:\n  _TF1_MODE_ALLOWED = False\nelse:\n  _TF1_MODE_ALLOWED = True\n\n\ntf_agents_gauge = monitoring.BoolGauge(\'/tensorflow/agents/agents\',\n                                       \'TF-Agents usage\', \'method\')\n\n\nMISSING_RESOURCE_VARIABLES_ERROR = """"""\nResource variables are not enabled.  Please enable them by adding the following\ncode to your main() method:\n  tf.compat.v1.enable_resource_variables()\nFor unit tests, subclass `tf_agents.utils.test_utils.TestCase`.\n""""""\n\n\ndef check_tf1_allowed():\n  """"""Raises an error if running in TF1 (non-eager) mode and this is disabled.""""""\n  if _TF1_MODE_ALLOWED:\n    return\n  if not tf2_checker.enabled():\n    raise RuntimeError(\n        \'You are using TF1 or running TF with eager mode disabled.  \'\n        \'TF-Agents no longer supports TF1 mode (except for a shrinking list of \'\n        \'internal whitelisted users).  If this negatively affects you, please \'\n        \'reach out to the TF-Agents team.  Otherwise please use TF2.\')\n\n\ndef resource_variables_enabled():\n  return tf.compat.v1.resource_variables_enabled()\n\n\n_IN_LEGACY_TF1 = (\n    tf.__git_version__ != \'unknown\'\n    and tf.__version__ != \'1.15.0\'\n    and (distutils.version.LooseVersion(tf.__version__) <=\n         distutils.version.LooseVersion(\'1.15.0.dev20190821\')))\n\n\ndef in_legacy_tf1():\n  return _IN_LEGACY_TF1\n\n\ndef set_default_tf_function_parameters(*args, **kwargs):\n  """"""Generates a decorator that sets default parameters for `tf.function`.\n\n  Args:\n    *args: default arguments for the `tf.function`.\n    **kwargs: default keyword arguments for the `tf.function`.\n\n  Returns:\n    Function decorator with preconfigured defaults for `tf.function`.\n  """"""\n  def maybe_wrap(fn):\n    """"""Helper function.""""""\n    wrapped = [None]\n\n    @functools.wraps(fn)\n    def preconfigured_function(*fn_args, **fn_kwargs):\n      if tf.executing_eagerly():\n        return fn(*fn_args, **fn_kwargs)\n      if wrapped[0] is None:\n        wrapped[0] = function(*((fn,) + args), **kwargs)\n      return wrapped[0](*fn_args, **fn_kwargs)  # pylint: disable=not-callable\n\n    return preconfigured_function\n\n  return maybe_wrap\n\n\ndef function(*args, **kwargs):\n  """"""Wrapper for tf.function with TF Agents-specific customizations.\n\n  Example:\n\n  ```python\n  @common.function()\n  def my_eager_code(x, y):\n    ...\n  ```\n\n  Args:\n    *args: Args for tf.function.\n    **kwargs: Keyword args for tf.function.\n\n  Returns:\n    A tf.function wrapper.\n  """"""\n  autograph = kwargs.pop(\'autograph\', False)\n  experimental_relax_shapes = kwargs.pop(\'experimental_relax_shapes\', True)\n  return tf.function(  # allow-tf-function\n      *args,\n      autograph=autograph,\n      experimental_relax_shapes=experimental_relax_shapes,\n      **kwargs)\n\n\ndef has_eager_been_enabled():\n  """"""Returns true iff in TF2 or in TF1 with eager execution enabled.""""""\n  with tf.init_scope():\n    return tf.executing_eagerly()\n\n\ndef function_in_tf1(*args, **kwargs):\n  """"""Wrapper that returns common.function if using TF1.\n\n  This allows for code that assumes autodeps is available to be written once,\n  in the same way, for both TF1 and TF2.\n\n  Usage:\n\n  ```python\n  train = function_in_tf1()(agent.train)\n  loss = train(experience)\n  ```\n\n  Args:\n    *args: Arguments for common.function.\n    **kwargs: Keyword arguments for common.function.\n\n  Returns:\n    A callable that wraps a function.\n  """"""\n\n  def maybe_wrap(fn):\n    """"""Helper function.""""""\n    # We\'re in TF1 mode and want to wrap in common.function to get autodeps.\n    wrapped = [None]\n\n    @functools.wraps(fn)\n    def with_check_resource_vars(*fn_args, **fn_kwargs):\n      """"""Helper function for calling common.function.""""""\n      check_tf1_allowed()\n      if has_eager_been_enabled():\n        # We\'re either in eager mode or in tf.function mode (no in-between); so\n        # autodep-like behavior is already expected of fn.\n        return fn(*fn_args, **fn_kwargs)\n      if not resource_variables_enabled():\n        raise RuntimeError(MISSING_RESOURCE_VARIABLES_ERROR)\n      if wrapped[0] is None:\n        wrapped[0] = function(*((fn,) + args), **kwargs)\n      return wrapped[0](*fn_args, **fn_kwargs)  # pylint: disable=not-callable\n\n    return with_check_resource_vars\n\n  return maybe_wrap\n\n\ndef create_variable(name,\n                    initial_value=0,\n                    shape=(),\n                    dtype=tf.int64,\n                    use_local_variable=False,\n                    trainable=False,\n                    initializer=None,\n                    unique_name=True):\n  """"""Create a variable.""""""\n  check_tf1_allowed()\n  if has_eager_been_enabled():\n    if initializer is None:\n      if shape:\n        initial_value = tf.constant(initial_value, shape=shape, dtype=dtype)\n      else:\n        initial_value = tf.convert_to_tensor(initial_value, dtype=dtype)\n    else:\n      if callable(initializer):\n        initial_value = lambda: initializer(shape)\n      else:\n        initial_value = initializer\n    return tf.compat.v2.Variable(\n        initial_value, trainable=trainable, dtype=dtype, name=name)\n  collections = [tf.compat.v1.GraphKeys.GLOBAL_VARIABLES]\n  if use_local_variable:\n    collections = [tf.compat.v1.GraphKeys.LOCAL_VARIABLES]\n  if initializer is None:\n    initializer = tf.compat.v1.initializers.constant(initial_value, dtype=dtype)\n    if shape is None:\n      shape = tf.convert_to_tensor(initial_value).shape\n  if unique_name:\n    name = tf.compat.v1.get_default_graph().unique_name(name)\n  return tf.compat.v1.get_variable(\n      name=name,\n      shape=shape,\n      dtype=dtype,\n      initializer=initializer,\n      collections=collections,\n      use_resource=True,\n      trainable=trainable)\n\n\ndef soft_variables_update(source_variables,\n                          target_variables,\n                          tau=1.0,\n                          tau_non_trainable=None,\n                          sort_variables_by_name=False):\n  """"""Performs a soft/hard update of variables from the source to the target.\n\n  For each variable v_t in target variables and its corresponding variable v_s\n  in source variables, a soft update is:\n  v_t = (1 - tau) * v_t + tau * v_s\n\n  When tau is 1.0 (the default), then it does a hard update:\n  v_t = v_s\n\n  Args:\n    source_variables: list of source variables.\n    target_variables: list of target variables.\n    tau: A float scalar in [0, 1]. When tau is 1.0 (the default), we do a hard\n      update. This is used for trainable variables.\n    tau_non_trainable: A float scalar in [0, 1] for non_trainable variables. If\n      None, will copy from tau.\n    sort_variables_by_name: A bool, when True would sort the variables by name\n      before doing the update.\n\n  Returns:\n    An operation that updates target variables from source variables.\n  Raises:\n    ValueError: if `tau not in [0, 1]`.\n    ValueError: if `len(source_variables) != len(target_variables)`.\n  """"""\n  if tau < 0 or tau > 1:\n    raise ValueError(\'Input `tau` should be in [0, 1].\')\n  if tau_non_trainable is None:\n    tau_non_trainable = tau\n\n  if tau_non_trainable < 0 or tau_non_trainable > 1:\n    raise ValueError(\'Input `tau_non_trainable` should be in [0, 1].\')\n\n  updates = []\n\n  op_name = \'soft_variables_update\'\n  if tau == 0.0 or not source_variables or not target_variables:\n    return tf.no_op(name=op_name)\n  if len(source_variables) != len(target_variables):\n    raise ValueError(\n        \'Source and target variable lists have different lengths: \'\n        \'{} vs. {}\'.format(len(source_variables), len(target_variables)))\n  if sort_variables_by_name:\n    source_variables = sorted(source_variables, key=lambda x: x.name)\n    target_variables = sorted(target_variables, key=lambda x: x.name)\n\n  strategy = tf.distribute.get_strategy()\n\n  for (v_s, v_t) in zip(source_variables, target_variables):\n    v_t.shape.assert_is_compatible_with(v_s.shape)\n\n    def update_fn(v1, v2):\n      """"""Update variables.""""""\n      # For not trainable variables do hard updates.\n      # This helps stabilaze BatchNorm moving averagees TODO(b/144455039)\n      if not v1.trainable:\n        current_tau = tau_non_trainable\n      else:\n        current_tau = tau\n\n      if current_tau == 1.0:\n        return v1.assign(v2)\n      else:\n        return v1.assign((1 - current_tau) * v1 + current_tau * v2)\n\n    # TODO(b/142508640): remove this when b/142802462 is fixed.\n    # Workaround for b/142508640, only use extended.update for\n    # MirroredVariable variables (which are trainable variables).\n    # For other types of variables (i.e. SyncOnReadVariables, for example\n    # batch norm stats) do a regular assign, which will cause a sync and\n    # broadcast from replica 0, so will have slower performance but will be\n    # correct and not cause a failure.\n    if tf.distribute.has_strategy() and v_t.trainable:\n      # Assignment happens independently on each replica,\n      # see b/140690837 #46.\n      update = strategy.extended.update(v_t, update_fn, args=(v_s,))\n    else:\n      update = update_fn(v_t, v_s)\n\n    updates.append(update)\n  return tf.group(*updates, name=op_name)\n\n\ndef join_scope(parent_scope, child_scope):\n  """"""Joins a parent and child scope using `/`, checking for empty/none.\n\n  Args:\n    parent_scope: (string) parent/prefix scope.\n    child_scope: (string) child/suffix scope.\n\n  Returns:\n    joined scope: (string) parent and child scopes joined by /.\n  """"""\n  if not parent_scope:\n    return child_scope\n  if not child_scope:\n    return parent_scope\n  return \'/\'.join([parent_scope, child_scope])\n\n\n# TODO(b/138322868): Add an optional action_spec for validation.\ndef index_with_actions(q_values, actions, multi_dim_actions=False):\n  """"""Index into q_values using actions.\n\n  Note: this supports multiple outer dimensions (e.g. time, batch etc).\n\n  Args:\n    q_values: A float tensor of shape [outer_dim1, ... outer_dimK, action_dim1,\n      ..., action_dimJ].\n    actions: An int tensor of shape [outer_dim1, ... outer_dimK]    if\n      multi_dim_actions=False [outer_dim1, ... outer_dimK, J] if\n      multi_dim_actions=True I.e. in the multidimensional case,\n      actions[outer_dim1, ... outer_dimK] is a vector [actions_1, ...,\n      actions_J] where each element actions_j is an action in the range [0,\n      num_actions_j). While in the single dimensional case, actions[outer_dim1,\n      ... outer_dimK] is a scalar.\n    multi_dim_actions: whether the actions are multidimensional.\n\n  Returns:\n    A [outer_dim1, ... outer_dimK] tensor of q_values for the given actions.\n\n  Raises:\n    ValueError: If actions have unknown rank.\n  """"""\n  if actions.shape.rank is None:\n    raise ValueError(\'actions should have known rank.\')\n  batch_dims = actions.shape.rank\n  if multi_dim_actions:\n    # In the multidimensional case, the last dimension of actions indexes the\n    # vector of actions for each batch, so exclude it from the batch dimensions.\n    batch_dims -= 1\n\n  outer_shape = tf.shape(input=actions)\n  batch_indices = tf.meshgrid(\n      *[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing=\'ij\')\n  batch_indices = [tf.cast(tf.expand_dims(batch_index, -1), dtype=tf.int32)\n                   for batch_index in batch_indices]\n  if not multi_dim_actions:\n    actions = tf.expand_dims(actions, -1)\n  # Cast actions to tf.int32 in order to avoid a TypeError in tf.concat.\n  actions = tf.cast(actions, dtype=tf.int32)\n  action_indices = tf.concat(batch_indices + [actions], -1)\n  return tf.gather_nd(q_values, action_indices)\n\n\ndef periodically(body, period, name=\'periodically\'):\n  """"""Periodically performs the tensorflow op in `body`.\n\n  The body tensorflow op will be executed every `period` times the periodically\n  op is executed. More specifically, with `n` the number of times the op has\n  been executed, the body will be executed when `n` is a non zero positive\n  multiple of `period` (i.e. there exist an integer `k > 0` such that\n  `k * period == n`).\n\n  If `period` is `None`, it will not perform any op and will return a\n  `tf.no_op()`.\n\n  If `period` is 1, it will just execute the body, and not create any counters\n  or conditionals.\n\n  Args:\n    body: callable that returns the tensorflow op to be performed every time an\n      internal counter is divisible by the period. The op must have no output\n      (for example, a tf.group()).\n    period: inverse frequency with which to perform the op.\n    name: name of the variable_scope.\n\n  Raises:\n    TypeError: if body is not a callable.\n\n  Returns:\n    An op that periodically performs the specified op.\n  """"""\n  if tf.executing_eagerly():\n    if isinstance(period, tf.Variable):\n      return Periodically(body, period, name)\n    return EagerPeriodically(body, period)\n  else:\n    return Periodically(body, period, name)()\n\n\nclass Periodically(tf.Module):\n  """"""Periodically performs the ops defined in `body`.""""""\n\n  def __init__(self, body, period, name=\'periodically\'):\n    """"""Periodically performs the ops defined in `body`.\n\n    The body tensorflow op will be executed every `period` times the\n    periodically op is executed. More specifically, with `n` the number of times\n    the op has been executed, the body will be executed when `n` is a non zero\n    positive multiple of `period` (i.e. there exist an integer `k > 0` such that\n    `k * period == n`).\n\n    If `period` is `None`, it will not perform any op and will return a\n    `tf.no_op()`.\n\n    If `period` is 1, it will just execute the body, and not create any counters\n    or conditionals.\n\n    Args:\n      body: callable that returns the tensorflow op to be performed every time\n        an internal counter is divisible by the period. The op must have no\n        output (for example, a tf.group()).\n      period: inverse frequency with which to perform the op. It can be a Tensor\n        or a Variable.\n      name: name of the object.\n\n    Raises:\n      TypeError: if body is not a callable.\n\n    Returns:\n      An op that periodically performs the specified op.\n    """"""\n    super(Periodically, self).__init__(name=name)\n    if not callable(body):\n      raise TypeError(\'body must be callable.\')\n    self._body = body\n    self._period = period\n    self._counter = create_variable(self.name + \'/counter\', 0)\n\n  def __call__(self):\n\n    def call(strategy=None):\n      del strategy  # unused\n      if self._period is None:\n        return tf.no_op()\n      if self._period == 1:\n        return self._body()\n      period = tf.cast(self._period, self._counter.dtype)\n      remainder = tf.math.mod(self._counter.assign_add(1), period)\n      return tf.cond(\n          pred=tf.equal(remainder, 0), true_fn=self._body, false_fn=tf.no_op)\n\n    # TODO(b/129083817) add an explicit unit test to ensure correct behavior\n    ctx = tf.distribute.get_replica_context()\n    if ctx:\n      return tf.distribute.get_replica_context().merge_call(call)\n    else:\n      return call()\n\n\nclass EagerPeriodically(object):\n  """"""EagerPeriodically performs the ops defined in `body`.\n\n  Only works in Eager mode.\n  """"""\n\n  def __init__(self, body, period):\n    """"""EagerPeriodically performs the ops defined in `body`.\n\n    Args:\n      body: callable that returns the tensorflow op to be performed every time\n        an internal counter is divisible by the period. The op must have no\n        output (for example, a tf.group()).\n      period: inverse frequency with which to perform the op. Must be a simple\n        python int/long.\n\n    Raises:\n      TypeError: if body is not a callable.\n\n    Returns:\n      An op that periodically performs the specified op.\n    """"""\n    if not callable(body):\n      raise TypeError(\'body must be callable.\')\n    self._body = body\n    self._period = period\n    self._counter = 0\n\n  def __call__(self):\n    if self._period is None:\n      return tf.no_op()\n    if self._period == 1:\n      return self._body()\n    self._counter += 1\n    if self._counter % self._period == 0:\n      self._body()\n\n\ndef clip_to_spec(value, spec):\n  """"""Clips value to a given bounded tensor spec.\n\n  Args:\n    value: (tensor) value to be clipped.\n    spec: (BoundedTensorSpec) spec containing min. and max. values for clipping.\n\n  Returns:\n    clipped_value: (tensor) `value` clipped to be compatible with `spec`.\n  """"""\n  return tf.clip_by_value(value, spec.minimum, spec.maximum)\n\n\ndef spec_means_and_magnitudes(action_spec):\n  """"""Get the center and magnitude of the ranges in action spec.""""""\n  action_means = tf.nest.map_structure(\n      lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n  action_magnitudes = tf.nest.map_structure(\n      lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n  return tf.cast(\n      action_means, dtype=tf.float32), tf.cast(\n          action_magnitudes, dtype=tf.float32)\n\n\ndef scale_to_spec(tensor, spec):\n  """"""Shapes and scales a batch into the given spec bounds.\n\n  Args:\n    tensor: A [batch x n] tensor with values in the range of [-1, 1].\n    spec: (BoundedTensorSpec) to use for scaling the action.\n\n  Returns:\n    A batch scaled the given spec bounds.\n  """"""\n  tensor = tf.reshape(tensor, [-1] + spec.shape.as_list())\n\n  # Scale the tensor.\n  means, magnitudes = spec_means_and_magnitudes(spec)\n  tensor = means + magnitudes * tensor\n\n  # Set type.\n  return tf.cast(tensor, spec.dtype)\n\n\ndef ornstein_uhlenbeck_process(initial_value,\n                               damping=0.15,\n                               stddev=0.2,\n                               seed=None,\n                               scope=\'ornstein_uhlenbeck_noise\'):\n  """"""An op for generating noise from a zero-mean Ornstein-Uhlenbeck process.\n\n  The Ornstein-Uhlenbeck process is a process that generates temporally\n  correlated noise via a random walk with damping. This process describes\n  the velocity of a particle undergoing brownian motion in the presence of\n  friction. This can be useful for exploration in continuous action environments\n  with momentum.\n\n  The temporal update equation is:\n  `x_next = (1 - damping) * x + N(0, std_dev)`\n\n  Args:\n    initial_value: Initial value of the process.\n    damping: The rate at which the noise trajectory is damped towards the mean.\n      We must have 0 <= damping <= 1, where a value of 0 gives an undamped\n      random walk and a value of 1 gives uncorrelated Gaussian noise. Hence in\n      most applications a small non-zero value is appropriate.\n    stddev: Standard deviation of the Gaussian component.\n    seed: Seed for random number generation.\n    scope: Scope of the variables.\n\n  Returns:\n    An op that generates noise.\n  """"""\n  if tf.executing_eagerly():\n    return OUProcess(initial_value, damping, stddev, seed, scope)\n  else:\n    return OUProcess(initial_value, damping, stddev, seed, scope)()\n\n\nclass OUProcess(tf.Module):\n  """"""A zero-mean Ornstein-Uhlenbeck process.""""""\n\n  def __init__(self,\n               initial_value,\n               damping=0.15,\n               stddev=0.2,\n               seed=None,\n               scope=\'ornstein_uhlenbeck_noise\'):\n    """"""A Class for generating noise from a zero-mean Ornstein-Uhlenbeck process.\n\n    The Ornstein-Uhlenbeck process is a process that generates temporally\n    correlated noise via a random walk with damping. This process describes\n    the velocity of a particle undergoing brownian motion in the presence of\n    friction. This can be useful for exploration in continuous action\n    environments with momentum.\n\n    The temporal update equation is:\n    `x_next = (1 - damping) * x + N(0, std_dev)`\n\n    Args:\n      initial_value: Initial value of the process.\n      damping: The rate at which the noise trajectory is damped towards the\n        mean. We must have 0 <= damping <= 1, where a value of 0 gives an\n        undamped random walk and a value of 1 gives uncorrelated Gaussian noise.\n        Hence in most applications a small non-zero value is appropriate.\n      stddev: Standard deviation of the Gaussian component.\n      seed: Seed for random number generation.\n      scope: Scope of the variables.\n    """"""\n    super(OUProcess, self).__init__()\n    self._damping = damping\n    self._stddev = stddev\n    self._seed = seed\n    with tf.name_scope(scope):\n      self._x = tf.compat.v2.Variable(\n          initial_value=initial_value, trainable=False)\n\n  def __call__(self):\n    noise = tf.random.normal(\n        shape=self._x.shape,\n        stddev=self._stddev,\n        dtype=self._x.dtype,\n        seed=self._seed)\n    return self._x.assign((1. - self._damping) * self._x + noise)\n\n\ndef log_probability(distributions, actions, action_spec):\n  """"""Computes log probability of actions given distribution.\n\n  Args:\n    distributions: A possibly batched tuple of distributions.\n    actions: A possibly batched action tuple.\n    action_spec: A nested tuple representing the action spec.\n\n  Returns:\n    A Tensor representing the log probability of each action in the batch.\n  """"""\n  outer_rank = nest_utils.get_outer_rank(actions, action_spec)\n\n  def _compute_log_prob(single_distribution, single_action):\n    # sum log-probs over everything but the batch\n    single_log_prob = single_distribution.log_prob(single_action)\n    rank = single_log_prob.shape.rank\n    reduce_dims = list(range(outer_rank, rank))\n    return tf.reduce_sum(\n        input_tensor=single_log_prob,\n        axis=reduce_dims)\n\n  nest_utils.assert_same_structure(distributions, actions)\n  log_probs = [\n      _compute_log_prob(dist, action)\n      for (dist, action\n          ) in zip(tf.nest.flatten(distributions), tf.nest.flatten(actions))\n  ]\n\n  # sum log-probs over action tuple\n  total_log_probs = tf.add_n(log_probs)\n\n  return total_log_probs\n\n\n# TODO(ofirnachum): Move to distribution utils.\ndef entropy(distributions, action_spec):\n  """"""Computes total entropy of distribution.\n\n  Args:\n    distributions: A possibly batched tuple of distributions.\n    action_spec: A nested tuple representing the action spec.\n\n  Returns:\n    A Tensor representing the entropy of each distribution in the batch.\n    Assumes actions are independent, so that marginal entropies of each action\n    may be summed.\n  """"""\n  nested_modes = tf.nest.map_structure(lambda d: d.mode(), distributions)\n  outer_rank = nest_utils.get_outer_rank(nested_modes, action_spec)\n\n  def _compute_entropy(single_distribution):\n    entropies = single_distribution.entropy()\n    # Sum entropies over everything but the batch.\n    rank = entropies.shape.rank\n    reduce_dims = list(range(outer_rank, rank))\n    return tf.reduce_sum(input_tensor=entropies, axis=reduce_dims)\n\n  entropies = [\n      _compute_entropy(dist) for dist in tf.nest.flatten(distributions)\n  ]\n\n  # Sum entropies over action tuple.\n  total_entropies = tf.add_n(entropies)\n\n  return total_entropies\n\n\ndef discounted_future_sum(values, gamma, num_steps):\n  """"""Discounted future sum of batch-major values.\n\n  Args:\n    values: A Tensor of shape [batch_size, total_steps] and dtype float32.\n    gamma: A float discount value.\n    num_steps: A positive integer number of future steps to sum.\n\n  Returns:\n    A Tensor of shape [batch_size, total_steps], where each entry `(i, j)` is\n      the result of summing the entries of values starting from\n      `gamma^0 * values[i, j]` to\n      `gamma^(num_steps - 1) * values[i, j + num_steps - 1]`,\n      with zeros padded to values.\n\n      For example, values=[5, 6, 7], gamma=0.9, will result in sequence:\n      ```python\n      [(5 * 0.9^0 + 6 * 0.9^1 + 7 * 0.9^2), (6 * 0.9^0 + 7 * 0.9^1), 7 * 0.9^0]\n      ```\n\n  Raises:\n    ValueError: If values is not of rank 2.\n  """"""\n  if values.get_shape().rank != 2:\n    raise ValueError(\'Input must be rank 2 tensor.  Got %d.\' %\n                     values.get_shape().rank)\n\n  (batch_size, total_steps) = values.get_shape().as_list()\n\n  num_steps = tf.minimum(num_steps, total_steps)\n  discount_filter = tf.reshape(gamma**tf.cast(tf.range(num_steps), tf.float32),\n                               [-1, 1, 1])\n  padded_values = tf.concat([values, tf.zeros([batch_size, num_steps - 1])], 1)\n\n  convolved_values = tf.squeeze(\n      tf.nn.conv1d(\n          input=tf.expand_dims(padded_values, -1),\n          filters=discount_filter,\n          stride=1,\n          padding=\'VALID\'), -1)\n\n  return convolved_values\n\n\ndef discounted_future_sum_masked(values, gamma, num_steps, episode_lengths):\n  """"""Discounted future sum of batch-major values.\n\n  Args:\n    values: A Tensor of shape [batch_size, total_steps] and dtype float32.\n    gamma: A float discount value.\n    num_steps: A positive integer number of future steps to sum.\n    episode_lengths: A vector shape [batch_size] with num_steps per episode.\n\n  Returns:\n    A Tensor of shape [batch_size, total_steps], where each entry is the\n      discounted sum as in discounted_future_sum, except with values after\n      the end of episode_lengths masked to 0.\n\n  Raises:\n    ValueError: If values is not of rank 2, or if total_steps is not defined.\n  """"""\n  if values.shape.rank != 2:\n    raise ValueError(\'Input must be a rank 2 tensor.  Got %d.\' % values.shape)\n\n  total_steps = tf.compat.dimension_value(values.shape[1])\n  if total_steps is None:\n    raise ValueError(\'total_steps dimension in input \'\n                     \'values[batch_size, total_steps] must be fully defined.\')\n\n  episode_mask = tf.cast(\n      tf.sequence_mask(episode_lengths, total_steps), tf.float32)\n  values *= episode_mask\n  return discounted_future_sum(values, gamma, num_steps)\n\n\ndef shift_values(values, gamma, num_steps, final_values=None):\n  """"""Shifts batch-major values in time by some amount.\n\n  Args:\n    values: A Tensor of shape [batch_size, total_steps] and dtype float32.\n    gamma: A float discount value.\n    num_steps: A nonnegative integer amount to shift values by.\n    final_values: A float32 Tensor of shape [batch_size] corresponding to the\n      values at step num_steps + 1.  Defaults to None (all zeros).\n\n  Returns:\n    A Tensor of shape [batch_size, total_steps], where each entry (i, j) is\n    gamma^num_steps * values[i, j + num_steps] if j + num_steps < total_steps;\n    gamma^(total_steps - j) * final_values[i] otherwise.\n\n  Raises:\n    ValueError: If values is not of rank 2.\n  """"""\n  if values.get_shape().rank != 2:\n    raise ValueError(\'Input must be rank 2 tensor.  Got %d.\' %\n                     values.get_shape().rank)\n\n  (batch_size, total_steps) = values.get_shape().as_list()\n  num_steps = tf.minimum(num_steps, total_steps)\n\n  if final_values is None:\n    final_values = tf.zeros([batch_size])\n\n  padding_exponent = tf.expand_dims(\n      tf.cast(tf.range(num_steps, 0, -1), tf.float32), 0)\n  final_pad = tf.expand_dims(final_values, 1) * gamma**padding_exponent\n  return tf.concat([\n      gamma**tf.cast(num_steps, tf.float32) * values[:, num_steps:], final_pad\n  ], 1)\n\n\ndef get_episode_mask(time_steps):\n  """"""Create a mask that is 0.0 for all final steps, 1.0 elsewhere.\n\n  Args:\n    time_steps: A TimeStep namedtuple representing a batch of steps.\n\n  Returns:\n    A float32 Tensor with 0s where step_type == LAST and 1s otherwise.\n  """"""\n  episode_mask = tf.cast(\n      tf.not_equal(time_steps.step_type, ts.StepType.LAST), tf.float32)\n  return episode_mask\n\n\ndef get_contiguous_sub_episodes(next_time_steps_discount):\n  """"""Computes mask on sub-episodes which includes only contiguous components.\n\n  Args:\n    next_time_steps_discount: Tensor of shape [batch_size, total_steps]\n      corresponding to environment discounts on next time steps (i.e.\n      next_time_steps.discount).\n\n  Returns:\n    A float Tensor of shape [batch_size, total_steps] specifying mask including\n      only contiguous components. Each row will be of the form\n      [1.0] * a + [0.0] * b, where a >= 1 and b >= 0, and in which the initial\n      sequence of ones corresponds to a contiguous sub-episode.\n  """"""\n  episode_end = tf.equal(next_time_steps_discount,\n                         tf.constant(0, dtype=next_time_steps_discount.dtype))\n  mask = tf.math.cumprod(\n      1.0 - tf.cast(episode_end, tf.float32), axis=1, exclusive=True)\n  return mask\n\n\ndef convert_q_logits_to_values(logits, support):\n  """"""Converts a set of Q-value logits into Q-values using the provided support.\n\n  Args:\n    logits: A Tensor representing the Q-value logits.\n    support: The support of the underlying distribution.\n\n  Returns:\n    A Tensor containing the expected Q-values.\n  """"""\n  probabilities = tf.nn.softmax(logits)\n  return tf.reduce_sum(input_tensor=support * probabilities, axis=-1)\n\n\ndef generate_tensor_summaries(tag, tensor, step):\n  """"""Generates various summaries of `tensor` such as histogram, max, min, etc.\n\n  Args:\n    tag: A namescope tag for the summaries.\n    tensor: The tensor to generate summaries of.\n    step: Variable to use for summaries.\n  """"""\n  with tf.name_scope(tag):\n    tf.compat.v2.summary.histogram(name=\'histogram\', data=tensor, step=step)\n    tf.compat.v2.summary.scalar(\n        name=\'mean\', data=tf.reduce_mean(input_tensor=tensor), step=step)\n    tf.compat.v2.summary.scalar(\n        name=\'mean_abs\',\n        data=tf.reduce_mean(input_tensor=tf.abs(tensor)),\n        step=step)\n    tf.compat.v2.summary.scalar(\n        name=\'max\', data=tf.reduce_max(input_tensor=tensor), step=step)\n    tf.compat.v2.summary.scalar(\n        name=\'min\', data=tf.reduce_min(input_tensor=tensor), step=step)\n\n\n# TODO(kbanoop): Support batch mode\ndef compute_returns(rewards, discounts):\n  """"""Compute the return from each index in an episode.\n\n  Args:\n    rewards: Tensor of per-timestep reward in the episode.\n    discounts: Tensor of per-timestep discount factor. Should be 0 for final\n      step of each episode.\n\n  Returns:\n    Tensor of per-timestep cumulative returns.\n  """"""\n  rewards.shape.assert_is_compatible_with(discounts.shape)\n  if (not rewards.shape.is_fully_defined() or\n      not discounts.shape.is_fully_defined()):\n    check_shape = tf.compat.v1.assert_equal(\n        tf.shape(input=rewards), tf.shape(input=discounts))\n  else:\n    check_shape = tf.no_op()\n  with tf.control_dependencies([check_shape]):\n    # Reverse the rewards and discounting for accumulation.\n    rewards, discounts = tf.reverse(rewards, [0]), tf.reverse(discounts, [0])\n\n  def discounted_accumulate_rewards(next_step_return, reward_and_discount):\n    reward, discount = reward_and_discount\n    return next_step_return * discount + reward\n\n  # Cumulatively sum discounted reward R_t.\n  #   R_t = r_t + discount * (r_t+1 + discount * (r_t+2 * discount( ...\n  # As discount is 0 for terminal states, ends of episode will not include\n  #   reward from subsequent timesteps.\n  returns = tf.scan(\n      discounted_accumulate_rewards, [rewards, discounts],\n      initializer=tf.constant(0, dtype=discounts.dtype))\n  returns = tf.reverse(returns, [0])\n  return returns\n\n\ndef initialize_uninitialized_variables(session, var_list=None):\n  """"""Initialize any pending variables that are uninitialized.""""""\n  if var_list is None:\n    var_list = tf.compat.v1.global_variables() + tf.compat.v1.local_variables()\n  is_initialized = session.run(\n      [tf.compat.v1.is_variable_initialized(v) for v in var_list])\n  uninitialized_vars = []\n  for flag, v in zip(is_initialized, var_list):\n    if not flag:\n      uninitialized_vars.append(v)\n  if uninitialized_vars:\n    logging.info(\'uninitialized_vars:\')\n    for v in uninitialized_vars:\n      logging.info(v)\n    session.run(tf.compat.v1.variables_initializer(uninitialized_vars))\n\n\nclass Checkpointer(object):\n  """"""Checkpoints training state, policy state, and replay_buffer state.""""""\n\n  def __init__(self, ckpt_dir, max_to_keep=20, **kwargs):\n    """"""A class for making checkpoints.\n\n    If ckpt_dir doesn\'t exists it creates it.\n\n    Args:\n      ckpt_dir: The directory to save checkpoints.\n      max_to_keep: Maximum number of checkpoints to keep (if greater than the\n        max are saved, the oldest checkpoints are deleted).\n      **kwargs: Items to include in the checkpoint.\n    """"""\n    self._checkpoint = tf.train.Checkpoint(**kwargs)\n\n    if not tf.io.gfile.exists(ckpt_dir):\n      tf.io.gfile.makedirs(ckpt_dir)\n\n    self._manager = tf.train.CheckpointManager(\n        self._checkpoint, directory=ckpt_dir, max_to_keep=max_to_keep)\n\n    if self._manager.latest_checkpoint is not None:\n      logging.info(\'Checkpoint available: %s\', self._manager.latest_checkpoint)\n      self._checkpoint_exists = True\n    else:\n      logging.info(\'No checkpoint available at %s\', ckpt_dir)\n      self._checkpoint_exists = False\n    self._load_status = self._checkpoint.restore(\n        self._manager.latest_checkpoint)\n\n  @property\n  def checkpoint_exists(self):\n    return self._checkpoint_exists\n\n  @property\n  def manager(self):\n    """"""Returns the underlying tf.train.CheckpointManager.""""""\n    return self._manager\n\n  def initialize_or_restore(self, session=None):\n    """"""Initialize or restore graph (based on checkpoint if exists).""""""\n    self._load_status.initialize_or_restore(session)\n    return self._load_status\n\n  def save(self, global_step):\n    """"""Save state to checkpoint.""""""\n    saved_checkpoint = self._manager.save(checkpoint_number=global_step)\n    self._checkpoint_exists = True\n    logging.info(\'%s\', \'Saved checkpoint: {}\'.format(saved_checkpoint))\n\n\ndef replicate(tensor, outer_shape):\n  """"""Replicates a tensor so as to match the given outer shape.\n\n  Example:\n  - t = [[1, 2, 3], [4, 5, 6]] (shape = [2, 3])\n  - outer_shape = [2, 1]\n  The shape of the resulting tensor is: [2, 1, 2, 3]\n  and its content is: [[t], [t]]\n\n  Args:\n    tensor: A tf.Tensor.\n    outer_shape: Outer shape given as a 1D tensor of type list, numpy or\n      tf.Tensor.\n\n  Returns:\n    The replicated tensor.\n\n  Raises:\n    ValueError: when the outer shape is incorrect.\n  """"""\n  outer_shape = tf.convert_to_tensor(value=outer_shape)\n  if len(outer_shape.shape) != 1:\n    raise ValueError(\'The outer shape must be a 1D tensor\')\n  outer_ndims = int(outer_shape.shape[0])\n  tensor_ndims = len(tensor.shape)\n\n  # No need to replicate anything if there is no outer dim to add.\n  if outer_ndims == 0:\n    return tensor\n\n  # Calculate target shape of replicated tensor\n  target_shape = tf.concat([outer_shape, tf.shape(input=tensor)], axis=0)\n\n  # tf.tile expects `tensor` to be at least 1D\n  if tensor_ndims == 0:\n    tensor = tensor[None]\n\n  # Replicate tensor ""t"" along the 1st dimension.\n  tiled_tensor = tf.tile(tensor, [tf.reduce_prod(input_tensor=outer_shape)] +\n                         [1] * (tensor_ndims - 1))\n\n  # Reshape to match outer_shape.\n  return tf.reshape(tiled_tensor, target_shape)\n\n\ndef assert_members_are_not_overridden(base_cls,\n                                      instance,\n                                      white_list=(),\n                                      black_list=()):\n  """"""Asserts public members of `base_cls` are not overridden in `instance`.\n\n  If both `white_list` and `black_list` are empty, no public member of\n  `base_cls` can be overridden. If a `white_list` is provided, only public\n  members in `white_list` can be overridden. If a `black_list` is provided,\n  all public members except those in `black_list` can be overridden. Both\n  `white_list` and `black_list` cannot be provided at the same, if so a\n  ValueError will be raised.\n\n  Args:\n    base_cls: A Base class.\n    instance: An instance of a subclass of `base_cls`.\n    white_list: Optional list of `base_cls` members that can be overridden.\n    black_list: Optional list of `base_cls` members that cannot be overridden.\n\n  Raises:\n    ValueError if both white_list and black_list are provided.\n  """"""\n\n  if black_list and white_list:\n    raise ValueError(\'Both `black_list` and `white_list` cannot be provided.\')\n\n  instance_type = type(instance)\n  subclass_members = set(instance_type.__dict__.keys())\n  public_members = set(\n      [m for m in base_cls.__dict__.keys() if not m.startswith(\'_\')])\n  common_members = public_members & subclass_members\n\n  if white_list:\n    common_members = common_members - set(white_list)\n  elif black_list:\n    common_members = common_members & set(black_list)\n\n  overridden_members = [\n      m for m in common_members\n      if base_cls.__dict__[m] != instance_type.__dict__[m]\n  ]\n  if overridden_members:\n    raise ValueError(\n        \'Subclasses of {} cannot override most of its base members, but \'\n        \'{} overrides: {}\'.format(base_cls, instance_type, overridden_members))\n\n\ndef element_wise_squared_loss(x, y):\n  return tf.compat.v1.losses.mean_squared_error(\n      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n\n\ndef element_wise_huber_loss(x, y):\n  return tf.compat.v1.losses.huber_loss(\n      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n\n\ndef transpose_batch_time(x):\n  """"""Transposes the batch and time dimensions of a Tensor.\n\n  If the input tensor has rank < 2 it returns the original tensor. Retains as\n  much of the static shape information as possible.\n\n  Args:\n    x: A Tensor.\n\n  Returns:\n    x transposed along the first two dimensions.\n  """"""\n  x_static_shape = x.get_shape()\n  if x_static_shape.rank is not None and x_static_shape.rank < 2:\n    return x\n\n  x_rank = tf.rank(x)\n  x_t = tf.transpose(a=x, perm=tf.concat(([1, 0], tf.range(2, x_rank)), axis=0))\n  x_t.set_shape(\n      tf.TensorShape(\n          [x_static_shape.dims[1].value,\n           x_static_shape.dims[0].value]).concatenate(x_static_shape[2:]))\n  return x_t\n\n\ndef save_spec(spec, file_path):\n  """"""Saves the given spec nest as a StructProto.\n\n  **Note**: Currently this will convert BoundedTensorSpecs into regular\n    TensorSpecs.\n\n  Args:\n    spec: A nested structure of TensorSpecs.\n    file_path: Path to save the encoded spec to.\n  """"""\n  signature_encoder = nested_structure_coder.StructureCoder()\n  spec = tensor_spec.from_spec(spec)\n  spec_proto = signature_encoder.encode_structure(spec)\n\n  dir_path = os.path.dirname(file_path)\n  if not tf.io.gfile.exists(dir_path):\n    tf.io.gfile.makedirs(dir_path)\n\n  with tf.compat.v2.io.gfile.GFile(file_path, \'wb\') as gfile:\n    gfile.write(spec_proto.SerializeToString())\n\n\ndef load_spec(file_path):\n  """"""Loads a data spec from a file.\n\n  **Note**: Types for Named tuple classes will not match. Users need to convert\n    to these manually:\n\n    # Convert from:\n    # \'tensorflow.python.saved_model.nested_structure_coder.Trajectory\'\n    # to proper TrajectorySpec.\n    # trajectory_spec = trajectory.Trajectory(*spec)\n\n  Args:\n    file_path: Path to the saved data spec.\n  Returns:\n    A nested structure of TensorSpecs.\n  """"""\n  with tf.compat.v2.io.gfile.GFile(file_path, \'rb\') as gfile:\n    signature_proto = struct_pb2.StructuredValue.FromString(gfile.read())\n\n  signature_encoder = nested_structure_coder.StructureCoder()\n  return signature_encoder.decode_proto(signature_proto)\n\n\ndef extract_shared_variables(variables_1, variables_2):\n  """"""Separates shared variables from the given collections.\n\n  Args:\n    variables_1: An iterable of Variables\n    variables_2: An iterable of Variables\n\n  Returns:\n    A Tuple of ObjectIdentitySets described by the set operations\n\n    ```\n    (variables_1 - variables_2,\n     variables_2 - variables_1,\n     variables_1 & variables_2)\n    ```\n  """"""\n  var_refs1 = object_identity.ObjectIdentitySet(variables_1)\n  var_refs2 = object_identity.ObjectIdentitySet(variables_2)\n\n  shared_vars = var_refs1.intersection(var_refs2)\n  return (var_refs1.difference(shared_vars), var_refs2.difference(shared_vars),\n          shared_vars)\n\n\ndef check_no_shared_variables(network_1, network_2):\n  """"""Checks that there are no shared trainable variables in the two networks.\n\n  Args:\n    network_1: A network.Network.\n    network_2: A network.Network.\n\n  Raises:\n    ValueError: if there are any common trainable variables.\n    ValueError: if one of the networks has not yet been built\n      (e.g. user must call `create_variables`).\n  """"""\n  variables_1 = {id(v): v for v in network_1.trainable_variables}\n  variables_2 = {id(v): v for v in network_2.trainable_variables}\n  shared = set(variables_1.keys()) & set(variables_2.keys())\n  if shared:\n    shared_variables = [variables_1[v] for v in shared]\n    raise ValueError(\n        \'After making a copy of network \\\'{}\\\' to create a target \'\n        \'network \\\'{}\\\', the target network shares weights with \'\n        \'the original network.  This is not allowed.  If \'\n        \'you want explicitly share weights with the target network, or \'\n        \'if your input network shares weights with others, please \'\n        \'provide a target network which explicitly, selectively, shares \'\n        \'layers/weights with the input network.  If you are not intending to \'\n        \'share weights make sure all the weights are created inside the Network\'\n        \' since a copy will be created by creating a new Network with the same \'\n        \'args but a new name. Shared variables found: \'\n        \'\\\'{}\\\'.\'.format(network_1.name, network_2.name, shared_variables))\n\n\ndef check_matching_networks(network_1, network_2):\n  """"""Check that two networks have matching input specs and variables.\n\n  Args:\n    network_1: A network.Network.\n    network_2: A network.Network.\n\n  Raises:\n    ValueError: if the networks differ in input_spec, variables (number, dtype,\n      or shape).\n    ValueError: if either of the networks has not been built yet\n      (e.g. user must call `create_variables`).\n  """"""\n  if network_1.input_tensor_spec != network_2.input_tensor_spec:\n    raise ValueError(\'Input tensor specs of network and target network \'\n                     \'do not match: {} vs. {}.\'.format(\n                         network_1.input_tensor_spec,\n                         network_2.input_tensor_spec))\n  if len(network_1.variables) != len(network_2.variables):\n    raise ValueError(\n        \'Variables lengths do not match between Q network and target network: \'\n        \'{} vs. {}\'.format(network_1.variables, network_2.variables))\n  for v1, v2 in zip(network_1.variables, network_2.variables):\n    if v1.dtype != v2.dtype or v1.shape != v2.shape:\n      raise ValueError(\n          \'Variable dtypes or shapes do not match: {} vs. {}\'.format(v1, v2))\n\n\ndef maybe_copy_target_network_with_checks(network, target_network=None,\n                                          name=\'TargetNetwork\'):\n  """"""Copies the network into target if None and checks for shared variables.""""""\n  if target_network is None:\n    target_network = network.copy(name=name)\n    target_network.create_variables()\n  # Copy may have been shallow, and variables may inadvertently be shared\n  # between the target and the original networks. This would be an unusual\n  # setup, so we throw an error to protect users from accidentally doing so.\n  # If you explicitly want this to be enabled, please open a feature request\n  # with the team.\n  check_no_shared_variables(network, target_network)\n  check_matching_networks(network, target_network)\n  return target_network\n\n\nAggregatedLosses = cs.namedtuple(\n    \'AggregatedLosses\',\n    [\'total_loss\',  # Total loss = weighted + regularization\n     \'weighted\',  # Weighted sum of per_example_loss by sample_weight.\n     \'regularization\',  # Total of regularization losses.\n    ])\n\n\ndef aggregate_losses(per_example_loss=None,\n                     sample_weight=None,\n                     global_batch_size=None,\n                     regularization_loss=None):\n  """"""Aggregates and scales per example loss and regularization losses.\n\n  If `global_batch_size` is given it would be used for scaling, otherwise it\n  would use the batch_dim of per_example_loss and number of replicas.\n\n  Args:\n    per_example_loss: Per-example loss [B].\n    sample_weight: Optional weighting for each example [B].\n    global_batch_size: Optional global batch size value. Defaults to (size of\n    first dimension of `losses`) * (number of replicas).\n    regularization_loss: Regularization loss.\n\n  Returns:\n    An AggregatedLosses named tuple with scalar losses to optimize.\n  """"""\n  total_loss, weighted_loss, reg_loss = None, None, None\n  # Compute loss that is scaled by global batch size.\n  if per_example_loss is not None:\n    loss_rank = per_example_loss.shape.rank\n    if loss_rank is not None and loss_rank == 0:\n      err_msg = (\n          \'Need to use a loss function that computes losses per sample, ex: \'\n          \'replace losses.mean_squared_error with tf.math.squared_difference. \'\n          \'Invalid value passed for `per_example_loss`. Expected a tensor \'\n          \'tensor with at least rank 1, received: {}\'.format(per_example_loss))\n      if tf.distribute.has_strategy():\n        raise ValueError(err_msg)\n      else:\n        logging.warning(err_msg)\n        # Add extra dimension to prevent error in compute_average_loss.\n        per_example_loss = tf.expand_dims(per_example_loss, 0)\n    weighted_loss = tf.nn.compute_average_loss(\n        per_example_loss,\n        sample_weight=sample_weight,\n        global_batch_size=global_batch_size)\n    total_loss = weighted_loss\n  # Add scaled regularization losses.\n  if regularization_loss is not None:\n    reg_loss = tf.nn.scale_regularization_loss(regularization_loss)\n    if total_loss is None:\n      total_loss = reg_loss\n    else:\n      total_loss += reg_loss\n  return AggregatedLosses(total_loss, weighted_loss, reg_loss)\n\n\ndef summarize_scalar_dict(name_data, step, name_scope=\'Losses/\'):\n  if name_data:\n    with tf.name_scope(name_scope):\n      for name, data in name_data.items():\n        if data is not None:\n          tf.compat.v2.summary.scalar(\n              name=name, data=data, step=step)\n\n\n@contextlib.contextmanager\ndef soft_device_placement():\n  """"""Context manager for soft device placement, allowing summaries on CPU.\n\n  Eager and graph contexts have different default device placements. See\n  b/148408921 for details. This context manager should be used whenever using\n  summary writers contexts to make sure summaries work when executing on TPUs.\n\n  Yields:\n    Sets `tf.config.set_soft_device_placement(True)` within the context\n  """"""\n  original_setting = tf.config.get_soft_device_placement()\n  try:\n    tf.config.set_soft_device_placement(True)\n    yield\n  finally:\n    tf.config.set_soft_device_placement(original_setting)\n\n\ndef deduped_network_variables(network, *args):\n  """"""Returns a list of variables in net1 that are not in any other nets.\n\n  Args:\n    network: A Keras network.\n    *args: other networks to check for duplicate variables.\n  """"""\n  other_vars = object_identity.ObjectIdentitySet(\n      [v for n in args for v in n.variables])  # pylint:disable=g-complex-comprehension\n  return [v for v in network.variables if v not in other_vars]\n'"
tf_agents/utils/common_members_not_overridden_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.common.assert_members_are_not_overridden().""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import common\n\n\nclass Base(object):\n\n  def __init__(self, white_list=(), black_list=()):\n    common.assert_members_are_not_overridden(\n        base_cls=Base,\n        instance=self,\n        white_list=white_list,\n        black_list=black_list)\n\n  def method1(self):\n    pass\n\n  def method2(self):\n    pass\n\n\ndef child_class(cls, white_list=(), black_list=()):\n\n  class ChildNoOverrides(Base):\n\n    def __init__(self):\n      super(ChildNoOverrides, self).__init__(white_list, black_list)\n\n  class ChildOverrideMethod1(Base):\n\n    def __init__(self):\n      super(ChildOverrideMethod1, self).__init__(white_list, black_list)\n\n    def method1(self):\n      return 1\n\n  class ChildOverrideBoth(Base):\n\n    def __init__(self):\n      super(ChildOverrideBoth, self).__init__(white_list, black_list)\n\n    def method1(self):\n      return 1\n\n    def method2(self):\n      return 1\n\n  if cls == \'ChildNoOverrides\':\n    return ChildNoOverrides\n  elif cls == \'ChildOverrideMethod1\':\n    return ChildOverrideMethod1\n  elif cls == \'ChildOverrideBoth\':\n    return ChildOverrideBoth\n\n\nclass AssertMembersAreNotOverriddenTest(tf.test.TestCase):\n\n  def testNoOverridePublic(self):\n    child_cls = child_class(\'ChildNoOverrides\')\n    child_cls()\n\n  def testValueErrorOverridePublic(self):\n    child_cls = child_class(\'ChildOverrideMethod1\')\n    with self.assertRaises(ValueError):\n      child_cls()\n\n  def testWhiteListedCanBeOverridden(self):\n    child_cls = child_class(\'ChildOverrideMethod1\', white_list=(\'method1\',))\n    child_cls()\n\n  def testNonWhiteListedCannotBeOverridden(self):\n    child_cls = child_class(\'ChildOverrideBoth\', white_list=(\'method1\',))\n    with self.assertRaises(ValueError):\n      child_cls()\n\n  def testNonBlackListedCanBeOverridden(self):\n    child_cls = child_class(\'ChildOverrideMethod1\', black_list=(\'method2\',))\n    child_cls()\n\n  def testBlackListedCannotBeOverridden(self):\n    child_cls = child_class(\'ChildOverrideBoth\', black_list=(\'method2\',))\n    with self.assertRaises(ValueError):\n      child_cls()\n\n  def testWhiteListAndBlackListRaisesError(self):\n    child_cls = child_class(\'ChildNoOverrides\',\n                            white_list=(\'method1\',),\n                            black_list=(\'method2\',))\n    with self.assertRaises(ValueError):\n      child_cls()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/common_test.py,140,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.common.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport random\n\nfrom absl import flags\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.networks import test_utils as networks_test_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass CreateCounterTest(test_utils.TestCase):\n\n  def testDefaults(self):\n    counter = common.create_variable(\'counter\')\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(counter), 0)\n\n  def testInitialValue(self):\n    counter = common.create_variable(\'counter\', 1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(counter), 1)\n\n  def testIncrement(self):\n    counter = common.create_variable(\'counter\', 0)\n    inc_counter = counter.assign_add(1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(inc_counter), 1)\n\n  def testMultipleCounters(self):\n    counter1 = common.create_variable(\'counter\', 1)\n    counter2 = common.create_variable(\'counter\', 2)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(counter1), 1)\n    self.assertEqual(self.evaluate(counter2), 2)\n\n  def testInitialValueWithShape(self):\n    counter = common.create_variable(\'counter\', 1, shape=(2,))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(counter), [1, 1])\n\n  def testNonScalarInitialValue(self):\n    var = common.create_variable(\'var\', [1, 2], shape=None)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(var), [1, 2])\n\n\nclass SoftVariablesUpdateTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(0.0, 0.5, 1.0)\n  def testUpdateOnlyTargetVariables(self, tau):\n    inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    source_net = tf.keras.layers.Dense(2, name=\'source_net\')\n    target_net = tf.keras.layers.Dense(2, name=\'target_net\')\n\n    # Force variable creation\n    source_net(inputs)\n    target_net(inputs)\n\n    source_vars = source_net.trainable_weights\n    target_vars = target_net.trainable_weights\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    v_s, v_t = self.evaluate([source_vars, target_vars])\n\n    update_op = common.soft_variables_update(source_vars, target_vars, tau)\n    self.evaluate(update_op)\n    new_v_s, new_v_t = self.evaluate([source_vars, target_vars])\n\n    for i_v_s, i_v_t, n_v_s, n_v_t in zip(v_s, v_t, new_v_s, new_v_t):\n      # Source variables don\'t change\n      self.assertAllClose(n_v_s, i_v_s)\n      # Target variables are updated\n      self.assertAllClose(n_v_t, tau*i_v_s + (1-tau)*i_v_t)\n\n  @parameterized.parameters(0.0, 0.5, 1.0)\n  def testShuffleOrderVariables(self, tau):\n    inputs = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    source_net = tf.keras.layers.Dense(2, name=\'source_net\')\n    target_net = tf.keras.layers.Dense(2, name=\'target_net\')\n\n    # Force variable creation\n    source_net(inputs)\n    target_net(inputs)\n\n    source_vars = source_net.trainable_weights\n    target_vars = target_net.trainable_weights\n\n    shuffled_source_vars = sorted(source_vars,\n                                  key=lambda x: random.random())\n    shuffled_target_vars = sorted(target_vars,\n                                  key=lambda x: random.random())\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    v_s, v_t = self.evaluate([source_vars, target_vars])\n    update_op = common.soft_variables_update(shuffled_source_vars,\n                                             shuffled_target_vars,\n                                             tau,\n                                             sort_variables_by_name=True)\n\n    self.evaluate(update_op)\n    new_v_s, new_v_t = self.evaluate([source_vars, target_vars])\n    for i_v_s, i_v_t, n_v_s, n_v_t in zip(v_s, v_t, new_v_s, new_v_t):\n      # Source variables don\'t change\n      self.assertAllClose(n_v_s, i_v_s)\n      # Target variables are updated\n      self.assertAllClose(n_v_t, tau*i_v_s + (1-tau)*i_v_t)\n\n\nclass JoinScopeTest(test_utils.TestCase):\n\n  def _test_scopes(self, parent_scope, child_scope, expected_joined_scope):\n    joined_scope = common.join_scope(parent_scope, child_scope)\n    self.assertEqual(joined_scope, expected_joined_scope)\n\n  def testJoin(self):\n    self._test_scopes(\'parent\', \'child\', \'parent/child\')\n\n  def testJoinEmptyChild(self):\n    self._test_scopes(\'parent\', \'\', \'parent\')\n\n  def testJoinEmptyParent(self):\n    self._test_scopes(\'\', \'child\', \'child\')\n\n  def testJoinEmptyChildEmptyParent(self):\n    self._test_scopes(\'\', \'\', \'\')\n\n\nclass IndexWithActionsTest(test_utils.TestCase):\n\n  def checkCorrect(self,\n                   q_values,\n                   actions,\n                   expected_values,\n                   multi_dim_actions=False):\n    q_values = tf.constant(q_values, dtype=tf.float32)\n    actions = tf.constant(actions, dtype=tf.int32)\n    selected_q_values = common.index_with_actions(q_values, actions,\n                                                  multi_dim_actions)\n    selected_q_values_ = self.evaluate(selected_q_values)\n    self.assertAllClose(selected_q_values_, expected_values)\n\n  def testOneOuterDim(self):\n    q_values = [[1., 2., 3.],\n                [4., 5., 6.]]\n    actions = [2, 1]\n    expected_q_values = [3., 5.]\n    self.checkCorrect(q_values, actions, expected_q_values)\n\n  def testTwoOuterDims(self):\n    q_values = [[[1., 2., 3.],\n                 [4., 5., 6.]],\n                [[7., 8., 9.],\n                 [10., 11., 12.]]]\n    actions = [[2, 1], [0, 2]]\n    expected_q_values = [[3., 5.], [7., 12.]]\n    self.checkCorrect(q_values, actions, expected_q_values)\n\n  def testOneOuterDimTwoActionDims(self):\n    q_values = [[[1., 2., 3.],\n                 [4., 5., 6.]],\n                [[7., 8., 9.],\n                 [10., 11., 12.]]]\n    actions = [[1, 2], [0, 1]]\n    expected_q_values = [6., 8.]\n    self.checkCorrect(\n        q_values, actions, expected_q_values, multi_dim_actions=True)\n\n  def testOneOuterDimThreeActionDims(self):\n    q_values = [[[[1., 2., 3.],\n                  [4., 5., 6.]],\n                 [[7., 8., 9.],\n                  [10., 11., 12.]]],\n                [[[13., 14., 15.],\n                  [16., 17., 18.]],\n                 [[19., 20., 21.],\n                  [22., 23., 24.]]]]\n    actions = [[0, 1, 2], [1, 0, 1]]\n    expected_q_values = [6., 20.]\n    self.checkCorrect(\n        q_values, actions, expected_q_values, multi_dim_actions=True)\n\n  def testTwoOuterDimsUnknownShape(self):\n    q_values = tf.convert_to_tensor(\n        value=np.array([[[50, 51], [52, 53]]], dtype=np.float32))\n    actions = tf.convert_to_tensor(value=np.array([[1, 0]], dtype=np.int32))\n    values = common.index_with_actions(q_values, actions)\n\n    self.assertAllClose([[51, 52]], self.evaluate(values))\n\n\nclass PeriodicallyTest(test_utils.TestCase):\n  """"""Tests function periodically.""""""\n\n  def testPeriodically(self):\n    """"""Tests that a function is called exactly every `period` steps.""""""\n    target = tf.compat.v2.Variable(0)\n    period = 3\n\n    periodic_update = common.periodically(\n        body=lambda: tf.group(target.assign_add(1)), period=period)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    desired_values = [0, 0, 0, 1, 1, 1, 2, 2, 2, 3]\n    for desired_value in desired_values:\n      result = self.evaluate(target)\n      self.assertEqual(desired_value, result)\n      self.evaluate(periodic_update)\n\n  def testPeriodOne(self):\n    """"""Tests that the function is called every time if period == 1.""""""\n    target = tf.compat.v2.Variable(0)\n\n    periodic_update = common.periodically(\n        lambda: tf.group(target.assign_add(1)), period=1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for desired_value in range(0, 10):\n      result = self.evaluate(target)\n      self.assertEqual(desired_value, result)\n      self.evaluate(periodic_update)\n\n  def testPeriodNone(self):\n    """"""Tests that the function is never called if period == None.""""""\n    target = tf.compat.v2.Variable(0)\n\n    periodic_update = common.periodically(\n        body=lambda: target.assign_add(1), period=None)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    desired_value = 0\n    for _ in range(1, 11):\n      _, result = self.evaluate([periodic_update, target])\n      self.assertEqual(desired_value, result)\n\n  def testFunctionNotCallable(self):\n    """"""Tests value error when argument fn is not a callable.""""""\n    self.assertRaises(\n        TypeError, common.periodically, body=1, period=2)\n\n  def testPeriodVariable(self):\n    """"""Tests that a function is called exactly every `period` steps.""""""\n    target = tf.compat.v2.Variable(0)\n    period = tf.compat.v2.Variable(1)\n\n    periodic_update = common.periodically(\n        body=lambda: tf.group(target.assign_add(1)), period=period)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # With period = 1\n    desired_values = [0, 1, 2]\n    for desired_value in desired_values:\n      result = self.evaluate(target)\n      self.assertEqual(desired_value, result)\n      self.evaluate(periodic_update)\n\n    self.evaluate(target.assign(0))\n    self.evaluate(period.assign(3))\n    # With period = 3\n    desired_values = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n    for desired_value in desired_values:\n      result = self.evaluate(target)\n      self.assertEqual(desired_value, result)\n      self.evaluate(periodic_update)\n\n  def testMultiplePeriodically(self):\n    """"""Tests that 2 periodically ops run independently.""""""\n    target1 = tf.compat.v2.Variable(0)\n    periodic_update1 = common.periodically(\n        body=lambda: tf.group(target1.assign_add(1)), period=1)\n\n    target2 = tf.compat.v2.Variable(0)\n    periodic_update2 = common.periodically(\n        body=lambda: tf.group(target2.assign_add(2)), period=2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # With period = 1, increment = 1\n    desired_values1 = [0, 1, 2, 3]\n    # With period = 2, increment = 2\n    desired_values2 = [0, 0, 2, 2]\n\n    for i in range(len(desired_values1)):\n      result1 = self.evaluate(target1)\n      self.assertEqual(desired_values1[i], result1)\n      result2 = self.evaluate(target2)\n      self.assertEqual(desired_values2[i], result2)\n      self.evaluate([periodic_update1, periodic_update2])\n\n\nclass ClipToSpecTest(test_utils.TestCase):\n\n  def testClipToBounds(self):\n    value = tf.constant([1, 2, 4, -3])\n    spec = tensor_spec.BoundedTensorSpec((4,), tf.float32, [0, 0, 0, 0],\n                                         [3, 3, 3, 3])\n    expected_clipped_value = np.array([1, 2, 3, 0])\n    clipped_value = common.clip_to_spec(value, spec)\n\n    clipped_value_ = self.evaluate(clipped_value)\n    self.assertAllClose(expected_clipped_value, clipped_value_)\n\n\nclass ScaleToSpecTest(test_utils.TestCase):\n\n  def testSpecMeansAndMagnitudes(self):\n    spec = tensor_spec.BoundedTensorSpec(\n        (3, 2),\n        tf.float32,\n        [[-5, -5], [-4, -4], [-2, -6]],\n        [[5, 5], [4, 4], [2, 6]],\n    )\n    means, magnitudes = self.evaluate(common.spec_means_and_magnitudes(spec))\n    expected_means = np.zeros((3, 2), dtype=np.float32)\n    expected_magnitudes = np.array([[5.0, 5.0], [4.0, 4.0], [2.0, 6.0]],\n                                   dtype=np.float32)\n    self.assertAllClose(expected_means, means)\n    self.assertAllClose(expected_magnitudes, magnitudes)\n\n  def testScaleToSpec(self):\n    value = tf.constant([[1, -1], [0.5, -0.5], [1.0, 0.0]])\n    spec = tensor_spec.BoundedTensorSpec(\n        (3, 2),\n        tf.float32,\n        [[-5, -5], [-4, -4], [-2, -6]],\n        [[5, 5], [4, 4], [2, 6]],\n    )\n    expected_scaled_value = np.array([[[5, -5], [2.0, -2.0], [2.0, 0.0]]])\n    scaled_value = common.scale_to_spec(value, spec)\n\n    scaled_value_ = self.evaluate(scaled_value)\n    self.assertAllClose(expected_scaled_value, scaled_value_)\n\n\nclass OrnsteinUhlenbeckSamplesTest(test_utils.TestCase):\n\n  def testSamples(self):\n    """"""Tests that samples follow Ornstein-Uhlenbeck process.\n\n    This is done by checking that the successive differences\n    `x_next - (1-theta) * x` have the expected mean and variance.\n    """"""\n    # Increasing the number of samples can help reduce the variance and make the\n    # sample mean closer to the distribution mean.\n    num_samples = 1000\n    theta, sigma = 0.1, 0.2\n    ou = common.ornstein_uhlenbeck_process(\n        tf.zeros([10]), damping=theta, stddev=sigma)\n    samples = np.ndarray([num_samples, 10])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for i in range(num_samples):\n      samples[i] = self.evaluate(ou)\n\n    diffs = np.ndarray([num_samples-1, 10])\n    for i in range(num_samples - 1):\n      diffs[i] = samples[i+1] - (1-theta) * samples[i]\n    flat_diffs = diffs.reshape([-1])\n\n    mean, variance = flat_diffs.mean(), flat_diffs.var()\n    # To avoid flakiness, we can only expect the sample statistics to match\n    # the population statistics to one or two decimal places.\n    self.assertAlmostEqual(mean, 0.0, places=1)\n    self.assertAlmostEqual(variance, sigma*sigma, places=2)\n\n  def testMultipleSamples(self):\n    """"""Tests that creates different samples.\n\n    """"""\n    theta, sigma = 0.1, 0.2\n    ou1 = common.ornstein_uhlenbeck_process(\n        tf.zeros([10]), damping=theta, stddev=sigma)\n    ou2 = common.ornstein_uhlenbeck_process(\n        tf.zeros([10]), damping=theta, stddev=sigma)\n\n    samples = np.ndarray([100, 10, 2])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for i in range(100):\n      samples[i, :, 0], samples[i, :, 1] = self.evaluate([ou1, ou2])\n\n    diffs = samples[:, :, 0] - samples[:, :, 1]\n    difference = np.absolute(diffs).mean()\n\n    self.assertGreater(difference, 0.0)\n\n\nclass LogProbabilityTest(test_utils.TestCase):\n\n  def testLogProbability(self):\n    action_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1)\n    distribution = tfp.distributions.Normal([0.0, 0.0], [1.0, 1.0])\n    actions = tf.constant([0.0, 0.0])\n    log_probs = common.log_probability(distribution, actions, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    log_probs_ = self.evaluate(log_probs)\n    self.assertEqual(len(log_probs_.shape), 0)\n    self.assertNear(log_probs_, 2 * -0.5 * np.log(2 * 3.14159), 0.001)\n\n  def testLogProbabilityOneHot(self):\n    action_spec = tensor_spec.BoundedTensorSpec([3], tf.int32, 0, 1)\n    distribution = tfp.distributions.OneHotCategorical(probs=[0.6, 0.3, 0.1])\n    actions = tf.constant([1, 0, 0])\n    log_probs = common.log_probability(distribution, actions, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    log_probs_ = self.evaluate(log_probs)\n    self.assertEqual(len(log_probs_.shape), 0)\n    self.assertNear(log_probs_, np.log(0.6), 0.00001)\n\n  def testNestedLogProbability(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1),\n        [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1),\n         tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]]\n    distribution = [\n        tfp.distributions.Normal([0.0, 0.0], [1.0, 1.0]),\n        [\n            tfp.distributions.Normal([0.5], [1.0]),\n            tfp.distributions.Normal([-0.5], [1.0])\n        ]\n    ]\n    actions = [tf.constant([0.0, 0.0]),\n               [tf.constant([0.5]), tf.constant([-0.5])]]\n    log_probs = common.log_probability(distribution, actions, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    log_probs_ = self.evaluate(log_probs)\n    self.assertEqual(len(log_probs_.shape), 0)\n    self.assertNear(log_probs_, 4 * -0.5 * np.log(2 * 3.14159), 0.001)\n\n  def testBatchedNestedLogProbability(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1),\n        [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1),\n         tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]]\n    distribution = [\n        tfp.distributions.Normal([[0.0, 0.0], [0.0, 0.0]],\n                                 [[1.0, 1.0], [2.0, 2.0]]),\n        [\n            tfp.distributions.Normal([[0.5], [0.5]], [[1.0], [2.0]]),\n            tfp.distributions.Normal([[-0.5], [-0.5]], [[1.0], [2.0]])\n        ]\n    ]\n    actions = [tf.constant([[0.0, 0.0], [0.0, 0.0]]),\n               [tf.constant([[0.5], [0.5]]), tf.constant([[-0.5], [-0.5]])]]\n    log_probs = common.log_probability(distribution, actions, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    log_probs_ = self.evaluate(log_probs)\n    self.assertEqual(log_probs_.shape, (2,))\n    self.assertAllClose(log_probs_,\n                        [4 * -0.5 * np.log(2 * 3.14159),\n                         4 * -0.5 * np.log(8 * 3.14159)], 0.001)\n\n\nclass EntropyTest(test_utils.TestCase):\n\n  def testEntropy(self):\n    action_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1)\n    distribution = tfp.distributions.Normal([0.0, 0.0], [1.0, 2.0])\n    entropies = common.entropy(distribution, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    entropies_ = self.evaluate(entropies)\n    self.assertEqual(len(entropies_.shape), 0)\n    self.assertNear(entropies_,\n                    1.0 + 0.5 * np.log(2 * 3.14) + 0.5 * np.log(8 * 3.14159),\n                    0.001)\n\n  def testNestedEntropy(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1),\n        [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1),\n         tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]]\n    distribution = [\n        tfp.distributions.Normal([0.0, 0.0], [1.0, 2.0]),\n        [\n            tfp.distributions.Normal([0.5], [1.0]),\n            tfp.distributions.Normal([-0.5], [2.0])\n        ]\n    ]\n    entropies = common.entropy(distribution, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    entropies_ = self.evaluate(entropies)\n    self.assertEqual(len(entropies_.shape), 0)\n    self.assertNear(entropies_,\n                    2.0 + np.log(2 * 3.14) + np.log(8 * 3.14159),\n                    0.001)\n\n  def testBatchedNestedEntropy(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([2], tf.float32, -1, 1),\n        [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1),\n         tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]]\n    distribution = [\n        tfp.distributions.Normal([[0.0, 0.0], [0.0, 0.0]],\n                                 [[1.0, 1.0], [2.0, 2.0]]),\n        [\n            tfp.distributions.Normal([[0.5], [0.5]], [[1.0], [2.0]]),\n            tfp.distributions.Normal([[-0.5], [-0.5]], [[1.0], [2.0]])\n        ]\n    ]\n    entropies = common.entropy(distribution, action_spec)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    entropies_ = self.evaluate(entropies)\n    self.assertEqual(entropies_.shape, (2,))\n    self.assertAllClose(entropies_,\n                        [4 * (0.5 + 0.5 * np.log(2 * 3.14159)),\n                         4 * (0.5 + 0.5 * np.log(8 * 3.14159))], 0.001)\n\n\nclass DiscountedFutureSumTest(test_utils.TestCase):\n\n  def testNumSteps(self):\n    values = [[0, 1, 2, 3],\n              [1, 2, 3, 4],\n              [2, 3, 4, 5]]\n    tensor = tf.constant(values, dtype=tf.float32)\n\n    result_step1 = common.discounted_future_sum(tensor, 1.0, 1)\n    result_step3 = common.discounted_future_sum(tensor, 1.0, 3)\n    result_step20 = common.discounted_future_sum(tensor, 1.0, 20)\n\n    expected_result_step1 = values\n    expected_result_step3 = [\n        [3, 6, 5, 3],\n        [6, 9, 7, 4],\n        [9, 12, 9, 5]]\n    expected_result_step20 = [\n        [6, 6, 5, 3],\n        [10, 9, 7, 4],\n        [14, 12, 9, 5]]\n\n    self.assertAllClose(expected_result_step1, self.evaluate(result_step1))\n    self.assertAllClose(expected_result_step3, self.evaluate(result_step3))\n    self.assertAllClose(expected_result_step20, self.evaluate(result_step20))\n\n  def testGamma(self):\n    values = [[0, 1, 2, 3],\n              [1, 2, 3, 4],\n              [2, 3, 4, 5]]\n    tensor = tf.constant(values, dtype=tf.float32)\n\n    result_gamma0 = common.discounted_future_sum(tensor, 0.0, 3)\n    result_gamma09 = common.discounted_future_sum(tensor, 0.9, 3)\n    result_gamma1 = common.discounted_future_sum(tensor, 1.0, 3)\n    result_gamma2 = common.discounted_future_sum(tensor, 2.0, 3)\n\n    values = np.array(values)\n    values_shift1 = np.pad(values[:, 1:], ((0, 0), (0, 1)), \'constant\')\n    values_shift2 = np.pad(values[:, 2:], ((0, 0), (0, 2)), \'constant\')\n    expected_result_gamma0 = values\n    expected_result_gamma09 = (values + 0.9 * values_shift1 +\n                               0.81 * values_shift2)\n    expected_result_gamma1 = values + values_shift1 + values_shift2\n    expected_result_gamma2 = values + 2 * values_shift1 + 4 * values_shift2\n\n    self.assertAllClose(expected_result_gamma0, self.evaluate(result_gamma0))\n    self.assertAllClose(expected_result_gamma09, self.evaluate(result_gamma09))\n    self.assertAllClose(expected_result_gamma1, self.evaluate(result_gamma1))\n    self.assertAllClose(expected_result_gamma2, self.evaluate(result_gamma2))\n\n  def testMaskedReturns(self):\n    rewards = tf.ones(shape=(3, 7), dtype=tf.float32)\n    gamma = 0.9\n    num_steps = 7\n    episode_lengths = tf.constant([3, 7, 4])\n    discounted_returns = common.discounted_future_sum_masked(\n        rewards, gamma, num_steps, episode_lengths)\n\n    # Episodes should end at indices 2, 6, and 3, respectively.\n    # Values, counting back from the end of episode should be:\n    #   [.9^0, (.9^1 + .9^0), (.9^2 + .9^1 + .9^0), ...]\n    expected_returns = tf.constant([[2.71, 1.9, 1, 0, 0, 0, 0],\n                                    [5.217, 4.686, 4.095, 3.439, 2.71, 1.9, 1],\n                                    [3.439, 2.71, 1.9, 1, 0, 0, 0]],\n                                   dtype=tf.float32)\n    self.assertAllClose(self.evaluate(expected_returns),\n                        self.evaluate(discounted_returns), atol=0.001)\n\n\nclass ShiftValuesTest(test_utils.TestCase):\n\n  def testNumSteps(self):\n    values = [[0, 1, 2, 3],\n              [1, 2, 3, 4],\n              [2, 3, 4, 5]]\n    tensor = tf.constant(values, dtype=tf.float32)\n\n    result_step0 = common.shift_values(tensor, 1.0, 0)\n    result_step1 = common.shift_values(tensor, 1.0, 1)\n    result_step3 = common.shift_values(tensor, 1.0, 3)\n    result_step20 = common.shift_values(tensor, 1.0, 20)\n\n    values = np.array(values)\n    expected_result_step0 = values\n    expected_result_step1 = np.pad(values[:, 1:], ((0, 0), (0, 1)), \'constant\')\n    expected_result_step3 = np.pad(values[:, 3:], ((0, 0), (0, 3)), \'constant\')\n    expected_result_step20 = np.zeros_like(values)\n\n    self.assertAllClose(expected_result_step0, self.evaluate(result_step0))\n    self.assertAllClose(expected_result_step1, self.evaluate(result_step1))\n    self.assertAllClose(expected_result_step3, self.evaluate(result_step3))\n    self.assertAllClose(expected_result_step20, self.evaluate(result_step20))\n\n  def testGamma(self):\n    values = [[0, 1, 2, 3],\n              [1, 2, 3, 4],\n              [2, 3, 4, 5]]\n    tensor = tf.constant(values, dtype=tf.float32)\n\n    result_gamma0 = common.shift_values(tensor, 0.0, 3)\n    result_gamma09 = common.shift_values(tensor, 0.9, 3)\n    result_gamma1 = common.shift_values(tensor, 1.0, 3)\n    result_gamma2 = common.shift_values(tensor, 2.0, 3)\n\n    values = np.array(values)\n    values_shift3 = np.pad(values[:, 3:], ((0, 0), (0, 3)), \'constant\')\n    expected_result_gamma0 = np.zeros_like(values)\n    expected_result_gamma09 = 0.9 ** 3 * values_shift3\n    expected_result_gamma1 = values_shift3\n    expected_result_gamma2 = 2 ** 3 * values_shift3\n\n    self.assertAllClose(expected_result_gamma0, self.evaluate(result_gamma0))\n    self.assertAllClose(expected_result_gamma09, self.evaluate(result_gamma09))\n    self.assertAllClose(expected_result_gamma1, self.evaluate(result_gamma1))\n    self.assertAllClose(expected_result_gamma2, self.evaluate(result_gamma2))\n\n  def testFinalValues(self):\n    values = [[0, 1, 2, 3],\n              [1, 2, 3, 4],\n              [2, 3, 4, 5]]\n    tensor = tf.constant(values, dtype=tf.float32)\n    final_values = tf.constant([11, 12, 13], dtype=tf.float32)\n\n    result_gamma1 = common.shift_values(tensor, 1.0, 2, final_values)\n    result_gamma09 = common.shift_values(tensor, 0.9, 2, final_values)\n    result_step20 = common.shift_values(tensor, 0.9, 20, final_values)\n\n    expected_result_gamma1 = [\n        [2, 3, 11, 11],\n        [3, 4, 12, 12],\n        [4, 5, 13, 13]]\n    expected_result_gamma09 = [\n        [2 * 0.81, 3 * 0.81, 11 * 0.81, 11 * 0.9],\n        [3 * 0.81, 4 * 0.81, 12 * 0.81, 12 * 0.9],\n        [4 * 0.81, 5 * 0.81, 13 * 0.81, 13 * 0.9]]\n    expected_result_step20 = [\n        [11 * 0.9 ** 4, 11 * 0.9 ** 3, 11 * 0.9 ** 2, 11 * 0.9 ** 1],\n        [12 * 0.9 ** 4, 12 * 0.9 ** 3, 12 * 0.9 ** 2, 12 * 0.9 ** 1],\n        [13 * 0.9 ** 4, 13 * 0.9 ** 3, 13 * 0.9 ** 2, 13 * 0.9 ** 1]]\n\n    self.assertAllClose(expected_result_gamma1, self.evaluate(result_gamma1))\n    self.assertAllClose(expected_result_gamma09, self.evaluate(result_gamma09))\n    self.assertAllClose(expected_result_step20, self.evaluate(result_step20))\n\n\nclass GetEpisodeMaskTest(test_utils.TestCase):\n\n  def test(self):\n    first = ts.StepType.FIRST\n    mid = ts.StepType.MID\n    last = ts.StepType.LAST\n    step_types = [first, mid, mid, last, mid, mid, mid, last]\n    discounts = [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n    time_steps = ts.TimeStep(\n        step_type=step_types, discount=discounts, reward=discounts,\n        observation=discounts)\n    # TODO(b/123941561): Remove tf.function conversion.\n    get_episode_mask = common.function(common.get_episode_mask)\n    episode_mask = get_episode_mask(time_steps)\n\n    expected_mask = [1, 1, 1, 0, 1, 1, 1, 0]\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(expected_mask, self.evaluate(episode_mask))\n\n\nclass GetContiguousSubEpisodesTest(test_utils.TestCase):\n\n  def testNumSteps(self):\n    discounts = [\n        [0.9, 0.9, 0.9, 0.9],  # No episode termination.\n        [0.0, 0.9, 0.9, 0.9],  # Episode terminates on first step.\n        [0.9, 0.9, 0.0, 0.9]]  # Episode terminates on third step.\n\n    tensor = tf.constant(discounts, dtype=tf.float32)\n    result = common.get_contiguous_sub_episodes(tensor)\n\n    expected_result = [\n        [1.0, 1.0, 1.0, 1.0],\n        [1.0, 0.0, 0.0, 0.0],\n        [1.0, 1.0, 1.0, 0.0]]\n\n    self.assertAllClose(expected_result, self.evaluate(result))\n\n\nclass ConvertQLogitsToValuesTest(test_utils.TestCase):\n\n  def testConvertQLogitsToValues(self):\n    logits = tf.constant([[2., 4., 2.], [1., 1., 20.]])\n    support = tf.constant([10., 20., 30.])\n    values = common.convert_q_logits_to_values(logits, support)\n    values_ = self.evaluate(values)\n    self.assertAllClose(values_, [20.0, 30.0], 0.001)\n\n  def testConvertQLogitsToValuesBatch(self):\n    logits = tf.constant([[[1., 20., 1.], [1., 1., 20.]],\n                          [[20., 1., 1.], [1., 20., 20.]]])\n    support = tf.constant([10., 20., 30.])\n    values = common.convert_q_logits_to_values(logits, support)\n    values_ = self.evaluate(values)\n    self.assertAllClose(values_, [[20.0, 30.0], [10., 25.]], 0.001)\n\n\nclass ComputeReturnsTest(test_utils.TestCase):\n\n  def testComputeReturns(self):\n    rewards = tf.constant(np.ones(9), dtype=tf.float32)\n    discounts = tf.constant([1, 1, 1, 1, 0, 0.9, 0.9, 0.9, 0], dtype=tf.float32)\n    returns = common.compute_returns(rewards, discounts)\n    expected_returns = [5, 4, 3, 2, 1, 3.439, 2.71, 1.9, 1]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    returns = self.evaluate(returns)\n    self.assertAllClose(returns, expected_returns)\n\n  def testComputeReturnsRandomized(self):\n    rewards = tf.constant(np.random.random([20]), dtype=tf.float32)\n    discounts = tf.constant(np.random.random([20]), dtype=tf.float32)\n    returns = common.compute_returns(rewards, discounts)\n\n    def _compute_returns_fn(rewards, discounts):\n      """"""Python implementation of computing discounted returns.""""""\n      returns = np.zeros(len(rewards))\n      next_state_return = 0.0\n      for t in range(len(returns) - 1, -1, -1):\n        returns[t] = rewards[t] + discounts[t] * next_state_return\n        next_state_return = returns[t]\n      return returns.astype(np.float32)\n\n    expected_returns = tf.compat.v1.py_func(_compute_returns_fn,\n                                            [rewards, discounts],\n                                            tf.float32)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    returns = self.evaluate(returns)\n    expected_returns = self.evaluate(expected_returns)\n    self.assertAllClose(returns, expected_returns)\n\n\nclass ReplicateTensorTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\'list\', \'tf_constant\')\n  def testReplicateTensor(self, outer_shape_type):\n    value = np.array([[1., 2., 3.], [4., 5., 6.]])\n    if outer_shape_type == \'tf_constant\':\n      outer_shape = tf.constant([2, 1])\n    else:\n      outer_shape = [2, 1]\n    expected_replicated_value = np.array([[value], [value]])\n\n    tf_value = tf.constant(value)\n    replicated_value = self.evaluate(common.replicate(tf_value, outer_shape))\n    self.assertAllEqual(expected_replicated_value, replicated_value)\n\n    if isinstance(outer_shape, np.ndarray):\n      # The shape should be fully defined in this case.\n      self.assertEqual(tf.TensorShape(outer_shape + list(value.shape)),\n                       replicated_value.shape)\n\n  def testReplicateScalarTensor(self):\n    value = 1\n    outer_shape = [2, 1]\n    expected_replicated_value = np.array([[value], [value]])\n\n    tf_value = tf.constant(value, shape=())\n    replicated_value = self.evaluate(common.replicate(tf_value, outer_shape))\n    self.assertAllEqual(expected_replicated_value, replicated_value)\n\n\nclass FunctionTest(test_utils.TestCase):\n\n  def testFunction(self):\n    outer_graph = tf.compat.v1.get_default_graph()\n\n    @common.function_in_tf1()\n    def add(x, y):\n      if common.has_eager_been_enabled():\n        # In TF2, this should be executed in eager mode.\n        self.assertTrue(tf.executing_eagerly())\n      else:\n        # In TF1, this should be inside a temporary graph because it\'s being\n        # created inside a tf.function.\n        inner_graph = tf.compat.v1.get_default_graph()\n        self.assertNotEqual(outer_graph, inner_graph)\n      return x + y\n\n    z = add(tf.constant(1.0), 2.0)\n\n    self.assertAllClose(3.0, self.evaluate(z))\n\n\nclass DefaultTFFunctionParams(test_utils.TestCase):\n\n  def testAutographRequired(self):\n\n    def inner_fn_requires_autograph(a, b):\n      for v in a:\n        b += v\n      return b\n\n    inner_fn = common.function(inner_fn_requires_autograph)\n    # Using general exception to avoid internal TF import.\n    with self.assertRaises(Exception):\n      inner_fn(tf.convert_to_tensor([1, 2, 3]), tf.constant(0))\n\n  def testAutographEnabling(self):\n\n    @common.set_default_tf_function_parameters(autograph=True)\n    def inner_fn_requires_autograph(a, b):\n      for v in a:\n        b += v\n      return b\n\n    inner_fn = common.function(inner_fn_requires_autograph)\n    inner_fn(tf.convert_to_tensor([1, 2, 3]), tf.constant(0))\n\n\nclass SpecSaveTest(tf.test.TestCase, parameterized.TestCase):\n\n  def test_save_and_load(self):\n    spec = {\n        \'spec_1\':\n            tensor_spec.TensorSpec((2, 3), tf.int32),\n        \'bounded_spec_1\':\n            tensor_spec.BoundedTensorSpec((2, 3), tf.float32, -10, 10),\n        \'bounded_spec_2\':\n            tensor_spec.BoundedTensorSpec((2, 3), tf.int8, -10, -10),\n        \'bounded_array_spec_3\':\n            tensor_spec.BoundedTensorSpec((2,), tf.int32, [-10, -10], [10, 10]),\n        \'bounded_array_spec_4\':\n            tensor_spec.BoundedTensorSpec((2,), tf.float16, [-10, -9], [10, 9]),\n        \'dict_spec\': {\n            \'spec_2\':\n                tensor_spec.TensorSpec((2, 3), tf.float32),\n            \'bounded_spec_2\':\n                tensor_spec.BoundedTensorSpec((2, 3), tf.int16, -10, 10)\n        },\n        \'tuple_spec\': (\n            tensor_spec.TensorSpec((2, 3), tf.int32),\n            tensor_spec.BoundedTensorSpec((2, 3), tf.float64, -10, 10),\n        ),\n        \'list_spec\': [\n            tensor_spec.TensorSpec((2, 3), tf.int64),\n            (tensor_spec.TensorSpec((2, 3), tf.float32),\n             tensor_spec.BoundedTensorSpec((2, 3), tf.float32, -10, 10)),\n        ],\n    }\n\n    spec_save_path = os.path.join(flags.FLAGS.test_tmpdir, \'spec.tfrecord\')\n    common.save_spec(spec, spec_save_path)\n\n    loaded_spec_nest = common.load_spec(spec_save_path)\n\n    self.assertAllEqual(sorted(spec.keys()), sorted(loaded_spec_nest.keys()))\n\n    for expected_spec, loaded_spec in zip(\n        tf.nest.flatten(spec), tf.nest.flatten(loaded_spec_nest)):\n      self.assertAllEqual(expected_spec.shape, loaded_spec.shape)\n      self.assertEqual(expected_spec.dtype, loaded_spec.dtype)\n\n\nclass NetworkVariableChecks(tf.test.TestCase):\n\n  def setUp(self):\n    super(tf.test.TestCase, self).setUp()\n    self._observation_spec = tensor_spec.TensorSpec([1, 2], tf.float32)\n    self._action_spec = [tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)]\n\n  def test_check_no_shared_variables(self):\n    layer_1 = tf.keras.layers.Dense(3)\n    layer_2 = tf.keras.layers.Dense(3)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_1)\n    q_net_2 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_2)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    common.check_no_shared_variables(q_net_1, q_net_2)\n\n  def test_check_no_shared_variables_expect_fail(self):\n    dense_layer = tf.keras.layers.Dense(3)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, dense_layer)\n    q_net_2 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, dense_layer)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    with self.assertRaises(ValueError):\n      common.check_no_shared_variables(q_net_1, q_net_2)\n\n  def test_check_matching_networks(self):\n    layer_1 = tf.keras.layers.Dense(3)\n    layer_2 = tf.keras.layers.Dense(3)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_1)\n    q_net_2 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_2)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    common.check_matching_networks(q_net_1, q_net_2)\n\n  def test_check_matching_networks_different_input_spec(self):\n    layer_1 = tf.keras.layers.Dense(3)\n    layer_2 = tf.keras.layers.Dense(3)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_1)\n    q_net_2 = networks_test_utils.KerasLayersNet(\n        tensor_spec.TensorSpec([3], tf.float32), self._action_spec, layer_2)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    with self.assertRaisesRegexp(\n        ValueError, \'Input tensor specs of network and target network \'\n        \'do not match\'):\n      common.check_matching_networks(q_net_1, q_net_2)\n\n  def test_check_matching_networks_different_vars(self):\n    layer_1 = tf.keras.layers.Dense(3)\n    layer_2 = tf.keras.layers.GRU(3)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_1)\n    q_net_2 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_2)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    with self.assertRaisesRegexp(ValueError, \'Variables lengths do not match\'):\n      common.check_matching_networks(q_net_1, q_net_2)\n\n  def test_check_matching_networks_different_shape(self):\n    layer_1 = tf.keras.layers.Dense(3)\n    layer_2 = tf.keras.layers.Dense(4)\n    q_net_1 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_1)\n    q_net_2 = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                 self._action_spec, layer_2)\n    q_net_1.create_variables()\n    q_net_2.create_variables()\n    with self.assertRaisesRegexp(ValueError,\n                                 \'Variable dtypes or shapes do not match\'):\n      common.check_matching_networks(q_net_1, q_net_2)\n\n\nclass LegacyTF1Test(test_utils.TestCase):\n\n  def test_in_legacy_tf1(self):\n    self.assertIsInstance(common.in_legacy_tf1(), bool)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/composite.py,42,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for dealing with CompositeTensors.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef shape(tensor):\n  if isinstance(tensor, tf.SparseTensor):\n    return tensor.dense_shape\n  else:\n    return tf.shape(input=tensor, out_type=tf.int64)\n\n\ndef reshape(t, shape):  # pylint: disable=redefined-outer-name\n  """"""Reshape composite tensor `t` to `shape`.\n\n  Args:\n    t: A `Tensor` or `SparseTensor`.\n    shape: `1D` tensor, array, or list.  The new shape.\n\n  Returns:\n    The reshaped tensor.\n  """"""\n  return (tf.sparse.reshape(t, shape) if isinstance(t, tf.SparseTensor)\n          else tf.reshape(t, shape))\n\n\ndef squeeze(t, axis):\n  """"""Squeeze composite tensor along axis `axis`.\n\n  Args:\n    t: A `Tensor` or `SparseTensor`.\n    axis: A python integer.\n\n  Returns:\n    The tensor with dimension `axis` removed.\n\n  Raises:\n    InvalidArgumentError: If `t` is a `SparseTensor` and has more than one index\n    stored along `axis`.\n  """"""\n  if isinstance(t, tf.SparseTensor):\n    # Fill in a dummy value if there are no elements in the tensor.\n    indices_axis = t.indices[:, axis]\n    all_zero = tf.reduce_all(tf.equal(indices_axis, 0))\n    with tf.control_dependencies([\n        tf.Assert(\n            all_zero,\n            [\'Unable to squeeze SparseTensor {} axis {} \'\n             \'because indices are not all equal to 0:\', indices_axis])]):\n      return tf.SparseTensor(\n          indices=tf.concat(\n              (t.indices[:, :axis], t.indices[:, axis + 1:]),\n              axis=1),\n          values=t.values,\n          dense_shape=tf.concat(\n              (t.dense_shape[:axis], t.dense_shape[axis + 1:]),\n              axis=0))\n  else:\n    return tf.squeeze(t, [axis])\n\n\ndef expand_dims(t, axis):\n  """"""Add a new dimension to tensor `t` along `axis`.\n\n  Args:\n    t: A `tf.Tensor` or `tf.SparseTensor`.\n    axis: A `0D` integer scalar.\n\n  Returns:\n    An expanded tensor.\n\n  Raises:\n    NotImplementedError: If `t` is a `SparseTensor` and `axis != 0`.\n  """"""\n  if isinstance(t, tf.SparseTensor):\n    if tf.is_tensor(axis) or axis != 0:\n      raise NotImplementedError(\n          \'Can only expand_dims on SparseTensor {} on static axis 0, \'\n          \'but received axis {}\'.format(t, axis))\n    n_elem = (\n        t.indices.shape[0] or tf.get_static_shape(t.dense_shape)[0]\n        or tf.shape(t.indices)[0])\n    shape_ = tf.cast(t.shape, tf.int64)\n    return tf.SparseTensor(\n        indices=tf.concat((tf.zeros([n_elem, 1], dtype=tf.int64),\n                           t.indices),\n                          axis=1),\n        values=t.values,\n        dense_shape=tf.concat(([1], shape_), axis=0))\n  else:\n    return tf.expand_dims(t, axis)\n\n\ndef slice_from(tensor, axis, start):\n  """"""Slice a composite tensor along `axis` from `start`.\n\n  Examples:\n\n  ```python\n  slice_from(tensor, 2, 1) === tensor[:, :, 1:]\n  sparse_to_dense(slice_from(sparse_tensor, 2, 1))\n    === sparse_to_dense(sparse_tensor)[:, :, 1:]\n  ```\n\n  Args:\n    tensor: A `Tensor` or `SparseTensor`.\n    axis: A python integer.\n    start: A `0D` scalar.\n\n  Returns:\n    The sliced composite tensor.\n  """"""\n  if isinstance(tensor, tf.SparseTensor):\n    if not tf.is_tensor(start) and start < 0:\n      start = tensor.dense_shape[axis] + start\n    all_but_first = tf.reshape(\n        tf.where(tensor.indices[:, axis] >= start),\n        [-1])\n    indices = tf.gather(tensor.indices, all_but_first)\n    indices = tf.unstack(indices, axis=1)\n    indices = tf.stack(indices[:axis]\n                       + [indices[axis] - start]\n                       + indices[axis + 1:],\n                       axis=1)\n    new_shape = tf.unstack(tensor.dense_shape)\n    new_shape[axis] = new_shape[axis] - start\n    return tf.SparseTensor(\n        indices=indices,\n        values=tf.gather(tensor.values, all_but_first),\n        dense_shape=tf.stack(new_shape))\n  else:\n    ndims = len(tensor.shape)\n    if ndims is None:\n      raise ValueError(\n          \'Unable to slice a tensor with unknown rank: {}\'.format(tensor))\n    slices = tuple([slice(None)] * axis\n                   + [slice(start, None)]\n                   + [slice(None)] * (ndims - axis - 1))\n    return tensor[slices]\n\n\ndef slice_to(tensor, axis, end):\n  """"""Slice a composite tensor along `axis` from 0 to `end`.\n\n  Examples:\n\n  ```python\n  slice_to(tensor, 2, -1) === tensor[:, :, :-1]\n  sparse_to_dense(slice_to(sparse_tensor, 2, -1))\n    === sparse_to_dense(sparse_tensor)[:, :, :-1]\n  ```\n\n  Args:\n    tensor: A `Tensor` or `SparseTensor`.\n    axis: A python integer.\n    end: A `0D` scalar.\n\n  Returns:\n    The sliced composite tensor.\n  """"""\n  if isinstance(tensor, tf.SparseTensor):\n    if not tf.is_tensor(end) and end < 0:\n      end = tensor.dense_shape[axis] + end\n    all_but_first = tf.reshape(\n        tf.where(tensor.indices[:, axis] < end),\n        [-1])\n    new_shape = tf.unstack(tensor.dense_shape)\n    new_shape[axis] = end\n    return tf.SparseTensor(\n        indices=tf.gather(tensor.indices, all_but_first),\n        values=tf.gather(tensor.values, all_but_first),\n        dense_shape=tf.stack(new_shape))\n  else:\n    ndims = len(tensor.shape)\n    if ndims is None:\n      raise ValueError(\n          \'Unable to slice a tensor with unknown rank: {}\'.format(tensor))\n    slices = tuple([slice(None)] * axis\n                   + [slice(None, end)]\n                   + [slice(None)] * (ndims - axis - 1))\n    return tensor[slices]\n'"
tf_agents/utils/composite_test.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.composite.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import composite\n\n\ndef _to_dense(st):\n  return tf.scatter_nd(st.indices, st.values, st.dense_shape)\n\n\nclass CompositeTest(tf.test.TestCase):\n  """"""Tests functions related to composite tensors.""""""\n\n  def setUp(self):\n    super(CompositeTest, self).setUp()\n    self._x = tf.random.uniform([4, 5, 6], dtype=tf.int32, maxval=1000)\n    self._sx = tf.sparse.reorder(\n        tf.SparseTensor(\n            indices=tf.random.uniform([40, 3], maxval=10, dtype=tf.int64),\n            values=tf.random.uniform([40], maxval=1000, dtype=tf.int32),\n            dense_shape=[10, 10, 10]))\n\n  def testSliceFrom(self):\n    from_1 = composite.slice_from(self._x, axis=1, start=1)\n    from_n1 = composite.slice_from(self._x, axis=1, start=-1)\n    x, from_1, from_n1 = self.evaluate((self._x, from_1, from_n1))\n    self.assertAllEqual(from_1, x[:, 1:, :])\n    self.assertAllEqual(from_n1, x[:, -1:, :])\n\n    s_from_1 = _to_dense(composite.slice_from(self._sx, axis=1, start=1))\n    s_from_n1 = _to_dense(composite.slice_from(self._sx, axis=1, start=-1))\n    sx = _to_dense(self._sx)\n    sx, s_from_1, s_from_n1 = self.evaluate((sx, s_from_1, s_from_n1))\n    self.assertAllEqual(s_from_1, sx[:, 1:, :])\n    self.assertAllEqual(s_from_n1, sx[:, -1:, :])\n\n  def testSliceTo(self):\n    to_1 = composite.slice_to(self._x, axis=1, end=1)\n    to_n1 = composite.slice_to(self._x, axis=1, end=-1)\n    x, to_1, to_n1 = self.evaluate((self._x, to_1, to_n1))\n    self.assertAllEqual(to_1, x[:, :1, :])\n    self.assertAllEqual(to_n1, x[:, :-1, :])\n\n    s_from_1 = _to_dense(composite.slice_to(self._sx, axis=1, end=1))\n    s_from_n1 = _to_dense(composite.slice_to(self._sx, axis=1, end=-1))\n    sx = _to_dense(self._sx)\n    sx, s_from_1, s_from_n1 = self.evaluate((sx, s_from_1, s_from_n1))\n    self.assertAllEqual(s_from_1, sx[:, :1, :])\n    self.assertAllEqual(s_from_n1, sx[:, :-1, :])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/eager_utils.py,100,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utilities for TF-Agents.\n\nExample of usage:\n\n  ```python\n  from tf_agents.utils import eager_utils\n\n  @eager_utils.run_in_graph_and_eager_modes\n  def loss_fn(x, y):\n    v = tf.get_variable(\'v\', initializer=tf.ones_initializer(), shape=())\n    return v + x - y\n\n  with tfe.graph_mode():\n    # loss and train_step are Tensors/Ops in the graph\n    loss_op = loss_fn(inputs, labels)\n    train_step_op = eager_utils.create_train_step(loss_op, optimizer)\n    # Compute the loss and apply gradients to the variables using the optimizer.\n    with tf.Session() as sess:\n      sess.run(tf.compat.v1.global_variables_initializer())\n      for _ in range(num_train_steps):\n        loss_value = sess.run(train_step_op)\n\n  with tfe.eager_mode():\n    # loss and train_step are lambda functions that can be called.\n    loss = loss_fn(inputs, labels)\n    train_step = eager_utils.create_train_step(loss, optimizer)\n    # Compute the loss and apply gradients to the variables using the optimizer.\n    for _ in range(num_train_steps):\n      loss_value = train_step()\n  ```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport functools\nimport inspect\nfrom absl import logging\n\nimport numpy as np\nimport six\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\n\nfrom tensorflow.python.util import tf_decorator  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n_USE_GLOBAL_STEP = 0\n\n\ndef has_self_cls_arg(func_or_method):\n  """"""Checks if it is method which takes self/cls as the first argument.""""""\n  if isinstance(func_or_method, staticmethod):\n    return False\n  if inspect.ismethod(func_or_method):\n    return True\n  if isinstance(func_or_method, classmethod):\n    return True\n  if six.PY2:\n    arg_names = inspect.getargspec(func_or_method).args\n  else:\n    arg_names = list(inspect.signature(func_or_method).parameters)\n  if arg_names and arg_names[0] in (\'self\', \'cls\'):\n    return True\n  return False\n\n\ndef is_unbound(method):\n  """"""Checks if it is an unbounded method.""""""\n  return not (hasattr(method, \'__self__\') and method.__self__)\n\n\nclass Future(object):\n  """"""Converts a function or class method call into a future callable.""""""\n\n  def __init__(self, func_or_method, *args, **kwargs):\n    self._func_or_method = func_or_method\n    self._args = args\n    self._kwargs = copy.copy(kwargs)\n    getargspec = inspect.getargspec if six.PY2 else inspect.getfullargspec\n    arg_names = getargspec(func_or_method).args\n    self._arg_names = arg_names\n    self._self = None\n    if has_self_cls_arg(func_or_method):\n      # Skip the first arg_name self/cls\n      self._arg_names = arg_names[1:]\n      if is_unbound(func_or_method):\n        # For unbound methods we require self/cls as args[0].\n        if not args:\n          raise ValueError(\n              \'func_or_method is unbond, but not class/instance provided.\')\n        else:\n          self._self = args[0]\n          self._args = args[1:]\n\n  def __call__(self, *args, **kwargs):\n    """"""If *args/**kwargs are given they would replace those given at init.\n\n    Args:\n      *args: List of extra arguments.\n      **kwargs: Dict of extra keyword arguments.\n    Returns:\n      The result of func_or_method(*args, **kwargs).\n    """"""\n    # By default use the init args.\n    call_args = args or self._args\n    call_kwargs = copy.copy(self._kwargs)\n    for arg_name in self._arg_names[:len(args)]:\n      # Remove any original kwargs replaced by new positional args.\n      call_kwargs.pop(arg_name, None)\n    call_kwargs.update(kwargs)\n    if self._self:\n      return self._func_or_method(self._self, *call_args, **call_kwargs)\n    return self._func_or_method(*call_args, **call_kwargs)\n\n\ndef future_in_eager_mode(func_or_method):\n  """"""Decorator that allow a function/method to run in graph and in eager modes.\n\n  When applied in graph mode it calls the function and return its outputs.\n  When applied in eager mode it returns a lambda function that when called\n  returns the outputs.\n\n  ```python\n  @eager_utils.future_in_eager_mode\n  def loss_fn(x):\n    v = tf.get_variable(\'v\', initializer=tf.ones_initializer(), shape=())\n    return v + x\n\n  with context.graph_mode():\n    loss_op = loss_fn(inputs)\n    loss_value = sess.run(loss_op)\n\n  with context.eager_mode():\n    loss = loss_fn(inputs)\n    # Now loss is a Future callable.\n    loss_value = loss()\n\n  Args:\n    func_or_method: A function or method to decorate.\n\n  Returns:\n    Either the output ops of the function/method or a Future (lambda function).\n  """"""\n  if not callable(func_or_method):\n    raise TypeError(\'func_or_method must be callable.\')\n\n  def decorator(*args, **kwargs):\n    if tf.executing_eagerly():\n      return Future(func_or_method, *args, **kwargs)\n    else:\n      return func_or_method(*args, **kwargs)\n\n  return tf_decorator.make_decorator(func_or_method, decorator)\n\n\ndef add_variables_summaries(grads_and_vars, step):\n  """"""Add summaries for variables.\n\n  Args:\n    grads_and_vars: A list of (gradient, variable) pairs.\n    step: Variable to use for summaries.\n  """"""\n  with tf.name_scope(\'summarize_vars\'):\n    for _, var in grads_and_vars:\n      if isinstance(var, tf.IndexedSlices):\n        var_values = var.values\n      else:\n        var_values = var\n      var_name = var.name.replace(\':\', \'_\')\n      tf.compat.v2.summary.histogram(\n          name=var_name + \'_value\', data=var_values, step=step)\n      tf.compat.v2.summary.scalar(\n          name=var_name + \'_value_norm\',\n          data=tf.linalg.global_norm([var_values]),\n          step=step)\n\n\ndef add_gradients_summaries(grads_and_vars, step):\n  """"""Add summaries to gradients.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n    step: Variable to use for summaries.\n  """"""\n  with tf.name_scope(\'summarize_grads\'):\n    for grad, var in grads_and_vars:\n      if grad is not None:\n        if isinstance(grad, tf.IndexedSlices):\n          grad_values = grad.values\n        else:\n          grad_values = grad\n        var_name = var.name.replace(\':\', \'_\')\n        tf.compat.v2.summary.histogram(\n            name=var_name + \'_gradient\', data=grad_values, step=step)\n        tf.compat.v2.summary.scalar(\n            name=var_name + \'_gradient_norm\',\n            data=tf.linalg.global_norm([grad_values]),\n            step=step)\n      else:\n        logging.info(\'Var %s has no gradient\', var.name)\n\n\ndef clip_gradient_norms(gradients_to_variables, max_norm):\n  """"""Clips the gradients by the given value.\n\n  Args:\n    gradients_to_variables: A list of gradient to variable pairs (tuples).\n    max_norm: the maximum norm value.\n\n  Returns:\n    A list of clipped gradient to variable pairs.\n  """"""\n  clipped_grads_and_vars = []\n  for grad, var in gradients_to_variables:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        tmp = tf.clip_by_norm(grad.values, max_norm)\n        grad = tf.IndexedSlices(tmp, grad.indices, grad.dense_shape)\n      else:\n        grad = tf.clip_by_norm(grad, max_norm)\n    clipped_grads_and_vars.append((grad, var))\n  return clipped_grads_and_vars\n\n\ndef clip_gradient_norms_fn(max_norm):\n  """"""Returns a `transform_grads_fn` function for gradient clipping.""""""\n  def clip_norms(gradients_to_variables):\n    return clip_gradient_norms(gradients_to_variables, max_norm)\n  return clip_norms\n\n\ndef create_train_step(loss,\n                      optimizer,\n                      global_step=_USE_GLOBAL_STEP,\n                      total_loss_fn=None,\n                      update_ops=None,\n                      variables_to_train=None,\n                      transform_grads_fn=None,\n                      summarize_gradients=False,\n                      gate_gradients=tf.compat.v1.train.Optimizer.GATE_OP,\n                      aggregation_method=None,\n                      check_numerics=True):\n  """"""Creates a train_step that evaluates the gradients and returns the loss.\n\n  Args:\n    loss: A (possibly nested tuple of) `Tensor` or function representing\n      the loss.\n    optimizer: A tf.Optimizer to use for computing the gradients.\n    global_step: A `Tensor` representing the global step variable. If left as\n      `_USE_GLOBAL_STEP`, then tf.train.get_or_create_global_step() is used.\n    total_loss_fn: Function to call on loss value to access the final\n     item to minimize.\n    update_ops: An optional list of updates to execute. If `update_ops` is\n      `None`, then the update ops are set to the contents of the\n      `tf.GraphKeys.UPDATE_OPS` collection. If `update_ops` is not `None`, but\n      it doesn\'t contain all of the update ops in `tf.GraphKeys.UPDATE_OPS`,\n      a warning will be displayed.\n    variables_to_train: an optional list of variables to train. If None, it will\n      default to all tf.trainable_variables().\n    transform_grads_fn: A function which takes a single argument, a list of\n      gradient to variable pairs (tuples), performs any requested gradient\n      updates, such as gradient clipping or multipliers, and returns the updated\n      list.\n    summarize_gradients: Whether or not add summaries for each gradient.\n    gate_gradients: How to gate the computation of gradients. See tf.Optimizer.\n    aggregation_method: Specifies the method used to combine gradient terms.\n      Valid values are defined in the class `AggregationMethod`.\n    check_numerics: Whether or not we apply check_numerics.\n\n  Returns:\n    In graph mode: A (possibly nested tuple of) `Tensor` that when evaluated,\n      calculates the current loss, computes the gradients, applies the\n      optimizer, and returns the current loss.\n    In eager mode: A lambda function that when is called, calculates the loss,\n      then computes and applies the gradients and returns the original\n      loss values.\n  Raises:\n    ValueError: if loss is not callable.\n    RuntimeError: if resource variables are not enabled.\n  """"""\n  if total_loss_fn is None:\n    total_loss_fn = lambda x: x\n  if not callable(total_loss_fn):\n    raise ValueError(\'`total_loss_fn` should be a function.\')\n  if not common.resource_variables_enabled():\n    raise RuntimeError(common.MISSING_RESOURCE_VARIABLES_ERROR)\n  if not tf.executing_eagerly():\n    if callable(loss):\n      loss = loss()\n    if callable(variables_to_train):\n      variables_to_train = variables_to_train()\n    # Calculate loss first, then calculate train op, then return the original\n    # loss conditioned on executing the train op.\n    with tf.control_dependencies(tf.nest.flatten(loss)):\n      loss = tf.nest.map_structure(\n          lambda t: tf.identity(t, \'loss_pre_train\'), loss)\n    train_op = create_train_op(\n        total_loss_fn(loss),\n        optimizer,\n        global_step=global_step,\n        update_ops=update_ops,\n        variables_to_train=variables_to_train,\n        transform_grads_fn=transform_grads_fn,\n        summarize_gradients=summarize_gradients,\n        gate_gradients=gate_gradients,\n        aggregation_method=aggregation_method,\n        check_numerics=check_numerics)\n\n    with tf.control_dependencies([train_op]):\n      return tf.nest.map_structure(\n          lambda t: tf.identity(t, \'loss_post_train\'), loss)\n\n  if global_step is _USE_GLOBAL_STEP:\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n\n  if not callable(loss):\n    raise ValueError(\'`loss` should be a function in eager mode.\')\n\n  if not isinstance(loss, Future):\n    logging.warning(\'loss should be an instance of eager_utils.Future\')\n\n  with tf.GradientTape() as tape:\n    loss_value = loss()\n    total_loss_value = total_loss_fn(loss_value)\n  if variables_to_train is None:\n    variables_to_train = tape.watched_variables()\n  elif callable(variables_to_train):\n    variables_to_train = variables_to_train()\n  variables_to_train = tf.nest.flatten(variables_to_train)\n  grads = tape.gradient(total_loss_value, variables_to_train)\n  grads_and_vars = zip(grads, variables_to_train)\n\n  if transform_grads_fn:\n    grads_and_vars = transform_grads_fn(grads_and_vars)\n\n  if summarize_gradients:\n    with tf.name_scope(\'summarize_grads\'):\n      add_gradients_summaries(grads_and_vars, global_step)\n\n  if check_numerics:\n    with tf.name_scope(\'train_op\'):\n      tf.debugging.check_numerics(total_loss_value, \'Loss is inf or nan\')\n\n  optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n  return loss_value\n\n\ndef create_train_op(total_loss,\n                    optimizer,\n                    global_step=_USE_GLOBAL_STEP,\n                    update_ops=None,\n                    variables_to_train=None,\n                    transform_grads_fn=None,\n                    summarize_gradients=False,\n                    gate_gradients=tf.compat.v1.train.Optimizer.GATE_OP,\n                    aggregation_method=None,\n                    check_numerics=True):\n  """"""Creates an `Operation` that evaluates the gradients and returns the loss.\n\n  Args:\n    total_loss: A `Tensor` representing the total loss.\n    optimizer: A tf.Optimizer to use for computing the gradients.\n    global_step: A `Tensor` representing the global step variable. If left as\n      `_USE_GLOBAL_STEP`, then tf.train.get_or_create_global_step() is used.\n    update_ops: An optional list of updates to execute. If `update_ops` is\n      `None`, then the update ops are set to the contents of the\n      `tf.GraphKeys.UPDATE_OPS` collection. If `update_ops` is not `None`, but\n      it doesn\'t contain all of the update ops in `tf.GraphKeys.UPDATE_OPS`,\n      a warning will be displayed.\n    variables_to_train: an optional list of variables to train. If None, it will\n      default to all tf.trainable_variables().\n    transform_grads_fn: A function which takes a single argument, a list of\n      gradient to variable pairs (tuples), performs any requested gradient\n      updates, such as gradient clipping or multipliers, and returns the updated\n      list.\n    summarize_gradients: Whether or not add summaries for each gradient.\n    gate_gradients: How to gate the computation of gradients. See tf.Optimizer.\n    aggregation_method: Specifies the method used to combine gradient terms.\n      Valid values are defined in the class `AggregationMethod`.\n    check_numerics: Whether or not we apply check_numerics.\n\n  Returns:\n    A `Tensor` that when evaluated, computes the gradients and returns the total\n      loss value.\n  """"""\n  if global_step is _USE_GLOBAL_STEP:\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n\n  # Update ops use GraphKeys.UPDATE_OPS collection if update_ops is None.\n  global_update_ops = set(\n      tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS))\n  if update_ops is None:\n    update_ops = global_update_ops\n  else:\n    update_ops = set(update_ops)\n  if not global_update_ops.issubset(update_ops):\n    tf.compat.v1.logging.warning(\n        \'update_ops in create_train_op does not contain all the \'\n        \'update_ops in GraphKeys.UPDATE_OPS\')\n\n  # Make sure update_ops are computed before total_loss.\n  if update_ops:\n    with tf.control_dependencies(update_ops):\n      barrier = tf.no_op(name=\'update_barrier\')\n    with tf.control_dependencies([barrier]):\n      total_loss = tf.identity(total_loss)\n\n  if variables_to_train is None:\n    # Default to tf.trainable_variables()\n    variables_to_train = tf.compat.v1.trainable_variables()\n  else:\n    # Make sure that variables_to_train are in tf.trainable_variables()\n    trainable_variables = tf.compat.v1.trainable_variables()\n    for v in variables_to_train:\n      assert v.trainable or v in trainable_variables\n\n  assert variables_to_train\n\n  # Create the gradients. Note that apply_gradients adds the gradient\n  # computation to the current graph.\n  grads = optimizer.compute_gradients(\n      total_loss,\n      variables_to_train,\n      gate_gradients=gate_gradients,\n      aggregation_method=aggregation_method)\n\n  if transform_grads_fn:\n    grads = transform_grads_fn(grads)\n\n  # Summarize gradients.\n  if summarize_gradients:\n    with tf.name_scope(\'summarize_grads\'):\n      add_gradients_summaries(grads, global_step)\n\n  # Create gradient updates.\n  grad_updates = optimizer.apply_gradients(grads, global_step=global_step)\n\n  with tf.name_scope(\'train_op\'):\n    # Make sure total_loss is valid.\n    if check_numerics:\n      total_loss = tf.debugging.check_numerics(total_loss,\n                                               \'LossTensor is inf or nan\')\n\n    # Ensure the train_tensor computes grad_updates.\n    with tf.control_dependencies([grad_updates]):\n      train_op = tf.identity(total_loss, name=\'train_op\')\n\n  return train_op\n\n\ndef np_function(func=None, output_dtypes=None):\n  """"""Decorator that allow a numpy function to be used in Eager and Graph modes.\n\n  Similar to `tf.py_func` and `tf.py_function` but it doesn\'t require defining\n  the inputs or the dtypes of the outputs a priori.\n\n  In Eager mode it would convert the tf.Tensors to np.arrays before passing to\n  `func` and then convert back the outputs from np.arrays to tf.Tensors.\n\n  In Graph mode it would create different tf.py_function for each combination\n  of dtype of the inputs and cache them for reuse.\n\n  NOTE: In Graph mode: if `output_dtypes` is not provided then `func` would\n  be called with `np.ones()` to infer the output dtypes, and therefore `func`\n  should be stateless.\n\n  ```python\n  Instead of doing:\n\n  def sum(x):\n    return np.sum(x)\n  inputs = tf.constant([3, 4])\n  outputs = tf.py_function(sum, inputs, Tout=[tf.int64])\n\n  inputs = tf.constant([3., 4.])\n  outputs = tf.py_function(sum, inputs, Tout=[tf.float32])\n\n  Do:\n  @eager_utils.np_function\n  def sum(x):\n    return np.sum(x)\n\n  inputs = tf.constant([3, 4])\n  outputs = sum(inputs)  # Infers that Tout is tf.int64\n\n  inputs = tf.constant([3., 4.])\n  outputs = sum(inputs)  # Infers that Tout is tf.float32\n\n  # Output dtype is always float32 for valid input dtypes.\n  @eager_utils.np_function(output_dtypes=np.float32)\n  def mean(x):\n    return np.mean(x)\n\n  # Output dtype depends on the input dtype.\n  @eager_utils.np_function(output_dtypes=lambda x: (x, x))\n  def repeat(x):\n    return x, x\n\n  with context.graph_mode():\n    outputs = sum(tf.constant([3, 4]))\n    outputs2 = sum(tf.constant([3., 4.]))\n    sess.run(outputs) # np.array(7)\n    sess.run(outputs2) # np.array(7.)\n\n  with context.eager_mode():\n    inputs = tf.constant([3, 4])\n    outputs = sum(tf.constant([3, 4])) # tf.Tensor([7])\n    outputs = sum(tf.constant([3., 4.])) # tf.Tensor([7.])\n\n  ```\n  Args:\n    func: A numpy function, that takes numpy arrays as inputs and return numpy\n      arrays as outputs.\n    output_dtypes: Optional list of dtypes or a function that maps input dtypes\n      to output dtypes. Examples: output_dtypes=[tf.float32],\n      output_dtypes=lambda x: x (outputs have the same dtype as inputs).\n      If it is not provided in Graph mode the `func` would be called to infer\n      the output dtypes.\n  Returns:\n    A wrapped function that can be used with TF code.\n  """"""\n  def decorated(func):\n    """"""Decorated func.""""""\n    dtype_map = {}\n    def wrapper(*args, **kwargs):\n      """"""Wrapper to add nested input and outputs support.""""""\n      func_with_kwargs = functools.partial(func, **kwargs)\n      def func_flat_outputs(*args):\n        return tf.nest.flatten(func_with_kwargs(*args))\n\n      def compute_output_dtypes(*args):\n        """"""Calls the func to compute output dtypes.""""""\n        result = func(*args, **kwargs)\n        return tf.nest.map_structure(lambda x: x.dtype, result)\n\n      if tf.executing_eagerly():\n        result = func_with_kwargs(\n            *tf.nest.map_structure(lambda x: x.numpy(), args))\n        convert = lambda x: x if x is None else tf.convert_to_tensor(value=x)\n        return tf.nest.map_structure(convert, result)\n      else:\n        input_dtypes = tuple([x.dtype for x in tf.nest.flatten(args)])\n        if input_dtypes not in dtype_map:\n          if output_dtypes is None:\n            dummy_args = tf.nest.map_structure(\n                lambda x: np.ones(x.shape, x.dtype.as_numpy_dtype), args)\n            dtype_map[input_dtypes] = compute_output_dtypes(*dummy_args)\n          elif isinstance(output_dtypes, (list, tuple)):\n            # output_dtypes define the output dtypes.\n            dtype_map[input_dtypes] = output_dtypes\n          else:\n            try:\n              # See if output_dtypes define the output dtype directly.\n              tf.as_dtype(output_dtypes)\n              dtype_map[input_dtypes] = output_dtypes\n            except TypeError:\n              if callable(output_dtypes):\n                # output_dtypes is mapping from input_dtypes to output_dtypes.\n                dtype_map[input_dtypes] = output_dtypes(*input_dtypes)\n              else:\n                raise ValueError(\n                    \'output_dtypes not a list of dtypes or a callable.\')\n\n      flat_output_dtypes = tf.nest.flatten(dtype_map[input_dtypes])\n      flat_outputs = tf.py_function(func_flat_outputs,\n                                    inp=args,\n                                    Tout=flat_output_dtypes)\n      return tf.nest.pack_sequence_as(dtype_map[input_dtypes], flat_outputs)\n\n    return tf_decorator.make_decorator(func, wrapper)\n  # This code path is for the `foo = np_function(foo, ...)` use case\n  if func is not None:\n    return decorated(func)\n\n  # This code path is for the decorator\n  # @np_function(...)\n  # def foo(...):\n  return decorated\n\n\ndef dataset_iterator(dataset):\n  """"""Constructs a `Dataset` iterator.\n\n  The method used to construct the iterator is conditioned on whether Graph mode\n  is enabled. `dataset_iterator` and `get_next` are useful when we need to\n  construct an iterator and iterate through it inside a `tensorflow.function`.\n\n  Args:\n    dataset: a `tf.data.Dataset`.\n  Returns:\n    A `tf.data.Iterator` if Graph mode is enabled; a tf.data.EagerIterator if\n    in eager mode.\n  """"""\n  if tf.executing_eagerly():\n    return iter(dataset)\n  try:\n    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n  except ValueError:\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n  return iterator\n\n\ndef get_next(iterator):\n  """"""Returns the next element in a `Dataset` iterator.\n\n  The syntax used to retrieve the next item is conditioned on whether Graph mode\n  is enabled. `dataset_iterator` and `get_next` are useful when we need to\n  construct an iterator and iterate through it inside a `tensorflow.function`.\n\n  Args:\n    iterator: a `tf.data.Iterator` if in Graph mode; a `tf.data.EagerIterator`\n      if in eager mode.\n  Returns:\n    A `tf.data.Iterator` if Graph mode is enabled; a Python iterator if in eager\n    mode.\n  """"""\n  if tf.executing_eagerly():\n    return next(iterator)\n  return iterator.get_next()\n'"
tf_agents/utils/eager_utils_test.py,46,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.eager_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import test_utils\n\nfrom tensorflow.python.eager import context  # TF internal\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\ndef input_fn():\n  tf.compat.v1.set_random_seed(1)\n  inputs = tf.constant([[1, 2], [2, 3], [3, 4]], dtype=tf.float32)\n  labels = tf.constant([[0], [1], [2]])\n  return inputs, labels\n\n\nclass Network(tf.keras.layers.Layer):\n\n  def __init__(self, name=None):\n    super(Network, self).__init__(name=name)\n    self._layer = tf.keras.layers.Dense(\n        3, kernel_initializer=tf.compat.v1.initializers.ones(), name=\'logits\')\n\n  def call(self, inputs):\n    return self._layer(inputs)\n\n\nclass Model(object):\n\n  def __init__(self, name, network):\n    self._name = name\n    self._network = network\n\n  def __call__(self, inputs):\n    return self._network(inputs)\n\n  @property\n  def variables(self):\n    return self._network.variables\n\n  @property\n  def trainable_variables(self):\n    return self._network.trainable_variables\n\n  @eager_utils.future_in_eager_mode\n  def loss_fn(self, inputs, labels):\n    logits = self._network(inputs)\n    return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels, logits)\n\n\n@eager_utils.future_in_eager_mode\ndef minimize_loss(loss, optimizer):\n  return optimizer.minimize(loss)\n\n\nclass Aux(object):\n\n  def __init__(self):\n    pass\n\n  def method(self, inputs, labels, param=0):\n    assert isinstance(self, Aux), self\n    return inputs, labels, tf.convert_to_tensor(value=param)\n\n\ndef aux_function(inputs, labels, param=0):\n  return inputs, labels, tf.convert_to_tensor(value=param)\n\n\n@parameterized.named_parameters(\n    (\'.func_eager\', aux_function, context.eager_mode),\n    (\'.func_graph\', aux_function, context.graph_mode),\n    (\'.method_eager\', Aux().method, context.eager_mode),\n    (\'.method_graph\', Aux().method, context.graph_mode),\n)\nclass FutureTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testCreate(self, func_or_method, run_mode):\n    with run_mode():\n      future = eager_utils.Future(input_fn)\n      self.assertTrue(callable(future))\n      self.assertIsInstance(future, eager_utils.Future)\n      inputs, labels = future()\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n\n  def testArgsAtInit(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, inputs, labels)\n      inputs, labels, param = future()\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method)\n      inputs, labels, param = future(inputs, labels)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testArgsAtCallOverwriteKwargsInit(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, param=1)\n      inputs, labels, param = future(inputs, labels, 0)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testKWArgsAtInit(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(\n          func_or_method, inputs=inputs, labels=labels, param=1)\n      inputs, labels, param = future()\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 1)\n\n  def testKWArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method)\n      inputs, labels, param = future(inputs=inputs, labels=labels, param=1)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 1)\n\n  def testArgsAtInitKWArgsAtInit(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, inputs, labels=labels)\n      inputs, labels, param = future()\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testArgsAtInitKWArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, inputs, param=1)\n      inputs, labels, param = future(labels=labels)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 1)\n\n  def testOverwriteKWArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, param=-1)\n      inputs, labels, param = future(inputs, labels, param=1)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 1)\n\n  def testArgsatInitOverwritedKWArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, inputs, param=-1)\n      inputs, labels, param = future(labels=labels, param=1)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 1)\n\n  def testPartialArgsAtCallRaisesError(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, inputs)\n      with self.assertRaisesRegexp(TypeError, \'argument\'):\n        future(labels)\n\n  def testArgsAtInitArgsReplacedAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, labels, inputs)\n      inputs, labels, param = future(inputs, labels)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testArgsAtCallKWArgsAtInit(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method, labels=labels)\n      inputs, labels, param = future(inputs)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n  def testArgsAtCallKWArgsAtCall(self, func_or_method, run_mode):\n    with run_mode():\n      inputs, labels = input_fn()\n      future = eager_utils.Future(func_or_method)\n      inputs, labels, param = future(inputs, labels=labels)\n      self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n      self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n      self.assertEqual(self.evaluate(param), 0)\n\n\nclass FutureInEagerModeTest(test_utils.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testCreate(self):\n    decorator = eager_utils.future_in_eager_mode(input_fn)\n    self.assertTrue(callable(decorator))\n    if context.executing_eagerly():\n      self.assertTrue(isinstance(decorator(), eager_utils.Future))\n      inputs, labels = decorator()()\n    else:\n      inputs, labels = decorator()\n    self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n    self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n\n  def testDecorator(self):\n    @eager_utils.future_in_eager_mode\n    def aux_fn(inputs, labels):\n      return inputs, labels\n\n    self.assertTrue(callable(aux_fn))\n    inputs, labels = input_fn()\n    outputs = aux_fn(inputs, labels)\n\n    if context.executing_eagerly():\n      self.assertTrue(isinstance(outputs, eager_utils.Future))\n      inputs, labels = outputs.__call__()\n    else:\n      inputs, labels = outputs\n    self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n    self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n\n  def testDelayedArgs(self):\n    @eager_utils.future_in_eager_mode\n    def aux_fn(inputs, labels):\n      return inputs, labels\n\n    self.assertTrue(callable(aux_fn))\n    inputs, labels = input_fn()\n    outputs = aux_fn(inputs, labels)\n\n    if context.executing_eagerly():\n      self.assertTrue(isinstance(outputs, eager_utils.Future))\n      inputs, labels = outputs.__call__()\n    else:\n      inputs, labels = outputs\n    self.assertAllEqual(self.evaluate(inputs), [[1, 2], [2, 3], [3, 4]])\n    self.assertAllEqual(self.evaluate(labels), [[0], [1], [2]])\n\n\nclass EagerUtilsTest(test_utils.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testModel(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    loss = model.loss_fn(inputs, labels)\n    expected_loss = 1.098612\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testLossDecreasesAfterTrainStep(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    loss = model.loss_fn(inputs, labels)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n    train_step = minimize_loss(loss, optimizer)\n    initial_loss = 1.098612\n    final_loss = 1.064379\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), initial_loss)\n    self.evaluate(train_step)\n    self.assertAllClose(self.evaluate(loss), final_loss)\n\n\nclass ClipGradsTest(test_utils.TestCase):\n\n  def testClipGrads(self):\n    xs = tf.Variable(0.0)\n    grads = tf.constant(4.0)\n    gradients_to_variables = [(grads, xs)]\n    clipped_gradients_to_variables = eager_utils.clip_gradient_norms(\n        gradients_to_variables, 3.0)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAlmostEqual(4.0, self.evaluate(gradients_to_variables[0][0]))\n    self.assertAlmostEqual(3.0,\n                           self.evaluate(clipped_gradients_to_variables[0][0]))\n\n  def testClipGradsIndexedSlices(self):\n    xs = tf.Variable(0.0)\n    grads = tf.IndexedSlices(values=tf.constant(4.0),\n                             indices=tf.constant([0]),\n                             dense_shape=None)\n    gradients_to_variables = [(grads, xs)]\n    clipped_gradients_to_variables = eager_utils.clip_gradient_norms(\n        gradients_to_variables, 3.0)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAlmostEqual(\n        4.0, self.evaluate(gradients_to_variables[0][0].values))\n    self.assertAlmostEqual(\n        3.0, self.evaluate(clipped_gradients_to_variables[0][0].values))\n\n  def testClipGradsFn(self):\n    xs = tf.Variable(0.0)\n    grads = tf.constant(4.0)\n    gradients_to_variables = [(grads, xs)]\n    clipped_gradients_to_variables = eager_utils.clip_gradient_norms_fn(3.0)(\n        gradients_to_variables)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAlmostEqual(4.0, self.evaluate(gradients_to_variables[0][0]))\n    self.assertAlmostEqual(3.0,\n                           self.evaluate(clipped_gradients_to_variables[0][0]))\n\n\nclass CreateTrainOpTest(test_utils.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testLossDecreasesAfterTrainOp(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    loss = model.loss_fn(inputs, labels)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n    train_step = eager_utils.create_train_step(loss, optimizer)\n    initial_loss = 1.098612\n    final_loss = 1.064379\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(train_step), initial_loss)\n    self.assertAllClose(self.evaluate(loss), final_loss)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testCreateTrainOpWithTotalLossFn(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    loss = model.loss_fn(inputs, labels)\n    model_2 = Model(\'model_2\', Network())\n    loss_2 = model_2.loss_fn(inputs, labels)\n\n    @eager_utils.future_in_eager_mode\n    def tuple_loss(loss, loss_2):\n      return (loss() if callable(loss) else loss,\n              loss_2() if callable(loss_2) else loss_2)\n    tuple_loss_value = tuple_loss(loss, loss_2)\n\n    def first_element(tuple_value):\n      return tuple_value[0]\n\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n    loss = eager_utils.create_train_step(\n        tuple_loss_value, optimizer, total_loss_fn=first_element)\n    expected_loss = 1.098612\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    train_step_model_0, train_step_model_1 = self.evaluate(loss)\n    self.assertAllClose(train_step_model_0, expected_loss)\n    self.assertAllClose(train_step_model_1, expected_loss)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testMultipleCallsTrainStep(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    loss = model.loss_fn(inputs, labels)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n    train_step = eager_utils.create_train_step(loss, optimizer)\n    initial_loss = 1.098612\n    final_loss = 1.033917\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(train_step), initial_loss)\n    if context.executing_eagerly():\n      for _ in range(5):\n        train_step = eager_utils.create_train_step(loss, optimizer)\n      train_step = eager_utils.create_train_step(loss, optimizer)\n      self.assertAllClose(self.evaluate(train_step), final_loss)\n    else:\n      for _ in range(5):\n        self.evaluate(train_step)\n      self.assertAllClose(self.evaluate(train_step), final_loss)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testVariablesToTrain(self):\n    inputs, labels = input_fn()\n    model = Model(\'model\', Network())\n    if context.executing_eagerly():\n      variables_to_train = lambda: model.trainable_variables\n    else:\n      model(inputs)\n      variables_to_train = model.trainable_variables\n      self.assertEqual(len(variables_to_train), 2)\n    loss = model.loss_fn(inputs, labels)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1)\n    train_step = eager_utils.create_train_step(\n        loss, optimizer, variables_to_train=variables_to_train)\n    expected_loss = 1.098612\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(train_step), expected_loss)\n    self.assertEqual(len(model.trainable_variables), 2)\n\n\nclass HasSelfClsArgTest(test_utils.TestCase):\n\n  def testDirect(self):\n\n    def func():\n      pass\n\n    func2 = lambda: 0\n\n    class A(object):\n\n      def method(self):\n        pass\n\n      @classmethod\n      def class_method(cls):\n        pass\n\n      @staticmethod\n      def static_method():\n        pass\n\n    self.assertFalse(eager_utils.has_self_cls_arg(func))\n    self.assertFalse(eager_utils.has_self_cls_arg(func2))\n    self.assertFalse(eager_utils.has_self_cls_arg(A.static_method))\n\n    self.assertTrue(eager_utils.has_self_cls_arg(A.method))\n    self.assertTrue(eager_utils.has_self_cls_arg(A().method))\n    self.assertTrue(eager_utils.has_self_cls_arg(A.class_method))\n    self.assertTrue(eager_utils.has_self_cls_arg(A().class_method))\n\n    self.assertTrue(eager_utils.has_self_cls_arg(A.__dict__[\'method\']))\n    self.assertTrue(eager_utils.has_self_cls_arg(A.__dict__[\'class_method\']))\n    self.assertFalse(eager_utils.has_self_cls_arg(A.__dict__[\'static_method\']))\n\n  def testDecorator(self):\n\n    def decorator(method):\n\n      @functools.wraps(method)\n      def _decorator(*args, **kwargs):\n        method(*args, **kwargs)\n\n      return _decorator\n\n    class A(object):\n\n      @decorator\n      def method(self):\n        pass\n\n      @staticmethod\n      @decorator\n      def static_method():\n        pass\n\n      @classmethod\n      @decorator\n      def class_method(cls):\n        pass\n\n    self.assertTrue(eager_utils.has_self_cls_arg(A.method))\n    self.assertTrue(eager_utils.has_self_cls_arg(A.class_method))\n    self.assertFalse(eager_utils.has_self_cls_arg(A.static_method))\n\n\n@eager_utils.np_function\ndef meshgrid(low, high, nx=2, ny=3):\n  x = np.linspace(low, high, nx)\n  y = np.linspace(low, high, ny)\n  return np.meshgrid(x, y)\n\n\n@eager_utils.np_function(output_dtypes=np.float32)\ndef mean(x):\n  return np.mean(x)\n\n\n@eager_utils.np_function(output_dtypes=lambda x: (x, x))\ndef repeat(x):\n  return x, x\n\n\n@eager_utils.np_function(output_dtypes=lambda x, y: {\'x\': x, \'y\': y})\ndef dictionary(x, y):\n  return {\'x\': x, \'y\': y}\n\n\nclass NpFunctionTest(test_utils.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testMeshGrid(self):\n    xv, yv = meshgrid(tf.constant(0), tf.constant(1))\n    self.assertAllEqual(self.evaluate(xv), [[0., 1.], [0., 1.], [0., 1.]])\n    self.assertAllEqual(self.evaluate(yv), [[0., 0.], [.5, .5], [1., 1.]])\n    xv, yv = meshgrid(tf.constant(0.), tf.constant(1.))\n    self.assertAllEqual(self.evaluate(xv), [[0., 1.], [0., 1.], [0., 1.]])\n    self.assertAllEqual(self.evaluate(yv), [[0., 0.], [.5, .5], [1., 1.]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testMeshGridKwargs(self):\n    xv, yv = meshgrid(tf.constant(0), tf.constant(1), nx=2, ny=2)\n    self.assertAllEqual(self.evaluate(xv), [[0., 1.], [0., 1.]])\n    self.assertAllEqual(self.evaluate(yv), [[0., 0.], [1., 1.]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testVariables(self):\n    a, b = tf.Variable(0), tf.Variable(1)\n    xv, yv = meshgrid(a, b, nx=2, ny=2)\n    self.evaluate(tf.compat.v1.initializers.global_variables())\n    self.assertAllEqual(self.evaluate(xv), [[0., 1.], [0., 1.]])\n    self.assertAllEqual(self.evaluate(yv), [[0., 0.], [1., 1.]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetOutputDtypesInts2Floats(self):\n    x = tf.constant([1, 2, 3])\n    mean_x = mean(x)\n    self.assertEqual(self.evaluate(mean_x), 2.)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testGetOutputDtypesFloats2Floats(self):\n    x = tf.constant([1., 2., 3.])\n    mean_x = mean(x)\n    self.assertEqual(self.evaluate(mean_x), 2.)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testIdentityDtypes(self):\n    x = tf.constant([1])\n    self.assertAllEqual(self.evaluate(repeat(x)), ([1], [1]))\n    y = tf.constant([1.])\n    self.assertAllEqual(self.evaluate(repeat(y)), ([1.], [1.]))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testInline(self):\n    square = eager_utils.np_function(np.square)\n    x = tf.constant([1, 2, 3])\n    self.assertAllEqual([1, 4, 9], self.evaluate(square(x)))\n    y = tf.constant([1., 2., 3.])\n    self.assertAllEqual([1., 4., 9.], self.evaluate(square(y)))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testOutputDictionary(self):\n    x = tf.constant([1])\n    y = tf.constant([1.])\n    outputs = dictionary(x, y)\n    self.assertAllEqual([1], self.evaluate(outputs[\'x\']))\n    self.assertAllEqual([1.], self.evaluate(outputs[\'y\']))\n\n\n@eager_utils.np_function(output_dtypes=np.float32)\ndef np_descent(x, d, mu, n_epochs):\n  n = len(x)\n  f = 2 / n\n\n  y = np.zeros(n)\n  err = np.zeros(n)\n  w = np.zeros(2)\n  grad = np.zeros(2)\n\n  for _ in itertools.repeat(None, n_epochs):\n    np.subtract(d, y, out=err)\n    grad[:] = [f * np.sum(err), f * np.dot(err, x)]\n    w = w + mu * grad\n    y = w[0] + w[1] * x\n  return w\n\n\nclass NpDescentTest(test_utils.TestCase):\n\n  def setUp(self):\n    np.random.seed(444)\n    n = 10000\n    sigma = 0.1\n    noise = sigma * np.random.randn(n)\n    self._x = np.linspace(0, 2, n)\n    self._d = 3 + 2 * self._x + noise\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testSolve(self):\n    x, d = tf.constant(self._x), tf.constant(self._d)\n    w = np_descent(x, d, mu=0.001, n_epochs=10000)\n    self.assertAllClose([2.96, 2.03], self.evaluate(w), atol=0.01, rtol=0.01)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DatasetIteratorTest(test_utils.TestCase):\n\n  def testIteration(self):\n    data = np.arange(100)\n    ds = tf.data.Dataset.from_tensor_slices(data)\n    itr = eager_utils.dataset_iterator(ds)\n    for d in data:\n      self.assertEqual(np.array([d]),\n                       self.evaluate(eager_utils.get_next(itr)))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/example_encoding.py,38,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for easily encoding nests of numpy arrays into example protos.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport gin\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ndef get_example_encoder(spec):\n  """"""Get example encoder function for the given spec.\n\n  Given a spec, returns an example encoder function. The example encoder\n  function takes a nest of np.array feature values as input and returns a\n  TF Example proto.\n\n  Example:\n    spec = {\n        \'lidar\': array_spec.ArraySpec((900,), np.float32),\n        \'joint_positions\': {\n            \'arm\': array_spec.ArraySpec((7,), np.float32),\n            \'hand\': array_spec.BoundedArraySpec((3, 3), np.int32, -1, 1)\n        },\n    }\n\n    example_encoder = get_example_encoder(spec)\n    serialized = example_encoder({\n        \'lidar\': np.zeros((900,), np.float32),\n        \'joint_positions\': {\n            \'arm\': np.array([0.0, 1.57, 0.707, 0.2, 0.0, -1.57, 0.0],\n                            np.float32),\n            \'hand\': np.ones((3, 3), np.int32)\n        },\n    })\n\n  The returned example encoder function requires that the feature nest passed\n  has the shape and exact dtype specified in the spec. For example, it is\n  an error to pass an array with np.float64 dtype where np.float32 is expected.\n\n  Args:\n    spec: list/tuple/nest of ArraySpecs describing a single example.\n\n  Returns:\n    Function\n\n    ```python\n    encoder(features_nest of np.arrays) -> tf.train.Example\n    ```\n  """"""\n  # pylint: disable=g-complex-comprehension\n  feature_encoders = [(path, _get_feature_encoder(spec.shape, spec.dtype))\n                      for (path,\n                           spec) in nest_utils.flatten_with_joined_paths(spec)]\n\n  # pylint: enable=g-complex-comprehension\n\n  def _example_encoder(features_nest):\n    flat_features = tf.nest.flatten(features_nest)\n    feature_dict = {\n        path: feature_encoder(feature)\n        for feature, (path,\n                      feature_encoder) in zip(flat_features, feature_encoders)\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature_dict))\n\n  return _example_encoder\n\n\ndef get_example_serializer(spec):\n  """"""Returns string serializer of example protos.""""""\n  encoder = get_example_encoder(spec)\n  return lambda features_nest: encoder(features_nest).SerializeToString()\n\n\ndef get_example_decoder(example_spec, batched=False):\n  """"""Get an example decoder function for a nested spec.\n\n  Given a spec, returns an example decoder function. The decoder function parses\n  string serialized example protos into tensors according to the given spec.\n\n  Args:\n    example_spec: list/tuple/nest of ArraySpecs describing a single example.\n    batched: Boolean indicating if the decoder will receive batches of\n      serialized data.\n\n  Returns:\n    Function\n\n    ```python\n    decoder(serialized_proto: tf.tensor[string]) -> example_spec nest of tensors\n    ```\n  """"""\n  features_dict = {}\n  parsers = []\n\n  for (path, spec) in nest_utils.flatten_with_joined_paths(example_spec):\n    feature, parser = _get_feature_parser(spec.shape, spec.dtype)\n    features_dict[path] = feature\n    parsers.append((path, parser))\n\n  def _example_decoder(serialized):\n    """"""Parses string serialized example protos into tensors.""""""\n    if batched:\n      raw_features = tf.io.parse_example(\n          serialized=serialized, features=features_dict)\n\n      decoded_features = []\n\n      dtypes = [s.dtype for s in tf.nest.flatten(example_spec)]\n      for (path, parser), dtype in zip(parsers, dtypes):\n        decoded_features.append(\n            tf.map_fn(parser, raw_features[path], dtype=dtype))\n\n      return tf.nest.pack_sequence_as(example_spec, decoded_features)\n    else:\n      raw_features = tf.io.parse_single_example(\n          serialized=serialized, features=features_dict)\n      return tf.nest.pack_sequence_as(\n          example_spec,\n          [parser(raw_features[path]) for path, parser in parsers])\n\n  return _example_decoder\n\n\ndef _validate_shape(shape):\n  """"""Check that shape is a valid array shape.""""""\n  if not isinstance(shape, collections.Iterable):\n    raise TypeError(\'shape must be a tuple or other iterable object, not %s\' %\n                    type(shape).__name__)\n\n  validated_shape = []\n  for i, dim in enumerate(shape):\n    if not dim or dim != int(dim) or int(dim) <= 0:\n      raise ValueError(\n          \'Dimension %d is invalid in %s, expected positive int\' % (i, shape))\n    validated_shape.append(int(dim))\n\n  return tuple(validated_shape)\n\n\ndef _validate_dtype(dtype):\n  """"""Check that dtype is supported by tf.decode_raw.""""""\n  dtype = tf.as_dtype(dtype)\n  supported_dtypes = (tf.half, tf.float32, tf.float64, tf.uint8, tf.int8,\n                      tf.uint16, tf.int16, tf.int32, tf.int64)\n  if dtype not in supported_dtypes:\n    raise ValueError(\'%s is not supported, dtype must be one of %s\' %\n                     (dtype.name, \', \'.join(d.name for d in supported_dtypes)))\n  return dtype\n\n\ndef _check_shape_and_dtype(value, shape, dtype):\n  """"""Check that `value` has expected shape and dtype.""""""\n  value_dtype = tf.as_dtype(value.dtype.newbyteorder(\'N\'))\n  if shape != value.shape or dtype != value_dtype:\n    raise ValueError(\'Expected shape %s of %s, got: shape %s of %s\' %\n                     (shape, dtype.name, value.shape, value_dtype.name))\n\n\n@gin.configurable\ndef _get_feature_encoder(shape, dtype, compress_image=False, image_quality=95):\n  """"""Get feature encoder function for shape and dtype.\n\n  Args:\n    shape: An array shape\n    dtype: A list of dtypes.\n    compress_image: Whether to compress image. It is assumed that any uint8\n      tensor of rank 3 with shape (w,h,3) is an image.\n    image_quality: An optional int. Defaults to 95. Quality of the compression\n      from 0 to 100 (higher is better and slower).\n\n  Returns:\n    A tf.train.Feature encoder.\n  """"""\n  shape = _validate_shape(shape)\n  dtype = _validate_dtype(dtype)\n\n  if compress_image and len(shape) == 3 and shape[2] == 3 and dtype == tf.uint8:\n    if not common.has_eager_been_enabled():\n      raise ValueError(\'Only supported in TF2.x.\')\n    def _encode_to_jpeg_bytes_list(value):\n      value = tf.io.encode_jpeg(value, quality=image_quality)\n\n      return tf.train.Feature(\n          bytes_list=tf.train.BytesList(value=[value.numpy()]))\n\n    return _encode_to_jpeg_bytes_list\n\n  if dtype == tf.float32:  # Serialize float32 to FloatList.\n\n    def _encode_to_float_list(value):\n      value = np.asarray(value)\n      _check_shape_and_dtype(value, shape, dtype)\n      return tf.train.Feature(\n          float_list=tf.train.FloatList(\n              value=value.flatten(order=\'C\').tolist()))\n\n    return _encode_to_float_list\n  elif dtype == tf.int64:  # Serialize int64 to Int64List.\n\n    def _encode_to_int64_list(value):\n      value = np.asarray(value)\n      _check_shape_and_dtype(value, shape, dtype)\n      return tf.train.Feature(\n          int64_list=tf.train.Int64List(\n              value=value.flatten(order=\'C\').tolist()))\n\n    return _encode_to_int64_list\n  else:  # Serialize anything else to BytesList in little endian order.\n    le_dtype = dtype.as_numpy_dtype(0).newbyteorder(\'L\')\n\n    def _encode_to_bytes_list(value):\n      value = np.asarray(value)\n      _check_shape_and_dtype(value, shape, dtype)\n      bytes_list_value = np.require(\n          value, dtype=le_dtype, requirements=\'C\').tostring()\n      return tf.train.Feature(\n          bytes_list=tf.train.BytesList(value=[bytes_list_value]))\n\n    return _encode_to_bytes_list\n\n\n@gin.configurable\ndef _get_feature_parser(shape, dtype, compress_image=False):\n  """"""Get tf.train.Features entry and decoder function for parsing feature.\n\n  Args:\n    shape: An array shape\n    dtype: A list of dtypes.\n    compress_image: Whether to decompress image. It is assumed that any uint8\n      tensor of rank 3 with shape (w,h,3) is an image.\n      If the tensor was compressed in the encoder, it needs to be decompressed.\n\n  Returns:\n    A tuple containing tf.io.FixedLenFeature decoder and decode function.\n  """"""\n  shape = _validate_shape(shape)\n  dtype = _validate_dtype(dtype)\n\n  if compress_image and len(shape) == 3 and shape[2] == 3 and dtype == tf.uint8:\n    return (tf.io.FixedLenFeature(shape=[], dtype=tf.string), tf.io.decode_jpeg)\n\n  if dtype == tf.float32:\n    return (tf.io.FixedLenFeature(shape=shape, dtype=tf.float32), lambda x: x)\n  elif dtype == tf.int64:\n    return (tf.io.FixedLenFeature(shape=shape, dtype=tf.int64), lambda x: x)\n\n  def decode(x):\n    return tf.reshape(tf.io.decode_raw(x, dtype), shape)\n\n  return (tf.io.FixedLenFeature(shape=[], dtype=tf.string), decode)\n  # pylint: enable=g-long-lambda\n'"
tf_agents/utils/example_encoding_dataset.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for for interacting with datasets of encoded examples of TFRecords.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import example_encoding\nfrom tf_agents.utils import nest_utils\n\nfrom tensorflow.core.protobuf import struct_pb2  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n# File extension used when saving data specs to file\n_SPEC_FILE_EXTENSION = \'.spec\'\n\n\ndef encode_spec_to_file(output_path, tensor_data_spec):\n  """"""Save a tensor data spec to a tfrecord file.\n\n  Args:\n    output_path: The path to the TFRecord file which will contain the spec.\n    tensor_data_spec: Nested list/tuple or dict of TensorSpecs, describing the\n      shape of the non-batched Tensors.\n  """"""\n  spec_proto = tensor_spec.to_proto(tensor_data_spec)\n  with tf.io.TFRecordWriter(output_path) as writer:\n    writer.write(spec_proto.SerializeToString())\n\n\ndef parse_encoded_spec_from_file(input_path):\n  """"""Returns the tensor data spec stored at a path.\n\n  Args:\n    input_path: The path to the TFRecord file which contains the spec.\n\n  Returns:\n    `TensorSpec` nested structure parsed from the TFRecord file.\n  Raises:\n    IOError: File at input path does not exist.\n  """"""\n  if not tf.io.gfile.exists(input_path):\n    raise IOError(\'Could not find spec file at %s.\' % input_path)\n  dataset = tf.data.TFRecordDataset(input_path, buffer_size=1)\n  dataset_iterator = eager_utils.dataset_iterator(dataset)\n  signature_proto_string = eager_utils.get_next(dataset_iterator)\n  if tf.executing_eagerly():\n    signature_proto = struct_pb2.StructuredValue.FromString(\n        signature_proto_string.numpy())\n  else:\n    # In non-eager mode a session must be run in order to get the value\n    with tf.Session() as sess:\n      signature_proto_string_value = sess.run(signature_proto_string)\n    signature_proto = struct_pb2.StructuredValue.FromString(\n        signature_proto_string_value)\n  return tensor_spec.from_proto(signature_proto)\n\n\nclass TFRecordObserver(object):\n  """"""Observer for writing experience to TFRecord file.\n\n  To use this observer, create an instance using a trajectory spec object\n  and a dataset path:\n\n  trajectory_spec = agent.collect_data_spec\n  dataset_path = \'/tmp/my_example_dataset\'\n  tfrecord_observer = TFRecordObserver(dataset_path, trajectory_spec)\n\n  Then add it to the observers kwarg for the driver:\n\n  collect_op = MyDriver(\n    ...\n    observers=[..., tfrecord_observer],\n    num_steps=collect_steps_per_iteration).run()\n\n  *Note*: Depending on your driver you may have to do\n    `common.function(tfrecord_observer)` to handle the use of a callable with no\n    return within a `tf.group` operation.\n  """"""\n\n  def __init__(self, output_path, tensor_data_spec, py_mode=False):\n    """"""Creates observer object.\n\n    Args:\n      output_path: The path to the TFRecords file.\n      tensor_data_spec: Nested list/tuple or dict of TensorSpecs, describing the\n        shape of the non-batched Tensors.\n      py_mode: Whether the observer is being used in a py_driver.\n\n    Raises:\n      ValueError: if the tensors and specs have incompatible dimensions or\n      shapes.\n    """"""\n    self._py_mode = py_mode\n    self._array_data_spec = tensor_spec.to_nest_array_spec(tensor_data_spec)\n    self._encoder = example_encoding.get_example_serializer(\n        self._array_data_spec)\n    # Two output files: a tfrecord file and a file with the serialized spec\n    self.output_path = output_path\n    self._writer = tf.io.TFRecordWriter(self.output_path)\n    logging.info(\'Writing dataset to TFRecord at %s\', self.output_path)\n    # Save the tensor spec used to write the dataset to file\n    spec_output_path = self.output_path + _SPEC_FILE_EXTENSION\n    encode_spec_to_file(spec_output_path, tensor_data_spec)\n\n  def write(self, *data):\n    """"""Encodes and writes (to file) a batch of data.\n\n    Args:\n      *data: (unpacked) list/tuple of batched np.arrays.\n    """"""\n    if self._py_mode:\n      structured_data = data\n    else:\n      data = nest_utils.unbatch_nested_array(data)\n      structured_data = tf.nest.pack_sequence_as(self._array_data_spec, data)\n    self._writer.write(self._encoder(structured_data))\n\n  def flush(self):\n    """"""Manually flush TFRecord writer.""""""\n    self._writer.flush()\n\n  def close(self):\n    """"""Close the TFRecord writer.""""""\n    self._writer.close()\n    logging.info(\'Closing TFRecord file at %s\', self.output_path)\n\n  def __del__(self):\n    self.close()\n\n  def __call__(self, data):\n    """"""If not in py_mode Wraps write() into a TF op for eager execution.""""""\n    if self._py_mode:\n      self.write(data)\n    else:\n      flat_data = tf.nest.flatten(data)\n      tf.numpy_function(self.write, flat_data, [], name=\'encoder_observer\')\n\n\ndef load_tfrecord_dataset(dataset_files, buffer_size=1000, as_experience=False,\n                          as_trajectories=False, add_batch_dim=True):\n  """"""Loads a TFRecord dataset from file, sequencing samples as Trajectories.\n\n  Args:\n    dataset_files: List of paths to one or more datasets\n    buffer_size: (int) number of bytes in the read buffer. 0 means no buffering.\n    as_experience: (bool) Returns dataset as a pair of Trajectories. Samples\n      will be shaped as if they had been pulled from a replay buffer with\n      `num_steps=2`. These samples can be fed directly to agent\'s `train`\n      method.\n    as_trajectories: (bool) Remaps the data into trajectory objects. This should\n      be enabled when the resulting types must be trajectories as expected by\n      agents.\n    add_batch_dim: (bool) If True the data will have a batch dim of 1 to conform\n      with the expected tensor batch convention. Set to false if you want to\n      batch the data on your own.\n\n  Returns:\n    A dataset of type tf.data.Dataset. Samples follow the dataset\'s spec nested\n    structure. Samples are generated with a leading batch dim of 1\n    (or 2 if as_experience is enabled).\n  Raises:\n    IOError: One or more of the dataset files does not exist.\n  """"""\n\n  specs = []\n  for dataset_file in dataset_files:\n    spec_path = dataset_file + _SPEC_FILE_EXTENSION\n    dataset_spec = parse_encoded_spec_from_file(spec_path)\n    specs.append(dataset_spec)\n    if not all([dataset_spec == spec for spec in specs]):\n      raise IOError(\'One or more of the encoding specs do not match.\')\n  decoder = example_encoding.get_example_decoder(specs[0])\n  logging.info(\'Loading TFRecord dataset...\')\n  dataset = tf.data.TFRecordDataset(\n      dataset_files,\n      buffer_size=buffer_size,\n      num_parallel_reads=len(dataset_files))\n\n  def decode_fn(proto):\n    """"""Decodes a proto object.""""""\n    return decoder(proto)\n\n  def decode_and_batch_fn(proto):\n    """"""Decodes a proto object, and batch output tensors.""""""\n    sample = decoder(proto)\n    return nest_utils.batch_nested_tensors(sample)\n\n  if as_experience:\n    dataset = dataset.map(decode_fn).batch(2)\n  elif add_batch_dim:\n    dataset = dataset.map(decode_and_batch_fn)\n  else:\n    dataset = dataset.map(decode_fn)\n\n  if as_trajectories:\n    as_trajectories_fn = lambda sample: trajectory.Trajectory(*sample)\n    dataset = dataset.map(as_trajectories_fn)\n  return dataset\n\n'"
tf_agents/utils/example_encoding_dataset_test.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for example_encoding_dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.drivers import test_utils as driver_test_utils\nfrom tf_agents.drivers import tf_driver\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import example_encoding_dataset\nfrom tf_agents.utils import test_utils\n\nSimpleSpec = collections.namedtuple(""SimpleSpec"", (""step_type"", ""value""))\n\n\nclass TFRecordsUtilsTest(test_utils.TestCase):\n  """"""Tests for TFRecordObserver class and dataset/spec file functions.""""""\n\n  def setUp(self):\n    super(TFRecordsUtilsTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    # Create a simple mock tensor data spec for testing\n    # Spec has a batch dimmension of 1 to simulate environment samples\n    self.simple_data_spec = SimpleSpec(\n        step_type=tf.TensorSpec(shape=(1), dtype=tf.int32, name=""step_type""),\n        value=tf.TensorSpec(shape=(1, 2), dtype=tf.float64, name=""value""))\n    self.dataset_path = os.path.join(self.get_temp_dir(),\n                                     ""test_tfrecord_dataset.tfrecord"")\n\n  def test_tfrecord_observer(self):\n    tfrecord_observer = example_encoding_dataset.TFRecordObserver(\n        self.dataset_path, self.simple_data_spec)\n    # Draw a random sample from the simple spec\n    sample = tensor_spec.sample_spec_nest(\n        self.simple_data_spec, np.random.RandomState(0), outer_dims=(1,))\n    # Write to file using __call__() function\n    for _ in range(3):\n      tfrecord_observer(sample)\n    # Manually flush\n    tfrecord_observer.flush()\n    # Delete should call close() function\n    del tfrecord_observer\n\n  def test_load_tfrecord_dataset(self):\n    # Make sure an example tfrecord file exists before attempting to load\n    self.test_tfrecord_observer()\n    example_encoding_dataset.load_tfrecord_dataset([self.dataset_path],\n                                                   buffer_size=2)\n    example_encoding_dataset.load_tfrecord_dataset([self.dataset_path],\n                                                   buffer_size=2,\n                                                   as_experience=True)\n    with self.assertRaises(IOError):\n      example_encoding_dataset.load_tfrecord_dataset([""fake_file.tfrecord""])\n\n  def test_spec_to_from_file(self):\n    example_encoding_dataset.encode_spec_to_file(self.dataset_path,\n                                                 self.simple_data_spec)\n    loaded_spec = example_encoding_dataset.parse_encoded_spec_from_file(\n        self.dataset_path)\n    self.assertTupleEqual(loaded_spec, self.simple_data_spec)\n    with self.assertRaises(IOError):\n      example_encoding_dataset.parse_encoded_spec_from_file(\n          ""fake_file.tfrecord"")\n\n  def test_conflicting_specs(self):\n    # If two different specs are encountered an error should be thrown\n    self.other_data_spec = SimpleSpec(\n        step_type=tf.TensorSpec(shape=(1), dtype=tf.int32, name=""step_type""),\n        value=tf.TensorSpec(shape=(1, 5), dtype=tf.float64, name=""value""))\n    self.other_dataset_path = os.path.join(\n        self.get_temp_dir(), ""other_test_tfrecord_dataset.tfrecord"")\n    example_encoding_dataset.encode_spec_to_file(self.other_dataset_path,\n                                                 self.other_data_spec)\n    with self.assertRaises(IOError):\n      example_encoding_dataset.load_tfrecord_dataset(\n          [self.dataset_path, self.other_dataset_path])\n\n  def test_with_tf_driver(self):\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    trajectory_spec = trajectory.from_transition(tf_env.time_step_spec(),\n                                                 policy.policy_step_spec,\n                                                 tf_env.time_step_spec())\n\n    tfrecord_observer = example_encoding_dataset.TFRecordObserver(\n        self.dataset_path, trajectory_spec)\n    driver = tf_driver.TFDriver(\n        tf_env, policy, [tfrecord_observer], max_steps=10)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step = self.evaluate(tf_env.reset())\n    initial_policy_state = policy.get_initial_state(batch_size=1)\n    self.evaluate(common.function(driver.run)(time_step, initial_policy_state))\n    tfrecord_observer.flush()\n\n    dataset = example_encoding_dataset.load_tfrecord_dataset(\n        [self.dataset_path], buffer_size=2, as_trajectories=True)\n    iterator = eager_utils.dataset_iterator(dataset)\n    sample = self.evaluate(eager_utils.get_next(iterator))\n    self.assertIsInstance(sample, trajectory.Trajectory)\n\n  def test_with_py_driver(self):\n    env = driver_test_utils.PyEnvironmentMock()\n    policy = driver_test_utils.PyPolicyMock(env.time_step_spec(),\n                                            env.action_spec())\n    trajectory_spec = trajectory.from_transition(env.time_step_spec(),\n                                                 policy.policy_step_spec,\n                                                 env.time_step_spec())\n    trajectory_spec = tensor_spec.from_spec(trajectory_spec)\n\n    tfrecord_observer = example_encoding_dataset.TFRecordObserver(\n        self.dataset_path, trajectory_spec, py_mode=True)\n\n    driver = py_driver.PyDriver(env, policy, [tfrecord_observer], max_steps=10)\n    time_step = env.reset()\n    driver.run(time_step)\n    tfrecord_observer.flush()\n\n    dataset = example_encoding_dataset.load_tfrecord_dataset(\n        [self.dataset_path], buffer_size=2, as_trajectories=True)\n\n    iterator = eager_utils.dataset_iterator(dataset)\n    sample = self.evaluate(eager_utils.get_next(iterator))\n    self.assertIsInstance(sample, trajectory.Trajectory)\n\n  def test_with_dynamic_step_driver(self):\n    env = driver_test_utils.PyEnvironmentMock()\n    tf_env = tf_py_environment.TFPyEnvironment(env)\n    policy = driver_test_utils.TFPolicyMock(tf_env.time_step_spec(),\n                                            tf_env.action_spec())\n\n    trajectory_spec = trajectory.from_transition(tf_env.time_step_spec(),\n                                                 policy.policy_step_spec,\n                                                 tf_env.time_step_spec())\n\n    tfrecord_observer = example_encoding_dataset.TFRecordObserver(\n        self.dataset_path, trajectory_spec)\n    driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        policy,\n        observers=[common.function(tfrecord_observer)],\n        num_steps=10)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step = self.evaluate(tf_env.reset())\n    initial_policy_state = policy.get_initial_state(batch_size=1)\n    self.evaluate(common.function(driver.run)(time_step, initial_policy_state))\n    tfrecord_observer.flush()\n\n    dataset = example_encoding_dataset.load_tfrecord_dataset(\n        [self.dataset_path], buffer_size=2, as_trajectories=True)\n    iterator = eager_utils.dataset_iterator(dataset)\n    sample = self.evaluate(eager_utils.get_next(iterator))\n    self.assertIsInstance(sample, trajectory.Trajectory)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/utils/example_encoding_test.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for example_encoding.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.specs import array_spec\nfrom tf_agents.utils import common\nfrom tf_agents.utils import example_encoding\n\n\nTYPE_PARAMETERS = (\n    (""np.uint8"", np.uint8),\n    (""np.uint16"", np.uint16),\n    (""np.int8"", np.int8),\n    (""np.int16"", np.int16),\n    (""np.int32"", np.int32),\n    (""np.int64"", np.int64),\n    (""np.float16"", np.float16),\n    (""np.float32"", np.float32),\n    (""np.float64"", np.float64),\n    (""python float"", float),\n    (""python int"", int),\n)\n\n\ndef example_nested_spec(dtype):\n  """"""Return an example nested array spec.""""""\n  low = -10\n  high = 10\n  if dtype in (np.uint8, np.uint16):\n    low += -low\n  return {\n      ""array_spec_1"":\n          array_spec.ArraySpec((2, 3), dtype),\n      ""bounded_spec_1"":\n          array_spec.BoundedArraySpec((2, 3), dtype, low, high),\n      ""empty_shape"":\n          array_spec.BoundedArraySpec((), dtype, low, high),\n      ""dict_spec"": {\n          ""array_spec_2"":\n              array_spec.ArraySpec((2, 3), dtype),\n          ""bounded_spec_2"":\n              array_spec.BoundedArraySpec((2, 3), dtype, low, high)\n      },\n      ""tuple_spec"": (\n          array_spec.ArraySpec((2, 3), dtype),\n          array_spec.BoundedArraySpec((2, 3), dtype, low, high),\n      ),\n      ""list_spec"": [\n          array_spec.ArraySpec((2, 3), dtype),\n          (array_spec.ArraySpec((2, 3), dtype),\n           array_spec.BoundedArraySpec((2, 3), dtype, low, high)),\n      ],\n  }\n\n\nclass NestExampleEncodeUtilsTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(*TYPE_PARAMETERS)\n  def test_serialize_deserialize(self, dtype):\n    spec = example_nested_spec(dtype)\n    serializer = example_encoding.get_example_serializer(spec)\n    decoder = example_encoding.get_example_decoder(spec)\n\n    sample = array_spec.sample_spec_nest(spec, np.random.RandomState(0))\n    example_proto = serializer(sample)\n\n    recovered = self.evaluate(decoder(example_proto))\n    tf.nest.map_structure(np.testing.assert_almost_equal, sample, recovered)\n\n  def test_endian_encodings(self):\n    spec = {\n        ""a"": array_spec.ArraySpec((2,), np.int16),\n        ""b"": array_spec.ArraySpec((2,), np.int32),\n        ""c"": array_spec.ArraySpec((2,), np.float32),\n    }\n\n    serializer = example_encoding.get_example_serializer(spec)\n    decoder = example_encoding.get_example_decoder(spec)\n\n    # Little endian encoding.\n    le_sample = {\n        ""a"": np.array([100, 25000]).astype(""<i2""),\n        ""b"": np.array([-5, 80000000]).astype(""<i4""),\n        ""c"": np.array([12.5, np.pi]).astype(""<f4"")\n    }\n\n    example_proto = serializer(le_sample)\n    recovered = self.evaluate(decoder(example_proto))\n    tf.nest.map_structure(np.testing.assert_almost_equal, le_sample, recovered)\n\n    # Big endian encoding.\n    be_sample = {\n        ""a"": np.array([100, 25000]).astype("">i2""),\n        ""b"": np.array([-5, 80000000]).astype("">i4""),\n        ""c"": np.array([12.5, np.pi]).astype("">f4"")\n    }\n\n    example_proto = serializer(be_sample)\n    recovered = self.evaluate(decoder(example_proto))\n    tf.nest.map_structure(np.testing.assert_almost_equal, be_sample, recovered)\n\n  def test_shape_validation(self):\n    with self.assertRaisesRegexp(ValueError, ""is invalid""):\n      example_encoding._validate_shape([1, 2, 3, -1])\n\n    with self.assertRaisesRegexp(ValueError, ""is invalid""):\n      example_encoding._validate_shape([1, None, 3])\n\n    with self.assertRaisesRegexp(ValueError, ""is invalid""):\n      example_encoding._validate_shape([1, 2.3, 3])\n\n  def test_compress_image(self):\n    if not common.has_eager_been_enabled():\n      self.skipTest(""Image compression only supported in TF2.x"")\n\n    gin.parse_config_files_and_bindings([], """"""\n    _get_feature_encoder.compress_image=True\n    _get_feature_parser.compress_image=True\n    """""")\n    spec = {\n        ""image"": array_spec.ArraySpec((128, 128, 3), np.uint8)\n    }\n    serializer = example_encoding.get_example_serializer(spec)\n    decoder = example_encoding.get_example_decoder(spec)\n\n    sample = {\n        ""image"": 128 * np.ones([128, 128, 3], dtype=np.uint8)\n    }\n    example_proto = serializer(sample)\n\n    recovered = self.evaluate(decoder(example_proto))\n    tf.nest.map_structure(np.testing.assert_almost_equal, sample, recovered)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/utils/nest_utils.py,81,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for handling nested tensors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nimport numbers\n\nfrom absl import logging\nimport numpy as np\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import composite\nimport wrapt\n\n\n# TODO(b/128613858): Update to a public facing API.\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ntry:\n  # Python 3.3 and above.\n  collections_abc = collections.abc\nexcept AttributeError:\n  collections_abc = collections\n\n\nflatten_with_tuple_paths = nest.flatten_with_tuple_paths\nmap_structure_with_paths = nest.map_structure_with_paths\n\n\nclass _Dot(object):\n  """"""An object whose representation is a simple \'.\'.""""""\n\n  def __repr__(self):\n    return \'.\'\n\n  def __str__(self):\n    return \'.\'\n\n\n_DOT = _Dot()\n\n\ndef assert_same_structure(nest1,\n                          nest2,\n                          check_types=True,\n                          expand_composites=False,\n                          message=None):\n  """"""Same as tf.nest.assert_same_structure but with cleaner error messages.\n\n  Args:\n    nest1: an arbitrarily nested structure.\n    nest2: an arbitrarily nested structure.\n    check_types: if `True` (default) types of sequences are checked as well,\n      including the keys of dictionaries. If set to `False`, for example a list\n      and a tuple of objects will look the same if they have the same size. Note\n      that namedtuples with identical name and fields are always considered to\n      have the same shallow structure. Two types will also be considered the\n      same if they are both list subtypes (which allows ""list"" and\n      ""_ListWrapper"" from trackable dependency tracking to compare equal).\n    expand_composites: If true, then composite tensors such as `tf.SparseTensor`\n      and `tf.RaggedTensor` are expanded into their component tensors.\n    message: Optional error message to provide in case of failure.\n\n  Raises:\n    ValueError: If the two structures do not have the same number of elements\n      or if the two structures are not nested in the same way.\n    TypeError: If the two structures differ in the type of sequence in any\n      of their substructures. Only possible if `check_types is True`.\n  """"""\n  if not isinstance(check_types, bool):\n    raise TypeError(\n        \'check_types must be a bool but saw: \\\'{}\\\'\'.format(check_types))\n  if not isinstance(expand_composites, bool):\n    raise TypeError(\'expand_composites must be a bool but saw: \\\'{}\\\'\'.format(\n        expand_composites))\n  message = message or \'The two structures do not match\'\n  exception = None\n  try:\n    return tf.nest.assert_same_structure(\n        nest1,\n        nest2,\n        check_types=check_types,\n        expand_composites=expand_composites)\n  except (TypeError, ValueError) as e:\n    exception = type(e)\n\n  if exception:\n    str1 = tf.nest.map_structure(\n        lambda _: _DOT, nest1, expand_composites=expand_composites)\n    str2 = tf.nest.map_structure(\n        lambda _: _DOT, nest2, expand_composites=expand_composites)\n    raise exception(\'{}:\\n  {}\\nvs.\\n  {}\'.format(message, str1, str2))\n\n\ndef flatten_with_joined_paths(structure, expand_composites=False):\n  flattened = flatten_with_tuple_paths(\n      structure, expand_composites=expand_composites)\n\n  def stringify_and_join(path_elements):\n    return \'/\'.join(str(path_element) for path_element in path_elements)\n\n  return [(stringify_and_join(path), value) for (path, value) in flattened]\n\n\ndef fast_map_structure_flatten(func, structure, *flat_structure, **kwargs):\n  expand_composites = kwargs.get(\'expand_composites\', False)\n  entries = zip(*flat_structure)\n  return tf.nest.pack_sequence_as(\n      structure, [func(*x) for x in entries],\n      expand_composites=expand_composites)\n\n\ndef fast_map_structure(func, *structure, **kwargs):\n  expand_composites = kwargs.get(\'expand_composites\', False)\n  flat_structure = [\n      tf.nest.flatten(s, expand_composites=expand_composites) for s in structure\n  ]\n  entries = zip(*flat_structure)\n\n  return tf.nest.pack_sequence_as(\n      structure[0], [func(*x) for x in entries],\n      expand_composites=expand_composites)\n\n\ndef has_tensors(*x):\n  return np.any(\n      [tf.is_tensor(t) for t in tf.nest.flatten(x, expand_composites=True)])\n\n\ndef _is_namedtuple(x):\n  return (isinstance(x, tuple)\n          and isinstance(getattr(x, \'_fields\', None), collections_abc.Sequence))\n\n\ndef _is_attrs(x):\n  return getattr(type(x), \'__attrs_attrs__\', None) is not None\n\n\ndef _attr_items(x):\n  attrs = getattr(type(x), \'__attrs_attrs__\')\n  attr_names = [a.name for a in attrs]\n  return [(attr_name, getattr(x, attr_name)) for attr_name in attr_names]\n\n\ndef prune_extra_keys(narrow, wide):\n  """"""Recursively prunes keys from `wide` if they don\'t appear in `narrow`.\n\n  Often used as preprocessing prior to calling `tf.nest.flatten`\n  or `tf.nest.map_structure`.\n\n  This function is more forgiving than the ones in `nest`; if two substructures\'\n  types or structures don\'t agree, we consider it invalid and `prune_extra_keys`\n  will return the `wide` substructure as is.  Typically, additional checking is\n  needed: you will also want to use\n  `nest.assert_same_structure(narrow, prune_extra_keys(narrow, wide))`\n  to ensure the result of pruning is still a correct structure.\n\n  Examples:\n  ```python\n  wide = [{""a"": ""a"", ""b"": ""b""}]\n  # Narrows \'wide\'\n  assert prune_extra_keys([{""a"": 1}], wide) == [{""a"": ""a""}]\n  # \'wide\' lacks ""c"", is considered invalid.\n  assert prune_extra_keys([{""c"": 1}], wide) == wide\n  # \'wide\' contains a different type from \'narrow\', is considered invalid\n  assert prune_extra_keys(""scalar"", wide) == wide\n  # \'wide\' substructure for key ""d"" does not match the one in \'narrow\' and\n  # therefore is returned unmodified.\n  assert (prune_extra_keys({""a"": {""b"": 1}, ""d"": None},\n                           {""a"": {""b"": ""b"", ""c"": ""c""}, ""d"": [1, 2]})\n          == {""a"": {""b"": ""b""}, ""d"": [1, 2]})\n  ```\n\n  Args:\n    narrow: A nested structure.\n    wide: A nested structure that may contain dicts with more fields than\n      `narrow`.\n\n  Returns:\n    A structure with the same nested substructures as `wide`, but with\n    dicts whose entries are limited to the keys found in the associated\n    substructures of `narrow`.\n\n    In case of substructure or size mismatches, the returned substructures\n    will be returned as is.  Note that ObjectProxy-wrapped objects are\n    considered equivalent to their non-ObjectProxy types.\n  """"""\n  if isinstance(wide, wrapt.ObjectProxy):\n    return type(wide)(prune_extra_keys(narrow, wide.__wrapped__))\n\n  narrow_raw = (narrow.__wrapped__ if isinstance(narrow, wrapt.ObjectProxy)\n                else narrow)\n  wide_raw = (wide.__wrapped__ if isinstance(wide, wrapt.ObjectProxy) else wide)\n\n  if ((type(narrow_raw) != type(wide_raw))  # pylint: disable=unidiomatic-typecheck\n      and not (isinstance(narrow_raw, list) and isinstance(wide_raw, list))\n      and not (isinstance(narrow_raw, collections_abc.Mapping)\n               and isinstance(wide_raw, collections_abc.Mapping))):\n    # We return early if the types are different; but we make some exceptions:\n    #  list subtypes are considered the same (e.g. ListWrapper and list())\n    #  Mapping subtypes are considered the same (e.g. DictWrapper and dict())\n    #  (TupleWrapper subtypes are handled by unwrapping ObjectProxy above)\n    return wide\n\n  if isinstance(narrow, collections_abc.Mapping):\n    if len(narrow) > len(wide):\n      # wide lacks a required key from narrow; return early.\n      return wide\n\n    narrow_keys = set(narrow.keys())\n    wide_keys = set(wide.keys())\n    if not wide_keys.issuperset(narrow_keys):\n      # wide lacks a required key from narrow; return early.\n      return wide\n    ordered_items = [\n        (k, prune_extra_keys(v, wide[k]))\n        for k, v in narrow.items()]\n    if isinstance(wide, collections.defaultdict):\n      subset = type(wide)(wide.default_factory, ordered_items)\n    else:\n      subset = type(wide)(ordered_items)\n    return subset\n\n  if nest.is_sequence(narrow):\n    if _is_attrs(wide):\n      items = [prune_extra_keys(n, w)\n               for n, w in zip(_attr_items(narrow), _attr_items(wide))]\n      return type(wide)(*items)\n\n    # Not an attrs, so can treat as lists or tuples from here on.\n    if len(narrow) != len(wide):\n      # wide\'s size is different than narrow; return early.\n      return wide\n\n    items = [prune_extra_keys(n, w) for n, w in zip(narrow, wide)]\n    if _is_namedtuple(wide):\n      return type(wide)(*items)\n    elif _is_attrs(wide):\n      return type(wide)\n    return type(wide)(items)\n\n  # narrow is a leaf, just return wide\n  return wide\n\n\ndef matching_dtypes_and_inner_shapes(tensors, specs, allow_extra_fields=False):\n  """"""Returns `True` if tensors and specs have matching dtypes and inner shapes.\n\n  Args:\n    tensors: A nest of tensor objects.\n    specs: A nest of `tf.TypeSpec` objects.\n    allow_extra_fields: If `True`, then `tensors` may contain more keys\n      or list fields than strictly required by `specs`.\n\n  Returns:\n    A python `bool`.\n  """"""\n  if allow_extra_fields:\n    tensors = prune_extra_keys(specs, tensors)\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n\n  flat_tensors = nest.flatten(tensors)\n  flat_specs = tf.nest.flatten(specs)\n\n  tensor_shapes = [t.shape for t in flat_tensors]\n  tensor_dtypes = [t.dtype for t in flat_tensors]\n  spec_shapes = [spec_shape(s) for s in flat_specs]\n  spec_dtypes = [t.dtype for t in flat_specs]\n\n  if any(s_dtype != t_dtype\n         for s_dtype, t_dtype in zip(spec_dtypes, tensor_dtypes)):\n    return False\n\n  for s_shape, t_shape in zip(spec_shapes, tensor_shapes):\n    if s_shape.ndims is None or t_shape.ndims is None:\n      continue\n    if s_shape.ndims > t_shape.ndims:\n      return False\n    if not s_shape.is_compatible_with(t_shape[-s_shape.ndims:]):\n      return False\n\n  return True\n\n\ndef is_batched_nested_tensors(\n    tensors,\n    specs,\n    num_outer_dims=1,\n    allow_extra_fields=False):\n  """"""Compares tensors to specs to determine if all tensors are batched or not.\n\n  For each tensor, it checks the dimensions and dtypes with respect to specs.\n\n  Returns `True` if all tensors are batched and `False` if all tensors are\n  unbatched.\n\n  Raises a `ValueError` if the shapes are incompatible or a mix of batched and\n  unbatched tensors are provided.\n\n  Raises a `TypeError` if tensors\' dtypes do not match specs.\n\n  Args:\n    tensors: Nested list/tuple/dict of Tensors.\n    specs: Nested list/tuple/dict of Tensors or CompositeTensors describing the\n      shape of unbatched tensors.\n    num_outer_dims: The integer number of dimensions that are considered batch\n      dimensions.  Default 1.\n    allow_extra_fields: If `True`, then `tensors` may have extra\n      subfields which are not in specs.  In this case, the extra subfields\n      will not be checked.  For example:\n\n      ```python\n      tensors = {""a"": tf.zeros((3, 4), dtype=tf.float32),\n                 ""b"": tf.zeros((5, 6), dtype=tf.float32)}\n      specs = {""a"": tf.TensorSpec(shape=(4,), dtype=tf.float32)}\n      assert is_batched_nested_tensors(tensors, specs, allow_extra_fields=True)\n      ```\n\n      The above example would raise a ValueError if `allow_extra_fields`\n      was False.\n\n  Returns:\n    True if all Tensors are batched and False if all Tensors are unbatched.\n\n  Raises:\n    ValueError: If\n      1. Any of the tensors or specs have shapes with ndims == None, or\n      2. The shape of Tensors are not compatible with specs, or\n      3. A mix of batched and unbatched tensors are provided.\n      4. The tensors are batched but have an incorrect number of outer dims.\n    TypeError: If `dtypes` between tensors and specs are not compatible.\n  """"""\n  if allow_extra_fields:\n    tensors = prune_extra_keys(specs, tensors)\n\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n  flat_tensors = nest.flatten(tensors)\n  flat_specs = tf.nest.flatten(specs)\n\n  tensor_shapes = [t.shape for t in flat_tensors]\n  tensor_dtypes = [t.dtype for t in flat_tensors]\n  spec_shapes = [spec_shape(s) for s in flat_specs]\n  spec_dtypes = [t.dtype for t in flat_specs]\n\n  if any(s_shape.rank is None for s_shape in spec_shapes):\n    raise ValueError(\'All specs should have ndims defined.  Saw shapes: %s\' %\n                     (tf.nest.pack_sequence_as(specs, spec_shapes),))\n\n  if any(t_shape.rank is None for t_shape in tensor_shapes):\n    raise ValueError(\'All tensors should have ndims defined.  Saw shapes: %s\' %\n                     (tf.nest.pack_sequence_as(specs, tensor_shapes),))\n\n  if any(s_dtype != t_dtype\n         for s_dtype, t_dtype in zip(spec_dtypes, tensor_dtypes)):\n    raise TypeError(\'Tensor dtypes do not match spec dtypes:\\n{}\\nvs.\\n{}\'\n                    .format(tf.nest.pack_sequence_as(specs, tensor_dtypes),\n                            tf.nest.pack_sequence_as(specs, spec_dtypes)))\n  is_unbatched = [\n      s_shape.is_compatible_with(t_shape)\n      for s_shape, t_shape in zip(spec_shapes, tensor_shapes)\n  ]\n\n  if all(is_unbatched):\n    return False\n\n  tensor_ndims_discrepancy = [\n      t_shape.rank - s_shape.rank\n      for s_shape, t_shape in zip(spec_shapes, tensor_shapes)\n  ]\n\n  tensor_matches_spec = [\n      s_shape.is_compatible_with(t_shape[discrepancy:])\n      for discrepancy, s_shape, t_shape in zip(\n          tensor_ndims_discrepancy, spec_shapes, tensor_shapes)\n  ]\n\n  # Check if all tensors match and have correct number of outer_dims.\n  is_batched = (\n      all(discrepancy == num_outer_dims\n          for discrepancy in tensor_ndims_discrepancy) and\n      all(tensor_matches_spec))\n\n  if is_batched:\n    return True\n\n  # Check if tensors match but have incorrect number of batch dimensions.\n  if all(\n      discrepancy == tensor_ndims_discrepancy[0]\n      for discrepancy in tensor_ndims_discrepancy) and all(tensor_matches_spec):\n    return False\n\n  raise ValueError(\n      \'Received a mix of batched and unbatched Tensors, or Tensors\'\n      \' are not compatible with Specs.  num_outer_dims: %d.\\n\'\n      \'Saw tensor_shapes:\\n   %s\\n\'\n      \'And spec_shapes:\\n   %s\' %\n      (num_outer_dims, tf.nest.pack_sequence_as(specs, tensor_shapes),\n       tf.nest.pack_sequence_as(specs, spec_shapes)))\n\n\ndef spec_shape(t):\n  if isinstance(t, tf.SparseTensor):\n    rank = tf.dimension_value(t.dense_shape.shape[0])\n    return tf.TensorShape([None] * rank)\n  else:\n    return t.shape\n\n\ndef batch_nested_tensors(tensors, specs=None):\n  """"""Add batch dimension if needed to nested tensors while checking their specs.\n\n  If specs is None, a batch dimension is added to each tensor.\n  If specs are provided, each tensor is compared to the corresponding spec,\n  and a batch dimension is added only if the tensor doesn\'t already have it.\n\n  For each tensor, it checks the dimensions with respect to specs, and adds an\n  extra batch dimension if it doesn\'t already have it.\n\n  Args:\n    tensors: Nested list/tuple or dict of Tensors.\n    specs: Nested list/tuple or dict of TensorSpecs, describing the shape of the\n      non-batched Tensors.\n\n  Returns:\n    A nested batched version of each tensor.\n  Raises:\n    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n  """"""\n  if specs is None:\n    return tf.nest.map_structure(lambda x: composite.expand_dims(x, 0), tensors)\n\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n\n  flat_tensors = tf.nest.flatten(tensors)\n  flat_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n  batched_tensors = []\n\n  tensor_rank = lambda tensor: tensor.shape.rank\n  for tensor, shape in zip(flat_tensors, flat_shapes):\n    if tensor_rank(tensor) == shape.rank:\n      tensor.shape.assert_is_compatible_with(shape)\n      tensor = composite.expand_dims(tensor, 0)\n    elif tensor_rank(tensor) == shape.rank + 1:\n      tensor.shape[1:].assert_is_compatible_with(shape)\n    else:\n      raise ValueError(\'Tensor does not have the correct dimensions. \'\n                       \'tensor.shape {} expected shape {}\'.format(\n                           tensor.shape, shape))\n    batched_tensors.append(tensor)\n  return tf.nest.pack_sequence_as(tensors, batched_tensors)\n\n\ndef _flatten_and_check_shape_nested_tensors(tensors, specs, num_outer_dims=1):\n  """"""Flatten nested tensors and check their shape for use in other functions.""""""\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n  flat_tensors = tf.nest.flatten(tensors)\n  flat_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n  for tensor, shape in zip(flat_tensors, flat_shapes):\n    if tensor.shape.rank == shape.rank:\n      tensor.shape.assert_is_compatible_with(shape)\n    elif tensor.shape.rank == shape.rank + num_outer_dims:\n      tensor.shape[num_outer_dims:].assert_is_compatible_with(shape)\n    else:\n      raise ValueError(\'Tensor does not have the correct dimensions. \'\n                       \'tensor.shape {} expected shape {}\'.format(\n                           tensor.shape, [None] + shape.as_list()))\n  return flat_tensors, flat_shapes\n\n\ndef flatten_and_check_shape_nested_specs(specs, reference_specs):\n  """"""Flatten nested specs and check their shape for use in other functions.""""""\n  try:\n    flat_specs, flat_shapes = _flatten_and_check_shape_nested_tensors(\n        specs, reference_specs, num_outer_dims=0)\n  except ValueError:\n    raise ValueError(\'specs must be compatible with reference_specs\'\n                     \'; instead got specs=%s, reference_specs=%s\' %\n                     (specs, reference_specs))\n  return flat_specs, flat_shapes\n\n\ndef unbatch_nested_tensors(tensors, specs=None):\n  """"""Remove the batch dimension if needed from nested tensors using their specs.\n\n  If specs is None, the first dimension of each tensor will be removed.\n  If specs are provided, each tensor is compared to the corresponding spec,\n  and the first dimension is removed only if the tensor was batched.\n\n  Args:\n    tensors: Nested list/tuple or dict of batched Tensors.\n    specs: Nested list/tuple or dict of TensorSpecs, describing the shape of the\n      non-batched Tensors.\n\n  Returns:\n    A nested non-batched version of each tensor.\n  Raises:\n    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n  """"""\n  if specs is None:\n    return tf.nest.map_structure(lambda x: composite.squeeze(x, 0), tensors)\n\n  unbatched_tensors = []\n  flat_tensors, flat_shapes = _flatten_and_check_shape_nested_tensors(\n      tensors, specs)\n  for tensor, shape in zip(flat_tensors, flat_shapes):\n    if tensor.shape.rank == shape.rank + 1:\n      tensor = composite.squeeze(tensor, 0)\n    unbatched_tensors.append(tensor)\n  return tf.nest.pack_sequence_as(tensors, unbatched_tensors)\n\n\ndef split_nested_tensors(tensors, specs, num_or_size_splits):\n  """"""Split batched nested tensors, on batch dim (outer dim), into a list.\n\n  Args:\n    tensors: Nested list/tuple or dict of batched Tensors.\n    specs: Nested list/tuple or dict of TensorSpecs, describing the shape of the\n      non-batched Tensors.\n    num_or_size_splits: Same as argument for tf.split. Either a python integer\n      indicating the number of splits along batch_dim or a list of integer\n      Tensors containing the sizes of each output tensor along batch_dim. If a\n      scalar then it must evenly divide value.shape[axis]; otherwise the sum of\n      sizes along the split dimension must match that of the value. For\n      `SparseTensor` inputs, `num_or_size_splits` must be the scalar `num_split`\n      (see documentation of `tf.sparse.split` for more details).\n\n  Returns:\n    A list of nested non-batched version of each tensor, where each list item\n      corresponds to one batch item.\n  Raises:\n    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n    ValueError: if a non-scalar is passed and there are SparseTensors in the\n      structure.\n  """"""\n  split_tensor_lists = []\n  flat_tensors, flat_shapes = _flatten_and_check_shape_nested_tensors(\n      tensors, specs)\n  for tensor, shape in zip(flat_tensors, flat_shapes):\n    if tensor.shape.rank == shape.rank:\n      raise ValueError(\'Can only split tensors with a batch dimension.\')\n    if tensor.shape.rank == shape.rank + 1:\n      if isinstance(tensor, tf.SparseTensor):\n        if not isinstance(num_or_size_splits, numbers.Number):\n          raise ValueError(\n              \'Saw a SparseTensor, for which num_or_size_splits must be a \'\n              \'scalar.  But it is not: {}\'.format(num_or_size_splits))\n        split_tensors = tf.sparse.split(\n            sp_input=tensor, num_split=num_or_size_splits, axis=0)\n      else:\n        split_tensors = tf.split(tensor, num_or_size_splits)\n    split_tensor_lists.append(split_tensors)\n  split_tensors_zipped = zip(*split_tensor_lists)\n  return [\n      tf.nest.pack_sequence_as(tensors, zipped)\n      for zipped in split_tensors_zipped\n  ]\n\n\ndef unstack_nested_tensors(tensors, specs):\n  """"""Make list of unstacked nested tensors.\n\n  Args:\n    tensors: Nested tensors whose first dimension is to be unstacked.\n    specs: Tensor specs for tensors.\n\n  Returns:\n    A list of the unstacked nested tensors.\n  Raises:\n    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n  """"""\n  unstacked_tensor_lists = []\n  flat_tensors, flat_shapes = _flatten_and_check_shape_nested_tensors(\n      tensors, specs)\n  for tensor, shape in zip(flat_tensors, flat_shapes):\n    if tensor.shape.rank == shape.rank:\n      raise ValueError(\'Can only unstack tensors with a batch dimension.\')\n    if tensor.shape.rank == shape.rank + 1:\n      unstacked_tensors = tf.unstack(tensor)\n    unstacked_tensor_lists.append(unstacked_tensors)\n  unstacked_tensors_zipped = zip(*unstacked_tensor_lists)\n  return [\n      tf.nest.pack_sequence_as(tensors, zipped)\n      for zipped in unstacked_tensors_zipped\n  ]\n\n\ndef stack_nested_tensors(tensors, axis=0):\n  """"""Stacks a list of nested tensors along the dimension specified.\n\n  Args:\n    tensors: A list of nested tensors to be stacked.\n    axis: the axis along which the stack operation is applied.\n\n  Returns:\n    A stacked nested tensor.\n  """"""\n  return tf.nest.map_structure(lambda *tensors: tf.stack(tensors, axis=axis),\n                               *tensors)\n\n\ndef flatten_multi_batched_nested_tensors(tensors, specs):\n  """"""Reshape tensors to contain only one batch dimension.\n\n  For each tensor, it checks the number of extra dimensions beyond those in\n  the spec, and reshapes tensor to have only one batch dimension.\n  NOTE: Each tensor\'s batch dimensions must be the same.\n\n  Args:\n    tensors: Nested list/tuple or dict of batched Tensors or SparseTensors.\n    specs: Nested list/tuple or dict of TensorSpecs, describing the shape of the\n      non-batched Tensors.\n\n  Returns:\n    A nested version of each tensor with a single batch dimension.\n    A list of the batch dimensions which were flattened.\n  Raises:\n    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n  """"""\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n  flat_tensors = tf.nest.flatten(tensors)\n  flat_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n  out_tensors = []\n  batch_dims = []\n  for i, (tensor, shape) in enumerate(zip(flat_tensors, flat_shapes)):\n    if i == 0:  # Set batch_dims based on first tensor.\n      batch_dims = tensor.shape[:tensor.shape.rank - shape.rank]\n      if batch_dims.is_fully_defined():\n        batch_dims = batch_dims.as_list()\n        batch_prod = np.prod(batch_dims)\n        batch_dims = tf.constant(batch_dims, dtype=tf.int64)\n      else:\n        batch_dims = tf.shape(tensor)[:tensor.shape.rank - shape.rank]\n        batch_prod = tf.reduce_prod(batch_dims)\n    reshaped_dims = [batch_prod] + shape.as_list()\n    out_tensors.append(composite.reshape(tensor, reshaped_dims))\n  return tf.nest.pack_sequence_as(tensors, out_tensors), batch_dims\n\n\ndef get_outer_shape(nested_tensor, spec):\n  """"""Runtime batch dims of tensor\'s batch dimension `dim`.""""""\n  assert_same_structure(\n      nested_tensor,\n      spec,\n      message=\'Tensors and specs do not have matching structures\')\n  first_tensor = tf.nest.flatten(nested_tensor)[0]\n  first_spec = tf.nest.flatten(spec)[0]\n\n  # Check tensors have same batch shape.\n  num_outer_dims = (len(first_tensor.shape) - len(first_spec.shape))\n  if not is_batched_nested_tensors(\n      nested_tensor, spec, num_outer_dims=num_outer_dims):\n    return []\n\n  return tf.shape(input=first_tensor)[:num_outer_dims]\n\n\ndef get_outer_rank(tensors, specs):\n  """"""Compares tensors to specs to determine the number of batch dimensions.\n\n  For each tensor, it checks the dimensions with respect to specs and\n  returns the number of batch dimensions if all nested tensors and\n  specs agree with each other.\n\n  Args:\n    tensors: Nested list/tuple/dict of Tensors or SparseTensors.\n    specs: Nested list/tuple/dict of TensorSpecs, describing the shape of\n      unbatched tensors.\n\n  Returns:\n    The number of outer dimensions for all Tensors (zero if all are\n      unbatched or empty).\n  Raises:\n    ValueError: If\n      1. Any of the tensors or specs have shapes with ndims == None, or\n      2. The shape of Tensors are not compatible with specs, or\n      3. A mix of batched and unbatched tensors are provided.\n      4. The tensors are batched but have an incorrect number of outer dims.\n  """"""\n  assert_same_structure(\n      tensors,\n      specs,\n      message=\'Tensors and specs do not have matching structures\')\n  tensor_shapes = [t.shape for t in tf.nest.flatten(tensors)]\n  spec_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n\n  if any(s_shape.rank is None for s_shape in spec_shapes):\n    raise ValueError(\'All specs should have ndims defined.  Saw shapes: %s\' %\n                     spec_shapes)\n\n  if any(t_shape.rank is None for t_shape in tensor_shapes):\n    raise ValueError(\'All tensors should have ndims defined.  Saw shapes: %s\' %\n                     tensor_shapes)\n\n  is_unbatched = [\n      s_shape.is_compatible_with(t_shape)\n      for s_shape, t_shape in zip(spec_shapes, tensor_shapes)\n  ]\n  if all(is_unbatched):\n    return 0\n\n  tensor_ndims_discrepancy = [\n      t_shape.rank - s_shape.rank\n      for s_shape, t_shape in zip(spec_shapes, tensor_shapes)\n  ]\n\n  tensor_matches_spec = [\n      s_shape.is_compatible_with(t_shape[discrepancy:])\n      for discrepancy, s_shape, t_shape in zip(\n          tensor_ndims_discrepancy, spec_shapes, tensor_shapes)\n  ]\n\n  # At this point we are guaranteed to have at least one tensor/spec.\n  num_outer_dims = tensor_ndims_discrepancy[0]\n\n  # Check if all tensors match and have correct number of batch dimensions.\n  is_batched = (\n      all(discrepancy == num_outer_dims\n          for discrepancy in tensor_ndims_discrepancy) and\n      all(tensor_matches_spec))\n\n  if is_batched:\n    return num_outer_dims\n\n  # Check if tensors match but have incorrect number of batch dimensions.\n  incorrect_batch_dims = (\n      tensor_ndims_discrepancy and\n      all(discrepancy == tensor_ndims_discrepancy[0] and discrepancy >= 0\n          for discrepancy in tensor_ndims_discrepancy) and\n      all(tensor_matches_spec))\n\n  if incorrect_batch_dims:\n    raise ValueError(\'Received tensors with %d outer dimensions. \'\n                     \'Expected %d.\' %\n                     (tensor_ndims_discrepancy[0], num_outer_dims))\n\n  raise ValueError(\'Received a mix of batched and unbatched Tensors, or Tensors\'\n                   \' are not compatible with Specs.  num_outer_dims: %d.\\n\'\n                   \'Saw tensor_shapes:\\n   %s\\n\'\n                   \'And spec_shapes:\\n   %s\' %\n                   (num_outer_dims, tensor_shapes, spec_shapes))\n\n\ndef batch_nested_array(nested_array):\n  return tf.nest.map_structure(lambda x: np.expand_dims(x, 0), nested_array)\n\n\ndef unbatch_nested_array(nested_array):\n  return tf.nest.map_structure(lambda x: np.squeeze(x, 0), nested_array)\n\n\ndef unbatch_nested_tensors_to_arrays(nested_tensors):\n\n  def _to_unbatched_numpy(tensor):\n    return np.squeeze(tensor.numpy(), 0)\n\n  return tf.nest.map_structure(_to_unbatched_numpy, nested_tensors)\n\n\ndef _unstack_nested_arrays_into_flat_item_iterator(nested_array):\n\n  def _unstack(array):\n    # Use numpy views instead of np.split, it\'s 5x+ faster.\n    return [array[i] for i in range(len(array))]\n\n  return zip(*[_unstack(array) for array in tf.nest.flatten(nested_array)])\n\n\ndef unstack_nested_arrays(nested_array):\n  """"""Unstack/unbatch a nest of numpy arrays.\n\n  Args:\n    nested_array: Nest of numpy arrays where each array has shape [batch_size,\n      ...].\n\n  Returns:\n    A list of length batch_size where each item in the list is a nest\n      having the same structure as `nested_array`.\n  """"""\n\n  return [\n      tf.nest.pack_sequence_as(nested_array, zipped)\n      for zipped in _unstack_nested_arrays_into_flat_item_iterator(nested_array)\n  ]\n\n\ndef unstack_nested_arrays_into_flat_items(nested_array):\n  """"""Unstack/unbatch a nest of numpy arrays into flat items.\n\n  Rebuild the nested structure of the unbatched elements is expensive. On the\n  other hand it is sometimes unnecessary (e.g. if the downstream processing\n  requires flattened structure, e.g. some replay buffer writers which flattens\n  the items anyway).\n\n  Args:\n    nested_array: Nest of numpy arrays where each array has shape [batch_size,\n      ...].\n\n  Returns:\n    A list of length batch_size where each item in the list is the flattened\n      version of the corresponding item of the input.\n  """"""\n\n  return list(_unstack_nested_arrays_into_flat_item_iterator(nested_array))\n\n\ndef stack_nested_arrays(nested_arrays):\n  """"""Stack/batch a list of nested numpy arrays.\n\n  Args:\n    nested_arrays: A list of nested numpy arrays of the same shape/structure.\n\n  Returns:\n    A nested array containing batched items, where each batched item is obtained\n      by stacking corresponding items from the list of nested_arrays.\n  """"""\n  nested_arrays_flattened = [tf.nest.flatten(a) for a in nested_arrays]\n  batched_nested_array_flattened = [\n      np.stack(a) for a in zip(*nested_arrays_flattened)\n  ]\n  return tf.nest.pack_sequence_as(nested_arrays[0],\n                                  batched_nested_array_flattened)\n\n\ndef get_outer_array_shape(nested_array, spec):\n  """"""Batch dims of array\'s batch dimension `dim`.""""""\n  first_array = tf.nest.flatten(nested_array)[0]\n  first_spec = tf.nest.flatten(spec)[0]\n  num_outer_dims = len(first_array.shape) - len(first_spec.shape)\n  return first_array.shape[:num_outer_dims]\n\n\ndef where(condition, true_outputs, false_outputs):\n  """"""Generalization of tf.where for nested structures.\n\n  This generalization handles applying where across nested structures and the\n  special case where the rank of the condition is smaller than the rank of the\n  true and false cases.\n\n  Args:\n    condition: A boolean Tensor of shape [B, ...]. The shape of condition must\n      be equal to or a prefix of the shape of true_outputs and false_outputs. If\n      condition\'s rank is smaller than the rank of true_outputs and\n      false_outputs, dimensions of size 1 are added to condition to make its\n      rank match that of true_outputs and false_outputs in order to satisfy the\n      requirements of tf.where.\n    true_outputs: Tensor or nested tuple of Tensors of any dtype, each with\n      shape [B, ...], to be split based on `condition`.\n    false_outputs: Tensor or nested tuple of Tensors of any dtype, each with\n      shape [B, ...], to be split based on `condition`.\n\n  Returns:\n    Interleaved output from `true_outputs` and `false_outputs` based on\n    `condition`.\n  """"""\n  assert_same_structure(\n      true_outputs,\n      false_outputs,\n      message=\'""true_outputs"" and ""false_outputs"" structures do not match\')\n  if tf.nest.flatten(true_outputs):\n    case_rank = tf.rank(tf.nest.flatten(true_outputs)[0])\n    rank_difference = case_rank - tf.rank(condition)\n    condition_shape = tf.concat(\n        [tf.shape(condition),\n         tf.ones(rank_difference, dtype=tf.int32)], axis=0)\n    condition = tf.reshape(condition, condition_shape)\n\n  return tf.nest.map_structure(\n      lambda t, f: tf.compat.v2.where(condition, t, f), true_outputs,\n      false_outputs)\n\n\ndef remove_singleton_batch_spec_dim(spec: tf.TypeSpec,\n                                    outer_ndim: int) -> tf.TypeSpec:\n  """"""Look for `spec`\'s shape, check that outer dim is 1, and remove it.\n\n  If `spec.shape[i] != 1` for any `i in range(outer_ndim)`, we stop removing\n  singleton batch dimensions at `i` and return what\'s left.  This is necessary\n  to handle the outputs of inconsistent layers like `tf.keras.layers.LSTM()`\n  which may take as input `(batch, time, dim) = (1, 1, Nin)` and emits only the\n  batch entry if `time == 1`: output shape is `(1, Nout)`.  We log an error\n  in these cases.\n\n  Args:\n    spec: A `tf.TypeSpec`.\n    outer_ndim: The maximum number of outer singleton dims to remove.\n\n  Returns:\n    A `tf.TypeSpec`, the spec without its outer batch dimension(s).\n\n  Raises:\n    ValueError: If `spec` lacks a `shape` property.\n  """"""\n  shape = getattr(spec, \'shape\', None)\n  if shape is None:\n    shape = getattr(spec, \'_shape\', None)\n  if shape is None:\n    raise ValueError(\n        \'Could not remove singleton batch dim from spec; it lacks a shape: {}\'\n        .format(spec))\n  for i in range(outer_ndim):\n    if tf.compat.dimension_value(shape[i]) != 1:\n      logging.error(\n          \'Could not remove singleton batch dim from spec; shape[%d] != 1: %s \'\n          \'(shape: %s).  Skipping.\', i, spec, shape)\n      break\n    spec = spec._unbatch()  # pylint: disable=protected-access\n  return spec\n'"
tf_agents/utils/nest_utils_test.py,129,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.nest_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import nest_utils\n\n# We use this to build {Dict,Tuple,List}Wrappers for testing nesting code.\nfrom tensorflow.python.training.tracking import data_structures  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\n\nclass NestedTensorsTest(tf.test.TestCase):\n  """"""Tests functions related to nested tensors.""""""\n\n  def nest_spec(self, shape=(2, 3), dtype=tf.float32, include_sparse=True):\n    spec = {\n        \'tensor_spec_1\':\n            tensor_spec.TensorSpec(shape, dtype),\n        \'bounded_spec_1\':\n            tensor_spec.BoundedTensorSpec(shape, dtype, -10, 10),\n        \'dict_spec\': {\n            \'tensor_spec_2\':\n                tensor_spec.TensorSpec(shape, dtype),\n            \'bounded_spec_2\':\n                tensor_spec.BoundedTensorSpec(shape, dtype, -10, 10)\n        },\n        \'tuple_spec\': (\n            tensor_spec.TensorSpec(shape, dtype),\n            tensor_spec.BoundedTensorSpec(shape, dtype, -10, 10),\n        ),\n        \'list_spec\': [\n            tensor_spec.TensorSpec(shape, dtype),\n            (tensor_spec.TensorSpec(shape, dtype),\n             tensor_spec.BoundedTensorSpec(shape, dtype, -10, 10)),\n        ],\n        \'sparse_tensor_spec\': tf.SparseTensorSpec(\n            shape=shape, dtype=dtype)\n    }\n    if not include_sparse:\n      del spec[\'sparse_tensor_spec\']\n    return spec\n\n  def zeros_from_spec(self, spec, batch_size=None, extra_sizes=None):\n    """"""Return tensors matching spec with desired additional dimensions.\n\n    Args:\n      spec: A `tf.TypeSpec`, e.g. `tf.TensorSpec` or `tf.SparseTensorSpec`.\n      batch_size: The desired batch size; the size of the first dimension of\n        all tensors.\n      extra_sizes: An optional list of additional dimension sizes beyond the\n        batch_size.\n\n    Returns:\n      A possibly nested tuple of Tensors matching the spec.\n    """"""\n    tensors = []\n    extra_sizes = extra_sizes or []\n    for s in tf.nest.flatten(spec):\n      if isinstance(s, tf.SparseTensorSpec):\n        if batch_size:\n          shape = [batch_size] + extra_sizes + s.shape\n          rank = 1 + len(extra_sizes) + 2\n        else:\n          shape = s.shape\n          rank = 2\n        tensors.append(\n            tf.SparseTensor(\n                indices=tf.zeros([7, rank], dtype=tf.int64),\n                values=tf.zeros([7], dtype=s.dtype),\n                dense_shape=tf.constant(shape.as_list(), dtype=tf.int64)))\n      elif isinstance(s, tf.TensorSpec):\n        if batch_size:\n          shape = tf.TensorShape([batch_size] + extra_sizes).concatenate(\n              s.shape)\n        else:\n          shape = s.shape\n        tensors.append(tf.zeros(shape, dtype=s.dtype))\n      else:\n        raise TypeError(\'Unexpected spec type: {}\'.format(s))\n\n    return tf.nest.pack_sequence_as(spec, tensors)\n\n  def placeholders_from_spec(self, spec):\n    """"""Return tensors matching spec with an added unknown batch dimension.\n\n    Args:\n      spec: A `tf.TypeSpec`, e.g. `tf.TensorSpec` or `tf.SparseTensorSpec`.\n\n    Returns:\n      A possibly nested tuple of Tensors matching the spec.\n    """"""\n    tensors = []\n    for s in tf.nest.flatten(spec):\n      if isinstance(s, tf.SparseTensorSpec):\n        raise NotImplementedError(\n            \'Support for SparseTensor placeholders not implemented.\')\n      elif isinstance(s, tf.TensorSpec):\n        shape = tf.TensorShape([None]).concatenate(s.shape)\n        tensors.append(tf.placeholder(dtype=s.dtype, shape=shape))\n      else:\n        raise TypeError(\'Unexpected spec type: {}\'.format(s))\n\n    return tf.nest.pack_sequence_as(spec, tensors)\n\n  def testGetOuterShapeNotBatched(self):\n    tensor = tf.zeros([2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    batch_size = nest_utils.get_outer_shape(tensor, spec)\n    self.assertEqual(self.evaluate(batch_size), [])\n\n  def testGetOuterShapeOneDim(self):\n    tensor = tf.zeros([5, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    batch_size = nest_utils.get_outer_shape(tensor, spec)\n    self.assertEqual(self.evaluate(batch_size), [5])\n\n  def testGetOuterShapeTwoDims(self):\n    tensor = tf.zeros([7, 5, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    batch_dim = nest_utils.get_outer_shape(tensor, spec)\n    self.assertAllEqual(self.evaluate(batch_dim), [7, 5])\n\n  def testGetOuterShapeDynamicShapeBatched(self):\n    spec = tensor_spec.TensorSpec([1], dtype=tf.float32)\n    tensor = tf.convert_to_tensor(value=[[0.0]] * 8)\n    batch_size = self.evaluate(nest_utils.get_outer_shape(tensor, spec))\n    self.assertAllEqual(batch_size, [8])\n\n  def testGetOuterShapeDynamicShapeNotBatched(self):\n    spec = tensor_spec.TensorSpec([None, 1], dtype=tf.float32)\n    tensor = tf.convert_to_tensor(value=[[0.0]] * 8)\n    batch_size = self.evaluate(nest_utils.get_outer_shape(tensor, spec))\n    self.assertEqual(batch_size, [])\n\n  def testGetOuterDimsSingleTensorUnbatched(self):\n    tensor = tf.zeros([2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    batch_dims = nest_utils.get_outer_rank(tensor, spec)\n    self.assertFalse(batch_dims)\n\n  def testGetOuterDimsSingleTensorBatched(self):\n    tensor = tf.zeros([5, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    batch_dims = nest_utils.get_outer_rank(tensor, spec)\n    self.assertEqual(batch_dims, 1)\n\n  def testGetOuterDimsSpecMismatchUnbatched(self):\n    tensor = tf.zeros([1, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      nest_utils.get_outer_rank(tensor, spec)\n\n  def testGetOuterDimsSpecMismatchBatched(self):\n    tensor = tf.zeros([5, 1, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      nest_utils.get_outer_rank(tensor, spec)\n\n  def testGetOuterDimsNestedTensorsUnbatched(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs)\n\n    batch_dims = nest_utils.get_outer_rank(tensors, specs)\n    self.assertFalse(batch_dims)\n\n  def testGetOuterDimsNestedTensorsBatched(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n\n    batch_dims = nest_utils.get_outer_rank(tensors, specs)\n    self.assertEqual(batch_dims, 1)\n\n  def testGetOuterDimsNestedTensorsMixed(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n    tensors[\'tensor_spec_1\'] = tf.zeros(shape)\n\n    with self.assertRaises(ValueError):\n      nest_utils.get_outer_rank(tensors, specs)\n\n  def testGetOuterDimsNestedTensorsMultipleBatchDims(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2, extra_sizes=[2])\n\n    batch_dims = nest_utils.get_outer_rank(tensors, specs)\n    self.assertEqual(batch_dims, 2)\n\n  def testGetOuterDimsNestedTensorsMultipleBatchDimsMixed(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2, extra_sizes=[2])\n\n    # Tensors are ok.\n    self.assertEqual(nest_utils.get_outer_rank(tensors, specs), 2)\n    with self.assertRaises(ValueError):\n      tensors[\'tensor_spec_1\'] = tf.zeros_like(tensors[\'tensor_spec_1\'][0])\n      # Tensors are not ok.\n      nest_utils.get_outer_rank(tensors, specs)\n\n  def testIsBatchedSingleTensorFalse(self):\n    tensor = tf.zeros([2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    is_batched = nest_utils.is_batched_nested_tensors(tensor, spec)\n    self.assertFalse(is_batched)\n\n  def testIsBatchedSingleTensorTrue(self):\n    tensor = tf.zeros([5, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    is_batched = nest_utils.is_batched_nested_tensors(tensor, spec)\n    self.assertTrue(is_batched)\n\n  def testIsBatchedSingleTensorValueErrorUnBatched(self):\n    tensor = tf.zeros([1, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      nest_utils.is_batched_nested_tensors(tensor, spec)\n\n  def testIsBatchedSingleTensorValueErrorBatched(self):\n    tensor = tf.zeros([5, 1, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      nest_utils.is_batched_nested_tensors(tensor, spec)\n\n  def testIsBatchedNestedTensorsFalse(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs)\n\n    is_batched = nest_utils.is_batched_nested_tensors(tensors, specs)\n    self.assertFalse(is_batched)\n\n  def testIsBatchedNestedTensorsTrue(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n\n    is_batched = nest_utils.is_batched_nested_tensors(tensors, specs)\n    self.assertTrue(is_batched)\n\n  def testIsBatchedNestedTensorsAllowExtraFields(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n    tensors[\'extra_field\'] = tf.constant([1, 2, 3])\n    is_batched = nest_utils.is_batched_nested_tensors(\n        tensors, specs, allow_extra_fields=True)\n    self.assertTrue(is_batched)\n\n  def testIsBatchedNestedTensorsMixed(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n    tensors[\'tensor_spec_1\'] = tf.zeros(shape)\n\n    with self.assertRaises(ValueError):\n      nest_utils.is_batched_nested_tensors(tensors, specs)\n\n  def testIsBatchedNestedTensorsMultipleBatchDimsFalse(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs)\n\n    is_batched = nest_utils.is_batched_nested_tensors(\n        tensors, specs, num_outer_dims=2)\n    self.assertFalse(is_batched)\n\n  def testIsBatchedNestedTensorsMultipleBatchDimsTrue(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2, extra_sizes=[2])\n\n    is_batched = nest_utils.is_batched_nested_tensors(\n        tensors, specs, num_outer_dims=2)\n    self.assertTrue(is_batched)\n\n  def testIsBatchedNestedTensorsMultipleBatchDimsWrongBatchDimNumber(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    # Tensors only have one batch dim.\n    tensors = self.zeros_from_spec(specs, batch_size=2)\n\n    is_batched = nest_utils.is_batched_nested_tensors(tensors,\n                                                      specs,\n                                                      num_outer_dims=2)\n    self.assertFalse(is_batched)\n\n  def testIsBatchedNestedTensorsMultipleBatchDimsRightBatchDimNumber(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    # Tensors only have one batch dim.\n    tensors = self.zeros_from_spec(specs, batch_size=2, extra_sizes=[1])\n\n    is_batched = nest_utils.is_batched_nested_tensors(tensors,\n                                                      specs,\n                                                      num_outer_dims=2)\n    self.assertTrue(is_batched)\n\n  def testIsBatchedNestedTensorsMultipleBatchDimsMixed(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=2, extra_sizes=[2])\n\n    # Tensors are ok.\n    nest_utils.is_batched_nested_tensors(tensors, specs, num_outer_dims=2)\n    with self.assertRaises(ValueError):\n      tensors[\'tensor_spec_1\'] = tf.zeros_like(tensors[\'tensor_spec_1\'][0])\n      # Tensors are not ok.\n      nest_utils.is_batched_nested_tensors(tensors, specs, num_outer_dims=2)\n\n  def testBatchSingleTensor(self):\n    tensor = tf.zeros([2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n\n    batched_tensor = nest_utils.batch_nested_tensors(tensor, spec)\n\n    self.assertEqual(batched_tensor.shape.as_list(), [1, 2, 3])\n\n  def testBatchedSingleTensor(self):\n    tensor = tf.zeros([5, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n\n    batched_tensor = nest_utils.batch_nested_tensors(tensor, spec)\n\n    self.assertEqual(batched_tensor.shape.as_list(), [5, 2, 3])\n\n  def testWrongShapeRaisesValueError(self):\n    tensor = tf.zeros([3, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n\n    with self.assertRaises(ValueError):\n      nest_utils.batch_nested_tensors(tensor, spec)\n\n  def testBatchNestedTensorsNoSpec(self):\n    shape = [2, 3]\n    batch_shape = [1] + shape\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs)\n    tf.nest.assert_same_structure(tensors, specs)\n\n    batched_tensors = nest_utils.batch_nested_tensors(tensors)\n\n    tf.nest.assert_same_structure(specs, batched_tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), batch_shape)\n    tf.nest.map_structure(assert_shapes, batched_tensors)\n\n  def testBatchNestedTensors(self):\n    shape = [2, 3]\n    batch_shape = [1] + shape\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs)\n    tf.nest.assert_same_structure(tensors, specs)\n\n    batched_tensors = nest_utils.batch_nested_tensors(tensors, specs)\n\n    tf.nest.assert_same_structure(specs, batched_tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), batch_shape)\n    tf.nest.map_structure(assert_shapes, batched_tensors)\n\n  def testBatchedNestedTensors(self):\n    shape = [2, 3]\n    batch_size = 5\n    batch_shape = [batch_size] + shape\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(tensors, specs)\n\n    batched_tensors = nest_utils.batch_nested_tensors(tensors, specs)\n\n    tf.nest.assert_same_structure(specs, batched_tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), batch_shape)\n    tf.nest.map_structure(assert_shapes, batched_tensors)\n\n  def testUnBatchSingleTensor(self):\n    batched_tensor = tf.zeros([1, 2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n\n    tensor = nest_utils.unbatch_nested_tensors(batched_tensor, spec)\n\n    self.assertEqual(tensor.shape.as_list(), [2, 3])\n\n  def testUnBatchedSingleTensor(self):\n    tensor = tf.zeros([2, 3], dtype=tf.float32)\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n\n    unbatched_tensor = nest_utils.unbatch_nested_tensors(tensor, spec)\n\n    self.assertEqual(unbatched_tensor.shape.as_list(), [2, 3])\n\n  def testUnBatchNestedTensorsNoSpec(self):\n    shape = [2, 3]\n    batch_size = 1\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    batched_tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(batched_tensors, specs)\n\n    tensors = nest_utils.unbatch_nested_tensors(batched_tensors)\n\n    tf.nest.assert_same_structure(specs, tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), shape, t)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testUnBatchNestedTensors(self):\n    shape = [2, 3]\n    batch_size = 1\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    batched_tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(batched_tensors, specs)\n\n    tensors = nest_utils.unbatch_nested_tensors(batched_tensors, specs)\n\n    tf.nest.assert_same_structure(specs, tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), shape, t)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testSplitNestedTensors(self):\n    shape = [2, 3]\n    batch_size = 7\n\n    specs = self.nest_spec(shape, include_sparse=True)\n    batched_tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(batched_tensors, specs)\n\n    tensors = nest_utils.split_nested_tensors(batched_tensors, specs,\n                                              batch_size)\n    self.assertEqual(batch_size, len(tensors))\n\n    for t in tensors:\n      tf.nest.assert_same_structure(specs, t)\n\n    def assert_shapes(t):\n      if not tf.executing_eagerly() and isinstance(t, tf.SparseTensor):\n        # Constant value propagation in SparseTensors does not allow us to infer\n        # the value of output t.shape from input\'s t.shape; only its rank.\n        self.assertEqual(len(t.shape), 1 + len(shape))\n      else:\n        self.assertEqual(t.shape.as_list(), [1] + shape)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testSplitNestedTensorsSizeSplits(self):\n    shape = [2, 3]\n    batch_size = 9\n    size_splits = [2, 4, 3]\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    batched_tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(batched_tensors, specs)\n\n    tensors = nest_utils.split_nested_tensors(\n        batched_tensors, specs, size_splits)\n    self.assertEqual(len(tensors), len(size_splits))\n\n    for i, tensor in enumerate(tensors):\n      tf.nest.assert_same_structure(specs, tensor)\n      tf.nest.map_structure(\n          lambda t: self.assertEqual(t.shape.as_list()[0], size_splits[i]),  # pylint: disable=cell-var-from-loop\n          tensor)\n\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list()[1:], shape)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testUnstackNestedTensors(self):\n    shape = [5, 8]\n    batch_size = 7\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    batched_tensors = self.zeros_from_spec(specs, batch_size=batch_size)\n    tf.nest.assert_same_structure(batched_tensors, specs)\n\n    tensors = nest_utils.unstack_nested_tensors(batched_tensors, specs)\n    self.assertEqual(batch_size, len(tensors))\n\n    for t in tensors:\n      tf.nest.assert_same_structure(specs, t)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), shape)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testStackNestedTensors(self):\n    shape = [5, 8]\n    batch_size = 3\n    batched_shape = [batch_size,] + shape\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    unstacked_tensors = [self.zeros_from_spec(specs) for _ in range(batch_size)]\n    stacked_tensor = nest_utils.stack_nested_tensors(unstacked_tensors)\n\n    tf.nest.assert_same_structure(specs, stacked_tensor)\n    assert_shapes = lambda tensor: self.assertEqual(tensor.shape, batched_shape)\n    tf.nest.map_structure(assert_shapes, stacked_tensor)\n\n  def testStackNestedTensorsAxis1(self):\n    shape = [5, 8]\n    stack_dim = 3\n    stacked_shape = [5, 3, 8]\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    unstacked_tensors = [self.zeros_from_spec(specs)] * stack_dim\n    stacked_tensor = nest_utils.stack_nested_tensors(unstacked_tensors, axis=1)\n\n    tf.nest.assert_same_structure(specs, stacked_tensor)\n    assert_shapes = lambda tensor: self.assertEqual(tensor.shape, stacked_shape)\n    tf.nest.map_structure(assert_shapes, stacked_tensor)\n\n  def testUnBatchedNestedTensors(self, include_sparse=False):\n    shape = [2, 3]\n\n    specs = self.nest_spec(shape, include_sparse=False)\n    unbatched_tensors = self.zeros_from_spec(specs)\n    tf.nest.assert_same_structure(unbatched_tensors, specs)\n\n    tensors = nest_utils.unbatch_nested_tensors(unbatched_tensors, specs)\n\n    tf.nest.assert_same_structure(specs, tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), shape, t)\n    tf.nest.map_structure(assert_shapes, tensors)\n\n  def testFlattenMultiBatchedSingleTensor(self):\n    spec = tensor_spec.TensorSpec([2, 3], dtype=tf.float32)\n    tensor = self.zeros_from_spec(spec, batch_size=7, extra_sizes=[5])\n\n    (batch_flattened_tensor,\n     batch_dims) = nest_utils.flatten_multi_batched_nested_tensors(tensor, spec)\n\n    self.assertEqual(batch_flattened_tensor.shape.as_list(), [35, 2, 3])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    batch_dims_ = self.evaluate(batch_dims)\n    self.assertAllEqual(batch_dims_, [7, 5])\n\n  def testFlattenMultiBatchedNestedTensors(self):\n    shape = [2, 3]\n    specs = self.nest_spec(shape)\n    tensors = self.zeros_from_spec(specs, batch_size=7, extra_sizes=[5])\n\n    (batch_flattened_tensors,\n     batch_dims) = nest_utils.flatten_multi_batched_nested_tensors(\n         tensors, specs)\n\n    tf.nest.assert_same_structure(specs, batch_flattened_tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), [35, 2, 3])\n    tf.nest.map_structure(assert_shapes, batch_flattened_tensors)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    batch_dims_ = self.evaluate(batch_dims)\n    self.assertAllEqual(batch_dims_, [7, 5])\n\n  def testFlattenMultiBatchedNestedTensorsWithPartiallyKnownShape(self):\n    if tf.executing_eagerly():\n      self.skipTest(\'Do not check nest processing of data in eager mode. \'\n                    \'Placeholders are not compatible with eager execution.\')\n    shape = [2, 3]\n    specs = self.nest_spec(shape, include_sparse=False)\n    tensors = self.placeholders_from_spec(specs)\n\n    (batch_flattened_tensors,\n     _) = nest_utils.flatten_multi_batched_nested_tensors(\n         tensors, specs)\n\n    tf.nest.assert_same_structure(specs, batch_flattened_tensors)\n    assert_shapes = lambda t: self.assertEqual(t.shape.as_list(), [None, 2, 3])\n    tf.nest.map_structure(assert_shapes, batch_flattened_tensors)\n\n\nclass NestedArraysTest(tf.test.TestCase):\n  """"""Tests functions related to nested arrays.""""""\n\n  def nest_spec(self, shape=(2, 3), dtype=np.float32):\n    return {\n        \'array_spec_1\':\n            array_spec.ArraySpec(shape, dtype),\n        \'bounded_spec_1\':\n            array_spec.BoundedArraySpec(shape, dtype, -10, 10),\n        \'dict_spec\': {\n            \'tensor_spec_2\':\n                array_spec.ArraySpec(shape, dtype),\n            \'bounded_spec_2\':\n                array_spec.BoundedArraySpec(shape, dtype, -10, 10)\n        },\n        \'tuple_spec\': (\n            array_spec.ArraySpec(shape, dtype),\n            array_spec.BoundedArraySpec(shape, dtype, -10, 10),\n        ),\n        \'list_spec\': [\n            array_spec.ArraySpec(shape, dtype),\n            (array_spec.ArraySpec(shape, dtype),\n             array_spec.BoundedArraySpec(shape, dtype, -10, 10)),\n        ],\n    }\n\n  def zeros_from_spec(self, specs, outer_dims=None):\n    """"""Return arrays matching spec with desired additional dimensions.\n\n    Args:\n      specs: A nested array spec.\n      outer_dims: An optional list of outer dimensions, e.g. batch size.\n\n    Returns:\n      A nested tuple of arrays matching the spec.\n    """"""\n    outer_dims = outer_dims or []\n\n    def _zeros(spec):\n      return np.zeros(type(spec.shape)(outer_dims) + spec.shape, spec.dtype)\n\n    return tf.nest.map_structure(_zeros, specs)\n\n  def testUnstackNestedArrays(self):\n    shape = (5, 8)\n    batch_size = 3\n\n    specs = self.nest_spec(shape)\n    batched_arrays = self.zeros_from_spec(specs, outer_dims=[batch_size])\n    unbatched_arrays = nest_utils.unstack_nested_arrays(batched_arrays)\n    self.assertEqual(batch_size, len(unbatched_arrays))\n\n    for array in unbatched_arrays:\n      tf.nest.assert_same_structure(specs, array)\n    assert_shapes = lambda a: self.assertEqual(a.shape, shape)\n    tf.nest.map_structure(assert_shapes, unbatched_arrays)\n\n  def testUnstackNestedArraysIntoFlatItems(self):\n    shape = (5, 8)\n    batch_size = 3\n\n    specs = self.nest_spec(shape)\n    batched_arrays = self.zeros_from_spec(specs, outer_dims=[batch_size])\n    unbatched_flat_items = nest_utils.unstack_nested_arrays_into_flat_items(\n        batched_arrays)\n    self.assertEqual(batch_size, len(unbatched_flat_items))\n\n    for nested_array, flat_item in zip(\n        nest_utils.unstack_nested_arrays(batched_arrays), unbatched_flat_items):\n      self.assertAllEqual(flat_item, tf.nest.flatten(nested_array))\n      tf.nest.assert_same_structure(specs,\n                                    tf.nest.pack_sequence_as(specs, flat_item))\n    assert_shapes = lambda a: self.assertEqual(a.shape, shape)\n    tf.nest.map_structure(assert_shapes, unbatched_flat_items)\n\n  def testUnstackNestedArray(self):\n    shape = (5, 8)\n    batch_size = 1\n\n    specs = self.nest_spec(shape)\n    batched_arrays = self.zeros_from_spec(specs, outer_dims=[batch_size])\n    unbatched_arrays = nest_utils.unstack_nested_arrays(batched_arrays)\n    self.assertEqual(batch_size, len(unbatched_arrays))\n\n    for array in unbatched_arrays:\n      tf.nest.assert_same_structure(specs, array)\n    assert_shapes = lambda a: self.assertEqual(a.shape, shape)\n    tf.nest.map_structure(assert_shapes, unbatched_arrays)\n\n  def testStackNestedArrays(self):\n    shape = (5, 8)\n    batch_size = 3\n    batched_shape = (batch_size,) + shape\n\n    specs = self.nest_spec(shape)\n    unstacked_arrays = [self.zeros_from_spec(specs) for _ in range(batch_size)]\n    stacked_array = nest_utils.stack_nested_arrays(unstacked_arrays)\n\n    tf.nest.assert_same_structure(specs, stacked_array)\n    assert_shapes = lambda a: self.assertEqual(a.shape, batched_shape)\n    tf.nest.map_structure(assert_shapes, stacked_array)\n\n  def testGetOuterArrayShape(self):\n    spec = (\n        array_spec.ArraySpec([5, 8], np.float32),\n        (array_spec.ArraySpec([1], np.int32),\n         array_spec.ArraySpec([2, 2, 2], np.float32))\n    )\n\n    batch_size = 3\n    unstacked_arrays = [self.zeros_from_spec(spec) for _ in range(batch_size)]\n\n    outer_dims = nest_utils.get_outer_array_shape(unstacked_arrays[0], spec)\n    self.assertEqual((), outer_dims)\n\n    stacked_array = nest_utils.stack_nested_arrays(unstacked_arrays)\n    outer_dims = nest_utils.get_outer_array_shape(stacked_array, spec)\n    self.assertEqual((batch_size,), outer_dims)\n\n    time_dim = [nest_utils.batch_nested_array(arr) for arr in unstacked_arrays]\n    batch_time = nest_utils.stack_nested_arrays(time_dim)\n    outer_dims = nest_utils.get_outer_array_shape(batch_time, spec)\n    self.assertEqual((batch_size, 1), outer_dims)\n\n  def testWhere(self):\n    condition = tf.convert_to_tensor([True, False, False, True, False])\n    true_output = tf.nest.map_structure(tf.convert_to_tensor,\n                                        (np.array([0] * 5), np.arange(1, 6)))\n    false_output = tf.nest.map_structure(tf.convert_to_tensor,\n                                         (np.array([1] * 5), np.arange(6, 11)))\n\n    result = nest_utils.where(condition, true_output, false_output)\n    result = self.evaluate(result)\n\n    expected = (np.array([0, 1, 1, 0, 1]), np.array([1, 7, 8, 4, 10]))\n    self.assertAllEqual(expected, result)\n\n  def testWhereDifferentRanks(self):\n    condition = tf.convert_to_tensor([True, False, False, True, False])\n    true_output = tf.nest.map_structure(\n        tf.convert_to_tensor,\n        (np.reshape(np.array([0] * 10),\n                    (5, 2)), np.reshape(np.arange(1, 11), (5, 2))))\n    false_output = tf.nest.map_structure(\n        tf.convert_to_tensor,\n        (np.reshape(np.array([1] * 10),\n                    (5, 2)), np.reshape(np.arange(12, 22), (5, 2))))\n\n    result = nest_utils.where(condition, true_output, false_output)\n    result = self.evaluate(result)\n\n    expected = (np.array([[0, 0], [1, 1], [1, 1], [0, 0], [1, 1]]),\n                np.array([[1, 2], [14, 15], [16, 17], [7, 8], [20, 21]]))\n    self.assertAllEqual(expected, result)\n\n  def testWhereSameRankDifferentDimension(self):\n    condition = tf.convert_to_tensor([True, False, True])\n    true_output = (tf.convert_to_tensor([1]), tf.convert_to_tensor([2]))\n    false_output = (tf.convert_to_tensor([3, 4, 5]),\n                    tf.convert_to_tensor([6, 7, 8]))\n\n    result = nest_utils.where(condition, true_output, false_output)\n    result = self.evaluate(result)\n\n    expected = (np.array([1, 4, 1]), np.array([2, 7, 2]))\n    self.assertAllEqual(expected, result)\n\n\nclass PruneExtraKeysTest(tf.test.TestCase):\n\n  def testPruneExtraKeys(self):\n    self.assertEqual(nest_utils.prune_extra_keys({}, {\'a\': 1}), {})\n    self.assertEqual(nest_utils.prune_extra_keys(\n        {\'a\': 1}, {\'a\': \'a\'}), {\'a\': \'a\'})\n    self.assertEqual(\n        nest_utils.prune_extra_keys({\'a\': 1}, {\'a\': \'a\', \'b\': 2}), {\'a\': \'a\'})\n    self.assertEqual(\n        nest_utils.prune_extra_keys([{\'a\': 1}], [{\'a\': \'a\', \'b\': 2}]),\n        [{\'a\': \'a\'}])\n    self.assertEqual(\n        nest_utils.prune_extra_keys(\n            {\'a\': {\'aa\': 1, \'ab\': 2}, \'b\': {\'ba\': 1}},\n            {\'a\': {\'aa\': \'aa\', \'ab\': \'ab\', \'ac\': \'ac\'},\n             \'b\': {\'ba\': \'ba\', \'bb\': \'bb\'},\n             \'c\': \'c\'}),\n        {\'a\': {\'aa\': \'aa\', \'ab\': \'ab\'}, \'b\': {\'ba\': \'ba\'}})\n\n  def testInvalidWide(self):\n    self.assertEqual(nest_utils.prune_extra_keys(None, {\'a\': 1}), {\'a\': 1})\n    self.assertEqual(nest_utils.prune_extra_keys({\'a\': 1}, {}), {})\n    self.assertEqual(nest_utils.prune_extra_keys(\n        {\'a\': 1}, {\'c\': \'c\'}), {\'c\': \'c\'})\n    self.assertEqual(nest_utils.prune_extra_keys([], [\'a\']), [\'a\'])\n    self.assertEqual(\n        nest_utils.prune_extra_keys([{}, {}], [{\'a\': 1}]), [{\'a\': 1}])\n\n  def testNamedTuple(self):\n\n    class A(collections.namedtuple(\'A\', (\'a\', \'b\'))):\n      pass\n\n    self.assertEqual(\n        nest_utils.prune_extra_keys(\n            [A(a={\'aa\': 1}, b=3), {\'c\': 4}],\n            [A(a={\'aa\': \'aa\', \'ab\': \'ab\'}, b=\'b\'), {\'c\': \'c\', \'d\': \'d\'}]),\n        [A(a={\'aa\': \'aa\'}, b=\'b\'), {\'c\': \'c\'}])\n\n  def testSubtypesOfListAndDict(self):\n\n    class A(collections.namedtuple(\'A\', (\'a\', \'b\'))):\n      pass\n\n    # pylint: disable=invalid-name\n    DictWrapper = data_structures.wrap_or_unwrap\n    TupleWrapper = data_structures.wrap_or_unwrap\n    # pylint: enable=invalid-name\n\n    self.assertEqual(\n        nest_utils.prune_extra_keys(\n            [data_structures.ListWrapper([None, DictWrapper({\'a\': 3, \'b\': 4})]),\n             None,\n             TupleWrapper((DictWrapper({\'g\': 5}),)),\n             TupleWrapper(A(None, DictWrapper({\'h\': 6}))),\n            ],\n            [[\'x\', {\'a\': \'a\', \'b\': \'b\', \'c\': \'c\'}],\n             \'d\',\n             ({\'g\': \'g\', \'gg\': \'gg\'},),\n             A(None, {\'h\': \'h\', \'hh\': \'hh\'}),\n            ]),\n        [data_structures.ListWrapper([\n            \'x\', DictWrapper({\'a\': \'a\', \'b\': \'b\'})]),\n         \'d\',\n         TupleWrapper((DictWrapper({\'g\': \'g\'}),)),\n         TupleWrapper(A(None, DictWrapper({\'h\': \'h\'}),)),\n        ])\n\n  def testOrderedDict(self):\n    OD = collections.OrderedDict  # pylint: disable=invalid-name\n\n    self.assertEqual(\n        nest_utils.prune_extra_keys(\n            OD([(\'a\', OD([(\'aa\', 1), (\'ab\', 2)])),\n                (\'b\', OD([(\'ba\', 1)]))]),\n            OD([(\'a\', OD([(\'aa\', \'aa\'), (\'ab\', \'ab\'), (\'ac\', \'ac\')])),\n                (\'b\', OD([(\'ba\', \'ba\'), (\'bb\', \'bb\')])),\n                (\'c\', \'c\')])),\n        OD([(\'a\', OD([(\'aa\', \'aa\'), (\'ab\', \'ab\')])),\n            (\'b\', OD([(\'ba\', \'ba\')]))])\n    )\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/numpy_storage.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""NumpyStorage stores nested objects across multiple numpy arrays.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport io\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import array_spec\n\n# pylint:disable=g-direct-tensorflow-import\nfrom tensorflow.python.training.tracking import base  # TF internal\nfrom tensorflow.python.training.tracking import data_structures  # TF internal\n# pylint:enable=g-direct-tensorflow-import\n\n\n# TODO(b/126551076) Migrate to public APIs\nclass NumpyState(base.Trackable):\n  """"""A checkpointable object whose NumPy array attributes are saved/restored.\n\n  Example usage:\n\n  ```python\n  arrays = numpy_storage.NumpyState()\n  checkpoint = tf.train.Checkpoint(numpy_arrays=arrays)\n  arrays.x = np.ones([3, 4])\n  directory = self.get_temp_dir()\n  prefix = os.path.join(directory, \'ckpt\')\n  save_path = checkpoint.save(prefix)\n  arrays.x[:] = 0.\n  assert (arrays.x == np.zeros([3, 4])).all()\n  checkpoint.restore(save_path)\n  assert (arrays.x == np.ones([3, 4])).all()\n\n  second_checkpoint = tf.train.Checkpoint(\n      numpy_arrays=numpy_storage.NumpyState())\n  # Attributes of NumpyState objects are created automatically by restore()\n  second_checkpoint.restore(save_path)\n  assert (second_checkpoint.numpy_arrays.x == np.ones([3, 4])).all()\n  ```\n\n  Note that `NumpyState` objects re-create the attributes of the previously\n  saved object on `restore()`. This is in contrast to TensorFlow variables, for\n  which a `Variable` object must be created and assigned to an attribute.\n\n  This snippet works both when graph building and when executing eagerly. On\n  save, the NumPy array(s) are fed as strings to be saved in the checkpoint (via\n  a placeholder when graph building, or as a string constant when executing\n  eagerly). When restoring they skip the TensorFlow graph entirely, and so no\n  restore ops need be run. This means that restoration always happens eagerly,\n  rather than waiting for `checkpoint.restore(...).run_restore_ops()` like\n  TensorFlow variables when graph building.\n  """"""\n\n  def _lookup_dependency(self, name):\n    """"""Create placeholder NumPy arrays for to-be-restored attributes.\n\n    Typically `_lookup_dependency` is used to check by name whether a dependency\n    exists. We cheat slightly by creating a checkpointable object for `name` if\n    we don\'t already have one, giving us attribute re-creation behavior when\n    loading a checkpoint.\n\n    Args:\n      name: The name of the dependency being checked.\n\n    Returns:\n      An existing dependency if one exists, or a new `_NumpyWrapper` placeholder\n      dependency (which will generally be restored immediately).\n    """"""\n    value = super(NumpyState, self)._lookup_dependency(name)\n    if value is None:\n      value = _NumpyWrapper(np.array([]))\n      new_reference = base.TrackableReference(name=name, ref=value)\n      self._unconditional_checkpoint_dependencies.append(new_reference)\n      self._unconditional_dependency_names[name] = value\n      super(NumpyState, self).__setattr__(name, value)\n    return value\n\n  def __getattribute__(self, name):\n    """"""Un-wrap `_NumpyWrapper` objects when accessing attributes.""""""\n    value = super(NumpyState, self).__getattribute__(name)\n    if isinstance(value, _NumpyWrapper):\n      return value.array\n    return value\n\n  def __setattr__(self, name, value):\n    """"""Automatically wrap NumPy arrays assigned to attributes.""""""\n    # TODO(b/126429928): Consider supporting lists/tuples.\n    if isinstance(value, (np.ndarray, np.generic)):\n      try:\n        existing = super(NumpyState, self).__getattribute__(name)\n        existing.array = value\n        return\n      except AttributeError:\n        value = _NumpyWrapper(value)\n        self._track_trackable(value, name=name, overwrite=True)\n    elif (name not in (\'_self_setattr_tracking\', \'_self_update_uid\',\n                       # TODO(b/130295584): Remove these non-_self aliases when\n                       # sync issues are resolved.\n                       \'_setattr_tracking\', \'_update_uid\')\n          and getattr(self, \'_setattr_tracking\', True)):\n      # Mixing restore()-created attributes with user-added checkpointable\n      # objects is tricky, since we can\'t use the `_lookup_dependency` trick to\n      # re-create attributes (we might accidentally steal the restoration for\n      # another checkpointable object). For now `NumpyState` objects must be\n      # leaf nodes. Theoretically we could add some extra arguments to\n      # `_lookup_dependency` to figure out whether we should create a NumPy\n      # array for the attribute or not.\n      raise NotImplementedError(\n          (\'Assigned %s to the %s property of %s, which is not a NumPy array. \'\n           \'Currently mixing NumPy arrays and other checkpointable objects is \'\n           \'not supported. File a feature request if this limitation bothers \'\n           \'you.\') % (value, name, self))\n    super(NumpyState, self).__setattr__(name, value)\n\n\nclass _NumpyWrapper(tf.train.experimental.PythonState):\n  """"""Wraps a NumPy array for storage in an object-based checkpoint.""""""\n\n  def __init__(self, array):\n    """"""Specify a NumPy array to wrap.\n\n    Args:\n      array: The NumPy array to save and restore (may be overwritten).\n    """"""\n    self.array = array\n\n  def serialize(self):\n    """"""Callback to serialize the array.""""""\n    string_file = io.BytesIO()\n    try:\n      np.save(string_file, self.array, allow_pickle=False)\n      serialized = string_file.getvalue()\n    finally:\n      string_file.close()\n    return serialized\n\n  def deserialize(self, string_value):\n    """"""Callback to deserialize the array.""""""\n    string_file = io.BytesIO(string_value)\n    try:\n      self.array = np.load(string_file, allow_pickle=False)\n    finally:\n      string_file.close()\n\n\nclass NumpyStorage(tf.Module):\n  """"""A class to store nested objects in a collection of numpy arrays.\n\n  If a data_spec of `{\'foo\': ArraySpec(shape=(4,), dtype=np.uint8), \'bar\':\n  ArraySpec(shape=(3, 7), dtype=np.float32)}` were used, then this would create\n  two arrays, one for the \'foo\' key and one for the \'bar\' key. The .get and\n  .set methods would return/take Python dictionaries, but break down the\n  component arrays before storing them.\n  """"""\n\n  def __init__(self, data_spec, capacity):\n    """"""Creates a NumpyStorage object.\n\n    Args:\n      data_spec: An ArraySpec or a list/tuple/nest of ArraySpecs describing a\n        single item that can be stored in this table.\n      capacity: The maximum number of items that can be stored in the buffer.\n\n    Raises:\n      ValueError: If data_spec is not an instance or nest of ArraySpecs.\n    """"""\n    self._capacity = capacity\n    if not all([\n        isinstance(spec, array_spec.ArraySpec)\n        for spec in tf.nest.flatten(data_spec)\n    ]):\n      raise ValueError(\'The data_spec parameter must be an instance or nest of \'\n                       \'array_spec.ArraySpec. Got: {}\'.format(data_spec))\n    self._data_spec = data_spec\n    self._flat_specs = tf.nest.flatten(data_spec)\n    self._np_state = NumpyState()\n\n    self._buf_names = data_structures.NoDependency([])\n    for idx in range(len(self._flat_specs)):\n      self._buf_names.append(\'buffer{}\'.format(idx))\n      # Set each buffer to a sentinel value (real buffers will never be\n      # scalars) rather than a real value so that if they are restored from\n      # checkpoint, we don\'t end up double-initializing. We don\'t leave them\n      # as unset because setting them to a numpy value tells the checkpointer\n      # this will be a checkpointed attribute and it creates TF ops for it.\n      setattr(self._np_state, self._buf_names[idx], np.int64(0))\n\n  def _array(self, index):\n    """"""Creates or retrieves one of the numpy arrays backing the storage.""""""\n    array = getattr(self._np_state, self._buf_names[index])\n    if np.isscalar(array) or array.ndim == 0:\n      spec = self._flat_specs[index]\n      shape = (self._capacity,) + spec.shape\n      array = np.zeros(shape=shape, dtype=spec.dtype)\n      setattr(self._np_state, self._buf_names[index], array)\n    return array\n\n  def get(self, idx):\n    """"""Get value stored at idx.""""""\n    encoded_item = []\n    for buf_idx in range(len(self._flat_specs)):\n      encoded_item.append(self._array(buf_idx)[idx])\n    return tf.nest.pack_sequence_as(self._data_spec, encoded_item)\n\n  def set(self, table_idx, value):\n    """"""Set table_idx to value.""""""\n    for nest_idx, element in enumerate(tf.nest.flatten(value)):\n      self._array(nest_idx)[table_idx] = element\n'"
tf_agents/utils/numpy_storage_test.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.numpy_storage.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import numpy_storage\n\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass NumpyStorageTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testSaveRestore(self):\n    arrays = numpy_storage.NumpyState()\n    checkpoint = tf.train.Checkpoint(numpy_arrays=arrays)\n    arrays.x = np.ones([3, 4])\n    directory = self.get_temp_dir()\n    prefix = os.path.join(directory, \'ckpt\')\n    save_path = checkpoint.save(prefix)\n    arrays.x[:] = 0.\n    self.assertAllEqual(arrays.x, np.zeros([3, 4]))\n    checkpoint.restore(save_path).assert_consumed()\n    self.assertAllEqual(arrays.x, np.ones([3, 4]))\n\n    second_checkpoint = tf.train.Checkpoint(\n        numpy_arrays=numpy_storage.NumpyState())\n    # Attributes of NumpyState objects are created automatically by restore()\n    second_checkpoint.restore(save_path).assert_consumed()\n    self.assertAllEqual(np.ones([3, 4]), second_checkpoint.numpy_arrays.x)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/object_identity.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for collecting objects based on ""is"" comparison.""""""\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\n# pylint: disable=g-import-not-at-top\ntry:  # pylint: disable=g-statement-before-imports\n  # This import only works on python 3.3 and above.\n  import collections.abc as collections_abc\nexcept ImportError:\n  import collections as collections_abc\n# pylint: enable=unused-import\n# pylint: enable=g-import-not-at-top\n\nimport weakref\n\n\nclass _ObjectIdentityWrapper(object):\n  """"""Wraps an object, mapping __eq__ on wrapper to ""is"" on wrapped.\n\n  Since __eq__ is based on object identity, it\'s safe to also define __hash__\n  based on object ids. This lets us add unhashable types like trackable\n  _ListWrapper objects to object-identity collections.\n  """"""\n\n  __slots__ = [""_wrapped""]\n\n  def __init__(self, wrapped):\n    self._wrapped = wrapped\n\n  @property\n  def unwrapped(self):\n    return self._wrapped\n\n  def _assert_type(self, other):\n    if not isinstance(other, _ObjectIdentityWrapper):\n      raise TypeError(""Cannot compare wrapped object with unwrapped object"")\n\n  def __lt__(self, other):\n    self._assert_type(other)\n    return id(self._wrapped) < id(other._wrapped)  # pylint: disable=protected-access\n\n  def __gt__(self, other):\n    self._assert_type(other)\n    return id(self._wrapped) > id(other._wrapped)  # pylint: disable=protected-access\n\n  def __eq__(self, other):\n    if other is None:\n      return False\n    self._assert_type(other)\n    return self._wrapped is other._wrapped  # pylint: disable=protected-access\n\n  def __ne__(self, other):\n    return not self.__eq__(other)\n\n  def __hash__(self):\n    # Wrapper id() is also fine for weakrefs. In fact, we rely on\n    # id(weakref.ref(a)) == id(weakref.ref(a)) and weakref.ref(a) is\n    # weakref.ref(a) in _WeakObjectIdentityWrapper.\n    return id(self._wrapped)\n\n  def __repr__(self):\n    return ""<{} wrapping {!r}>"".format(type(self).__name__, self._wrapped)\n\n\nclass _WeakObjectIdentityWrapper(_ObjectIdentityWrapper):\n\n  def __init__(self, wrapped):\n    super(_WeakObjectIdentityWrapper, self).__init__(weakref.ref(wrapped))\n\n  @property\n  def unwrapped(self):\n    return self._wrapped()\n\n\nclass Reference(_ObjectIdentityWrapper):\n  """"""Reference that refers an object.\n\n  ```python\n  x = [1]\n  y = [1]\n\n  x_ref1 = Reference(x)\n  x_ref2 = Reference(x)\n  y_ref2 = Reference(y)\n\n  print(x_ref1 == x_ref2)\n  ==> True\n\n  print(x_ref1 == y)\n  ==> False\n  ```\n  """"""\n\n  # Disabling super class\' unwrapped field.\n  unwrapped = property()\n\n  def deref(self):\n    """"""Returns the referenced object.\n\n    ```python\n    x_ref = Reference(x)\n    print(x is x_ref.deref())\n    ==> True\n    ```\n    """"""\n    return self._wrapped\n\n\nclass ObjectIdentityDictionary(collections_abc.MutableMapping):\n  """"""A mutable mapping data structure which compares using ""is"".\n\n  This is necessary because we have trackable objects (_ListWrapper) which\n  have behavior identical to built-in Python lists (including being unhashable\n  and comparing based on the equality of their contents by default).\n  """"""\n\n  def __init__(self):\n    self._storage = {}\n\n  def _wrap_key(self, key):\n    return _ObjectIdentityWrapper(key)\n\n  def __getitem__(self, key):\n    return self._storage[self._wrap_key(key)]\n\n  def __setitem__(self, key, value):\n    self._storage[self._wrap_key(key)] = value\n\n  def __delitem__(self, key):\n    del self._storage[self._wrap_key(key)]\n\n  def __len__(self):\n    return len(self._storage)\n\n  def __iter__(self):\n    for key in self._storage:\n      yield key.unwrapped\n\n  def __repr__(self):\n    return ""ObjectIdentityDictionary(%s)"" % repr(self._storage)\n\n\nclass ObjectIdentityWeakKeyDictionary(ObjectIdentityDictionary):\n  """"""Like weakref.WeakKeyDictionary, but compares objects with ""is"".""""""\n\n  def _wrap_key(self, key):\n    return _WeakObjectIdentityWrapper(key)\n\n  def __len__(self):\n    # Iterate, discarding old weak refs\n    return len(list(self._storage))\n\n  def __iter__(self):\n    keys = self._storage.keys()\n    for key in keys:\n      unwrapped = key.unwrapped\n      if unwrapped is None:\n        del self[key]\n      else:\n        yield unwrapped\n\n\nclass ObjectIdentitySet(collections_abc.MutableSet):\n  """"""Like the built-in set, but compares objects with ""is"".""""""\n\n  def __init__(self, *args):\n    self._storage = set(self._wrap_key(obj) for obj in list(*args))\n\n  @staticmethod\n  def _from_storage(storage):\n    result = ObjectIdentitySet()\n    result._storage = storage  # pylint: disable=protected-access\n    return result\n\n  def _wrap_key(self, key):\n    return _ObjectIdentityWrapper(key)\n\n  def __contains__(self, key):\n    return self._wrap_key(key) in self._storage\n\n  def discard(self, key):\n    self._storage.discard(self._wrap_key(key))\n\n  def add(self, key):\n    self._storage.add(self._wrap_key(key))\n\n  def update(self, items):\n    self._storage.update([self._wrap_key(item) for item in items])\n\n  def clear(self):\n    self._storage.clear()\n\n  def intersection(self, items):\n    return ObjectIdentitySet._from_storage(\n        self._storage.intersection([self._wrap_key(item) for item in items]))\n\n  def difference(self, items):\n    return ObjectIdentitySet._from_storage(\n        self._storage.difference([self._wrap_key(item) for item in items]))\n\n  def __len__(self):\n    return len(self._storage)\n\n  def __iter__(self):\n    keys = list(self._storage)\n    for key in keys:\n      yield key.unwrapped\n\n\nclass ObjectIdentityWeakSet(ObjectIdentitySet):\n  """"""Like weakref.WeakSet, but compares objects with ""is"".""""""\n\n  def _wrap_key(self, key):\n    return _WeakObjectIdentityWrapper(key)\n\n  def __len__(self):\n    # Iterate, discarding old weak refs\n    return len([_ for _ in self])\n\n  def __iter__(self):\n    keys = list(self._storage)\n    for key in keys:\n      unwrapped = key.unwrapped\n      if unwrapped is None:\n        self.discard(key)\n      else:\n        yield unwrapped\n'"
tf_agents/utils/object_identity_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Unit tests for object_identity.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import object_identity\n\n\nclass ObjectIdentityWrapperTest(tf.test.TestCase):\n\n  def testWrapperNotEqualToWrapped(self):\n    class SettableHash(object):\n\n      def __init__(self):\n        self.hash_value = 8675309\n\n      def __hash__(self):\n        return self.hash_value\n\n    o = SettableHash()\n    wrap1 = object_identity._ObjectIdentityWrapper(o)\n    wrap2 = object_identity._ObjectIdentityWrapper(o)\n\n    self.assertEqual(wrap1, wrap1)\n    self.assertEqual(wrap1, wrap2)\n    self.assertEqual(o, wrap1.unwrapped)\n    self.assertEqual(o, wrap2.unwrapped)\n    with self.assertRaises(TypeError):\n      bool(o == wrap1)\n    with self.assertRaises(TypeError):\n      bool(wrap1 != o)\n\n    self.assertNotIn(o, set([wrap1]))\n    o.hash_value = id(o)\n    # Since there is now a hash collision we raise an exception\n    with self.assertRaises(TypeError):\n      bool(o in set([wrap1]))\n\n  def testNestFlatten(self):\n    a = object_identity._ObjectIdentityWrapper(\'a\')\n    b = object_identity._ObjectIdentityWrapper(\'b\')\n    c = object_identity._ObjectIdentityWrapper(\'c\')\n    flat = tf.nest.flatten([[[(a, b)]], c])\n    self.assertEqual(flat, [a, b, c])\n\n  def testNestMapStructure(self):\n    k = object_identity._ObjectIdentityWrapper(\'k\')\n    v1 = object_identity._ObjectIdentityWrapper(\'v1\')\n    v2 = object_identity._ObjectIdentityWrapper(\'v2\')\n    struct = tf.nest.map_structure(lambda a, b: (a, b), {k: v1}, {k: v2})\n    self.assertEqual(struct, {k: (v1, v2)})\n\n\nclass ObjectIdentitySetTest(tf.test.TestCase):\n\n  def testDifference(self):\n\n    class Element(object):\n      pass\n\n    a = Element()\n    b = Element()\n    c = Element()\n    set1 = object_identity.ObjectIdentitySet([a, b])\n    set2 = object_identity.ObjectIdentitySet([b, c])\n    diff_set = set1.difference(set2)\n    self.assertIn(a, diff_set)\n    self.assertNotIn(b, diff_set)\n    self.assertNotIn(c, diff_set)\n\n  def testDiscard(self):\n    a = object()\n    b = object()\n    set1 = object_identity.ObjectIdentitySet([a, b])\n    set1.discard(a)\n    self.assertIn(b, set1)\n    self.assertNotIn(a, set1)\n\n  def testClear(self):\n    a = object()\n    b = object()\n    set1 = object_identity.ObjectIdentitySet([a, b])\n    set1.clear()\n    self.assertLen(set1, 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/session_utils.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A class to create objects which needs a session to be functional.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\nclass SessionUser(object):\n  """"""A class which needs a TensorFlow session for some of its operations.\n\n  A `SessionUser` is a class which can be instantiated outside of a\n  TensorFlow session, but which needs to have access to a session-like object\n  for most of its operations.\n\n  A session-like object is an object on which we can call `run` to execute\n  some TensorFlow ops (e.g. `tf.Session()` and\n  `tf.train.(Singular)MonitoredSession`).\n\n  There are 2 ways of providing a session to a `SessionUser`:\n  - within a TensorFlow session context manager (e.g. within\n    `with tf.Session() as session:`), the session will be automatically\n    retrieved. Be aware that a `tf.train.(Singular)MonitoredSession` does not\n    enter a session context manager.\n  - if the session is constructed outside of a context manager, it must be\n    provided using the `session` setter.\n\n  The session can then be accessed using the `session` property.\n\n  The usual way to use a `SessionUser` is the following.\n  ```python\n  class MySessionUserClass(SessionUser):\n\n    def __init__(self):\n      self(MySessionUserClass, self).__init__()\n      self.op = tf.constant(0)\n\n    def run_some_op(self):\n      self.session.run(self.op)\n\n  my_session_owner = MySessionUserClass()\n  with tf.Session() as session:\n    my_session_owner.run_some_op()\n  ```\n\n  Since both `tf.train.SingularMonitoredSession` and `tf.train.MonitoredSession`\n  do not create a Session context manager, one will need to set the session\n  manually.\n  ```python\n  with tf.train.(Singular)MonitoredSession(...) as session:\n    my_session_owner.session = session\n    my_session_owner.run_some_op()\n  ```\n\n  For `tf.train.SingularMonitoredSession`, since one can access the\n  underlying raw session, one can also open a Session context manager.\n  ```python\n  with tf.train.SingularMonitoredSession(...) as mon_sess:\n    with mon_sess.raw_session().as_default():\n       while not mon_sess.should_stop():\n         my_session_owner.run_some_op()\n  ```\n\n  Advanced usage:\n\n  One can override the session setter by using the following code.\n  ```python\n\n\n  class MyClass(session_utils.SessionUser):\n\n    # This is overriding the `session` setter from `session_utils.SessionUser`.\n    @session_utils.SessionUser.session.setter\n    def session(self, session):\n      # This calls the setter of the `session_utils.SessionUser` class.\n      session_utils.SessionUser.session.fset(self, session)\n      # Then you can do other things such as setting the session of internal\n      # objects.\n  ```\n  """"""\n\n  @property\n  def session(self):\n    """"""Returns the TensorFlow session-like object used by this object.\n\n    Returns:\n      The internal TensorFlow session-like object. If it is `None`, it will\n      return the current TensorFlow session context manager.\n\n    Raises:\n      AttributeError: When no session-like object has been set, and no\n        session context manager has been entered.\n    """"""\n    if not hasattr(self, ""_session_user_internal_session""):\n      self._session_user_internal_session = None\n\n    if self._session_user_internal_session is not None:\n      return self._session_user_internal_session\n\n    default_session = tf.compat.v1.get_default_session()\n    if default_session is None:\n      raise AttributeError(\n          ""No TensorFlow session-like object was set on this {!r}, and none ""\n          ""could be retrieved using \'tf.get_default_session()\'."".format(\n              self.__class__.__name__))\n    return default_session\n\n  @session.setter\n  def session(self, session):\n    """"""Sets up the internal session-like object.""""""\n    self._session_user_internal_session = session\n'"
tf_agents/utils/session_utils_test.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for learning.reinforment_learning.utils.session_user.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import session_utils\n\n\nclass MySessionUser(session_utils.SessionUser):\n\n  def __init__(self):\n    super(MySessionUser, self).__init__()\n    self._op = tf.constant(0)\n\n  def run(self):\n    return self.session.run(self._op)\n\n\nclass TfRunnableTest(tf.test.TestCase):\n\n  def setUp(self):\n    if tf.executing_eagerly():\n      self.skipTest(""session_utils are not applicable when executing eagerly"")\n\n  def testWithoutSession(self):\n    session_user = MySessionUser()\n    with self.assertRaisesRegexp(AttributeError, ""No TensorFlow session""):\n      session_user.run()\n\n  def testSessionWithinContextManager(self):\n    session_user = MySessionUser()\n    with tf.compat.v1.Session() as session:\n      self.assertIs(session_user.session, session)\n      self.assertEqual(0, session_user.run())\n\n  def testTestSessionWithinContextManager(self):\n    session_user = MySessionUser()\n    with self.cached_session() as session:\n      self.assertIs(session_user.session, session)\n      self.assertEqual(0, session_user.run())\n\n  def testSessionWithinMonitoredSessionContextManagerRaisesError(self):\n    session_user = MySessionUser()\n    with tf.compat.v1.train.MonitoredSession() as _:\n      with self.assertRaisesRegexp(AttributeError, ""No TensorFlow session""):\n        session_user.run()\n\n  def testSessionWithSingularMonitoredSession(self):\n    session_user = MySessionUser()\n    with tf.compat.v1.train.SingularMonitoredSession() as session:\n      session_user.session = session\n      self.assertEqual(0, session_user.run())\n\n  def testSessionWithMonitoredSession(self):\n    session_user = MySessionUser()\n    with tf.compat.v1.train.MonitoredSession() as session:\n      session_user.session = session\n      self.assertEqual(0, session_user.run())\n\n  def testSessionProvidedUsingSetSession(self):\n    session_user = MySessionUser()\n    session = tf.compat.v1.Session()\n    session_user.session = session\n    self.assertIs(session_user.session, session)\n    self.assertEqual(0, session_user.run())\n\n  def testSettingSessionTakesPrecedenceOverDefaultSession(self):\n    session_user = MySessionUser()\n    with self.cached_session() as test_session:\n      session = tf.compat.v1.Session()\n      self.assertIsNot(test_session, session)\n      self.assertIs(session_user.session, test_session)\n      session_user.session = session\n      self.assertIs(session_user.session, session)\n\n  def testSessionUsesCurrent(self):\n    session_user = MySessionUser()\n    session1 = tf.compat.v1.Session()\n    session2 = tf.compat.v1.Session()\n    self.assertIsNot(session1, session2)\n    with session1.as_default():\n      self.assertIs(session_user.session, session1)\n    with session2.as_default():\n      self.assertIs(session_user.session, session2)\n    session1.close()\n    session2.close()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tf_agents/utils/tensor_normalizer.py,54,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tensor normalizer classses.\n\nThese encapsulate variables and function for tensor normalization.\n\nExample usage:\n\nobservation = tf.placeholder(tf.float32, shape=[])\ntensor_normalizer = StreamingTensorNormalizer(\n    tensor_spec.TensorSpec([], tf.float32), scope=\'normalize_observation\')\nnormalized_observation = tensor_normalizer.normalize(observation)\nupdate_normalization = tensor_normalizer.update(observation)\n\nwith tf.Session() as sess:\n  for o in observation_list:\n    # Compute normalized observation given current observation vars.\n    normalized_observation_ = sess.run(\n        normalized_observation, feed_dict = {observation: o})\n\n    # Update normalization params for next normalization op.\n    sess.run(update_normalization, feed_dict = {observation: o})\n\n    # Do something with normalized_observation_\n    ...\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ncreate_variable = common.create_variable\n\n\nclass TensorNormalizer(tf.Module):\n  """"""Encapsulates tensor normalization and owns normalization variables.""""""\n\n  def __init__(self, tensor_spec, scope=\'normalize_tensor\'):\n    super(TensorNormalizer, self).__init__(name=scope)\n    self._scope = scope\n    self._tensor_spec = tensor_spec\n    self._flat_tensor_spec = tf.nest.flatten(tensor_spec)\n    self._create_variables()\n\n  @property\n  def nested(self):\n    """"""True if tensor is nested, False otherwise.""""""\n    return tf.nest.is_nested(self._tensor_spec)\n\n  @abc.abstractmethod\n  def copy(self, scope=None):\n    """"""Copy constructor for TensorNormalizer.""""""\n\n  @abc.abstractmethod\n  def _create_variables(self):\n    """"""Uses self._scope and creates all variables needed for the normalizer.""""""\n\n  @property\n  @abc.abstractmethod\n  def variables(self):\n    """"""Returns a tuple of tf variables owned by this normalizer.""""""\n\n  @abc.abstractmethod\n  def _update_ops(self, tensor, outer_dims):\n    """"""Returns a list of ops which update normalizer variables for tensor.\n\n    Args:\n      tensor: The tensor, whose batch statistics to use for updating\n        normalization variables.\n      outer_dims: The dimensions to consider batch dimensions, to reduce over.\n    """"""\n\n  @abc.abstractmethod\n  def _get_mean_var_estimates(self):\n    """"""Returns this normalizer\'s current estimates for mean & variance.""""""\n\n  def update(self, tensor, outer_dims=(0,)):\n    """"""Updates tensor normalizer variables.""""""\n    tensor = tf.nest.map_structure(lambda t: tf.cast(t, tf.float32), tensor)\n    return tf.group(self._update_ops(tensor, outer_dims))\n\n  def normalize(self,\n                tensor,\n                clip_value=5.0,\n                center_mean=True,\n                variance_epsilon=1e-3):\n    """"""Applies normalization to tensor.\n\n    Args:\n      tensor: Tensor to normalize.\n      clip_value: Clips normalized observations between +/- this value if\n        clip_value > 0, otherwise does not apply clipping.\n      center_mean: If true, subtracts off mean from normalized tensor.\n      variance_epsilon: Epsilon to avoid division by zero in normalization.\n\n    Returns:\n      normalized_tensor: Tensor after applying normalization.\n    """"""\n    nest_utils.assert_same_structure(tensor, self._tensor_spec)\n    tensor = tf.nest.flatten(tensor)\n    tensor = tf.nest.map_structure(lambda t: tf.cast(t, tf.float32), tensor)\n\n    with tf.name_scope(self._scope + \'/normalize\'):\n      mean_estimate, var_estimate = self._get_mean_var_estimates()\n      mean = (\n          mean_estimate if center_mean else tf.nest.map_structure(\n              tf.zeros_like, mean_estimate))\n\n      def _normalize_single_tensor(single_tensor, single_mean, single_var):\n        return tf.nn.batch_normalization(\n            single_tensor,\n            single_mean,\n            single_var,\n            offset=None,\n            scale=None,\n            variance_epsilon=variance_epsilon,\n            name=\'normalized_tensor\')\n\n      normalized_tensor = nest.map_structure_up_to(\n          self._flat_tensor_spec,\n          _normalize_single_tensor,\n          tensor,\n          mean,\n          var_estimate,\n          check_types=False)\n\n      if clip_value > 0:\n\n        def _clip(t):\n          return tf.clip_by_value(\n              t, -clip_value, clip_value, name=\'clipped_normalized_tensor\')\n\n        normalized_tensor = tf.nest.map_structure(_clip, normalized_tensor)\n\n    normalized_tensor = tf.nest.pack_sequence_as(self._tensor_spec,\n                                                 normalized_tensor)\n    return normalized_tensor\n\n\nclass EMATensorNormalizer(TensorNormalizer):\n  """"""TensorNormalizer with exponential moving avg. mean and var estimates.""""""\n\n  def __init__(\n      self,\n      tensor_spec,\n      scope=\'normalize_tensor\',\n      norm_update_rate=0.001):\n    super(EMATensorNormalizer, self).__init__(tensor_spec, scope)\n    self._norm_update_rate = norm_update_rate\n\n  def copy(self, scope=None):\n    """"""Copy constructor for EMATensorNormalizer.""""""\n    scope = scope if scope is not None else self._scope\n    return EMATensorNormalizer(\n        self._tensor_spec, scope=scope, norm_update_rate=self._norm_update_rate)\n\n  def _create_variables(self):\n    """"""Creates the variables needed for EMATensorNormalizer.""""""\n    self._mean_moving_avg = tf.nest.map_structure(\n        lambda spec: create_variable(\'mean\', 0, spec.shape, tf.float32),\n        self._flat_tensor_spec)\n    self._var_moving_avg = tf.nest.map_structure(\n        lambda spec: create_variable(\'var\', 1, spec.shape, tf.float32),\n        self._flat_tensor_spec)\n\n  @property\n  def variables(self):\n    """"""Returns a tuple of tf variables owned by this EMATensorNormalizer.""""""\n    return (tf.nest.pack_sequence_as(self._tensor_spec, self._mean_moving_avg),\n            tf.nest.pack_sequence_as(self._tensor_spec, self._var_moving_avg))\n\n  def _update_ops(self, tensor, outer_dims):\n    """"""Returns a list of update obs for EMATensorNormalizer mean and var.\n\n    This normalizer tracks the mean & variance of the dimensions of the input\n    tensor using an exponential moving average. The batch mean comes from just\n    the batch statistics, and the batch variance comes from the squared\n    difference of tensor values from the current mean estimate. The mean &\n    variance are both updated as (old_value + update_rate *\n    (batch_value - old_value)).\n\n    Args:\n      tensor: The tensor of values to be normalized.\n      outer_dims: The batch dimensions over which to compute normalization\n        statistics.\n\n    Returns:\n      A list of ops, which when run will update all necessary normaliztion\n      variables.\n    """"""\n\n    def _tensor_update_ops(single_tensor, mean_var, var_var):\n      """"""Make update ops for a single non-nested tensor.""""""\n      # Take the moments across batch dimension. Calculate variance with\n      #   moving avg mean, so that this works even with batch size 1.\n      mean = tf.reduce_mean(input_tensor=single_tensor, axis=outer_dims)\n      var = tf.reduce_mean(\n          input_tensor=tf.square(single_tensor - mean_var), axis=outer_dims)\n\n      # Ops to update moving average. Make sure that all stats are computed\n      #   before updates are performed.\n      with tf.control_dependencies([mean, var]):\n        update_ops = [\n            mean_var.assign_add(self._norm_update_rate * (mean - mean_var)),\n            var_var.assign_add(self._norm_update_rate * (var - var_var))\n        ]\n      return update_ops\n\n    # Aggregate update ops for all parts of potentially nested tensor.\n    tensor = tf.nest.flatten(tensor)\n    updates = tf.nest.map_structure(_tensor_update_ops, tensor,\n                                    self._mean_moving_avg, self._var_moving_avg)\n    all_update_ops = tf.nest.flatten(updates)\n\n    return all_update_ops\n\n  def _get_mean_var_estimates(self):\n    """"""Returns EMANormalizer\'s current estimates for mean & variance.""""""\n    return self._mean_moving_avg, self._var_moving_avg\n\n\nclass StreamingTensorNormalizer(TensorNormalizer):\n  """"""Normalizes mean & variance based on full history of tensor values.""""""\n\n  def _create_variables(self):\n    """"""Uses self._scope and creates all variables needed for the normalizer.""""""\n    self._count = tf.nest.map_structure(\n        lambda spec: create_variable(\'count\', 1e-8, spec.shape, tf.float32),\n        self._flat_tensor_spec)\n    self._mean_sum = tf.nest.map_structure(\n        lambda spec: create_variable(\'mean_sum\', 0, spec.shape, tf.float32),\n        self._flat_tensor_spec)\n    self._var_sum = tf.nest.map_structure(\n        lambda spec: create_variable(\'var_sum\', 0, spec.shape, tf.float32),\n        self._flat_tensor_spec)\n\n  def copy(self, scope=None):\n    """"""Copy constructor for StreamingTensorNormalizer.""""""\n    scope = scope if scope is not None else self._scope\n    return StreamingTensorNormalizer(self._tensor_spec, scope=scope)\n\n  @property\n  def variables(self):\n    """"""Returns a tuple of tf variables owned by this normalizer.""""""\n    return (tf.nest.pack_sequence_as(self._tensor_spec, self._count),\n            tf.nest.pack_sequence_as(self._tensor_spec, self._mean_sum),\n            tf.nest.pack_sequence_as(self._tensor_spec, self._var_sum))\n\n  def _update_ops(self, tensor, outer_dims):\n    """"""Returns a list of ops which update normalizer variables for tensor.\n\n    This normalizer computes the absolute mean of all observed tensor values,\n    and keeps a biased estimator of variance, by summing all observed mean and\n    variance values and dividing the sum by the count of samples seen.\n\n    Args:\n      tensor: The tensor of values to be normalized.\n      outer_dims: The batch dimensions over which to compute normalization\n        statistics.\n\n    Returns:\n      A list of ops, which when run will update all necessary normaliztion\n      variables.\n    """"""\n\n    def _tensor_update_ops(single_tensor, single_mean_est, count_var, mean_var,\n                           var_var):\n      """"""Make update ops for a single non-nested tensor.""""""\n      # Num samples in batch is the product of batch dimensions.\n      num_samples = tf.cast(\n          tf.reduce_prod(\n              input_tensor=tf.gather(tf.shape(\n                  input=single_tensor), outer_dims)), tf.float32)\n      mean_sum = tf.reduce_sum(input_tensor=single_tensor, axis=outer_dims)\n      var_sum = tf.reduce_sum(\n          input_tensor=tf.square(single_tensor - single_mean_est),\n          axis=outer_dims)\n\n      # Ops to update streaming norm. Make sure that all stats are computed\n      #   before updates are performed.\n      with tf.control_dependencies([num_samples, mean_sum, var_sum]):\n        update_ops = [\n            tf.compat.v1.assign_add(\n                count_var,\n                tf.ones_like(count_var) * num_samples,\n                name=\'update_count\'),\n            tf.compat.v1.assign_add(mean_var, mean_sum, name=\'update_mean_sum\'),\n            tf.compat.v1.assign_add(var_var, var_sum, name=\'update_var_sum\'),\n        ]\n      return update_ops\n\n    mean_estimate, _ = self._get_mean_var_estimates()\n\n    tensor = tf.nest.flatten(tensor)\n    # Aggregate update ops for all parts of potentially nested tensor.\n    updates = tf.nest.map_structure(_tensor_update_ops, tensor, mean_estimate,\n                                    self._count, self._mean_sum, self._var_sum)\n    all_update_ops = tf.nest.flatten(updates)\n\n    return all_update_ops\n\n  def _get_mean_var_estimates(self):\n    """"""Returns this normalizer\'s current estimates for mean & variance.""""""\n    mean_estimate = nest.map_structure_up_to(self._flat_tensor_spec,\n                                             lambda a, b: a / b, self._mean_sum,\n                                             self._count)\n    var_estimate = nest.map_structure_up_to(self._flat_tensor_spec,\n                                            lambda a, b: a / b, self._var_sum,\n                                            self._count)\n    return mean_estimate, var_estimate\n'"
tf_agents/utils/tensor_normalizer_test.py,36,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.tensor_normalizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import tensor_normalizer\n\n\nclass EMATensorNormalizerTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(EMATensorNormalizerTest, self).setUp()\n    tf.compat.v1.reset_default_graph()\n    self._tensor_spec = tensor_spec.TensorSpec([3], tf.float32, \'obs\')\n    self._tensor_normalizer = tensor_normalizer.EMATensorNormalizer(\n        tensor_spec=self._tensor_spec)\n    self._dict_tensor_spec = {\'a\': self._tensor_spec, \'b\': self._tensor_spec}\n    self._dict_tensor_normalizer = tensor_normalizer.EMATensorNormalizer(\n        tensor_spec=self._dict_tensor_spec)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n  def testGetVariables(self):\n    means_var, variances_var = self._tensor_normalizer.variables\n    self.assertAllEqual(means_var.shape.as_list(),\n                        self._tensor_spec.shape.as_list())\n    self.assertAllEqual(variances_var.shape.as_list(),\n                        self._tensor_spec.shape.as_list())\n\n  def testUpdateVariables(self):\n    # Get original mean and variance.\n    original_means, original_variances = self.evaluate(\n        self._tensor_normalizer.variables)\n\n    # Construct and evaluate normalized tensor. Should update mean &\n    #   variance.\n    tensor = tf.constant([[1.3, 4.2, 7.5]], dtype=tf.float32)\n    update_norm_vars = self._tensor_normalizer.update(tensor)\n    self.evaluate(update_norm_vars)\n\n    # Get new mean and variance, and make sure they changed.\n    new_means, new_variances = self.evaluate(\n        self._tensor_normalizer.variables)\n    for new_val, old_val in (list(zip(new_means, original_means)) +\n                             list(zip(new_variances, original_variances))):\n      self.assertNotEqual(new_val, old_val)\n\n  def testUpdateVariablesDictNest(self):\n    # Get original mean and variance.\n    original_means, original_variances = self.evaluate(\n        self._dict_tensor_normalizer.variables)\n\n    # Construct and evaluate normalized tensor. Should update mean &\n    #   variance.\n    tensor = {\'a\': tf.constant([[1.3, 4.2, 7.5]], dtype=tf.float32),\n              \'b\': tf.constant([[1.3, 4.2, 7.5]], dtype=tf.float32)}\n    update_norm_vars = self._dict_tensor_normalizer.update(tensor)\n    self.evaluate(update_norm_vars)\n\n    # Get new mean and variance, and make sure they changed.\n    new_means, new_variances = self.evaluate(\n        self._dict_tensor_normalizer.variables)\n\n    def _assert_dict_changed(dict1, dict2):\n      self.assertAllEqual(sorted(dict1.keys()), sorted(dict2.keys()))\n      for k in dict1.keys():\n        for i in range(len(dict1[k])):\n          self.assertNotEqual(dict1[k][i], dict2[k][i])\n\n    _assert_dict_changed(original_means, new_means)\n    _assert_dict_changed(original_variances, new_variances)\n\n  @parameterized.named_parameters(\n      (\'OneReduceAxis\', 1),\n      (\'TwoReduceAxes\', 2),\n  )\n  def testNormalization(self, num_outer_dims):\n    means_var, variance_var = self._tensor_normalizer.variables\n    self.evaluate([\n        tf.compat.v1.assign(means_var, [10.0] * 3),\n        tf.compat.v1.assign(variance_var, [0.1] * 3)\n    ])\n\n    vector = [9.0, 10.0, 11.0]\n    # Above, the estimated mean was set to 10, and variance to 0.1. Thus the\n    # estimated stddev is sqrt(0.1) = 0.3162.\n    # The middle sample falls on the mean, so should be normalized to 0.0. Each\n    # of the other samples is 1 away from the mean. 1 / 0.3162 = 3.162\n    expected = [-3.1622776601, 0.0, 3.1622776601]\n    for _ in range(num_outer_dims - 1):\n      vector = [vector] * 2\n      expected = [expected] * 2\n    tensor = tf.constant(vector)\n\n    norm_obs = self._tensor_normalizer.normalize(\n        tensor, variance_epsilon=0.0)\n    self.assertAllClose(expected, self.evaluate(norm_obs), atol=0.0001)\n\n  def testNormalizationDictNest(self):\n    means_var, variance_var = self._dict_tensor_normalizer.variables\n    self.evaluate(  # For each var in nest, assign initial value.\n        [tf.compat.v1.assign(var, [10.0] * 3) for var in means_var.values()] +\n        [tf.compat.v1.assign(var, [.1] * 3) for var in variance_var.values()])\n\n    vector = [9.0, 10.0, 11.0]\n    expected = {\'a\': [-3.1622776601, 0.0, 3.1622776601],\n                \'b\': [-3.1622776601, 0.0, 3.1622776601]}\n    tensor = {\'a\': tf.constant(vector), \'b\': tf.constant(vector)}\n\n    norm_obs = self._dict_tensor_normalizer.normalize(\n        tensor, variance_epsilon=0.0)\n    self.assertAllClose(expected, self.evaluate(norm_obs), atol=0.0001)\n\n  def testShouldNotCenterMean(self):\n    means_var, variance_var = self._tensor_normalizer.variables\n    self.evaluate([\n        tf.compat.v1.assign(means_var, [10.0] * 3),\n        tf.compat.v1.assign(variance_var, [0.01] * 3)\n    ])\n    tensor = tf.constant([[9.0, 10.0, 11.0]])\n    norm_obs = self._tensor_normalizer.normalize(\n        tensor, center_mean=False,\n        variance_epsilon=0.0, clip_value=0.0)\n    expected = [[90.0, 100.0, 110.0]]\n    self.assertAllClose(expected, self.evaluate(norm_obs))\n\n\nclass StreamingTensorNormalizerTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(StreamingTensorNormalizerTest, self).setUp()\n    tf.compat.v1.reset_default_graph()\n    self._tensor_spec = tensor_spec.TensorSpec([3], tf.float32, \'obs\')\n    self._tensor_normalizer = tensor_normalizer.StreamingTensorNormalizer(\n        tensor_spec=self._tensor_spec)\n    self._dict_tensor_spec = {\'a\': self._tensor_spec, \'b\': self._tensor_spec}\n    self._dict_tensor_normalizer = tensor_normalizer.StreamingTensorNormalizer(\n        tensor_spec=self._dict_tensor_spec)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n  def testGetVariables(self):\n    count_var, means_var, variances_var = (self._tensor_normalizer.variables)\n    self.assertAllEqual(count_var.shape, self._tensor_spec.shape)\n    self.assertAllEqual(means_var.shape, self._tensor_spec.shape)\n    self.assertAllEqual(variances_var.shape, self._tensor_spec.shape)\n\n  def testUpdateVariables(self):\n    # Get original mean and variance.\n    original_count, original_mean_sum, original_variance_sum = self.evaluate(\n        self._tensor_normalizer.variables)\n\n    # Construct and evaluate normalized tensor. Should update mean &\n    #   variance.\n    np_array = np.array([[1.3, 4.2, 7.5],\n                         [8.3, 2.2, 9.5],\n                         [3.3, 5.2, 6.5]], np.float32)\n    tensor = tf.constant(np_array, dtype=tf.float32)\n    update_norm_vars = self._tensor_normalizer.update(tensor)\n    self.evaluate(update_norm_vars)\n\n    # Get new mean and variance, and make sure they changed.\n    new_count, new_mean_sum, new_variance_sum = self.evaluate(\n        self._tensor_normalizer.variables)\n\n    self.assertAllEqual(new_count,\n                        np.array([3, 3, 3], dtype=np.float32) + original_count)\n    self.assertAllClose(new_mean_sum,\n                        np.sum(np_array, axis=0) + original_mean_sum)\n    self.assertAllClose(\n        new_variance_sum,\n        np.sum(np.square(np_array - original_mean_sum), axis=0) +\n        original_variance_sum)\n\n  def testUpdateVariablesDictNest(self):\n    # Get original mean and variance.\n    original_count, original_mean_sum, original_variance_sum = self.evaluate(\n        self._dict_tensor_normalizer.variables)\n\n    # Construct and evaluate normalized tensor. Should update mean &\n    #   variance.\n    np_array = np.array([[1.3, 4.2, 7.5],\n                         [8.3, 2.2, 9.5],\n                         [3.3, 5.2, 6.5]], np.float32)\n    tensor = {\'a\': tf.constant(np_array, dtype=tf.float32),\n              \'b\': tf.constant(np_array, dtype=tf.float32)}\n    update_norm_vars = self._dict_tensor_normalizer.update(tensor)\n    self.evaluate(update_norm_vars)\n\n    # Get new mean and variance, and make sure they changed.\n    new_count, new_mean_sum, new_variance_sum = self.evaluate(\n        self._dict_tensor_normalizer.variables)\n\n    expected_count = {k: (np.array([3, 3, 3], dtype=np.float32) +\n                          original_count[k]) for k in original_count}\n    expected_mean_sum = {k: (np.sum(np_array, axis=0) +\n                             original_mean_sum[k]) for k in original_mean_sum}\n    expected_variance_sum = {\n        k: (np.sum(np.square(np_array - original_mean_sum[k]), axis=0) +\n            original_variance_sum[k]) for k in original_variance_sum}\n\n    def _assert_dicts_close(dict1, dict2):\n      self.assertAllEqual(sorted(dict1.keys()), sorted(dict2.keys()))\n      self.assertAllClose([dict1[k] for k in dict1.keys()],\n                          [dict2[k] for k in dict1.keys()])\n\n    _assert_dicts_close(new_count, expected_count)\n    _assert_dicts_close(new_mean_sum, expected_mean_sum)\n    _assert_dicts_close(new_variance_sum, expected_variance_sum)\n\n  @parameterized.named_parameters(\n      (\'OneReduceAxis\', 1),\n      (\'TwoReduceAxes\', 2),\n  )\n  def testNormalization(self, num_outer_dims):\n    count_var, means_var, variance_var = self._tensor_normalizer.variables\n    self.evaluate([\n        tf.compat.v1.assign(count_var, [1.0] * 3),\n        tf.compat.v1.assign(means_var, [10.0] * 3),\n        tf.compat.v1.assign(variance_var, [0.1] * 3)\n    ])\n\n    vector = [9.0, 10.0, 11.0]\n    # Above, the estimated mean was set to 10, and variance to 0.1. Thus the\n    # estimated stddev is sqrt(0.1) = 0.3162.\n    # The middle sample falls on the mean, so should be normalized to 0.0. Each\n    # of the other samples is 1 away from the mean. 1 / 0.3162 = 3.162\n    expected = [-3.1622776601, 0.0, 3.1622776601]\n    for _ in range(num_outer_dims - 1):\n      vector = [vector] * 2\n      expected = [expected] * 2\n    tensor = tf.constant(vector)\n\n    norm_obs = self._tensor_normalizer.normalize(\n        tensor, variance_epsilon=0.0)\n    self.assertAllClose(expected, self.evaluate(norm_obs), atol=0.0001)\n\n  def testNormalizationDictNest(self):\n    count_var, means_var, variance_var = self._dict_tensor_normalizer.variables\n    self.evaluate(  # For each var in nest, assign initial value.\n        [tf.compat.v1.assign(var, [1.0] * 3) for var in count_var.values()] +\n        [tf.compat.v1.assign(var, [10.0] * 3) for var in means_var.values()] +\n        [tf.compat.v1.assign(var, [.1] * 3) for var in variance_var.values()])\n\n    vector = [9.0, 10.0, 11.0]\n    expected = {\'a\': [-3.1622776601, 0.0, 3.1622776601],\n                \'b\': [-3.1622776601, 0.0, 3.1622776601]}\n    tensor = {\'a\': tf.constant(vector), \'b\': tf.constant(vector)}\n\n    norm_obs = self._dict_tensor_normalizer.normalize(\n        tensor, variance_epsilon=0.0)\n    self.assertAllClose(expected, self.evaluate(norm_obs), atol=0.0001)\n\n  def testShouldNotCenterMean(self):\n    count_var, means_var, variance_var = self._tensor_normalizer.variables\n    self.evaluate([\n        tf.compat.v1.assign(count_var, [1.0] * 3),\n        tf.compat.v1.assign(means_var, [10.0] * 3),\n        tf.compat.v1.assign(variance_var, [0.01] * 3)\n    ])\n    tensor = tf.constant([[9.0, 10.0, 11.0]])\n    norm_obs = self._tensor_normalizer.normalize(\n        tensor, center_mean=False,\n        variance_epsilon=0.0, clip_value=0.0)\n    expected = [[90.0, 100.0, 110.0]]\n    self.assertAllClose(expected, self.evaluate(norm_obs))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/test_utils.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utility functions for testing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import flags\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\nFLAGS = flags.FLAGS\n\n\ndef contains(list1, list2):\n  """"""Check if all items in list2 are in list1.\n\n  This function handles the case when the parameters are lists of np.arrays\n  (which wouldn\'t be handled by something like .issubset(...)\n\n  Args:\n    list1: List which may or may not contain list2.\n    list2: List to check if included in list 1.\n  Returns:\n    A boolean indicating whether list2 is contained in list1.\n  """"""\n  contains_result = True\n  for item2 in list2:\n    contains_result = contains_result and np.any(\n        [np.all(item2 == item1) for item1 in list1])\n    if not contains_result:\n      break\n  return contains_result\n\n\ndef test_src_dir_path(relative_path):\n  """"""Returns an absolute test srcdir path given a relative path.\n\n  Args:\n    relative_path: a path relative to tf_agents root.\n      e.g. ""environments/config"".\n\n  Returns:\n    An absolute path to the linked in runfiles.\n  """"""\n  return os.path.join(FLAGS.test_srcdir,\n                      \'tf_agents\',\n                      relative_path)\n\n\nclass TestCase(tf.test.TestCase):\n  """"""Base class for TF-Agents unit tests.""""""\n\n  def setUp(self):\n    super(TestCase, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    # Guard against tests calling gin.parse_config() without calling\n    # gin.clear_config(), which can cause nasty bugs that show up in a\n    # completely different test. See b/139088071 for example.\n    gin.clear_config()\n\n  def tearDown(self):\n    gin.clear_config()\n    super(TestCase, self).tearDown()\n\n  def initialize_v1_variables(self):\n    variables = tf.compat.v1.global_variables() + tf.compat.v1.local_variables()\n    self.evaluate(tf.compat.v1.variables_initializer(variables))\n\n\n# Main function so that users of `test_utils.TestCase` can also call\n# `test_utils.main()`.\ndef main():\n  tf.test.main()\n'"
tf_agents/utils/test_utils_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for utils/test_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom tf_agents.utils import test_utils\n\n\nclass TestUtilsTest(test_utils.TestCase):\n\n  def testBatchContainsSample(self):\n    batch = np.array([[1, 2], [3, 4]])\n    sample = np.array([3, 4])\n    self.assertTrue(test_utils.contains(batch, [sample]))\n\n  def testBatchDoesNotContainSample(self):\n    batch = np.array([[1, 2], [3, 4]])\n    sample = np.array([2, 4])\n    self.assertFalse(test_utils.contains(batch, [sample]))\n\n  def testBatchContainsBatch(self):\n    batch1 = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    batch2 = np.array([[3, 4], [3, 4], [1, 2]])\n    self.assertTrue(test_utils.contains(batch1, batch2))\n\n  def testBatchDoesNotContainBatch(self):\n    batch1 = np.array([[1, 2], [3, 4]])\n    batch2 = np.array([[1, 2], [5, 6]])\n    self.assertFalse(test_utils.contains(batch1, batch2))\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/utils/timer.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Timing utility for TF-Agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\n\nclass Timer(object):\n  """"""Context manager to time blocks of code.""""""\n\n  def __init__(self):\n    self._accumulator = 0\n    self._last = None\n\n  def __enter__(self):\n    self.start()\n\n  def __exit__(self, *args):\n    self.stop()\n\n  def start(self):\n    self._last = time.time()\n\n  def stop(self):\n    self._accumulator += time.time() - self._last\n\n  def value(self):\n    return self._accumulator\n\n  def reset(self):\n    self._accumulator = 0\n'"
tf_agents/utils/value_ops.py,25,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Methods for computing advantages and target values.\n""""""\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef discounted_return(rewards,\n                      discounts,\n                      final_value=None,\n                      time_major=True,\n                      provide_all_returns=True):\n  """"""Computes discounted return.\n\n  ```\n  Q_t = sum_{t\'=t}^T gamma^(t\'-t) * r_{t\'} + gamma^(T-t+1)*final_value.\n  ```\n\n  For details, see\n  ""Reinforcement Learning: An Introduction"" Second Edition\n  by Richard S. Sutton and Andrew G. Barto\n\n  Define abbreviations:\n  (B) batch size representing number of trajectories\n  (T) number of steps per trajectory\n\n  Args:\n    rewards: Tensor with shape [T, B] (or [T]) representing rewards.\n    discounts: Tensor with shape [T, B] (or [T]) representing discounts.\n    final_value: Tensor with shape [B] (or [1]) representing value estimate at\n      t=T. This is optional, when set, it allows final value to bootstrap the\n      reward to go computation. Otherwise it\'s zero.\n    time_major: A boolean indicating whether input tensors are time major. False\n      means input tensors have shape [B, T].\n    provide_all_returns: A boolean; if True, this will provide all of the\n      returns by time dimension; if False, this will only give the single\n      complete discounted return.\n\n  Returns:\n    If provide_all_returns is True:\n      A tensor with shape [T, B] (or [T]) representing the discounted returns.\n      Shape is [B, T] when time_major is false.\n    If provide_all_returns is False:\n      A tensor with shape [B] (or []) representing the discounted returns.\n  """"""\n  if not time_major:\n    with tf.name_scope(""to_time_major_tensors""):\n      discounts = tf.transpose(discounts)\n      rewards = tf.transpose(rewards)\n\n  if final_value is None:\n    final_value = tf.zeros_like(rewards[-1])\n\n  def discounted_return_fn(accumulated_discounted_reward, reward_discount):\n    reward, discount = reward_discount\n    return accumulated_discounted_reward * discount + reward\n\n  if provide_all_returns:\n    returns = tf.nest.map_structure(\n        tf.stop_gradient,\n        tf.scan(\n            fn=discounted_return_fn,\n            elems=(rewards, discounts),\n            reverse=True,\n            initializer=final_value))\n\n    if not time_major:\n      with tf.name_scope(""to_batch_major_tensors""):\n        returns = tf.transpose(returns)\n  else:\n    returns = tf.foldr(\n        fn=discounted_return_fn,\n        elems=(rewards, discounts),\n        initializer=final_value,\n        back_prop=False)\n\n  return tf.stop_gradient(returns)\n\n\ndef generalized_advantage_estimation(values,\n                                     final_value,\n                                     discounts,\n                                     rewards,\n                                     td_lambda=1.0,\n                                     time_major=True):\n  """"""Computes generalized advantage estimation (GAE).\n\n  For theory, see\n  ""High-Dimensional Continuous Control Using Generalized Advantage Estimation""\n  by John Schulman, Philipp Moritz et al.\n  See https://arxiv.org/abs/1506.02438 for full paper.\n\n  Define abbreviations:\n    (B) batch size representing number of trajectories\n    (T) number of steps per trajectory\n\n  Args:\n    values: Tensor with shape [T, B] representing value estimates.\n    final_value: Tensor with shape [B] representing value estimate at t=T.\n    discounts: Tensor with shape [T, B] representing discounts received by\n      following the behavior policy.\n    rewards: Tensor with shape [T, B] representing rewards received by following\n      the behavior policy.\n    td_lambda: A float32 scalar between [0, 1]. It\'s used for variance reduction\n      in temporal difference.\n    time_major: A boolean indicating whether input tensors are time major.\n      False means input tensors have shape [B, T].\n\n  Returns:\n    A tensor with shape [T, B] representing advantages. Shape is [B, T] when\n    time_major is false.\n  """"""\n\n  if not time_major:\n    with tf.name_scope(""to_time_major_tensors""):\n      discounts = tf.transpose(discounts)\n      rewards = tf.transpose(rewards)\n      values = tf.transpose(values)\n\n  with tf.name_scope(""gae""):\n\n    next_values = tf.concat(\n        [values[1:], tf.expand_dims(final_value, 0)], axis=0)\n    delta = rewards + discounts * next_values - values\n    weighted_discounts = discounts * td_lambda\n\n    def weighted_cumulative_td_fn(accumulated_td, reversed_weights_td_tuple):\n      weighted_discount, td = reversed_weights_td_tuple\n      return td + weighted_discount * accumulated_td\n\n    advantages = tf.nest.map_structure(\n        tf.stop_gradient,\n        tf.scan(\n            fn=weighted_cumulative_td_fn,\n            elems=(weighted_discounts, delta),\n            initializer=tf.zeros_like(final_value),\n            reverse=True))\n\n  if not time_major:\n    with tf.name_scope(""to_batch_major_tensors""):\n      advantages = tf.transpose(advantages)\n\n  return tf.stop_gradient(advantages)\n'"
tf_agents/utils/value_ops_test.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.utils.generalized_advantage_estimation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import value_ops\n\n\ndef _naive_gae_as_ground_truth(discounts, rewards, values, final_value,\n                               td_lambda):\n  """"""A naive GAE closely resembles equation (16) in the paper.\n\n  Slow, for testing purpose only.\n  For full paper see https://arxiv.org/abs/1506.02438.pdf\n  Args:\n    discounts: `np.array` with shape [T, B].\n    rewards: `np.array` with shape [T, B].\n    values: `np.array` with shape [T, B].\n    final_value: `np.array` with shape [B].\n    td_lambda: A float scalar.\n\n  Returns:\n    A `np.array` with shape[T, B] representing the advantages.\n  """"""\n\n  episode_length = len(values)\n  values_t_puls_1 = np.concatenate([values, final_value[None, :]], axis=0)\n\n  delta_v = [\n      (rewards[t] + discounts[t] * values_t_puls_1[t + 1] - values_t_puls_1[t])\n      for t in range(episode_length)\n  ]\n  weighted_discounts = discounts * td_lambda\n  advantages = []\n  for s in range(episode_length):\n    advantage = np.copy(delta_v[s])\n    for t in range(s + 1, episode_length):\n      advantage += np.prod(weighted_discounts[s:t], axis=0) * delta_v[t]\n    advantages.append(advantage)\n\n  return np.array(advantages)\n\n\ndef _numpy_discounted_return(rewards, discounts, final_value):\n  """"""A naive reward to do implemented in python.\n\n  Slow, for testing purpose only.\n  Args:\n    rewards: `np.array` with shape [T, B].\n    discounts: `np.array` with shape [T, B].\n    final_value: `np.array` with shape [B].\n\n  Returns:\n    A `np.array` with shape[T, B] representing the target values.\n  """"""\n  if final_value is None:\n    final_value = np.zeros_like(rewards[-1])\n\n  discounted_returns = np.zeros_like(rewards)\n  accumulated_rewards = final_value\n  for t in reversed(range(len(rewards))):\n    discounted_returns[t] = rewards[t] + discounts[t] * accumulated_rewards\n    accumulated_rewards = discounted_returns[t]\n  return discounted_returns\n\n\nclass DiscountedReturnTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'single_batch_single_step_without_final_value\', 1, 1, False),\n      (\'single_batch_single_step_with_final_value\', 1, 1, True),\n      (\'multiple_batch_multiple_step_without_final_value\', 7, 9, False),\n      (\'multiple_batch_multiple_step_with_final_value\', 7, 9, True),\n  )\n  def testDiscountedReturnIsCorrectlyComputed(self,\n                                              num_time_steps,\n                                              batch_size,\n                                              with_final_value):\n    rewards = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    discounts = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    final_value = np.random.rand(batch_size).astype(\n        np.float32) if with_final_value else None\n\n    discounted_return = value_ops.discounted_return(\n        rewards=rewards, discounts=discounts, final_value=final_value)\n\n    single_discounted_return = value_ops.discounted_return(\n        rewards=rewards, discounts=discounts, final_value=final_value,\n        provide_all_returns=False)\n\n    expected = _numpy_discounted_return(\n        rewards=rewards, discounts=discounts, final_value=final_value)\n\n    self.assertAllClose(discounted_return, expected)\n    self.assertAllClose(single_discounted_return, expected[0])\n\n  @parameterized.named_parameters(\n      (\'single_batch_single_step_without_final_value\', 1, 1, False),\n      (\'single_batch_single_step_with_final_value\', 1, 1, True),\n      (\'multiple_batch_multiple_step_without_final_value\', 7, 9, False),\n      (\'multiple_batch_multiple_step_with_final_value\', 7, 9, True),\n  )\n  def testTimeMajorBatchMajorDiscountedReturnsAreSame(self,\n                                                      num_time_steps,\n                                                      batch_size,\n                                                      with_final_value):\n    rewards = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    discounts = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    final_value = np.random.rand(batch_size).astype(\n        np.float32) if with_final_value else None\n\n    time_major_discounted_return = value_ops.discounted_return(\n        rewards=rewards,\n        discounts=discounts,\n        final_value=final_value)\n\n    batch_major_discounted_return = value_ops.discounted_return(\n        rewards=tf.transpose(rewards),\n        discounts=tf.transpose(discounts),\n        final_value=final_value,\n        time_major=False)\n\n    self.assertAllClose(time_major_discounted_return,\n                        tf.transpose(batch_major_discounted_return))\n\n    single_time_major_discounted_return = value_ops.discounted_return(\n        rewards=rewards,\n        discounts=discounts,\n        final_value=final_value,\n        provide_all_returns=False)\n\n    single_batch_major_discounted_return = value_ops.discounted_return(\n        rewards=tf.transpose(rewards),\n        discounts=tf.transpose(discounts),\n        final_value=final_value,\n        time_major=False,\n        provide_all_returns=False)\n\n    self.assertAllClose(single_time_major_discounted_return,\n                        time_major_discounted_return[0])\n    self.assertAllClose(single_batch_major_discounted_return,\n                        time_major_discounted_return[0])\n\n  def testDiscountedReturnWithFinalValueMatchPrecomputedResult(self):\n    discounted_return = value_ops.discounted_return(\n        rewards=tf.constant([1] * 9, dtype=tf.float32),\n        discounts=tf.constant(\n            [1, 1, 1, 1, 0, 0.9, 0.9, 0.9, 0.9], dtype=tf.float32),\n        final_value=tf.constant(8, dtype=tf.float32))\n\n    expected = [\n        5, 4, 3, 2, 1, 8 * 0.9**4 + 3.439, 8 * 0.9**3 + 2.71, 8 * 0.9**2 + 1.9,\n        8 * 0.9 + 1\n    ]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(discounted_return, expected)\n\n\nclass GeneralizedAdvantageEstimationTest(tf.test.TestCase,\n                                         parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'single_batch_single_step\', 1, 1, 0.7),\n      (\'multiple_batch_multiple_step\', 7, 9, 0.7),\n      (\'multiple_batch_multiple_step_lambda_0\', 7, 9, 0.),\n      (\'multiple_batch_multiple_step_lambda_1\', 7, 9, 1.),\n  )\n  def testAdvantagesAreCorrectlyComputed(self,\n                                         batch_size,\n                                         num_time_steps,\n                                         td_lambda):\n    rewards = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    discounts = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    values = np.random.rand(num_time_steps, batch_size).astype(np.float32)\n    final_value = np.random.rand(batch_size).astype(np.float32)\n    ground_truth = _naive_gae_as_ground_truth(\n        discounts=discounts,\n        rewards=rewards,\n        values=values,\n        final_value=final_value,\n        td_lambda=td_lambda)\n\n    advantages = value_ops.generalized_advantage_estimation(\n        discounts=discounts,\n        rewards=rewards,\n        values=values,\n        final_value=final_value,\n        td_lambda=td_lambda)\n\n    self.assertAllClose(advantages, ground_truth)\n\n  def testAdvantagesMatchPrecomputedResult(self):\n    advantages = value_ops.generalized_advantage_estimation(\n        discounts=tf.constant([[1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0],\n                               [1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0]]),\n        rewards=tf.fill([2, 9], 1.0),\n        values=tf.fill([2, 9], 3.0),\n        final_value=tf.fill([2], 3.0),\n        td_lambda=0.95,\n        time_major=False)\n\n    # Precomputed according to equation (16) in paper.\n    ground_truth = tf.constant([[\n        2.0808625, 1.13775, 0.145, -0.9, -2.0, 0.56016475, -0.16355, -1.01, -2.0\n    ], [\n        2.0808625, 1.13775, 0.145, -0.9, -2.0, 0.56016475, -0.16355, -1.01, -2.0\n    ]])\n\n    self.assertAllClose(advantages, ground_truth)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/utils/xla.py,16,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""XLA utilities for TF-Agents.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n# Dictionary mapping a device name to a python bool.\n_IS_XLA_AVAILABLE = {}\n\n\ndef is_xla_available():\n  """"""Is XLA compilation available for the current device context?""""""\n\n  global _IS_XLA_AVAILABLE\n  # There\'s unfortunately no cleaner way to get the device other than creating a\n  # new op and querying it.\n  with tf.name_scope(""is_xla_available""):\n    device = tf.constant(0.0).device\n  if device not in _IS_XLA_AVAILABLE:\n    try:\n      # Take ourselves outside of any tf.function calls.\n      with tf.init_scope():\n        # Create temporary xla subgraph\n        with tf.compat.v1.Graph().as_default():\n          # We\'ll use a session so we can be compatible with both TF1 and TF2\n          with tf.compat.v1.Session() as sess:\n            # Check for XLA on the given device.\n            with tf.device(device):\n              sess.run(tf.xla.experimental.compile(lambda: tf.constant(0.0)))\n    except (ValueError, tf.errors.InvalidArgumentError):\n      _IS_XLA_AVAILABLE[device] = False\n    else:\n      _IS_XLA_AVAILABLE[device] = True\n  return _IS_XLA_AVAILABLE[device]\n\n\ndef compile_in_graph_mode(fn):\n  """"""Decorator for XLA compilation iff in graph mode and XLA is available.\n\n  Example:\n\n  ```python\n  @compile_in_graph_mode\n  def fn(x, y, z):\n    return {\'a\': x + y, \'b\': y * z}\n\n  @common.function\n  def calls_fn(inputs):\n    return fn(inputs.x, inputs.y, inputs.z)\n\n  # Call calls_fn().\n\n  Args:\n    fn: A callable that accepts a list of possibly nested tensor arguments.\n      kwargs and inputs taking the value `None` are not supported.  Non-tensor\n      arguments are treated as nest objects, and leaves are converted to\n      tensors.\n\n  Returns:\n    A function that, when called, checks if XLA is compiled in and enabled\n    for the current device, and that it\'s being built in graph mode, and\n    returns an XLA-compiled version of `fn`.  If in eager mode, or XLA\n    is not available, then `fn` is called directly.\n  ```\n  """"""\n\n  @functools.wraps(fn)\n  def _compiled(*args, **kwargs):\n    """"""Helper function for optionally XLA compiling `fn`.""""""\n    if kwargs:\n      raise ValueError(\n          ""kwargs are not supported for functions that are XLA-compiled, ""\n          ""but saw kwargs: {}"".format(kwargs))\n    args = tf.nest.map_structure(tf.convert_to_tensor, args)\n    if tf.compat.v1.executing_eagerly() or not is_xla_available():\n      return fn(*args)\n    else:\n      # The flattening/unpacking is necessary because xla compile only allows\n      # flat inputs and outputs: no substructures.  But we provide support for\n      # nested inputs and outputs.\n      outputs_for_structure = [None]\n      flat_args = tf.nest.flatten(args)\n      def _fn(*flattened_args):\n        unflattened_args = tf.nest.pack_sequence_as(args, flattened_args)\n        fn_outputs = fn(*unflattened_args)\n        outputs_for_structure[0] = fn_outputs\n        return tf.nest.flatten(fn_outputs)\n      outputs = tf.xla.experimental.compile(_fn, flat_args)\n      return tf.nest.pack_sequence_as(outputs_for_structure[0], outputs)\n\n  return _compiled\n'"
tf_agents/utils/xla_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.utils.xla.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.utils import xla\n\n\nclass XLATest(tf.test.TestCase):\n\n  def setUp(self):\n    super(XLATest, self).setUp()\n    tf.config.set_soft_device_placement(True)\n\n  def testIsXLAAvailable(self):\n    available = False\n    try:\n      self.evaluate(tf.xla.experimental.compile(lambda: tf.constant(0.0)))\n      available = True\n    except:  # pylint: disable=bare-except\n      pass\n    self.assertEqual(available, xla.is_xla_available())\n\n  def testCompileInGraphMode(self):\n    if not xla.is_xla_available():\n      self.skipTest(\'Skipping test: XLA is not available.\')\n\n    @xla.compile_in_graph_mode\n    def add(x, y):\n      return x + y\n\n    z = add(1.0, 2.0)\n    self.assertAllClose(3.0, self.evaluate(z))\n\n    @xla.compile_in_graph_mode\n    def add_subtract(x, y):\n      return {\'add\': x + y, \'sub\': x - y}\n\n    z = add_subtract(1.0, 2.0)\n    self.assertAllClose({\'add\': 3.0, \'sub\': -1.0}, self.evaluate(z))\n\n    @xla.compile_in_graph_mode\n    def add_divide(x, yz):\n      return x + yz[\'y\'] / yz[\'z\']\n\n    z = add_divide(1.0, {\'y\': 2.0, \'z\': 3.0})\n    self.assertAllClose(1.0 + 2.0 / 3.0, self.evaluate(z))\n\n    if not tf.compat.v1.executing_eagerly():\n      # TF2 seems to have trouble with soft device placement (both in eager and\n      # tf.function mode); and here we\'re specifically testing what happens when\n      # XLA is not available, e.g., because we didn\'t compile with GPU support.\n      with tf.device(\'/gpu:0\'):\n        z = add_subtract(1.0, 2.0)\n      self.assertAllClose({\'add\': 3.0, \'sub\': -1.0}, self.evaluate(z))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/behavioral_cloning/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Behavioral Cloning agent.""""""\nfrom tf_agents.agents.behavioral_cloning import behavioral_cloning_agent\n'"
tf_agents/agents/behavioral_cloning/behavioral_cloning_agent.py,19,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Behavioral Cloning Agents.\n\nImplements generic form of behavioral cloning.\n\nUsers must provide their own loss functions.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import q_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\n\n\nclass BehavioralCloningLossInfo(collections.namedtuple(\n    \'BehavioralCloningLossInfo\', (\'loss\',))):\n  """"""Stores a per-batch-entry loss value.""""""\n  pass\n\n\n@gin.configurable\nclass BehavioralCloningAgent(tf_agent.TFAgent):\n  """"""An behavioral cloning Agent.\n\n  Implements behavioral cloning, wherein the network learns to clone\n  given experience.  Users must provide their own loss functions. Note this\n  implementation will use a QPolicy. To use with other policies subclass this\n  agent and override the `_get_policies` method. Note the cloning_network must\n  match the requirements of the generated policies.\n\n  Behavioral cloning was proposed in the following articles:\n\n  Pomerleau, D.A., 1991. Efficient training of artificial neural networks for\n  autonomous navigation. Neural Computation, 3(1), pp.88-97.\n\n  Russell, S., 1998, July. Learning agents for uncertain environments.\n  In Proceedings of the eleventh annual conference on Computational learning\n  theory (pp. 101-103). ACM.\n  """"""\n\n  # TODO(b/127327645): This causes a loop failure when RNNs are enabled.\n  _enable_functions = False\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      cloning_network: network.Network,\n      optimizer: types.Optimizer,\n      num_outer_dims: int = 1,\n      # Params for training.\n      epsilon_greedy: types.Float = 0.1,\n      loss_fn: types.LossFn = None,\n      gradient_clipping: Optional[types.Float] = None,\n      # Params for debugging.\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = None):\n    """"""Creates an behavioral cloning Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      cloning_network: A tf_agents.network.Network to be used by the agent.\n        The network will be called as\n\n          ```\n          network(observation, step_type, network_state=initial_state)\n          ```\n        and must return a 2-tuple with elements `(output, next_network_state)`\n        where `output` will be passed as the first argument to `loss_fn`, and\n        used by a `Policy`.  Input tensors will be shaped `[batch, time, ...]`\n        when training, and they will be shaped `[batch, ...]` when the network\n        is called within a `Policy`.  If `cloning_network` has an empty network\n        state, then for training `time` will always be `1`\n        (individual examples).\n      optimizer: The optimizer to use for training.\n      num_outer_dims: The number of outer dimensions for the agent. Must be\n        either 1 or 2. If 2, training will require both a batch_size and time\n        dimension on every Tensor; if 1, training will require only a batch_size\n        outer dimension.\n      epsilon_greedy: probability of choosing a random action in the default\n        epsilon-greedy collect policy (used only if a wrapper is not provided to\n        the collect_policy method).\n      loss_fn: A function for computing the error between the output of the\n        cloning network and the action that was taken. If None, the loss\n        depends on the action dtype.  If the dtype is integer, then `loss_fn`\n        is\n\n        ```python\n        def loss_fn(logits, action):\n          return tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(\n            labels=action - action_spec.minimum, logits=logits)\n        ```\n\n        If the dtype is floating point, the loss is\n        `tf.math.squared_difference`.\n\n        `loss_fn` must return a loss value for each element of the batch.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If `action_spec` contains more than one action, but a custom\n        `loss_fn` is not provided.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    flat_action_spec = tf.nest.flatten(action_spec)\n    self._num_actions = [\n        spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n    ]\n\n    if tf.nest.is_nested(action_spec) and not loss_fn:\n      raise ValueError(\n          \'When using nested actions (i.e., `tf.nest.is_nested(action_spec)` \'\n          \'is True), a custom loss_fn must be provided.\')\n\n    self._nested_actions = len(flat_action_spec) > 1\n\n    if loss_fn is None:\n      loss_fn = self._get_default_loss_fn(flat_action_spec[0])\n\n    self._cloning_network = cloning_network\n    self._loss_fn = loss_fn\n    self._epsilon_greedy = epsilon_greedy\n    self._optimizer = optimizer\n    self._gradient_clipping = gradient_clipping\n\n    policy, collect_policy = self._get_policies(time_step_spec, action_spec,\n                                                cloning_network)\n\n    super(BehavioralCloningAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=None,\n        num_outer_dims=num_outer_dims,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _get_default_loss_fn(self, spec):\n    if spec.dtype.is_floating:\n      return tf.math.squared_difference\n    if spec.shape.rank > 1:\n      raise NotImplementedError(\n          \'The default loss_fn only supports scalar, unnested integer actions.\')\n    # TODO(ebrevdo): Maybe move the subtraction of the minimum into a\n    # self._label_fn and rewrite this.\n    def xent_loss_fn(logits, actions):\n      # Subtract the minimum so that we get a proper cross entropy loss on\n      # [0, maximum - minimum).\n      return tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=actions - spec.minimum)\n\n    return xent_loss_fn\n\n  def _get_policies(self, time_step_spec, action_spec, cloning_network):\n    policy = q_policy.QPolicy(\n        time_step_spec, action_spec, q_network=self._cloning_network)\n    collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n        policy, epsilon=self._epsilon_greedy)\n    policy = greedy_policy.GreedyPolicy(policy)\n    return policy, collect_policy\n\n  def _initialize(self):\n    return tf.no_op()\n\n  def _train(self, experience, weights=None):\n    loss_info = self._loss(experience, weights=weights)\n\n    transform_grads_fn = None\n    if self._gradient_clipping is not None:\n      transform_grads_fn = eager_utils.clip_gradient_norms_fn(\n          self._gradient_clipping)\n\n    loss_info = eager_utils.create_train_step(\n        loss_info,\n        self._optimizer,\n        total_loss_fn=lambda loss_info: loss_info.loss,\n        global_step=self.train_step_counter,\n        transform_grads_fn=transform_grads_fn,\n        summarize_gradients=self._summarize_grads_and_vars,\n        variables_to_train=lambda: self._cloning_network.trainable_weights,\n    )\n\n    return loss_info\n\n  @eager_utils.future_in_eager_mode\n  # TODO(b/79688437): Figure out how to enable defun for Eager mode.\n  # @tfe.defun\n  def _loss(self, experience, weights=None):\n    """"""Computes loss for behavioral cloning.\n\n    Args:\n      experience: A `Trajectory` containing experience.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.\n\n    Returns:\n      loss: A `LossInfo` struct.\n\n    Raises:\n      ValueError:\n        If the number of actions is greater than 1.\n    """"""\n    with tf.name_scope(\'loss\'):\n      if self._nested_actions:\n        actions = experience.action\n      else:\n        actions = tf.nest.flatten(experience.action)[0]\n\n      batch_size = (\n          tf.compat.dimension_value(experience.step_type.shape[0]) or\n          tf.shape(experience.step_type)[0])\n      logits, _ = self._cloning_network(\n          experience.observation,\n          experience.step_type,\n          training=True,\n          network_state=self._cloning_network.get_initial_state(batch_size))\n\n      error = self._loss_fn(logits, actions)\n      error_dtype = tf.nest.flatten(error)[0].dtype\n      boundary_weights = tf.cast(~experience.is_boundary(), error_dtype)\n      error *= boundary_weights\n\n      if nest_utils.is_batched_nested_tensors(\n          experience.action, self.action_spec, num_outer_dims=2):\n        # Do a sum over the time dimension.\n        error = tf.reduce_sum(input_tensor=error, axis=1)\n\n      # Average across the elements of the batch.\n      # Note: We use an element wise loss above to ensure each element is always\n      #   weighted by 1/N where N is the batch size, even when some of the\n      #   weights are zero due to boundary transitions. Weighting by 1/K where K\n      #   is the actual number of non-zero weight would artificially increase\n      #   their contribution in the loss. Think about what would happen as\n      #   the number of boundary samples increases.\n\n      agg_loss = common.aggregate_losses(\n          per_example_loss=error,\n          sample_weight=weights,\n          regularization_loss=self._cloning_network.losses)\n      total_loss = agg_loss.total_loss\n\n      dict_losses = {\'loss\': agg_loss.weighted,\n                     \'reg_loss\': agg_loss.regularization,\n                     \'total_loss\': total_loss}\n\n      common.summarize_scalar_dict(dict_losses,\n                                   step=self.train_step_counter,\n                                   name_scope=\'Losses/\')\n\n      if self._summarize_grads_and_vars:\n        with tf.name_scope(\'Variables/\'):\n          for var in self._cloning_network.trainable_weights:\n            tf.compat.v2.summary.histogram(\n                name=var.name.replace(\':\', \'_\'),\n                data=var,\n                step=self.train_step_counter)\n\n      if self._debug_summaries:\n        common.generate_tensor_summaries(\'errors\', error,\n                                         self.train_step_counter)\n\n      return tf_agent.LossInfo(total_loss,\n                               BehavioralCloningLossInfo(loss=error))\n'"
tf_agents/agents/behavioral_cloning/behavioral_cloning_agent_test.py,46,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for agents.behavioral_cloning.behavioral_cloning_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.behavioral_cloning import behavioral_cloning_agent\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n# Number of times to train in test loops.\nTRAIN_ITERATIONS = 10\n\n\ndef create_arbitrary_trajectory():\n  """"""Creates an arbitrary trajectory for unit testing BehavioralCloningAgent.\n\n  This trajectory contains Tensors shaped `[6, 1, ...]` where `6` is the number\n  of time steps and `1` is the batch.\n\n  Observations are unbounded but actions are bounded to take values within\n  `[1, 2]`. The action space is discrete.\n\n  Policy info is not provided, as it is not used in BehavioralCloningAgent.\n\n  Returns:\n    traj: a hard coded `Trajectory` that matches time_step_spec and action_spec.\n    time_step_spec: a hard coded time spec.\n    action_spec: a hard coded action spec.\n  """"""\n\n  time_step_spec = ts.time_step_spec(\n      tensor_spec.TensorSpec([], tf.int32, name=\'observation\'))\n  action_spec = tensor_spec.BoundedTensorSpec([],\n                                              tf.int32,\n                                              minimum=1,\n                                              maximum=2,\n                                              name=\'action\')\n\n  observation = tf.constant([[1], [2], [3], [4], [5], [6]], dtype=tf.int32)\n  action = tf.constant([[1], [1], [1], [2], [2], [2]], dtype=tf.int32)\n  reward = tf.constant([[0], [0], [0], [0], [0], [0]], dtype=tf.float32)\n  step_type = tf.constant([[0], [1], [2], [0], [1], [2]], dtype=tf.int32)\n  next_step_type = tf.constant([[1], [2], [0], [1], [2], [0]], dtype=tf.int32)\n  discount = tf.constant([[1], [1], [1], [1], [1], [1]], dtype=tf.float32)\n\n  traj = trajectory.Trajectory(\n      observation=observation,\n      action=action,\n      policy_info=(),\n      reward=reward,\n      step_type=step_type,\n      next_step_type=next_step_type,\n      discount=discount,\n  )\n  return traj, time_step_spec, action_spec\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, unused_observation_spec, action_spec, name=None):\n    super(DummyNet, self).__init__(\n        unused_observation_spec, state_spec=(), name=name)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant([[2, 1],\n                                                                   [1, 1]]),\n            bias_initializer=tf.compat.v1.initializers.constant([[1], [1]]),\n            dtype=tf.float32)\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs[0], tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass ActorBCAgent(behavioral_cloning_agent.BehavioralCloningAgent):\n  """"""BehavioralCloningAgent for Actor policies/networks.""""""\n\n  def _get_policies(self, time_step_spec, action_spec, cloning_network):\n    policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        actor_network=cloning_network,\n        clip=True)\n\n    return policy, policy\n\n\nclass BehavioralCloningAgentTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(BehavioralCloningAgentTest, self).setUp()\n    self._obs_spec = [tensor_spec.TensorSpec([2], tf.float32)]\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1)\n    self._observation_spec = self._time_step_spec.observation\n\n  def testCreateAgent(self):\n    cloning_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        self._time_step_spec,\n        self._action_spec,\n        cloning_network=cloning_net,\n        optimizer=None)\n    self.assertIsNotNone(agent.policy)\n\n  def testCreateAgentNestSizeChecks(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1),\n        tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1)\n    ]\n\n    cloning_net = DummyNet(self._observation_spec, action_spec)\n    with self.assertRaisesRegexp(ValueError, \'.*nested actions.*\'):\n      behavioral_cloning_agent.BehavioralCloningAgent(\n          self._time_step_spec,\n          action_spec,\n          cloning_network=cloning_net,\n          optimizer=None)\n\n  def testCreateAgentWithMultipleActionsAndCustomLossFn(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1),\n        tensor_spec.BoundedTensorSpec([], tf.int32, 0, 1)\n    ]\n\n    cloning_net = DummyNet(self._observation_spec, action_spec)\n\n    # We create an ActorBCAgent here instead of a BehavioralCloningAgent since\n    # QPolicy currently doesn\'t accept action_specs with multiple actions.\n    ActorBCAgent(\n        self._time_step_spec,\n        action_spec,\n        cloning_network=cloning_net,\n        optimizer=None,\n        loss_fn=lambda logits, actions: 0)\n\n  def testCreateAgentWithListActionSpec(self):\n    action_spec = [tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)]\n    cloning_net = DummyNet(self._observation_spec, action_spec)\n    with self.assertRaisesRegexp(ValueError, \'.*nested actions.*\'):\n      behavioral_cloning_agent.BehavioralCloningAgent(\n          self._time_step_spec, action_spec,\n          cloning_network=cloning_net,\n          optimizer=None)\n\n  def testCreateAgentDimChecks(self):\n    action_spec = tensor_spec.BoundedTensorSpec([1, 2], tf.int32, 0, 1)\n    cloning_net = DummyNet(self._observation_spec, action_spec)\n    with self.assertRaisesRegexp(NotImplementedError, \'.*scalar, unnested.*\'):\n      behavioral_cloning_agent.BehavioralCloningAgent(\n          self._time_step_spec, action_spec,\n          cloning_network=cloning_net,\n          optimizer=None)\n\n  # TODO(kbanoop): Add a test where the target network has different values.\n  def testLoss(self):\n    cloning_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        self._time_step_spec,\n        self._action_spec,\n        cloning_network=cloning_net,\n        optimizer=None)\n\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n\n    experience = trajectory.first(\n        observation=observations,\n        action=actions,\n        policy_info=(),\n        reward=rewards,\n        discount=discounts)\n    loss_info = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    total_loss, _ = self.evaluate(loss_info)\n\n    expected_loss = tf.reduce_mean(\n        input_tensor=tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=cloning_net(observations)[0], labels=actions))\n\n    self.assertAllClose(total_loss, expected_loss)\n\n  @parameterized.named_parameters((\'TrainOnMultipleSteps\', False),\n                                  (\'TrainOnSingleStep\', True))\n  def testTrainWithNN(self, is_convert):\n    # Hard code a trajectory shaped (time=6, batch=1, ...).\n    traj, time_step_spec, action_spec = create_arbitrary_trajectory()\n\n    if is_convert:\n      # Convert to single step trajectory of shapes (batch=6, 1, ...).\n      traj = tf.nest.map_structure(common.transpose_batch_time, traj)\n    cloning_net = q_network.QNetwork(\n        time_step_spec.observation, action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        time_step_spec,\n        action_spec,\n        cloning_network=cloning_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001),\n        num_outer_dims=2)\n    # Disable clipping to make sure we can see the difference in behavior\n    agent.policy._clip = False\n    # TODO(b/123883319)\n    if tf.executing_eagerly():\n      train_and_loss = lambda: agent.train(traj)\n    else:\n      train_and_loss = agent.train(traj)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    initial_loss = self.evaluate(train_and_loss).loss\n    for _ in range(TRAIN_ITERATIONS - 1):\n      loss = self.evaluate(train_and_loss).loss\n\n    # We don\'t necessarily converge to the same actions as in trajectory after\n    # 10 steps of an untuned optimizer, but the loss should go down.\n    self.assertGreater(initial_loss, loss)\n\n  def testTrainWithSingleOuterDimension(self):\n    # Hard code a trajectory shaped (time=6, batch=1, ...).\n    traj, time_step_spec, action_spec = create_arbitrary_trajectory()\n    # Remove the batch dimension so there is only one outer dimension.\n    traj = tf.nest.map_structure(lambda x: tf.squeeze(x, axis=1), traj)\n\n    cloning_net = q_network.QNetwork(\n        time_step_spec.observation, action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        time_step_spec,\n        action_spec,\n        cloning_network=cloning_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.01))\n    # Disable clipping to make sure we can see the difference in behavior\n    agent.policy._clip = False\n    # Remove policy_info, as BehavioralCloningAgent expects none.\n    traj = traj.replace(policy_info=())\n    # TODO(b/123883319)\n    if tf.executing_eagerly():\n      train_and_loss = lambda: agent.train(traj)\n    else:\n      train_and_loss = agent.train(traj)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for _ in range(TRAIN_ITERATIONS):\n      self.evaluate(train_and_loss)\n    # Note that we skip the TrajectoryReplay since it requires a time dimension.\n\n  def testTrainWithRNN(self):\n    # Hard code a trajectory shaped (time=6, batch=1, ...).\n    traj, time_step_spec, action_spec = create_arbitrary_trajectory()\n    cloning_net = q_rnn_network.QRnnNetwork(\n        time_step_spec.observation, action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        time_step_spec,\n        action_spec,\n        cloning_network=cloning_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.01),\n        num_outer_dims=2)\n    # Disable clipping to make sure we can see the difference in behavior\n    agent.policy._clip = False\n    # Remove policy_info, as BehavioralCloningAgent expects none.\n    traj = traj.replace(policy_info=())\n    # TODO(b/123883319)\n    if tf.executing_eagerly():\n      train_and_loss = lambda: agent.train(traj)\n    else:\n      train_and_loss = agent.train(traj)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    initial_loss = self.evaluate(train_and_loss).loss\n    for _ in range(TRAIN_ITERATIONS - 1):\n      loss = self.evaluate(train_and_loss).loss\n\n    # We don\'t necessarily converge to the same actions as in trajectory after\n    # 10 steps of an untuned optimizer, but the loss should go down.\n    self.assertGreater(initial_loss, loss)\n\n  def testPolicy(self):\n    cloning_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        self._time_step_spec,\n        self._action_spec,\n        cloning_network=cloning_net,\n        optimizer=None)\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    # Batch size 2.\n    self.assertAllEqual(\n        [2] + self._action_spec.shape.as_list(),\n        action_step.action.shape,\n    )\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(all(actions_ <= self._action_spec.maximum))\n    self.assertTrue(all(actions_ >= self._action_spec.minimum))\n\n  def testInitializeRestoreAgent(self):\n    cloning_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = behavioral_cloning_agent.BehavioralCloningAgent(\n        self._time_step_spec,\n        self._action_spec,\n        cloning_network=cloning_net,\n        optimizer=None)\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    checkpoint = tf.train.Checkpoint(agent=agent)\n\n    latest_checkpoint = tf.train.latest_checkpoint(self.get_temp_dir())\n    checkpoint_load_status = checkpoint.restore(latest_checkpoint)\n\n    with self.cached_session() as sess:\n      checkpoint_load_status.initialize_or_restore(sess)\n      self.assertAllEqual(sess.run(action_step.action), [0, 0])\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/agents/categorical_dqn/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Categorical DQN (C51) agent.""""""\nfrom tf_agents.agents.categorical_dqn import categorical_dqn_agent\n'"
tf_agents/agents/categorical_dqn/categorical_dqn_agent.py,70,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Categorical DQN Agent.\n\nImplements the Categorical DQN agent from\n\n""A Distributional Perspective on Reinforcement Learning""\n  Bellemare et al., 2017\n  https://arxiv.org/abs/1707.06887\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.policies import boltzmann_policy\nfrom tf_agents.policies import categorical_q_policy\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import value_ops\n\n\n@gin.configurable\nclass CategoricalDqnAgent(dqn_agent.DqnAgent):\n  """"""A Categorical DQN Agent based on the DQN Agent.""""""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      categorical_q_network: network.Network,\n      optimizer: types.Optimizer,\n      observation_and_action_constraint_splitter: Optional[\n          types.Splitter] = None,\n      min_q_value: types.Float = -10.0,\n      max_q_value: types.Float = 10.0,\n      epsilon_greedy: types.Float = 0.1,\n      n_step_update: int = 1,\n      boltzmann_temperature: Optional[types.Float] = None,\n      # Params for target network updates\n      target_categorical_q_network: Optional[network.Network] = None,\n      target_update_tau: types.Float = 1.0,\n      target_update_period: types.Int = 1,\n      # Params for training.\n      td_errors_loss_fn: Optional[types.LossFn] = None,\n      gamma: types.Float = 1.0,\n      reward_scale_factor: types.Float = 1.0,\n      gradient_clipping: Optional[types.Float] = None,\n      # Params for debugging\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = None):\n    """"""Creates a Categorical DQN Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A `BoundedTensorSpec` representing the actions.\n      categorical_q_network: A categorical_q_network.CategoricalQNetwork that\n        returns the q_distribution for each action.\n      optimizer: The optimizer to use for training.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `q_network` is compatible with the network-specific\n        half of the output of the `observation_and_action_constraint_splitter`.\n        In particular, `observation_and_action_constraint_splitter` will be\n        called on the observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n      min_q_value: A float specifying the minimum Q-value, used for setting up\n        the support.\n      max_q_value: A float specifying the maximum Q-value, used for setting up\n        the support.\n      epsilon_greedy: probability of choosing a random action in the default\n        epsilon-greedy collect policy (used only if a wrapper is not provided to\n        the collect_policy method).\n      n_step_update: The number of steps to consider when computing TD error and\n        TD loss. Defaults to single-step updates. Note that this requires the\n        user to call train on Trajectory objects with a time dimension of\n        `n_step_update + 1`. However, note that we do not yet support\n        `n_step_update > 1` in the case of RNNs (i.e., non-empty\n        `q_network.state_spec`).\n      boltzmann_temperature: Temperature value to use for Boltzmann sampling of\n        the actions during data collection. The closer to 0.0, the higher the\n        probability of choosing the best action.\n      target_categorical_q_network: (Optional.)  A `tf_agents.network.Network`\n        to be used as the target network during Q learning.  Every\n        `target_update_period` train steps, the weights from\n        `categorical_q_network` are copied (possibly with smoothing via\n        `target_update_tau`) to `target_categorical_q_network`.\n\n        If `target_categorical_q_network` is not provided, it is created by\n        making a copy of `categorical_q_network`, which initializes a new\n        network with the same structure and its own layers and weights.\n\n        Network copying is performed via the `Network.copy` superclass method,\n        and may inadvertently lead to the resulting network to share weights\n        with the original.  This can happen if, for example, the original\n        network accepted a pre-built Keras layer in its `__init__`, or\n        accepted a Keras layer that wasn\'t built, but neglected to create\n        a new copy.\n\n        In these cases, it is up to you to provide a target Network having\n        weights that are not shared with the original `categorical_q_network`.\n        If you provide a `target_categorical_q_network` that shares any\n        weights with `categorical_q_network`, a warning will be logged but\n        no exception is thrown.\n\n        Note; shallow copies of Keras layers may be built via the code:\n\n        ```python\n        new_layer = type(layer).from_config(layer.get_config())\n        ```\n      target_update_tau: Factor for soft update of the target networks.\n      target_update_period: Period for soft update of the target networks.\n      td_errors_loss_fn: A function for computing the TD errors loss. If None, a\n        default value of huber_loss is used. This function takes as input the\n        target and the estimated Q values and returns the loss for each element\n        of the batch.\n      gamma: A discount factor for future rewards.\n      reward_scale_factor: Multiplicative scale for the reward.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      TypeError: If the action spec contains more than one action.\n    """"""\n    super(CategoricalDqnAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        categorical_q_network,\n        optimizer,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        epsilon_greedy=epsilon_greedy,\n        n_step_update=n_step_update,\n        boltzmann_temperature=boltzmann_temperature,\n        target_q_network=target_categorical_q_network,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter,\n        name=name)\n\n    def check_atoms(net, label):\n      num_atoms = getattr(net, \'num_atoms\', None)\n      if num_atoms is None:\n        raise TypeError(\'Expected {} to have property `num_atoms`, but it \'\n                        \'doesn\\\'t. (Note: you likely want to use a \'\n                        \'CategoricalQNetwork.) Network is: {}\'.format(\n                            label, net))\n      return num_atoms\n\n    num_atoms = check_atoms(self._q_network, \'categorical_q_network\')\n    target_num_atoms = check_atoms(\n        self._target_q_network, \'target_categorical_q_network\')\n    if num_atoms != target_num_atoms:\n      raise ValueError(\n          \'categorical_q_network and target_categorical_q_network have \'\n          \'different numbers of atoms: {} vs. {}\'.format(\n              num_atoms, target_num_atoms))\n    self._num_atoms = num_atoms\n\n    policy = categorical_q_policy.CategoricalQPolicy(\n        time_step_spec,\n        self._action_spec,\n        self._q_network,\n        min_q_value,\n        max_q_value,\n        observation_and_action_constraint_splitter=(\n            self._observation_and_action_constraint_splitter))\n\n    if boltzmann_temperature is not None:\n      self._collect_policy = boltzmann_policy.BoltzmannPolicy(\n          policy, temperature=self._boltzmann_temperature)\n    else:\n      self._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n          policy, epsilon=self._epsilon_greedy)\n    self._policy = greedy_policy.GreedyPolicy(policy)\n\n    target_policy = categorical_q_policy.CategoricalQPolicy(\n        time_step_spec,\n        self._action_spec,\n        self._target_q_network,\n        min_q_value,\n        max_q_value,\n        observation_and_action_constraint_splitter=(\n            self._observation_and_action_constraint_splitter))\n    self._target_greedy_policy = greedy_policy.GreedyPolicy(target_policy)\n\n    min_q_value = tf.convert_to_tensor(min_q_value, dtype_hint=tf.float32)\n    max_q_value = tf.convert_to_tensor(max_q_value, dtype_hint=tf.float32)\n    self._support = tf.linspace(min_q_value, max_q_value, num_atoms)\n\n  def _loss(self,\n            experience,\n            td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n            gamma=1.0,\n            reward_scale_factor=1.0,\n            weights=None,\n            training=False):\n    """"""Computes critic loss for CategoricalDQN training.\n\n    See Algorithm 1 and the discussion immediately preceding it in page 6 of\n    ""A Distributional Perspective on Reinforcement Learning""\n      Bellemare et al., 2017\n      https://arxiv.org/abs/1707.06887\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.policy.step_spec`.\n        All tensors in `experience` must be shaped `[batch, time, ...]` where\n        `time` must be equal to `self.required_experience_time_steps` if that\n        property is not `None`.\n      td_errors_loss_fn: A function(td_targets, predictions) to compute loss.\n      gamma: Discount for future rewards.\n      reward_scale_factor: Multiplicative factor to scale rewards.\n      weights: Optional weights used for importance sampling.\n      training: Whether the loss is being used for training.\n    Returns:\n      critic_loss: A scalar critic loss.\n    Raises:\n      ValueError:\n        if the number of actions is greater than 1.\n    """"""\n    # Check that `experience` includes two outer dimensions [B, T, ...]. This\n    # method requires a time dimension to compute the loss properly.\n    self._check_trajectory_dimensions(experience)\n\n    squeeze_time_dim = not self._q_network.state_spec\n    if self._n_step_update == 1:\n      time_steps, policy_steps, next_time_steps = (\n          trajectory.experience_to_transitions(experience, squeeze_time_dim))\n      actions = policy_steps.action\n    else:\n      # To compute n-step returns, we need the first time steps, the first\n      # actions, and the last time steps. Therefore we extract the first and\n      # last transitions from our Trajectory.\n      first_two_steps = tf.nest.map_structure(lambda x: x[:, :2], experience)\n      last_two_steps = tf.nest.map_structure(lambda x: x[:, -2:], experience)\n      time_steps, policy_steps, _ = (\n          trajectory.experience_to_transitions(\n              first_two_steps, squeeze_time_dim))\n      actions = policy_steps.action\n      _, _, next_time_steps = (\n          trajectory.experience_to_transitions(\n              last_two_steps, squeeze_time_dim))\n\n    with tf.name_scope(\'critic_loss\'):\n      nest_utils.assert_same_structure(actions, self.action_spec)\n      nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n      nest_utils.assert_same_structure(next_time_steps, self.time_step_spec)\n\n      rank = nest_utils.get_outer_rank(time_steps.observation,\n                                       self._time_step_spec.observation)\n\n      # If inputs have a time dimension and the q_network is stateful,\n      # combine the batch and time dimension.\n      batch_squash = (None\n                      if rank <= 1 or self._q_network.state_spec in ((), None)\n                      else utils.BatchSquash(rank))\n\n      network_observation = time_steps.observation\n\n      if self._observation_and_action_constraint_splitter is not None:\n        network_observation, _ = (\n            self._observation_and_action_constraint_splitter(\n                network_observation))\n\n      # q_logits contains the Q-value logits for all actions.\n      q_logits, _ = self._q_network(network_observation, time_steps.step_type,\n                                    training=training)\n\n      if batch_squash is not None:\n        # Squash outer dimensions to a single dimensions for facilitation\n        # computing the loss the following. Required for supporting temporal\n        # inputs, for example.\n        q_logits = batch_squash.flatten(q_logits)\n        actions = batch_squash.flatten(actions)\n        next_time_steps = tf.nest.map_structure(batch_squash.flatten,\n                                                next_time_steps)\n\n      next_q_distribution = self._next_q_distribution(next_time_steps)\n\n      if actions.shape.rank > 1:\n        actions = tf.squeeze(actions, list(range(1, actions.shape.rank)))\n\n      # Project the sample Bellman update \\hat{T}Z_{\\theta} onto the original\n      # support of Z_{\\theta} (see Figure 1 in paper).\n      batch_size = q_logits.shape[0] or tf.shape(q_logits)[0]\n      tiled_support = tf.tile(self._support, [batch_size])\n      tiled_support = tf.reshape(tiled_support, [batch_size, self._num_atoms])\n\n      if self._n_step_update == 1:\n        discount = next_time_steps.discount\n        if discount.shape.rank == 1:\n          # We expect discount to have a shape of [batch_size], while\n          # tiled_support will have a shape of [batch_size, num_atoms]. To\n          # multiply these, we add a second dimension of 1 to the discount.\n          discount = tf.expand_dims(discount, -1)\n        next_value_term = tf.multiply(discount,\n                                      tiled_support,\n                                      name=\'next_value_term\')\n\n        reward = next_time_steps.reward\n        if reward.shape.rank == 1:\n          # See the explanation above.\n          reward = tf.expand_dims(reward, -1)\n        reward_term = tf.multiply(reward_scale_factor,\n                                  reward,\n                                  name=\'reward_term\')\n\n        target_support = tf.add(reward_term, gamma * next_value_term,\n                                name=\'target_support\')\n      else:\n        # When computing discounted return, we need to throw out the last time\n        # index of both reward and discount, which are filled with dummy values\n        # to match the dimensions of the observation.\n        rewards = reward_scale_factor * experience.reward[:, :-1]\n        discounts = gamma * experience.discount[:, :-1]\n\n        # TODO(b/134618876): Properly handle Trajectories that include episode\n        # boundaries with nonzero discount.\n\n        discounted_returns = value_ops.discounted_return(\n            rewards=rewards,\n            discounts=discounts,\n            final_value=tf.zeros([batch_size], dtype=discounts.dtype),\n            time_major=False,\n            provide_all_returns=False)\n\n        # Convert discounted_returns from [batch_size] to [batch_size, 1]\n        discounted_returns = tf.expand_dims(discounted_returns, -1)\n\n        final_value_discount = tf.reduce_prod(discounts, axis=1)\n        final_value_discount = tf.expand_dims(final_value_discount, -1)\n\n        # Save the values of discounted_returns and final_value_discount in\n        # order to check them in unit tests.\n        self._discounted_returns = discounted_returns\n        self._final_value_discount = final_value_discount\n\n        target_support = tf.add(discounted_returns,\n                                final_value_discount * tiled_support,\n                                name=\'target_support\')\n\n      target_distribution = tf.stop_gradient(project_distribution(\n          target_support, next_q_distribution, self._support))\n\n      # Obtain the current Q-value logits for the selected actions.\n      indices = tf.range(batch_size)\n      indices = tf.cast(indices, actions.dtype)\n      reshaped_actions = tf.stack([indices, actions], axis=-1)\n      chosen_action_logits = tf.gather_nd(q_logits, reshaped_actions)\n\n      # Compute the cross-entropy loss between the logits. If inputs have\n      # a time dimension, compute the sum over the time dimension before\n      # computing the mean over the batch dimension.\n      if batch_squash is not None:\n        target_distribution = batch_squash.unflatten(target_distribution)\n        chosen_action_logits = batch_squash.unflatten(chosen_action_logits)\n        critic_loss = tf.reduce_sum(\n            tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n                labels=target_distribution,\n                logits=chosen_action_logits),\n            axis=1)\n      else:\n        critic_loss = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n            labels=target_distribution,\n            logits=chosen_action_logits)\n\n      agg_loss = common.aggregate_losses(\n          per_example_loss=critic_loss,\n          regularization_loss=self._q_network.losses)\n      total_loss = agg_loss.total_loss\n\n      dict_losses = {\'critic_loss\': agg_loss.weighted,\n                     \'reg_loss\': agg_loss.regularization,\n                     \'total_loss\': total_loss}\n\n      common.summarize_scalar_dict(dict_losses,\n                                   step=self.train_step_counter,\n                                   name_scope=\'Losses/\')\n\n      if self._debug_summaries:\n        distribution_errors = target_distribution - chosen_action_logits\n        with tf.name_scope(\'distribution_errors\'):\n          common.generate_tensor_summaries(\n              \'distribution_errors\', distribution_errors,\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              \'mean\', tf.reduce_mean(distribution_errors),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              \'mean_abs\', tf.reduce_mean(tf.abs(distribution_errors)),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              \'max\', tf.reduce_max(distribution_errors),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              \'min\', tf.reduce_min(distribution_errors),\n              step=self.train_step_counter)\n        with tf.name_scope(\'target_distribution\'):\n          common.generate_tensor_summaries(\n              \'target_distribution\', target_distribution,\n              step=self.train_step_counter)\n\n      # TODO(b/127318640): Give appropriate values for td_loss and td_error for\n      # prioritized replay.\n      return tf_agent.LossInfo(total_loss, dqn_agent.DqnLossInfo(td_loss=(),\n                                                                 td_error=()))\n\n  def _next_q_distribution(self, next_time_steps):\n    """"""Compute the q distribution of the next state for TD error computation.\n\n    Args:\n      next_time_steps: A batch of next timesteps\n\n    Returns:\n      A [batch_size, num_atoms] tensor representing the Q-distribution for the\n      next state.\n    """"""\n    network_observation = next_time_steps.observation\n\n    if self._observation_and_action_constraint_splitter is not None:\n      network_observation, _ = self._observation_and_action_constraint_splitter(\n          network_observation)\n\n    next_target_logits, _ = self._target_q_network(\n        network_observation, next_time_steps.step_type, training=False)\n    batch_size = next_target_logits.shape[0] or tf.shape(next_target_logits)[0]\n    next_target_probabilities = tf.nn.softmax(next_target_logits)\n    next_target_q_values = tf.reduce_sum(\n        self._support * next_target_probabilities, axis=-1)\n    dummy_state = self._target_greedy_policy.get_initial_state(batch_size)\n    # Find the greedy actions using our target greedy policy. This ensures that\n    # action constraints are respected and helps centralize the greedy logic.\n    greedy_actions = self._target_greedy_policy.action(\n        next_time_steps, dummy_state).action\n    next_qt_argmax = tf.cast(greedy_actions, tf.int32)[:, None]\n    batch_indices = tf.range(\n        tf.cast(tf.shape(next_target_q_values)[0], tf.int32))[:, None]\n    next_qt_argmax = tf.concat([batch_indices, next_qt_argmax], axis=-1)\n    return tf.gather_nd(next_target_probabilities, next_qt_argmax)\n\n\n# The following method is copied from the Dopamine codebase with permission\n# (https://github.com/google/dopamine). Thanks to Marc Bellemare and also to\n# Pablo Castro, who wrote the original version of this method.\ndef project_distribution(supports: types.Tensor,\n                         weights: types.Tensor,\n                         target_support: types.Tensor,\n                         validate_args: bool = False) -> types.Tensor:\n  """"""Projects a batch of (support, weights) onto target_support.\n\n  Based on equation (7) in (Bellemare et al., 2017):\n    https://arxiv.org/abs/1707.06887\n  In the rest of the comments we will refer to this equation simply as Eq7.\n\n  This code is not easy to digest, so we will use a running example to clarify\n  what is going on, with the following sample inputs:\n\n    * supports =       [[0, 2, 4, 6, 8],\n                        [1, 3, 4, 5, 6]]\n    * weights =        [[0.1, 0.6, 0.1, 0.1, 0.1],\n                        [0.1, 0.2, 0.5, 0.1, 0.1]]\n    * target_support = [4, 5, 6, 7, 8]\n\n  In the code below, comments preceded with \'Ex:\' will be referencing the above\n  values.\n\n  Args:\n    supports: Tensor of shape (batch_size, num_dims) defining supports for the\n      distribution.\n    weights: Tensor of shape (batch_size, num_dims) defining weights on the\n      original support points. Although for the CategoricalDQN agent these\n      weights are probabilities, it is not required that they are.\n    target_support: Tensor of shape (num_dims) defining support of the projected\n      distribution. The values must be monotonically increasing. Vmin and Vmax\n      will be inferred from the first and last elements of this tensor,\n      respectively. The values in this tensor must be equally spaced.\n    validate_args: Whether we will verify the contents of the\n      target_support parameter.\n\n  Returns:\n    A Tensor of shape (batch_size, num_dims) with the projection of a batch of\n    (support, weights) onto target_support.\n\n  Raises:\n    ValueError: If target_support has no dimensions, or if shapes of supports,\n      weights, and target_support are incompatible.\n  """"""\n  target_support_deltas = target_support[1:] - target_support[:-1]\n  # delta_z = `\\Delta z` in Eq7.\n  delta_z = target_support_deltas[0]\n  validate_deps = []\n  supports.shape.assert_is_compatible_with(weights.shape)\n  supports[0].shape.assert_is_compatible_with(target_support.shape)\n  target_support.shape.assert_has_rank(1)\n  if validate_args:\n    # Assert that supports and weights have the same shapes.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(tf.equal(tf.shape(supports), tf.shape(weights))),\n            [supports, weights]))\n    # Assert that elements of supports and target_support have the same shape.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(\n                tf.equal(tf.shape(supports)[1], tf.shape(target_support))),\n            [supports, target_support]))\n    # Assert that target_support has a single dimension.\n    validate_deps.append(\n        tf.Assert(\n            tf.equal(tf.size(tf.shape(target_support)), 1), [target_support]))\n    # Assert that the target_support is monotonically increasing.\n    validate_deps.append(\n        tf.Assert(tf.reduce_all(target_support_deltas > 0), [target_support]))\n    # Assert that the values in target_support are equally spaced.\n    validate_deps.append(\n        tf.Assert(\n            tf.reduce_all(tf.equal(target_support_deltas, delta_z)),\n            [target_support]))\n\n  with tf.control_dependencies(validate_deps):\n    # Ex: `v_min, v_max = 4, 8`.\n    v_min, v_max = target_support[0], target_support[-1]\n    # Ex: `batch_size = 2`.\n    batch_size = tf.shape(supports)[0]\n    # `N` in Eq7.\n    # Ex: `num_dims = 5`.\n    num_dims = tf.shape(target_support)[0]\n    # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n    # Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]\n    #                         [[ 4.  4.  4.  5.  6.]]]`.\n    clipped_support = tf.clip_by_value(supports, v_min, v_max)[:, None, :]\n    # Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]\n    #                         [ 4.  4.  4.  6.  8.]]\n    #                        [[ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]\n    #                         [ 4.  4.  4.  5.  6.]]]]`.\n    tiled_support = tf.tile([clipped_support], [1, 1, num_dims, 1])\n    # Ex: `reshaped_target_support = [[[ 4.]\n    #                                  [ 5.]\n    #                                  [ 6.]\n    #                                  [ 7.]\n    #                                  [ 8.]]\n    #                                 [[ 4.]\n    #                                  [ 5.]\n    #                                  [ 6.]\n    #                                  [ 7.]\n    #                                  [ 8.]]]`.\n    reshaped_target_support = tf.tile(target_support[:, None], [batch_size, 1])\n    reshaped_target_support = tf.reshape(reshaped_target_support,\n                                         [batch_size, num_dims, 1])\n    # numerator = `|clipped_support - z_i|` in Eq7.\n    # Ex: `numerator = [[[[ 0.  0.  0.  2.  4.]\n    #                     [ 1.  1.  1.  1.  3.]\n    #                     [ 2.  2.  2.  0.  2.]\n    #                     [ 3.  3.  3.  1.  1.]\n    #                     [ 4.  4.  4.  2.  0.]]\n    #                    [[ 0.  0.  0.  1.  2.]\n    #                     [ 1.  1.  1.  0.  1.]\n    #                     [ 2.  2.  2.  1.  0.]\n    #                     [ 3.  3.  3.  2.  1.]\n    #                     [ 4.  4.  4.  3.  2.]]]]`.\n    numerator = tf.abs(tiled_support - reshaped_target_support)\n    quotient = 1 - (numerator / delta_z)\n    # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n    # Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  1.  0.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  0.  1.]]\n    #                           [[ 1.  1.  1.  0.  0.]\n    #                            [ 0.  0.  0.  1.  0.]\n    #                            [ 0.  0.  0.  0.  1.]\n    #                            [ 0.  0.  0.  0.  0.]\n    #                            [ 0.  0.  0.  0.  0.]]]]`.\n    clipped_quotient = tf.clip_by_value(quotient, 0, 1)\n    # Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]\n    #                 [ 0.1  0.2  0.5  0.1  0.1]]`.\n    weights = weights[:, None, :]\n    # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x\', \\pi(x\'))`\n    # in Eq7.\n    # Ex: `inner_prod = [[[[ 0.1  0.6  0.1  0.  0. ]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.1 0. ]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.  0.1]]\n    #                     [[ 0.1  0.2  0.5  0.  0. ]\n    #                      [ 0.   0.   0.   0.1 0. ]\n    #                      [ 0.   0.   0.   0.  0.1]\n    #                      [ 0.   0.   0.   0.  0. ]\n    #                      [ 0.   0.   0.   0.  0. ]]]]`.\n    inner_prod = clipped_quotient * weights\n    # Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]\n    #                    [ 0.8 0.1 0.1 0.0 0.0]]`.\n    projection = tf.reduce_sum(inner_prod, 3)\n    projection = tf.reshape(projection, [batch_size, num_dims])\n    return projection\n'"
tf_agents/agents/categorical_dqn/categorical_dqn_agent_test.py,69,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for agents.dqn.categorical_dqn_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.agents.categorical_dqn import categorical_dqn_agent\nfrom tf_agents.networks import categorical_q_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import test_utils\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\n\nclass DummyCategoricalNet(network.Network):\n\n  def __init__(self,\n               input_tensor_spec,\n               num_atoms=51,\n               num_actions=2,\n               name=None):\n    self._num_atoms = num_atoms\n    self._num_actions = num_actions\n    super(DummyCategoricalNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    # In CategoricalDQN we are dealing with a distribution over Q-values, which\n    # are represented as num_atoms bins, ranging from min_q_value to\n    # max_q_value. In order to replicate the setup in the non-categorical\n    # network (namely, [[2, 1], [1, 1]]), we use the following ""logits"":\n    # [[0, 1, ..., num_atoms-1, num_atoms, 1, ..., 1],\n    #  [1, ......................................, 1]]\n    # The important bit is that the first half of the first list (which\n    # corresponds to the logits for the first action) place more weight on the\n    # higher q_values than on the lower ones, thereby resulting in a higher\n    # value for the first action.\n    weights_initializer = np.array([\n        np.concatenate((np.arange(num_atoms), np.ones(num_atoms))),\n        np.concatenate((np.ones(num_atoms), np.ones(num_atoms)))])\n    kernel_initializer = tf.compat.v1.initializers.constant(\n        weights_initializer, verify_shape=True)\n    bias_initializer = tf.compat.v1.initializers.ones()\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = []\n    self._dummy_layers.append(\n        tf.keras.layers.Dense(\n            num_actions * num_atoms,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer))\n\n  @property\n  def num_atoms(self):\n    return self._num_atoms\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    logits = tf.reshape(inputs, [-1, self._num_actions, self._num_atoms])\n    return logits, network_state\n\n\nclass KerasLayersNet(network.Network):\n\n  def __init__(self, observation_spec, action_spec, layer, num_atoms=5,\n               name=None):\n    super(KerasLayersNet, self).__init__(observation_spec, state_spec=(),\n                                         name=name)\n    self._layer = layer\n    self.num_atoms = num_atoms  # Dummy, this doesn\'t match the layer output.\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    return self._layer(inputs), network_state\n\n\nclass DummyCategoricalQRnnNetwork(q_rnn_network.QRnnNetwork):\n\n  def __init__(self,\n               input_tensor_spec,\n               action_spec,\n               num_atoms=51,\n               **kwargs):\n    if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n      raise TypeError(\'action_spec must be a BoundedTensorSpec. Got: %s\' % (\n          action_spec,))\n\n    self._num_actions = action_spec.maximum - action_spec.minimum + 1\n    self._num_atoms = num_atoms\n\n    q_network_action_spec = tensor_spec.BoundedTensorSpec(\n        (), tf.int32, minimum=0, maximum=self._num_actions * num_atoms - 1)\n\n    super(DummyCategoricalQRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        action_spec=q_network_action_spec,\n        **kwargs)\n\n  @property\n  def num_atoms(self):\n    return self._num_atoms\n\n  def call(self, observations, step_type=None, network_state=()):\n    logits, network_state = super(DummyCategoricalQRnnNetwork, self).call(\n        observations, step_type, network_state)\n    shape = logits.shape.as_list()\n    assert shape[-1] == self._num_actions * self._num_atoms\n    new_shape = shape[:-1] + [self._num_actions, self._num_atoms]\n    logits = tf.reshape(logits, new_shape)\n    return logits, network_state\n\n\nclass CategoricalDqnAgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(CategoricalDqnAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 1)\n    self._categorical_net = categorical_q_network.CategoricalQNetwork(\n        self._obs_spec,\n        self._action_spec,\n        fc_layer_params=[4])\n    self._dummy_categorical_net = DummyCategoricalNet(self._obs_spec)\n    self._optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)\n\n  def testCreateAgentNestSizeChecks(self):\n    action_spec = [\n        tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n    ]\n\n    with self.assertRaisesRegexp(\n        ValueError, \'.*Only one dimensional actions.*\'):\n      categorical_dqn_agent.CategoricalDqnAgent(\n          self._time_step_spec,\n          action_spec,\n          self._dummy_categorical_net,\n          self._optimizer)\n\n  def testCreateAgentDimChecks(self):\n    action_spec = [tensor_spec.BoundedTensorSpec([1, 2], tf.int32, 0, 1)]\n\n    with self.assertRaisesRegexp(\n        ValueError, \'.*Only one dimensional actions.*\'):\n      categorical_dqn_agent.CategoricalDqnAgent(\n          self._time_step_spec,\n          action_spec,\n          self._dummy_categorical_net,\n          self._optimizer)\n\n  def testCreateAgentDefaultNetwork(self):\n    categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._categorical_net,\n        self._optimizer)\n\n  def testCreateAgentWithPrebuiltPreprocessingLayers(self):\n    dense_layer = tf.keras.layers.Dense(3)\n    q_net = KerasLayersNet(\n        self._time_step_spec.observation, self._action_spec, dense_layer)\n    with self.assertRaisesRegexp(\n        ValueError, \'shares weights with the original network\'):\n      categorical_dqn_agent.CategoricalDqnAgent(\n          self._time_step_spec,\n          self._action_spec,\n          categorical_q_network=q_net,\n          optimizer=None)\n\n    # Explicitly share weights between q and target networks.\n    # This would be an unusual setup so we check that an error is thrown.\n    q_target_net = KerasLayersNet(\n        self._time_step_spec.observation, self._action_spec, dense_layer)\n    with self.assertRaisesRegexp(\n        ValueError, \'shares weights with the original network\'):\n      categorical_dqn_agent.CategoricalDqnAgent(\n          self._time_step_spec,\n          self._action_spec,\n          categorical_q_network=q_net,\n          optimizer=None,\n          target_categorical_q_network=q_target_net)\n\n  def testCreateAgentWithPrebuiltPreprocessingLayersDiffAtoms(self):\n    dense_layer = tf.keras.layers.Dense(3)\n    q_net = KerasLayersNet(\n        self._time_step_spec.observation, self._action_spec, dense_layer)\n    dense_layer_target = tf.keras.layers.Dense(3)\n    q_bad_target_net = KerasLayersNet(\n        self._time_step_spec.observation, self._action_spec, dense_layer_target,\n        num_atoms=3)\n    with self.assertRaisesRegexp(ValueError, \'have different numbers of atoms\'):\n      categorical_dqn_agent.CategoricalDqnAgent(\n          self._time_step_spec,\n          self._action_spec,\n          categorical_q_network=q_net,\n          optimizer=None,\n          target_categorical_q_network=q_bad_target_net)\n\n  def testCriticLoss(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._dummy_categorical_net,\n        self._optimizer)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # Due to the constant initialization of the DummyCategoricalNet, we can\n    # expect the same loss every time.\n    expected_loss = 2.19525\n    loss_info = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    evaluated_loss = self.evaluate(loss_info).loss\n    self.assertAllClose(evaluated_loss, expected_loss, atol=1e-4)\n\n  def testCriticLossWithMaskedActions(self):\n    # Observations are now a tuple of the usual observation and an action mask.\n    observation_spec_with_mask = (\n        self._obs_spec,\n        tensor_spec.BoundedTensorSpec([2], tf.int32, 0, 1))\n    time_step_spec = ts.time_step_spec(observation_spec_with_mask)\n    dummy_categorical_net = DummyCategoricalNet(self._obs_spec)\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        time_step_spec,\n        self._action_spec,\n        dummy_categorical_net,\n        self._optimizer,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]))\n\n    # For `observations`, the masks are set up so that only one action is valid\n    # for each element in the batch.\n    observations = (tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n                    tf.constant([[1, 0], [0, 1]], dtype=tf.int32))\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n\n    # For `next_observations`, the masks are set up so the opposite actions as\n    # before are valid.\n    next_observations = (tf.constant([[5, 6], [7, 8]], dtype=tf.float32),\n                         tf.constant([[0, 1], [1, 0]], dtype=tf.int32))\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # Due to the constant initialization of the DummyCategoricalNet, we can\n    # expect the same loss every time. Note this is different from the loss in\n    # testCriticLoss above due to previously optimal actions being masked out.\n    expected_loss = 5.062895\n    loss_info = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    evaluated_loss = self.evaluate(loss_info).loss\n    self.assertAllClose(evaluated_loss, expected_loss, atol=1e-4)\n\n  def testCriticLossNStep(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._dummy_categorical_net,\n        self._optimizer,\n        n_step_update=2)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    third_observations = tf.constant([[9, 10], [11, 12]], dtype=tf.float32)\n    third_time_steps = ts.transition(third_observations, rewards, discounts)\n\n    experience1 = trajectory.from_transition(\n        time_steps, action_steps, next_time_steps)\n    experience2 = trajectory.from_transition(\n        next_time_steps, action_steps, third_time_steps)\n    experience3 = trajectory.from_transition(\n        third_time_steps, action_steps, third_time_steps)\n\n    experience = tf.nest.map_structure(\n        lambda x, y, z: tf.stack([x, y, z], axis=1),\n        experience1, experience2, experience3)\n\n    loss_info = agent._loss(experience)\n\n    # discounted_returns should evaluate to 10 + 0.9 * 10 = 19 and\n    # 20 + 0.9 * 20 = 38.\n    evaluated_discounted_returns = self.evaluate(agent._discounted_returns)\n    self.assertAllClose(evaluated_discounted_returns, [[19], [38]], atol=1e-4)\n\n    # Both final_value_discount values should be 0.9 * 0.9 = 0.81.\n    evaluated_final_value_discount = self.evaluate(agent._final_value_discount)\n    self.assertAllClose(evaluated_final_value_discount, [[0.81], [0.81]],\n                        atol=1e-4)\n\n    # Due to the constant initialization of the DummyCategoricalNet, we can\n    # expect the same loss every time.\n    expected_loss = 2.19525\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    evaluated_loss = self.evaluate(loss_info).loss\n    self.assertAllClose(evaluated_loss, expected_loss, atol=1e-4)\n\n  def testPolicy(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._categorical_net,\n        self._optimizer)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions, _, _ = agent.policy.action(time_steps)\n    self.assertEqual(actions.shape, [2])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(actions)\n    self.assertTrue(all(actions_ <= self._action_spec.maximum))\n    self.assertTrue(all(actions_ >= self._action_spec.minimum))\n\n  def testInitialize(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._categorical_net,\n        self._optimizer)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_time_steps = ts.transition(observations, rewards, discounts)\n\n    experience = test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    loss_info = agent._loss(experience)\n    initialize = agent.initialize()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    losses = self.evaluate(loss_info).loss\n    self.assertGreater(losses, 0.0)\n\n    critic_variables = agent._q_network.variables\n    target_critic_variables = agent._target_q_network.variables\n    self.assertTrue(critic_variables)\n    self.assertTrue(target_critic_variables)\n    self.evaluate(initialize)\n    for s, t in zip(critic_variables, target_critic_variables):\n      self.assertAllClose(self.evaluate(s), self.evaluate(t))\n\n  def testUpdateTarget(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._categorical_net,\n        self._optimizer)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n    experience = test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, time_steps)\n\n    loss_info = agent._loss(experience)\n    update_targets = agent._update_target()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    losses = self.evaluate(loss_info).loss\n    self.assertGreater(losses, 0.0)\n    self.evaluate(update_targets)\n\n  def testTrain(self):\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        self._action_spec,\n        self._dummy_categorical_net,\n        self._optimizer)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    train_step = agent.train(experience, weights=None)\n\n    # Due to the constant initialization of the DummyCategoricalNet, we can\n    # expect the same loss every time.\n    expected_loss = 2.19525\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    evaluated_loss, _ = self.evaluate(train_step)\n    self.assertAllClose(evaluated_loss, expected_loss, atol=1e-4)\n\n  def testTrainWithRnn(self):\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1)\n\n    batch_size = 5\n    observations = tf.constant(\n        [[[1, 2], [3, 4], [5, 6]]] * batch_size, dtype=tf.float32)\n    actions = tf.constant([[[0], [1], [1]]] * batch_size, dtype=tf.int32)\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[1] * 3] * batch_size, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        observation=[observations])\n\n    experience = trajectory.Trajectory(\n        step_type=time_steps.step_type,\n        observation=observations,\n        action=actions,\n        policy_info=(),\n        next_step_type=time_steps.step_type,\n        reward=time_steps.reward,\n        discount=time_steps.discount)\n\n    categorical_q_rnn_network = DummyCategoricalQRnnNetwork(\n        self._obs_spec,\n        action_spec,\n        conv_layer_params=None,\n        input_fc_layer_params=(16,),\n        preprocessing_combiner=None,\n        lstm_size=(40,),\n        output_fc_layer_params=(16,),\n    )\n\n    counter = common.create_variable(\'test_train_counter\')\n\n    agent = categorical_dqn_agent.CategoricalDqnAgent(\n        self._time_step_spec,\n        action_spec,\n        categorical_q_rnn_network,\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n    )\n\n    # Force variable creation.\n    agent.policy.variables()\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(counter), 0)\n    self.evaluate(loss)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ddpg/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Deep Deterministic Policy Gradient (DDPG) agent and its networks.""""""\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.ddpg import ddpg_agent\n'"
tf_agents/agents/ddpg/actor_network.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Actor network to use with DDPG agents.\n\nNote: This network scales actions to fit the given spec by using `tanh`. Due to\nthe nature of the `tanh` function, actions near the spec bounds cannot be\nreturned.\n""""""\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.utils import common\n\n\n@gin.configurable\nclass ActorNetwork(network.Network):\n  """"""Creates an actor network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               fc_layer_params=None,\n               dropout_layer_params=None,\n               conv_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               name=\'ActorNetwork\'):\n    """"""Creates an instance of `ActorNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        inputs.\n      output_tensor_spec: A nest of `tensor_spec.BoundedTensorSpec` representing\n        the outputs.\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, each item\n        is the fraction of input units to drop or a dictionary of parameters\n        according to the keras.Dropout documentation. The additional parameter\n        `permanent\', if set to True, allows to apply dropout at inference for\n        approximated Bayesian inference. The dropout layers are interleaved with\n        the fully connected layers; there is a dropout layer after each fully\n        connected layer, except if the entry in the list is None. This list must\n        have the same length of fc_layer_params, or be None.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If `input_tensor_spec` or `action_spec` contains more than one\n        item, or if the action data type is not `float`.\n    """"""\n\n    super(ActorNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    if len(tf.nest.flatten(input_tensor_spec)) > 1:\n      raise ValueError(\'Only a single observation is supported by this network\')\n\n    flat_action_spec = tf.nest.flatten(output_tensor_spec)\n    if len(flat_action_spec) > 1:\n      raise ValueError(\'Only a single action is supported by this network\')\n    self._single_action_spec = flat_action_spec[0]\n\n    if self._single_action_spec.dtype not in [tf.float32, tf.float64]:\n      raise ValueError(\'Only float actions are supported by this network.\')\n\n    # TODO(kbanoop): Replace mlp_layers with encoding networks.\n    self._mlp_layers = utils.mlp_layers(\n        conv_layer_params,\n        fc_layer_params,\n        dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n            scale=1. / 3., mode=\'fan_in\', distribution=\'uniform\'),\n        name=\'input_mlp\')\n\n    self._mlp_layers.append(\n        tf.keras.layers.Dense(\n            flat_action_spec[0].shape.num_elements(),\n            activation=tf.keras.activations.tanh,\n            kernel_initializer=tf.keras.initializers.RandomUniform(\n                minval=-0.003, maxval=0.003),\n            name=\'action\'))\n\n    self._output_tensor_spec = output_tensor_spec\n\n  def call(self, observations, step_type=(), network_state=(), training=False):\n    del step_type  # unused.\n    observations = tf.nest.flatten(observations)\n    output = tf.cast(observations[0], tf.float32)\n    for layer in self._mlp_layers:\n      output = layer(output, training=training)\n\n    actions = common.scale_to_spec(output, self._single_action_spec)\n    output_actions = tf.nest.pack_sequence_as(self._output_tensor_spec,\n                                              [actions])\n\n    return output_actions, network_state\n'"
tf_agents/agents/ddpg/actor_network_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.ddpg.actor_network.""""""\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.specs import tensor_spec\n\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\nclass ActorNetworkTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testBuild(self):\n    batch_size = 3\n    num_obs_dims = 5\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2., 3.)\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions.shape.as_list(),\n                        [batch_size] + action_spec.shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 2)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testAddConvLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([3, 3, num_obs_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2., 3.)\n\n    actor_net = actor_network.ActorNetwork(\n        obs_spec, action_spec, conv_layer_params=[(16, 3, 2)])\n\n    obs = tf.random.uniform([batch_size, 3, 3, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions.shape.as_list(),\n                        [batch_size] + action_spec.shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 4)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testAddFCLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2., 3.)\n\n    actor_net = actor_network.ActorNetwork(\n        obs_spec, action_spec, fc_layer_params=[100])\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions.shape.as_list(),\n                        [batch_size] + action_spec.shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 4)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testScalarAction(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec([], tf.float32, 2., 3.)\n\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions.shape.as_list(),\n                        [batch_size] + action_spec.shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 2)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def test2DAction(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec([2, 3], tf.float32, 2., 3.)\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions.shape.as_list(),\n                        [batch_size] + action_spec.shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 2)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testActionsWithinRange(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec([2, 3], tf.float32, 2., 3.)\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(actions)\n    self.assertTrue(np.all(actions_ >= action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= action_spec.maximum))\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testListOfSingleAction(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = [tensor_spec.BoundedTensorSpec([1], tf.float32, 2., 3.)]\n\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n\n    self.assertAllEqual(actions[0].shape.as_list(),\n                        [batch_size] + action_spec[0].shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 2)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testDictOfSingleAction(self):\n    batch_size = 3\n    num_obs_dims = 5\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = {\n        \'motor\': tensor_spec.BoundedTensorSpec([1], tf.float32, 2., 3.)\n    }\n    actor_net = actor_network.ActorNetwork(obs_spec, action_spec)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions, _ = actor_net(obs)\n    self.assertAllEqual(actions[\'motor\'].shape.as_list(),\n                        [batch_size] + action_spec[\'motor\'].shape.as_list())\n    self.assertEqual(len(actor_net.trainable_variables), 2)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ddpg/actor_rnn_network.py,20,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample recurrent Actor network to use with DDPG agents.\n\nNote: This network scales actions to fit the given spec by using `tanh`. Due to\nthe nature of the `tanh` function, actions near the spec bounds cannot be\nreturned.\n""""""\n\nimport functools\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.keras_layers import dynamic_unroll_layer\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\n# TODO(kbanoop): Reduce code duplication with other actor networks.\n@gin.configurable\nclass ActorRnnNetwork(network.Network):\n  """"""Creates a recurrent actor network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               conv_layer_params=None,\n               input_fc_layer_params=(200, 100),\n               lstm_size=(40,),\n               output_fc_layer_params=(200, 100),\n               activation_fn=tf.keras.activations.relu,\n               name=\'ActorRnnNetwork\'):\n    """"""Creates an instance of `ActorRnnNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      output_tensor_spec: A nest of `tensor_spec.BoundedTensorSpec` representing\n        the actions.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      input_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied before\n        the LSTM cell.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied after the\n        LSTM cell.\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      name: A string representing name of the network.\n\n    Returns:\n      A nest of action tensors matching the action_spec.\n\n    Raises:\n      ValueError: If `input_tensor_spec` contains more than one observation.\n    """"""\n    if len(tf.nest.flatten(input_tensor_spec)) > 1:\n      raise ValueError(\'Only a single observation is supported by this network\')\n\n    input_layers = utils.mlp_layers(\n        conv_layer_params,\n        input_fc_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n        name=\'input_mlp\')\n\n    # Create RNN cell\n    if len(lstm_size) == 1:\n      cell = tf.keras.layers.LSTMCell(lstm_size[0])\n    else:\n      cell = tf.keras.layers.StackedRNNCells(\n          [tf.keras.layers.LSTMCell(size) for size in lstm_size])\n\n    state_spec = tf.nest.map_structure(\n        functools.partial(\n            tensor_spec.TensorSpec, dtype=tf.float32,\n            name=\'network_state_spec\'), list(cell.state_size))\n\n    output_layers = utils.mlp_layers(fc_layer_params=output_fc_layer_params,\n                                     name=\'output\')\n\n    flat_action_spec = tf.nest.flatten(output_tensor_spec)\n    action_layers = [\n        tf.keras.layers.Dense(\n            single_action_spec.shape.num_elements(),\n            activation=tf.keras.activations.tanh,\n            kernel_initializer=tf.keras.initializers.RandomUniform(\n                minval=-0.003, maxval=0.003),\n            name=\'action\') for single_action_spec in flat_action_spec\n    ]\n\n    super(ActorRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=state_spec,\n        name=name)\n\n    self._output_tensor_spec = output_tensor_spec\n    self._flat_action_spec = flat_action_spec\n    self._conv_layer_params = conv_layer_params\n    self._input_layers = input_layers\n    self._dynamic_unroll = dynamic_unroll_layer.DynamicUnroll(cell)\n    self._output_layers = output_layers\n    self._action_layers = action_layers\n\n  # TODO(kbanoop): Standardize argument names across different networks.\n  def call(self, observation, step_type, network_state=(), training=False):\n    num_outer_dims = nest_utils.get_outer_rank(observation,\n                                               self.input_tensor_spec)\n    if num_outer_dims not in (1, 2):\n      raise ValueError(\n          \'Input observation must have a batch or batch x time outer shape.\')\n\n    has_time_dim = num_outer_dims == 2\n    if not has_time_dim:\n      # Add a time dimension to the inputs.\n      observation = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                          observation)\n      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                        step_type)\n\n    states = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n    batch_squash = utils.BatchSquash(2)  # Squash B, and T dims.\n    states = batch_squash.flatten(states)  # [B, T, ...] -> [B x T, ...]\n\n    for layer in self._input_layers:\n      states = layer(states, training=training)\n\n    states = batch_squash.unflatten(states)  # [B x T, ...] -> [B, T, ...]\n\n    with tf.name_scope(\'reset_mask\'):\n      reset_mask = tf.equal(step_type, time_step.StepType.FIRST)\n    # Unroll over the time sequence.\n    states, network_state = self._dynamic_unroll(\n        states,\n        reset_mask=reset_mask,\n        initial_state=network_state,\n        training=training)\n\n    states = batch_squash.flatten(states)  # [B, T, ...] -> [B x T, ...]\n\n    for layer in self._output_layers:\n      states = layer(states, training=training)\n\n    actions = []\n    for layer, spec in zip(self._action_layers, self._flat_action_spec):\n      action = layer(states, training=training)\n      action = common.scale_to_spec(action, spec)\n      action = batch_squash.unflatten(action)  # [B x T, ...] -> [B, T, ...]\n      if not has_time_dim:\n        action = tf.squeeze(action, axis=1)\n      actions.append(action)\n\n    output_actions = tf.nest.pack_sequence_as(self._output_tensor_spec, actions)\n    return output_actions, network_state\n'"
tf_agents/agents/ddpg/actor_rnn_network_test.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.ddpg.actor_rnn_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tensorflow.python.framework import test_util  # TF internal\n\n\nclass ActorRnnNetworkTest(tf.test.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testBuilds(self):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.float32, 0, 3)\n    ]\n    net = actor_rnn_network.ActorRnnNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        lstm_size=(3,),\n        output_fc_layer_params=(5,))\n\n    actions, network_state = net(time_step.observation, time_step.step_type,\n                                 net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1, 2], actions[0].shape.as_list())\n    self.assertEqual([1, 3], actions[1].shape.as_list())\n\n    self.assertEqual(13, len(net.variables))\n    # Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Fc Kernel\n    self.assertEqual((64, 5), net.variables[2].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[3].shape)\n    # LSTM Cell Kernel\n    self.assertEqual((5, 12), net.variables[4].shape)\n    # LSTM Cell Recurrent Kernel\n    self.assertEqual((3, 12), net.variables[5].shape)\n    # LSTM Cell Bias\n    self.assertEqual((12,), net.variables[6].shape)\n    # Fc Kernel\n    self.assertEqual((3, 5), net.variables[7].shape)\n    # Fc Bias\n    self.assertEqual((5,), net.variables[8].shape)\n    # Action 1 Kernel\n    self.assertEqual((5, 2), net.variables[9].shape)\n    # Action 1 Bias\n    self.assertEqual((2,), net.variables[10].shape)\n    # Action 2 Kernel\n    self.assertEqual((5, 3), net.variables[11].shape)\n    # Action 2 Bias\n    self.assertEqual((3,), net.variables[12].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 3), network_state[0].shape)\n    self.assertEqual((1, 3), network_state[1].shape)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testActionsWithinRange(self):\n    observation_spec = tensor_spec.BoundedTensorSpec(\n        (8, 8, 3), tf.float32, 0, 1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = [\n        tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3),\n        tensor_spec.BoundedTensorSpec((3,), tf.float32, 0, 3)\n    ]\n    net = actor_rnn_network.ActorRnnNetwork(\n        observation_spec,\n        action_spec,\n        conv_layer_params=[(4, 2, 2)],\n        input_fc_layer_params=(5,),\n        output_fc_layer_params=(5,),\n        lstm_size=(3,))\n\n    actions, _ = net(time_step.observation, time_step.step_type,\n                     net.get_initial_state(batch_size=1))\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    for (action, spec) in zip(actions, action_spec):\n      action_ = self.evaluate(action)\n      self.assertTrue(np.all(action_ >= spec.minimum))\n      self.assertTrue(np.all(action_ <= spec.maximum))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ddpg/critic_network.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample Critic/Q network to use with DDPG agents.""""""\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\n\n\n@gin.configurable\nclass CriticNetwork(network.Network):\n  """"""Creates a critic network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               observation_conv_layer_params=None,\n               observation_fc_layer_params=None,\n               observation_dropout_layer_params=None,\n               action_fc_layer_params=None,\n               action_dropout_layer_params=None,\n               joint_fc_layer_params=None,\n               joint_dropout_layer_params=None,\n               activation_fn=tf.nn.relu,\n               output_activation_fn=None,\n               kernel_initializer=None,\n               last_kernel_initializer=None,\n               name=\'CriticNetwork\'):\n    """"""Creates an instance of `CriticNetwork`.\n\n    Args:\n      input_tensor_spec: A tuple of (observation, action) each a nest of\n        `tensor_spec.TensorSpec` representing the inputs.\n      observation_conv_layer_params: Optional list of convolution layer\n        parameters for observations, where each item is a length-three tuple\n        indicating (num_units, kernel_size, stride).\n      observation_fc_layer_params: Optional list of fully connected parameters\n        for observations, where each item is the number of units in the layer.\n      observation_dropout_layer_params: Optional list of dropout layer\n        parameters, each item is the fraction of input units to drop or a\n        dictionary of parameters according to the keras.Dropout documentation.\n        The additional parameter `permanent\', if set to True, allows to apply\n        dropout at inference for approximated Bayesian inference. The dropout\n        layers are interleaved with the fully connected layers; there is a\n        dropout layer after each fully connected layer, except if the entry in\n        the list is None. This list must have the same length of\n        observation_fc_layer_params, or be None.\n      action_fc_layer_params: Optional list of fully connected parameters for\n        actions, where each item is the number of units in the layer.\n      action_dropout_layer_params: Optional list of dropout layer parameters,\n        each item is the fraction of input units to drop or a dictionary of\n        parameters according to the keras.Dropout documentation. The additional\n        parameter `permanent\', if set to True, allows to apply dropout at\n        inference for approximated Bayesian inference. The dropout layers are\n        interleaved with the fully connected layers; there is a dropout layer\n        after each fully connected layer, except if the entry in the list is\n        None. This list must have the same length of action_fc_layer_params, or\n        be None.\n      joint_fc_layer_params: Optional list of fully connected parameters after\n        merging observations and actions, where each item is the number of units\n        in the layer.\n      joint_dropout_layer_params: Optional list of dropout layer parameters,\n        each item is the fraction of input units to drop or a dictionary of\n        parameters according to the keras.Dropout documentation. The additional\n        parameter `permanent\', if set to True, allows to apply dropout at\n        inference for approximated Bayesian inference. The dropout layers are\n        interleaved with the fully connected layers; there is a dropout layer\n        after each fully connected layer, except if the entry in the list is\n        None. This list must have the same length of joint_fc_layer_params, or\n        be None.\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      output_activation_fn: Activation function for the last layer. This can be\n        used to restrict the range of the output. For example, one can pass\n        tf.keras.activations.sigmoid here to restrict the output to be bounded\n        between 0 and 1.\n      kernel_initializer: kernel initializer for all layers except for the value\n        regression layer. If None, a VarianceScaling initializer will be used.\n      last_kernel_initializer: kernel initializer for the value regression\n         layer. If None, a RandomUniform initializer will be used.\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If `observation_spec` or `action_spec` contains more than one\n        observation.\n    """"""\n    super(CriticNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    observation_spec, action_spec = input_tensor_spec\n\n    if len(tf.nest.flatten(observation_spec)) > 1:\n      raise ValueError(\'Only a single observation is supported by this network\')\n\n    flat_action_spec = tf.nest.flatten(action_spec)\n    if len(flat_action_spec) > 1:\n      raise ValueError(\'Only a single action is supported by this network\')\n    self._single_action_spec = flat_action_spec[0]\n\n    if kernel_initializer is None:\n      kernel_initializer = tf.compat.v1.keras.initializers.VarianceScaling(\n          scale=1. / 3., mode=\'fan_in\', distribution=\'uniform\')\n    if last_kernel_initializer is None:\n      last_kernel_initializer = tf.keras.initializers.RandomUniform(\n          minval=-0.003, maxval=0.003)\n\n    # TODO(kbanoop): Replace mlp_layers with encoding networks.\n    self._observation_layers = utils.mlp_layers(\n        observation_conv_layer_params,\n        observation_fc_layer_params,\n        observation_dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'observation_encoding\')\n\n    self._action_layers = utils.mlp_layers(\n        None,\n        action_fc_layer_params,\n        action_dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'action_encoding\')\n\n    self._joint_layers = utils.mlp_layers(\n        None,\n        joint_fc_layer_params,\n        joint_dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'joint_mlp\')\n\n    self._joint_layers.append(\n        tf.keras.layers.Dense(\n            1,\n            activation=output_activation_fn,\n            kernel_initializer=last_kernel_initializer,\n            name=\'value\'))\n\n  def call(self, inputs, step_type=(), network_state=(), training=False):\n    observations, actions = inputs\n    del step_type  # unused.\n    observations = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    for layer in self._observation_layers:\n      observations = layer(observations, training=training)\n\n    actions = tf.cast(tf.nest.flatten(actions)[0], tf.float32)\n    for layer in self._action_layers:\n      actions = layer(actions, training=training)\n\n    joint = tf.concat([observations, actions], 1)\n    for layer in self._joint_layers:\n      joint = layer(joint, training=training)\n\n    return tf.reshape(joint, [-1]), network_state\n'"
tf_agents/agents/ddpg/critic_network_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.ddpg.critic_network.""""""\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.specs import tensor_spec\n\n\nclass CriticNetworkTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testBuild(self):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    critic_net = critic_network.CriticNetwork((obs_spec, action_spec))\n\n    q_values, _ = critic_net((obs, actions))\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size])\n    self.assertLen(critic_net.trainable_variables, 2)\n\n  def testAddObsConvLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n\n    obs_spec = tensor_spec.TensorSpec([3, 3, num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n    critic_net = critic_network.CriticNetwork(\n        (obs_spec, action_spec), observation_conv_layer_params=[(16, 3, 2)])\n\n    obs = tf.random.uniform([batch_size, 3, 3, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    q_values, _ = critic_net((obs, actions))\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size])\n    self.assertLen(critic_net.trainable_variables, 4)\n\n  def testAddObsFCLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n\n    obs_spec = tensor_spec.TensorSpec([3, 3, num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n    critic_net = critic_network.CriticNetwork(\n        (obs_spec, action_spec), observation_fc_layer_params=[20, 10])\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    q_values, _ = critic_net((obs, actions))\n\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size])\n    self.assertLen(critic_net.trainable_variables, 6)\n\n  def testAddActionFCLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n    critic_net = critic_network.CriticNetwork(\n        (obs_spec, action_spec), action_fc_layer_params=[20])\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    q_values, _ = critic_net((obs, actions))\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size])\n    self.assertLen(critic_net.trainable_variables, 4)\n\n  def testAddJointFCLayers(self):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n    critic_net = critic_network.CriticNetwork(\n        (obs_spec, action_spec), joint_fc_layer_params=[20])\n\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    q_values, _ = critic_net((obs, actions))\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size])\n    self.assertLen(critic_net.trainable_variables, 4)\n\n  @parameterized.named_parameters(\n      (\'TrainingTrue\', True,),\n      (\'TrainingFalse\', False))\n  def testDropoutJointFCLayers(self, training):\n    batch_size = 3\n    num_obs_dims = 5\n    num_actions_dims = 2\n\n    obs_spec = tensor_spec.TensorSpec([num_obs_dims], tf.float32)\n    action_spec = tensor_spec.TensorSpec([num_actions_dims], tf.float32)\n    critic_net = critic_network.CriticNetwork(\n        (obs_spec, action_spec),\n        joint_fc_layer_params=[20],\n        joint_dropout_layer_params=[0.5])\n    obs = tf.random.uniform([batch_size, num_obs_dims])\n    actions = tf.random.uniform([batch_size, num_actions_dims])\n    q_values1, _ = critic_net((obs, actions), training=training)\n    q_values2, _ = critic_net((obs, actions), training=training)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    q_values1, q_values2 = self.evaluate([q_values1, q_values2])\n    if training:\n      self.assertGreater(np.linalg.norm(q_values1 - q_values2), 0)\n    else:\n      self.assertAllEqual(q_values1, q_values2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ddpg/critic_rnn_network.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Sample recurrent Critic network to use with DDPG agents.""""""\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.keras_layers import dynamic_unroll_layer\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import nest_utils\n\nKERAS_LSTM_FUSED_IMPLEMENTATION = 2\n\n\n@gin.configurable\nclass CriticRnnNetwork(network.Network):\n  """"""Creates a recurrent Critic network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               observation_conv_layer_params=None,\n               observation_fc_layer_params=(200,),\n               action_fc_layer_params=(200,),\n               joint_fc_layer_params=(100,),\n               lstm_size=None,\n               output_fc_layer_params=(200, 100),\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               last_kernel_initializer=None,\n               rnn_construction_fn=None,\n               rnn_construction_kwargs=None,\n               name=\'CriticRnnNetwork\'):\n    """"""Creates an instance of `CriticRnnNetwork`.\n\n    Args:\n      input_tensor_spec: A tuple of (observation, action) each of type\n        `tensor_spec.TensorSpec` representing the inputs.\n      observation_conv_layer_params: Optional list of convolution layers\n        parameters to apply to the observations, where each item is a\n        length-three tuple indicating (filters, kernel_size, stride).\n      observation_fc_layer_params: Optional list of fully_connected parameters,\n        where each item is the number of units in the layer. This is applied\n        after the observation convultional layer.\n      action_fc_layer_params: Optional list of parameters for a fully_connected\n        layer to apply to the actions, where each item is the number of units\n        in the layer.\n      joint_fc_layer_params: Optional list of parameters for a fully_connected\n        layer to apply after merging observations and actions, where each item\n        is the number of units in the layer.\n      lstm_size: An iterable of ints specifying the LSTM cell sizes to use.\n      output_fc_layer_params: Optional list of fully_connected parameters, where\n        each item is the number of units in the layer. This is applied after the\n        LSTM cell.\n      activation_fn: Activation function, e.g. tf.nn.relu, slim.leaky_relu, ...\n      kernel_initializer: kernel initializer for all layers except for the value\n        regression layer. If None, a VarianceScaling initializer will be used.\n      last_kernel_initializer: kernel initializer for the value regression layer\n        . If None, a RandomUniform initializer will be used.\n      rnn_construction_fn: (Optional.) Alternate RNN construction function, e.g.\n        tf.keras.layers.LSTM, tf.keras.layers.CuDNNLSTM. It is invalid to\n        provide both rnn_construction_fn and lstm_size.\n      rnn_construction_kwargs: (Optional.) Dictionary or arguments to pass to\n        rnn_construction_fn.\n\n        The RNN will be constructed via:\n\n        ```\n        rnn_layer = rnn_construction_fn(**rnn_construction_kwargs)\n        ```\n      name: A string representing name of the network.\n\n    Raises:\n      ValueError: If `observation_spec` or `action_spec` contains more than one\n        item.\n      ValueError: If neither `lstm_size` nor `rnn_construction_fn` are provided.\n      ValueError: If both `lstm_size` and `rnn_construction_fn` are provided.\n    """"""\n    if lstm_size is None and rnn_construction_fn is None:\n      raise ValueError(\'Need to provide either custom rnn_construction_fn or \'\n                       \'lstm_size.\')\n    if lstm_size and rnn_construction_fn:\n      raise ValueError(\'Cannot provide both custom rnn_construction_fn and \'\n                       \'lstm_size.\')\n\n    observation_spec, action_spec = input_tensor_spec\n\n    if len(tf.nest.flatten(observation_spec)) > 1:\n      raise ValueError(\n          \'Only a single observation is supported by this network.\')\n\n    if len(tf.nest.flatten(action_spec)) > 1:\n      raise ValueError(\'Only a single action is supported by this network.\')\n\n    if kernel_initializer is None:\n      kernel_initializer = tf.compat.v1.keras.initializers.VarianceScaling(\n          scale=1. / 3., mode=\'fan_in\', distribution=\'uniform\')\n    if last_kernel_initializer is None:\n      last_kernel_initializer = tf.keras.initializers.RandomUniform(\n          minval=-0.003, maxval=0.003)\n\n    observation_layers = utils.mlp_layers(\n        observation_conv_layer_params,\n        observation_fc_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'observation_encoding\')\n\n    action_layers = utils.mlp_layers(\n        None,\n        action_fc_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'action_encoding\')\n\n    joint_layers = utils.mlp_layers(\n        None,\n        joint_fc_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        name=\'joint_mlp\')\n\n    # Create RNN cell\n    if rnn_construction_fn:\n      rnn_construction_kwargs = rnn_construction_kwargs or {}\n      lstm_network = rnn_construction_fn(**rnn_construction_kwargs)\n    else:\n      if len(lstm_size) == 1:\n        cell = tf.keras.layers.LSTMCell(lstm_size[0])\n      else:\n        cell = tf.keras.layers.StackedRNNCells(\n            [tf.keras.layers.LSTMCell(size) for size in lstm_size])\n      lstm_network = dynamic_unroll_layer.DynamicUnroll(cell)\n\n    counter = [-1]\n\n    def create_spec(size):\n      counter[0] += 1\n      return tensor_spec.TensorSpec(\n          size, dtype=tf.float32, name=\'network_state_%d\' % counter[0])\n\n    state_spec = tf.nest.map_structure(create_spec,\n                                       lstm_network.cell.state_size)\n\n    output_layers = utils.mlp_layers(fc_layer_params=output_fc_layer_params,\n                                     name=\'output\')\n\n    output_layers.append(\n        tf.keras.layers.Dense(\n            1,\n            activation=None,\n            kernel_initializer=last_kernel_initializer,\n            name=\'value\'))\n\n    super(CriticRnnNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=state_spec,\n        name=name)\n\n    self._observation_layers = observation_layers\n    self._action_layers = action_layers\n    self._joint_layers = joint_layers\n    self._lstm_network = lstm_network\n    self._output_layers = output_layers\n\n  # TODO(kbanoop): Standardize argument names across different networks.\n  def call(self, inputs, step_type, network_state=(), training=False):\n    observation, action = inputs\n    observation_spec, _ = self.input_tensor_spec\n    num_outer_dims = nest_utils.get_outer_rank(observation,\n                                               observation_spec)\n    if num_outer_dims not in (1, 2):\n      raise ValueError(\n          \'Input observation must have a batch or batch x time outer shape.\')\n\n    has_time_dim = num_outer_dims == 2\n    if not has_time_dim:\n      # Add a time dimension to the inputs.\n      observation = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                          observation)\n      action = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), action)\n      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n                                        step_type)\n\n    observation = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n    action = tf.cast(tf.nest.flatten(action)[0], tf.float32)\n\n    batch_squash = utils.BatchSquash(2)  # Squash B, and T dims.\n    observation = batch_squash.flatten(observation)  # [B, T, ...] -> [BxT, ...]\n    action = batch_squash.flatten(action)\n\n    for layer in self._observation_layers:\n      observation = layer(observation, training=training)\n\n    for layer in self._action_layers:\n      action = layer(action, training=training)\n\n    joint = tf.concat([observation, action], -1)\n    for layer in self._joint_layers:\n      joint = layer(joint, training=training)\n\n    joint = batch_squash.unflatten(joint)  # [B x T, ...] -> [B, T, ...]\n\n    network_kwargs = {}\n    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n      network_kwargs[\'reset_mask\'] = tf.equal(step_type,\n                                              time_step.StepType.FIRST,\n                                              name=\'mask\')\n\n    # Unroll over the time sequence.\n    output = self._lstm_network(\n        inputs=joint,\n        initial_state=network_state,\n        training=training,\n        **network_kwargs)\n    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n      joint, network_state = output\n    else:\n      joint = output[0]\n      network_state = tf.nest.pack_sequence_as(\n          self._lstm_network.cell.state_size, tf.nest.flatten(output[1:]))\n\n    output = batch_squash.flatten(joint)  # [B, T, ...] -> [B x T, ...]\n\n    for layer in self._output_layers:\n      output = layer(output, training=training)\n\n    q_value = tf.reshape(output, [-1])\n    q_value = batch_squash.unflatten(q_value)  # [B x T, ...] -> [B, T, ...]\n    if not has_time_dim:\n      q_value = tf.squeeze(q_value, axis=1)\n\n    return q_value, network_state\n'"
tf_agents/agents/ddpg/critic_rnn_network_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.ddpg.critic_rnn_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom absl.testing import parameterized\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\n\n\ndef lstm_keras_fn(lstm_size):\n  return tf.keras.layers.LSTM(lstm_size, return_state=True,\n                              return_sequences=True)\n\n\ndef rnn_keras_fn(lstm_size):\n  cell = tf.keras.layers.SimpleRNNCell(lstm_size)\n  return tf.keras.layers.RNN(cell, return_state=True,\n                             return_sequences=True)\n\n\nclass CriticRnnNetworkTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      (\'RNNKerasUnroll\', None, rnn_keras_fn),\n  )\n  def testBuildsRnn(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3)\n    network_input_spec = (observation_spec, action_spec)\n    action = tensor_spec.sample_spec_nest(action_spec, outer_dims=(1,))\n    net = critic_rnn_network.CriticRnnNetwork(\n        network_input_spec,\n        observation_conv_layer_params=[(4, 2, 2)],\n        observation_fc_layer_params=(4,),\n        action_fc_layer_params=(5,),\n        joint_fc_layer_params=(5,),\n        lstm_size=lstm_size,\n        output_fc_layer_params=(5,),\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    network_input = (time_step.observation, action)\n    q_values, network_state = net(network_input, time_step.step_type,\n                                  net.get_initial_state(batch_size=1))\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1], q_values.shape.as_list())\n\n    self.assertLen(net.variables, 15)\n    # Obs Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Obs Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Obs Fc Kernel\n    self.assertEqual((64, 4), net.variables[2].shape)\n    # Obs Fc Bias\n    self.assertEqual((4,), net.variables[3].shape)\n    # Action Fc Kernel\n    self.assertEqual((2, 5), net.variables[4].shape)\n    # Action Fc Bias\n    self.assertEqual((5,), net.variables[5].shape)\n    # Joint Fc Kernel\n    self.assertEqual((9, 5), net.variables[6].shape)\n    # Joint Fc Bias\n    self.assertEqual((5,), net.variables[7].shape)\n    # RNN Cell Kernel\n    self.assertEqual((5, 3), net.variables[8].shape)\n    # RNN Cell Recurrent Kernel\n    self.assertEqual((3, 3), net.variables[9].shape)\n    # RNN Cell Bias\n    self.assertEqual((3,), net.variables[10].shape)\n    # Output Fc Kernel\n    self.assertEqual((3, 5), net.variables[11].shape)\n    # Output Fc Bias\n    self.assertEqual((5,), net.variables[12].shape)\n    # Q Value Kernel\n    self.assertEqual((5, 1), net.variables[13].shape)\n    # Q Value Bias\n    self.assertEqual((1,), net.variables[14].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((3,), network_state[0].shape)\n\n  @parameterized.named_parameters(\n      (\'DynamicUnroll\', (3,), None),\n      (\'LSTMKerasUnroll\', None, lstm_keras_fn),\n  )\n  def testBuilds(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    time_step = tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(1,))\n\n    action_spec = tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3)\n    network_input_spec = (observation_spec, action_spec)\n    action = tensor_spec.sample_spec_nest(action_spec, outer_dims=(1,))\n    net = critic_rnn_network.CriticRnnNetwork(\n        network_input_spec,\n        observation_conv_layer_params=[(4, 2, 2)],\n        observation_fc_layer_params=(4,),\n        action_fc_layer_params=(5,),\n        joint_fc_layer_params=(5,),\n        lstm_size=lstm_size,\n        output_fc_layer_params=(5,),\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n    network_input = (time_step.observation, action)\n    q_values, network_state = net(network_input, time_step.step_type,\n                                  net.get_initial_state(batch_size=1))\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual([1], q_values.shape.as_list())\n\n    self.assertLen(net.variables, 15)\n    print(\'variables %s\', net.variables)\n    # Obs Conv Net Kernel\n    self.assertEqual((2, 2, 3, 4), net.variables[0].shape)\n    # Obs Conv Net bias\n    self.assertEqual((4,), net.variables[1].shape)\n    # Obs Fc Kernel\n    self.assertEqual((64, 4), net.variables[2].shape)\n    # Obs Fc Bias\n    self.assertEqual((4,), net.variables[3].shape)\n    # Action Fc Kernel\n    self.assertEqual((2, 5), net.variables[4].shape)\n    # Action Fc Bias\n    self.assertEqual((5,), net.variables[5].shape)\n    # Joint Fc Kernel\n    self.assertEqual((9, 5), net.variables[6].shape)\n    # Joint Fc Bias\n    self.assertEqual((5,), net.variables[7].shape)\n    # LSTM Cell Kernel\n    self.assertEqual((5, 12), net.variables[8].shape)\n    # LSTM Cell Recurrent Kernel\n    self.assertEqual((3, 12), net.variables[9].shape)\n    # LSTM Cell Bias\n    self.assertEqual((12,), net.variables[10].shape)\n    # Output Fc Kernel\n    self.assertEqual((3, 5), net.variables[11].shape)\n    # Output Fc Bias\n    self.assertEqual((5,), net.variables[12].shape)\n    # Q Value Kernel\n    self.assertEqual((5, 1), net.variables[13].shape)\n    # Q Value Bias\n    self.assertEqual((1,), net.variables[14].shape)\n\n    # Assert LSTM cell is created.\n    self.assertEqual((1, 3), network_state[0].shape)\n    self.assertEqual((1, 3), network_state[1].shape)\n\n  @parameterized.named_parameters(\n      (\'DynamicUnroll\', (3,), None),\n      (\'LSTMKerasUnroll\', None, lstm_keras_fn),\n      (\'RNNKerasUnroll\', None, rnn_keras_fn),\n  )\n  def testInit(self, lstm_size, rnn_construction_fn):\n    observation_spec = tensor_spec.BoundedTensorSpec((8, 8, 3), tf.float32, 0,\n                                                     1)\n    action_spec = tensor_spec.BoundedTensorSpec((2,), tf.float32, 2, 3)\n    network_input_spec = (observation_spec, action_spec)\n    critic_rnn_network.CriticRnnNetwork(\n        network_input_spec,\n        lstm_size=lstm_size,\n        rnn_construction_fn=rnn_construction_fn,\n        rnn_construction_kwargs={\'lstm_size\': 3})\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ddpg/ddpg_agent.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A DDPG Agent.\n\nImplements the Deep Deterministic Policy Gradient (DDPG) algorithm from\n""Continuous control with deep reinforcement learning"" - Lilicrap et al.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import ou_noise_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\n\n\nclass DdpgInfo(collections.namedtuple(\n    \'DdpgInfo\', (\'actor_loss\', \'critic_loss\'))):\n  pass\n\n\n@gin.configurable\nclass DdpgAgent(tf_agent.TFAgent):\n  """"""A DDPG Agent.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               actor_network: network.Network,\n               critic_network: network.Network,\n               actor_optimizer: Optional[types.Optimizer] = None,\n               critic_optimizer: Optional[types.Optimizer] = None,\n               ou_stddev: types.Float = 1.0,\n               ou_damping: types.Float = 1.0,\n               target_actor_network: Optional[network.Network] = None,\n               target_critic_network: Optional[network.Network] = None,\n               target_update_tau: types.Float = 1.0,\n               target_update_period: types.Int = 1,\n               dqda_clipping: Optional[types.Float] = None,\n               td_errors_loss_fn: Optional[types.LossFn] = None,\n               gamma: types.Float = 1.0,\n               reward_scale_factor: types.Float = 1.0,\n               gradient_clipping: Optional[types.Float] = None,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               train_step_counter: Optional[tf.Variable] = None,\n               name: Optional[Text] = None):\n    """"""Creates a DDPG Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      actor_network: A tf_agents.network.Network to be used by the agent. The\n        network will be called with call(observation, step_type[, policy_state])\n        and should return (action, new_state).\n      critic_network: A tf_agents.network.Network to be used by the agent. The\n        network will be called with call((observation, action), step_type[,\n        policy_state]) and should return (q_value, new_state).\n      actor_optimizer: The optimizer to use for the actor network.\n      critic_optimizer: The optimizer to use for the critic network.\n      ou_stddev: Standard deviation for the Ornstein-Uhlenbeck (OU) noise added\n        in the default collect policy.\n      ou_damping: Damping factor for the OU noise added in the default collect\n        policy.\n      target_actor_network: (Optional.)  A `tf_agents.network.Network` to be\n        used as the actor target network during Q learning.  Every\n        `target_update_period` train steps, the weights from `actor_network` are\n        copied (possibly withsmoothing via `target_update_tau`) to `\n        target_q_network`.\n\n        If `target_actor_network` is not provided, it is created by making a\n        copy of `actor_network`, which initializes a new network with the same\n        structure and its own layers and weights.\n\n        Performing a `Network.copy` does not work when the network instance\n        already has trainable parameters (e.g., has already been built, or\n        when the network is sharing layers with another).  In these cases, it is\n        up to you to build a copy having weights that are not\n        shared with the original `actor_network`, so that this can be used as a\n        target network.  If you provide a `target_actor_network` that shares any\n        weights with `actor_network`, a warning will be logged but no exception\n        is thrown.\n      target_critic_network: (Optional.) Similar network as target_actor_network\n         but for the critic_network. See documentation for target_actor_network.\n      target_update_tau: Factor for soft update of the target networks.\n      target_update_period: Period for soft update of the target networks.\n      dqda_clipping: when computing the actor loss, clips the gradient dqda\n        element-wise between [-dqda_clipping, dqda_clipping]. Does not perform\n        clipping if dqda_clipping == 0.\n      td_errors_loss_fn:  A function for computing the TD errors loss. If None,\n        a default value of elementwise huber_loss is used.\n      gamma: A discount factor for future rewards.\n      reward_scale_factor: Multiplicative scale for the reward.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n    self._actor_network = actor_network\n    actor_network.create_variables()\n    if target_actor_network:\n      target_actor_network.create_variables()\n    self._target_actor_network = common.maybe_copy_target_network_with_checks(\n        self._actor_network, target_actor_network, \'TargetActorNetwork\')\n    self._critic_network = critic_network\n    critic_network.create_variables()\n    if target_critic_network:\n      target_critic_network.create_variables()\n    self._target_critic_network = common.maybe_copy_target_network_with_checks(\n        self._critic_network, target_critic_network, \'TargetCriticNetwork\')\n\n    self._actor_optimizer = actor_optimizer\n    self._critic_optimizer = critic_optimizer\n\n    self._ou_stddev = ou_stddev\n    self._ou_damping = ou_damping\n    self._target_update_tau = target_update_tau\n    self._target_update_period = target_update_period\n    self._dqda_clipping = dqda_clipping\n    self._td_errors_loss_fn = (\n        td_errors_loss_fn or common.element_wise_huber_loss)\n    self._gamma = gamma\n    self._reward_scale_factor = reward_scale_factor\n    self._gradient_clipping = gradient_clipping\n\n    self._update_target = self._get_target_updater(\n        target_update_tau, target_update_period)\n\n    policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec,\n        actor_network=self._actor_network, clip=True)\n    collect_policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec,\n        actor_network=self._actor_network, clip=False)\n    collect_policy = ou_noise_policy.OUNoisePolicy(\n        collect_policy,\n        ou_stddev=self._ou_stddev,\n        ou_damping=self._ou_damping,\n        clip=True)\n\n    super(DdpgAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=2 if not self._actor_network.state_spec else None,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _initialize(self):\n    common.soft_variables_update(\n        self._critic_network.variables,\n        self._target_critic_network.variables,\n        tau=1.0)\n    common.soft_variables_update(\n        self._actor_network.variables,\n        self._target_actor_network.variables,\n        tau=1.0)\n\n  def _get_target_updater(self, tau=1.0, period=1):\n    """"""Performs a soft update of the target network parameters.\n\n    For each weight w_s in the original network, and its corresponding\n    weight w_t in the target network, a soft update is:\n    w_t = (1- tau) x w_t + tau x ws\n\n    Args:\n      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n      period: Step interval at which the target networks are updated.\n    Returns:\n      An operation that performs a soft update of the target network parameters.\n    """"""\n    with tf.name_scope(\'get_target_updater\'):\n      def update():\n        """"""Update target network.""""""\n        # TODO(b/124381161): What about observation normalizer variables?\n        critic_update = common.soft_variables_update(\n            self._critic_network.variables,\n            self._target_critic_network.variables,\n            tau,\n            tau_non_trainable=1.0)\n        actor_update = common.soft_variables_update(\n            self._actor_network.variables,\n            self._target_actor_network.variables,\n            tau,\n            tau_non_trainable=1.0)\n        return tf.group(critic_update, actor_update)\n\n      return common.Periodically(update, period, \'periodic_update_targets\')\n\n  def _train(self, experience, weights=None):\n    squeeze_time_dim = not self._actor_network.state_spec\n    time_steps, policy_steps, next_time_steps = (\n        trajectory.experience_to_transitions(experience, squeeze_time_dim))\n    actions = policy_steps.action\n\n    # TODO(b/124382524): Apply a loss mask or filter boundary transitions.\n    trainable_critic_variables = self._critic_network.trainable_variables\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_critic_variables, (\'No trainable critic variables to \'\n                                          \'optimize.\')\n      tape.watch(trainable_critic_variables)\n      critic_loss = self.critic_loss(time_steps, actions, next_time_steps,\n                                     weights=weights, training=True)\n    tf.debugging.check_numerics(critic_loss, \'Critic loss is inf or nan.\')\n    critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n    self._apply_gradients(critic_grads, trainable_critic_variables,\n                          self._critic_optimizer)\n\n    trainable_actor_variables = self._actor_network.trainable_variables\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_actor_variables, (\'No trainable actor variables to \'\n                                         \'optimize.\')\n      tape.watch(trainable_actor_variables)\n      actor_loss = self.actor_loss(time_steps, weights=weights, training=True)\n    tf.debugging.check_numerics(actor_loss, \'Actor loss is inf or nan.\')\n    actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n    self._apply_gradients(actor_grads, trainable_actor_variables,\n                          self._actor_optimizer)\n\n    self.train_step_counter.assign_add(1)\n    self._update_target()\n\n    # TODO(b/124382360): Compute per element TD loss and return in loss_info.\n    total_loss = actor_loss + critic_loss\n    return tf_agent.LossInfo(total_loss,\n                             DdpgInfo(actor_loss, critic_loss))\n\n  def _apply_gradients(self, gradients, variables, optimizer):\n    # Tuple is used for py3, where zip is a generator producing values once.\n    grads_and_vars = tuple(zip(gradients, variables))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n                                                       self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      eager_utils.add_variables_summaries(grads_and_vars,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n\n    optimizer.apply_gradients(grads_and_vars)\n\n  def critic_loss(self,\n                  time_steps: ts.TimeStep,\n                  actions: types.NestedTensor,\n                  next_time_steps: ts.TimeStep,\n                  weights: Optional[types.Tensor] = None,\n                  training: bool = False) -> types.Tensor:\n    """"""Computes the critic loss for DDPG training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      actions: A batch of actions.\n      next_time_steps: A batch of next timesteps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.\n      training: Whether this loss is being used for training.\n    Returns:\n      critic_loss: A scalar critic loss.\n    """"""\n    with tf.name_scope(\'critic_loss\'):\n      target_actions, _ = self._target_actor_network(\n          next_time_steps.observation, next_time_steps.step_type,\n          training=False)\n      target_critic_net_input = (next_time_steps.observation, target_actions)\n      target_q_values, _ = self._target_critic_network(\n          target_critic_net_input, next_time_steps.step_type,\n          training=False)\n\n      td_targets = tf.stop_gradient(\n          self._reward_scale_factor * next_time_steps.reward +\n          self._gamma * next_time_steps.discount * target_q_values)\n\n      critic_net_input = (time_steps.observation, actions)\n      q_values, _ = self._critic_network(critic_net_input,\n                                         time_steps.step_type,\n                                         training=training)\n\n      critic_loss = self._td_errors_loss_fn(td_targets, q_values)\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Do a sum over the time dimension.\n        critic_loss = tf.reduce_sum(critic_loss, axis=1)\n      if weights is not None:\n        critic_loss *= weights\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      with tf.name_scope(\'Losses/\'):\n        tf.compat.v2.summary.scalar(\n            name=\'critic_loss\', data=critic_loss, step=self.train_step_counter)\n\n      if self._debug_summaries:\n        td_errors = td_targets - q_values\n        common.generate_tensor_summaries(\'td_errors\', td_errors,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'td_targets\', td_targets,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'q_values\', q_values,\n                                         self.train_step_counter)\n\n      return critic_loss\n\n  def actor_loss(self,\n                 time_steps: ts.TimeStep,\n                 weights: Optional[types.Tensor] = None,\n                 training: bool = False) -> types.Tensor:\n    """"""Computes the actor_loss for DDPG training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.\n      training: Whether this loss is being used for training.\n      # TODO(b/124383618): Add an action norm regularizer.\n    Returns:\n      actor_loss: A scalar actor loss.\n    """"""\n    with tf.name_scope(\'actor_loss\'):\n      actions, _ = self._actor_network(time_steps.observation,\n                                       time_steps.step_type,\n                                       training=training)\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(actions)\n        q_values, _ = self._critic_network((time_steps.observation, actions),\n                                           time_steps.step_type,\n                                           training=False)\n        actions = tf.nest.flatten(actions)\n\n      dqdas = tape.gradient([q_values], actions)\n\n      actor_losses = []\n      for dqda, action in zip(dqdas, actions):\n        if self._dqda_clipping is not None:\n          dqda = tf.clip_by_value(dqda, -1 * self._dqda_clipping,\n                                  self._dqda_clipping)\n        loss = common.element_wise_squared_loss(\n            tf.stop_gradient(dqda + action), action)\n        if nest_utils.is_batched_nested_tensors(\n            time_steps, self.time_step_spec, num_outer_dims=2):\n          # Sum over the time dimension.\n          loss = tf.reduce_sum(loss, axis=1)\n        if weights is not None:\n          loss *= weights\n        loss = tf.reduce_mean(loss)\n        actor_losses.append(loss)\n\n      actor_loss = tf.add_n(actor_losses)\n\n      with tf.name_scope(\'Losses/\'):\n        tf.compat.v2.summary.scalar(\n            name=\'actor_loss\', data=actor_loss, step=self.train_step_counter)\n\n    return actor_loss\n'"
tf_agents/agents/ddpg/ddpg_agent_test.py,30,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.ddpg.ddpg_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActorNetwork(network.Network):\n  """"""Creates an actor network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               unbounded_actions=False,\n               name=None):\n    super(DummyActorNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    self._output_tensor_spec = output_tensor_spec\n    self._unbounded_actions = unbounded_actions\n    activation = None if unbounded_actions else tf.keras.activations.tanh\n\n    self._single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    self._layer = tf.keras.layers.Dense(\n        self._single_action_spec.shape.num_elements(),\n        activation=activation,\n        kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n        bias_initializer=tf.compat.v1.initializers.constant([5]),\n        name=\'action\')\n\n  def call(self, observations, step_type=(), network_state=()):\n    del step_type  # unused.\n    observations = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    output = self._layer(observations)\n    actions = tf.reshape(output,\n                         [-1] + self._single_action_spec.shape.as_list())\n\n    if not self._unbounded_actions:\n      actions = common.scale_to_spec(actions, self._single_action_spec)\n\n    output_actions = tf.nest.pack_sequence_as(self._output_tensor_spec,\n                                              [actions])\n    return output_actions, network_state\n\n\nclass DummyCriticNetwork(network.Network):\n\n  def __init__(self, input_tensor_spec, name=None):\n    super(DummyCriticNetwork, self).__init__(\n        input_tensor_spec, state_spec=(), name=name)\n\n    self._obs_layer = tf.keras.layers.Flatten()\n    self._action_layer = tf.keras.layers.Flatten()\n    self._joint_layer = tf.keras.layers.Dense(\n        1,\n        kernel_initializer=tf.compat.v1.initializers.constant([1, 3, 2]),\n        bias_initializer=tf.compat.v1.initializers.constant([4]))\n\n  def call(self, inputs, step_type=None, network_state=()):\n    observations, actions = inputs\n    del step_type\n    observations = self._obs_layer(tf.nest.flatten(observations)[0])\n    actions = self._action_layer(tf.nest.flatten(actions)[0])\n    joint = tf.concat([observations, actions], 1)\n    q_value = self._joint_layer(joint)\n    q_value = tf.reshape(q_value, [-1])\n    return q_value, network_state\n\n\nclass DdpgAgentTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(DdpgAgentTest, self).setUp()\n    self._obs_spec = [tensor_spec.TensorSpec([2], tf.float32)]\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]\n\n    network_input_spec = (self._obs_spec, self._action_spec)\n    self._critic_net = DummyCriticNetwork(network_input_spec)\n    self._bounded_actor_net = DummyActorNetwork(\n        self._obs_spec, self._action_spec, unbounded_actions=False)\n    self._unbounded_actor_net = DummyActorNetwork(\n        self._obs_spec, self._action_spec, unbounded_actions=True)\n\n  def testCreateAgent(self):\n    agent = ddpg_agent.DdpgAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=self._bounded_actor_net,\n        critic_network=self._critic_net,\n        actor_optimizer=None,\n        critic_optimizer=None,\n    )\n    self.assertIsNotNone(agent.policy)\n    self.assertIsNotNone(agent.collect_policy)\n\n  def testCriticLoss(self):\n    agent = ddpg_agent.DdpgAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=self._unbounded_actor_net,\n        critic_network=self._critic_net,\n        actor_optimizer=None,\n        critic_optimizer=None,\n    )\n\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = [tf.constant([[5], [6]], dtype=tf.float32)]\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = [tf.constant([[5, 6], [7, 8]], dtype=tf.float32)]\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    expected_loss = 59.6\n    loss = agent.critic_loss(time_steps, actions, next_time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testActorLoss(self):\n    agent = ddpg_agent.DdpgAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=self._unbounded_actor_net,\n        critic_network=self._critic_net,\n        actor_optimizer=None,\n        critic_optimizer=None,\n    )\n\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n\n    expected_loss = 4.0\n    loss = agent.actor_loss(time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testPolicy(self):\n    agent = ddpg_agent.DdpgAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=self._unbounded_actor_net,\n        critic_network=self._critic_net,\n        actor_optimizer=None,\n        critic_optimizer=None,\n    )\n\n    observations = [tf.constant([[1, 2]], dtype=tf.float32)]\n    time_steps = ts.restart(observations)\n    action_step = agent.policy.action(time_steps)\n    self.assertEqual(action_step.action[0].shape.as_list(), [1, 1])\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(all(actions_[0] <= self._action_spec[0].maximum))\n    self.assertTrue(all(actions_[0] >= self._action_spec[0].minimum))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/dqn/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A DQN (Deep Q Network) agent.""""""\nfrom tf_agents.agents.dqn import dqn_agent\n'"
tf_agents/agents/dqn/dqn_agent.py,22,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A DQN Agent.\n\nImplements the DQN algorithm from\n\n""Human level control through deep reinforcement learning""\n  Mnih et al., 2015\n  https://deepmind.com/research/dqn/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import boltzmann_policy\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import q_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import value_ops\n\n\nclass DqnLossInfo(collections.namedtuple(\'DqnLossInfo\',\n                                         (\'td_loss\', \'td_error\'))):\n  """"""DqnLossInfo is stored in the `extras` field of the LossInfo instance.\n\n  Both `td_loss` and `td_error` have a validity mask applied to ensure that\n  no loss or error is calculated for episode boundaries.\n\n  td_loss: The **weighted** TD loss (depends on choice of loss metric and\n    any weights passed to the DQN loss function.\n  td_error: The **unweighted** TD errors, which are just calculated as:\n\n    ```\n    td_error = td_targets - q_values\n    ```\n\n    These can be used to update Prioritized Replay Buffer priorities.\n\n    Note that, unlike `td_loss`, `td_error` may contain a time dimension when\n    training with RNN mode.  For `td_loss`, this axis is averaged out.\n  """"""\n  pass\n\n\ndef compute_td_targets(next_q_values: types.Tensor,\n                       rewards: types.Tensor,\n                       discounts: types.Tensor) -> types.Tensor:\n  return tf.stop_gradient(rewards + discounts * next_q_values)\n\n\n@gin.configurable\nclass DqnAgent(tf_agent.TFAgent):\n  """"""A DQN Agent.\n\n  Implements the DQN algorithm from\n\n  ""Human level control through deep reinforcement learning""\n    Mnih et al., 2015\n    https://deepmind.com/research/dqn/\n\n  This agent also implements n-step updates. See ""Rainbow: Combining\n  Improvements in Deep Reinforcement Learning"" by Hessel et al., 2017, for a\n  discussion on its benefits: https://arxiv.org/abs/1710.02298\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      q_network: network.Network,\n      optimizer: types.Optimizer,\n      observation_and_action_constraint_splitter: Optional[\n          types.Splitter] = None,\n      epsilon_greedy: types.Float = 0.1,\n      n_step_update: int = 1,\n      boltzmann_temperature: Optional[types.Int] = None,\n      emit_log_probability: bool = False,\n      # Params for target network updates\n      target_q_network: Optional[network.Network] = None,\n      target_update_tau: types.Float = 1.0,\n      target_update_period: int = 1,\n      # Params for training.\n      td_errors_loss_fn: Optional[types.LossFn] = None,\n      gamma: types.Float = 1.0,\n      reward_scale_factor: types.Float = 1.0,\n      gradient_clipping: Optional[types.Float] = None,\n      # Params for debugging\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = None):\n    """"""Creates a DQN Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      q_network: A `tf_agents.network.Network` to be used by the agent. The\n        network will be called with `call(observation, step_type)` and should\n        emit logits over the action space.\n      optimizer: The optimizer to use for training.\n      observation_and_action_constraint_splitter: A function used to process\n        observations with action constraints. These constraints can indicate,\n        for example, a mask of valid/invalid actions for a given state of the\n        environment.\n        The function takes in a full observation and returns a tuple consisting\n        of 1) the part of the observation intended as input to the network and\n        2) the constraint. An example\n        `observation_and_action_constraint_splitter` could be as simple as:\n        ```\n        def observation_and_action_constraint_splitter(observation):\n          return observation[\'network_input\'], observation[\'constraint\']\n        ```\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the provided `q_network` is compatible with the network-specific\n        half of the output of the `observation_and_action_constraint_splitter`.\n        In particular, `observation_and_action_constraint_splitter` will be\n        called on the observation before passing to the network.\n        If `observation_and_action_constraint_splitter` is None, action\n        constraints are not applied.\n      epsilon_greedy: probability of choosing a random action in the default\n        epsilon-greedy collect policy (used only if a wrapper is not provided to\n        the collect_policy method).\n      n_step_update: The number of steps to consider when computing TD error and\n        TD loss. Defaults to single-step updates. Note that this requires the\n        user to call train on Trajectory objects with a time dimension of\n        `n_step_update + 1`. However, note that we do not yet support\n        `n_step_update > 1` in the case of RNNs (i.e., non-empty\n        `q_network.state_spec`).\n      boltzmann_temperature: Temperature value to use for Boltzmann sampling of\n        the actions during data collection. The closer to 0.0, the higher the\n        probability of choosing the best action.\n      emit_log_probability: Whether policies emit log probabilities or not.\n      target_q_network: (Optional.)  A `tf_agents.network.Network`\n        to be used as the target network during Q learning.  Every\n        `target_update_period` train steps, the weights from\n        `q_network` are copied (possibly with smoothing via\n        `target_update_tau`) to `target_q_network`.\n\n        If `target_q_network` is not provided, it is created by\n        making a copy of `q_network`, which initializes a new\n        network with the same structure and its own layers and weights.\n\n        Network copying is performed via the `Network.copy` superclass method,\n        and may inadvertently lead to the resulting network to share weights\n        with the original.  This can happen if, for example, the original\n        network accepted a pre-built Keras layer in its `__init__`, or\n        accepted a Keras layer that wasn\'t built, but neglected to create\n        a new copy.\n\n        In these cases, it is up to you to provide a target Network having\n        weights that are not shared with the original `q_network`.\n        If you provide a `target_q_network` that shares any\n        weights with `q_network`, a warning will be logged but\n        no exception is thrown.\n\n        Note; shallow copies of Keras layers may be built via the code:\n\n        ```python\n        new_layer = type(layer).from_config(layer.get_config())\n        ```\n      target_update_tau: Factor for soft update of the target networks.\n      target_update_period: Period for soft update of the target networks.\n      td_errors_loss_fn: A function for computing the TD errors loss. If None, a\n        default value of element_wise_huber_loss is used. This function takes as\n        input the target and the estimated Q values and returns the loss for\n        each element of the batch.\n      gamma: A discount factor for future rewards.\n      reward_scale_factor: Multiplicative scale for the reward.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the action spec contains more than one action or action\n        spec minimum is not equal to 0.\n      NotImplementedError: If `q_network` has non-empty `state_spec` (i.e., an\n        RNN is provided) and `n_step_update > 1`.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    self._check_action_spec(action_spec)\n\n    if epsilon_greedy is not None and boltzmann_temperature is not None:\n      raise ValueError(\n          \'Configured both epsilon_greedy value {} and temperature {}, \'\n          \'however only one of them can be used for exploration.\'.format(\n              epsilon_greedy, boltzmann_temperature))\n\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n    self._q_network = q_network\n    q_network.create_variables()\n    if target_q_network:\n      target_q_network.create_variables()\n    self._target_q_network = common.maybe_copy_target_network_with_checks(\n        self._q_network, target_q_network, \'TargetQNetwork\')\n\n    self._epsilon_greedy = epsilon_greedy\n    self._n_step_update = n_step_update\n    self._boltzmann_temperature = boltzmann_temperature\n    self._optimizer = optimizer\n    self._td_errors_loss_fn = (\n        td_errors_loss_fn or common.element_wise_huber_loss)\n    self._gamma = gamma\n    self._reward_scale_factor = reward_scale_factor\n    self._gradient_clipping = gradient_clipping\n    self._update_target = self._get_target_updater(\n        target_update_tau, target_update_period)\n\n    policy, collect_policy = self._setup_policy(time_step_spec, action_spec,\n                                                boltzmann_temperature,\n                                                emit_log_probability)\n\n    if q_network.state_spec and n_step_update != 1:\n      raise NotImplementedError(\n          \'DqnAgent does not currently support n-step updates with stateful \'\n          \'networks (i.e., RNNs), but n_step_update = {}\'.format(n_step_update))\n\n    train_sequence_length = (\n        n_step_update + 1 if not q_network.state_spec else None)\n\n    super(DqnAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=train_sequence_length,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _check_action_spec(self, action_spec):\n    flat_action_spec = tf.nest.flatten(action_spec)\n    self._num_actions = [\n        spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n    ]\n\n    # TODO(oars): Get DQN working with more than one dim in the actions.\n    if len(flat_action_spec) > 1 or flat_action_spec[0].shape.rank > 1:\n      raise ValueError(\'Only one dimensional actions are supported now.\')\n\n    # TODO(b/119321125): Disable this once index_with_actions supports\n    # negative-valued actions.\n    if not all(spec.minimum == 0 for spec in flat_action_spec):\n      raise ValueError(\n          \'Action specs should have minimum of 0, but saw: {0}\'.format(\n              [spec.minimum for spec in flat_action_spec]))\n\n  def _setup_policy(self, time_step_spec, action_spec,\n                    boltzmann_temperature, emit_log_probability):\n\n    policy = q_policy.QPolicy(\n        time_step_spec,\n        action_spec,\n        q_network=self._q_network,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            self._observation_and_action_constraint_splitter))\n\n    if boltzmann_temperature is not None:\n      collect_policy = boltzmann_policy.BoltzmannPolicy(\n          policy, temperature=self._boltzmann_temperature)\n    else:\n      collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n          policy, epsilon=self._epsilon_greedy)\n    policy = greedy_policy.GreedyPolicy(policy)\n\n    # Create self._target_greedy_policy in order to compute target Q-values.\n    target_policy = q_policy.QPolicy(\n        time_step_spec,\n        action_spec,\n        q_network=self._target_q_network,\n        observation_and_action_constraint_splitter=(\n            self._observation_and_action_constraint_splitter))\n    self._target_greedy_policy = greedy_policy.GreedyPolicy(target_policy)\n\n    return policy, collect_policy\n\n  def _initialize(self):\n    common.soft_variables_update(\n        self._q_network.variables, self._target_q_network.variables, tau=1.0)\n\n  def _get_target_updater(self, tau=1.0, period=1):\n    """"""Performs a soft update of the target network parameters.\n\n    For each weight w_s in the q network, and its corresponding\n    weight w_t in the target_q_network, a soft update is:\n    w_t = (1 - tau) * w_t + tau * w_s\n\n    Args:\n      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n      period: Step interval at which the target network is updated.\n\n    Returns:\n      A callable that performs a soft update of the target network parameters.\n    """"""\n    with tf.name_scope(\'update_targets\'):\n\n      def update():\n        return common.soft_variables_update(\n            self._q_network.variables,\n            self._target_q_network.variables,\n            tau,\n            tau_non_trainable=1.0)\n\n      return common.Periodically(update, period, \'periodic_update_targets\')\n\n  # Use @common.function in graph mode or for speeding up.\n  def _train(self, experience, weights):\n    with tf.GradientTape() as tape:\n      loss_info = self._loss(\n          experience,\n          td_errors_loss_fn=self._td_errors_loss_fn,\n          gamma=self._gamma,\n          reward_scale_factor=self._reward_scale_factor,\n          weights=weights,\n          training=True)\n    tf.debugging.check_numerics(loss_info.loss, \'Loss is inf or nan\')\n    variables_to_train = self._q_network.trainable_weights\n    non_trainable_weights = self._q_network.non_trainable_weights\n    assert list(variables_to_train), ""No variables in the agent\'s q_network.""\n    grads = tape.gradient(loss_info.loss, variables_to_train)\n    # Tuple is used for py3, where zip is a generator producing values once.\n    grads_and_vars = list(zip(grads, variables_to_train))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n                                                       self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      grads_and_vars_with_non_trainable = (\n          grads_and_vars + [(None, v) for v in non_trainable_weights])\n      eager_utils.add_variables_summaries(grads_and_vars_with_non_trainable,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n    self._optimizer.apply_gradients(grads_and_vars)\n    self.train_step_counter.assign_add(1)\n\n    self._update_target()\n\n    return loss_info\n\n  def _loss(self,\n            experience,\n            td_errors_loss_fn=common.element_wise_huber_loss,\n            gamma=1.0,\n            reward_scale_factor=1.0,\n            weights=None,\n            training=False):\n    """"""Computes loss for DQN training.\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.policy.step_spec`.\n        All tensors in `experience` must be shaped `[batch, time, ...]` where\n        `time` must be equal to `self.train_sequence_length` if that\n        property is not `None`.\n      td_errors_loss_fn: A function(td_targets, predictions) to compute the\n        element wise loss.\n      gamma: Discount for future rewards.\n      reward_scale_factor: Multiplicative factor to scale rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output td_loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether this loss is being used for training.\n\n    Returns:\n      loss: An instance of `DqnLossInfo`.\n    Raises:\n      ValueError:\n        if the number of actions is greater than 1.\n    """"""\n    # Check that `experience` includes two outer dimensions [B, T, ...]. This\n    # method requires a time dimension to compute the loss properly.\n    self._check_trajectory_dimensions(experience)\n\n    squeeze_time_dim = not self._q_network.state_spec\n    if self._n_step_update == 1:\n      time_steps, policy_steps, next_time_steps = (\n          trajectory.experience_to_transitions(experience, squeeze_time_dim))\n      actions = policy_steps.action\n    else:\n      # To compute n-step returns, we need the first time steps, the first\n      # actions, and the last time steps. Therefore we extract the first and\n      # last transitions from our Trajectory.\n      first_two_steps = tf.nest.map_structure(lambda x: x[:, :2], experience)\n      last_two_steps = tf.nest.map_structure(lambda x: x[:, -2:], experience)\n      time_steps, policy_steps, _ = (\n          trajectory.experience_to_transitions(\n              first_two_steps, squeeze_time_dim))\n      actions = policy_steps.action\n      _, _, next_time_steps = (\n          trajectory.experience_to_transitions(\n              last_two_steps, squeeze_time_dim))\n\n    with tf.name_scope(\'loss\'):\n      q_values = self._compute_q_values(time_steps, actions, training=training)\n\n      next_q_values = self._compute_next_q_values(\n          next_time_steps, policy_steps.info)\n\n      if self._n_step_update == 1:\n        # Special case for n = 1 to avoid a loss of performance.\n        td_targets = compute_td_targets(\n            next_q_values,\n            rewards=reward_scale_factor * next_time_steps.reward,\n            discounts=gamma * next_time_steps.discount)\n      else:\n        # When computing discounted return, we need to throw out the last time\n        # index of both reward and discount, which are filled with dummy values\n        # to match the dimensions of the observation.\n        rewards = reward_scale_factor * experience.reward[:, :-1]\n        discounts = gamma * experience.discount[:, :-1]\n\n        # TODO(b/134618876): Properly handle Trajectories that include episode\n        # boundaries with nonzero discount.\n\n        td_targets = value_ops.discounted_return(\n            rewards=rewards,\n            discounts=discounts,\n            final_value=next_q_values,\n            time_major=False,\n            provide_all_returns=False)\n\n      valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n      td_error = valid_mask * (td_targets - q_values)\n\n      td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)\n\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Do a sum over the time dimension.\n        td_loss = tf.reduce_sum(input_tensor=td_loss, axis=1)\n\n      # Aggregate across the elements of the batch and add regularization loss.\n      # Note: We use an element wise loss above to ensure each element is always\n      #   weighted by 1/N where N is the batch size, even when some of the\n      #   weights are zero due to boundary transitions. Weighting by 1/K where K\n      #   is the actual number of non-zero weight would artificially increase\n      #   their contribution in the loss. Think about what would happen as\n      #   the number of boundary samples increases.\n\n      agg_loss = common.aggregate_losses(\n          per_example_loss=td_loss,\n          sample_weight=weights,\n          regularization_loss=self._q_network.losses)\n      total_loss = agg_loss.total_loss\n\n      losses_dict = {\'td_loss\': agg_loss.weighted,\n                     \'reg_loss\': agg_loss.regularization,\n                     \'total_loss\': total_loss}\n\n      common.summarize_scalar_dict(losses_dict,\n                                   step=self.train_step_counter,\n                                   name_scope=\'Losses/\')\n\n      if self._summarize_grads_and_vars:\n        with tf.name_scope(\'Variables/\'):\n          for var in self._q_network.trainable_weights:\n            tf.compat.v2.summary.histogram(\n                name=var.name.replace(\':\', \'_\'),\n                data=var,\n                step=self.train_step_counter)\n\n      if self._debug_summaries:\n        diff_q_values = q_values - next_q_values\n        common.generate_tensor_summaries(\'td_error\', td_error,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'td_loss\', td_loss,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'q_values\', q_values,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'next_q_values\', next_q_values,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'diff_q_values\', diff_q_values,\n                                         self.train_step_counter)\n\n      return tf_agent.LossInfo(total_loss, DqnLossInfo(td_loss=td_loss,\n                                                       td_error=td_error))\n\n  def _compute_q_values(self, time_steps, actions, training=False):\n    network_observation = time_steps.observation\n\n    if self._observation_and_action_constraint_splitter is not None:\n      network_observation, _ = self._observation_and_action_constraint_splitter(\n          network_observation)\n\n    q_values, _ = self._q_network(network_observation, time_steps.step_type,\n                                  training=training)\n    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n    multi_dim_actions = self._action_spec.shape.rank > 0\n    return common.index_with_actions(\n        q_values,\n        tf.cast(actions, dtype=tf.int32),\n        multi_dim_actions=multi_dim_actions)\n\n  def _compute_next_q_values(self, next_time_steps, info):\n    """"""Compute the q value of the next state for TD error computation.\n\n    Args:\n      next_time_steps: A batch of next timesteps\n      info: PolicyStep.info that may be used by other agents inherited from\n        dqn_agent.\n\n    Returns:\n      A tensor of Q values for the given next state.\n    """"""\n    network_observation = next_time_steps.observation\n\n    if self._observation_and_action_constraint_splitter is not None:\n      network_observation, _ = self._observation_and_action_constraint_splitter(\n          network_observation)\n\n    next_target_q_values, _ = self._target_q_network(\n        network_observation, next_time_steps.step_type)\n    batch_size = (\n        next_target_q_values.shape[0] or tf.shape(next_target_q_values)[0])\n    dummy_state = self._target_greedy_policy.get_initial_state(batch_size)\n    # Find the greedy actions using our target greedy policy. This ensures that\n    # action constraints are respected and helps centralize the greedy logic.\n    greedy_actions = self._target_greedy_policy.action(\n        next_time_steps, dummy_state).action\n\n    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n    multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.rank > 0\n    return common.index_with_actions(\n        next_target_q_values,\n        greedy_actions,\n        multi_dim_actions=multi_dim_actions)\n\n\n@gin.configurable\nclass DdqnAgent(DqnAgent):\n  """"""A Double DQN Agent.\n\n  Implements the Double-DQN algorithm from\n\n  ""Deep Reinforcement Learning with Double Q-learning""\n   Hasselt et al., 2015\n   https://arxiv.org/abs/1509.06461\n\n  """"""\n\n  def _compute_next_q_values(self, next_time_steps, info):\n    """"""Compute the q value of the next state for TD error computation.\n\n    Args:\n      next_time_steps: A batch of next timesteps\n      info: PolicyStep.info that may be used by other agents inherited from\n        dqn_agent.\n\n    Returns:\n      A tensor of Q values for the given next state.\n    """"""\n    del info\n    # TODO(b/117175589): Add binary tests for DDQN.\n    network_observation = next_time_steps.observation\n\n    if self._observation_and_action_constraint_splitter is not None:\n      network_observation, _ = self._observation_and_action_constraint_splitter(\n          network_observation)\n\n    next_target_q_values, _ = self._target_q_network(\n        network_observation, next_time_steps.step_type)\n    batch_size = (\n        next_target_q_values.shape[0] or tf.shape(next_target_q_values)[0])\n    dummy_state = self._policy.get_initial_state(batch_size)\n    # Find the greedy actions using our greedy policy. This ensures that action\n    # constraints are respected and helps centralize the greedy logic.\n    best_next_actions = self._policy.action(next_time_steps, dummy_state).action\n\n    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n    multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.rank > 0\n    return common.index_with_actions(\n        next_target_q_values,\n        best_next_actions,\n        multi_dim_actions=multi_dim_actions)\n'"
tf_agents/agents/dqn/dqn_agent_test.py,86,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for agents.dqn.dqn_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\nfrom tf_agents.networks import test_utils as networks_test_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import test_utils as trajectories_test_utils\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self,\n               observation_spec,\n               action_spec,\n               l2_regularization_weight=0.0,\n               name=None):\n    super(DummyNet, self).__init__(\n        observation_spec, state_spec=(), name=name)\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_regularizer=tf.keras.regularizers.l2(\n                l2_regularization_weight),\n            kernel_initializer=tf.compat.v1.initializers.constant([[2, 1],\n                                                                   [1, 1]]),\n            bias_initializer=tf.compat.v1.initializers.constant([[1], [1]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass ComputeTDTargetsTest(test_utils.TestCase):\n\n  def testComputeTDTargets(self):\n    next_q_values = tf.constant([10, 20], dtype=tf.float32)\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n\n    expected_td_targets = [19., 38.]\n    td_targets = dqn_agent.compute_td_targets(next_q_values, rewards, discounts)\n    self.assertAllClose(self.evaluate(td_targets), expected_td_targets)\n\n\n@parameterized.named_parameters(\n    (\'DqnAgent\', dqn_agent.DqnAgent),\n    (\'DdqnAgent\', dqn_agent.DdqnAgent))\nclass DqnAgentTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(DqnAgentTest, self).setUp()\n    self._observation_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._observation_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 1)\n\n  def testCreateAgent(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n    self.assertIsNotNone(agent.policy)\n\n  def testCreateAgentWithPrebuiltPreprocessingLayers(self, agent_class):\n    dense_layer = tf.keras.layers.Dense(3)\n    q_net = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                               self._action_spec,\n                                               dense_layer)\n    with self.assertRaisesRegexp(\n        ValueError, \'shares weights with the original network\'):\n      agent_class(\n          self._time_step_spec,\n          self._action_spec,\n          q_network=q_net,\n          optimizer=None)\n\n    # Explicitly share weights between q and target networks.\n    # This would be an unusual setup so we check that an error is thrown.\n    q_target_net = networks_test_utils.KerasLayersNet(self._observation_spec,\n                                                      self._action_spec,\n                                                      dense_layer)\n    with self.assertRaisesRegexp(\n        ValueError, \'shares weights with the original network\'):\n      agent_class(\n          self._time_step_spec,\n          self._action_spec,\n          q_network=q_net,\n          optimizer=None,\n          target_q_network=q_target_net)\n\n  def testInitializeAgent(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n    init_op = agent.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n\n  def testCreateAgentDimChecks(self, agent_class):\n    action_spec = tensor_spec.BoundedTensorSpec([1, 2], tf.int32, 0, 1)\n    q_net = DummyNet(self._observation_spec, action_spec)\n    with self.assertRaisesRegexp(ValueError, \'.*one dimensional.*\'):\n      agent_class(\n          self._time_step_spec, action_spec, q_network=q_net, optimizer=None)\n\n  # TODO(b/127383724): Add a test where the target network has different values.\n  def testLoss(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = trajectories_test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # Using the kernel initializer [[2, 1], [1, 1]] and bias initializer\n    # [[1], [1]] from DummyNet above, we can calculate the following values:\n    # Q-value for first observation/action pair: 2 * 1 + 1 * 2 + 1 = 5\n    # Q-value for second observation/action pair: 1 * 3 + 1 * 4 + 1 = 8\n    # (Here we use the second row of the kernel initializer above, since the\n    # chosen action is now 1 instead of 0.)\n    #\n    # For target Q-values, action 0 produces a greater Q-value with a kernel of\n    # [2, 1] instead of [1, 1] for action 1.\n    # Target Q-value for first next_observation: 2 * 5 + 1 * 6 + 1 = 17\n    # Target Q-value for second next_observation: 2 * 7 + 1 * 8 + 1 = 23\n    # TD targets: 10 + 0.9 * 17 = 25.3 and 20 + 0.9 * 23 = 40.7\n    # TD errors: 25.3 - 5 = 20.3 and 40.7 - 8 = 32.7\n    # TD loss: 19.8 and 32.2 (Huber loss subtracts 0.5)\n    # Overall loss: (19.8 + 32.2) / 2 = 26\n    expected_loss = 26.0\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testLossWithChangedOptimalActions(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n\n    # Note that instead of [[5, 6], [7, 8]] as before, we now have -5 and -7.\n    next_observations = tf.constant([[-5, 6], [-7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = trajectories_test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # Using the kernel initializer [[2, 1], [1, 1]] and bias initializer\n    # [[1], [1]] from DummyNet above, we can calculate the following values:\n    # Q-value for first observation/action pair: 2 * 1 + 1 * 2 + 1 = 5\n    # Q-value for second observation/action pair: 1 * 3 + 1 * 4 + 1 = 8\n    # (Here we use the second row of the kernel initializer above, since the\n    # chosen action is now 1 instead of 0.)\n    #\n    # For the target Q-values here, note that since we\'ve replaced 5 and 7 with\n    # -5 and -7, it is better to use action 1 with a kernel of [1, 1] instead of\n    # action 0 with a kernel of [2, 1].\n    # Target Q-value for first next_observation: 1 * -5 + 1 * 6 + 1 = 2\n    # Target Q-value for second next_observation: 1 * -7 + 1 * 8 + 1 = 2\n    # TD targets: 10 + 0.9 * 2 = 11.8 and 20 + 0.9 * 2 = 21.8\n    # TD errors: 11.8 - 5 = 6.8 and 21.8 - 8 = 13.8\n    # TD loss: 6.3 and 13.3 (Huber loss subtracts 0.5)\n    # Overall loss: (6.3 + 13.3) / 2 = 9.8\n    expected_loss = 9.8\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testLossWithL2Regularization(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec,\n                     l2_regularization_weight=1.0)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = trajectories_test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # See the loss explanation in testLoss above.\n    # L2_regularization_loss: 2^2 + 1^2 + 1^2 + 1^2 = 7.0\n    # Overall loss: 26.0 (from testLoss) + 7.0 = 33.0\n    expected_loss = 33.0\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testLossNStep(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None,\n        n_step_update=2)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    third_observations = tf.constant([[9, 10], [11, 12]], dtype=tf.float32)\n    third_time_steps = ts.transition(third_observations, rewards, discounts)\n\n    experience1 = trajectory.from_transition(\n        time_steps, action_steps, next_time_steps)\n    experience2 = trajectory.from_transition(\n        next_time_steps, action_steps, third_time_steps)\n    experience3 = trajectory.from_transition(\n        third_time_steps, action_steps, third_time_steps)\n\n    experience = tf.nest.map_structure(\n        lambda x, y, z: tf.stack([x, y, z], axis=1),\n        experience1, experience2, experience3)\n\n    # We can extend the analysis from testLoss above as follows:\n    # Original Q-values are still 5 and 8 for the same reasons.\n    # Q-value for first third_observation: 2 * 9 + 1 * 10 + 1 = 29\n    # Q-value for second third_observation: 2 * 11 + 1 * 12 + 1 = 35\n    # TD targets: 10 + 0.9 * (10 + 0.9 * 29) = 42.49\n    # 20 + 0.9 * (20 + 0.9 * 35) = 66.35\n    # TD errors: 42.49 - 5 = 37.49 and 66.35 - 8 = 58.35\n    # TD loss: 36.99 and 57.85 (Huber loss subtracts 0.5)\n    # Overall loss: (36.99 + 57.85) / 2 = 47.42\n    expected_loss = 47.42\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testLossNStepMidMidLastFirst(self, agent_class):\n    """"""Tests that n-step loss handles LAST time steps properly.""""""\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None,\n        n_step_update=3)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    # MID: use ts.transition\n    time_steps = ts.transition(observations, rewards, discounts)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    second_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    # MID: use ts.transition\n    second_time_steps = ts.transition(second_observations, rewards, discounts)\n\n    third_observations = tf.constant([[9, 10], [11, 12]], dtype=tf.float32)\n    # LAST: use ts.termination\n    third_time_steps = ts.termination(third_observations, rewards)\n\n    fourth_observations = tf.constant([[13, 14], [15, 16]], dtype=tf.float32)\n    # FIRST: use ts.restart\n    fourth_time_steps = ts.restart(fourth_observations, batch_size=2)\n\n    experience1 = trajectory.from_transition(\n        time_steps, action_steps, second_time_steps)\n    experience2 = trajectory.from_transition(\n        second_time_steps, action_steps, third_time_steps)\n    experience3 = trajectory.from_transition(\n        third_time_steps, action_steps, fourth_time_steps)\n    experience4 = trajectory.from_transition(\n        fourth_time_steps, action_steps, fourth_time_steps)\n\n    experience = tf.nest.map_structure(\n        lambda w, x, y, z: tf.stack([w, x, y, z], axis=1),\n        experience1, experience2, experience3, experience4)\n\n    # Once again we can extend the analysis from testLoss above as follows:\n    # Original Q-values are still 5 and 8 for the same reasons.\n    # However next Q-values are now zeroed out due to the LAST time step in\n    # between. Thus the TD targets become the discounted reward sums, or:\n    # 10 + 0.9 * 10 = 19 and 20 + 0.9 * 20 = 38\n    # TD errors: 19 - 5 = 14 and 38 - 8 = 30\n    # TD loss: 13.5 and 29.5 (Huber loss subtracts 0.5)\n    # Overall loss: (13.5 + 29.5) / 2 = 21.5\n    expected_loss = 21.5\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testLossWithMaskedActions(self, agent_class):\n    # Observations are now a tuple of the usual observation and an action mask.\n    observation_spec_with_mask = (\n        self._observation_spec,\n        tensor_spec.BoundedTensorSpec([2], tf.int32, 0, 1))\n    time_step_spec = ts.time_step_spec(observation_spec_with_mask)\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]))\n\n    # For `observations`, the masks are set up so that all actions are valid.\n    observations = (tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n                    tf.constant([[1, 1], [1, 1]], dtype=tf.int32))\n    time_steps = ts.restart(observations, batch_size=2)\n\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    action_steps = policy_step.PolicyStep(actions)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n\n    # For `next_observations`, the masks are set up so that only one action is\n    # valid for each element in the batch.\n    next_observations = (tf.constant([[5, 6], [7, 8]], dtype=tf.float32),\n                         tf.constant([[0, 1], [1, 0]], dtype=tf.int32))\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    experience = trajectories_test_utils.stacked_trajectory_from_transition(\n        time_steps, action_steps, next_time_steps)\n\n    # Using the kernel initializer [[2, 1], [1, 1]] and bias initializer\n    # [[1], [1]] from DummyNet above, we can calculate the following values:\n    # Q-value for first observation/action pair: 2 * 1 + 1 * 2 + 1 = 5\n    # Q-value for second observation/action pair: 1 * 3 + 1 * 4 + 1 = 8\n    # (Here we use the second row of the kernel initializer above, since the\n    # chosen action is now 1 instead of 0.)\n    #\n    # For target Q-values, because of the masks we only have one valid choice of\n    # action for each next_observation:\n    # Target Q-value for first next_observation (only action 1 is valid):\n    # 1 * 5 + 1 * 6 + 1 = 12\n    # Target Q-value for second next_observation (only action 0 is valid):\n    # 2 * 7 + 1 * 8 + 1 = 23\n    # TD targets: 10 + 0.9 * 12 = 20.8 and 20 + 0.9 * 23 = 40.7\n    # TD errors: 20.8 - 5 = 15.8 and 40.7 - 8 = 32.7\n    # TD loss: 15.3 and 32.2 (Huber loss subtracts 0.5)\n    # Overall loss: (15.3 + 32.2) / 2 = 23.75\n    expected_loss = 23.75\n    loss, _ = agent._loss(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(self.evaluate(loss), expected_loss)\n\n  def testPolicy(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    # Batch size 2.\n    self.assertAllEqual(action_step.action.shape,\n                        [2] + self._action_spec.shape.as_list())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(all(actions_ <= self._action_spec.maximum))\n    self.assertTrue(all(actions_ >= self._action_spec.minimum))\n\n  def testInitializeRestoreAgent(self, agent_class):\n    q_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = agent_class(\n        self._time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=None)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    checkpoint = tf.train.Checkpoint(agent=agent)\n\n    latest_checkpoint = tf.train.latest_checkpoint(self.get_temp_dir())\n    checkpoint_load_status = checkpoint.restore(latest_checkpoint)\n\n    if tf.executing_eagerly():\n      self.evaluate(checkpoint_load_status.initialize_or_restore())\n      self.assertAllEqual(self.evaluate(action_step.action), [0, 0])\n    else:\n      with self.cached_session() as sess:\n        checkpoint_load_status.initialize_or_restore(sess)\n        self.assertAllEqual(sess.run(action_step.action), [0, 0])\n\n  def testTrainWithSparseTensorAndDenseFeaturesLayer(self, agent_class):\n    obs_spec = {\n        \'dense\': tensor_spec.BoundedTensorSpec(\n            dtype=tf.float32, shape=[3], minimum=-10.0, maximum=10.0),\n        \'sparse_terms\': tf.SparseTensorSpec(dtype=tf.string, shape=[4]),\n        \'sparse_frequencies\': tf.SparseTensorSpec(dtype=tf.float32, shape=[4]),\n    }\n    cat_column = (\n        tf.compat.v2.feature_column.categorical_column_with_hash_bucket(\n            \'sparse_terms\', hash_bucket_size=5))\n    weighted_cat_column = (\n        tf.compat.v2.feature_column.weighted_categorical_column(\n            cat_column, weight_feature_key=\'sparse_frequencies\'))\n    feature_columns = [\n        tf.compat.v2.feature_column.numeric_column(\'dense\', [3]),\n        tf.compat.v2.feature_column.embedding_column(weighted_cat_column, 3),\n    ]\n    dense_features_layer = tf.compat.v2.keras.layers.DenseFeatures(\n        feature_columns)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    q_net = q_network.QNetwork(\n        time_step_spec.observation,\n        self._action_spec,\n        preprocessing_combiner=dense_features_layer)\n    agent = agent_class(\n        time_step_spec,\n        self._action_spec,\n        q_network=q_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer())\n\n    observations = tensor_spec.sample_spec_nest(obs_spec, outer_dims=[5, 2])\n    # sparse_terms and sparse_frequencies must be defined on matching indices.\n    observations[\'sparse_terms\'] = tf.SparseTensor(\n        indices=observations[\'sparse_frequencies\'].indices,\n        values=tf.as_string(\n            tf.math.round(observations[\'sparse_frequencies\'].values)),\n        dense_shape=observations[\'sparse_frequencies\'].dense_shape)\n    if not tf.executing_eagerly():\n      # Mimic unknown inner dims on the SparseTensor\n      def _unknown_inner_shape(t):\n        if not isinstance(t, tf.SparseTensor):\n          return t\n        return tf.SparseTensor(\n            indices=t.indices, values=t.values,\n            dense_shape=tf.compat.v1.placeholder_with_default(\n                t.dense_shape, shape=t.dense_shape.shape))\n      observations = tf.nest.map_structure(_unknown_inner_shape, observations)\n      self.assertIsNone(tf.get_static_value(\n          observations[\'sparse_terms\'].dense_shape))\n\n    time_step = ts.restart(observations, batch_size=[5, 2])\n    action_step = tensor_spec.sample_spec_nest(\n        self._action_spec, outer_dims=[5, 2])\n    p_step = policy_step.PolicyStep(action=action_step, state=(), info=())\n    traj = trajectory.from_transition(time_step, p_step, time_step)\n    loss_info = agent.train(traj)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss_info)\n    self.assertGreater(loss_info.loss, 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ppo/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""PPO Agents.""""""\nfrom tf_agents.agents.ppo import ppo_agent\nfrom tf_agents.agents.ppo import ppo_clip_agent\nfrom tf_agents.agents.ppo import ppo_kl_penalty_agent\nfrom tf_agents.agents.ppo import ppo_policy\nfrom tf_agents.agents.ppo import ppo_utils\n'"
tf_agents/agents/ppo/ppo_agent.py,145,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""A PPO Agent.\n\nImplements the PPO algorithm from (Schulman, 2017):\nhttps://arxiv.org/abs/1707.06347\n\nIf you do not rely on using a combination of KL penalty and importance ratio\nclipping, use `PPOClipAgent` or `PPOKLPenaltyAgent` instead.\n\nPPO is a simplification of the TRPO algorithm, both of which add stability to\npolicy gradient RL, while allowing multiple updates per batch of on-policy data,\nby limiting the KL divergence between the policy that sampled the data and the\nupdated policy.\n\nTRPO enforces a hard optimization constraint, but is a complex algorithm, which\noften makes it harder to use in practice. PPO approximates the effect of TRPO\nby using a soft constraint. There are two methods presented in the paper for\nimplementing the soft constraint: an adaptive KL loss penalty, and\nlimiting the objective value based on a clipped version of the policy importance\nratio. This code implements both, and allows the user to use either method or\nboth by modifying hyperparameters.\n\nThe importance ratio clipping is described in eq (7) and the adaptive KL penatly\nis described in eq (8) of https://arxiv.org/pdf/1707.06347.pdf\n- To disable IR clipping, set the importance_ratio_clipping parameter to 0.0\n- To disable the adaptive KL penalty, set the initial_adaptive_kl_beta parameter\n  to 0.0\n- To disable the fixed KL cutoff penalty, set the kl_cutoff_factor parameter\n  to 0.0\n\nIn order to compute KL divergence, the replay buffer must store action\ndistribution parameters from data collection. For now, it is assumed that\ncontinuous actions are represented by a Normal distribution with mean & stddev,\nand discrete actions are represented by a Categorical distribution with logits.\n\nNote that the objective function chooses the lower value of the clipped and\nunclipped objectives. Thus, if the importance ratio exceeds the clipped bounds,\nthen the optimizer will still not be incentivized to pass the bounds, as it is\nonly optimizing the minimum.\n\nAdvantage is computed using Generalized Advantage Estimation (GAE):\nhttps://arxiv.org/abs/1506.02438\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Optional, Text, Tuple\n\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.agents.ppo import ppo_policy\nfrom tf_agents.agents.ppo import ppo_utils\nfrom tf_agents.networks import network\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import object_identity\nfrom tf_agents.utils import tensor_normalizer\nfrom tf_agents.utils import value_ops\n\n\nPPOLossInfo = collections.namedtuple(\'PPOLossInfo\', (\n    \'policy_gradient_loss\',\n    \'value_estimation_loss\',\n    \'l2_regularization_loss\',\n    \'entropy_regularization_loss\',\n    \'kl_penalty_loss\',\n))\n\n\ndef _normalize_advantages(advantages, axes=(0,), variance_epsilon=1e-8):\n  adv_mean, adv_var = tf.nn.moments(x=advantages, axes=axes, keepdims=True)\n  normalized_advantages = ((advantages - adv_mean) /\n                           (tf.sqrt(adv_var) + variance_epsilon))\n  return normalized_advantages\n\n\n@gin.configurable\nclass PPOAgent(tf_agent.TFAgent):\n  """"""A PPO Agent.""""""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      optimizer: Optional[types.Optimizer] = None,\n      actor_net: Optional[network.Network] = None,\n      value_net: Optional[network.Network] = None,\n      importance_ratio_clipping: types.Float = 0.0,\n      lambda_value: types.Float = 0.95,\n      discount_factor: types.Float = 0.99,\n      entropy_regularization: types.Float = 0.0,\n      policy_l2_reg: types.Float = 0.0,\n      value_function_l2_reg: types.Float = 0.0,\n      shared_vars_l2_reg: types.Float = 0.0,\n      value_pred_loss_coef: types.Float = 0.5,\n      num_epochs: int = 25,\n      use_gae: bool = False,\n      use_td_lambda_return: bool = False,\n      normalize_rewards: bool = True,\n      reward_norm_clipping: types.Float = 10.0,\n      normalize_observations: bool = True,\n      log_prob_clipping: types.Float = 0.0,\n      kl_cutoff_factor: types.Float = 2.0,\n      kl_cutoff_coef: types.Float = 1000.0,\n      initial_adaptive_kl_beta: types.Float = 1.0,\n      adaptive_kl_target: types.Float = 0.01,\n      adaptive_kl_tolerance: types.Float = 0.3,\n      gradient_clipping: bool = None,\n      value_clipping: Optional[types.Float] = None,\n      check_numerics: bool = False,\n      # TODO(b/150244758): Change the default to False once we move\n      # clients onto Reverb.\n      compute_value_and_advantage_in_train: bool = True,\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = None):\n    """"""Creates a PPO Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      optimizer: Optimizer to use for the agent, default to using\n        `tf.compat.v1.train.AdamOptimizer`.\n      actor_net: A `network.DistributionNetwork` which maps observations to\n        action distributions. Commonly, it is set to\n        `actor_distribution_network.ActorDistributionNetwork`.\n      value_net: A `Network` which returns the value prediction for input\n        states, with `call(observation, step_type, network_state)`. Commonly, it\n        is set to `value_network.ValueNetwork`.\n      importance_ratio_clipping: Epsilon in clipped, surrogate PPO objective.\n        For more detail, see explanation at the top of the doc.\n      lambda_value: Lambda parameter for TD-lambda computation.\n      discount_factor: Discount factor for return computation. Default to `0.99`\n        which is the value used for all environments from (Schulman, 2017).\n      entropy_regularization: Coefficient for entropy regularization loss term.\n        Default to `0.0` because no entropy bonus was used in (Schulman, 2017).\n      policy_l2_reg: Coefficient for L2 regularization of unshared actor_net\n        weights. Default to `0.0` because no L2 regularization was applied on\n        the policy network weights in (Schulman, 2017).\n      value_function_l2_reg: Coefficient for l2 regularization of unshared value\n        function weights. Default to `0.0` because no L2 regularization was\n        applied on the policy network weights in (Schulman, 2017).\n      shared_vars_l2_reg: Coefficient for l2 regularization of weights shared\n        between actor_net and value_net. Default to `0.0` because no L2\n        regularization was applied on the policy network or value network\n        weights in (Schulman, 2017).\n      value_pred_loss_coef: Multiplier for value prediction loss to balance with\n        policy gradient loss. Default to `0.5`, which was used for all\n        environments in the OpenAI baseline implementation. This parameters is\n        irrelevant unless you are sharing part of actor_net and value_net. In\n        that case, you would want to tune this coeeficient, whose value depends\n        on the network architecture of your choice.\n      num_epochs: Number of epochs for computing policy updates. (Schulman,2017)\n        sets this to 10 for Mujoco, 15 for Roboschool and 3 for Atari.\n      use_gae: If True (default False), uses generalized advantage estimation\n        for computing per-timestep advantage. Else, just subtracts value\n        predictions from empirical return.\n      use_td_lambda_return: If True (default False), uses td_lambda_return for\n        training value function; here:\n        `td_lambda_return = gae_advantage + value_predictions`.\n        `use_gae` must be set to `True` as well to enable TD -lambda returns. If\n        `use_td_lambda_return` is set to True while `use_gae` is False, the\n        empirical return will be used and a warning will be logged.\n      normalize_rewards: If true, keeps moving variance of rewards and\n        normalizes incoming rewards. While not mentioned directly in (Schulman,\n        2017), reward normalization was implemented in OpenAI baselines and\n        (Ilyas et al., 2018) pointed out that it largely improves performance.\n        You may refer to Figure 1 of https://arxiv.org/pdf/1811.02553.pdf for a\n        comparison with and without reward scaling.\n      reward_norm_clipping: Value above and below to clip normalized reward.\n        Additional optimization proposed in (Ilyas et al., 2018) set to\n        `5` or `10`.\n      normalize_observations: If `True`, keeps moving mean and\n        variance of observations and normalizes incoming observations.\n        Additional optimization proposed in (Ilyas et al., 2018).\n      log_prob_clipping: +/- value for clipping log probs to prevent inf / NaN\n        values.  Default: no clipping.\n      kl_cutoff_factor: Only meaningful when `kl_cutoff_coef > 0.0`. A multipler\n        used for calculating the KL cutoff ( =\n        `kl_cutoff_factor * adaptive_kl_target`). If policy KL averaged across\n        the batch changes more than the cutoff, a squared cutoff loss would\n        be added to the loss function.\n      kl_cutoff_coef: kl_cutoff_coef and kl_cutoff_factor are additional params\n        if one wants to use a KL cutoff loss term in addition to the adaptive KL\n        loss term. Default to 0.0 to disable the KL cutoff loss term as this was\n        not used in the paper.  kl_cutoff_coef is the coefficient to mulitply by\n        the KL cutoff loss term, before adding to the total loss function.\n      initial_adaptive_kl_beta: Initial value for beta coefficient of adaptive\n        KL penalty. This initial value is not important in practice because the\n        algorithm quickly adjusts to it. A common default is 1.0.\n      adaptive_kl_target: Desired KL target for policy updates. If actual KL is\n        far from this target, adaptive_kl_beta will be updated. You should tune\n        this for your environment. 0.01 was found to perform well for Mujoco.\n      adaptive_kl_tolerance: A tolerance for adaptive_kl_beta. Mean KL above\n        `(1 + tol) * adaptive_kl_target`, or below\n        `(1 - tol) * adaptive_kl_target`,\n        will cause `adaptive_kl_beta` to be updated. `0.5` was chosen\n        heuristically in the paper, but the algorithm is not very\n        sensitive to it.\n      gradient_clipping: Norm length to clip gradients.  Default: no clipping.\n      value_clipping: Difference between new and old value predictions are\n        clipped to this threshold. Value clipping could be helpful when training\n        very deep networks. Default: no clipping.\n      check_numerics: If true, adds `tf.debugging.check_numerics` to help find\n        NaN / Inf values. For debugging only.\n      compute_value_and_advantage_in_train: A bool to indicate where value\n        prediction and advantage calculation happen.  If True, both happen in\n        agent.train(). If False, value prediction is computed during data\n        collection. This argument must be set to `False` if mini batch learning\n        is enabled.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If true, gradient summaries will be written.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the actor_net is not a DistributionNetwork or value_net is\n        not a Network.\n    """"""\n    if not isinstance(actor_net, network.DistributionNetwork):\n      raise ValueError(\n          \'actor_net must be an instance of a network.DistributionNetwork.\')\n    if not isinstance(value_net, network.Network):\n      raise ValueError(\'value_net must be an instance of a network.Network.\')\n\n    actor_net.create_variables()\n    value_net.create_variables()\n\n    tf.Module.__init__(self, name=name)\n\n    self._optimizer = optimizer\n    self._actor_net = actor_net\n    self._value_net = value_net\n    self._importance_ratio_clipping = importance_ratio_clipping\n    self._lambda = lambda_value\n    self._discount_factor = discount_factor\n    self._entropy_regularization = entropy_regularization\n    self._policy_l2_reg = policy_l2_reg\n    self._value_function_l2_reg = value_function_l2_reg\n    self._shared_vars_l2_reg = shared_vars_l2_reg\n    self._value_pred_loss_coef = value_pred_loss_coef\n    self._num_epochs = num_epochs\n    self._use_gae = use_gae\n    self._use_td_lambda_return = use_td_lambda_return\n    self._reward_norm_clipping = reward_norm_clipping\n    self._log_prob_clipping = log_prob_clipping\n    self._kl_cutoff_factor = kl_cutoff_factor\n    self._kl_cutoff_coef = kl_cutoff_coef\n    self._adaptive_kl_target = adaptive_kl_target\n    self._adaptive_kl_tolerance = adaptive_kl_tolerance\n    self._gradient_clipping = gradient_clipping or 0.0\n    self._value_clipping = value_clipping or 0.0\n    self._check_numerics = check_numerics\n    self._compute_value_and_advantage_in_train = (\n        compute_value_and_advantage_in_train)\n\n    if initial_adaptive_kl_beta > 0.0:\n      # TODO(kbanoop): Rename create_variable.\n      self._adaptive_kl_beta = common.create_variable(\n          \'adaptive_kl_beta\', initial_adaptive_kl_beta, dtype=tf.float32)\n    else:\n      self._adaptive_kl_beta = None\n\n    self._reward_normalizer = None\n    if normalize_rewards:\n      self._reward_normalizer = tensor_normalizer.StreamingTensorNormalizer(\n          tensor_spec.TensorSpec([], tf.float32), scope=\'normalize_reward\')\n\n    self._observation_normalizer = None\n    if normalize_observations:\n      self._observation_normalizer = (\n          tensor_normalizer.StreamingTensorNormalizer(\n              time_step_spec.observation, scope=\'normalize_observations\'))\n\n    policy = greedy_policy.GreedyPolicy(\n        ppo_policy.PPOPolicy(\n            time_step_spec=time_step_spec,\n            action_spec=action_spec,\n            actor_network=actor_net,\n            value_network=value_net,\n            observation_normalizer=self._observation_normalizer,\n            clip=False,\n            collect=False))\n\n    collect_policy = ppo_policy.PPOPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        actor_network=actor_net,\n        value_network=value_net,\n        observation_normalizer=self._observation_normalizer,\n        clip=False,\n        collect=True,\n        compute_value_and_advantage_in_train=(\n            self._compute_value_and_advantage_in_train),\n    )\n\n    self._action_distribution_spec = (self._actor_net.output_spec)\n\n    # Set training_data_spec to collect_data_spec with augmented policy info,\n    # iff return and normalized advantage are saved in preprocess_sequence.\n    if self._compute_value_and_advantage_in_train:\n      training_data_spec = None\n    else:\n      training_policy_info = collect_policy.trajectory_spec.policy_info.copy()\n      training_policy_info.update({\n          \'value_prediction\':\n              collect_policy.trajectory_spec.policy_info[\'value_prediction\'],\n          \'return\':\n              tensor_spec.TensorSpec(shape=[], dtype=tf.float32),\n          \'normalized_advantage\':\n              tensor_spec.TensorSpec(shape=[], dtype=tf.float32),\n      })\n      training_data_spec = collect_policy.trajectory_spec.replace(\n          policy_info=training_policy_info)\n\n    super(PPOAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=None,\n        training_data_spec=training_data_spec,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  @property\n  def actor_net(self) -> network.Network:\n    """"""Returns actor_net TensorFlow template function.""""""\n    return self._actor_net\n\n  def _initialize(self):\n    pass\n\n  def compute_advantages(self,\n                         rewards: types.NestedTensor,\n                         returns: types.Tensor,\n                         discounts: types.Tensor,\n                         value_preds: types.Tensor) -> types.Tensor:\n    """"""Compute advantages, optionally using GAE.\n\n    Based on baselines ppo1 implementation. Removes final timestep, as it needs\n    to use this timestep for next-step value prediction for TD error\n    computation.\n\n    Args:\n      rewards: Tensor of per-timestep rewards.\n      returns: Tensor of per-timestep returns.\n      discounts: Tensor of per-timestep discounts. Zero for terminal timesteps.\n      value_preds: Cached value estimates from the data-collection policy.\n\n    Returns:\n      advantages: Tensor of length (len(rewards) - 1), because the final\n        timestep is just used for next-step value prediction.\n    """"""\n    # Arg value_preds was appended with final next_step value. Make tensors\n    #   next_value_preds by stripping first and last elements respectively.\n    final_value_pred = value_preds[:, -1]\n    value_preds = value_preds[:, :-1]\n\n    if not self._use_gae:\n      with tf.name_scope(\'empirical_advantage\'):\n        advantages = returns - value_preds\n    else:\n      advantages = value_ops.generalized_advantage_estimation(\n          values=value_preds,\n          final_value=final_value_pred,\n          rewards=rewards,\n          discounts=discounts,\n          td_lambda=self._lambda,\n          time_major=False)\n\n    return advantages\n\n  def get_loss(self,\n               time_steps: ts.TimeStep,\n               actions: types.NestedTensorSpec,\n               act_log_probs: types.Tensor,\n               returns: types.Tensor,\n               normalized_advantages: types.Tensor,\n               action_distribution_parameters: types.NestedTensor,\n               weights: types.Tensor,\n               train_step: tf.Variable,\n               debug_summaries: bool,\n               old_value_predictions: Optional[types.Tensor] = None,\n               training: bool = False) -> tf_agent.LossInfo:\n    """"""Compute the loss and create optimization op for one training epoch.\n\n    All tensors should have a single batch dimension.\n\n    Args:\n      time_steps: A minibatch of TimeStep tuples.\n      actions: A minibatch of actions.\n      act_log_probs: A minibatch of action probabilities (probability under the\n        sampling policy).\n      returns: A minibatch of per-timestep returns.\n      normalized_advantages: A minibatch of normalized per-timestep advantages.\n      action_distribution_parameters: Parameters of data-collecting action\n        distribution. Needed for KL computation.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  Includes a mask for invalid timesteps.\n      train_step: A train_step variable to increment for each train step.\n        Typically the global_step.\n      debug_summaries: True if debug summaries should be created.\n      old_value_predictions: (Optional) The saved value predictions, used\n        for calculating the value estimation loss when value clipping is\n        performed.\n      training: Whether this loss is being used for training.\n\n    Returns:\n      A tf_agent.LossInfo named tuple with the total_loss and all intermediate\n        losses in the extra field contained in a PPOLossInfo named tuple.\n    """"""\n    # Evaluate the current policy on timesteps.\n\n    # batch_size from time_steps\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    policy_state = self._collect_policy.get_initial_state(batch_size)\n    # We must use _distribution because the distribution API doesn\'t pass down\n    # the training= kwarg.\n    distribution_step = self._collect_policy._distribution(  # pylint: disable=protected-access\n        time_steps,\n        policy_state,\n        training=training)\n    # TODO(eholly): Rename policy distributions to something clear and uniform.\n    current_policy_distribution = distribution_step.action\n\n    # Call all loss functions and add all loss values.\n    value_estimation_loss = self.value_estimation_loss(\n        time_steps=time_steps,\n        returns=returns,\n        old_value_predictions=old_value_predictions,\n        weights=weights,\n        debug_summaries=debug_summaries,\n        training=training)\n    policy_gradient_loss = self.policy_gradient_loss(\n        time_steps,\n        actions,\n        tf.stop_gradient(act_log_probs),\n        tf.stop_gradient(normalized_advantages),\n        current_policy_distribution,\n        weights,\n        debug_summaries=debug_summaries)\n\n    if (self._policy_l2_reg > 0.0 or self._value_function_l2_reg > 0.0 or\n        self._shared_vars_l2_reg > 0.0):\n      l2_regularization_loss = self.l2_regularization_loss(debug_summaries)\n    else:\n      l2_regularization_loss = tf.zeros_like(policy_gradient_loss)\n\n    if self._entropy_regularization > 0.0:\n      entropy_regularization_loss = self.entropy_regularization_loss(\n          time_steps, current_policy_distribution, weights, debug_summaries)\n    else:\n      entropy_regularization_loss = tf.zeros_like(policy_gradient_loss)\n\n    kl_penalty_loss = self.kl_penalty_loss(time_steps,\n                                           action_distribution_parameters,\n                                           current_policy_distribution, weights,\n                                           debug_summaries)\n\n    total_loss = (\n        policy_gradient_loss + value_estimation_loss + l2_regularization_loss +\n        entropy_regularization_loss + kl_penalty_loss)\n\n    return tf_agent.LossInfo(\n        total_loss,\n        PPOLossInfo(\n            policy_gradient_loss=policy_gradient_loss,\n            value_estimation_loss=value_estimation_loss,\n            l2_regularization_loss=l2_regularization_loss,\n            entropy_regularization_loss=entropy_regularization_loss,\n            kl_penalty_loss=kl_penalty_loss,\n        ))\n\n  def compute_return_and_advantage(\n      self,\n      next_time_steps: ts.TimeStep,\n      value_preds: types.Tensor) -> Tuple[types.Tensor, types.Tensor]:\n    """"""Compute the Monte Carlo return and advantage.\n\n    Normalazation will be applied to the computed returns and advantages if\n    it\'s enabled.\n\n    Args:\n      next_time_steps: batched tensor of TimeStep tuples after action is taken.\n      value_preds: Batched value prediction tensor. Should have one more entry\n        in time index than time_steps, with the final value corresponding to the\n        value prediction of the final state.\n\n    Returns:\n      tuple of (return, normalized_advantage), both are batched tensors.\n    """"""\n    discounts = next_time_steps.discount * tf.constant(\n        self._discount_factor, dtype=tf.float32)\n\n    rewards = next_time_steps.reward\n    if self._debug_summaries:\n      # Summarize rewards before they get normalized below.\n      tf.compat.v2.summary.histogram(\n          name=\'rewards\', data=rewards, step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'rewards_mean\',\n          data=tf.reduce_mean(rewards),\n          step=self.train_step_counter)\n\n    # Normalize rewards if self._reward_normalizer is defined.\n    if self._reward_normalizer:\n      rewards = self._reward_normalizer.normalize(\n          rewards, center_mean=False, clip_value=self._reward_norm_clipping)\n      if self._debug_summaries:\n        tf.compat.v2.summary.histogram(\n            name=\'rewards_normalized\',\n            data=rewards,\n            step=self.train_step_counter)\n        tf.compat.v2.summary.scalar(\n            name=\'rewards_normalized_mean\',\n            data=tf.reduce_mean(rewards),\n            step=self.train_step_counter)\n\n    # Make discount 0.0 at end of each episode to restart cumulative sum\n    #   end of each episode.\n    episode_mask = common.get_episode_mask(next_time_steps)\n    discounts *= episode_mask\n\n    # Compute Monte Carlo returns. Data from incomplete trajectories, not\n    #   containing the end of an episode will also be used, with a bootstrapped\n    #   estimation from the last value.\n    # Note that when a trajectory driver is used, then the final step is\n    #   terminal, the bootstrapped estimation will not be used, as it will be\n    #   multiplied by zero (the discount on the last step).\n    final_value_bootstrapped = value_preds[:, -1]\n    returns = value_ops.discounted_return(\n        rewards,\n        discounts,\n        time_major=False,\n        final_value=final_value_bootstrapped)\n    if self._debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'returns\', data=returns, step=self.train_step_counter)\n\n    # Compute advantages.\n    advantages = self.compute_advantages(rewards, returns, discounts,\n                                         value_preds)\n    normalized_advantages = _normalize_advantages(advantages, axes=(0, 1))\n    if self._debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'advantages\', data=advantages, step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'advantages_normalized\',\n          data=normalized_advantages,\n          step=self.train_step_counter)\n\n    # Return TD-Lambda returns if both use_td_lambda_return and use_gae.\n    if self._use_td_lambda_return:\n      if not self._use_gae:\n        logging.warning(\'use_td_lambda_return was True, but use_gae was \'\n                        \'False. Using Monte Carlo return.\')\n      else:\n        returns = tf.add(\n            advantages, value_preds[:, :-1], name=\'td_lambda_returns\')\n\n    return returns, normalized_advantages\n\n  def _preprocess(self, experience):\n    """"""Performs advantage calculation for the collected experience.\n\n    Args:\n      experience: A (batch of) experience in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.collect_data_spec`.\n        All tensors in `experience` must be shaped `[batch, time + 1, ...]` or\n        [time + 1, ...]. The ""+1"" is needed as the last action from the set of\n        trajectories cannot be used for training, as its advantage and returns\n        are unknown.\n\n    Returns:\n      The processed experience which has normalized_advantages and returns\n      filled in its policy info. The advantages and returns for the last\n      transition is filled with 0s as they cannot be calculated.\n    """"""\n    if self._compute_value_and_advantage_in_train:\n      outer_rank = nest_utils.get_outer_rank(experience,\n                                             self.training_data_spec)\n    else:\n      outer_rank = nest_utils.get_outer_rank(experience,\n                                             self.collect_data_spec)\n    # Add 1 as the batch dimension for inputs that just have the time dimension,\n    # as all utility functions below require the batch dimension.\n    if outer_rank == 1:\n      batched_experience = nest_utils.batch_nested_tensors(experience)\n    else:\n      batched_experience = experience\n\n    # Get individual tensors from experience.\n    num_steps = batched_experience.step_type.shape[1]\n    if num_steps and num_steps <= 1:\n      raise ValueError(\n          \'Experience used for advantage calculation must have >1 num_steps.\')\n\n    (time_steps, _,\n     next_time_steps) = trajectory.to_transition(batched_experience)\n\n    # Compute the value predictions for states using the current value function.\n    # To be used for return & advantage computation.\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    if self._compute_value_and_advantage_in_train:\n      value_state = self._collect_policy.get_initial_value_state(batch_size)\n      value_preds, _ = self._collect_policy.apply_value_network(\n          batched_experience.observation,\n          batched_experience.step_type,\n          value_state=value_state,\n          training=False)\n      value_preds = tf.stop_gradient(value_preds)\n    else:\n      value_preds = batched_experience.policy_info[\'value_prediction\']\n    new_policy_info = {\n        \'dist_params\': batched_experience.policy_info[\'dist_params\'],\n        \'value_prediction\': value_preds,\n    }\n\n    # Add the calculated advantage and return into the input experience.\n    returns, normalized_advantages = self.compute_return_and_advantage(\n        next_time_steps, value_preds)\n\n    # Pad returns and normalized_advantages in the time dimension so that the\n    # time dimensions are aligned with the input experience\'s time dimension.\n    # When the output trajectory gets sliced by trajectory.to_transition during\n    # training, the padded last timesteps will be automatically dropped.\n    last_transition_padding = tf.zeros((batch_size, 1), dtype=tf.float32)\n    new_policy_info[\'return\'] = tf.concat([returns, last_transition_padding],\n                                          axis=1)\n    new_policy_info[\'normalized_advantage\'] = tf.concat(\n        [normalized_advantages, last_transition_padding], axis=1)\n\n    # Remove the batch dimension iff the input experience does not have it.\n    if outer_rank == 1:\n      new_policy_info = nest_utils.unbatch_nested_tensors(new_policy_info)\n    # The input experience with its policy info filled with the calculated\n    # advantages and returns for each action.\n    return experience.replace(policy_info=new_policy_info)\n\n  def _preprocess_sequence(self, experience):\n    """"""Performs advantage calculation for the collected experience.\n\n    This function is a no-op if self._compute_value_and_advantage_in_train is\n    True, which means advantage calculation happens as part of agent.train().\n\n    Args:\n      experience: A (batch of) experience in the form of a `Trajectory`. The\n        structure of `experience` must match that of `self.collect_data_spec`.\n        All tensors in `experience` must be shaped `[batch, time + 1, ...]` or\n        [time + 1, ...]. The ""+1"" is needed as the last action from the set of\n        trajectories cannot be used for training, as its advantage and returns\n        are unknown.\n\n    Returns:\n      A post processed `Trajectory` with the same shape as the input, with\n        `return` and `normalized_advantage` stored inside of the policy info\n        dictionary. The advantages and returns for the last transition is filled\n        with 0s as they cannot be calculated.\n    """"""\n    if self._compute_value_and_advantage_in_train:\n      return experience\n\n    return self._preprocess(experience)\n\n  def _train(self, experience, weights):\n    if self._compute_value_and_advantage_in_train:\n      processed_experience = self._preprocess(experience)\n    else:\n      processed_experience = experience\n\n    # Mask trajectories that cannot be used for training.\n    valid_mask = ppo_utils.make_trajectory_mask(processed_experience)\n    if weights is None:\n      masked_weights = valid_mask\n    else:\n      masked_weights = weights * valid_mask\n\n    # Reconstruct per-timestep policy distribution from stored distribution\n    #   parameters.\n    old_action_distribution_parameters = processed_experience.policy_info[\n        \'dist_params\']\n    old_actions_distribution = (\n        distribution_spec.nested_distributions_from_specs(\n            self._action_distribution_spec, old_action_distribution_parameters))\n    # Compute log probability of actions taken during data collection, using the\n    #   collect policy distribution.\n    old_act_log_probs = common.log_probability(old_actions_distribution,\n                                               processed_experience.action,\n                                               self._action_spec)\n\n    if self._debug_summaries:\n      actions_list = tf.nest.flatten(processed_experience.action)\n      show_action_index = len(actions_list) != 1\n      for i, single_action in enumerate(actions_list):\n        action_name = (\'actions_{}\'.format(i)\n                       if show_action_index else \'actions\')\n        tf.compat.v2.summary.histogram(\n            name=action_name, data=single_action, step=self.train_step_counter)\n\n    time_steps = ts.TimeStep(\n        step_type=processed_experience.step_type,\n        reward=processed_experience.reward,\n        discount=processed_experience.discount,\n        observation=processed_experience.observation)\n    actions = processed_experience.action\n    returns = processed_experience.policy_info[\'return\']\n    normalized_advantages = processed_experience.policy_info[\n        \'normalized_advantage\']\n    old_value_predictions = processed_experience.policy_info[\'value_prediction\']\n\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    # Loss tensors across batches will be aggregated for summaries.\n    policy_gradient_losses = []\n    value_estimation_losses = []\n    l2_regularization_losses = []\n    entropy_regularization_losses = []\n    kl_penalty_losses = []\n\n    loss_info = None  # TODO(b/123627451): Remove.\n    variables_to_train = list(\n        object_identity.ObjectIdentitySet(self._actor_net.trainable_weights +\n                                          self._value_net.trainable_weights))\n    for i_epoch in range(self._num_epochs):\n      with tf.name_scope(\'epoch_%d\' % i_epoch):\n        # Only save debug summaries for first and last epochs.\n        debug_summaries = (\n            self._debug_summaries and\n            (i_epoch == 0 or i_epoch == self._num_epochs - 1))\n\n        with tf.GradientTape() as tape:\n          loss_info = self.get_loss(\n              time_steps,\n              actions,\n              old_act_log_probs,\n              returns,\n              normalized_advantages,\n              old_action_distribution_parameters,\n              masked_weights,\n              self.train_step_counter,\n              debug_summaries,\n              old_value_predictions=old_value_predictions,\n              training=True)\n\n        grads = tape.gradient(loss_info.loss, variables_to_train)\n        # Tuple is used for py3, where zip is a generator producing values once.\n        grads_and_vars = tuple(zip(grads, variables_to_train))\n        if self._gradient_clipping > 0:\n          grads_and_vars = eager_utils.clip_gradient_norms(\n              grads_and_vars, self._gradient_clipping)\n\n        # If summarize_gradients, create functions for summarizing both\n        # gradients and variables.\n        if self._summarize_grads_and_vars and debug_summaries:\n          eager_utils.add_gradients_summaries(grads_and_vars,\n                                              self.train_step_counter)\n          eager_utils.add_variables_summaries(grads_and_vars,\n                                              self.train_step_counter)\n\n        self._optimizer.apply_gradients(\n            grads_and_vars, global_step=self.train_step_counter)\n\n        policy_gradient_losses.append(loss_info.extra.policy_gradient_loss)\n        value_estimation_losses.append(loss_info.extra.value_estimation_loss)\n        l2_regularization_losses.append(loss_info.extra.l2_regularization_loss)\n        entropy_regularization_losses.append(\n            loss_info.extra.entropy_regularization_loss)\n        kl_penalty_losses.append(loss_info.extra.kl_penalty_loss)\n\n    # After update epochs, update adaptive kl beta, then update observation\n    #   normalizer and reward normalizer.\n    policy_state = self._collect_policy.get_initial_state(batch_size)\n    # Compute the mean kl from previous action distribution.\n    kl_divergence = self._kl_divergence(\n        time_steps, old_action_distribution_parameters,\n        self._collect_policy.distribution(time_steps, policy_state).action)\n    self.update_adaptive_kl_beta(kl_divergence)\n\n    if self._observation_normalizer:\n      self._observation_normalizer.update(\n          time_steps.observation, outer_dims=[0, 1])\n    else:\n      # TODO(b/127661780): Verify performance of reward_normalizer when obs are\n      #                    not normalized\n      if self._reward_normalizer:\n        self._reward_normalizer.update(\n            processed_experience.reward, outer_dims=[0, 1])\n\n    loss_info = tf.nest.map_structure(tf.identity, loss_info)\n\n    # Make summaries for total loss averaged across all epochs.\n    # The *_losses lists will have been populated by\n    #   calls to self.get_loss. Assumes all the losses have same length.\n    with tf.name_scope(\'Losses/\'):\n      num_epochs = len(policy_gradient_losses)\n      total_policy_gradient_loss = tf.add_n(policy_gradient_losses) / num_epochs\n      total_value_estimation_loss = tf.add_n(\n          value_estimation_losses) / num_epochs\n      total_l2_regularization_loss = tf.add_n(\n          l2_regularization_losses) / num_epochs\n      total_entropy_regularization_loss = tf.add_n(\n          entropy_regularization_losses) / num_epochs\n      total_kl_penalty_loss = tf.add_n(kl_penalty_losses) / num_epochs\n      tf.compat.v2.summary.scalar(\n          name=\'policy_gradient_loss\',\n          data=total_policy_gradient_loss,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'value_estimation_loss\',\n          data=total_value_estimation_loss,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'l2_regularization_loss\',\n          data=total_l2_regularization_loss,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'entropy_regularization_loss\',\n          data=total_entropy_regularization_loss,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'kl_penalty_loss\',\n          data=total_kl_penalty_loss,\n          step=self.train_step_counter)\n\n      total_abs_loss = (\n          tf.abs(total_policy_gradient_loss) +\n          tf.abs(total_value_estimation_loss) +\n          tf.abs(total_entropy_regularization_loss) +\n          tf.abs(total_l2_regularization_loss) + tf.abs(total_kl_penalty_loss))\n\n      tf.compat.v2.summary.scalar(\n          name=\'total_abs_loss\',\n          data=total_abs_loss,\n          step=self.train_step_counter)\n\n    if self._summarize_grads_and_vars:\n      with tf.name_scope(\'Variables/\'):\n        all_vars = (\n            self._actor_net.trainable_weights +\n            self._value_net.trainable_weights)\n        for var in all_vars:\n          tf.compat.v2.summary.histogram(\n              name=var.name.replace(\':\', \'_\'),\n              data=var,\n              step=self.train_step_counter)\n\n    return loss_info\n\n  def l2_regularization_loss(self,\n                             debug_summaries: bool = False) -> types.Tensor:\n    if (self._policy_l2_reg > 0 or self._value_function_l2_reg > 0 or\n        self._shared_vars_l2_reg > 0):\n      with tf.name_scope(\'l2_regularization\'):\n        # Regularize policy weights.\n        policy_vars_to_regularize = (\n            v for v in self._actor_net.trainable_weights if \'kernel\' in v.name)\n        vf_vars_to_regularize = (\n            v for v in self._value_net.trainable_weights if \'kernel\' in v.name)\n\n        (unshared_policy_vars_to_regularize, unshared_vf_vars_to_regularize,\n         shared_vars_to_regularize) = common.extract_shared_variables(\n             policy_vars_to_regularize, vf_vars_to_regularize)\n\n        # Regularize policy weights.\n        policy_l2_losses = [\n            tf.reduce_sum(input_tensor=tf.square(v)) * self._policy_l2_reg\n            for v in unshared_policy_vars_to_regularize\n        ]\n\n        # Regularize value function weights.\n        vf_l2_losses = [\n            tf.reduce_sum(input_tensor=tf.square(v)) *\n            self._value_function_l2_reg for v in unshared_vf_vars_to_regularize\n        ]\n\n        # Regularize shared weights\n        shared_l2_losses = [\n            tf.reduce_sum(input_tensor=tf.square(v)) * self._shared_vars_l2_reg\n            for v in shared_vars_to_regularize\n        ]\n\n        l2_losses = policy_l2_losses + vf_l2_losses + shared_l2_losses\n        total_l2_loss = tf.add_n(l2_losses, name=\'l2_loss\')\n\n        if self._check_numerics:\n          total_l2_loss = tf.debugging.check_numerics(total_l2_loss,\n                                                      \'total_l2_loss\')\n\n        if debug_summaries:\n          tf.compat.v2.summary.histogram(\n              name=\'l2_loss\', data=total_l2_loss, step=self.train_step_counter)\n    else:\n      total_l2_loss = tf.constant(0.0, dtype=tf.float32, name=\'zero_l2_loss\')\n\n    return total_l2_loss\n\n  def entropy_regularization_loss(\n      self,\n      time_steps: ts.TimeStep,\n      current_policy_distribution: types.NestedDistribution,\n      weights: types.Tensor,\n      debug_summaries: bool = False) -> types.Tensor:\n    """"""Create regularization loss tensor based on agent parameters.""""""\n    if self._entropy_regularization > 0:\n      nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n      with tf.name_scope(\'entropy_regularization\'):\n        entropy = tf.cast(\n            common.entropy(current_policy_distribution, self.action_spec),\n            tf.float32)\n\n        entropy *= weights\n\n        entropy_reg_loss = (\n            tf.reduce_mean(input_tensor=-entropy) *\n            self._entropy_regularization)\n        if self._check_numerics:\n          entropy_reg_loss = tf.debugging.check_numerics(\n              entropy_reg_loss, \'entropy_reg_loss\')\n\n        if debug_summaries:\n          tf.compat.v2.summary.histogram(\n              name=\'entropy_reg_loss\',\n              data=entropy_reg_loss,\n              step=self.train_step_counter)\n    else:\n      entropy_reg_loss = tf.constant(\n          0.0, dtype=tf.float32, name=\'zero_entropy_reg_loss\')\n\n    return entropy_reg_loss\n\n  def value_estimation_loss(\n      self,\n      time_steps: ts.TimeStep,\n      returns: types.Tensor,\n      weights: types.Tensor,\n      old_value_predictions: Optional[types.Tensor] = None,\n      debug_summaries: bool = False,\n      training: bool = False) -> types.Tensor:\n    """"""Computes the value estimation loss for actor-critic training.\n\n    All tensors should have a single batch dimension.\n\n    Args:\n      time_steps: A batch of timesteps.\n      returns: Per-timestep returns for value function to predict. (Should come\n        from TD-lambda computation.)\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  Includes a mask for invalid timesteps.\n      old_value_predictions: (Optional) The saved value predictions from\n        policy_info, required when self._value_clipping > 0.\n      debug_summaries: True if debug summaries should be created.\n      training: Whether this loss is going to be used for training.\n\n    Returns:\n      value_estimation_loss: A scalar value_estimation_loss loss.\n\n    Raises:\n      ValueError: If old_value_predictions was not passed in, but value clipping\n        was performed.\n    """"""\n\n    observation = time_steps.observation\n    if debug_summaries:\n      observation_list = tf.nest.flatten(observation)\n      show_observation_index = len(observation_list) != 1\n      for i, single_observation in enumerate(observation_list):\n        observation_name = (\'observations_{}\'.format(i)\n                            if show_observation_index else \'observations\')\n        tf.compat.v2.summary.histogram(\n            name=observation_name,\n            data=single_observation,\n            step=self.train_step_counter)\n\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    value_state = self._collect_policy.get_initial_value_state(batch_size)\n\n    value_preds, _ = self._collect_policy.apply_value_network(\n        time_steps.observation,\n        time_steps.step_type,\n        value_state=value_state,\n        training=training)\n    value_estimation_error = tf.math.squared_difference(returns, value_preds)\n\n    if self._value_clipping > 0:\n      if old_value_predictions is None:\n        raise ValueError(\n            \'old_value_predictions is None but needed for value clipping.\'\n        )\n      clipped_value_preds = old_value_predictions + tf.clip_by_value(\n          value_preds - old_value_predictions, -self._value_clipping,\n          self._value_clipping)\n      clipped_value_estimation_error = tf.math.squared_difference(\n          returns, clipped_value_preds)\n      value_estimation_error = tf.maximum(value_estimation_error,\n                                          clipped_value_estimation_error)\n\n    value_estimation_error *= weights\n\n    value_estimation_loss = (\n        tf.reduce_mean(input_tensor=value_estimation_error) *\n        self._value_pred_loss_coef)\n    if debug_summaries:\n      tf.compat.v2.summary.scalar(\n          name=\'value_pred_avg\',\n          data=tf.reduce_mean(input_tensor=value_preds),\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'value_actual_avg\',\n          data=tf.reduce_mean(input_tensor=returns),\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'value_estimation_loss\',\n          data=value_estimation_loss,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'value_preds\', data=value_preds, step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'value_estimation_error\',\n          data=value_estimation_error,\n          step=self.train_step_counter)\n\n    if self._check_numerics:\n      value_estimation_loss = tf.debugging.check_numerics(\n          value_estimation_loss, \'value_estimation_loss\')\n\n    return value_estimation_loss\n\n  def policy_gradient_loss(\n      self,\n      time_steps: ts.TimeStep,\n      actions: types.NestedTensor,\n      sample_action_log_probs: types.Tensor,\n      advantages: types.Tensor,\n      current_policy_distribution: types.NestedDistribution,\n      weights: types.Tensor,\n      debug_summaries: bool = False) -> types.Tensor:\n    """"""Create tensor for policy gradient loss.\n\n    All tensors should have a single batch dimension.\n\n    Args:\n      time_steps: TimeSteps with observations for each timestep.\n      actions: Tensor of actions for timesteps, aligned on index.\n      sample_action_log_probs: Tensor of sample probability of each action.\n      advantages: Tensor of advantage estimate for each timestep, aligned on\n        index. Works better when advantage estimates are normalized.\n      current_policy_distribution: The policy distribution, evaluated on all\n        time_steps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  Includes a mask for invalid timesteps.\n      debug_summaries: True if debug summaries should be created.\n\n    Returns:\n      policy_gradient_loss: A tensor that will contain policy gradient loss for\n        the on-policy experience.\n    """"""\n    nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n    action_log_prob = common.log_probability(current_policy_distribution,\n                                             actions, self._action_spec)\n    action_log_prob = tf.cast(action_log_prob, tf.float32)\n    if self._log_prob_clipping > 0.0:\n      action_log_prob = tf.clip_by_value(action_log_prob,\n                                         -self._log_prob_clipping,\n                                         self._log_prob_clipping)\n    if self._check_numerics:\n      action_log_prob = tf.debugging.check_numerics(action_log_prob,\n                                                    \'action_log_prob\')\n\n    # Prepare both clipped and unclipped importance ratios.\n    importance_ratio = tf.exp(action_log_prob - sample_action_log_probs)\n    importance_ratio_clipped = tf.clip_by_value(\n        importance_ratio, 1 - self._importance_ratio_clipping,\n        1 + self._importance_ratio_clipping)\n\n    if self._check_numerics:\n      importance_ratio = tf.debugging.check_numerics(importance_ratio,\n                                                     \'importance_ratio\')\n      if self._importance_ratio_clipping > 0.0:\n        importance_ratio_clipped = tf.debugging.check_numerics(\n            importance_ratio_clipped, \'importance_ratio_clipped\')\n\n    # Pessimistically choose the minimum objective value for clipped and\n    #   unclipped importance ratios.\n    per_timestep_objective = importance_ratio * advantages\n    per_timestep_objective_clipped = importance_ratio_clipped * advantages\n    per_timestep_objective_min = tf.minimum(per_timestep_objective,\n                                            per_timestep_objective_clipped)\n\n    if self._importance_ratio_clipping > 0.0:\n      policy_gradient_loss = -per_timestep_objective_min\n    else:\n      policy_gradient_loss = -per_timestep_objective\n\n    policy_gradient_loss = tf.reduce_mean(input_tensor=policy_gradient_loss *\n                                          weights)\n\n    if debug_summaries:\n      if self._importance_ratio_clipping > 0.0:\n        clip_fraction = tf.reduce_mean(\n            input_tensor=tf.cast(\n                tf.greater(\n                    tf.abs(importance_ratio -\n                           1.0), self._importance_ratio_clipping), tf.float32))\n        tf.compat.v2.summary.scalar(\n            name=\'clip_fraction\',\n            data=clip_fraction,\n            step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'action_log_prob\',\n          data=action_log_prob,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'action_log_prob_sample\',\n          data=sample_action_log_probs,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'importance_ratio\',\n          data=importance_ratio,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'importance_ratio_mean\',\n          data=tf.reduce_mean(input_tensor=importance_ratio),\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'importance_ratio_clipped\',\n          data=importance_ratio_clipped,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'per_timestep_objective\',\n          data=per_timestep_objective,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'per_timestep_objective_clipped\',\n          data=per_timestep_objective_clipped,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'per_timestep_objective_min\',\n          data=per_timestep_objective_min,\n          step=self.train_step_counter)\n      entropy = common.entropy(current_policy_distribution, self.action_spec)\n      tf.compat.v2.summary.histogram(\n          name=\'policy_entropy\', data=entropy, step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'policy_entropy_mean\',\n          data=tf.reduce_mean(input_tensor=entropy),\n          step=self.train_step_counter)\n      for i, (single_action, single_distribution) in enumerate(\n          zip(\n              tf.nest.flatten(self.action_spec),\n              tf.nest.flatten(current_policy_distribution))):\n        # Categorical distribution (used for discrete actions) doesn\'t have a\n        # mean.\n        distribution_index = \'_{}\'.format(i) if i > 0 else \'\'\n        if not tensor_spec.is_discrete(single_action):\n          tf.compat.v2.summary.histogram(\n              name=\'actions_distribution_mean\' + distribution_index,\n              data=single_distribution.mean(),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.histogram(\n              name=\'actions_distribution_stddev\' + distribution_index,\n              data=single_distribution.stddev(),\n              step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'policy_gradient_loss\',\n          data=policy_gradient_loss,\n          step=self.train_step_counter)\n\n    if self._check_numerics:\n      policy_gradient_loss = tf.debugging.check_numerics(\n          policy_gradient_loss, \'policy_gradient_loss\')\n\n    return policy_gradient_loss\n\n  def kl_cutoff_loss(self,\n                     kl_divergence: types.Tensor,\n                     debug_summaries: bool = False) -> types.Tensor:\n    # Squared penalization for mean KL divergence above some threshold.\n    if self._kl_cutoff_factor <= 0.0:\n      return tf.constant(0.0, dtype=tf.float32, name=\'zero_kl_cutoff_loss\')\n    kl_cutoff = self._kl_cutoff_factor * self._adaptive_kl_target\n    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n    kl_over_cutoff = tf.maximum(mean_kl - kl_cutoff, 0.0)\n    kl_cutoff_loss = self._kl_cutoff_coef * tf.square(kl_over_cutoff)\n\n    if debug_summaries:\n      tf.compat.v2.summary.scalar(\n          name=\'kl_cutoff_count\',\n          data=tf.reduce_sum(\n              input_tensor=tf.cast(kl_divergence > kl_cutoff, dtype=tf.int64)),\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'kl_cutoff_loss\',\n          data=kl_cutoff_loss,\n          step=self.train_step_counter)\n\n    return tf.identity(kl_cutoff_loss, name=\'kl_cutoff_loss\')\n\n  def adaptive_kl_loss(self,\n                       kl_divergence: types.Tensor,\n                       debug_summaries: bool = False) -> types.Tensor:\n    if self._adaptive_kl_beta is None:\n      return tf.constant(0.0, dtype=tf.float32, name=\'zero_adaptive_kl_loss\')\n\n    # Define the loss computation, which depends on the update computation.\n    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n    adaptive_kl_loss = self._adaptive_kl_beta * mean_kl\n\n    if debug_summaries:\n      tf.compat.v2.summary.scalar(\n          name=\'adaptive_kl_loss\',\n          data=adaptive_kl_loss,\n          step=self.train_step_counter)\n\n    return adaptive_kl_loss\n\n  def _kl_divergence(self, time_steps, action_distribution_parameters,\n                     current_policy_distribution):\n    outer_dims = list(\n        range(nest_utils.get_outer_rank(time_steps, self.time_step_spec)))\n\n    old_actions_distribution = (\n        distribution_spec.nested_distributions_from_specs(\n            self._action_distribution_spec, action_distribution_parameters))\n\n    kl_divergence = ppo_utils.nested_kl_divergence(\n        old_actions_distribution,\n        current_policy_distribution,\n        outer_dims=outer_dims)\n    return kl_divergence\n\n  def kl_penalty_loss(self,\n                      time_steps: ts.TimeStep,\n                      action_distribution_parameters: types.NestedTensor,\n                      current_policy_distribution: types.NestedDistribution,\n                      weights: types.Tensor,\n                      debug_summaries: bool = False) -> types.Tensor:\n    """"""Compute a loss that penalizes policy steps with high KL.\n\n    Based on KL divergence from old (data-collection) policy to new (updated)\n    policy.\n\n    All tensors should have a single batch dimension.\n\n    Args:\n      time_steps: TimeStep tuples with observations for each timestep. Used for\n        computing new action distributions.\n      action_distribution_parameters: Action distribution params of the data\n        collection policy, used for reconstruction old action distributions.\n      current_policy_distribution: The policy distribution, evaluated on all\n        time_steps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  Inlcudes a mask for invalid timesteps.\n      debug_summaries: True if debug summaries should be created.\n\n    Returns:\n      kl_penalty_loss: The sum of a squared penalty for KL over a constant\n        threshold, plus an adaptive penalty that encourages updates toward a\n        target KL divergence.\n    """"""\n    kl_divergence = self._kl_divergence(time_steps,\n                                        action_distribution_parameters,\n                                        current_policy_distribution)\n    kl_divergence *= weights\n\n    if debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'kl_divergence\',\n          data=kl_divergence,\n          step=self.train_step_counter)\n\n    kl_cutoff_loss = self.kl_cutoff_loss(kl_divergence, debug_summaries)\n    adaptive_kl_loss = self.adaptive_kl_loss(kl_divergence, debug_summaries)\n    return tf.add(kl_cutoff_loss, adaptive_kl_loss, name=\'kl_penalty_loss\')\n\n  def update_adaptive_kl_beta(\n      self, kl_divergence: types.Tensor) -> Optional[tf.Operation]:\n    """"""Create update op for adaptive KL penalty coefficient.\n\n    Args:\n      kl_divergence: KL divergence of old policy to new policy for all\n        timesteps.\n\n    Returns:\n      update_op: An op which runs the update for the adaptive kl penalty term.\n    """"""\n    if self._adaptive_kl_beta is None:\n      return tf.no_op()\n\n    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n\n    # Update the adaptive kl beta after each time it is computed.\n    mean_kl_below_bound = (\n        mean_kl < self._adaptive_kl_target *\n        (1.0 - self._adaptive_kl_tolerance))\n    mean_kl_above_bound = (\n        mean_kl > self._adaptive_kl_target *\n        (1.0 + self._adaptive_kl_tolerance))\n    adaptive_kl_update_factor = tf.case(\n        [\n            (mean_kl_below_bound,\n             lambda: tf.constant(1.0 / 1.5, dtype=tf.float32)),\n            (mean_kl_above_bound, lambda: tf.constant(1.5, dtype=tf.float32)),\n        ],\n        default=lambda: tf.constant(1.0, dtype=tf.float32),\n        exclusive=True)\n\n    new_adaptive_kl_beta = tf.maximum(\n        self._adaptive_kl_beta * adaptive_kl_update_factor, 10e-16)\n    tf.compat.v1.assign(self._adaptive_kl_beta, new_adaptive_kl_beta)\n\n    if self._debug_summaries:\n      tf.compat.v2.summary.scalar(\n          name=\'adaptive_kl_update_factor\',\n          data=adaptive_kl_update_factor,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'mean_kl_divergence\', data=mean_kl, step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'adaptive_kl_beta\',\n          data=self._adaptive_kl_beta,\n          step=self.train_step_counter)\n\n    return self._adaptive_kl_beta\n'"
tf_agents/agents/ppo/ppo_agent_test.py,174,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Tests for TF Agents ppo_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nfrom absl.testing import parameterized\nfrom absl.testing.absltest import mock\n\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.ppo import ppo_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import random_tf_environment\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils as network_utils\nfrom tf_agents.networks import value_network\nfrom tf_agents.networks import value_rnn_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import test_utils\n\nFLAGS = flags.FLAGS\n\n\nclass DummyActorNet(network.DistributionNetwork):\n\n  def __init__(self,\n               input_spec,\n               action_spec,\n               preprocessing_layers=None,\n               name=None):\n    output_spec = self._get_normal_distribution_spec(action_spec)\n    super(DummyActorNet, self).__init__(\n        input_spec, (), output_spec=output_spec, name=\'DummyActorNet\')\n    self._action_spec = action_spec\n    self._flat_action_spec = tf.nest.flatten(self._action_spec)[0]\n\n    self._dummy_layers = (preprocessing_layers or []) + [\n        tf.keras.layers.Dense(\n            self._flat_action_spec.shape.num_elements() * 2,\n            kernel_initializer=tf.compat.v1.initializers.constant([[2.0, 1.0],\n                                                                   [1.0, 1.0]]),\n            bias_initializer=tf.compat.v1.initializers.constant([5.0, 5.0]),\n            activation=None,\n        )\n    ]\n\n  def _get_normal_distribution_spec(self, sample_spec):\n    input_param_shapes = tfp.distributions.Normal.param_static_shapes(\n        sample_spec.shape)\n    input_param_spec = tf.nest.map_structure(\n        lambda tensor_shape: tensor_spec.TensorSpec(  # pylint: disable=g-long-lambda\n            shape=tensor_shape,\n            dtype=sample_spec.dtype),\n        input_param_shapes)\n\n    return distribution_spec.DistributionSpec(\n        tfp.distributions.Normal, input_param_spec, sample_spec=sample_spec)\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n\n    # Calls coming from agent.train() has a time dimension. Direct loss calls\n    # may not have a time dimension. It order to make BatchSquash work, we need\n    # to specify the outer dimension properly.\n    has_time_dim = nest_utils.get_outer_rank(inputs,\n                                             self.input_tensor_spec) == 2\n    outer_rank = 2 if has_time_dim else 1\n    batch_squash = network_utils.BatchSquash(outer_rank)\n    hidden_state = batch_squash.flatten(hidden_state)\n\n    for layer in self._dummy_layers:\n      hidden_state = layer(hidden_state)\n\n    actions, stdevs = tf.split(hidden_state, 2, axis=1)\n    actions = batch_squash.unflatten(actions)\n    stdevs = batch_squash.unflatten(stdevs)\n    actions = tf.nest.pack_sequence_as(self._action_spec, [actions])\n    stdevs = tf.nest.pack_sequence_as(self._action_spec, [stdevs])\n\n    return self.output_spec.build_distribution(\n        loc=actions, scale=stdevs), network_state\n\n\nclass DummyValueNet(network.Network):\n\n  def __init__(self,\n               observation_spec,\n               preprocessing_layers=None,\n               name=None,\n               outer_rank=1):\n    super(DummyValueNet, self).__init__(observation_spec, (), \'DummyValueNet\')\n    self._outer_rank = outer_rank\n    self._dummy_layers = (preprocessing_layers or []) + [\n        tf.keras.layers.Dense(\n            1,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n    batch_squash = network_utils.BatchSquash(self._outer_rank)\n    hidden_state = batch_squash.flatten(hidden_state)\n    for layer in self._dummy_layers:\n      hidden_state = layer(hidden_state)\n    value_pred = tf.squeeze(batch_squash.unflatten(hidden_state), axis=-1)\n    return value_pred, network_state\n\n\ndef _compute_returns_fn(rewards, discounts, next_state_return=0.0):\n  """"""Python implementation of computing discounted returns.""""""\n  returns = np.zeros_like(rewards)\n  for t in range(len(returns) - 1, -1, -1):\n    returns[t] = rewards[t] + discounts[t] * next_state_return\n    next_state_return = returns[t]\n  return returns\n\n\ndef _create_joint_actor_value_networks(observation_spec, action_spec):\n  shared_layers = [\n      tf.keras.layers.Dense(\n          tf.nest.flatten(observation_spec)[0].shape.num_elements(),\n          kernel_initializer=tf.compat.v1.initializers.constant([[3.0, 1.0],\n                                                                 [1.0, 1.0]]),\n          bias_initializer=tf.compat.v1.initializers.constant([5.0, 5.0]),\n          activation=None,\n      )\n  ]\n  actor_net = DummyActorNet(observation_spec, action_spec, shared_layers)\n  value_net = DummyValueNet(observation_spec, shared_layers)\n  return actor_net, value_net\n\n\nclass PPOAgentTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(PPOAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n\n  def testCreateAgent(self):\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        check_numerics=True)\n    agent.initialize()\n\n  def testComputeAdvantagesNoGae(self):\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        use_gae=False)\n    rewards = tf.constant([[1.0] * 9, [1.0] * 9])\n    discounts = tf.constant([[1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0],\n                             [1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0]])\n    returns = tf.constant([[5.0, 4.0, 3.0, 2.0, 1.0, 3.439, 2.71, 1.9, 1.0],\n                           [3.0, 4.0, 7.0, 2.0, -1.0, 5.439, 2.71, -2.9, 1.0]])\n    value_preds = tf.constant([\n        [3.0] * 10,\n        [3.0] * 10,\n    ])  # One extra for final time_step.\n\n    expected_advantages = returns - value_preds[:, :-1]\n    advantages = agent.compute_advantages(rewards, returns, discounts,\n                                          value_preds)\n    self.assertAllClose(expected_advantages, advantages)\n\n  def testComputeAdvantagesWithGae(self):\n    gae_lambda = 0.95\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(\n            self._obs_spec,\n            self._action_spec,\n        ),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        use_gae=True,\n        lambda_value=gae_lambda)\n    rewards = tf.constant([[1.0] * 9, [1.0] * 9])\n    discounts = tf.constant([[1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0],\n                             [1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.9, 0.9, 0.0]])\n    returns = tf.constant([[5.0, 4.0, 3.0, 2.0, 1.0, 3.439, 2.71, 1.9, 1.0],\n                           [5.0, 4.0, 3.0, 2.0, 1.0, 3.439, 2.71, 1.9, 1.0]])\n    value_preds = tf.constant([[3.0] * 10,\n                               [3.0] * 10])  # One extra for final time_step.\n\n    gae_vals = tf.constant([[\n        2.0808625, 1.13775, 0.145, -0.9, -2.0, 0.56016475, -0.16355, -1.01, -2.0\n    ], [\n        2.0808625, 1.13775, 0.145, -0.9, -2.0, 0.56016475, -0.16355, -1.01, -2.0\n    ]])\n    advantages = agent.compute_advantages(rewards, returns, discounts,\n                                          value_preds)\n    self.assertAllClose(gae_vals, advantages)\n\n  def testSequencePreprocess(self):\n    counter = common.create_variable(\'test_train_counter\')\n    batch_size = 2\n    n_time_steps = 3\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(\n            self._obs_spec,\n            self._action_spec,\n        ),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        num_epochs=1,\n        use_gae=False,\n        use_td_lambda_return=False,\n        compute_value_and_advantage_in_train=False,\n        train_step_counter=counter)\n    observations = tf.constant([\n        [[1, 2], [3, 4], [5, 6]],\n        [[1, 2], [3, 4], [5, 6]],\n    ],\n                               dtype=tf.float32)\n\n    mid_time_step_val = ts.StepType.MID.tolist()\n    time_steps = ts.TimeStep(\n        step_type=tf.constant(\n            [[mid_time_step_val] * n_time_steps] * batch_size, dtype=tf.int32),\n        reward=tf.constant([[1] * n_time_steps] * batch_size, dtype=tf.float32),\n        discount=tf.constant(\n            [[1] * n_time_steps] * batch_size, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]], [[0], [1], [1]]], dtype=tf.float32)\n\n    old_action_distribution_parameters = {\n        \'loc\':\n            tf.constant(\n                [[[0.0]] * n_time_steps] * batch_size, dtype=tf.float32),\n        \'scale\':\n            tf.constant(\n                [[[1.0]] * n_time_steps] * batch_size, dtype=tf.float32),\n    }\n\n    value_preds = tf.constant([[9., 15., 21.], [9., 15., 21.]],\n                              dtype=tf.float32)\n    policy_info = {\n        \'dist_params\': old_action_distribution_parameters,\n        \'value_prediction\': value_preds,\n    }\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, policy_info,\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n\n    returned_experience = agent.preprocess_sequence(experience)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n    self.assertAllClose(observations, returned_experience.observation)\n    self.assertAllClose(actions, returned_experience.action)\n\n    expected_value_preds = tf.constant([[9., 15., 21.], [9., 15., 21.]],\n                                       dtype=tf.float32)\n    (_, _, next_time_steps) = trajectory.to_transition(experience)\n    expected_returns, expected_normalized_advantages = agent.compute_return_and_advantage(\n        next_time_steps, expected_value_preds)\n    self.assertAllClose(old_action_distribution_parameters,\n                        returned_experience.policy_info[\'dist_params\'])\n    self.assertEqual((batch_size, n_time_steps),\n                     returned_experience.policy_info[\'return\'].shape)\n    self.assertAllClose(expected_returns,\n                        returned_experience.policy_info[\'return\'][:, :-1])\n    self.assertEqual(\n        (batch_size, n_time_steps),\n        returned_experience.policy_info[\'normalized_advantage\'].shape)\n    self.assertAllClose(\n        expected_normalized_advantages,\n        returned_experience.policy_info[\'normalized_advantage\'][:, :-1])\n\n  def testSequencePreprocessNotBatched(self):\n    counter = common.create_variable(\'test_train_counter\')\n    n_time_steps = 3\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(\n            self._obs_spec,\n            self._action_spec,\n        ),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        num_epochs=1,\n        use_gae=False,\n        use_td_lambda_return=False,\n        compute_value_and_advantage_in_train=False,\n        train_step_counter=counter)\n    observations = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)\n\n    mid_time_step_val = ts.StepType.MID.tolist()\n    time_steps = ts.TimeStep(\n        step_type=tf.constant(\n            [mid_time_step_val] * n_time_steps, dtype=tf.int32),\n        reward=tf.constant([1] * n_time_steps, dtype=tf.float32),\n        discount=tf.constant([1] * n_time_steps, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[0], [1], [1]], dtype=tf.float32)\n\n    old_action_distribution_parameters = {\n        \'loc\': tf.constant([[0.0]] * n_time_steps, dtype=tf.float32),\n        \'scale\': tf.constant([[1.0]] * n_time_steps, dtype=tf.float32),\n    }\n\n    value_preds = tf.constant([9., 15., 21.], dtype=tf.float32)\n    policy_info = {\n        \'dist_params\': old_action_distribution_parameters,\n        \'value_prediction\': value_preds,\n    }\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, policy_info,\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n\n    returned_experience = agent.preprocess_sequence(experience)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n    self.assertAllClose(observations, returned_experience.observation)\n    self.assertAllClose(actions, returned_experience.action)\n\n    self.assertAllClose(old_action_distribution_parameters,\n                        returned_experience.policy_info[\'dist_params\'])\n    self.assertEqual(n_time_steps,\n                     returned_experience.policy_info[\'return\'].shape)\n    self.assertAllClose([40.4821, 30.79],\n                        returned_experience.policy_info[\'return\'][:-1])\n    self.assertEqual(\n        n_time_steps,\n        returned_experience.policy_info[\'normalized_advantage\'].shape)\n    self.assertAllClose(\n        [1., -1.], returned_experience.policy_info[\'normalized_advantage\'][:-1])\n\n  @parameterized.named_parameters([\n      (\'OneEpochValueInTrain\', 1, True, True),\n      (\'FiveEpochsValueInCollect\', 5, False, False),\n      (\'IncompleteEpisodesReturnNonZeroLoss\', 1, False, True),\n  ])\n  def testTrain(self, num_epochs, use_td_lambda_return,\n                compute_value_and_advantage_in_train):\n    # Mock the build_train_op to return an op for incrementing this counter.\n    counter = common.create_variable(\'test_train_counter\')\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(\n            self._obs_spec,\n            self._action_spec,\n        ),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        num_epochs=num_epochs,\n        use_gae=use_td_lambda_return,\n        use_td_lambda_return=use_td_lambda_return,\n        compute_value_and_advantage_in_train=compute_value_and_advantage_in_train,\n        train_step_counter=counter)\n    observations = tf.constant([\n        [[1, 2], [3, 4], [5, 6]],\n        [[1, 2], [3, 4], [5, 6]],\n    ],\n                               dtype=tf.float32)\n\n    mid_time_step_val = ts.StepType.MID.tolist()\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[mid_time_step_val] * 3] * 2, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]], [[0], [1], [1]]], dtype=tf.float32)\n\n    action_distribution_parameters = {\n        \'loc\': tf.constant([[[0.0]] * 3] * 2, dtype=tf.float32),\n        \'scale\': tf.constant([[[1.0]] * 3] * 2, dtype=tf.float32),\n    }\n    value_preds = tf.constant([[9., 15., 21.], [9., 15., 21.]],\n                              dtype=tf.float32)\n\n    policy_info = {\n        \'dist_params\': action_distribution_parameters,\n    }\n    if not compute_value_and_advantage_in_train:\n      policy_info[\'value_prediction\'] = value_preds\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, policy_info,\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n    if not compute_value_and_advantage_in_train:\n      experience = agent._preprocess(experience)\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    # Assert that counter starts out at zero.\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    self.assertEqual(0, self.evaluate(counter))\n    loss_type = self.evaluate(loss)\n    loss_numpy = loss_type.loss\n\n    # Assert that loss is not zero as we are training in a non-episodic env.\n    self.assertNotEqual(\n        loss_numpy,\n        0.0,\n        msg=(\'Loss is exactly zero, looks like no training \'\n             \'was performed due to incomplete episodes.\'))\n\n    # Assert that train_op ran increment_counter num_epochs times.\n    self.assertEqual(num_epochs, self.evaluate(counter))\n\n  def testGetEpochLoss(self):\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        normalize_rewards=False,\n        value_pred_loss_coef=1.0,\n        policy_l2_reg=1e-4,\n        value_function_l2_reg=1e-4,\n        entropy_regularization=0.1,\n        importance_ratio_clipping=10,\n    )\n    observations = tf.constant([[1, 2], [3, 4], [1, 2], [3, 4]],\n                               dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1], [0], [1]], dtype=tf.float32)\n    returns = tf.constant([1.9, 1.0, 1.9, 1.0], dtype=tf.float32)\n    sample_action_log_probs = tf.constant([0.9, 0.3, 0.9, 0.3],\n                                          dtype=tf.float32)\n    advantages = tf.constant([1.9, 1.0, 1.9, 1.0], dtype=tf.float32)\n    weights = tf.constant([1.0, 1.0, 0.0, 0.0], dtype=tf.float32)\n    sample_action_distribution_parameters = {\n        \'loc\': tf.constant([[9.0], [15.0], [9.0], [15.0]], dtype=tf.float32),\n        \'scale\': tf.constant([[8.0], [12.0], [8.0], [12.0]], dtype=tf.float32),\n    }\n    train_step = tf.compat.v1.train.get_or_create_global_step()\n\n    loss_info = agent.get_loss(\n        time_steps,\n        actions,\n        sample_action_log_probs,\n        returns,\n        advantages,\n        sample_action_distribution_parameters,\n        weights,\n        train_step,\n        debug_summaries=False)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    total_loss, extra_loss_info = self.evaluate(loss_info)\n    (policy_gradient_loss, value_estimation_loss, l2_regularization_loss,\n     entropy_reg_loss, kl_penalty_loss) = extra_loss_info\n\n    # Check loss values are as expected. Factor of 2/4 is because four timesteps\n    # were included in the data, but two were masked out. Reduce_means in losses\n    # will divide by 4, but computed loss values are for first 2 timesteps.\n    expected_pg_loss = -0.0164646133 * 2 / 4\n    expected_ve_loss = 123.205 * 2 / 4\n    expected_l2_loss = 1e-4 * 12 * 2 / 4\n    expected_ent_loss = -0.370111 * 2 / 4\n    expected_kl_penalty_loss = 0.0\n    self.assertAllClose(\n        expected_pg_loss + expected_ve_loss + expected_l2_loss +\n        expected_ent_loss + expected_kl_penalty_loss,\n        total_loss,\n        atol=0.001,\n        rtol=0.001)\n    self.assertAllClose(expected_pg_loss, policy_gradient_loss)\n    self.assertAllClose(expected_ve_loss, value_estimation_loss)\n    self.assertAllClose(\n        expected_l2_loss, l2_regularization_loss, atol=0.001, rtol=0.001)\n    self.assertAllClose(expected_ent_loss, entropy_reg_loss)\n    self.assertAllClose(expected_kl_penalty_loss, kl_penalty_loss)\n\n  @parameterized.named_parameters([\n      (\'IsZero\', 0),\n      (\'NotZero\', 1),\n  ])\n  def testL2RegularizationLoss(self, not_zero):\n    l2_reg = 1e-4 * not_zero\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        policy_l2_reg=l2_reg,\n        value_function_l2_reg=l2_reg,\n    )\n\n    # Call other loss functions to make sure trainable variables are\n    #   constructed.\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n    sample_action_log_probs = tf.constant([[0.9], [0.3]], dtype=tf.float32)\n    advantages = tf.constant([1.9, 1.0], dtype=tf.float32)\n    current_policy_distribution, unused_network_state = DummyActorNet(\n        self._obs_spec, self._action_spec)(time_steps.observation,\n                                           time_steps.step_type, ())\n    weights = tf.ones_like(advantages)\n    agent.policy_gradient_loss(time_steps, actions, sample_action_log_probs,\n                               advantages, current_policy_distribution, weights)\n    agent.value_estimation_loss(time_steps, returns, weights)\n\n    # Now request L2 regularization loss.\n    # Value function weights are [2, 1], actor net weights are [2, 1, 1, 1].\n    expected_loss = l2_reg * ((2**2 + 1) + (2**2 + 1 + 1 + 1))\n    # Make sure the network is built before we try to get variables.\n    agent.policy.action(\n        tensor_spec.sample_spec_nest(self._time_step_spec, outer_dims=(2,)))\n    loss = agent.l2_regularization_loss()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  @parameterized.named_parameters([\n      (\'IsZero\', 0),\n      (\'NotZero\', 1),\n  ])\n  def testL2RegularizationLossWithSharedVariables(self, not_zero):\n    policy_l2_reg = 4e-4 * not_zero\n    value_function_l2_reg = 2e-4 * not_zero\n    shared_vars_l2_reg = 1e-4 * not_zero\n    actor_net, value_net = _create_joint_actor_value_networks(\n        self._obs_spec, self._action_spec)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        normalize_observations=False,\n        policy_l2_reg=policy_l2_reg,\n        value_function_l2_reg=value_function_l2_reg,\n        shared_vars_l2_reg=shared_vars_l2_reg,\n    )\n\n    # Call other loss functions to make sure trainable variables are\n    #   constructed.\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n    sample_action_log_probs = tf.constant([[0.9], [0.3]], dtype=tf.float32)\n    advantages = tf.constant([1.9, 1.0], dtype=tf.float32)\n    current_policy_distribution, unused_network_state = DummyActorNet(\n        self._obs_spec, self._action_spec)(time_steps.observation,\n                                           time_steps.step_type, ())\n    weights = tf.ones_like(advantages)\n    agent.policy_gradient_loss(time_steps, actions, sample_action_log_probs,\n                               advantages, current_policy_distribution, weights)\n    agent.value_estimation_loss(time_steps, returns, weights)\n\n    # Now request L2 regularization loss.\n    # Value function weights are [2, 1], actor net weights are [2, 1, 1, 1],\n    # shared weights are [3, 1, 1, 1].\n    expected_loss = value_function_l2_reg * (2**2 + 1) + policy_l2_reg * (\n        2**2 + 1 + 1 + 1) + shared_vars_l2_reg * (3**2 + 1 + 1 + 1)\n    # Make sure the network is built before we try to get variables.\n    agent.policy.action(\n        tensor_spec.sample_spec_nest(self._time_step_spec, outer_dims=(2,)))\n    loss = agent.l2_regularization_loss()\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  @parameterized.named_parameters([\n      (\'IsZero\', 0),\n      (\'NotZero\', 1),\n  ])\n  def testEntropyRegularizationLoss(self, not_zero):\n    ent_reg = 0.1 * not_zero\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        normalize_observations=False,\n        entropy_regularization=ent_reg,\n    )\n\n    # Call other loss functions to make sure trainable variables are\n    #   constructed.\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n    sample_action_log_probs = tf.constant([[0.9], [0.3]], dtype=tf.float32)\n    advantages = tf.constant([1.9, 1.0], dtype=tf.float32)\n    weights = tf.ones_like(advantages)\n    current_policy_distribution, unused_network_state = DummyActorNet(\n        self._obs_spec, self._action_spec)(time_steps.observation,\n                                           time_steps.step_type, ())\n    agent.policy_gradient_loss(time_steps, actions, sample_action_log_probs,\n                               advantages, current_policy_distribution, weights)\n    agent.value_estimation_loss(time_steps, returns, weights)\n\n    # Now request entropy regularization loss.\n    # Action stdevs should be ~1.0, and mean entropy ~3.70111.\n    expected_loss = -3.70111 * ent_reg\n    loss = agent.entropy_regularization_loss(time_steps,\n                                             current_policy_distribution,\n                                             weights)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testValueEstimationLoss(self):\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=DummyValueNet(self._obs_spec),\n        value_pred_loss_coef=1.0,\n        normalize_observations=False,\n    )\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n    weights = tf.ones_like(returns)\n\n    expected_loss = 123.205\n    loss = agent.value_estimation_loss(time_steps, returns, weights)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testPolicyGradientLoss(self):\n    actor_net = DummyActorNet(self._obs_spec, self._action_spec)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        normalize_observations=False,\n        normalize_rewards=False,\n        actor_net=actor_net,\n        value_net=DummyValueNet(self._obs_spec),\n        importance_ratio_clipping=10.0,\n    )\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    sample_action_log_probs = tf.constant([0.9, 0.3], dtype=tf.float32)\n    advantages = tf.constant([1.9, 1.0], dtype=tf.float32)\n    weights = tf.ones_like(advantages)\n\n    current_policy_distribution, unused_network_state = actor_net(\n        time_steps.observation, time_steps.step_type, ())\n\n    expected_loss = -0.0164646133\n    loss = agent.policy_gradient_loss(time_steps, actions,\n                                      sample_action_log_probs, advantages,\n                                      current_policy_distribution, weights)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testKlPenaltyLoss(self):\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        fc_layer_params=None)\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        kl_cutoff_factor=5.0,\n        adaptive_kl_target=0.1,\n        kl_cutoff_coef=100,\n    )\n\n    agent.kl_cutoff_loss = mock.MagicMock(\n        return_value=tf.constant(3.0, dtype=tf.float32))\n    agent.adaptive_kl_loss = mock.MagicMock(\n        return_value=tf.constant(4.0, dtype=tf.float32))\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    action_distribution_parameters = {\n        \'loc\': tf.constant([1.0, 1.0], dtype=tf.float32),\n        \'scale\': tf.constant([1.0, 1.0], dtype=tf.float32),\n    }\n    current_policy_distribution, unused_network_state = DummyActorNet(\n        self._obs_spec, self._action_spec)(time_steps.observation,\n                                           time_steps.step_type, ())\n    weights = tf.ones_like(time_steps.discount)\n\n    expected_kl_penalty_loss = 7.0\n\n    kl_penalty_loss = agent.kl_penalty_loss(time_steps,\n                                            action_distribution_parameters,\n                                            current_policy_distribution,\n                                            weights)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    kl_penalty_loss_ = self.evaluate(kl_penalty_loss)\n    self.assertEqual(expected_kl_penalty_loss, kl_penalty_loss_)\n\n  @parameterized.named_parameters([\n      (\'IsZero\', 0),\n      (\'NotZero\', 1),\n  ])\n  def testKlCutoffLoss(self, not_zero):\n    kl_cutoff_coef = 30.0 * not_zero\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        fc_layer_params=None)\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        kl_cutoff_factor=5.0,\n        adaptive_kl_target=0.1,\n        kl_cutoff_coef=kl_cutoff_coef,\n    )\n    kl_divergence = tf.constant([[1.5, -0.5, 6.5, -1.5, -2.3]],\n                                dtype=tf.float32)\n    expected_kl_cutoff_loss = kl_cutoff_coef * (.24**2)  # (0.74 - 0.5) ^ 2\n\n    loss = agent.kl_cutoff_loss(kl_divergence)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose([loss_], [expected_kl_cutoff_loss])\n\n  def testAdaptiveKlLoss(self):\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        fc_layer_params=None)\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        initial_adaptive_kl_beta=1.0,\n        adaptive_kl_target=10.0,\n        adaptive_kl_tolerance=0.5,\n    )\n\n    # Initialize variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Loss should not change if data kl is target kl.\n    loss_1 = agent.adaptive_kl_loss([10.0])\n    loss_2 = agent.adaptive_kl_loss([10.0])\n    self.assertEqual(self.evaluate(loss_1), self.evaluate(loss_2))\n\n    # If data kl is low, kl penalty should decrease between calls.\n    loss_1 = self.evaluate(agent.adaptive_kl_loss([1.0]))\n    adaptive_kl_beta_update_fn = common.function(agent.update_adaptive_kl_beta)\n    self.evaluate(adaptive_kl_beta_update_fn([1.0]))\n    loss_2 = self.evaluate(agent.adaptive_kl_loss([1.0]))\n    self.assertGreater(loss_1, loss_2)\n\n    # # # If data kl is low, kl penalty should increase between calls.\n    loss_1 = self.evaluate(agent.adaptive_kl_loss([100.0]))\n    self.evaluate(adaptive_kl_beta_update_fn([100.0]))\n    loss_2 = self.evaluate(agent.adaptive_kl_loss([100.0]))\n    self.assertLess(loss_1, loss_2)\n\n  def testUpdateAdaptiveKlBeta(self):\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        fc_layer_params=None)\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        initial_adaptive_kl_beta=1.0,\n        adaptive_kl_target=10.0,\n        adaptive_kl_tolerance=0.5,\n    )\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # When KL is target kl, beta should not change.\n    update_adaptive_kl_beta_fn = common.function(agent.update_adaptive_kl_beta)\n    beta_0 = update_adaptive_kl_beta_fn([10.0])\n    expected_beta_0 = 1.0\n    self.assertEqual(expected_beta_0, self.evaluate(beta_0))\n\n    # When KL is large, beta should increase.\n    beta_1 = update_adaptive_kl_beta_fn([100.0])\n    expected_beta_1 = 1.5\n    self.assertEqual(expected_beta_1, self.evaluate(beta_1))\n\n    # When KL is small, beta should decrease.\n    beta_2 = update_adaptive_kl_beta_fn([1.0])\n    expected_beta_2 = 1.0\n    self.assertEqual(expected_beta_2, self.evaluate(beta_2))\n\n  def testPolicy(self):\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=DummyActorNet(self._obs_spec, self._action_spec),\n        value_net=value_net)\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=1)\n    action_step = agent.policy.action(time_steps)\n    actions = action_step.action\n    self.assertEqual(actions.shape.as_list(), [1, 1])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    _ = self.evaluate(actions)\n\n  def testNormalizeAdvantages(self):\n    advantages = np.array([1.1, 3.2, -1.5, 10.9, 5.6])\n    mean = np.sum(advantages) / float(len(advantages))\n    variance = np.sum(np.square(advantages - mean)) / float(len(advantages))\n    stdev = np.sqrt(variance)\n    expected_advantages = (advantages - mean) / stdev\n    normalized_advantages = ppo_agent._normalize_advantages(\n        tf.constant(advantages, dtype=tf.float32), variance_epsilon=0.0)\n    self.assertAllClose(expected_advantages,\n                        self.evaluate(normalized_advantages))\n\n  def testRNNTrain(self):\n    actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        input_fc_layer_params=None,\n        output_fc_layer_params=None,\n        lstm_size=(20,))\n    value_net = value_rnn_network.ValueRnnNetwork(\n        self._time_step_spec.observation,\n        input_fc_layer_params=None,\n        output_fc_layer_params=None,\n        lstm_size=(10,))\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        optimizer=tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        num_epochs=1,\n        train_step_counter=global_step,\n    )\n    # Use a random env, policy, and replay buffer to collect training data.\n    random_env = random_tf_environment.RandomTFEnvironment(\n        self._time_step_spec, self._action_spec, batch_size=1)\n    collection_policy = random_tf_policy.RandomTFPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        info_spec=agent.collect_policy.info_spec)\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        collection_policy.trajectory_spec, batch_size=1, max_length=7)\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        random_env,\n        collection_policy,\n        observers=[replay_buffer.add_batch],\n        num_episodes=1)\n\n    # In graph mode: finish building the graph so the optimizer\n    # variables are created.\n    if not tf.executing_eagerly():\n      _, _ = agent.train(experience=replay_buffer.gather_all())\n\n    # Initialize.\n    self.evaluate(agent.initialize())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Train one step.\n    self.assertEqual(0, self.evaluate(global_step))\n    self.evaluate(collect_driver.run())\n    self.evaluate(agent.train(experience=replay_buffer.gather_all()))\n    self.assertEqual(1, self.evaluate(global_step))\n\n  @parameterized.named_parameters([\n      (\'ValueCalculationInTrain\', True),\n      (\'ValueCalculationInCollect\', False),\n  ])\n  def testStatelessValueNetTrain(self, compute_value_and_advantage_in_train):\n    counter = common.create_variable(\'test_train_counter\')\n    actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        self._time_step_spec.observation,\n        self._action_spec,\n        input_fc_layer_params=None,\n        output_fc_layer_params=None,\n        lstm_size=(20,))\n    value_net = value_network.ValueNetwork(\n        self._time_step_spec.observation, fc_layer_params=None)\n    agent = ppo_agent.PPOAgent(\n        self._time_step_spec,\n        self._action_spec,\n        optimizer=tf.compat.v1.train.AdamOptimizer(),\n        actor_net=actor_net,\n        value_net=value_net,\n        num_epochs=1,\n        train_step_counter=counter,\n        compute_value_and_advantage_in_train=compute_value_and_advantage_in_train\n    )\n    observations = tf.constant([\n        [[1, 2], [3, 4], [5, 6]],\n        [[1, 2], [3, 4], [5, 6]],\n    ],\n                               dtype=tf.float32)\n\n    mid_time_step_val = ts.StepType.MID.tolist()\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[mid_time_step_val] * 3] * 2, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]], [[0], [1], [1]]], dtype=tf.float32)\n\n    action_distribution_parameters = {\n        \'loc\': tf.constant([[[0.0]] * 3] * 2, dtype=tf.float32),\n        \'scale\': tf.constant([[[1.0]] * 3] * 2, dtype=tf.float32),\n    }\n    value_preds = tf.constant([[9., 15., 21.], [9., 15., 21.]],\n                              dtype=tf.float32)\n\n    policy_info = {\n        \'dist_params\': action_distribution_parameters,\n    }\n    if not compute_value_and_advantage_in_train:\n      policy_info[\'value_prediction\'] = value_preds\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, policy_info,\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n    if not compute_value_and_advantage_in_train:\n      experience = agent._preprocess(experience)\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n    loss_type = self.evaluate(loss)\n    loss_numpy = loss_type.loss\n    # Assert that loss is not zero as we are training in a non-episodic env.\n    self.assertNotEqual(\n        loss_numpy,\n        0.0,\n        msg=(\'Loss is exactly zero, looks like no training \'\n             \'was performed due to incomplete episodes.\'))\n\n  def testAgentDoesNotFailWhenNestedObservationActionAndDebugSummaries(self):\n    summary_writer = tf.compat.v2.summary.create_file_writer(\n        FLAGS.test_tmpdir, flush_millis=10000)\n    summary_writer.set_as_default()\n\n    nested_obs_spec = (self._obs_spec, self._obs_spec, {\n        \'a\': self._obs_spec,\n        \'b\': self._obs_spec,\n    })\n    nested_time_spec = ts.time_step_spec(nested_obs_spec)\n\n    nested_act_spec = (self._action_spec, {\n        \'c\': self._action_spec,\n        \'d\': self._action_spec\n    })\n\n    class NestedActorNet(network.DistributionNetwork):\n\n      def __init__(self, dummy_model):\n        output_spec = (dummy_model.output_spec, {\n            \'c\': dummy_model.output_spec,\n            \'d\': dummy_model.output_spec,\n        })\n        super(NestedActorNet, self).__init__(\n            dummy_model.input_tensor_spec, (),\n            output_spec=output_spec,\n            name=\'NestedActorNet\')\n        self.dummy_model = dummy_model\n\n      def call(self, inputs, network_state, *args, **kwargs):\n        dummy_ans, _ = self.dummy_model(\n            inputs, network_state=network_state, *args, **kwargs)\n        return (dummy_ans, {\'c\': dummy_ans, \'d\': dummy_ans}), ()\n\n    dummy_model = DummyActorNet(nested_obs_spec, self._action_spec)\n    agent = ppo_agent.PPOAgent(\n        nested_time_spec,\n        nested_act_spec,\n        tf.compat.v1.train.AdamOptimizer(),\n        actor_net=NestedActorNet(dummy_model),\n        value_net=DummyValueNet(nested_obs_spec),\n        compute_value_and_advantage_in_train=False,\n        debug_summaries=True)\n\n    observations = tf.constant([\n        [[1, 2], [3, 4], [5, 6]],\n        [[1, 2], [3, 4], [5, 6]],\n    ],\n                               dtype=tf.float32)\n\n    observations = (observations, observations, {\n        \'a\': observations,\n        \'b\': observations,\n    })\n\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[1] * 3] * 2, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]], [[0], [1], [1]]], dtype=tf.float32)\n\n    actions = (actions, {\n        \'c\': actions,\n        \'d\': actions,\n    })\n\n    action_distribution_parameters = {\n        \'loc\': tf.constant([[[0.0]] * 3] * 2, dtype=tf.float32),\n        \'scale\': tf.constant([[[1.0]] * 3] * 2, dtype=tf.float32),\n    }\n    action_distribution_parameters = (action_distribution_parameters, {\n        \'c\': action_distribution_parameters,\n        \'d\': action_distribution_parameters,\n    })\n\n    value_preds = tf.constant([[9., 15., 21.], [9., 15., 21.]],\n                              dtype=tf.float32)\n    policy_info = {\n        \'dist_params\': action_distribution_parameters,\n        \'value_prediction\': value_preds,\n    }\n\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, policy_info,\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n    experience = agent._preprocess(experience)\n\n    agent.train(experience)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ppo/ppo_clip_agent.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""A PPO Agent implementing the clipped probability ratios.\n\nPlease see details of the algorithm in (Schulman,2017):\nhttps://arxiv.org/abs/1707.06347.\n\nDisclaimer: We intend for this class to eventually fully replicate:\nhttps://github.com/openai/baselines/tree/master/baselines/ppo2\n\nCurrently, this agent surpasses the paper performance for average returns on\nHalf-Cheetah when wider networks and higher learning rates are used. However,\nsome details from this class still differ from the paper implementation.\nFor example, we do not perform mini-batch learning and learning rate annealing\nyet. We are in working progress to reproduce the paper implementation exactly.\n\nPPO is a simplification of the TRPO algorithm, both of which add stability to\npolicy gradient RL, while allowing multiple updates per batch of on-policy data.\n\nTRPO enforces a hard optimization constraint, but is a complex algorithm, which\noften makes it harder to use in practice. PPO approximates the effect of TRPO\nby using a soft constraint. There are two methods presented in the paper for\nimplementing the soft constraint: an adaptive KL loss penalty, and\nlimiting the objective value based on a clipped version of the policy importance\nratio. This agent implements the clipped version.\n\nThe importance ratio clipping is described in eq (7) of\nhttps://arxiv.org/pdf/1707.06347.pdf\n- To disable IR clipping, set the importance_ratio_clipping parameter to 0.0.\n\nNote that the objective function chooses the lower value of the clipped and\nunclipped objectives. Thus, if the importance ratio exceeds the clipped bounds,\nthen the optimizer will still not be incentivized to pass the bounds, as it is\nonly optimizing the minimum.\n\nAdvantage is computed using Generalized Advantage Estimation (GAE):\nhttps://arxiv.org/abs/1506.02438\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf\n\nfrom tf_agents.agents.ppo import ppo_agent\nfrom tf_agents.networks import network\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\n\n@gin.configurable\nclass PPOClipAgent(ppo_agent.PPOAgent):\n  """"""A PPO Agent implementing the clipped probability ratios.""""""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      optimizer: Optional[types.Optimizer] = None,\n      actor_net: Optional[network.Network] = None,\n      value_net: Optional[network.Network] = None,\n      importance_ratio_clipping: types.Float = 0.0,\n      lambda_value: types.Float = 0.95,\n      discount_factor: types.Float = 0.99,\n      entropy_regularization: types.Float = 0.0,\n      policy_l2_reg: types.Float = 0.0,\n      value_function_l2_reg: types.Float = 0.0,\n      shared_vars_l2_reg: types.Float = 0.0,\n      value_pred_loss_coef: types.Float = 0.5,\n      num_epochs: int = 25,\n      use_gae: bool = False,\n      use_td_lambda_return: bool = False,\n      normalize_rewards: bool = True,\n      reward_norm_clipping: types.Float = 10.0,\n      normalize_observations: bool = True,\n      log_prob_clipping: types.Float = 0.0,\n      gradient_clipping: Optional[types.Float] = None,\n      value_clipping: Optional[types.Float] = None,\n      check_numerics: bool = False,\n      # TODO(b/150244758): Change the default to False once we move\n      # clients onto Reverb.\n      compute_value_and_advantage_in_train: bool = True,\n      debug_summaries: bool = False,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = \'PPOClipAgent\'):\n    """"""Creates a PPO Agent implementing the clipped probability ratios.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      optimizer: Optimizer to use for the agent.\n      actor_net: A function actor_net(observations, action_spec) that returns\n        tensor of action distribution params for each observation. Takes nested\n        observation and returns nested action.\n      value_net: A function value_net(time_steps) that returns value tensor from\n        neural net predictions for each observation. Takes nested observation\n        and returns batch of value_preds.\n      importance_ratio_clipping: Epsilon in clipped, surrogate PPO objective.\n        For more detail, see explanation at the top of the doc.\n      lambda_value: Lambda parameter for TD-lambda computation.\n      discount_factor: Discount factor for return computation.\n      entropy_regularization: Coefficient for entropy regularization loss term.\n      policy_l2_reg: Coefficient for l2 regularization of unshared policy\n        weights.\n      value_function_l2_reg: Coefficient for l2 regularization of unshared value\n       function weights.\n      shared_vars_l2_reg: Coefficient for l2 regularization of weights shared\n        between the policy and value functions.\n      value_pred_loss_coef: Multiplier for value prediction loss to balance with\n        policy gradient loss.\n      num_epochs: Number of epochs for computing policy updates.\n      use_gae: If True (default False), uses generalized advantage estimation\n        for computing per-timestep advantage. Else, just subtracts value\n        predictions from empirical return.\n      use_td_lambda_return: If True (default False), uses td_lambda_return for\n        training value function. (td_lambda_return = gae_advantage +\n        value_predictions)\n      normalize_rewards: If true, keeps moving variance of rewards and\n        normalizes incoming rewards.\n      reward_norm_clipping: Value above and below to clip normalized reward.\n      normalize_observations: If true, keeps moving mean and variance of\n        observations and normalizes incoming observations.\n      log_prob_clipping: +/- value for clipping log probs to prevent inf / NaN\n        values.  Default: no clipping.\n      gradient_clipping: Norm length to clip gradients.  Default: no clipping.\n      value_clipping: Difference between new and old value predictions are\n        clipped to this threshold. Value clipping could be helpful when training\n        very deep networks. Default: no clipping.\n      check_numerics: If true, adds tf.debugging.check_numerics to help find NaN\n        / Inf values. For debugging only.\n      compute_value_and_advantage_in_train: A bool to indicate where value\n        prediction and advantage calculation happen.  If True, both happen in\n        agent.train(). If False, value prediction is computed during data\n        collection. This argument must be set to `False` if mini batch learning\n        is enabled.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If true, gradient summaries will be written.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the actor_net is not a DistributionNetwork.\n    """"""\n    super(PPOClipAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        optimizer,\n        actor_net,\n        value_net,\n        importance_ratio_clipping,\n        lambda_value,\n        discount_factor,\n        entropy_regularization,\n        policy_l2_reg,\n        value_function_l2_reg,\n        shared_vars_l2_reg,\n        value_pred_loss_coef,\n        num_epochs,\n        use_gae,\n        use_td_lambda_return,\n        normalize_rewards,\n        reward_norm_clipping,\n        normalize_observations,\n        gradient_clipping=gradient_clipping,\n        value_clipping=value_clipping,\n        check_numerics=check_numerics,\n        compute_value_and_advantage_in_train=compute_value_and_advantage_in_train,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter,\n        name=name,\n        # Skips parameters used for the adaptive KL loss penalty version of PPO.\n        log_prob_clipping=0.0,\n        kl_cutoff_factor=0.0,\n        kl_cutoff_coef=0.0,\n        initial_adaptive_kl_beta=0.0,\n        adaptive_kl_target=0.0,\n        adaptive_kl_tolerance=0.0)\n'"
tf_agents/agents/ppo/ppo_kl_penalty_agent.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""A PPO Agent implementing the KL penalty loss.\n\nPlease see details of the algorithm in (Schulman,2017):\nhttps://arxiv.org/abs/1707.06347.\n\nDisclaimer: We intend for this class to eventually fully replicate the KL\npenalty version of PPO from:\nhttps://github.com/openai/baselines/tree/master/baselines/ppo1\nWe are still working on resolving the differences in implementation details,\nsuch as mini batch learning and learning rate annealing.\n\nPPO is a simplification of the TRPO algorithm, both of which add stability to\npolicy gradient RL, while allowing multiple updates per batch of on-policy data.\n\nTRPO enforces a hard optimization constraint, but is a complex algorithm, which\noften makes it harder to use in practice. PPO approximates the effect of TRPO\nby using a soft constraint. There are two methods presented in the paper for\nimplementing the soft constraint: an adaptive KL loss penalty, and\nlimiting the objective value based on a clipped version of the policy importance\nratio. This agent implements the KL penalty version.\n\nNote that PPOKLPenaltyAgent is known to have worse performance than PPOClipAgent\n(Schulman,2017). We included the implementation as it is an important baseline.\n\nNote that PPOKLPenaltyAgent\'s behavior can be reproduced by the parent\n""PPOAgent"" if the right set of parameters are set. However, we strongly\nencourage future clients to use PPOKLPenaltyAgent instead if you rely on the KL\npenalty version of PPO, because PPOKLPenaltyAgent abstracts away the\nparameters unrelated to this particular PPO version, making it less error prone.\n\nAdvantage is computed using Generalized Advantage Estimation (GAE):\nhttps://arxiv.org/abs/1506.02438\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf\n\nfrom tf_agents.agents.ppo import ppo_agent\nfrom tf_agents.networks import network\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\n\n@gin.configurable\nclass PPOKLPenaltyAgent(ppo_agent.PPOAgent):\n  """"""A PPO Agent implementing the KL penalty loss.""""""\n\n  def __init__(\n      self,\n      time_step_spec: ts.TimeStep,\n      action_spec: types.NestedTensorSpec,\n      actor_net: network.Network,\n      value_net: network.Network,\n      num_epochs: int,\n      initial_adaptive_kl_beta: types.Float,\n      adaptive_kl_target: types.Float,\n      adaptive_kl_tolerance: types.Float,\n      optimizer: Optional[types.Optimizer] = None,\n      use_gae: bool = True,\n      use_td_lambda_return: bool = True,\n      lambda_value: types.Float = 0.95,\n      discount_factor: types.Float = 0.99,\n      value_pred_loss_coef: types.Float = 0.5,\n      entropy_regularization: types.Float = 0.0,\n      policy_l2_reg: types.Float = 0.0,\n      value_function_l2_reg: types.Float = 0.0,\n      shared_vars_l2_reg: types.Float = 0.0,\n      normalize_observations: bool = False,\n      normalize_rewards: bool = True,\n      reward_norm_clipping: types.Float = 0.0,\n      log_prob_clipping: types.Float = 0.0,\n      gradient_clipping: Optional[types.Float] = None,\n      value_clipping: Optional[types.Float] = None,\n      kl_cutoff_coef: types.Float = 0.0,\n      kl_cutoff_factor: Optional[types.Float] = None,\n      check_numerics: bool = False,\n      debug_summaries: bool = False,\n      # TODO(b/150244758): Change the default to False once we move\n      # clients onto Reverb.\n      compute_value_and_advantage_in_train: bool = True,\n      summarize_grads_and_vars: bool = False,\n      train_step_counter: Optional[tf.Variable] = None,\n      name: Optional[Text] = None):\n    """"""Creates a PPO Agent implementing the KL penalty loss.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      actor_net: A `network.DistributionNetwork` which maps observations to\n        action distributions. Commonly, it is set to\n        `actor_distribution_network.ActorDistributionNetwork`.\n      value_net: A `Network` which returns the value prediction for input\n        states, with `call(observation, step_type, network_state)`. Commonly, it\n        is set to `value_network.ValueNetwork`.\n      num_epochs: Number of epochs for computing policy updates. (Schulman,2017)\n        sets this to 10 for Mujoco, 15 for Roboschool and 3 for Atari.\n      initial_adaptive_kl_beta: Initial value for beta coefficient of adaptive\n        KL penalty. This initial value is not important in practice because the\n        algorithm quickly adjusts to it. A common default is 1.0.\n      adaptive_kl_target: Desired KL target for policy updates. If actual KL is\n        far from this target, adaptive_kl_beta will be updated. You should tune\n        this for your environment. 0.01 was found to perform well for Mujoco.\n      adaptive_kl_tolerance: A tolerance for adaptive_kl_beta. Mean KL above\n        `(1 + tol) * adaptive_kl_target`, or below\n        `(1 - tol) * adaptive_kl_target`,\n        will cause `adaptive_kl_beta` to be updated. `0.5` was chosen\n        heuristically in the paper, but the algorithm is not very\n        sensitive to it.\n      optimizer: Optimizer to use for the agent, default to using\n        `tf.compat.v1.train.AdamOptimizer`.\n      use_gae: If `True`, uses generalized advantage estimation for computing\n        per-timestep advantage. Else, just subtracts value predictions from\n        empirical return.\n      use_td_lambda_return: If `True`, uses `td_lambda_return` for training\n        value function; here:\n        `td_lambda_return = gae_advantage + value_predictions`.\n        `use_gae` must be set to `True` as well to enable TD -lambda returns. If\n        `use_td_lambda_return` is set to True while `use_gae` is False, the\n        empirical return will be used and a warning will be logged.\n      lambda_value: Lambda parameter for TD-lambda computation. Default to\n       `0.95` which is the value used for all environments from the paper.\n      discount_factor: Discount factor for return computation. Default to `0.99`\n        which is the value used for all environments from the paper.\n      value_pred_loss_coef: Multiplier for value prediction loss to balance with\n        policy gradient loss. Default to `0.5`, which was used for all\n        environments in the OpenAI baseline implementation. This parameters is\n        irrelevant unless you are sharing part of actor_net and value_net. In\n        that case, you would want to tune this coeeficient, whose value depends\n        on the network architecture of your choice\n      entropy_regularization: Coefficient for entropy regularization loss term.\n        Default to `0.0` because no entropy bonus was applied in the PPO paper.\n      policy_l2_reg: Coefficient for L2 regularization of unshared actor_net\n        weights. Default to `0.0` because no L2 regularization was applied on\n        the policy network weights in the PPO paper.\n      value_function_l2_reg: Coefficient for l2 regularization of unshared value\n       function weights. Default to `0.0` because no L2 regularization was\n       applied on the policy network weights in the PPO paper.\n      shared_vars_l2_reg: Coefficient for l2 regularization of weights shared\n        between actor_net and value_net. Default to `0.0` because no L2\n        regularization was applied on either network in the PPO paper.\n      normalize_observations: If `True` (default `False`), keeps moving mean and\n        variance of observations and normalizes incoming observations.\n        Additional optimization proposed in (Ilyas et al., 2018).\n      normalize_rewards: If `True`, keeps moving variance of rewards and\n        normalizes incoming rewards. While not mentioned directly in the PPO\n        paper, reward normalization was implemented in OpenAI baselines and\n        (Ilyas et al., 2018) pointed out that it largely improves performance.\n        You may refer to Figure 1 of https://arxiv.org/pdf/1811.02553.pdf for a\n          comparison with and without reward scaling.\n      reward_norm_clipping: Value above and below to clip normalized reward.\n        Additional optimization proposed in (Ilyas et al., 2018) set to\n        `5` or `10`.\n      log_prob_clipping: +/- value for clipping log probs to prevent inf / NaN\n        values.  Default: no clipping.\n      gradient_clipping: Norm length to clip gradients.  Default: no clipping.\n      value_clipping: Difference between new and old value predictions are\n        clipped to this threshold. Value clipping could be helpful when training\n        very deep networks. Default: no clipping.\n      kl_cutoff_coef: kl_cutoff_coef and kl_cutoff_factor are additional params\n        if one wants to use a KL cutoff loss term in addition to the adaptive KL\n        loss term. Default to 0.0 to disable the KL cutoff loss term as this was\n        not used in the paper.  kl_cutoff_coef is the coefficient to mulitply by\n        the KL cutoff loss term, before adding to the total loss function.\n      kl_cutoff_factor: Only meaningful when `kl_cutoff_coef > 0.0`. A multipler\n        used for calculating the KL cutoff ( =\n        `kl_cutoff_factor * adaptive_kl_target`). If policy KL averaged across\n        the batch changes more than the cutoff, a squared cutoff loss would\n        be added to the loss function.\n      check_numerics: If true, adds `tf.debugging.check_numerics` to help find\n        NaN / Inf values. For debugging only.\n      debug_summaries: A bool to gather debug summaries.\n      compute_value_and_advantage_in_train: A bool to indicate where value\n        prediction and advantage calculation happen.  If True, both happen in\n        agent.train(). If False, value prediction is computed during data\n        collection. This argument must be set to `False` if mini batch learning\n        is enabled.\n      summarize_grads_and_vars: If true, gradient summaries will be written.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the actor_net is not a DistributionNetwork or value_net is\n        not a Network.\n      ValueError: If kl_cutoff_coef > 0.0 (indicating that a KL cutoff loss term\n        will not be added), but kl_cutoff_factor is None.\n    """"""\n    if kl_cutoff_coef > 0.0 and kl_cutoff_factor is None:\n      raise ValueError(\n          \'kl_cutoff_factor needs to be set if kl_cutoff_coef is non-zero.\')\n\n    super(PPOKLPenaltyAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        optimizer=optimizer,\n        actor_net=actor_net,\n        value_net=value_net,\n        lambda_value=lambda_value,\n        discount_factor=discount_factor,\n        entropy_regularization=entropy_regularization,\n        policy_l2_reg=policy_l2_reg,\n        value_function_l2_reg=value_function_l2_reg,\n        shared_vars_l2_reg=shared_vars_l2_reg,\n        value_pred_loss_coef=value_pred_loss_coef,\n        num_epochs=num_epochs,\n        use_gae=use_gae,\n        use_td_lambda_return=use_td_lambda_return,\n        normalize_rewards=normalize_rewards,\n        reward_norm_clipping=reward_norm_clipping,\n        normalize_observations=normalize_observations,\n        log_prob_clipping=log_prob_clipping,\n        kl_cutoff_factor=kl_cutoff_factor,\n        kl_cutoff_coef=kl_cutoff_coef,\n        initial_adaptive_kl_beta=initial_adaptive_kl_beta,\n        adaptive_kl_target=adaptive_kl_target,\n        adaptive_kl_tolerance=adaptive_kl_tolerance,\n        gradient_clipping=gradient_clipping,\n        value_clipping=value_clipping,\n        check_numerics=check_numerics,\n        debug_summaries=debug_summaries,\n        compute_value_and_advantage_in_train=compute_value_and_advantage_in_train,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter,\n        name=name,\n        # Skips parameters specific to PPOClipAgent.\n        importance_ratio_clipping=0.0,\n    )\n'"
tf_agents/agents/ppo/ppo_policy.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An ActorPolicy that also returns policy_info needed for PPO training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.ppo import ppo_utils\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import tensor_normalizer\n\n\ntfd = tfp.distributions\n\n\n@gin.configurable(module=\'tf_agents\')\nclass PPOPolicy(actor_policy.ActorPolicy):\n  """"""An ActorPolicy that also returns policy_info needed for PPO training.\n\n  This policy requires two networks: the usual `actor_network` and the\n  additional `value_network`. The value network can be executed with the\n  `apply_value_network()` method.\n\n  When the networks have state (RNNs, LSTMs) you must be careful to pass the\n  state for the actor network to `action()` and the state of the value network\n  to `apply_value_network()`. Use `get_initial_value_state()` to access\n  the state of the value network.\n  """"""\n\n  def __init__(self,\n               time_step_spec: Optional[ts.TimeStep] = None,\n               action_spec: Optional[types.NestedTensorSpec] = None,\n               actor_network: Optional[network.Network] = None,\n               value_network: Optional[network.Network] = None,\n               observation_normalizer: Optional[\n                   tensor_normalizer.TensorNormalizer] = None,\n               clip: bool = True,\n               collect: bool = True,\n               compute_value_and_advantage_in_train: bool = False):\n    """"""Builds a PPO Policy given network Templates or functions.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      actor_network: An instance of a tf_agents.networks.network.Network, with\n        call(observation, step_type, network_state).  Network should\n        return one of the following: 1. a nested tuple of tfp.distributions\n          objects matching action_spec, or 2. a nested tuple of tf.Tensors\n          representing actions.\n      value_network:  An instance of a tf_agents.networks.network.Network, with\n        call(observation, step_type, network_state).  Network should return\n        value predictions for the input state.\n      observation_normalizer: An object to use for obervation normalization.\n      clip: Whether to clip actions to spec before returning them.  Default\n        True. Most policy-based algorithms (PCL, PPO, REINFORCE) use unclipped\n        continuous actions for training.\n      collect: If True, creates ops for actions_log_prob, value_preds, and\n        action_distribution_params. (default True)\n      compute_value_and_advantage_in_train: A bool to indicate where value\n        prediction and advantage calculation happen.  If True, both happen in\n        agent.train(), therefore no need to save the value prediction inside of\n        policy info. If False, value prediction is computed during data\n        collection. This argument must be set to `False` if mini batch learning\n        is enabled.\n\n    Raises:\n      ValueError: if actor_network or value_network is not of type\n        tf_agents.networks.network.Network.\n    """"""\n    if not isinstance(actor_network, network.Network):\n      raise ValueError(\'actor_network is not of type network.Network\')\n    if not isinstance(value_network, network.Network):\n      raise ValueError(\'value_network is not of type network.Network\')\n\n    self._compute_value_and_advantage_in_train = compute_value_and_advantage_in_train\n\n    if collect:\n      # TODO(oars): Cleanup how we handle non distribution networks.\n      if isinstance(actor_network, network.DistributionNetwork):\n        network_output_spec = actor_network.output_spec\n      else:\n        network_output_spec = tf.nest.map_structure(\n            distribution_spec.deterministic_distribution_from_spec, action_spec)\n      info_spec = {\n          \'dist_params\':\n              tf.nest.map_structure(lambda spec: spec.input_params_spec,\n                                    network_output_spec)\n      }\n\n      if not self._compute_value_and_advantage_in_train:\n        info_spec[\'value_prediction\'] = tensor_spec.TensorSpec(\n            shape=[], dtype=tf.float32)\n    else:\n      info_spec = ()\n\n    policy_state_spec = {}\n    if actor_network.state_spec:\n      policy_state_spec[\'actor_network_state\'] = actor_network.state_spec\n    if (collect and value_network.state_spec and\n        not self._compute_value_and_advantage_in_train):\n      policy_state_spec[\'value_network_state\'] = value_network.state_spec\n    if not policy_state_spec:\n      policy_state_spec = ()\n\n    super(PPOPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy_state_spec=policy_state_spec,\n        info_spec=info_spec,\n        actor_network=actor_network,\n        observation_normalizer=observation_normalizer,\n        clip=clip)\n\n    self._collect = collect\n    if value_network is not None:\n      value_network.create_variables()\n    self._value_network = value_network\n\n  def get_initial_value_state(self,\n                              batch_size: types.Int) -> types.NestedTensor:\n    """"""Returns the initial state of the value network.\n\n    Args:\n      batch_size: A constant or Tensor holding the batch size. Can be None, in\n        which case the state will not have a batch dimension added.\n\n    Returns:\n      A nest of zero tensors matching the spec of the value network state.\n    """"""\n    return tensor_spec.zero_spec_nest(\n        self._value_network.state_spec,\n        outer_dims=None if batch_size is None else [batch_size])\n\n  def apply_value_network(self,\n                          observations: types.NestedTensor,\n                          step_types: types.Tensor,\n                          value_state: Optional[types.NestedTensor] = None,\n                          training: bool = False) -> types.NestedTensor:\n    """"""Apply value network to time_step, potentially a sequence.\n\n    If observation_normalizer is not None, applies observation normalization.\n\n    Args:\n      observations: A (possibly nested) observation tensor with outer_dims\n        either (batch_size,) or (batch_size, time_index). If observations is a\n        time series and network is RNN, will run RNN steps over time series.\n      step_types: A (possibly nested) step_types tensor with same outer_dims as\n        observations.\n      value_state: Optional. Initial state for the value_network. If not\n        provided the behavior depends on the value network itself.\n      training: Whether the output value is going to be used for training.\n\n    Returns:\n      The output of value_net, which is a tuple of:\n        - value_preds with same outer_dims as time_step\n        - value_state at the end of the time series\n    """"""\n    if self._observation_normalizer:\n      observations = self._observation_normalizer.normalize(observations)\n    return self._value_network(observations, step_types, value_state,\n                               training=training)\n\n  def _apply_actor_network(self, time_step, policy_state, training=False):\n    observation = time_step.observation\n    if self._observation_normalizer:\n      observation = self._observation_normalizer.normalize(observation)\n\n    return self._actor_network(\n        observation,\n        time_step.step_type,\n        network_state=policy_state,\n        training=training)\n\n  def _variables(self):\n    var_list = self._actor_network.variables[:]\n    var_list += self._value_network.variables[:]\n    if self._observation_normalizer:\n      var_list += self._observation_normalizer.variables\n    return var_list\n\n  def _distribution(self, time_step, policy_state, training=False):\n    if not policy_state:\n      policy_state = {\'actor_network_state\': (), \'value_network_state\': ()}\n    else:\n      policy_state = policy_state.copy()\n\n    if \'actor_network_state\' not in policy_state:\n      policy_state[\'actor_network_state\'] = ()\n    if \'value_network_state\' not in policy_state:\n      policy_state[\'value_network_state\'] = ()\n\n    new_policy_state = {\'actor_network_state\': (), \'value_network_state\': ()}\n\n    def _to_distribution(action_or_distribution):\n      if isinstance(action_or_distribution, tf.Tensor):\n        # This is an action tensor, so wrap it in a deterministic distribution.\n        return tfp.distributions.Deterministic(loc=action_or_distribution)\n      return action_or_distribution\n\n    (actions_or_distributions,\n     new_policy_state[\'actor_network_state\']) = self._apply_actor_network(\n         time_step, policy_state[\'actor_network_state\'], training=training)\n    distributions = tf.nest.map_structure(_to_distribution,\n                                          actions_or_distributions)\n\n    if self._collect:\n      policy_info = {\n          \'dist_params\': ppo_utils.get_distribution_params(distributions)\n      }\n      if not self._compute_value_and_advantage_in_train:\n        # If value_prediction is not computed in agent.train it needs to be\n        # computed and saved here.\n        (policy_info[\'value_prediction\'],\n         new_policy_state[\'value_network_state\']) = self.apply_value_network(\n             time_step.observation,\n             time_step.step_type,\n             value_state=policy_state[\'value_network_state\'],\n             training=False)\n    else:\n      policy_info = ()\n\n    if (not new_policy_state[\'actor_network_state\'] and\n        not new_policy_state[\'value_network_state\']):\n      new_policy_state = ()\n    elif not new_policy_state[\'value_network_state\']:\n      new_policy_state.pop(\'value_network_state\', None)\n    elif not new_policy_state[\'actor_network_state\']:\n      new_policy_state.pop(\'actor_network_state\', None)\n\n    return policy_step.PolicyStep(distributions, new_policy_state, policy_info)\n'"
tf_agents/agents/ppo/ppo_policy_test.py,44,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.policies.actor_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.ppo import ppo_policy\nfrom tf_agents.networks import network\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActorNet(network.Network):\n\n  def __init__(self, action_spec, name=None):\n    super(DummyActorNet, self).__init__(\n        tensor_spec.TensorSpec([2], tf.float32), (), \'DummyActorNet\')\n    self._action_spec = action_spec\n    self._flat_action_spec = tf.nest.flatten(self._action_spec)[0]\n\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            self._flat_action_spec.shape.num_elements(),\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]),\n            activation=tf.keras.activations.tanh,\n        )\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n    for layer in self._dummy_layers:\n      hidden_state = layer(hidden_state)\n\n    means = tf.reshape(hidden_state,\n                       [-1] + self._flat_action_spec.shape.as_list())\n    spec_means = (\n        self._flat_action_spec.maximum + self._flat_action_spec.minimum) / 2.0\n    spec_ranges = (\n        self._flat_action_spec.maximum - self._flat_action_spec.minimum) / 2.0\n    action_means = spec_means + spec_ranges * means\n\n    return tf.nest.pack_sequence_as(self._action_spec,\n                                    [action_means]), network_state\n\n\nclass DummyActorDistributionNet(network.DistributionNetwork):\n\n  def __init__(self, action_spec, name=None):\n    output_spec = tf.nest.map_structure(self._get_normal_distribution_spec,\n                                        action_spec)\n    super(DummyActorDistributionNet, self).__init__(\n        tensor_spec.TensorSpec([2], tf.float32),\n        (),\n        output_spec=output_spec,\n        name=\'DummyActorDistributionNet\')\n    self._action_net = DummyActorNet(action_spec)\n\n  def _get_normal_distribution_spec(self, sample_spec):\n    input_param_shapes = tfp.distributions.Normal.param_static_shapes(\n        sample_spec.shape)\n    input_param_spec = tf.nest.map_structure(\n        lambda tensor_shape: tensor_spec.TensorSpec(  # pylint: disable=g-long-lambda\n            shape=tensor_shape,\n            dtype=sample_spec.dtype),\n        input_param_shapes)\n\n    return distribution_spec.DistributionSpec(\n        tfp.distributions.Normal, input_param_spec, sample_spec=sample_spec)\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    action_means, network_state = self._action_net(inputs, network_state)\n\n    def _action_distribution(action_mean):\n      action_std = tf.ones_like(action_mean)\n      return tfp.distributions.Normal(action_mean, action_std)\n\n    return tf.nest.map_structure(_action_distribution,\n                                 action_means), network_state\n\n\nclass DummyValueNet(network.Network):\n\n  def __init__(self, name=None):\n    super(DummyValueNet, self).__init__(\n        tensor_spec.TensorSpec([2], tf.float32), (), \'DummyValueNet\')\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            1,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n    for layer in self._dummy_layers:\n      hidden_state = layer(hidden_state)\n    return hidden_state, network_state\n\n\ndef _test_cases(prefix=\'\'):\n  return [{\n      \'testcase_name\': \'%s0\' % prefix,\n      \'network_cls\': DummyActorNet,\n  }, {\n      \'testcase_name\': \'%s1\' % prefix,\n      \'network_cls\': DummyActorDistributionNet,\n  }]\n\n\nclass PPOPolicyTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(PPOPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, 2, 3)\n\n  @property\n  def _time_step(self):\n    return ts.TimeStep(\n        step_type=tf.constant([1], dtype=tf.int32),\n        reward=tf.constant([1], dtype=tf.float32),\n        discount=tf.constant([1], dtype=tf.float32),\n        observation=tf.constant([[1, 2]], dtype=tf.float32))\n\n  @property\n  def _time_step_batch(self):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[2], name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\'observation\'))\n\n  @parameterized.named_parameters(*_test_cases(\'test_build\'))\n  def testBuild(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n  @parameterized.named_parameters(*_test_cases(\'test_reset\'))\n  def testReset(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    policy_state = policy.get_initial_state(batch_size=1)\n\n    # Dummy network has no policy_state so expect empty tuple from reset.\n    self.assertEqual((), policy_state)\n\n  @parameterized.named_parameters(*_test_cases(\'test_action\'))\n  def testAction(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    action_step = policy.action(self._time_step)\n    self.assertEqual(action_step.action.shape.as_list(), [1, 1])\n    self.assertEqual(action_step.action.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  @parameterized.named_parameters(*_test_cases(\'test_action\'))\n  def testValueInPolicyInfo(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    policy_step = policy.action(self._time_step)\n    self.assertEqual(policy_step.info[\'value_prediction\'].shape.as_list(),\n                     [1, 1])\n    self.assertEqual(policy_step.info[\'value_prediction\'].dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(policy_step.info[\'value_prediction\'])\n\n  @parameterized.named_parameters(*_test_cases(\'test_action_list\'))\n  def testActionList(self, network_cls):\n    action_spec = [self._action_spec]\n    actor_network = network_cls(action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    action_step = policy.action(self._time_step)\n    self.assertIsInstance(action_step.action, list)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= action_spec[0].minimum))\n    self.assertTrue(np.all(actions_ <= action_spec[0].maximum))\n\n  @parameterized.named_parameters(*_test_cases(\'test_action_batch\'))\n  def testActionBatch(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    action_step = policy.action(self._time_step_batch)\n    self.assertEqual(action_step.action.shape.as_list(), [2, 1])\n    self.assertEqual(action_step.action.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    self.assertTrue(np.all(actions_ >= self._action_spec.minimum))\n    self.assertTrue(np.all(actions_ <= self._action_spec.maximum))\n\n  @parameterized.named_parameters(*_test_cases(\'test_action\'))\n  def testValue(self, network_cls):\n    actor_network = network_cls(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    batch_size = tf.compat.dimension_value(self._time_step.step_type.shape[0])\n    policy_state = policy.get_initial_state(batch_size=batch_size)\n    value_pred, unused_policy_state = policy.apply_value_network(\n        self._time_step.observation, self._time_step.step_type, policy_state)\n    self.assertEqual(value_pred.shape.as_list(), [1, 1])\n    self.assertEqual(value_pred.dtype, tf.float32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(value_pred)\n\n  def testUpdate(self):\n    tf.compat.v1.set_random_seed(1)\n    actor_network = DummyActorNet(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n    new_policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    action_step = policy.action(self._time_step)\n    new_action_step = new_policy.action(self._time_step)\n\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(new_policy.update(policy))\n    actions_, new_actions_ = self.evaluate(\n        [action_step.action, new_action_step.action])\n    self.assertAllEqual(actions_, new_actions_)\n\n  def testDeterministicDistribution(self):\n    actor_network = DummyActorNet(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    action_step = policy.action(self._time_step)\n    distribution_step = policy.distribution(self._time_step)\n    self.assertIsInstance(distribution_step.action,\n                          tfp.distributions.Deterministic)\n    distribution_mean = distribution_step.action.mean()\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions_ = self.evaluate(action_step.action)\n    distribution_mean_ = self.evaluate(distribution_mean)\n    self.assertNear(actions_, distribution_mean_, 1e-6)\n\n  def testGaussianDistribution(self):\n    actor_network = DummyActorDistributionNet(self._action_spec)\n    value_network = DummyValueNet()\n\n    policy = ppo_policy.PPOPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_network,\n        value_network=value_network)\n\n    distribution_step = policy.distribution(self._time_step)\n    self.assertIsInstance(distribution_step.action, tfp.distributions.Normal)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/ppo/ppo_utils.py,16,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utils functions for ppo_agent.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Sequence\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\n\n\ndef make_trajectory_mask(batched_traj: trajectory.Trajectory) -> types.Tensor:\n  """"""Mask boundary trajectories and those with invalid returns and advantages.\n\n  Args:\n    batched_traj: Trajectory, doubly-batched [batch_dim, time_dim,...]. It must\n      be preprocessed already.\n\n  Returns:\n    A mask, type tf.float32, that is 0.0 for all between-episode Trajectory\n      (batched_traj.step_type is LAST) and 0.0 if the return value is\n      unavailable.\n  """"""\n  # 1.0 for all valid trajectories. 0.0 where between episodes.\n  not_between_episodes = ~batched_traj.is_boundary()\n\n  # 1.0 for trajectories with valid return values. 0.0 where return and\n  # advantage are both 0. This happens to the last item when the experience gets\n  # preprocessed, as insufficient information was available for calculating\n  # advantages.\n  valid_return_value = ~(\n      tf.equal(batched_traj.policy_info[\'return\'], 0)\n      & tf.equal(batched_traj.policy_info[\'normalized_advantage\'], 0))\n\n  return tf.cast(not_between_episodes & valid_return_value, tf.float32)\n\n\ndef make_timestep_mask(batched_next_time_step: ts.TimeStep,\n                       allow_partial_episodes: bool = False) -> types.Tensor:\n  """"""Create a mask for transitions and optionally final incomplete episodes.\n\n  Args:\n    batched_next_time_step: Next timestep, doubly-batched [batch_dim, time_dim,\n      ...].\n    allow_partial_episodes: If true, then steps on incomplete episodes are\n      allowed.\n\n  Returns:\n    A mask, type tf.float32, that is 0.0 for all between-episode timesteps\n      (batched_next_time_step is FIRST). If allow_partial_episodes is set to\n      False, the mask has 0.0 for incomplete episode at the end of the sequence.\n  """"""\n  if allow_partial_episodes:\n    episode_is_complete = None\n  else:\n    # 1.0 for timesteps of all complete episodes. 0.0 for incomplete episode at\n    #   the end of the sequence.\n    episode_is_complete = tf.cumsum(\n        tf.cast(batched_next_time_step.is_last(), tf.float32),\n        axis=1,\n        reverse=True) > 0\n\n  # 1.0 for all valid timesteps. 0.0 where between episodes.\n  not_between_episodes = ~batched_next_time_step.is_first()\n\n  if allow_partial_episodes:\n    return tf.cast(not_between_episodes, tf.float32)\n  else:\n    return tf.cast(episode_is_complete & not_between_episodes, tf.float32)\n\n\ndef get_distribution_params(\n    nested_distribution: types.NestedDistribution) -> types.NestedTensor:\n  """"""Get the params for an optionally nested action distribution.\n\n  Only returns parameters that have tf.Tensor values.\n\n  Args:\n    nested_distribution: The nest of distributions whose parameter tensors to\n      extract.\n  Returns:\n    A nest of distribution parameters. Each leaf is a dict corresponding to one\n      distribution, with keys as parameter name and values as tensors containing\n      parameter values.\n  """"""\n  def _tensor_parameters_only(params):\n    return {k: params[k] for k in params if isinstance(params[k], tf.Tensor)}\n\n  return tf.nest.map_structure(\n      lambda single_dist: _tensor_parameters_only(single_dist.parameters),\n      nested_distribution)\n\n\ndef nested_kl_divergence(nested_from_distribution: types.NestedDistribution,\n                         nested_to_distribution: types.NestedDistribution,\n                         outer_dims: Sequence[int] = ()) -> types.Tensor:\n  """"""Given two nested distributions, sum the KL divergences of the leaves.""""""\n  nest_utils.assert_same_structure(nested_from_distribution,\n                                   nested_to_distribution)\n\n  # Make list pairs of leaf distributions.\n  flat_from_distribution = tf.nest.flatten(nested_from_distribution)\n  flat_to_distribution = tf.nest.flatten(nested_to_distribution)\n  all_kl_divergences = [from_dist.kl_divergence(to_dist)\n                        for from_dist, to_dist\n                        in zip(flat_from_distribution, flat_to_distribution)]\n\n  # Sum the kl of the leaves.\n  summed_kl_divergences = tf.add_n(all_kl_divergences)\n\n  # Reduce_sum over non-batch dimensions.\n  reduce_dims = list(range(len(summed_kl_divergences.shape)))\n  for dim in outer_dims:\n    reduce_dims.remove(dim)\n  total_kl = tf.reduce_sum(input_tensor=summed_kl_divergences, axis=reduce_dims)\n\n  return total_kl\n\n\ndef get_metric_observers(metrics):\n  """"""Returns a list of observers, one for each metric.""""""\n  def get_metric_observer(metric):\n\n    def metric_observer(time_step, action, next_time_step,\n                        policy_state):\n      action_step = policy_step.PolicyStep(action, policy_state, ())\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n      return metric(traj)\n    return metric_observer\n  return [get_metric_observer(m) for m in metrics]\n'"
tf_agents/agents/ppo/ppo_utils_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for TF Agents ppo_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.ppo import ppo_utils\nfrom tf_agents.trajectories import time_step as ts\n\n\nclass PPOUtilsTest(parameterized.TestCase, tf.test.TestCase):\n\n  @parameterized.named_parameters(\n      (\'OnNotAllowPartialReturnsZerosOnIncompleteSteps\', False),\n      (\'OnAllowPartialReturnsOnesOnIncompleteStepsAndZerosBetween\', True))\n  def testMakeTimestepMaskWithPartialEpisode(self, allow_partial):\n    first, mid, last = ts.StepType.FIRST, ts.StepType.MID, ts.StepType.LAST\n\n    next_step_types = tf.constant([[mid, mid, last, first,\n                                    mid, mid, last, first,\n                                    mid, mid],\n                                   [mid, mid, last, first,\n                                    mid, mid, mid, mid,\n                                    mid, last]])\n    zeros = tf.zeros_like(next_step_types)\n    next_time_step = ts.TimeStep(next_step_types, zeros, zeros, zeros)\n\n    if not allow_partial:\n      # Mask should be 0.0 for transition timesteps (3, 7) and for all timesteps\n      #   belonging to the final, incomplete episode.\n      expected_mask = [[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n                       [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n    else:\n      # Zeros only between episodes. Incomplete episodes are valid and not\n      # zeroed out.\n      expected_mask = [[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0],\n                       [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n    timestep_mask = ppo_utils.make_timestep_mask(\n        next_time_step, allow_partial_episodes=allow_partial)\n\n    timestep_mask_ = self.evaluate(timestep_mask)\n    self.assertAllClose(expected_mask, timestep_mask_)\n\n  def test_nested_kl_divergence(self):\n    zero = tf.constant([0.0] * 3, dtype=tf.float32)\n    one = tf.constant([1.0] * 3, dtype=tf.float32)\n    dist_neg_one = tfp.distributions.Normal(loc=-one, scale=one)\n    dist_zero = tfp.distributions.Normal(loc=zero, scale=one)\n    dist_one = tfp.distributions.Normal(loc=one, scale=one)\n\n    nested_dist1 = [dist_zero, [dist_neg_one, dist_one]]\n    nested_dist2 = [dist_one, [dist_one, dist_zero]]\n    kl_divergence = ppo_utils.nested_kl_divergence(\n        nested_dist1, nested_dist2)\n    expected_kl_divergence = 3 * 3.0  # 3 * (0.5 + (2.0 + 0.5))\n\n    kl_divergence_ = self.evaluate(kl_divergence)\n    self.assertAllClose(expected_kl_divergence, kl_divergence_)\n\n  def test_get_distribution_params(self):\n    ones = tf.ones(shape=[2], dtype=tf.float32)\n    distribution = (tfp.distributions.Categorical(logits=ones),\n                    tfp.distributions.Normal(ones, ones))\n    params = ppo_utils.get_distribution_params(distribution)\n    self.assertAllEqual([set([\'logits\']), set([\'loc\', \'scale\'])],\n                        [set(d.keys()) for d in params])\n    self.assertAllEqual([[[2]], [[2], [2]]],\n                        [[d[k].shape.as_list() for k in d] for d in params])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/random/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A random Agent.""""""\nfrom tf_agents.agents.random import fixed_policy_agent\nfrom tf_agents.agents.random import random_agent\n\n'"
tf_agents/agents/random/fixed_policy_agent.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An agent with a fixed policy.\n\nAn agent following one specific policy, without training.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Callable, Union, Optional, Text\n\nimport gin\nimport tensorflow.compat.v2 as tf\nimport tf_agents.agents.tf_agent as tf_agent\nfrom tf_agents.policies import py_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\n\nPolicyClassType = Callable[..., Union[tf_policy.TFPolicy, py_policy.PyPolicy]]\n\n\n@gin.configurable\nclass FixedPolicyAgent(tf_agent.TFAgent):\n  """"""An agent with a fixed policy and no learning.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               policy_class: PolicyClassType,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               train_step_counter: Optional[tf.Variable] = None,\n               num_outer_dims: int = 1,\n               name: Optional[Text] = None):\n    """"""Creates a fixed-policy agent with no-op for training.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      policy_class: a tf_policy.TFPolicy or py_policy.PyPolicy class to use as a\n        policy.\n      debug_summaries: A bool to gather debug summaries. Used to initialize the\n        base class\n      summarize_grads_and_vars: If true, gradient summaries will be written.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step. Used to initialize the\n        base class\n      num_outer_dims: Used to initialize the base class\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name. Used to initialize the\n        base class.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    policy = policy_class(time_step_spec=time_step_spec,\n                          action_spec=action_spec)\n\n    collect_policy = policy\n\n    super(FixedPolicyAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=None,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter,\n        num_outer_dims=num_outer_dims)\n\n  def _initialize(self):\n    pass\n\n  def _train(self, experience, weights):\n    """"""Do nothing. Arguments are ignored and loss is always 0.""""""\n    del experience  # Unused\n    del weights  # Unused\n\n    # Incrementing the step counter.\n    self.train_step_counter.assign_add(1)\n\n    # Returning 0 loss.\n    return tf_agent.LossInfo(0.0, None)\n'"
tf_agents/agents/random/random_agent.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A random agent.\n\nAn agent implementing a random policy without training. Useful as a baseline\nwhen comparing to other agents.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow.compat.v2 as tf\nfrom tf_agents.agents.random import fixed_policy_agent\nimport tf_agents.policies.random_tf_policy as random_tf_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\n\n\n@gin.configurable\nclass RandomAgent(fixed_policy_agent.FixedPolicyAgent):\n  """"""An agent with a random policy and no learning.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               train_step_counter: Optional[tf.Variable] = None,\n               num_outer_dims: int = 1,\n               name: Optional[Text] = None):\n    """"""Creates a random agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If true, gradient summaries will be written.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      num_outer_dims: same as base class.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    policy_class = random_tf_policy.RandomTFPolicy\n\n    super(RandomAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy_class=policy_class,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter,\n        num_outer_dims=num_outer_dims)\n'"
tf_agents/agents/random/random_agent_test.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.agents.random.random_agent.""""""\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport tensorflow.compat.v2 as tf\n\nfrom tf_agents.agents.random import random_agent\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\ntf.enable_v2_behavior()\n\n\nclass RandomAgentTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(RandomAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n\n  def testCreateAgent(self):\n    agent = random_agent.RandomAgent(\n        self._time_step_spec,\n        self._action_spec,)\n    agent.initialize()\n\n  def testTrain(self):\n    # Define the train step counter.\n    counter = common.create_variable(\'test_train_counter\')\n    agent = random_agent.RandomAgent(\n        self._time_step_spec,\n        self._action_spec,\n        train_step_counter=counter,\n        num_outer_dims=2)\n    observations = tf.constant([\n        [[1, 2], [3, 4], [5, 6]],\n        [[1, 2], [3, 4], [5, 6]],\n    ],\n                               dtype=tf.float32)\n\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[1] * 3] * 2, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * 2, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]], [[0], [1], [1]]], dtype=tf.float32)\n\n    experience = trajectory.Trajectory(time_steps.step_type, observations,\n                                       actions, (),\n                                       time_steps.step_type, time_steps.reward,\n                                       time_steps.discount)\n\n    # Assert that counter starts out at zero.\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(0, self.evaluate(counter))\n\n    agent.train(experience)\n\n    # Now we should have one iteration.\n    self.assertEqual(1, self.evaluate(counter))\n\n  def testPolicy(self):\n    agent = random_agent.RandomAgent(\n        self._time_step_spec,\n        self._action_spec,)\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=1)\n    action_step = agent.policy.action(time_steps)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(action_step.action)\n    self.assertEqual(list(actions.shape), [1, 1])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/reinforce/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A REINFORCE agent.""""""\nfrom tf_agents.agents.reinforce import reinforce_agent\n\n'"
tf_agents/agents/reinforce/reinforce_agent.py,38,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A REINFORCE Agent.\n\nImplements the REINFORCE algorithm from (Williams, 1992):\nhttp://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Callable, Optional, Text\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory as traj\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import value_ops\n\n# A function `advantage(returns, value_predictions) -> advantages.\nAdvantageFnType = Callable[[types.Tensor, types.Tensor], types.Tensor]\n\n\ndef _standard_normalize(values, axes=(0,)):\n  """"""Standard normalizes values `values`.\n\n  Args:\n    values: Tensor with values to be standardized.\n    axes: Axes used to compute mean and variances.\n\n  Returns:\n    Standardized values (values - mean(values[axes])) / std(values[axes]).\n  """"""\n  values_mean, values_var = tf.nn.moments(x=values, axes=axes, keepdims=True)\n  epsilon = np.finfo(values.dtype.as_numpy_dtype).eps\n  normalized_values = ((values - values_mean) / (tf.sqrt(values_var) + epsilon))\n  return normalized_values\n\n\ndef _entropy_loss(distributions, spec, weights=None):\n  """"""Computes entropy loss.\n\n  Args:\n    distributions: A possibly batched tuple of distributions.\n    spec: A nested tuple representing the action spec.\n    weights: Optional scalar or element-wise (per-batch-entry) importance\n      weights.  Includes a mask for invalid timesteps.\n\n  Returns:\n    A Tensor representing the entropy loss.\n  """"""\n  with tf.name_scope(\'entropy_regularization\'):\n    entropy = -tf.cast(common.entropy(distributions, spec), tf.float32)\n    if weights is not None:\n      entropy *= weights\n    return tf.reduce_mean(input_tensor=entropy)\n\n\ndef _get_initial_policy_state(policy, time_steps):\n  """"""Gets the initial state of a policy.""""""\n  batch_size = (\n      tf.compat.dimension_at_index(time_steps.discount.shape, 0) or\n      tf.shape(time_steps.discount)[0])\n  return policy.get_initial_state(batch_size=batch_size)\n\n\nclass ReinforceAgentLossInfo(\n    collections.namedtuple(\n        \'ReinforceAgentLossInfo\',\n        (\'policy_gradient_loss\', \'policy_network_regularization_loss\',\n         \'entropy_regularization_loss\', \'value_estimation_loss\',\n         \'value_network_regularization_loss\'))):\n  """"""ReinforceAgentLossInfo is stored in the `extras` field of the LossInfo.\n\n  All losses, except for `policy_network_regularization_loss` have a validity\n  mask applied to ensure no loss or error is calculated for episode boundaries.\n\n  policy_gradient_loss: The weighted policy_gradient loss.\n  policy_network_regularization_loss: The regularization loss terms from the\n    policy network used to generate the `policy_gradient_loss`.\n  entropy_regularization_loss: The entropy regularization loss.\n  value_estimation_loss: If value estimation network is being used, the loss\n    associated with that network.\n\n  """"""\n  pass\n\n\n@gin.configurable\nclass ReinforceAgent(tf_agent.TFAgent):\n  """"""A REINFORCE Agent.\n\n  Implements:\n\n  REINFORCE algorithm from\n\n  ""Simple statistical gradient-following algorithms for connectionist\n  reinforcement learning""\n  Williams, R.J., 1992.\n  http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\n\n  REINFORCE with state-value baseline, where state-values are estimated with\n  function approximation, from\n\n  ""Reinforcement learning: An introduction"" (Sec. 13.4)\n  Sutton, R.S. and Barto, A.G., 2018.\n  http://incompleteideas.net/book/the-book-2nd.html\n\n  The REINFORCE agent can be optionally provided with:\n  - value_network: A `tf_agents.network.Network` which parameterizes state-value\n    estimation as a neural network. The network will be called with\n    call(observation, step_type) and returns a floating point state-values\n    tensor.\n  - value_estimation_loss_coef: Weight on the value prediction loss.\n\n  If value_network and value_estimation_loss_coef are provided, advantages are\n  computed as\n    `advantages = (discounted accumulated rewards) - (estimated state-values)`\n  and the overall learning objective becomes:\n    `(total loss) =\n      (policy gradient loss) +\n      value_estimation_loss_coef * (squared error of estimated state-values)`\n\n  """"""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.TensorSpec,\n               actor_network: network.Network,\n               optimizer: types.Optimizer,\n               value_network: Optional[network.Network] = None,\n               value_estimation_loss_coef: types.Float = 0.2,\n               advantage_fn: Optional[AdvantageFnType] = None,\n               use_advantage_loss: bool = True,\n               gamma: types.Float = 1.0,\n               normalize_returns: bool = True,\n               gradient_clipping: Optional[types.Float] = None,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               entropy_regularization: Optional[types.Float] = None,\n               train_step_counter: Optional[tf.Variable] = None,\n               name: Optional[Text] = None):\n    """"""Creates a REINFORCE Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      actor_network: A tf_agents.network.Network to be used by the agent. The\n        network will be called with call(observation, step_type).\n      optimizer: Optimizer for the actor network.\n      value_network: (Optional) A `tf_agents.network.Network` to be used by the\n        agent. The network will be called with call(observation, step_type) and\n        returns a floating point value tensor.\n      value_estimation_loss_coef: (Optional) Multiplier for value prediction\n        loss to balance with policy gradient loss.\n      advantage_fn: A function `A(returns, value_preds)` that takes returns and\n        value function predictions as input and returns advantages. The default\n        is `A(returns, value_preds) = returns - value_preds` if a value network\n        is specified and `use_advantage_loss=True`, otherwise `A(returns,\n        value_preds) = returns`.\n      use_advantage_loss: Whether to use value function predictions for\n        computing returns. `use_advantage_loss=False` is equivalent to setting\n        `advantage_fn=lambda returns, value_preds: returns`.\n      gamma: A discount factor for future rewards.\n      normalize_returns: Whether to normalize returns across episodes when\n        computing the loss.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      entropy_regularization: Coefficient for entropy regularization loss term.\n      train_step_counter: An optional counter to increment every time the train\n        op is run. Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    actor_network.create_variables()\n    self._actor_network = actor_network\n    if value_network:\n      value_network.create_variables()\n    self._value_network = value_network\n\n    collect_policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        actor_network=self._actor_network,\n        clip=True)\n\n    policy = greedy_policy.GreedyPolicy(collect_policy)\n\n    self._optimizer = optimizer\n    self._gamma = gamma\n    self._normalize_returns = normalize_returns\n    self._gradient_clipping = gradient_clipping\n    self._entropy_regularization = entropy_regularization\n    self._value_estimation_loss_coef = value_estimation_loss_coef\n    self._baseline = self._value_network is not None\n    self._advantage_fn = advantage_fn\n    if self._advantage_fn is None:\n      if use_advantage_loss and self._baseline:\n        self._advantage_fn = lambda returns, value_preds: returns - value_preds\n      else:\n        self._advantage_fn = lambda returns, _: returns\n\n    super(ReinforceAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=None,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _initialize(self):\n    pass\n\n  def _train(self, experience, weights=None):\n    # Add a mask to ensure we reset the return calculation at episode\n    # boundaries. This is needed in cases where episodes are truncated before\n    # reaching a terminal state. Note experience is a batch of trajectories\n    # where reward=next_step.reward so the mask may look shifted at first.\n    non_last_mask = tf.cast(\n        tf.math.not_equal(experience.next_step_type, ts.StepType.LAST),\n        tf.float32)\n    discounts = non_last_mask * experience.discount * self._gamma\n    returns = value_ops.discounted_return(\n        experience.reward, discounts, time_major=False)\n\n    if self._debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'rewards\', data=experience.reward, step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'discounts\',\n          data=experience.discount,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'returns\', data=returns, step=self.train_step_counter)\n\n    with tf.GradientTape() as tape:\n      loss_info = self.total_loss(\n          experience, tf.stop_gradient(returns), weights=weights,\n          training=True)\n      tf.debugging.check_numerics(loss_info.loss, \'Loss is inf or nan\')\n    variables_to_train = self._actor_network.trainable_weights\n    if self._baseline:\n      variables_to_train += self._value_network.trainable_weights\n    grads = tape.gradient(loss_info.loss, variables_to_train)\n\n    grads_and_vars = list(zip(grads, variables_to_train))\n    if self._gradient_clipping:\n      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n                                                       self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      eager_utils.add_variables_summaries(grads_and_vars,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n\n    self._optimizer.apply_gradients(\n        grads_and_vars, global_step=self.train_step_counter)\n\n    return tf.nest.map_structure(tf.identity, loss_info)\n\n  def total_loss(self,\n                 experience: traj.Trajectory,\n                 returns: types.Tensor,\n                 weights: types.Tensor,\n                 training: bool = False) -> tf_agent.LossInfo:\n    # Ensure we see at least one full episode.\n    time_steps = ts.TimeStep(experience.step_type,\n                             tf.zeros_like(experience.reward),\n                             tf.zeros_like(experience.discount),\n                             experience.observation)\n    is_last = experience.is_last()\n    num_episodes = tf.reduce_sum(tf.cast(is_last, tf.float32))\n    tf.debugging.assert_greater(\n        num_episodes,\n        0.0,\n        message=\'No complete episode found. REINFORCE requires full episodes \'\n        \'to compute losses.\')\n\n    # Mask out partial episodes at the end of each batch of time_steps.\n    # NOTE: We use is_last rather than is_boundary because the last transition\n    # is the transition with the last valid reward.  In other words, the\n    # reward on the boundary transitions do not have valid rewards.  Since\n    # REINFORCE is calculating a loss w.r.t. the returns (and not bootstrapping)\n    # keeping the boundary transitions is irrelevant.\n    valid_mask = tf.cast(experience.is_last(), dtype=tf.float32)\n    valid_mask = tf.math.cumsum(valid_mask, axis=1, reverse=True)\n    valid_mask = tf.cast(valid_mask > 0, dtype=tf.float32)\n    if weights is not None:\n      weights *= valid_mask\n    else:\n      weights = valid_mask\n\n    advantages = returns\n    value_preds = None\n\n    if self._baseline:\n      value_preds, _ = self._value_network(time_steps.observation,\n                                           time_steps.step_type,\n                                           training=True)\n      if self._debug_summaries:\n        tf.compat.v2.summary.histogram(\n            name=\'value_preds\', data=value_preds, step=self.train_step_counter)\n\n    advantages = self._advantage_fn(returns, value_preds)\n    if self._debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'advantages\', data=advantages, step=self.train_step_counter)\n\n    # TODO(b/126592060): replace with tensor normalizer.\n    if self._normalize_returns:\n      advantages = _standard_normalize(advantages, axes=(0, 1))\n      if self._debug_summaries:\n        tf.compat.v2.summary.histogram(\n            name=\'normalized_%s\' %\n            (\'advantages\' if self._baseline else \'returns\'),\n            data=advantages,\n            step=self.train_step_counter)\n\n    nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n    policy_state = _get_initial_policy_state(self.collect_policy, time_steps)\n    actions_distribution = self.collect_policy.distribution(\n        time_steps, policy_state=policy_state).action\n\n    policy_gradient_loss = self.policy_gradient_loss(\n        actions_distribution,\n        experience.action,\n        experience.is_boundary(),\n        advantages,\n        num_episodes,\n        weights,\n    )\n\n    entropy_regularization_loss = self.entropy_regularization_loss(\n        actions_distribution, weights)\n\n    network_regularization_loss = tf.nn.scale_regularization_loss(\n        self._actor_network.losses)\n\n    total_loss = (policy_gradient_loss +\n                  network_regularization_loss +\n                  entropy_regularization_loss)\n\n    losses_dict = {\n        \'policy_gradient_loss\': policy_gradient_loss,\n        \'policy_network_regularization_loss\': network_regularization_loss,\n        \'entropy_regularization_loss\': entropy_regularization_loss,\n        \'value_estimation_loss\': 0.0,\n        \'value_network_regularization_loss\': 0.0,\n    }\n\n    value_estimation_loss = None\n    if self._baseline:\n      value_estimation_loss = self.value_estimation_loss(\n          value_preds, returns, num_episodes, weights)\n      value_network_regularization_loss = tf.nn.scale_regularization_loss(\n          self._value_network.losses)\n      total_loss += value_estimation_loss + value_network_regularization_loss\n      losses_dict[\'value_estimation_loss\'] = value_estimation_loss\n      losses_dict[\'value_network_regularization_loss\'] = (\n          value_network_regularization_loss)\n\n    loss_info_extra = ReinforceAgentLossInfo._make(losses_dict)\n\n    losses_dict[\'total_loss\'] = total_loss  # Total loss not in loss_info_extra.\n\n    common.summarize_scalar_dict(losses_dict,\n                                 self.train_step_counter,\n                                 name_scope=\'Losses/\')\n\n    return tf_agent.LossInfo(total_loss, loss_info_extra)\n\n  def policy_gradient_loss(\n      self,\n      actions_distribution: types.NestedDistribution,\n      actions: types.NestedTensor,\n      is_boundary: types.Tensor,\n      returns: types.Tensor,\n      num_episodes: types.Int,\n      weights: Optional[types.Tensor] = None) -> types.Tensor:\n    """"""Computes the policy gradient loss.\n\n    Args:\n      actions_distribution: A possibly batched tuple of action distributions.\n      actions: Tensor with a batch of actions.\n      is_boundary: Tensor of booleans that indicate if the corresponding action\n        was in a boundary trajectory and should be ignored.\n      returns: Tensor with a return from each timestep, aligned on index. Works\n        better when returns are normalized.\n      num_episodes: Number of episodes contained in the training data.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  May include a mask for invalid timesteps.\n\n    Returns:\n      policy_gradient_loss: A tensor that will contain policy gradient loss for\n        the on-policy experience.\n    """"""\n    # TODO(b/126594799): Add class IndependentNested(tfd.Distribution) to handle\n    # nests of independent distributions like this.\n    action_log_prob = common.log_probability(actions_distribution, actions,\n                                             self.action_spec)\n\n    # Filter out transitions between end state of previous episode and start\n    # state of next episode.\n    valid_mask = tf.cast(~is_boundary, tf.float32)\n    action_log_prob *= valid_mask\n\n    action_log_prob_times_return = action_log_prob * returns\n\n    if weights is not None:\n      action_log_prob_times_return *= weights\n\n    if self._debug_summaries:\n      tf.compat.v2.summary.histogram(\n          name=\'action_log_prob\',\n          data=action_log_prob,\n          step=self.train_step_counter)\n      tf.compat.v2.summary.histogram(\n          name=\'action_log_prob_times_return\',\n          data=action_log_prob_times_return,\n          step=self.train_step_counter)\n\n    # Policy gradient loss is defined as the sum, over timesteps, of action\n    #   log-probability times the cumulative return from that timestep onward.\n    #   For more information, see (Williams, 1992).\n    policy_gradient_loss = -tf.reduce_sum(\n        input_tensor=action_log_prob_times_return)\n\n    # We take the mean over episodes by dividing by num_episodes.\n    policy_gradient_loss = policy_gradient_loss / num_episodes\n\n    return policy_gradient_loss\n\n  def entropy_regularization_loss(\n      self,\n      actions_distribution: types.NestedDistribution,\n      weights: Optional[types.Tensor] = None) -> types.Tensor:\n    """"""Computes the optional entropy regularization loss.\n\n    Extending REINFORCE by entropy regularization was originally proposed in\n    ""Function optimization using connectionist reinforcement learning\n    algorithms."" (Williams and Peng, 1991).\n\n    Args:\n      actions_distribution: A possibly batched tuple of action distributions.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  May include a mask for invalid timesteps.\n\n    Returns:\n      entropy_regularization_loss: A tensor with the entropy regularization\n      loss.\n    """"""\n    if self._entropy_regularization:\n      loss = _entropy_loss(actions_distribution, self.action_spec, weights)\n      loss *= self._entropy_regularization\n    else:\n      loss = tf.constant(0.0, dtype=tf.float32)\n\n    return loss\n\n  def value_estimation_loss(\n      self,\n      value_preds: types.Tensor,\n      returns: types.Tensor,\n      num_episodes: types.Int,\n      weights: Optional[types.Tensor] = None) -> types.Tensor:\n    """"""Computes the value estimation loss.\n\n    Args:\n      value_preds: Per-timestep estimated values.\n      returns: Per-timestep returns for value function to predict.\n      num_episodes: Number of episodes contained in the training data.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.  May include a mask for invalid timesteps.\n\n    Returns:\n      value_estimation_loss: A scalar value_estimation_loss loss.\n    """"""\n    value_estimation_error = tf.math.squared_difference(returns, value_preds)\n    if weights is not None:\n      value_estimation_error *= weights\n\n    value_estimation_loss = (\n        tf.reduce_sum(input_tensor=value_estimation_error) *\n        self._value_estimation_loss_coef)\n\n    # We take the mean over episodes by dividing by num_episodes.\n    value_estimation_loss = value_estimation_loss / num_episodes\n\n    return value_estimation_loss\n'"
tf_agents/agents/reinforce/reinforce_agent_test.py,168,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for TF Agents reinforce_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom absl.testing.absltest import mock\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.reinforce import reinforce_agent\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils as network_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\nfrom tensorflow.python.util import nest  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass DummyActorNet(network.Network):\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               unbounded_actions=False,\n               stateful=False):\n    # When unbounded_actions=True, we skip the final tanh activation and the\n    # action shift and scale. This allows us to compute the actor and critic\n    # losses by hand more easily.\n    # If stateful=True, the network state has the same shape as\n    # `input_tensor_spec`. Otherwise it is empty.\n    state_spec = (tf.TensorSpec(input_tensor_spec.shape, tf.float32)\n                  if stateful else ())\n    super(DummyActorNet, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=state_spec,\n        name=\'DummyActorNet\')\n    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    activation_fn = None if unbounded_actions else tf.nn.tanh\n    self._output_tensor_spec = output_tensor_spec\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            single_action_spec.shape.num_elements() * 2,\n            activation=activation_fn,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[2, 1], [1, 1]]),\n            bias_initializer=tf.compat.v1.initializers.constant(5),\n        ),\n    ]\n\n  def call(self, observations, step_type, network_state):\n    del step_type\n\n    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    for layer in self._dummy_layers:\n      states = layer(states)\n\n    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n    actions, stdevs = states[..., 0], states[..., 1]\n    actions = tf.reshape(actions, [-1] + single_action_spec.shape.as_list())\n    stdevs = tf.reshape(stdevs, [-1] + single_action_spec.shape.as_list())\n    actions = tf.nest.pack_sequence_as(self._output_tensor_spec, [actions])\n    stdevs = tf.nest.pack_sequence_as(self._output_tensor_spec, [stdevs])\n\n    distribution = nest.map_structure_up_to(\n        self._output_tensor_spec, tfp.distributions.Normal, actions, stdevs)\n    return distribution, network_state\n\n\nclass DummyValueNet(network.Network):\n\n  def __init__(self, observation_spec, name=None, outer_rank=1):\n    super(DummyValueNet, self).__init__(observation_spec, (), \'DummyValueNet\')\n    self._outer_rank = outer_rank\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            1,\n            kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n            bias_initializer=tf.compat.v1.initializers.constant([5]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    hidden_state = tf.cast(tf.nest.flatten(inputs), tf.float32)[0]\n    batch_squash = network_utils.BatchSquash(self._outer_rank)\n    hidden_state = batch_squash.flatten(hidden_state)\n    for layer in self._dummy_layers:\n      hidden_state = layer(hidden_state)\n    value_pred = tf.squeeze(batch_squash.unflatten(hidden_state), axis=-1)\n    return value_pred, network_state\n\n\nclass ReinforceAgentTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(ReinforceAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n\n  def testCreateAgent(self):\n    reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False),\n        optimizer=None,\n    )\n\n  def testCreateAgentWithValueNet(self):\n    reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False),\n        value_network=DummyValueNet(self._obs_spec),\n        value_estimation_loss_coef=0.5,\n        optimizer=None,\n    )\n\n  def testPolicyGradientLoss(self):\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n\n    expected_loss = 10.983667373657227\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testPolicyGradientLossMultipleEpisodes(self):\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    step_type = tf.constant(\n        [ts.StepType.FIRST, ts.StepType.LAST, ts.StepType.FIRST,\n         ts.StepType.LAST])\n    reward = tf.constant([0, 0, 0, 0], dtype=tf.float32)\n    discount = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([1.9, 1.9, 1.0, 1.0], dtype=tf.float32)\n\n    expected_loss = 5.140229225158691\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testMaskingRewardSingleEpisodeRewardOnFirst(self):\n    # Test that policy_gradient_loss reacts correctly to rewards when there are:\n    #   * A single MDP episode\n    #   * Returns on the tf.StepType.FIRST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]\n    # action      :   [0]       [1]\n    # reward      :    3         0\n    # ~is_boundary:    1         0\n    # is_last     :    1         0\n    # valid reward:   3*1       4*0\n    #\n    # The second action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is > 0.0 in this case, only LAST should be excluded.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST])\n    reward = tf.constant([3, 4], dtype=tf.float32)\n    discount = tf.constant([1, 0], dtype=tf.float32)\n    observations = tf.constant([[1, 2], [1, 2]], dtype=tf.float32)\n    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([3.0, 0.0], dtype=tf.float32)\n\n    # Returns on the StepType.FIRST should be counted.\n    expected_loss = 10.8935775757\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testMaskingReturnSingleEpisodeRewardOnLast(self):\n    # Test that policy_gradient_loss reacts correctly to rewards when there are:\n    #   * A single MDP episode\n    #   * Returns on the tf.StepType.LAST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]\n    # action      :   [0]       [1]\n    # reward      :    0         3\n    # ~is_boundary:    1         0\n    # is_last     :    1         0\n    # valid reward:   0*1       3*0\n    #\n    # The second action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.  The first has a 0 reward.\n    #\n    # The expected_loss is 0.0 in this case.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST])\n    reward = tf.constant([0, 3], dtype=tf.float32)\n    discount = tf.constant([1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2]], dtype=tf.float32)\n    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([0.0, 3.0], dtype=tf.float32)\n\n    # Returns on the StepType.LAST should not be counted.\n    expected_loss = 0.0\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testMaskingReturnMultipleEpisodesRewardOnFirst(self):\n    # Test that policy_gradient_loss reacts correctly to rewards when there are:\n    #   * Multiple MDP episodes\n    #   * Returns on the tf.StepType.FIRST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F) -> (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]    [1, 2]    [1, 2]\n    # action      :   [0]       [1]       [2]       [3]\n    # reward      :    3         0         4         0\n    # ~is_boundary:    1         0         1         0\n    # is_last     :    1         0         1         0\n    # valid reward:   3*1       0*0       4*1       0*0\n    #\n    # The second & fourth action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is > 0.0 in this case, only LAST should be excluded.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    step_type = tf.constant(\n        [ts.StepType.FIRST, ts.StepType.LAST, ts.StepType.FIRST,\n         ts.StepType.LAST])\n    reward = tf.constant([3, 0, 4, 0], dtype=tf.float32)\n    discount = tf.constant([1, 0, 1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([3.0, 0.0, 4.0, 0.0], dtype=tf.float32)\n\n    # Returns on the StepType.FIRST should be counted.\n    expected_loss = 12.2091741562\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testMaskingReturnMultipleEpisodesRewardOnLast(self):\n    # Test that policy_gradient_loss reacts correctly to returns when there are:\n    #   * Multiple MDP episodes\n    #   * Returns on the tf.StepType.LAST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F) -> (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]    [1, 2]    [1, 2]\n    # action      :   [0]       [1]       [2]       [3]\n    # reward      :    0         3         0         4\n    # ~is_boundary:    1         0         1         0\n    # is_last     :    1         0         1         0\n    # valid reward:   0*1       3*0       0*1       4*0\n    #\n    # The second & fourth action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is 0.0 in this case.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=None,\n    )\n\n    step_type = tf.constant(\n        [ts.StepType.FIRST, ts.StepType.LAST, ts.StepType.FIRST,\n         ts.StepType.LAST])\n    reward = tf.constant([0, 3, 0, 4], dtype=tf.float32)\n    discount = tf.constant([1, 0, 1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n    actions_distribution = agent.collect_policy.distribution(\n        time_steps).action\n    returns = tf.constant([0.0, 3.0, 0.0, 4.0], dtype=tf.float32)\n\n    # Returns on the StepType.LAST should not be counted.\n    expected_loss = 0.0\n    loss = agent.policy_gradient_loss(\n        actions_distribution, actions, time_steps.is_last(), returns, 2)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  @parameterized.parameters(\n      ([[[0.8, 0.2]]], [1],),\n      ([[[0.8, 0.2]], [[0.3, 0.7]]], [0.5, 0.5],),\n  )\n  def testEntropyLoss(self, probs, weights):\n    probs = tf.convert_to_tensor(probs)\n    distribution = tfp.distributions.Categorical(probs=probs)\n    shape = probs.shape.as_list()\n    action_spec = tensor_spec.TensorSpec(shape[2:-1], dtype=tf.int32)\n    expected = tf.reduce_mean(\n        -tf.reduce_mean(distribution.entropy()) * weights)\n    actual = reinforce_agent._entropy_loss(\n        distribution, action_spec, weights)\n    self.assertAlmostEqual(self.evaluate(actual), self.evaluate(expected),\n                           places=4)\n\n  def testValueEstimationLoss(self):\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False),\n        value_network=DummyValueNet(self._obs_spec),\n        value_estimation_loss_coef=0.5,\n        optimizer=None,\n    )\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n    value_preds, _ = agent._value_network(time_steps.observation,\n                                          time_steps.step_type)\n\n    expected_loss = 123.20500\n    loss = agent.value_estimation_loss(\n        value_preds, returns, 1)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testTrainMaskingRewardSingleBanditEpisode(self):\n    # Test that train reacts correctly to experience when there is only a\n    # single Bandit episode.  Bandit episodes are encoded differently than\n    # MDP episodes.  They have only a single transition with\n    # step_type=StepType.FIRST and next_step_type=StepType.LAST.\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L)\n    # observation : [1, 2]\n    # action      :   [0]\n    # reward      :    3\n    # ~is_boundary:    0\n    # is_last     :    1\n    # valid reward:   3*1\n    #\n    # The single bandit transition is valid and not masked.\n    #\n    # The expected_loss is > 0.0 in this case, matching the expected_loss of the\n    # testMaskingRewardSingleEpisodeRewardOnFirst policy_gradient_loss test.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST])\n    next_step_type = tf.constant([ts.StepType.LAST])\n    reward = tf.constant([3], dtype=tf.float32)\n    discount = tf.constant([0], dtype=tf.float32)\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards should be counted.\n    expected_loss = 10.8935775757\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingRewardMultipleBanditEpisodes(self):\n    # Test that train reacts correctly to experience when there are multiple\n    # Bandit episodes.  Bandit episodes are encoded differently than\n    # MDP episodes.  They (each) have only a single transition with\n    # step_type=StepType.FIRST and next_step_type=StepType.LAST.  This test\n    # helps ensure that LAST->FIRST->LAST transitions are handled correctly.\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (F, L)\n    # observation : [1, 2]    [1, 2]\n    # action      :   [0]       [2]\n    # reward      :    3         4\n    # ~is_boundary:    0         0\n    # is_last     :    1         1\n    # valid reward:   3*1       4*1\n    #\n    # All bandit transitions are valid and none are masked.\n    #\n    # The expected_loss is > 0.0 in this case, matching the expected_loss of the\n    # testMaskingRewardMultipleEpisodesRewardOnFirst policy_gradient_loss test.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.FIRST])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.LAST])\n    reward = tf.constant([3, 4], dtype=tf.float32)\n    discount = tf.constant([0, 0], dtype=tf.float32)\n    observations = tf.constant([[1, 2], [1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0], [2]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.FIRST should be counted.\n    expected_loss = 12.2091741562\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingRewardSingleEpisodeRewardOnFirst(self):\n    # Test that train reacts correctly to experience when there are:\n    #   * A single MDP episode\n    #   * Rewards on the tf.StepType.FIRST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]\n    # action      :   [0]       [1]\n    # reward      :    3         4\n    # ~is_boundary:    1         0\n    # is_last     :    1         0\n    # valid reward:   3*1       4*0\n    #\n    # The second action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is > 0.0 in this case, matching the expected_loss of the\n    # testMaskingRewardSingleEpisodeRewardOnFirst policy_gradient_loss test.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.FIRST])\n    reward = tf.constant([3, 4], dtype=tf.float32)\n    discount = tf.constant([1, 0], dtype=tf.float32)\n    observations = tf.constant([[1, 2], [1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.FIRST should be counted.\n    expected_loss = 10.8935775757\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingRewardSingleEpisodeRewardOnLast(self):\n    # Test that train reacts correctly to experience when there are:\n    #   * A single MDP episode\n    #   * Rewards on the tf.StepType.LAST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]\n    # action      :   [0]       [1]\n    # reward      :    0         3\n    # ~is_boundary:    1         0\n    # is_last     :    1         0\n    # valid reward:   0*1       3*0\n    #\n    # The second action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.  The first has a 0 reward.\n    #\n    # The expected_loss is = 0.0 in this case.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.FIRST])\n    reward = tf.constant([0, 3], dtype=tf.float32)\n    discount = tf.constant([1, 0], dtype=tf.float32)\n    observations = tf.constant([[1, 2], [1, 2]], dtype=tf.float32)\n\n    actions = tf.constant([[0], [1]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.LAST should not be counted.\n    expected_loss = 0.0\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingRewardMultipleEpisodesRewardOnFirst(self):\n    # Test that train reacts correctly to experience when there are:\n    #   * Multiple MDP episodes\n    #   * Rewards on the tf.StepType.FIRST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F) -> (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]    [1, 2]    [1, 2]\n    # action      :   [0]       [1]       [2]       [3]\n    # reward      :    3         0         4         0\n    # ~is_boundary:    1         0         1         0\n    # is_last     :    1         0         1         0\n    # valid reward:   3*1       0*0       4*1       0*0\n    #\n    # The second & fourth action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is > 0.0 in this case, matching the expected_loss of the\n    # testMaskingRewardMultipleEpisodesRewardOnFirst policy_gradient_loss test.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST,\n                             ts.StepType.FIRST, ts.StepType.LAST])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.FIRST,\n                                  ts.StepType.LAST, ts.StepType.FIRST])\n    reward = tf.constant([3, 0, 4, 0], dtype=tf.float32)\n    discount = tf.constant([1, 0, 1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.FIRST should be counted.\n    expected_loss = 12.2091741562\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingPartialEpisodeMultipleEpisodesRewardOnFirst(self):\n    # Test that train reacts correctly to experience when there are:\n    #   * Multiple MDP episodes\n    #   * Rewards on the tf.StepType.FIRST transitions\n    #   * Partial episode at end of experience\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F) -> (F, M) -> (M, M)\n    # observation : [1, 2]    [1, 2]    [1, 2]    [1, 2]\n    # action      :   [0]       [1]       [2]       [3]\n    # reward      :    3         0         4         0\n    # ~is_boundary:    1         0         1         1\n    # is_last     :    1         0         0         0\n    # valid reward:   3*1       0*0       4*0       0*0\n    #\n    # The second action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.  The third & fourth transitions\n    # should get masked out for everything due to it being an incomplete episode\n    # (notice there is no trailing step_type=(F,L)).\n    #\n    # The expected_loss is > 0.0 in this case, matching the expected_loss of the\n    # testMaskingRewardSingleEpisodeRewardOnFirst policy_gradient_loss test,\n    # because the partial second episode should be masked out.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST,\n                             ts.StepType.FIRST, ts.StepType.MID])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.FIRST,\n                                  ts.StepType.MID, ts.StepType.MID])\n    reward = tf.constant([3, 0, 4, 0], dtype=tf.float32)\n    discount = tf.constant([1, 0, 1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.FIRST should be counted.\n    expected_loss = 10.8935775757\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testTrainMaskingRewardMultipleEpisodesRewardOnLast(self):\n    # Test that train reacts correctly to experience when there are:\n    #   * Multiple MDP episodes\n    #   * Rewards on the tf.StepType.LAST transitions\n    #\n    # F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n    #\n    # Experience looks like this:\n    # Trajectories: (F, L) -> (L, F) -> (F, L) -> (L, F)\n    # observation : [1, 2]    [1, 2]    [1, 2]    [1, 2]\n    # action      :   [0]       [1]       [2]       [3]\n    # reward      :    0         3         0         4\n    # ~is_boundary:    1         0         1         0\n    # is_last     :    1         0         1         0\n    # valid reward:   0*1       3*0       0*1       4*0\n    #\n    # The second & fourth action & reward should be masked out due to being on a\n    # boundary (step_type=(L, F)) transition.\n    #\n    # The expected_loss is = 0.0 in this case.\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=True),\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        use_advantage_loss=False,\n        normalize_returns=False,\n    )\n\n    step_type = tf.constant([ts.StepType.FIRST, ts.StepType.LAST,\n                             ts.StepType.FIRST, ts.StepType.LAST])\n    next_step_type = tf.constant([ts.StepType.LAST, ts.StepType.FIRST,\n                                  ts.StepType.LAST, ts.StepType.FIRST])\n    reward = tf.constant([0, 3, 0, 4], dtype=tf.float32)\n    discount = tf.constant([1, 0, 1, 0], dtype=tf.float32)\n    observations = tf.constant(\n        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n\n    experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount))\n\n    # Rewards on the StepType.LAST should be counted.\n    expected_loss = 0.0\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = self.evaluate(loss)\n    self.assertAllClose(loss_info.loss, expected_loss)\n\n  def testPolicy(self):\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False),\n        optimizer=None,\n    )\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = agent.policy.action(time_steps).action\n    self.assertEqual(actions.shape.as_list(), [1, 1])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action_values = self.evaluate(actions)\n    tf.nest.map_structure(\n        lambda v, s: self.assertAllInRange(v, s.minimum, s.maximum),\n        action_values, self._action_spec)\n\n  @parameterized.parameters(\n      (False,),\n      (True,),\n  )\n  def testGetInitialPolicyState(self, stateful):\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False,\n            stateful=stateful),\n        optimizer=None,\n    )\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=3)\n    initial_state = reinforce_agent._get_initial_policy_state(\n        agent.collect_policy, time_steps)\n    if stateful:\n      self.assertAllEqual(self.evaluate(initial_state),\n                          self.evaluate(tf.zeros((3, 2), dtype=tf.float32)))\n    else:\n      self.assertEqual(initial_state, ())\n\n  def testTrainWithRnn(self):\n    actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        self._obs_spec,\n        self._action_spec,\n        input_fc_layer_params=None,\n        output_fc_layer_params=None,\n        conv_layer_params=None,\n        lstm_size=(40,))\n\n    counter = common.create_variable(\'test_train_counter\')\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=actor_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n        train_step_counter=counter\n    )\n\n    batch_size = 5\n    observations = tf.constant(\n        [[[1, 2], [3, 4], [5, 6]]] * batch_size, dtype=tf.float32)\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[1, 1, 2]] * batch_size, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        observation=observations)\n    actions = tf.constant([[[0], [1], [1]]] * batch_size, dtype=tf.float32)\n\n    experience = trajectory.Trajectory(\n        time_steps.step_type, observations, actions, (),\n        time_steps.step_type, time_steps.reward, time_steps.discount)\n\n    # Force variable creation.\n    agent.policy.variables()\n\n    if tf.executing_eagerly():\n      loss = lambda: agent.train(experience)\n    else:\n      loss = agent.train(experience)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(self.evaluate(counter), 0)\n    self.evaluate(loss)\n    self.assertEqual(self.evaluate(counter), 1)\n\n  @parameterized.parameters(\n      (False,), (True,)\n  )\n  def testWithAdvantageFn(self, with_value_network):\n    advantage_fn = mock.Mock(\n        side_effect=lambda returns, _: returns)\n\n    value_network = (DummyValueNet(self._obs_spec) if with_value_network\n                     else None)\n    agent = reinforce_agent.ReinforceAgent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=DummyActorNet(\n            self._obs_spec, self._action_spec, unbounded_actions=False),\n        value_network=value_network,\n        advantage_fn=advantage_fn,\n        optimizer=None,\n    )\n\n    step_type = tf.constant([[ts.StepType.FIRST, ts.StepType.LAST,\n                              ts.StepType.FIRST, ts.StepType.LAST]])\n    next_step_type = tf.constant([[ts.StepType.LAST, ts.StepType.FIRST,\n                                   ts.StepType.LAST, ts.StepType.FIRST]])\n    reward = tf.constant([[0, 0, 0, 0]], dtype=tf.float32)\n    discount = tf.constant([[1, 1, 1, 1]], dtype=tf.float32)\n    observations = tf.constant(\n        [[[1, 2], [1, 2], [1, 2], [1, 2]]], dtype=tf.float32)\n    actions = tf.constant([[[0], [1], [2], [3]]], dtype=tf.float32)\n\n    experience = trajectory.Trajectory(\n        step_type, observations, actions, (), next_step_type, reward, discount)\n\n    agent.total_loss(experience, reward, None)\n\n    advantage_fn.assert_called_once()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/sac/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A Soft Actor Critic agent.""""""\nfrom tf_agents.agents.sac import sac_agent\n'"
tf_agents/agents/sac/sac_agent.py,39,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""A Soft Actor-Critic Agent.\n\nImplements the Soft Actor-Critic (SAC) algorithm from\n""Soft Actor-Critic Algorithms and Applications"" by Haarnoja et al (2019).\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Callable, Optional, Text\n\nimport gin\nimport numpy as np\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import object_identity\n\n\nSacLossInfo = collections.namedtuple(\n    \'SacLossInfo\', (\'critic_loss\', \'actor_loss\', \'alpha_loss\'))\n\n\n# TODO(b/148889463): deprecate std_clip_transform\n@gin.configurable\ndef std_clip_transform(stddevs: types.NestedTensor) -> types.NestedTensor:\n  stddevs = tf.nest.map_structure(lambda t: tf.clip_by_value(t, -20, 2),\n                                  stddevs)\n  return tf.exp(stddevs)\n\n\n@gin.configurable\nclass SacAgent(tf_agent.TFAgent):\n  """"""A SAC Agent.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensorSpec,\n               critic_network: network.Network,\n               actor_network: network.Network,\n               actor_optimizer: types.Optimizer,\n               critic_optimizer: types.Optimizer,\n               alpha_optimizer: types.Optimizer,\n               actor_loss_weight: types.Float = 1.0,\n               critic_loss_weight: types.Float = 0.5,\n               alpha_loss_weight: types.Float = 1.0,\n               actor_policy_ctor: Callable[\n                   ..., tf_policy.TFPolicy] = actor_policy.ActorPolicy,\n               critic_network_2: Optional[network.Network] = None,\n               target_critic_network: Optional[network.Network] = None,\n               target_critic_network_2: Optional[network.Network] = None,\n               target_update_tau: types.Float = 1.0,\n               target_update_period: types.Int = 1,\n               td_errors_loss_fn: types.LossFn = tf.math.squared_difference,\n               gamma: types.Float = 1.0,\n               reward_scale_factor: types.Float = 1.0,\n               initial_log_alpha: types.Float = 0.0,\n               use_log_alpha_in_alpha_loss: bool = True,\n               target_entropy: Optional[types.Float] = None,\n               gradient_clipping: Optional[types.Float] = None,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               train_step_counter: Optional[tf.Variable] = None,\n               name: Optional[Text] = None):\n    """"""Creates a SAC Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      critic_network: A function critic_network((observations, actions)) that\n        returns the q_values for each observation and action.\n      actor_network: A function actor_network(observation, action_spec) that\n        returns action distribution.\n      actor_optimizer: The optimizer to use for the actor network.\n      critic_optimizer: The default optimizer to use for the critic network.\n      alpha_optimizer: The default optimizer to use for the alpha variable.\n      actor_loss_weight: The weight on actor loss.\n      critic_loss_weight: The weight on critic loss.\n      alpha_loss_weight: The weight on alpha loss.\n      actor_policy_ctor: The policy class to use.\n      critic_network_2: (Optional.)  A `tf_agents.network.Network` to be used as\n        the second critic network during Q learning.  The weights from\n        `critic_network` are copied if this is not provided.\n      target_critic_network: (Optional.)  A `tf_agents.network.Network` to be\n        used as the target critic network during Q learning. Every\n        `target_update_period` train steps, the weights from `critic_network`\n        are copied (possibly withsmoothing via `target_update_tau`) to `\n        target_critic_network`.  If `target_critic_network` is not provided, it\n        is created by making a copy of `critic_network`, which initializes a new\n        network with the same structure and its own layers and weights.\n        Performing a `Network.copy` does not work when the network instance\n        already has trainable parameters (e.g., has already been built, or when\n        the network is sharing layers with another).  In these cases, it is up\n        to you to build a copy having weights that are not shared with the\n        original `critic_network`, so that this can be used as a target network.\n        If you provide a `target_critic_network` that shares any weights with\n        `critic_network`, a warning will be logged but no exception is thrown.\n      target_critic_network_2: (Optional.) Similar network as\n        target_critic_network but for the critic_network_2. See documentation\n        for target_critic_network. Will only be used if \'critic_network_2\' is\n        also specified.\n      target_update_tau: Factor for soft update of the target networks.\n      target_update_period: Period for soft update of the target networks.\n      td_errors_loss_fn:  A function for computing the elementwise TD errors\n        loss.\n      gamma: A discount factor for future rewards.\n      reward_scale_factor: Multiplicative scale for the reward.\n      initial_log_alpha: Initial value for log_alpha.\n      use_log_alpha_in_alpha_loss: A boolean, whether using log_alpha or alpha\n        in alpha loss. Certain implementations of SAC use log_alpha as log\n        values are generally nicer to work with.\n      target_entropy: The target average policy entropy, for updating alpha. The\n        default value is negative of the total number of actions.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall under\n        that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n\n    self._check_action_spec(action_spec)\n\n    self._critic_network_1 = critic_network\n    self._critic_network_1.create_variables()\n    if target_critic_network:\n      target_critic_network.create_variables()\n    self._target_critic_network_1 = (\n        common.maybe_copy_target_network_with_checks(self._critic_network_1,\n                                                     target_critic_network,\n                                                     \'TargetCriticNetwork1\'))\n\n    if critic_network_2 is not None:\n      self._critic_network_2 = critic_network_2\n    else:\n      self._critic_network_2 = critic_network.copy(name=\'CriticNetwork2\')\n      # Do not use target_critic_network_2 if critic_network_2 is None.\n      target_critic_network_2 = None\n    self._critic_network_2.create_variables()\n    if target_critic_network_2:\n      target_critic_network_2.create_variables()\n    self._target_critic_network_2 = (\n        common.maybe_copy_target_network_with_checks(self._critic_network_2,\n                                                     target_critic_network_2,\n                                                     \'TargetCriticNetwork2\'))\n\n    if actor_network:\n      actor_network.create_variables()\n    self._actor_network = actor_network\n\n    policy = actor_policy_ctor(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        actor_network=self._actor_network,\n        training=False)\n\n    self._train_policy = actor_policy_ctor(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        actor_network=self._actor_network,\n        training=True)\n\n    self._log_alpha = common.create_variable(\n        \'initial_log_alpha\',\n        initial_value=initial_log_alpha,\n        dtype=tf.float32,\n        trainable=True)\n\n    if target_entropy is None:\n      target_entropy = self._get_default_target_entropy(action_spec)\n\n    self._use_log_alpha_in_alpha_loss = use_log_alpha_in_alpha_loss\n    self._target_update_tau = target_update_tau\n    self._target_update_period = target_update_period\n    self._actor_optimizer = actor_optimizer\n    self._critic_optimizer = critic_optimizer\n    self._alpha_optimizer = alpha_optimizer\n    self._actor_loss_weight = actor_loss_weight\n    self._critic_loss_weight = critic_loss_weight\n    self._alpha_loss_weight = alpha_loss_weight\n    self._td_errors_loss_fn = td_errors_loss_fn\n    self._gamma = gamma\n    self._reward_scale_factor = reward_scale_factor\n    self._target_entropy = target_entropy\n    self._gradient_clipping = gradient_clipping\n    self._debug_summaries = debug_summaries\n    self._summarize_grads_and_vars = summarize_grads_and_vars\n    self._update_target = self._get_target_updater(\n        tau=self._target_update_tau, period=self._target_update_period)\n\n    train_sequence_length = 2 if not critic_network.state_spec else None\n\n    super(SacAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy=policy,\n        collect_policy=policy,\n        train_sequence_length=train_sequence_length,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _check_action_spec(self, action_spec):\n    flat_action_spec = tf.nest.flatten(action_spec)\n    for spec in flat_action_spec:\n      if spec.dtype.is_integer:\n        raise NotImplementedError(\n            \'SacAgent does not currently support discrete actions. \'\n            \'Action spec: {}\'.format(action_spec))\n\n  def _get_default_target_entropy(self, action_spec):\n    # If target_entropy was not passed, set it to -dim(A)/2.0\n    # Note that the original default entropy target is -dim(A) in the SAC paper.\n    # However this formulation has also been used in practice by the original\n    # authors and has in our experience been more stable for gym/mujoco.\n    flat_action_spec = tf.nest.flatten(action_spec)\n    target_entropy = -np.sum([\n        np.product(single_spec.shape.as_list())\n        for single_spec in flat_action_spec\n    ]) / 2.0\n    return target_entropy\n\n  def _initialize(self):\n    """"""Returns an op to initialize the agent.\n\n    Copies weights from the Q networks to the target Q network.\n    """"""\n    common.soft_variables_update(\n        self._critic_network_1.variables,\n        self._target_critic_network_1.variables,\n        tau=1.0)\n    common.soft_variables_update(\n        self._critic_network_2.variables,\n        self._target_critic_network_2.variables,\n        tau=1.0)\n\n  def _train(self, experience, weights):\n    """"""Returns a train op to update the agent\'s networks.\n\n    This method trains with the provided batched experience.\n\n    Args:\n      experience: A time-stacked trajectory object.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.\n\n    Returns:\n      A train_op.\n\n    Raises:\n      ValueError: If optimizers are None and no default value was provided to\n        the constructor.\n    """"""\n    squeeze_time_dim = not self._critic_network_1.state_spec\n    time_steps, policy_steps, next_time_steps = (\n        trajectory.experience_to_transitions(experience, squeeze_time_dim))\n    actions = policy_steps.action\n\n    trainable_critic_variables = list(object_identity.ObjectIdentitySet(\n        self._critic_network_1.trainable_variables +\n        self._critic_network_2.trainable_variables))\n\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_critic_variables, (\'No trainable critic variables to \'\n                                          \'optimize.\')\n      tape.watch(trainable_critic_variables)\n      critic_loss = self._critic_loss_weight*self.critic_loss(\n          time_steps,\n          actions,\n          next_time_steps,\n          td_errors_loss_fn=self._td_errors_loss_fn,\n          gamma=self._gamma,\n          reward_scale_factor=self._reward_scale_factor,\n          weights=weights,\n          training=True)\n\n    tf.debugging.check_numerics(critic_loss, \'Critic loss is inf or nan.\')\n    critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n    self._apply_gradients(critic_grads, trainable_critic_variables,\n                          self._critic_optimizer)\n\n    trainable_actor_variables = self._actor_network.trainable_variables\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_actor_variables, (\'No trainable actor variables to \'\n                                         \'optimize.\')\n      tape.watch(trainable_actor_variables)\n      actor_loss = self._actor_loss_weight*self.actor_loss(\n          time_steps, weights=weights)\n    tf.debugging.check_numerics(actor_loss, \'Actor loss is inf or nan.\')\n    actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n    self._apply_gradients(actor_grads, trainable_actor_variables,\n                          self._actor_optimizer)\n\n    alpha_variable = [self._log_alpha]\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert alpha_variable, \'No alpha variable to optimize.\'\n      tape.watch(alpha_variable)\n      alpha_loss = self._alpha_loss_weight*self.alpha_loss(\n          time_steps, weights=weights)\n    tf.debugging.check_numerics(alpha_loss, \'Alpha loss is inf or nan.\')\n    alpha_grads = tape.gradient(alpha_loss, alpha_variable)\n    self._apply_gradients(alpha_grads, alpha_variable, self._alpha_optimizer)\n\n    with tf.name_scope(\'Losses\'):\n      tf.compat.v2.summary.scalar(\n          name=\'critic_loss\', data=critic_loss, step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'actor_loss\', data=actor_loss, step=self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'alpha_loss\', data=alpha_loss, step=self.train_step_counter)\n\n    self.train_step_counter.assign_add(1)\n    self._update_target()\n\n    total_loss = critic_loss + actor_loss + alpha_loss\n\n    extra = SacLossInfo(\n        critic_loss=critic_loss, actor_loss=actor_loss, alpha_loss=alpha_loss)\n\n    return tf_agent.LossInfo(loss=total_loss, extra=extra)\n\n  def _apply_gradients(self, gradients, variables, optimizer):\n    # list(...) is required for Python3.\n    grads_and_vars = list(zip(gradients, variables))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n                                                       self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      eager_utils.add_variables_summaries(grads_and_vars,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n\n    optimizer.apply_gradients(grads_and_vars)\n\n  def _get_target_updater(self, tau=1.0, period=1):\n    """"""Performs a soft update of the target network parameters.\n\n    For each weight w_s in the original network, and its corresponding\n    weight w_t in the target network, a soft update is:\n    w_t = (1- tau) x w_t + tau x ws\n\n    Args:\n      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n      period: Step interval at which the target network is updated.\n\n    Returns:\n      A callable that performs a soft update of the target network parameters.\n    """"""\n    with tf.name_scope(\'update_target\'):\n\n      def update():\n        """"""Update target network.""""""\n        critic_update_1 = common.soft_variables_update(\n            self._critic_network_1.variables,\n            self._target_critic_network_1.variables,\n            tau,\n            tau_non_trainable=1.0)\n\n        critic_2_update_vars = common.deduped_network_variables(\n            self._critic_network_2, self._critic_network_1)\n\n        target_critic_2_update_vars = common.deduped_network_variables(\n            self._target_critic_network_2, self._target_critic_network_1)\n\n        critic_update_2 = common.soft_variables_update(\n            critic_2_update_vars,\n            target_critic_2_update_vars,\n            tau,\n            tau_non_trainable=1.0)\n\n        return tf.group(critic_update_1, critic_update_2)\n\n      return common.Periodically(update, period, \'update_targets\')\n\n  def _actions_and_log_probs(self, time_steps):\n    """"""Get actions and corresponding log probabilities from policy.""""""\n    # Get raw action distribution from policy, and initialize bijectors list.\n    batch_size = nest_utils.get_outer_shape(time_steps, self._time_step_spec)[0]\n    policy_state = self._train_policy.get_initial_state(batch_size)\n    action_distribution = self._train_policy.distribution(\n        time_steps, policy_state=policy_state).action\n\n    # Sample actions and log_pis from transformed distribution.\n    actions = tf.nest.map_structure(lambda d: d.sample(), action_distribution)\n    log_pi = common.log_probability(action_distribution, actions,\n                                    self.action_spec)\n\n    return actions, log_pi\n\n  def critic_loss(self,\n                  time_steps: ts.TimeStep,\n                  actions: types.Tensor,\n                  next_time_steps: ts.TimeStep,\n                  td_errors_loss_fn: types.LossFn,\n                  gamma: types.Float = 1.0,\n                  reward_scale_factor: types.Float = 1.0,\n                  weights: Optional[types.Tensor] = None,\n                  training: bool = False) -> types.Tensor:\n    """"""Computes the critic loss for SAC training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      actions: A batch of actions.\n      next_time_steps: A batch of next timesteps.\n      td_errors_loss_fn: A function(td_targets, predictions) to compute\n        elementwise (per-batch-entry) loss.\n      gamma: Discount for future rewards.\n      reward_scale_factor: Multiplicative factor to scale rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.\n      training: Whether this loss is being used for training.\n\n    Returns:\n      critic_loss: A scalar critic loss.\n    """"""\n    with tf.name_scope(\'critic_loss\'):\n      nest_utils.assert_same_structure(actions, self.action_spec)\n      nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n      nest_utils.assert_same_structure(next_time_steps, self.time_step_spec)\n\n      next_actions, next_log_pis = self._actions_and_log_probs(next_time_steps)\n      target_input = (next_time_steps.observation, next_actions)\n      target_q_values1, unused_network_state1 = self._target_critic_network_1(\n          target_input, next_time_steps.step_type, training=False)\n      target_q_values2, unused_network_state2 = self._target_critic_network_2(\n          target_input, next_time_steps.step_type, training=False)\n      target_q_values = (\n          tf.minimum(target_q_values1, target_q_values2) -\n          tf.exp(self._log_alpha) * next_log_pis)\n\n      td_targets = tf.stop_gradient(\n          reward_scale_factor * next_time_steps.reward +\n          gamma * next_time_steps.discount * target_q_values)\n\n      pred_input = (time_steps.observation, actions)\n      pred_td_targets1, _ = self._critic_network_1(\n          pred_input, time_steps.step_type, training=training)\n      pred_td_targets2, _ = self._critic_network_2(\n          pred_input, time_steps.step_type, training=training)\n      critic_loss1 = td_errors_loss_fn(td_targets, pred_td_targets1)\n      critic_loss2 = td_errors_loss_fn(td_targets, pred_td_targets2)\n      critic_loss = critic_loss1 + critic_loss2\n\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Sum over the time dimension.\n        critic_loss = tf.reduce_sum(input_tensor=critic_loss, axis=1)\n\n      agg_loss = common.aggregate_losses(\n          per_example_loss=critic_loss,\n          sample_weight=weights,\n          regularization_loss=(self._critic_network_1.losses +\n                               self._critic_network_2.losses))\n      critic_loss = agg_loss.total_loss\n\n      self._critic_loss_debug_summaries(td_targets, pred_td_targets1,\n                                        pred_td_targets2)\n\n      return critic_loss\n\n  def actor_loss(self,\n                 time_steps: ts.TimeStep,\n                 weights: Optional[types.Tensor] = None) -> types.Tensor:\n    """"""Computes the actor_loss for SAC training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.\n\n    Returns:\n      actor_loss: A scalar actor loss.\n    """"""\n    with tf.name_scope(\'actor_loss\'):\n      nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n\n      actions, log_pi = self._actions_and_log_probs(time_steps)\n      target_input = (time_steps.observation, actions)\n      target_q_values1, _ = self._critic_network_1(\n          target_input, time_steps.step_type, training=False)\n      target_q_values2, _ = self._critic_network_2(\n          target_input, time_steps.step_type, training=False)\n      target_q_values = tf.minimum(target_q_values1, target_q_values2)\n      actor_loss = tf.exp(self._log_alpha) * log_pi - target_q_values\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Sum over the time dimension.\n        actor_loss = tf.reduce_sum(input_tensor=actor_loss, axis=1)\n      reg_loss = self._actor_network.losses if self._actor_network else None\n      agg_loss = common.aggregate_losses(\n          per_example_loss=actor_loss,\n          sample_weight=weights,\n          regularization_loss=reg_loss)\n      actor_loss = agg_loss.total_loss\n      self._actor_loss_debug_summaries(actor_loss, actions, log_pi,\n                                       target_q_values, time_steps)\n\n      return actor_loss\n\n  def alpha_loss(self,\n                 time_steps: ts.TimeStep,\n                 weights: Optional[types.Tensor] = None) -> types.Tensor:\n    """"""Computes the alpha_loss for EC-SAC training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.\n\n    Returns:\n      alpha_loss: A scalar alpha loss.\n    """"""\n    with tf.name_scope(\'alpha_loss\'):\n      nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n\n      unused_actions, log_pi = self._actions_and_log_probs(time_steps)\n      entropy_diff = tf.stop_gradient(-log_pi - self._target_entropy)\n      if self._use_log_alpha_in_alpha_loss:\n        alpha_loss = (self._log_alpha * entropy_diff)\n      else:\n        alpha_loss = (tf.exp(self._log_alpha) * entropy_diff)\n\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Sum over the time dimension.\n        alpha_loss = tf.reduce_sum(input_tensor=alpha_loss, axis=1)\n      else:\n        alpha_loss = tf.expand_dims(alpha_loss, 0)\n\n      agg_loss = common.aggregate_losses(\n          per_example_loss=alpha_loss, sample_weight=weights)\n      alpha_loss = agg_loss.total_loss\n\n      self._alpha_loss_debug_summaries(alpha_loss, entropy_diff)\n\n      return alpha_loss\n\n  def _critic_loss_debug_summaries(self, td_targets, pred_td_targets1,\n                                   pred_td_targets2):\n    if self._debug_summaries:\n      td_errors1 = td_targets - pred_td_targets1\n      td_errors2 = td_targets - pred_td_targets2\n      td_errors = tf.concat([td_errors1, td_errors2], axis=0)\n      common.generate_tensor_summaries(\'td_errors\', td_errors,\n                                       self.train_step_counter)\n      common.generate_tensor_summaries(\'td_targets\', td_targets,\n                                       self.train_step_counter)\n      common.generate_tensor_summaries(\'pred_td_targets1\', pred_td_targets1,\n                                       self.train_step_counter)\n      common.generate_tensor_summaries(\'pred_td_targets2\', pred_td_targets2,\n                                       self.train_step_counter)\n\n  def _actor_loss_debug_summaries(self, actor_loss, actions, log_pi,\n                                  target_q_values, time_steps):\n    if self._debug_summaries:\n      common.generate_tensor_summaries(\'actor_loss\', actor_loss,\n                                       self.train_step_counter)\n      try:\n        common.generate_tensor_summaries(\'actions\', actions,\n                                         self.train_step_counter)\n      except ValueError:\n        pass  # Guard against internal SAC variants that do not directly\n        # generate actions.\n\n      common.generate_tensor_summaries(\'log_pi\', log_pi,\n                                       self.train_step_counter)\n      tf.compat.v2.summary.scalar(\n          name=\'entropy_avg\',\n          data=-tf.reduce_mean(input_tensor=log_pi),\n          step=self.train_step_counter)\n      common.generate_tensor_summaries(\'target_q_values\', target_q_values,\n                                       self.train_step_counter)\n      batch_size = nest_utils.get_outer_shape(time_steps,\n                                              self._time_step_spec)[0]\n      policy_state = self._train_policy.get_initial_state(batch_size)\n      action_distribution = self._train_policy.distribution(\n          time_steps, policy_state).action\n      if isinstance(action_distribution, tfp.distributions.Normal):\n        common.generate_tensor_summaries(\'act_mean\', action_distribution.loc,\n                                         self.train_step_counter)\n        common.generate_tensor_summaries(\'act_stddev\',\n                                         action_distribution.scale,\n                                         self.train_step_counter)\n      elif isinstance(action_distribution, tfp.distributions.Categorical):\n        common.generate_tensor_summaries(\'act_mode\', action_distribution.mode(),\n                                         self.train_step_counter)\n      try:\n        common.generate_tensor_summaries(\'entropy_action\',\n                                         action_distribution.entropy(),\n                                         self.train_step_counter)\n      except NotImplementedError:\n        pass  # Some distributions do not have an analytic entropy.\n\n  def _alpha_loss_debug_summaries(self, alpha_loss, entropy_diff):\n    if self._debug_summaries:\n      common.generate_tensor_summaries(\'alpha_loss\', alpha_loss,\n                                       self.train_step_counter)\n      common.generate_tensor_summaries(\'entropy_diff\', entropy_diff,\n                                       self.train_step_counter)\n\n      tf.compat.v2.summary.scalar(\n          name=\'log_alpha\', data=self._log_alpha, step=self.train_step_counter)\n'"
tf_agents/agents/sac/sac_agent_test.py,58,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.sac.sac_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.sac import sac_agent\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.trajectories.policy_step import PolicyStep\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass _MockDistribution(object):\n\n  def __init__(self, action):\n    self._action = action\n\n  def sample(self):\n    return self._action\n\n  def log_prob(self, unused_sample):\n    return tf.constant(10., shape=[1])\n\n\nclass DummyActorPolicy(object):\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               actor_network,\n               training=False):\n    del time_step_spec\n    del actor_network\n    del training\n    single_action_spec = tf.nest.flatten(action_spec)[0]\n    # Action is maximum of action range.\n    self._action = single_action_spec.maximum\n    self._action_spec = action_spec\n\n  def action(self, time_step):\n    observation = time_step.observation\n    batch_size = observation.shape[0]\n    action = tf.constant(self._action, dtype=tf.float32, shape=[batch_size, 1])\n    return PolicyStep(action=action)\n\n  def distribution(self, time_step, policy_state=()):\n    del policy_state\n    action = self.action(time_step).action\n    return PolicyStep(action=_MockDistribution(action))\n\n  def get_initial_state(self, batch_size):\n    del batch_size\n    return ()\n\n\nclass DummyCriticNet(network.Network):\n\n  def __init__(self, l2_regularization_weight=0.0, shared_layer=None):\n    super(DummyCriticNet, self).__init__(\n        input_tensor_spec=(tensor_spec.TensorSpec([2], tf.float32),\n                           tensor_spec.TensorSpec([1], tf.float32)),\n        state_spec=(),\n        name=None)\n    self._l2_regularization_weight = l2_regularization_weight\n    self._value_layer = tf.keras.layers.Dense(\n        1,\n        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n        kernel_initializer=tf.compat.v1.initializers.constant([[0], [1]]),\n        bias_initializer=tf.compat.v1.initializers.constant([[0]]))\n    self._shared_layer = shared_layer\n    self._action_layer = tf.keras.layers.Dense(\n        1,\n        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n        kernel_initializer=tf.compat.v1.initializers.constant([[1]]),\n        bias_initializer=tf.compat.v1.initializers.constant([[0]]))\n\n  def copy(self, name=\'\'):\n    del name\n    return DummyCriticNet(\n        l2_regularization_weight=self._l2_regularization_weight,\n        shared_layer=self._shared_layer)\n\n  def call(self, inputs, step_type, network_state=()):\n    del step_type\n    observation, actions = inputs\n    actions = tf.cast(tf.nest.flatten(actions)[0], tf.float32)\n\n    states = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n\n    s_value = self._value_layer(states)\n    if self._shared_layer:\n      s_value = self._shared_layer(s_value)\n    a_value = self._action_layer(actions)\n    # Biggest state is best state.\n    q_value = tf.reshape(s_value + a_value, [-1])\n    return q_value, network_state\n\n\nclass SacAgentTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(SacAgentTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n\n  def testCreateAgent(self):\n    sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        actor_policy_ctor=DummyActorPolicy)\n\n  def testCriticLoss(self):\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        actor_policy_ctor=DummyActorPolicy)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.constant([[5], [6]], dtype=tf.float32)\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    td_targets = [7.3, 19.1]\n    pred_td_targets = [7., 10.]\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Expected critic loss has factor of 2, for the two TD3 critics.\n    expected_loss = self.evaluate(2 * tf.compat.v1.losses.mean_squared_error(\n        tf.constant(td_targets), tf.constant(pred_td_targets)))\n\n    loss = agent.critic_loss(\n        time_steps,\n        actions,\n        next_time_steps,\n        td_errors_loss_fn=tf.math.squared_difference)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testCriticRegLoss(self):\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(0.5),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        actor_policy_ctor=DummyActorPolicy)\n\n    observations = tf.zeros((2, 2), dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = tf.zeros((2, 1), dtype=tf.float32)\n\n    rewards = tf.zeros((2,), dtype=tf.float32)\n    discounts = tf.zeros((2,), dtype=tf.float32)\n    next_observations = tf.zeros((2, 2), dtype=tf.float32)\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    # Expected loss only regularization loss.\n    expected_loss = 2.0\n\n    loss = agent.critic_loss(\n        time_steps,\n        actions,\n        next_time_steps,\n        td_errors_loss_fn=tf.math.squared_difference)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testActorLoss(self):\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        actor_policy_ctor=DummyActorPolicy)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    expected_loss = (2 * 10 - (2 + 1) - (4 + 1)) / 2\n    loss = agent.actor_loss(time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testAlphaLoss(self):\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        target_entropy=3.0,\n        initial_log_alpha=4.0,\n        actor_policy_ctor=DummyActorPolicy)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n\n    expected_loss = 4.0 * (-10 - 3)\n    loss = agent.alpha_loss(time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testPolicy(self):\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=DummyCriticNet(),\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        actor_policy_ctor=DummyActorPolicy)\n\n    observations = tf.constant([[1, 2]], dtype=tf.float32)\n    time_steps = ts.restart(observations)\n    action_step = agent.policy.action(time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action_ = self.evaluate(action_step.action)\n    self.assertLessEqual(action_, self._action_spec.maximum)\n    self.assertGreaterEqual(action_, self._action_spec.minimum)\n\n  def testTrainWithRnn(self):\n    actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        self._obs_spec,\n        self._action_spec,\n        input_fc_layer_params=None,\n        output_fc_layer_params=None,\n        conv_layer_params=None,\n        lstm_size=(40,),\n    )\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        (self._obs_spec, self._action_spec),\n        observation_fc_layer_params=(16,),\n        action_fc_layer_params=(16,),\n        joint_fc_layer_params=(16,),\n        lstm_size=(16,),\n        output_fc_layer_params=None,\n    )\n\n    counter = common.create_variable(\'test_train_counter\')\n\n    optimizer_fn = tf.compat.v1.train.AdamOptimizer\n\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=critic_net,\n        actor_network=actor_net,\n        actor_optimizer=optimizer_fn(1e-3),\n        critic_optimizer=optimizer_fn(1e-3),\n        alpha_optimizer=optimizer_fn(1e-3),\n        train_step_counter=counter,\n    )\n\n    batch_size = 5\n    observations = tf.constant(\n        [[[1, 2], [3, 4], [5, 6]]] * batch_size, dtype=tf.float32)\n    actions = tf.constant([[[0], [1], [1]]] * batch_size, dtype=tf.float32)\n    time_steps = ts.TimeStep(\n        step_type=tf.constant([[1] * 3] * batch_size, dtype=tf.int32),\n        reward=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        discount=tf.constant([[1] * 3] * batch_size, dtype=tf.float32),\n        observation=observations)\n\n    experience = trajectory.Trajectory(\n        time_steps.step_type, observations, actions, (),\n        time_steps.step_type, time_steps.reward, time_steps.discount)\n\n    # Force variable creation.\n    agent.policy.variables()\n\n    if not tf.executing_eagerly():\n      # Get experience first to make sure optimizer variables are created and\n      # can be initialized.\n      experience = agent.train(experience)\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n      self.assertEqual(self.evaluate(counter), 0)\n      self.evaluate(experience)\n      self.assertEqual(self.evaluate(counter), 1)\n    else:\n      self.assertEqual(self.evaluate(counter), 0)\n      self.evaluate(agent.train(experience))\n      self.assertEqual(self.evaluate(counter), 1)\n\n  def testSharedLayer(self):\n    shared_layer = tf.keras.layers.Dense(\n        1,\n        kernel_initializer=tf.compat.v1.initializers.constant([0]),\n        bias_initializer=tf.compat.v1.initializers.constant([0]),\n        name=\'shared\')\n\n    critic_net_1 = DummyCriticNet(shared_layer=shared_layer)\n    critic_net_2 = DummyCriticNet(shared_layer=shared_layer)\n\n    target_shared_layer = tf.keras.layers.Dense(\n        1,\n        kernel_initializer=tf.compat.v1.initializers.constant([0]),\n        bias_initializer=tf.compat.v1.initializers.constant([0]),\n        name=\'shared_target\')\n\n    target_critic_net_1 = DummyCriticNet(shared_layer=target_shared_layer)\n    target_critic_net_2 = DummyCriticNet(shared_layer=target_shared_layer)\n\n    agent = sac_agent.SacAgent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=critic_net_1,\n        critic_network_2=critic_net_2,\n        target_critic_network=target_critic_net_1,\n        target_critic_network_2=target_critic_net_2,\n        actor_network=None,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        alpha_optimizer=None,\n        target_entropy=3.0,\n        initial_log_alpha=4.0,\n        target_update_tau=0.5,\n        actor_policy_ctor=DummyActorPolicy)\n\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.local_variables_initializer()\n    ])\n\n    self.evaluate(agent.initialize())\n\n    for v in shared_layer.variables:\n      self.evaluate(v.assign(v * 0 + 1))\n\n    self.evaluate(agent._update_target())\n\n    self.assertEqual(1.0, self.evaluate(shared_layer.variables[0][0][0]))\n    self.assertEqual(0.5, self.evaluate(target_shared_layer.variables[0][0][0]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/sac/tanh_normal_projection_network.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Project inputs to a tanh-squashed MultivariateNormalDiag distribution.\n\nThis network reproduces Soft Actor-Critic refererence implementation in:\nhttps://github.com/rail-berkeley/softlearning/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Callable, Optional, Text\n\nimport gin\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom tf_agents.distributions import utils as distribution_utils\nfrom tf_agents.networks import network\nfrom tf_agents.networks import utils as network_utils\nfrom tf_agents.specs import distribution_spec\nfrom tf_agents.specs import tensor_spec\n\nfrom tf_agents.typing import types\n\n\ndef tanh_squash_to_spec(inputs: types.Tensor,\n                        spec: types.TensorSpec) -> types.Tensor:\n  """"""Maps inputs with arbitrary range to range defined by spec using `tanh`.""""""\n  means = (spec.maximum + spec.minimum) / 2.0\n  magnitudes = (spec.maximum - spec.minimum) / 2.0\n\n  return means + magnitudes * tf.tanh(inputs)\n\n\n@gin.configurable\nclass TanhNormalProjectionNetwork(network.DistributionNetwork):\n  """"""Generates a tanh-squashed MultivariateNormalDiag distribution.\n\n  Note: This network uses `tanh_squash_to_spec` to normalize its\n  output. Due to the nature of the `tanh` function, values near the spec bounds\n  cannot be returned.\n  """"""\n\n  def __init__(self,\n               sample_spec: types.TensorSpec,\n               activation_fn: Optional[Callable[[types.Tensor],\n                                                types.Tensor]] = None,\n               std_transform: Optional[Callable[[types.Tensor],\n                                                types.Tensor]] = tf.exp,\n               name: Text = \'TanhNormalProjectionNetwork\'):\n    """"""Creates an instance of TanhNormalProjectionNetwork.\n\n    Args:\n      sample_spec: A `tensor_spec.BoundedTensorSpec` detailing the shape and\n        dtypes of samples pulled from the output distribution.\n      activation_fn: Activation function to use in dense layer.\n      std_transform: Transformation function to apply to the stddevs.\n      name: A string representing name of the network.\n    """"""\n    if len(tf.nest.flatten(sample_spec)) != 1:\n      raise ValueError(\'Tanh Normal Projection network only supports single\'\n                       \' spec samples.\')\n    output_spec = self._output_distribution_spec(sample_spec, name)\n    super(TanhNormalProjectionNetwork, self).__init__(\n        # We don\'t need these, but base class requires them.\n        input_tensor_spec=None,\n        state_spec=(),\n        output_spec=output_spec,\n        name=name)\n\n    self._sample_spec = sample_spec\n    self._std_transform = std_transform\n\n    self._projection_layer = tf.keras.layers.Dense(\n        sample_spec.shape.num_elements() * 2,\n        activation=activation_fn,\n        name=\'projection_layer\')\n\n  def _output_distribution_spec(self, sample_spec, network_name):\n    input_param_shapes = {\n        \'loc\': sample_spec.shape,\n        \'scale_diag\': sample_spec.shape\n    }\n    input_param_spec = {\n        name: tensor_spec.TensorSpec(  # pylint: disable=g-complex-comprehension\n            shape=shape,\n            dtype=sample_spec.dtype,\n            name=network_name + \'_\' + name)\n        for name, shape in input_param_shapes.items()\n    }\n\n    def distribution_builder(*args, **kwargs):\n      distribution = tfp.distributions.MultivariateNormalDiag(*args, **kwargs)\n      return distribution_utils.scale_distribution_to_spec(\n          distribution, sample_spec)\n\n    return distribution_spec.DistributionSpec(\n        distribution_builder, input_param_spec, sample_spec=sample_spec)\n\n  def call(self,\n           inputs: types.NestedTensor,\n           outer_rank: int,\n           training: bool = False,\n           mask: Optional[types.NestedTensor] = None) -> types.NestedTensor:\n    if inputs.dtype != self._sample_spec.dtype:\n      raise ValueError(\'Inputs to TanhNormalProjectionNetwork must match the \'\n                       \'sample_spec.dtype.\')\n\n    if mask is not None:\n      raise NotImplementedError(\n          \'TanhNormalProjectionNetwork does not yet implement action masking; \'\n          \'got mask={}\'.format(mask))\n\n    # outer_rank is needed because the projection is not done on the raw\n    # observations so getting the outer rank is hard as there is no spec to\n    # compare to.\n    batch_squash = network_utils.BatchSquash(outer_rank)\n    inputs = batch_squash.flatten(inputs)\n\n    means_and_stds = self._projection_layer(inputs, training=training)\n    means, stds = tf.split(means_and_stds, num_or_size_splits=2, axis=-1)\n    means = tf.reshape(means, [-1] + self._sample_spec.shape.as_list())\n    means = tf.cast(means, self._sample_spec.dtype)\n\n    if self._std_transform is not None:\n      stds = self._std_transform(stds)\n    stds = tf.cast(stds, self._sample_spec.dtype)\n\n    means = batch_squash.unflatten(means)\n    stds = batch_squash.unflatten(stds)\n\n    return self.output_spec.build_distribution(loc=means, scale_diag=stds), ()\n'"
tf_agents/agents/sac/tanh_normal_projection_network_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.networks.normal_projection_network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents.sac import tanh_normal_projection_network\nfrom tf_agents.specs import tensor_spec\n\n\ndef _get_inputs(batch_size, num_input_dims):\n  return tf.random.uniform([batch_size, num_input_dims])\n\n\nclass TanhNormalProjectionNetworkTest(tf.test.TestCase):\n\n  def testBuild(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = tanh_normal_projection_network.TanhNormalProjectionNetwork(\n        output_spec)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    distribution, _ = network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(tfp.distributions.MultivariateNormalDiag,\n                     type(distribution.input_distribution))\n\n    means = distribution.input_distribution.loc\n    stds = distribution.input_distribution.scale\n\n    self.assertAllEqual(means.shape.as_list(),\n                        [3] + output_spec.shape.as_list())\n    self.assertAllEqual(stds.shape.as_list(),\n                        [3] + output_spec.shape.as_list()*2)\n\n  def testTrainableVariables(self):\n    output_spec = tensor_spec.BoundedTensorSpec([2], tf.float32, 0, 1)\n    network = tanh_normal_projection_network.TanhNormalProjectionNetwork(\n        output_spec)\n\n    inputs = _get_inputs(batch_size=3, num_input_dims=5)\n\n    network(inputs, outer_rank=1)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Dense kernel and bias.\n    self.assertEqual(2, len(network.trainable_variables))\n    self.assertEqual((5, 4), network.trainable_variables[0].shape)\n    self.assertEqual((4,), network.trainable_variables[1].shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/agents/td3/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Twin Delayed Deep Deterministic policy gradient (TD3) agent.""""""\nfrom tf_agents.agents.td3 import td3_agent\n\n'"
tf_agents/agents/td3/td3_agent.py,57,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Twin Delayed Deep Deterministic policy gradient (TD3) agent.\n\nTD3 extends DDPG by adding an extra critic network and using the minimum of the\ntwo critic values to reduce overestimation bias.\n\n""Addressing Function Approximation Error in Actor-Critic Methods""\nby Fujimoto et al.\n\nFor the full paper, see https://arxiv.org/abs/1802.09477.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nimport collections\nfrom typing import Optional, Text\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.networks import network\nfrom tf_agents.policies import actor_policy\nfrom tf_agents.policies import gaussian_policy\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.typing import types\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.utils import object_identity\n\n\nclass Td3Info(collections.namedtuple(\n    \'Td3Info\', (\'actor_loss\', \'critic_loss\'))):\n  pass\n\n\n@gin.configurable\nclass Td3Agent(tf_agent.TFAgent):\n  """"""A TD3 Agent.""""""\n\n  def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedTensor,\n               actor_network: network.Network,\n               critic_network: network.Network,\n               actor_optimizer: types.Optimizer,\n               critic_optimizer: types.Optimizer,\n               exploration_noise_std: types.Float = 0.1,\n               critic_network_2: Optional[network.Network] = None,\n               target_actor_network: Optional[network.Network] = None,\n               target_critic_network: Optional[network.Network] = None,\n               target_critic_network_2: Optional[network.Network] = None,\n               target_update_tau: types.Float = 1.0,\n               target_update_period: types.Int = 1,\n               actor_update_period: types.Int = 1,\n               dqda_clipping: Optional[types.Float] = None,\n               td_errors_loss_fn: Optional[types.LossFn] = None,\n               gamma: types.Float = 1.0,\n               reward_scale_factor: types.Float = 1.0,\n               target_policy_noise: types.Float = 0.2,\n               target_policy_noise_clip: types.Float = 0.5,\n               gradient_clipping: Optional[types.Float] = None,\n               debug_summaries: bool = False,\n               summarize_grads_and_vars: bool = False,\n               train_step_counter: Optional[tf.Variable] = None,\n               name: Text = None):\n    """"""Creates a Td3Agent Agent.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      actor_network: A tf_agents.network.Network to be used by the agent. The\n        network will be called with call(observation, step_type).\n      critic_network: A tf_agents.network.Network to be used by the agent. The\n        network will be called with call(observation, action, step_type).\n      actor_optimizer: The default optimizer to use for the actor network.\n      critic_optimizer: The default optimizer to use for the critic network.\n      exploration_noise_std: Scale factor on exploration policy noise.\n      critic_network_2: (Optional.)  A `tf_agents.network.Network` to be used as\n        the second critic network during Q learning.  The weights from\n        `critic_network` are copied if this is not provided.\n      target_actor_network: (Optional.)  A `tf_agents.network.Network` to be\n        used as the target actor network during Q learning. Every\n        `target_update_period` train steps, the weights from `actor_network` are\n        copied (possibly withsmoothing via `target_update_tau`) to `\n        target_actor_network`.  If `target_actor_network` is not provided, it is\n        created by making a copy of `actor_network`, which initializes a new\n        network with the same structure and its own layers and weights.\n        Performing a `Network.copy` does not work when the network instance\n        already has trainable parameters (e.g., has already been built, or when\n        the network is sharing layers with another).  In these cases, it is up\n        to you to build a copy having weights that are not shared with the\n        original `actor_network`, so that this can be used as a target network.\n        If you provide a `target_actor_network` that shares any weights with\n        `actor_network`, a warning will be logged but no exception is thrown.\n      target_critic_network: (Optional.) Similar network as target_actor_network\n        but for the critic_network. See documentation for target_actor_network.\n      target_critic_network_2: (Optional.) Similar network as\n        target_actor_network but for the critic_network_2. See documentation for\n        target_actor_network. Will only be used if \'critic_network_2\' is also\n        specified.\n      target_update_tau: Factor for soft update of the target networks.\n      target_update_period: Period for soft update of the target networks.\n      actor_update_period: Period for the optimization step on actor network.\n      dqda_clipping: A scalar or float clips the gradient dqda element-wise\n        between [-dqda_clipping, dqda_clipping]. Default is None representing no\n        clippiing.\n      td_errors_loss_fn:  A function for computing the TD errors loss. If None,\n        a default value of elementwise huber_loss is used.\n      gamma: A discount factor for future rewards.\n      reward_scale_factor: Multiplicative scale for the reward.\n      target_policy_noise: Scale factor on target action noise\n      target_policy_noise_clip: Value to clip noise.\n      gradient_clipping: Norm length to clip gradients.\n      debug_summaries: A bool to gather debug summaries.\n      summarize_grads_and_vars: If True, gradient and network variable summaries\n        will be written during training.\n      train_step_counter: An optional counter to increment every time the train\n        op is run.  Defaults to the global_step.\n      name: The name of this agent. All variables in this module will fall\n        under that name. Defaults to the class name.\n    """"""\n    tf.Module.__init__(self, name=name)\n    self._actor_network = actor_network\n    actor_network.create_variables()\n    if target_actor_network:\n      target_actor_network.create_variables()\n    self._target_actor_network = common.maybe_copy_target_network_with_checks(\n        self._actor_network, target_actor_network, \'TargetActorNetwork\')\n\n    self._critic_network_1 = critic_network\n    critic_network.create_variables()\n    if target_critic_network:\n      target_critic_network.create_variables()\n    self._target_critic_network_1 = (\n        common.maybe_copy_target_network_with_checks(self._critic_network_1,\n                                                     target_critic_network,\n                                                     \'TargetCriticNetwork1\'))\n\n    if critic_network_2 is not None:\n      self._critic_network_2 = critic_network_2\n    else:\n      self._critic_network_2 = critic_network.copy(name=\'CriticNetwork2\')\n      # Do not use target_critic_network_2 if critic_network_2 is None.\n      target_critic_network_2 = None\n    self._critic_network_2.create_variables()\n    if target_critic_network_2:\n      target_critic_network_2.create_variables()\n    self._target_critic_network_2 = (\n        common.maybe_copy_target_network_with_checks(self._critic_network_2,\n                                                     target_critic_network_2,\n                                                     \'TargetCriticNetwork2\'))\n\n    self._actor_optimizer = actor_optimizer\n    self._critic_optimizer = critic_optimizer\n\n    self._exploration_noise_std = exploration_noise_std\n    self._target_update_tau = target_update_tau\n    self._target_update_period = target_update_period\n    self._actor_update_period = actor_update_period\n    self._dqda_clipping = dqda_clipping\n    self._td_errors_loss_fn = (\n        td_errors_loss_fn or common.element_wise_huber_loss)\n    self._gamma = gamma\n    self._reward_scale_factor = reward_scale_factor\n    self._target_policy_noise = target_policy_noise\n    self._target_policy_noise_clip = target_policy_noise_clip\n    self._gradient_clipping = gradient_clipping\n\n    self._update_target = self._get_target_updater(\n        target_update_tau, target_update_period)\n\n    policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec,\n        actor_network=self._actor_network, clip=True)\n    collect_policy = actor_policy.ActorPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec,\n        actor_network=self._actor_network, clip=False)\n    collect_policy = gaussian_policy.GaussianPolicy(\n        collect_policy,\n        scale=self._exploration_noise_std,\n        clip=True)\n\n    super(Td3Agent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy,\n        train_sequence_length=2 if not self._actor_network.state_spec else None,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  def _initialize(self):\n    """"""Initialize the agent.\n\n    Copies weights from the actor and critic networks to the respective\n    target actor and critic networks.\n    """"""\n    common.soft_variables_update(\n        self._critic_network_1.variables,\n        self._target_critic_network_1.variables,\n        tau=1.0)\n    common.soft_variables_update(\n        self._critic_network_2.variables,\n        self._target_critic_network_2.variables,\n        tau=1.0)\n    common.soft_variables_update(\n        self._actor_network.variables,\n        self._target_actor_network.variables,\n        tau=1.0)\n\n  def _get_target_updater(self, tau=1.0, period=1):\n    """"""Performs a soft update of the target network parameters.\n\n    For each weight w_s in the original network, and its corresponding\n    weight w_t in the target network, a soft update is:\n    w_t = (1- tau) x w_t + tau x ws\n\n    Args:\n      tau: A float scalar in [0, 1]. Default `tau=1.0` means hard update.\n      period: Step interval at which the target networks are updated.\n\n    Returns:\n      A callable that performs a soft update of the target network parameters.\n    """"""\n    with tf.name_scope(\'update_targets\'):\n\n      def update():  # pylint: disable=missing-docstring\n        # TODO(b/124381161): What about observation normalizer variables?\n        critic_update_1 = common.soft_variables_update(\n            self._critic_network_1.variables,\n            self._target_critic_network_1.variables,\n            tau,\n            tau_non_trainable=1.0)\n\n        critic_2_update_vars = common.deduped_network_variables(\n            self._critic_network_2, self._critic_network_1)\n        target_critic_2_update_vars = common.deduped_network_variables(\n            self._target_critic_network_2, self._target_critic_network_1)\n\n        critic_update_2 = common.soft_variables_update(\n            critic_2_update_vars,\n            target_critic_2_update_vars,\n            tau,\n            tau_non_trainable=1.0)\n\n        actor_update_vars = common.deduped_network_variables(\n            self._actor_network, self._critic_network_1, self._critic_network_2)\n        target_actor_update_vars = common.deduped_network_variables(\n            self._target_actor_network, self._target_critic_network_1,\n            self._target_critic_network_2)\n\n        actor_update = common.soft_variables_update(\n            actor_update_vars,\n            target_actor_update_vars,\n            tau,\n            tau_non_trainable=1.0)\n        return tf.group(critic_update_1, critic_update_2, actor_update)\n\n      return common.Periodically(update, period, \'update_targets\')\n\n  def _train(self, experience, weights=None):\n    # TODO(b/120034503): Move the conversion to transitions to the base class.\n    squeeze_time_dim = not self._actor_network.state_spec\n    time_steps, policy_steps, next_time_steps = (\n        trajectory.experience_to_transitions(experience, squeeze_time_dim))\n    actions = policy_steps.action\n\n    trainable_critic_variables = list(object_identity.ObjectIdentitySet(\n        self._critic_network_1.trainable_variables +\n        self._critic_network_2.trainable_variables))\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_critic_variables, (\'No trainable critic variables to \'\n                                          \'optimize.\')\n      tape.watch(trainable_critic_variables)\n      critic_loss = self.critic_loss(time_steps, actions, next_time_steps,\n                                     weights=weights, training=True)\n    tf.debugging.check_numerics(critic_loss, \'Critic loss is inf or nan.\')\n    critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n    self._apply_gradients(critic_grads, trainable_critic_variables,\n                          self._critic_optimizer)\n\n    trainable_actor_variables = self._actor_network.trainable_variables\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n      assert trainable_actor_variables, (\'No trainable actor variables to \'\n                                         \'optimize.\')\n      tape.watch(trainable_actor_variables)\n      actor_loss = self.actor_loss(time_steps, weights=weights, training=True)\n    tf.debugging.check_numerics(actor_loss, \'Actor loss is inf or nan.\')\n\n    # We only optimize the actor every actor_update_period training steps.\n    def optimize_actor():\n      actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n      return self._apply_gradients(actor_grads, trainable_actor_variables,\n                                   self._actor_optimizer)\n\n    remainder = tf.math.mod(self.train_step_counter, self._actor_update_period)\n    tf.cond(\n        pred=tf.equal(remainder, 0), true_fn=optimize_actor, false_fn=tf.no_op)\n\n    self.train_step_counter.assign_add(1)\n    self._update_target()\n\n    # TODO(b/124382360): Compute per element TD loss and return in loss_info.\n    total_loss = actor_loss + critic_loss\n\n    return tf_agent.LossInfo(total_loss,\n                             Td3Info(actor_loss, critic_loss))\n\n  def _apply_gradients(self, gradients, variables, optimizer):\n    # Tuple is used for py3, where zip is a generator producing values once.\n    grads_and_vars = tuple(zip(gradients, variables))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(\n          grads_and_vars, self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      eager_utils.add_variables_summaries(grads_and_vars,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n\n    return optimizer.apply_gradients(grads_and_vars)\n\n  def critic_loss(self,\n                  time_steps: ts.TimeStep,\n                  actions: types.Tensor,\n                  next_time_steps: ts.TimeStep,\n                  weights: Optional[types.Tensor] = None,\n                  training: bool = False) -> types.Tensor:\n    """"""Computes the critic loss for TD3 training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      actions: A batch of actions.\n      next_time_steps: A batch of next timesteps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.\n      training: Whether this loss is being used for training.\n\n    Returns:\n      critic_loss: A scalar critic loss.\n    """"""\n    with tf.name_scope(\'critic_loss\'):\n      target_actions, _ = self._target_actor_network(\n          next_time_steps.observation, next_time_steps.step_type,\n          training=training)\n\n      # Add gaussian noise to each action before computing target q values\n      def add_noise_to_action(action):  # pylint: disable=missing-docstring\n        dist = tfp.distributions.Normal(loc=tf.zeros_like(action),\n                                        scale=self._target_policy_noise * \\\n                                        tf.ones_like(action))\n        noise = dist.sample()\n        noise = tf.clip_by_value(noise, -self._target_policy_noise_clip,\n                                 self._target_policy_noise_clip)\n        return action + noise\n\n      noisy_target_actions = tf.nest.map_structure(add_noise_to_action,\n                                                   target_actions)\n\n      # Target q-values are the min of the two networks\n      target_q_input_1 = (next_time_steps.observation, noisy_target_actions)\n      target_q_values_1, _ = self._target_critic_network_1(\n          target_q_input_1,\n          next_time_steps.step_type,\n          training=False)\n      target_q_input_2 = (next_time_steps.observation, noisy_target_actions)\n      target_q_values_2, _ = self._target_critic_network_2(\n          target_q_input_2,\n          next_time_steps.step_type,\n          training=False)\n      target_q_values = tf.minimum(target_q_values_1, target_q_values_2)\n\n      td_targets = tf.stop_gradient(\n          self._reward_scale_factor * next_time_steps.reward +\n          self._gamma * next_time_steps.discount * target_q_values)\n\n      pred_input_1 = (time_steps.observation, actions)\n      pred_td_targets_1, _ = self._critic_network_1(\n          pred_input_1, time_steps.step_type, training=training)\n      pred_input_2 = (time_steps.observation, actions)\n      pred_td_targets_2, _ = self._critic_network_2(\n          pred_input_2, time_steps.step_type, training=training)\n      pred_td_targets_all = [pred_td_targets_1, pred_td_targets_2]\n\n      if self._debug_summaries:\n        tf.compat.v2.summary.histogram(\n            name=\'td_targets\', data=td_targets, step=self.train_step_counter)\n        with tf.name_scope(\'td_targets\'):\n          tf.compat.v2.summary.scalar(\n              name=\'mean\',\n              data=tf.reduce_mean(input_tensor=td_targets),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              name=\'max\',\n              data=tf.reduce_max(input_tensor=td_targets),\n              step=self.train_step_counter)\n          tf.compat.v2.summary.scalar(\n              name=\'min\',\n              data=tf.reduce_min(input_tensor=td_targets),\n              step=self.train_step_counter)\n\n        for td_target_idx in range(2):\n          pred_td_targets = pred_td_targets_all[td_target_idx]\n          td_errors = td_targets - pred_td_targets\n          with tf.name_scope(\'critic_net_%d\' % (td_target_idx + 1)):\n            tf.compat.v2.summary.histogram(\n                name=\'td_errors\', data=td_errors, step=self.train_step_counter)\n            tf.compat.v2.summary.histogram(\n                name=\'pred_td_targets\',\n                data=pred_td_targets,\n                step=self.train_step_counter)\n            with tf.name_scope(\'td_errors\'):\n              tf.compat.v2.summary.scalar(\n                  name=\'mean\',\n                  data=tf.reduce_mean(input_tensor=td_errors),\n                  step=self.train_step_counter)\n              tf.compat.v2.summary.scalar(\n                  name=\'mean_abs\',\n                  data=tf.reduce_mean(input_tensor=tf.abs(td_errors)),\n                  step=self.train_step_counter)\n              tf.compat.v2.summary.scalar(\n                  name=\'max\',\n                  data=tf.reduce_max(input_tensor=td_errors),\n                  step=self.train_step_counter)\n              tf.compat.v2.summary.scalar(\n                  name=\'min\',\n                  data=tf.reduce_min(input_tensor=td_errors),\n                  step=self.train_step_counter)\n            with tf.name_scope(\'pred_td_targets\'):\n              tf.compat.v2.summary.scalar(\n                  name=\'mean\',\n                  data=tf.reduce_mean(input_tensor=pred_td_targets),\n                  step=self.train_step_counter)\n              tf.compat.v2.summary.scalar(\n                  name=\'max\',\n                  data=tf.reduce_max(input_tensor=pred_td_targets),\n                  step=self.train_step_counter)\n              tf.compat.v2.summary.scalar(\n                  name=\'min\',\n                  data=tf.reduce_min(input_tensor=pred_td_targets),\n                  step=self.train_step_counter)\n\n      critic_loss = (self._td_errors_loss_fn(td_targets, pred_td_targets_1)\n                     + self._td_errors_loss_fn(td_targets, pred_td_targets_2))\n      if nest_utils.is_batched_nested_tensors(\n          time_steps, self.time_step_spec, num_outer_dims=2):\n        # Sum over the time dimension.\n        critic_loss = tf.reduce_sum(input_tensor=critic_loss, axis=1)\n\n      if weights is not None:\n        critic_loss *= weights\n\n      return tf.reduce_mean(input_tensor=critic_loss)\n\n  def actor_loss(self,\n                 time_steps: ts.TimeStep,\n                 weights: types.Tensor = None,\n                 training: bool = False) -> types.Tensor:\n    """"""Computes the actor_loss for TD3 training.\n\n    Args:\n      time_steps: A batch of timesteps.\n      weights: Optional scalar or element-wise (per-batch-entry) importance\n        weights.\n      training: Whether this loss is being used for training.\n      # TODO(b/124383618): Add an action norm regularizer.\n    Returns:\n      actor_loss: A scalar actor loss.\n    """"""\n    with tf.name_scope(\'actor_loss\'):\n      actions, _ = self._actor_network(time_steps.observation,\n                                       time_steps.step_type,\n                                       training=training)\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(actions)\n        q_values, _ = self._critic_network_1((time_steps.observation, actions),\n                                             time_steps.step_type,\n                                             training=False)\n        actions = tf.nest.flatten(actions)\n\n      dqdas = tape.gradient([q_values], actions)\n\n      actor_losses = []\n      for dqda, action in zip(dqdas, actions):\n        if self._dqda_clipping is not None:\n          dqda = tf.clip_by_value(dqda, -1 * self._dqda_clipping,\n                                  self._dqda_clipping)\n        loss = common.element_wise_squared_loss(\n            tf.stop_gradient(dqda + action), action)\n        if nest_utils.is_batched_nested_tensors(\n            time_steps, self.time_step_spec, num_outer_dims=2):\n          # Sum over the time dimension.\n          loss = tf.reduce_sum(loss, axis=1)\n        if weights is not None:\n          loss *= weights\n        loss = tf.reduce_mean(loss)\n        actor_losses.append(loss)\n\n      actor_loss = tf.add_n(actor_losses)\n\n      with tf.name_scope(\'Losses/\'):\n        tf.compat.v2.summary.scalar(\n            name=\'actor_loss\', data=actor_loss, step=self.train_step_counter)\n\n    return actor_loss\n'"
tf_agents/agents/td3/td3_agent_test.py,42,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.td3.td3_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.agents.td3 import td3_agent\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\n\n\nclass DummyActorNetwork(network.Network):\n  """"""Creates an actor network.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               output_tensor_spec,\n               unbounded_actions=False,\n               shared_layer=None,\n               name=None):\n    super(DummyActorNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    self._unbounded_actions = unbounded_actions\n    activation = None if unbounded_actions else tf.keras.activations.tanh\n\n    self._output_tensor_spec = output_tensor_spec\n    self._single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n    self._layer = tf.keras.layers.Dense(\n        self._single_action_spec.shape.num_elements(),\n        activation=activation,\n        kernel_initializer=tf.compat.v1.initializers.constant([2, 1]),\n        bias_initializer=tf.compat.v1.initializers.constant([5]),\n        name=\'action\')\n    self._shared_layer = shared_layer\n\n  def call(self, observations, step_type=(), network_state=()):\n    del step_type  # unused.\n    observations = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n    if self._shared_layer:\n      observations = self._shared_layer(observations)\n    output = self._layer(observations)\n    actions = tf.reshape(output,\n                         [-1] + self._single_action_spec.shape.as_list())\n\n    if not self._unbounded_actions:\n      actions = common.scale_to_spec(actions, self._single_action_spec)\n\n    output_actions = tf.nest.pack_sequence_as(self._output_tensor_spec,\n                                              [actions])\n    return output_actions, network_state\n\n\nclass DummyCriticNetwork(network.Network):\n\n  def __init__(self, input_tensor_spec, shared_layer=None, name=None):\n    super(DummyCriticNetwork, self).__init__(\n        input_tensor_spec, state_spec=(), name=name)\n\n    self._obs_layer = tf.keras.layers.Flatten()\n    self._shared_layer = shared_layer\n    self._action_layer = tf.keras.layers.Flatten()\n    self._joint_layer = tf.keras.layers.Dense(\n        1,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.constant([1, 3, 2]),\n        bias_initializer=tf.compat.v1.initializers.constant([4]))\n\n  def call(self, inputs, step_type=None, network_state=()):\n    observations, actions = inputs\n    del step_type\n    observations = self._obs_layer(tf.nest.flatten(observations)[0])\n    if self._shared_layer:\n      observations = self._shared_layer(observations)\n    actions = self._action_layer(tf.nest.flatten(actions)[0])\n    joint = tf.concat([observations, actions], 1)\n    q_value = self._joint_layer(joint)\n    q_value = tf.reshape(q_value, [-1])\n    return q_value, network_state\n\n\nclass TD3AgentTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(TD3AgentTest, self).setUp()\n    self._obs_spec = [tensor_spec.TensorSpec([2], tf.float32)]\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = [tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)]\n\n    input_spec = (self._obs_spec, self._action_spec)\n    self._critic_net = DummyCriticNetwork(input_spec)\n    self._bounded_actor_net = DummyActorNetwork(\n        self._obs_spec, self._action_spec, unbounded_actions=False)\n    self._unbounded_actor_net = DummyActorNetwork(\n        self._obs_spec, self._action_spec, unbounded_actions=True)\n\n  def testCreateAgent(self):\n    td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=self._critic_net,\n        actor_network=self._bounded_actor_net,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        )\n\n  def testCriticLoss(self):\n    # The loss is now 119.3098526. Investigate this.\n    self.skipTest(\'b/123772477\')\n    agent = td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=self._critic_net,\n        actor_network=self._unbounded_actor_net,\n        actor_optimizer=None,\n        critic_optimizer=None)\n\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n    actions = [tf.constant([[5], [6]], dtype=tf.float32)]\n\n    rewards = tf.constant([10, 20], dtype=tf.float32)\n    discounts = tf.constant([0.9, 0.9], dtype=tf.float32)\n    next_observations = [tf.constant([[5, 6], [7, 8]], dtype=tf.float32)]\n    next_time_steps = ts.transition(next_observations, rewards, discounts)\n\n    # TODO(b/123772477): The loss changed from 119.054 to 118.910903931.\n    expected_loss = 118.9109\n    loss = agent.critic_loss(time_steps, actions, next_time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testActorLoss(self):\n    agent = td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=self._critic_net,\n        actor_network=self._unbounded_actor_net,\n        actor_optimizer=None,\n        critic_optimizer=None)\n\n    observations = [tf.constant([[1, 2], [3, 4]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=2)\n\n    expected_loss = 4.0\n    loss = agent.actor_loss(time_steps)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_ = self.evaluate(loss)\n    self.assertAllClose(loss_, expected_loss)\n\n  def testPolicyProducesBoundedAction(self):\n    agent = td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=self._critic_net,\n        actor_network=self._bounded_actor_net,\n        actor_optimizer=None,\n        critic_optimizer=None)\n\n    observations = [tf.constant([[1, 2]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=1)\n    action = agent.policy.action(time_steps).action[0]\n    self.assertEqual(action.shape.as_list(), [1, 1])\n\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.local_variables_initializer()\n    ])\n    py_action = self.evaluate(action)\n    self.assertTrue(all(py_action <= self._action_spec[0].maximum))\n    self.assertTrue(all(py_action >= self._action_spec[0].minimum))\n\n  def testPolicyAndCollectPolicyProducesDifferentActions(self):\n    agent = td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        critic_network=self._critic_net,\n        actor_network=self._bounded_actor_net,\n        actor_optimizer=None,\n        critic_optimizer=None)\n\n    observations = [tf.constant([[1, 2]], dtype=tf.float32)]\n    time_steps = ts.restart(observations, batch_size=1)\n    action = agent.policy.action(time_steps).action[0]\n    collect_policy_action = agent.collect_policy.action(time_steps).action[0]\n    self.assertEqual(action.shape, collect_policy_action.shape)\n\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.local_variables_initializer()\n    ])\n    py_action, py_collect_policy_action = self.evaluate(\n        [action, collect_policy_action])\n    self.assertNotEqual(py_action, py_collect_policy_action)\n\n  def testSharedLayer(self):\n    input_spec = (self._obs_spec, self._action_spec)\n\n    shared_layer = tf.keras.layers.Dense(\n        2,\n        kernel_initializer=tf.compat.v1.initializers.constant([0]),\n        bias_initializer=tf.compat.v1.initializers.constant([0]),\n        name=\'shared\')\n\n    critic_net_1 = DummyCriticNetwork(input_spec, shared_layer=shared_layer)\n    critic_net_2 = DummyCriticNetwork(input_spec, shared_layer=shared_layer)\n\n    bounded_actor_net = DummyActorNetwork(\n        self._obs_spec,\n        self._action_spec,\n        shared_layer=shared_layer,\n        unbounded_actions=False)\n\n    target_shared_layer = tf.keras.layers.Dense(\n        2,\n        kernel_initializer=tf.compat.v1.initializers.constant([0]),\n        bias_initializer=tf.compat.v1.initializers.constant([0]),\n        name=\'shared\')\n\n    target_critic_net_1 = DummyCriticNetwork(\n        input_spec, shared_layer=target_shared_layer)\n    target_critic_net_2 = DummyCriticNetwork(\n        input_spec, shared_layer=target_shared_layer)\n    target_bounded_actor_net = DummyActorNetwork(\n        self._obs_spec,\n        self._action_spec,\n        shared_layer=target_shared_layer,\n        unbounded_actions=False)\n\n    agent = td3_agent.Td3Agent(\n        self._time_step_spec,\n        self._action_spec,\n        actor_network=bounded_actor_net,\n        critic_network=critic_net_1,\n        critic_network_2=critic_net_2,\n        target_actor_network=target_bounded_actor_net,\n        target_critic_network=target_critic_net_1,\n        target_critic_network_2=target_critic_net_2,\n        actor_optimizer=None,\n        critic_optimizer=None,\n        target_update_tau=0.5)\n\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.local_variables_initializer()\n    ])\n\n    self.evaluate(agent.initialize())\n\n    for v in shared_layer.variables:\n      self.evaluate(v.assign(v * 0 + 1))\n\n    self.evaluate(agent._update_target())\n\n    self.assertEqual(1.0, self.evaluate(shared_layer.variables[0][0][0]))\n    self.assertEqual(0.5, self.evaluate(target_shared_layer.variables[0][0][0]))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all agents.""""""\n\nfrom tf_agents.bandits.agents import dropout_thompson_sampling_agent\nfrom tf_agents.bandits.agents import exp3_agent\nfrom tf_agents.bandits.agents import exp3_mixture_agent\nfrom tf_agents.bandits.agents import greedy_reward_prediction_agent\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent\nfrom tf_agents.bandits.agents import mixture_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.agents import static_mixture_agent\nfrom tf_agents.bandits.agents import utils\n'"
tf_agents/bandits/agents/constraints.py,20,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An API for representing constraints.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nimport functools\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import loss_utils\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nclass BaseConstraint(tf.Module):\n  """"""Abstract base class for representing constraints.\n\n  The constraint class provides feasibility computation functionality for\n  computing the probability of actions being feasible.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      name=None):\n    """"""Initialization of the BaseConstraint class.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      name: Python str name of this constraint.\n    """"""\n    super(BaseConstraint, self).__init__(name=name)\n    if not isinstance(time_step_spec, ts.TimeStep):\n      raise ValueError(\n          \'The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.\'\n          .format(type(time_step_spec)))\n\n    self._time_step_spec = time_step_spec\n    self._action_spec = action_spec\n\n  # Subclasses must implement these methods.\n  @abc.abstractmethod\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n\n\nclass NeuralConstraint(BaseConstraint):\n  """"""Class for representing a trainable constraint using a neural network.\n\n  This constraint class uses a neural network to compute the action feasibility.\n  In this case, the loss function needs to be exposed for training the neural\n  network weights, typically done by the agent that uses this constraint.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      constraint_network,\n      error_loss_fn=tf.compat.v1.losses.mean_squared_error,\n      name=\'NeuralConstraint\'):\n    """"""Creates a trainable constraint using a neural network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      constraint_network: An instance of `tf_agents.network.Network` used to\n        provide estimates of action feasibility. The input structure should be\n        consistent with the `observation_spec`.\n      error_loss_fn: A function for computing the loss used to train the\n        constraint network. The default is `tf.losses.mean_squared_error`.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n    """"""\n    super(NeuralConstraint, self).__init__(\n        time_step_spec,\n        action_spec,\n        name)\n\n    self._num_actions = bandit_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n\n    with self.name_scope:\n      constraint_network.create_variables()\n    self._constraint_network = constraint_network\n    self._error_loss_fn = error_loss_fn\n\n  def initialize(self):\n    """"""Returns an op to initialize the constraint.""""""\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def compute_loss(\n      self, observations, actions, rewards, weights=None, training=False):\n    """"""Computes loss for training the constraint network.\n\n    Args:\n      observations: A batch of observations.\n      actions: A batch of actions.\n      rewards: A batch of rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output batch loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether the loss is being used for training.\n\n    Returns:\n      loss: A `Tensor` containing the loss for the training step.\n    """"""\n    with tf.name_scope(\'constraint_loss\'):\n      sample_weights = weights if weights else 1\n      predicted_values, _ = self._constraint_network(\n          observations, training=training)\n      action_predicted_values = common.index_with_actions(\n          predicted_values,\n          tf.cast(actions, dtype=tf.int32))\n      loss = self._error_loss_fn(\n          rewards,\n          action_predicted_values,\n          sample_weights,\n          reduction=tf.compat.v1.losses.Reduction.MEAN)\n      return loss\n\n  # Subclasses can override this function.\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    batch_dims = nest_utils.get_outer_shape(\n        observation, self._time_step_spec.observation)\n    shape = tf.concat([batch_dims, tf.constant(\n        self._num_actions, shape=[1], dtype=batch_dims.dtype)], axis=-1)\n    return tf.ones(shape)\n\n\nclass QuantileConstraint(NeuralConstraint):\n  """"""Class for representing a trainable quantile constraint.\n\n  This constraint class implements a quantile constraint such as\n  ```\n  Q_tau(x) >= v\n  ```\n  or\n  ```\n  Q_tau(x) <= v\n  ```\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      constraint_network,\n      quantile=0.5,\n      comparator_fn=tf.greater,\n      quantile_value=0.0,\n      name=\'QuantileConstraint\'):\n    """"""Creates a trainable quantile constraint using a neural network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      constraint_network: An instance of `tf_agents.network.Network` used to\n        provide estimates of action feasibility.  The input structure should be\n        consistent with the `observation_spec`.\n      quantile: A float between 0. and 1., the quantile we want to regress.\n      comparator_fn: a comparator function, such as tf.greater or tf.less.\n      quantile_value: the desired bound (float) we want to enforce on the\n        quantile.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n    """"""\n    self._quantile_value = quantile_value\n    self._comparator_fn = comparator_fn\n    self._error_loss_fn = functools.partial(\n        loss_utils.pinball_loss,\n        quantile=quantile)\n\n    super(QuantileConstraint, self).__init__(\n        time_step_spec,\n        action_spec,\n        constraint_network,\n        error_loss_fn=self._error_loss_fn,\n        name=name)\n\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    predicted_quantiles, _ = self._constraint_network(\n        observation, training=False)\n    is_satisfied = self._comparator_fn(\n        predicted_quantiles, self._quantile_value)\n    return tf.cast(is_satisfied, tf.float32)\n\n\nclass RelativeQuantileConstraint(NeuralConstraint):\n  """"""Class for representing a trainable relative quantile constraint.\n\n  This constraint class implements a relative quantile constraint such as\n  ```\n  Q_tau(action) >= Q_tau(baseline_action)\n  ```\n  or\n  ```\n  Q_tau(action) <= Q_tau(baseline_action)\n  ```\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      constraint_network,\n      quantile=0.5,\n      comparator_fn=tf.greater,\n      baseline_action_fn=None,\n      name=\'RelativeQuantileConstraint\'):\n    """"""Creates a trainable relative quantile constraint using a neural network.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      constraint_network: An instance of `tf_agents.network.Network` used to\n        provide estimates of action feasibility.  The input structure should be\n        consistent with the `observation_spec`.\n      quantile: A float between 0. and 1., the quantile we want to regress.\n      comparator_fn: a comparator function, such as tf.greater or tf.less.\n      baseline_action_fn: a callable that given the observation returns the\n         baseline action. If None, the baseline action is set to 0.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n    """"""\n    self._baseline_action_fn = baseline_action_fn\n    self._comparator_fn = comparator_fn\n    self._error_loss_fn = functools.partial(\n        loss_utils.pinball_loss,\n        quantile=quantile)\n\n    super(RelativeQuantileConstraint, self).__init__(\n        time_step_spec,\n        action_spec,\n        constraint_network,\n        error_loss_fn=self._error_loss_fn,\n        name=name)\n\n  def _reshape_tensor(self, input_tensor, to_shape):\n    input_tensor = tf.reshape(input_tensor, [-1, 1])\n    return tf.broadcast_to(input_tensor, to_shape)\n\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    predicted_quantiles, _ = self._constraint_network(\n        observation, training=False)\n    batch_dims = nest_utils.get_outer_shape(\n        observation, self._time_step_spec.observation)\n\n    if self._baseline_action_fn is not None:\n      baseline_action = self._baseline_action_fn(observation)\n      baseline_action.shape.assert_is_compatible_with(batch_dims)\n    else:\n      baseline_action = tf.zeros(batch_dims, dtype=tf.int32)\n\n    predicted_quantiles_for_baseline_actions = common.index_with_actions(\n        predicted_quantiles,\n        tf.cast(baseline_action, dtype=tf.int32))\n    predicted_quantiles_for_baseline_actions = self._reshape_tensor(\n        predicted_quantiles_for_baseline_actions, tf.shape(predicted_quantiles))\n    is_satisfied = self._comparator_fn(\n        predicted_quantiles, predicted_quantiles_for_baseline_actions)\n    return tf.cast(is_satisfied, tf.float32)\n'"
tf_agents/bandits/agents/constraints_test.py,42,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.constraints.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import constraints\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\n\ntf.compat.v1.enable_v2_behavior()\n\n\nclass GreaterThan2Constraint(constraints.BaseConstraint):\n\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    if actions is None:\n      actions = tf.range(self._action_spec.minimum, self._action_spec.maximum)\n    feasibility_prob = tf.cast(tf.greater(actions, 2), tf.float32)\n    return feasibility_prob\n\n\nclass BaseConstraintTest(tf.test.TestCase):\n\n  def testSimpleCase(self):\n    obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=5)\n    gt2c = GreaterThan2Constraint(time_step_spec, action_spec)\n    feasibility_prob = gt2c(observation=None)\n    self.assertAllEqual([0, 0, 0, 1, 1], self.evaluate(feasibility_prob))\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, unused_observation_spec, action_spec, name=None):\n    super(DummyNet, self).__init__(\n        unused_observation_spec, state_spec=(), name=name)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[1, 1.5, 2],\n                 [1, 1.5, 4]]),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                [[1], [1], [-10]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass NeuralConstraintTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(NeuralConstraintTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n    self._observation_spec = self._time_step_spec.observation\n\n  def testCreateConstraint(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n  def testInitializeConstraint(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n    init_op = neural_constraint.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n\n  def testComputeLoss(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    rewards = tf.constant([0.5, 3.0], dtype=tf.float32)\n\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n    init_op = neural_constraint.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n    loss = neural_constraint.compute_loss(\n        observations,\n        actions,\n        rewards)\n    self.assertAllClose(self.evaluate(loss), 42.25)\n\n  def testComputeLossWithArmFeatures(self):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        global_dim=2, per_arm_dim=3, max_num_actions=3)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    constraint_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec,\n            global_layers=(4,),\n            arm_layers=(4,),\n            common_layers=(4,)))\n    neural_constraint = constraints.NeuralConstraint(\n        time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(18), shape=[2, 3, 3]), dtype=tf.float32)\n    }\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    rewards = tf.constant([0.5, 3.0], dtype=tf.float32)\n\n    init_op = neural_constraint.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n    loss = neural_constraint.compute_loss(\n        observations,\n        actions,\n        rewards)\n    self.assertGreater(self.evaluate(loss), 0.0)\n\n  def testComputeActionFeasibility(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n    init_op = neural_constraint.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    feasibility_prob = neural_constraint(observation)\n    self.assertAllClose(self.evaluate(feasibility_prob), np.ones([2, 3]))\n\n\nclass QuantileConstraintTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(QuantileConstraintTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n    self._observation_spec = self._time_step_spec.observation\n\n  def testCreateConstraint(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    constraints.QuantileConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n  def testComputeActionFeasibility(self):\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n\n    quantile_constraint = constraints.QuantileConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n    init_op = quantile_constraint.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    feasibility_prob = quantile_constraint(observation)\n    self.assertAllGreaterEqual(self.evaluate(feasibility_prob), 0.0)\n    self.assertAllLessEqual(self.evaluate(feasibility_prob), 1.0)\n\n\nclass RelativeQuantileConstraintTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(RelativeQuantileConstraintTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n\n  def testComputeActionFeasibilityNoBaselineActionFn(self):\n    constraint_net = DummyNet(self._obs_spec, self._action_spec)\n    quantile_constraint = constraints.RelativeQuantileConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net,\n        baseline_action_fn=None)\n    init_op = quantile_constraint.initialize()\n    self.evaluate(init_op)\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    feasibility_prob = quantile_constraint(observation)\n    self.assertAllEqual(self.evaluate(feasibility_prob),\n                        np.array([[0.0, 1.0, 0.0], [0.0, 1.0, 1.0]]))\n\n  def testComputeActionFeasibility(self):\n    constraint_net = DummyNet(self._obs_spec, self._action_spec)\n    baseline_action_fn = lambda _: tf.constant([1, 1], dtype=tf.int32)\n    quantile_constraint = constraints.RelativeQuantileConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net,\n        baseline_action_fn=baseline_action_fn)\n    init_op = quantile_constraint.initialize()\n    self.evaluate(init_op)\n\n    observation = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    feasibility_prob = quantile_constraint(observation)\n    self.assertAllEqual(self.evaluate(feasibility_prob),\n                        np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 1.0]]))\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/dropout_thompson_sampling_agent.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A neural network based agent that implements Thompson sampling via dropout.\n\nImplements an agent based on a neural network that predicts arm rewards.\nThe neural network internally uses dropout to approximate Thompson sampling.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import greedy_reward_prediction_agent\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\nfrom tf_agents.networks import q_network\n\n\n# TODO(b/146206372): refactor DropoutThompsonSamplingAgent API to be compliant\n# with other APIs which take a reward network at initialisation\n@gin.configurable\nclass DropoutThompsonSamplingAgent(\n    greedy_reward_prediction_agent.GreedyRewardPredictionAgent):\n  """"""A neural network based Thompson sampling agent.\n\n  This agent receives parameters for a neural network and trains it to predict\n  rewards. The action is chosen greedily with respect to the prediction.\n  The neural network implements dropout for exploration.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      optimizer,\n      # Network params.\n      dropout_rate,\n      network_layers,\n      dropout_only_top_layer=True,\n      observation_and_action_constraint_splitter=None,\n      constraints=(),\n      # Params for training.\n      error_loss_fn=tf.compat.v1.losses.mean_squared_error,\n      gradient_clipping=None,\n      heteroscedastic=False,\n      # Params for debugging.\n      debug_summaries=False,\n      summarize_grads_and_vars=False,\n      enable_summaries=True,\n      emit_policy_info=(),\n      train_step_counter=None,\n      laplacian_matrix=None,\n      laplacian_smoothing_weight=0.001,\n      name=None):\n    """"""Creates a Dropout Thompson Sampling Agent.\n\n    For more details about the Laplacian smoothing regularization, please see\n    the documentation of the `GreedyRewardPredictionAgent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      optimizer: The optimizer to use for training.\n      dropout_rate: Float in `(0, 1)`, the dropout rate.\n      network_layers: Tuple of ints determining the sizes of the network layers.\n      dropout_only_top_layer: Boolean parameter determining if dropout should be\n        done only in the top layer. True by default.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      constraints: iterable of constraints objects that are instances of\n        `tf_agents.bandits.agents.NeuralConstraint`.\n      error_loss_fn: A function for computing the error loss, taking parameters\n        labels, predictions, and weights (any function from tf.losses would\n        work). The default is `tf.losses.mean_squared_error`.\n      gradient_clipping: A float representing the norm length to clip gradients\n        (or None for no clipping.)\n      heteroscedastic: If True, the variance per action is estimated and the\n        losses are weighted appropriately.\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      train_step_counter: An optional `tf.Variable` to increment every time the\n        train op is run.  Defaults to the `global_step`.\n      laplacian_matrix: A float `Tensor` shaped `[num_actions, num_actions]`.\n        This holds the Laplacian matrix used to regularize the smoothness of the\n        estimated expected reward function. This only applies to problems where\n        the actions have a graph structure. If `None`, the regularization is not\n        applied.\n      laplacian_smoothing_weight: A float that determines the weight of the\n        regularization term. Note that this has no effect if `laplacian_matrix`\n        above is `None`.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the action spec contains more than one action or or it is\n      not a bounded scalar int32 spec with minimum 0.\n    """"""\n    fc_layer_params = network_layers\n    dropout_param = {\'rate\': dropout_rate, \'permanent\': True}\n    if dropout_only_top_layer:\n      dropout_layer_params = [None] * (len(fc_layer_params) - 1)\n      dropout_layer_params.append(dropout_param)\n    else:\n      dropout_layer_params = [dropout_param] * len(fc_layer_params)\n    if observation_and_action_constraint_splitter is not None:\n      input_tensor_spec, _ = observation_and_action_constraint_splitter(\n          time_step_spec.observation)\n    else:\n      input_tensor_spec = time_step_spec.observation\n\n    if heteroscedastic:\n      reward_network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n          input_tensor_spec=input_tensor_spec,\n          action_spec=action_spec,\n          fc_layer_params=fc_layer_params,\n          dropout_layer_params=dropout_layer_params)\n    else:\n      reward_network = q_network.QNetwork(\n          input_tensor_spec=input_tensor_spec,\n          action_spec=action_spec,\n          fc_layer_params=fc_layer_params,\n          dropout_layer_params=dropout_layer_params)\n\n    super(DropoutThompsonSamplingAgent, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        reward_network=reward_network,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        constraints=constraints,\n        error_loss_fn=error_loss_fn,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        emit_policy_info=emit_policy_info,\n        train_step_counter=train_step_counter,\n        laplacian_matrix=laplacian_matrix,\n        laplacian_smoothing_weight=laplacian_smoothing_weight,\n        name=name)\n'"
tf_agents/bandits/agents/dropout_thompson_sampling_agent_test.py,40,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for dropout_thompson_sampling_agent.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import dropout_thompson_sampling_agent\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\n\n\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ndef _get_initial_and_final_steps(observations, rewards):\n  batch_size = observations.shape[0]\n  initial_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.FIRST, dtype=tf.int32, shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(observations, dtype=tf.float32, name=\'observation\'))\n  final_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.LAST, dtype=tf.int32, shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(rewards, dtype=tf.float32, name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(observations + 100.0, dtype=tf.float32, name=\'observation\'))\n  return initial_step, final_step\n\n\ndef _get_initial_and_final_steps_with_action_mask(batch_size,\n                                                  context_dim,\n                                                  num_actions):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  observation = tf.constant(observation, dtype=tf.float32)\n  mask = 1 - tf.eye(batch_size, num_columns=num_actions, dtype=tf.int32)\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation, mask))\n  final_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation + 100.0, mask))\n  return initial_step, final_step\n\n\ndef _get_action_step(action):\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action),\n      info=policy_utilities.PolicyInfo())\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1),\n      single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(AgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n\n  def testCreateAgent(self):\n    agent = dropout_thompson_sampling_agent.DropoutThompsonSamplingAgent(\n        self._time_step_spec,\n        self._action_spec,\n        optimizer=None,\n        dropout_rate=0.1,\n        network_layers=(20, 20, 20))\n    self.assertIsNotNone(agent.policy)\n\n  def testTrainAgent(self):\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = dropout_thompson_sampling_agent.DropoutThompsonSamplingAgent(\n        self._time_step_spec,\n        self._action_spec,\n        optimizer=optimizer,\n        dropout_rate=0.1,\n        network_layers=(20, 20, 20),\n        dropout_only_top_layer=False)\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllGreater(self.evaluate(loss_before), 0)\n    self.assertAllGreater(self.evaluate(loss_after), 0)\n\n  def testAgentWithMask(self):\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    obs_spec = (tensor_spec.TensorSpec([2], tf.float32),\n                tensor_spec.TensorSpec([3], tf.int32))\n    agent = dropout_thompson_sampling_agent.DropoutThompsonSamplingAgent(\n        ts.time_step_spec(obs_spec),\n        self._action_spec,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        dropout_rate=0.1,\n        network_layers=(20, 20, 20),\n        dropout_only_top_layer=False)\n    actions = np.array([0, 1], dtype=np.int32)\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        2, 2, 3)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllGreater(self.evaluate(loss_before), 0)\n    self.assertAllGreater(self.evaluate(loss_after), 0)\n\n  def testTrainAgentHeteroscedastic(self):\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = dropout_thompson_sampling_agent.DropoutThompsonSamplingAgent(\n        self._time_step_spec,\n        self._action_spec,\n        optimizer=optimizer,\n        dropout_rate=0.1,\n        network_layers=(20, 20, 20),\n        dropout_only_top_layer=False,\n        heteroscedastic=True)\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertIsNotNone(self.evaluate(loss_before))\n    self.assertIsNotNone(self.evaluate(loss_after))\n\n  def testAgentWithMaskHeteroscedastic(self):\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    obs_spec = (tensor_spec.TensorSpec([2], tf.float32),\n                tensor_spec.TensorSpec([3], tf.int32))\n    agent = dropout_thompson_sampling_agent.DropoutThompsonSamplingAgent(\n        ts.time_step_spec(obs_spec),\n        self._action_spec,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        dropout_rate=0.1,\n        network_layers=(20, 20, 20),\n        dropout_only_top_layer=False,\n        heteroscedastic=True)\n    actions = np.array([0, 1], dtype=np.int32)\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        2, 2, 3)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertIsNotNone(self.evaluate(loss_before))\n    self.assertIsNotNone(self.evaluate(loss_after))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/exp3_agent.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implements the EXP3 bandit algorithm.\n\nImplementation based on\n\n""Bandit Algorithms""\n  Lattimore and Szepesvari, 2019\n  https://tor-lattimore.com/downloads/book/book.pdf\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.policies import categorical_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.utils import common\n\n\ndef selective_sum(values, partitions, num_partitions):\n  """"""Sums entries in `values`, partitioned using `partitions`.\n\n  For example,\n\n  ```python\n     # Returns `[0 + 4 + 5, 2 + 3 + 4]` i.e. `[9, 6]`.\n     selective_sum(values=[0, 1, 2, 3, 4, 5],\n                   partitions=[0, 1, 1, 1, 0, 0]),\n                   num_partitions=2)\n  ```\n\n  Args:\n    values: a `Tensor` with numerical type.\n    partitions: an integer `Tensor` with the same shape as `values`. Entry\n      `partitions[i]` indicates the partition to which `values[i]` belongs.\n    num_partitions: the number of partitions. All values in `partitions` must\n      lie in `[0, num_partitions)`.\n  Returns:\n    A vector of size `num_partitions` with the same dtype as `values`. Entry `i`\n    is the sum of all entries in `values` belonging to partition `i`.\n  """"""\n  partitioned_values = tf.dynamic_partition(values, partitions, num_partitions)\n  return tf.stack([tf.reduce_sum(partition)\n                   for partition in partitioned_values])\n\n\ndef exp3_update_value(reward, log_prob):\n  return 1. - (1. - reward) / tf.exp(log_prob)\n\n\n@gin.configurable\nclass Exp3Agent(tf_agent.TFAgent):\n  """"""An agent implementing the EXP3 bandit algorithm.\n\n  Implementation based on\n\n  ""Bandit Algorithms""\n    Lattimore and Szepesvari, 2019\n    http://downloads.tor-lattimore.com/book.pdf\n  """"""\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               learning_rate,\n               name=None):\n    """"""Initialize an instance of `Exp3Agent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec describing the expected `TimeStep`s.\n      action_spec: A scalar `BoundedTensorSpec` with `int32` or `int64` dtype\n        describing the number of actions for this agent.\n      learning_rate: A float valued scalar. A higher value will force the agent\n        to converge on a single action more quickly. A lower value will\n        encourage more exploration. This value corresponds to the\n        `inverse_temperature` argument passed to `CategoricalPolicy`.\n      name: a name for this instance of `Exp3Agent`.\n    """"""\n    tf.Module.__init__(self, name=name)\n    common.tf_agents_gauge.get_cell(\'TFABandit\').set(True)\n    self._num_actions = bandit_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n    self._weights = tf.compat.v2.Variable(\n        tf.zeros(self._num_actions), name=\'weights\')\n    self._learning_rate = tf.compat.v2.Variable(\n        learning_rate, name=\'learning_rate\')\n    policy = categorical_policy.CategoricalPolicy(\n        weights=self._weights,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        inverse_temperature=self._learning_rate)\n    # TODO(b/127462472): consider policy=GreedyPolicy(collect_policy).\n    super(Exp3Agent, self).__init__(time_step_spec=time_step_spec,\n                                    action_spec=policy.action_spec,\n                                    policy=policy,\n                                    collect_policy=policy,\n                                    train_sequence_length=None)\n\n  @property\n  def num_actions(self):\n    return self._num_actions\n\n  @property\n  def weights(self):\n    return tf.identity(self._weights)\n\n  @property\n  def learning_rate(self):\n    return tf.identity(self._learning_rate)\n\n  @learning_rate.setter\n  def learning_rate(self, learning_rate):\n    return tf.compat.v1.assign(self._learning_rate, learning_rate)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def _train(self, experience, weights=None):\n    """"""Updates the policy based on the data in `experience`.\n\n    Note that `experience` should only contain data points that this agent has\n    not previously seen. If `experience` comes from a replay buffer, this buffer\n    should be cleared between each call to `train`.\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`.\n      weights: Unused.\n\n    Returns:\n      A `LossInfo` containing the loss *before* the training step is taken.\n        Note that the loss does not depend on policy state and comes directly\n        from the experience (and is therefore not differentiable).\n\n        In most cases, if `weights` is provided, the entries of this tuple will\n        have been calculated with the weights.  Note that each Agent chooses\n        its own method of applying weights.\n    """"""\n    del weights  # unused\n    reward = experience.reward\n    log_prob = policy_step.get_log_probability(experience.policy_info)\n    action = experience.action\n    update_value = exp3_update_value(reward, log_prob)\n    weight_update = selective_sum(values=update_value,\n                                  partitions=action,\n                                  num_partitions=self.num_actions)\n    tf.compat.v1.assign_add(self._weights, weight_update)\n\n    batch_size = tf.cast(tf.size(reward), dtype=tf.int64)\n    self._train_step_counter.assign_add(batch_size)\n\n    return tf_agent.LossInfo(loss=-tf.reduce_sum(experience.reward), extra=())\n'"
tf_agents/bandits/agents/exp3_agent_test.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.exp3_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import exp3_agent\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef _get_initial_and_final_steps(observation_shape, reward):\n  batch_size = reward.shape[0]\n  time_step_spec = time_step.time_step_spec(\n      tensor_spec.TensorSpec(observation_shape, tf.float32))\n  initial_step = tensor_spec.sample_spec_nest(\n      time_step_spec, outer_dims=(batch_size,))\n  final_step = initial_step._replace(reward=tf.convert_to_tensor(reward))\n  return initial_step, final_step\n\n\ndef _get_action_step(action, log_prob):\n  step = policy_step.PolicyStep(action=tf.convert_to_tensor(action))\n  return step._replace(\n      info=policy_step.set_log_probability(step.info,\n                                           tf.convert_to_tensor(log_prob)))\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1),\n      single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass Exp3AgentTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(Exp3AgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_trivial\',\n           observation_shape=[],\n           num_actions=1,\n           learning_rate=1.),\n      dict(testcase_name=\'_2_2_obs\',\n           observation_shape=[2, 2],\n           num_actions=5,\n           learning_rate=.3),\n  )\n  def testInitializeAgent(self,\n                          observation_shape,\n                          num_actions,\n                          learning_rate):\n    time_step_spec = time_step.time_step_spec(\n        tensor_spec.TensorSpec(observation_shape, tf.float32))\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = exp3_agent.Exp3Agent(time_step_spec=time_step_spec,\n                                 action_spec=action_spec,\n                                 learning_rate=learning_rate)\n    self.evaluate(agent.initialize())\n\n  @parameterized.parameters(\n      dict(values=np.array([0.]),\n           partitions=np.array([0]),\n           num_partitions=1,\n           expected_output=np.array([0.])),\n      dict(values=np.array([0, 1, 2, 3, 4, 5]),\n           partitions=np.array([0, 1, 1, 1, 0, 0]),\n           num_partitions=2,\n           expected_output=np.array([9, 6])),\n      )\n  def testSelectiveSum(\n      self, values, partitions, num_partitions, expected_output):\n    actual_output = exp3_agent.selective_sum(values, partitions, num_partitions)\n    self.assertAllCloseAccordingToType(expected_output, actual_output)\n\n  @parameterized.parameters(\n      dict(shape=[],\n           seed=1234),\n      dict(shape=[100],\n           seed=2345),\n      dict(shape=[3, 4, 3, 6, 2],\n           seed=3456),\n      )\n  def testExp3UpdateValueShape(self, shape, seed):\n    tf.compat.v1.set_random_seed(seed)\n    reward = tfd.Uniform(0., 1.).sample(shape)\n    log_prob = tfd.Normal(0., 1.).sample(shape)\n    update_value = exp3_agent.exp3_update_value(reward, log_prob)\n    self.assertAllEqual(shape, update_value.shape)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_trivial\',\n           observation_shape=[],\n           num_actions=1,\n           action=np.array([0], dtype=np.int32),\n           log_prob=np.array([0.], dtype=np.float32),\n           reward=np.array([0.], dtype=np.float32),\n           learning_rate=1.),\n      dict(testcase_name=\'_8_rewards\',\n           observation_shape=[2, 2],\n           num_actions=5,\n           action=np.array([0, 1, 1, 2, 4, 3, 4, 3], dtype=np.int32),\n           log_prob=np.log([.1, .2, .2, .4, .6, .2, .4, .2], dtype=np.float32),\n           reward=np.array([0., .4, .3, .4, .9, .4, .5, .3], dtype=np.float32),\n           learning_rate=.3),\n      )\n  def testExp3Update(self,\n                     observation_shape,\n                     num_actions,\n                     action,\n                     log_prob,\n                     reward,\n                     learning_rate):\n    """"""Check EXP3 updates for specified actions and rewards.""""""\n\n    # Compute expected update for each action.\n    expected_update_value = exp3_agent.exp3_update_value(reward, log_prob)\n    expected_update = np.zeros(num_actions)\n    for a, u in zip(action, self.evaluate(expected_update_value)):\n      expected_update[a] += u\n\n    # Construct a `Trajectory` for the given action, log prob and reward.\n    time_step_spec = time_step.time_step_spec(\n        tensor_spec.TensorSpec(observation_shape, tf.float32))\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observation_shape, reward)\n    action_step = _get_action_step(action, log_prob)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update. Record initial and final\n    # weights.\n    agent = exp3_agent.Exp3Agent(time_step_spec=time_step_spec,\n                                 action_spec=action_spec,\n                                 learning_rate=learning_rate)\n    self.evaluate(agent.initialize())\n    initial_weights = self.evaluate(agent.weights)\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_weights = self.evaluate(agent.weights)\n    update = final_weights - initial_weights\n\n    # Check that the actual update matches expectations.\n    self.assertAllClose(expected_update, update)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/exp3_mixture_agent.py,25,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A mixture agent that updates the mixture distribution based on EXP3.\n\nFor a reference on EXP3, see `Bandit Algorithms` by Tor Lattimore and Csaba\nSzepesvari (https://tor-lattimore.com/downloads/book/book.pdf).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.agents import mixture_agent\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.utils import nest_utils\n\ntfd = tfp.distributions\n\n\n@gin.configurable\nclass Exp3MixtureVariableCollection(tf.Module):\n  """"""A collection of variables used by subclasses of `MixtureAgent`.\n\n  Note that this variable collection only contains the mixture weights. The\n  variables of the sub-agents that the mixture agent mixes are in variable\n  collections of the respective sub-agents.\n  """"""\n\n  def __init__(self,\n               num_agents,\n               reward_aggregates=None,\n               inverse_temperature=0.0):\n    """"""Initializes an instace of \'Exp3MixtureVariableCollection\'.\n\n    Args:\n      num_agents: (int) the number of agents mixed by the mixture agent.\n      reward_aggregates: A list of floats containing the reward aggregates for\n        each agent. If not set, the initial values will be 0.\n      inverse_temperature: The initial value for the inverse temperature\n        variable used by the mixture agent.\n    """"""\n    if reward_aggregates is None:\n      reward_aggregates = [0.0] * num_agents\n    else:\n      if num_agents != len(reward_aggregates):\n        raise ValueError(\'`reward_aggregates` must have `num_agents` elements.\')\n    self._reward_aggregates = tf.Variable(\n        reward_aggregates, name=\'reward_aggregates\', dtype=tf.float32)\n    self._inverse_temperature = tf.Variable(\n        inverse_temperature, dtype=tf.float32)\n\n  @property\n  def reward_aggregates(self):\n    return self._reward_aggregates\n\n  @property\n  def inverse_temperature(self):\n    return self._inverse_temperature\n\n\n@gin.configurable\nclass Exp3MixtureAgent(mixture_agent.MixtureAgent):\n  """"""An agent that mixes a set of agents and updates the weights with Exp3.\n\n  For a reference on EXP3, see `Bandit Algorithms` by Tor Lattimore and Csaba\n  Szepesvari (https://tor-lattimore.com/downloads/book/book.pdf).\n\n  The update uses a slighlty modified version of EXP3 to make sure that the\n  weights do not go to one seemingly good agent in the very beginning. To smooth\n  the weights, two extra measures are taken:\n\n  1. A forgetting factor makes sure that the aggregated reward estimates do not\n  grow indefinitely.\n  2. The `inverse temperature` has a maximum parameter that prevents it from\n  growing indefinitely.\n\n  It is generally a good idea to set\n\n  ```\n  forgetting_factor = 1 - (1 / max_inverse_temperature)\n  ```\n\n  so that the two smoothing factors work together nicely.\n\n  For every data sample, the agent updates the sub-agent that was used to make\n  the action choice in that sample. For this update to happen, the mixture agent\n  needs to have the information on which sub-agent is ""responsible"" for the\n  action. This information is in a policy info field `mixture_choice_info`.\n  """"""\n\n  def __init__(self,\n               agents,\n               variable_collection=None,\n               forgetting=0.999,\n               max_inverse_temperature=1000.0,\n               name=None):\n    """"""Initializes an instance of `Exp3MixtureAgent`.\n\n    Args:\n      agents: List of TF-Agents agents that this mixture agent trains.\n      variable_collection: An instance of `Exp3VariableCollection`. If not set,\n        A default one will be created. It contains all the variables that are\n        needed to restore the mixture agent, excluding the variables of the\n        subagents.\n      forgetting: A float value in (0, 1]. This is how much the estimated\n        reward aggregates are shrinked in every training step.\n      max_inverse_temperature: This value caps the inverse temperature that\n       would otherwise grow as the square root of the number of samples seen.\n      name: Name fo this instance of `Exp3MixtureAgent`.\n    """"""\n    self._num_agents = len(agents)\n    self._forgetting = forgetting\n    self._max_inverse_temperature = max_inverse_temperature\n    if variable_collection is None:\n      variable_collection = Exp3MixtureVariableCollection(\n          self._num_agents)\n    elif not isinstance(variable_collection,\n                        Exp3MixtureVariableCollection):\n      raise TypeError(\'Parameter `variable_collection` should be \'\n                      \'of type `MixtureVariableCollection`.\')\n    elif variable_collection.reward_aggregates.shape != self._num_agents:\n      raise ValueError(\'`variable_collection.reward_aggregates` should have \'\n                       \'shape `[len(agents)]`.\')\n    self._variable_collection = variable_collection\n\n    # The `_mixture_weights` value is reassigned in every training step and only\n    # depends on reward aggregates and inverse temperature. This variable is not\n    # part of the variable collection because it is not needed to restore an\n    # agent. The only reason why this value is a tf.Variable is because this way\n    # the categorical distribution is dynamically parameterized.\n\n    self._mixture_weights = tf.Variable(\n        tf.zeros_like(variable_collection.reward_aggregates))\n    mixture_distribution = tfd.Categorical(\n        logits=self._mixture_weights)\n    super(Exp3MixtureAgent, self).__init__(\n        mixture_distribution, agents, name=name)\n\n  def _update_mixture_distribution(self, experience):\n\n    reward, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.reward, self._time_step_spec.reward)\n    policy_choice, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.policy_info[mixture_policy.MIXTURE_AGENT_ID],\n        self._time_step_spec.reward)\n    batch_size = tf.compat.dimension_value(\n        reward.shape[0]) or tf.shape(reward)[0]\n    unnormalized_probabilities = tf.exp(self._mixture_weights)\n    probabilities = unnormalized_probabilities / tf.norm(\n        unnormalized_probabilities, 1)\n\n    normalizer = tf.reduce_sum(unnormalized_probabilities)\n    probabilities = unnormalized_probabilities / normalizer\n    self._summarize_probabilities(probabilities)\n    repeated_probs = tf.tile(\n        tf.expand_dims(probabilities, axis=0), [batch_size, 1])\n    probs_per_step = tf.gather(\n        repeated_probs, policy_choice, batch_dims=1)\n    per_step_update_term = tf.expand_dims((1 - reward) / probs_per_step, axis=0)\n    one_hot_policy_choice = tf.one_hot(\n        policy_choice, depth=self._num_agents)\n    update_term = 1 - tf.squeeze(\n        tf.matmul(per_step_update_term, one_hot_policy_choice))\n    self._update_aggregates(update_term)\n    self._update_inverse_temperature(batch_size)\n    return self._mixture_weights.assign(\n        self._variable_collection.reward_aggregates /\n        self._variable_collection.inverse_temperature)\n\n  def _summarize_probabilities(self, probabilities):\n    for k in range(self._num_agents):\n      tf.compat.v2.summary.scalar(\n          name=\'policy_{}_prob\'.format(k),\n          data=probabilities[k],\n          step=self.train_step_counter)\n\n  def _update_aggregates(self, update_term):\n    self._variable_collection.reward_aggregates.assign(\n        self._forgetting *\n        (self._variable_collection.reward_aggregates + update_term))\n\n  def _update_inverse_temperature(self, batch_size):\n    self._variable_collection.inverse_temperature.assign(\n        tf.maximum(\n            self._max_inverse_temperature,\n            tf.sqrt(\n                tf.square(self._variable_collection.inverse_temperature) +\n                tf.cast(batch_size, dtype=tf.float32))))\n'"
tf_agents/bandits/agents/exp3_mixture_agent_test.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.exp3_mixture_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.agents import exp3_mixture_agent\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef _get_initial_and_final_steps(batch_size, context_dim):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.5, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation + 100.0,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  return initial_step, final_step\n\n\ndef _get_action_step(action, num_agents, num_actions):\n  batch_size = tf.shape(action)[0]\n  choices = tf.constant(num_agents - 1, shape=action.shape, dtype=tf.int32)\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action),\n      info={\n          mixture_policy.MIXTURE_AGENT_ID:\n              choices,\n          mixture_policy.SUBPOLICY_INFO:\n              policy_utilities.PolicyInfo(\n                  predicted_rewards_mean=tf.zeros([batch_size, num_actions]))\n      })\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1), single_experience)\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_contextdim10_numagents2\',\n          \'batch_size\': 1,\n          \'context_dim\': 10,\n          \'num_agents\': 2,\n      }, {\n          \'testcase_name\': \'_batch3_contextdim7_numagents17\',\n          \'batch_size\': 3,\n          \'context_dim\': 7,\n          \'num_agents\': 17,\n      }, {\n          \'testcase_name\': \'_batch4_contextdim5_numagents10\',\n          \'batch_size\': 4,\n          \'context_dim\': 5,\n          \'num_agents\': 10,\n      })\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass Exp3MixtureAgentTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(Exp3MixtureAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n\n  @test_cases()\n  def testInitializeAgent(self, batch_size, context_dim, num_agents):\n    num_actions = 7\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agents = [\n        lin_ucb_agent.LinearUCBAgent(time_step_spec, action_spec)\n        for _ in range(num_agents)\n    ]\n    mixed_agent = exp3_mixture_agent.Exp3MixtureAgent(agents)\n    self.evaluate(mixed_agent.initialize())\n\n  @test_cases()\n  def testMixtureUpdate(self, batch_size, context_dim, num_agents):\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agents = []\n    for _ in range(num_agents):\n      agents.append(\n          lin_ucb_agent.LinearUCBAgent(\n              time_step_spec,\n              action_spec,\n              emit_policy_info=(\n                  policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,)))\n    mixed_agent = exp3_mixture_agent.Exp3MixtureAgent(agents)\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action, num_agents, num_actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    self.evaluate(mixed_agent.initialize())\n    self.evaluate(mixed_agent._variable_collection.reward_aggregates)\n    self.evaluate(mixed_agent.train(experience))\n    reward_aggregates = self.evaluate(\n        mixed_agent._variable_collection.reward_aggregates)\n    self.assertAllInSet(reward_aggregates[:num_agents - 1], [0.999])\n    agent_prob = 1 / num_agents\n    est_rewards = 0.5 / agent_prob\n    per_step_update = est_rewards\n    last_agent_update = 1 - batch_size * per_step_update\n    self.assertAllClose(reward_aggregates[-1], last_agent_update * 0.999)\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/greedy_reward_prediction_agent.py,30,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An agent that uses and trains a greedy reward prediction policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\nfrom tf_agents.bandits.policies import greedy_reward_prediction_policy as greedy_reward_policy\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\n\n\n@gin.configurable\nclass GreedyRewardPredictionAgent(tf_agent.TFAgent):\n  """"""A neural reward network based bandit agent.\n\n  This agent receives a neural network that it trains to predict rewards. The\n  action is chosen greedily with respect to the prediction.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      reward_network,\n      optimizer,\n      observation_and_action_constraint_splitter=None,\n      accepts_per_arm_features=False,\n      constraints=(),\n      # Params for training.\n      error_loss_fn=tf.compat.v1.losses.mean_squared_error,\n      gradient_clipping=None,\n      # Params for debugging.\n      debug_summaries=False,\n      summarize_grads_and_vars=False,\n      enable_summaries=True,\n      emit_policy_info=(),\n      train_step_counter=None,\n      laplacian_matrix=None,\n      laplacian_smoothing_weight=0.001,\n      name=None):\n    """"""Creates a Greedy Reward Network Prediction Agent.\n\n     In some use cases, the actions are not independent and they are related to\n     each other (e.g., when the actions are ordinal integers). Assuming that\n     the relations between arms can be modeled by a graph, we may want to\n     enforce that the estimated reward function is smooth over the graph. This\n     implies that the estimated rewards `r_i` and `r_j` for two related actions\n     `i` and `j`, should be close to each other. To quantify this smoothness\n     criterion we use the Laplacian matrix `L` of the graph over the actions.\n     When the laplacian smoothing is enabled, the loss is extended to:\n     ```\n       Loss_new := Loss + lambda r^T * L * r,\n     ```\n     where `r` is the estimated reward vector for all actions. The second\n     term is the laplacian smoothing regularization term and `lambda` is the\n     weight that determines how strongly we enforce the regularization.\n     For more details, please see:\n     ""Bandits on graphs and structures"", Michal Valko\n     https://hal.inria.fr/tel-01359757/document\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      reward_network: A `tf_agents.network.Network` to be used by the agent. The\n        network will be called with call(observation, step_type) and it is\n        expected to provide a reward prediction for all actions.\n      optimizer: The optimizer to use for training.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      constraints: iterable of constraints objects that are instances of\n        `tf_agents.bandits.agents.NeuralConstraint`.\n      error_loss_fn: A function for computing the error loss, taking parameters\n        labels, predictions, and weights (any function from tf.losses would\n        work). The default is `tf.losses.mean_squared_error`.\n      gradient_clipping: A float representing the norm length to clip gradients\n        (or None for no clipping.)\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      train_step_counter: An optional `tf.Variable` to increment every time the\n        train op is run.  Defaults to the `global_step`.\n      laplacian_matrix: A float `Tensor` or a numpy array shaped\n        `[num_actions, num_actions]`. This holds the Laplacian matrix used to\n        regularize the smoothness of the estimated expected reward function.\n        This only applies to problems where the actions have a graph structure.\n        If `None`, the regularization is not applied.\n      laplacian_smoothing_weight: A float that determines the weight of the\n        regularization term. Note that this has no effect if `laplacian_matrix`\n        above is `None`.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the action spec contains more than one action or or it is\n      not a bounded scalar int32 spec with minimum 0.\n      InvalidArgumentError: if the Laplacian provided is not None and not valid.\n    """"""\n    tf.Module.__init__(self, name=name)\n    common.tf_agents_gauge.get_cell(\'TFABandit\').set(True)\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n    self._num_actions = bandit_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n    self._accepts_per_arm_features = accepts_per_arm_features\n    self._constraints = constraints\n\n    reward_network.create_variables()\n    self._reward_network = reward_network\n    self._optimizer = optimizer\n    self._error_loss_fn = error_loss_fn\n    self._gradient_clipping = gradient_clipping\n    self._heteroscedastic = isinstance(\n        reward_network, heteroscedastic_q_network.HeteroscedasticQNetwork)\n    self._laplacian_matrix = None\n    if laplacian_matrix is not None:\n      self._laplacian_matrix = tf.convert_to_tensor(\n          laplacian_matrix, dtype=tf.float32)\n      # Check the validity of the laplacian matrix.\n      tf.debugging.assert_near(\n          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 1)))\n      tf.debugging.assert_near(\n          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 0)))\n    self._laplacian_smoothing_weight = laplacian_smoothing_weight\n\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        time_step_spec,\n        action_spec,\n        reward_network,\n        observation_and_action_constraint_splitter,\n        constraints=constraints,\n        accepts_per_arm_features=accepts_per_arm_features,\n        emit_policy_info=emit_policy_info)\n    training_data_spec = None\n    if accepts_per_arm_features:\n      training_data_spec = bandit_spec_utils.drop_arm_observation(\n          policy.trajectory_spec, observation_and_action_constraint_splitter)\n\n    super(GreedyRewardPredictionAgent, self).__init__(\n        time_step_spec,\n        action_spec,\n        policy,\n        collect_policy=policy,\n        train_sequence_length=None,\n        training_data_spec=training_data_spec,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        train_step_counter=train_step_counter)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def _variables_to_train(self):\n    variables_to_train = self._reward_network.variables\n    for c in self._constraints:\n      variables_to_train.extend(c.variables)\n    return variables_to_train\n\n  def _train(self, experience, weights):\n    (observations, actions,\n     rewards) = bandit_utils.process_experience_for_neural_agents(\n         experience, self._observation_and_action_constraint_splitter,\n         self._accepts_per_arm_features, self.training_data_spec)\n\n    with tf.GradientTape() as tape:\n      loss_info = self.loss(observations,\n                            actions,\n                            rewards,\n                            weights=weights,\n                            training=True)\n\n    variables_to_train = self._variables_to_train()\n    if not variables_to_train:\n      logging.info(\'No variable to train in the agent.\')\n      return loss_info\n\n    grads = tape.gradient(loss_info.loss, variables_to_train)\n    # Tuple is used for py3, where zip is a generator producing values once.\n    grads_and_vars = tuple(zip(grads, variables_to_train))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n                                                       self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      eager_utils.add_variables_summaries(grads_and_vars,\n                                          self.train_step_counter)\n      eager_utils.add_gradients_summaries(grads_and_vars,\n                                          self.train_step_counter)\n\n    self._optimizer.apply_gradients(grads_and_vars)\n    self.train_step_counter.assign_add(1)\n\n    return loss_info\n\n  def reward_loss(self,\n                  observations,\n                  actions,\n                  rewards,\n                  weights=None,\n                  training=False):\n    """"""Computes loss for reward prediction training.\n\n    Args:\n      observations: A batch of observations.\n      actions: A batch of actions.\n      rewards: A batch of rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output batch loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether the loss is being used for training.\n\n    Returns:\n      loss: A `Tensor` containing the loss for the training step.\n    Raises:\n      ValueError:\n        if the number of actions is greater than 1.\n    """"""\n    with tf.name_scope(\'loss\'):\n      sample_weights = weights if weights else 1\n      if self._heteroscedastic:\n        predictions, _ = self._reward_network(observations,\n                                              training=training)\n        predicted_values = predictions.q_value_logits\n        predicted_log_variance = predictions.log_variance\n        action_predicted_log_variance = common.index_with_actions(\n            predicted_log_variance, tf.cast(actions, dtype=tf.int32))\n        sample_weights = sample_weights * 0.5 * tf.exp(\n            -action_predicted_log_variance)\n\n        loss = 0.5 * tf.reduce_mean(action_predicted_log_variance)\n        # loss = 1/(2 * var(x)) * (y - f(x))^2 + 1/2 * log var(x)\n        # Kendall, Alex, and Yarin Gal. ""What Uncertainties Do We Need in\n        # Bayesian Deep Learning for Computer Vision?."" Advances in Neural\n        # Information Processing Systems. 2017. https://arxiv.org/abs/1703.04977\n      else:\n        predicted_values, _ = self._reward_network(observations,\n                                                   training=training)\n        loss = tf.constant(0.0)\n\n      action_predicted_values = common.index_with_actions(\n          predicted_values,\n          tf.cast(actions, dtype=tf.int32))\n\n      # Apply Laplacian smoothing on the estimated rewards, if applicable.\n      if self._laplacian_matrix is not None:\n        smoothness_batched = tf.matmul(\n            predicted_values,\n            tf.matmul(self._laplacian_matrix, predicted_values,\n                      transpose_b=True))\n        loss += (self._laplacian_smoothing_weight * tf.reduce_mean(\n            tf.linalg.tensor_diag_part(smoothness_batched) * sample_weights))\n\n      loss += self._error_loss_fn(\n          rewards,\n          action_predicted_values,\n          sample_weights,\n          reduction=tf.compat.v1.losses.Reduction.MEAN)\n\n    return loss\n\n  def loss(self,\n           observations,\n           actions,\n           rewards,\n           weights=None,\n           training=False):\n    """"""Computes loss for training the reward and constraint networks.\n\n    Args:\n      observations: A batch of observations.\n      actions: A batch of actions.\n      rewards: A batch of rewards. In the case we have constraints, we assume\n        that rewards is a 2-rank tensor where the first column corresponds to\n        the reward signal and the following columns correspond to the\n        constraint signals.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output batch loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether the loss is being used for training.\n\n    Returns:\n      loss: A `LossInfo` containing the loss for the training step.\n    Raises:\n      ValueError:\n        if the number of actions is greater than 1.\n    """"""\n    # We assume that the first column is the reward signal followed by the\n    # constraint signals.\n    rewards_tensor = rewards\n    if self._constraints:\n      rewards_tensor = rewards[:, 0]\n    reward_loss = self.reward_loss(\n        observations, actions, rewards_tensor, weights, training)\n\n    constraint_loss = tf.constant(0.0)\n    for i, c in enumerate(self._constraints, 1):\n      constraint_loss += c.compute_loss(\n          observations, actions, rewards[:, i], weights, training)\n\n    self.compute_summaries(reward_loss, constraint_loss=(\n        constraint_loss if self._constraints else None))\n\n    total_loss = reward_loss\n    if self._constraints:\n      total_loss += constraint_loss\n    return tf_agent.LossInfo(total_loss, extra=())\n\n  def compute_summaries(self, loss, constraint_loss=None):\n    if self.summaries_enabled:\n      with tf.name_scope(\'Losses/\'):\n        tf.compat.v2.summary.scalar(\n            name=\'loss\', data=loss, step=self.train_step_counter)\n        if constraint_loss is not None:\n          tf.compat.v2.summary.scalar(\n              name=\'constraint_loss\',\n              data=constraint_loss,\n              step=self.train_step_counter)\n\n      if self._summarize_grads_and_vars:\n        with tf.name_scope(\'Variables/\'):\n          for var in self._variables_to_train():\n            tf.compat.v2.summary.histogram(\n                name=var.name.replace(\':\', \'_\'),\n                data=var,\n                step=self.train_step_counter)\n'"
tf_agents/bandits/agents/greedy_reward_prediction_agent_test.py,89,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for greedy_reward_prediction_agent.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import constraints\nfrom tf_agents.bandits.agents import greedy_reward_prediction_agent as greedy_agent\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\nfrom tensorflow.python.framework import errors  # pylint:disable=g-direct-tensorflow-import  # TF internal\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, unused_observation_spec, action_spec, name=None):\n    super(DummyNet, self).__init__(\n        unused_observation_spec, state_spec=(), name=name)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[1, 1.5, 2],\n                 [1, 1.5, 4]]),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                [[1], [1], [-10]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\ndef _get_initial_and_final_steps(observations, rewards):\n  batch_size = tf.nest.flatten(observations)[0].shape[0]\n  if isinstance(observations, np.ndarray):\n    observations = tf.constant(\n        observations, dtype=tf.float32, name=\'observation\')\n  initial_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      observations)\n  final_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(rewards, dtype=tf.float32, name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      observations)\n  return initial_step, final_step\n\n\ndef _get_initial_and_final_steps_with_action_mask(observations, rewards):\n  batch_size = tf.nest.flatten(observations)[0].shape[0]\n  initial_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observations[0], observations[1]))\n  final_step = ts.TimeStep(\n      tf.constant(\n          ts.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(rewards, dtype=tf.float32, name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size],\n                  name=\'discount\'), (tf.nest.map_structure(\n                      lambda x: x + 100., observations[0]), observations[1]))\n  return initial_step, final_step\n\n\ndef _get_action_step(action):\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action),\n      info=policy_utilities.PolicyInfo())\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1),\n      single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(AgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n    self._observation_spec = self._time_step_spec.observation\n\n  def testCreateAgent(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None)\n    self.assertIsNotNone(agent.policy)\n\n  def testInitializeAgent(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None)\n    init_op = agent.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n\n  def testLoss(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    actions = tf.constant([0, 1], dtype=tf.int32)\n    rewards = tf.constant([0.5, 3.0], dtype=tf.float32)\n\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None)\n    init_op = agent.initialize()\n    if not tf.executing_eagerly():\n      with self.cached_session() as sess:\n        common.initialize_uninitialized_variables(sess)\n        self.assertIsNone(sess.run(init_op))\n    loss, _ = agent.loss(observations,\n                         actions,\n                         rewards)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    self.assertAllClose(self.evaluate(loss), 42.25)\n\n  def testPolicy(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    # Batch size 2.\n    self.assertAllEqual([2], action_step.action.shape)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    actions = self.evaluate(action_step.action)\n    self.assertAllEqual(actions, [1, 2])\n\n  def testInitializeRestoreAgent(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n    checkpoint = tf.train.Checkpoint(agent=agent)\n\n    latest_checkpoint = tf.train.latest_checkpoint(self.get_temp_dir())\n    checkpoint_load_status = checkpoint.restore(latest_checkpoint)\n\n    if tf.executing_eagerly():\n      self.evaluate(checkpoint_load_status.initialize_or_restore())\n      self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n    else:\n      with self.cached_session() as sess:\n        checkpoint_load_status.initialize_or_restore(sess)\n        self.assertAllEqual(sess.run(action_step.action), [1, 2])\n\n  def testTrainAgent(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer)\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    self.assertAllClose(self.evaluate(loss_before), 42.25)\n    self.assertAllClose(self.evaluate(loss_after), 93.46)\n\n  def testTrainAgentWithConstraint(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n    reward_spec = tensor_spec.TensorSpec(\n        shape=(2,), dtype=tf.float32, name=\'reward\')\n    self._time_step_spec = ts.time_step_spec(self._obs_spec, reward_spec)\n\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer,\n        constraints=[neural_constraint])\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([[0.5, 6.0], [3.0, 4.0]], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    # The loss is the sum of the reward loss and the constraint loss.\n    self.assertAllClose(self.evaluate(loss_before), 42.25 + 30.125)\n\n  def testTrainAgentWithMask(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    time_step_spec = ts.time_step_spec((tensor_spec.TensorSpec([2], tf.float32),\n                                        tensor_spec.TensorSpec([3], tf.int32)))\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]))\n    observations = (np.array([[1, 2], [3, 4]], dtype=np.float32),\n                    np.array([[1, 0, 0], [1, 1, 0]], dtype=np.int32))\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    loss_after, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    self.assertAllClose(self.evaluate(loss_before), 42.25)\n    self.assertAllClose(self.evaluate(loss_after), 93.46)\n\n  def testTrainAgentWithMaskAndConstraint(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    reward_spec = tensor_spec.TensorSpec(\n        shape=(2,), dtype=tf.float32, name=\'reward\')\n    observation_and_mask_spec = (tensor_spec.TensorSpec([2], tf.float32),\n                                 tensor_spec.TensorSpec([3], tf.int32))\n    time_step_spec = ts.time_step_spec(observation_and_mask_spec, reward_spec)\n\n    constraint_net = DummyNet(self._observation_spec, self._action_spec)\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        constraints=[neural_constraint])\n    observations = (np.array([[1, 2], [3, 4]], dtype=np.float32),\n                    np.array([[1, 0, 0], [1, 1, 0]], dtype=np.int32))\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([[0.5, 6.0], [3.0, 4.0]], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    # The loss is the sum of the reward loss and the constraint loss.\n    self.assertAllClose(self.evaluate(loss_before), 42.25 + 30.125)\n\n  def testTrainAgentWithLaplacianSmoothing(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    laplacian_matrix = tf.constant([[1.0, -1.0, 0.0],\n                                    [-1.0, 2.0, -1.0],\n                                    [0.0, -1.0, 1.0]])\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer,\n        laplacian_matrix=laplacian_matrix,\n        laplacian_smoothing_weight=1.0)\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_before, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    # The Laplacian smoothing term ends up adding 22.5 to the loss.\n    self.assertAllClose(self.evaluate(loss_before), 42.25 + 22.5)\n\n  def testTrainAgentWithLaplacianSmoothingInvalidMatrix(self):\n    if tf.executing_eagerly:\n      return\n\n    observations = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    actions = np.array([0, 1], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = _get_action_step(actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    with self.assertRaisesRegexp(errors.InvalidArgumentError, \'\'):\n      reward_net = DummyNet(self._observation_spec, self._action_spec)\n      optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n      # Set the Laplacian matrix to be the identity, which is not a valid\n      # Laplacian.\n      laplacian_matrix = tf.eye(3)\n      agent = greedy_agent.GreedyRewardPredictionAgent(\n          self._time_step_spec,\n          self._action_spec,\n          reward_network=reward_net,\n          optimizer=optimizer,\n          laplacian_matrix=laplacian_matrix,\n          laplacian_smoothing_weight=1.0)\n      self.evaluate(tf.compat.v1.initialize_all_variables())\n      loss_before, _ = agent.train(experience, None)\n      self.evaluate(loss_before)\n\n  def testTrainPerArmAgent(self):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        2, 3, 4, add_num_actions_feature=True)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    reward_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        accepts_per_arm_features=True,\n        optimizer=optimizer)\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(24), shape=[2, 4, 3]), dtype=tf.float32),\n        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n            tf.ones([2], dtype=tf.int32)\n    }\n    actions = np.array([0, 3], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(actions),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.array([[1, 2, 3], [3, 2, 1]],\n                                         dtype=np.float32)))\n    experience = _get_experience(initial_step, action_step, final_step)\n    agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n  def testTrainPerArmAgentWithConstraint(self):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(2, 3, 4)\n    reward_spec = tensor_spec.TensorSpec(\n        shape=(2,), dtype=tf.float32, name=\'reward\')\n    time_step_spec = ts.time_step_spec(obs_spec, reward_spec)\n    reward_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    constraint_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n    neural_constraint = constraints.NeuralConstraint(\n        time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        accepts_per_arm_features=True,\n        optimizer=optimizer,\n        constraints=[neural_constraint])\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(24), shape=[2, 4, 3]), dtype=tf.float32)\n    }\n    actions = np.array([0, 3], dtype=np.int32)\n    rewards = np.array([[0.5, 6.0], [3.0, 4.0]], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps(\n        observations, rewards)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(actions),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.array([[1, 2, 3], [3, 2, 1]],\n                                         dtype=np.float32)))\n    experience = _get_experience(initial_step, action_step, final_step)\n    agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n  def testTrainPerArmAgentWithMask(self):\n    num_actions = 4\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        2, 3, num_actions, add_action_mask=True)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    reward_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec[0], (4, 3), (3, 4), (4, 2)))\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = greedy_agent.GreedyRewardPredictionAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        observation_and_action_constraint_splitter=lambda x: [x[0], x[1]],\n        accepts_per_arm_features=True,\n        optimizer=optimizer)\n    observations = ({\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(24), shape=[2, 4, 3]), dtype=tf.float32)\n    }, tf.ones([2, num_actions], dtype=tf.int32))\n    actions = np.array([0, 3], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        observations, rewards)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(actions),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.array([[1, 2, 3], [3, 2, 1]],\n                                         dtype=np.float32)))\n    experience = _get_experience(initial_step, action_step, final_step)\n    agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/lin_ucb_agent.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implements the Linear UCB bandit algorithm.\n\n  Reference:\n  ""A Contextual Bandit Approach to Personalized News Article Recommendation"",\n  Lihong Li, Wei Chu, John Langford, Robert Schapire, WWW 2010.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import linear_bandit_agent as lin_agent\n\n\n@gin.configurable\nclass LinearUCBAgent(lin_agent.LinearBanditAgent):\n  """"""An agent implementing the Linear UCB bandit algorithm.\n\n  Reference:\n  ""A Contextual Bandit Approach to Personalized News Article Recommendation"",\n  Lihong Li, Wei Chu, John Langford, Robert Schapire, WWW 2010.\n  """"""\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               alpha=1.0,\n               gamma=1.0,\n               use_eigendecomp=False,\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               emit_log_probability=False,\n               observation_and_action_constraint_splitter=None,\n               accepts_per_arm_features=False,\n               debug_summaries=False,\n               summarize_grads_and_vars=False,\n               enable_summaries=True,\n               dtype=tf.float32,\n               name=None):\n    """"""Initialize an instance of `LinearUCBAgent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec describing the expected `TimeStep`s.\n      action_spec: A scalar `BoundedTensorSpec` with `int32` or `int64` dtype\n        describing the number of actions for this agent.\n      alpha: (float) positive scalar. This is the exploration parameter that\n        multiplies the confidence intervals.\n      gamma: a float forgetting factor in [0.0, 1.0]. When set to\n        1.0, the algorithm does not forget.\n      use_eigendecomp: whether to use eigen-decomposition or not. The default\n        solver is Conjugate Gradient.\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: Whether the LinearUCBPolicy emits log-probabilities\n        or not. Since the policy is deterministic, the probability is just 1.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the agent accepts per-arm\n        features.\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      dtype: The type of the parameters stored and updated by the agent. Should\n        be one of `tf.float32` and `tf.float64`. Defaults to `tf.float32`.\n      name: a name for this instance of `LinearUCBAgent`.\n\n    Raises:\n      ValueError if dtype is not one of `tf.float32` or `tf.float64`.\n    """"""\n    super(LinearUCBAgent, self).__init__(\n        exploration_policy=lin_agent.ExplorationPolicy.linear_ucb_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        alpha=alpha,\n        gamma=gamma,\n        use_eigendecomp=use_eigendecomp,\n        tikhonov_weight=tikhonov_weight,\n        add_bias=add_bias,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=accepts_per_arm_features,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        dtype=dtype,\n        name=name)\n'"
tf_agents/bandits/agents/linear_bandit_agent.py,82,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An agent that maintains linear estimates for rewards and their uncertainty.\n\nLinUCB and Linear Thompson Sampling agents are subclasses of this agent.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom enum import Enum\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.policies import linalg\nfrom tf_agents.bandits.policies import linear_bandit_policy as lin_policy\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\nclass ExplorationPolicy(Enum):\n  """"""Possible exploration policies.""""""\n  linear_ucb_policy = 1\n  linear_thompson_sampling_policy = 2\n\n\nclass LinearBanditVariableCollection(tf.Module):\n  """"""A collection of variables used by `LinearBanditAgent`.""""""\n\n  def __init__(self,\n               context_dim,\n               num_models,\n               use_eigendecomp=False,\n               dtype=tf.float32,\n               name=None):\n    """"""Initializes an instance of `LinearBanditVariableCollection`.\n\n    It creates all the variables needed for `LinearBanditAgent`.\n\n    Args:\n      context_dim: (int) The context dimension of the bandit environment the\n        agent will be used on.\n      num_models: (int) The number of models maintained by the agent. This is\n        either the same as the number of arms, or, if the agent accepts per-arm\n        features, 1.\n      use_eigendecomp: (bool) Whether the agent uses eigen decomposition for\n        maintaining its internal state.\n      dtype: The type of the variables. Should be one of `tf.float32` and\n        `tf.float64`.\n      name: (string) the name of this instance.\n    """"""\n    tf.Module.__init__(self, name=name)\n    self.cov_matrix_list = []\n    self.data_vector_list = []\n    self.eig_matrix_list = []\n    self.eig_vals_list = []\n    self.num_samples_list = []\n    for k in range(num_models):\n      self.cov_matrix_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros([context_dim, context_dim], dtype=dtype),\n              name=\'a_\' + str(k)))\n      self.data_vector_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros(context_dim, dtype=dtype), name=\'b_{}\'.format(k)))\n      self.num_samples_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros([], dtype=dtype), name=\'num_samples_{}\'.format(k)))\n      if use_eigendecomp:\n        self.eig_matrix_list.append(\n            tf.compat.v2.Variable(\n                tf.eye(context_dim, dtype=dtype),\n                name=\'eig_matrix{}\'.format(k)))\n        self.eig_vals_list.append(\n            tf.compat.v2.Variable(\n                tf.ones([context_dim], dtype=dtype),\n                name=\'eig_vals{}\'.format(k)))\n      else:\n        self.eig_matrix_list.append(\n            tf.compat.v2.Variable(\n                tf.constant([], dtype=dtype), name=\'eig_matrix{}\'.format(k)))\n        self.eig_vals_list.append(\n            tf.compat.v2.Variable(\n                tf.constant([], dtype=dtype), name=\'eig_vals{}\'.format(k)))\n\n\ndef update_a_and_b_with_forgetting(\n    a_prev, b_prev, r, x, gamma, compute_eigendecomp=False):\n  r""""""Update the covariance matrix `a` and the weighted sum of rewards `b`.\n\n  This function updates the covariance matrix `a` and the sum of weighted\n  rewards `b` using a forgetting factor `gamma`.\n\n  Args:\n    a_prev: previous estimate of `a`.\n    b_prev: previous estimate of `b`.\n    r: a `Tensor` of shape [`batch_size`]. This is the rewards of the batched\n      observations.\n    x: a `Tensor` of shape [`batch_size`, `context_dim`]. This is the matrix\n      with the (batched) observations.\n    gamma: a float forgetting factor in [0.0, 1.0].\n    compute_eigendecomp: whether to compute the eigen-decomposition of the new\n      covariance matrix.\n\n  Returns:\n    The updated estimates of `a` and `b` and optionally the eigenvalues and\n    eigenvectors of `a`.\n  """"""\n  a_new = gamma * a_prev + tf.matmul(x, x, transpose_a=True)\n  b_new = gamma * b_prev + bandit_utils.sum_reward_weighted_observations(r, x)\n\n  eig_vals = tf.constant([], dtype=a_new.dtype)\n  eig_matrix = tf.constant([], dtype=a_new.dtype)\n  if compute_eigendecomp:\n    eig_vals, eig_matrix = tf.linalg.eigh(a_new)\n  return a_new, b_new, eig_vals, eig_matrix\n\n\n@gin.configurable\nclass LinearBanditAgent(tf_agent.TFAgent):\n  """"""An agent that maintains linear reward estimates and their uncertainties.""""""\n\n  def __init__(self,\n               exploration_policy,\n               time_step_spec,\n               action_spec,\n               variable_collection=None,\n               alpha=1.0,\n               gamma=1.0,\n               use_eigendecomp=False,\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               emit_log_probability=False,\n               observation_and_action_constraint_splitter=None,\n               accepts_per_arm_features=False,\n               debug_summaries=False,\n               summarize_grads_and_vars=False,\n               enable_summaries=True,\n               dtype=tf.float32,\n               name=None):\n    """"""Initialize an instance of `LinearBanditAgent`.\n\n    Args:\n      exploration_policy: An Enum of type `ExplorationPolicy`. The kind of\n        policy we use for exploration. Currently supported policies are\n        `LinUCBPolicy` and `LinearThompsonSamplingPolicy`.\n      time_step_spec: A `TimeStep` spec describing the expected `TimeStep`s.\n      action_spec: A scalar `BoundedTensorSpec` with `int32` or `int64` dtype\n        describing the number of actions for this agent.\n      variable_collection: Instance of `LinearBanditVariableCollection`.\n        Collection of variables to be updated by the agent. If `None`, a new\n        instance of `LinearBanditVariableCollection` will be created.\n      alpha: (float) positive scalar. This is the exploration parameter that\n        multiplies the confidence intervals.\n      gamma: a float forgetting factor in [0.0, 1.0]. When set to 1.0, the\n        algorithm does not forget.\n      use_eigendecomp: whether to use eigen-decomposition or not. The default\n        solver is Conjugate Gradient.\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: Whether the policy emits log-probabilities or not.\n        Since the policy is deterministic, the probability is just 1.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the agent accepts per-arm\n        features.\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      dtype: The type of the parameters stored and updated by the agent. Should\n        be one of `tf.float32` and `tf.float64`. Defaults to `tf.float32`.\n      name: a name for this instance of `LinearBanditAgent`.\n\n    Raises:\n      ValueError if dtype is not one of `tf.float32` or `tf.float64`.\n      TypeError if variable_collection is not an instance of\n        `LinearBanditVariableCollection`.\n    """"""\n    tf.Module.__init__(self, name=name)\n    common.tf_agents_gauge.get_cell(\'TFABandit\').set(True)\n    self._num_actions = bandit_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n    self._num_models = 1 if accepts_per_arm_features else self._num_actions\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n    self._time_step_spec = time_step_spec\n    self._accepts_per_arm_features = accepts_per_arm_features\n    self._add_bias = add_bias\n    if observation_and_action_constraint_splitter is not None:\n      context_spec, _ = observation_and_action_constraint_splitter(\n          time_step_spec.observation)\n    else:\n      context_spec = time_step_spec.observation\n\n    (self._global_context_dim,\n     self._arm_context_dim) = bandit_spec_utils.get_context_dims_from_spec(\n         context_spec, accepts_per_arm_features)\n    if self._add_bias:\n      # The bias is added via a constant 1 feature.\n      self._global_context_dim += 1\n    self._overall_context_dim = self._global_context_dim + self._arm_context_dim\n\n    self._alpha = alpha\n    if variable_collection is None:\n      variable_collection = LinearBanditVariableCollection(\n          context_dim=self._overall_context_dim,\n          num_models=self._num_models,\n          use_eigendecomp=use_eigendecomp,\n          dtype=dtype)\n    elif not isinstance(variable_collection, LinearBanditVariableCollection):\n      raise TypeError(\'Parameter `variable_collection` should be \'\n                      \'of type `LinearBanditVariableCollection`.\')\n    self._variable_collection = variable_collection\n    self._cov_matrix_list = variable_collection.cov_matrix_list\n    self._data_vector_list = variable_collection.data_vector_list\n    self._eig_matrix_list = variable_collection.eig_matrix_list\n    self._eig_vals_list = variable_collection.eig_vals_list\n    # We keep track of the number of samples per arm.\n    self._num_samples_list = variable_collection.num_samples_list\n    self._gamma = gamma\n    if self._gamma < 0.0 or self._gamma > 1.0:\n      raise ValueError(\'Forgetting factor `gamma` must be in [0.0, 1.0].\')\n    self._dtype = dtype\n    if dtype not in (tf.float32, tf.float64):\n      raise ValueError(\n          \'Agent dtype should be either `tf.float32 or `tf.float64`.\')\n    self._use_eigendecomp = use_eigendecomp\n    self._tikhonov_weight = tikhonov_weight\n\n    if exploration_policy == ExplorationPolicy.linear_ucb_policy:\n      exploration_strategy = lin_policy.ExplorationStrategy.optimistic\n    elif exploration_policy == (\n        ExplorationPolicy.linear_thompson_sampling_policy):\n      exploration_strategy = lin_policy.ExplorationStrategy.sampling\n    else:\n      raise ValueError(\'Linear bandit agent with policy %s not implemented\' %\n                       exploration_policy)\n    policy = lin_policy.LinearBanditPolicy(\n        action_spec=action_spec,\n        cov_matrix=self._cov_matrix_list,\n        data_vector=self._data_vector_list,\n        num_samples=self._num_samples_list,\n        time_step_spec=time_step_spec,\n        exploration_strategy=exploration_strategy,\n        alpha=alpha,\n        eig_vals=self._eig_vals_list if self._use_eigendecomp else (),\n        eig_matrix=self._eig_matrix_list if self._use_eigendecomp else (),\n        tikhonov_weight=self._tikhonov_weight,\n        add_bias=add_bias,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=emit_log_probability,\n        accepts_per_arm_features=accepts_per_arm_features,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter))\n    training_data_spec = None\n    if accepts_per_arm_features:\n      training_data_spec = bandit_spec_utils.drop_arm_observation(\n          policy.trajectory_spec, observation_and_action_constraint_splitter)\n    super(LinearBanditAgent, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        policy=policy,\n        collect_policy=policy,\n        training_data_spec=training_data_spec,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        train_sequence_length=None)\n\n  @property\n  def num_actions(self):\n    return self._num_actions\n\n  @property\n  def cov_matrix(self):\n    return self._cov_matrix_list\n\n  @property\n  def eig_matrix(self):\n    return self._eig_matrix_list\n\n  @property\n  def eig_vals(self):\n    return self._eig_vals_list\n\n  @property\n  def data_vector(self):\n    return self._data_vector_list\n\n  @property\n  def num_samples(self):\n    return self._num_samples_list\n\n  @property\n  def alpha(self):\n    return self._alpha\n\n  def update_alpha(self, alpha):\n    return tf.compat.v1.assign(self._alpha, alpha)\n\n  @property\n  def theta(self):\n    """"""Returns the matrix of per-arm feature weights.\n\n    The returned matrix has shape (num_actions, context_dim).\n    It\'s equivalent to a stacking of theta vectors from the paper.\n    """"""\n    thetas = []\n    for k in range(self._num_models):\n      thetas.append(\n          tf.squeeze(\n              linalg.conjugate_gradient_solve(\n                  self._cov_matrix_list[k] + self._tikhonov_weight *\n                  tf.eye(self._overall_context_dim, dtype=self._dtype),\n                  tf.expand_dims(self._data_vector_list[k], axis=-1)),\n              axis=-1))\n\n    return tf.stack(thetas, axis=0)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def compute_summaries(self, loss):\n    if self.summaries_enabled:\n      with tf.name_scope(\'Losses/\'):\n        tf.compat.v2.summary.scalar(\n            name=\'loss\', data=loss, step=self.train_step_counter)\n\n      if self._summarize_grads_and_vars:\n        with tf.name_scope(\'Variables/\'):\n          for var in self.policy.variables():\n            var_name = var.name.replace(\':\', \'_\')\n            tf.compat.v2.summary.histogram(\n                name=var_name,\n                data=var,\n                step=self.train_step_counter)\n            tf.compat.v2.summary.scalar(\n                name=var_name + \'_value_norm\',\n                data=tf.linalg.global_norm([var]),\n                step=self.train_step_counter)\n        if self._add_bias:\n          thetas = self.theta\n          biases = thetas[:, self._global_context_dim - 1]\n          bias_list = tf.unstack(biases, axis=0)\n          for i in range(self._num_actions):\n            tf.compat.v2.summary.scalar(\n                name=\'bias/action_\' + str(i),\n                data=bias_list[i],\n                step=self.train_step_counter)\n\n  def _process_experience(self, experience):\n    """"""Given an experience, returns reward, action, observation, and batch size.""""""\n\n    if self._accepts_per_arm_features:\n      return self._process_experience_per_arm(experience)\n    else:\n      return self._process_experience_global(experience)\n\n  def _process_experience_per_arm(self, experience):\n    """"""Processes the experience in case the agent accepts per-arm features.\n\n    In the experience coming from the replay buffer, the reward (and all other\n    elements) have two batch dimensions `batch_size` and `time_steps`, where\n    `time_steps` is the number of driver steps executed in each training loop.\n    We flatten the tensors in order to reflect the effective batch size. Then,\n    all the necessary processing on the observation is done, including splitting\n    the action mask if it is present.\n\n    After the preprocessing, the per-arm part of the observation is copied over\n    from the respective policy info field and concatenated with the global\n    observation. The action tensor will be replaced by zeros, since in the\n    per-arm case, there is only one reward model to update.\n\n    Args:\n      experience: An instance of trajectory. Every element in the trajectory has\n      two batch dimensions.\n\n    Returns:\n      A tuple of reward, action, observation, and batch_size. All the outputs\n        (except `batch_size`) have a single batch dimension of value\n        `batch_size`.\n    """"""\n    reward, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.reward, self._time_step_spec.reward)\n    observation, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.observation, self.training_data_spec.observation)\n\n    if self._observation_and_action_constraint_splitter is not None:\n      observation, _ = self._observation_and_action_constraint_splitter(\n          observation)\n    batch_size = tf.cast(\n        tf.compat.dimension_value(tf.shape(reward)[0]), dtype=tf.int64)\n    global_observation = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    if self._add_bias:\n      # The bias is added via a constant 1 feature.\n      global_observation = tf.concat(\n          [global_observation,\n           tf.ones([batch_size, 1], dtype=global_observation.dtype)],\n          axis=1)\n\n    # The arm observation we train on needs to be copied from the respective\n    # policy info field to the per arm observation field. Pretending there was\n    # only one action, we fill the action field with zeros.\n    action = tf.zeros(shape=[batch_size], dtype=tf.int64)\n    chosen_action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.policy_info.chosen_arm_features,\n        self.policy.info_spec.chosen_arm_features)\n    arm_observation = chosen_action\n    overall_observation = tf.concat([global_observation, arm_observation],\n                                    axis=1)\n    overall_observation = tf.reshape(\n        tf.cast(overall_observation, self._dtype), [batch_size, -1])\n    reward = tf.cast(reward, self._dtype)\n\n    return reward, action, overall_observation, batch_size\n\n  def _process_experience_global(self, experience):\n    """"""Processes the experience in case the agent accepts only global features.\n\n    In the experience coming from the replay buffer, the reward (and all other\n    elements) have two batch dimensions `batch_size` and `time_steps`, where\n    `time_steps` is the number of driver steps executed in each training loop.\n    We flatten the tensors in order to reflect the effective batch size. Then,\n    all the necessary processing on the observation is done, including splitting\n    the action mask if it is present.\n\n    Args:\n      experience: An instance of trajectory. Every element in the trajectory has\n      two batch dimensions.\n\n    Returns:\n      A tuple of reward, action, observation, and batch_size. All the outputs\n        (except `batch_size`) have a single batch dimension of value\n        `batch_size`.\n    """"""\n    reward, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.reward, self._time_step_spec.reward)\n    observation, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.observation, self.training_data_spec.observation)\n    action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.action, self._action_spec)\n    batch_size = tf.cast(\n        tf.compat.dimension_value(tf.shape(reward)[0]), dtype=tf.int64)\n\n    if self._observation_and_action_constraint_splitter is not None:\n      observation, _ = self._observation_and_action_constraint_splitter(\n          observation)\n    if self._add_bias:\n      # The bias is added via a constant 1 feature.\n      observation = tf.concat(\n          [observation,\n           tf.ones([batch_size, 1], dtype=observation.dtype)],\n          axis=1)\n\n    observation = tf.reshape(\n        tf.cast(observation, self._dtype), [batch_size, -1])\n    reward = tf.cast(reward, self._dtype)\n\n    return reward, action, observation, batch_size\n\n  def _distributed_train_step(self, experience, weights=None):\n    """"""Distributed train fn to be passed as input to run().""""""\n    del weights  # unused\n    reward, action, observation, batch_size = self._process_experience(\n        experience)\n    self._train_step_counter.assign_add(batch_size)\n\n    for k in range(self._num_models):\n      diag_mask = tf.linalg.tensor_diag(\n          tf.cast(tf.equal(action, k), self._dtype))\n      observations_for_arm = tf.matmul(diag_mask, observation)\n      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n\n      # Compute local updates for the matrix A and b of this arm.\n      cov_matrix_local_udpate = tf.matmul(\n          observations_for_arm, observations_for_arm, transpose_a=True)\n      data_vector_local_update = bandit_utils.sum_reward_weighted_observations(\n          rewards_for_arm, observations_for_arm)\n\n      def _merge_fn(strategy, per_replica_cov_matrix_update,\n                    per_replica_data_vector_update):\n        """"""Merge the per-replica-updates.""""""\n        # Reduce the per-replica-updates using SUM.\n        # pylint: disable=cell-var-from-loop\n        updates_and_vars = [\n            (per_replica_cov_matrix_update, self._cov_matrix_list[k]),\n            (per_replica_data_vector_update, self._data_vector_list[k])\n        ]\n\n        reduced_updates = strategy.extended.batch_reduce_to(\n            tf.distribute.ReduceOp.SUM, updates_and_vars)\n\n        # Update the model variables.\n        self._cov_matrix_list[k].assign_add(reduced_updates[0])\n        self._data_vector_list[k].assign_add(reduced_updates[1])\n\n        # Compute the eigendecomposition, if needed.\n        if self._use_eigendecomp:\n          eig_vals, eig_matrix = tf.linalg.eigh(self._cov_matrix_list[k])\n          self._eig_vals_list[k].assign(eig_vals)\n          self._eig_matrix_list[k].assign(eig_matrix)\n\n      # Passes the local_updates to the _merge_fn() above that performs custom\n      # computation on the per-replica values.\n      # All replicas pause their execution until merge_call() is done and then,\n      # execution is resumed.\n      replica_context = tf.distribute.get_replica_context()\n      replica_context.merge_call(\n          _merge_fn,\n          args=(cov_matrix_local_udpate, data_vector_local_update))\n\n    loss = -1. * tf.reduce_sum(reward)\n    return tf_agent.LossInfo(loss=(loss), extra=())\n\n  def _train(self, experience, weights=None):\n    """"""Updates the policy based on the data in `experience`.\n\n    Note that `experience` should only contain data points that this agent has\n    not previously seen. If `experience` comes from a replay buffer, this buffer\n    should be cleared between each call to `train`.\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`.\n      weights: Unused.\n\n    Returns:\n        A `LossInfo` containing the loss *before* the training step is taken.\n        In most cases, if `weights` is provided, the entries of this tuple will\n        have been calculated with the weights.  Note that each Agent chooses\n        its own method of applying weights.\n    """"""\n    if tf.distribute.has_strategy():\n      return self._distributed_train_step(experience)\n\n    del weights  # unused\n\n    reward, action, observation, batch_size = self._process_experience(\n        experience)\n\n    for k in range(self._num_models):\n      diag_mask = tf.linalg.tensor_diag(\n          tf.cast(tf.equal(action, k), self._dtype))\n      observations_for_arm = tf.matmul(diag_mask, observation)\n      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n\n      num_samples_for_arm_current = tf.reduce_sum(diag_mask)\n      tf.compat.v1.assign_add(self._num_samples_list[k],\n                              num_samples_for_arm_current)\n      num_samples_for_arm_total = self._num_samples_list[k].read_value()\n\n      # Update the matrix A and b.\n      # pylint: disable=cell-var-from-loop,g-long-lambda\n      def update(cov_matrix, data_vector):\n        return update_a_and_b_with_forgetting(\n            cov_matrix, data_vector, rewards_for_arm, observations_for_arm,\n            self._gamma, self._use_eigendecomp)\n      a_new, b_new, eig_vals, eig_matrix = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0,\n          lambda: update(self._cov_matrix_list[k], self._data_vector_list[k]),\n          lambda: (self._cov_matrix_list[k], self._data_vector_list[k],\n                   self._eig_vals_list[k], self._eig_matrix_list[k]))\n\n      tf.compat.v1.assign(self._cov_matrix_list[k], a_new)\n      tf.compat.v1.assign(self._data_vector_list[k], b_new)\n      tf.compat.v1.assign(self._eig_vals_list[k], eig_vals)\n      tf.compat.v1.assign(self._eig_matrix_list[k], eig_matrix)\n\n    loss = -1. * tf.reduce_sum(reward)\n    self.compute_summaries(loss)\n\n    self._train_step_counter.assign_add(batch_size)\n\n    return tf_agent.LossInfo(loss=(loss), extra=())\n'"
tf_agents/bandits/agents/linear_bandit_agent_test.py,165,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.linear_bandit_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import linear_bandit_agent as linear_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import common\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\':\n              \'_batch1_contextdim10_float32\',\n          \'batch_size\':\n              1,\n          \'context_dim\':\n              10,\n          \'exploration_policy\':\n              linear_agent.ExplorationPolicy.linear_ucb_policy,\n          \'dtype\':\n              tf.float32,\n      }, {\n          \'testcase_name\':\n              \'_batch4_contextdim5_float64_UCB\',\n          \'batch_size\':\n              4,\n          \'context_dim\':\n              5,\n          \'exploration_policy\':\n              linear_agent.ExplorationPolicy.linear_ucb_policy,\n          \'dtype\':\n              tf.float64,\n      }, {\n          \'testcase_name\':\n              \'_batch4_contextdim5_float64_TS\',\n          \'batch_size\':\n              4,\n          \'context_dim\':\n              5,\n          \'exploration_policy\':\n              linear_agent.ExplorationPolicy.linear_thompson_sampling_policy,\n          \'dtype\':\n              tf.float64,\n      }, {\n          \'testcase_name\':\n              \'_batch4_contextdim5_float64_decomp\',\n          \'batch_size\':\n              4,\n          \'context_dim\':\n              5,\n          \'exploration_policy\':\n              linear_agent.ExplorationPolicy.linear_ucb_policy,\n          \'dtype\':\n              tf.float64,\n          \'use_eigendecomp\':\n              True,\n      })\n\n\ndef _get_initial_and_final_steps(batch_size, context_dim):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation + 100.0,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  return initial_step, final_step\n\n\ndef _get_initial_and_final_steps_with_per_arm_features(\n    batch_size,\n    global_context_dim,\n    num_actions,\n    arm_context_dim,\n    apply_action_mask=False,\n    num_actions_feature=False):\n  global_observation = np.array(range(batch_size * global_context_dim)).reshape(\n      [batch_size, global_context_dim])\n  arm_observation = np.array(range(\n      batch_size * num_actions * arm_context_dim)).reshape(\n          [batch_size, num_actions, arm_context_dim])\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  observation = {\n      \'global\':\n          tf.constant(\n              global_observation,\n              dtype=tf.float32,\n              shape=[batch_size, global_context_dim],\n              name=\'global_observation\'),\n      \'per_arm\':\n          tf.constant(\n              arm_observation,\n              dtype=tf.float32,\n              shape=[batch_size, num_actions, arm_context_dim],\n              name=\'arm_observation\')\n  }\n  if num_actions_feature:\n    observation.update({\n        \'num_actions\': tf.ones([batch_size], dtype=tf.int32) * (num_actions - 1)\n    })\n  if apply_action_mask:\n    observation = (observation,\n                   tf.ones([batch_size, num_actions], dtype=tf.int32))\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      observation)\n  observation = {\n      \'global\':\n          tf.constant(\n              global_observation + 100.0,\n              dtype=tf.float32,\n              shape=[batch_size, global_context_dim],\n              name=\'global_observation\'),\n      \'arm\':\n          tf.constant(\n              arm_observation + 100.0,\n              dtype=tf.float32,\n              shape=[batch_size, num_actions, arm_context_dim],\n              name=\'arm_observation\')\n  }\n  if num_actions_feature:\n    observation.update(\n        {\'num_actions\': tf.ones([batch_size], dtype=tf.int32) * num_actions})\n  if apply_action_mask:\n    observation = (observation,\n                   tf.ones([batch_size, num_actions], dtype=tf.int32))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      observation)\n  return initial_step, final_step\n\n\ndef _get_initial_and_final_steps_with_action_mask(batch_size,\n                                                  context_dim,\n                                                  num_actions=None):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  observation = tf.constant(observation, dtype=tf.float32)\n  mask = 1 - tf.eye(batch_size, num_columns=num_actions, dtype=tf.int32)\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation, mask))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation + 100.0, mask))\n  return initial_step, final_step\n\n\ndef _get_action_step(action):\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action), info=policy_utilities.PolicyInfo())\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1), single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LinearBanditAgentTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(LinearBanditAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n\n  @test_cases()\n  def testInitializeAgent(self,\n                          batch_size,\n                          context_dim,\n                          exploration_policy,\n                          dtype,\n                          use_eigendecomp=False):\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n\n  def testInitializeAgentEmptyObservationSpec(self):\n    dtype = tf.float32\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec((), tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=linear_agent.ExplorationPolicy.linear_ucb_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n\n  @test_cases()\n  def testLinearAgentUpdate(self,\n                            batch_size,\n                            context_dim,\n                            exploration_policy,\n                            dtype,\n                            use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    variable_collection = linear_agent.LinearBanditVariableCollection(\n        context_dim, num_actions, use_eigendecomp, dtype)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        variable_collection=variable_collection,\n        use_eigendecomp=use_eigendecomp,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n    final_theta = self.evaluate(agent.theta)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(experience.observation, [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(experience.reward, [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    expected_theta_updated_list = []\n    for _, (observations_for_arm,\n            rewards_for_arm) in enumerate(zip(observations_list, rewards_list)):\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float32)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            observations_for_arm, observations_for_arm, transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, observations_for_arm)\n        return a_new, b_new\n\n      def false_fn():\n        return tf.zeros([context_dim, context_dim]), tf.zeros([context_dim])\n\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0, true_fn, false_fn)\n      theta_new = tf.squeeze(\n          tf.linalg.solve(\n              tf.eye(context_dim) + a_new, tf.expand_dims(b_new, axis=-1)),\n          axis=-1)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n      expected_theta_updated_list.append(self.evaluate(theta_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n    self.assertAllClose(\n        self.evaluate(tf.stack(expected_theta_updated_list)),\n        final_theta,\n        atol=0.1,\n        rtol=0.05)\n\n  @test_cases()\n  def testLinearAgentUpdatePerArmFeatures(self,\n                                          batch_size,\n                                          context_dim,\n                                          exploration_policy,\n                                          dtype,\n                                          use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    global_context_dim = context_dim\n    arm_context_dim = 3\n    initial_step, final_step = (\n        _get_initial_and_final_steps_with_per_arm_features(\n            batch_size,\n            global_context_dim,\n            num_actions,\n            arm_context_dim,\n            num_actions_feature=True))\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(action),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.arange(\n                batch_size * arm_context_dim, dtype=np.float32).reshape(\n                    [batch_size, arm_context_dim])))\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        context_dim, arm_context_dim, num_actions, add_num_actions_feature=True)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        use_eigendecomp=use_eigendecomp,\n        accepts_per_arm_features=True,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    global_observation = experience.observation[\n        bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    arm_observation = experience.policy_info.chosen_arm_features\n    overall_observation = tf.squeeze(\n        tf.concat([global_observation, arm_observation], axis=-1), axis=1)\n    rewards = tf.squeeze(experience.reward, axis=1)\n\n    expected_a_new = tf.matmul(\n        overall_observation, overall_observation, transpose_a=True)\n    expected_b_new = bandit_utils.sum_reward_weighted_observations(\n        rewards, overall_observation)\n    self.assertAllClose(expected_a_new, final_a[0])\n    self.assertAllClose(expected_b_new, final_b[0])\n\n  @test_cases()\n  def testLinearAgentUpdatePerArmFeaturesAndMask(self,\n                                                 batch_size,\n                                                 context_dim,\n                                                 exploration_policy,\n                                                 dtype,\n                                                 use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    global_context_dim = context_dim\n    arm_context_dim = 3\n    initial_step, final_step = (\n        _get_initial_and_final_steps_with_per_arm_features(\n            batch_size,\n            global_context_dim,\n            num_actions,\n            arm_context_dim,\n            apply_action_mask=True))\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(action),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.arange(\n                batch_size * arm_context_dim, dtype=np.float32).reshape(\n                    [batch_size, arm_context_dim])))\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        context_dim, arm_context_dim, num_actions, add_action_mask=True)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        use_eigendecomp=use_eigendecomp,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        accepts_per_arm_features=True,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    global_observation = experience.observation[0][\n        bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    arm_observation = experience.policy_info.chosen_arm_features\n    overall_observation = tf.squeeze(\n        tf.concat([global_observation, arm_observation], axis=-1), axis=1)\n    rewards = tf.squeeze(experience.reward, axis=1)\n\n    expected_a_new = tf.matmul(\n        overall_observation, overall_observation, transpose_a=True)\n    expected_b_new = bandit_utils.sum_reward_weighted_observations(\n        rewards, overall_observation)\n    self.assertAllClose(expected_a_new, final_a[0])\n    self.assertAllClose(expected_b_new, final_b[0])\n\n  @test_cases()\n  def testLinearAgentUpdateWithBias(self,\n                                    batch_size,\n                                    context_dim,\n                                    exploration_policy,\n                                    dtype,\n                                    use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    variable_collection = linear_agent.LinearBanditVariableCollection(\n        context_dim + 1, num_actions, use_eigendecomp, dtype)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        variable_collection=variable_collection,\n        use_eigendecomp=use_eigendecomp,\n        add_bias=True,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n    final_theta = self.evaluate(agent.theta)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(experience.observation, [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(experience.reward, [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    expected_theta_updated_list = []\n    for _, (observations_for_arm,\n            rewards_for_arm) in enumerate(zip(observations_list, rewards_list)):\n      observations_for_arm = tf.concat(\n          [observations_for_arm,\n           tf.ones_like(observations_for_arm[:, 0:1])],\n          axis=1)\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float32)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            observations_for_arm, observations_for_arm, transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, observations_for_arm)\n        return a_new, b_new\n\n      def false_fn():\n        return tf.zeros([context_dim + 1,\n                         context_dim + 1]), tf.zeros([context_dim + 1])\n\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0, true_fn, false_fn)\n      theta_new = tf.squeeze(\n          tf.linalg.solve(a_new + tf.eye(context_dim + 1),\n                          tf.expand_dims(b_new, axis=-1)),\n          axis=-1)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n      expected_theta_updated_list.append(self.evaluate(theta_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n    self.assertAllClose(\n        self.evaluate(tf.stack(expected_theta_updated_list)),\n        final_theta,\n        atol=0.1,\n        rtol=0.05)\n\n  @test_cases()\n  def testLinearAgentUpdateWithMaskedActions(self,\n                                             batch_size,\n                                             context_dim,\n                                             exploration_policy,\n                                             dtype,\n                                             use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        batch_size, context_dim, num_actions=num_actions)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = (tensor_spec.TensorSpec([context_dim], tf.float32),\n                        tensor_spec.TensorSpec([num_actions], tf.int32))\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n\n    def observation_and_action_constraint_splitter(obs):\n      return obs[0], obs[1]\n\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(\n            observation_and_action_constraint_splitter(\n                experience.observation)[0], [batch_size, -1]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(experience.reward, [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    for _, (observations_for_arm,\n            rewards_for_arm) in enumerate(zip(observations_list, rewards_list)):\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float32)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            observations_for_arm, observations_for_arm, transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, observations_for_arm)\n        return a_new, b_new\n\n      def false_fn():\n        return tf.zeros([context_dim, context_dim]), tf.zeros([context_dim])\n\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0, true_fn, false_fn)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n\n  @test_cases()\n  def testLinearAgentUpdateWithForgetting(self,\n                                          batch_size,\n                                          context_dim,\n                                          exploration_policy,\n                                          dtype,\n                                          use_eigendecomp=False):\n    """"""Check that the agent updates for specified actions and rewards.""""""\n    # We should rewrite this test as it currently does not depend on\n    # the value of `gamma`. To properly test the forgetting factor, we need to\n    # call `train` twice.\n    gamma = 0.9\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        gamma=gamma,\n        dtype=dtype,\n        use_eigendecomp=use_eigendecomp)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_info = agent.train(experience)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n    final_eig_vals = self.evaluate(agent.eig_vals)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(experience.observation, [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(experience.reward, [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    expected_eigvals_updated_list = []\n    for _, (observations_for_arm,\n            rewards_for_arm) in enumerate(zip(observations_list, rewards_list)):\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float32)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            observations_for_arm, observations_for_arm, transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, observations_for_arm)\n        eigmatrix_new = tf.constant([], dtype=dtype)\n        eigvals_new = tf.constant([], dtype=dtype)\n        if use_eigendecomp:\n          eigvals_new, eigmatrix_new = tf.linalg.eigh(a_new)\n        return a_new, b_new, eigvals_new, eigmatrix_new\n\n      def false_fn():\n        if use_eigendecomp:\n          return (tf.zeros([context_dim, context_dim]), tf.zeros([context_dim]),\n                  tf.ones([context_dim]), tf.eye(context_dim))\n        else:\n          return (tf.zeros([context_dim, context_dim]), tf.zeros([context_dim]),\n                  tf.constant([], dtype=dtype), tf.constant([], dtype=dtype))\n\n      a_new, b_new, eig_vals_new, _ = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0, true_fn, false_fn)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n      expected_eigvals_updated_list.append(self.evaluate(eig_vals_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n    self.assertAllClose(\n        expected_eigvals_updated_list, final_eig_vals, atol=1e-4, rtol=1e-4)\n\n  @test_cases()\n  def testDistributedLinearAgentUpdate(self,\n                                       batch_size,\n                                       context_dim,\n                                       exploration_policy,\n                                       dtype,\n                                       use_eigendecomp=False):\n    """"""Same as above, but uses the distributed train function of the agent.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n\n    agent = linear_agent.LinearBanditAgent(\n        exploration_policy=exploration_policy,\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        dtype=dtype)\n    self.evaluate(agent.initialize())\n    train_fn = common.function_in_tf1()(agent._distributed_train_step)\n    loss_info = train_fn(experience=experience)\n    self.evaluate(loss_info)\n\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(experience.observation, [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(experience.reward, [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    expected_theta_updated_list = []\n    for _, (observations_for_arm,\n            rewards_for_arm) in enumerate(zip(observations_list, rewards_list)):\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float32)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            observations_for_arm, observations_for_arm, transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, observations_for_arm)\n        return a_new, b_new\n\n      def false_fn():\n        return tf.zeros([context_dim, context_dim]), tf.zeros([context_dim])\n\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0, true_fn, false_fn)\n      theta_new = tf.squeeze(\n          tf.linalg.solve(a_new + tf.eye(context_dim),\n                          tf.expand_dims(b_new, axis=-1)),\n          axis=-1)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n      expected_theta_updated_list.append(self.evaluate(theta_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n\n  def testInitializeRestoreVariableCollection(self):\n    if not tf.executing_eagerly():\n      self.skipTest(\'Test only works in eager mode.\')\n    context_dim = 7\n    num_actions = 5\n    variable_collection = linear_agent.LinearBanditVariableCollection(\n        context_dim=context_dim, num_models=num_actions)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(variable_collection.num_samples_list)\n    checkpoint = tf.train.Checkpoint(variable_collection=variable_collection)\n    checkpoint_dir = self.get_temp_dir()\n    checkpoint_prefix = os.path.join(checkpoint_dir, \'checkpoint\')\n    checkpoint.save(file_prefix=checkpoint_prefix)\n\n    variable_collection.num_samples_list[2].assign(14)\n\n    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n    checkpoint_load_status = checkpoint.restore(latest_checkpoint)\n    self.evaluate(checkpoint_load_status.initialize_or_restore())\n    self.assertEqual(self.evaluate(variable_collection.num_samples_list[2]), 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/linear_thompson_sampling_agent.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implements the Linear Thompson Sampling bandit algorithm.\n\n  Reference:\n  ""Thompson Sampling for Contextual Bandits with Linear Payoffs"",\n  Shipra Agrawal, Navin Goyal, ICML 2013. The actual algorithm implemented is\n  `Algorithm 3` from the supplementary material of the paper from\n  `http://proceedings.mlr.press/v28/agrawal13-supp.pdf`.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import linear_bandit_agent as lin_agent\n\n\n@gin.configurable\nclass LinearThompsonSamplingAgent(lin_agent.LinearBanditAgent):\n  """"""Linear Thompson Sampling Agent.\n\n  Implements the Linear Thompson Sampling Agent from the following paper:\n  ""Thompson Sampling for Contextual Bandits with Linear Payoffs"",\n  Shipra Agrawal, Navin Goyal, ICML 2013. The actual algorithm implemented is\n  `Algorithm 3` from the supplementary material of the paper from\n  `http://proceedings.mlr.press/v28/agrawal13-supp.pdf`.\n\n  In a nutshell, the agent maintains two parameters `weight_covariances` and\n  `parameter_estimators`, and updates them based on experience. The inverse of\n  the weight covariance parameters are updated with the outer product of the\n  observations using the Woodbury inverse matrix update, while the parameter\n  estimators are updated by the reward-weighted observation vectors for every\n  action.\n  """"""\n\n  def __init__(self,\n               time_step_spec,\n               action_spec,\n               alpha=1.0,\n               gamma=1.0,\n               use_eigendecomp=False,\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               observation_and_action_constraint_splitter=None,\n               accepts_per_arm_features=False,\n               debug_summaries=False,\n               summarize_grads_and_vars=False,\n               enable_summaries=True,\n               dtype=tf.float32,\n               name=None):\n    """"""Initialize an instance of `LinearThompsonSamplingAgent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec describing the expected `TimeStep`s.\n      action_spec: A scalar `BoundedTensorSpec` with `int32` or `int64` dtype\n        describing the number of actions for this agent.\n      alpha: (float) positive scalar. This is the exploration parameter that\n        multiplies the confidence intervals.\n      gamma: a float forgetting factor in [0.0, 1.0]. When set to\n        1.0, the algorithm does not forget.\n      use_eigendecomp: whether to use eigen-decomposition or not. The default\n        solver is Conjugate Gradient.\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the agent accepts per-arm\n        features.\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      dtype: The type of the parameters stored and updated by the agent. Should\n        be one of `tf.float32` and `tf.float64`. Defaults to `tf.float32`.\n      name: a name for this instance of `LinearThompsonSamplingAgent`.\n\n    Raises:\n      ValueError if dtype is not one of `tf.float32` or `tf.float64`.\n    """"""\n    super(LinearThompsonSamplingAgent, self).__init__(\n        exploration_policy=(\n            lin_agent.ExplorationPolicy.linear_thompson_sampling_policy),\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        alpha=alpha,\n        gamma=gamma,\n        use_eigendecomp=use_eigendecomp,\n        tikhonov_weight=tikhonov_weight,\n        add_bias=add_bias,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=False,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=accepts_per_arm_features,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        dtype=dtype,\n        name=name)\n'"
tf_agents/bandits/agents/loss_utils.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Loss utility code.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\n\ndef pinball_loss(\n    y_true, y_pred, weights=1.0, scope=None,\n    loss_collection=tf.compat.v1.GraphKeys.LOSSES,\n    reduction=tf.compat.v1.losses.Reduction.SUM_BY_NONZERO_WEIGHTS,\n    quantile=0.5):\n  """"""Adds a Pinball loss for quantile regression.\n\n    ```\n  loss = quantile * (y_true - y_pred)         if y_true > y_pred\n  loss = (quantile - 1) * (y_true - y_pred)   otherwise\n  ```\n\n  See: https://en.wikipedia.org/wiki/Quantile_regression#Quantiles\n\n\n  `weights` acts as a coefficient for the loss. If a scalar is provided, then\n  the loss is simply scaled by the given value. If `weights` is a tensor of size\n  `[batch_size]`, then the total loss for each sample of the batch is rescaled\n  by the corresponding element in the `weights` vector. If the shape of\n  `weights` matches the shape of `predictions`, then the loss of each\n  measurable element of `predictions` is scaled by the corresponding value of\n  `weights`.\n\n  Args:\n    y_true: tensor of true targets.\n    y_pred: tensor of predicted targets.\n    weights: Optional `Tensor` whose rank is either 0, or the same rank as\n      `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n      be either `1`, or the same as the corresponding `losses` dimension).\n    scope: The scope for the operations performed in computing the loss.\n    loss_collection: collection to which the loss will be added.\n    reduction: Type of reduction to apply to loss.\n    quantile: A float between 0. and 1., the quantile we want to regress.\n\n  Returns:\n    Weighted Pinball loss float `Tensor`. If `reduction` is `NONE`, this has the\n    same shape as `labels`; otherwise, it is scalar.\n\n  Raises:\n    ValueError: If the shape of `predictions` doesn\'t match that of `labels` or\n      if the shape of `weights` is invalid.  Also if `labels` or `predictions`\n      is None.\n\n  @compatibility(eager)\n  The `loss_collection` argument is ignored when executing eagerly. Consider\n  holding on to the return value or collecting losses via a `tf.keras.Model`.\n  @end_compatibility\n  """"""\n  if y_true is None:\n    raise ValueError(\'y_true must not be None.\')\n  if y_pred is None:\n    raise ValueError(\'y_pred must not be None.\')\n  with tf.compat.v1.name_scope(scope, \'pinball_loss\',\n                               (y_pred, y_true, weights)) as scope:\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred.get_shape().assert_is_compatible_with(y_true.get_shape())\n    error = tf.subtract(y_true, y_pred)\n    loss_tensor = tf.maximum(quantile * error, (quantile - 1) * error)\n    return tf.compat.v1.losses.compute_weighted_loss(\n        loss_tensor, weights, scope, loss_collection, reduction=reduction)\n'"
tf_agents/bandits/agents/loss_utils_test.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.loss_utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import loss_utils\n\n\ntf.compat.v1.enable_v2_behavior()\n\n\nclass LossUtilsTest(tf.test.TestCase):\n\n  def testBaseCase(self):\n    # Example taken from:\n    # https://en.wikipedia.org/wiki/Quantile_regression\n    # Random variable takes values 1...9 with equal probability.\n    y_true = tf.constant(np.arange(1, 10), dtype=tf.float32)\n\n    # Compute the loss for the median.\n    # We see that the value `y_pred = 5` minimizes the loss.\n\n    p_loss = loss_utils.pinball_loss(\n        y_true, y_pred=3 * tf.ones_like(y_true), quantile=0.5)\n    self.assertNear(24.0, 9.0 / 0.5 * self.evaluate(p_loss), err=1e-3)\n\n    p_loss = loss_utils.pinball_loss(\n        y_true, y_pred=4 * tf.ones_like(y_true), quantile=0.5)\n    self.assertNear(21.0, 9.0 / 0.5 * self.evaluate(p_loss), err=1e-3)\n\n    p_loss = loss_utils.pinball_loss(\n        y_true, y_pred=5 * tf.ones_like(y_true), quantile=0.5)\n    self.assertNear(20.0, 9.0 / 0.5 * self.evaluate(p_loss), err=1e-3)\n\n    p_loss = loss_utils.pinball_loss(\n        y_true, y_pred=6 * tf.ones_like(y_true), quantile=0.5)\n    self.assertNear(21.0, 9.0 / 0.5 * self.evaluate(p_loss), err=1e-3)\n\n    p_loss = loss_utils.pinball_loss(\n        y_true, y_pred=7 * tf.ones_like(y_true), quantile=0.5)\n    self.assertNear(24.0, 9.0 / 0.5 * self.evaluate(p_loss), err=1e-3)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/mixture_agent.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An agent that mixes a list of agents with a constant mixture distribution.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\ndef _dynamic_partition_of_nested_tensors(nested_tensor, partitions,\n                                         num_partitions):\n  """"""This function takes a nested structure and partitions every element of it.\n\n  Specifically it outputs a list of nest that all have the same structure as the\n  original, and every element of the list is a nest that contains a dynamic\n  partition of the corresponding original tensors.\n\n  Note that this function uses tf.dynamic_partition, and thus\n  \'MixtureAgent\' is not compatible with XLA.\n\n  Args:\n    nested_tensor: The input nested structure to partition.\n    partitions: int32 tensor based on which the partitioning happens.\n    num_partitions: The number of expected partitions.\n\n  Returns:\n    A list of nested tensors with the same structure as `nested_tensor`.\n  """"""\n  flattened_tensors = tf.nest.flatten(nested_tensor)\n  partitioned_flat_tensors = [\n      tf.dynamic_partition(\n          data=t, partitions=partitions, num_partitions=num_partitions)\n      for t in flattened_tensors\n  ]\n  list_of_partitions = list(map(list, zip(*partitioned_flat_tensors)))\n  return [\n      tf.nest.pack_sequence_as(nested_tensor, i) for i in list_of_partitions\n  ]\n\n\n@gin.configurable\nclass MixtureAgent(tf_agent.TFAgent):\n  """"""An agent that mixes a set of agents with a given mixture.\n\n  For every data sample, the agent updates the sub-agent that was used to make\n  the action choice in that sample. For this update to happen, the mixture agent\n  needs to have the information on which sub-agent is ""responsible"" for the\n  action. This information is in a policy info field `mixture_agent_id`.\n\n  Note that this agent makes use of `tf.dynamic_partition`, and thus it is not\n  compatible with XLA.\n  """"""\n\n  def __init__(self, mixture_distribution, agents, name=None):\n    """"""Initializes an instance of `MixtureAgent`.\n\n    Args:\n      mixture_distribution: An instance of `tfd.Categorical` distribution. This\n        distribution is used to draw sub-policies by the mixture policy. The\n        parameters of the distribution is trained by the mixture agent.\n      agents: List of instances of TF-Agents bandit agents. These agents will be\n        trained and used to select actions. The length of this list should match\n        that of `mixture_weights`.\n      name: The name of this instance of `MixtureAgent`.\n    """"""\n    tf.Module.__init__(self, name=name)\n    time_step_spec = agents[0].time_step_spec\n    action_spec = agents[0].action_spec\n    self._original_info_spec = agents[0].policy.info_spec\n    error_message = None\n    for agent in agents[1:]:\n      if action_spec != agent.action_spec:\n        error_message = \'Inconsistent action specs.\'\n      if time_step_spec != agent.time_step_spec:\n        error_message = \'Inconsistent time step specs.\'\n      if self._original_info_spec != agent.policy.info_spec:\n        error_message = \'Inconsistent info specs.\'\n    if error_message is not None:\n      raise ValueError(error_message)\n    self._agents = agents\n    self._num_agents = len(agents)\n    self._mixture_distribution = mixture_distribution\n    policies = [agent.collect_policy for agent in agents]\n    policy = mixture_policy.MixturePolicy(mixture_distribution, policies)\n    super(MixtureAgent, self).__init__(\n        time_step_spec, action_spec, policy, policy, train_sequence_length=None)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n    for agent in self._agents:\n      agent.initialize()\n\n  # Subclasses must implement this method.\n  @abc.abstractmethod\n  def _update_mixture_distribution(self, experience):\n    """"""This function updates the mixture weights given training experience.""""""\n    raise NotImplementedError(\'`_update_mixture_distribution` should be \'\n                              \'implemented by subclasses of `MixtureAgent`.\')\n\n  def _train(self, experience, weights=None):\n    del weights  # unused\n\n    reward, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.reward, self._time_step_spec.reward)\n    action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.action, self._action_spec)\n    observation, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.observation, self._time_step_spec.observation)\n    policy_choice, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.policy_info[mixture_policy.MIXTURE_AGENT_ID],\n        self._time_step_spec.reward)\n    original_infos, _ = nest_utils.flatten_multi_batched_nested_tensors(\n        experience.policy_info[mixture_policy.SUBPOLICY_INFO],\n        self._original_info_spec)\n\n    partitioned_nested_infos = nest_utils.batch_nested_tensors(\n        _dynamic_partition_of_nested_tensors(original_infos, policy_choice,\n                                             self._num_agents))\n\n    partitioned_nested_rewards = [\n        nest_utils.batch_nested_tensors(t)\n        for t in _dynamic_partition_of_nested_tensors(reward, policy_choice,\n                                                      self._num_agents)\n    ]\n    partitioned_nested_actions = [\n        nest_utils.batch_nested_tensors(t)\n        for t in _dynamic_partition_of_nested_tensors(action, policy_choice,\n                                                      self._num_agents)\n    ]\n    partitioned_nested_observations = [\n        nest_utils.batch_nested_tensors(t)\n        for t in _dynamic_partition_of_nested_tensors(\n            observation, policy_choice, self._num_agents)\n    ]\n    loss = 0\n    for k in range(self._num_agents):\n      per_policy_experience = trajectory.single_step(\n          observation=partitioned_nested_observations[k],\n          action=partitioned_nested_actions[k],\n          policy_info=partitioned_nested_infos[k],\n          reward=partitioned_nested_rewards[k],\n          discount=tf.zeros_like(partitioned_nested_rewards[k]))\n      loss_info = self._agents[k].train(per_policy_experience)\n      loss += loss_info.loss\n    common.function_in_tf1()(self._update_mixture_distribution)(experience)\n    return tf_agent.LossInfo(loss=(loss), extra=())\n'"
tf_agents/bandits/agents/mixture_agent_test.py,40,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.mixture_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import mixture_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.networks import q_network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\nclass WeightRotatingMixtureAgent(mixture_agent.MixtureAgent):\n  """"""A mixture agent for testing purposes that \'rotates\' the weights.\n\n  In every training step, the agent applies a rotation permuation on the agent\n  mixture weights.\n  """"""\n\n  def _update_mixture_distribution(self, experience):\n    weight_values = tf.identity(self._mixture_distribution.probs)\n    new_values = tf.concat(\n        [weight_values[1:], weight_values[0:1]], 0)\n    self._mixture_distribution.probs.assign(new_values)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def _variables(self):\n    return self._mixture_weights\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_contextdim10_numagents2\',\n          \'batch_size\': 1,\n          \'context_dim\': 10,\n          \'num_agents\': 2,\n      }, {\n          \'testcase_name\': \'_batch4_contextdim5_numagents10\',\n          \'batch_size\': 4,\n          \'context_dim\': 5,\n          \'num_agents\': 10,\n      })\n\n\ndef _get_initial_and_final_steps(batch_size, context_dim):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(\n          observation + 100.0,\n          dtype=tf.float32,\n          shape=[batch_size, context_dim],\n          name=\'observation\'))\n  return initial_step, final_step\n\n\ndef _get_action_step(action, num_agents, num_actions):\n  batch_size = tf.shape(action)[0]\n  choices = tf.random.uniform(\n      shape=tf.shape(action), minval=0, maxval=num_agents - 1, dtype=tf.int32)\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action),\n      info={\n          mixture_policy.MIXTURE_AGENT_ID:\n              choices,\n          mixture_policy.SUBPOLICY_INFO:\n              policy_utilities.PolicyInfo(\n                  predicted_rewards_mean=tf.zeros([batch_size, num_actions]))\n      })\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1), single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass MixtureAgentTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(MixtureAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n\n  @test_cases()\n  def testInitializeAgent(self, batch_size, context_dim, num_agents):\n    num_actions = 7\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agents = [\n        lin_ucb_agent.LinearUCBAgent(time_step_spec, action_spec)\n        for _ in range(num_agents)\n    ]\n    dist = tfd.Categorical(\n        probs=tf.Variable(tf.range(num_agents, dtype=tf.float32)))\n    mixed_agent = WeightRotatingMixtureAgent(dist, agents)\n    self.evaluate(mixed_agent.initialize())\n\n  @test_cases()\n  def testAgentUpdate(self, batch_size, context_dim, num_agents):\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agents = []\n    for _ in range(num_agents):\n      agents.append(\n          lin_ucb_agent.LinearUCBAgent(\n              time_step_spec,\n              action_spec,\n              emit_policy_info=(\n                  policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,)))\n    dist = tfd.Categorical(\n        probs=tf.Variable(tf.range(num_agents, dtype=tf.float32)))\n    mixed_agent = WeightRotatingMixtureAgent(dist, agents)\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action, num_agents, num_actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    self.evaluate(mixed_agent.initialize())\n    loss_info = mixed_agent.train(experience)\n    self.evaluate(loss_info)\n\n  def testAgentWithDifferentSubagentsUpdate(self):\n    num_actions = 3\n    context_dim = 2\n    batch_size = 7\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    agent1 = lin_ucb_agent.LinearUCBAgent(\n        time_step_spec,\n        action_spec,\n        emit_policy_info=(policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,))\n    reward_net = q_network.QNetwork(\n        input_tensor_spec=observation_spec,\n        action_spec=action_spec,\n        fc_layer_params=(4, 3, 2))\n    agent2 = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        time_step_spec,\n        action_spec,\n        reward_network=reward_net,\n        emit_policy_info=(policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,),\n        optimizer=tf.compat.v1.train.GradientDescentOptimizer(\n            learning_rate=0.1),\n        epsilon=0.1)\n    agents = [agent1, agent2]\n    dist = tfd.Categorical(probs=tf.Variable([0., 1.]))\n    mixed_agent = WeightRotatingMixtureAgent(dist, agents)\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action, 2, num_actions)\n    experience = _get_experience(initial_step, action_step, final_step)\n    self.evaluate(mixed_agent.initialize())\n    loss_info = mixed_agent.train(experience)\n    self.evaluate(loss_info)\n\n  @test_cases()\n  def testDynamicPartitionOfNestedTensors(self, batch_size, context_dim,\n                                          num_agents):\n    tensor1 = tf.reshape(\n        tf.range(batch_size * context_dim), shape=[batch_size, context_dim])\n    tensor2 = tf.reshape(\n        tf.range(batch_size * num_agents), shape=[batch_size, num_agents])\n    nested_structure = [{\'a\': tensor1}, tensor2]\n    partition_array = [0, 1] * (batch_size // 2) + [0] * (batch_size % 2)\n    partition = tf.constant(partition_array, dtype=tf.int32)\n    partitioned = mixture_agent._dynamic_partition_of_nested_tensors(\n        nested_structure, partition, num_agents)\n    evaluated = self.evaluate(partitioned)\n    self.assertLen(partitioned, num_agents)\n    for k in range(num_agents):\n      tf.nest.assert_same_structure(evaluated[k], nested_structure)\n    self.assertAllEqual(evaluated[0][0][\'a\'].shape,\n                        [(batch_size + 1) // 2, context_dim])\n    self.assertAllEqual(evaluated[1][0][\'a\'].shape,\n                        [batch_size // 2, context_dim])\n    self.assertAllEqual(evaluated[0][1].shape,\n                        [(batch_size + 1) // 2, num_agents])\n    self.assertAllEqual(evaluated[1][1].shape, [batch_size // 2, num_agents])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/neural_epsilon_greedy_agent.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A neural network based agent that implements epsilon greedy exploration.\n\nImplements an agent based on a neural network that predicts arm rewards.\nThe policy adds epsilon greedy exploration.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import greedy_reward_prediction_agent\nfrom tf_agents.policies import epsilon_greedy_policy\n\n\n@gin.configurable\nclass NeuralEpsilonGreedyAgent(\n    greedy_reward_prediction_agent.GreedyRewardPredictionAgent):\n  """"""A neural network based epsilon greedy agent.\n\n  This agent receives a neural network that it trains to predict rewards. The\n  action is chosen greedily with respect to the prediction with probability\n  `1 - epsilon`, and uniformly randomly with probability `epsilon`.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      reward_network,\n      optimizer,\n      epsilon,\n      observation_and_action_constraint_splitter=None,\n      accepts_per_arm_features=False,\n      constraints=(),\n      # Params for training.\n      error_loss_fn=tf.compat.v1.losses.mean_squared_error,\n      gradient_clipping=None,\n      # Params for debugging.\n      debug_summaries=False,\n      summarize_grads_and_vars=False,\n      enable_summaries=True,\n      emit_policy_info=(),\n      train_step_counter=None,\n      laplacian_matrix=None,\n      laplacian_smoothing_weight=0.001,\n      name=None):\n    """"""Creates a Neural Epsilon Greedy Agent.\n\n    For more details about the Laplacian smoothing regularization, please see\n    the documentation of the `GreedyRewardPredictionAgent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of `BoundedTensorSpec` representing the actions.\n      reward_network: A `tf_agents.network.Network` to be used by the agent. The\n        network will be called with call(observation, step_type) and it is\n        expected to provide a reward prediction for all actions.\n        *Note*: when using `observation_and_action_constraint_splitter`, make\n        sure the `reward_network` is compatible with the network-specific half\n        of the output of the `observation_and_action_constraint_splitter`. In\n        particular, `observation_and_action_constraint_splitter` will be called\n        on the observation before passing to the network.\n      optimizer: The optimizer to use for training.\n      epsilon: A float representing the probability of choosing a random action\n        instead of the greedy action.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      constraints: iterable of constraints objects that are instances of\n        `tf_agents.bandits.agents.NeuralConstraint`.\n      error_loss_fn: A function for computing the error loss, taking parameters\n        labels, predictions, and weights (any function from tf.losses would\n        work). The default is `tf.losses.mean_squared_error`.\n      gradient_clipping: A float representing the norm length to clip gradients\n        (or None for no clipping.)\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      enable_summaries: A Python bool, default True. When False, all summaries\n        (debug or otherwise) should not be written.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      train_step_counter: An optional `tf.Variable` to increment every time the\n        train op is run.  Defaults to the `global_step`.\n      laplacian_matrix: A float `Tensor` shaped `[num_actions, num_actions]`.\n        This holds the Laplacian matrix used to regularize the smoothness of the\n        estimated expected reward function. This only applies to problems where\n        the actions have a graph structure. If `None`, the regularization is not\n        applied.\n      laplacian_smoothing_weight: A float that determines the weight of the\n        regularization term. Note that this has no effect if `laplacian_matrix`\n        above is `None`.\n      name: Python str name of this agent. All variables in this module will\n        fall under that name. Defaults to the class name.\n\n    Raises:\n      ValueError: If the action spec contains more than one action or or it is\n      not a bounded scalar int32 spec with minimum 0.\n    """"""\n    super(NeuralEpsilonGreedyAgent, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        reward_network=reward_network,\n        optimizer=optimizer,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=accepts_per_arm_features,\n        constraints=constraints,\n        error_loss_fn=error_loss_fn,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        enable_summaries=enable_summaries,\n        emit_policy_info=emit_policy_info,\n        train_step_counter=train_step_counter,\n        laplacian_matrix=laplacian_matrix,\n        laplacian_smoothing_weight=laplacian_smoothing_weight,\n        name=name)\n    self._policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n        self._policy, epsilon=epsilon)\n    self._collect_policy = self._policy\n'"
tf_agents/bandits/agents/neural_epsilon_greedy_agent_test.py,22,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for neural_epsilon_greedy_agent.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\n\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, observation_spec, action_spec, name=None):\n    super(DummyNet, self).__init__(observation_spec, state_spec=(), name=name)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[1, 1.5, 2],\n                 [1, 1.5, 4]]),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                [[1], [1], [-10]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass AgentTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(AgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=2)\n    self._observation_spec = self._time_step_spec.observation\n\n  def testPolicyWithEpsilonGreedy(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None,\n        epsilon=0.1)\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    # Batch size 2.\n    self.assertAllEqual([2], action_step.action.shape)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(action_step.action)\n    self.assertIn(actions[0], [0, 1, 2])\n    self.assertIn(actions[1], [0, 1, 2])\n\n  def testPolicyWithEpsilonGreedyAndActionMask(self):\n    reward_net = DummyNet(self._observation_spec, self._action_spec)\n    obs_spec = (tensor_spec.TensorSpec([2], tf.float32),\n                tensor_spec.TensorSpec([3], tf.int32))\n    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        ts.time_step_spec(obs_spec),\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=None,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        epsilon=0.1)\n    observations = (tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n                    tf.constant([[0, 0, 1], [0, 1, 0]], dtype=tf.int32))\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    # Batch size 2.\n    self.assertAllEqual([2], action_step.action.shape)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(action_step.action)\n    self.assertAllEqual(actions, [2, 1])\n\n  def testTrainPerArmAgent(self):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(2, 3, 3)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    reward_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        time_step_spec,\n        self._action_spec,\n        reward_network=reward_net,\n        optimizer=optimizer,\n        epsilon=0.1,\n        accepts_per_arm_features=True)\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(18), shape=[2, 3, 3]), dtype=tf.float32)\n    }\n    time_steps = ts.restart(observations, batch_size=2)\n    policy = agent.policy\n    action_step = policy.action(time_steps)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    actions = self.evaluate(action_step.action)\n    self.assertAllEqual(actions.shape, (2,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/neural_linucb_agent.py,70,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Implements the Neural + LinUCB bandit algorithm.\n\n  Applies LinUCB on top of an encoding network.\n  Since LinUCB is a linear method, the encoding network is used to capture the\n  non-linear relationship between the context features and the expected rewards.\n  The encoding network may be already trained or not; if not trained, the\n  method can optionally train it using epsilon greedy.\n\n  Reference:\n  Carlos Riquelme, George Tucker, Jasper Snoek,\n  `Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep\n  Networks for Thompson Sampling`, ICLR 2018.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.bandits.agents import linear_bandit_agent as linear_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.policies import neural_linucb_policy\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.utils import common\nfrom tf_agents.utils import eager_utils\n\n\nclass NeuralLinUCBVariableCollection(tf.Module):\n  """"""A collection of variables used by `NeuralLinUCBAgent`.""""""\n\n  def __init__(self, num_actions, encoding_dim, dtype=tf.float64, name=None):\n    """"""Initializes an instance of `NeuralLinUCBVariableCollection`.\n\n    Args:\n      num_actions: (int) number of actions the agent acts on.\n      encoding_dim: (int) The dimensionality of the output of the encoding\n        network.\n      dtype: The type of the variables. Should be one of `tf.float32` and\n        `tf.float64`.\n      name:  (string) the name of this instance.\n    """"""\n    tf.Module.__init__(self, name=name)\n    self.actions_from_reward_layer = tf.compat.v2.Variable(\n        True, dtype=tf.bool, name=\'is_action_from_reward_layer\')\n\n    self.cov_matrix_list = []\n    self.data_vector_list = []\n    # We keep track of the number of samples per arm.\n    self.num_samples_list = []\n\n    for k in range(num_actions):\n      self.cov_matrix_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros([encoding_dim, encoding_dim], dtype=dtype),\n              name=\'a_{}\'.format(k)))\n      self.data_vector_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros(encoding_dim, dtype=dtype), name=\'b_{}\'.format(k)))\n      self.num_samples_list.append(\n          tf.compat.v2.Variable(\n              tf.zeros([], dtype=dtype), name=\'num_samples_{}\'.format(k)))\n\n\n@gin.configurable\nclass NeuralLinUCBAgent(tf_agent.TFAgent):\n  """"""An agent implementing the LinUCB algorithm on top of a neural network.\n  """"""\n\n  def __init__(\n      self,\n      time_step_spec,\n      action_spec,\n      encoding_network,\n      encoding_network_num_train_steps,\n      encoding_dim,\n      optimizer,\n      variable_collection=None,\n      alpha=1.0,\n      gamma=1.0,\n      epsilon_greedy=0.0,\n      observation_and_action_constraint_splitter=None,\n      accepts_per_arm_features=False,\n      distributed_train_encoding_network=False,\n      # Params for training.\n      error_loss_fn=tf.compat.v1.losses.mean_squared_error,\n      gradient_clipping=None,\n      # Params for debugging.\n      debug_summaries=False,\n      summarize_grads_and_vars=False,\n      train_step_counter=None,\n      emit_policy_info=(),\n      emit_log_probability=False,\n      dtype=tf.float64,\n      name=None):\n    """"""Initialize an instance of `NeuralLinUCBAgent`.\n\n    Args:\n      time_step_spec: A `TimeStep` spec describing the expected `TimeStep`s.\n      action_spec: A scalar `BoundedTensorSpec` with `int32` or `int64` dtype\n        describing the number of actions for this agent.\n      encoding_network: a Keras network that encodes the observations.\n      encoding_network_num_train_steps: how many training steps to run for\n        training the encoding network before switching to LinUCB. If negative,\n        the encoding network is assumed to be already trained.\n      encoding_dim: the dimension of encoded observations.\n      optimizer: The optimizer to use for training.\n      variable_collection: Instance of `NeuralLinUCBVariableCollection`.\n        Collection of variables to be updated by the agent. If `None`, a new\n        instance of `LinearBanditVariables` will be created. Note that this\n        collection excludes the variables owned by the encoding network.\n      alpha: (float) positive scalar. This is the exploration parameter that\n        multiplies the confidence intervals.\n      gamma: a float forgetting factor in [0.0, 1.0]. When set to\n        1.0, the algorithm does not forget.\n      epsilon_greedy: A float representing the probability of choosing a random\n        action instead of the greedy action.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit agent and\n        policy, and 2) the boolean mask. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      distributed_train_encoding_network: (bool) whether to train the encoding\n        network or not. This applies only in distributed training setting. When\n        set to true this agent will train the encoding network. Otherwise, it\n        will assume the encoding network is already trained and will train\n        LinUCB on top of it.\n      error_loss_fn: A function for computing the error loss, taking parameters\n        labels, predictions, and weights (any function from tf.losses would\n        work). The default is `tf.losses.mean_squared_error`.\n      gradient_clipping: A float representing the norm length to clip gradients\n        (or None for no clipping.)\n      debug_summaries: A Python bool, default False. When True, debug summaries\n        are gathered.\n      summarize_grads_and_vars: A Python bool, default False. When True,\n        gradients and network variable summaries are written during training.\n      train_step_counter: An optional `tf.Variable` to increment every time the\n        train op is run.  Defaults to the `global_step`.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: Whether the NeuralLinUCBPolicy emits\n        log-probabilities or not. Since the policy is deterministic, the\n        probability is just 1.\n      dtype: The type of the parameters stored and updated by the agent. Should\n        be one of `tf.float32` and `tf.float64`. Defaults to `tf.float64`.\n      name: a name for this instance of `NeuralLinUCBAgent`.\n\n    Raises:\n      TypeError if variable_collection is not an instance of\n        `NeuralLinUCBVariableCollection`.\n      ValueError if dtype is not one of `tf.float32` or `tf.float64`.\n    """"""\n    tf.Module.__init__(self, name=name)\n    common.tf_agents_gauge.get_cell(\'TFABandit\').set(True)\n    self._num_actions = bandit_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n    self._num_models = 1 if accepts_per_arm_features else self._num_actions\n    self._observation_and_action_constraint_splitter = (\n        observation_and_action_constraint_splitter)\n    self._accepts_per_arm_features = accepts_per_arm_features\n    self._alpha = alpha\n    if variable_collection is None:\n      variable_collection = NeuralLinUCBVariableCollection(\n          self._num_models, encoding_dim, dtype)\n    elif not isinstance(variable_collection, NeuralLinUCBVariableCollection):\n      raise TypeError(\'Parameter `variable_collection` should be \'\n                      \'of type `NeuralLinUCBVariableCollection`.\')\n    self._variable_collection = variable_collection\n    self._gamma = gamma\n    if self._gamma < 0.0 or self._gamma > 1.0:\n      raise ValueError(\'Forgetting factor `gamma` must be in [0.0, 1.0].\')\n    self._dtype = dtype\n    if dtype not in (tf.float32, tf.float64):\n      raise ValueError(\n          \'Agent dtype should be either `tf.float32 or `tf.float64`.\')\n    self._epsilon_greedy = epsilon_greedy\n\n    reward_layer = tf.keras.layers.Dense(\n        self._num_models,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03),\n        use_bias=False,\n        activation=None,\n        name=\'reward_layer\')\n\n    encoding_network.create_variables()\n    self._encoding_network = encoding_network\n    reward_layer.build(input_shape=tf.TensorShape([None, encoding_dim]))\n    self._reward_layer = reward_layer\n    self._encoding_network_num_train_steps = encoding_network_num_train_steps\n    self._encoding_dim = encoding_dim\n    self._optimizer = optimizer\n    self._error_loss_fn = error_loss_fn\n    self._gradient_clipping = gradient_clipping\n    train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n    self._distributed_train_encoding_network = (\n        distributed_train_encoding_network)\n\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        encoding_network=self._encoding_network,\n        encoding_dim=self._encoding_dim,\n        reward_layer=self._reward_layer,\n        epsilon_greedy=self._epsilon_greedy,\n        actions_from_reward_layer=self.actions_from_reward_layer,\n        cov_matrix=self.cov_matrix,\n        data_vector=self.data_vector,\n        num_samples=self.num_samples,\n        time_step_spec=time_step_spec,\n        alpha=alpha,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=emit_log_probability,\n        accepts_per_arm_features=accepts_per_arm_features,\n        distributed_use_reward_layer=distributed_train_encoding_network,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter))\n\n    training_data_spec = None\n    if accepts_per_arm_features:\n      training_data_spec = bandit_spec_utils.drop_arm_observation(\n          policy.trajectory_spec, observation_and_action_constraint_splitter)\n    super(NeuralLinUCBAgent, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=policy.action_spec,\n        policy=policy,\n        collect_policy=policy,\n        train_sequence_length=None,\n        training_data_spec=training_data_spec,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=train_step_counter)\n\n  @property\n  def num_actions(self):\n    return self._num_actions\n\n  @property\n  def actions_from_reward_layer(self):\n    return self._variable_collection.actions_from_reward_layer\n\n  @property\n  def cov_matrix(self):\n    return self._variable_collection.cov_matrix_list\n\n  @property\n  def data_vector(self):\n    return self._variable_collection.data_vector_list\n\n  @property\n  def num_samples(self):\n    return self._variable_collection.num_samples_list\n\n  @property\n  def alpha(self):\n    return self._alpha\n\n  @property\n  def variables(self):\n    return (self.num_samples + self.cov_matrix + self.data_vector +\n            self._encoding_network.trainable_weights +\n            self._reward_layer.trainable_weights + [self.train_step_counter])\n\n  @alpha.setter\n  def update_alpha(self, alpha):\n    return tf.compat.v1.assign(self._alpha, alpha)\n\n  def _initialize(self):\n    tf.compat.v1.variables_initializer(self.variables)\n\n  def compute_summaries(self, loss):\n    with tf.name_scope(\'Losses/\'):\n      tf.compat.v2.summary.scalar(\n          name=\'total_loss\', data=loss, step=self.train_step_counter)\n\n    if self._summarize_grads_and_vars:\n      with tf.name_scope(\'Variables/\'):\n        trainable_variables = (\n            self._encoding_network.trainable_weights +\n            self._reward_layer.trainable_weights)\n        for var in trainable_variables:\n          tf.compat.v2.summary.histogram(\n              name=var.name.replace(\':\', \'_\'),\n              data=var,\n              step=self.train_step_counter)\n\n  def loss(self, observations, actions, rewards, weights=None, training=False):\n    """"""Computes loss for reward prediction training.\n\n    Args:\n      observations: A batch of observations.\n      actions: A batch of actions.\n      rewards: A batch of rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output batch loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether the loss is being used for training.\n\n    Returns:\n      loss: A `LossInfo` containing the loss for the training step.\n    """"""\n    with tf.name_scope(\'loss\'):\n      encoded_observation, _ = self._encoding_network(\n          observations, training=training)\n      encoded_observation = tf.reshape(\n          encoded_observation,\n          shape=[-1, self._encoding_dim])\n      predicted_rewards = self._reward_layer(\n          encoded_observation, training=training)\n      chosen_actions_predicted_rewards = common.index_with_actions(\n          predicted_rewards,\n          tf.cast(actions, dtype=tf.int32))\n\n      loss = self._error_loss_fn(rewards,\n                                 chosen_actions_predicted_rewards,\n                                 weights if weights else 1)\n      if self._summarize_grads_and_vars:\n        with tf.name_scope(\'Per_arm_loss/\'):\n          for k in range(self._num_models):\n            loss_mask_for_arm = tf.cast(tf.equal(actions, k), tf.float32)\n            loss_for_arm = self._error_loss_fn(\n                rewards,\n                chosen_actions_predicted_rewards,\n                weights=loss_mask_for_arm)\n            tf.compat.v2.summary.scalar(\n                name=\'loss_arm_\' + str(k),\n                data=loss_for_arm,\n                step=self.train_step_counter)\n\n    return tf_agent.LossInfo(loss, extra=())\n\n  def compute_loss_using_reward_layer(\n      self, observation, action, reward, weights, training=False):\n    """"""Computes loss using the reward layer.\n\n    Args:\n      observation: A batch of observations.\n      action: A batch of actions.\n      reward: A batch of rewards.\n      weights: Optional scalar or elementwise (per-batch-entry) importance\n        weights.  The output batch loss will be scaled by these weights, and\n        the final scalar loss is the mean of these values.\n      training: Whether the loss is being used for training.\n\n    Returns:\n      loss: A `LossInfo` containing the loss for the training step.\n    """"""\n    # Update the neural network params.\n    with tf.GradientTape() as tape:\n      loss_info = self.loss(\n          observation, action, reward, weights, training=training)\n    tf.debugging.check_numerics(loss_info[0], \'Loss is inf or nan\')\n    if self._summarize_grads_and_vars:\n      self.compute_summaries(loss_info.loss)\n    variables_to_train = (self._encoding_network.trainable_weights +\n                          self._reward_layer.trainable_weights)\n    if not variables_to_train:\n      raise ValueError(\'No variable to train in the agent.\')\n\n    grads = tape.gradient(loss_info.loss, variables_to_train)\n    grads_and_vars = tuple(zip(grads, variables_to_train))\n    if self._gradient_clipping is not None:\n      grads_and_vars = eager_utils.clip_gradient_norms(\n          grads_and_vars, self._gradient_clipping)\n\n    if self._summarize_grads_and_vars:\n      with tf.name_scope(\'Reward_network/\'):\n        eager_utils.add_variables_summaries(grads_and_vars,\n                                            self.train_step_counter)\n        eager_utils.add_gradients_summaries(grads_and_vars,\n                                            self.train_step_counter)\n\n    self._optimizer.apply_gradients(grads_and_vars)\n    self.train_step_counter.assign_add(1)\n\n    return loss_info\n\n  def compute_loss_using_linucb(self, observation, action, reward, weights,\n                                training=False):\n    """"""Computes the loss using LinUCB.\n\n    Args:\n      observation: A batch of observations.\n      action: A batch of actions.\n      reward: A batch of rewards.\n      weights: unused weights.\n      training: Whether the loss is being used to train.\n\n    Returns:\n      loss: A `LossInfo` containing the loss for the training step.\n    """"""\n    del weights  # unused\n\n    # The network is trained now. Update the covariance matrix.\n    encoded_observation, _ = self._encoding_network(\n        observation, training=training)\n    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n    encoded_observation = tf.reshape(\n        encoded_observation, shape=[-1, self._encoding_dim])\n    for k in range(self._num_models):\n      diag_mask = tf.linalg.tensor_diag(\n          tf.cast(tf.equal(action, k), self._dtype))\n      observations_for_arm = tf.matmul(diag_mask, encoded_observation)\n      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n\n      num_samples_for_arm_current = tf.reduce_sum(diag_mask)\n      tf.compat.v1.assign_add(self.num_samples[k], num_samples_for_arm_current)\n      num_samples_for_arm_total = self.num_samples[k].read_value()\n\n      # Update the matrix A and b.\n      # pylint: disable=cell-var-from-loop\n      def update(cov_matrix, data_vector):\n        a_new, b_new, _, _ = linear_agent.update_a_and_b_with_forgetting(\n            cov_matrix, data_vector, rewards_for_arm, observations_for_arm,\n            self._gamma)\n        return a_new, b_new\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0,\n          lambda: update(self.cov_matrix[k], self.data_vector[k]),\n          lambda: (self.cov_matrix[k], self.data_vector[k]))\n      tf.compat.v1.assign(self.cov_matrix[k], a_new)\n      tf.compat.v1.assign(self.data_vector[k], b_new)\n\n    loss_tensor = tf.cast(-1. * tf.reduce_sum(reward), dtype=tf.float32)\n    loss_info = tf_agent.LossInfo(loss=loss_tensor, extra=())\n    self.train_step_counter.assign_add(1)\n    return loss_info\n\n  def compute_loss_using_linucb_distributed(\n      self, observation, action, reward, weights, training=False):\n    """"""Computes the loss using LinUCB distributively.\n\n    Args:\n      observation: A batch of observations.\n      action: A batch of actions.\n      reward: A batch of rewards.\n      weights: unused weights.\n      training: Whether the loss is being used to train.\n\n    Returns:\n      loss: A `LossInfo` containing the loss for the training step.\n    """"""\n    del weights  # unused\n\n    # The network is trained now. Update the covariance matrix.\n    encoded_observation, _ = self._encoding_network(\n        observation, training=training)\n    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n    encoded_observation = tf.reshape(\n        encoded_observation, shape=[-1, self._encoding_dim])\n\n    self._train_step_counter.assign_add(1)\n\n    for k in range(self._num_models):\n      diag_mask = tf.linalg.tensor_diag(\n          tf.cast(tf.equal(action, k), self._dtype))\n      observations_for_arm = tf.matmul(diag_mask, encoded_observation)\n      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n\n      # Compute local updates for the matrix A and b of this arm.\n      cov_matrix_local_udpate = tf.matmul(\n          observations_for_arm, observations_for_arm, transpose_a=True)\n      data_vector_local_update = bandit_utils.sum_reward_weighted_observations(\n          rewards_for_arm, observations_for_arm)\n\n      def _merge_fn(strategy, per_replica_cov_matrix_update,\n                    per_replica_data_vector_update):\n        """"""Merge the per-replica-updates.""""""\n        # Reduce the per-replica-updates using SUM.\n        # pylint: disable=cell-var-from-loop\n        updates_and_vars = [\n            (per_replica_cov_matrix_update, self.cov_matrix[k]),\n            (per_replica_data_vector_update, self.data_vector[k])\n        ]\n\n        reduced_updates = strategy.extended.batch_reduce_to(\n            tf.distribute.ReduceOp.SUM, updates_and_vars)\n\n        # Update the model variables.\n        self.cov_matrix[k].assign(\n            self._gamma * self.cov_matrix[k] + reduced_updates[0])\n        self.data_vector[k].assign(\n            self._gamma * self.data_vector[k] + reduced_updates[1])\n\n      # Passes the local_updates to the _merge_fn() above that performs custom\n      # computation on the per-replica values.\n      # All replicas pause their execution until merge_call() is done and then,\n      # execution is resumed.\n      replica_context = tf.distribute.get_replica_context()\n      replica_context.merge_call(\n          _merge_fn,\n          args=(cov_matrix_local_udpate, data_vector_local_update))\n\n    loss = -1. * tf.reduce_sum(reward)\n    return tf_agent.LossInfo(loss=(loss), extra=())\n\n  def _train(self, experience, weights=None):\n    """"""Updates the policy based on the data in `experience`.\n\n    Note that `experience` should only contain data points that this agent has\n    not previously seen. If `experience` comes from a replay buffer, this buffer\n    should be cleared between each call to `train`.\n\n    Args:\n      experience: A batch of experience data in the form of a `Trajectory`.\n      weights: (optional) sample weights.\n\n    Returns:\n        A `LossInfo` containing the loss *before* the training step is taken.\n        In most cases, if `weights` is provided, the entries of this tuple will\n        have been calculated with the weights.  Note that each Agent chooses\n        its own method of applying weights.\n    """"""\n    (observation, action,\n     reward) = bandit_utils.process_experience_for_neural_agents(\n         experience, self._observation_and_action_constraint_splitter,\n         self._accepts_per_arm_features, self.training_data_spec)\n    reward = tf.cast(reward, self._dtype)\n\n    if tf.distribute.has_strategy():\n      if self._distributed_train_encoding_network:\n        loss_info = self.compute_loss_using_reward_layer(\n            observation, action, reward, weights, training=True)\n      else:\n        loss_info = self.compute_loss_using_linucb_distributed(\n            observation, action, reward, weights, training=True)\n      return loss_info\n\n    tf.compat.v1.assign(\n        self.actions_from_reward_layer,\n        tf.less(self._train_step_counter,\n                self._encoding_network_num_train_steps))\n\n    def use_actions_from_reward_layer():\n      return self.compute_loss_using_reward_layer(\n          observation, action, reward, weights, training=True)\n\n    def no_actions_from_reward_layer():\n      return self.compute_loss_using_linucb(\n          observation, action, reward, weights, training=True)\n\n    loss_info = tf.cond(\n        self.actions_from_reward_layer,\n        use_actions_from_reward_layer,\n        no_actions_from_reward_layer)\n    return loss_info\n'"
tf_agents/bandits/agents/neural_linucb_agent_test.py,118,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.neural_linucb_agent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import neural_linucb_agent\nfrom tf_agents.bandits.agents import utils as bandit_utils\nfrom tf_agents.bandits.drivers import driver_utils\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import common\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\n\ntfd = tfp.distributions\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, observation_spec, encoding_dim=10):\n    super(DummyNet, self).__init__(\n        observation_spec, state_spec=(), name=\'DummyNet\')\n    context_dim = observation_spec.shape[0]\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            encoding_dim,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                np.ones([context_dim, encoding_dim])),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                np.zeros([encoding_dim])))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_contextdim10\',\n          \'batch_size\': 1,\n          \'context_dim\': 10,\n      }, {\n          \'testcase_name\': \'_batch4_contextdim5\',\n          \'batch_size\': 4,\n          \'context_dim\': 5,\n      })\n\n\ndef _get_initial_and_final_steps(batch_size, context_dim):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST, dtype=tf.int32, shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(observation, dtype=tf.float32,\n                  shape=[batch_size, context_dim], name=\'observation\'))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST, dtype=tf.int32, shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      tf.constant(observation + 100.0, dtype=tf.float32,\n                  shape=[batch_size, context_dim], name=\'observation\'))\n  return initial_step, final_step\n\n\ndef _get_initial_and_final_steps_with_action_mask(batch_size,\n                                                  context_dim,\n                                                  num_actions=None):\n  observation = np.array(range(batch_size * context_dim)).reshape(\n      [batch_size, context_dim])\n  observation = tf.constant(observation, dtype=tf.float32)\n  mask = 1 - tf.eye(batch_size, num_columns=num_actions, dtype=tf.int32)\n  reward = np.random.uniform(0.0, 1.0, [batch_size])\n  initial_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.FIRST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation, mask))\n  final_step = time_step.TimeStep(\n      tf.constant(\n          time_step.StepType.LAST,\n          dtype=tf.int32,\n          shape=[batch_size],\n          name=\'step_type\'),\n      tf.constant(reward, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n      tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n      (observation + 100.0, mask))\n  return initial_step, final_step\n\n\ndef _get_action_step(action):\n  return policy_step.PolicyStep(\n      action=tf.convert_to_tensor(action),\n      info=policy_utilities.PolicyInfo())\n\n\ndef _get_experience(initial_step, action_step, final_step):\n  single_experience = driver_utils.trajectory_for_bandit(\n      initial_step, action_step, final_step)\n  # Adds a \'time\' dimension.\n  return tf.nest.map_structure(\n      lambda x: tf.expand_dims(tf.convert_to_tensor(x), 1),\n      single_experience)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass NeuralLinUCBAgentTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(NeuralLinUCBAgentTest, self).setUp()\n    tf.compat.v1.enable_resource_variables()\n\n  @test_cases()\n  def testInitializeAgentNumTrainSteps0(self, batch_size, context_dim):\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n\n    encoder = DummyNet(observation_spec)\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=0,\n        encoding_dim=10,\n        optimizer=None)\n    self.evaluate(agent.initialize())\n\n  @test_cases()\n  def testInitializeAgentNumTrainSteps10(self, batch_size, context_dim):\n    num_actions = 5\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n\n    encoder = DummyNet(observation_spec)\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=10,\n        encoding_dim=10,\n        optimizer=None)\n    self.evaluate(agent.initialize())\n\n  @test_cases()\n  def testNeuralLinUCBUpdateNumTrainSteps0(self, batch_size=1, context_dim=10):\n    """"""Check NeuralLinUCBAgent updates when behaving like LinUCB.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoder = DummyNet(observation_spec)\n    encoding_dim = 10\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=0,\n        encoding_dim=encoding_dim,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2))\n\n    loss_info = agent.train(experience)\n    self.evaluate(agent.initialize())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(tf.cast(experience.observation, tf.float64),\n                        [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(tf.cast(experience.reward, tf.float64), [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    for _, (observations_for_arm, rewards_for_arm) in enumerate(zip(\n        observations_list, rewards_list)):\n\n      encoded_observations_for_arm, _ = encoder(observations_for_arm)\n      encoded_observations_for_arm = tf.cast(\n          encoded_observations_for_arm, dtype=tf.float64)\n\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float64)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            encoded_observations_for_arm,\n            encoded_observations_for_arm,\n            transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, encoded_observations_for_arm)\n        return a_new, b_new\n      def false_fn():\n        return (tf.zeros([encoding_dim, encoding_dim], dtype=tf.float64),\n                tf.zeros([encoding_dim], dtype=tf.float64))\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0,\n          true_fn,\n          false_fn)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n\n  @test_cases()\n  def testNeuralLinUCBUpdateDistributed(self, batch_size=1, context_dim=10):\n    """"""Same as above but with distributed LinUCB updates.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoder = DummyNet(observation_spec)\n    encoding_dim = 10\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=0,\n        encoding_dim=encoding_dim,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2))\n\n    self.evaluate(agent.initialize())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    # Call the distributed LinUCB training instead of agent.train().\n    train_fn = common.function_in_tf1()(\n        agent.compute_loss_using_linucb_distributed)\n    reward = tf.cast(experience.reward, agent._dtype)\n    loss_info = train_fn(\n        experience.observation, action, reward, weights=None)\n    self.evaluate(loss_info)\n    final_a = self.evaluate(agent.cov_matrix)\n    final_b = self.evaluate(agent.data_vector)\n\n    # Compute the expected updated estimates.\n    observations_list = tf.dynamic_partition(\n        data=tf.reshape(tf.cast(experience.observation, tf.float64),\n                        [batch_size, context_dim]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    rewards_list = tf.dynamic_partition(\n        data=tf.reshape(tf.cast(experience.reward, tf.float64), [batch_size]),\n        partitions=tf.convert_to_tensor(action),\n        num_partitions=num_actions)\n    expected_a_updated_list = []\n    expected_b_updated_list = []\n    for _, (observations_for_arm, rewards_for_arm) in enumerate(zip(\n        observations_list, rewards_list)):\n\n      encoded_observations_for_arm, _ = encoder(observations_for_arm)\n      encoded_observations_for_arm = tf.cast(\n          encoded_observations_for_arm, dtype=tf.float64)\n\n      num_samples_for_arm_current = tf.cast(\n          tf.shape(rewards_for_arm)[0], tf.float64)\n      num_samples_for_arm_total = num_samples_for_arm_current\n\n      # pylint: disable=cell-var-from-loop\n      def true_fn():\n        a_new = tf.matmul(\n            encoded_observations_for_arm,\n            encoded_observations_for_arm,\n            transpose_a=True)\n        b_new = bandit_utils.sum_reward_weighted_observations(\n            rewards_for_arm, encoded_observations_for_arm)\n        return a_new, b_new\n      def false_fn():\n        return (tf.zeros([encoding_dim, encoding_dim], dtype=tf.float64),\n                tf.zeros([encoding_dim], dtype=tf.float64))\n      a_new, b_new = tf.cond(\n          tf.squeeze(num_samples_for_arm_total) > 0,\n          true_fn,\n          false_fn)\n\n      expected_a_updated_list.append(self.evaluate(a_new))\n      expected_b_updated_list.append(self.evaluate(b_new))\n\n    # Check that the actual updated estimates match the expectations.\n    self.assertAllClose(expected_a_updated_list, final_a)\n    self.assertAllClose(expected_b_updated_list, final_b)\n\n  @test_cases()\n  def testNeuralLinUCBUpdateNumTrainSteps10(self, batch_size=1, context_dim=10):\n    """"""Check NeuralLinUCBAgent updates when behaving like eps-greedy.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps(\n        batch_size, context_dim)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoder = DummyNet(observation_spec)\n    encoding_dim = 10\n    variable_collection = neural_linucb_agent.NeuralLinUCBVariableCollection(\n        num_actions, encoding_dim)\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=10,\n        encoding_dim=encoding_dim,\n        variable_collection=variable_collection,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001))\n\n    loss_info, _ = agent.train(experience)\n    self.evaluate(agent.initialize())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_value = self.evaluate(loss_info)\n    self.assertGreater(loss_value, 0.0)\n\n  @test_cases()\n  def testNeuralLinUCBUpdateNumTrainSteps10MaskedActions(\n      self, batch_size=1, context_dim=10):\n    """"""Check updates when behaving like eps-greedy and using masked actions.""""""\n\n    # Construct a `Trajectory` for the given action, observation, reward.\n    num_actions = 5\n    initial_step, final_step = _get_initial_and_final_steps_with_action_mask(\n        batch_size, context_dim, num_actions)\n    action = np.random.randint(num_actions, size=batch_size, dtype=np.int32)\n    action_step = _get_action_step(action)\n    experience = _get_experience(initial_step, action_step, final_step)\n\n    # Construct an agent and perform the update.\n    observation_spec = (tensor_spec.TensorSpec([context_dim], tf.float32),\n                        tensor_spec.TensorSpec([num_actions], tf.int32))\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoder = DummyNet(observation_spec[0])\n    encoding_dim = 10\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=10,\n        encoding_dim=encoding_dim,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001),\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]))\n\n    loss_info, _ = agent.train(experience)\n    self.evaluate(agent.initialize())\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    loss_value = self.evaluate(loss_info)\n    self.assertGreater(loss_value, 0.0)\n\n  def testInitializeRestoreVariableCollection(self):\n    if not tf.executing_eagerly():\n      self.skipTest(\'Test only works in eager mode.\')\n    num_actions = 5\n    encoding_dim = 7\n    variable_collection = neural_linucb_agent.NeuralLinUCBVariableCollection(\n        num_actions=num_actions, encoding_dim=encoding_dim)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(variable_collection.num_samples_list)\n    checkpoint = tf.train.Checkpoint(variable_collection=variable_collection)\n    checkpoint_dir = self.get_temp_dir()\n    checkpoint_prefix = os.path.join(checkpoint_dir, \'checkpoint\')\n    checkpoint.save(file_prefix=checkpoint_prefix)\n\n    variable_collection.actions_from_reward_layer.assign(False)\n\n    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n    checkpoint_load_status = checkpoint.restore(latest_checkpoint)\n    self.evaluate(checkpoint_load_status.initialize_or_restore())\n    self.assertEqual(\n        self.evaluate(variable_collection.actions_from_reward_layer), True)\n\n  def testTrainPerArmAgentWithMask(self):\n    num_actions = 5\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        2, 3, num_actions, add_action_mask=True)\n    time_step_spec = time_step.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoding_dim = 10\n    encoder = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec[0], (4, 3), (3, 4), (4, 2), encoding_dim))\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=10,\n        encoding_dim=encoding_dim,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]),\n        accepts_per_arm_features=True,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001))\n    observations = ({\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(30), shape=[2, 5, 3]), dtype=tf.float32)\n    }, tf.ones(shape=(2, num_actions), dtype=tf.int32))\n    actions = np.array([0, 3], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step = time_step.TimeStep(\n        tf.constant(\n            time_step.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[2],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        observations)\n    final_step = time_step.TimeStep(\n        tf.constant(\n            time_step.StepType.LAST,\n            dtype=tf.int32,\n            shape=[2],\n            name=\'step_type\'),\n        tf.constant(rewards, dtype=tf.float32, name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        observations)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(actions),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.array([[1, 2, 3], [3, 2, 1]],\n                                         dtype=np.float32)))\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_info, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    loss_value = self.evaluate(loss_info)\n    self.assertGreater(loss_value, 0.0)\n\n  def testTrainPerArmAgentVariableActions(self):\n    num_actions = 5\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        2, 3, num_actions, add_num_actions_feature=True)\n    time_step_spec = time_step.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions - 1)\n    encoding_dim = 10\n    encoder = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2), encoding_dim))\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        encoding_network=encoder,\n        encoding_network_num_train_steps=10,\n        encoding_dim=encoding_dim,\n        accepts_per_arm_features=True,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.001))\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            tf.cast(\n                tf.reshape(tf.range(30), shape=[2, 5, 3]), dtype=tf.float32),\n        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n            tf.constant([3, 4], dtype=tf.int32)\n    }\n    actions = np.array([0, 3], dtype=np.int32)\n    rewards = np.array([0.5, 3.0], dtype=np.float32)\n    initial_step = time_step.TimeStep(\n        tf.constant(\n            time_step.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[2],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[2], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        observations)\n    final_step = time_step.TimeStep(\n        tf.constant(\n            time_step.StepType.LAST,\n            dtype=tf.int32,\n            shape=[2],\n            name=\'step_type\'),\n        tf.constant(rewards, dtype=tf.float32, name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[2], name=\'discount\'),\n        observations)\n    action_step = policy_step.PolicyStep(\n        action=tf.convert_to_tensor(actions),\n        info=policy_utilities.PerArmPolicyInfo(\n            chosen_arm_features=np.array([[1, 2, 3], [3, 2, 1]],\n                                         dtype=np.float32)))\n    experience = _get_experience(initial_step, action_step, final_step)\n    loss_info, _ = agent.train(experience, None)\n    self.evaluate(tf.compat.v1.initialize_all_variables())\n    loss_value = self.evaluate(loss_info)\n    self.assertGreater(loss_value, 0.0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/static_mixture_agent.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An agent that mixes a list of agents with a constant mixture distribution.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\n\nfrom tf_agents.bandits.agents import mixture_agent\n\n\n@gin.configurable\nclass StaticMixtureAgent(mixture_agent.MixtureAgent):\n  """"""An agent that mixes a set of agents with a given static mixture.\n\n  For every data sample, the agent updates the sub-agent that was used to make\n  the action choice in that sample. For this update to happen, the mixture agent\n  needs to have the information on which sub-agent is ""responsible"" for the\n  action. This information is in a policy info field `mixture_agent_id`.\n\n  Note that this agent makes use of `tf.dynamic_partition`, and thus it is not\n  compatible with XLA.\n  """"""\n\n  def _update_mixture_distribution(self, experience):\n    pass\n'"
tf_agents/bandits/agents/utils.py,25,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Common utility code and linear algebra functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import nest_utils\n\n\ndef sum_reward_weighted_observations(r, x):\n  """"""Calculates an update used by some Bandit algorithms.\n\n  Given an observation `x` and corresponding reward `r`, the weigthed\n  observations vector (denoted `b` here) should be updated as `b = b + r * x`.\n  This function calculates the sum of weighted rewards for batched\n  observations `x`.\n\n  Args:\n    r: a `Tensor` of shape [`batch_size`]. This is the rewards of the batched\n      observations.\n    x: a `Tensor` of shape [`batch_size`, `context_dim`]. This is the matrix\n      with the (batched) observations.\n\n  Returns:\n    The update that needs to be added to `b`. Has the same shape as `b`.\n    If the observation matrix `x` is empty, a zero vector is returned.\n  """"""\n  batch_size = tf.shape(x)[0]\n\n  return tf.reduce_sum(tf.reshape(r, [batch_size, 1]) * x, axis=0)\n\n\ndef get_num_actions_from_tensor_spec(action_spec):\n  """"""Validates `action_spec` and returns number of actions.\n\n  `action_spec` must specify a scalar int32 or int64 with minimum zero.\n\n  Args:\n    action_spec: a `TensorSpec`.\n\n  Returns:\n    The number of actions described by `action_spec`.\n\n  Raises:\n    ValueError: if `action_spec` is not an bounded scalar int32 or int64 spec\n      with minimum 0.\n  """"""\n  if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n    raise ValueError(\'Action spec must be a `BoundedTensorSpec`; \'\n                     \'got {}\'.format(type(action_spec)))\n  if action_spec.shape.rank != 0:\n    raise ValueError(\'Action spec must be a scalar; \'\n                     \'got shape{}\'.format(action_spec.shape))\n  if action_spec.dtype not in (tf.int32, tf.int64):\n    raise ValueError(\'Action spec must be have dtype int32 or int64; \'\n                     \'got {}\'.format(action_spec.dtype))\n  if action_spec.minimum != 0:\n    raise ValueError(\'Action spec must have minimum 0; \'\n                     \'got {}\'.format(action_spec.minimum))\n  return action_spec.maximum + 1\n\n\n@gin.configurable\ndef build_laplacian_over_ordinal_integer_actions(action_spec):\n  """"""Build the unnormalized Laplacian matrix over ordinal integer actions.\n\n  Assuming integer actions, this functions builds the (unnormalized) Laplacian\n  matrix of the graph implied over the action space. The graph vertices are the\n  integers {0...action_spec.maximum - 1}. Two vertices are adjacent if they\n  correspond to consecutive integer actions. The `action_spec` must specify\n  a scalar int32 or int64 with minimum zero.\n\n  Args:\n    action_spec: a `BoundedTensorSpec`.\n\n  Returns:\n    The graph Laplacian matrix (float tensor) of size equal to the number of\n    actions. The diagonal elements are equal to 2 and the off-diagonal elements\n    are equal to -1.\n\n  Raises:\n    ValueError: if `action_spec` is not a bounded scalar int32 or int64 spec\n      with minimum 0.\n  """"""\n  num_actions = get_num_actions_from_tensor_spec(action_spec)\n  adjacency_matrix = np.zeros([num_actions, num_actions])\n  for i in range(num_actions - 1):\n    adjacency_matrix[i, i + 1] = 1.0\n    adjacency_matrix[i + 1, i] = 1.0\n  laplacian_matrix = np.diag(np.sum(adjacency_matrix,\n                                    axis=0)) - adjacency_matrix\n  return laplacian_matrix\n\n\ndef compute_pairwise_distances(input_vecs):\n  """"""Compute the pairwise distances matrix.\n\n  Given input embedding vectors, this utility computes the (squared) pairwise\n  distances matrix.\n\n  Args:\n    input_vecs: a `Tensor`. Input embedding vectors (one per row).\n\n  Returns:\n    The (squared) pairwise distances matrix. A dense float `Tensor` of shape\n    [`num_vectors`, `num_vectors`], where `num_vectors` is the number of input\n    embedding vectors.\n  """"""\n  r = tf.reduce_sum(input_vecs * input_vecs, axis=1, keepdims=True)\n  pdistance_matrix = (\n      r - 2 * tf.matmul(input_vecs, input_vecs, transpose_b=True)\n      + tf.transpose(r))\n  return tf.cast(pdistance_matrix, dtype=tf.float32)\n\n\n@gin.configurable\ndef build_laplacian_nearest_neighbor_graph(input_vecs, k=1):\n  """"""Build the Laplacian matrix of a nearest neighbor graph.\n\n  Given input embedding vectors, this utility returns the Laplacian matrix of\n  the induced k-nearest-neighbor graph.\n\n  Args:\n    input_vecs: a `Tensor`. Input embedding vectors (one per row).  Shaped\n      `[num_vectors, ...]`.\n    k : an integer. Number of nearest neighbors to use.\n\n  Returns:\n    The graph Laplacian matrix. A dense float `Tensor` of shape\n    `[num_vectors, num_vectors]`, where `num_vectors` is the number of input\n    embedding vectors (`Tensor`).\n  """"""\n  num_actions = tf.shape(input_vecs)[0]\n  pdistance_matrix = compute_pairwise_distances(input_vecs)\n  sorted_indices = tf.argsort(values=pdistance_matrix)\n  selected_indices = tf.reshape(sorted_indices[:, 1 : k + 1], [-1, 1])\n  rng = tf.tile(\n      tf.expand_dims(tf.range(num_actions), axis=-1), [1, k])\n  rng = tf.reshape(rng, [-1, 1])\n  full_indices = tf.concat([rng, selected_indices], axis=1)\n  adjacency_matrix = tf.zeros([num_actions, num_actions], dtype=tf.float32)\n  adjacency_matrix = tf.tensor_scatter_nd_update(\n      tensor=adjacency_matrix,\n      indices=full_indices,\n      updates=tf.ones([k * num_actions], dtype=tf.float32))\n  # Symmetrize it.\n  adjacency_matrix = adjacency_matrix + tf.transpose(adjacency_matrix)\n  adjacency_matrix = tf.minimum(\n      adjacency_matrix, tf.ones_like(adjacency_matrix))\n  degree_matrix = tf.linalg.tensor_diag(tf.reduce_sum(adjacency_matrix, axis=1))\n  laplacian_matrix = degree_matrix - adjacency_matrix\n  return laplacian_matrix\n\n\ndef process_experience_for_neural_agents(\n    experience,\n    observation_and_action_constraint_splitter,\n    accepts_per_arm_features,\n    training_data_spec):\n  """"""Processes the experience and prepares it for the network of the agent.\n\n  First the reward, the action, and the observation are flattened to have only\n  one batch dimension. Then the action mask is removed if it is there. Finally,\n  if the experience includes chosen action features in the policy info, it gets\n  copied in place of the per-arm observation.\n\n  Args:\n    experience: The experience coming from the replay buffer.\n    observation_and_action_constraint_splitter: If the agent accepts action\n      masks, this function splits the mask from the observation.\n    accepts_per_arm_features: Whether the agent accepts per-arm features.\n    training_data_spec: The data spec describing what the agent expects.\n\n  Returns:\n    A tuple of (reward, action, observation) tensors to be consumed by the train\n      function of the neural agent.\n  """"""\n  flattened_experience, _ = nest_utils.flatten_multi_batched_nested_tensors(\n      experience, training_data_spec)\n\n  observation = flattened_experience.observation\n  action = flattened_experience.action\n  reward = flattened_experience.reward\n\n  if observation_and_action_constraint_splitter is not None:\n    observation, _ = observation_and_action_constraint_splitter(\n        observation)\n  if accepts_per_arm_features:\n    # The arm observation we train on needs to be copied from the respective\n    # policy info field to the per arm observation field. Pretending there was\n    # only one action, we fill the action field with zeros.\n    chosen_arm_features = flattened_experience.policy_info.chosen_arm_features\n    observation[bandit_spec_utils.PER_ARM_FEATURE_KEY] = tf.nest.map_structure(\n        lambda t: tf.expand_dims(t, axis=1), chosen_arm_features)\n    action = tf.zeros_like(action)\n    if bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation:\n      # This change is not crucial but since in training there will be only one\n      # action per sample, it\'s good to follow the convention that the feature\n      # value for `num_actions` be less than or equal to the maximum available\n      # number of actions.\n      observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY] = tf.ones_like(\n          observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY])\n\n  return observation, action, reward\n'"
tf_agents/bandits/agents/utils_test.py,42,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.agents import utils\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.trajectories import trajectory\n\ntfd = tfp.distributions\ntf.compat.v1.enable_v2_behavior()\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_contextdim10\',\n          \'batch_size\': 1,\n          \'context_dim\': 10,\n      }, {\n          \'testcase_name\': \'_batch4_contextdim5\',\n          \'batch_size\': 4,\n          \'context_dim\': 5,\n      })\n\n\nclass UtilsTest(tf.test.TestCase, parameterized.TestCase):\n\n  def testNumActionsFromTensorSpecGoodSpec(self):\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=15)\n    num_actions = utils.get_num_actions_from_tensor_spec(action_spec)\n    self.assertEqual(num_actions, 16)\n\n  def testNumActionsFromTensorSpecWrongRank(self):\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(2, 3), minimum=0, maximum=15)\n\n    with self.assertRaisesRegexp(ValueError, r\'Action spec must be a scalar\'):\n      utils.get_num_actions_from_tensor_spec(action_spec)\n\n  @test_cases()\n  def testBUpdate(self, batch_size, context_dim):\n    b_array = np.array(range(context_dim))\n    r_array = np.array(range(batch_size)).reshape((batch_size, 1))\n    x_array = np.array(range(batch_size * context_dim)).reshape(\n        (batch_size, context_dim))\n    rx = r_array * x_array\n    expected_b_updated_array = b_array + np.sum(rx, axis=0)\n\n    b = tf.constant(b_array, dtype=tf.float32, shape=[context_dim])\n    r = tf.constant(r_array, dtype=tf.float32, shape=[batch_size])\n    x = tf.constant(x_array, dtype=tf.float32, shape=[batch_size, context_dim])\n    b_update = utils.sum_reward_weighted_observations(r, x)\n    self.assertAllClose(expected_b_updated_array, self.evaluate(b + b_update))\n\n  @test_cases()\n  def testBUpdateEmptyObservations(self, batch_size, context_dim):\n    r = tf.constant([], dtype=tf.float32, shape=[0, 1])\n    x = tf.constant([], dtype=tf.float32, shape=[0, context_dim])\n    b_update = utils.sum_reward_weighted_observations(r, x)\n    expected_b_update_array = np.zeros([context_dim], dtype=np.float32)\n    self.assertAllClose(expected_b_update_array, self.evaluate(b_update))\n\n  def testLaplacian1D(self):\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=4)\n    num_actions = utils.get_num_actions_from_tensor_spec(action_spec)\n    laplacian_matrix = tf.convert_to_tensor(\n        utils.build_laplacian_over_ordinal_integer_actions(action_spec),\n        dtype=tf.float32)\n    res = tf.matmul(\n        laplacian_matrix, tf.ones([num_actions, 1], dtype=tf.float32))\n    # The vector of ones is in the null space of the Laplacian matrix.\n    self.assertAllClose(0.0, self.evaluate(tf.norm(res)))\n\n    # The row sum is zero.\n    row_sum = tf.reduce_sum(laplacian_matrix, 1)\n    self.assertAllClose(0.0, self.evaluate(tf.norm(row_sum)))\n\n    # The column sum is zero.\n    column_sum = tf.reduce_sum(laplacian_matrix, 0)\n    self.assertAllClose(0.0, self.evaluate(tf.norm(column_sum)))\n\n    # The diagonal elements are 2.0.\n    self.assertAllClose(2.0, laplacian_matrix[1, 1])\n\n    laplacian_matrix_expected = np.array(\n        [[1.0, -1.0, 0.0, 0.0, 0.0],\n         [-1.0, 2.0, -1.0, 0.0, 0.0],\n         [0.0, -1.0, 2.0, -1.0, 0.0],\n         [0.0, 0.0, -1.0, 2.0, -1.0],\n         [0.0, 0.0, 0.0, -1.0, 1.0]])\n    self.assertAllClose(laplacian_matrix_expected,\n                        self.evaluate(laplacian_matrix))\n\n  def testComputePairwiseDistances(self):\n    input_vects = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    pdist_matrix = np.array(\n        [[0.0, 27.0, 108.0,],\n         [27.0, 0.0, 27.0],\n         [108.0, 27.0, 0.0]])\n    tf_dist_matrix = utils.compute_pairwise_distances(\n        tf.constant(input_vects, dtype=tf.float32))\n    self.assertAllClose(pdist_matrix, self.evaluate(tf_dist_matrix))\n\n  def testBuildLaplacianNearestNeighborGraph(self):\n    input_vects = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9],\n                            [10, 11, 12], [13, 14, 15]])\n    num_actions = input_vects.shape[0]\n    laplacian_matrix = utils.build_laplacian_nearest_neighbor_graph(\n        tf.constant(input_vects, dtype=tf.float32), k=2)\n\n    # The vector of ones is in the null space of the Laplacian matrix.\n    res = tf.matmul(\n        laplacian_matrix, tf.ones([num_actions, 1], dtype=tf.float32))\n    self.assertAllClose(0.0, self.evaluate(tf.norm(res)))\n\n    # The row sum is zero.\n    row_sum = tf.reduce_sum(laplacian_matrix, 1)\n    self.assertAllClose(0.0, self.evaluate(tf.norm(row_sum)))\n\n    # The column sum is zero.\n    column_sum = tf.reduce_sum(laplacian_matrix, 0)\n    self.assertAllClose(0.0, self.evaluate(tf.norm(column_sum)))\n\n    self.assertAllClose(2.0, laplacian_matrix[0, 0])\n    self.assertAllClose(4.0, laplacian_matrix[2, 2])\n\n  def testProcessExperienceGlobalFeatures(self):\n    observation_spec = {\n        \'f1\': tf.TensorSpec(shape=(5,), dtype=tf.string),\n        \'f2\': tf.TensorSpec(shape=(5, 2), dtype=tf.int32)\n    }\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    training_data_spec = trajectory.Trajectory(\n        step_type=time_step_spec.step_type,\n        observation=time_step_spec.observation,\n        action=tensor_spec.BoundedTensorSpec(\n            shape=(), minimum=0, maximum=4, dtype=tf.int32),\n        policy_info=(),\n        next_step_type=time_step_spec.step_type,\n        reward=tensor_spec.BoundedTensorSpec(\n            shape=(), minimum=0, maximum=2, dtype=tf.float32),\n        discount=time_step_spec.discount)\n    experience = tensor_spec.sample_spec_nest(\n        training_data_spec, outer_dims=(7, 2))\n    observation, action, reward = utils.process_experience_for_neural_agents(\n        experience, None, False, training_data_spec)\n    self.assertAllEqual(\n        observation[\'f1\'][0], experience.observation[\'f1\'][0, 0])\n    self.assertEqual(action[0], experience.action[0, 0])\n    self.assertEqual(reward[0], experience.reward[0, 0])\n\n  def testProcessExperiencePerArmFeaturesWithMask(self):\n    mask_spec = tensor_spec.BoundedTensorSpec(\n        shape=(5,), minimum=0, maximum=1, dtype=tf.int32)\n    observation_spec = ({\n        \'global\': tf.TensorSpec(shape=(4,), dtype=tf.float32),\n        \'per_arm\': {\n            \'f1\': tf.TensorSpec(shape=(5,), dtype=tf.string),\n            \'f2\': tf.TensorSpec(shape=(5, 2), dtype=tf.int32)\n        }\n    }, mask_spec)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    policy_info_spec = policy_utilities.PerArmPolicyInfo(\n        chosen_arm_features={\n            \'f1\': tf.TensorSpec(shape=(), dtype=tf.string),\n            \'f2\': tf.TensorSpec(shape=(2,), dtype=tf.int32)\n        })\n    training_data_spec = trajectory.Trajectory(\n        step_type=time_step_spec.step_type,\n        observation=time_step_spec.observation,\n        action=tensor_spec.BoundedTensorSpec(\n            shape=(), minimum=0, maximum=4, dtype=tf.int32),\n        policy_info=policy_info_spec,\n        next_step_type=time_step_spec.step_type,\n        reward=tensor_spec.BoundedTensorSpec(\n            shape=(), minimum=0, maximum=2, dtype=tf.float32),\n        discount=time_step_spec.discount)\n    experience = tensor_spec.sample_spec_nest(\n        training_data_spec, outer_dims=(7, 2))\n    observation, action, reward = utils.process_experience_for_neural_agents(\n        experience, lambda x: (x[0], x[1]), True, training_data_spec)\n    self.assertEqual(observation[\'per_arm\'][\'f1\'][0],\n                     experience.policy_info.chosen_arm_features[\'f1\'][0, 0])\n    self.assertAllEqual(action, tf.zeros(14, dtype=tf.int32))\n    self.assertEqual(reward[0], experience.reward[0, 0])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/drivers/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all driver libraries.""""""\n\nfrom tf_agents.bandits.drivers import driver_utils\n'"
tf_agents/bandits/drivers/driver_utils.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Driver utilities for use with bandit policies and environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.trajectories import trajectory\n\nnest = tf.compat.v2.nest\n\n\ndef trajectory_for_bandit(initial_step, action_step, final_step):\n  """"""Builds a trajectory from a single-step bandit episode.\n\n  Since all episodes consist of a single step, the returned `Trajectory` has no\n  time dimension. All input and output `Tensor`s/arrays are expected to have\n  shape `[batch_size, ...]`.\n\n  Args:\n    initial_step: A `TimeStep` returned from `environment.step(...)`.\n    action_step: A `PolicyStep` returned by `policy.action(...)`.\n    final_step: A `TimeStep` returned from `environment.step(...)`.\n  Returns:\n    A `Trajectory` containing zeros for discount value and `StepType.LAST` for\n    both `step_type` and `next_step_type`.\n  """"""\n  return trajectory.Trajectory(observation=initial_step.observation,\n                               action=action_step.action,\n                               policy_info=action_step.info,\n                               reward=final_step.reward,\n                               discount=final_step.discount,\n                               step_type=initial_step.step_type,\n                               next_step_type=final_step.step_type)\n'"
tf_agents/bandits/environments/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all environments.""""""\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.environments import bandit_tf_environment\nfrom tf_agents.bandits.environments import bernoulli_action_mask_tf_environment\nfrom tf_agents.bandits.environments import bernoulli_py_environment\nfrom tf_agents.bandits.environments import classification_environment\nfrom tf_agents.bandits.environments import drifting_linear_environment\nfrom tf_agents.bandits.environments import mushroom_environment_utilities\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment\nfrom tf_agents.bandits.environments import piecewise_bernoulli_py_environment\nfrom tf_agents.bandits.environments import piecewise_stochastic_environment\nfrom tf_agents.bandits.environments import random_bandit_environment\nfrom tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment\nfrom tf_agents.bandits.environments import stationary_stochastic_structured_py_environment\nfrom tf_agents.bandits.environments import wheel_py_environment\n'"
tf_agents/bandits/environments/bandit_py_environment.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for Bandit Python environments.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport numpy as np\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.trajectories import time_step as ts\n\n\nclass BanditPyEnvironment(py_environment.PyEnvironment):\n  """"""Base class for Bandit Python environments.\n\n  Every bandit Python environment should derive from this class.\n  Subclasses need to implement functions _observe() and _apply_action().\n\n  Usage:\n\n  To receive the first observation, the environment\'s reset() function should be\n  called. To take an action, use the step(action) function. The time step\n  returned by step(action) will contain the reward and the next observation.\n  """"""\n\n  def __init__(self, observation_spec, action_spec, reward_spec=None):\n    self._observation_spec = observation_spec\n    self._action_spec = action_spec\n    self._reward_spec = reward_spec\n    super(BanditPyEnvironment, self).__init__()\n\n  def _reset(self):\n    """"""Returns a time step containing an observation.\n\n    It should not be overridden by Bandit environment implementations.\n\n    Returns:\n      A time step of type FIRST containing an observation.\n    """"""\n    return ts.restart(self._observe(), batch_size=self.batch_size,\n                      reward_spec=self.reward_spec())\n\n  def _step(self, action):\n    """"""Returns a time step containing the reward for the action taken.\n\n    The returning time step also contains the next observation.\n    It should not be overridden by bandit environment implementations.\n\n    Args:\n      action: The action taken by the Bandit policy.\n\n    Returns:\n      A time step of type LAST containing the reward for the action taken and\n      the next observation.\n    """"""\n    # This step will take an action and return a reward.\n    reward = self._apply_action(action)\n    return ts.termination(self._observe(), reward)\n\n  def action_spec(self):\n    return self._action_spec\n\n  def observation_spec(self):\n    return self._observation_spec\n\n  def reward_spec(self):\n    return self._reward_spec\n\n  def _empty_observation(self):\n    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n                                 self.observation_spec())\n\n  @abc.abstractmethod\n  def _apply_action(self, action):\n    """"""Applies `action` to the Environment and returns the corresponding reward.\n\n    Args:\n      action: A value conforming action_spec that will be taken as action in the\n        environment.\n\n    Returns:\n      A float value that is the reward received by the environment.\n    """"""\n\n  @abc.abstractmethod\n  def _observe(self):\n    """"""Returns an observation.""""""\n'"
tf_agents/bandits/environments/bandit_tf_environment.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Base class for bandit environments implemented in TensorFlow.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass BanditTFEnvironment(tf_environment.TFEnvironment):\n  """"""Base class for bandit environments implemented in TensorFlow.\n\n  Subclasses should implement the `_apply_action` and `_observe` methods.\n\n  Example usage with eager mode:\n  ```\n    # reset() creates the initial time_step and resets the environment.\n    time_step = environment.reset()\n    for _ in tf.range(num_steps):\n      action_step = policy.action(time_step)\n      time_step = environment.step(action_step.action)\n  ```\n\n  Example usage with graph mode:\n  ```\n    # current_time_step() creates the initial TimeStep.\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    # Apply the action and return the new TimeStep.\n    next_time_step = environment.step(action_step.action)\n\n    sess.run([time_step, action_step, next_time_step])\n  ```\n  """"""\n\n  def __init__(self, time_step_spec=None, action_spec=None, batch_size=1):\n    """"""Initialize instances of `BanditTFEnvironment`.\n\n    Args:\n      time_step_spec: A `TimeStep` namedtuple containing `TensorSpec`s\n        defining the tensors returned by\n        `step()` (step_type, reward, discount, and observation).\n      action_spec: A nest of BoundedTensorSpec representing the actions of the\n        environment.\n      batch_size: The batch size expected for the actions and observations.\n    """"""\n    self._reset_called = tf.compat.v2.Variable(\n        False, trainable=False, name=\'reset_called\')\n\n    def _variable_from_spec(name, spec):\n      full_shape = [batch_size] + spec.shape.as_list()\n      if not name:\n        name = \'spec_var\'\n      return common.create_variable(name, 0, shape=full_shape, dtype=spec.dtype)\n\n    paths_and_specs = nest_utils.flatten_with_joined_paths(time_step_spec)\n    variables = [\n        _variable_from_spec(path, spec) for path, spec in paths_and_specs\n    ]\n    self._time_step_variables = tf.nest.pack_sequence_as(\n        time_step_spec, variables)\n\n    super(BanditTFEnvironment, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        batch_size=batch_size)\n\n  def _update_time_step(self, time_step):\n    tf.nest.map_structure(lambda var, value: var.assign(value),\n                          self._time_step_variables, time_step)\n\n  @common.function()\n  def _current_time_step(self):\n    def true_fn():\n      return tf.nest.map_structure(tf.identity, self._time_step_variables)\n    def false_fn():\n      current_time_step = self.reset()\n      return current_time_step\n\n    return tf.cond(self._reset_called, true_fn, false_fn)\n\n  @common.function\n  def _reset(self):\n    current_time_step = ts.restart(\n        self._observe(), batch_size=self.batch_size,\n        reward_spec=self.time_step_spec().reward)\n    tf.compat.v1.assign(self._reset_called, True)\n    self._update_time_step(current_time_step)\n    return current_time_step\n\n  @common.function\n  def _step(self, action):\n    reward = self._apply_action(action)\n    current_time_step = ts.termination(self._observe(), reward)\n    self._update_time_step(current_time_step)\n    return current_time_step\n\n  @abc.abstractmethod\n  def _apply_action(self, action):\n    """"""Returns a reward for the given action.""""""\n\n  @abc.abstractmethod\n  def _observe(self):\n    """"""Returns an observation.""""""\n'"
tf_agents/bandits/environments/bandit_tf_environment_test.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.bandit_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.environments import bandit_tf_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\nclass ZerosEnvironment(bandit_tf_environment.BanditTFEnvironment):\n  """"""A simple environment that returns zeros for observations and rewards.""""""\n\n  def __init__(self, observation_shape, batch_size=1):\n    observation_spec = tensor_spec.TensorSpec(shape=observation_shape,\n                                              dtype=tf.float32,\n                                              name=\'observation\')\n    time_step_spec = ts.time_step_spec(observation_spec)\n    super(ZerosEnvironment, self).__init__(time_step_spec=time_step_spec,\n                                           batch_size=batch_size)\n\n  def _apply_action(self, action):\n    return tf.zeros(self.batch_size)\n\n  def _observe(self):\n    observation_shape = [self.batch_size] + list(self.observation_spec().shape)\n    return tf.zeros(observation_shape)\n\n\nclass MultipleRewardsEnvironment(bandit_tf_environment.BanditTFEnvironment):\n  """"""A simple multiple-rewards environment.""""""\n\n  def __init__(self, observation_shape, batch_size=1, num_rewards=1):\n    self._num_rewards = num_rewards\n    reward_spec = tensor_spec.TensorSpec(shape=[self._num_rewards],\n                                         dtype=tf.float32,\n                                         name=\'reward\')\n\n    observation_spec = tensor_spec.TensorSpec(shape=observation_shape,\n                                              dtype=tf.float32,\n                                              name=\'observation\')\n    time_step_spec = ts.time_step_spec(observation_spec, reward_spec)\n    super(MultipleRewardsEnvironment, self).__init__(\n        time_step_spec=time_step_spec,\n        batch_size=batch_size)\n\n  def _apply_action(self, action):\n    return tf.zeros([self.batch_size, self._num_rewards])\n\n  def _observe(self):\n    observation_shape = [self.batch_size] + list(self.observation_spec().shape)\n    return tf.zeros(observation_shape)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BanditTFEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[]_batch_1\',\n           observation_shape=[3],\n           batch_size=1),\n      dict(testcase_name=\'_observation_[7]_batch_32\',\n           observation_shape=[7],\n           batch_size=32),\n      dict(testcase_name=\'_observation_[3, 4, 5]_batch_11\',\n           observation_shape=[3, 4, 5],\n           batch_size=11)\n      )\n  def testObservationAndRewardShapes(self, batch_size, observation_shape):\n    """"""Exercise `reset` and `step`. Ensure correct shapes are returned.""""""\n    env = ZerosEnvironment(batch_size=batch_size,\n                           observation_shape=observation_shape)\n\n    @common.function\n    def observation_and_reward():\n      observation = env.reset().observation\n      reward = env.step(tf.zeros(batch_size)).reward\n      return observation, reward\n\n    observation, reward = observation_and_reward()\n\n    expected_observation = np.zeros([batch_size] + observation_shape)\n    expected_reward = np.zeros(batch_size)\n\n    np.testing.assert_array_almost_equal(\n        expected_observation, self.evaluate(observation))\n    np.testing.assert_array_almost_equal(\n        expected_reward, self.evaluate(reward))\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'\',\n           observation_shape=[32],\n           batch_size=12),\n      )\n  def testTwoConsecutiveSteps(self, batch_size, observation_shape):\n    """"""Test two consecutive calls to `step`.""""""\n    env = ZerosEnvironment(batch_size=batch_size,\n                           observation_shape=observation_shape)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(env.reset())\n    self.evaluate(env.step(tf.zeros(batch_size)))\n    self.evaluate(env.step(tf.zeros(batch_size)))\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[3]_batch5_2rewards\',\n           observation_shape=[3],\n           batch_size=5,\n           num_rewards=2),\n      dict(testcase_name=\'_observation_[7]_batch8_4rewards\',\n           observation_shape=[7],\n           batch_size=8,\n           num_rewards=4),\n      )\n  def testMultipleRewardsEnvironment(\n      self, batch_size, observation_shape, num_rewards):\n    """"""Test the multiple-rewards case. Ensure correct shapes are returned.""""""\n\n    env = MultipleRewardsEnvironment(\n        observation_shape=observation_shape,\n        batch_size=batch_size,\n        num_rewards=num_rewards)\n\n    observation = env.reset().observation\n    reward = env.step(tf.zeros(batch_size)).reward\n\n    expected_observation = np.zeros([batch_size] + observation_shape)\n    expected_reward = np.zeros([batch_size, num_rewards])\n\n    np.testing.assert_array_almost_equal(\n        expected_observation, self.evaluate(observation))\n    np.testing.assert_array_almost_equal(\n        expected_reward, self.evaluate(reward))\n\n    self.assertEqual(env.reward_spec().shape, num_rewards)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/bernoulli_action_mask_tf_environment.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Environment wrapper that adds action masks to a bandit environment.\n\nThis environment wrapper takes a `BanditTFEnvironment` as input, and generates\na new environment where the observations are joined with boolean action\nmasks. These masks describe which actions are allowed in a given time step. If a\ndisallowed action is chosen in a time step, the environment will raise an\nerror. The masks are drawn independently from Bernoulli-distributed random\nvariables with parameter `action_probability`.\n\nThe observations from the original environment and the mask are joined by the\ngiven `join_fn` function, and the result of the join function will be the\nobservation in the new environment.\n\nUsage:\n\n \'\'\'\n env = MyFavoriteBanditEnvironment(...)\n def join_fn(context, mask):\n   return (context, mask)\n masked_env = BernoulliActionMaskTFEnvironment(env, join_fn, 0.5)\n \'\'\'\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.agents import utils as agent_utils\nfrom tf_agents.bandits.environments import bandit_tf_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\ntfd = tfp.distributions\n\n\n@common.function\ndef _maybe_add_one_action(mask):\n  """"""For time steps where the mask is all zeros, adds one action randomly.""""""\n  batch_size = tf.shape(mask)[0]\n  num_actions = tf.shape(mask)[1]\n  extra_actions = tf.one_hot(\n      tf.random.uniform([batch_size], 0, num_actions, dtype=tf.int32),\n      depth=num_actions,\n      dtype=tf.int32)\n  cond = tf.cast(tf.equal(tf.reduce_max(mask, axis=1), 1), tf.bool)\n  return tf.compat.v1.where(cond, mask, extra_actions)\n\n\n@gin.configurable\nclass BernoulliActionMaskTFEnvironment(bandit_tf_environment.BanditTFEnvironment\n                                      ):\n  """"""An environment wrapper that adds action masks to observations.""""""\n\n  def __init__(self, original_environment, action_constraint_join_fn,\n               action_probability):\n    """"""Initializes a `BernoulliActionMaskTFEnvironment`.\n\n    Args:\n      original_environment: Instance of `BanditTFEnvironment`. This environment\n        will be wrapped.\n      action_constraint_join_fn: A function that joins the osbervation from the\n        original environment with the generated masks.\n      action_probability: The probability that any action in the action space is\n        allower by the generated mask.\n    """"""\n    self._original_environment = original_environment\n    assert isinstance(\n        original_environment, bandit_tf_environment.BanditTFEnvironment\n    ), \'The wrapped environment needs to be a `BanditTFEnvironment`.\'\n    self._action_constraint_join_fn = action_constraint_join_fn\n    self._action_probability = action_probability\n    self._batch_size = self._original_environment.batch_size\n    action_spec = self._original_environment.action_spec()\n    observation_spec_without_mask = (\n        self._original_environment.time_step_spec().observation)\n    self._num_actions = agent_utils.get_num_actions_from_tensor_spec(\n        action_spec)\n\n    mask_spec = tf.TensorSpec([self._num_actions], dtype=tf.int32)\n    joined_observation_spec = self._action_constraint_join_fn(\n        observation_spec_without_mask, mask_spec)\n    time_step_spec = ts.time_step_spec(joined_observation_spec)\n\n    self._current_mask = tf.compat.v2.Variable(\n        tf.ones([self.batch_size, self._num_actions], dtype=tf.int32))\n\n    super(BernoulliActionMaskTFEnvironment, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        batch_size=self._batch_size)\n\n  @property\n  def original_environment(self):\n    return self._original_environment\n\n  @common.function\n  def _check_action_with_mask(self, action):\n    is_allowed = tf.gather(\n        self._current_mask, tf.expand_dims(action, axis=1), batch_dims=1)\n    tf.assert_equal(is_allowed, 1, message=\'Action not in allowed action set.\')\n\n  @common.function\n  def _apply_action(self, action):\n    self._check_action_with_mask(action)\n    # pylint: disable=protected-access\n    reward = self.original_environment._apply_action(action)\n    return reward\n\n  @common.function\n  def _observe(self):\n    # pylint: disable=protected-access\n    original_observation = self._original_environment._observe()\n    mask = tfd.Bernoulli(self._action_probability).sample(\n        sample_shape=[self._batch_size, self._num_actions])\n    mask = _maybe_add_one_action(mask)\n    tf.compat.v1.assign(self._current_mask, mask)\n\n    return self._action_constraint_join_fn(original_observation, mask)\n'"
tf_agents/bandits/environments/bernoulli_action_mask_tf_environment_test.py,18,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.bernoulli_action_mask_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.environments import bernoulli_action_mask_tf_environment as masked_tf_env\nfrom tf_agents.bandits.environments import random_bandit_environment\nfrom tf_agents.specs import tensor_spec\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass BernoulliActionMaskTfEnvironmentTest(tf.test.TestCase,\n                                           parameterized.TestCase):\n\n  @parameterized.parameters([(7, 4), (8, 5)])\n  def testMaybeAddOneAction(self, batch_size, num_actions):\n    original_mask = tf.eye(batch_size, num_actions, dtype=tf.int32)\n    new_mask = self.evaluate(masked_tf_env._maybe_add_one_action(original_mask))\n    self.assertAllEqual(original_mask[:num_actions, :],\n                        new_mask[:num_actions, :])\n    ones = tf.ones([batch_size], dtype=tf.int32)\n    self.assertAllEqual(tf.reduce_max(new_mask, axis=1), ones)\n    self.assertAllEqual(tf.reduce_sum(new_mask, axis=1), ones)\n\n  @parameterized.parameters([(7, 4), (10, 3)])\n  def testMaskedEnvironment(self, batch_size, num_actions):\n    observation_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros([batch_size, 2]), tf.ones([batch_size, 2])))\n    reward_distribution = tfd.Normal(tf.zeros(batch_size), tf.ones(batch_size))\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(), minimum=0, maximum=num_actions - 1, dtype=tf.int32)\n\n    env = random_bandit_environment.RandomBanditEnvironment(\n        observation_distribution, reward_distribution, action_spec)\n    masked_env = masked_tf_env.BernoulliActionMaskTFEnvironment(\n        env, lambda x, y: (x, y), 0.5)\n    context, mask = self.evaluate(masked_env.reset().observation)\n    self.assertAllEqual(tf.shape(context), [batch_size, 2])\n    self.assertAllEqual(tf.shape(mask), [batch_size, num_actions])\n    surely_allowed_actions = tf.argmax(mask, axis=-1, output_type=tf.int32)\n    rewards = self.evaluate(masked_env.step(surely_allowed_actions).reward)\n    self.assertAllEqual(tf.shape(rewards), [batch_size])\n\n  @parameterized.parameters([(7, 4), (10, 3)])\n  def testDisallowedAction(self, batch_size, num_actions):\n    observation_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros([batch_size, 2]), tf.ones([batch_size, 2])))\n    reward_distribution = tfd.Normal(tf.zeros(batch_size), tf.ones(batch_size))\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(), minimum=0, maximum=num_actions - 1, dtype=tf.int32)\n\n    env = random_bandit_environment.RandomBanditEnvironment(\n        observation_distribution, reward_distribution, action_spec)\n    masked_env = masked_tf_env.BernoulliActionMaskTFEnvironment(\n        env, lambda x, y: (x, y), 0.0)\n    _, mask = self.evaluate(masked_env.reset().observation)\n    surely_disallowed_actions = tf.argmin(mask, axis=-1, output_type=tf.int32)\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n                                \'not in allowed\'):\n      self.evaluate(masked_env.step(surely_disallowed_actions).reward)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/bernoulli_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class implementation of Python Bernoulli Bandit environment.""""""\nimport gin\nimport numpy as np\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.specs import array_spec\n\n\n@gin.configurable\nclass BernoulliPyEnvironment(bandit_py_environment.BanditPyEnvironment):\n  """"""Implements finite-armed Bernoulli Bandits.\n\n  This environment implements a finite-armed non-contextual Bernoulli Bandit\n  environment as a subclass of BanditPyEnvironment. For every arm, the reward\n  distribution is 0/1 (Bernoulli) with parameter p set at the initialization.\n  For a reference, see e.g., Example 1.1 in ""A Tutorial on Thompson Sampling"" by\n  Russo et al. (https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n  """"""\n\n  def __init__(self, means, batch_size=1):\n    """"""Initializes a Bernoulli Bandit environment.\n\n    Args:\n      means: vector of floats in [0, 1], the mean rewards for actions. The\n        number of arms is determined by its length.\n      batch_size: (int) The batch size.\n    """"""\n    self._num_actions = len(means)\n    self._means = means\n    self._batch_size = batch_size\n    if any(x < 0 or x > 1 for x in means):\n      raise ValueError(\'All parameters should be floats in [0, 1].\')\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n    observation_spec = array_spec.ArraySpec(\n        shape=(1,), dtype=np.int32, name=\'observation\')\n    super(BernoulliPyEnvironment, self).__init__(observation_spec, action_spec)\n\n  def _observe(self):\n    return np.zeros(\n        shape=[self._batch_size] + list(self.observation_spec().shape),\n        dtype=self.observation_spec().dtype)\n\n  def _apply_action(self, action):\n    return [np.floor(self._means[i] + np.random.random()) for i in action]\n\n  @property\n  def batched(self):\n    return True\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n'"
tf_agents/bandits/environments/bernoulli_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Bernoulli Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import bernoulli_py_environment\n\n\nclass BernoulliBanditPyEnvironmentTest(tf.test.TestCase):\n\n  def test_bernoulli_bandit_py_environment(self):\n\n    env = bernoulli_py_environment.BernoulliPyEnvironment(\n        [0.1, 0.2, 0.3], batch_size=2)\n    observation_step = env.reset()\n    self.assertAllEqual(observation_step.observation.shape, [2, 1])\n    reward_step = env.step([0, 1])\n    self.assertAllEqual(len(reward_step.reward), 2)\n\n  def test_out_of_bound_parameter(self):\n    with self.assertRaisesRegexp(\n        ValueError, r\'All parameters should be floats in \\[0, 1\\]\\.\'):\n      bernoulli_py_environment.BernoulliPyEnvironment(\n          [0.1, 1.2, 0.3], batch_size=1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/classification_environment.py,24,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""An environment based on an arbitrary classification problem.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.environments import bandit_tf_environment as bte\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import eager_utils\n\ntfd = tfp.distributions\n\n\ndef _batched_table_lookup(tbl, row, col):\n  """"""Mapped 2D table lookup.\n\n  Args:\n    tbl: a `Tensor` of shape `[r, s, t]`.\n    row: a `Tensor` of dtype `int32` with shape `[r]` and values in\n      the range `[0, s - 1]`.\n    col: a `Tensor` of dtype `int32` with shape `[r]` and values in\n      the range `[0, t - 1]`.\n  Returns:\n    A `Tensor` `x` with shape `[r]` where `x[i] = tbl[i, row[i], col[i]`.\n  """"""\n  assert_correct_shapes = tf.group(\n      tf.assert_equal(tf.shape(row), tf.shape(col)),\n      tf.assert_equal(tf.shape(row)[0], tf.shape(tbl)[0]))\n  rng = tf.range(tf.shape(row)[0])\n  idx = tf.stack([rng, row, col], axis=-1)\n  with tf.control_dependencies([assert_correct_shapes]):\n    values = tf.gather_nd(tbl, idx)\n  return values\n\n\n@gin.configurable\nclass ClassificationBanditEnvironment(bte.BanditTFEnvironment):\n  """"""An environment based on an arbitrary classification problem.""""""\n\n  def __init__(self, dataset, reward_distribution, batch_size,\n               label_dtype_cast=None, shuffle_buffer_size=None,\n               repeat_dataset=True, prefetch_size=None, seed=None):\n    """"""Initialize `ClassificationBanditEnvironment`.\n\n    Args:\n      dataset: a `tf.data.Dataset` consisting of two `Tensor`s, [inputs, labels]\n        where inputs can be of any shape, while labels are integer class labels.\n        The label tensor can be of any rank as long as it has 1 element.\n      reward_distribution: a `tfd.Distribution` with event_shape\n        `[num_classes, num_actions]`. Entry `[i, j]` is the reward for taking\n        action `j` for an instance of class `i`.\n      batch_size: if `dataset` is batched, this is the size of the batches.\n      label_dtype_cast: if not None, casts dataset labels to this dtype.\n      shuffle_buffer_size: If None, do not shuffle.  Otherwise, a shuffle buffer\n        of the specified size is used in the environment\'s `dataset`.\n      repeat_dataset: Makes the environment iterate on the `dataset` once\n        avoiding `OutOfRangeError:  End of sequence` errors when the environment\n        is stepped past the end of the `dataset`.\n      prefetch_size: If None, do not prefetch.  Otherwise, a prefetch buffer\n        of the specified size is used in the environment\'s `dataset`.\n      seed: Used to make results deterministic.\n    Raises:\n      ValueError: if `reward_distribution` does not have an event shape with\n        rank 2.\n    """"""\n\n    # Computing `action_spec`.\n    event_shape = reward_distribution.event_shape\n    if len(event_shape) != 2:\n      raise ValueError(\n          \'reward_distribution must have event shape of rank 2; \'\n          \'got event shape {}\'.format(event_shape))\n    _, num_actions = event_shape\n    action_spec = tensor_spec.BoundedTensorSpec(shape=(),\n                                                dtype=tf.int32,\n                                                minimum=0,\n                                                maximum=num_actions - 1,\n                                                name=\'action\')\n    output_shapes = tf.compat.v1.data.get_output_shapes(dataset)\n\n    # Computing `time_step_spec`.\n    if len(output_shapes) != 2:\n      raise ValueError(\'Dataset must have exactly two outputs; got {}\'.format(\n          len(output_shapes)))\n    context_shape = output_shapes[0]\n    context_dtype, lbl_dtype = tf.compat.v1.data.get_output_types(dataset)\n    if label_dtype_cast:\n      lbl_dtype = label_dtype_cast\n    observation_spec = tensor_spec.TensorSpec(\n        shape=context_shape, dtype=context_dtype)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n\n    super(ClassificationBanditEnvironment, self).__init__(\n        action_spec=action_spec,\n        time_step_spec=time_step_spec,\n        batch_size=batch_size)\n\n    if shuffle_buffer_size:\n      dataset = dataset.shuffle(buffer_size=shuffle_buffer_size,\n                                seed=seed,\n                                reshuffle_each_iteration=True)\n    if repeat_dataset:\n      dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    if prefetch_size:\n      dataset = dataset.prefetch(prefetch_size)\n    self._data_iterator = eager_utils.dataset_iterator(dataset)\n    self._current_label = tf.compat.v2.Variable(\n        tf.zeros(batch_size, dtype=lbl_dtype))\n    self._previous_label = tf.compat.v2.Variable(\n        tf.zeros(batch_size, dtype=lbl_dtype))\n    self._reward_distribution = reward_distribution\n    self._label_dtype = lbl_dtype\n\n    reward_means = self._reward_distribution.mean()\n    self._optimal_action_table = tf.argmax(\n        reward_means, axis=1, output_type=self._action_spec.dtype)\n    self._optimal_reward_table = tf.reduce_max(reward_means, axis=1)\n\n  def _observe(self):\n    context, lbl = eager_utils.get_next(self._data_iterator)\n    self._previous_label.assign(self._current_label)\n    self._current_label.assign(tf.reshape(\n        tf.cast(lbl, dtype=self._label_dtype), shape=[self._batch_size]))\n    return tf.reshape(\n        context,\n        shape=[self._batch_size] + self._time_step_spec.observation.shape)\n\n  def _apply_action(self, action):\n    action = tf.reshape(\n        action, shape=[self._batch_size] + self._action_spec.shape)\n    reward_samples = self._reward_distribution.sample(tf.shape(action))\n    return _batched_table_lookup(reward_samples, self._current_label, action)\n\n  def compute_optimal_action(self):\n    return tf.gather(\n        params=self._optimal_action_table, indices=self._previous_label)\n\n  def compute_optimal_reward(self):\n    return tf.gather(\n        params=self._optimal_reward_table, indices=self._previous_label)\n'"
tf_agents/bandits/environments/classification_environment_test.py,32,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.classification_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom absl.testing.absltest import mock\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.environments import classification_environment as ce\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef deterministic_reward_distribution(reward_table):\n  """"""Returns a deterministic distribution centered at `reward_table`.""""""\n  return tfd.Independent(tfd.Deterministic(loc=reward_table),\n                         reinterpreted_batch_ndims=2)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ClassificationEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_3x2x3\',\n           tbl=[[[0, 1, 2],\n                 [3, 4, 5]],\n                [[6, 7, 8],\n                 [9, 10, 11]],\n                [[12, 13, 14],\n                 [15, 16, 17]]],\n           row=[0, 1, 1],\n           col=[0, 2, 0],\n           expected=[0, 11, 15]),\n      )\n  def testBatchedTableLookup(self, tbl, row, col, expected):\n    actual = ce._batched_table_lookup(tbl, row, col)\n    np.testing.assert_almost_equal(expected, self.evaluate(actual))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\'_scalar_batch_1\',\n          context=np.array([[0], [1]]),\n          labels=np.array([0, 1]),\n          batch_size=1),\n      dict(\n          testcase_name=\'_multi_dim_batch_23\',\n          context=np.arange(100).reshape(10, 10),\n          labels=np.arange(10),\n          batch_size=23),\n  )\n  def testObservationShapeAndValue(self, context, labels, batch_size):\n    """"""Test that observations have correct shape and values from `context`.""""""\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(\n            (context, labels)).repeat().shuffle(4 * batch_size))\n    # Rewards of 1. is given when action == label\n    reward_distribution = deterministic_reward_distribution(\n        tf.eye(len(set(labels))))\n    env = ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size)\n    expected_observation_shape = [batch_size] + list(context.shape[1:])\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for _ in range(100):\n      observation = self.evaluate(env.reset().observation)\n      np.testing.assert_array_equal(observation.shape,\n                                    expected_observation_shape)\n      for o in observation:\n        self.assertIn(o, context)\n\n  def testReturnsCorrectRewards(self):\n    """"""Test that rewards are being returned correctly for a simple case.""""""\n    # Reward of 1 is given if action == (context % 3)\n    context = tf.reshape(tf.range(128), shape=[128, 1])\n    labels = tf.math.mod(context, 3)\n    batch_size = 32\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(\n            (context, labels)).repeat().shuffle(4 * batch_size))\n    reward_distribution = deterministic_reward_distribution(tf.eye(3))\n    env = ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    for _ in range(10):\n      # Take the \'correct\' action\n      observation = env.reset().observation\n      action = tf.math.mod(observation, 3)\n      reward = env.step(action).reward\n      np.testing.assert_almost_equal(self.evaluate(reward),\n                                     self.evaluate(tf.ones_like(reward)))\n\n    for _ in range(10):\n      # Take the \'incorrect\' action\n      observation = env.reset().observation\n      action = tf.math.mod(observation + 1, 3)\n      reward = env.step(action).reward\n      np.testing.assert_almost_equal(self.evaluate(reward),\n                                     self.evaluate(tf.zeros_like(reward)))\n\n  def testPreviousLabelIsSetCorrectly(self):\n    """"""Test that the previous label is set correctly for a simple case.""""""\n    # Reward of 1 is given if action == (context % 3)\n    context = tf.reshape(tf.range(128), shape=[128, 1])\n    labels = tf.math.mod(context, 3)\n    batch_size = 4\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(\n            (context, labels)).repeat().shuffle(4 * batch_size))\n    reward_distribution = deterministic_reward_distribution(tf.eye(3))\n    env = ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    time_step = env.reset()\n    time_step_label = tf.squeeze(tf.math.mod(time_step.observation, 3))\n    action = tf.math.mod(time_step.observation, 3)\n    next_time_step = env.step(action)\n    next_time_step_label = tf.squeeze(\n        tf.math.mod(next_time_step.observation, 3))\n\n    if tf.executing_eagerly():\n      np.testing.assert_almost_equal(\n          self.evaluate(time_step_label),\n          self.evaluate(env._previous_label))\n      np.testing.assert_almost_equal(\n          self.evaluate(next_time_step_label),\n          self.evaluate(env._current_label))\n    else:\n      with self.cached_session() as sess:\n        time_step_label_value, next_time_step_label_value = (\n            sess.run([time_step_label, next_time_step_label]))\n\n        previous_label_value = self.evaluate(env._previous_label)\n        np.testing.assert_almost_equal(\n            time_step_label_value, previous_label_value)\n        current_label_value = self.evaluate(env._current_label)\n        np.testing.assert_almost_equal(\n            next_time_step_label_value,\n            current_label_value)\n\n  def testShuffle(self):\n    """"""Test that dataset is being shuffled when asked.""""""\n    # Reward of 1 is given if action == (context % 3)\n    context = tf.reshape(tf.range(128), shape=[128, 1])\n    labels = tf.math.mod(context, 3)\n    batch_size = 32\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(\n            (context, labels)).repeat().shuffle(4 * batch_size))\n    reward_distribution = deterministic_reward_distribution(tf.eye(3))\n\n    # Note - shuffle should hapen *first* in call chain, so this\n    # test will fail if shuffle is called e.g. after batch or prefetch.\n    dataset.shuffle = mock.Mock(spec=dataset.shuffle,\n                                side_effect=dataset.shuffle)\n    ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size)\n    dataset.shuffle.assert_not_called()\n    ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size, shuffle_buffer_size=3, seed=7)\n    dataset.shuffle.assert_called_with(\n        buffer_size=3, reshuffle_each_iteration=True, seed=7)\n\n  @mock.patch(\'tf_agents.bandits.environments.classification_environment\'+\n              \'.eager_utils.dataset_iterator\')\n  def testPrefetch(self, mock_dataset_iterator):\n    """"""Test that dataset is being prefetched when asked.""""""\n    mock_dataset_iterator.return_value = \'mock_iterator_result\'\n    # Reward of 1 is given if action == (context % 3)\n    context = tf.reshape(tf.range(128), shape=[128, 1])\n    labels = tf.math.mod(context, 3)\n    batch_size = 32\n    dataset = tf.data.Dataset.from_tensor_slices((context, labels))\n    reward_distribution = deterministic_reward_distribution(tf.eye(3))\n\n    # Operation order should be batch() then prefetch(), have to jump\n    # through a couple hoops to get this sequence tested correctly.\n\n    # Save dataset.prefetch in temp mock_prefetch, return batched dataset to\n    # make down-stream logic work correctly with batch dimensions.\n    batched_dataset = dataset.batch(batch_size)\n    mock_prefetch = mock.Mock(spec=dataset.prefetch,\n                              return_value=batched_dataset)\n    # Replace dataset.batch with mock batch that returns original dataset,\n    # in order to make mocking out it\'s prefetch call easier.\n    dataset.batch = mock.Mock(spec=batched_dataset,\n                              return_value=batched_dataset)\n    # Replace dataset.prefetch with mock_prefetch.\n    batched_dataset.prefetch = mock_prefetch\n    env = ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size, repeat_dataset=False)\n    dataset.batch.assert_called_with(batch_size, drop_remainder=True)\n    batched_dataset.prefetch.assert_not_called()\n    mock_dataset_iterator.assert_called_with(batched_dataset)\n    self.assertEqual(env._data_iterator, \'mock_iterator_result\')\n    env = ce.ClassificationBanditEnvironment(\n        dataset, reward_distribution, batch_size, repeat_dataset=False,\n        prefetch_size=3)\n    dataset.batch.assert_called_with(batch_size, drop_remainder=True)\n    batched_dataset.prefetch.assert_called_with(3)\n    mock_dataset_iterator.assert_called_with(batched_dataset)\n    self.assertEqual(env._data_iterator, \'mock_iterator_result\')\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/drifting_linear_environment.py,31,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bandit drifting linear environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nsse\nfrom tf_agents.specs import tensor_spec\n\n\ndef _raise_batch_shape_error(tensor_name, batch_shape):\n  raise ValueError(\'`{tensor_name}` must have batch shape with length 1; \'\n                   \'got {batch_shape}.\'.format(\n                       tensor_name=tensor_name,\n                       batch_shape=batch_shape))\n\n\ndef update_row(input_x, updates, row_index):\n  """"""Updates the i-th row of tensor `x` with the values given in `updates`.\n\n  Args:\n    input_x: the input tensor.\n    updates: the values to place on the i-th row of `x`.\n    row_index: which row to update.\n\n  Returns:\n    The updated tensor (same shape as `x`).\n  """"""\n  n = tf.compat.dimension_value(input_x.shape[1]) or tf.shape(input_x)[1]\n  indices = tf.concat(\n      [row_index * tf.ones([n, 1], dtype=tf.int32),\n       tf.reshape(tf.range(n, dtype=tf.int32), [n, 1])], axis=-1)\n  return tf.tensor_scatter_nd_update(\n      tensor=input_x, indices=indices, updates=tf.squeeze(updates))\n\n\ndef apply_givens_rotation(cosa, sina, axis_i, axis_j, input_x):\n  """"""Applies a Givens rotation on tensor `x`.\n\n  Reference on Givens rotations:\n  https://en.wikipedia.org/wiki/Givens_rotation\n\n  Args:\n    cosa: the cosine of the angle.\n    sina: the sine of the angle.\n    axis_i: the first axis of rotation.\n    axis_j: the second axis of rotation.\n    input_x: the input tensor.\n\n  Returns:\n    The rotated tensor (same shape as `x`).\n  """"""\n  output = update_row(\n      input_x, cosa * input_x[axis_i, :] - sina * input_x[axis_j, :], axis_i)\n  output = update_row(\n      output, sina * input_x[axis_i, :] + cosa * input_x[axis_j, :], axis_j)\n  return output\n\n\n@gin.configurable\nclass DriftingLinearDynamics(nsse.EnvironmentDynamics):\n  """"""A drifting linear environment dynamics.\n\n  This is a drifting linear environment which computes rewards as:\n\n  rewards(t) = observation(t) * observation_to_reward(t) + additive_reward(t)\n\n  where `t` is the environment time. `observation_to_reward` slowly rotates over\n  time. The environment time is incremented in the base class after the reward\n  is computed. The parameters `observation_to_reward` and `additive_reward` are\n  updated at each time step.\n  In order to preserve the norm of the `observation_to_reward` (and the range\n  of values of the reward) the drift is applied in form of rotations, i.e.,\n\n  observation_to_reward(t) = R(theta(t)) * observation_to_reward(t - 1)\n\n  where `theta` is the angle of the rotation. The angle is sampled from a\n  provided input distribution.\n  """"""\n\n  def __init__(self,\n               observation_distribution,\n               observation_to_reward_distribution,\n               drift_distribution,\n               additive_reward_distribution):\n    """"""Initialize the parameters of the drifting linear dynamics.\n\n    Args:\n      observation_distribution: A distribution from tfp.distributions with shape\n        `[batch_size, observation_dim]` Note that the values of `batch_size` and\n        `observation_dim` are deduced from the distribution.\n      observation_to_reward_distribution: A distribution from\n        `tfp.distributions` with shape `[observation_dim, num_actions]`. The\n        value `observation_dim` must match the second dimension of\n        `observation_distribution`.\n      drift_distribution: A scalar distribution from `tfp.distributions` of\n        type tf.float32. It represents the angle of rotation.\n      additive_reward_distribution: A distribution from `tfp.distributions` with\n        shape `[num_actions]`. This models the non-contextual behavior of the\n        bandit.\n    """"""\n    self._observation_distribution = observation_distribution\n    self._drift_distribution = drift_distribution\n    self._observation_to_reward_distribution = (\n        observation_to_reward_distribution)\n    self._additive_reward_distribution = additive_reward_distribution\n\n    observation_batch_shape = observation_distribution.batch_shape\n    reward_batch_shape = additive_reward_distribution.batch_shape\n    if observation_batch_shape.rank != 2:\n      _raise_batch_shape_error(\n          \'observation_distribution\', observation_batch_shape)\n    if reward_batch_shape.rank != 1:\n      _raise_batch_shape_error(\n          \'additive_reward_distribution\', reward_batch_shape)\n    if additive_reward_distribution.dtype != tf.float32:\n      raise ValueError(\'Reward  must have dtype float32; got {}\'.format(\n          self._reward.dtype))\n    self._observation_dim = self._observation_distribution.batch_shape[1]\n\n    expected_observation_to_reward_shape = [\n        tf.compat.dimension_value(\n            self._observation_distribution.batch_shape[1:]),\n        tf.compat.dimension_value(\n            self._additive_reward_distribution.batch_shape[0])]\n    observation_to_reward_shape = [\n        tf.compat.dimension_value(x)\n        for x in observation_to_reward_distribution.batch_shape]\n    if (observation_to_reward_shape !=\n        expected_observation_to_reward_shape):\n      raise ValueError(\n          \'Observation to reward has {} as expected shape; got {}\'.format(\n              expected_observation_to_reward_shape,\n              observation_to_reward_shape))\n\n    self._current_observation_to_reward = tf.compat.v2.Variable(\n        observation_to_reward_distribution.sample(),\n        dtype=tf.float32,\n        name=\'observation_to_reward\')\n    self._current_additive_reward = tf.compat.v2.Variable(\n        additive_reward_distribution.sample(),\n        dtype=tf.float32,\n        name=\'additive_reward\')\n\n  @property\n  def batch_size(self):\n    return tf.compat.dimension_value(\n        self._observation_distribution.batch_shape[0])\n\n  @property\n  def observation_spec(self):\n    return tensor_spec.TensorSpec(\n        shape=self._observation_distribution.batch_shape[1:],\n        dtype=self._observation_distribution.dtype,\n        name=\'observation_spec\')\n\n  @property\n  def action_spec(self):\n    return tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=tf.compat.dimension_value(\n            self._additive_reward_distribution.batch_shape[0]) - 1,\n        name=\'action\')\n\n  def observation(self, unused_t):\n    return self._observation_distribution.sample()\n\n  def reward(self, observation, t):\n    # Apply the drift.\n    theta = self._drift_distribution.sample()\n    random_i = tf.random.uniform(\n        [], minval=0, maxval=self._observation_dim - 1, dtype=tf.int32)\n    random_j = tf.math.mod(random_i + 1, self._observation_dim)\n    tf.compat.v1.assign(\n        self._current_observation_to_reward,\n        apply_givens_rotation(\n            tf.cos(theta), tf.sin(theta), random_i, random_j,\n            self._current_observation_to_reward))\n    tf.compat.v1.assign(self._current_additive_reward,\n                        self._additive_reward_distribution.sample())\n\n    reward = (tf.matmul(observation, self._current_observation_to_reward) +\n              self._current_additive_reward)\n    return reward\n\n  @gin.configurable\n  def compute_optimal_reward(self, observation):\n    deterministic_reward = tf.matmul(\n        observation, self._current_observation_to_reward)\n    optimal_action_reward = tf.reduce_max(deterministic_reward, axis=-1)\n    return optimal_action_reward\n\n  @gin.configurable\n  def compute_optimal_action(self, observation):\n    deterministic_reward = tf.matmul(\n        observation, self._current_observation_to_reward)\n    optimal_action = tf.argmax(\n        deterministic_reward, axis=-1, output_type=tf.int32)\n    return optimal_action\n\n\n@gin.configurable\nclass DriftingLinearEnvironment(nsse.NonStationaryStochasticEnvironment):\n  """"""Implements a drifting linear environment.""""""\n\n  def __init__(self,\n               observation_distribution,\n               observation_to_reward_distribution,\n               drift_distribution,\n               additive_reward_distribution):\n    """"""Initialize the environment with the dynamics parameters.\n\n    Args:\n      observation_distribution: A distribution from `tfp.distributions` with\n        shape `[batch_size, observation_dim]`. Note that the values of\n        `batch_size` and `observation_dim` are deduced from the distribution.\n      observation_to_reward_distribution: A distribution from\n        `tfp.distributions` with shape `[observation_dim, num_actions]`. The\n        value `observation_dim` must match the second dimension of\n        `observation_distribution`.\n      drift_distribution: A scalar distribution from `tfp.distributions` of\n        type tf.float32. It represents the angle of rotation.\n      additive_reward_distribution: A distribution from `tfp.distributions` with\n        shape `[num_actions]`. This models the non-contextual behavior of the\n        bandit.\n    """"""\n    super(DriftingLinearEnvironment, self).__init__(\n        DriftingLinearDynamics(\n            observation_distribution,\n            observation_to_reward_distribution,\n            drift_distribution,\n            additive_reward_distribution))\n'"
tf_agents/bandits/environments/drifting_linear_environment_test.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.drifting_linear_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.environments import drifting_linear_environment as dle\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ntfd = tfp.distributions\n\n\ndef test_cases():\n  return  parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[5]_action_[3]_batch_1\',\n           observation_shape=[5],\n           action_shape=[3],\n           batch_size=1,\n           seed=12345),\n      dict(testcase_name=\'_observation_[3]_action_[5]_batch_2\',\n           observation_shape=[3],\n           action_shape=[5],\n           batch_size=2,\n           seed=98765),\n      )\n\n\ndef get_deterministic_gaussian_non_stationary_environment(\n    observation_shape, action_shape, batch_size, drift_mean=0.0,\n    drift_scale=1.0):\n  """"""Returns a DriftingLinearEnvironment.""""""\n  overall_shape = [batch_size] + observation_shape\n  observation_distribution = tfd.Normal(\n      loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n  observation_to_reward_shape = observation_shape + action_shape\n  observation_to_reward_distribution = tfd.Normal(\n      loc=tf.zeros(observation_to_reward_shape),\n      scale=tf.ones(observation_to_reward_shape))\n  drift_distribution = tfd.Normal(loc=drift_mean, scale=drift_scale)\n  additive_reward_distribution = tfd.Normal(\n      loc=tf.zeros(action_shape),\n      scale=tf.ones(action_shape))\n  return dle.DriftingLinearEnvironment(\n      observation_distribution,\n      observation_to_reward_distribution,\n      drift_distribution,\n      additive_reward_distribution)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass DriftingLinearEnvironmentTest(tf.test.TestCase,\n                                    parameterized.TestCase):\n\n  def run_environment_steps_helper(self, env, batch_size, num_steps):\n    observation_to_reward_samples = []\n    additive_reward_samples = []\n    env_time = env._env_time\n    observation_to_reward = (\n        env._environment_dynamics._current_observation_to_reward)\n    additive_reward = env._environment_dynamics._current_additive_reward\n\n    if tf.executing_eagerly():\n      for t in range(0, num_steps):\n        ts = env.reset()\n        reward = env.step(tf.zeros([batch_size])).reward\n\n        (observation_to_reward_sample,\n         additive_reward_sample,\n         env_time_sample) = self.evaluate([observation_to_reward,\n                                           additive_reward,\n                                           env_time])\n        observation_to_reward_samples.append(observation_to_reward_sample)\n        additive_reward_samples.append(additive_reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * batch_size)\n    else:\n      ts = env.reset()\n      reward = env.step(tf.zeros([batch_size])).reward\n\n      for t in range(0, num_steps):\n        unused_ts_sample = self.evaluate(ts)\n        unused_reward_sample = self.evaluate(reward)\n\n        (observation_to_reward_sample,\n         additive_reward_sample,\n         env_time_sample) = self.evaluate([observation_to_reward,\n                                           additive_reward,\n                                           env_time])\n        observation_to_reward_samples.append(observation_to_reward_sample)\n        additive_reward_samples.append(additive_reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * batch_size)\n    return observation_to_reward_samples, additive_reward_samples\n\n  @test_cases()\n  def testObservationToRewardsDoesNotVary(\n      self, observation_shape, action_shape, batch_size, seed):\n    """"""Ensure that `observation_to_reward` does not change with zero drift.""""""\n    tf.compat.v1.set_random_seed(seed)\n    env = get_deterministic_gaussian_non_stationary_environment(\n        observation_shape, action_shape, batch_size, drift_mean=0.0,\n        drift_scale=0.0)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    observation_to_reward_samples, additive_reward_samples = (\n        self.run_environment_steps_helper(env, batch_size, num_steps=10))\n\n    for t in range(1, 10):\n      self.assertAllClose(\n          observation_to_reward_samples[t-1],\n          observation_to_reward_samples[t])\n      # The additive reward should change in every step.\n      self.assertNotAllClose(\n          additive_reward_samples[t-1],\n          additive_reward_samples[t])\n\n  @test_cases()\n  def testObservationToRewardsVaries(\n      self, observation_shape, action_shape, batch_size, seed):\n    """"""Ensure that `observation_to_reward` changes with non-zero drift.""""""\n    tf.compat.v1.set_random_seed(seed)\n    env = get_deterministic_gaussian_non_stationary_environment(\n        observation_shape, action_shape, batch_size, drift_mean=1.0,\n        drift_scale=1.0)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    observation_to_reward_samples, additive_reward_samples = (\n        self.run_environment_steps_helper(env, batch_size, num_steps=10))\n\n    for t in range(1, 10):\n      # The `observation_to_reward` changes, but its norm should be preserved.\n      self.assertNotAllClose(\n          observation_to_reward_samples[t-1],\n          observation_to_reward_samples[t])\n      self.assertAllClose(\n          np.linalg.norm(observation_to_reward_samples[t-1]),\n          np.linalg.norm(observation_to_reward_samples[t]))\n      # The additive reward should change in every step.\n      self.assertNotAllClose(\n          additive_reward_samples[t-1],\n          additive_reward_samples[t])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/environment_utilities.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Utility functions for configuring environments with Gin.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport gin\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import utils\nfrom tf_agents.bandits.environments import wheel_py_environment\n\n\n@gin.configurable\nclass LinearNormalReward(object):\n  """"""A class that acts as linear reward function when called.""""""\n\n  def __init__(self, theta, sigma):\n    self.theta = theta\n    self.sigma = sigma\n\n  def __call__(self, x, enable_noise=True):\n    """"""Outputs reward given observation.\n\n    Args:\n      x: Observation vector.\n      enable_noise: Whether to add normal noise to the reward or not.\n\n    Returns:\n      A scalar value: the reward.\n    """"""\n    mu = np.dot(x, self.theta)\n    if enable_noise:\n      return np.random.normal(mu, self.sigma)\n    return mu\n\n\n@gin.configurable\ndef linear_reward_fn_generator(theta_list, variance):\n  return [LinearNormalReward(theta, variance) for theta in theta_list]\n\n\n@gin.configurable\ndef sliding_linear_reward_fn_generator(context_dim, num_actions, variance):\n  """"""A function that returns `num_actions` noisy linear functions.\n\n  Every linear function has an underlying parameter consisting of `context_dim`\n  consecutive integers. For example, with `context_dim = 3` and\n  `num_actions = 2`, the parameter of the linear function associated with\n  action 1 is `[1.0, 2.0, 3.0]`.\n\n  Args:\n    context_dim: Number of parameters per function.\n    num_actions: Number of functions returned.\n    variance: Variance of the noisy linear functions.\n\n  Returns:\n    A list of noisy linear functions.\n  """"""\n\n  def _float_range(begin, end):\n    return [float(j) for j in range(begin, end)]\n\n  return linear_reward_fn_generator(\n      [_float_range(i, i + context_dim) for i in range(num_actions)], variance)\n\n\n@gin.configurable\ndef normalized_sliding_linear_reward_fn_generator(context_dim, num_actions,\n                                                  variance):\n  """"""Similar to the function above, but returns smaller-range functions.\n\n  Every linear function has an underlying parameter consisting of `context_dim`\n  floats of equal distance from each other. For example, with `context_dim = 3`,\n  `num_actions = 2`, the parameter of the linear function associated with\n  action 1 is `[1.0 / 5, 2.0 / 5, 3.0/ 5]`.\n\n  Args:\n    context_dim: Number of parameters per function.\n    num_actions: Number of functions returned.\n    variance: Variance of the noisy linear functions.\n\n  Returns:\n    A list of noisy linear functions.\n  """"""\n\n  def _float_range(begin, end, normalizer=1):\n    return [float(j) / normalizer for j in range(begin, end)]\n\n  return linear_reward_fn_generator([\n      _float_range(i, i + context_dim, normalizer=context_dim + num_actions)\n      for i in range(num_actions)\n  ], variance)\n\n\n@gin.configurable\ndef structured_linear_reward_fn_generator(context_dim, num_actions, variance,\n                                          drift_coefficient=0.1):\n  """"""A function that returns `num_actions` noisy linear functions.\n\n  Every linear function is related to its previous one:\n  ```\n  theta_new = theta_previous + a * drift\n  ```\n\n  Args:\n    context_dim: Number of parameters per function.\n    num_actions: Number of functions returned.\n    variance: Variance of the noisy linear functions.\n    drift_coefficient: the amount of drift (see `a` in the formula above).\n\n  Returns:\n    A list of noisy linear functions that are related to each other.\n  """"""\n  theta_list = []\n  theta_previous = np.random.rand(context_dim)\n  theta_list.append(theta_previous)\n  for _ in range(1, num_actions):\n    drift = np.random.rand(context_dim)\n    theta_new = theta_previous + drift_coefficient * drift\n    theta_previous = theta_new.copy()\n    theta_list.append(theta_new)\n\n  return linear_reward_fn_generator(theta_list, variance)\n\n\n@gin.configurable\ndef context_sampling_fn(batch_size, context_dim):\n  return np.random.randint(\n      -10, 10, [batch_size, context_dim]).astype(np.float32)\n\n\n@gin.configurable\ndef build_laplacian_over_ordinal_integer_actions_from_env(env):\n  return utils.build_laplacian_over_ordinal_integer_actions(env.action_spec())\n\n\n@gin.configurable\nclass LinearNormalMultipleRewards(object):\n  """"""Linear multiple rewards generator.""""""\n\n  def __init__(self, thetas):\n    self.thetas = thetas\n\n  def __call__(self, x):\n    # `x` is of shape [`batch_size`, \'context_dim\']\n    # `theta` is of shape [`context_dim`, \'num_rewards\']\n    # The result `predicted_rewards` has shape [`batch_size`, `num_rewards`]\n    predicted_rewards = np.matmul(x, self.thetas)\n    return predicted_rewards\n\n\n@gin.configurable\ndef linear_multiple_reward_fn_generator(per_action_theta_list):\n  return [LinearNormalMultipleRewards(theta) for theta in per_action_theta_list]\n\n\n@gin.configurable\ndef random_linear_multiple_reward_fn_generator(\n    context_dim, num_actions, num_rewards):\n  """"""A function that returns `num_actions` linear functions.\n\n  For each action, the corresponding linear function has underlying parameters\n  of shape [`context_dim`, \'num_rewards\'].\n\n  Args:\n    context_dim: Number of parameters per function.\n    num_actions: Number of functions returned.\n    num_rewards: Numer of rewards we want to generate.\n\n  Returns:\n    A list of linear functions.\n  """"""\n\n  def _gen_multiple_rewards_for_action():\n    return np.random.rand(context_dim, num_rewards)\n\n  return linear_multiple_reward_fn_generator(\n      [_gen_multiple_rewards_for_action() for _ in range(num_actions)])\n\n\n@gin.configurable\ndef compute_optimal_reward(observation, per_action_reward_fns,\n                           enable_noise=False):\n  """"""Computes the optimal reward.\n\n  Args:\n    observation: a (possibly batched) observation.\n    per_action_reward_fns: a list of reward functions; one per action. Each\n      reward function generates a reward when called with an observation.\n    enable_noise: (bool) whether to add noise to the rewards.\n\n  Returns:\n    The optimal reward.\n  """"""\n  num_actions = len(per_action_reward_fns)\n  rewards = np.stack(\n      [per_action_reward_fns[a](observation, enable_noise)\n       for a in range(num_actions)],\n      axis=-1)\n  # `rewards` should be of shape [`batch_size`, `num_actions`].\n  optimal_action_reward = np.max(rewards, axis=-1)\n  return optimal_action_reward\n\n\n@gin.configurable\ndef tf_compute_optimal_reward(observation,\n                              per_action_reward_fns,\n                              enable_noise=False):\n  """"""TF wrapper around `compute_optimal_reward` to be used in `tf_metrics`.""""""\n  compute_optimal_reward_fn = functools.partial(\n      compute_optimal_reward,\n      per_action_reward_fns=per_action_reward_fns,\n      enable_noise=enable_noise)\n  return tf.py_function(compute_optimal_reward_fn, [observation], tf.float32)\n\n\n@gin.configurable\ndef compute_optimal_action(observation,\n                           per_action_reward_fns,\n                           enable_noise=False):\n  """"""Computes the optimal action.\n\n  Args:\n    observation: a (possibly batched) observation.\n    per_action_reward_fns: a list of reward functions; one per action. Each\n      reward function generates a reward when called with an observation.\n    enable_noise: (bool) whether to add noise to the rewards.\n\n  Returns:\n    The optimal action, that is, the one with the highest reward.\n  """"""\n  num_actions = len(per_action_reward_fns)\n  rewards = np.stack([\n      per_action_reward_fns[a](observation, enable_noise)\n      for a in range(num_actions)\n  ],\n                     axis=-1)\n\n  optimal_action = np.argmax(rewards, axis=-1)\n  return optimal_action\n\n\n@gin.configurable\ndef tf_compute_optimal_action(observation,\n                              per_action_reward_fns,\n                              enable_noise=False,\n                              action_dtype=tf.int32):\n  """"""TF wrapper around `compute_optimal_action` to be used in `tf_metrics`.""""""\n  compute_optimal_action_fn = functools.partial(\n      compute_optimal_action,\n      per_action_reward_fns=per_action_reward_fns,\n      enable_noise=enable_noise)\n  return tf.py_function(compute_optimal_action_fn, [observation], action_dtype)\n\n\n@gin.configurable\ndef compute_optimal_reward_with_environment_dynamics(\n    observation, environment_dynamics):\n  """"""Computes the optimal reward using the environment dynamics.\n\n  Args:\n    observation: a (possibly batched) observation.\n    environment_dynamics: environment dynamics object (an instance of\n      `non_stationary_stochastic_environment.EnvironmentDynamics`)\n\n  Returns:\n    The optimal reward.\n  """"""\n  return environment_dynamics.compute_optimal_reward(observation)\n\n\n@gin.configurable\ndef compute_optimal_action_with_environment_dynamics(\n    observation, environment_dynamics):\n  """"""Computes the optimal action using the environment dynamics.\n\n  Args:\n    observation: a (possibly batched) observation.\n    environment_dynamics: environment dynamics object (an instance of\n      `non_stationary_stochastic_environment.EnvironmentDynamics`)\n\n  Returns:\n    The optimal action.\n  """"""\n  return environment_dynamics.compute_optimal_action(observation)\n\n\n@gin.configurable\ndef compute_optimal_action_with_classification_environment(\n    observation, environment):\n  """"""Helper function for gin configurable SuboptimalArms metric.""""""\n  del observation\n  return environment.compute_optimal_action()\n\n\n@gin.configurable\ndef compute_optimal_reward_with_classification_environment(\n    observation, environment):\n  """"""Helper function for gin configurable Regret metric.""""""\n  del observation\n  return environment.compute_optimal_reward()\n\n\n@gin.configurable\ndef tf_wheel_bandit_compute_optimal_action(observation,\n                                           delta,\n                                           action_dtype=tf.int32):\n  """"""TF wrapper around `compute_optimal_action` to be used in `tf_metrics`.""""""\n  return tf.py_function(wheel_py_environment.compute_optimal_action,\n                        [observation, delta], action_dtype)\n\n\n@gin.configurable\ndef tf_wheel_bandit_compute_optimal_reward(observation, delta, mu_inside,\n                                           mu_high):\n  """"""TF wrapper around `compute_optimal_reward` to be used in `tf_metrics`.""""""\n  return tf.py_function(wheel_py_environment.compute_optimal_reward,\n                        [observation, delta, mu_inside, mu_high], tf.float32)\n'"
tf_agents/bandits/environments/mushroom_environment_utilities.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\n""""""Helper functions for the mushroom dataset.\n\nThe dataset is expected to be a CSV with the first column being the label, the\nother columns represent features. All features, as well as the label, are string\nfeatures that will be used for one-hot embeddings.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\n\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\n\n\ndef _validate_mushroom_data(numpy_data):\n  """"""Checks if the numpy array looks like the mushroom dataset.\n\n  Args:\n    numpy_data: numpy array of rank 2 consisting of single characters. It should\n    contain the mushroom dataset with each column being a feature and each row\n    being a sample.\n  """"""\n  assert numpy_data.shape[1] == 23, \'The dataset should have 23 columns.\'\n  assert set(numpy_data[:, 0]) == {\n      \'e\', \'p\'\n  }, \'The first column should be the label with values `e` and `p`.\'\n\n\ndef _one_hot(data):\n  """"""Encodes columns of a numpy array as one-hot.\n\n  Args:\n    data: A numpy array of rank 2. Every column is a categorical feature and\n      every row is a sample.\n\n  Returns:\n    A 0/1 numpy array of rank 2 containing the same number of rows as the input.\n    The number of columns is equal to the sum of distinct elements per column of\n    the input array.\n  """"""\n  num_rows, num_cols = np.shape(data)\n  encoded = np.array([], dtype=np.int32).reshape((num_rows, 0))\n  for i in range(num_cols):\n    vocabulary = sorted(list(set(data[:, i])))\n    lookup = dict(list(zip(vocabulary, list(range(len(vocabulary))))))\n    int_encoded = np.array([lookup[x] for x in data[:, i]])\n    new_cols = np.eye(len(vocabulary), dtype=np.int32)[int_encoded]\n    encoded = np.append(encoded, new_cols, axis=1)\n  return encoded\n\n\n@gin.configurable\ndef convert_mushroom_csv_to_tf_dataset(file_path, buffer_size=40000):\n  """"""Converts the mushroom CSV dataset into a `tf.Dataset`.\n\n  The dataset CSV contains the label in the first column, then the features.\n  Two example rows:\n    `p,x,s,n,t,p,f,c,n,k,e,e,s,s,w,w,p,w,o,p,k,s,u`: poisonous;\n    `e,x,s,y,t,a,f,c,b,k,e,c,s,s,w,w,p,w,o,p,n,n,g`: edible.\n\n  Args:\n    file_path: Path to the CSV file.\n    buffer_size: The buffer to use for shuffling the data.\n\n  Returns:\n    A `tf.Dataset`, infinitely looped, shuffled, not batched.\n\n  Raises:\n    AssertionError: If the CSV file does not conform to the syntax of the\n      mushroom environment.\n  """"""\n  with tf.gfile.Open(file_path, \'r\') as infile:\n    nd = np.genfromtxt(infile, dtype=np.str, delimiter=\',\')\n  _validate_mushroom_data(nd)\n  encoded = _one_hot(nd)\n  contexts = encoded[:, 2:]\n  context_tensor = tf.cast(contexts, tf.float32)\n  labels = encoded[:, 0]\n  label_tensor = tf.cast(labels, tf.int32)\n  dataset = tf.data.Dataset.from_tensor_slices((context_tensor, label_tensor))\n  return dataset.repeat().shuffle(buffer_size=buffer_size)\n\n\n@gin.configurable\ndef mushroom_reward_distribution(r_noeat, r_eat_safe, r_eat_poison_bad,\n                                 r_eat_poison_good, prob_poison_bad):\n  """"""Creates a distribution for rewards for the mushroom environment.\n\n  Args:\n    r_noeat: (float) Reward value for not eating the mushroom.\n    r_eat_safe: (float) Reward value for eating an edible mushroom.\n    r_eat_poison_bad: (float) Reward value for eating and getting poisoned from\n      a poisonous mushroom.\n    r_eat_poison_good: (float) Reward value for surviving after eating a\n      poisonous mushroom.\n    prob_poison_bad: Probability of getting poisoned by a poisonous mushroom.\n\n  Returns:\n    A reward distribution table, instance of `tfd.Distribution`.\n  """"""\n\n  # The function works by first creating a 2x2 Bernoulli with all but one having\n  # parameter 0. The fourth one, that corresponds to eating a poisonous mushroom\n  # has parameter `prob_poison_bad`. Then, the whole table is shifted and scaled\n  # to the desired values.\n\n  distr = tfd.Bernoulli(probs=[[0, prob_poison_bad], [0, 0]], dtype=tf.float32)\n  reward_distr = (\n      tfp.bijectors.Shift(\n          [[r_noeat, r_eat_poison_bad],\n           [r_noeat, r_eat_safe]])\n      (tfp.bijectors.Scale(\n          [[1, r_eat_poison_good - r_eat_poison_bad],\n           [1, 1]])\n       (distr)))\n  return tfd.Independent(reward_distr, reinterpreted_batch_ndims=2)\n'"
tf_agents/bandits/environments/mushroom_environment_utilities_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.mushroom_environment_utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import mushroom_environment_utilities\n\n\nclass MushroomEnvironmentUtilitiesTest(tf.test.TestCase):\n\n  def testOneHot(self):\n    data = np.array([[1, 2], [1, 3], [2, 2], [1, 1]], dtype=np.int32)\n    encoded = mushroom_environment_utilities._one_hot(data)\n    expected = [[1, 0, 0, 1, 0], [1, 0, 0, 0, 1], [0, 1, 0, 1, 0],\n                [1, 0, 1, 0, 0]]\n    np.testing.assert_array_equal(encoded, expected)\n\n  def testRewardDistribution(self):\n    reward_distr = mushroom_environment_utilities.mushroom_reward_distribution(\n        r_noeat=0.0,\n        r_eat_safe=5.0,\n        r_eat_poison_bad=-35.0,\n        r_eat_poison_good=5.0,\n        prob_poison_bad=0.5)\n    np.testing.assert_array_equal(reward_distr.mean(), [[0, -15.], [0, 5.]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/non_stationary_stochastic_environment.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bandit environment that returns random observations and rewards.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport gin\nimport six\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import bandit_tf_environment as bte\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import common\n\n\n@six.add_metaclass(abc.ABCMeta)\n@gin.configurable\nclass EnvironmentDynamics(tf.Module):\n  """"""Abstract class to represent a non-stationary environment dynamics.\n\n  This class is used with the NonStationaryStochasticEnvironment class below to\n  obtain a non-stationary environment.\n  To define a dynamics, derive from this class and define the abstract methods\n  and properties below.\n  To work correctly with graph and eager mode, Tensorflow variables must be\n  defined in the constructor of this class. When used within a\n  `BanditTFEnvironment` autodeps in reset and step functions will handle\n  automatically the operation order.\n\n  """"""\n\n  @abc.abstractproperty\n  def batch_size(self):\n    """"""Returns the batch size used for observations and rewards.""""""\n    pass\n\n  @abc.abstractproperty\n  def observation_spec(self):\n    """"""Specification of the observations.""""""\n    pass\n\n  @abc.abstractproperty\n  def action_spec(self):\n    """"""Specification of the actions.""""""\n    pass\n\n  @abc.abstractmethod\n  def observation(self, env_time):\n    """"""Returns an observation batch for the given time.\n\n    Args:\n      env_time: The scalar int64 tensor of the environment time step. This is\n        incremented by the environment after the reward is computed.\n\n    Returns:\n      The observation batch with spec according to `observation_spec.`\n    """"""\n    pass\n\n  @abc.abstractmethod\n  def reward(self, observation, env_time):\n    """"""Reward for the given observation and time step.\n\n    Args:\n      observation: A batch of observations with spec according to\n        `observation_spec.`\n      env_time: The scalar int64 tensor of the environment time step. This is\n        incremented by the environment after the reward is computed.\n\n    Returns:\n      A batch of rewards with spec shape [batch_size, num_actions] containing\n      rewards for all arms.\n    """"""\n    pass\n\n\ndef create_variable_from_spec_nest(specs, batch_size):\n  def create_variable(spec):\n    return common.create_variable(\n        name=spec.name,\n        dtype=spec.dtype,\n        shape=[batch_size] + spec.shape.as_list())\n  return tf.nest.map_structure(create_variable, specs)\n\n\ndef assign_variable_nest(variables, values):\n  return tf.nest.map_structure(lambda variable, value: variable.assign(value),\n                               variables,\n                               values)\n\n\ndef read_value_nest(variables):\n  return tf.nest.map_structure(lambda variable: variable.read_value(),\n                               variables)\n\n\n@gin.configurable\nclass NonStationaryStochasticEnvironment(bte.BanditTFEnvironment):\n  """"""Implements a general non-stationary environment.\n\n  This environment keeps a Tensorflow variable (`env_time`) to keep track of the\n  current timme. This is incremented after every update of the reward tensor.\n  The `EnvironmentDynamics` object passed to the constructor determines how\n  observations and rewards are computed for the current time.\n  """"""\n\n  def __init__(self, environment_dynamics):\n    """"""Initializes a non-stationary environment with the given dynamics.\n\n    Args:\n      environment_dynamics: An instance of `EnvironmentDynamics` defining how\n        the environment evolves over time.\n    """"""\n    self._env_time = tf.compat.v2.Variable(\n        0, trainable=False, name=\'env_time\', dtype=tf.int64)\n    self._environment_dynamics = environment_dynamics\n    observation_spec = environment_dynamics.observation_spec\n    self._observation = create_variable_from_spec_nest(\n        observation_spec, environment_dynamics.batch_size)\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    super(NonStationaryStochasticEnvironment, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=environment_dynamics.action_spec,\n        batch_size=environment_dynamics.batch_size)\n\n  @property\n  def environment_dynamics(self):\n    return self._environment_dynamics\n\n  def _apply_action(self, action):\n    self._reward = self._environment_dynamics.reward(self._observation,\n                                                     self._env_time)\n    tf.compat.v1.assign_add(self._env_time,\n                            self._environment_dynamics.batch_size)\n    return common.index_with_actions(\n        self._reward, tf.cast(action, dtype=tf.int32))\n\n  def _observe(self):\n    assign_variable_nest(\n        self._observation,\n        self._environment_dynamics.observation(self._env_time))\n    return read_value_nest(self._observation)\n'"
tf_agents/bandits/environments/non_stationary_stochastic_environment_test.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.bandit_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nsse\nfrom tf_agents.specs import tensor_spec\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\nclass DummyDynamics(nsse.EnvironmentDynamics):\n\n  @property\n  def batch_size(self):\n    return 2\n\n  @property\n  def observation_spec(self):\n    return tensor_spec.TensorSpec(\n        shape=[3],\n        dtype=tf.float32,\n        name=\'observation_spec\')\n\n  @property\n  def action_spec(self):\n    return tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=5,\n        name=\'action\')\n\n  def observation(self, t):\n    return (tf.constant([[1.0, 2.0, 3.0], [0.0, 4.0, 5.0]], dtype=tf.float32) +\n            tf.reshape(tf.cast(t, dtype=tf.float32), [1, 1]))\n\n  def reward(self, observation, t):\n    return (tf.concat([observation, tf.zeros([2, 2])], axis=1) -\n            tf.reshape(tf.cast(t, dtype=tf.float32), [1, 1]))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass NonStationaryStochasticEnvironmentTest(tf.test.TestCase):\n\n  def testObservationAndRewardsVary(self):\n    """"""Ensure that observations and rewards change in consecutive calls.""""""\n    dynamics = DummyDynamics()\n    env = nsse.NonStationaryStochasticEnvironment(dynamics)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    env_time = env._env_time\n    observation_samples = []\n    reward_samples = []\n\n    if tf.executing_eagerly():\n      for t in range(0, 10):\n        ts = env.reset()\n        observation = ts.observation\n        reward = env.step(tf.zeros([2])).reward\n\n        [observation_sample, reward_sample, env_time_sample] = self.evaluate(\n            [observation, reward, env_time])\n        observation_samples.append(observation_sample)\n        reward_samples.append(reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * dynamics.batch_size)\n\n    else:\n      ts = env.reset()\n      observation = ts.observation\n      reward = env.step(tf.zeros([2])).reward\n\n      for t in range(0, 10):\n        # The order of evaluations below matters. We first compute observation\n        # batch, then the reward, and finally the env_time tensor. Joining the\n        # evaluations in a single call does not guarantee the right order.\n\n        observation_sample = self.evaluate(observation)\n        reward_sample = self.evaluate(reward)\n        env_time_sample = self.evaluate(env_time)\n        observation_samples.append(observation_sample)\n        reward_samples.append(reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * dynamics.batch_size)\n\n    for t in range(0, 10):\n      t_b = t * dynamics.batch_size\n      self.assertAllClose(observation_samples[t],\n                          [[1.0 + t_b, 2.0 + t_b, 3.0 + t_b],\n                           [0.0 + t_b, 4.0 + t_b, 5.0 + t_b]])\n      self.assertAllClose(reward_samples[t], [1, 0])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/piecewise_bernoulli_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class implementation of Python Bernoulli Bandit environment.""""""\nimport gin\nimport numpy as np\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.specs import array_spec\n\n\n@gin.configurable\nclass PiecewiseBernoulliPyEnvironment(\n    bandit_py_environment.BanditPyEnvironment):\n  """"""Implements piecewise stationary finite-armed Bernoulli Bandits.\n\n  This environment implements piecewise stationary finite-armed non-contextual\n  Bernoulli Bandit environment as a subclass of BanditPyEnvironment.\n  With respect to Bernoulli stationary environment, the reward distribution\n  parameters undergo abrupt changes at given time steps. The current time is\n  kept by the environment and increased by a unit at each call of _apply_action.\n  For each stationary piece, the reward distribution is 0/1 (Bernoulli) with\n  the parameter p valid for the current piece.\n\n  Examples:\n\n  means = [[0.1, 0.5], [0.5, 0.1], [0.5, 0.5]]  # 3 pieces, 2 arms.\n\n  def constant_duration_gen(delta):\n    while True:\n      yield delta\n\n  env_piecewise_10_steps = PiecewiseBernoulliPyEnvironment(\n    means, constant_duration_gen(10))\n\n  def random_duration_gen(a, b):\n     while True:\n       yield random.randint(a, b)\n\n  env_rnd_piecewise_10_to_20_steps =  PiecewiseBernoulliPyEnvironment(\n    means, random_duration_gen(10, 20))\n\n  For a reference on bandits see e.g., Example 1.1 in ""A Tutorial on Thompson\n  Sampling"" by Russo et al. (https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n  A paper using piecewise stationary environments is Qingyun Wu, Naveen Iyer,\n  Hongning Wang, ``Learning Contextual Bandits in a Non-stationary\n  Environment,\'\' Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (https://arxiv.org/pdf/1805.09365.pdf.)\n  """"""\n\n  def __init__(self, piece_means, change_duration_generator, batch_size=1):\n    """"""Initializes a piecewise stationary Bernoulli Bandit environment.\n\n    Args:\n      piece_means: a matrix (list of lists) with shape (num_pieces, num_arms)\n        containing floats in [0, 1]. Each list contains the mean rewards for\n        the num_arms actions of the num_pieces pieces. The list is wrapped\n        around after the last piece.\n      change_duration_generator: a generator of the time durations. If this\n        yields the values d0, d1, d2, ..., then the reward parameters change at\n        steps d0, d0 + d1, d0 + d1 + d2, ..., as following:\n\n        piece_means[0] for 0 <= t < d0\n        piece_means[1] for d0 <= t < d0 + d1\n        piece_means[2] for d0 + d1 <= t < d0 + d1 + d2\n        ...\n\n        Note that the values generated have to be non-negative. The value zero\n        means that the corresponding parameters in the piece_means list are\n        skipped, i.e. the duration of the piece is zero steps.\n        If the generator ends (e.g. if it is obtained with iter(<list>)) and the\n        step goes beyond the last piece, a StopIteration exception is raised.\n      batch_size: If specified, this is the batch size for observation and\n        actions.\n    """"""\n    self._batch_size = batch_size\n    self._piece_means = np.asarray(piece_means, dtype=np.float32)\n    if np.any(self._piece_means > 1.0) or np.any(self._piece_means < 0):\n      raise ValueError(\'All parameters should be floats in [0, 1].\')\n    self._num_pieces, self._num_actions = self._piece_means.shape\n    self._change_duration_generator = change_duration_generator\n    self._current_time = -1\n    self._current_piece = -1\n    self._next_change = 0\n    self._increment_time()\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n    observation_spec = array_spec.ArraySpec(\n        shape=(1,), dtype=np.int32, name=\'observation\')\n    super(PiecewiseBernoulliPyEnvironment, self).__init__(\n        observation_spec, action_spec)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def batched(self):\n    return True\n\n  def _increment_time(self):\n    self._current_time += 1\n    while self._current_time >= self._next_change:\n      duration = int(next(self._change_duration_generator))\n      if duration < 0:\n        raise ValueError(\n            \'Generated duration must be non-negative. Got {}.\'.format(duration))\n      self._next_change += duration\n      self._current_piece = (self._current_piece + 1) % self._num_pieces\n\n  def _observe(self):\n    return np.zeros(\n        shape=[self._batch_size, 1],\n        dtype=self.observation_spec().dtype)\n\n  def _apply_action(self, action):\n    reward = np.floor(self._piece_means[self._current_piece, action] +\n                      np.random.random((self._batch_size,)))\n    self._increment_time()\n    return reward\n'"
tf_agents/bandits/environments/piecewise_bernoulli_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Bernoulli Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import piecewise_bernoulli_py_environment as pbe\n\n\nclass PiecewiseBernoulliBanditPyEnvironmentTest(tf.test.TestCase,\n                                                parameterized.TestCase):\n\n  def deterministic_duration_generator(self):\n    while True:\n      yield 10\n\n  def test_out_of_bound_parameter(self):\n    with self.assertRaisesRegexp(\n        ValueError, r\'All parameters should be floats in \\[0, 1\\]\\.\'):\n      pbe.PiecewiseBernoulliPyEnvironment(\n          [[0.1, 1.2, 0.3]], self.deterministic_duration_generator())\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_batch_1\',\n           batch_size=1),\n      dict(testcase_name=\'_batch_4\',\n           batch_size=4),\n  )\n  def test_correct_piece(self, batch_size):\n    env = pbe.PiecewiseBernoulliPyEnvironment(\n        [[0.1, 0.2, 0.3], [0.3, 0.2, 0.1], [0.1, 0.12, 0.14]],\n        self.deterministic_duration_generator(), batch_size)\n    for t in range(100):\n      env.reset()\n      self.assertEqual(int(t / 10) % 3, env._current_piece)\n      _ = env.step([0])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/piecewise_stochastic_environment.py,28,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bandit piecewise linear stationary environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nsse\nfrom tf_agents.specs import tensor_spec\n\n\ndef _raise_batch_shape_error(tensor_name, batch_shape):\n  raise ValueError(\'`{tensor_name}` must have batch shape with length 1; \'\n                   \'got {batch_shape}.\'.format(\n                       tensor_name=tensor_name,\n                       batch_shape=batch_shape))\n\n\n@gin.configurable\nclass PiecewiseStationaryDynamics(nsse.EnvironmentDynamics):\n  """"""A piecewise stationary environment dynamics.\n\n  This is a piecewise stationary environment which computes rewards as:\n\n  rewards(t) = observation(t) * observation_to_reward(i) + additive_reward(i)\n\n  where t is the environment time (env_time) and i is the index of each piece.\n  The environment time is incremented after the reward is computed while the\n  piece index is incremented at the end of the time interval. The parameters\n  observation_to_reward(i), additive_reward(i), and the length of interval, are\n  drawn from given distributions at the beginning of each temporal interval.\n  """"""\n\n  def __init__(self,\n               observation_distribution,\n               interval_distribution,\n               observation_to_reward_distribution,\n               additive_reward_distribution):\n    """"""Initialize the parameters of the piecewise dynamics.\n\n    Args:\n      observation_distribution: A distribution from tfp.distributions with shape\n        `[batch_size, observation_dim]` Note that the values of `batch_size` and\n        `observation_dim` are deduced from the distribution.\n      interval_distribution: A scalar distribution from `tfp.distributions`. The\n        value is casted to `int64` to update the time range.\n      observation_to_reward_distribution: A distribution from\n        `tfp.distributions` with shape `[observation_dim, num_actions]`. The\n        value `observation_dim` must match the second dimension of\n        `observation_distribution`.\n      additive_reward_distribution: A distribution from `tfp.distributions` with\n        shape `[num_actions]`. This models the non-contextual behavior of the\n        bandit.\n    """"""\n    self._observation_distribution = observation_distribution\n    self._interval_distribution = interval_distribution\n    self._observation_to_reward_distribution = (\n        observation_to_reward_distribution)\n    self._additive_reward_distribution = additive_reward_distribution\n\n    observation_batch_shape = observation_distribution.batch_shape\n    reward_batch_shape = additive_reward_distribution.batch_shape\n\n    if observation_batch_shape.rank != 2:\n      _raise_batch_shape_error(\n          \'observation_distribution\', observation_batch_shape)\n\n    if reward_batch_shape.rank != 1:\n      _raise_batch_shape_error(\n          \'additive_reward_distribution\', reward_batch_shape)\n\n    if additive_reward_distribution.dtype != tf.float32:\n      raise ValueError(\'Reward  must have dtype float32; got {}\'.format(\n          self._reward.dtype))\n\n    expected_observation_to_reward_shape = [\n        tf.compat.dimension_value(\n            self._observation_distribution.batch_shape[1:]),\n        tf.compat.dimension_value(\n            self._additive_reward_distribution.batch_shape[0])]\n\n    observation_to_reward_shape = [\n        tf.compat.dimension_value(x)\n        for x in observation_to_reward_distribution.batch_shape]\n\n    if (observation_to_reward_shape !=\n        expected_observation_to_reward_shape):\n      raise ValueError(\n          \'Observation to reward has {} as expected shape; got {}\'.format(\n              observation_to_reward_shape,\n              expected_observation_to_reward_shape))\n\n    self._current_interval = tf.compat.v2.Variable(\n        tf.cast(interval_distribution.sample(), dtype=tf.int64),\n        dtype=tf.int64, name=\'interval\')\n    self._current_observation_to_reward = tf.compat.v2.Variable(\n        observation_to_reward_distribution.sample(),\n        dtype=tf.float32,\n        name=\'observation_to_reward\')\n    self._current_additive_reward = tf.compat.v2.Variable(\n        additive_reward_distribution.sample(),\n        dtype=tf.float32,\n        name=\'additive_reward\')\n\n  @property\n  def batch_size(self):\n    return tf.compat.dimension_value(\n        self._observation_distribution.batch_shape[0])\n\n  @property\n  def observation_spec(self):\n    return tensor_spec.TensorSpec(\n        shape=self._observation_distribution.batch_shape[1:],\n        dtype=self._observation_distribution.dtype,\n        name=\'observation_spec\')\n\n  @property\n  def action_spec(self):\n    return tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=tf.compat.dimension_value(\n            self._additive_reward_distribution.batch_shape[0]) - 1,\n        name=\'action\')\n\n  def observation(self, unused_t):\n    return self._observation_distribution.sample()\n\n  def reward(self, observation, t):\n    def same_interval_parameters():\n      """"""Returns the parameters of the current piece.\n\n      Returns:\n        The pair of `tf.Tensor` `(observation_to_reward, additive_reward)`.\n      """"""\n      return [self._current_observation_to_reward,\n              self._current_additive_reward]\n\n    def new_interval_parameters():\n      """"""Update and returns the piece parameters.\n\n      Returns:\n        The pair of `tf.Tensor` `(observation_to_reward, additive_reward)`.\n      """"""\n      tf.compat.v1.assign_add(\n          self._current_interval,\n          tf.cast(self._interval_distribution.sample(), dtype=tf.int64))\n      tf.compat.v1.assign(self._current_observation_to_reward,\n                          self._observation_to_reward_distribution.sample())\n      tf.compat.v1.assign(self._current_additive_reward,\n                          self._additive_reward_distribution.sample())\n\n      return [self._current_observation_to_reward,\n              self._current_additive_reward]\n\n    observation_to_reward, additive_reward = tf.cond(\n        t < self._current_interval,\n        same_interval_parameters,\n        new_interval_parameters)\n\n    reward = (tf.matmul(observation, observation_to_reward) +\n              tf.reshape(additive_reward, [1, -1]))\n    return reward\n\n  @gin.configurable\n  def compute_optimal_reward(self, observation):\n    deterministic_reward = tf.matmul(\n        observation, self._current_observation_to_reward)\n    optimal_action_reward = tf.reduce_max(deterministic_reward, axis=-1)\n    return optimal_action_reward\n\n  @gin.configurable\n  def compute_optimal_action(self, observation):\n    deterministic_reward = tf.matmul(\n        observation, self._current_observation_to_reward)\n    optimal_action = tf.argmax(\n        deterministic_reward, axis=-1, output_type=tf.int32)\n    return optimal_action\n\n\n@gin.configurable\nclass PiecewiseStochasticEnvironment(nsse.NonStationaryStochasticEnvironment):\n  """"""Implements a piecewise stationary linear environment.""""""\n\n  def __init__(self,\n               observation_distribution,\n               interval_distribution,\n               observation_to_reward_distribution,\n               additive_reward_distribution):\n    """"""Initialize the environment with the dynamics parameters.\n\n    Args:\n      observation_distribution: A distribution from `tfp.distributions` with\n        shape `[batch_size, observation_dim]`. Note that the values of\n        `batch_size` and `observation_dim` are deduced from the distribution.\n      interval_distribution: A scalar distribution from `tfp.distributions`. The\n        value is casted to `int64` to update the time range.\n      observation_to_reward_distribution: A distribution from\n        `tfp.distributions` with shape `[observation_dim, num_actions]`. The\n        value `observation_dim` must match the second dimension of\n        `observation_distribution`.\n      additive_reward_distribution: A distribution from `tfp.distributions` with\n        shape `[num_actions]`. This models the non-contextual behavior of the\n        bandit.\n    """"""\n    super(PiecewiseStochasticEnvironment, self).__init__(\n        PiecewiseStationaryDynamics(\n            observation_distribution,\n            interval_distribution,\n            observation_to_reward_distribution,\n            additive_reward_distribution))\n'"
tf_agents/bandits/environments/piecewise_stochastic_environment_test.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.bandit_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.environments import piecewise_stochastic_environment as pse\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\ntfd = tfp.distributions\n\n\ndef get_deterministic_gaussian_non_stationary_environment(\n    observation_shape, action_shape, batch_size, interval):\n  """"""Returns a PiecewiseStochasticEnvironment with deterministic intervals.""""""\n  overall_shape = [batch_size] + observation_shape\n  observation_distribution = tfd.Normal(\n      loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n  interval_distribution = tfd.Deterministic(interval)\n  observation_to_reward_shape = observation_shape + action_shape\n  observation_to_reward_distribution = tfd.Normal(\n      loc=tf.zeros(observation_to_reward_shape),\n      scale=tf.ones(observation_to_reward_shape))\n  additive_reward_distribution = tfd.Normal(\n      loc=tf.zeros(action_shape),\n      scale=tf.ones(action_shape))\n  return pse.PiecewiseStochasticEnvironment(\n      observation_distribution,\n      interval_distribution,\n      observation_to_reward_distribution,\n      additive_reward_distribution)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass PiecewiseStochasticEnvironmentTest(tf.test.TestCase,\n                                         parameterized.TestCase):\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[5]_action_[3]_batch_1\',\n           observation_shape=[5],\n           action_shape=[3],\n           batch_size=1,\n           seed=12345),\n      dict(testcase_name=\'_observation_[3]_action_[5]_batch_2\',\n           observation_shape=[3],\n           action_shape=[5],\n           batch_size=2,\n           seed=98765),\n  )\n\n  def testObservationAndRewardsVary(\n      self, observation_shape, action_shape, batch_size, seed):\n    """"""Ensure that observations and rewards change in consecutive calls.""""""\n    interval = 4\n\n    env = get_deterministic_gaussian_non_stationary_environment(\n        observation_shape, action_shape, batch_size, interval)\n    tf.compat.v1.set_random_seed(seed)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    observation_to_reward_samples = []\n    additive_reward_samples = []\n    env_time = env._env_time\n    observation_to_reward = (\n        env._environment_dynamics._current_observation_to_reward)\n    additive_reward = env._environment_dynamics._current_additive_reward\n\n    if tf.executing_eagerly():\n      for t in range(0, 10):\n        ts = env.reset()\n        reward = env.step(tf.zeros([batch_size])).reward\n\n        (observation_to_reward_sample,\n         additive_reward_sample,\n         env_time_sample) = self.evaluate([observation_to_reward,\n                                           additive_reward,\n                                           env_time])\n        observation_to_reward_samples.append(observation_to_reward_sample)\n        additive_reward_samples.append(additive_reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * batch_size)\n\n    else:\n      ts = env.reset()\n      reward = env.step(tf.zeros([batch_size])).reward\n\n      for t in range(0, 10):\n        unused_ts_sample = self.evaluate(ts)\n        unused_reward_sample = self.evaluate(reward)\n\n        (observation_to_reward_sample,\n         additive_reward_sample,\n         env_time_sample) = self.evaluate([observation_to_reward,\n                                           additive_reward,\n                                           env_time])\n\n        observation_to_reward_samples.append(observation_to_reward_sample)\n        additive_reward_samples.append(additive_reward_sample)\n        self.assertEqual(env_time_sample, (t + 1) * batch_size)\n\n    for t in range(0, 10):\n      self.assertAllClose(\n          observation_to_reward_samples[int(t - t % (interval / batch_size))],\n          observation_to_reward_samples[t])\n      self.assertAllClose(\n          additive_reward_samples[int(t - t % (interval / batch_size))],\n          additive_reward_samples[t])\n\n    for t in range(interval // batch_size, 10, interval // batch_size):\n      self.assertNotAllClose(\n          observation_to_reward_samples[int(t) - 1],\n          observation_to_reward_samples[int(t)])\n      self.assertNotAllClose(\n          additive_reward_samples[int(t) - 1],\n          additive_reward_samples[int(t)])\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[5]_action_[3]_batch_1\',\n           observation_shape=[5],\n           action_shape=[3],\n           batch_size=1),\n      dict(testcase_name=\'_observation_[7]_action_[5]_batch_32\',\n           observation_shape=[7],\n           action_shape=[5],\n           batch_size=32),\n      )\n  def testActionSpec(\n      self, observation_shape, action_shape, batch_size):\n    """"""Ensure that the action spec is set correctly.""""""\n    interval = 3\n    env = get_deterministic_gaussian_non_stationary_environment(\n        observation_shape, action_shape, batch_size, interval)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertEqual(env.action_spec().maximum, action_shape[0] - 1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/random_bandit_environment.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bandit environment that returns random observations and rewards.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.environments import bandit_tf_environment as bte\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step\n\n__all__ = [\'RandomBanditEnvironment\']\n\n\ndef _raise_batch_shape_error(distribution_name, batch_shape):\n  raise ValueError(\'`{distribution_name}` must have batch shape with length 1; \'\n                   \'got {batch_shape}. Consider using \'\n                   \'`tensorflow_probability.distributions.Independent` \'\n                   \'to manipulate batch and event shapes.\'.format(\n                       distribution_name=distribution_name,\n                       batch_shape=batch_shape))\n\n\nclass RandomBanditEnvironment(bte.BanditTFEnvironment):\n  """"""Bandit environment that returns random observations and rewards.""""""\n\n  def __init__(self,\n               observation_distribution,\n               reward_distribution,\n               action_spec=None):\n    """"""Initializes an environment that returns random observations and rewards.\n\n    Note that `observation_distribution` and `reward_distribution` are expected\n    to have batch rank 1. That is, `observation_distribution.batch_shape` should\n    have length exactly 1. `tensorflow_probability.distributions.Independent` is\n    useful for manipulating batch and event shapes. For example,\n\n    ```python\n    observation_distribution = tfd.Independent(tfd.Normal(tf.zeros([12, 3, 4]),\n                                                          tf.ones([12, 3, 4])))\n    env = RandomBanditEnvironment(observation_distribution, ...)\n    env.observation_spec  # tensor_spec.TensorSpec(shape=[3, 4], ...)\n    env.batch_size  # 12\n    ```\n\n    Args:\n      observation_distribution: a `tensorflow_probability.Distribution`.\n        Batches of observations will be drawn from this distribution. The\n        `batch_shape` of this distribution must have length 1 and be the same as\n        the `batch_shape` of `reward_distribution`.\n      reward_distribution: a `tensorflow_probability.Distribution`.\n        Batches of rewards will be drawn from this distribution. The\n        `batch_shape` of this distribution must have length 1 and be the same as\n        the `batch_shape` of `observation_distribution`.\n      action_spec: a `TensorSpec` describing the expected action. Note that\n        actions are ignored and do not affect rewards.\n    """"""\n    observation_batch_shape = observation_distribution.batch_shape\n    reward_batch_shape = reward_distribution.batch_shape\n    reward_event_shape = reward_distribution.event_shape\n\n    if observation_batch_shape.rank != 1:\n      _raise_batch_shape_error(\n          \'observation_distribution\', observation_batch_shape)\n\n    if reward_batch_shape.rank != 1:\n      _raise_batch_shape_error(\n          \'reward_distribution\', observation_batch_shape)\n\n    if reward_event_shape.rank != 0:\n      raise ValueError(\'`reward_distribution` must have event_shape (); \'\n                       \'got {}\'.format(reward_event_shape))\n\n    if reward_distribution.dtype != tf.float32:\n      raise ValueError(\'`reward_distribution` must have dtype float32; \'\n                       \'got {}\'.format(reward_distribution.float32))\n\n    if observation_batch_shape[0] != reward_batch_shape[0]:\n      raise ValueError(\n          \'`reward_distribution` and `observation_distribution` must have the \'\n          \'same batch shape; got {} and {}\'.format(\n              reward_batch_shape, observation_batch_shape))\n    batch_size = tf.compat.dimension_value(observation_batch_shape[0])\n    self._observation_distribution = observation_distribution\n    self._reward_distribution = reward_distribution\n    observation_spec = tensor_spec.TensorSpec(\n        shape=self._observation_distribution.event_shape,\n        dtype=self._observation_distribution.dtype,\n        name=\'observation_spec\')\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    super(RandomBanditEnvironment, self).__init__(time_step_spec=time_step_spec,\n                                                  action_spec=action_spec,\n                                                  batch_size=batch_size)\n\n  def _apply_action(self, action):\n    del action  # unused\n    return self._reward_distribution.sample()\n\n  def _observe(self):\n    return self._observation_distribution.sample()\n'"
tf_agents/bandits/environments/random_bandit_environment_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.environments.bandit_tf_environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.environments import random_bandit_environment\nfrom tf_agents.specs import tensor_spec\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef get_gaussian_random_environment(\n    observation_shape, action_shape, batch_size):\n  """"""Returns a RandomBanditEnvironment with Gaussian observation and reward.""""""\n  overall_shape = [batch_size] + observation_shape\n  observation_distribution = tfd.Independent(\n      tfd.Normal(loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape)))\n  reward_distribution = tfd.Normal(\n      loc=tf.zeros(batch_size), scale=tf.ones(batch_size))\n  action_spec = tensor_spec.TensorSpec(shape=action_shape, dtype=tf.float32)\n  return random_bandit_environment.RandomBanditEnvironment(\n      observation_distribution,\n      reward_distribution,\n      action_spec)\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass RandomBanditEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(overall_observation_shape=[3, 4, 5, 6],\n           batch_dims=2),\n      dict(overall_observation_shape=[3, 3, 3, 3],\n           batch_dims=0),\n      )\n  def testInvalidObservationBatchShape(\n      self, overall_observation_shape, batch_dims):\n    observation_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros(overall_observation_shape),\n                   tf.ones(overall_observation_shape)),\n        reinterpreted_batch_ndims=batch_dims)\n    reward_distribution = tfd.Normal(tf.zeros(overall_observation_shape[0]),\n                                     tf.ones(overall_observation_shape[0]))\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'`observation_distribution` must have batch shape with length 1\'):\n      random_bandit_environment.RandomBanditEnvironment(\n          observation_distribution, reward_distribution)\n\n  @parameterized.parameters(\n      dict(overall_reward_shape=[3, 4, 5, 6],\n           batch_dims=2),\n      dict(overall_reward_shape=[4, 5, 6],\n           batch_dims=0),\n      )\n  def testInvalidRewardBatchShape(\n      self, overall_reward_shape, batch_dims):\n    observation_distribution = tfd.Normal(\n        tf.zeros(overall_reward_shape[0]),\n        tf.ones(overall_reward_shape[0]))\n    reward_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros(overall_reward_shape),\n                   tf.ones(overall_reward_shape)),\n        reinterpreted_batch_ndims=batch_dims)\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'`reward_distribution` must have batch shape with length 1\'):\n      random_bandit_environment.RandomBanditEnvironment(\n          observation_distribution, reward_distribution)\n\n  @parameterized.parameters(\n      dict(overall_reward_shape=[3, 4, 5, 6]),\n      dict(overall_reward_shape=[4, 5, 6]),\n      )\n  def testInvalidRewardEventShape(self, overall_reward_shape):\n    observation_distribution = tfd.Normal(\n        tf.zeros(overall_reward_shape[0]),\n        tf.ones(overall_reward_shape[0]))\n    reward_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros(overall_reward_shape),\n                   tf.ones(overall_reward_shape)))\n    with self.assertRaisesRegexp(\n        ValueError, \'`reward_distribution` must have event_shape ()\'):\n      random_bandit_environment.RandomBanditEnvironment(\n          observation_distribution, reward_distribution)\n\n  @parameterized.parameters(\n      dict(overall_observation_shape=[4, 5, 6],\n           overall_reward_shape=[3]),\n      dict(overall_observation_shape=[3],\n           overall_reward_shape=[1]),\n      )\n  def testMismatchedBatchShape(\n      self, overall_observation_shape, overall_reward_shape):\n    observation_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros(overall_observation_shape),\n                   tf.ones(overall_observation_shape)))\n    reward_distribution = tfd.Independent(\n        tfd.Normal(tf.zeros(overall_reward_shape),\n                   tf.ones(overall_reward_shape)))\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'`reward_distribution` and `observation_distribution` must have the \'\n        \'same batch shape\'):\n      random_bandit_environment.RandomBanditEnvironment(\n          observation_distribution, reward_distribution)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[]_action_[]_batch_1\',\n           observation_shape=[],\n           action_shape=[],\n           batch_size=1),\n      dict(testcase_name=\'_observation_[3, 4, 5, 6]_action_[2, 3, 4]_batch_32\',\n           observation_shape=[3, 4, 5, 6],\n           action_shape=[2, 3, 4],\n           batch_size=32),\n      )\n  def testObservationAndRewardShapes(\n      self, observation_shape, action_shape, batch_size):\n    """"""Exercise `reset` and `step`. Ensure correct shapes are returned.""""""\n    env = get_gaussian_random_environment(\n        observation_shape, action_shape, batch_size)\n    observation = env.reset().observation\n    reward = env.step(tf.zeros(batch_size)).reward\n\n    expected_observation_shape = np.array([batch_size] + observation_shape)\n    expected_reward_shape = np.array([batch_size])\n\n    self.assertAllEqual(\n        expected_observation_shape, self.evaluate(tf.shape(observation)))\n    self.assertAllEqual(\n        expected_reward_shape, self.evaluate(tf.shape(reward)))\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_observation_[]_action_[]_batch_1\',\n           observation_shape=[],\n           action_shape=[],\n           batch_size=1,\n           seed=12345),\n      dict(testcase_name=\'_observation_[3, 4, 5, 6]_action_[2, 3, 4]_batch_32\',\n           observation_shape=[3, 4, 5, 6],\n           action_shape=[2, 3, 4],\n           batch_size=32,\n           seed=98765),\n      )\n  def testObservationAndRewardsVary(\n      self, observation_shape, action_shape, batch_size, seed):\n    """"""Ensure that observations and rewards change in consecutive calls.""""""\n    tf.compat.v1.set_random_seed(seed)\n    env = get_gaussian_random_environment(\n        observation_shape, action_shape, batch_size)\n\n    observation0 = env.reset().observation\n    reward0 = env.step(tf.zeros([batch_size] + action_shape)).reward\n    observation0 = self.evaluate(observation0)\n    reward0 = self.evaluate(reward0)\n\n    observation1 = env.reset().observation\n    reward1 = env.step(tf.zeros([batch_size] + action_shape)).reward\n    self.evaluate(observation1)\n    self.evaluate(reward1)\n\n    self.assertNotAllClose(observation0, observation1)\n    self.assertNotAllClose(reward0, reward1)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/stationary_stochastic_per_arm_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Stationary Stochastic Python Bandit environment with per-arm features.""""""\nimport gin\nimport numpy as np\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import array_spec\n\nGLOBAL_KEY = bandit_spec_utils.GLOBAL_FEATURE_KEY\nPER_ARM_KEY = bandit_spec_utils.PER_ARM_FEATURE_KEY\nNUM_ACTIONS_KEY = bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY\n\n\n@gin.configurable\nclass StationaryStochasticPerArmPyEnvironment(\n    bandit_py_environment.BanditPyEnvironment):\n  """"""Stationary Stochastic Bandit environment with per-arm features.""""""\n\n  def __init__(self,\n               global_context_sampling_fn,\n               arm_context_sampling_fn,\n               max_num_actions,\n               reward_fn,\n               num_actions_fn=None,\n               batch_size=1,\n               add_num_actions_feature=True):\n    """"""Initializes the environment.\n\n    In each round, global context is generated by global_context_sampling_fn,\n    per-arm contexts are generated by arm_context_sampling_fn. The reward_fn\n    function takes the concatenation of a gloabl and a per-arm feature, and\n    outputs a possibly random reward.\n    In case `num_action_fn` is specified, the number of actions will be dynamic.\n    The actual number of actions can be encoded in two ways, depending on the\n    value of `add_num_actions_feature`:\n\n    If `add_num_actions_feature` is True then an extra feature key `num_actions`\n    is added to the observation, with integer feature value indicating the\n    number of available actions.\n\n    If `add_num_actions_feature` is False, then the actually available actions\n    are encoded by an action mask added to the observation in the format of\n    `(observation, [1 1 ... 1 0 ... 0])`.\n\n    Example:\n      def global_context_sampling_fn():\n        return np.random.randint(0, 10, [2])  # 2-dimensional global features.\n\n      def arm_context_sampling_fn():\n        return np.random.randint(-3, 4, [3])  # 3-dimensional arm features.\n\n      def reward_fn(x):\n        return sum(x)\n\n      def num_actions_fn():\n        return np.random.randint(2, 6)\n\n      env = StationaryStochasticPerArmPyEnvironment(global_context_sampling_fn,\n                                                    arm_context_sampling_fn,\n                                                    5,\n                                                    reward_fn,\n                                                    num_actions_fn)\n\n    Args:\n      global_context_sampling_fn: A function that outputs a random 1d array or\n        list of ints or floats. This output is the global context. Its shape and\n        type must be consistent accross calls.\n      arm_context_sampling_fn: A function that outputs a random 1 array or list\n        of ints or floats (same type as the output of\n        `global_context_sampling_fn`). This output is the per-arm context. Its\n        shape must be consistent accross calls.\n      max_num_actions: (int) the maximum number of actions in every sample. If\n        `num_actions_fn` is not set, this many actions are available in every\n        time step.\n      reward_fn: A function that generates a reward when called with an\n        observation.\n      num_actions_fn: If set, it should be a function that outputs a single\n        integer specifying the number of actions for a given time step. The\n        value output by this function will be capped between 1 and\n        `max_num_actions`. Depending on the value of `add_num_actions_feature`,\n        the number of actions will be encoded by either the feature key\n        `num_actions`, or an action mask \'[1 1 ... 1 0 0 ... 0]\'.\n      batch_size: The batch size.\n      add_num_actions_feature: (bool) If True (default), the number of actions\n        is governed by the feature `num_actions`. If False, action masking is\n        used to lower the number of actions for a given sample.\n    """"""\n    self._global_context_sampling_fn = global_context_sampling_fn\n    self._arm_context_sampling_fn = arm_context_sampling_fn\n    self._max_num_actions = max_num_actions\n    self._reward_fn = reward_fn\n    self._batch_size = batch_size\n    self._num_actions_fn = num_actions_fn\n    self._add_num_actions_feature = add_num_actions_feature\n\n    observation_spec = {\n        GLOBAL_KEY:\n            array_spec.ArraySpec.from_array(global_context_sampling_fn()),\n        PER_ARM_KEY:\n            array_spec.add_outer_dims_nest(\n                array_spec.ArraySpec.from_array(arm_context_sampling_fn()),\n                (max_num_actions,))\n    }\n    if self._num_actions_fn is not None:\n      if self._add_num_actions_feature:\n        num_actions_spec = array_spec.BoundedArraySpec(\n            shape=(),\n            dtype=np.dtype(type(self._num_actions_fn())),\n            minimum=1,\n            maximum=max_num_actions)\n        observation_spec.update({NUM_ACTIONS_KEY: num_actions_spec})\n      else:\n        mask_spec = array_spec.BoundedArraySpec(\n            shape=(self._max_num_actions,),\n            dtype=np.int32,\n            minimum=0,\n            maximum=1)\n        observation_spec = (observation_spec, mask_spec)\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=max_num_actions - 1,\n        name=\'action\')\n\n    super(StationaryStochasticPerArmPyEnvironment,\n          self).__init__(observation_spec, action_spec)\n\n  def batched(self):\n    return True\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  def _observe(self):\n    global_obs = np.stack(\n        [self._global_context_sampling_fn() for _ in range(self._batch_size)])\n    arm_obs = np.reshape([\n        self._arm_context_sampling_fn()\n        for _ in range(self._batch_size * self._max_num_actions)\n    ], (self._batch_size, self._max_num_actions, -1))\n    self._observation = {GLOBAL_KEY: global_obs, PER_ARM_KEY: arm_obs}\n\n    if self._num_actions_fn:\n      num_actions = [self._num_actions_fn() for _ in range(self._batch_size)]\n      num_actions = np.maximum(num_actions, 1)\n      num_actions = np.minimum(num_actions, self._max_num_actions)\n      if self._add_num_actions_feature:\n        self._observation.update({NUM_ACTIONS_KEY: num_actions})\n      else:\n        mask = np.array(\n            np.less(range(self._max_num_actions), [[i] for i in num_actions]),\n            dtype=np.int32)\n        self._observation = (self._observation, mask)\n    return self._observation\n\n  def _apply_action(self, action):\n    if action.shape[0] != self.batch_size:\n      raise ValueError(\'Number of actions must match batch size.\')\n    observation = self._observation\n    if not self._add_num_actions_feature:\n      observation = self._observation[0]\n    global_obs = observation[GLOBAL_KEY]\n    batch_size_range = range(self.batch_size)\n    arm_obs = observation[PER_ARM_KEY][batch_size_range, action, :]\n    reward = np.stack([\n        self._reward_fn(np.concatenate((global_obs[b, :], arm_obs[b, :])))\n        for b in batch_size_range\n    ])\n    return reward\n'"
tf_agents/bandits/environments/stationary_stochastic_per_arm_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Stationary Stochastic Per-Arm Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as sspe\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\n\n\ndef normal_with_sigma_1_sampler(mu):\n  return np.random.normal(mu, 1)\n\n\ndef check_unbatched_time_step_spec(time_step, time_step_spec, batch_size):\n  """"""Checks if time step conforms array spec, even if batched.""""""\n  if batch_size is None:\n    return array_spec.check_arrays_nest(time_step, time_step_spec)\n\n  return array_spec.check_arrays_nest(\n      time_step, array_spec.add_outer_dims_nest(time_step_spec, (batch_size,)))\n\n\nclass LinearNormalReward(object):\n\n  def __init__(self, theta):\n    self.theta = theta\n\n  def __call__(self, x):\n    mu = np.dot(x, self.theta)\n    return np.random.normal(mu, 1)\n\n\nclass StationaryStochasticPerArmBanditPyEnvironmentTest(tf.test.TestCase):\n\n  def test_with_uniform_context_and_normal_mu_reward(self):\n\n    def _global_context_sampling_fn():\n      return np.random.randint(-10, 10, [4])\n\n    def _arm_context_sampling_fn():\n      return np.random.randint(-2, 3, [5])\n\n    reward_fn = LinearNormalReward([0, 1, 2, 3, 4, 5, 6, 7, 8])\n\n    env = sspe.StationaryStochasticPerArmPyEnvironment(\n        _global_context_sampling_fn,\n        _arm_context_sampling_fn,\n        6,\n        reward_fn,\n        batch_size=2)\n    time_step_spec = env.time_step_spec()\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(), minimum=0, maximum=5, dtype=np.int32)\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      self.assertAllEqual(action.shape, [2])\n      self.assertAllGreaterEqual(action, 0)\n      self.assertAllLess(action, 6)\n      time_step = env.step(action)\n\n  def test_with_variable_num_actions_masking(self):\n\n    def _global_context_sampling_fn():\n      return np.random.randint(-10, 10, [4])\n\n    def _arm_context_sampling_fn():\n      return np.random.randint(-2, 3, [5])\n\n    def _num_actions_fn():\n      return np.random.randint(0, 7)\n\n    reward_fn = LinearNormalReward([0, 1, 2, 3, 4, 5, 6, 7, 8])\n\n    env = sspe.StationaryStochasticPerArmPyEnvironment(\n        _global_context_sampling_fn,\n        _arm_context_sampling_fn,\n        6,\n        reward_fn,\n        _num_actions_fn,\n        batch_size=2,\n        add_num_actions_feature=False)\n    time_step_spec = env.time_step_spec()\n    self.assertAllEqual(time_step_spec.observation[1].shape, [6])\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(), minimum=0, maximum=5, dtype=np.int32)\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      self.assertAllEqual(action.shape, [2])\n      self.assertAllGreaterEqual(action, 0)\n      self.assertAllLess(action, 6)\n      time_step = env.step(action)\n\n  def test_with_variable_num_actions_featurized(self):\n\n    def _global_context_sampling_fn():\n      return np.random.randint(-10, 10, [4])\n\n    def _arm_context_sampling_fn():\n      return np.random.randint(-2, 3, [5])\n\n    def _num_actions_fn():\n      return np.random.randint(0, 7)\n\n    reward_fn = LinearNormalReward([0, 1, 2, 3, 4, 5, 6, 7, 8])\n\n    env = sspe.StationaryStochasticPerArmPyEnvironment(\n        _global_context_sampling_fn,\n        _arm_context_sampling_fn,\n        6,\n        reward_fn,\n        _num_actions_fn,\n        batch_size=2)\n    time_step_spec = env.time_step_spec()\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(), minimum=0, maximum=5, dtype=np.int32)\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      self.assertAllEqual(action.shape, [2])\n      self.assertAllGreaterEqual(action, 0)\n      self.assertAllLess(action, 6)\n      time_step = env.step(action)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/stationary_stochastic_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class implementation of Stationary Stochastic Python Bandit environment.""""""\nimport gin\nimport numpy as np\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.specs import utils as bandits_spec_utils\nfrom tf_agents.specs import array_spec\n\n\n@gin.configurable\nclass StationaryStochasticPyEnvironment(\n    bandit_py_environment.BanditPyEnvironment):\n  """"""Implements Stationary Stochastic Bandit environments.""""""\n\n  def __init__(self, context_sampling_fn, reward_fns, constraint_fns=None,\n               batch_size=1):\n    """"""Initializes a Stationary Stochastic Bandit environment.\n\n    In each round, context is generated by context_sampling_fn, this context is\n    passed through a reward_function for each arm.\n\n    Example:\n      def context_sampling_fn():\n        return np.random.randint(0, 10, [1, 2])  # 2-dim ints between 0 and 10\n\n      def reward_fn1(x):\n        return x[0]\n      def reward_fn2(x):\n        return x[1]\n      reward_fns = [reward_fn1, reward_fn2]  # Two arms\n\n      env = StationaryStochasticPyEnvironment(context_sampling_fn,\n                                              reward_fns)\n\n    Args:\n      context_sampling_fn: A function that outputs a random 2d array or list of\n        ints or floats, where the first dimension is batch size.\n      reward_fns: A function that generates a (perhaps non-scalar) reward when\n        called with an observation.\n      constraint_fns: A function that generates a (perhaps non-scalar)\n        constraint metric when called with an observation.\n      batch_size: The batch size. Must match the outer dimension of the output\n        of context_sampling_fn.\n    """"""\n    self._context_sampling_fn = context_sampling_fn\n    self._reward_fns = reward_fns\n    self._num_actions = len(reward_fns)\n    self._constraint_fns = constraint_fns\n    self._batch_size = batch_size\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n\n    example_observation = self._context_sampling_fn()\n    observation_spec = array_spec.ArraySpec.from_array(example_observation[0])\n    if example_observation.shape[0] != batch_size:\n      raise ValueError(\n          \'The outer dimension of the observations should match the batch size.\'\n      )\n\n    # Figure out the reward spec.\n    # If we have constraints, the reward_spec will be a nested dict with keys:\n    # \'reward\' and \'constraint\' (defined in tf_agents.bandits.specs.utils).\n    example_reward = np.asarray(reward_fns[0](example_observation[0]))\n    reward_spec = array_spec.ArraySpec(\n        example_reward.shape, np.float32, name=\'reward\')\n    if self._constraint_fns is not None:\n      example_constraint = np.asarray(constraint_fns[0](example_observation[0]))\n      constraint_spec = array_spec.ArraySpec(\n          example_constraint.shape, np.float32, name=\'constraint\')\n      reward_spec = {\n          bandits_spec_utils.REWARD_SPEC_KEY: reward_spec,\n          bandits_spec_utils.CONSTRAINTS_SPEC_KEY: constraint_spec\n      }\n\n    super(StationaryStochasticPyEnvironment, self).__init__(\n        observation_spec, action_spec, reward_spec)\n\n  def batched(self):\n    return True\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  def _observe(self):\n    self._observation = self._context_sampling_fn()\n    return self._observation\n\n  def _apply_action(self, action):\n    if len(action) != self.batch_size:\n      raise ValueError(\'Number of actions must match batch size.\')\n    reward = np.stack(\n        [self._reward_fns[a](o) for a, o in zip(action, self._observation)])\n    if self._constraint_fns is not None:\n      constraint = np.stack(\n          [self._constraint_fns[a](o) for a, o in zip(action,\n                                                      self._observation)])\n      reward = {\n          bandits_spec_utils.REWARD_SPEC_KEY: reward,\n          bandits_spec_utils.CONSTRAINTS_SPEC_KEY: constraint\n      }\n    return reward\n'"
tf_agents/bandits/environments/stationary_stochastic_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Stationary Stochastic Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\n\n\ndef normal_with_sigma_1_sampler(mu):\n  return np.random.normal(mu, 1)\n\n\ndef check_unbatched_time_step_spec(time_step, time_step_spec, batch_size):\n  """"""Checks if time step conforms array spec, even if batched.""""""\n  if batch_size is None:\n    return array_spec.check_arrays_nest(time_step, time_step_spec)\n\n  if not all([spec.shape[0] == batch_size for spec in time_step]):\n    return False\n\n  unbatched_time_step = ts.TimeStep(\n      step_type=time_step.step_type[0],\n      reward=time_step.reward[0],\n      discount=time_step.discount[0],\n      observation=time_step.observation[0])\n  return array_spec.check_arrays_nest(unbatched_time_step, time_step_spec)\n\n\nclass LinearNormalReward(object):\n\n  def __init__(self, theta):\n    self.theta = theta\n\n  def __call__(self, x):\n    mu = np.dot(x, self.theta)\n    return np.random.normal(mu, 1)\n\n\nclass LinearDeterministicReward(object):\n\n  def __init__(self, theta):\n    self.theta = theta\n\n  def __call__(self, x):\n    return np.dot(x, self.theta)\n\n\nclass LinearDeterministicMultipleRewards(object):\n\n  def __init__(self, thetas):\n    self.thetas = thetas\n\n  def __call__(self, x):\n    return [np.dot(x, theta) for theta in self.thetas]\n\n\nclass StationaryStochasticBanditPyEnvironmentTest(tf.test.TestCase):\n\n  def test_with_uniform_context_and_normal_mu_reward(self):\n\n    def _context_sampling_fn():\n      return np.random.randint(-10, 10, [1, 4])\n\n    reward_fns = [\n        LinearNormalReward(theta)\n        for theta in ([0, 1, 2, 3], [3, 2, 1, 0], [-1, -2, -3, -4])\n    ]\n\n    env = sspe.StationaryStochasticPyEnvironment(_context_sampling_fn,\n                                                 reward_fns)\n    time_step_spec = env.time_step_spec()\n    action_spec = env.action_spec()\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      time_step = env.step(action)\n\n  def test_with_normal_context_and_normal_reward(self):\n\n    def _context_sampling_fn():\n      return np.random.normal(0, 3, [1, 2])\n\n    def _reward_fn(x):\n      return np.random.normal(2 * x[0], abs(x[1]) + 1)\n\n    env = sspe.StationaryStochasticPyEnvironment(_context_sampling_fn,\n                                                 [_reward_fn])\n    time_step_spec = env.time_step_spec()\n    action_spec = env.action_spec()\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      time_step = env.step(action)\n\n  def test_deterministic_with_batch_2(self):\n\n    def _context_sampling_fn():\n      return np.array([[4, 3], [4, 3]])\n\n    reward_fns = [\n        LinearDeterministicReward(theta)\n        for theta in ([0, 1], [1, 2], [2, 3], [3, 4])\n    ]\n    env = sspe.StationaryStochasticPyEnvironment(\n        _context_sampling_fn, reward_fns, batch_size=2)\n    time_step = env.reset()\n    self.assertAllEqual(time_step.observation, [[4, 3], [4, 3]])\n    time_step = env.step([0, 1])\n    self.assertAllEqual(time_step.reward, [3, 10])\n    env.reset()\n    time_step = env.step([2, 3])\n    self.assertAllEqual(time_step.reward, [17, 24])\n\n  def test_non_scalar_rewards(self):\n\n    def _context_sampling_fn():\n      return np.array([[4, 3], [4, 3], [5, 6]])\n\n    # Build a case with 4 arms and 2-dimensional rewards and batch size 3.\n    reward_fns = [\n        LinearDeterministicMultipleRewards(theta)  # pylint: disable=g-complex-comprehension\n        for theta in [np.array([[0, 1], [1, 0]]),\n                      np.array([[1, 2], [2, 1]]),\n                      np.array([[2, 3], [3, 2]]),\n                      np.array([[3, 4], [4, 3]])]\n    ]\n    env = sspe.StationaryStochasticPyEnvironment(\n        _context_sampling_fn, reward_fns, batch_size=3)\n    time_step = env.reset()\n    self.assertAllEqual(time_step.observation, [[4, 3], [4, 3], [5, 6]])\n    time_step = env.step([0, 1, 2])\n    self.assertAllEqual(time_step.reward,\n                        [[3., 4.],\n                         [10., 11.],\n                         [28., 27.]])\n    env.reset()\n    time_step = env.step([2, 3, 0])\n    self.assertAllEqual(time_step.reward,\n                        [[17., 18.],\n                         [24., 25.],\n                         [6., 5.]])\n    # Check that the reward vectors in the reward spec are 2-dimensional.\n    time_step_spec = env.time_step_spec()\n    self.assertEqual(time_step_spec.reward.shape[0], 2)\n\n  def test_non_scalar_rewards_and_constraints(self):\n\n    def _context_sampling_fn():\n      return np.array([[4, 3], [4, 3], [5, 6]])\n\n    # Build a case with 4 arms and 2-dimensional rewards and batch size 3.\n    reward_fns = [\n        LinearDeterministicMultipleRewards(theta)  # pylint: disable=g-complex-comprehension\n        for theta in [np.array([[0, 1], [1, 0]]),\n                      np.array([[1, 2], [2, 1]]),\n                      np.array([[2, 3], [3, 2]]),\n                      np.array([[3, 4], [4, 3]])]\n    ]\n    constraint_fns = reward_fns\n    env = sspe.StationaryStochasticPyEnvironment(\n        _context_sampling_fn, reward_fns, constraint_fns, batch_size=3)\n    time_step = env.reset()\n    self.assertAllEqual(time_step.observation, [[4, 3], [4, 3], [5, 6]])\n    time_step = env.step([0, 1, 2])\n\n    self.assertAllEqual(time_step.reward[\'reward\'],\n                        [[3., 4.],\n                         [10., 11.],\n                         [28., 27.]])\n    self.assertAllEqual(time_step.reward[\'constraint\'],\n                        [[3., 4.],\n                         [10., 11.],\n                         [28., 27.]])\n\n    env.reset()\n    time_step = env.step([2, 3, 0])\n    self.assertAllEqual(time_step.reward[\'reward\'],\n                        [[17., 18.],\n                         [24., 25.],\n                         [6., 5.]])\n    self.assertAllEqual(time_step.reward[\'constraint\'],\n                        [[17., 18.],\n                         [24., 25.],\n                         [6., 5.]])\n    # Check that the reward vectors in the reward spec and in the\n    # constraint_spec are 2-dimensional.\n    time_step_spec = env.time_step_spec()\n    self.assertEqual(time_step_spec.reward[\'reward\'].shape[0], 2)\n    self.assertEqual(time_step_spec.reward[\'constraint\'].shape[0], 2)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/stationary_stochastic_structured_py_environment.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Stationary Stochastic Python Bandit environment with structured features.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport numpy as np\nimport tensorflow as tf\n\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.utils import nest_utils\n\nGLOBAL_KEY = bandit_spec_utils.GLOBAL_FEATURE_KEY\nPER_ARM_KEY = bandit_spec_utils.PER_ARM_FEATURE_KEY\n\n\n@gin.configurable\nclass StationaryStochasticStructuredPyEnvironment(\n    bandit_py_environment.BanditPyEnvironment):\n  """"""Stationary Stochastic Bandit environment with structured features.\n\n  This environment can generate global and per-arm observations of any nested\n  structure.\n  """"""\n\n  def __init__(self,\n               global_context_sampling_fn,\n               arm_context_sampling_fn,\n               num_actions,\n               reward_fn,\n               batch_size=1):\n    """"""Initializes the environment.\n\n    In each round, global context is generated by global_context_sampling_fn,\n    per-arm contexts are generated by arm_context_sampling_fn.\n\n    The two feature generating functions should output a single observation, not\n    including either the batch_size or the number of actions.\n\n    The reward_fn function takes a global and a per-arm feature, and outputs a\n    possibly random reward.\n\n    Example:\n      def global_context_sampling_fn():\n        return np.random.randint(0, 10, [2])  # 2-dimensional global features.\n\n      def arm_context_sampling_fn():\n        return {\'armf1\': np.random.randint(-3, 4, [3]),    # A dictionary of\n                \'armf2\': np.random.randint(0, 2, [4, 5])}  # arm features.\n\n      def reward_fn(global, arm):\n        return sum(global) + arm[\'armf1\'][0] + arm[\'armf2\'][3, 3]\n\n      env = StationaryStochasticPyEnvironment(global_context_sampling_fn,\n                                              arm_context_sampling_fn,\n                                              5,\n                                              reward_fn,\n                                              batch_size=5)\n\n    Args:\n      global_context_sampling_fn: A function that outputs a possibly nested\n        structure of features. This output is the global context. Its shapes and\n        types must be consistent accross calls.\n      arm_context_sampling_fn: A function that outputs a possibly nested\n        structure of features. This output is the per-arm context. Its shapes\n        must be consistent accross calls.\n      num_actions: (int) the number of actions in every sample.\n      reward_fn: A function that generates a reward when called with a global\n        and a per-arm observation.\n      batch_size: The batch size.\n    """"""\n    self._global_context_sampling_fn = global_context_sampling_fn\n    self._arm_context_sampling_fn = arm_context_sampling_fn\n    self._num_actions = num_actions\n    self._reward_fn = reward_fn\n    self._batch_size = batch_size\n\n    global_example = global_context_sampling_fn()\n    arm_example = arm_context_sampling_fn()\n    observation_spec = {\n        GLOBAL_KEY:\n            tf.nest.map_structure(array_spec.ArraySpec.from_array,\n                                  global_example),\n        PER_ARM_KEY:\n            array_spec.add_outer_dims_nest(\n                tf.nest.map_structure(array_spec.ArraySpec.from_array,\n                                      arm_example), (num_actions,))\n    }\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=num_actions - 1,\n        name=\'action\')\n\n    super(StationaryStochasticStructuredPyEnvironment,\n          self).__init__(observation_spec, action_spec)\n\n  def batched(self):\n    return True\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  def _generate_batch_of_observations(self, generator_fn, num_samples):\n    unstacked_obs = [generator_fn() for _ in range(num_samples)]\n    return nest_utils.stack_nested_arrays(unstacked_obs)\n\n  def _observe(self):\n    global_obs = self._generate_batch_of_observations(\n        self._global_context_sampling_fn, self._batch_size)\n    arm_obs = self._generate_batch_of_observations(\n        self._arm_context_sampling_fn, self._batch_size * self._num_actions)\n    arm_obs = tf.nest.map_structure(\n        lambda x: x.reshape((self.batch_size, self._num_actions) + x.shape[1:]),\n        arm_obs)\n    self._observation = {GLOBAL_KEY: global_obs, PER_ARM_KEY: arm_obs}\n    return self._observation\n\n  def _apply_action(self, action):\n    if len(action) != self.batch_size:\n      raise ValueError(\'Number of actions must match batch size.\')\n    global_obs = self._observation[GLOBAL_KEY]\n    batch_size_range = list(range(self.batch_size))\n    arm_obs = tf.nest.map_structure(lambda x: x[batch_size_range, action, :],\n                                    self._observation[PER_ARM_KEY])\n    def _get_element_from_batch(structure, index):\n      return tf.nest.map_structure(lambda x: x[index], structure)\n\n    reward = np.stack([\n        self._reward_fn(\n            _get_element_from_batch(global_obs, b),\n            _get_element_from_batch(arm_obs, b)) for b in batch_size_range\n    ])\n    return reward\n'"
tf_agents/bandits/environments/stationary_stochastic_structured_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Stationary Stochastic Structured Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.environments import stationary_stochastic_structured_py_environment as ssspe\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.specs import array_spec\n\n\ndef check_unbatched_time_step_spec(time_step, time_step_spec, batch_size):\n  """"""Checks if time step conforms array spec, even if batched.""""""\n  if batch_size is None:\n    return array_spec.check_arrays_nest(time_step, time_step_spec)\n\n  return array_spec.check_arrays_nest(\n      time_step, array_spec.add_outer_dims_nest(time_step_spec, (batch_size,)))\n\n\nclass StationaryStochasticStructuredBanditPyEnvironmentTest(tf.test.TestCase):\n\n  def test_with_random_policy(self):\n\n    def _global_context_sampling_fn():\n      abc = np.array([\'a\', \'b\', \'c\'])\n      return {\'global1\': np.random.randint(-2, 3, [3, 4]),\n              \'global2\': abc[np.random.randint(0, 2, [1])]}\n\n    def _arm_context_sampling_fn():\n      aabbcc = np.array([\'aa\', \'bb\', \'cc\'])\n      return {\'arm1\': np.random.randint(-3, 4, [5]),\n              \'arm2\': np.random.randint(-3, 4, [3, 1]),\n              \'arm3\': aabbcc[np.random.randint(0, 2, [1])]}\n\n    def _reward_fn(global_obs, arm_obs):\n      return global_obs[\'global1\'][2, 1] + arm_obs[\'arm1\'][4]\n\n    env = ssspe.StationaryStochasticStructuredPyEnvironment(\n        _global_context_sampling_fn,\n        _arm_context_sampling_fn,\n        6,\n        _reward_fn,\n        batch_size=2)\n    time_step_spec = env.time_step_spec()\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(), minimum=0, maximum=5, dtype=np.int32)\n\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec=time_step_spec, action_spec=action_spec)\n\n    for _ in range(5):\n      time_step = env.reset()\n      self.assertTrue(\n          check_unbatched_time_step_spec(\n              time_step=time_step,\n              time_step_spec=time_step_spec,\n              batch_size=env.batch_size))\n\n      action = random_policy.action(time_step).action\n      self.assertAllEqual(action.shape, [2])\n      self.assertAllGreaterEqual(action, 0)\n      self.assertAllLess(action, 6)\n      time_step = env.step(action)\n      self.assertEqual(time_step.reward.shape, (2,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/environments/wheel_py_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Class implementation of Python Wheel Bandit environment.""""""\nfrom __future__ import absolute_import\n\nimport gin\nimport numpy as np\nfrom tf_agents.bandits.environments import bandit_py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\n\n_NUM_ACTIONS = 5\n_CONTEXT_DIM = 2\n_SIGNS_TO_OPT_ACTION = {\n    (1.0, 1.0): 1,\n    (1.0, -1.0): 2,\n    (-1.0, 1.0): 3,\n    (-1.0, -1.0): 4,\n}\n\n\n@gin.configurable\ndef compute_optimal_action(observation, delta):\n  batch_size = observation.shape[0]\n  optimal_actions = np.zeros(batch_size, dtype=np.int32)\n  is_outer = np.int32(np.linalg.norm(observation, ord=2, axis=1) > delta)\n  signs = np.sign(observation)\n  optimal_actions += is_outer * [_SIGNS_TO_OPT_ACTION[tuple(x)] for x in signs]\n  return optimal_actions\n\n\n@gin.configurable\ndef compute_optimal_reward(observation, delta, mu_inside, mu_high):\n  is_inside = np.float32(np.linalg.norm(observation, ord=2, axis=1) <= delta)\n  return is_inside * mu_inside + (1 - is_inside) * mu_high\n\n\n@gin.configurable\nclass WheelPyEnvironment(bandit_py_environment.BanditPyEnvironment):\n  """"""Implements the Wheel Bandit environment.\n\n  This environment implements the wheel bandit from Section 5.4 of [1] (please\n  see references below) as a subclass of BanditPyEnvironment.\n\n  Context features are sampled uniformly at random in the unit circle in R^2.\n  There are 5 possible actions. There exists an exploration parameter `delta`\n  in (0, 1) that determines the difficulty of the problem and the need for\n  exploration.\n\n  References:\n  [1]. Carlos Riquelme, George Tucker, Jasper Snoek\n  ""Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep\n  Networks for Thompson Sampling"", International Conference on Learning\n  Representations (ICLR) 2018.\n  https://arxiv.org/abs/1802.09127\n\n  """"""\n\n  def __init__(self, delta, mu_base, std_base, mu_high, std_high,\n               batch_size=None):\n    """"""Initializes the Wheel Bandit environment.\n\n    Args:\n      delta: float in `(0, 1)`. Exploration parameter.\n      mu_base: (vector of float) Mean reward for each action, if the context\n          norm is below delta. The size of the vector is expected to be 5 (i.e.,\n          equal to the number of actions.)\n      std_base: (vector of float) std of the Gaussian reward for each action if\n          the context norm is below delta. The size of the vector is expected to\n          be 5 (i.e., equal to the number of actions.)\n      mu_high: (float) Mean reward for the optimal action if the context norm\n          is above delta.\n      std_high: (float) Reward std for optimal action if the context norm is\n          above delta.\n      batch_size: (optional) (int) Number of observations generated per call.\n    """"""\n    self._batch_size = 1 if batch_size is None else batch_size\n    if (delta <= 0 or delta >= 1):\n      raise ValueError(\'Delta must be in (0, 1), but saw delta: %g\' % (delta,))\n    self._delta = delta\n    self._mu_base = np.asarray(mu_base, dtype=np.float32)\n    if self._mu_base.shape != (5,):\n      raise ValueError(\'The length of \\\'mu_base\\\' must be 5, but saw \'\n                       \'\\\'mu_base\\\': %s\' % (self._mu_base,))\n    self._std_base = np.asarray(std_base, dtype=np.float32)\n    if self._std_base.shape != (5,):\n      raise ValueError(\'The length of \\\'std_base\\\' must be 5.\')\n\n    self._mu_high = mu_high\n    self._std_high = std_high\n\n    # The first action should have higher mean reward that the other actions.\n    if self._mu_base[0] != max(self._mu_base):\n      raise ValueError(\'The first action in mu_base should have the highest \'\n                       \'reward; got {}\'.format(self._mu_base))\n\n    action_spec = array_spec.BoundedArraySpec(\n        shape=(),\n        dtype=np.int32,\n        minimum=0,\n        maximum=_NUM_ACTIONS - 1,\n        name=\'action\')\n    observation_spec = array_spec.ArraySpec(\n        shape=(_CONTEXT_DIM,), dtype=np.float32, name=\'observation\')\n    self._time_step_spec = ts.time_step_spec(observation_spec)\n    self._observation = np.zeros((self._batch_size, _CONTEXT_DIM))\n    super(WheelPyEnvironment, self).__init__(observation_spec, action_spec)\n\n  @property\n  def batch_size(self):\n    return self._batch_size\n\n  @property\n  def batched(self):\n    return True\n\n  def _reward_fn(self, observation, action):\n    # Sample rewards for all actions.\n    r_all = np.random.normal(\n        self._mu_base, self._std_base, size=(self._batch_size, _NUM_ACTIONS))\n\n    # Compute the reward inside.\n    row_norms = np.linalg.norm(observation, ord=2, axis=1)\n    is_norm_below_delta = np.float32(row_norms <= self._delta)\n    reward_inside = (\n        is_norm_below_delta * r_all[np.arange(self._batch_size), action])\n\n    # Compute the reward outside.\n    high_reward = np.random.normal(\n        self._mu_high, self._std_high, size=(self._batch_size))\n    signs = np.sign(observation)\n    optimal_actions = [_SIGNS_TO_OPT_ACTION[tuple(x)] for x in signs]\n    r_outside = r_all\n    r_outside[np.arange(self._batch_size), optimal_actions] = high_reward\n\n    reward_outside = ((1.0 - is_norm_below_delta) *\n                      r_outside[np.arange(self._batch_size), action])\n\n    reward_final = reward_inside + reward_outside\n    return reward_final\n\n  def _observe(self):\n    """"""Returns 2-dim samples falling in the unit circle.""""""\n    theta = np.random.uniform(0.0, 2.0 * np.pi, (self._batch_size))\n    r = np.sqrt(np.random.uniform(size=self._batch_size))\n    batched_observations = np.stack(\n        [r * np.cos(theta), r * np.sin(theta)], axis=1)\n    self._observation = batched_observations.astype(\n        self._observation_spec.dtype)\n    return self._observation\n\n  def _apply_action(self, action):\n    """"""Computes the reward for the input actions.""""""\n    return self._reward_fn(self._observation, action)\n'"
tf_agents/bandits/environments/wheel_py_environment_test.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for the Wheel Bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.environments import wheel_py_environment\n\n\nclass WheelBanditPyEnvironmentTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_delta_0\',\n           delta=0.0),\n      dict(testcase_name=\'_delta_2\',\n           delta=2.0)\n  )\n  def test_delta_out_of_bound_parameter(self, delta):\n    with self.assertRaisesRegexp(\n        ValueError, r\'Delta must be in \\(0, 1\\)\\, but saw delta: %g\' % delta):\n      wheel_py_environment.WheelPyEnvironment(\n          delta=delta, mu_base=[1.2, 1.0, 1.0, 1.0, 1.0],\n          std_base=0.01 * np.ones(5), mu_high=50.0, std_high=0.01)\n\n  def test_mu_base_out_of_bound_parameter(self):\n    mu_base = [1.2, 1.0, 1.0, 1.0, 1.0, 1.0]\n    with self.assertRaisesRegexp(\n        ValueError, \'The length of \\\'mu_base\\\' must be 5, but saw \'\n        \'\\\'mu_base\\\':.*\'):\n      wheel_py_environment.WheelPyEnvironment(\n          delta=0.5, mu_base=mu_base,\n          std_base=0.01 * np.ones(5), mu_high=50.0, std_high=0.01)\n\n  def test_std_base_out_of_bound_parameter(self):\n    with self.assertRaisesRegexp(\n        ValueError, r\'The length of \\\'std_base\\\' must be 5\\.\'):\n      wheel_py_environment.WheelPyEnvironment(\n          delta=0.5,\n          mu_base=[1.2, 1.0, 1.0, 1.0, 1.0],\n          std_base=0.01 * np.ones(6),\n          mu_high=50.0,\n          std_high=0.01)\n\n  def test_compute_optimal_action_and_reward(self):\n    observation = np.array([[0.1, 0.2], [0.3, -0.7], [-0.3, -0.7], [0.3, 0.7],\n                            [0.1, 0.3]])\n    actual_actions = wheel_py_environment.compute_optimal_action(\n        observation, 0.5)\n    expected_actions = [0, 2, 4, 1, 0]\n    self.assertAllEqual(actual_actions, expected_actions)\n    actual_rewards = wheel_py_environment.compute_optimal_reward(\n        observation, 0.5, 1.5, 3.0)\n    expected_rewards = [1.5, 3.0, 3.0, 3.0, 1.5]\n    self.assertAllEqual(actual_rewards, expected_rewards)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_batch_1\',\n           batch_size=1),\n      dict(testcase_name=\'_batch_4\',\n           batch_size=4)\n  )\n  def test_observation_validity(self, batch_size):\n    """"""Tests that the observations fall into the unit circle.""""""\n    env = wheel_py_environment.WheelPyEnvironment(\n        delta=0.5, mu_base=[1.2, 1.0, 1.0, 1.0, 1.0],\n        std_base=0.01 * np.ones(5), mu_high=50.0, std_high=0.01,\n        batch_size=batch_size)\n\n    for _ in range(5):\n      observation = env.reset().observation\n      self.assertEqual(list(observation.shape),\n                       [batch_size] + list(env.observation_spec().shape))\n      for i in range(batch_size):\n        self.assertLessEqual(np.linalg.norm(observation[i, :]), 1)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_batch_1\',\n           batch_size=1),\n      dict(testcase_name=\'_batch_4\',\n           batch_size=4),\n  )\n  def test_rewards_validity(self, batch_size):\n    """"""Tests that the rewards are valid.""""""\n    env = wheel_py_environment.WheelPyEnvironment(\n        delta=0.5, mu_base=[1.2, 1.0, 1.0, 1.0, 1.0],\n        std_base=0.01 * np.ones(5), mu_high=50.0, std_high=0.01,\n        batch_size=batch_size)\n    time_step = env.reset()\n    time_step = env.step(np.arange(batch_size))\n    self.assertEqual(time_step.reward.shape, (batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/metrics/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all metrics.""""""\n\nfrom tf_agents.bandits.metrics import tf_metrics\n'"
tf_agents/bandits/metrics/tf_metrics.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""TF metrics for Bandits algorithms.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.metrics import tf_metric\nfrom tf_agents.utils import common\n\n\n@gin.configurable\nclass RegretMetric(tf_metric.TFStepMetric):\n  """"""Computes the regret with respect to a baseline.""""""\n\n  def __init__(self, baseline_reward_fn, name=\'RegretMetric\', dtype=tf.float32):\n    """"""Computes the regret with respect to a baseline.\n\n    The regret is computed by computing the difference of the current reward\n    from the baseline action reward. The latter is computed by calling the input\n    `baseline_reward_fn` function that given a (batched) observation computes\n    the baseline action reward.\n\n    Args:\n      baseline_reward_fn: function that computes the reward used as a baseline\n        for computing the regret.\n      name: (str) name of the metric\n      dtype: dtype of the metric value.\n    """"""\n    self._baseline_reward_fn = baseline_reward_fn\n    self.dtype = dtype\n    self.regret = common.create_variable(\n        initial_value=0, dtype=self.dtype, shape=(), name=\'regret\')\n    super(RegretMetric, self).__init__(name=name)\n\n  def call(self, trajectory):\n    """"""Update the regret value.\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    baseline_reward = self._baseline_reward_fn(trajectory.observation)\n    trajectory_reward = trajectory.reward\n    if isinstance(trajectory.reward, dict):\n      trajectory_reward = trajectory.reward[bandit_spec_utils.REWARD_SPEC_KEY]\n    trajectory_regret = baseline_reward - trajectory_reward\n    self.regret.assign(tf.reduce_mean(trajectory_regret))\n    return trajectory\n\n  def result(self):\n    return tf.identity(\n        self.regret, name=self.name)\n\n\n@gin.configurable\nclass SuboptimalArmsMetric(tf_metric.TFStepMetric):\n  """"""Computes the number of suboptimal arms with respect to a baseline.""""""\n\n  def __init__(self, baseline_action_fn, name=\'SuboptimalArmsMetric\',\n               dtype=tf.float32):\n    """"""Computes the number of suboptimal arms with respect to a baseline.\n\n    Args:\n      baseline_action_fn: function that computes the action used as a baseline\n        for computing the metric.\n      name: (str) name of the metric\n      dtype: dtype of the metric value.\n    """"""\n    self._baseline_action_fn = baseline_action_fn\n    self.dtype = dtype\n    self.suboptimal_arms = common.create_variable(\n        initial_value=0, dtype=self.dtype, shape=(), name=\'suboptimal_arms\')\n    super(SuboptimalArmsMetric, self).__init__(name=name)\n\n  def call(self, trajectory):\n    """"""Update the metric value.\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    baseline_action = self._baseline_action_fn(trajectory.observation)\n    disagreement = tf.cast(\n        tf.not_equal(baseline_action, trajectory.action), tf.float32)\n    self.suboptimal_arms.assign(tf.reduce_mean(disagreement))\n    return trajectory\n\n  def result(self):\n    return tf.identity(\n        self.suboptimal_arms, name=self.name)\n\n\n@gin.configurable\nclass ConstraintViolationsMetric(tf_metric.TFStepMetric):\n  """"""Computes the violations of a certain constraint.""""""\n\n  def __init__(self,\n               constraint,\n               name=\'ConstraintViolationMetric\',\n               dtype=tf.float32):\n    """"""Computes the constraint violations given an input constraint.\n\n    Given a certain constraint, this metric computes how often the selected\n    actions in the trajectory violate the constraint.\n\n    Args:\n      constraint: an instance of `tf_agents.bandits.agents.BaseConstraint`.\n      name: (str) name of the metric\n      dtype: dtype of the metric value.\n    """"""\n    self._constraint = constraint\n    self.dtype = dtype\n    self.constraint_violations = common.create_variable(\n        initial_value=0.0,\n        dtype=self.dtype,\n        shape=(),\n        name=\'constraint_violations\')\n    super(ConstraintViolationsMetric, self).__init__(name=name)\n\n  def call(self, trajectory):\n    """"""Update the constraint violations metric.\n\n    Args:\n      trajectory: A tf_agents.trajectory.Trajectory\n\n    Returns:\n      The arguments, for easy chaining.\n    """"""\n    feasibility_prob_all_actions = self._constraint(trajectory.observation)\n    feasibility_prob_selected_actions = common.index_with_actions(\n        feasibility_prob_all_actions,\n        tf.cast(trajectory.action, dtype=tf.int32))\n    self.constraint_violations.assign(tf.reduce_mean(\n        1.0 - feasibility_prob_selected_actions))\n    return trajectory\n\n  def result(self):\n    return tf.identity(self.constraint_violations, name=self.name)\n'"
tf_agents/bandits/metrics/tf_metrics_test.py,35,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for tf_agents.bandits.metrics.tf_metrics.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import constraints\nfrom tf_agents.bandits.metrics import tf_metrics\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tensorflow.python.eager import context  # pylint: disable=g-direct-tensorflow-import  # TF internal\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\n\ndef compute_optimal_reward(unused_observation):\n  return tf.constant(10.0)\n\n\ndef compute_optimal_action(unused_observation):\n  return tf.constant(5, dtype=tf.int32)\n\n\nclass SimpleThresholdConstraint(constraints.BaseConstraint):\n\n  def __init__(self, time_step_spec, action_spec, batch_size, threshold,\n               name=None):\n    self.batch_size = batch_size\n    self.threshold = threshold\n    super(SimpleThresholdConstraint, self).__init__(\n        time_step_spec, action_spec, name=\'SimpleThresholdConstraint\')\n\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    if actions is None:\n      actions = tf.range(\n          self._action_spec.minimum, self._action_spec.maximum + 1)\n      actions = tf.reshape(actions, [1, -1])\n      actions = tf.tile(actions, [self.batch_size, 1])\n    feasibility_prob = tf.cast(tf.greater(actions, self.threshold), tf.float32)\n    return feasibility_prob\n\n\nclass TFMetricsTest(parameterized.TestCase, tf.test.TestCase):\n\n  def _create_trajectory(self):\n    return trajectory.Trajectory(observation=(),\n                                 action=(tf.constant(1)),\n                                 policy_info=(),\n                                 reward=tf.constant(1.0),\n                                 discount=tf.constant(1.0),\n                                 step_type=ts.StepType.FIRST,\n                                 next_step_type=ts.StepType.LAST)\n\n  def _create_batched_trajectory(self, batch_size):\n    return trajectory.Trajectory(observation=(),\n                                 action=tf.range(batch_size, dtype=tf.int32),\n                                 policy_info=(),\n                                 reward=tf.range(batch_size, dtype=tf.float32),\n                                 discount=tf.ones(batch_size),\n                                 step_type=ts.StepType.FIRST,\n                                 next_step_type=ts.StepType.LAST)\n\n  def _create_test_trajectory(self, batch_size):\n    num_actions = tf.cast(batch_size / 2, dtype=tf.int32)\n    action_tensor = tf.concat([\n        tf.range(num_actions, dtype=tf.int32),\n        tf.range(num_actions, dtype=tf.int32)], axis=-1)\n    return trajectory.Trajectory(observation=tf.ones(batch_size),\n                                 action=action_tensor,\n                                 policy_info=(),\n                                 reward=tf.range(batch_size, dtype=tf.float32),\n                                 discount=tf.ones(batch_size),\n                                 step_type=ts.StepType.FIRST,\n                                 next_step_type=ts.StepType.LAST)\n\n  def _create_batched_trajectory_with_reward_dict(self, batch_size):\n    reward_dict = {\n        \'reward\': tf.range(batch_size, dtype=tf.float32),\n        \'constraint\': tf.range(batch_size, dtype=tf.float32),\n    }\n    return trajectory.Trajectory(observation=(),\n                                 action=tf.range(batch_size, dtype=tf.int32),\n                                 policy_info=(),\n                                 reward=reward_dict,\n                                 discount=tf.ones(batch_size),\n                                 step_type=ts.StepType.FIRST,\n                                 next_step_type=ts.StepType.LAST)\n\n  @parameterized.named_parameters(\n      (\'RegretMetricName\', tf_metrics.RegretMetric, compute_optimal_reward,\n       \'RegretMetric\'),\n      (\'SuboptimalArmsMetricName\', tf_metrics.SuboptimalArmsMetric,\n       compute_optimal_action, \'SuboptimalArmsMetric\')\n  )\n  def testName(self, metric_class, fn, expected_name):\n    metric = metric_class(fn)\n    self.assertEqual(expected_name, metric.name)\n\n  @parameterized.named_parameters([\n      (\'TestRegretGraph\', context.graph_mode, tf_metrics.RegretMetric,\n       compute_optimal_reward, 9),\n      (\'TestRegretEager\', context.eager_mode, tf_metrics.RegretMetric,\n       compute_optimal_reward, 9),\n      (\'TestSuboptimalArmsGraph\', context.graph_mode,\n       tf_metrics.SuboptimalArmsMetric, compute_optimal_action, 1),\n      (\'TestSuboptimalArmsEager\', context.eager_mode,\n       tf_metrics.SuboptimalArmsMetric, compute_optimal_action, 1),\n  ])\n  def testRegretMetric(self, run_mode, metric_class, fn, expected_result):\n    with run_mode():\n      traj = self._create_trajectory()\n      metric = metric_class(fn)\n      self.evaluate(metric.init_variables())\n      traj_out = metric(traj)\n      deps = tf.nest.flatten(traj_out)\n      with tf.control_dependencies(deps):\n        result = metric.result()\n      result_ = self.evaluate(result)\n      self.assertEqual(result_, expected_result)\n\n  @parameterized.named_parameters([\n      (\'TestRegretGraphBatched\', context.graph_mode, tf_metrics.RegretMetric,\n       compute_optimal_reward, 8, 6.5),\n      (\'TestRegretEagerBatched\', context.eager_mode, tf_metrics.RegretMetric,\n       compute_optimal_reward, 8, 6.5),\n      (\'TestSuboptimalArmsGraphBatched\', context.graph_mode,\n       tf_metrics.SuboptimalArmsMetric, compute_optimal_action, 8, 7.0 / 8.0),\n      (\'TestSuboptimalArmsEagerBatched\', context.eager_mode,\n       tf_metrics.SuboptimalArmsMetric, compute_optimal_action, 8, 7.0 / 8.0),\n  ])\n  def testRegretMetricBatched(self, run_mode, metric_class, fn, batch_size,\n                              expected_result):\n    with run_mode():\n      traj = self._create_batched_trajectory(batch_size)\n      metric = metric_class(fn)\n      self.evaluate(metric.init_variables())\n      traj_out = metric(traj)\n      deps = tf.nest.flatten(traj_out)\n      with tf.control_dependencies(deps):\n        result = metric.result()\n      result_ = self.evaluate(result)\n      self.assertEqual(result_, expected_result)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testRegretMetricWithRewardDict(\n      self, metric_class=tf_metrics.RegretMetric, fn=compute_optimal_reward,\n      batch_size=8, expected_result=6.5):\n    traj = self._create_batched_trajectory_with_reward_dict(batch_size)\n    metric = metric_class(fn)\n    self.evaluate(metric.init_variables())\n    traj_out = metric(traj)\n    deps = tf.nest.flatten(traj_out)\n    with tf.control_dependencies(deps):\n      result = metric.result()\n    result_ = self.evaluate(result)\n    self.assertEqual(result_, expected_result)\n\n  @parameterized.named_parameters([\n      (\'TestConstraintViolationTh1\', 8, 1, 0.5),\n      (\'TestConstraintViolationTh2\', 8, 2, 0.75),\n  ])\n  def testConstraintViolationMetric(\n      self, batch_size, threshold, expected_result):\n    traj = self._create_test_trajectory(batch_size)\n    num_actions = batch_size / 2\n\n    obs_spec = tensor_spec.TensorSpec([], tf.float32)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        dtype=tf.int32, shape=(), minimum=0, maximum=num_actions-1)\n    stc = SimpleThresholdConstraint(\n        time_step_spec, action_spec, batch_size=batch_size,\n        threshold=threshold)\n    metric = tf_metrics.ConstraintViolationsMetric(constraint=stc)\n    self.evaluate(metric.init_variables())\n    traj_out = metric(traj)\n    deps = tf.nest.flatten(traj_out)\n    with tf.control_dependencies(deps):\n      result = metric.result()\n    result_ = self.evaluate(result)\n    self.assertEqual(result_, expected_result)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/networks/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all networks.""""""\n\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\n'"
tf_agents/bandits/networks/global_and_arm_feature_network.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Networks that take as input global and per-arm features, and output rewards.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\nfrom tf_agents.specs import tensor_spec\n\n\ndef _remove_num_actions_dim_from_spec(observation_spec):\n  """"""Removes the extra `num_actions` dimension from the observation spec.""""""\n  obs_spec_no_num_actions = {\n      bandit_spec_utils.GLOBAL_FEATURE_KEY:\n          observation_spec[bandit_spec_utils.GLOBAL_FEATURE_KEY],\n      bandit_spec_utils.PER_ARM_FEATURE_KEY:\n          tensor_spec.remove_outer_dims_nest(\n              observation_spec[bandit_spec_utils.PER_ARM_FEATURE_KEY], 1)\n  }\n  if bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation_spec:\n    obs_spec_no_num_actions.update({\n        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n            observation_spec[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY]\n    })\n  return obs_spec_no_num_actions\n\n\ndef create_feed_forward_common_tower_network(observation_spec, global_layers,\n                                             arm_layers, common_layers,\n                                             output_dim=1,\n                                             global_preprocessing_combiner=None,\n                                             arm_preprocessing_combiner=None):\n  """"""Creates a common tower network with feedforward towers.\n\n  The network produced by this function can be used either in\n  `GreedyRewardPredictionPolicy`, or `NeuralLinUCBPolicy`.\n  In the former case, the network must have `output_dim=1`, it is going to be an\n  instance of `QNetwork`, and used in the policy as a reward prediction network.\n  In the latter case, the network will be an encoding network with its output\n  consumed by a reward layer or a LinUCB method. The specified `output_dim` will\n  be the encoding dimension.\n\n  Args:\n    observation_spec: A nested tensor spec containing the specs for global as\n      well as per-arm observations.\n    global_layers: Iterable of ints. Specifies the layers of the global tower.\n    arm_layers: Iterable of ints. Specifies the layers of the arm tower.\n    common_layers: Iterable of ints. Specifies the layers of the common tower.\n    output_dim: The output dimension of the network. If 1, the common tower will\n      be a QNetwork. Otherwise, the common tower will be an encoding network\n      with the specified output dimension.\n    global_preprocessing_combiner: Preprocessing combiner for global features.\n    arm_preprocessing_combiner: Preprocessing combiner for the arm features.\n\n  Returns:\n    A network that takes observations adhering observation_spec and outputs\n    reward estimates for every action.\n  """"""\n  obs_spec_no_num_actions = _remove_num_actions_dim_from_spec(observation_spec)\n  global_network = encoding_network.EncodingNetwork(\n      input_tensor_spec=obs_spec_no_num_actions[\n          bandit_spec_utils.GLOBAL_FEATURE_KEY],\n      fc_layer_params=global_layers,\n      preprocessing_combiner=global_preprocessing_combiner)\n\n  arm_network = encoding_network.EncodingNetwork(\n      input_tensor_spec=obs_spec_no_num_actions[\n          bandit_spec_utils.PER_ARM_FEATURE_KEY],\n      fc_layer_params=arm_layers,\n      preprocessing_combiner=arm_preprocessing_combiner)\n  common_input_dim = global_layers[-1] + arm_layers[-1]\n  common_input_spec = tensor_spec.TensorSpec(\n      shape=(common_input_dim,), dtype=tf.float32)\n  if output_dim == 1:\n    common_network = q_network.QNetwork(\n        input_tensor_spec=common_input_spec,\n        action_spec=tensor_spec.BoundedTensorSpec(\n            shape=(), minimum=0, maximum=0, dtype=tf.int32),\n        fc_layer_params=common_layers)\n  else:\n    common_network = encoding_network.EncodingNetwork(\n        input_tensor_spec=common_input_spec,\n        fc_layer_params=list(common_layers) + [output_dim])\n  return GlobalAndArmCommonTowerNetwork(obs_spec_no_num_actions, global_network,\n                                        arm_network, common_network)\n\n\ndef create_feed_forward_dot_product_network(observation_spec, global_layers,\n                                            arm_layers):\n  """"""Creates a dot product network with feedforward towers.\n\n  Args:\n    observation_spec: A nested tensor spec containing the specs for global as\n      well as per-arm observations.\n    global_layers: Iterable of ints. Specifies the layers of the global tower.\n    arm_layers: Iterable of ints. Specifies the layers of the arm tower. The\n      last element of arm_layers has to be equal to that of global_layers.\n\n  Returns:\n    A dot product network that takes observations adhering observation_spec and\n    outputs reward estimates for every action.\n\n  Raises:\n    ValueError: If the last arm layer does not match the last global layer.\n  """"""\n\n  if arm_layers[-1] != global_layers[-1]:\n    raise ValueError(\'Last layer size of global and arm layers should match.\')\n\n  obs_spec_no_num_actions = _remove_num_actions_dim_from_spec(observation_spec)\n  global_network = encoding_network.EncodingNetwork(\n      input_tensor_spec=obs_spec_no_num_actions[\n          bandit_spec_utils.GLOBAL_FEATURE_KEY],\n      fc_layer_params=global_layers)\n  arm_network = encoding_network.EncodingNetwork(\n      input_tensor_spec=obs_spec_no_num_actions[\n          bandit_spec_utils.PER_ARM_FEATURE_KEY],\n      fc_layer_params=arm_layers)\n  return GlobalAndArmDotProductNetwork(obs_spec_no_num_actions, global_network,\n                                       arm_network)\n\n\n@gin.configurable\nclass GlobalAndArmCommonTowerNetwork(network.Network):\n  """"""A network that takes global and arm observations and outputs rewards.\n\n  This network takes the output of the global and per-arm networks, and leads\n  them through a common network, that in turn outputs reward estimates.\n  """"""\n\n  def __init__(self,\n               observation_spec,\n               global_network,\n               arm_network,\n               common_network,\n               name=\'GlobalAndArmCommonTowerNetwork\'):\n    """"""Initializes an instance of `GlobalAndArmCommonTowerNetwork`.\n\n    The network architecture contains networks for both the global and the arm\n    features. The outputs of these networks are concatenated and led through a\n    third (common) network which in turn outputs reward estimates.\n\n    Args:\n      observation_spec: The observation spec for the policy that uses this\n        network.\n      global_network: The network that takes the global features as input.\n      arm_network: The network that takes the arm features as input.\n      common_network: The network that takes as input the concatenation of the\n        outputs of the global and the arm networks.\n      name: The name of this instance of `GlobalAndArmCommonTowerNetwork`.\n    """"""\n    super(GlobalAndArmCommonTowerNetwork, self).__init__(\n        input_tensor_spec=observation_spec, state_spec=(), name=name)\n    self._global_network = global_network\n    self._arm_network = arm_network\n    self._common_network = common_network\n\n  def call(self, observation, step_type=None, network_state=()):\n    """"""Runs the observation through the network.""""""\n\n    global_obs = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    arm_obs = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n    arm_output, arm_state = self._arm_network(\n        arm_obs, step_type=step_type, network_state=network_state)\n    arm_output_shape = tf.shape(arm_output)\n    arm_output = tf.reshape(\n        arm_output, shape=[arm_output_shape[0], -1, arm_output_shape[-1]])\n\n    global_output, global_state = self._global_network(\n        global_obs, step_type=step_type, network_state=network_state)\n\n    num_actions = tf.shape(arm_output)[1]\n    global_output = tf.tile(\n        tf.expand_dims(global_output, axis=1), [1, num_actions, 1])\n\n    common_input = tf.concat([global_output, arm_output], axis=-1)\n\n    output, state = self._common_network(common_input,\n                                         (global_state, arm_state))\n    if isinstance(self._common_network, q_network.QNetwork):\n      output = tf.squeeze(output, axis=-1)\n    return output, state\n\n\n@gin.configurable\nclass GlobalAndArmDotProductNetwork(network.Network):\n  """"""A network that takes global and arm observations and outputs rewards.\n\n  This network calculates the dot product of the output of the global and\n  per-arm networks and returns them as reward estimates.\n  """"""\n\n  def __init__(self,\n               observation_spec,\n               global_network,\n               arm_network,\n               name=\'GlobalAndArmDotProductNetwork\'):\n    """"""Initializes an instance of `GlobalAndArmDotProductNetwork`.\n\n    The network architecture contains networks for both the global and the arm\n    features. The reward estimates will be the dot product of the global and per\n    arm outputs.\n\n    Args:\n      observation_spec: The observation spec for the policy that uses this\n        network.\n      global_network: The network that takes the global features as input.\n      arm_network: The network that takes the arm features as input.\n      name: The name of this instance of `GlobalAndArmDotProductNetwork`.\n    """"""\n    super(GlobalAndArmDotProductNetwork, self).__init__(\n        input_tensor_spec=observation_spec, state_spec=(), name=name)\n    self._global_network = global_network\n    self._arm_network = arm_network\n\n  def call(self, observation, step_type=None, network_state=()):\n    """"""Runs the observation through the network.""""""\n\n    global_obs = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    arm_obs = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n\n    global_output, global_state = self._global_network(\n        global_obs, step_type=step_type, network_state=network_state)\n\n    arm_output, arm_state = self._arm_network(\n        arm_obs, step_type=step_type, network_state=network_state)\n\n    dot_product = tf.linalg.matvec(arm_output, global_output)\n    return dot_product, global_state + arm_state\n'"
tf_agents/bandits/networks/global_and_arm_feature_network_test.py,22,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.networks.global_and_arm_feature_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.networks import global_and_arm_feature_network as gafn\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import test_utils\n\n\nparameters = parameterized.named_parameters(\n    {\n        \'testcase_name\': \'batch2feat4act3\',\n        \'batch_size\': 2,\n        \'feature_dim\': 4,\n        \'num_actions\': 3\n    }, {\n        \'testcase_name\': \'batch1feat7act9\',\n        \'batch_size\': 1,\n        \'feature_dim\': 7,\n        \'num_actions\': 9\n    })\n\n\nclass GlobalAndArmFeatureNetworkTest(parameterized.TestCase,\n                                     test_utils.TestCase):\n\n  @parameters\n  def testCreateFeedForwardCommonTowerNetwork(self, batch_size, feature_dim,\n                                              num_actions):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        7, feature_dim, num_actions)\n    net = gafn.create_feed_forward_common_tower_network(obs_spec, (4, 3, 2),\n                                                        (6, 5, 4), (7, 6, 5))\n    input_nest = tensor_spec.sample_spec_nest(\n        obs_spec, outer_dims=(batch_size,))\n    output, _ = net(input_nest)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    output = self.evaluate(output)\n    self.assertAllEqual(output.shape, (batch_size, num_actions))\n\n  @parameters\n  def testCreateFeedForwardDotProductNetwork(self, batch_size, feature_dim,\n                                             num_actions):\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        7, feature_dim, num_actions)\n    net = gafn.create_feed_forward_dot_product_network(obs_spec, (4, 3, 4),\n                                                       (6, 5, 4))\n    input_nest = tensor_spec.sample_spec_nest(\n        obs_spec, outer_dims=(batch_size,))\n    output, _ = net(input_nest)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    output = self.evaluate(output)\n    self.assertAllEqual(output.shape, (batch_size, num_actions))\n\n  def testCreateFeedForwardCommonTowerNetworkWithFeatureColumns(\n      self, batch_size=2, feature_dim=4, num_actions=3):\n    obs_spec = {\n        \'global\': {\n            \'dense\':\n                tensor_spec.TensorSpec(shape=(feature_dim,), dtype=tf.float32),\n            \'composer\':\n                tensor_spec.TensorSpec((), tf.string)\n        },\n        \'per_arm\': {\n            \'name\': tensor_spec.TensorSpec((num_actions,), tf.string),\n            \'fruit\': tensor_spec.TensorSpec((num_actions,), tf.string)\n        }\n    }\n    columns_dense = tf.feature_column.numeric_column(\n        \'dense\', shape=(feature_dim,))\n    columns_composer = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'composer\', [\'wolfgang\', \'amadeus\', \'mozart\']))\n\n    columns_name = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'name\', [\'bob\', \'george\', \'wanda\']))\n    columns_fruit = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'fruit\', [\'banana\', \'kiwi\', \'pear\']))\n\n    net = gafn.create_feed_forward_common_tower_network(\n        observation_spec=obs_spec,\n        global_layers=(4, 3, 2),\n        arm_layers=(6, 5, 4),\n        common_layers=(7, 6, 5),\n        global_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n            [columns_dense, columns_composer]),\n        arm_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n            [columns_name, columns_fruit]))\n    input_nest = {\n        \'global\': {\n            \'dense\':\n                tf.constant(\n                    np.random.rand(batch_size, feature_dim).astype(np.float32)),\n            \'composer\':\n                tf.constant([\'wolfgang\', \'mozart\'])\n        },\n        \'per_arm\': {\n            \'name\':\n                tf.constant([[[\'george\'], [\'george\'], [\'george\']],\n                             [[\'bob\'], [\'bob\'], [\'bob\']]]),\n            \'fruit\':\n                tf.constant([[[\'banana\'], [\'banana\'], [\'banana\']],\n                             [[\'kiwi\'], [\'kiwi\'], [\'kiwi\']]])\n        }\n    }\n\n    output, _ = net(input_nest)\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.tables_initializer()\n    ])\n    output = self.evaluate(output)\n    self.assertAllEqual(output.shape, (batch_size, num_actions))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/networks/heteroscedastic_q_network.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Network Outputting Expected Value and Variance of Rewards.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.networks import encoding_network\nfrom tf_agents.networks import network\nfrom tf_agents.networks import q_network\n\n\nclass QBanditNetworkResult(collections.namedtuple(\n    \'QBanditNetworkResult\', (\'q_value_logits\', \'log_variance\'))):\n  pass\n\n\n@gin.configurable\nclass HeteroscedasticQNetwork(network.Network):\n  """"""Network Outputting Expected Value and Variance of Rewards.""""""\n\n  def __init__(self,\n               input_tensor_spec,\n               action_spec,\n               preprocessing_layers=None,\n               preprocessing_combiner=None,\n               conv_layer_params=None,\n               fc_layer_params=(75, 40),\n               dropout_layer_params=None,\n               activation_fn=tf.keras.activations.relu,\n               kernel_initializer=None,\n               batch_squash=True,\n               min_variance=0.1,\n               max_variance=10000.0,\n               dtype=tf.float32,\n               name=\'HeteroscedasticQNetwork\'):\n    """"""Creates an instance of `HeteroscedasticQNetwork`.\n\n    Args:\n      input_tensor_spec: A nest of `tensor_spec.TensorSpec` representing the\n        input observations.\n      action_spec: A nest of `tensor_spec.BoundedTensorSpec` representing the\n        actions.\n      preprocessing_layers: (Optional.) A nest of `tf.keras.layers.Layer`\n        representing preprocessing for the different observations. All of these\n        layers must not be already built. For more details see the documentation\n        of `networks.EncodingNetwork`.\n      preprocessing_combiner: (Optional.) A keras layer that takes a flat list\n        of tensors and combines them. Good options include `tf.keras.layers.Add`\n        and `tf.keras.layers.Concatenate(axis=-1)`. This layer must not be\n        already built. For more details see the documentation of\n        `networks.EncodingNetwork`.\n      conv_layer_params: Optional list of convolution layers parameters, where\n        each item is a length-three tuple indicating (filters, kernel_size,\n        stride).\n      fc_layer_params: Optional list of fully_connected parameters, where each\n        item is the number of units in the layer.\n      dropout_layer_params: Optional list of dropout layer parameters, where\n        each item is the fraction of input units to drop. The dropout layers are\n        interleaved with the fully connected layers; there is a dropout layer\n        after each fully connected layer, except if the entry in the list is\n        None. This list must have the same length of fc_layer_params, or be\n        None.\n      activation_fn: Activation function, e.g. tf.keras.activations.relu.\n      kernel_initializer: Initializer to use for the kernels of the conv and\n        dense layers. If none is provided a default variance_scaling_initializer\n      batch_squash: If True the outer_ranks of the observation are squashed into\n        the batch dimension. This allow encoding networks to be used with\n        observations with shape [BxTx...].\n      min_variance: Float. The minimum allowed predicted variance. Predicted\n        variances less than this value will be clipped to this value.\n      max_variance: Float. The maximum allowed predicted variance. Predicted\n        variances greater than this value will be clipped to this value.\n      dtype: The dtype to use by the convolution and fully connected layers.\n      name: A string representing the name of the network.\n\n    Raises:\n      ValueError: If `input_tensor_spec` contains more than one observation. Or\n        if `action_spec` contains more than one action.\n    """"""\n    q_network.validate_specs(action_spec, input_tensor_spec)\n    action_spec = tf.nest.flatten(action_spec)[0]\n    num_actions = action_spec.maximum - action_spec.minimum + 1\n    encoder_input_tensor_spec = input_tensor_spec\n\n    encoder = encoding_network.EncodingNetwork(\n        encoder_input_tensor_spec,\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=preprocessing_combiner,\n        conv_layer_params=conv_layer_params,\n        fc_layer_params=fc_layer_params,\n        dropout_layer_params=dropout_layer_params,\n        activation_fn=activation_fn,\n        kernel_initializer=kernel_initializer,\n        batch_squash=batch_squash,\n        dtype=dtype)\n\n    q_value_layer = tf.keras.layers.Dense(\n        num_actions,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03),\n        bias_initializer=tf.compat.v1.initializers.constant(-0.2),\n        dtype=dtype)\n\n    super(HeteroscedasticQNetwork, self).__init__(\n        input_tensor_spec=input_tensor_spec,\n        state_spec=(),\n        name=name)\n\n    self._encoder = encoder\n    self._q_value_layer = q_value_layer\n\n    self._log_variance_layer = tf.keras.layers.Dense(\n        num_actions,\n        activation=None,\n        kernel_initializer=tf.compat.v1.initializers.random_uniform(\n            minval=-0.03, maxval=0.03),\n        dtype=dtype)\n\n    self._min_variance = min_variance\n    self._max_variance = max_variance\n\n  def call(self, observation, step_type=None, network_state=()):\n    """"""Runs the given observation through the network.\n\n    Args:\n      observation: The observation to provide to the network.\n      step_type: The step type for the given observation. See `StepType` in\n        time_step.py.\n      network_state: A state tuple to pass to the network, mainly used by RNNs.\n\n    Returns:\n      An instance of `QBanditNetworkResult`.\n    """"""\n    state, network_state = self._encoder(\n        observation, step_type=step_type, network_state=network_state)\n\n    log_variance = tf.clip_by_value(\n        self._log_variance_layer(state), tf.math.log(self._min_variance),\n        tf.math.log(self._max_variance))\n\n    q_value_logits = self._q_value_layer(state)\n\n    result = QBanditNetworkResult(q_value_logits, log_variance)\n\n    return (result, network_state)\n'"
tf_agents/bandits/networks/heteroscedastic_q_network_test.py,76,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.network.heteroscedastic_q_network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\nfrom tf_agents.specs import tensor_spec\n\n\nclass SingleObservationSingleActionTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(SingleObservationSingleActionTest, self).setUp()\n    gin.clear_config()\n\n  def testBuild(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertAllEqual(log_variances.shape.as_list(),\n                        [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_weights), 8)\n\n  def testChangeHiddenLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        fc_layer_params=(40,))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertAllEqual(log_variances.shape.as_list(),\n                        [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_variables), 6)\n\n  def testAddConvLayers(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, 5, 5, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([5, 5, num_state_dims],\n                                                 tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        conv_layer_params=((16, 3, 2),))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertAllEqual(log_variances.shape.as_list(),\n                        [batch_size, num_actions])\n    self.assertEqual(len(network.trainable_variables), 10)\n\n  def testAddPreprocessingLayers(self):\n    batch_size = 3\n    num_actions = 2\n    states = (tf.random.uniform([batch_size,\n                                 1]), tf.random.uniform([batch_size]))\n    preprocessing_layers = (tf.keras.layers.Dense(4),\n                            tf.keras.Sequential([\n                                tf.keras.layers.Reshape((1,)),\n                                tf.keras.layers.Dense(4)\n                            ]))\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=(tensor_spec.TensorSpec([1], tf.float32),\n                           tensor_spec.TensorSpec([], tf.float32)),\n        preprocessing_layers=preprocessing_layers,\n        preprocessing_combiner=tf.keras.layers.Add(),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0,\n                                                  num_actions - 1))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertAllEqual(log_variances.shape.as_list(),\n                        [batch_size, num_actions])\n    # At least 2 variables each for the preprocessing layers.\n    self.assertGreater(len(network.trainable_variables), 6)\n\n  def testCorrectOutputShape(self):\n    batch_size = 3\n    num_state_dims = 5\n    num_actions = 2\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_values.shape.as_list(), [batch_size, num_actions])\n    self.assertAllEqual(log_variances.shape.as_list(),\n                        [batch_size, num_actions])\n\n  def testNetworkVariablesAreReused(self):\n    batch_size = 3\n    num_state_dims = 5\n    states = tf.ones([batch_size, num_state_dims])\n    next_states = tf.ones([batch_size, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1))\n    preds, _ = network(states)\n    q_values, log_variances = preds.q_value_logits, preds.log_variance\n    preds, _ = network(next_states)\n    next_q_values, next_log_variances = preds.q_value_logits, preds.log_variance\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(q_values, next_q_values)\n    self.assertAllClose(log_variances, next_log_variances)\n\n  def testNumericFeatureColumnInput(self):\n    key = \'feature_key\'\n    batch_size = 3\n    state_dims = 5\n    column = tf.feature_column.numeric_column(key, [state_dims])\n    state = {key: tf.ones([batch_size, state_dims], tf.int32)}\n    state_spec = {key: tensor_spec.TensorSpec([state_dims], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online = online_network(state)[0].q_value_logits\n    q_target = target_network(state)[0].q_value_logits\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testIndicatorFeatureColumnInput(self):\n    key = \'feature_key\'\n    vocab_list = [2, 3, 4]\n    column = tf.feature_column.categorical_column_with_vocabulary_list(\n        key, vocab_list)\n    column = tf.feature_column.indicator_column(column)\n    feature_tensor = tf.convert_to_tensor([3, 2, 2, 4, 3])\n    state = {key: tf.expand_dims(feature_tensor, -1)}\n    state_spec = {key: tensor_spec.TensorSpec([1], tf.int32)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online = online_network(state)[0].q_value_logits\n    q_target = target_network(state)[0].q_value_logits\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testEmbeddingFeatureColumnInput(self):\n    key = \'feature_key\'\n    vocab_list = [\'a\', \'b\']\n    column = tf.feature_column.categorical_column_with_vocabulary_list(\n        key, vocab_list)\n    column = tf.feature_column.embedding_column(column, 3)\n    feature_tensor = tf.convert_to_tensor([\'a\', \'b\', \'c\', \'a\', \'c\'])\n    state = {key: tf.expand_dims(feature_tensor, -1)}\n    state_spec = {key: tensor_spec.TensorSpec([1], tf.string)}\n\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures([column])\n    online_network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=state_spec,\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online = online_network(state)[0].q_value_logits\n    q_target = target_network(state)[0].q_value_logits\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testCombinedFeatureColumnInput(self):\n    columns = {}\n    state_tensors = {}\n    state_specs = {}\n    expected_dim = 0\n\n    indicator_key = \'indicator_key\'\n    vocab_list = [2, 3, 4]\n    column1 = tf.feature_column.categorical_column_with_vocabulary_list(\n        indicator_key, vocab_list)\n    columns[indicator_key] = tf.feature_column.indicator_column(column1)\n    state_tensors[indicator_key] = tf.expand_dims([3, 2, 2, 4, 3], -1)\n    state_specs[indicator_key] = tensor_spec.TensorSpec([1], tf.int32)\n    expected_dim += len(vocab_list)\n\n    embedding_key = \'embedding_key\'\n    embedding_dim = 3\n    vocab_list = [2, 3, 4]\n    column2 = tf.feature_column.categorical_column_with_vocabulary_list(\n        embedding_key, vocab_list)\n    columns[embedding_key] = tf.feature_column.embedding_column(\n        column2, embedding_dim)\n    state_tensors[embedding_key] = tf.expand_dims([3, 2, 2, 4, 3], -1)\n    state_specs[embedding_key] = tensor_spec.TensorSpec([1], tf.int32)\n    expected_dim += embedding_dim\n\n    numeric_key = \'numeric_key\'\n    batch_size = 5\n    state_dims = 3\n    input_shape = (batch_size, state_dims)\n    columns[numeric_key] = tf.feature_column.numeric_column(\n        numeric_key, [state_dims])\n    state_tensors[numeric_key] = tf.ones(input_shape, tf.int32)\n    state_specs[numeric_key] = tensor_spec.TensorSpec([state_dims], tf.int32)\n    expected_dim += state_dims\n\n    num_actions = 4\n    action_spec = tensor_spec.BoundedTensorSpec([1], tf.int32, 0,\n                                                num_actions - 1)\n    dense_features = tf.compat.v2.keras.layers.DenseFeatures(columns.values())\n    online_network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        state_specs, action_spec, preprocessing_combiner=dense_features)\n    target_network = online_network.copy(name=\'TargetNetwork\')\n    q_online = online_network(state_tensors)[0].q_value_logits\n    q_target = target_network(state_tensors)[0].q_value_logits\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(tf.compat.v1.initializers.tables_initializer())\n\n    expected_shape = (batch_size, num_actions)\n    self.assertEqual(expected_shape, q_online.shape)\n    self.assertEqual(expected_shape, q_target.shape)\n    self.assertAllClose(q_online, q_target, rtol=1.0, atol=1.0)\n\n  def testPreprocessingLayersSingleObservations(self):\n    """"""Tests using preprocessing_layers without preprocessing_combiner.""""""\n    num_state_dims = 5\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        preprocessing_layers=tf.keras.layers.Lambda(lambda x: x),\n        preprocessing_combiner=None)\n    preds, _ = network(tf.ones((3, num_state_dims)))\n    q_logits, log_variances = preds.q_value_logits, preds.log_variance\n    self.assertAllEqual(q_logits.shape.as_list(), [3, 2])\n    self.assertAllEqual(log_variances.shape.as_list(), [3, 2])\n\n  def testVarianceBoundaryConditions(self):\n    """"""Tests that min/max variance conditions are satisfied.""""""\n    batch_size = 3\n    num_state_dims = 5\n    min_variance = 1.0\n    max_variance = 2.0\n    eps = 0.0001\n    states = tf.random.uniform([batch_size, num_state_dims])\n    network = heteroscedastic_q_network.HeteroscedasticQNetwork(\n        input_tensor_spec=tensor_spec.TensorSpec([num_state_dims], tf.float32),\n        action_spec=tensor_spec.BoundedTensorSpec([1], tf.int32, 0, 1),\n        min_variance=min_variance,\n        max_variance=max_variance)\n    log_variances = network(states)[0].log_variance\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllGreater(log_variances, math.log(min_variance) - eps)\n    self.assertAllLess(log_variances, math.log(max_variance) + eps)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all policies.""""""\n\nfrom tf_agents.bandits.policies import categorical_policy\nfrom tf_agents.bandits.policies import greedy_reward_prediction_policy\nfrom tf_agents.bandits.policies import lin_ucb_policy\nfrom tf_agents.bandits.policies import linalg\nfrom tf_agents.bandits.policies import linear_thompson_sampling_policy\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.bandits.policies import neural_linucb_policy\nfrom tf_agents.bandits.policies import policy_utilities\n'"
tf_agents/bandits/policies/categorical_policy.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy that chooses actions based on a categorical distribution.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.utils import common\nfrom tf_agents.utils import nest_utils\n\ntfd = tfp.distributions\n\n\ndef _validate_weights(weights):\n  if len(weights.shape) != 1:\n    raise ValueError(\n        \'Expected a 1D `Tensor` of weights; got {}.\'.format(weights))\n\n\nclass CategoricalPolicy(tf_policy.TFPolicy):\n  """"""Policy that chooses an action based on a categorical distribution.\n\n  The distribution is specified by a set of weights for each action and an\n  inverse temperature. The unnormalized probability distribution is given by\n  `exp(weight * inv_temp)`. Weights and inverse temperature are typically\n  maintained as `Variable`s, and are updated by an `Agent`.\n\n  Note that this policy does not make use of `time_step.observation` at all.\n  That is, it is a non-contextual bandit policy.\n  """"""\n\n  def __init__(self,\n               weights,\n               time_step_spec,\n               action_spec,\n               inverse_temperature=1.,\n               emit_log_probability=True,\n               name=None):\n    """"""Initializes `CategoricalPolicy`.\n\n    The `weights` and `inverse_temperature` arguments may be either `Tensor`s or\n      `tf.Variable`s. If they are variables, then any assignments to those\n      variables will be reflected in the output of the policy. The shape of\n      `weights` is used to determine the action_spec for this policy;\n      `action_spec.maximum = weights.shape[0]`.\n\n\n    If `emit_log_probability=True`, the info field of the `PolicyStep` returned\n      will be a `PolicyInfo` tuple with log-probability that can be accessed\n      using `policy_step.get_log_probability(step.info)`.\n\n    Args:\n      weights: a vector of weights, corresponding to the unscaled log\n        probabilities of a categorical distribution.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A `tensor_spec` of action specification.\n      inverse_temperature: a float value used to scale `weights`. Lower values\n        will induce a more uniform distribution over actions; higher values will\n        result in a sharper distribution.\n      emit_log_probability: Whether to emit log probabilities or not.\n      name: The name of this policy.\n\n    Raises:\n      ValueError: If the number of actions specified by the action_spec does not\n        match the dimension of weights.\n    """"""\n    _validate_weights(weights)\n    self._weights = weights\n    self._inverse_temperature = inverse_temperature\n    if action_spec.maximum + 1 != tf.compat.dimension_value(weights.shape[0]):\n      raise ValueError(\n          \'Number of actions ({}) does not match weights dimension ({}).\'\n          .format(action_spec.maximum + 1,\n                  tf.compat.dimension_value(weights.shape[0])))\n    super(CategoricalPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        emit_log_probability=True,\n        name=name)\n\n  def _variables(self):\n    return [v for v in [self._weights, self._inverse_temperature]\n            if isinstance(v, tf.Variable)]\n\n  def _distribution(self, time_step, policy_state):\n    """"""Implementation of `distribution`. Returns a `Categorical` distribution.\n\n    The returned `Categorical` distribution has (unnormalized) probabilities\n    `exp(inverse_temperature * weights)`.\n\n    Args:\n      time_step: A `TimeStep` tuple corresponding to `time_step_spec()`.\n      policy_state: Unused in `CategoricalPolicy`. It is simply passed through.\n\n    Returns:\n      A `PolicyStep` named tuple containing:\n        `action`: A (optionally nested) of tfp.distribution.Distribution\n          capturing the distribution of next actions.\n        `state`: A policy state tensor for the next call to distribution.\n        `info`: Optional side information such as action log probabilities.\n    """"""\n    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n    logits = (\n        self._inverse_temperature *\n        common.replicate(self._weights, outer_shape))\n    action_distribution = tfd.Independent(\n        tfd.Categorical(logits=logits, dtype=self._action_spec.dtype))\n    return policy_step.PolicyStep(action_distribution, policy_state)\n'"
tf_agents/bandits/policies/categorical_policy_test.py,30,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.policies.categorical_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.policies import categorical_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\nTEMP_UPDATE_TEST_INITIAL_INVERSE_TEMP = 1e-3\nTEMP_UPDATE_TEST_FINAL_INVERSE_TEMP = 1e3\nTEMP_UPDATE_TEST_BATCH_SIZE = 10000\n\n\ndef _get_dummy_observation_step(observation_shape, batch_size):\n  obs_spec = tensor_spec.TensorSpec(observation_shape, tf.float32)\n  time_step_spec = time_step.time_step_spec(obs_spec)\n  return tensor_spec.sample_spec_nest(time_step_spec, outer_dims=(batch_size,))\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass CategoricalPolicyTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(\n          observation_shape=[1],\n          batch_size=1,\n          weights=np.ones(10),\n          inverse_temperature=1.),\n      dict(\n          observation_shape=[2, 1, 3],\n          batch_size=32,\n          weights=np.arange(17),\n          inverse_temperature=10.),\n  )\n  def testActionShape(self, observation_shape, batch_size, weights,\n                      inverse_temperature):\n    observation_spec = tensor_spec.TensorSpec(\n        shape=observation_shape, dtype=tf.float32, name=\'observation_spec\')\n    time_step_spec = time_step.time_step_spec(observation_spec)\n\n    weights = tf.compat.v2.Variable(weights, dtype=tf.float32)\n    inverse_temperature = tf.compat.v2.Variable(\n        inverse_temperature, dtype=tf.float32)\n\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=tf.compat.dimension_value(weights.shape[0]) - 1,\n        name=\'action\')\n\n    policy = categorical_policy.CategoricalPolicy(weights, time_step_spec,\n                                                  action_spec,\n                                                  inverse_temperature)\n    observation_step = _get_dummy_observation_step(observation_shape,\n                                                   batch_size)\n    action_time_step = policy.action(observation_step)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(action_time_step.action.shape.as_list(), [batch_size])\n\n  @parameterized.parameters(\n      dict(observation_shape=[1], batch_size=1, weights=np.ones(10)),\n      dict(observation_shape=[2, 1, 3], batch_size=32, weights=np.arange(17)),\n  )\n  def testVariableWeightsDefaultTemp(self, observation_shape, batch_size,\n                                     weights):\n    observation_spec = tensor_spec.TensorSpec(\n        shape=observation_shape, dtype=tf.float32, name=\'observation_spec\')\n    time_step_spec = time_step.time_step_spec(observation_spec)\n\n    weights = tf.compat.v2.Variable(weights, dtype=tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=tf.compat.dimension_value(weights.shape[0]) - 1,\n        name=\'action\')\n    policy = categorical_policy.CategoricalPolicy(weights, time_step_spec,\n                                                  action_spec)\n    observation_step = _get_dummy_observation_step(observation_shape,\n                                                   batch_size)\n    action_time_step = policy.action(observation_step)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(action_time_step.action.shape.as_list(), [batch_size])\n\n  @parameterized.parameters(\n      dict(observation_shape=[1], weights=np.array([0., 1.]), seed=934585),\n      dict(observation_shape=[2, 1, 3], weights=np.arange(10), seed=345789),\n  )\n  def testInverseTempUpdate(self, observation_shape, weights, seed):\n    """"""Test that temperature updates perform as expected as it is increased.""""""\n    observation_spec = tensor_spec.TensorSpec(\n        shape=observation_shape, dtype=tf.float32, name=\'observation_spec\')\n    time_step_spec = time_step.time_step_spec(observation_spec)\n\n    weight_var = tf.compat.v2.Variable(weights, dtype=tf.float32)\n    inverse_temperature_var = tf.compat.v2.Variable(\n        TEMP_UPDATE_TEST_INITIAL_INVERSE_TEMP, dtype=tf.float32)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int64,\n        minimum=0,\n        maximum=tf.compat.dimension_value(weight_var.shape[0]) - 1,\n        name=\'action\')\n    policy = categorical_policy.CategoricalPolicy(weight_var, time_step_spec,\n                                                  action_spec,\n                                                  inverse_temperature_var)\n    observation_step = _get_dummy_observation_step(observation_shape,\n                                                   TEMP_UPDATE_TEST_BATCH_SIZE)\n    tf.compat.v1.set_random_seed(seed)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    # Set the inverse temperature to a large value.\n    self.evaluate(\n        tf.compat.v1.assign(inverse_temperature_var,\n                            TEMP_UPDATE_TEST_FINAL_INVERSE_TEMP))\n\n    final_action_time_step = self.evaluate(\n        policy.action(observation_step, seed=seed))\n    self.assertAllEqual(\n        final_action_time_step.action,\n        np.full([TEMP_UPDATE_TEST_BATCH_SIZE], np.argmax(weights)))\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\'_uniform\',\n          observation_shape=[1],\n          batch_size=1,\n          weights=np.ones(10, dtype=np.float32),\n          inverse_temperature=1.,\n          seed=48579),\n      dict(\n          testcase_name=\'_low_to_high\',\n          observation_shape=[3],\n          batch_size=32,\n          weights=np.linspace(-2, 2, 20, dtype=np.float32),\n          inverse_temperature=2.,\n          seed=37595),\n  )\n  def testActionProbabilities(self, observation_shape, batch_size, weights,\n                              inverse_temperature, seed):\n    observation_spec = tensor_spec.TensorSpec(\n        shape=observation_shape, dtype=tf.float32, name=\'observation_spec\')\n    time_step_spec = time_step.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=tf.compat.dimension_value(weights.shape[0]) - 1,\n        name=\'action\')\n    policy = categorical_policy.CategoricalPolicy(weights, time_step_spec,\n                                                  action_spec,\n                                                  inverse_temperature)\n    observation_step = _get_dummy_observation_step(observation_shape,\n                                                   batch_size)\n    action_time_step = policy.action(observation_step, seed=seed)\n\n    logits = inverse_temperature * weights\n    z = tf.reduce_logsumexp(logits)\n    expected_logprob = logits - z\n    expected_action_prob = tf.exp(\n        tf.gather(expected_logprob, action_time_step.action))\n    actual_action_prob = tf.exp(\n        policy_step.get_log_probability(action_time_step.info))\n    expected_action_prob_val, actual_action_prob_val = self.evaluate(\n        [expected_action_prob, actual_action_prob])\n    self.assertAllClose(expected_action_prob_val, actual_action_prob_val)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/greedy_reward_prediction_policy.py,6,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Policy for greedy reward prediction.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\n\n\n@gin.configurable\nclass GreedyRewardPredictionPolicy(tf_policy.TFPolicy):\n  """"""Class to build GreedyNNPredictionPolicies.""""""\n\n  def __init__(self,\n               time_step_spec=None,\n               action_spec=None,\n               reward_network=None,\n               observation_and_action_constraint_splitter=None,\n               accepts_per_arm_features=False,\n               constraints=(),\n               emit_policy_info=(),\n               name=None):\n    """"""Builds a GreedyRewardPredictionPolicy given a reward tf_agents.Network.\n\n    This policy takes a tf_agents.Network predicting rewards and generates the\n    action corresponding to the largest predicted reward.\n\n    Args:\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      action_spec: A nest of BoundedTensorSpec representing the actions.\n      reward_network: An instance of a `tf_agents.network.Network`,\n        callable via `network(observation, step_type) -> (output, final_state)`.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the network and 2) the\n        mask.  The mask should be a 0-1 `Tensor` of shape\n        `[batch_size, num_actions]`. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      constraints: iterable of constraints objects that are instances of\n        `tf_agents.bandits.agents.NeuralConstraint`.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      name: The name of this policy. All variables in this module will fall\n        under that name. Defaults to the class name.\n\n    Raises:\n      NotImplementedError: If `action_spec` contains more than one\n        `BoundedTensorSpec` or the `BoundedTensorSpec` is not valid.\n    """"""\n    flat_action_spec = tf.nest.flatten(action_spec)\n    if len(flat_action_spec) > 1:\n      raise NotImplementedError(\n          \'action_spec can only contain a single BoundedTensorSpec.\')\n\n    action_spec = flat_action_spec[0]\n    if (not tensor_spec.is_bounded(action_spec) or\n        not tensor_spec.is_discrete(action_spec) or\n        action_spec.shape.rank > 1 or\n        action_spec.shape.num_elements() != 1):\n      raise NotImplementedError(\n          \'action_spec must be a BoundedTensorSpec of type int32 and shape (). \'\n          \'Found {}.\'.format(action_spec))\n    self._expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n    self._action_offset = action_spec.minimum\n    reward_network.create_variables()\n    self._reward_network = reward_network\n    self._constraints = constraints\n\n    self._emit_policy_info = emit_policy_info\n    predicted_rewards_mean = ()\n    if policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN in emit_policy_info:\n      predicted_rewards_mean = tensor_spec.TensorSpec(\n          [self._expected_num_actions])\n    bandit_policy_type = ()\n    if policy_utilities.InfoFields.BANDIT_POLICY_TYPE in emit_policy_info:\n      bandit_policy_type = (\n          policy_utilities.create_bandit_policy_type_tensor_spec(shape=[1]))\n    if accepts_per_arm_features:\n      # The features for the chosen arm is saved to policy_info.\n      chosen_arm_features_info = (\n          policy_utilities.create_chosen_arm_features_info_spec(\n              time_step_spec.observation,\n              observation_and_action_constraint_splitter))\n      info_spec = policy_utilities.PerArmPolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          bandit_policy_type=bandit_policy_type,\n          chosen_arm_features=chosen_arm_features_info)\n    else:\n      info_spec = policy_utilities.PolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          bandit_policy_type=bandit_policy_type)\n\n    self._accepts_per_arm_features = accepts_per_arm_features\n\n    super(GreedyRewardPredictionPolicy, self).__init__(\n        time_step_spec, action_spec,\n        policy_state_spec=reward_network.state_spec,\n        clip=False,\n        info_spec=info_spec,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n\n  @property\n  def accepts_per_arm_features(self):\n    return self._accepts_per_arm_features\n\n  def _variables(self):\n    policy_variables = self._reward_network.variables\n    for c in self._constraints:\n      policy_variables.append(c.variables)\n    return policy_variables\n\n  def _distribution(self, time_step, policy_state):\n    observation = time_step.observation\n    if self.observation_and_action_constraint_splitter is not None:\n      observation, _ = self.observation_and_action_constraint_splitter(\n          observation)\n\n    predictions, policy_state = self._reward_network(\n        observation, time_step.step_type, policy_state)\n    batch_size = tf.shape(predictions)[0]\n\n    if isinstance(self._reward_network,\n                  heteroscedastic_q_network.HeteroscedasticQNetwork):\n      predicted_reward_values = predictions.q_value_logits\n    else:\n      predicted_reward_values = predictions\n\n    predicted_reward_values.shape.with_rank_at_least(2)\n    predicted_reward_values.shape.with_rank_at_most(3)\n    if predicted_reward_values.shape[-1] != self._expected_num_actions:\n      raise ValueError(\n          \'The number of actions ({}) does not match the reward_network output\'\n          \' size ({}).\'.format(self._expected_num_actions,\n                               predicted_reward_values.shape[1]))\n\n    mask = policy_utilities.construct_mask_from_multiple_sources(\n        time_step.observation, self._observation_and_action_constraint_splitter,\n        self._constraints, self._expected_num_actions)\n\n    # Argmax.\n    if mask is not None:\n      actions = policy_utilities.masked_argmax(\n          predicted_reward_values, mask, output_type=self.action_spec.dtype)\n    else:\n      actions = tf.argmax(\n          predicted_reward_values, axis=-1, output_type=self.action_spec.dtype)\n\n    actions += self._action_offset\n\n    bandit_policy_values = tf.fill([batch_size, 1],\n                                   policy_utilities.BanditPolicyType.GREEDY)\n\n    if self._accepts_per_arm_features:\n      # Saving the features for the chosen action to the policy_info.\n      def gather_observation(obs):\n        return tf.gather(params=obs, indices=actions, batch_dims=1)\n\n      chosen_arm_features = tf.nest.map_structure(\n          gather_observation,\n          observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n      policy_info = policy_utilities.PerArmPolicyInfo(\n          predicted_rewards_mean=(\n              predicted_reward_values if policy_utilities.InfoFields\n              .PREDICTED_REWARDS_MEAN in self._emit_policy_info else ()),\n          bandit_policy_type=(bandit_policy_values\n                              if policy_utilities.InfoFields.BANDIT_POLICY_TYPE\n                              in self._emit_policy_info else ()),\n          chosen_arm_features=chosen_arm_features)\n    else:\n      policy_info = policy_utilities.PolicyInfo(\n          predicted_rewards_mean=(\n              predicted_reward_values if policy_utilities.InfoFields\n              .PREDICTED_REWARDS_MEAN in self._emit_policy_info else ()),\n          bandit_policy_type=(bandit_policy_values\n                              if policy_utilities.InfoFields.BANDIT_POLICY_TYPE\n                              in self._emit_policy_info else ()))\n\n    return policy_step.PolicyStep(\n        tfp.distributions.Deterministic(loc=actions), policy_state, policy_info)\n'"
tf_agents/bandits/policies/greedy_reward_prediction_policy_test.py,96,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Test for greedy_reward_prediction_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import constraints\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.networks import heteroscedastic_q_network\nfrom tf_agents.bandits.policies import greedy_reward_prediction_policy as greedy_reward_policy\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, observation_spec, num_actions=3):\n    super(DummyNet, self).__init__(observation_spec, (), \'DummyNet\')\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[1, 1.5, 2], [1, 1.5, 4]]),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                [[1], [1], [-10]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\nclass HeteroscedasticDummyNet(\n    heteroscedastic_q_network.HeteroscedasticQNetwork):\n\n  def __init__(self, name=None, num_actions=3):\n    input_spec = array_spec.ArraySpec([2], np.float32)\n    action_spec = array_spec.BoundedArraySpec([1], np.float32, 1, num_actions)\n\n    input_tensor_spec = tensor_spec.from_spec(input_spec)\n    action_tensor_spec = tensor_spec.from_spec(action_spec)\n\n    super(HeteroscedasticDummyNet, self).__init__(input_tensor_spec,\n                                                  action_tensor_spec)\n    self._value_layer = tf.keras.layers.Dense(\n        num_actions,\n        kernel_initializer=tf.compat.v1.initializers.constant(\n            [[1, 1.5, 2], [1, 1.5, 4]]),\n        bias_initializer=tf.compat.v1.initializers.constant(\n            [[1], [1], [-10]]))\n\n    self._log_variance_layer = tf.keras.layers.Dense(\n        num_actions,\n        kernel_initializer=tf.compat.v1.initializers.constant(\n            [[1, 1.5, 2], [1, 1.5, 4]]),\n        bias_initializer=tf.compat.v1.initializers.constant(\n            [[1], [1], [-10]]))\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    value = self._value_layer(inputs)\n    log_variance = self._log_variance_layer(inputs)\n    predictions = collections.namedtuple(\'QBanditNetworkResult\',\n                                         (\'q_value_logits\', \'log_variance\'))\n    predictions = predictions(value, log_variance)\n\n    return predictions, network_state\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass GreedyRewardPredictionPolicyTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(GreedyRewardPredictionPolicyTest, self).setUp()\n    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n\n  def testBuild(self):\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec))\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n    self.assertEqual(policy.action_spec, self._action_spec)\n\n  def testMultipleActionsRaiseError(self):\n    action_spec = [tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)] * 2\n    with self.assertRaisesRegexp(\n        NotImplementedError,\n        \'action_spec can only contain a single BoundedTensorSpec\'):\n      greedy_reward_policy.GreedyRewardPredictionPolicy(\n          self._time_step_spec,\n          action_spec,\n          reward_network=DummyNet(self._obs_spec))\n\n  def testWrongActionsRaiseError(self):\n    action_spec = tensor_spec.BoundedTensorSpec((5, 6, 7), tf.float32, 0, 2)\n    with self.assertRaisesRegexp(\n        NotImplementedError,\n        \'action_spec must be a BoundedTensorSpec of type int32.*\'):\n      greedy_reward_policy.GreedyRewardPredictionPolicy(\n          self._time_step_spec,\n          action_spec,\n          reward_network=DummyNet(self._obs_spec))\n\n  def testWrongOutputLayerRaiseError(self):\n    tf.compat.v1.set_random_seed(1)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 10, 20)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        action_spec,\n        reward_network=DummyNet(self._obs_spec))\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    with self.assertRaisesRegexp(\n        ValueError,\n        r\'The number of actions \\(11\\) does not match the reward_network output\'\n        r\' size \\(3\\)\\.\'):\n      policy.action(time_step, seed=1)\n\n  def testAction(self):\n    tf.compat.v1.set_random_seed(1)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec))\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n\n  def testActionHeteroscedastic(self):\n    tf.compat.v1.set_random_seed(1)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec, self._action_spec,\n        reward_network=HeteroscedasticDummyNet())\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n\n  def testActionScalarSpec(self):\n    tf.compat.v1.set_random_seed(1)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        action_spec,\n        reward_network=DummyNet(self._obs_spec))\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n\n  def testActionScalarSpecWithShift(self):\n    tf.compat.v1.set_random_seed(1)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 10, 12)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        action_spec,\n        reward_network=DummyNet(self._obs_spec))\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [11, 12])\n\n  def testMaskedAction(self):\n    tf.compat.v1.set_random_seed(1)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n    observation_spec = (tensor_spec.TensorSpec([2], tf.float32),\n                        tensor_spec.TensorSpec([3], tf.int32))\n    time_step_spec = ts.time_step_spec(observation_spec)\n\n    def split_fn(obs):\n      return obs[0], obs[1]\n\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        time_step_spec,\n        action_spec,\n        reward_network=DummyNet(observation_spec[0]),\n        observation_and_action_constraint_splitter=split_fn)\n\n    observations = (tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n                    tf.constant([[0, 0, 1], [0, 1, 0]], dtype=tf.int32))\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [2, 1])\n\n  def testUpdate(self):\n    tf.compat.v1.set_random_seed(1)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec))\n    new_policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec))\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n\n    action_step = policy.action(time_step, seed=1)\n    new_action_step = new_policy.action(time_step, seed=1)\n\n    self.assertEqual(len(policy.variables()), 2)\n    self.assertEqual(len(new_policy.variables()), 2)\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertIsNone(self.evaluate(new_policy.update(policy)))\n\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n    self.assertAllEqual(self.evaluate(new_action_step.action), [1, 2])\n\n  def testPredictedRewards(self):\n    tf.compat.v1.set_random_seed(1)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec),\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n    # The expected values are obtained by passing the observation through the\n    # Keras dense layer of the DummyNet (defined above).\n    predicted_rewards_expected_array = np.array([[4.0, 5.5, 0.0],\n                                                 [8.0, 11.5, 12.0]])\n    p_info = self.evaluate(action_step.info)\n    self.assertAllClose(p_info.predicted_rewards_mean,\n                        predicted_rewards_expected_array)\n\n  def testPerArmRewards(self):\n    tf.compat.v1.set_random_seed(3000)\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(2, 3, 4)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 3)\n    reward_network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        time_step_spec,\n        action_spec,\n        reward_network=reward_network,\n        accepts_per_arm_features=True,\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    action_feature = tf.cast(\n        tf.reshape(tf.random.shuffle(tf.range(24)), shape=[2, 4, 3]),\n        dtype=tf.float32)\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY: action_feature\n    }\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action, p_info, first_arm_features = self.evaluate([\n        action_step.action, action_step.info,\n        observations[bandit_spec_utils.PER_ARM_FEATURE_KEY][0]\n    ])\n    self.assertAllEqual(action.shape, [2])\n    self.assertAllEqual(p_info.predicted_rewards_mean.shape, [2, 4])\n    self.assertAllEqual(p_info.chosen_arm_features.shape, [2, 3])\n    first_action = action[0]\n    self.assertAllEqual(p_info.chosen_arm_features[0],\n                        first_arm_features[first_action])\n\n    # Check that zeroing out some of the actions does not affect the predicted\n    # rewards for unchanged actions. This is to make sure that action feature\n    # padding does not influence the behavior.\n\n    if not tf.executing_eagerly():\n      # The below comparison will only work in tf2 because of the random per-arm\n      # observations get re-drawn in tf1.\n      return\n    padded_action_feature = tf.concat(\n        [action_feature[:, 0:1, :],\n         tf.zeros(shape=[2, 3, 3], dtype=tf.float32)],\n        axis=1)\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY: padded_action_feature\n    }\n    time_step = ts.restart(observations, batch_size=2)\n    padded_action_step = policy.action(time_step, seed=1)\n    padded_p_info = self.evaluate(padded_action_step.info)\n    self.assertAllEqual(p_info.predicted_rewards_mean[:, 0],\n                        padded_p_info.predicted_rewards_mean[:, 0])\n\n  def testPerArmRewardsVariableNumActions(self):\n    tf.compat.v1.set_random_seed(3000)\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        2, 3, 4, add_num_actions_feature=True)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 3)\n    reward_network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2)))\n\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        time_step_spec,\n        action_spec,\n        reward_network=reward_network,\n        accepts_per_arm_features=True,\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    action_feature = tf.cast(\n        tf.reshape(tf.random.shuffle(tf.range(24)), shape=[2, 4, 3]),\n        dtype=tf.float32)\n    observations = {\n        bandit_spec_utils.GLOBAL_FEATURE_KEY:\n            tf.constant([[1, 2], [3, 4]], dtype=tf.float32),\n        bandit_spec_utils.PER_ARM_FEATURE_KEY:\n            action_feature,\n        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n            tf.constant([2, 3], dtype=tf.int32)\n    }\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action, p_info, first_arm_features = self.evaluate([\n        action_step.action, action_step.info,\n        observations[bandit_spec_utils.PER_ARM_FEATURE_KEY][0]\n    ])\n    self.assertAllEqual(action.shape, [2])\n    self.assertAllEqual(p_info.predicted_rewards_mean.shape, [2, 4])\n    self.assertAllEqual(p_info.chosen_arm_features.shape, [2, 3])\n    first_action = action[0]\n    self.assertAllEqual(p_info.chosen_arm_features[0],\n                        first_arm_features[first_action])\n\n  def testPerArmRewardsSparseObs(self):\n    tf.compat.v1.set_random_seed(3000)\n    obs_spec = {\n        \'global\': {\'sport\': tensor_spec.TensorSpec((), tf.string)},\n        \'per_arm\': {\n            \'name\': tensor_spec.TensorSpec((3,), tf.string),\n            \'fruit\': tensor_spec.TensorSpec((3,), tf.string)\n        }\n    }\n    columns_a = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'name\', [\'bob\', \'george\', \'wanda\']))\n    columns_b = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'fruit\', [\'banana\', \'kiwi\', \'pear\']))\n    columns_c = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'sport\', [\'bridge\', \'chess\', \'snooker\']))\n\n    reward_network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            observation_spec=obs_spec,\n            global_layers=(4, 3, 2),\n            arm_layers=(6, 5, 4),\n            common_layers=(7, 6, 5),\n            global_preprocessing_combiner=(\n                tf.compat.v2.keras.layers.DenseFeatures([columns_c])),\n            arm_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n                [columns_a, columns_b])))\n\n    time_step_spec = ts.time_step_spec(obs_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        time_step_spec,\n        action_spec,\n        reward_network=reward_network,\n        accepts_per_arm_features=True,\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    observations = {\n        \'global\': {\n            \'sport\': tf.constant([\'snooker\', \'chess\'])\n        },\n        \'per_arm\': {\n            \'name\':\n                tf.constant([[\'george\', \'george\', \'george\'],\n                             [\'bob\', \'bob\', \'bob\']]),\n            \'fruit\':\n                tf.constant([[\'banana\', \'banana\', \'banana\'],\n                             [\'kiwi\', \'kiwi\', \'kiwi\']])\n        }\n    }\n\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate([\n        tf.compat.v1.global_variables_initializer(),\n        tf.compat.v1.tables_initializer()\n    ])\n    action, p_info, first_arm_name_feature = self.evaluate([\n        action_step.action, action_step.info,\n        observations[bandit_spec_utils.PER_ARM_FEATURE_KEY][\'name\'][0]\n    ])\n    self.assertAllEqual(action.shape, [2])\n    self.assertAllEqual(p_info.predicted_rewards_mean.shape, [2, 3])\n    self.assertAllEqual(p_info.chosen_arm_features[\'name\'].shape, [2])\n    self.assertAllEqual(p_info.chosen_arm_features[\'fruit\'].shape, [2])\n    first_action = action[0]\n    self.assertAllEqual(p_info.chosen_arm_features[\'name\'][0],\n                        first_arm_name_feature[first_action])\n\n  def testPolicyWithConstraints(self):\n    constraint_net = DummyNet(self._obs_spec)\n    neural_constraint = constraints.NeuralConstraint(\n        self._time_step_spec,\n        self._action_spec,\n        constraint_network=constraint_net)\n\n    tf.compat.v1.set_random_seed(1)\n    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n        self._time_step_spec,\n        self._action_spec,\n        reward_network=DummyNet(self._obs_spec),\n        constraints=[neural_constraint],\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    time_step = ts.restart(observations, batch_size=2)\n    action_step = policy.action(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.assertAllEqual(self.evaluate(action_step.action), [1, 2])\n    # The expected values are obtained by passing the observation through the\n    # Keras dense layer of the DummyNet (defined above).\n    predicted_rewards_expected_array = np.array([[4.0, 5.5, 0.0],\n                                                 [8.0, 11.5, 12.0]])\n    p_info = self.evaluate(action_step.info)\n    self.assertAllClose(p_info.predicted_rewards_mean,\n                        predicted_rewards_expected_array)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/lin_ucb_policy.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Linear UCB Policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tf_agents.bandits.policies import linear_bandit_policy as lin_policy\n\n\nclass LinearUCBPolicy(lin_policy.LinearBanditPolicy):\n  """"""Linear UCB Policy.\n\n  Implements the Linear UCB Policy from the following paper:\n  ""A Contextual Bandit Approach to Personalized News Article Recommendation"",\n  Lihong Li, Wei Chu, John Langford, Robert Schapire, WWW 2010.\n\n  """"""\n\n  def __init__(self,\n               action_spec,\n               cov_matrix,\n               data_vector,\n               num_samples,\n               time_step_spec=None,\n               alpha=1.0,\n               eig_vals=(),\n               eig_matrix=(),\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               emit_log_probability=False,\n               observation_and_action_constraint_splitter=None,\n               name=None):\n    """"""Initializes `LinUCBPolicy`.\n\n    The `a` and `b` arguments may be either `Tensor`s or `tf.Variable`s.\n    If they are variables, then any assignements to those variables will be\n    reflected in the output of the policy.\n\n    Args:\n      action_spec: `TensorSpec` containing action specification.\n      cov_matrix: list of the covariance matrices A in the paper. There exists\n        one A matrix per arm.\n      data_vector: list of the b vectors in the paper. The b vector is a\n        weighted sum of the observations, where the weight is the corresponding\n        reward. Each arm has its own vector b.\n      num_samples: list of number of samples per arm.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      alpha: a float value used to scale the confidence intervals.\n      eig_vals: list of eigenvalues for each covariance matrix (one per arm).\n      eig_matrix: list of eigenvectors for each covariance matrix (one per arm).\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: Whether to emit log probabilities.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit policy and 2)\n        the mask. The mask should be a 0-1 `Tensor` of shape\n        `[batch_size, num_actions]`. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      name: The name of this policy.\n    """"""\n    super(LinearUCBPolicy, self).__init__(\n        action_spec=action_spec,\n        cov_matrix=cov_matrix,\n        data_vector=data_vector,\n        num_samples=num_samples,\n        time_step_spec=time_step_spec,\n        exploration_strategy=lin_policy.ExplorationStrategy.optimistic,\n        alpha=alpha,\n        eig_vals=eig_vals,\n        eig_matrix=eig_matrix,\n        tikhonov_weight=tikhonov_weight,\n        add_bias=add_bias,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n'"
tf_agents/bandits/policies/linalg.py,31,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utility code for linear algebra functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.utils import common\n\n\ndef _cg_check_shapes(a_mat, b):\n  if a_mat.shape[0] != a_mat.shape[1] or a_mat.shape.rank != 2:\n    raise ValueError(\'`a_mat` must be rank 2 square matrix; \'\n                     \'got shape {}.\'.format(a_mat.shape))\n  if a_mat.shape[1] != b.shape[0]:\n    raise ValueError(\'The dims of `a_mat` and `b` are not compatible; \'\n                     \'got shapes {} and {}.\'.format(a_mat.shape, b.shape))\n\n\n@common.function\ndef conjugate_gradient(a_mat, b, tol=1e-10):\n  """"""Returns `x` such that `A * x = b`.\n\n  Implements the Conjugate Gradient method.\n  https://en.wikipedia.org/wiki/Conjugate_gradient_method\n\n  Args:\n    a_mat: a Symmetric Positive Definite matrix, represented as a `Tensor` of\n      shape `[n, n]`.\n    b: a `Tensor` of shape `[n, 1]`.\n    tol: (float) desired tolerance on the residual.\n\n  Returns:\n    x: `Tensor` `x` of shape `[n, 1]` such that `A * x = b`.\n\n  Raises:\n    ValueError: if `a_mat` is not square or `a_mat` and `b` have incompatible\n    shapes.\n  """"""\n  _cg_check_shapes(a_mat, b)\n  n = tf.shape(b)[0]\n  x = tf.zeros_like(b)\n\n  r = b - tf.matmul(a_mat, x)\n  p = r\n  rs_old = tf.reduce_sum(r * r)\n  rs_new = rs_old\n\n  def body_fn(i, x, p, r, rs_old, rs_new):\n    """"""One iteration of CG.""""""\n    a_x_p = tf.matmul(a_mat, p)\n    alpha = rs_old / tf.reduce_sum(p * a_x_p)\n    x = x + alpha * p\n    r = r - alpha * a_x_p\n    rs_new = tf.reduce_sum(r * r)\n    p = r + (rs_new / rs_old) * p\n    rs_old = rs_new\n    i = i + 1\n    return i, x, p, r, rs_old, rs_new\n\n  def while_exit_cond(i, x, p, r, rs_old, rs_new):\n    """"""Exit the loop when n is reached or when the residual becomes small.""""""\n    del x  # unused\n    del p  # unused\n    del r  # unused\n    del rs_old  # unused\n    i_cond = tf.less(i, n)\n    residual_cond = tf.greater(tf.sqrt(rs_new), tol)\n    return tf.logical_and(i_cond, residual_cond)\n\n  _, x, _, _, _, _ = tf.while_loop(\n      while_exit_cond,\n      body_fn,\n      [tf.constant(0), x, p, r, rs_old, rs_new],\n      parallel_iterations=1)\n  return x\n\n\n@common.function\ndef conjugate_gradient_solve(a_mat, b_mat, tol=1e-10):\n  """"""Returns `X` such that `A * X = B`.\n\n  Uses Conjugate Gradient to solve many linear systems of equations with the\n  same matrix `a_mat` and multiple right hand sides provided as columns in\n  the matrix `b_mat`.\n\n  Args:\n    a_mat: a Symmetric Positive Definite matrix, represented as a `Tensor` of\n      shape `[n, n]`.\n    b_mat: a `Tensor` of shape `[n, k]`.\n    tol: (float) desired tolerance on the residual.\n\n  Returns:\n    X: `Tensor` `X` of shape `[n, k]` such that `A * X = B`.\n\n  Raises:\n    ValueError: if `a_mat` is not square or `a_mat` and `b_mat` have\n    incompatible shapes.\n  """"""\n  # Allows for flexible shape handling. If the shape is statically known, it\n  # will use the first part. If the shape is not statically known, tf.shape()\n  # will be used.\n  n = tf.compat.dimension_value(b_mat.shape[0]) or tf.shape(b_mat)[0]\n  k = tf.compat.dimension_value(b_mat.shape[1]) or tf.shape(b_mat)[1]\n  x = tf.zeros_like(b_mat)\n\n  def body_fn(i, x):\n    """"""Solve one linear system of equations with the `i`-th column of b_mat.""""""\n    b_vec = tf.slice(b_mat, begin=[0, i], size=[n, 1])\n    x_sol = conjugate_gradient(a_mat, b_vec, tol)\n    indices = tf.concat([tf.reshape(tf.range(n, dtype=tf.int32), [n, 1]),\n                         i * tf.ones([n, 1], dtype=tf.int32)], axis=-1)\n    x = tf.tensor_scatter_nd_update(\n        tensor=x, indices=indices, updates=tf.squeeze(x_sol, 1))\n    x.set_shape(b_mat.shape)\n    i = i + 1\n    return i, x\n\n  _, x = tf.while_loop(\n      lambda i, _: i < k,\n      body_fn,\n      loop_vars=[tf.constant(0), x],\n      parallel_iterations=10)\n  return x\n\n\ndef _check_shapes(a_inv, u):\n  if a_inv.shape[0] != a_inv.shape[1] or a_inv.shape.rank != 2:\n    raise ValueError(\'`a_inv` must be rank 2 square matrix; \'\n                     \'got shape {}.\'.format(a_inv.shape))\n  if u.shape.rank != 2:\n    raise ValueError(\'`u` must be rank 2 matrix; \'\n                     \'got shape {}.\'.format(u.shape))\n  if a_inv.shape[1] != u.shape[1]:\n    raise ValueError(\'`a_inv` and `u` must have shapes [m, m] and [n, m]; \'\n                     \'got shapes {} and {}.\'.format(a_inv.shape, u.shape))\n\n\ndef simplified_woodbury_update(a_inv, u):\n  """"""Returns `w` such that `inverse(a + u.T.dot(u)) = a_inv + w`.\n\n  Makes use of the Woodbury matrix identity. See\n  https://en.wikipedia.org/wiki/Woodbury_matrix_identity.\n\n  **NOTE**: This implementation assumes that a_inv is symmetric. Since it\'s too\n  expensive to check symmetricity, the function silently outputs a wrong answer\n  in case `a` is not symmetric.\n\n  Args:\n    a_inv: an invertible SYMMETRIC `Tensor` of shape `[m, m]`.\n    u: a `Tensor` of shape `[n, m]`.\n  Returns:\n    A `Tensor` `w` of shape `[m, m]` such that\n    `inverse(a + u.T.dot(u)) = a_inv + w`.\n  Raises:\n    ValueError: if `a_inv` is not square or `a_inv` and `u` have incompatible\n    shapes.\n  """"""\n  _check_shapes(a_inv, u)\n  u_x_a_inv = tf.matmul(u, a_inv)\n  capacitance = (\n      tf.eye(tf.shape(u)[0], dtype=u.dtype) +\n      tf.matmul(u_x_a_inv, u, transpose_b=True))\n  return -1. * tf.matmul(\n      u_x_a_inv, tf.linalg.solve(capacitance, u_x_a_inv), transpose_a=True)\n\n\ndef update_inverse(a_inv, x):\n  """"""Updates the inverse using the Woodbury matrix identity.\n\n  Given a matrix `A` of size d-by-d and a matrix `X` of size k-by-d, this\n  function computes the inverse of B = A + X^T X, assuming that the inverse of\n  A is available.\n\n  Reference:\n  https://en.wikipedia.org/wiki/Woodbury_matrix_identity\n\n  Args:\n    a_inv: a `Tensor` of shape [`d`, `d`]. This is the current inverse of `A`.\n    x: a `Tensor` of shape [`k`, `d`].\n\n  Returns:\n    The update that needs to be added to \'a_inv\' to compute the inverse.\n    If `x` is empty, a zero matrix is returned.\n  """"""\n  batch_size = tf.shape(x)[0]\n\n  def true_fn():\n    return tf.zeros_like(a_inv)\n\n  def false_fn():\n    return simplified_woodbury_update(a_inv, x)\n\n  a_inv_update = tf.cond(tf.equal(batch_size, 0), true_fn, false_fn)\n  return a_inv_update\n'"
tf_agents/bandits/policies/linalg_test.py,27,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.linalg.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.policies import linalg\nfrom tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\ntf.compat.v1.enable_v2_behavior()\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_contextdim10\',\n          \'batch_size\': 1,\n          \'context_dim\': 10,\n      }, {\n          \'testcase_name\': \'_batch4_contextdim5\',\n          \'batch_size\': 4,\n          \'context_dim\': 5,\n      })\n\n\nclass LinalgTest(tf.test.TestCase, parameterized.TestCase):\n\n  @test_cases()\n  def testAInvUpdate(self, batch_size, context_dim):\n    a_array = 2 * np.eye(context_dim) + np.array(\n        range(context_dim * context_dim)).reshape((context_dim, context_dim))\n    a_array = a_array + a_array.T\n    a_inv_array = np.linalg.inv(a_array)\n    x_array = np.array(range(batch_size * context_dim)).reshape(\n        (batch_size, context_dim))\n    expected_a_inv_updated_array = np.linalg.inv(\n        a_array + np.matmul(np.transpose(x_array), x_array))\n\n    a_inv = tf.constant(\n        a_inv_array, dtype=tf.float32, shape=[context_dim, context_dim])\n    x = tf.constant(x_array, dtype=tf.float32, shape=[batch_size, context_dim])\n    a_inv_update = linalg.update_inverse(a_inv, x)\n    self.assertAllClose(expected_a_inv_updated_array,\n                        self.evaluate(a_inv + a_inv_update))\n\n  @test_cases()\n  def testAInvUpdateEmptyObservations(self, batch_size, context_dim):\n    a_array = 2 * np.eye(context_dim) + np.array(\n        range(context_dim * context_dim)).reshape((context_dim, context_dim))\n    a_array = a_array + a_array.T\n    a_inv_array = np.linalg.inv(a_array)\n    expected_a_inv_update_array = np.zeros([context_dim, context_dim],\n                                           dtype=np.float32)\n\n    a_inv = tf.constant(\n        a_inv_array, dtype=tf.float32, shape=[context_dim, context_dim])\n    x = tf.constant([], dtype=tf.float32, shape=[0, context_dim])\n    a_inv_update = linalg.update_inverse(a_inv, x)\n    self.assertAllClose(expected_a_inv_update_array,\n                        self.evaluate(a_inv_update))\n\n\ndef cg_test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_n_1\',\n          \'n\': 1,\n          \'rhs\': 1,\n      }, {\n          \'testcase_name\': \'_n_10\',\n          \'n\': 10,\n          \'rhs\': 1,\n      }, {\n          \'testcase_name\': \'_n_100\',\n          \'n\': 100,\n          \'rhs\': 5,\n      })\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass ConjugateGradientTest(tf.test.TestCase, parameterized.TestCase):\n\n  @cg_test_cases()\n  def testConjugateGradientBasic(self, n, rhs):\n    x_obs = tf.constant(np.random.rand(n, 2), dtype=tf.float32, shape=[n, 2])\n    a_mat = tf.eye(n) + tf.matmul(x_obs, tf.linalg.matrix_transpose(x_obs))\n    x_exact = tf.constant(np.random.rand(n), dtype=tf.float32, shape=[n, 1])\n    b = tf.matmul(a_mat, x_exact)\n    x_approx = self.evaluate(linalg.conjugate_gradient(a_mat, b))\n    x_exact_numpy = self.evaluate(x_exact)\n    self.assertAllClose(x_exact_numpy, x_approx, rtol=1e-4, atol=1e-4)\n\n  @cg_test_cases()\n  def testConjugateGradientMultipleRHS(self, n, rhs):\n    x_obs = tf.constant(np.random.rand(n, 2), dtype=tf.float32, shape=[n, 2])\n    a_mat = tf.eye(n) + tf.matmul(x_obs, tf.linalg.matrix_transpose(x_obs))\n    x_exact = tf.constant(\n        np.random.rand(n, rhs), dtype=tf.float32, shape=[n, rhs])\n    b_mat = tf.matmul(a_mat, x_exact)\n    x_approx = self.evaluate(\n        linalg.conjugate_gradient_solve(a_mat, b_mat))\n    x_exact_numpy = self.evaluate(x_exact)\n    self.assertAllClose(x_exact_numpy, x_approx, rtol=1e-4, atol=1e-4)\n\n  @cg_test_cases()\n  def testConjugateGradientMultipleRHSPlaceholders(self, n, rhs):\n    # Test the case where a_mat and b_mat are placeholders and they have unknown\n    # dimension values.\n\n    if tf.executing_eagerly():\n      return\n\n    x_obs = tf.constant(np.random.rand(n, 2), dtype=tf.float32, shape=[n, 2])\n    a_mat = tf.eye(n) + tf.matmul(x_obs, tf.linalg.matrix_transpose(x_obs))\n    a_mat_ph = tf.compat.v1.placeholder(tf.float32, shape=(None, None))\n    a_mat_value = self.evaluate(a_mat)\n\n    x_exact = tf.constant(\n        np.random.rand(n, rhs), dtype=tf.float32, shape=[n, rhs])\n    b_mat = tf.matmul(a_mat, x_exact)\n    b_mat_ph = tf.compat.v1.placeholder(tf.float32, shape=(None, None))\n    b_mat_value = self.evaluate(b_mat)\n\n    x_exact_numpy = self.evaluate(x_exact)\n    with self.cached_session() as sess:\n      x_approx = linalg.conjugate_gradient_solve(a_mat_ph, b_mat_ph)\n      x_approx_value = sess.run(\n          x_approx,\n          feed_dict={a_mat_ph: a_mat_value, b_mat_ph: b_mat_value})\n      self.assertAllClose(x_exact_numpy, x_approx_value, rtol=1e-4, atol=1e-4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/linear_bandit_policy.py,28,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Linear Bandit Policy.\n\nLinUCB and Linear Thompson Sampling policies derive from this class.\n\nThis linear policy handles two main forms of feature input.\n1. A single global feature is received per time step. In this case, the policy\nmaintains an independent linear reward model for each arm.\n2. Apart from the global feature as in case 1, an arm-feature vector is\nreceived for each arm in every time step. In this case, only one model is\nmaintained by the policy, and the reward estimates are calculated for every arm\nby using their own per-arm features.\n\nThe above two cases can be triggered by setting the boolean parameter\n`accepts_per_arm_features` appropriately.\n\nA detailed explanation for the two above cases can be found in the paper\n""Thompson Sampling for Contextual Bandits with Linear Payoffs"",\nShipra Agrawal, Navin Goyal, ICML 2013\n(http://proceedings.mlr.press/v28/agrawal13.pdf), and its supplementary material\n(http://proceedings.mlr.press/v28/agrawal13-supp.pdf).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom enum import Enum\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.policies import linalg\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\n\ntfd = tfp.distributions\n\n\nclass ExplorationStrategy(Enum):\n  """"""Possible exploration strategies.""""""\n  optimistic = 1\n  sampling = 2\n\n\nclass LinearBanditPolicy(tf_policy.TFPolicy):\n  """"""Linear Bandit Policy to be used by LinUCB, LinTS and possibly others.""""""\n\n  def __init__(self,\n               action_spec,\n               cov_matrix,\n               data_vector,\n               num_samples,\n               time_step_spec=None,\n               exploration_strategy=ExplorationStrategy.optimistic,\n               alpha=1.0,\n               eig_vals=(),\n               eig_matrix=(),\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               emit_log_probability=False,\n               accepts_per_arm_features=False,\n               observation_and_action_constraint_splitter=None,\n               name=None):\n    """"""Initializes `LinearBanditPolicy`.\n\n    The `a` and `b` arguments may be either `Tensor`s or `tf.Variable`s.\n    If they are variables, then any assignements to those variables will be\n    reflected in the output of the policy.\n\n    Args:\n      action_spec: `TensorSpec` containing action specification.\n      cov_matrix: list of the covariance matrices A in the paper. If the policy\n        accepts per-arm features, the lenght of this list is 1, as there is only\n        one model. Otherwise, there is one A matrix per arm.\n      data_vector: list of the b vectors in the paper. The b vector is a\n        weighted sum of the observations, where the weight is the corresponding\n        reward. If the policy accepts per-arm features, this list should be of\n        length 1, as there only 1 reward model maintained. Otherwise, each arm\n        has its own vector b.\n      num_samples: list of number of samples per arm, unless the policy accepts\n      per-arm features, in which case this is just the number of samples seen.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      exploration_strategy: An Enum of type ExplortionStrategy. The strategy\n        used for choosing the actions to incorporate exploration. Currently\n        supported strategies are `optimistic` and `sampling`.\n      alpha: a float value used to scale the confidence intervals.\n      eig_vals: list of eigenvalues for each covariance matrix (one per arm,\n        unless the policy accepts per-arm features).\n      eig_matrix: list of eigenvectors for each covariance matrix (one per arm,\n        unless the policy accepts per-arm features).\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: Whether to emit log probabilities.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit policy and 2)\n        the mask. The mask should be a 0-1 `Tensor` of shape `[batch_size,\n        num_actions]`. This function should also work with a `TensorSpec` as\n        input, and should output `TensorSpec` objects for the observation and\n        mask.\n      name: The name of this policy.\n    """"""\n    if not isinstance(cov_matrix, (list, tuple)):\n      raise ValueError(\'cov_matrix must be a list of matrices (Tensors).\')\n    self._cov_matrix = cov_matrix\n\n    if not isinstance(data_vector, (list, tuple)):\n      raise ValueError(\'data_vector must be a list of vectors (Tensors).\')\n    self._data_vector = data_vector\n\n    if not isinstance(num_samples, (list, tuple)):\n      raise ValueError(\'num_samples must be a list of vectors (Tensors).\')\n    self._num_samples = num_samples\n\n    if not isinstance(eig_vals, (list, tuple)):\n      raise ValueError(\'eig_vals must be a list of vectors (Tensors).\')\n    self._eig_vals = eig_vals\n\n    if not isinstance(eig_matrix, (list, tuple)):\n      raise ValueError(\'eig_matrix must be a list of vectors (Tensors).\')\n    self._eig_matrix = eig_matrix\n\n    self._exploration_strategy = exploration_strategy\n    if exploration_strategy == ExplorationStrategy.sampling:\n      # We do not have a way to calculate log probabilities for TS yet.\n      emit_log_probability = False\n\n    self._alpha = alpha\n    self._use_eigendecomp = False\n    if eig_matrix:\n      self._use_eigendecomp = True\n    self._tikhonov_weight = tikhonov_weight\n    self._add_bias = add_bias\n    self._accepts_per_arm_features = accepts_per_arm_features\n    if tf.nest.is_nested(action_spec):\n      raise ValueError(\'Nested `action_spec` is not supported.\')\n\n    self._num_actions = action_spec.maximum + 1\n    self._check_input_variables()\n    if observation_and_action_constraint_splitter is not None:\n      context_spec, _ = observation_and_action_constraint_splitter(\n          time_step_spec.observation)\n    else:\n      context_spec = time_step_spec.observation\n    (self._global_context_dim,\n     self._arm_context_dim) = bandit_spec_utils.get_context_dims_from_spec(\n         context_spec, accepts_per_arm_features)\n\n    if self._add_bias:\n      # The bias is added via a constant 1 feature.\n      self._global_context_dim += 1\n    self._overall_context_dim = self._global_context_dim + self._arm_context_dim\n    cov_matrix_dim = tf.compat.dimension_value(cov_matrix[0].shape[0])\n    if self._overall_context_dim != cov_matrix_dim:\n      raise ValueError(\'The dimension of matrix `cov_matrix` must match \'\n                       \'overall context dimension {}. \'\n                       \'Got {} for `cov_matrix`.\'.format(\n                           self._overall_context_dim, cov_matrix_dim))\n\n    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n    if self._overall_context_dim != data_vector_dim:\n      raise ValueError(\'The dimension of vector `data_vector` must match \'\n                       \'context  dimension {}. \'\n                       \'Got {} for `data_vector`.\'.format(\n                           self._overall_context_dim, data_vector_dim))\n\n    self._dtype = self._data_vector[0].dtype\n    self._emit_policy_info = emit_policy_info\n    info_spec = self._populate_policy_info_spec(\n        time_step_spec.observation, observation_and_action_constraint_splitter)\n\n    super(LinearBanditPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=info_spec,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n\n  def _variables(self):\n    all_vars = [self._cov_matrix,\n                self._data_vector,\n                self._num_samples,\n                self._eig_matrix,\n                self._eig_vals]\n    return [v for v in tf.nest.flatten(all_vars) if isinstance(v, tf.Variable)]\n\n  def _distribution(self, time_step, policy_state):\n    observation = time_step.observation\n    if self.observation_and_action_constraint_splitter is not None:\n      observation, _ = self.observation_and_action_constraint_splitter(\n          observation)\n    observation = tf.nest.map_structure(lambda o: tf.cast(o, dtype=self._dtype),\n                                        observation)\n    global_observation, arm_observations = self._split_observation(observation)\n\n    if self._add_bias:\n      # The bias is added via a constant 1 feature.\n      global_observation = tf.concat([\n          global_observation,\n          tf.ones([tf.shape(global_observation)[0], 1], dtype=self._dtype)\n      ],\n                                     axis=1)\n    # Check the shape of the observation matrix. The observations can be\n    # batched.\n    if not global_observation.shape.is_compatible_with(\n        [None, self._global_context_dim]):\n      raise ValueError(\n          \'Global observation shape is expected to be {}. Got {}.\'.format(\n              [None, self._global_context_dim],\n              global_observation.shape.as_list()))\n    global_observation = tf.reshape(global_observation,\n                                    [-1, self._global_context_dim])\n\n    est_rewards = []\n    confidence_intervals = []\n    for k in range(self._num_actions):\n      current_observation = self._get_current_observation(\n          global_observation, arm_observations, k)\n      model_index = policy_utilities.get_model_index(\n          k, self._accepts_per_arm_features)\n      if self._use_eigendecomp:\n        q_t_b = tf.matmul(\n            self._eig_matrix[model_index],\n            tf.linalg.matrix_transpose(current_observation),\n            transpose_a=True)\n        lambda_inv = tf.divide(\n            tf.ones_like(self._eig_vals[model_index]),\n            self._eig_vals[model_index] + self._tikhonov_weight)\n        a_inv_x = tf.matmul(self._eig_matrix[model_index],\n                            tf.einsum(\'j,jk->jk\', lambda_inv, q_t_b))\n      else:\n        a_inv_x = linalg.conjugate_gradient_solve(\n            self._cov_matrix[model_index] + self._tikhonov_weight *\n            tf.eye(self._overall_context_dim, dtype=self._dtype),\n            tf.linalg.matrix_transpose(current_observation))\n      est_mean_reward = tf.einsum(\'j,jk->k\', self._data_vector[model_index],\n                                  a_inv_x)\n      est_rewards.append(est_mean_reward)\n\n      ci = tf.reshape(\n          tf.linalg.tensor_diag_part(tf.matmul(current_observation, a_inv_x)),\n          [-1, 1])\n      confidence_intervals.append(ci)\n\n    if self._exploration_strategy == ExplorationStrategy.optimistic:\n      optimistic_estimates = [\n          tf.reshape(mean_reward, [-1, 1]) + self._alpha * tf.sqrt(confidence)\n          for mean_reward, confidence in zip(est_rewards, confidence_intervals)\n      ]\n      # Keeping the batch dimension during the squeeze, even if batch_size == 1.\n      rewards_for_argmax = tf.squeeze(\n          tf.stack(optimistic_estimates, axis=-1), axis=[1])\n    elif self._exploration_strategy == ExplorationStrategy.sampling:\n      mu_sampler = tfd.Normal(\n          loc=tf.stack(est_rewards, axis=-1),\n          scale=self._alpha *\n          tf.sqrt(tf.squeeze(tf.stack(confidence_intervals, axis=-1), axis=1)))\n      rewards_for_argmax = mu_sampler.sample()\n    else:\n      raise ValueError(\'Exploraton strategy %s not implemented.\' %\n                       self._exploration_strategy)\n\n    mask = policy_utilities.construct_mask_from_multiple_sources(\n        time_step.observation, self._observation_and_action_constraint_splitter,\n        (), self._num_actions)\n    if mask is not None:\n      chosen_actions = policy_utilities.masked_argmax(\n          rewards_for_argmax, mask, output_type=self._action_spec.dtype)\n    else:\n      chosen_actions = tf.argmax(\n          rewards_for_argmax, axis=-1, output_type=self._action_spec.dtype)\n\n    action_distributions = tfp.distributions.Deterministic(loc=chosen_actions)\n\n    policy_info = policy_utilities.populate_policy_info(\n        arm_observations, chosen_actions, rewards_for_argmax,\n        tf.stack(est_rewards, axis=-1), self._emit_policy_info,\n        self._accepts_per_arm_features)\n\n    return policy_step.PolicyStep(\n        action_distributions, policy_state, policy_info)\n\n  def _check_input_variables(self):\n    if len(self._cov_matrix) != len(self._data_vector):\n      raise ValueError(\'The size of list cov_matrix must match the size of \'\n                       \'list data_vector. Got {} for cov_matrix and {} \'\n                       \'for data_vector\'.format(\n                           len(self._cov_matrix), len((self._data_vector))))\n    if len(self._num_samples) != len(self._cov_matrix):\n      raise ValueError(\'The size of num_samples must match the size of \'\n                       \'list cov_matrix. Got {} for num_samples and {} \'\n                       \'for cov_matrix\'.format(\n                           len(self._num_samples), len((self._cov_matrix))))\n\n    if self._accepts_per_arm_features:\n      if len(self._cov_matrix) != 1:\n        raise ValueError(\n            \'If the policy accepts per-arm features, the length of `cov_matrix`\'\n            \' list must be 1. Got {} instead.\'.format(len(self._cov_matrix)))\n    else:\n      if self._num_actions != len(self._cov_matrix):\n        raise ValueError(\n            \'The number of elements in `cov_matrix` ({}) must match \'\n            \'the number of actions derived from `action_spec` ({}).\'.format(\n                len(self._cov_matrix), self._num_actions))\n\n  def _populate_policy_info_spec(self, observation_spec,\n                                 observation_and_action_constraint_splitter):\n    predicted_rewards_mean = ()\n    if (policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN in\n        self._emit_policy_info):\n      predicted_rewards_mean = tensor_spec.TensorSpec([self._num_actions],\n                                                      dtype=self._dtype)\n    predicted_rewards_sampled = ()\n    if (policy_utilities.InfoFields.PREDICTED_REWARDS_SAMPLED in\n        self._emit_policy_info):\n      predicted_rewards_sampled = tensor_spec.TensorSpec([self._num_actions],\n                                                         dtype=self._dtype)\n    if self._accepts_per_arm_features:\n      # The features for the chosen arm is saved to policy_info.\n      chosen_arm_features_info = (\n          policy_utilities.create_chosen_arm_features_info_spec(\n              observation_spec, observation_and_action_constraint_splitter))\n      info_spec = policy_utilities.PerArmPolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          predicted_rewards_sampled=predicted_rewards_sampled,\n          chosen_arm_features=chosen_arm_features_info)\n    else:\n      info_spec = policy_utilities.PolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          predicted_rewards_sampled=predicted_rewards_sampled)\n    return info_spec\n\n  def _get_current_observation(self, global_observation, arm_observations,\n                               arm_index):\n    """"""Helper function to construct the observation for a specific arm.\n\n    This function constructs the observation depending if the policy accepts\n    per-arm features or not. If not, it simply returns the original observation.\n    If yes, it concatenates the global observation with the observation of the\n    arm indexed by `arm_index`.\n\n    Args:\n      global_observation: A tensor of shape `[batch_size, global_context_dim]`.\n        The global part of the observation.\n      arm_observations: A tensor of shape `[batch_size, num_actions,\n        arm_context_dim]`. The arm part of the observation, for all arms. If the\n        policy does not accept per-arm features, this paramater is unused.\n      arm_index: (int) The arm for which the observations to be returned.\n\n    Returns:\n      A tensor of shape `[batch_size, overall_context_dim]`, containing the\n      observation for arm `arm_index`.\n    """"""\n    if self._accepts_per_arm_features:\n      current_arm = arm_observations[:, arm_index, :]\n      current_observation = tf.concat([global_observation, current_arm],\n                                      axis=-1)\n      return current_observation\n    else:\n      return global_observation\n\n  def _split_observation(self, observation):\n    """"""Splits the observation into global and arm observations.""""""\n    if self._accepts_per_arm_features:\n      global_observation = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n      arm_observations = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n      if not arm_observations.shape.is_compatible_with(\n          [None, self._num_actions, self._arm_context_dim]):\n        raise ValueError(\n            \'Arm observation shape is expected to be {}. Got {}.\'.format(\n                [None, self._num_actions, self._arm_context_dim],\n                arm_observations.shape.as_list()))\n    else:\n      global_observation = observation\n      arm_observations = None\n    return global_observation, arm_observations\n'"
tf_agents/bandits/policies/linear_bandit_policy_test.py,65,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.policies.linear_bandit_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.policies import linear_bandit_policy as linear_policy\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n_POLICY_VARIABLES_OFFSET = 10.0\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'batch1UCB\',\n          \'batch_size\': 1,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.optimistic,\n      }, {\n          \'testcase_name\': \'batch4UCB\',\n          \'batch_size\': 4,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.optimistic,\n      })\n\n\ndef test_cases_with_strategy():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'batch1UCB\',\n          \'batch_size\': 1,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.optimistic,\n      }, {\n          \'testcase_name\': \'batch4UCB\',\n          \'batch_size\': 4,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.optimistic,\n      }, {\n          \'testcase_name\': \'batch1TS\',\n          \'batch_size\': 1,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.sampling,\n      }, {\n          \'testcase_name\': \'batch4TS\',\n          \'batch_size\': 4,\n          \'exploration_strategy\': linear_policy.ExplorationStrategy.sampling,\n      })\n\n\ndef test_cases_with_decomposition():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'batch1\',\n          \'batch_size\': 1,\n          \'use_decomposition\': False\n      }, {\n          \'testcase_name\': \'batch4\',\n          \'batch_size\': 4,\n          \'use_decomposition\': True\n      })\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass LinearBanditPolicyTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(LinearBanditPolicyTest, self).setUp()\n    self._obs_dim = 2\n    self._num_actions = 5\n    self._obs_spec = tensor_spec.TensorSpec([self._obs_dim], tf.float32)\n    self._obs_spec_with_mask = (tensor_spec.TensorSpec([self._obs_dim],\n                                                       tf.float32),\n                                tensor_spec.TensorSpec([self._num_actions],\n                                                       tf.int32))\n    self._per_arm_obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        self._obs_dim, 4, self._num_actions, add_num_actions_feature=True)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._time_step_spec_with_mask = ts.time_step_spec(self._obs_spec_with_mask)\n    self._per_arm_time_step_spec = ts.time_step_spec(self._per_arm_obs_spec)\n    self._alpha = 1.0\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n\n  @property\n  def _a(self):\n    a_for_one_arm = tf.constant([[4, 1], [1, 4]], dtype=tf.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  @property\n  def _a_numpy(self):\n    a_for_one_arm = np.array([[4, 1], [1, 4]], dtype=np.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  @property\n  def _b(self):\n    return [tf.constant([r, r], dtype=tf.float32)\n            for r in range(self._num_actions)]\n\n  @property\n  def _b_numpy(self):\n    return [np.array([r, r], dtype=np.float32)\n            for r in range(self._num_actions)]\n\n  @property\n  def _num_samples_per_arm(self):\n    a_for_one_arm = tf.constant([1], dtype=tf.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  @property\n  def _num_samples_per_arm_numpy(self):\n    return np.ones(self._num_actions)\n\n  def _time_step_batch(self, batch_size):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        tf.constant(np.array(range(batch_size * self._obs_dim)),\n                    dtype=tf.float32, shape=[batch_size, self._obs_dim],\n                    name=\'observation\'))\n\n  def _per_arm_time_step_batch(self, batch_size):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        {\n            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n                tf.constant(\n                    np.array(range(batch_size * self._obs_dim)),\n                    dtype=tf.float32,\n                    shape=[batch_size, self._obs_dim],\n                    name=\'observation\'),\n            bandit_spec_utils.PER_ARM_FEATURE_KEY:\n                tf.constant(\n                    np.array(range(batch_size * self._num_actions * 4)),\n                    dtype=tf.float32,\n                    shape=[batch_size, self._num_actions, 4],\n                    name=\'observation\'),\n            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n                tf.ones([batch_size], dtype=tf.int32) * 2\n\n        })\n\n  def _time_step_batch_with_mask(self, batch_size):\n    no_mask_observation = tf.constant(\n        np.array(range(batch_size * self._obs_dim)),\n        dtype=tf.float32,\n        shape=[batch_size, self._obs_dim])\n    mask = tf.eye(batch_size, num_columns=self._num_actions, dtype=tf.int32)\n    observation = (no_mask_observation, mask)\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        observation)\n\n  @parameterized.parameters([\n      linear_policy.ExplorationStrategy.optimistic,\n      linear_policy.ExplorationStrategy.sampling\n  ])\n  def testBuild(self, exploration_strategy):\n    policy = linear_policy.LinearBanditPolicy(self._action_spec, self._a,\n                                              self._b,\n                                              self._num_samples_per_arm,\n                                              self._time_step_spec,\n                                              exploration_strategy)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n\n  @test_cases_with_strategy()\n  def testObservationShapeMismatch(self, batch_size, exploration_strategy):\n    policy = linear_policy.LinearBanditPolicy(self._action_spec, self._a,\n                                              self._b,\n                                              self._num_samples_per_arm,\n                                              self._time_step_spec,\n                                              exploration_strategy)\n\n    current_time_step = ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        tf.constant(\n            np.array(range(batch_size * (self._obs_dim + 1))),\n            dtype=tf.float32,\n            shape=[batch_size, self._obs_dim + 1],\n            name=\'observation\'))\n    with self.assertRaisesRegex(\n        ValueError, r\'Global observation shape is expected to be \\[None, 2\\].\'\n        r\' Got \\[%d, 3\\].\' % batch_size):\n      policy.action(current_time_step)\n\n  @test_cases_with_strategy()\n  def testActionBatch(self, batch_size, exploration_strategy):\n    policy = linear_policy.LinearBanditPolicy(self._action_spec, self._a,\n                                              self._b,\n                                              self._num_samples_per_arm,\n                                              self._time_step_spec,\n                                              exploration_strategy)\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    actions_ = self.evaluate(action_step.action)\n    self.assertAllGreaterEqual(actions_, self._action_spec.minimum)\n    self.assertAllLessEqual(actions_, self._action_spec.maximum)\n\n  @test_cases_with_strategy()\n  def testActionBatchWithBias(self, batch_size, exploration_strategy):\n    a = [tf.constant([[4, 1, 2], [1, 5, 3], [2, 3, 6]], dtype=tf.float32)\n        ] * self._num_actions\n    b = [\n        tf.constant([r, r, r], dtype=tf.float32)\n        for r in range(self._num_actions)\n    ]\n    policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        a,\n        b,\n        self._num_samples_per_arm,\n        self._time_step_spec,\n        exploration_strategy,\n        add_bias=True)\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    actions_ = self.evaluate(action_step.action)\n    self.assertAllGreaterEqual(actions_, self._action_spec.minimum)\n    self.assertAllLessEqual(actions_, self._action_spec.maximum)\n\n  @test_cases_with_strategy()\n  def testActionBatchWithMask(self, batch_size, exploration_strategy):\n\n    def split_fn(obs):\n      return obs[0], obs[1]\n\n    policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        self._a,\n        self._b,\n        self._num_samples_per_arm,\n        self._time_step_spec_with_mask,\n        exploration_strategy,\n        observation_and_action_constraint_splitter=split_fn)\n\n    action_step = policy.action(\n        self._time_step_batch_with_mask(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    actions_ = self.evaluate(action_step.action)\n    self.assertAllEqual(actions_, range(batch_size))\n\n  @test_cases()\n  def testActionBatchWithVariablesAndPolicyUpdate(self, batch_size,\n                                                  exploration_strategy):\n    a_list = []\n    a_new_list = []\n    b_list = []\n    b_new_list = []\n    num_samples_list = []\n    num_samples_new_list = []\n    for k in range(1, self._num_actions + 1):\n      a_initial_value = tf.constant(\n          [[2 * k + 1, k + 1], [k + 1, 2 * k+1]],\n          dtype=tf.float32)\n      a_for_one_arm = tf.compat.v2.Variable(a_initial_value)\n      a_list.append(a_for_one_arm)\n      b_initial_value = tf.constant([k, k], dtype=tf.float32)\n      b_for_one_arm = tf.compat.v2.Variable(b_initial_value)\n      b_list.append(b_for_one_arm)\n      num_samples_initial_value = tf.constant([1], dtype=tf.float32)\n      num_samples_for_one_arm = tf.compat.v2.Variable(num_samples_initial_value)\n      num_samples_list.append(num_samples_for_one_arm)\n\n      # Variables for the new policy (they differ by an offset).\n      a_new_for_one_arm = tf.compat.v2.Variable(\n          a_initial_value + _POLICY_VARIABLES_OFFSET)\n      a_new_list.append(a_new_for_one_arm)\n      b_new_for_one_arm = tf.compat.v2.Variable(\n          b_initial_value + _POLICY_VARIABLES_OFFSET)\n      b_new_list.append(b_new_for_one_arm)\n      num_samples_for_one_arm_new = tf.compat.v2.Variable(\n          num_samples_initial_value + _POLICY_VARIABLES_OFFSET)\n      num_samples_new_list.append(num_samples_for_one_arm_new)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n\n    policy = linear_policy.LinearBanditPolicy(self._action_spec, a_list, b_list,\n                                              num_samples_list,\n                                              self._time_step_spec,\n                                              exploration_strategy)\n    self.assertLen(policy.variables(), 3 * self._num_actions)\n\n    new_policy = linear_policy.LinearBanditPolicy(self._action_spec, a_new_list,\n                                                  b_new_list,\n                                                  num_samples_new_list,\n                                                  self._time_step_spec,\n                                                  exploration_strategy)\n    self.assertLen(new_policy.variables(), 3 * self._num_actions)\n\n    self.evaluate(new_policy.update(policy))\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    new_action_step = new_policy.action(\n        self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n    actions_, new_actions_ = self.evaluate(\n        [action_step.action, new_action_step.action])\n    self.assertAllEqual(actions_, new_actions_)\n\n  @test_cases()\n  def testPerArmActionBatchWithVariablesAndPolicyUpdate(self, batch_size,\n                                                        exploration_strategy):\n    a_value = tf.reshape(tf.range(36, dtype=tf.float32), shape=[6, 6])\n    a_list = [tf.compat.v2.Variable(a_value)]\n    a_new_list = [tf.compat.v2.Variable(a_value + _POLICY_VARIABLES_OFFSET)]\n    b_value = tf.constant([2, 2, 2, 2, 2, 2], dtype=tf.float32)\n    b_list = [tf.compat.v2.Variable(b_value)]\n    b_new_list = [tf.compat.v2.Variable(b_value + _POLICY_VARIABLES_OFFSET)]\n    num_samples_list = [\n        tf.compat.v2.Variable(tf.constant([1], dtype=tf.float32))\n    ]\n    num_samples_new_list = [\n        tf.compat.v2.Variable(\n            tf.constant([1 + _POLICY_VARIABLES_OFFSET], dtype=tf.float32))\n    ]\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        a_list,\n        b_list,\n        num_samples_list,\n        self._per_arm_time_step_spec,\n        exploration_strategy,\n        accepts_per_arm_features=True)\n    self.assertLen(policy.variables(), 3)\n\n    new_policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        a_new_list,\n        b_new_list,\n        num_samples_new_list,\n        self._per_arm_time_step_spec,\n        exploration_strategy,\n        accepts_per_arm_features=True)\n    self.assertLen(new_policy.variables(), 3)\n\n    self.evaluate(new_policy.update(policy))\n\n    step_batch = self._per_arm_time_step_batch(batch_size=batch_size)\n    action_step = policy.action(step_batch)\n    new_action_step = new_policy.action(step_batch)\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n    actions_, new_actions_, info = self.evaluate(\n        [action_step.action, new_action_step.action, action_step.info])\n    self.assertAllEqual(actions_, new_actions_)\n    arm_obs = step_batch.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n    first_action = actions_[0]\n    first_arm_features = arm_obs[0]\n    self.assertAllEqual(info.chosen_arm_features[0],\n                        first_arm_features[first_action])\n\n  @test_cases_with_decomposition()\n  def testComparisonWithNumpy(self, batch_size, use_decomposition=False):\n    eig_matrix_list = ()\n    eig_vals_list = ()\n    if use_decomposition:\n      eig_vals_one_arm, eig_matrix_one_arm = tf.linalg.eigh(self._a[0])\n      eig_vals_list = [eig_vals_one_arm] * self._num_actions\n      eig_matrix_list = [eig_matrix_one_arm] * self._num_actions\n\n    policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        self._a,\n        self._b,\n        self._num_samples_per_arm,\n        self._time_step_spec,\n        eig_vals=eig_vals_list,\n        eig_matrix=eig_matrix_list)\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    actions_ = self.evaluate(action_step.action)\n\n    observation_numpy = np.array(\n        range(batch_size * self._obs_dim), dtype=np.float32).reshape(\n            [batch_size, self._obs_dim])\n\n    p_values = []\n    for k in range(self._num_actions):\n      a_inv = np.linalg.inv(self._a_numpy[k] + np.eye(self._obs_dim))\n      theta = np.matmul(\n          a_inv, self._b_numpy[k].reshape([self._obs_dim, 1]))\n      confidence_intervals = np.sqrt(np.diag(\n          np.matmul(observation_numpy,\n                    np.matmul(a_inv, np.transpose(observation_numpy)))))\n      p_value = (np.matmul(observation_numpy, theta) +\n                 self._alpha * confidence_intervals.reshape([-1, 1]))\n      p_values.append(p_value)\n\n    actions_numpy = np.argmax(np.stack(p_values, axis=-1), axis=-1).reshape(\n        [batch_size])\n    self.assertAllEqual(actions_.reshape([batch_size]), actions_numpy)\n\n  @test_cases_with_strategy()\n  def testPredictedRewards(self, batch_size, exploration_strategy):\n    policy = linear_policy.LinearBanditPolicy(\n        self._action_spec,\n        self._a,\n        self._b,\n        self._num_samples_per_arm,\n        self._time_step_spec,\n        exploration_strategy,\n        emit_policy_info=(policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,))\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n\n    observation_numpy = np.array(\n        range(batch_size * self._obs_dim), dtype=np.float32).reshape(\n            [batch_size, self._obs_dim])\n\n    p_values = []\n    predicted_rewards_expected = []\n    for k in range(self._num_actions):\n      a_inv = np.linalg.inv(self._a_numpy[k] + np.eye(self._obs_dim))\n      theta = np.matmul(\n          a_inv, self._b_numpy[k].reshape([self._obs_dim, 1]))\n      confidence_intervals = np.sqrt(np.diag(\n          np.matmul(observation_numpy,\n                    np.matmul(a_inv, np.transpose(observation_numpy)))))\n      est_mean_reward = np.matmul(observation_numpy, theta)\n      predicted_rewards_expected.append(est_mean_reward)\n      p_value = (est_mean_reward +\n                 self._alpha * confidence_intervals.reshape([-1, 1]))\n      p_values.append(p_value)\n\n    predicted_rewards_expected_array = np.stack(\n        predicted_rewards_expected, axis=-1).reshape(\n            batch_size, self._num_actions)\n    p_info = self.evaluate(action_step.info)\n    self.assertAllClose(p_info.predicted_rewards_mean,\n                        predicted_rewards_expected_array)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/linear_thompson_sampling_policy.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Linear Thompson Sampling Policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tf_agents.bandits.policies import linear_bandit_policy as lin_policy\n\n\nclass LinearThompsonSamplingPolicy(lin_policy.LinearBanditPolicy):\n  """"""Linear Thompson Sampling Policy.\n\n  Implements the Linear Thompson Sampling Policy from the following paper:\n  ""Thompson Sampling for Contextual Bandits with Linear Payoffs"",\n  Shipra Agrawal, Navin Goyal, ICML 2013. The actual algorithm implemented is\n  `Algorithm 3` from the supplementary material of the paper from\n  `http://proceedings.mlr.press/v28/agrawal13-supp.pdf`.\n\n  In a nutshell, the algorithm estimates reward distributions based on\n  parameters `B_inv` and `f` for every action. Then for each\n  action we sample a reward and take the argmax.\n  """"""\n\n  def __init__(self,\n               action_spec,\n               cov_matrix,\n               data_vector,\n               num_samples,\n               time_step_spec=None,\n               alpha=1.0,\n               eig_vals=(),\n               eig_matrix=(),\n               tikhonov_weight=1.0,\n               add_bias=False,\n               emit_policy_info=(),\n               observation_and_action_constraint_splitter=None,\n               name=None):\n    """"""Initializes `LinearThompsonSamplingPolicy`.\n\n    The `a` and `b` arguments may be either `Tensor`s or `tf.Variable`s.\n    If they are variables, then any assignments to those variables will be\n    reflected in the output of the policy.\n\n    Args:\n      action_spec: `TensorSpec` containing action specification.\n      cov_matrix: list of the covariance matrices A in the paper. There exists\n        one A matrix per arm.\n      data_vector: list of the b vectors in the paper. The b vector is a\n        weighted sum of the observations, where the weight is the corresponding\n        reward. Each arm has its own vector b.\n      num_samples: list of number of samples per arm.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      alpha: a float value used to scale the confidence intervals.\n      eig_vals: list of eigenvalues for each covariance matrix (one per arm).\n      eig_matrix: list of eigenvectors for each covariance matrix (one per arm).\n      tikhonov_weight: (float) tikhonov regularization term.\n      add_bias: If true, a bias term will be added to the linear reward\n        estimation.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit policy and 2)\n        the mask. The mask should be a 0-1 `Tensor` of shape\n        `[batch_size, num_actions]`. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      name: The name of this policy.\n    """"""\n    super(LinearThompsonSamplingPolicy, self).__init__(\n        action_spec=action_spec,\n        cov_matrix=cov_matrix,\n        data_vector=data_vector,\n        num_samples=num_samples,\n        time_step_spec=time_step_spec,\n        exploration_strategy=lin_policy.ExplorationStrategy.sampling,\n        alpha=alpha,\n        eig_vals=eig_vals,\n        eig_matrix=eig_matrix,\n        tikhonov_weight=tikhonov_weight,\n        add_bias=add_bias,\n        emit_policy_info=emit_policy_info,\n        emit_log_probability=False,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        name=name)\n'"
tf_agents/bandits/policies/mixture_policy.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""A policy class that chooses from a set of policies to get the actions from.\n\nThis mixture policy takes a list of policies and will randomly choose one of\nthem for every observation. The distribution is defined by the\n`mixture_distribution`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.utils import nest_utils\n\ntfd = tfp.distributions\n\n\nMIXTURE_AGENT_ID = \'mixture_agent_id\'\nSUBPOLICY_INFO = \'subpolicy_info\'\n\n\n@gin.configurable\nclass MixturePolicy(tf_policy.TFPolicy):\n  """"""A policy that chooses from a set of policies to decide the action.""""""\n\n  def __init__(self, mixture_distribution, policies, name=None):\n    """"""Initializes an instance of `MixturePolicy`.\n\n    Args:\n      mixture_distribution: A `tfd.Categorical` distribution on the domain `[0,\n        len(policies) -1]`. This distribution is used by the mixture policy to\n        choose which policy to listen to.\n      policies: List of TF Policies. These are the policies that the mixture\n        policy chooses from in every time step.\n      name: The name of this instance of `MixturePolicy`.\n    """"""\n    self._policies = policies\n    if not isinstance(mixture_distribution, tfd.Categorical):\n      raise TypeError(\n          \'mixture distribution must be an instance of `tfd.Categorical`.\')\n    self._mixture_distribution = mixture_distribution\n    action_spec = policies[0].action_spec\n    time_step_spec = policies[0].time_step_spec\n    for policy in policies[1:]:\n      assert action_spec == policy.action_spec, \'Inconsistent action specs.\'\n      assert time_step_spec == policy.time_step_spec, (\'Inconsistent time step \'\n                                                       \'specs.\')\n      assert policies[0].info_spec == policy.info_spec, (\'Inconsistent info \'\n                                                         \'specs.\')\n\n    info_spec = {\n        MIXTURE_AGENT_ID:\n            tensor_spec.BoundedTensorSpec(\n                shape=(), dtype=tf.int32, minimum=0, maximum=len(policies) - 1),\n        SUBPOLICY_INFO:\n            policies[0].info_spec\n    }\n\n    super(MixturePolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec=info_spec,\n        name=name)\n\n  def _variables(self):\n    variables = sum([p.variables() for p in self._policies], [])\n    variables.extend(self._mixture_distribution.variables)\n    return variables\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'_distribution is not implemented for this policy.\')\n\n  def _action(self, time_step, policy_state, seed=None):\n    first_obs = tf.nest.flatten(time_step.observation)[0]\n    batch_size = tf.compat.dimension_value(\n        first_obs.shape[0]) or tf.shape(first_obs)[0]\n    policy_choice = self._mixture_distribution.sample(batch_size)\n    policy_steps = [\n        policy.action(time_step, policy_state) for policy in self._policies\n    ]\n    policy_actions = nest_utils.stack_nested_tensors(\n        [step.action for step in policy_steps], axis=-1)\n    policy_infos = nest_utils.stack_nested_tensors(\n        [step.info for step in policy_steps], axis=-1)\n\n    expanded_choice = tf.expand_dims(policy_choice, axis=-1)\n    mixture_action = tf.nest.map_structure(\n        lambda t: tf.gather(t, policy_choice, batch_dims=1), policy_actions)\n\n    expanded_mixture_info = tf.nest.map_structure(\n        lambda t: tf.gather(t, expanded_choice, batch_dims=1), policy_infos)\n    mixture_info = tf.nest.map_structure(lambda t: tf.squeeze(t, axis=1),\n                                         expanded_mixture_info)\n    return policy_step.PolicyStep(mixture_action, policy_state, {\n        MIXTURE_AGENT_ID: policy_choice,\n        SUBPOLICY_INFO: mixture_info\n    })\n'"
tf_agents/bandits/policies/mixture_policy_test.py,41,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.policies.mixture_policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.policies import mixture_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\n\ntfd = tfp.distributions\n\n\nclass ConstantPolicy(tf_policy.TFPolicy):\n  """"""A policy that outputs a constant action, for testing purposes.""""""\n\n  def __init__(self, action_spec, time_step_spec, action):\n    self._constant_action = action\n    super(ConstantPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        info_spec={\'a\': tensor_spec.TensorSpec(shape=(), dtype=tf.int32)})\n\n  def _variables(self):\n    return []\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'This policy outputs an action and not a distribution.\')\n\n  def _action(self, time_step, policy_state, seed=None):\n    batch_size = tf.compat.dimension_value(tf.shape(time_step.observation)[0])\n    return policy_step.PolicyStep(\n        tf.fill([batch_size], self._constant_action), policy_state,\n        {\'a\': tf.range(batch_size, dtype=tf.int32)})\n\n\nclass MixturePolicyTest(test_utils.TestCase):\n\n  def testMixturePolicyInconsistentSpecs(self):\n    context_dim = 11\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(), dtype=tf.int32, minimum=0, maximum=9, name=\'action\')\n    sub_policies = [\n        ConstantPolicy(action_spec, time_step_spec, i) for i in range(9)\n    ]\n    wrong_obs_spec = tensor_spec.TensorSpec([context_dim + 1], tf.float32)\n    wrong_time_step_spec = ts.time_step_spec(wrong_obs_spec)\n    wrong_policy = ConstantPolicy(action_spec, wrong_time_step_spec, 9)\n    sub_policies.append(wrong_policy)\n    weights = [0, 0, 0.2, 0, 0, -0.3, 0, 0, 0.5, 0]\n    dist = tfd.Categorical(probs=weights)\n    with self.assertRaisesRegexp(AssertionError,\n                                 \'Inconsistent time step specs\'):\n      mixture_policy.MixturePolicy(dist, sub_policies)\n\n  def testMixturePolicyChoices(self):\n    context_dim = 34\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(), dtype=tf.int32, minimum=0, maximum=9, name=\'action\')\n    sub_policies = [\n        ConstantPolicy(action_spec, time_step_spec, i) for i in range(10)\n    ]\n    weights = [0, 0, 0.2, 0, 0, 0.3, 0, 0, 0.5, 0]\n    dist = tfd.Categorical(probs=weights)\n    policy = mixture_policy.MixturePolicy(dist, sub_policies)\n    batch_size = 15\n    time_step = ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        tf.constant(\n            list(range(batch_size * context_dim)),\n            dtype=tf.float32,\n            shape=[batch_size, context_dim],\n            name=\'observation\'))\n    action_step = policy.action(time_step)\n    actions, infos = self.evaluate([action_step.action, action_step.info])\n    tf.nest.assert_same_structure(policy.info_spec, infos)\n    self.assertAllEqual(actions.shape, [batch_size])\n    self.assertAllInSet(actions, [2, 5, 8])\n\n  def testMixturePolicyDynamicBatchSize(self):\n    context_dim = 35\n    observation_spec = tensor_spec.TensorSpec([context_dim], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(), dtype=tf.int32, minimum=0, maximum=9, name=\'action\')\n    sub_policies = [\n        ConstantPolicy(action_spec, time_step_spec, i) for i in range(10)\n    ]\n    weights = [0, 0, 0.2, 0, 0, 0.3, 0, 0, 0.5, 0]\n    dist = tfd.Categorical(probs=weights)\n\n    policy = mixture_policy.MixturePolicy(dist, sub_policies)\n    batch_size = tf.random.uniform(\n        shape=(), minval=10, maxval=15, dtype=tf.int32)\n    time_step = ts.TimeStep(\n        tf.fill(\n            tf.expand_dims(batch_size, axis=0),\n            ts.StepType.FIRST,\n            name=\'step_type\'),\n        tf.zeros(shape=[batch_size], dtype=tf.float32, name=\'reward\'),\n        tf.ones(shape=[batch_size], dtype=tf.float32, name=\'discount\'),\n        tf.reshape(\n            tf.range(\n                tf.cast(batch_size * context_dim, dtype=tf.float32),\n                dtype=tf.float32),\n            shape=[-1, context_dim],\n            name=\'observation\'))\n    action_step = policy.action(time_step)\n    actions, bsize = self.evaluate([action_step.action, batch_size])\n    self.assertAllEqual(actions.shape, [bsize])\n    self.assertAllInSet(actions, [2, 5, 8])\n\n    train_step = tf.compat.v1.train.get_or_create_global_step()\n    saver = policy_saver.PolicySaver(policy, train_step=train_step)\n    location = os.path.join(self.get_temp_dir(), \'saved_policy\')\n    if not tf.executing_eagerly():\n      with self.cached_session():\n        self.evaluate(tf.compat.v1.global_variables_initializer())\n        saver.save(location)\n    else:\n      saver.save(location)\n    loaded_policy = tf.compat.v2.saved_model.load(location)\n    new_batch_size = 3\n    new_time_step = ts.TimeStep(\n        tf.fill(\n            tf.expand_dims(new_batch_size, axis=0),\n            ts.StepType.FIRST,\n            name=\'step_type\'),\n        tf.zeros(shape=[new_batch_size], dtype=tf.float32, name=\'reward\'),\n        tf.ones(shape=[new_batch_size], dtype=tf.float32, name=\'discount\'),\n        tf.reshape(\n            tf.range(\n                tf.cast(new_batch_size * context_dim, dtype=tf.float32),\n                dtype=tf.float32),\n            shape=[-1, context_dim],\n            name=\'observation\'))\n    new_action = self.evaluate(loaded_policy.action(new_time_step).action)\n    self.assertAllEqual(new_action.shape, [new_batch_size])\n    self.assertAllInSet(new_action, [2, 5, 8])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/neural_linucb_policy.py,38,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Neural + LinUCB Policy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.policies import linalg\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.distributions import masked\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\n\ntfd = tfp.distributions\n\n\nclass NeuralLinUCBPolicy(tf_policy.TFPolicy):\n  """"""Neural LinUCB Policy.\n\n  Applies LinUCB on top of an encoding network.\n  Since LinUCB is a linear method, the encoding network is used to capture the\n  non-linear relationship between the context features and the expected rewards.\n  The policy starts with exploration based on epsilon greedy and then switches\n  to LinUCB for exploring more efficiently.\n\n  This policy supports both the global-only observation model and the global and\n  per-arm model:\n\n  -- In the global-only case, there is one single observation per\n     time step, and every arm has its own reward estimation function.\n  -- In the per-arm case, all arms receive individual observations, and the\n     reward estimation function is identical for all arms.\n\n  Reference:\n  Carlos Riquelme, George Tucker, Jasper Snoek,\n  `Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep\n  Networks for Thompson Sampling`, ICLR 2018.\n  """"""\n\n  def __init__(self,\n               encoding_network,\n               encoding_dim,\n               reward_layer,\n               epsilon_greedy,\n               actions_from_reward_layer,\n               cov_matrix,\n               data_vector,\n               num_samples,\n               time_step_spec=None,\n               alpha=1.0,\n               emit_policy_info=(),\n               emit_log_probability=False,\n               accepts_per_arm_features=False,\n               distributed_use_reward_layer=False,\n               observation_and_action_constraint_splitter=None,\n               name=None):\n    """"""Initializes `NeuralLinUCBPolicy`.\n\n    Args:\n      encoding_network: network that encodes the observations.\n      encoding_dim: (int) dimension of the encoded observations.\n      reward_layer: final layer that predicts the expected reward per arm. In\n        case the policy accepts per-arm features, the output of this layer has\n        to be a scalar. This is because in the per-arm case, all encoded\n        observations have to go through the same computation to get the reward\n        estimates. The `num_actions` dimension of the encoded observation is\n        treated as a batch dimension in the reward layer.\n      epsilon_greedy: (float) representing the probability of choosing a random\n        action instead of the greedy action.\n      actions_from_reward_layer: (boolean variable) whether to get actions from\n        the reward layer or from LinUCB.\n      cov_matrix: list of the covariance matrices. There exists one covariance\n        matrix per arm, unless the policy accepts per-arm features, in which\n        case this list must have a single element.\n      data_vector: list of the data vectors. A data vector is a weighted sum\n        of the observations, where the weight is the corresponding reward. Each\n        arm has its own data vector, unless the policy accepts per-arm features,\n        in which case this list must have a single element.\n      num_samples: list of number of samples per arm. If the policy accepts per-\n        arm features, this is a single-element list counting the number of\n        steps.\n      time_step_spec: A `TimeStep` spec of the expected time_steps.\n      alpha: (float) non-negative weight multiplying the confidence intervals.\n      emit_policy_info: (tuple of strings) what side information we want to get\n        as part of the policy info. Allowed values can be found in\n        `policy_utilities.PolicyInfo`.\n      emit_log_probability: (bool) whether to emit log probabilities.\n      accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n        features.\n      distributed_use_reward_layer: (bool) Whether to pick the actions using\n        the network or use LinUCB. This applies only in distributed training\n        setting and has a similar role to the `actions_from_reward_layer`\n        mentioned above.\n      observation_and_action_constraint_splitter: A function used for masking\n        valid/invalid actions with each state of the environment. The function\n        takes in a full observation and returns a tuple consisting of 1) the\n        part of the observation intended as input to the bandit policy and 2)\n        the mask. The mask should be a 0-1 `Tensor` of shape\n        `[batch_size, num_actions]`. This function should also work with a\n        `TensorSpec` as input, and should output `TensorSpec` objects for the\n        observation and mask.\n      name: The name of this policy.\n    """"""\n    encoding_network.create_variables()\n    self._encoding_network = encoding_network\n    self._reward_layer = reward_layer\n    self._encoding_dim = encoding_dim\n\n    if accepts_per_arm_features and reward_layer.units != 1:\n      raise ValueError(\'The output dimension of the reward layer must be 1, got\'\n                       \' {}\'.format(reward_layer.units))\n\n    if not isinstance(cov_matrix, (list, tuple)):\n      raise ValueError(\'cov_matrix must be a list of matrices (Tensors).\')\n    self._cov_matrix = cov_matrix\n\n    if not isinstance(data_vector, (list, tuple)):\n      raise ValueError(\'data_vector must be a list of vectors (Tensors).\')\n    self._data_vector = data_vector\n\n    if not isinstance(num_samples, (list, tuple)):\n      raise ValueError(\'num_samples must be a list of vectors (Tensors).\')\n    self._num_samples = num_samples\n\n    self._alpha = alpha\n    self._actions_from_reward_layer = actions_from_reward_layer\n    self._epsilon_greedy = epsilon_greedy\n    self._dtype = self._data_vector[0].dtype\n    self._distributed_use_reward_layer = distributed_use_reward_layer\n\n    if len(cov_matrix) != len(data_vector):\n      raise ValueError(\'The size of list cov_matrix must match the size of \'\n                       \'list data_vector. Got {} for cov_matrix and {} \'\n                       \'for data_vector\'.format(\n                           len(self._cov_matrix), len((data_vector))))\n    if len(num_samples) != len(cov_matrix):\n      raise ValueError(\'The size of num_samples must match the size of \'\n                       \'list cov_matrix. Got {} for num_samples and {} \'\n                       \'for cov_matrix\'.format(\n                           len(self._num_samples), len((cov_matrix))))\n\n    self._accepts_per_arm_features = accepts_per_arm_features\n    if observation_and_action_constraint_splitter is not None:\n      context_spec, _ = observation_and_action_constraint_splitter(\n          time_step_spec.observation)\n    else:\n      context_spec = time_step_spec.observation\n    if accepts_per_arm_features:\n      self._num_actions = tf.nest.flatten(context_spec[\n          bandit_spec_utils.PER_ARM_FEATURE_KEY])[0].shape.as_list()[0]\n      self._num_models = 1\n    else:\n      self._num_actions = len(cov_matrix)\n      self._num_models = self._num_actions\n    cov_matrix_dim = tf.compat.dimension_value(cov_matrix[0].shape[0])\n    if self._encoding_dim != cov_matrix_dim:\n      raise ValueError(\'The dimension of matrix `cov_matrix` must match \'\n                       \'encoding dimension {}.\'\n                       \'Got {} for `cov_matrix`.\'.format(\n                           self._encoding_dim, cov_matrix_dim))\n    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n    if self._encoding_dim != data_vector_dim:\n      raise ValueError(\'The dimension of vector `data_vector` must match \'\n                       \'encoding  dimension {}. \'\n                       \'Got {} for `data_vector`.\'.format(\n                           self._encoding_dim, data_vector_dim))\n    action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n\n    self._emit_policy_info = emit_policy_info\n    predicted_rewards_mean = ()\n    if policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN in emit_policy_info:\n      predicted_rewards_mean = tensor_spec.TensorSpec(\n          [self._num_actions],\n          dtype=tf.float32)\n    predicted_rewards_optimistic = ()\n    if (policy_utilities.InfoFields.PREDICTED_REWARDS_OPTIMISTIC in\n        emit_policy_info):\n      predicted_rewards_optimistic = tensor_spec.TensorSpec(\n          [self._num_actions],\n          dtype=tf.float32)\n    if accepts_per_arm_features:\n      chosen_arm_features_info_spec = (\n          policy_utilities.create_chosen_arm_features_info_spec(\n              time_step_spec.observation,\n              observation_and_action_constraint_splitter))\n      info_spec = policy_utilities.PerArmPolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          predicted_rewards_optimistic=predicted_rewards_optimistic,\n          chosen_arm_features=chosen_arm_features_info_spec)\n    else:\n      info_spec = policy_utilities.PolicyInfo(\n          predicted_rewards_mean=predicted_rewards_mean,\n          predicted_rewards_optimistic=predicted_rewards_optimistic)\n\n    super(NeuralLinUCBPolicy, self).__init__(\n        time_step_spec=time_step_spec,\n        action_spec=action_spec,\n        emit_log_probability=emit_log_probability,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        info_spec=info_spec,\n        name=name)\n\n  def _variables(self):\n    all_variables = [self._cov_matrix, self._data_vector,\n                     self._num_samples, self._actions_from_reward_layer,\n                     self._encoding_network.variables,\n                     self._reward_layer.variables]\n    return [v for v in tf.nest.flatten(all_variables)\n            if isinstance(v, tf.Variable)]\n\n  def _get_actions_from_reward_layer(self, encoded_observation, mask):\n    # Get the predicted expected reward.\n    est_mean_reward = tf.reshape(self._reward_layer(encoded_observation),\n                                 shape=[-1, self._num_actions])\n    if mask is None:\n      greedy_actions = tf.argmax(est_mean_reward, axis=-1, output_type=tf.int32)\n    else:\n      greedy_actions = policy_utilities.masked_argmax(\n          est_mean_reward, mask, output_type=tf.int32)\n\n    # Add epsilon greedy on top, if needed.\n    if self._epsilon_greedy:\n      batch_size = (tf.compat.dimension_value(encoded_observation.shape[0]) or\n                    tf.shape(encoded_observation)[0])\n      if mask is None:\n        random_actions = tf.random.uniform(\n            [batch_size], maxval=self._num_actions,\n            dtype=tf.int32)\n      else:\n        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n        masked_categorical = masked.MaskedCategorical(\n            zero_logits, mask, dtype=tf.int32)\n        random_actions = masked_categorical.sample()\n\n      rng = tf.random.uniform([batch_size], maxval=1.0)\n      cond = tf.greater(rng, self._epsilon_greedy)\n      chosen_actions = tf.compat.v1.where(cond, greedy_actions, random_actions)\n    else:\n      chosen_actions = greedy_actions\n\n    return chosen_actions, est_mean_reward, est_mean_reward\n\n  def _get_actions_from_linucb(self, encoded_observation, mask):\n    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n\n    p_values = []\n    est_rewards = []\n    for k in range(self._num_actions):\n      encoded_observation_for_arm = self._get_encoded_observation_for_arm(\n          encoded_observation, k)\n      model_index = policy_utilities.get_model_index(\n          k, self._accepts_per_arm_features)\n      a_inv_x = linalg.conjugate_gradient_solve(\n          self._cov_matrix[model_index] +\n          tf.eye(self._encoding_dim, dtype=self._dtype),\n          tf.linalg.matrix_transpose(encoded_observation_for_arm))\n      mean_reward_est = tf.einsum(\'j,jk->k\', self._data_vector[model_index],\n                                  a_inv_x)\n      est_rewards.append(mean_reward_est)\n\n      ci = tf.reshape(\n          tf.linalg.tensor_diag_part(\n              tf.matmul(encoded_observation_for_arm, a_inv_x)), [-1, 1])\n      p_values.append(\n          tf.reshape(mean_reward_est, [-1, 1]) + self._alpha * tf.sqrt(ci))\n\n    stacked_p_values = tf.squeeze(tf.stack(p_values, axis=-1), axis=[1])\n    if mask is None:\n      chosen_actions = tf.argmax(\n          stacked_p_values,\n          axis=-1,\n          output_type=tf.int32)\n    else:\n      chosen_actions = policy_utilities.masked_argmax(\n          stacked_p_values, mask, output_type=tf.int32)\n\n    est_mean_reward = tf.cast(tf.stack(est_rewards, axis=-1), tf.float32)\n    return chosen_actions, est_mean_reward, tf.cast(stacked_p_values,\n                                                    tf.float32)\n\n  def _distribution(self, time_step, policy_state):\n    raise NotImplementedError(\n        \'This policy outputs an action and not a distribution.\')\n\n  def _action(self, time_step, policy_state, seed):\n    observation = time_step.observation\n    if self.observation_and_action_constraint_splitter is not None:\n      observation, _ = self.observation_and_action_constraint_splitter(\n          observation)\n    mask = policy_utilities.construct_mask_from_multiple_sources(\n        time_step.observation, self._observation_and_action_constraint_splitter,\n        (), self._num_actions)\n    # Pass the observations through the encoding network.\n    encoded_observation, _ = self._encoding_network(observation)\n    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n\n    if tf.distribute.has_strategy():\n      if self._distributed_use_reward_layer:\n        chosen_actions, est_mean_rewards, est_rewards_optimistic = (\n            self._get_actions_from_reward_layer(encoded_observation, mask))\n      else:\n        chosen_actions, est_mean_rewards, est_rewards_optimistic = (\n            self._get_actions_from_linucb(encoded_observation, mask))\n    else:\n      chosen_actions, est_mean_rewards, est_rewards_optimistic = tf.cond(\n          self._actions_from_reward_layer,\n          # pylint: disable=g-long-lambda\n          lambda: self._get_actions_from_reward_layer(\n              encoded_observation, mask),\n          lambda: self._get_actions_from_linucb(encoded_observation, mask))\n\n    arm_observations = ()\n    if self._accepts_per_arm_features:\n      arm_observations = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n    policy_info = policy_utilities.populate_policy_info(\n        arm_observations, chosen_actions, est_rewards_optimistic,\n        est_mean_rewards, self._emit_policy_info,\n        self._accepts_per_arm_features)\n    return policy_step.PolicyStep(chosen_actions, policy_state, policy_info)\n\n  def _get_encoded_observation_for_arm(self, encoded_observation, arm_index):\n    if self._accepts_per_arm_features:\n      return(encoded_observation[:, arm_index, :])\n    else:\n      return encoded_observation\n'"
tf_agents/bandits/policies/neural_linucb_policy_test.py,98,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.policies.neural_linucb_policy.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.networks import global_and_arm_feature_network as arm_network\nfrom tf_agents.bandits.policies import neural_linucb_policy\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\n_POLICY_VARIABLES_OFFSET = 10.0\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, observation_spec, obs_dim=2, encoding_dim=10):\n    super(DummyNet, self).__init__(observation_spec, (), \'DummyNet\')\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            encoding_dim,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                np.ones([obs_dim, encoding_dim])),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                np.zeros([encoding_dim])))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\ndef get_reward_layer(num_actions=5, encoding_dim=10):\n  return tf.keras.layers.Dense(\n      num_actions,\n      activation=None,\n      kernel_initializer=tf.compat.v1.initializers.constant(\n          np.ones([encoding_dim, num_actions])),\n      bias_initializer=tf.compat.v1.initializers.constant(\n          np.array(range(num_actions))))\n\n\ndef get_per_arm_reward_layer(encoding_dim=10):\n  return tf.keras.layers.Dense(\n      units=1,\n      activation=None,\n      use_bias=False,\n      kernel_initializer=tf.compat.v1.initializers.constant(\n          list(range(encoding_dim))))\n\n\ndef test_cases():\n  return parameterized.named_parameters(\n      {\n          \'testcase_name\': \'_batch1_numtrainsteps0\',\n          \'batch_size\': 1,\n          \'actions_from_reward_layer\': False,\n      }, {\n          \'testcase_name\': \'_batch4_numtrainsteps10\',\n          \'batch_size\': 4,\n          \'actions_from_reward_layer\': True,\n      })\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass NeuralLinUCBPolicyTest(parameterized.TestCase, test_utils.TestCase):\n\n  def setUp(self):\n    super(NeuralLinUCBPolicyTest, self).setUp()\n    self._obs_dim = 2\n    self._obs_spec = tensor_spec.TensorSpec([self._obs_dim], tf.float32)\n    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n    self._num_actions = 5\n    self._alpha = 1.0\n    self._action_spec = tensor_spec.BoundedTensorSpec(\n        shape=(),\n        dtype=tf.int32,\n        minimum=0,\n        maximum=self._num_actions - 1,\n        name=\'action\')\n    self._encoding_dim = 10\n\n  @property\n  def _a(self):\n    a_for_one_arm = 1.0 + 4.0 * tf.eye(self._encoding_dim, dtype=tf.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  @property\n  def _a_numpy(self):\n    a_for_one_arm = 1.0 + 4.0 * np.eye(self._encoding_dim, dtype=np.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  @property\n  def _b(self):\n    return [tf.constant(r * np.ones(self._encoding_dim), dtype=tf.float32)\n            for r in range(self._num_actions)]\n\n  @property\n  def _b_numpy(self):\n    return [np.array([r * np.ones(self._encoding_dim)], dtype=np.float32)\n            for r in range(self._num_actions)]\n\n  @property\n  def _num_samples_per_arm(self):\n    a_for_one_arm = tf.constant([1], dtype=tf.float32)\n    return [a_for_one_arm] * self._num_actions\n\n  def _time_step_batch(self, batch_size):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        tf.constant(np.array(range(batch_size * self._obs_dim)),\n                    dtype=tf.float32, shape=[batch_size, self._obs_dim],\n                    name=\'observation\'))\n\n  def _time_step_batch_with_mask(self, batch_size):\n    observation = tf.constant(\n        np.array(range(batch_size * self._obs_dim)),\n        dtype=tf.float32,\n        shape=[batch_size, self._obs_dim])\n    mask = tf.eye(batch_size, num_columns=self._num_actions, dtype=tf.int32)\n    observation_with_mask = (observation, mask)\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        observation_with_mask)\n\n  def _per_arm_time_step_batch(self, batch_size, global_obs_dim, arm_obs_dim):\n    return ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST,\n            dtype=tf.int32,\n            shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        {\n            bandit_spec_utils.GLOBAL_FEATURE_KEY:\n                tf.constant(\n                    np.array(range(batch_size * global_obs_dim)),\n                    dtype=tf.float32,\n                    shape=[batch_size, global_obs_dim],\n                    name=\'observation\'),\n            bandit_spec_utils.PER_ARM_FEATURE_KEY:\n                tf.constant(\n                    np.array(\n                        range(batch_size * self._num_actions * arm_obs_dim)),\n                    dtype=tf.float32,\n                    shape=[batch_size, self._num_actions, arm_obs_dim],\n                    name=\'observation\'),\n            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY:\n                tf.ones([batch_size], dtype=tf.int32) * (self._num_actions - 1)\n        })\n\n  def _get_predicted_rewards_from_linucb(self, observation_numpy, batch_size):\n    """"""Runs one step of LinUCB using numpy arrays.""""""\n    observation_numpy.reshape([batch_size, self._encoding_dim])\n\n    predicted_rewards = []\n    for k in range(self._num_actions):\n      a_inv = np.linalg.inv(self._a_numpy[k] + np.eye(self._encoding_dim))\n      theta = np.matmul(\n          a_inv, self._b_numpy[k].reshape([self._encoding_dim, 1]))\n      est_mean_reward = np.matmul(observation_numpy, theta)\n      predicted_rewards.append(est_mean_reward)\n    predicted_rewards_array = np.stack(\n        predicted_rewards, axis=-1).reshape(batch_size, self._num_actions)\n    return predicted_rewards_array\n\n  def _get_predicted_rewards_from_per_arm_linucb(self, observation_numpy,\n                                                 batch_size):\n    """"""Runs one step of LinUCB using numpy arrays.""""""\n    observation_numpy.reshape(\n        [batch_size, self._num_actions, self._encoding_dim])\n\n    predicted_rewards = []\n    for k in range(self._num_actions):\n      a_inv = np.linalg.inv(self._a_numpy[0] + np.eye(self._encoding_dim))\n      theta = np.matmul(\n          a_inv, self._b_numpy[0].reshape([self._encoding_dim, 1]))\n      est_mean_reward = np.matmul(observation_numpy[:, k, :], theta)\n      predicted_rewards.append(est_mean_reward)\n    predicted_rewards_array = np.stack(\n        predicted_rewards, axis=-1).reshape((batch_size, self._num_actions))\n    return predicted_rewards_array\n\n  @test_cases()\n  def testBuild(self, batch_size, actions_from_reward_layer):\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        DummyNet(self._obs_spec),\n        self._encoding_dim,\n        get_reward_layer(),\n        actions_from_reward_layer=actions_from_reward_layer,\n        cov_matrix=self._a,\n        data_vector=self._b,\n        num_samples=self._num_samples_per_arm,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec)\n\n    self.assertEqual(policy.time_step_spec, self._time_step_spec)\n\n  @test_cases()\n  def testObservationShapeMismatch(self, batch_size, actions_from_reward_layer):\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        DummyNet(self._obs_spec),\n        self._encoding_dim,\n        get_reward_layer(),\n        actions_from_reward_layer=actions_from_reward_layer,\n        cov_matrix=self._a,\n        data_vector=self._b,\n        num_samples=self._num_samples_per_arm,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec)\n\n    current_time_step = ts.TimeStep(\n        tf.constant(\n            ts.StepType.FIRST, dtype=tf.int32, shape=[batch_size],\n            name=\'step_type\'),\n        tf.constant(0.0, dtype=tf.float32, shape=[batch_size], name=\'reward\'),\n        tf.constant(1.0, dtype=tf.float32, shape=[batch_size], name=\'discount\'),\n        tf.constant(np.array(range(batch_size * (self._obs_dim + 1))),\n                    dtype=tf.float32, shape=[batch_size, self._obs_dim + 1],\n                    name=\'observation\'))\n    if tf.executing_eagerly():\n      error_type = tf.errors.InvalidArgumentError\n      regexp = r\'Matrix size-incompatible: In\\[0\\]: \\[%d,3\\]\' % batch_size\n    else:\n      error_type = ValueError\n      regexp = r\'with shape \\[%d, 3\\]\' % batch_size\n    with self.assertRaisesRegex(error_type, regexp):\n      policy.action(current_time_step)\n\n  @test_cases()\n  def testActionBatch(self, batch_size, actions_from_reward_layer):\n\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        DummyNet(self._obs_spec),\n        self._encoding_dim,\n        get_reward_layer(),\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=self._a,\n        data_vector=self._b,\n        num_samples=self._num_samples_per_arm,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec)\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(self._time_step_batch(batch_size=batch_size))\n    actions_ = self.evaluate(action_step.action)\n    self.assertAllGreaterEqual(actions_, self._action_spec.minimum)\n    self.assertAllLessEqual(actions_, self._action_spec.maximum)\n\n  @test_cases()\n  def testActionBatchWithMask(self, batch_size, actions_from_reward_layer):\n    obs_spec = (tensor_spec.TensorSpec([self._obs_dim], tf.float32),\n                tensor_spec.TensorSpec([self._num_actions], tf.int32))\n    time_step_spec = ts.time_step_spec(obs_spec)\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        DummyNet(obs_spec[0]),\n        self._encoding_dim,\n        get_reward_layer(),\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=self._a,\n        data_vector=self._b,\n        num_samples=self._num_samples_per_arm,\n        epsilon_greedy=0.5,\n        time_step_spec=time_step_spec,\n        observation_and_action_constraint_splitter=lambda x: (x[0], x[1]))\n\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(\n        self._time_step_batch_with_mask(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape.as_list(), [batch_size])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    actions = self.evaluate(action_step.action)\n    self.assertAllEqual(actions, range(batch_size))\n\n  @test_cases()\n  def testActionBatchWithVariablesAndPolicyUpdate(\n      self, batch_size, actions_from_reward_layer):\n\n    a_list = []\n    a_new_list = []\n    b_list = []\n    b_new_list = []\n    num_samples_list = []\n    num_samples_new_list = []\n    for k in range(1, self._num_actions + 1):\n      a_initial_value = k + 1 + 2 * k * tf.eye(\n          self._encoding_dim, dtype=tf.float32)\n      a_for_one_arm = tf.compat.v2.Variable(a_initial_value)\n      a_list.append(a_for_one_arm)\n      b_initial_value = tf.constant(\n          k * np.ones(self._encoding_dim), dtype=tf.float32)\n      b_for_one_arm = tf.compat.v2.Variable(b_initial_value)\n      b_list.append(b_for_one_arm)\n      num_samples_initial_value = tf.constant([1], dtype=tf.float32)\n      num_samples_for_one_arm = tf.compat.v2.Variable(num_samples_initial_value)\n      num_samples_list.append(num_samples_for_one_arm)\n\n      # Variables for the new policy (they differ by an offset).\n      a_new_for_one_arm = tf.compat.v2.Variable(\n          a_initial_value + _POLICY_VARIABLES_OFFSET)\n      a_new_list.append(a_new_for_one_arm)\n      b_new_for_one_arm = tf.compat.v2.Variable(\n          b_initial_value + _POLICY_VARIABLES_OFFSET)\n      b_new_list.append(b_new_for_one_arm)\n      num_samples_for_one_arm_new = tf.compat.v2.Variable(\n          num_samples_initial_value + _POLICY_VARIABLES_OFFSET)\n      num_samples_new_list.append(num_samples_for_one_arm_new)\n\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        encoding_network=DummyNet(self._obs_spec),\n        encoding_dim=self._encoding_dim,\n        reward_layer=get_reward_layer(),\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=a_list,\n        data_vector=b_list,\n        num_samples=num_samples_list,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec)\n\n    new_policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        encoding_network=DummyNet(self._obs_spec),\n        encoding_dim=self._encoding_dim,\n        reward_layer=get_reward_layer(),\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=a_new_list,\n        data_vector=b_new_list,\n        num_samples=num_samples_new_list,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec)\n\n    action_step = policy.action(self._time_step_batch(batch_size=batch_size))\n    new_action_step = new_policy.action(\n        self._time_step_batch(batch_size=batch_size))\n    self.assertEqual(action_step.action.shape, new_action_step.action.shape)\n    self.assertEqual(action_step.action.dtype, new_action_step.action.dtype)\n\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    self.evaluate(new_policy.update(policy))\n\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(self._time_step_batch(batch_size=batch_size))\n    new_action_fn = common.function_in_tf1()(new_policy.action)\n    new_action_step = new_action_fn(\n        self._time_step_batch(batch_size=batch_size))\n\n    actions_, new_actions_ = self.evaluate(\n        [action_step.action, new_action_step.action])\n    self.assertAllEqual(actions_, new_actions_)\n\n  @test_cases()\n  def testPredictedRewards(\n      self, batch_size, actions_from_reward_layer):\n    dummy_net = DummyNet(self._obs_spec)\n    reward_layer = get_reward_layer()\n\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        dummy_net,\n        self._encoding_dim,\n        reward_layer,\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=self._a,\n        data_vector=self._b,\n        num_samples=self._num_samples_per_arm,\n        epsilon_greedy=0.0,\n        time_step_spec=self._time_step_spec,\n        emit_policy_info=(\'predicted_rewards_mean\',))\n\n    current_time_step = self._time_step_batch(batch_size=batch_size)\n    action_step = policy.action(current_time_step)\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(current_time_step)\n\n    input_observation = current_time_step.observation\n    encoded_observation, _ = dummy_net(input_observation)\n    predicted_rewards_from_reward_layer = reward_layer(encoded_observation)\n    if actions_from_reward_layer:\n      predicted_rewards_expected = self.evaluate(\n          predicted_rewards_from_reward_layer)\n    else:\n      observation_numpy = self.evaluate(encoded_observation)\n      predicted_rewards_expected = self._get_predicted_rewards_from_linucb(\n          observation_numpy, batch_size)\n\n    p_info = self.evaluate(action_step.info)\n    self.assertEqual(p_info.predicted_rewards_mean.dtype, np.float32)\n    self.assertAllClose(p_info.predicted_rewards_mean,\n                        predicted_rewards_expected)\n\n  @test_cases()\n  def testPerArmObservation(self, batch_size, actions_from_reward_layer):\n    global_obs_dim = 7\n    arm_obs_dim = 3\n    obs_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        global_obs_dim,\n        arm_obs_dim,\n        self._num_actions,\n        add_num_actions_feature=True)\n    time_step_spec = ts.time_step_spec(obs_spec)\n    dummy_net = arm_network.create_feed_forward_common_tower_network(\n        obs_spec,\n        global_layers=(3, 4, 5),\n        arm_layers=(3, 2),\n        common_layers=(4, 3),\n        output_dim=self._encoding_dim)\n    reward_layer = get_per_arm_reward_layer(encoding_dim=self._encoding_dim)\n\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        dummy_net,\n        self._encoding_dim,\n        reward_layer,\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=self._a[0:1],\n        data_vector=self._b[0:1],\n        num_samples=self._num_samples_per_arm[0:1],\n        epsilon_greedy=0.0,\n        time_step_spec=time_step_spec,\n        accepts_per_arm_features=True,\n        emit_policy_info=(\'predicted_rewards_mean\',\n                          \'predicted_rewards_optimistic\'))\n\n    current_time_step = self._per_arm_time_step_batch(\n        batch_size=batch_size,\n        global_obs_dim=global_obs_dim,\n        arm_obs_dim=arm_obs_dim)\n    action_step = policy.action(current_time_step)\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    self.evaluate(tf.compat.v1.global_variables_initializer())\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(current_time_step)\n\n    input_observation = current_time_step.observation\n    encoded_observation, _ = dummy_net(input_observation)\n\n    if actions_from_reward_layer:\n      predicted_rewards_from_reward_layer = reward_layer(encoded_observation)\n      predicted_rewards_expected = self.evaluate(\n          predicted_rewards_from_reward_layer).reshape((-1, self._num_actions))\n    else:\n      observation_numpy = self.evaluate(encoded_observation)\n      predicted_rewards_expected = (\n          self._get_predicted_rewards_from_per_arm_linucb(\n              observation_numpy, batch_size))\n\n    p_info = self.evaluate(action_step.info)\n    self.assertEqual(p_info.predicted_rewards_mean.dtype, np.float32)\n    self.assertAllClose(p_info.predicted_rewards_mean,\n                        predicted_rewards_expected)\n    self.assertAllGreaterEqual(\n        p_info.predicted_rewards_optimistic - predicted_rewards_expected, 0)\n\n  @test_cases()\n  def testSparseObs(self, batch_size, actions_from_reward_layer):\n    obs_spec = {\n        \'global\': {\'sport\': tensor_spec.TensorSpec((), tf.string)},\n        \'per_arm\': {\n            \'name\': tensor_spec.TensorSpec((3,), tf.string),\n            \'fruit\': tensor_spec.TensorSpec((3,), tf.string)\n        }\n    }\n    columns_a = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'name\', [\'bob\', \'george\', \'wanda\']))\n    columns_b = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'fruit\', [\'banana\', \'kiwi\', \'pear\']))\n    columns_c = tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            \'sport\', [\'bridge\', \'chess\', \'snooker\']))\n\n    dummy_net = arm_network.create_feed_forward_common_tower_network(\n        obs_spec,\n        global_layers=(3, 4, 5),\n        arm_layers=(3, 2),\n        common_layers=(4, 3),\n        output_dim=self._encoding_dim,\n        global_preprocessing_combiner=(tf.compat.v2.keras.layers.DenseFeatures(\n            [columns_c])),\n        arm_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n            [columns_a, columns_b]))\n    time_step_spec = ts.time_step_spec(obs_spec)\n    reward_layer = get_per_arm_reward_layer(encoding_dim=self._encoding_dim)\n    policy = neural_linucb_policy.NeuralLinUCBPolicy(\n        dummy_net,\n        self._encoding_dim,\n        reward_layer,\n        actions_from_reward_layer=tf.constant(\n            actions_from_reward_layer, dtype=tf.bool),\n        cov_matrix=self._a[0:1],\n        data_vector=self._b[0:1],\n        num_samples=self._num_samples_per_arm[0:1],\n        epsilon_greedy=0.0,\n        time_step_spec=time_step_spec,\n        accepts_per_arm_features=True,\n        emit_policy_info=(\'predicted_rewards_mean\',))\n    observations = {\n        \'global\': {\n            \'sport\': tf.constant([\'snooker\', \'chess\'])\n        },\n        \'per_arm\': {\n            \'name\':\n                tf.constant([[\'george\', \'george\', \'george\'],\n                             [\'bob\', \'bob\', \'bob\']]),\n            \'fruit\':\n                tf.constant([[\'banana\', \'banana\', \'banana\'],\n                             [\'kiwi\', \'kiwi\', \'kiwi\']])\n        }\n    }\n\n    time_step = ts.restart(observations, batch_size=2)\n    action_fn = common.function_in_tf1()(policy.action)\n    action_step = action_fn(time_step, seed=1)\n    self.assertEqual(action_step.action.shape.as_list(), [2])\n    self.assertEqual(action_step.action.dtype, tf.int32)\n    # Initialize all variables\n    self.evaluate([tf.compat.v1.global_variables_initializer(),\n                   tf.compat.v1.tables_initializer()])\n    action = self.evaluate(action_step.action)\n    self.assertAllEqual(action.shape, [2])\n    p_info = self.evaluate(action_step.info)\n    self.assertAllEqual(p_info.predicted_rewards_mean.shape, [2, 3])\n    self.assertAllEqual(p_info.chosen_arm_features[\'name\'].shape, [2])\n    self.assertAllEqual(p_info.chosen_arm_features[\'fruit\'].shape, [2])\n    first_action = action[0]\n    first_arm_name_feature = observations[\n        bandit_spec_utils.PER_ARM_FEATURE_KEY][\'name\'][0]\n    self.assertAllEqual(p_info.chosen_arm_features[\'name\'][0],\n                        first_arm_name_feature[first_action])\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/policies/policy_utilities.py,19,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities for bandit policies.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom absl import logging\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\n\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.utils import common\n\n\nclass InfoFields(object):\n  """"""Strings which can be used in the policy info fields.""""""\n  LOG_PROBABILITY = policy_step.CommonFields.LOG_PROBABILITY\n  # Mean of predicted rewards (per arm).\n  PREDICTED_REWARDS_MEAN = \'predicted_rewards_mean\'\n  # Optimistic estimates of predicted rewards (per arm).\n  PREDICTED_REWARDS_OPTIMISTIC = \'predicted_rewards_optimistic\'\n  # Samples of predicted rewards (per arm).\n  PREDICTED_REWARDS_SAMPLED = \'predicted_rewards_sampled\'\n  # Type of bandit policy (see enumerations in `BanditPolicyType`).\n  BANDIT_POLICY_TYPE = \'bandit_policy_type\'\n  # Used to store the chosen action for a per-arm model.\n  CHOSEN_ARM_FEATURES = \'chosen_arm_features\'\n\n\nPolicyInfo = collections.namedtuple(  # pylint: disable=invalid-name\n    \'PolicyInfo\',\n    (InfoFields.LOG_PROBABILITY,\n     InfoFields.PREDICTED_REWARDS_MEAN,\n     InfoFields.PREDICTED_REWARDS_OPTIMISTIC,\n     InfoFields.PREDICTED_REWARDS_SAMPLED,\n     InfoFields.BANDIT_POLICY_TYPE))\n# Set default empty tuple for all fields.\nPolicyInfo.__new__.__defaults__ = ((),) * len(PolicyInfo._fields)\n\n\nPerArmPolicyInfo = collections.namedtuple(  # pylint: disable=invalid-name\n    \'PerArmPolicyInfo\',\n    (InfoFields.LOG_PROBABILITY,\n     InfoFields.PREDICTED_REWARDS_MEAN,\n     InfoFields.PREDICTED_REWARDS_OPTIMISTIC,\n     InfoFields.PREDICTED_REWARDS_SAMPLED,\n     InfoFields.BANDIT_POLICY_TYPE,\n     InfoFields.CHOSEN_ARM_FEATURES))\n# Set default empty tuple for all fields.\nPerArmPolicyInfo.__new__.__defaults__ = ((),) * len(PerArmPolicyInfo._fields)\n\n\ndef populate_policy_info(arm_observations, chosen_actions, rewards_for_argmax,\n                         est_rewards, emit_policy_info,\n                         accepts_per_arm_features):\n  """"""Populates policy info given all needed input.\n\n  Args:\n    arm_observations: In case the policy accepts per-arm feautures, this is a\n      Tensor with the per-arm features. Otherwise its value is unused.\n    chosen_actions: A Tensor with the indices of the chosen actions.\n    rewards_for_argmax: The sampled or optimistically boosted reward estimates\n      based on which the policy chooses the action greedily.\n    est_rewards: A Tensor with the rewards estimated by the model.\n    emit_policy_info: A set of policy info keys, specifying wich info fields to\n      populate\n    accepts_per_arm_features: (bool) Whether the policy accepts per-arm\n      features.\n\n  Returns:\n    A policy info.\n  """"""\n  if accepts_per_arm_features:\n    # Saving the features for the chosen action to the policy_info.\n    chosen_arm_features = tf.nest.map_structure(\n        lambda t: tf.gather(params=t, indices=chosen_actions, batch_dims=1),\n        arm_observations)\n    policy_info = PerArmPolicyInfo(\n        predicted_rewards_optimistic=(\n            rewards_for_argmax\n            if InfoFields.PREDICTED_REWARDS_OPTIMISTIC in emit_policy_info else\n            ()),\n        predicted_rewards_sampled=(\n            rewards_for_argmax if\n            InfoFields.PREDICTED_REWARDS_SAMPLED in emit_policy_info else ()),\n        predicted_rewards_mean=(\n            est_rewards\n            if InfoFields.PREDICTED_REWARDS_MEAN in emit_policy_info else ()),\n        chosen_arm_features=chosen_arm_features)\n  else:\n    policy_info = PolicyInfo(\n        predicted_rewards_optimistic=(\n            rewards_for_argmax\n            if InfoFields.PREDICTED_REWARDS_OPTIMISTIC in emit_policy_info else\n            ()),\n        predicted_rewards_sampled=(\n            rewards_for_argmax if\n            InfoFields.PREDICTED_REWARDS_SAMPLED in emit_policy_info else ()),\n        predicted_rewards_mean=(\n            est_rewards\n            if InfoFields.PREDICTED_REWARDS_MEAN in emit_policy_info else ()))\n  return policy_info\n\n\nclass BanditPolicyType(object):\n  """"""Enumeration of bandit policy types.""""""\n  # No bandit policy type specified.\n  UNKNOWN = 0\n  # Greedy decision made by bandit agent.\n  GREEDY = 1\n  # Random decision for exploration made by epsilon-greedy agent sampled from\n  # uniform distribution over actions.\n  UNIFORM = 2\n\n\ndef create_bandit_policy_type_tensor_spec(shape):\n  """"""Create tensor spec for bandit policy type.""""""\n  return tensor_spec.BoundedTensorSpec(\n      shape=shape, dtype=tf.int32,\n      minimum=BanditPolicyType.UNKNOWN, maximum=BanditPolicyType.UNIFORM)\n\n\n@common.function\ndef masked_argmax(input_tensor, mask, output_type=tf.int32):\n  """"""Computes the argmax where the allowed elements are given by a mask.\n\n  If a row of `mask` contains all zeros, then this method will return -1 for the\n  corresponding row of `input_tensor`.\n\n  Args:\n    input_tensor: Rank-2 Tensor of floats.\n    mask: 0-1 valued Tensor of the same shape as input.\n    output_type: Integer type of the output.\n\n  Returns:\n    A Tensor of rank 1 and type `output_type`, with the masked argmax of every\n    row of `input_tensor`.\n  """"""\n  input_tensor.shape.assert_is_compatible_with(mask.shape)\n  neg_inf = tf.constant(-float(\'Inf\'), input_tensor.dtype)\n  modified_input = tf.compat.v2.where(\n      tf.cast(mask, tf.bool), input_tensor, neg_inf)\n  argmax_tensor = tf.argmax(modified_input, axis=-1, output_type=output_type)\n  # Replace results for invalid mask rows with -1.\n  reduce_mask = tf.cast(tf.reduce_max(mask, axis=1), tf.bool)\n  neg_one = tf.constant(-1, output_type)\n  return tf.compat.v2.where(reduce_mask, argmax_tensor, neg_one)\n\n\ndef has_bandit_policy_type(info, check_for_tensor=False):\n  """"""Check if policy info has `bandit_policy_type` field/tensor.""""""\n  if info in ((), None):\n    return False\n  fields = getattr(info, \'_fields\', None)\n  has_field = fields is not None and InfoFields.BANDIT_POLICY_TYPE in fields\n  if has_field and check_for_tensor:\n    return isinstance(info.bandit_policy_type, tf.Tensor)\n  else:\n    return has_field\n\n\ndef set_bandit_policy_type(info, bandit_policy_type):\n  """"""Sets the InfoFields.BANDIT_POLICY_TYPE on info to bandit_policy_type.\n\n  If policy `info` does not support InfoFields.BANDIT_POLICY_TYPE, this method\n  returns `info` as-is (without any modification).\n\n  Args:\n    info: Policy info on which to set bandit policy type.\n    bandit_policy_type: Tensor containing BanditPolicyType enums or TensorSpec\n      from `create_bandit_policy_type_tensor_spec()`.\n\n  Returns:\n    Policy info with modified field (if possible).\n  """"""\n  if info in ((), None):\n    return PolicyInfo(bandit_policy_type=bandit_policy_type)\n  fields = getattr(info, \'_fields\', None)\n  if fields is not None and InfoFields.BANDIT_POLICY_TYPE in fields:\n    return info._replace(bandit_policy_type=bandit_policy_type)\n  try:\n    info[InfoFields.BANDIT_POLICY_TYPE] = bandit_policy_type\n  except TypeError:\n    pass\n  return info\n\n\n@common.function\ndef bandit_policy_uniform_mask(values, mask):\n  """"""Set bandit policy type tensor to BanditPolicyType.UNIFORM based on mask.\n\n  Set bandit policy type `values` to BanditPolicyType.UNIFORM; returns tensor\n  where output[i] is BanditPolicyType.UNIFORM if mask[i] is True, otherwise it\n  is left as values[i].\n\n  Args:\n    values: Tensor containing `BanditPolicyType` enumerations.\n    mask: Tensor of the same shape as `values` with boolean flags indicating\n      values to set to `BanditPolicyType.UNIFORM`.\n\n  Returns:\n    Tensor containing `BanditPolicyType` enumerations with masked values.\n  """"""\n  return tf.where(\n      mask, tf.fill(tf.shape(values), BanditPolicyType.UNIFORM), values)\n\n\ndef get_model_index(arm_index, accepts_per_arm_features):\n  """"""Returns the model index for a specific arm.\n\n  The number of models depends on the observation format: If the policy accepts\n  per-arm features, there is only one single model used for every arm. Otherwise\n  there is a model for every arm.\n\n  Args:\n    arm_index: The index of the arm for which the model index is needed.\n    accepts_per_arm_features: (bool) Whether the policy works with per-arm\n      features.\n\n  Returns:\n    The index of the model for the arm requested.\n  """"""\n  return 0 if accepts_per_arm_features else arm_index\n\n\ndef compute_feasibility_probability(observation, constraints, batch_size,\n                                    num_actions, action_mask=None):\n  """"""Helper function to compute the action feasibility probability.""""""\n  feasibility_prob = tf.ones([batch_size, num_actions])\n  if action_mask is not None:\n    feasibility_prob = tf.cast(action_mask, tf.float32)\n  for c in constraints:\n    # We assume the constraints are independent.\n    action_feasibility = c(observation)\n    feasibility_prob *= action_feasibility\n  return feasibility_prob\n\n\ndef construct_mask_from_multiple_sources(\n    observation, observation_and_action_constraint_splitter, constraints,\n    max_num_actions):\n  """"""Constructs an action mask from multiple sources.\n\n  The sources include:\n  -- The action mask encoded in the observation,\n  -- the `num_actions` feature restricting the number of actions per sample,\n  -- the feasibility mask implied by constraints.\n\n  The resulting mask disables all actions that are masked out in any of the\n  three sources.\n\n  Args:\n    observation: A nest of Tensors containing the observation.\n    observation_and_action_constraint_splitter: The observation action mask\n      splitter function if the observation has action mask.\n    constraints: Iterable of constraints objects that are instances of\n        `tf_agents.bandits.agents.NeuralConstraint`.\n    max_num_actions: The maximum number of actions per sample.\n\n  Returns:\n    An action mask in the form of a `[batch_size, max_num_actions]` 0-1 tensor.\n  """"""\n  mask = None\n  if observation_and_action_constraint_splitter is not None:\n    observation, mask = observation_and_action_constraint_splitter(observation)\n  batch_size = tf.nest.flatten(observation)[0].shape[0]\n  if (isinstance(observation, dict) and\n      bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation):\n    number_of_actions = observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY]\n    mask = tf.sequence_mask(\n        lengths=number_of_actions, maxlen=max_num_actions, dtype=tf.int32)\n\n  if constraints:\n    feasibility_prob = compute_feasibility_probability(\n        observation, constraints, batch_size,\n        max_num_actions, mask)\n    # Probabilistic masking.\n    mask = tfp.distributions.Bernoulli(probs=feasibility_prob).sample()\n  return mask\n\n\ndef create_chosen_arm_features_info_spec(\n    observation_spec, observation_and_action_constraint_splitter):\n  """"""Creates the chosen arm features info spec from the arm observation spec.""""""\n  if observation_and_action_constraint_splitter is not None:\n    observation_spec = observation_and_action_constraint_splitter(\n        observation_spec)[0]\n    if bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation_spec:\n      raise ValueError(\'Variable number of actions and action masking \'\n                       \'should not be used together.\')\n    logging.warning(\n        \'Action masking with per-arm features is discouraged. \'\n        \'Instead, use variable number of actions via the `%s` feature key.\',\n        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY)\n  arm_spec = observation_spec[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n  return tensor_spec.remove_outer_dims_nest(arm_spec, 1)\n'"
tf_agents/bandits/policies/policy_utilities_test.py,31,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.policies.policy_utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import constraints\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.networks import network\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import test_utils\nfrom tensorflow.python.framework import test_util  # pylint:disable=g-direct-tensorflow-import  # TF internal\n\n\n_GREEDY = policy_utilities.BanditPolicyType.GREEDY\n_UNIFORM = policy_utilities.BanditPolicyType.UNIFORM\n\n\nclass SimpleConstraint(constraints.BaseConstraint):\n\n  def __call__(self, observation, actions=None):\n    """"""Returns the probability of input actions being feasible.""""""\n    batch_size = tf.shape(observation)[0]\n    num_actions = self._action_spec.maximum - self._action_spec.minimum + 1\n    feasibility_prob = 0.5 * tf.ones([batch_size, num_actions], tf.float32)\n    return feasibility_prob\n\n\nclass DummyNet(network.Network):\n\n  def __init__(self, observation_spec, num_actions=3):\n    super(DummyNet, self).__init__(observation_spec, (), \'DummyNet\')\n\n    # Store custom layers that can be serialized through the Checkpointable API.\n    self._dummy_layers = [\n        tf.keras.layers.Dense(\n            num_actions,\n            kernel_initializer=tf.compat.v1.initializers.constant(\n                [[1, 1.5, 2], [1, 1.5, 4]]),\n            bias_initializer=tf.compat.v1.initializers.constant(\n                [[1], [1], [-10]]))\n    ]\n\n  def call(self, inputs, step_type=None, network_state=()):\n    del step_type\n    inputs = tf.cast(inputs, tf.float32)\n    for layer in self._dummy_layers:\n      inputs = layer(inputs)\n    return inputs, network_state\n\n\n@test_util.run_all_in_graph_and_eager_modes\nclass PolicyUtilitiesTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      dict(\n          input_tensor=[[4, 8, 2, -3], [0, 5, -234, 64]],\n          mask=[[1, 0, 0, 1], [0, 1, 1, 1]],\n          expected=[0, 3]),\n      dict(\n          input_tensor=[[3, 0.2, -3.3], [987, -2.5, 64], [0, 0, 0], [4, 3, 8]],\n          mask=[[1, 0, 0], [1, 0, 1], [1, 1, 1], [0, 1, 1]],\n          expected=[0, 0, 0, 2]),\n      dict(input_tensor=[[1, 2]], mask=[[1, 0]], expected=[0]))\n  def testMaskedArgmax(self, input_tensor, mask, expected):\n    actual = policy_utilities.masked_argmax(\n        tf.constant(input_tensor, dtype=tf.float32), tf.constant(mask))\n    self.assertAllEqual(actual, expected)\n\n  def testBadMask(self):\n    input_tensor = tf.reshape(tf.range(12, dtype=tf.float32), shape=[3, 4])\n    mask = [[1, 0, 0, 1], [0, 0, 0, 0], [1, 0, 1, 1]]\n    expected = [3, -1, 3]\n    actual = self.evaluate(\n        policy_utilities.masked_argmax(input_tensor, tf.constant(mask)))\n    self.assertAllEqual(actual, expected)\n\n  def testSetBanditPolicyType(self):\n    dims = (10, 1)\n    bandit_policy_spec = (\n        policy_utilities.create_bandit_policy_type_tensor_spec(dims))\n    info = policy_utilities.set_bandit_policy_type(None, bandit_policy_spec)\n    self.assertIsInstance(info, policy_utilities.PolicyInfo)\n    self.assertIsInstance(info.bandit_policy_type,\n                          tensor_spec.BoundedTensorSpec)\n    self.assertEqual(info.bandit_policy_type.shape, dims)\n    self.assertEqual(info.bandit_policy_type.dtype, tf.int32)\n    # Set to tensor.\n    input_tensor = tf.fill(dims, value=_GREEDY)\n    info = policy_utilities.set_bandit_policy_type(info, input_tensor)\n    self.assertIsInstance(info.bandit_policy_type, tf.Tensor)\n    self.assertEqual(info.bandit_policy_type.shape, input_tensor.shape)\n    expected = [[_GREEDY] for _ in range(dims[0])]\n    self.assertAllEqual(info.bandit_policy_type, expected)\n\n  def testWrongPolicyInfoType(self):\n    dims = (10, 1)\n    log_probability = tf.fill(dims, value=-0.5)\n    info = policy_step.PolicyInfo(log_probability=log_probability)\n    input_tensor = tf.fill(dims, value=_GREEDY)\n    result = policy_utilities.set_bandit_policy_type(info, input_tensor)\n    self.assertNotIsInstance(result, policy_utilities.PolicyInfo)\n    self.assertAllEqual(info.log_probability, result.log_probability)\n\n  def testBanditPolicyUniformMask(self):\n    dims = (10, 1)\n    input_tensor = tf.fill(dims, value=_GREEDY)\n    # Overwrite some values with UNIFORM.\n    mask_idx = (range(dims[0])[1:dims[0]:2])\n    mask = [[True if idx in mask_idx else False] for idx in range(dims[0])]\n    expected = [[_UNIFORM if mask_value[0]  else _GREEDY]\n                for mask_value in mask]\n    result = policy_utilities.bandit_policy_uniform_mask(input_tensor, mask)\n    self.assertAllEqual(result, expected)\n\n  def testComputeFeasibilityMask(self):\n    observation_spec = tensor_spec.TensorSpec([2], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n    simple_constraint = SimpleConstraint(time_step_spec, action_spec)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    feasibility_prob = policy_utilities.compute_feasibility_probability(\n        observations, [simple_constraint], batch_size=2, num_actions=3,\n        action_mask=None)\n    self.assertAllEqual(0.5 * np.ones([2, 3]), self.evaluate(feasibility_prob))\n\n  def testComputeFeasibilityMaskWithActionMask(self):\n    observation_spec = tensor_spec.TensorSpec([2], tf.float32)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 2)\n    constraint_net = DummyNet(observation_spec)\n    neural_constraint = constraints.NeuralConstraint(\n        time_step_spec,\n        action_spec,\n        constraint_network=constraint_net)\n\n    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    action_mask = tf.constant([[0, 0, 1], [0, 1, 0]], dtype=tf.int32)\n    feasibility_prob = policy_utilities.compute_feasibility_probability(\n        observations, [neural_constraint], batch_size=2, num_actions=3,\n        action_mask=action_mask)\n    self.assertAllEqual(self.evaluate(tf.cast(action_mask, tf.float32)),\n                        self.evaluate(feasibility_prob))\n\n  def testComputeMaskFromMultipleSourcesNumActionsFeature(self):\n    observation_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        4, 5, 6, add_num_actions_feature=True)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 5)\n    constraint_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            observation_spec, (3, 4), (4, 3), (2, 3)))\n    neural_constraint = constraints.NeuralConstraint(\n        time_step_spec,\n        action_spec,\n        constraint_network=constraint_net)\n\n    observations = {\n        \'global\': tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=tf.float32),\n        \'per_arm\': tf.reshape(tf.range(60, dtype=tf.float32), shape=[2, 6, 5]),\n        \'num_actions\': tf.constant([4, 3], dtype=tf.int32)\n    }\n    mask = policy_utilities.construct_mask_from_multiple_sources(\n        observations, None, [neural_constraint], 6)\n    implied_mask = [[1, 1, 1, 1, 0, 0], [1, 1, 1, 0, 0, 0]]\n    self.assertAllGreaterEqual(implied_mask - mask, 0)\n\n  def testComputeMaskFromMultipleSourcesMask(self):\n    observation_spec = bandit_spec_utils.create_per_arm_observation_spec(\n        4, 5, 6)\n    time_step_spec = ts.time_step_spec(observation_spec)\n    action_spec = tensor_spec.BoundedTensorSpec((), tf.int32, 0, 5)\n    constraint_net = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            observation_spec, (3, 4), (4, 3), (2, 3)))\n    neural_constraint = constraints.NeuralConstraint(\n        time_step_spec,\n        action_spec,\n        constraint_network=constraint_net)\n    original_mask = [[1, 1, 1, 1, 0, 0], [1, 1, 1, 0, 0, 0]]\n    observations = ({\n        \'global\': tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=tf.float32),\n        \'per_arm\': tf.reshape(tf.range(60, dtype=tf.float32), shape=[2, 6, 5]),\n    }, original_mask)\n    mask = policy_utilities.construct_mask_from_multiple_sources(\n        observations, lambda x: (x[0], x[1]), [neural_constraint], 6)\n    self.assertAllGreaterEqual(original_mask - mask, 0)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/specs/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module importing all specs modules.""""""\n\nfrom tf_agents.bandits.specs import utils\n'"
tf_agents/bandits/specs/utils.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Bandit related tensor spec utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nfrom absl import logging\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.specs import tensor_spec\n\nGLOBAL_FEATURE_KEY = \'global\'\nPER_ARM_FEATURE_KEY = \'per_arm\'\nNUM_ACTIONS_FEATURE_KEY = \'num_actions\'\n\n# For constrained optimization, the reward spec is expected to be a dictionary\n# with the following keys that split the reward spec and the constraints spec.\nREWARD_SPEC_KEY = \'reward\'\nCONSTRAINTS_SPEC_KEY = \'constraint\'\n\n\ndef create_per_arm_observation_spec(global_dim,\n                                    per_arm_dim,\n                                    max_num_actions=None,\n                                    add_num_actions_feature=False,\n                                    add_action_mask=False):\n  """"""Creates an observation spec with per-arm features and possibly action mask.\n\n  Args:\n    global_dim: (int) The global feature dimension.\n    per_arm_dim: (int) The per-arm feature dimension.\n    max_num_actions: If specified (int), this is the maximum number of actions\n      in any sample, and the num_actions dimension of the per-arm features\n      will be set to this number. The actual number of actions for a given\n      sample can be lower than this parameter: it can be specified via the\n      NUM_ACTIONS_FEATURE_KEY, or an action mask.\n    add_num_actions_feature: (bool) whether to use the `num_actions` feature key\n      to encode the number of actions per sample.\n    add_action_mask: (bool) whether to use an action mask to encode the number\n      of actions per sample. This option is discouraged for problems with per-\n      arm features, as the `num_actions` feature key is more natural. Using the\n      feature and the mask together is prohibited.\n\n  Returns:\n    A nested structure of observation spec.\n  """"""\n  assert not (\n      add_num_actions_feature and add_action_mask\n  ), \'Action mask and `num_actions` feature key can not be used together.\'\n  global_obs_spec = tensor_spec.TensorSpec((global_dim,), tf.float32)\n  arm_obs_spec = tensor_spec.TensorSpec((max_num_actions, per_arm_dim),\n                                        tf.float32)\n  observation_spec = {GLOBAL_FEATURE_KEY: global_obs_spec,\n                      PER_ARM_FEATURE_KEY: arm_obs_spec}\n  if add_num_actions_feature:\n    observation_spec.update({\n        NUM_ACTIONS_FEATURE_KEY:\n            tensor_spec.BoundedTensorSpec((),\n                                          minimum=1,\n                                          maximum=max_num_actions,\n                                          dtype=tf.int32)\n    })\n  elif add_action_mask:\n    logging.warning(\'Action masking with per-arm features is discouraged. \'\n                    \'Instead, use variable number of actions via the `%s` \'\n                    \'feature key.\', NUM_ACTIONS_FEATURE_KEY)\n    mask_spec = tensor_spec.BoundedTensorSpec(\n        shape=(max_num_actions,), minimum=0, maximum=1, dtype=tf.int32)\n    observation_spec = (observation_spec, mask_spec)\n  return observation_spec\n\n\ndef get_context_dims_from_spec(context_spec, accepts_per_arm_features):\n  """"""Returns the global and per-arm context dimensions.\n\n  If the policy accepts per-arm features, this function returns the tuple of\n  the global and per-arm context dimension. Otherwise, it returns the (global)\n  context dim and zero.\n\n  Args:\n    context_spec: A nest of tensor specs, containing the observation spec.\n    accepts_per_arm_features: (bool) Whether the context_spec is for a policy\n      that accepts per-arm features.\n\n  Returns: A 2-tuple of ints, the global and per-arm context dimension. If the\n    policy does not accept per-arm features, the per-arm context dim is 0.\n  """"""\n  if accepts_per_arm_features:\n    global_context_dim = context_spec[GLOBAL_FEATURE_KEY].shape.as_list()[0]\n    arm_context_dim = context_spec[PER_ARM_FEATURE_KEY].shape.as_list()[1]\n  else:\n    spec_shape = context_spec.shape.as_list()\n    global_context_dim = spec_shape[0] if spec_shape else 1\n    arm_context_dim = 0\n  return global_context_dim, arm_context_dim\n\n\ndef drop_arm_observation(trajectory,\n                         observation_and_action_constraint_splitter=None):\n  """"""Drops the per-arm observation from a given trajectory (or trajectory spec).""""""\n  transformed_trajectory = copy.deepcopy(trajectory)\n  if observation_and_action_constraint_splitter is None:\n    del transformed_trajectory.observation[PER_ARM_FEATURE_KEY]\n  else:\n    del observation_and_action_constraint_splitter(\n        transformed_trajectory.observation)[0][PER_ARM_FEATURE_KEY]\n  return transformed_trajectory\n'"
tf_agents/environments/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/environments/examples/masked_cartpole.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Example registering of a new Gym environment.\n\nSee agents/dqn/examples/train_eval_gym_rnn.py for usage.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gym\nfrom gym.envs.classic_control import cartpole\nfrom gym.envs.registration import register\nimport numpy as np\n\n\nclass MaskedCartPoleEnv(cartpole.CartPoleEnv):\n  """"""Cartpole environment with masked velocity components.\n\n  This environment is useful as a unit tests for agents that utilize recurrent\n  networks.\n  """"""\n\n  def __init__(self):\n    super(MaskedCartPoleEnv, self).__init__()\n    high = np.array([\n        self.x_threshold * 2,\n        self.theta_threshold_radians * 2,\n    ])\n\n    self.observation_space = gym.spaces.Box(-high, high)\n\n  def _mask_observation(self, observation):\n    return observation[[0, 2]]\n\n  def reset(self):\n    observation = super(MaskedCartPoleEnv, self).reset()\n    # Get rid of velocity components at index 1, and 3.\n    return self._mask_observation(observation)\n\n  def step(self, action):\n    observation, reward, done, info = super(MaskedCartPoleEnv,\n                                            self).step(action)\n    # Get rid of velocity components at index 1, and 3.\n    return self._mask_observation(observation), reward, done, info\n\n\nregister(\n    id=\'MaskedCartPole-v0\',\n    entry_point=MaskedCartPoleEnv,\n    max_episode_steps=200,\n    reward_threshold=195.0,\n)\n\nregister(\n    id=\'MaskedCartPole-v1\',\n    entry_point=MaskedCartPoleEnv,\n    max_episode_steps=500,\n    reward_threshold=475.0,\n)\n'"
tf_agents/environments/examples/tic_tac_toe_environment.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""A state-settable environment for Tic-Tac-Toe game.""""""\n\nimport copy\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import BoundedArraySpec\nfrom tf_agents.trajectories.time_step import StepType\nfrom tf_agents.trajectories.time_step import TimeStep\n\n\nclass TicTacToeEnvironment(py_environment.PyEnvironment):\n  """"""A state-settable environment for Tic-Tac-Toe game.\n\n  For MCTS/AlphaZero, we need to keep states of the environment in a node and\n  later restore them once MCTS selects which node to visit. This requires\n  calling into get_state() and set_state() functions.\n\n  The states are a 3 x 3 array where 0 = empty, 1 = player, 2 = opponent.\n  The action is a 2-d vector to indicate the position for the player\'s move.\n  """"""\n  REWARD_WIN = np.asarray(1., dtype=np.float32)\n  REWARD_LOSS = np.asarray(-1., dtype=np.float32)\n  REWARD_DRAW_OR_NOT_FINAL = np.asarray(0., dtype=np.float32)\n  # A very small number such that it does not affect the value calculation.\n  REWARD_ILLEGAL_MOVE = np.asarray(-.001, dtype=np.float32)\n\n  REWARD_WIN.setflags(write=False)\n  REWARD_LOSS.setflags(write=False)\n  REWARD_DRAW_OR_NOT_FINAL.setflags(write=False)\n\n  def __init__(self, rng: np.random.RandomState = None, discount=1.0):\n    """"""Initializes TicTacToeEnvironment.\n\n    Args:\n      rng: If a random generator is provided, the opponent will choose a random\n        empty space. If None is provided, the opponent will choose the first\n        empty space.\n      discount: Discount for reward.\n    """"""\n    super(TicTacToeEnvironment, self).__init__()\n    self._rng = rng\n    self._discount = np.asarray(discount, dtype=np.float32)\n\n    self._states = None\n\n  def action_spec(self):\n    return BoundedArraySpec((2,), np.int32, minimum=0, maximum=2)\n\n  def observation_spec(self):\n    return BoundedArraySpec((3, 3), np.int32, minimum=0, maximum=2)\n\n  def _reset(self):\n    self._states = np.zeros((3, 3), np.int32)\n    return TimeStep(StepType.FIRST, np.asarray(0.0, dtype=np.float32),\n                    self._discount, self._states)\n\n  def _legal_actions(self, states: np.ndarray):\n    return list(zip(*np.where(states == 0)))\n\n  def _opponent_play(self, states: np.ndarray):\n    actions = self._legal_actions(np.array(states))\n    if not actions:\n      raise RuntimeError(\'There is no empty space for opponent to play at.\')\n\n    if self._rng:\n      i = self._rng.randint(len(actions))\n    else:\n      i = 0\n    return actions[i]\n\n  def get_state(self) -> TimeStep:\n    # Returning an unmodifiable copy of the state.\n    return copy.deepcopy(self._current_time_step)\n\n  def set_state(self, time_step: TimeStep):\n    self._current_time_step = time_step\n    self._states = time_step.observation\n\n  def _step(self, action: np.ndarray):\n    if self._current_time_step.is_last():\n      return self._reset()\n\n    action = tuple(action)\n    if self._states[action] != 0:\n      return TimeStep(StepType.LAST, TicTacToeEnvironment.REWARD_ILLEGAL_MOVE,\n                      self._discount, self._states)\n\n    self._states[action] = 1\n\n    is_final, reward = self._check_states(self._states)\n    if is_final:\n      return TimeStep(StepType.LAST, reward, self._discount,\n                      self._states)\n\n    # TODO(b/152638947): handle multiple agents properly.\n    # Opponent places \'2\' on the board.\n    opponent_action = self._opponent_play(self._states)\n    self._states[opponent_action] = 2\n\n    is_final, reward = self._check_states(self._states)\n\n    step_type = StepType.MID\n    if np.all(self._states == 0):\n      step_type = StepType.FIRST\n    elif is_final:\n      step_type = StepType.LAST\n\n    return TimeStep(step_type, reward, self._discount, self._states)\n\n  def _check_states(self, states: np.ndarray):\n    """"""Check if the given states are final and calculate reward.\n\n    Args:\n      states: states of the board.\n\n    Returns:\n      A tuple of (is_final, reward) where is_final means whether the states\n      are final are not, and reward is the reward for stepping into the states\n      The meaning of reward: 0 = not decided or draw, 1 = win, -1 = loss\n    """"""\n    seqs = np.array([\n        # each row\n        states[0, :], states[1, :], states[2, :],\n        # each column\n        states[:, 0], states[:, 1], states[:, 2],\n        # diagonal\n        states[(0, 1, 2), (0, 1, 2)],\n        states[(2, 1, 0), (0, 1, 2)],\n    ])\n    seqs = seqs.tolist()\n    if [1, 1, 1] in seqs:\n      return True, TicTacToeEnvironment.REWARD_WIN  # win\n    if [2, 2, 2] in seqs:\n      return True, TicTacToeEnvironment.REWARD_LOSS  # loss\n    if 0 in states:\n      # Not final\n      return False, TicTacToeEnvironment.REWARD_DRAW_OR_NOT_FINAL\n    return True, TicTacToeEnvironment.REWARD_DRAW_OR_NOT_FINAL  # draw\n'"
tf_agents/environments/examples/tic_tac_toe_environment_test.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Tests for tf_agents.environments.examples.tic_tac_toe_environment.""""""\n\nimport numpy as np\n\nfrom tf_agents.environments import utils as env_utils\nfrom tf_agents.environments.examples.tic_tac_toe_environment import TicTacToeEnvironment\nfrom tf_agents.trajectories.time_step import StepType\nfrom tf_agents.trajectories.time_step import TimeStep\nfrom tf_agents.utils import test_utils\n\n\nclass TicTacToeEnvironmentTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(TicTacToeEnvironmentTest, self).setUp()\n    np.random.seed(0)\n    self.discount = np.asarray(1., dtype=np.float32)\n    self.env = TicTacToeEnvironment()\n    ts = self.env.reset()\n    np.testing.assert_array_equal(np.zeros((3, 3), np.int32), ts.observation)\n\n  def test_validate_specs(self):\n    env_utils.validate_py_environment(self.env, episodes=10)\n\n  def test_check_states(self):\n    self.assertEqual(\n        (False, 0.),\n        self.env._check_states(np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]])))\n    self.assertEqual(\n        (True, -1.),\n        self.env._check_states(np.array([[2, 2, 2], [0, 1, 1], [0, 0, 0]])))\n    self.assertEqual(\n        (True, 1.),\n        self.env._check_states(np.array([[2, 2, 0], [1, 1, 1], [0, 0, 0]])))\n    self.assertEqual(\n        (False, 0.),\n        self.env._check_states(np.array([[2, 2, 1], [1, 2, 1], [1, 0, 0]])))\n    self.assertEqual(\n        (True, 0.),\n        self.env._check_states(np.array([[2, 1, 2], [1, 2, 1], [1, 2, 1]])))\n\n  def test_legal_actions(self):\n    states = np.array([[0, 0, 0], [1, 0, 0], [2, 1, 0]])\n    self.assertEqual([(0, 0), (0, 1), (0, 2), (1, 1), (1, 2), (2, 2)],\n                     self.env._legal_actions(states))\n\n  def test_opponent_play_deterministic(self):\n    # Chooses the first available space.\n    self.assertEqual((0, 0),\n                     self.env._opponent_play([[0, 0, 0], [0, 0, 0], [0, 0, 1]]))\n    self.assertEqual((2, 2),\n                     self.env._opponent_play([[1, 1, 1], [1, 1, 1], [1, 1, 0]]))\n\n  def test_opponent_play_random(self):\n    self.env = TicTacToeEnvironment(rng=np.random.RandomState(0))\n    s = set()\n    states = np.array([[0, 1, 2], [0, 0, 0], [0, 0, 0]])\n    legal_actions = self.env._legal_actions(states)\n\n    # Make sure that each legal action has been played.\n    for _ in range(100):\n      s.add(self.env._opponent_play(states))\n    self.assertEqual(set(legal_actions), s)\n\n  def test_step_win(self):\n    self.env.set_state(\n        TimeStep(StepType.MID, TicTacToeEnvironment.REWARD_DRAW_OR_NOT_FINAL,\n                 self.discount, np.array([[2, 2, 0], [0, 1, 1], [0, 0, 0]])))\n\n    current_time_step = self.env.current_time_step()\n    self.assertEqual(StepType.MID, current_time_step.step_type)\n\n    ts = self.env.step(np.array([1, 0]))\n\n    np.testing.assert_array_equal([[2, 2, 0], [1, 1, 1], [0, 0, 0]],\n                                  ts.observation)\n    self.assertEqual(StepType.LAST, ts.step_type)\n    self.assertEqual(1., ts.reward)\n\n    # Reset if an action is taken after final state is reached.\n    ts = self.env.step(np.array([2, 0]))\n    self.assertEqual(StepType.FIRST, ts.step_type)\n    self.assertEqual(0., ts.reward)\n\n  def test_step_loss(self):\n    self.env.set_state(\n        TimeStep(StepType.MID, TicTacToeEnvironment.REWARD_DRAW_OR_NOT_FINAL,\n                 self.discount, np.array([[2, 2, 0], [0, 1, 1], [0, 0, 0]])))\n\n    current_time_step = self.env.current_time_step()\n    self.assertEqual(StepType.MID, current_time_step.step_type)\n\n    ts = self.env.step(np.array([2, 0]))\n\n    np.testing.assert_array_equal([[2, 2, 2], [0, 1, 1], [1, 0, 0]],\n                                  ts.observation)\n    self.assertEqual(StepType.LAST, ts.step_type)\n    self.assertEqual(-1., ts.reward)\n\n    # Reset if an action is taken after final state is reached.\n    ts = self.env.step(np.array([2, 0]))\n    self.assertEqual(StepType.FIRST, ts.step_type)\n    self.assertEqual(0., ts.reward)\n\n  def test_step_illegal_move(self):\n    self.env.set_state(\n        TimeStep(StepType.MID, TicTacToeEnvironment.REWARD_DRAW_OR_NOT_FINAL,\n                 self.discount, np.array([[2, 2, 0], [0, 1, 1], [0, 0, 0]])))\n\n    current_time_step = self.env.current_time_step()\n    self.assertEqual(StepType.MID, current_time_step.step_type)\n\n    # Taking an illegal move.\n    ts = self.env.step(np.array([0, 0]))\n\n    np.testing.assert_array_equal([[2, 2, 0], [0, 1, 1], [0, 0, 0]],\n                                  ts.observation)\n    self.assertEqual(StepType.LAST, ts.step_type)\n    self.assertEqual(TicTacToeEnvironment.REWARD_ILLEGAL_MOVE, ts.reward)\n\n    # Reset if an action is taken after final state is reached.\n    ts = self.env.step(np.array([2, 0]))\n    self.assertEqual(StepType.FIRST, ts.step_type)\n    self.assertEqual(0., ts.reward)\n\n\nif __name__ == \'__main__\':\n  test_utils.main()\n'"
tf_agents/system/default/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n'"
tf_agents/system/default/multiprocessing_core.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n""""""Multiprocessing hooks for TF-Agents.""""""\n\nimport abc\nimport multiprocessing as _multiprocessing\n\nfrom typing import Any, Text\n\n__all__ = [\n    \'StateSaver\',\n    \'handle_main\',\n    \'handle_test_main\',\n    \'enable_interactive_mode\',\n]\n\n_INITIALIZED = [False]\n_INTERACTIVE = [False]\n_STATE_SAVERS = []\n\n\ndef initialized():\n  return _INITIALIZED[0]\n\n\nclass StateSaver(object):\n  """"""Class for getting and setting global state.""""""\n\n  @abc.abstractmethod\n  def collect_state(self) -> Any:\n    pass\n\n  @abc.abstractmethod\n  def restore_state(self, state: Any) -> None:\n    pass\n\n\ndef get_context(method: Text = None) -> _multiprocessing.context.BaseContext:\n  return _multiprocessing.get_context(method)\n\n\ndef handle_main(parent_main_fn, *args, **kwargs):\n  """"""Function that wraps the main function in a multiprocessing-friendly way.\n\n  This function additionally accepts an `extra_state_savers` kwarg;\n  users can provide a list of `tf_agents.multiprocessing.StateSaver` instances,\n  where a `StateSaver` tells multiprocessing how to store some global state\n  and how to restore it in the subprocess.\n\n  Args:\n    parent_main_fn: A callable.\n    *args: rgs for `parent_main_fn`.\n    **kwargs: kwargs for `parent_main_fn`.\n      This may also include `extra_state_savers` kwarg.\n\n  Returns:\n    Output of `parent_main_fn`.\n  """"""\n  extra_state_savers = kwargs.pop(\'extra_state_savers\', [])\n  _STATE_SAVERS.extend(extra_state_savers)\n  _INITIALIZED[0] = True\n  return parent_main_fn(*args, **kwargs)\n\n\ndef handle_test_main(parent_main_fn, *args, **kwargs):\n  """"""Function that wraps the test main in a multiprocessing-friendly way.\n\n  This function additionally accepts an `extra_state_savers` kwarg;\n  users can provide a list of `tf_agents.multiprocessing.StateSaver` instances,\n  where a `StateSaver` tells multiprocessing how to store some global state\n  and how to restore it in the subprocess.\n\n  Args:\n    parent_main_fn: A callable.\n    *args: rgs for `parent_main_fn`.\n    **kwargs: kwargs for `parent_main_fn`.\n      This may also include `extra_state_savers` kwarg.\n\n  Returns:\n    Output of `parent_main_fn`.\n  """"""\n  extra_state_savers = kwargs.pop(\'extra_state_savers\', [])\n  _STATE_SAVERS.extend(extra_state_savers)\n  _INITIALIZED[0] = True\n  return parent_main_fn(*args, **kwargs)\n\n\ndef enable_interactive_mode(extra_state_savers=None):\n  """"""Function that enables multiprocessing in interactive mode.\n\n  This function accepts an `extra_state_savers` argument;\n  users can provide a list of `tf_agents.multiprocessing.StateSaver` instances,\n  where a `StateSaver` tells multiprocessing how to store some global state\n  and how to restore it in the subprocess.\n\n  Args:\n    extra_state_savers: A list of `StateSaver` instances.\n  """"""\n  if _INITIALIZED[0]:\n    raise ValueError(\'Multiprocessing already initialized\')\n  extra_state_savers = extra_state_savers or []\n  _STATE_SAVERS.extend(extra_state_savers)\n  _INITIALIZED[0] = True\n'"
tf_agents/agents/categorical_dqn/examples/train_eval_atari.py,21,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval Categorical DQN on Atari environments.\n\nTraining and evaluation proceeds alternately in iterations, where each\niteration consists of a 1M frame training phase followed by a 500K frame\nevaluation phase. In the literature, some papers report averages of the train\nphases, while others report averages of the eval phases.\n\nThis example is configured to use dopamine.atari.preprocessing, which, among\nother things, repeats every action it receives for 4 frames, and then returns\nthe max-pool over the last 2 frames in the group. In this example, when we\nrefer to ""ALE frames"" we refer to the frames before the max-pooling step (i.e.\nthe raw data available for processing). Because of this, many of the\nconfiguration parameters (like initial_collect_steps) are divided by 4 in the\nbody of the trainer (e.g. if you want to evaluate with 400 frames in the\ninitial collection, you actually only need to .step the environment 100 times).\n\nFor a good survey of training on Atari, see Machado, et al. 2017:\nhttps://arxiv.org/pdf/1709.06009.pdf.\n\nTo run:\n\n```bash\ntf_agents/agents/categorical_dqn/examples/train_eval_atari \\\n --root_dir=$HOME/atari/pong \\\n --atari_roms_path=/tmp\n --alsologtostderr\n```\n\nAdditional flags are available such as `--replay_buffer_capacity` and\n`--n_step_update`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.categorical_dqn import categorical_dqn_agent\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.networks import categorical_q_network\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.replay_buffers import py_hashed_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import timer\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_string(\'game_name\', \'Pong\', \'Name of Atari game to run.\')\nflags.DEFINE_integer(\'num_iterations\', None,\n                     \'Number of train/eval iterations to run.\')\nflags.DEFINE_integer(\'initial_collect_steps\', None,\n                     \'Number of frames to ALE frames to process before \'\n                     \'beginning to train. Since this is in ALE frames, there \'\n                     \'will be initial_collect_steps/4 items in the replay \'\n                     \'buffer when training starts.\')\nflags.DEFINE_integer(\'replay_buffer_capacity\', None,\n                     \'Maximum number of items to store in the replay buffer.\')\nflags.DEFINE_integer(\'train_steps_per_iteration\', None,\n                     \'Number of ALE frames to run through for each iteration \'\n                     \'of training.\')\nflags.DEFINE_integer(\'n_step_update\', None, \'The number of steps to consider \'\n                     \'when computing TD error and TD loss.\')\nflags.DEFINE_integer(\'eval_steps_per_iteration\', None,\n                     \'Number of ALE frames to run through for each iteration \'\n                     \'of evaluation.\')\nFLAGS = flags.FLAGS\n\n# AtariPreprocessing runs 4 frames at a time, max-pooling over the last 2\n# frames. We need to account for this when computing things like update\n# intervals.\nATARI_FRAME_SKIP = 4\n\n\nclass AtariCategoricalQNetwork(categorical_q_network.CategoricalQNetwork):\n  """"""CategoricalQNetwork subclass that divides observations by 255.""""""\n\n  def call(self, observation, step_type=None, network_state=()):\n    state = tf.cast(observation, tf.float32)\n    # We divide the grayscale pixel values by 255 here rather than storing\n    # normalized values beause uint8s are 4x cheaper to store than float32s.\n    # TODO(b/129805821): handle the division by 255 for train_eval_atari.py in\n    # a preprocessing layer instead.\n    state = state / 255\n    return super(AtariCategoricalQNetwork, self).call(\n        state, step_type=step_type, network_state=network_state)\n\n\ndef log_metric(metric, prefix):\n  tag = common.join_scope(prefix, metric.name)\n  logging.info(\'%s\', \'{0} = {1}\'.format(tag, metric.result()))\n\n\n@gin.configurable\nclass TrainEval(object):\n  """"""Train and evaluate DQN on Atari.""""""\n\n  def __init__(\n      self,\n      root_dir,\n      env_name,\n      num_iterations=200,\n      max_episode_frames=108000,  # ALE frames\n      terminal_on_life_loss=False,\n      conv_layer_params=((32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)),\n      fc_layer_params=(512,),\n      # Params for collect\n      initial_collect_steps=80000,  # ALE frames\n      epsilon_greedy=0.01,\n      epsilon_decay_period=1000000,  # ALE frames\n      replay_buffer_capacity=1000000,\n      # Params for train\n      train_steps_per_iteration=1000000,  # ALE frames\n      update_period=16,  # ALE frames\n      target_update_tau=1.0,\n      target_update_period=32000,  # ALE frames\n      batch_size=32,\n      learning_rate=2.5e-4,\n      n_step_update=2,\n      gamma=0.99,\n      reward_scale_factor=1.0,\n      gradient_clipping=None,\n      # Params for eval\n      do_eval=True,\n      eval_steps_per_iteration=500000,  # ALE frames\n      eval_epsilon_greedy=0.001,\n      # Params for checkpoints, summaries, and logging\n      log_interval=1000,\n      summary_interval=1000,\n      summaries_flush_secs=10,\n      debug_summaries=True,\n      summarize_grads_and_vars=True,\n      eval_metrics_callback=None):\n    """"""A simple Atari train and eval for DQN.\n\n    Args:\n      root_dir: Directory to write log files to.\n      env_name: Fully-qualified name of the Atari environment (i.e. Pong-v0).\n      num_iterations: Number of train/eval iterations to run.\n      max_episode_frames: Maximum length of a single episode, in ALE frames.\n      terminal_on_life_loss: Whether to simulate an episode termination when a\n        life is lost.\n      conv_layer_params: Params for convolutional layers of QNetwork.\n      fc_layer_params: Params for fully connected layers of QNetwork.\n      initial_collect_steps: Number of frames to ALE frames to process before\n        beginning to train. Since this is in ALE frames, there will be\n        initial_collect_steps/4 items in the replay buffer when training starts.\n      epsilon_greedy: Final epsilon value to decay to for training.\n      epsilon_decay_period: Period over which to decay epsilon, from 1.0 to\n        epsilon_greedy (defined above).\n      replay_buffer_capacity: Maximum number of items to store in the replay\n        buffer.\n      train_steps_per_iteration: Number of ALE frames to run through for each\n        iteration of training.\n      update_period: Run a train operation every update_period ALE frames.\n      target_update_tau: Coeffecient for soft target network updates (1.0 ==\n        hard updates).\n      target_update_period: Period, in ALE frames, to copy the live network to\n        the target network.\n      batch_size: Number of frames to include in each training batch.\n      learning_rate: RMS optimizer learning rate.\n      n_step_update: The number of steps to consider when computing TD error and\n        TD loss. Applies standard single-step updates when set to 1.\n      gamma: Discount for future rewards.\n      reward_scale_factor: Scaling factor for rewards.\n      gradient_clipping: Norm length to clip gradients.\n      do_eval: If True, run an eval every iteration. If False, skip eval.\n      eval_steps_per_iteration: Number of ALE frames to run through for each\n        iteration of evaluation.\n      eval_epsilon_greedy: Epsilon value to use for the evaluation policy (0 ==\n        totally greedy policy).\n      log_interval: Log stats to the terminal every log_interval training\n        steps.\n      summary_interval: Write TF summaries every summary_interval training\n        steps.\n      summaries_flush_secs: Flush summaries to disk every summaries_flush_secs\n        seconds.\n      debug_summaries: If True, write additional summaries for debugging (see\n        dqn_agent for which summaries are written).\n      summarize_grads_and_vars: Include gradients in summaries.\n      eval_metrics_callback: A callback function that takes (metric_dict,\n        global_step) as parameters. Called after every eval with the results of\n        the evaluation.\n    """"""\n    self._update_period = update_period / ATARI_FRAME_SKIP\n    self._train_steps_per_iteration = (train_steps_per_iteration\n                                       / ATARI_FRAME_SKIP)\n    self._do_eval = do_eval\n    self._eval_steps_per_iteration = eval_steps_per_iteration / ATARI_FRAME_SKIP\n    self._eval_epsilon_greedy = eval_epsilon_greedy\n    self._initial_collect_steps = initial_collect_steps / ATARI_FRAME_SKIP\n    self._summary_interval = summary_interval\n    self._num_iterations = num_iterations\n    self._log_interval = log_interval\n    self._eval_metrics_callback = eval_metrics_callback\n\n    with gin.unlock_config():\n      gin.bind_parameter((\'tf_agents.environments.atari_preprocessing.\'\n                          \'AtariPreprocessing.terminal_on_life_loss\'),\n                         terminal_on_life_loss)\n\n    root_dir = os.path.expanduser(root_dir)\n    train_dir = os.path.join(root_dir, \'train\')\n    eval_dir = os.path.join(root_dir, \'eval\')\n\n    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n        train_dir, flush_millis=summaries_flush_secs * 1000)\n    train_summary_writer.set_as_default()\n    self._train_summary_writer = train_summary_writer\n\n    self._eval_summary_writer = None\n    if self._do_eval:\n      self._eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n          eval_dir, flush_millis=summaries_flush_secs * 1000)\n      self._eval_metrics = [\n          py_metrics.AverageReturnMetric(\n              name=\'PhaseAverageReturn\', buffer_size=np.inf),\n          py_metrics.AverageEpisodeLengthMetric(\n              name=\'PhaseAverageEpisodeLength\', buffer_size=np.inf),\n      ]\n\n    self._global_step = tf.compat.v1.train.get_or_create_global_step()\n    with tf.compat.v2.summary.record_if(\n        lambda: tf.math.equal(self._global_step % self._summary_interval, 0)):\n      self._env = suite_atari.load(\n          env_name,\n          max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n          gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n      self._env = batched_py_environment.BatchedPyEnvironment([self._env])\n\n      observation_spec = tensor_spec.from_spec(self._env.observation_spec())\n      time_step_spec = ts.time_step_spec(observation_spec)\n      action_spec = tensor_spec.from_spec(self._env.action_spec())\n\n      with tf.device(\'/cpu:0\'):\n        epsilon = tf.compat.v1.train.polynomial_decay(\n            1.0,\n            self._global_step,\n            epsilon_decay_period / ATARI_FRAME_SKIP / self._update_period,\n            end_learning_rate=epsilon_greedy)\n\n      with tf.device(\'/gpu:0\'):\n        optimizer = tf.compat.v1.train.RMSPropOptimizer(\n            learning_rate=learning_rate,\n            decay=0.95,\n            momentum=0.0,\n            epsilon=0.00001,\n            centered=True)\n        categorical_q_net = AtariCategoricalQNetwork(\n            observation_spec,\n            action_spec,\n            conv_layer_params=conv_layer_params,\n            fc_layer_params=fc_layer_params)\n        agent = categorical_dqn_agent.CategoricalDqnAgent(\n            time_step_spec,\n            action_spec,\n            categorical_q_network=categorical_q_net,\n            optimizer=optimizer,\n            epsilon_greedy=epsilon,\n            n_step_update=n_step_update,\n            target_update_tau=target_update_tau,\n            target_update_period=(\n                target_update_period / ATARI_FRAME_SKIP / self._update_period),\n            gamma=gamma,\n            reward_scale_factor=reward_scale_factor,\n            gradient_clipping=gradient_clipping,\n            debug_summaries=debug_summaries,\n            summarize_grads_and_vars=summarize_grads_and_vars,\n            train_step_counter=self._global_step)\n\n        self._collect_policy = py_tf_policy.PyTFPolicy(agent.collect_policy)\n\n        if self._do_eval:\n          self._eval_policy = py_tf_policy.PyTFPolicy(\n              epsilon_greedy_policy.EpsilonGreedyPolicy(\n                  policy=agent.policy,\n                  epsilon=self._eval_epsilon_greedy))\n\n        py_observation_spec = self._env.observation_spec()\n        py_time_step_spec = ts.time_step_spec(py_observation_spec)\n        py_action_spec = policy_step.PolicyStep(self._env.action_spec())\n        data_spec = trajectory.from_transition(\n            py_time_step_spec, py_action_spec, py_time_step_spec)\n        self._replay_buffer = py_hashed_replay_buffer.PyHashedReplayBuffer(\n            data_spec=data_spec, capacity=replay_buffer_capacity)\n\n      with tf.device(\'/cpu:0\'):\n        ds = self._replay_buffer.as_dataset(\n            sample_batch_size=batch_size, num_steps=n_step_update + 1)\n        ds = ds.prefetch(4)\n        ds = ds.apply(tf.data.experimental.prefetch_to_device(\'/gpu:0\'))\n\n      with tf.device(\'/gpu:0\'):\n        self._ds_itr = tf.compat.v1.data.make_one_shot_iterator(ds)\n        experience = self._ds_itr.get_next()\n        self._train_op = agent.train(experience)\n\n        self._env_steps_metric = py_metrics.EnvironmentSteps()\n        self._step_metrics = [\n            py_metrics.NumberOfEpisodes(),\n            self._env_steps_metric,\n        ]\n        self._train_metrics = self._step_metrics + [\n            py_metrics.AverageReturnMetric(buffer_size=10),\n            py_metrics.AverageEpisodeLengthMetric(buffer_size=10),\n        ]\n        # The _train_phase_metrics average over an entire train iteration,\n        # rather than the rolling average of the last 10 episodes.\n        self._train_phase_metrics = [\n            py_metrics.AverageReturnMetric(\n                name=\'PhaseAverageReturn\', buffer_size=np.inf),\n            py_metrics.AverageEpisodeLengthMetric(\n                name=\'PhaseAverageEpisodeLength\', buffer_size=np.inf),\n        ]\n        self._iteration_metric = py_metrics.CounterMetric(name=\'Iteration\')\n\n        # Summaries written from python should run every time they are\n        # generated.\n        with tf.compat.v2.summary.record_if(True):\n          self._steps_per_second_ph = tf.compat.v1.placeholder(\n              tf.float32, shape=(), name=\'steps_per_sec_ph\')\n          self._steps_per_second_summary = tf.compat.v2.summary.scalar(\n              name=\'global_steps_per_sec\', data=self._steps_per_second_ph,\n              step=self._global_step)\n\n          for metric in self._train_metrics:\n            metric.tf_summaries(\n                train_step=self._global_step, step_metrics=self._step_metrics)\n\n          for metric in self._train_phase_metrics:\n            metric.tf_summaries(\n                train_step=self._global_step,\n                step_metrics=(self._iteration_metric,))\n          self._iteration_metric.tf_summaries(train_step=self._global_step)\n\n          if self._do_eval:\n            with self._eval_summary_writer.as_default():\n              for metric in self._eval_metrics:\n                metric.tf_summaries(\n                    train_step=self._global_step,\n                    step_metrics=(self._iteration_metric,))\n\n        self._train_checkpointer = common.Checkpointer(\n            ckpt_dir=train_dir,\n            agent=agent,\n            global_step=self._global_step,\n            optimizer=optimizer,\n            metrics=metric_utils.MetricsGroup(\n                self._train_metrics + self._train_phase_metrics +\n                [self._iteration_metric], \'train_metrics\'))\n        self._policy_checkpointer = common.Checkpointer(\n            ckpt_dir=os.path.join(train_dir, \'policy\'),\n            policy=agent.policy,\n            global_step=self._global_step)\n        self._rb_checkpointer = common.Checkpointer(\n            ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n            max_to_keep=1,\n            replay_buffer=self._replay_buffer)\n\n        self._init_agent_op = agent.initialize()\n\n  def game_over(self):\n    return self._env.envs[0].game_over\n\n  def run(self):\n    """"""Execute the train/eval loop.""""""\n    with tf.compat.v1.Session(\n        config=tf.compat.v1.ConfigProto(allow_soft_placement=True)) as sess:\n      # Initialize the graph.\n      self._initialize_graph(sess)\n\n      # Initial collect\n      self._initial_collect()\n\n      while self._iteration_metric.result() < self._num_iterations:\n        # Train phase\n        env_steps = 0\n        for metric in self._train_phase_metrics:\n          metric.reset()\n        while env_steps < self._train_steps_per_iteration:\n          env_steps += self._run_episode(\n              sess, self._train_metrics + self._train_phase_metrics, train=True)\n        for metric in self._train_phase_metrics:\n          log_metric(metric, prefix=\'Train/Metrics\')\n        py_metric.run_summaries(\n            self._train_phase_metrics + [self._iteration_metric])\n\n        global_step_val = sess.run(self._global_step)\n\n        if self._do_eval:\n          # Eval phase\n          env_steps = 0\n          for metric in self._eval_metrics:\n            metric.reset()\n          while env_steps < self._eval_steps_per_iteration:\n            env_steps += self._run_episode(\n                sess, self._eval_metrics, train=False)\n\n          py_metric.run_summaries(self._eval_metrics + [self._iteration_metric])\n          if self._eval_metrics_callback:\n            results = dict((metric.name, metric.result())\n                           for metric in self._eval_metrics)\n            self._eval_metrics_callback(results, global_step_val)\n          for metric in self._eval_metrics:\n            log_metric(metric, prefix=\'Eval/Metrics\')\n\n        self._iteration_metric()\n\n        self._train_checkpointer.save(global_step=global_step_val)\n        self._policy_checkpointer.save(global_step=global_step_val)\n        self._rb_checkpointer.save(global_step=global_step_val)\n\n  def _initialize_graph(self, sess):\n    """"""Initialize the graph for sess.""""""\n    self._train_checkpointer.initialize_or_restore(sess)\n    self._rb_checkpointer.initialize_or_restore(sess)\n    common.initialize_uninitialized_variables(sess)\n\n    sess.run(self._init_agent_op)\n\n    self._train_step_call = sess.make_callable(self._train_op)\n\n    self._collect_timer = timer.Timer()\n    self._train_timer = timer.Timer()\n    self._action_timer = timer.Timer()\n    self._step_timer = timer.Timer()\n    self._observer_timer = timer.Timer()\n\n    global_step_val = sess.run(self._global_step)\n    self._timed_at_step = global_step_val\n\n    # Call save to initialize the save_counter (need to do this before\n    # finalizing the graph).\n    self._train_checkpointer.save(global_step=global_step_val)\n    self._policy_checkpointer.save(global_step=global_step_val)\n    self._rb_checkpointer.save(global_step=global_step_val)\n    sess.run(self._train_summary_writer.init())\n\n    if self._do_eval:\n      sess.run(self._eval_summary_writer.init())\n\n  def _initial_collect(self):\n    """"""Collect initial experience before training begins.""""""\n    logging.info(\'Collecting initial experience...\')\n    time_step_spec = ts.time_step_spec(self._env.observation_spec())\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec, self._env.action_spec())\n    time_step = self._env.reset()\n    while self._replay_buffer.size < self._initial_collect_steps:\n      if self.game_over():\n        time_step = self._env.reset()\n      action_step = random_policy.action(time_step)\n      next_time_step = self._env.step(action_step.action)\n      self._replay_buffer.add_batch(trajectory.from_transition(\n          time_step, action_step, next_time_step))\n      time_step = next_time_step\n    logging.info(\'Done.\')\n\n  def _run_episode(self, sess, metric_observers, train=False):\n    """"""Run a single episode.""""""\n    env_steps = 0\n    time_step = self._env.reset()\n    while True:\n      with self._collect_timer:\n        time_step = self._collect_step(\n            time_step,\n            metric_observers,\n            train=train)\n        env_steps += 1\n\n      if self.game_over():\n        break\n      elif train and self._env_steps_metric.result() % self._update_period == 0:\n        with self._train_timer:\n          total_loss = self._train_step_call()\n          global_step_val = sess.run(self._global_step)\n        self._maybe_log(sess, global_step_val, total_loss)\n        self._maybe_record_summaries(global_step_val)\n\n    return env_steps\n\n  def _observe(self, metric_observers, traj):\n    with self._observer_timer:\n      for observer in metric_observers:\n        observer(traj)\n\n  def _store_to_rb(self, traj):\n    # Clip the reward to (-1, 1) to normalize rewards in training.\n    traj = traj._replace(\n        reward=np.asarray(np.clip(traj.reward, -1, 1)))\n    self._replay_buffer.add_batch(traj)\n\n  def _collect_step(self, time_step, metric_observers, train=False):\n    """"""Run a single step (or 2 steps on life loss) in the environment.""""""\n    if train:\n      policy = self._collect_policy\n    else:\n      policy = self._eval_policy\n\n    with self._action_timer:\n      action_step = policy.action(time_step)\n    with self._step_timer:\n      next_time_step = self._env.step(action_step.action)\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    if next_time_step.is_last() and not self.game_over():\n      traj = traj._replace(discount=np.array([1.0], dtype=np.float32))\n\n    if train:\n      self._store_to_rb(traj)\n\n    # When AtariPreprocessing.terminal_on_life_loss is True, we receive LAST\n    # time_steps when lives are lost but the game is not over. In this mode, the\n    # replay buffer and agent\'s policy must see the life loss as a LAST step\n    # and the subsequent step as a FIRST step. However, we do not want to\n    # actually terminate the episode and metrics should be computed as if all\n    # steps were MID steps, since life loss is not actually a terminal event\n    # (it is mostly a trick to make it easier to propagate rewards backwards by\n    # shortening episode durations from the agent\'s perspective).\n    if next_time_step.is_last() and not self.game_over():\n      # Update metrics as if this is a mid-episode step.\n      next_time_step = ts.transition(\n          next_time_step.observation, next_time_step.reward)\n      self._observe(metric_observers, trajectory.from_transition(\n          time_step, action_step, next_time_step))\n\n      # Produce the next step as if this is the first step of an episode and\n      # store to RB as such. The next_time_step will be a MID time step.\n      reward = time_step.reward\n      time_step = ts.restart(next_time_step.observation)\n      with self._action_timer:\n        action_step = policy.action(time_step)\n      with self._step_timer:\n        next_time_step = self._env.step(action_step.action)\n      if train:\n        self._store_to_rb(trajectory.from_transition(\n            time_step, action_step, next_time_step))\n\n      # Update metrics as if this is a mid-episode step.\n      time_step = ts.transition(time_step.observation, reward)\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    self._observe(metric_observers, traj)\n\n    return next_time_step\n\n  def _maybe_record_summaries(self, global_step_val):\n    """"""Record summaries if global_step_val is a multiple of summary_interval.""""""\n    if global_step_val % self._summary_interval == 0:\n      py_metric.run_summaries(self._train_metrics)\n\n  def _maybe_log(self, sess, global_step_val, total_loss):\n    """"""Log some stats if global_step_val is a multiple of log_interval.""""""\n    if global_step_val % self._log_interval == 0:\n      logging.info(\'step = %d, loss = %f\', global_step_val, total_loss.loss)\n      logging.info(\'%s\', \'action_time = {}\'.format(self._action_timer.value()))\n      logging.info(\'%s\', \'step_time = {}\'.format(self._step_timer.value()))\n      logging.info(\'%s\', \'observer_time = {}\'.format(\n          self._observer_timer.value()))\n      steps_per_sec = ((global_step_val - self._timed_at_step) /\n                       (self._collect_timer.value()\n                        + self._train_timer.value()))\n      sess.run(self._steps_per_second_summary,\n               feed_dict={self._steps_per_second_ph: steps_per_sec})\n      logging.info(\'%.3f steps/sec\', steps_per_sec)\n      logging.info(\'%s\', \'collect_time = {}, train_time = {}\'.format(\n          self._collect_timer.value(), self._train_timer.value()))\n      for metric in self._train_metrics:\n        log_metric(metric, prefix=\'Train/Metrics\')\n      self._timed_at_step = global_step_val\n      self._collect_timer.reset()\n      self._train_timer.reset()\n      self._action_timer.reset()\n      self._step_timer.reset()\n      self._observer_timer.reset()\n\n\ndef get_run_args():\n  """"""Builds a dict of run arguments from flags.""""""\n  run_args = {}\n  if FLAGS.num_iterations:\n    run_args[\'num_iterations\'] = FLAGS.num_iterations\n  if FLAGS.initial_collect_steps:\n    run_args[\'initial_collect_steps\'] = FLAGS.initial_collect_steps\n  if FLAGS.replay_buffer_capacity:\n    run_args[\'replay_buffer_capacity\'] = FLAGS.replay_buffer_capacity\n  if FLAGS.train_steps_per_iteration:\n    run_args[\'train_steps_per_iteration\'] = FLAGS.train_steps_per_iteration\n  if FLAGS.n_step_update:\n    run_args[\'n_step_update\'] = FLAGS.n_step_update\n  if FLAGS.eval_steps_per_iteration:\n    run_args[\'eval_steps_per_iteration\'] = FLAGS.eval_steps_per_iteration\n  return run_args\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  TrainEval(FLAGS.root_dir, suite_atari.game(name=FLAGS.game_name),\n            **get_run_args()).run()\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/ddpg/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/dqn/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ppo/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/reinforce/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/sac/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/td3/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/bandits/agents/examples/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ddpg/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ddpg/examples/v1/train_eval.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DDPG.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ddpg_v1/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/ddpg/examples/v1/train_eval.py \\\n  --root_dir=$HOME/tmp/ddpg_v1/gym/HalfCheetah-v2/ \\\n  --num_iterations=2000000 \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    eval_env_name=None,\n    env_load_fn=suite_mujoco.load,\n    num_iterations=2000000,\n    actor_fc_layers=(400, 300),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    num_parallel_environments=1,\n    replay_buffer_capacity=100000,\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=10000,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=20000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for DDPG.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if num_parallel_environments > 1:\n      tf_env = tf_py_environment.TFPyEnvironment(\n          parallel_py_environment.ParallelPyEnvironment(\n              [lambda: env_load_fn(env_name)] * num_parallel_environments))\n    else:\n      tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n    eval_env_name = eval_env_name or env_name\n    eval_py_env = env_load_fn(eval_env_name)\n\n    actor_net = actor_network.ActorNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers,\n    )\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_network.CriticNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n    )\n\n    tf_agent = ddpg_agent.DdpgAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        ou_stddev=ou_stddev,\n        ou_damping=ou_damping,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n    initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=initial_collect_steps).run()\n\n    collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=collect_steps_per_iteration).run()\n\n    # Dataset generates trajectories with shape [Bx2x...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=2).prefetch(3)\n\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    trajectories, unused_info = iterator.get_next()\n    train_fn = common.function(tf_agent.train)\n    train_op = train_fn(experience=trajectories)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      # TODO(b/126239733) Remove once Periodically can be saved.\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      sess.run(initial_collect_op)\n\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      global_step_call = sess.make_callable(global_step)\n\n      timed_at_step = sess.run(global_step)\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _ = train_step_call()\n        time_acc += time.time() - start_time\n        global_step_val = global_step_call()\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True,\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/ddpg/examples/v1/train_eval_rnn.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DDPG.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ddpg_rnn_v1/dm/CartPole-Balance/ --port 2223 &\n\npython tf_agents/agents/ddpg/examples/v1/train_eval_rnn.py \\\n  --root_dir=$HOME/tmp/ddpg_rnn_v1/dm/CartPole-Balance/ \\\n  --num_iterations=100000 \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'cartpole\',\n    task_name=\'balance\',\n    observations_whitelist=\'position\',\n    num_iterations=100000,\n    actor_fc_layers=(400, 300),\n    actor_output_fc_layers=(100,),\n    actor_lstm_size=(40,),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    critic_output_fc_layers=(100,),\n    critic_lstm_size=(40,),\n    # Params for collect\n    initial_collect_steps=1,\n    collect_episodes_per_iteration=1,\n    replay_buffer_capacity=100000,\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=200,\n    batch_size=64,\n    train_sequence_length=10,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=10000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for DDPG.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if observations_whitelist is not None:\n      env_wrappers = [\n          functools.partial(\n              wrappers.FlattenObservationsWrapper,\n              observations_whitelist=[observations_whitelist])\n      ]\n    else:\n      env_wrappers = []\n    environment = suite_dm_control.load(\n        env_name, task_name, env_wrappers=env_wrappers)\n\n    tf_env = tf_py_environment.TFPyEnvironment(environment)\n    eval_py_env = suite_dm_control.load(\n        env_name, task_name, env_wrappers=env_wrappers)\n\n    actor_net = actor_rnn_network.ActorRnnNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        input_fc_layer_params=actor_fc_layers,\n        lstm_size=actor_lstm_size,\n        output_fc_layer_params=actor_output_fc_layers)\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        lstm_size=critic_lstm_size,\n        output_fc_layer_params=critic_output_fc_layers,\n    )\n\n    tf_agent = ddpg_agent.DdpgAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        ou_stddev=ou_stddev,\n        ou_damping=ou_damping,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        dqda_clipping=dqda_clipping,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n\n    initial_collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=initial_collect_steps).run()\n\n    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration).run()\n\n    # Need extra step to generate transitions of train_sequence_length.\n    # Dataset generates trajectories with shape [BxTx...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    trajectories, unused_info = iterator.get_next()\n\n    train_fn = common.function(tf_agent.train)\n    train_op = train_fn(experience=trajectories)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      # TODO(b/126239733) Remove once Periodically can be saved.\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      sess.run(initial_collect_op)\n\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      global_step_call = sess.make_callable(global_step)\n\n      timed_at_step = global_step_call()\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _ = train_step_call()\n          global_step_val = global_step_call()\n        time_acc += time.time() - start_time\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/ddpg/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ddpg/examples/v2/train_eval.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DDPG.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ddpg/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/ddpg/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/ddpg/gym/HalfCheetah-v2/ \\\n  --num_iterations=2000000 \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Paths to the gin-config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding parameters.\')\n\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    eval_env_name=None,\n    env_load_fn=suite_mujoco.load,\n    num_iterations=2000000,\n    actor_fc_layers=(400, 300),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    num_parallel_environments=1,\n    replay_buffer_capacity=100000,\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=10000,\n    # Params for checkpoints, summaries, and logging\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for DDPG.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if num_parallel_environments > 1:\n      tf_env = tf_py_environment.TFPyEnvironment(\n          parallel_py_environment.ParallelPyEnvironment(\n              [lambda: env_load_fn(env_name)] * num_parallel_environments))\n    else:\n      tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n    eval_env_name = eval_env_name or env_name\n    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(eval_env_name))\n\n    actor_net = actor_network.ActorNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers,\n    )\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_network.CriticNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n    )\n\n    tf_agent = ddpg_agent.DdpgAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        ou_stddev=ou_stddev,\n        ou_damping=ou_damping,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch],\n        num_steps=initial_collect_steps)\n\n    collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=collect_steps_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    # Collect initial replay data.\n    logging.info(\n        \'Initializing replay buffer by collecting experience for %d steps with \'\n        \'a random policy.\', initial_collect_steps)\n    initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Dataset generates trajectories with shape [Bx2x...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=2).prefetch(3)\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      if global_step.numpy() % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step.numpy(),\n                     train_loss.loss)\n        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step.numpy() % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step.numpy())\n        metric_utils.log_metrics(eval_metrics)\n\n    return train_loss\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/ddpg/examples/v2/train_eval_rnn.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DDPG.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ --port 2223 &\n\npython tf_agents/agents/ddpg/examples/v2/train_eval_rnn.py \\\n  --root_dir=$HOME/tmp/ddpg_rnn/dm/CartPole-Balance/ \\\n  --num_iterations=100000 \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Paths to the gin-config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding parameters.\')\n\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'cartpole\',\n    task_name=\'balance\',\n    observations_whitelist=\'position\',\n    num_iterations=100000,\n    actor_fc_layers=(400, 300),\n    actor_output_fc_layers=(100,),\n    actor_lstm_size=(40,),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    critic_output_fc_layers=(100,),\n    critic_lstm_size=(40,),\n    # Params for collect\n    initial_collect_episodes=1,\n    collect_episodes_per_iteration=1,\n    replay_buffer_capacity=100000,\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    # Params for train\n    train_steps_per_iteration=200,\n    batch_size=64,\n    train_sequence_length=10,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=None,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints, summaries, and logging\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=True,\n    summarize_grads_and_vars=True,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for DDPG.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if observations_whitelist is not None:\n      env_wrappers = [\n          functools.partial(\n              wrappers.FlattenObservationsWrapper,\n              observations_whitelist=[observations_whitelist])\n      ]\n    else:\n      env_wrappers = []\n\n    tf_env = tf_py_environment.TFPyEnvironment(\n        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n    eval_tf_env = tf_py_environment.TFPyEnvironment(\n        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n\n    actor_net = actor_rnn_network.ActorRnnNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        input_fc_layer_params=actor_fc_layers,\n        lstm_size=actor_lstm_size,\n        output_fc_layer_params=actor_output_fc_layers)\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        lstm_size=critic_lstm_size,\n        output_fc_layer_params=critic_output_fc_layers,\n    )\n\n    tf_agent = ddpg_agent.DdpgAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        ou_stddev=ou_stddev,\n        ou_damping=ou_damping,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=initial_collect_episodes)\n\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    # Collect initial replay data.\n    logging.info(\n        \'Initializing replay buffer by collecting experience for %d episodes \'\n        \'with a random policy.\', initial_collect_episodes)\n    initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Dataset generates trajectories with shape [BxTx...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      if global_step.numpy() % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step.numpy(),\n                     train_loss.loss)\n        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step.numpy() % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step.numpy())\n        metric_utils.log_metrics(eval_metrics)\n\n    return train_loss\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/dqn/examples/v1/oog_train_eval.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Sample tf-agents trainer using a mix of graph and out of graph components.\n\nIn this example we keep the agent\'s network and training ops in graph and use a\nsimple python replay buffer backed by a collections.deque. The python\nenvironment is used and placeholders for the training op are created to allow us\nto easily feed in data to train on.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/dqn_v1/gym/CartPole-v0/ --port 2223 &\n\npython tf_agents/agents/dqn/examples/v1/oog_train_eval.py \\\n  --root_dir=$HOME/tmp/dqn_v1/gym/CartPole-v0/ \\\n  --alsologtostderr\n```\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.replay_buffers import py_uniform_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 10000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\ndef collect_step(env, time_step, py_policy, replay_buffer):\n  """"""Steps the environment and collects experience into the replay buffer.""""""\n  action_step = py_policy.action(time_step)\n  next_time_step = env.step(action_step.action)\n  if not time_step.is_last():\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n    replay_buffer.add_batch(traj)\n  return next_time_step\n\n\ndef train_eval(\n    root_dir,\n    env_name=\'CartPole-v0\',\n    num_iterations=100000,\n    fc_layer_params=(100,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    epsilon_greedy=0.1,\n    replay_buffer_capacity=100000,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    learning_rate=1e-3,\n    n_step_update=1,\n    gamma=0.99,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints, summaries and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    log_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for DQN.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  # Note this is a python environment.\n  env = batched_py_environment.BatchedPyEnvironment([suite_gym.load(env_name)])\n  eval_py_env = suite_gym.load(env_name)\n\n  # Convert specs to BoundedTensorSpec.\n  action_spec = tensor_spec.from_spec(env.action_spec())\n  observation_spec = tensor_spec.from_spec(env.observation_spec())\n  time_step_spec = ts.time_step_spec(observation_spec)\n\n  q_net = q_network.QNetwork(\n      tensor_spec.from_spec(env.observation_spec()),\n      tensor_spec.from_spec(env.action_spec()),\n      fc_layer_params=fc_layer_params)\n\n  # The agent must be in graph.\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  agent = dqn_agent.DqnAgent(\n      time_step_spec,\n      action_spec,\n      q_network=q_net,\n      epsilon_greedy=epsilon_greedy,\n      n_step_update=n_step_update,\n      target_update_tau=target_update_tau,\n      target_update_period=target_update_period,\n      optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n      td_errors_loss_fn=common.element_wise_squared_loss,\n      gamma=gamma,\n      reward_scale_factor=reward_scale_factor,\n      gradient_clipping=gradient_clipping,\n      debug_summaries=debug_summaries,\n      summarize_grads_and_vars=summarize_grads_and_vars,\n      train_step_counter=global_step)\n\n  tf_collect_policy = agent.collect_policy\n  collect_policy = py_tf_policy.PyTFPolicy(tf_collect_policy)\n  greedy_policy = py_tf_policy.PyTFPolicy(agent.policy)\n  random_policy = random_py_policy.RandomPyPolicy(env.time_step_spec(),\n                                                  env.action_spec())\n\n  # Python replay buffer.\n  replay_buffer = py_uniform_replay_buffer.PyUniformReplayBuffer(\n      capacity=replay_buffer_capacity,\n      data_spec=tensor_spec.to_nest_array_spec(agent.collect_data_spec))\n\n  time_step = env.reset()\n\n  # Initialize the replay buffer with some transitions. We use the random\n  # policy to initialize the replay buffer to make sure we get a good\n  # distribution of actions.\n  for _ in range(initial_collect_steps):\n    time_step = collect_step(env, time_step, random_policy, replay_buffer)\n\n  # TODO(b/112041045) Use global_step as counter.\n  train_checkpointer = common.Checkpointer(\n      ckpt_dir=train_dir, agent=agent, global_step=global_step)\n\n  policy_checkpointer = common.Checkpointer(\n      ckpt_dir=os.path.join(train_dir, \'policy\'),\n      policy=agent.policy,\n      global_step=global_step)\n\n  ds = replay_buffer.as_dataset(\n      sample_batch_size=batch_size, num_steps=n_step_update + 1)\n  ds = ds.prefetch(4)\n  itr = tf.compat.v1.data.make_initializable_iterator(ds)\n\n  experience = itr.get_next()\n\n  train_op = common.function(agent.train)(experience)\n\n  with eval_summary_writer.as_default(), \\\n       tf.compat.v2.summary.record_if(True):\n    for eval_metric in eval_metrics:\n      eval_metric.tf_summaries(train_step=global_step)\n\n  with tf.compat.v1.Session() as session:\n    train_checkpointer.initialize_or_restore(session)\n    common.initialize_uninitialized_variables(session)\n    session.run(itr.initializer)\n    # Copy critic network values to the target critic network.\n    session.run(agent.initialize())\n    train = session.make_callable(train_op)\n    global_step_call = session.make_callable(global_step)\n    session.run(train_summary_writer.init())\n    session.run(eval_summary_writer.init())\n\n    # Compute initial evaluation metrics.\n    global_step_val = global_step_call()\n    metric_utils.compute_summaries(\n        eval_metrics,\n        eval_py_env,\n        greedy_policy,\n        num_episodes=num_eval_episodes,\n        global_step=global_step_val,\n        log=True,\n        callback=eval_metrics_callback,\n    )\n\n    timed_at_step = global_step_val\n    collect_time = 0\n    train_time = 0\n    steps_per_second_ph = tf.compat.v1.placeholder(\n        tf.float32, shape=(), name=\'steps_per_sec_ph\')\n    steps_per_second_summary = tf.compat.v2.summary.scalar(\n        name=\'global_steps_per_sec\', data=steps_per_second_ph,\n        step=global_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      for _ in range(collect_steps_per_iteration):\n        time_step = collect_step(env, time_step, collect_policy, replay_buffer)\n      collect_time += time.time() - start_time\n      start_time = time.time()\n      for _ in range(train_steps_per_iteration):\n        loss = train()\n      train_time += time.time() - start_time\n      global_step_val = global_step_call()\n      if global_step_val % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step_val, loss.loss)\n        steps_per_sec = (\n            (global_step_val - timed_at_step) / (collect_time + train_time))\n        session.run(\n            steps_per_second_summary,\n            feed_dict={steps_per_second_ph: steps_per_sec})\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        logging.info(\'%s\', \'collect_time = {}, train_time = {}\'.format(\n            collect_time, train_time))\n        timed_at_step = global_step_val\n        collect_time = 0\n        train_time = 0\n\n      if global_step_val % train_checkpoint_interval == 0:\n        train_checkpointer.save(global_step=global_step_val)\n\n      if global_step_val % policy_checkpoint_interval == 0:\n        policy_checkpointer.save(global_step=global_step_val)\n\n      if global_step_val % eval_interval == 0:\n        metric_utils.compute_summaries(\n            eval_metrics,\n            eval_py_env,\n            greedy_policy,\n            num_episodes=num_eval_episodes,\n            global_step=global_step_val,\n            log=True,\n            callback=eval_metrics_callback,\n        )\n        # Reset timing to avoid counting eval time.\n        timed_at_step = global_step_val\n        start_time = time.time()\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v1/train_eval_atari.py,21,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval DQN on Atari environments.\n\nTraining and evaluation proceeds alternately in iterations, where each\niteration consists of a 1M frame training phase followed by a 500K frame\nevaluation phase. In the literature, some papers report averages of the train\nphases, while others report averages of the eval phases.\n\nThis example is configured to use dopamine.atari.preprocessing, which, among\nother things, repeats every action it receives for 4 frames, and then returns\nthe max-pool over the last 2 frames in the group. In this example, when we\nrefer to ""ALE frames"" we refer to the frames before the max-pooling step (i.e.\nthe raw data available for processing). Because of this, many of the\nconfiguration parameters (like initial_collect_steps) are divided by 4 in the\nbody of the trainer (e.g. if you want to evaluate with 400 frames in the\ninitial collection, you actually only need to .step the environment 100 times).\n\nFor a good survey of training on Atari, see Machado, et al. 2017:\nhttps://arxiv.org/pdf/1709.06009.pdf.\n\nTo run:\n\n```bash\ntf_agents/agents/dqn/examples/v1/train_eval_atari \\\n  --root_dir=$HOME/atari/pong \\\n  --atari_roms_path=/tmp\n  --alsologtostderr\n```\n\nAdditional flags are available such as `--replay_buffer_capacity` and\n`--n_step_update`.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.environments import batched_py_environment\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metric\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import epsilon_greedy_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_py_policy\nfrom tf_agents.replay_buffers import py_hashed_replay_buffer\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.utils import timer\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_string(\'environment_name\', None,\n                    \'Full name of Atari game to run, ex. PongNoFrameskip-v4.\')\nflags.DEFINE_string(\'game_name\', \'Pong\', \'Name of Atari game to run.\')\n\nflags.DEFINE_integer(\'num_iterations\', None,\n                     \'Number of train/eval iterations to run.\')\nflags.DEFINE_integer(\'initial_collect_steps\', None,\n                     \'Number of frames to ALE frames to process before \'\n                     \'beginning to train. Since this is in ALE frames, there \'\n                     \'will be initial_collect_steps/4 items in the replay \'\n                     \'buffer when training starts.\')\nflags.DEFINE_integer(\'replay_buffer_capacity\', None,\n                     \'Maximum number of items to store in the replay buffer.\')\nflags.DEFINE_integer(\'train_steps_per_iteration\', None,\n                     \'Number of ALE frames to run through for each iteration \'\n                     \'of training.\')\nflags.DEFINE_integer(\'n_step_update\', None, \'The number of steps to consider \'\n                     \'when computing TD error and TD loss.\')\nflags.DEFINE_integer(\'eval_steps_per_iteration\', None,\n                     \'Number of ALE frames to run through for each iteration \'\n                     \'of evaluation.\')\nFLAGS = flags.FLAGS\n\n# AtariPreprocessing runs 4 frames at a time, max-pooling over the last 2\n# frames. We need to account for this when computing things like update\n# intervals.\nATARI_FRAME_SKIP = 4\n\n\nclass AtariQNetwork(q_network.QNetwork):\n  """"""QNetwork subclass that divides observations by 255.""""""\n\n  def call(self,\n           observation,\n           step_type=None,\n           network_state=(),\n           training=False):\n    state = tf.cast(observation, tf.float32)\n    # We divide the grayscale pixel values by 255 here rather than storing\n    # normalized values beause uint8s are 4x cheaper to store than float32s.\n    state = state / 255\n    return super(AtariQNetwork, self).call(\n        state, step_type=step_type, network_state=network_state,\n        training=training)\n\n\ndef log_metric(metric, prefix):\n  tag = common.join_scope(prefix, metric.name)\n  logging.info(\'%s\', \'{0} = {1}\'.format(tag, metric.result()))\n\n\n@gin.configurable\nclass TrainEval(object):\n  """"""Train and evaluate DQN on Atari.""""""\n\n  def __init__(\n      self,\n      root_dir,\n      env_name,\n      num_iterations=200,\n      max_episode_frames=108000,  # ALE frames\n      terminal_on_life_loss=False,\n      conv_layer_params=((32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)),\n      fc_layer_params=(512,),\n      # Params for collect\n      initial_collect_steps=80000,  # ALE frames\n      epsilon_greedy=0.01,\n      epsilon_decay_period=1000000,  # ALE frames\n      replay_buffer_capacity=1000000,\n      # Params for train\n      train_steps_per_iteration=1000000,  # ALE frames\n      update_period=16,  # ALE frames\n      target_update_tau=1.0,\n      target_update_period=32000,  # ALE frames\n      batch_size=32,\n      learning_rate=2.5e-4,\n      n_step_update=1,\n      gamma=0.99,\n      reward_scale_factor=1.0,\n      gradient_clipping=None,\n      # Params for eval\n      do_eval=True,\n      eval_steps_per_iteration=500000,  # ALE frames\n      eval_epsilon_greedy=0.001,\n      # Params for checkpoints, summaries, and logging\n      log_interval=1000,\n      summary_interval=1000,\n      summaries_flush_secs=10,\n      debug_summaries=False,\n      summarize_grads_and_vars=False,\n      eval_metrics_callback=None):\n    """"""A simple Atari train and eval for DQN.\n\n    Args:\n      root_dir: Directory to write log files to.\n      env_name: Fully-qualified name of the Atari environment (i.e. Pong-v0).\n      num_iterations: Number of train/eval iterations to run.\n      max_episode_frames: Maximum length of a single episode, in ALE frames.\n      terminal_on_life_loss: Whether to simulate an episode termination when a\n        life is lost.\n      conv_layer_params: Params for convolutional layers of QNetwork.\n      fc_layer_params: Params for fully connected layers of QNetwork.\n      initial_collect_steps: Number of frames to ALE frames to process before\n        beginning to train. Since this is in ALE frames, there will be\n        initial_collect_steps/4 items in the replay buffer when training starts.\n      epsilon_greedy: Final epsilon value to decay to for training.\n      epsilon_decay_period: Period over which to decay epsilon, from 1.0 to\n        epsilon_greedy (defined above).\n      replay_buffer_capacity: Maximum number of items to store in the replay\n        buffer.\n      train_steps_per_iteration: Number of ALE frames to run through for each\n        iteration of training.\n      update_period: Run a train operation every update_period ALE frames.\n      target_update_tau: Coeffecient for soft target network updates (1.0 ==\n        hard updates).\n      target_update_period: Period, in ALE frames, to copy the live network to\n        the target network.\n      batch_size: Number of frames to include in each training batch.\n      learning_rate: RMS optimizer learning rate.\n      n_step_update: The number of steps to consider when computing TD error and\n        TD loss. Applies standard single-step updates when set to 1.\n      gamma: Discount for future rewards.\n      reward_scale_factor: Scaling factor for rewards.\n      gradient_clipping: Norm length to clip gradients.\n      do_eval: If True, run an eval every iteration. If False, skip eval.\n      eval_steps_per_iteration: Number of ALE frames to run through for each\n        iteration of evaluation.\n      eval_epsilon_greedy: Epsilon value to use for the evaluation policy (0 ==\n        totally greedy policy).\n      log_interval: Log stats to the terminal every log_interval training\n        steps.\n      summary_interval: Write TF summaries every summary_interval training\n        steps.\n      summaries_flush_secs: Flush summaries to disk every summaries_flush_secs\n        seconds.\n      debug_summaries: If True, write additional summaries for debugging (see\n        dqn_agent for which summaries are written).\n      summarize_grads_and_vars: Include gradients in summaries.\n      eval_metrics_callback: A callback function that takes (metric_dict,\n        global_step) as parameters. Called after every eval with the results of\n        the evaluation.\n    """"""\n    self._update_period = update_period / ATARI_FRAME_SKIP\n    self._train_steps_per_iteration = (train_steps_per_iteration\n                                       / ATARI_FRAME_SKIP)\n    self._do_eval = do_eval\n    self._eval_steps_per_iteration = eval_steps_per_iteration / ATARI_FRAME_SKIP\n    self._eval_epsilon_greedy = eval_epsilon_greedy\n    self._initial_collect_steps = initial_collect_steps / ATARI_FRAME_SKIP\n    self._summary_interval = summary_interval\n    self._num_iterations = num_iterations\n    self._log_interval = log_interval\n    self._eval_metrics_callback = eval_metrics_callback\n\n    with gin.unlock_config():\n      gin.bind_parameter(\'AtariPreprocessing.terminal_on_life_loss\',\n                         terminal_on_life_loss)\n\n    root_dir = os.path.expanduser(root_dir)\n    train_dir = os.path.join(root_dir, \'train\')\n    eval_dir = os.path.join(root_dir, \'eval\')\n\n    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n        train_dir, flush_millis=summaries_flush_secs * 1000)\n    train_summary_writer.set_as_default()\n    self._train_summary_writer = train_summary_writer\n\n    self._eval_summary_writer = None\n    if self._do_eval:\n      self._eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n          eval_dir, flush_millis=summaries_flush_secs * 1000)\n      self._eval_metrics = [\n          py_metrics.AverageReturnMetric(\n              name=\'PhaseAverageReturn\', buffer_size=np.inf),\n          py_metrics.AverageEpisodeLengthMetric(\n              name=\'PhaseAverageEpisodeLength\', buffer_size=np.inf),\n      ]\n\n    self._global_step = tf.compat.v1.train.get_or_create_global_step()\n    with tf.compat.v2.summary.record_if(\n        lambda: tf.math.equal(self._global_step % self._summary_interval, 0)):\n      self._env = suite_atari.load(\n          env_name,\n          max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n          gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n      self._env = batched_py_environment.BatchedPyEnvironment([self._env])\n\n      observation_spec = tensor_spec.from_spec(self._env.observation_spec())\n      time_step_spec = ts.time_step_spec(observation_spec)\n      action_spec = tensor_spec.from_spec(self._env.action_spec())\n\n      with tf.device(\'/cpu:0\'):\n        epsilon = tf.compat.v1.train.polynomial_decay(\n            1.0,\n            self._global_step,\n            epsilon_decay_period / ATARI_FRAME_SKIP / self._update_period,\n            end_learning_rate=epsilon_greedy)\n\n      with tf.device(\'/gpu:0\'):\n        optimizer = tf.compat.v1.train.RMSPropOptimizer(\n            learning_rate=learning_rate,\n            decay=0.95,\n            momentum=0.0,\n            epsilon=0.00001,\n            centered=True)\n        q_net = AtariQNetwork(\n            observation_spec,\n            action_spec,\n            conv_layer_params=conv_layer_params,\n            fc_layer_params=fc_layer_params)\n        agent = dqn_agent.DqnAgent(\n            time_step_spec,\n            action_spec,\n            q_network=q_net,\n            optimizer=optimizer,\n            epsilon_greedy=epsilon,\n            n_step_update=n_step_update,\n            target_update_tau=target_update_tau,\n            target_update_period=(\n                target_update_period / ATARI_FRAME_SKIP / self._update_period),\n            td_errors_loss_fn=common.element_wise_huber_loss,\n            gamma=gamma,\n            reward_scale_factor=reward_scale_factor,\n            gradient_clipping=gradient_clipping,\n            debug_summaries=debug_summaries,\n            summarize_grads_and_vars=summarize_grads_and_vars,\n            train_step_counter=self._global_step)\n\n        self._collect_policy = py_tf_policy.PyTFPolicy(agent.collect_policy)\n\n        if self._do_eval:\n          self._eval_policy = py_tf_policy.PyTFPolicy(\n              epsilon_greedy_policy.EpsilonGreedyPolicy(\n                  policy=agent.policy,\n                  epsilon=self._eval_epsilon_greedy))\n\n        py_observation_spec = self._env.observation_spec()\n        py_time_step_spec = ts.time_step_spec(py_observation_spec)\n        py_action_spec = policy_step.PolicyStep(self._env.action_spec())\n        data_spec = trajectory.from_transition(\n            py_time_step_spec, py_action_spec, py_time_step_spec)\n        self._replay_buffer = py_hashed_replay_buffer.PyHashedReplayBuffer(\n            data_spec=data_spec, capacity=replay_buffer_capacity)\n\n      with tf.device(\'/cpu:0\'):\n        ds = self._replay_buffer.as_dataset(\n            sample_batch_size=batch_size, num_steps=n_step_update + 1)\n        ds = ds.prefetch(4)\n        ds = ds.apply(tf.data.experimental.prefetch_to_device(\'/gpu:0\'))\n\n      with tf.device(\'/gpu:0\'):\n        self._ds_itr = tf.compat.v1.data.make_one_shot_iterator(ds)\n        experience = self._ds_itr.get_next()\n        self._train_op = agent.train(experience)\n\n        self._env_steps_metric = py_metrics.EnvironmentSteps()\n        self._step_metrics = [\n            py_metrics.NumberOfEpisodes(),\n            self._env_steps_metric,\n        ]\n        self._train_metrics = self._step_metrics + [\n            py_metrics.AverageReturnMetric(buffer_size=10),\n            py_metrics.AverageEpisodeLengthMetric(buffer_size=10),\n        ]\n        # The _train_phase_metrics average over an entire train iteration,\n        # rather than the rolling average of the last 10 episodes.\n        self._train_phase_metrics = [\n            py_metrics.AverageReturnMetric(\n                name=\'PhaseAverageReturn\', buffer_size=np.inf),\n            py_metrics.AverageEpisodeLengthMetric(\n                name=\'PhaseAverageEpisodeLength\', buffer_size=np.inf),\n        ]\n        self._iteration_metric = py_metrics.CounterMetric(name=\'Iteration\')\n\n        # Summaries written from python should run every time they are\n        # generated.\n        with tf.compat.v2.summary.record_if(True):\n          self._steps_per_second_ph = tf.compat.v1.placeholder(\n              tf.float32, shape=(), name=\'steps_per_sec_ph\')\n          self._steps_per_second_summary = tf.compat.v2.summary.scalar(\n              name=\'global_steps_per_sec\', data=self._steps_per_second_ph,\n              step=self._global_step)\n\n          for metric in self._train_metrics:\n            metric.tf_summaries(\n                train_step=self._global_step, step_metrics=self._step_metrics)\n\n          for metric in self._train_phase_metrics:\n            metric.tf_summaries(\n                train_step=self._global_step,\n                step_metrics=(self._iteration_metric,))\n          self._iteration_metric.tf_summaries(train_step=self._global_step)\n\n          if self._do_eval:\n            with self._eval_summary_writer.as_default():\n              for metric in self._eval_metrics:\n                metric.tf_summaries(\n                    train_step=self._global_step,\n                    step_metrics=(self._iteration_metric,))\n\n        self._train_dir = train_dir\n        self._policy_exporter = policy_saver.PolicySaver(\n            agent.policy, train_step=self._global_step)\n        self._train_checkpointer = common.Checkpointer(\n            ckpt_dir=train_dir,\n            agent=agent,\n            global_step=self._global_step,\n            optimizer=optimizer,\n            metrics=metric_utils.MetricsGroup(\n                self._train_metrics + self._train_phase_metrics +\n                [self._iteration_metric], \'train_metrics\'))\n        self._policy_checkpointer = common.Checkpointer(\n            ckpt_dir=os.path.join(train_dir, \'policy\'),\n            policy=agent.policy,\n            global_step=self._global_step)\n        self._rb_checkpointer = common.Checkpointer(\n            ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n            max_to_keep=1,\n            replay_buffer=self._replay_buffer)\n\n        self._init_agent_op = agent.initialize()\n\n  def game_over(self):\n    return self._env.envs[0].game_over\n\n  def run(self):\n    """"""Execute the train/eval loop.""""""\n    with tf.compat.v1.Session(\n        config=tf.compat.v1.ConfigProto(allow_soft_placement=True)) as sess:\n      # Initialize the graph.\n      self._initialize_graph(sess)\n\n      # Initial collect\n      self._initial_collect()\n\n      while self._iteration_metric.result() < self._num_iterations:\n        # Train phase\n        env_steps = 0\n        for metric in self._train_phase_metrics:\n          metric.reset()\n        while env_steps < self._train_steps_per_iteration:\n          env_steps += self._run_episode(\n              sess, self._train_metrics + self._train_phase_metrics, train=True)\n        for metric in self._train_phase_metrics:\n          log_metric(metric, prefix=\'Train/Metrics\')\n        py_metric.run_summaries(\n            self._train_phase_metrics + [self._iteration_metric])\n\n        global_step_val = sess.run(self._global_step)\n\n        if self._do_eval:\n          # Eval phase\n          env_steps = 0\n          for metric in self._eval_metrics:\n            metric.reset()\n          while env_steps < self._eval_steps_per_iteration:\n            env_steps += self._run_episode(\n                sess, self._eval_metrics, train=False)\n\n          py_metric.run_summaries(self._eval_metrics + [self._iteration_metric])\n          if self._eval_metrics_callback:\n            results = dict((metric.name, metric.result())\n                           for metric in self._eval_metrics)\n            self._eval_metrics_callback(results, global_step_val)\n          for metric in self._eval_metrics:\n            log_metric(metric, prefix=\'Eval/Metrics\')\n\n        self._iteration_metric()\n\n        self._train_checkpointer.save(global_step=global_step_val)\n        self._policy_checkpointer.save(global_step=global_step_val)\n        self._rb_checkpointer.save(global_step=global_step_val)\n\n        export_dir = os.path.join(self._train_dir, \'saved_policy\',\n                                  \'step_\' + (\'%d\' % global_step_val).zfill(8))\n        self._policy_exporter.save(export_dir)\n        common.save_spec(self._collect_policy.trajectory_spec,\n                         os.path.join(export_dir, \'trajectory_spec\'))\n\n  def _initialize_graph(self, sess):\n    """"""Initialize the graph for sess.""""""\n    self._train_checkpointer.initialize_or_restore(sess)\n    self._rb_checkpointer.initialize_or_restore(sess)\n    common.initialize_uninitialized_variables(sess)\n\n    sess.run(self._init_agent_op)\n\n    self._train_step_call = sess.make_callable(self._train_op)\n\n    self._collect_timer = timer.Timer()\n    self._train_timer = timer.Timer()\n    self._action_timer = timer.Timer()\n    self._step_timer = timer.Timer()\n    self._observer_timer = timer.Timer()\n\n    global_step_val = sess.run(self._global_step)\n    self._timed_at_step = global_step_val\n\n    # Call save to initialize the save_counter (need to do this before\n    # finalizing the graph).\n    self._train_checkpointer.save(global_step=global_step_val)\n    self._policy_checkpointer.save(global_step=global_step_val)\n    self._rb_checkpointer.save(global_step=global_step_val)\n    sess.run(self._train_summary_writer.init())\n\n    if self._do_eval:\n      sess.run(self._eval_summary_writer.init())\n\n  def _initial_collect(self):\n    """"""Collect initial experience before training begins.""""""\n    logging.info(\'Collecting initial experience...\')\n    time_step_spec = ts.time_step_spec(self._env.observation_spec())\n    random_policy = random_py_policy.RandomPyPolicy(\n        time_step_spec, self._env.action_spec())\n    time_step = self._env.reset()\n    while self._replay_buffer.size < self._initial_collect_steps:\n      if self.game_over():\n        time_step = self._env.reset()\n      action_step = random_policy.action(time_step)\n      next_time_step = self._env.step(action_step.action)\n      self._replay_buffer.add_batch(trajectory.from_transition(\n          time_step, action_step, next_time_step))\n      time_step = next_time_step\n    logging.info(\'Done.\')\n\n  def _run_episode(self, sess, metric_observers, train=False):\n    """"""Run a single episode.""""""\n    env_steps = 0\n    time_step = self._env.reset()\n    while True:\n      with self._collect_timer:\n        time_step = self._collect_step(\n            time_step,\n            metric_observers,\n            train=train)\n        env_steps += 1\n\n      if self.game_over():\n        break\n      elif train and self._env_steps_metric.result() % self._update_period == 0:\n        with self._train_timer:\n          total_loss = self._train_step_call()\n          global_step_val = sess.run(self._global_step)\n        self._maybe_log(sess, global_step_val, total_loss)\n        self._maybe_record_summaries(global_step_val)\n\n    return env_steps\n\n  def _observe(self, metric_observers, traj):\n    with self._observer_timer:\n      for observer in metric_observers:\n        observer(traj)\n\n  def _store_to_rb(self, traj):\n    # Clip the reward to (-1, 1) to normalize rewards in training.\n    traj = traj._replace(\n        reward=np.asarray(np.clip(traj.reward, -1, 1)))\n    self._replay_buffer.add_batch(traj)\n\n  def _collect_step(self, time_step, metric_observers, train=False):\n    """"""Run a single step (or 2 steps on life loss) in the environment.""""""\n    if train:\n      policy = self._collect_policy\n    else:\n      policy = self._eval_policy\n\n    with self._action_timer:\n      action_step = policy.action(time_step)\n    with self._step_timer:\n      next_time_step = self._env.step(action_step.action)\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    if next_time_step.is_last() and not self.game_over():\n      traj = traj._replace(discount=np.array([1.0], dtype=np.float32))\n\n    if train:\n      self._store_to_rb(traj)\n\n    # When AtariPreprocessing.terminal_on_life_loss is True, we receive LAST\n    # time_steps when lives are lost but the game is not over. In this mode, the\n    # replay buffer and agent\'s policy must see the life loss as a LAST step\n    # and the subsequent step as a FIRST step. However, we do not want to\n    # actually terminate the episode and metrics should be computed as if all\n    # steps were MID steps, since life loss is not actually a terminal event\n    # (it is mostly a trick to make it easier to propagate rewards backwards by\n    # shortening episode durations from the agent\'s perspective).\n    if next_time_step.is_last() and not self.game_over():\n      # Update metrics as if this is a mid-episode step.\n      next_time_step = ts.transition(\n          next_time_step.observation, next_time_step.reward)\n      self._observe(metric_observers, trajectory.from_transition(\n          time_step, action_step, next_time_step))\n\n      # Produce the next step as if this is the first step of an episode and\n      # store to RB as such. The next_time_step will be a MID time step.\n      reward = time_step.reward\n      time_step = ts.restart(next_time_step.observation)\n      with self._action_timer:\n        action_step = policy.action(time_step)\n      with self._step_timer:\n        next_time_step = self._env.step(action_step.action)\n      if train:\n        self._store_to_rb(trajectory.from_transition(\n            time_step, action_step, next_time_step))\n\n      # Update metrics as if this is a mid-episode step.\n      time_step = ts.transition(time_step.observation, reward)\n      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    self._observe(metric_observers, traj)\n\n    return next_time_step\n\n  def _maybe_record_summaries(self, global_step_val):\n    """"""Record summaries if global_step_val is a multiple of summary_interval.""""""\n    if global_step_val % self._summary_interval == 0:\n      py_metric.run_summaries(self._train_metrics)\n\n  def _maybe_log(self, sess, global_step_val, total_loss):\n    """"""Log some stats if global_step_val is a multiple of log_interval.""""""\n    if global_step_val % self._log_interval == 0:\n      logging.info(\'step = %d, loss = %f\', global_step_val, total_loss.loss)\n      logging.info(\'%s\', \'action_time = {}\'.format(self._action_timer.value()))\n      logging.info(\'%s\', \'step_time = {}\'.format(self._step_timer.value()))\n      logging.info(\'%s\', \'observer_time = {}\'.format(\n          self._observer_timer.value()))\n      steps_per_sec = ((global_step_val - self._timed_at_step) /\n                       (self._collect_timer.value()\n                        + self._train_timer.value()))\n      sess.run(self._steps_per_second_summary,\n               feed_dict={self._steps_per_second_ph: steps_per_sec})\n      logging.info(\'%.3f steps/sec\', steps_per_sec)\n      logging.info(\'%s\', \'collect_time = {}, train_time = {}\'.format(\n          self._collect_timer.value(), self._train_timer.value()))\n      for metric in self._train_metrics:\n        log_metric(metric, prefix=\'Train/Metrics\')\n      self._timed_at_step = global_step_val\n      self._collect_timer.reset()\n      self._train_timer.reset()\n      self._action_timer.reset()\n      self._step_timer.reset()\n      self._observer_timer.reset()\n\n\ndef get_run_args():\n  """"""Builds a dict of run arguments from flags.""""""\n  run_args = {}\n  if FLAGS.num_iterations:\n    run_args[\'num_iterations\'] = FLAGS.num_iterations\n  if FLAGS.initial_collect_steps:\n    run_args[\'initial_collect_steps\'] = FLAGS.initial_collect_steps\n  if FLAGS.replay_buffer_capacity:\n    run_args[\'replay_buffer_capacity\'] = FLAGS.replay_buffer_capacity\n  if FLAGS.train_steps_per_iteration:\n    run_args[\'train_steps_per_iteration\'] = FLAGS.train_steps_per_iteration\n  if FLAGS.n_step_update:\n    run_args[\'n_step_update\'] = FLAGS.n_step_update\n  if FLAGS.eval_steps_per_iteration:\n    run_args[\'eval_steps_per_iteration\'] = FLAGS.eval_steps_per_iteration\n  return run_args\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  environment_name = FLAGS.environment_name\n  if environment_name is None:\n    environment_name = suite_atari.game(name=FLAGS.game_name)\n  TrainEval(FLAGS.root_dir, environment_name, **get_run_args()).run()\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v1/train_eval_gym.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DQN.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/dqn_v1/gym/CartPole-v0/ --port 2223 &\n\npython tf_agents/agents/dqn/examples/v1/train_eval_gym.py \\\n  --root_dir=$HOME/tmp/dqn_v1/gym/CartPole-v0/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_bool(\'use_ddqn\', False,\n                  \'If True uses the DdqnAgent instead of the DqnAgent.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'CartPole-v0\',\n    num_iterations=100000,\n    fc_layer_params=(100,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    epsilon_greedy=0.1,\n    replay_buffer_capacity=100000,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    learning_rate=1e-3,\n    gamma=0.99,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=20000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    agent_class=dqn_agent.DqnAgent,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for DQN.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n    eval_py_env = suite_gym.load(env_name)\n\n    q_net = q_network.QNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=fc_layer_params)\n\n    # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n    tf_agent = agent_class(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        q_network=q_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n        epsilon_greedy=epsilon_greedy,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=common.element_wise_squared_loss,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    replay_observer = [replay_buffer.add_batch]\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n    initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        initial_collect_policy,\n        observers=replay_observer + train_metrics,\n        num_steps=initial_collect_steps).run()\n\n    collect_policy = tf_agent.collect_policy\n    collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=replay_observer + train_metrics,\n        num_steps=collect_steps_per_iteration).run()\n\n    # Dataset generates trajectories with shape [Bx2x...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=2).prefetch(3)\n\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    experience, _ = iterator.get_next()\n    train_op = common.function(tf_agent.train)(experience=experience)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      sess.run(initial_collect_op)\n\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      global_step_call = sess.make_callable(global_step)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n\n      timed_at_step = global_step_call()\n      collect_time = 0\n      train_time = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        # Train/collect/eval.\n        start_time = time.time()\n        collect_call()\n        collect_time += time.time() - start_time\n        start_time = time.time()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _ = train_step_call()\n        train_time += time.time() - start_time\n\n        global_step_val = global_step_call()\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (\n              (global_step_val - timed_at_step) / (collect_time + train_time))\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          logging.info(\'%s\', \'collect_time = {}, train_time = {}\'.format(\n              collect_time, train_time))\n          timed_at_step = global_step_val\n          collect_time = 0\n          train_time = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  agent_class = dqn_agent.DdqnAgent if FLAGS.use_ddqn else dqn_agent.DqnAgent\n  train_eval(\n      FLAGS.root_dir,\n      agent_class=agent_class,\n      num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v1/train_eval_rnn_gym.py,13,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DQN.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/dqn_rnn_v1/gym/MaskedCartPole-v0/ --port 2223 &\n\npython tf_agents/agents/dqn/examples/v1/train_eval_rnn_gym.py \\\n  --root_dir=$HOME/tmp/dqn_rnn_v1/gym/MaskedCartPole-v0/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments.examples import masked_cartpole  # pylint: disable=unused-import\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_integer(\'eval_interval\', 1000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\ndef train_eval(\n    root_dir,\n    env_name=\'MaskedCartPole-v0\',\n    num_iterations=100000,\n    input_fc_layer_params=(50,),\n    lstm_size=(20,),\n    output_fc_layer_params=(20,),\n    train_sequence_length=10,\n    # Params for collect\n    initial_collect_steps=50,\n    collect_episodes_per_iteration=1,\n    epsilon_greedy=0.1,\n    replay_buffer_capacity=100000,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=10,\n    batch_size=128,\n    learning_rate=1e-3,\n    gamma=0.99,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for summaries and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=20000,\n    log_interval=100,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for DQN.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    eval_py_env = suite_gym.load(env_name)\n    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n\n    q_net = q_rnn_network.QRnnNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        input_fc_layer_params=input_fc_layer_params,\n        lstm_size=lstm_size,\n        output_fc_layer_params=output_fc_layer_params)\n\n    # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n    tf_agent = dqn_agent.DqnAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        q_network=q_net,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n        epsilon_greedy=epsilon_greedy,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=common.element_wise_squared_loss,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n    initial_collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        initial_collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=initial_collect_steps).run()\n\n    collect_policy = tf_agent.collect_policy\n    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration).run()\n\n    # Need extra step to generate transitions of train_sequence_length.\n    # Dataset generates trajectories with shape [BxTx...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    experience, _ = iterator.get_next()\n    loss_info = common.function(tf_agent.train)(experience=experience)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      logging.info(\'Collecting initial experience.\')\n      sess.run(initial_collect_op)\n\n      # Compute evaluation metrics.\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([loss_info, summary_ops])\n      global_step_call = sess.make_callable(global_step)\n\n      timed_at_step = global_step_call()\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        # Train/collect/eval.\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _ = train_step_call()\n        time_acc += time.time() - start_time\n        global_step_val = global_step_call()\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              log=True,\n              callback=eval_metrics_callback,\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(\n      FLAGS.root_dir,\n      num_iterations=FLAGS.num_iterations,\n      eval_interval=FLAGS.eval_interval)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/dqn/examples/v2/train_eval.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval DQN.\n\nTo run DQN on CartPole:\n\n```bash\ntensorboard --logdir $HOME/tmp/dqn/gym/CartPole-v0/ --port 2223 &\n\npython tf_agents/agents/dqn/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/dqn/gym/CartPole-v0/ \\\n  --alsologtostderr\n```\n\nTo run DQN-RNNs on MaskedCartPole:\n\n```bash\npython tf_agents/agents/dqn/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/dqn_rnn/gym/MaskedCartPole-v0/ \\\n  --gin_param=\'train_eval.env_name=""MaskedCartPole-v0""\' \\\n  --gin_param=\'train_eval.train_sequence_length=10\' \\\n  --alsologtostderr\n```\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments.examples import masked_cartpole  # pylint: disable=unused-import\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.networks import q_rnn_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Paths to the gin-config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding parameters.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'CartPole-v0\',\n    num_iterations=100000,\n    train_sequence_length=1,\n    # Params for QNetwork\n    fc_layer_params=(100,),\n    # Params for QRnnNetwork\n    input_fc_layer_params=(50,),\n    lstm_size=(20,),\n    output_fc_layer_params=(20,),\n\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    epsilon_greedy=0.1,\n    replay_buffer_capacity=100000,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    learning_rate=1e-3,\n    n_step_update=1,\n    gamma=0.99,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=20000,\n    # Params for summaries and logging\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for DQN.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n\n    if train_sequence_length != 1 and n_step_update != 1:\n      raise NotImplementedError(\n          \'train_eval does not currently support n-step updates with stateful \'\n          \'networks (i.e., RNNs)\')\n\n    if train_sequence_length > 1:\n      q_net = q_rnn_network.QRnnNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          input_fc_layer_params=input_fc_layer_params,\n          lstm_size=lstm_size,\n          output_fc_layer_params=output_fc_layer_params)\n    else:\n      q_net = q_network.QNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          fc_layer_params=fc_layer_params)\n      train_sequence_length = n_step_update\n\n    # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n    tf_agent = dqn_agent.DqnAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        q_network=q_net,\n        epsilon_greedy=epsilon_greedy,\n        n_step_update=n_step_update,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n        td_errors_loss_fn=common.element_wise_squared_loss,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=collect_steps_per_iteration)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=eval_policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    train_checkpointer.initialize_or_restore()\n    rb_checkpointer.initialize_or_restore()\n\n    if use_tf_functions:\n      # To speed up collect use common.function.\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n\n    # Collect initial replay data.\n    logging.info(\n        \'Initializing replay buffer by collecting experience for %d steps with \'\n        \'a random policy.\', initial_collect_steps)\n    dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        initial_collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=initial_collect_steps).run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Dataset generates trajectories with shape [Bx2x...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      if global_step.numpy() % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step.numpy(),\n                     train_loss.loss)\n        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step.numpy() % train_checkpoint_interval == 0:\n        train_checkpointer.save(global_step=global_step.numpy())\n\n      if global_step.numpy() % policy_checkpoint_interval == 0:\n        policy_checkpointer.save(global_step=global_step.numpy())\n\n      if global_step.numpy() % rb_checkpoint_interval == 0:\n        rb_checkpointer.save(global_step=global_step.numpy())\n\n      if global_step.numpy() % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step.numpy())\n        metric_utils.log_metrics(eval_metrics)\n    return train_loss\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_v2_behavior()\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/dqn/examples/v2/train_eval_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.agents.dqn.examples.v2.train_eval.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import flags\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.dqn.examples.v2 import train_eval\n\nFLAGS = flags.FLAGS\n\n\nclass TrainEval(tf.test.TestCase):\n\n  def testDQNCartPole(self):\n    if not tf.executing_eagerly():\n      self.skipTest(\'Binary is eager-only.\')\n\n    root_dir = self.get_temp_dir()\n    train_loss = train_eval.train_eval(root_dir,\n                                       num_iterations=1,\n                                       num_eval_episodes=1,\n                                       initial_collect_steps=10)\n    self.assertGreater(train_loss.loss, 0.0)\n\n  def testRNNDQNMaskedCartPole(self):\n    if not tf.executing_eagerly():\n      self.skipTest(\'Binary is eager-only.\')\n\n    root_dir = self.get_temp_dir()\n    train_loss = train_eval.train_eval(\n        root_dir,\n        env_name=\'MaskedCartPole-v0\',\n        train_sequence_length=2,\n        initial_collect_steps=10,\n        num_eval_episodes=1,\n        num_iterations=1)\n    self.assertGreater(train_loss.loss, 0.0)\n\nif __name__ == \'__main__\':\n  tf.compat.v1.enable_v2_behavior()\n  tf.test.main()\n'"
tf_agents/agents/ppo/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ppo/examples/v1/train_eval_clip_agent.py,19,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval PPO Clip Agent.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ppo_clip_v1/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/ppo/examples/v1/train_eval_clip_agent.py \\\n  --root_dir=$HOME/tmp/ppo_clip_v1/gym/HalfCheetah-v2/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ppo import ppo_clip_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import batched_py_metric\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.metrics.py_metrics import AverageEpisodeLengthMetric\nfrom tf_agents.metrics.py_metrics import AverageReturnMetric\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import value_network\nfrom tf_agents.networks import value_rnn_network\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_string(\'master\', \'\', \'master session\')\nflags.DEFINE_string(\'env_name\', \'HalfCheetah-v2\', \'Name of an environment\')\nflags.DEFINE_integer(\'replay_buffer_capacity\', 1001,\n                     \'Replay buffer capacity per env.\')\nflags.DEFINE_integer(\'num_parallel_environments\', 30,\n                     \'Number of environments to run in parallel\')\nflags.DEFINE_integer(\'num_environment_steps\', 25000000,\n                     \'Number of environment steps to run before finishing.\')\nflags.DEFINE_integer(\'num_epochs\', 25,\n                     \'Number of epochs for computing policy updates.\')\nflags.DEFINE_integer(\n    \'collect_episodes_per_iteration\', 30,\n    \'The number of episodes to take in the environment before \'\n    \'each update. This is the total across all parallel \'\n    \'environments.\')\nflags.DEFINE_integer(\'num_eval_episodes\', 30,\n                     \'The number of episodes to run eval on.\')\nflags.DEFINE_boolean(\'use_rnns\', False,\n                     \'If true, use RNN for policy and value function.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    tf_master=\'\',\n    env_name=\'HalfCheetah-v2\',\n    env_load_fn=suite_mujoco.load,\n    random_seed=None,\n    # TODO(b/127576522): rename to policy_fc_layers.\n    actor_fc_layers=(200, 100),\n    value_fc_layers=(200, 100),\n    use_rnns=False,\n    # Params for collect\n    num_environment_steps=25000000,\n    collect_episodes_per_iteration=30,\n    num_parallel_environments=30,\n    replay_buffer_capacity=1001,  # Per-environment\n    # Params for train\n    num_epochs=25,\n    learning_rate=1e-3,\n    # Params for eval\n    num_eval_episodes=30,\n    eval_interval=500,\n    # Params for summaries and logging\n    train_checkpoint_interval=500,\n    policy_checkpoint_interval=500,\n    log_interval=50,\n    summary_interval=50,\n    summaries_flush_secs=1,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for PPO.""""""\n  if root_dir is None:\n    raise AttributeError(\'train_eval requires a root_dir.\')\n\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      batched_py_metric.BatchedPyMetric(\n          AverageReturnMetric,\n          metric_args={\'buffer_size\': num_eval_episodes},\n          batch_size=num_parallel_environments),\n      batched_py_metric.BatchedPyMetric(\n          AverageEpisodeLengthMetric,\n          metric_args={\'buffer_size\': num_eval_episodes},\n          batch_size=num_parallel_environments),\n  ]\n  eval_summary_writer_flush_op = eval_summary_writer.flush()\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if random_seed is not None:\n      tf.compat.v1.set_random_seed(random_seed)\n    eval_py_env = parallel_py_environment.ParallelPyEnvironment(\n        [lambda: env_load_fn(env_name)] * num_parallel_environments)\n    tf_env = tf_py_environment.TFPyEnvironment(\n        parallel_py_environment.ParallelPyEnvironment(\n            [lambda: env_load_fn(env_name)] * num_parallel_environments))\n    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\n    if use_rnns:\n      actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          input_fc_layer_params=actor_fc_layers,\n          lstm_size=(40,),\n          output_fc_layer_params=None)\n      value_net = value_rnn_network.ValueRnnNetwork(\n          tf_env.observation_spec(),\n          input_fc_layer_params=value_fc_layers,\n          output_fc_layer_params=None)\n    else:\n      actor_net = actor_distribution_network.ActorDistributionNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          fc_layer_params=actor_fc_layers,\n          activation_fn=tf.keras.activations.tanh)\n      value_net = value_network.ValueNetwork(\n          tf_env.observation_spec(),\n          fc_layer_params=value_fc_layers,\n          activation_fn=tf.keras.activations.tanh)\n\n    tf_agent = ppo_clip_agent.PPOClipAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        optimizer,\n        actor_net=actor_net,\n        value_net=value_net,\n        entropy_regularization=0.0,\n        importance_ratio_clipping=0.2,\n        normalize_observations=False,\n        normalize_rewards=False,\n        use_gae=True,\n        num_epochs=num_epochs,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=num_parallel_environments,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    environment_steps_metric = tf_metrics.EnvironmentSteps()\n    environment_steps_count = environment_steps_metric.result()\n    step_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        environment_steps_metric,\n    ]\n    train_metrics = step_metrics + [\n        tf_metrics.AverageReturnMetric(\n            batch_size=num_parallel_environments),\n        tf_metrics.AverageEpisodeLengthMetric(\n            batch_size=num_parallel_environments),\n    ]\n\n    # Add to replay buffer and other agent specific observers.\n    replay_buffer_observer = [replay_buffer.add_batch]\n\n    collect_policy = tf_agent.collect_policy\n\n    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=replay_buffer_observer + train_metrics,\n        num_episodes=collect_episodes_per_iteration).run()\n\n    trajectories = replay_buffer.gather_all()\n\n    train_op, _ = tf_agent.train(experience=trajectories)\n\n    with tf.control_dependencies([train_op]):\n      clear_replay_op = replay_buffer.clear()\n\n    with tf.control_dependencies([clear_replay_op]):\n      train_op = tf.identity(train_op)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=step_metrics))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(\n            train_step=global_step, step_metrics=step_metrics)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session(tf_master) as sess:\n      # Initialize graph.\n      train_checkpointer.initialize_or_restore(sess)\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n\n      collect_time = 0\n      train_time = 0\n      timed_at_step = sess.run(global_step)\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      while sess.run(environment_steps_count) < num_environment_steps:\n        global_step_val = sess.run(global_step)\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True,\n          )\n          sess.run(eval_summary_writer_flush_op)\n\n        start_time = time.time()\n        sess.run(collect_op)\n        collect_time += time.time() - start_time\n        start_time = time.time()\n        total_loss, _ = sess.run([train_op, summary_ops])\n        train_time += time.time() - start_time\n\n        global_step_val = sess.run(global_step)\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val, total_loss)\n          steps_per_sec = (\n              (global_step_val - timed_at_step) / (collect_time + train_time))\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          logging.info(\'%s\', \'collect_time = {}, train_time = {}\'.format(\n              collect_time, train_time))\n          timed_at_step = global_step_val\n          collect_time = 0\n          train_time = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n      # One final eval before exiting.\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n      sess.run(eval_summary_writer_flush_op)\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  if tf.executing_eagerly():\n    # self.skipTest(\'b/123777119\')  # Secondary bug: (\'b/123775375\')\n    return\n\n  logging.set_verbosity(logging.INFO)\n  train_eval(\n      FLAGS.root_dir,\n      tf_master=FLAGS.master,\n      env_name=FLAGS.env_name,\n      replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n      num_environment_steps=FLAGS.num_environment_steps,\n      num_parallel_environments=FLAGS.num_parallel_environments,\n      num_epochs=FLAGS.num_epochs,\n      collect_episodes_per_iteration=FLAGS.collect_episodes_per_iteration,\n      num_eval_episodes=FLAGS.num_eval_episodes,\n      use_rnns=FLAGS.use_rnns)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  multiprocessing.handle_main(lambda _: app.run(main))\n'"
tf_agents/agents/ppo/examples/v1/train_eval_clip_agent_atari.py,1,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval PPO Clip Agent, with required atari import.\n\nLaunch train eval binary:\n\nFor usage, see train_eval_clip_agent.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ppo.examples.v1 import train_eval_clip_agent\nfrom tf_agents.environments import suite_atari\nfrom tf_agents.system import system_multiprocessing as multiprocessing\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  logging.set_verbosity(logging.INFO)\n  train_eval_clip_agent.train_eval(\n      FLAGS.root_dir,\n      tf_master=FLAGS.master,\n      env_name=FLAGS.env_name,\n      env_load_fn=suite_atari.load,\n      replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n      num_environment_steps=FLAGS.num_environment_steps,\n      num_parallel_environments=FLAGS.num_parallel_environments,\n      num_epochs=FLAGS.num_epochs,\n      collect_episodes_per_iteration=FLAGS.collect_episodes_per_iteration,\n      num_eval_episodes=FLAGS.num_eval_episodes)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  multiprocessing.handle_main(lambda _: app.run(main))\n'"
tf_agents/agents/ppo/examples/v1/train_eval_clip_agent_random_py_env.py,2,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval PPO Clip Agent, with required random_py_environment import.\n\nLaunch train eval binary:\n\nFor usage, see train_eval_clip_agent.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ppo.examples.v1 import train_eval_clip_agent\nfrom tf_agents.environments import random_py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.system import system_multiprocessing as multiprocessing\n\nFLAGS = flags.FLAGS\n\n\ndef env_load_fn(env_name):\n  del env_name\n  obs_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n  action_spec = array_spec.BoundedArraySpec((2, 3), np.int32, -10, 10)\n  return random_py_environment.RandomPyEnvironment(\n      obs_spec, action_spec=action_spec, min_duration=2, max_duration=4)\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  if tf.executing_eagerly():\n    # self.skipTest(\'b/123777119\')  # Secondary bug: (\'b/123775375\')\n    return\n  logging.set_verbosity(logging.INFO)\n  train_eval_clip_agent.train_eval(\n      FLAGS.root_dir,\n      tf_master=FLAGS.master,\n      env_name=FLAGS.env_name,\n      env_load_fn=env_load_fn,\n      replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n      num_environment_steps=FLAGS.num_environment_steps,\n      num_parallel_environments=FLAGS.num_parallel_environments,\n      num_epochs=FLAGS.num_epochs,\n      collect_episodes_per_iteration=FLAGS.collect_episodes_per_iteration,\n      num_eval_episodes=FLAGS.num_eval_episodes,\n      use_rnns=FLAGS.use_rnns)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  multiprocessing.handle_main(lambda _: app.run(main))\n'"
tf_agents/agents/ppo/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Train and Eval PPO.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/ppo/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py \\\n  --root_dir=$HOME/tmp/ppo/gym/HalfCheetah-v2/ \\\n  --logtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ppo import ppo_clip_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.networks import value_network\nfrom tf_agents.networks import value_rnn_network\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.system import system_multiprocessing as multiprocessing\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_string(\'env_name\', \'HalfCheetah-v2\', \'Name of an environment\')\nflags.DEFINE_integer(\'replay_buffer_capacity\', 1001,\n                     \'Replay buffer capacity per env.\')\nflags.DEFINE_integer(\'num_parallel_environments\', 30,\n                     \'Number of environments to run in parallel\')\nflags.DEFINE_integer(\'num_environment_steps\', 25000000,\n                     \'Number of environment steps to run before finishing.\')\nflags.DEFINE_integer(\'num_epochs\', 25,\n                     \'Number of epochs for computing policy updates.\')\nflags.DEFINE_integer(\n    \'collect_episodes_per_iteration\', 30,\n    \'The number of episodes to take in the environment before \'\n    \'each update. This is the total across all parallel \'\n    \'environments.\')\nflags.DEFINE_integer(\'num_eval_episodes\', 30,\n                     \'The number of episodes to run eval on.\')\nflags.DEFINE_boolean(\'use_rnns\', False,\n                     \'If true, use RNN for policy and value function.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    env_load_fn=suite_mujoco.load,\n    random_seed=None,\n    # TODO(b/127576522): rename to policy_fc_layers.\n    actor_fc_layers=(200, 100),\n    value_fc_layers=(200, 100),\n    use_rnns=False,\n    # Params for collect\n    num_environment_steps=25000000,\n    collect_episodes_per_iteration=30,\n    num_parallel_environments=30,\n    replay_buffer_capacity=1001,  # Per-environment\n    # Params for train\n    num_epochs=25,\n    learning_rate=1e-3,\n    # Params for eval\n    num_eval_episodes=30,\n    eval_interval=500,\n    # Params for summaries and logging\n    train_checkpoint_interval=500,\n    policy_checkpoint_interval=500,\n    log_interval=50,\n    summary_interval=50,\n    summaries_flush_secs=1,\n    use_tf_functions=True,\n    debug_summaries=False,\n    summarize_grads_and_vars=False):\n  """"""A simple train and eval for PPO.""""""\n  if root_dir is None:\n    raise AttributeError(\'train_eval requires a root_dir.\')\n\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n  saved_model_dir = os.path.join(root_dir, \'policy_saved_model\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if random_seed is not None:\n      tf.compat.v1.set_random_seed(random_seed)\n    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n    tf_env = tf_py_environment.TFPyEnvironment(\n        parallel_py_environment.ParallelPyEnvironment(\n            [lambda: env_load_fn(env_name)] * num_parallel_environments))\n    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\n    if use_rnns:\n      actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          input_fc_layer_params=actor_fc_layers,\n          output_fc_layer_params=None)\n      value_net = value_rnn_network.ValueRnnNetwork(\n          tf_env.observation_spec(),\n          input_fc_layer_params=value_fc_layers,\n          output_fc_layer_params=None)\n    else:\n      actor_net = actor_distribution_network.ActorDistributionNetwork(\n          tf_env.observation_spec(),\n          tf_env.action_spec(),\n          fc_layer_params=actor_fc_layers,\n          activation_fn=tf.keras.activations.tanh)\n      value_net = value_network.ValueNetwork(\n          tf_env.observation_spec(),\n          fc_layer_params=value_fc_layers,\n          activation_fn=tf.keras.activations.tanh)\n\n    tf_agent = ppo_clip_agent.PPOClipAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        optimizer,\n        actor_net=actor_net,\n        value_net=value_net,\n        entropy_regularization=0.0,\n        importance_ratio_clipping=0.2,\n        normalize_observations=False,\n        normalize_rewards=False,\n        use_gae=True,\n        num_epochs=num_epochs,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    environment_steps_metric = tf_metrics.EnvironmentSteps()\n    step_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        environment_steps_metric,\n    ]\n\n    train_metrics = step_metrics + [\n        tf_metrics.AverageReturnMetric(\n            batch_size=num_parallel_environments),\n        tf_metrics.AverageEpisodeLengthMetric(\n            batch_size=num_parallel_environments),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=num_parallel_environments,\n        max_length=replay_buffer_capacity)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=eval_policy,\n        global_step=global_step)\n    saved_model = policy_saver.PolicySaver(\n        eval_policy, train_step=global_step)\n\n    train_checkpointer.initialize_or_restore()\n\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration)\n\n    def train_step():\n      trajectories = replay_buffer.gather_all()\n      return tf_agent.train(experience=trajectories)\n\n    if use_tf_functions:\n      # TODO(b/123828980): Enable once the cause for slowdown was identified.\n      collect_driver.run = common.function(collect_driver.run, autograph=False)\n      tf_agent.train = common.function(tf_agent.train, autograph=False)\n      train_step = common.function(train_step)\n\n    collect_time = 0\n    train_time = 0\n    timed_at_step = global_step.numpy()\n\n    while environment_steps_metric.result() < num_environment_steps:\n      global_step_val = global_step.numpy()\n      if global_step_val % eval_interval == 0:\n        metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n\n      start_time = time.time()\n      collect_driver.run()\n      collect_time += time.time() - start_time\n\n      start_time = time.time()\n      total_loss, _ = train_step()\n      replay_buffer.clear()\n      train_time += time.time() - start_time\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=step_metrics)\n\n      if global_step_val % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step_val, total_loss)\n        steps_per_sec = (\n            (global_step_val - timed_at_step) / (collect_time + train_time))\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        logging.info(\'collect_time = %.3f, train_time = %.3f\', collect_time,\n                     train_time)\n        with tf.compat.v2.summary.record_if(True):\n          tf.compat.v2.summary.scalar(\n              name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n          saved_model_path = os.path.join(\n              saved_model_dir, \'policy_\' + (\'%d\' % global_step_val).zfill(9))\n          saved_model.save(saved_model_path)\n\n        timed_at_step = global_step_val\n        collect_time = 0\n        train_time = 0\n\n    # One final eval before exiting.\n    metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_v2_behavior()\n  train_eval(\n      FLAGS.root_dir,\n      env_name=FLAGS.env_name,\n      use_rnns=FLAGS.use_rnns,\n      num_environment_steps=FLAGS.num_environment_steps,\n      collect_episodes_per_iteration=FLAGS.collect_episodes_per_iteration,\n      num_parallel_environments=FLAGS.num_parallel_environments,\n      replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n      num_epochs=FLAGS.num_epochs,\n      num_eval_episodes=FLAGS.num_eval_episodes)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  multiprocessing.handle_main(lambda _: app.run(main))\n'"
tf_agents/agents/reinforce/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/reinforce/examples/v1/train_eval.py,12,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval REINFORCE.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/reinforce_v1/gym/CartPole-v0/ --port 2223 &\n\npython tf_agents/agents/reinforce/examples/v1/train_eval.py \\\n  --root_dir=$HOME/tmp/reinforce_v1/gym/CartPole-v0/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.reinforce import reinforce_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import value_network\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 1000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\ndef train_eval(\n    root_dir,\n    env_name=\'CartPole-v0\',\n    num_iterations=1000,\n    # TODO(b/127576522): rename to policy_fc_layers.\n    actor_fc_layers=(100,),\n    value_net_fc_layers=(100,),\n    use_value_network=False,\n    # Params for collect\n    collect_episodes_per_iteration=2,\n    replay_buffer_capacity=2000,\n    # Params for train\n    learning_rate=1e-3,\n    gamma=0.9,\n    gradient_clipping=None,\n    normalize_returns=True,\n    value_estimation_loss_coef=0.2,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=100,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=100,\n    policy_checkpoint_interval=100,\n    rb_checkpoint_interval=200,\n    log_interval=100,\n    summary_interval=100,\n    summaries_flush_secs=1,\n    debug_summaries=True,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for Reinforce.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    eval_py_env = suite_gym.load(env_name)\n    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n\n    # TODO(b/127870767): Handle distributions without gin.\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers)\n\n    if use_value_network:\n      value_net = value_network.ValueNetwork(\n          tf_env.time_step_spec().observation,\n          fc_layer_params=value_net_fc_layers)\n\n    tf_agent = reinforce_agent.ReinforceAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        value_network=value_net if use_value_network else None,\n        value_estimation_loss_coef=value_estimation_loss_coef,\n        gamma=gamma,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n        normalize_returns=normalize_returns,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n\n    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration).run()\n\n    experience = replay_buffer.gather_all()\n    train_op = tf_agent.train(experience)\n    clear_rb_op = replay_buffer.clear()\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      # TODO(b/126239733): Remove once Periodically can be saved.\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n\n      # Compute evaluation metrics.\n      global_step_call = sess.make_callable(global_step)\n      global_step_val = global_step_call()\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      clear_rb_call = sess.make_callable(clear_rb_op)\n\n      timed_at_step = global_step_call()\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        total_loss, _ = train_step_call()\n        clear_rb_call()\n        time_acc += time.time() - start_time\n        global_step_val = global_step_call()\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val, total_loss.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n          )\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  logging.set_verbosity(logging.INFO)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/reinforce/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/reinforce/examples/v2/train_eval.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval REINFORCE.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/reinforce/gym/CartPole-v0/ --port 2223 &\n\npython tf_agents/agents/reinforce/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/reinforce/gym/CartPole-v0/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.reinforce import reinforce_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.networks import value_network\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 500,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\ndef train_eval(\n    root_dir,\n    env_name=\'CartPole-v0\',\n    num_iterations=1000,\n    actor_fc_layers=(100,),\n    value_net_fc_layers=(100,),\n    use_value_network=False,\n    use_tf_functions=True,\n    # Params for collect\n    collect_episodes_per_iteration=2,\n    replay_buffer_capacity=2000,\n    # Params for train\n    learning_rate=1e-3,\n    gamma=0.9,\n    gradient_clipping=None,\n    normalize_returns=True,\n    value_estimation_loss_coef=0.2,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=100,\n    # Params for checkpoints, summaries, and logging\n    log_interval=100,\n    summary_interval=100,\n    summaries_flush_secs=1,\n    debug_summaries=True,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for Reinforce.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers)\n\n    if use_value_network:\n      value_net = value_network.ValueNetwork(\n          tf_env.time_step_spec().observation,\n          fc_layer_params=value_net_fc_layers)\n\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    tf_agent = reinforce_agent.ReinforceAgent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        value_network=value_net if use_value_network else None,\n        value_estimation_loss_coef=value_estimation_loss_coef,\n        gamma=gamma,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n        normalize_returns=normalize_returns,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration)\n\n    def train_step():\n      experience = replay_buffer.gather_all()\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      # To speed up collect use TF function.\n      collect_driver.run = common.function(collect_driver.run)\n      # To speed up train use TF function.\n      tf_agent.train = common.function(tf_agent.train)\n      train_step = common.function(train_step)\n\n    # Compute evaluation metrics.\n    metrics = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    # TODO(b/126590894): Move this functionality into eager_compute_summaries\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(metrics, global_step.numpy())\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      total_loss = train_step()\n      replay_buffer.clear()\n      time_acc += time.time() - start_time\n\n      global_step_val = global_step.numpy()\n      if global_step_val % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step_val, total_loss.loss)\n        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step_val\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step_val % eval_interval == 0:\n        metrics = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        # TODO(b/126590894): Move this functionality into\n        # eager_compute_summaries.\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(metrics, global_step_val)\n\n\ndef main(_):\n  tf.compat.v1.enable_eager_execution(\n      config=tf.compat.v1.ConfigProto(allow_soft_placement=True))\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/sac/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/sac/examples/v1/train_eval.py,16,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval SAC.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/sac_v1/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/sac/examples/v1/train_eval.py \\\n  --root_dir=$HOME/tmp/sac_v1/gym/HalfCheetah-v2/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.sac import sac_agent\nfrom tf_agents.agents.sac import tanh_normal_projection_network\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.metrics import tf_py_metric\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_multi_string(\'gin_file\', None,\n                          \'Path to the gin config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding to pass through.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    eval_env_name=None,\n    env_load_fn=suite_mujoco.load,\n    # The SAC paper reported:\n    # Hopper and Cartpole results up to 1000000 iters,\n    # Humanoid results up to 10000000 iters,\n    # Other mujoco tasks up to 3000000 iters.\n    num_iterations=3000000,\n    actor_fc_layers=(256, 256),\n    critic_obs_fc_layers=None,\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(256, 256),\n    # Params for collect\n    # Follow https://github.com/haarnoja/sac/blob/master/examples/variants.py\n    # HalfCheetah and Ant take 10000 initial collection steps.\n    # Other mujoco tasks take 1000.\n    # Different choices roughly keep the initial episodes about the same.\n    initial_collect_steps=10000,\n    collect_steps_per_iteration=1,\n    replay_buffer_capacity=1000000,\n    # Params for target update\n    target_update_tau=0.005,\n    target_update_period=1,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=256,\n    actor_learning_rate=3e-4,\n    critic_learning_rate=3e-4,\n    alpha_learning_rate=3e-4,\n    td_errors_loss_fn=tf.math.squared_difference,\n    gamma=0.99,\n    reward_scale_factor=0.1,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=30,\n    eval_interval=10000,\n    # Params for summaries and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=50000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for SAC.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n  eval_summary_flush_op = eval_summary_writer.flush()\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    # Create the environment.\n    tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n    eval_env_name = eval_env_name or env_name\n    eval_py_env = env_load_fn(eval_env_name)\n\n    # Get the data specs from the environment\n    time_step_spec = tf_env.time_step_spec()\n    observation_spec = time_step_spec.observation\n    action_spec = tf_env.action_spec()\n\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        fc_layer_params=actor_fc_layers,\n        continuous_projection_net=tanh_normal_projection_network\n        .TanhNormalProjectionNetwork)\n    critic_net = critic_network.CriticNetwork(\n        (observation_spec, action_spec),\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        kernel_initializer=\'glorot_uniform\',\n        last_kernel_initializer=\'glorot_uniform\')\n\n    tf_agent = sac_agent.SacAgent(\n        time_step_spec,\n        action_spec,\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=alpha_learning_rate),\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n\n    # Make the replay buffer.\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=tf_agent.collect_data_spec,\n        batch_size=1,\n        max_length=replay_buffer_capacity)\n    replay_observer = [replay_buffer.add_batch]\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(\n        greedy_policy.GreedyPolicy(tf_agent.policy))\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_py_metric.TFPyMetric(py_metrics.AverageReturnMetric()),\n        tf_py_metric.TFPyMetric(py_metrics.AverageEpisodeLengthMetric()),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n\n    initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        initial_collect_policy,\n        observers=replay_observer + train_metrics,\n        num_steps=initial_collect_steps).run()\n\n    collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=replay_observer + train_metrics,\n        num_steps=collect_steps_per_iteration).run()\n\n    # Prepare replay buffer as dataset with invalid transitions filtered.\n    def _filter_invalid_transition(trajectories, unused_arg1):\n      return ~trajectories.is_boundary()[0]\n    dataset = replay_buffer.as_dataset(\n        sample_batch_size=batch_size,\n        num_steps=2).unbatch().filter(\n            _filter_invalid_transition).batch(batch_size).prefetch(5)\n    dataset_iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    trajectories, unused_info = dataset_iterator.get_next()\n    train_op = tf_agent.train(trajectories)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n\n      # Initialize training.\n      sess.run(dataset_iterator.initializer)\n      common.initialize_uninitialized_variables(sess)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n\n      global_step_val = sess.run(global_step)\n\n      if global_step_val == 0:\n        # Initial eval of randomly initialized policy\n        metric_utils.compute_summaries(\n            eval_metrics,\n            eval_py_env,\n            eval_py_policy,\n            num_episodes=num_eval_episodes,\n            global_step=global_step_val,\n            callback=eval_metrics_callback,\n            log=True,\n        )\n        sess.run(eval_summary_flush_op)\n\n        # Run initial collect.\n        logging.info(\'Global step %d: Running initial collect op.\',\n                     global_step_val)\n        sess.run(initial_collect_op)\n\n        # Checkpoint the initial replay buffer contents.\n        rb_checkpointer.save(global_step=global_step_val)\n\n        logging.info(\'Finished initial collect.\')\n      else:\n        logging.info(\'Global step %d: Skipping initial collect op.\',\n                     global_step_val)\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      global_step_call = sess.make_callable(global_step)\n\n      timed_at_step = global_step_call()\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          total_loss, _ = train_step_call()\n        time_acc += time.time() - start_time\n        global_step_val = global_step_call()\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val, total_loss.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True,\n          )\n          sess.run(eval_summary_flush_op)\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n\ndef main(_):\n  tf.compat.v1.enable_resource_variables()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/sac/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/agents/sac/examples/v2/train_eval.py,11,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval SAC.\n\nAll hyperparameters come from the SAC paper\nhttps://arxiv.org/pdf/1812.05905.pdf\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/sac/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/sac/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/sac/gym/HalfCheetah-v2/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.sac import sac_agent\nfrom tf_agents.agents.sac import tanh_normal_projection_network\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import actor_distribution_network\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Path to the trainer config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding to pass through.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    eval_env_name=None,\n    env_load_fn=suite_mujoco.load,\n    # The SAC paper reported:\n    # Hopper and Cartpole results up to 1000000 iters,\n    # Humanoid results up to 10000000 iters,\n    # Other mujoco tasks up to 3000000 iters.\n    num_iterations=3000000,\n    actor_fc_layers=(256, 256),\n    critic_obs_fc_layers=None,\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(256, 256),\n    # Params for collect\n    # Follow https://github.com/haarnoja/sac/blob/master/examples/variants.py\n    # HalfCheetah and Ant take 10000 initial collection steps.\n    # Other mujoco tasks take 1000.\n    # Different choices roughly keep the initial episodes about the same.\n    initial_collect_steps=10000,\n    collect_steps_per_iteration=1,\n    replay_buffer_capacity=1000000,\n    # Params for target update\n    target_update_tau=0.005,\n    target_update_period=1,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=256,\n    actor_learning_rate=3e-4,\n    critic_learning_rate=3e-4,\n    alpha_learning_rate=3e-4,\n    td_errors_loss_fn=tf.math.squared_difference,\n    gamma=0.99,\n    reward_scale_factor=0.1,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=30,\n    eval_interval=10000,\n    # Params for summaries and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=50000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for SAC.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n    eval_env_name = eval_env_name or env_name\n    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(eval_env_name))\n\n    time_step_spec = tf_env.time_step_spec()\n    observation_spec = time_step_spec.observation\n    action_spec = tf_env.action_spec()\n\n    actor_net = actor_distribution_network.ActorDistributionNetwork(\n        observation_spec,\n        action_spec,\n        fc_layer_params=actor_fc_layers,\n        continuous_projection_net=tanh_normal_projection_network\n        .TanhNormalProjectionNetwork)\n    critic_net = critic_network.CriticNetwork(\n        (observation_spec, action_spec),\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        kernel_initializer=\'glorot_uniform\',\n        last_kernel_initializer=\'glorot_uniform\')\n\n    tf_agent = sac_agent.SacAgent(\n        time_step_spec,\n        action_spec,\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=alpha_learning_rate),\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    # Make the replay buffer.\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=tf_agent.collect_data_spec,\n        batch_size=1,\n        max_length=replay_buffer_capacity)\n    replay_observer = [replay_buffer.add_batch]\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(\n            buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),\n        tf_metrics.AverageEpisodeLengthMetric(\n            buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),\n    ]\n\n    eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n    collect_policy = tf_agent.collect_policy\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=eval_policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    train_checkpointer.initialize_or_restore()\n    rb_checkpointer.initialize_or_restore()\n\n    if replay_buffer.num_frames() == 0:\n      initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n          tf_env,\n          initial_collect_policy,\n          observers=replay_observer + train_metrics,\n          num_steps=initial_collect_steps)\n\n    collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=replay_observer + train_metrics,\n        num_steps=collect_steps_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    if replay_buffer.num_frames() == 0:\n      # Collect initial replay data.\n      logging.info(\n          \'Initializing replay buffer by collecting experience for %d steps \'\n          \'with a random policy.\', initial_collect_steps)\n      initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Prepare replay buffer as dataset with invalid transitions filtered.\n    def _filter_invalid_transition(trajectories, unused_arg1):\n      return ~trajectories.is_boundary()[0]\n    dataset = replay_buffer.as_dataset(\n        sample_batch_size=batch_size,\n        num_steps=2).unbatch().filter(\n            _filter_invalid_transition).batch(batch_size).prefetch(5)\n    # Dataset generates trajectories with shape [Bx2x...]\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      global_step_val = global_step.numpy()\n\n      if global_step_val % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step_val,\n                     train_loss.loss)\n        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step_val\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step_val % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step_val)\n        metric_utils.log_metrics(eval_metrics)\n\n      if global_step_val % train_checkpoint_interval == 0:\n        train_checkpointer.save(global_step=global_step_val)\n\n      if global_step_val % policy_checkpoint_interval == 0:\n        policy_checkpointer.save(global_step=global_step_val)\n\n      if global_step_val % rb_checkpoint_interval == 0:\n        rb_checkpointer.save(global_step=global_step_val)\n    return train_loss\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir)\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/sac/examples/v2/train_eval_rnn.py,11,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval RNN SAC.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/sac_rnn/dm/CartPole-Balance/ --port 2223 &\n\npython tf_agents/agents/sac/examples/v2:train_eval_rnn --\\\n  --root_dir=$HOME/tmp/sac_rnn/dm/CartPole-Balance/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.sac import sac_agent\nfrom tf_agents.agents.sac import tanh_normal_projection_network\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import parallel_py_environment\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import actor_distribution_rnn_network\nfrom tf_agents.policies import greedy_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Path to the trainer config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding to pass through.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'cartpole\',\n    task_name=\'balance\',\n    observations_whitelist=\'position\',\n    eval_env_name=None,\n    num_iterations=1000000,\n    # Params for networks.\n    actor_fc_layers=(400, 300),\n    actor_output_fc_layers=(100,),\n    actor_lstm_size=(40,),\n    critic_obs_fc_layers=None,\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    critic_output_fc_layers=(100,),\n    critic_lstm_size=(40,),\n    num_parallel_environments=1,\n    # Params for collect\n    initial_collect_episodes=1,\n    collect_episodes_per_iteration=1,\n    replay_buffer_capacity=1000000,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=256,\n    critic_learning_rate=3e-4,\n    train_sequence_length=20,\n    actor_learning_rate=3e-4,\n    alpha_learning_rate=3e-4,\n    td_errors_loss_fn=tf.math.squared_difference,\n    gamma=0.99,\n    reward_scale_factor=0.1,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=30,\n    eval_interval=10000,\n    # Params for summaries and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=50000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n  """"""A simple train and eval for RNN SAC on DM control.""""""\n  root_dir = os.path.expanduser(root_dir)\n\n  summary_writer = tf.compat.v2.summary.create_file_writer(\n      root_dir, flush_millis=summaries_flush_secs * 1000)\n  summary_writer.set_as_default()\n\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if observations_whitelist is not None:\n      env_wrappers = [\n          functools.partial(\n              wrappers.FlattenObservationsWrapper,\n              observations_whitelist=[observations_whitelist])\n      ]\n    else:\n      env_wrappers = []\n\n    env_load_fn = functools.partial(suite_dm_control.load,\n                                    task_name=task_name,\n                                    env_wrappers=env_wrappers)\n\n    if num_parallel_environments == 1:\n      py_env = env_load_fn(env_name)\n    else:\n      py_env = parallel_py_environment.ParallelPyEnvironment(\n          [lambda: env_load_fn(env_name)]*num_parallel_environments)\n    tf_env = tf_py_environment.TFPyEnvironment(py_env)\n    eval_env_name = eval_env_name or env_name\n    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(eval_env_name))\n\n    time_step_spec = tf_env.time_step_spec()\n    observation_spec = time_step_spec.observation\n    action_spec = tf_env.action_spec()\n\n    actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n        observation_spec,\n        action_spec,\n        input_fc_layer_params=actor_fc_layers,\n        lstm_size=actor_lstm_size,\n        output_fc_layer_params=actor_output_fc_layers,\n        continuous_projection_net=tanh_normal_projection_network\n        .TanhNormalProjectionNetwork)\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        (observation_spec, action_spec),\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        lstm_size=critic_lstm_size,\n        output_fc_layer_params=critic_output_fc_layers,\n        kernel_initializer=\'glorot_uniform\',\n        last_kernel_initializer=\'glorot_uniform\')\n\n    tf_agent = sac_agent.SacAgent(\n        time_step_spec,\n        action_spec,\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=alpha_learning_rate),\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step)\n    tf_agent.initialize()\n\n    # Make the replay buffer.\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec=tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n    replay_observer = [replay_buffer.add_batch]\n\n    env_steps = tf_metrics.EnvironmentSteps(prefix=\'Train\')\n    average_return = tf_metrics.AverageReturnMetric(\n        prefix=\'Train\',\n        buffer_size=num_eval_episodes,\n        batch_size=tf_env.batch_size)\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(prefix=\'Train\'),\n        env_steps,\n        average_return,\n        tf_metrics.AverageEpisodeLengthMetric(\n            prefix=\'Train\',\n            buffer_size=num_eval_episodes,\n            batch_size=tf_env.batch_size),\n    ]\n\n    eval_policy = greedy_policy.GreedyPolicy(tf_agent.policy)\n    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n        tf_env.time_step_spec(), tf_env.action_spec())\n    collect_policy = tf_agent.collect_policy\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(root_dir, \'train\'),\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(root_dir, \'policy\'),\n        policy=eval_policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(root_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    train_checkpointer.initialize_or_restore()\n    rb_checkpointer.initialize_or_restore()\n\n    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        initial_collect_policy,\n        observers=replay_observer + train_metrics,\n        num_episodes=initial_collect_episodes)\n\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=replay_observer + train_metrics,\n        num_episodes=collect_episodes_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    # Collect initial replay data.\n    if env_steps.result() == 0 or replay_buffer.num_frames() == 0:\n      logging.info(\n          \'Initializing replay buffer by collecting experience for %d episodes \'\n          \'with a random policy.\', initial_collect_episodes)\n      initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=env_steps.result(),\n        summary_writer=summary_writer,\n        summary_prefix=\'Eval\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, env_steps.result())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    time_acc = 0\n    env_steps_before = env_steps.result().numpy()\n\n    # Prepare replay buffer as dataset with invalid transitions filtered.\n    def _filter_invalid_transition(trajectories, unused_arg1):\n      # Reduce filter_fn over full trajectory sampled. The sequence is kept only\n      # if all elements except for the last one pass the filter. This is to\n      # allow training on terminal steps.\n      return tf.reduce_all(~trajectories.is_boundary()[:-1])\n    dataset = replay_buffer.as_dataset(\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length+1).unbatch().filter(\n            _filter_invalid_transition).batch(batch_size).prefetch(5)\n    # Dataset generates trajectories with shape [Bx2x...]\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      start_env_steps = env_steps.result()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      episode_steps = env_steps.result() - start_env_steps\n      # TODO(b/152648849)\n      for _ in range(episode_steps):\n        for _ in range(train_steps_per_iteration):\n          train_step()\n        time_acc += time.time() - start_time\n\n        if global_step.numpy() % log_interval == 0:\n          logging.info(\'env steps = %d, average return = %f\',\n                       env_steps.result(), average_return.result())\n          env_steps_per_sec = (env_steps.result().numpy() -\n                               env_steps_before) / time_acc\n          logging.info(\'%.3f env steps/sec\', env_steps_per_sec)\n          tf.compat.v2.summary.scalar(\n              name=\'env_steps_per_sec\',\n              data=env_steps_per_sec,\n              step=env_steps.result())\n          time_acc = 0\n          env_steps_before = env_steps.result().numpy()\n\n        for train_metric in train_metrics:\n          train_metric.tf_summaries(train_step=env_steps.result())\n\n        if global_step.numpy() % eval_interval == 0:\n          results = metric_utils.eager_compute(\n              eval_metrics,\n              eval_tf_env,\n              eval_policy,\n              num_episodes=num_eval_episodes,\n              train_step=env_steps.result(),\n              summary_writer=summary_writer,\n              summary_prefix=\'Eval\',\n          )\n          if eval_metrics_callback is not None:\n            eval_metrics_callback(results, env_steps.numpy())\n          metric_utils.log_metrics(eval_metrics)\n\n        global_step_val = global_step.numpy()\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/td3/examples/v1/train_eval.py,15,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval TD3.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/td3_v1/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/td3/examples/v1/train_eval.py \\\n  --root_dir=$HOME/tmp/td3_v1/gym/HalfCheetah-v2/ \\\n  --num_iterations=2000000 \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.td3 import td3_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    num_iterations=2000000,\n    actor_fc_layers=(400, 300),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    replay_buffer_capacity=100000,\n    exploration_noise_std=0.1,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    actor_update_period=2,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=10000,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=20000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for TD3.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(suite_mujoco.load(env_name))\n    eval_py_env = suite_mujoco.load(env_name)\n\n    actor_net = actor_network.ActorNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers,\n    )\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_network.CriticNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n    )\n\n    tf_agent = td3_agent.Td3Agent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        exploration_noise_std=exploration_noise_std,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        actor_update_period=actor_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step,\n    )\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n    initial_collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=initial_collect_steps).run()\n\n    collect_op = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=collect_steps_per_iteration).run()\n\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=2).prefetch(3)\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    trajectories, unused_info = iterator.get_next()\n\n    train_fn = common.function(tf_agent.train)\n    train_op = train_fn(experience=trajectories)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      # TODO(b/126239733): Remove once Periodically can be saved.\n      common.initialize_uninitialized_variables(sess)\n\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      sess.run(initial_collect_op)\n\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops, global_step])\n\n      timed_at_step = sess.run(global_step)\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _, global_step_val = train_step_call()\n        time_acc += time.time() - start_time\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True,\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/td3/examples/v1/train_eval_rnn.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval TD3.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/td3_rnn_v1/dm/CartPole-Balance/ --port 2223 &\n\npython tf_agents/agents/td3/examples/v1/train_eval_rnn.py \\\n  --root_dir=$HOME/tmp/td3_rnn_v1/dm/CartPole-Balance/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.td3 import td3_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import py_metrics\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.policies import py_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'cartpole\',\n    task_name=\'balance\',\n    observations_whitelist=\'position\',\n    num_iterations=100000,\n    actor_fc_layers=(400, 300),\n    actor_output_fc_layers=(100,),\n    actor_lstm_size=(40,),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    critic_output_fc_layers=(100,),\n    critic_lstm_size=(40,),\n    # Params for collect\n    initial_collect_steps=1,\n    collect_episodes_per_iteration=1,\n    replay_buffer_capacity=100000,\n    exploration_noise_std=0.1,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=200,\n    batch_size=64,\n    actor_update_period=2,\n    train_sequence_length=10,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=1000,\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10000,\n    policy_checkpoint_interval=5000,\n    rb_checkpoint_interval=10000,\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for DDPG.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      py_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      py_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n  ]\n\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if observations_whitelist is not None:\n      env_wrappers = [\n          functools.partial(\n              wrappers.FlattenObservationsWrapper,\n              observations_whitelist=[observations_whitelist])\n      ]\n    else:\n      env_wrappers = []\n    environment = suite_dm_control.load(\n        env_name, task_name, env_wrappers=env_wrappers)\n    tf_env = tf_py_environment.TFPyEnvironment(environment)\n    eval_py_env = suite_dm_control.load(\n        env_name, task_name, env_wrappers=env_wrappers)\n\n    actor_net = actor_rnn_network.ActorRnnNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        input_fc_layer_params=actor_fc_layers,\n        lstm_size=actor_lstm_size,\n        output_fc_layer_params=actor_output_fc_layers)\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        lstm_size=critic_lstm_size,\n        output_fc_layer_params=critic_output_fc_layers,\n    )\n\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    tf_agent = td3_agent.Td3Agent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        exploration_noise_std=exploration_noise_std,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        actor_update_period=actor_update_period,\n        dqda_clipping=dqda_clipping,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        debug_summaries=debug_summaries,\n        train_step_counter=global_step)\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    eval_py_policy = py_tf_policy.PyTFPolicy(tf_agent.policy)\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    collect_policy = tf_agent.collect_policy\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n    initial_collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=initial_collect_steps).run(policy_state=policy_state)\n\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n    collect_op = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration).run(\n            policy_state=policy_state)\n\n    # Need extra step to generate transitions of train_sequence_length.\n    # Dataset generates trajectories with shape [BxTx...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n\n    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n    trajectories, unused_info = iterator.get_next()\n\n    train_fn = common.function(tf_agent.train)\n    train_op = train_fn(experience=trajectories)\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        agent=tf_agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        policy=tf_agent.policy,\n        global_step=global_step)\n    rb_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'replay_buffer\'),\n        max_to_keep=1,\n        replay_buffer=replay_buffer)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(train_metric.tf_summaries(\n          train_step=global_step, step_metrics=train_metrics[:2]))\n\n    with eval_summary_writer.as_default(), \\\n         tf.compat.v2.summary.record_if(True):\n      for eval_metric in eval_metrics:\n        eval_metric.tf_summaries(train_step=global_step)\n\n    init_agent_op = tf_agent.initialize()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      rb_checkpointer.initialize_or_restore(sess)\n      sess.run(iterator.initializer)\n      sess.run(init_agent_op)\n      sess.run(train_summary_writer.init())\n      sess.run(eval_summary_writer.init())\n      sess.run(initial_collect_op)\n\n      global_step_val = sess.run(global_step)\n      metric_utils.compute_summaries(\n          eval_metrics,\n          eval_py_env,\n          eval_py_policy,\n          num_episodes=num_eval_episodes,\n          global_step=global_step_val,\n          callback=eval_metrics_callback,\n          log=True,\n      )\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      global_step_call = sess.make_callable(global_step)\n\n      timed_at_step = global_step_call()\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\', data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(num_iterations):\n        start_time = time.time()\n        collect_call()\n        for _ in range(train_steps_per_iteration):\n          loss_info_value, _ = train_step_call()\n        time_acc += time.time() - start_time\n\n        global_step_val = global_step_call()\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val,\n                       loss_info_value.loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % rb_checkpoint_interval == 0:\n          rb_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % eval_interval == 0:\n          metric_utils.compute_summaries(\n              eval_metrics,\n              eval_py_env,\n              eval_py_policy,\n              num_episodes=num_eval_episodes,\n              global_step=global_step_val,\n              callback=eval_metrics_callback,\n              log=True,\n          )\n\n\ndef main(_):\n  logging.set_verbosity(logging.INFO)\n  tf.compat.v1.enable_resource_variables()\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/td3/examples/v2/train_eval.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval TD3.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/td3/gym/HalfCheetah-v2/ --port 2223 &\n\npython tf_agents/agents/td3/examples/v2/train_eval.py \\\n  --root_dir=$HOME/tmp/td3/gym/HalfCheetah-v2/ \\\n  --num_iterations=2000000 \\\n  --alsologtostderr\n```\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_network\nfrom tf_agents.agents.ddpg import critic_network\nfrom tf_agents.agents.td3 import td3_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_mujoco\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Paths to the gin-config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding parameters.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'HalfCheetah-v2\',\n    num_iterations=2000000,\n    actor_fc_layers=(400, 300),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    # Params for collect\n    initial_collect_steps=1000,\n    collect_steps_per_iteration=1,\n    replay_buffer_capacity=100000,\n    exploration_noise_std=0.1,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=1,\n    batch_size=64,\n    actor_update_period=2,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=tf.compat.v1.losses.huber_loss,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=10000,\n    # Params for checkpoints, summaries, and logging\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for TD3.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    tf_env = tf_py_environment.TFPyEnvironment(suite_mujoco.load(env_name))\n    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_mujoco.load(env_name))\n\n    actor_net = actor_network.ActorNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        fc_layer_params=actor_fc_layers,\n    )\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_network.CriticNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n    )\n\n    tf_agent = td3_agent.Td3Agent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        exploration_noise_std=exploration_noise_std,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        actor_update_period=actor_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step,\n    )\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch],\n        num_steps=initial_collect_steps)\n\n    collect_driver = dynamic_step_driver.DynamicStepDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_steps=collect_steps_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    # Collect initial replay data.\n    logging.info(\n        \'Initializing replay buffer by collecting experience for %d steps with \'\n        \'a random policy.\', initial_collect_steps)\n    initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Dataset generates trajectories with shape [Bx2x...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=2).prefetch(3)\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      if global_step.numpy() % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step.numpy(),\n                     train_loss.loss)\n        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step.numpy() % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step.numpy())\n        metric_utils.log_metrics(eval_metrics)\n\n    return train_loss\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/agents/td3/examples/v2/train_eval_rnn.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python2, python3\nr""""""Train and Eval TD3.\n\nTo run:\n\n```bash\ntensorboard --logdir $HOME/tmp/td3_rnn/dm/CartPole-Balance/ --port 2223 &\n\npython tf_agents/agents/td3/examples/v2/train_eval_rnn.py \\\n  --root_dir=$HOME/tmp/td3_rnn/dm/CartPole-Balance/ \\\n  --alsologtostderr\n```\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nimport gin\nfrom six.moves import range\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.agents.ddpg import actor_rnn_network\nfrom tf_agents.agents.ddpg import critic_rnn_network\nfrom tf_agents.agents.td3 import td3_agent\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.environments import suite_dm_control\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import wrappers\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_integer(\'num_iterations\', 100000,\n                     \'Total number train/eval iterations to perform.\')\nflags.DEFINE_multi_string(\'gin_file\', None, \'Paths to the gin-config files.\')\nflags.DEFINE_multi_string(\'gin_param\', None, \'Gin binding parameters.\')\n\nFLAGS = flags.FLAGS\n\n\n@gin.configurable\ndef train_eval(\n    root_dir,\n    env_name=\'cartpole\',\n    task_name=\'balance\',\n    observations_whitelist=\'position\',\n    num_iterations=100000,\n    actor_fc_layers=(400, 300),\n    actor_output_fc_layers=(100,),\n    actor_lstm_size=(40,),\n    critic_obs_fc_layers=(400,),\n    critic_action_fc_layers=None,\n    critic_joint_fc_layers=(300,),\n    critic_output_fc_layers=(100,),\n    critic_lstm_size=(40,),\n    # Params for collect\n    initial_collect_episodes=1,\n    collect_episodes_per_iteration=1,\n    replay_buffer_capacity=100000,\n    exploration_noise_std=0.1,\n    # Params for target update\n    target_update_tau=0.05,\n    target_update_period=5,\n    # Params for train\n    train_steps_per_iteration=200,\n    batch_size=64,\n    actor_update_period=2,\n    train_sequence_length=10,\n    actor_learning_rate=1e-4,\n    critic_learning_rate=1e-3,\n    dqda_clipping=None,\n    td_errors_loss_fn=None,\n    gamma=0.995,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    use_tf_functions=True,\n    # Params for eval\n    num_eval_episodes=10,\n    eval_interval=10000,\n    # Params for checkpoints, summaries, and logging\n    log_interval=1000,\n    summary_interval=1000,\n    summaries_flush_secs=10,\n    debug_summaries=False,\n    summarize_grads_and_vars=False,\n    eval_metrics_callback=None):\n\n  """"""A simple train and eval for TD3.""""""\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n  eval_dir = os.path.join(root_dir, \'eval\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n      train_dir, flush_millis=summaries_flush_secs * 1000)\n  train_summary_writer.set_as_default()\n\n  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n      eval_dir, flush_millis=summaries_flush_secs * 1000)\n  eval_metrics = [\n      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n  ]\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n    if observations_whitelist is not None:\n      env_wrappers = [\n          functools.partial(\n              wrappers.FlattenObservationsWrapper,\n              observations_whitelist=[observations_whitelist])\n      ]\n    else:\n      env_wrappers = []\n\n    tf_env = tf_py_environment.TFPyEnvironment(\n        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n    eval_tf_env = tf_py_environment.TFPyEnvironment(\n        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n\n    actor_net = actor_rnn_network.ActorRnnNetwork(\n        tf_env.time_step_spec().observation,\n        tf_env.action_spec(),\n        input_fc_layer_params=actor_fc_layers,\n        lstm_size=actor_lstm_size,\n        output_fc_layer_params=actor_output_fc_layers)\n\n    critic_net_input_specs = (tf_env.time_step_spec().observation,\n                              tf_env.action_spec())\n\n    critic_net = critic_rnn_network.CriticRnnNetwork(\n        critic_net_input_specs,\n        observation_fc_layer_params=critic_obs_fc_layers,\n        action_fc_layer_params=critic_action_fc_layers,\n        joint_fc_layer_params=critic_joint_fc_layers,\n        lstm_size=critic_lstm_size,\n        output_fc_layer_params=critic_output_fc_layers,\n    )\n\n    tf_agent = td3_agent.Td3Agent(\n        tf_env.time_step_spec(),\n        tf_env.action_spec(),\n        actor_network=actor_net,\n        critic_network=critic_net,\n        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=actor_learning_rate),\n        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n            learning_rate=critic_learning_rate),\n        exploration_noise_std=exploration_noise_std,\n        target_update_tau=target_update_tau,\n        target_update_period=target_update_period,\n        actor_update_period=actor_update_period,\n        dqda_clipping=dqda_clipping,\n        td_errors_loss_fn=td_errors_loss_fn,\n        gamma=gamma,\n        reward_scale_factor=reward_scale_factor,\n        gradient_clipping=gradient_clipping,\n        debug_summaries=debug_summaries,\n        summarize_grads_and_vars=summarize_grads_and_vars,\n        train_step_counter=global_step,\n    )\n    tf_agent.initialize()\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(),\n        tf_metrics.AverageEpisodeLengthMetric(),\n    ]\n\n    eval_policy = tf_agent.policy\n    collect_policy = tf_agent.collect_policy\n\n    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        tf_agent.collect_data_spec,\n        batch_size=tf_env.batch_size,\n        max_length=replay_buffer_capacity)\n\n    initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=initial_collect_episodes)\n\n    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n        tf_env,\n        collect_policy,\n        observers=[replay_buffer.add_batch] + train_metrics,\n        num_episodes=collect_episodes_per_iteration)\n\n    if use_tf_functions:\n      initial_collect_driver.run = common.function(initial_collect_driver.run)\n      collect_driver.run = common.function(collect_driver.run)\n      tf_agent.train = common.function(tf_agent.train)\n\n    # Collect initial replay data.\n    logging.info(\n        \'Initializing replay buffer by collecting experience for %d episodes \'\n        \'with a random policy.\', initial_collect_episodes)\n    initial_collect_driver.run()\n\n    results = metric_utils.eager_compute(\n        eval_metrics,\n        eval_tf_env,\n        eval_policy,\n        num_episodes=num_eval_episodes,\n        train_step=global_step,\n        summary_writer=eval_summary_writer,\n        summary_prefix=\'Metrics\',\n    )\n    if eval_metrics_callback is not None:\n      eval_metrics_callback(results, global_step.numpy())\n    metric_utils.log_metrics(eval_metrics)\n\n    time_step = None\n    policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n\n    timed_at_step = global_step.numpy()\n    time_acc = 0\n\n    # Dataset generates trajectories with shape [BxTx...]\n    dataset = replay_buffer.as_dataset(\n        num_parallel_calls=3,\n        sample_batch_size=batch_size,\n        num_steps=train_sequence_length + 1).prefetch(3)\n    iterator = iter(dataset)\n\n    def train_step():\n      experience, _ = next(iterator)\n      return tf_agent.train(experience)\n\n    if use_tf_functions:\n      train_step = common.function(train_step)\n\n    for _ in range(num_iterations):\n      start_time = time.time()\n      time_step, policy_state = collect_driver.run(\n          time_step=time_step,\n          policy_state=policy_state,\n      )\n      for _ in range(train_steps_per_iteration):\n        train_loss = train_step()\n      time_acc += time.time() - start_time\n\n      if global_step.numpy() % log_interval == 0:\n        logging.info(\'step = %d, loss = %f\', global_step.numpy(),\n                     train_loss.loss)\n        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n        logging.info(\'%.3f steps/sec\', steps_per_sec)\n        tf.compat.v2.summary.scalar(\n            name=\'global_steps_per_sec\', data=steps_per_sec, step=global_step)\n        timed_at_step = global_step.numpy()\n        time_acc = 0\n\n      for train_metric in train_metrics:\n        train_metric.tf_summaries(\n            train_step=global_step, step_metrics=train_metrics[:2])\n\n      if global_step.numpy() % eval_interval == 0:\n        results = metric_utils.eager_compute(\n            eval_metrics,\n            eval_tf_env,\n            eval_policy,\n            num_episodes=num_eval_episodes,\n            train_step=global_step,\n            summary_writer=eval_summary_writer,\n            summary_prefix=\'Metrics\',\n        )\n        if eval_metrics_callback is not None:\n          eval_metrics_callback(results, global_step.numpy())\n        metric_utils.log_metrics(eval_metrics)\n\n    return train_loss\n\n\ndef main(_):\n  tf.compat.v1.enable_v2_behavior()\n  logging.set_verbosity(logging.INFO)\n  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n\n\nif __name__ == \'__main__\':\n  flags.mark_flag_as_required(\'root_dir\')\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v1/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/bandits/agents/examples/v1/train_eval_drifting_linear.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against a drifting linear environment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v1 import trainer\nfrom tf_agents.bandits.environments import drifting_linear_environment as dle\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nse\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nDRIFT_VARIANCE = 0.01\nDRIFT_MEAN = 0.01\nBATCH_SIZE = 8\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 10.0\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_resource_variables()\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    observation_shape = [CONTEXT_DIM]\n    overall_shape = [BATCH_SIZE] + observation_shape\n    observation_distribution = tfd.Normal(\n        loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n    action_shape = [NUM_ACTIONS]\n    observation_to_reward_shape = observation_shape + action_shape\n    observation_to_reward_distribution = tfd.Normal(\n        loc=tf.zeros(observation_to_reward_shape),\n        scale=tf.ones(observation_to_reward_shape))\n    drift_distribution = tfd.Normal(loc=DRIFT_MEAN, scale=DRIFT_VARIANCE)\n    additive_reward_distribution = tfd.Normal(\n        loc=tf.zeros(action_shape),\n        scale=(REWARD_NOISE_VARIANCE * tf.ones(action_shape)))\n    environment_dynamics = dle.DriftingLinearDynamics(\n        observation_distribution,\n        observation_to_reward_distribution,\n        drift_distribution,\n        additive_reward_distribution)\n    environment = nse.NonStationaryStochasticEnvironment(environment_dynamics)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(\n        environment.environment_dynamics.compute_optimal_reward)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        environment.environment_dynamics.compute_optimal_action)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v1/train_eval_piecewise_linear.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against the piecewise linear environment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v1 import trainer\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nse\nfrom tf_agents.bandits.environments import piecewise_stochastic_environment as pse\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nBATCH_SIZE = 8\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\n# Total number of steps: BATCH_SIZE * STEPS_PER_LOOP * TRAINING_LOOPS\n# Set the stationarity length to be half of the total number of steps.\n# This gives us two pieces (each one is stationary inside).\nSTATIONARITY_LENGTH = 1600\nAGENT_ALPHA = 10.0\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_resource_variables()\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    observation_shape = [CONTEXT_DIM]\n    overall_shape = [BATCH_SIZE] + observation_shape\n    observation_distribution = tfd.Normal(\n        loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n    interval_distribution = tfd.Deterministic(STATIONARITY_LENGTH)\n    action_shape = [NUM_ACTIONS]\n    observation_to_reward_shape = observation_shape + action_shape\n    observation_to_reward_distribution = tfd.Normal(\n        loc=tf.zeros(observation_to_reward_shape),\n        scale=tf.ones(observation_to_reward_shape))\n    additive_reward_distribution = tfd.Normal(\n        loc=tf.zeros(action_shape),\n        scale=(REWARD_NOISE_VARIANCE * tf.ones(action_shape)))\n    environment_dynamics = pse.PiecewiseStationaryDynamics(\n        observation_distribution,\n        interval_distribution,\n        observation_to_reward_distribution,\n        additive_reward_distribution)\n    environment = nse.NonStationaryStochasticEnvironment(environment_dynamics)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(\n        environment.environment_dynamics.compute_optimal_reward)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        environment.environment_dynamics.compute_optimal_action)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v1/train_eval_stationary_linear.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under stationary linear environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v1 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.environments import tf_py_environment\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 0.1\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_resource_variables()\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    action_reward_fns = (\n        environment_utilities.sliding_linear_reward_fn_generator(\n            CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n\n    env = sspe.StationaryStochasticPyEnvironment(\n        functools.partial(\n            environment_utilities.context_sampling_fn,\n            batch_size=BATCH_SIZE,\n            context_dim=CONTEXT_DIM),\n        action_reward_fns,\n        batch_size=BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_reward,\n        per_action_reward_fns=action_reward_fns)\n\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_action,\n        per_action_reward_fns=action_reward_fns)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v1/train_eval_wheel.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under the wheel bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\nfrom tf_agents.bandits.agents.examples.v1 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import wheel_py_environment\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\', \'epsGreedy\'],\n    \'Which agent to use. Possible values: `LinUCB`, `LinTS`, `epsGreedy`.\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nTRAINING_LOOPS = 20000\nSTEPS_PER_LOOP = 2\n\nDELTA = 0.5\nMU_BASE = [5.2, 1.0, 1.1, 0.9, 1.2]\nSTD_BASE = [0.01] * 5\nMU_HIGH = 50.0\nSTD_HIGH = 0.01\n\n# LinUCB agent constants.\n\nAGENT_ALPHA = 10.0\n\n# epsilon Greedy constants.\n\nEPSILON = 0.05\nLAYERS = (50, 50, 50)\nLR = 0.001\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_resource_variables()\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    env = wheel_py_environment.WheelPyEnvironment(DELTA, MU_BASE, STD_BASE,\n                                                  MU_HIGH, STD_HIGH, BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_wheel_bandit_compute_optimal_reward,\n        delta=DELTA,\n        mu_inside=MU_BASE[0],\n        mu_high=MU_HIGH)\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_wheel_bandit_compute_optimal_action,\n        delta=DELTA)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'epsGreedy\':\n      network = q_network.QNetwork(\n          input_tensor_spec=environment.time_step_spec().observation,\n          action_spec=environment.action_spec(),\n          fc_layer_params=LAYERS)\n      agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          epsilon=EPSILON)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v1/trainer.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Generic TF1 trainer for TF-Agents bandits.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nfrom absl import logging\n\nfrom gin.tf import utils\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\n\n\ndef build_replay_buffer(agent, batch_size, steps_per_loop):\n  """"""Return a `TFUniformReplayBuffer` for the given `agent`.""""""\n  buf = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n      data_spec=agent.policy.trajectory_spec,\n      batch_size=batch_size,\n      max_length=steps_per_loop)\n  return buf\n\n\ndef train(\n    root_dir,\n    agent,\n    environment,\n    training_loops,\n    steps_per_loop=1,\n    additional_metrics=(),\n    # Params for checkpoints, summaries, and logging\n    train_checkpoint_interval=10,\n    policy_checkpoint_interval=10,\n    log_interval=10,\n    summary_interval=10):\n  """"""A training driver.""""""\n\n  if not common.resource_variables_enabled():\n    raise RuntimeError(common.MISSING_RESOURCE_VARIABLES_ERROR)\n\n  root_dir = os.path.expanduser(root_dir)\n  train_dir = os.path.join(root_dir, \'train\')\n\n  train_summary_writer = tf.compat.v2.summary.create_file_writer(train_dir)\n  train_summary_writer.set_as_default()\n\n  global_step = tf.compat.v1.train.get_or_create_global_step()\n  with tf.compat.v2.summary.record_if(\n      lambda: tf.math.equal(global_step % summary_interval, 0)):\n\n    train_metrics = [\n        tf_metrics.NumberOfEpisodes(),\n        tf_metrics.EnvironmentSteps(),\n        tf_metrics.AverageReturnMetric(batch_size=environment.batch_size),\n        tf_metrics.AverageEpisodeLengthMetric(\n            batch_size=environment.batch_size),\n    ] + list(additional_metrics)\n\n    # Add to replay buffer and other agent specific observers.\n    replay_buffer = build_replay_buffer(agent, environment.batch_size,\n                                        steps_per_loop)\n    agent_observers = [replay_buffer.add_batch] + train_metrics\n\n    driver = dynamic_step_driver.DynamicStepDriver(\n        env=environment,\n        policy=agent.policy,\n        num_steps=steps_per_loop * environment.batch_size,\n        observers=agent_observers)\n\n    collect_op, _ = driver.run()\n    batch_size = driver.env.batch_size\n    dataset = replay_buffer.as_dataset(\n        sample_batch_size=batch_size,\n        num_steps=steps_per_loop,\n        single_deterministic_pass=True)\n    trajectories, unused_info = tf.data.experimental.get_single_element(dataset)\n    train_op = agent.train(experience=trajectories)\n    clear_replay_op = replay_buffer.clear()\n\n    train_checkpointer = common.Checkpointer(\n        ckpt_dir=train_dir,\n        max_to_keep=1,\n        agent=agent,\n        global_step=global_step,\n        metrics=metric_utils.MetricsGroup(train_metrics, \'train_metrics\'))\n    policy_checkpointer = common.Checkpointer(\n        ckpt_dir=os.path.join(train_dir, \'policy\'),\n        max_to_keep=None,\n        policy=agent.policy,\n        global_step=global_step)\n\n    summary_ops = []\n    for train_metric in train_metrics:\n      summary_ops.append(\n          train_metric.tf_summaries(\n              train_step=global_step, step_metrics=train_metrics[:2]))\n\n    init_agent_op = agent.initialize()\n\n    config_saver = utils.GinConfigSaverHook(train_dir, summarize_config=True)\n    config_saver.begin()\n\n    with tf.compat.v1.Session() as sess:\n      # Initialize the graph.\n      train_checkpointer.initialize_or_restore(sess)\n      common.initialize_uninitialized_variables(sess)\n\n      config_saver.after_create_session(sess)\n\n      global_step_call = sess.make_callable(global_step)\n      global_step_val = global_step_call()\n\n      sess.run(train_summary_writer.init())\n      sess.run(collect_op)\n\n      if global_step_val == 0:\n        # Save an initial checkpoint so the evaluator runs for global_step=0.\n        policy_checkpointer.save(global_step=global_step_val)\n        sess.run(init_agent_op)\n\n      collect_call = sess.make_callable(collect_op)\n      train_step_call = sess.make_callable([train_op, summary_ops])\n      clear_replay_call = sess.make_callable(clear_replay_op)\n\n      timed_at_step = global_step_val\n      time_acc = 0\n      steps_per_second_ph = tf.compat.v1.placeholder(\n          tf.float32, shape=(), name=\'steps_per_sec_ph\')\n      steps_per_second_summary = tf.compat.v2.summary.scalar(\n          name=\'global_steps_per_sec\',\n          data=steps_per_second_ph,\n          step=global_step)\n\n      for _ in range(training_loops):\n        # Collect and train.\n        start_time = time.time()\n        collect_call()\n        total_loss, _ = train_step_call()\n        clear_replay_call()\n        global_step_val = global_step_call()\n\n        time_acc += time.time() - start_time\n\n        total_loss = total_loss.loss\n\n        if global_step_val % log_interval == 0:\n          logging.info(\'step = %d, loss = %f\', global_step_val, total_loss)\n          steps_per_sec = (global_step_val - timed_at_step) / time_acc\n          logging.info(\'%.3f steps/sec\', steps_per_sec)\n          sess.run(\n              steps_per_second_summary,\n              feed_dict={steps_per_second_ph: steps_per_sec})\n          timed_at_step = global_step_val\n          time_acc = 0\n\n        if global_step_val % train_checkpoint_interval == 0:\n          train_checkpointer.save(global_step=global_step_val)\n\n        if global_step_val % policy_checkpoint_interval == 0:\n          policy_checkpointer.save(global_step=global_step_val)\n'"
tf_agents/bandits/agents/examples/v1/trainer_test.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.examples.v1.trainer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\nimport unittest\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import exp3_agent\nfrom tf_agents.bandits.agents.examples.v1 import trainer\nfrom tf_agents.bandits.environments import random_bandit_environment\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import test_utils\n\nfrom tensorflow.python import tf2  # pylint: disable=g-direct-tensorflow-import  # TF internal\n\ntfd = tfp.distributions\n\n\ndef get_bounded_reward_random_environment(\n    observation_shape, action_shape, batch_size, num_actions):\n  """"""Returns a RandomBanditEnvironment with U(0, 1) observation and reward.""""""\n  overall_shape = [batch_size] + observation_shape\n  observation_distribution = tfd.Independent(\n      tfd.Uniform(low=tf.zeros(overall_shape), high=tf.ones(overall_shape)))\n  reward_distribution = tfd.Uniform(\n      low=tf.zeros(batch_size), high=tf.ones(batch_size))\n  action_spec = tensor_spec.BoundedTensorSpec(\n      shape=action_shape, dtype=tf.int32, minimum=0, maximum=num_actions - 1)\n  return random_bandit_environment.RandomBanditEnvironment(\n      observation_distribution,\n      reward_distribution,\n      action_spec)\n\n\nclass TrainerTF1Test(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_0\',\n           num_actions=11,\n           observation_shape=[8],\n           action_shape=[],\n           batch_size=32,\n           training_loops=10,\n           steps_per_loop=10,\n           learning_rate=.1)\n      )\n  @unittest.skipIf(tf2.enabled(), \'TF 1.x only test.\')\n  def testTrainerTF1ExportsCheckpoints(self,\n                                       num_actions,\n                                       observation_shape,\n                                       action_shape,\n                                       batch_size,\n                                       training_loops,\n                                       steps_per_loop,\n                                       learning_rate):\n    """"""Tests TF1 trainer code, checks that expected checkpoints are exported.""""""\n    root_dir = tempfile.mkdtemp(dir=os.getenv(\'TEST_TMPDIR\'))\n    environment = get_bounded_reward_random_environment(\n        observation_shape, action_shape, batch_size, num_actions)\n    agent = exp3_agent.Exp3Agent(learning_rate=learning_rate,\n                                 time_step_spec=environment.time_step_spec(),\n                                 action_spec=environment.action_spec())\n\n    trainer.train(root_dir, agent, environment, training_loops, steps_per_loop)\n    latest_checkpoint = tf.train.latest_checkpoint(\n        os.path.join(root_dir, \'train\'))\n    expected_checkpoint_regex = \'.*ckpt-{}\'.format(\n        training_loops * batch_size * steps_per_loop)\n    self.assertRegex(latest_checkpoint, expected_checkpoint_regex)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
tf_agents/bandits/agents/examples/v2/__init__.py,0,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'"
tf_agents/bandits/agents/examples/v2/train_eval_covertype.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against the \'covertype\' environment.\n\nForest Cover type dataset in the UCI Machine Learning Repository can be found at\nhttps://archive.ics.uci.edu/ml/datasets/covertype.\n\nWe turn this 7-class classification problem to a bandit problem with 7 actions.\nThe reward is 1 if the right class was chosen, 0 otherwise.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\nimport numpy as np\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import classification_environment as ce\nfrom tf_agents.bandits.environments import environment_utilities as env_util\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.networks import q_network\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'epsGreedy\', [\'LinUCB\', \'LinTS\', \'epsGreedy\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\nflags.DEFINE_string(\n    \'covertype_csv\', \'\',\n    \'Location of the csv file containing the covertype dataset.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nBATCH_SIZE = 8\nTRAINING_LOOPS = 15000\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 10.0\n\nEPSILON = 0.01\nLAYERS = (300, 200, 100, 100, 50, 50)\nLR = 0.002\n\n\ndef _convert_covertype_dataset(file_path, buffer_size=400000):\n  with tf.gfile.Open(file_path, \'r\') as infile:\n    data_array = np.genfromtxt(infile, dtype=np.int, delimiter=\',\')\n  contexts = data_array[:, :-1]\n  context_tensor = tf.cast(contexts, tf.float32)\n  labels = data_array[:, -1] - 1  # Classes are from [1, 7].\n  label_tensor = tf.cast(labels, tf.int32)\n  return tf.data.Dataset.from_tensor_slices(\n      (context_tensor, label_tensor)).repeat().shuffle(buffer_size=buffer_size)\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n\n    covertype_dataset = _convert_covertype_dataset(FLAGS.covertype_csv)\n    covertype_reward_distribution = tfd.Independent(\n        tfd.Deterministic(tf.eye(7)), reinterpreted_batch_ndims=2)\n    environment = ce.ClassificationBanditEnvironment(\n        covertype_dataset, covertype_reward_distribution, BATCH_SIZE)\n\n    optimal_reward_fn = functools.partial(\n        env_util.compute_optimal_reward_with_classification_environment,\n        environment=environment)\n\n    optimal_action_fn = functools.partial(\n        env_util.compute_optimal_action_with_classification_environment,\n        environment=environment)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'epsGreedy\':\n      network = q_network.QNetwork(\n          input_tensor_spec=environment.time_step_spec().observation,\n          action_spec=environment.action_spec(),\n          fc_layer_params=LAYERS)\n      agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          epsilon=EPSILON)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_dqn.py,3,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for the DQN agent on bandit environment with linear rewards.\n\nNote: The training samples are *not* drawn at random from the replay buffer.\nHence, this setup is not ideal for DQN.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\nfrom tf_agents.utils import common\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nTRAINING_LOOPS = 400\nSTEPS_PER_LOOP = 2  # DQN agent requires time dim=2.\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    action_reward_fns = (\n        environment_utilities.sliding_linear_reward_fn_generator(\n            CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n\n    env = sspe.StationaryStochasticPyEnvironment(\n        functools.partial(\n            environment_utilities.context_sampling_fn,\n            batch_size=BATCH_SIZE,\n            context_dim=CONTEXT_DIM),\n        action_reward_fns,\n        batch_size=BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_reward,\n        per_action_reward_fns=action_reward_fns)\n\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_action,\n        per_action_reward_fns=action_reward_fns)\n\n    q_net = q_network.QNetwork(\n        environment.observation_spec(),\n        environment.action_spec(),\n        fc_layer_params=(50, 50))\n\n    agent = dqn_agent.DqnAgent(\n        environment.time_step_spec(),\n        environment.action_spec(),\n        q_network=q_net,\n        epsilon_greedy=0.1,\n        target_update_tau=0.05,\n        target_update_period=5,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2),\n        td_errors_loss_fn=common.element_wise_squared_loss)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_drifting_linear.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against a drifting linear environment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import drifting_linear_environment as dle\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nse\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nDRIFT_VARIANCE = 0.01\nDRIFT_MEAN = 0.01\nBATCH_SIZE = 8\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 10.0\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    observation_shape = [CONTEXT_DIM]\n    overall_shape = [BATCH_SIZE] + observation_shape\n    observation_distribution = tfd.Normal(\n        loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n    action_shape = [NUM_ACTIONS]\n    observation_to_reward_shape = observation_shape + action_shape\n    observation_to_reward_distribution = tfd.Normal(\n        loc=tf.zeros(observation_to_reward_shape),\n        scale=tf.ones(observation_to_reward_shape))\n    drift_distribution = tfd.Normal(loc=DRIFT_MEAN, scale=DRIFT_VARIANCE)\n    additive_reward_distribution = tfd.Normal(\n        loc=tf.zeros(action_shape),\n        scale=(REWARD_NOISE_VARIANCE * tf.ones(action_shape)))\n    environment_dynamics = dle.DriftingLinearDynamics(\n        observation_distribution,\n        observation_to_reward_distribution,\n        drift_distribution,\n        additive_reward_distribution)\n    environment = nse.NonStationaryStochasticEnvironment(environment_dynamics)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(\n        environment.environment_dynamics.compute_optimal_reward)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        environment.environment_dynamics.compute_optimal_action)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_mushroom.py,4,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against a mushroom environment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import classification_environment as ce\nfrom tf_agents.bandits.environments import environment_utilities as env_util\nfrom tf_agents.bandits.environments import mushroom_environment_utilities\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\nflags.DEFINE_string(\n    \'mushroom_csv\', \'\',\n    \'Location of the csv file containing the mushroom dataset.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nBATCH_SIZE = 8\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 10.0\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n\n    mushroom_reward_distribution = (\n        mushroom_environment_utilities.mushroom_reward_distribution(\n            r_noeat=0.0, r_eat_safe=5.0, r_eat_poison_bad=-35.0,\n            r_eat_poison_good=5.0, prob_poison_bad=0.5))\n    mushroom_dataset = (\n        mushroom_environment_utilities.convert_mushroom_csv_to_tf_dataset(\n            FLAGS.mushroom_csv))\n    environment = ce.ClassificationBanditEnvironment(\n        mushroom_dataset, mushroom_reward_distribution, BATCH_SIZE)\n\n    optimal_reward_fn = functools.partial(\n        env_util.compute_optimal_reward_with_classification_environment,\n        environment=environment)\n\n    optimal_action_fn = functools.partial(\n        env_util.compute_optimal_action_with_classification_environment,\n        environment=environment)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_per_arm_stationary_linear.py,14,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under stationary linear environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.agents import neural_linucb_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import stationary_stochastic_per_arm_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.environments import tf_py_environment\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\', \'epsGreedy\', \'NeuralLinUCB\'],\n    \'Which agent to use. Possible values: `LinUCB`, `LinTS`, `epsGreedy`, and \'\n    \'`NeuralLinUCB`.\'\n)\n\nflags.DEFINE_enum(\n    \'network\', \'commontower\', [\'commontower\', \'dotproduct\'],\n    \'Which network architecture to use for the eps-Greedy agent. \'\n    \'Possible values are `commontower` and `dotproduct`.\')\n\nflags.DEFINE_bool(\'drop_arm_obs\', False, \'Whether to wipe the arm observations \'\n                  \'from the trajectories.\')\n\nflags.DEFINE_bool(\'add_num_actions_feature\', False,\n                  \'Whether to add a `num_actions` feature key.\')\n\nFLAGS = flags.FLAGS\n\n# Environment and driver parameters.\n\nBATCH_SIZE = 16\nNUM_ACTIONS = 7\nHIDDEN_PARAM = [0, 1, 2, 3, 4, 5, 6, 7, 8]\nTRAINING_LOOPS = 2000\nSTEPS_PER_LOOP = 2\n\n# Parameters for linear agents (LinUCB and LinTS).\n\nAGENT_ALPHA = 0.1\n\n# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n\nEPSILON = 0.01\nLR = 0.05\n\n# Parameters for NeuralLinUCB. ENCODING_DIM is the output dimension of the\n# encoding network. This output will be used by either a linear reward layer and\n# epsilon greedy exploration, or by a LinUCB logic, depending on the number of\n# training steps executed so far. If the number of steps is less than or equal\n# to EPS_PHASE_STEPS, epsilon greedy is used, otherwise LinUCB.\n\nENCODING_DIM = 9\nEPS_PHASE_STEPS = 1000\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  class LinearNormalReward(object):\n\n    def __init__(self, theta):\n      self.theta = theta\n\n    def __call__(self, x):\n      mu = np.dot(x, self.theta)\n      return np.random.normal(mu, 1)\n\n  def _global_context_sampling_fn():\n    return np.random.randint(-10, 10, [4]).astype(np.float32)\n\n  def _arm_context_sampling_fn():\n    return np.random.randint(-2, 3, [5]).astype(np.float32)\n\n  reward_fn = LinearNormalReward(HIDDEN_PARAM)\n\n  observation_and_action_constraint_splitter = None\n  num_actions_fn = None\n  if FLAGS.add_num_actions_feature:\n    num_actions_fn = lambda: NUM_ACTIONS\n\n  env = sspe.StationaryStochasticPerArmPyEnvironment(\n      _global_context_sampling_fn,\n      _arm_context_sampling_fn,\n      NUM_ACTIONS,\n      reward_fn,\n      num_actions_fn,\n      batch_size=BATCH_SIZE)\n  environment = tf_py_environment.TFPyEnvironment(env)\n\n  if FLAGS.agent == \'LinUCB\':\n    agent = lin_ucb_agent.LinearUCBAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        alpha=AGENT_ALPHA,\n        accepts_per_arm_features=True,\n        dtype=tf.float32)\n  elif FLAGS.agent == \'LinTS\':\n    agent = lin_ts_agent.LinearThompsonSamplingAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        alpha=AGENT_ALPHA,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=True,\n        dtype=tf.float32)\n  elif FLAGS.agent == \'epsGreedy\':\n    obs_spec = environment.observation_spec()\n    if FLAGS.network == \'commontower\':\n      network = (\n          global_and_arm_feature_network\n          .create_feed_forward_common_tower_network(obs_spec, (40, 30),\n                                                    (30, 40), (40, 20)))\n    elif FLAGS.network == \'dotproduct\':\n      network = (\n          global_and_arm_feature_network\n          .create_feed_forward_dot_product_network(obs_spec, (4, 3, 6),\n                                                   (3, 4, 6)))\n    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        reward_network=network,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n        epsilon=EPSILON,\n        observation_and_action_constraint_splitter=(\n            observation_and_action_constraint_splitter),\n        accepts_per_arm_features=True,\n        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN)\n  elif FLAGS.agent == \'NeuralLinUCB\':\n    obs_spec = environment.observation_spec()\n    network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (40, 30), (30, 40), (40, 20), ENCODING_DIM))\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        encoding_network=network,\n        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n        encoding_dim=ENCODING_DIM,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n        alpha=1.0,\n        gamma=1.0,\n        epsilon_greedy=EPSILON,\n        accepts_per_arm_features=True,\n        debug_summaries=True,\n        summarize_grads_and_vars=True,\n        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN)\n\n  def _all_rewards(observation, hidden_param):\n    """"""Outputs rewards for all actions, given an observation.""""""\n    hidden_param = tf.cast(hidden_param, dtype=tf.float32)\n    global_obs = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n    per_arm_obs = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n    num_actions = tf.shape(per_arm_obs)[1]\n    tiled_global = tf.tile(\n        tf.expand_dims(global_obs, axis=1), [1, num_actions, 1])\n    concatenated = tf.concat([tiled_global, per_arm_obs], axis=-1)\n    rewards = tf.linalg.matvec(concatenated, hidden_param)\n    return rewards\n\n  def optimal_reward(observation, hidden_param):\n    return tf.reduce_max(_all_rewards(observation, hidden_param), axis=1)\n\n  def optimal_action(observation, hidden_param):\n    return tf.argmax(\n        _all_rewards(observation, hidden_param), axis=1, output_type=tf.int32)\n\n  optimal_reward_fn = functools.partial(\n      optimal_reward, hidden_param=HIDDEN_PARAM)\n  optimal_action_fn = functools.partial(\n      optimal_action, hidden_param=HIDDEN_PARAM)\n  regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n  suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n      optimal_action_fn)\n\n  if FLAGS.drop_arm_obs:\n    drop_arm_feature_fn = functools.partial(\n        bandit_spec_utils.drop_arm_observation)\n  else:\n    drop_arm_feature_fn = None\n  trainer.train(\n      root_dir=FLAGS.root_dir,\n      agent=agent,\n      environment=environment,\n      training_loops=TRAINING_LOOPS,\n      steps_per_loop=STEPS_PER_LOOP,\n      additional_metrics=[regret_metric, suboptimal_arms_metric],\n      training_data_spec_transformation_fn=drop_arm_feature_fn)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_piecewise_linear.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandits against the piecewise linear environment.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import non_stationary_stochastic_environment as nse\nfrom tf_agents.bandits.environments import piecewise_stochastic_environment as pse\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\ntfd = tfp.distributions\n\n\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nBATCH_SIZE = 8\nTRAINING_LOOPS = 200\nSTEPS_PER_LOOP = 2\n# Total number of steps: BATCH_SIZE * STEPS_PER_LOOP * TRAINING_LOOPS\n# Set the stationarity length to be half of the total number of steps.\n# This gives us two pieces (each one is stationary inside).\nSTATIONARITY_LENGTH = 1600\nAGENT_ALPHA = 10.0\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    observation_shape = [CONTEXT_DIM]\n    overall_shape = [BATCH_SIZE] + observation_shape\n    observation_distribution = tfd.Normal(\n        loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n    interval_distribution = tfd.Deterministic(STATIONARITY_LENGTH)\n    action_shape = [NUM_ACTIONS]\n    observation_to_reward_shape = observation_shape + action_shape\n    observation_to_reward_distribution = tfd.Normal(\n        loc=tf.zeros(observation_to_reward_shape),\n        scale=tf.ones(observation_to_reward_shape))\n    additive_reward_distribution = tfd.Normal(\n        loc=tf.zeros(action_shape),\n        scale=(REWARD_NOISE_VARIANCE * tf.ones(action_shape)))\n    environment_dynamics = pse.PiecewiseStationaryDynamics(\n        observation_distribution,\n        interval_distribution,\n        observation_to_reward_distribution,\n        additive_reward_distribution)\n    environment = nse.NonStationaryStochasticEnvironment(environment_dynamics)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          emit_log_probability=False,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          gamma=0.95,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(\n        environment.environment_dynamics.compute_optimal_reward)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        environment.environment_dynamics.compute_optimal_action)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_sparse_features.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under sparse feature environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.agents import neural_linucb_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import stationary_stochastic_structured_py_environment as sspe\nfrom tf_agents.bandits.networks import global_and_arm_feature_network\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.bandits.specs import utils as bandit_spec_utils\nfrom tf_agents.environments import tf_py_environment\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\n\nflags.DEFINE_enum(\n    \'agent\', \'epsGreedy\', [\'epsGreedy\', \'NeuralLinUCB\'],\n    \'Which agent to use. Possible values: `epsGreedy` and `NeuralLinUCB`.\'\n)\n\nflags.DEFINE_bool(\'drop_arm_obs\', False, \'Whether to wipe the arm observations \'\n                  \'from the trajectories.\')\n\nFLAGS = flags.FLAGS\n\n# Environment parameters.\n\nDICTIONARY_SIZE = 100\nNUM_GLOBAL_FEATURES = 10\nNUM_ARM_FEATURES = 11\nNUM_ACTIONS = 7\n\n# Driver parameters.\n\nBATCH_SIZE = 16\nTRAINING_LOOPS = 2000\nSTEPS_PER_LOOP = 2\n\n# Parameters for neural agents (NeuralEpsGreedy and NerualLinUCB).\n\nEPSILON = 0.01\nLR = 0.05\n\n# Parameters for NeuralLinUCB. ENCODING_DIM is the output dimension of the\n# encoding network. This output will be used by either a linear reward layer and\n# epsilon greedy exploration, or by a LinUCB logic, depending on the number of\n# training steps executed so far. If the number of steps is less than or equal\n# to EPS_PHASE_STEPS, epsilon greedy is used, otherwise LinUCB.\n\nENCODING_DIM = 9\nEPS_PHASE_STEPS = 1000\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  feature_dict = np.array([str(i) for i in range(DICTIONARY_SIZE)])\n  def _global_context_sampling_fn():\n    """"""Generates one sample of global features.\n\n    It generates a dictionary of size `NUM_GLOBAL_FEATURES`, with the following\n    syntax:\n\n    {...,\n     \'global_feature_4\': [\'43\'],\n     ...\n    }\n\n    That is, the values are one-element numpy arrays of strings.\n\n    Returns:\n      A dictionary with string keys and numpy string array values.\n    """"""\n    generated_features = feature_dict[np.random.randint(0, DICTIONARY_SIZE,\n                                                        [NUM_GLOBAL_FEATURES])]\n    global_features = {\n        \'global_feature_{}\'.format(i): generated_features[[i]]\n        for i in range(NUM_GLOBAL_FEATURES)\n    }\n    return global_features\n\n  def _arm_context_sampling_fn():\n    """"""Generates one sample of arm features.\n\n    It generates a dictionary of size `NUM_ARM_FEATURES`, with the following\n    syntax:\n\n    {...,\n     \'arm_feature_7\': [\'29\'],\n     ...\n    }\n\n    That is, the values are one-element numpy arrays of strings. Note that the\n    output sample is for one arm and one non-batched time step.\n\n    Returns:\n      A dictionary with string keys and numpy string array values.\n    """"""\n    generated_features = feature_dict[np.random.randint(\n        0, DICTIONARY_SIZE, [NUM_ARM_FEATURES])]\n    arm_features = {\n        \'arm_feature_{}\'.format(i): generated_features[[i]]\n        for i in range(NUM_ARM_FEATURES)\n    }\n    return arm_features\n\n  def _reward_fn(global_features, arm_features):\n    """"""Outputs a [0, 1] float given a sample.\n\n    The output reward is generated by hashing the concatenation of feature keys\n    and values, then adding all up, taking modulo by 1000, and normalizing.\n\n    Args:\n      global_features: A dictionary with string keys and 1d string numpy array\n        values.\n      arm_features: A dictionary with string keys and 1d string numpy array\n        values.\n\n    Returns:\n      A float value between 0 and 1.\n    """"""\n    hashed_global = 0\n    for x, y in global_features.items():\n      hashed_global += hash(x + y[0])\n    hashed_arm = 0\n    for x, y in arm_features.items():\n      hashed_arm += hash(x + y[0])\n    return (hashed_global + hashed_arm) % 1000 / 1000\n\n  env = sspe.StationaryStochasticStructuredPyEnvironment(\n      _global_context_sampling_fn,\n      _arm_context_sampling_fn,\n      NUM_ACTIONS,\n      _reward_fn,\n      batch_size=BATCH_SIZE)\n  environment = tf_py_environment.TFPyEnvironment(env)\n\n  def make_string_feature(name):\n    return tf.feature_column.indicator_column(\n        tf.feature_column.categorical_column_with_vocabulary_list(\n            name, feature_dict))\n\n  global_columns = [\n      make_string_feature(\'global_feature_{}\'.format(i))\n      for i in range(NUM_GLOBAL_FEATURES)\n  ]\n  arm_columns = [\n      make_string_feature(\'arm_feature_{}\'.format(i))\n      for i in range(NUM_ARM_FEATURES)\n  ]\n  obs_spec = environment.observation_spec()\n  if FLAGS.agent == \'epsGredy\':\n    network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (4, 3), (3, 4), (4, 2),\n            global_preprocessing_combiner=tf.compat.v2.keras.layers\n            .DenseFeatures(global_columns),\n            arm_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n                arm_columns)))\n    agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        reward_network=network,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n        epsilon=EPSILON,\n        accepts_per_arm_features=True,\n        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN)\n  elif FLAGS.agent == \'NeuralLinUCB\':\n    network = (\n        global_and_arm_feature_network.create_feed_forward_common_tower_network(\n            obs_spec, (40, 30), (30, 40), (40, 20),\n            ENCODING_DIM,\n            global_preprocessing_combiner=tf.compat.v2.keras.layers\n            .DenseFeatures(global_columns),\n            arm_preprocessing_combiner=tf.compat.v2.keras.layers.DenseFeatures(\n                arm_columns)))\n    agent = neural_linucb_agent.NeuralLinUCBAgent(\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec(),\n        encoding_network=network,\n        encoding_network_num_train_steps=EPS_PHASE_STEPS,\n        encoding_dim=ENCODING_DIM,\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n        alpha=1.0,\n        gamma=1.0,\n        epsilon_greedy=EPSILON,\n        accepts_per_arm_features=True,\n        debug_summaries=True,\n        summarize_grads_and_vars=True,\n        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN)\n\n  if FLAGS.drop_arm_obs:\n    drop_arm_feature_fn = bandit_spec_utils.drop_arm_observation\n  else:\n    drop_arm_feature_fn = None\n  trainer.train(\n      root_dir=FLAGS.root_dir,\n      agent=agent,\n      environment=environment,\n      training_loops=TRAINING_LOOPS,\n      steps_per_loop=STEPS_PER_LOOP,\n      training_data_spec_transformation_fn=drop_arm_feature_fn)\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_stationary_linear.py,8,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under stationary linear environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import exp3_mixture_agent\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\', \'epsGreedy\', \'Mix\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`, `epsGreedy`,\'\n    \' and `Mix`.\'\n)\nflags.DEFINE_bool(\'normalize_reward_fns\', False, \'Whether to normalize the \'\n                  \'reward functions so that rewards are close to being in \'\n                  \'[0, 1].\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nCONTEXT_DIM = 15\nNUM_ACTIONS = 5\nREWARD_NOISE_VARIANCE = 0.01\nTRAINING_LOOPS = 2000\nSTEPS_PER_LOOP = 2\nAGENT_ALPHA = 0.1\n\nEPSILON = 0.05\nLAYERS = (50, 50, 50)\nLR = 0.001\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    if FLAGS.normalize_reward_fns:\n      action_reward_fns = (\n          environment_utilities.normalized_sliding_linear_reward_fn_generator(\n              CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n    else:\n      action_reward_fns = (\n          environment_utilities.sliding_linear_reward_fn_generator(\n              CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n\n    env = sspe.StationaryStochasticPyEnvironment(\n        functools.partial(\n            environment_utilities.context_sampling_fn,\n            batch_size=BATCH_SIZE,\n            context_dim=CONTEXT_DIM),\n        action_reward_fns,\n        batch_size=BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_reward,\n        per_action_reward_fns=action_reward_fns)\n\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_action,\n        per_action_reward_fns=action_reward_fns)\n\n    network = q_network.QNetwork(\n        input_tensor_spec=environment.time_step_spec().observation,\n        action_spec=environment.action_spec(),\n        fc_layer_params=LAYERS)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'epsGreedy\':\n      agent = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          epsilon=EPSILON)\n    elif FLAGS.agent == \'Mix\':\n      emit_policy_info = policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN\n      agent_linucb = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          emit_policy_info=emit_policy_info,\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n      agent_lints = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          emit_policy_info=emit_policy_info,\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n      agent_epsgreedy = neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          emit_policy_info=emit_policy_info,\n          epsilon=EPSILON)\n      agent = exp3_mixture_agent.Exp3MixtureAgent(\n          (agent_linucb, agent_lints, agent_epsgreedy))\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_structured_linear.py,5,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under structured linear environments.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\nfrom tf_agents.bandits.agents import utils\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\', \'epsGreedy\'],\n    \'Which agent to use. Possible values are `LinUCB` and `LinTS`.\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nCONTEXT_DIM = 100\nNUM_ACTIONS = 100\nREWARD_NOISE_VARIANCE = 0.0001\nTRAINING_LOOPS = 2000\nSTEPS_PER_LOOP = 4\nAGENT_ALPHA = 0.1\nNN_LEARNING_RATE = 1e-2\nREWARD_NETWORK_LAYER_PARAMS = (50, 50, 50)\nEPSILON = 0.1\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    action_reward_fns = (\n        environment_utilities.structured_linear_reward_fn_generator(\n            CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n\n    env = sspe.StationaryStochasticPyEnvironment(\n        functools.partial(\n            environment_utilities.context_sampling_fn,\n            batch_size=BATCH_SIZE,\n            context_dim=CONTEXT_DIM),\n        action_reward_fns,\n        batch_size=BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_reward,\n        per_action_reward_fns=action_reward_fns)\n\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_compute_optimal_action,\n        per_action_reward_fns=action_reward_fns)\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'epsGreedy\':\n      laplacian_matrix = utils.build_laplacian_over_ordinal_integer_actions(\n          environment.action_spec())\n\n      network = q_network.QNetwork(\n          input_tensor_spec=environment.time_step_spec().observation,\n          action_spec=environment.action_spec(),\n          fc_layer_params=REWARD_NETWORK_LAYER_PARAMS)\n      agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(\n              learning_rate=NN_LEARNING_RATE),\n          epsilon=EPSILON,\n          laplacian_matrix=laplacian_matrix,\n          laplacian_smoothing_weight=0.01)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/train_eval_wheel.py,10,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""End-to-end test for bandit training under the wheel bandit environment.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.bandits.agents import exp3_mixture_agent\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\nfrom tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import environment_utilities\nfrom tf_agents.bandits.environments import wheel_py_environment\nfrom tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\nfrom tf_agents.bandits.policies import policy_utilities\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.networks import q_network\n\n\nflags.DEFINE_string(\'root_dir\', os.getenv(\'TEST_UNDECLARED_OUTPUTS_DIR\'),\n                    \'Root directory for writing logs/summaries/checkpoints.\')\nflags.DEFINE_enum(\n    \'agent\', \'LinUCB\', [\'LinUCB\', \'LinTS\', \'epsGreedy\', \'random\', \'Mix\'],\n    \'Which agent to use. Possible values: `LinUCB`, `LinTS`, `epsGreedy`, \'\n    \'`random`, `Mix`.\')\n\nFLAGS = flags.FLAGS\n\nBATCH_SIZE = 8\nTRAINING_LOOPS = 20000\nSTEPS_PER_LOOP = 2\n\nDELTA = 0.5\nMU_BASE = [0.05, 0.01, 0.011, 0.009, 0.012]\nSTD_BASE = [0.001] * 5\nMU_HIGH = 0.5\nSTD_HIGH = 0.001\n\n\n# LinUCB agent constants.\n\nAGENT_ALPHA = 10.0\n\n# epsilon Greedy constants.\n\nEPSILON = 0.05\nLAYERS = (50, 50, 50)\nLR = 0.001\n\n\ndef main(unused_argv):\n  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n\n  with tf.device(\'/CPU:0\'):  # due to b/128333994\n    env = wheel_py_environment.WheelPyEnvironment(DELTA, MU_BASE, STD_BASE,\n                                                  MU_HIGH, STD_HIGH, BATCH_SIZE)\n    environment = tf_py_environment.TFPyEnvironment(env)\n\n    optimal_reward_fn = functools.partial(\n        environment_utilities.tf_wheel_bandit_compute_optimal_reward,\n        delta=DELTA,\n        mu_inside=MU_BASE[0],\n        mu_high=MU_HIGH)\n    optimal_action_fn = functools.partial(\n        environment_utilities.tf_wheel_bandit_compute_optimal_action,\n        delta=DELTA)\n    network = q_network.QNetwork(\n        input_tensor_spec=environment.time_step_spec().observation,\n        action_spec=environment.action_spec(),\n        fc_layer_params=(LAYERS))\n\n    if FLAGS.agent == \'LinUCB\':\n      agent = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'LinTS\':\n      agent = lin_ts_agent.LinearThompsonSamplingAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          dtype=tf.float32)\n    elif FLAGS.agent == \'epsGreedy\':\n      agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          epsilon=EPSILON)\n    elif FLAGS.agent == \'random\':\n      agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          epsilon=1.)\n    elif FLAGS.agent == \'Mix\':\n      emit_policy_info = (policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN,)\n      agent_epsgreedy = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          emit_policy_info=emit_policy_info,\n          epsilon=EPSILON)\n      agent_linucb = lin_ucb_agent.LinearUCBAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          alpha=AGENT_ALPHA,\n          emit_policy_info=emit_policy_info,\n          dtype=tf.float32)\n      agent_random = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          emit_policy_info=emit_policy_info,\n          epsilon=1.)\n      agent_halfrandom = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n          time_step_spec=environment.time_step_spec(),\n          action_spec=environment.action_spec(),\n          reward_network=network,\n          optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n          emit_policy_info=emit_policy_info,\n          epsilon=0.5)\n      agent = exp3_mixture_agent.Exp3MixtureAgent(\n          (agent_epsgreedy, agent_linucb, agent_random, agent_halfrandom))\n\n    regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n    suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n        optimal_action_fn)\n\n    trainer.train(\n        root_dir=FLAGS.root_dir,\n        agent=agent,\n        environment=environment,\n        training_loops=TRAINING_LOOPS,\n        steps_per_loop=STEPS_PER_LOOP,\n        additional_metrics=[regret_metric, suboptimal_arms_metric])\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tf_agents/bandits/agents/examples/v2/trainer.py,9,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr""""""Generic TF-Agents training function for bandits.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import logging\n\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\n\ntf = tf.compat.v2\n\nAGENT_CHECKPOINT_NAME = \'agent\'\nSTEP_CHECKPOINT_NAME = \'step\'\nCHECKPOINT_FILE_PREFIX = \'ckpt\'\n\n\ndef get_replay_buffer(data_spec,\n                      batch_size,\n                      steps_per_loop):\n  """"""Return a `TFUniformReplayBuffer` for the given `agent`.""""""\n  buf = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n      data_spec=data_spec,\n      batch_size=batch_size,\n      max_length=steps_per_loop)\n  return buf\n\n\ndef set_expected_shape(experience, num_steps):\n  def set_time_dim(input_tensor, steps):\n    tensor_shape = input_tensor.shape.as_list()\n    tensor_shape[1] = steps\n    input_tensor.set_shape(tensor_shape)\n  tf.nest.map_structure(lambda t: set_time_dim(t, num_steps), experience)\n\n\ndef get_training_loop_fn(driver, replay_buffer, agent, steps):\n  """"""Returns a `tf.function` that runs the driver and training loops.\n\n  Args:\n    driver: an instance of `Driver`.\n    replay_buffer: an instance of `ReplayBuffer`.\n    agent: an instance of `TFAgent`.\n    steps: an integer indicating how many driver steps should be\n      executed and presented to the trainer during each training loop.\n  """"""\n  def training_loop():\n    """"""Returns a `tf.function` that runs the training loop.""""""\n    driver.run()\n    batch_size = driver.env.batch_size\n    dataset = replay_buffer.as_dataset(\n        sample_batch_size=batch_size,\n        num_steps=steps,\n        single_deterministic_pass=True)\n    experience, unused_info = tf.data.experimental.get_single_element(dataset)\n    set_expected_shape(experience, steps)\n    loss_info = agent.train(experience)\n    replay_buffer.clear()\n    return loss_info\n  return training_loop\n\n\ndef restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n  """"""Restores from `root_dir` and returns a function that writes checkpoints.""""""\n  trackable_objects = {metric.name: metric for metric in metrics}\n  trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n  trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n  checkpoint = tf.train.Checkpoint(**trackable_objects)\n  checkpoint_manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n                                                  directory=root_dir,\n                                                  max_to_keep=5)\n  latest = checkpoint_manager.latest_checkpoint\n  if latest is not None:\n    logging.info(\'Restoring checkpoint from %s.\', latest)\n    checkpoint.restore(latest)\n    logging.info(\'Successfully restored to step %s.\', step_metric.result())\n  else:\n    logging.info(\'Did not find a pre-existing checkpoint. \'\n                 \'Starting from scratch.\')\n  return checkpoint_manager\n\n\ndef train(root_dir,\n          agent,\n          environment,\n          training_loops,\n          steps_per_loop,\n          additional_metrics=(),\n          training_data_spec_transformation_fn=None):\n  """"""Perform `training_loops` iterations of training.\n\n  Checkpoint results.\n\n  If one or more baseline_reward_fns are provided, the regret is computed\n  against each one of them. Here is example baseline_reward_fn:\n\n  def baseline_reward_fn(observation, per_action_reward_fns):\n   rewards = ... # compute reward for each arm\n   optimal_action_reward = ... # take the maximum reward\n   return optimal_action_reward\n\n  Args:\n    root_dir: path to the directory where checkpoints and metrics will be\n      written.\n    agent: an instance of `TFAgent`.\n    environment: an instance of `TFEnvironment`.\n    training_loops: an integer indicating how many training loops should be run.\n    steps_per_loop: an integer indicating how many driver steps should be\n      executed and presented to the trainer during each training loop.\n    additional_metrics: Tuple of metric objects to log, in addition to default\n      metrics `NumberOfEpisodes`, `AverageReturnMetric`, and\n      `AverageEpisodeLengthMetric`.\n    training_data_spec_transformation_fn: Optional function that transforms the\n    data items before they get to the replay buffer.\n  """"""\n\n  # TODO(b/127641485): create evaluation loop with configurable metrics.\n  if training_data_spec_transformation_fn is None:\n    data_spec = agent.policy.trajectory_spec\n  else:\n    data_spec = training_data_spec_transformation_fn(\n        agent.policy.trajectory_spec)\n  replay_buffer = get_replay_buffer(data_spec, environment.batch_size,\n                                    steps_per_loop)\n\n  # `step_metric` records the number of individual rounds of bandit interaction;\n  # that is, (number of trajectories) * batch_size.\n  step_metric = tf_metrics.EnvironmentSteps()\n  metrics = [\n      tf_metrics.NumberOfEpisodes(),\n      tf_metrics.AverageReturnMetric(batch_size=environment.batch_size),\n      tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size)\n  ] + list(additional_metrics)\n\n  if training_data_spec_transformation_fn is not None:\n    add_batch_fn = lambda data: replay_buffer.add_batch(  # pylint: disable=g-long-lambda\n        training_data_spec_transformation_fn(data))\n  else:\n    add_batch_fn = replay_buffer.add_batch\n\n  observers = [add_batch_fn, step_metric] + metrics\n\n  driver = dynamic_step_driver.DynamicStepDriver(\n      env=environment,\n      policy=agent.collect_policy,\n      num_steps=steps_per_loop * environment.batch_size,\n      observers=observers)\n\n  training_loop = get_training_loop_fn(\n      driver, replay_buffer, agent, steps_per_loop)\n  checkpoint_manager = restore_and_get_checkpoint_manager(\n      root_dir, agent, metrics, step_metric)\n  saver = policy_saver.PolicySaver(agent.policy)\n\n  summary_writer = tf.summary.create_file_writer(root_dir)\n  summary_writer.set_as_default()\n  for _ in range(training_loops):\n    training_loop()\n    metric_utils.log_metrics(metrics)\n    for metric in metrics:\n      tf.summary.scalar(\n          metric.name, metric.result(), step=step_metric.result())\n    checkpoint_manager.save()\n    saver.save(os.path.join(root_dir, \'policy_%d\' % step_metric.result()))\n'"
tf_agents/bandits/agents/examples/v2/trainer_test.py,7,"b'# coding=utf-8\n# Copyright 2018 The TF-Agents Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests for tf_agents.bandits.agents.examples.v2.trainer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tempfile\n\nfrom absl.testing import parameterized\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nimport tensorflow_probability as tfp\nfrom tf_agents.bandits.agents import exp3_agent\nfrom tf_agents.bandits.agents.examples.v2 import trainer\nfrom tf_agents.bandits.environments import random_bandit_environment\nfrom tf_agents.specs import tensor_spec\n\ntfd = tfp.distributions\n\ntf.compat.v1.enable_v2_behavior()\n\n\ndef get_bounded_reward_random_environment(\n    observation_shape, action_shape, batch_size, num_actions):\n  """"""Returns a RandomBanditEnvironment with U(0, 1) observation and reward.""""""\n  overall_shape = [batch_size] + observation_shape\n  observation_distribution = tfd.Independent(\n      tfd.Uniform(low=tf.zeros(overall_shape), high=tf.ones(overall_shape)))\n  reward_distribution = tfd.Uniform(\n      low=tf.zeros(batch_size), high=tf.ones(batch_size))\n  action_spec = tensor_spec.BoundedTensorSpec(\n      shape=action_shape, dtype=tf.int32, minimum=0, maximum=num_actions - 1)\n  return random_bandit_environment.RandomBanditEnvironment(\n      observation_distribution, reward_distribution, action_spec)\n\n\nclass TrainerTest(tf.test.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\'_0\',\n           num_actions=11,\n           observation_shape=[8],\n           action_shape=[],\n           batch_size=32,\n           training_loops=10,\n           steps_per_loop=10,\n           learning_rate=.1),\n      dict(testcase_name=\'_1\',\n           num_actions=73,\n           observation_shape=[5, 4, 3, 2],\n           action_shape=[],\n           batch_size=121,\n           training_loops=7,\n           steps_per_loop=8,\n           learning_rate=.5),\n      )\n  def testTrainerExportsCheckpoints(self,\n                                    num_actions,\n                                    observation_shape,\n                                    action_shape,\n                                    batch_size,\n                                    training_loops,\n                                    steps_per_loop,\n                                    learning_rate):\n    """"""Exercises trainer code, checks that expected checkpoints are exported.""""""\n    root_dir = tempfile.mkdtemp(dir=os.getenv(\'TEST_TMPDIR\'))\n    environment = get_bounded_reward_random_environment(\n        observation_shape, action_shape, batch_size, num_actions)\n    agent = exp3_agent.Exp3Agent(\n        learning_rate=learning_rate,\n        time_step_spec=environment.time_step_spec(),\n        action_spec=environment.action_spec())\n    for i in range(1, 4):\n      trainer.train(\n          root_dir=root_dir,\n          agent=agent,\n          environment=environment,\n          training_loops=training_loops,\n          steps_per_loop=steps_per_loop)\n      latest_checkpoint = tf.train.latest_checkpoint(root_dir)\n      expected_checkpoint_regex = \'.*-{}\'.format(i * training_loops)\n      self.assertRegex(latest_checkpoint, expected_checkpoint_regex)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
