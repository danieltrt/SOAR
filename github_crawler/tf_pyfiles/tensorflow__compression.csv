file_path,api_count,code
build_pip_pkg.py,0,"b'# Lint as: python3\n# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Setup for pip package.""""""\n\nimport atexit\nimport glob\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nimport setuptools\n\n# Version string is intentionally set to non-numeric value, so that non-release\n# built packages are different from release packages. During builds for formal\n# releases, we should temporarily change this value to pip release version.\n__version__ = \'custom-build-from-source\'\n\n\nclass BinaryDistribution(setuptools.Distribution):\n  """"""This class is needed in order to create OS specific wheels.""""""\n\n  def has_ext_modules(self):\n    return True\n\n\ndef main(srcdir):\n  tempdir = tempfile.mkdtemp()\n  atexit.register(shutil.rmtree, tempdir)\n\n  pkgdir = os.path.join(tempdir, \'tensorflow_compression\')\n  shutil.copytree(os.path.join(srcdir, \'tensorflow_compression\'), pkgdir)\n  shutil.copy2(os.path.join(srcdir, \'MANIFEST.in\'), tempdir)\n  shutil.copy2(os.path.join(srcdir, \'LICENSE\'), pkgdir)\n  shutil.copy2(os.path.join(srcdir, \'README.md\'), pkgdir)\n\n  if not os.path.exists(\n      os.path.join(pkgdir, \'cc/libtensorflow_compression.so\')):\n    raise RuntimeError(\'libtensorflow_compression.so not found. \'\n                       \'Did you \\\'bazel run?\\\'\')\n\n  print(\'=== Building wheel\')\n  atexit.register(os.chdir, os.getcwd())\n  os.chdir(tempdir)\n  setuptools.setup(\n      name=\'tensorflow-compression\',\n      version=__version__,\n      description=\'Data compression in TensorFlow\',\n      url=\'https://tensorflow.github.io/compression/\',\n      author=\'Google LLC\',\n      # Contained modules and scripts.\n      packages=setuptools.find_packages(),\n      install_requires=[\n          \'scipy >= 1\',\n      ],\n      script_args=[\'sdist\', \'bdist_wheel\'],\n      # Add in any packaged data.\n      include_package_data=True,\n      zip_safe=False,\n      distclass=BinaryDistribution,\n      # PyPI package information.\n      classifiers=[\n          \'Development Status :: 5 - Production/Stable\',\n          \'Intended Audience :: Developers\',\n          \'Intended Audience :: Education\',\n          \'Intended Audience :: Science/Research\',\n          \'License :: OSI Approved :: Apache Software License\',\n          \'Programming Language :: Python :: 3.3\',\n          \'Programming Language :: Python :: 3.4\',\n          \'Programming Language :: Python :: 3.5\',\n          \'Programming Language :: Python :: 3.6\',\n          \'Programming Language :: Python :: 3.7\',\n          \'Topic :: Scientific/Engineering :: Mathematics\',\n          \'Topic :: Software Development :: Libraries :: Python Modules\',\n          \'Topic :: Software Development :: Libraries\',\n      ],\n      project_urls={\n          \'Documentation\':\n              \'https://tensorflow.github.io/compression/docs/api_docs/python/tfc.html\',\n          \'Discussion\':\n              \'https://groups.google.com/forum/#!forum/tensorflow-compression\',\n          \'Source\': \'https://github.com/tensorflow/compression\',\n          \'Tracker\': \'https://github.com/tensorflow/compression/issues\',\n      },\n      license=\'Apache 2.0\',\n      keywords=(\'compression data-compression tensorflow machine-learning \'\n                \'python deep-learning deep-neural-networks neural-network ml\')\n  )\n\n  destdir = \'/tmp/tensorflow_compression\'\n  print(\'=== Copying wheel to \' + destdir)\n  if not os.path.exists(destdir): os.mkdir(destdir)\n  for path in glob.glob(os.path.join(tempdir, \'dist\', \'*.whl\')):\n    print(\'Copying into \' + os.path.join(destdir, os.path.basename(path)))\n    shutil.copy(path, destdir)\n\n\nif __name__ == \'__main__\':\n  main(sys.argv[1] if len(sys.argv) > 1 else \'\')\n'"
examples/bls2017.py,48,"b'# Lint as: python3\n# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Basic nonlinear transform coder for RGB images.\n\nThis is a close approximation of the image compression model published in:\nJ. Ball\xc3\xa9, V. Laparra, E.P. Simoncelli (2017):\n""End-to-end Optimized Image Compression""\nInt. Conf. on Learning Representations (ICLR), 2017\nhttps://arxiv.org/abs/1611.01704\n\nWith patches from Victor Xing <victor.t.xing@gmail.com>\n\nThis is meant as \'educational\' code - you can use this to get started with your\nown experiments. To reproduce the exact results from the paper, tuning of hyper-\nparameters may be necessary. To compress images with published models, see\n`tfci.py`.\n""""""\n\nimport argparse\nimport glob\nimport sys\n\nfrom absl import app\nfrom absl.flags import argparse_flags\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport tensorflow_compression as tfc\n\n\ndef read_png(filename):\n  """"""Loads a PNG image file.""""""\n  string = tf.read_file(filename)\n  image = tf.image.decode_image(string, channels=3)\n  image = tf.cast(image, tf.float32)\n  image /= 255\n  return image\n\n\ndef quantize_image(image):\n  image = tf.round(image * 255)\n  image = tf.saturate_cast(image, tf.uint8)\n  return image\n\n\ndef write_png(filename, image):\n  """"""Saves an image to a PNG file.""""""\n  image = quantize_image(image)\n  string = tf.image.encode_png(image)\n  return tf.write_file(filename, string)\n\n\nclass AnalysisTransform(tf.keras.layers.Layer):\n  """"""The analysis transform.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(AnalysisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (9, 9), name=""layer_0"", corr=True, strides_down=4,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""gdn_0"")),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""gdn_1"")),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_2"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=False,\n            activation=None),\n    ]\n    super(AnalysisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\nclass SynthesisTransform(tf.keras.layers.Layer):\n  """"""The synthesis transform.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(SynthesisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_0"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""igdn_0"", inverse=True)),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""igdn_1"", inverse=True)),\n        tfc.SignalConv2D(\n            3, (9, 9), name=""layer_2"", corr=False, strides_up=4,\n            padding=""same_zeros"", use_bias=True,\n            activation=None),\n    ]\n    super(SynthesisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\ndef train(args):\n  """"""Trains the model.""""""\n\n  if args.verbose:\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n  # Create input data pipeline.\n  with tf.device(""/cpu:0""):\n    train_files = glob.glob(args.train_glob)\n    if not train_files:\n      raise RuntimeError(\n          ""No training images found with glob \'{}\'."".format(args.train_glob))\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n    train_dataset = train_dataset.shuffle(buffer_size=len(train_files)).repeat()\n    train_dataset = train_dataset.map(\n        read_png, num_parallel_calls=args.preprocess_threads)\n    train_dataset = train_dataset.map(\n        lambda x: tf.random_crop(x, (args.patchsize, args.patchsize, 3)))\n    train_dataset = train_dataset.batch(args.batchsize)\n    train_dataset = train_dataset.prefetch(32)\n\n  num_pixels = args.batchsize * args.patchsize ** 2\n\n  # Get training patch from dataset.\n  x = train_dataset.make_one_shot_iterator().get_next()\n\n  # Instantiate model.\n  analysis_transform = AnalysisTransform(args.num_filters)\n  entropy_bottleneck = tfc.EntropyBottleneck()\n  synthesis_transform = SynthesisTransform(args.num_filters)\n\n  # Build autoencoder.\n  y = analysis_transform(x)\n  y_tilde, likelihoods = entropy_bottleneck(y, training=True)\n  x_tilde = synthesis_transform(y_tilde)\n\n  # Total number of bits divided by number of pixels.\n  train_bpp = tf.reduce_sum(tf.log(likelihoods)) / (-np.log(2) * num_pixels)\n\n  # Mean squared error across pixels.\n  train_mse = tf.reduce_mean(tf.squared_difference(x, x_tilde))\n  # Multiply by 255^2 to correct for rescaling.\n  train_mse *= 255 ** 2\n\n  # The rate-distortion cost.\n  train_loss = args.lmbda * train_mse + train_bpp\n\n  # Minimize loss and auxiliary loss, and execute update op.\n  step = tf.train.create_global_step()\n  main_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n  main_step = main_optimizer.minimize(train_loss, global_step=step)\n\n  aux_optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n  aux_step = aux_optimizer.minimize(entropy_bottleneck.losses[0])\n\n  train_op = tf.group(main_step, aux_step, entropy_bottleneck.updates[0])\n\n  tf.summary.scalar(""loss"", train_loss)\n  tf.summary.scalar(""bpp"", train_bpp)\n  tf.summary.scalar(""mse"", train_mse)\n\n  tf.summary.image(""original"", quantize_image(x))\n  tf.summary.image(""reconstruction"", quantize_image(x_tilde))\n\n  hooks = [\n      tf.train.StopAtStepHook(last_step=args.last_step),\n      tf.train.NanTensorHook(train_loss),\n  ]\n  with tf.train.MonitoredTrainingSession(\n      hooks=hooks, checkpoint_dir=args.checkpoint_dir,\n      save_checkpoint_secs=300, save_summaries_secs=60) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n\n\ndef compress(args):\n  """"""Compresses an image.""""""\n\n  # Load input image and add batch dimension.\n  x = read_png(args.input_file)\n  x = tf.expand_dims(x, 0)\n  x.set_shape([1, None, None, 3])\n  x_shape = tf.shape(x)\n\n  # Instantiate model.\n  analysis_transform = AnalysisTransform(args.num_filters)\n  entropy_bottleneck = tfc.EntropyBottleneck()\n  synthesis_transform = SynthesisTransform(args.num_filters)\n\n  # Transform and compress the image.\n  y = analysis_transform(x)\n  string = entropy_bottleneck.compress(y)\n\n  # Transform the quantized image back (if requested).\n  y_hat, likelihoods = entropy_bottleneck(y, training=False)\n  x_hat = synthesis_transform(y_hat)\n  x_hat = x_hat[:, :x_shape[1], :x_shape[2], :]\n\n  num_pixels = tf.cast(tf.reduce_prod(tf.shape(x)[:-1]), dtype=tf.float32)\n\n  # Total number of bits divided by number of pixels.\n  eval_bpp = tf.reduce_sum(tf.log(likelihoods)) / (-np.log(2) * num_pixels)\n\n  # Bring both images back to 0..255 range.\n  x *= 255\n  x_hat = tf.clip_by_value(x_hat, 0, 1)\n  x_hat = tf.round(x_hat * 255)\n\n  mse = tf.reduce_mean(tf.squared_difference(x, x_hat))\n  psnr = tf.squeeze(tf.image.psnr(x_hat, x, 255))\n  msssim = tf.squeeze(tf.image.ssim_multiscale(x_hat, x, 255))\n\n  with tf.Session() as sess:\n    # Load the latest model checkpoint, get the compressed string and the tensor\n    # shapes.\n    latest = tf.train.latest_checkpoint(checkpoint_dir=args.checkpoint_dir)\n    tf.train.Saver().restore(sess, save_path=latest)\n    tensors = [string, tf.shape(x)[1:-1], tf.shape(y)[1:-1]]\n    arrays = sess.run(tensors)\n\n    # Write a binary file with the shape information and the compressed string.\n    packed = tfc.PackedTensors()\n    packed.pack(tensors, arrays)\n    with open(args.output_file, ""wb"") as f:\n      f.write(packed.string)\n\n    # If requested, transform the quantized image back and measure performance.\n    if args.verbose:\n      eval_bpp, mse, psnr, msssim, num_pixels = sess.run(\n          [eval_bpp, mse, psnr, msssim, num_pixels])\n\n      # The actual bits per pixel including overhead.\n      bpp = len(packed.string) * 8 / num_pixels\n\n      print(""Mean squared error: {:0.4f}"".format(mse))\n      print(""PSNR (dB): {:0.2f}"".format(psnr))\n      print(""Multiscale SSIM: {:0.4f}"".format(msssim))\n      print(""Multiscale SSIM (dB): {:0.2f}"".format(-10 * np.log10(1 - msssim)))\n      print(""Information content in bpp: {:0.4f}"".format(eval_bpp))\n      print(""Actual bits per pixel: {:0.4f}"".format(bpp))\n\n\ndef decompress(args):\n  """"""Decompresses an image.""""""\n\n  # Read the shape information and compressed string from the binary file.\n  string = tf.placeholder(tf.string, [1])\n  x_shape = tf.placeholder(tf.int32, [2])\n  y_shape = tf.placeholder(tf.int32, [2])\n  with open(args.input_file, ""rb"") as f:\n    packed = tfc.PackedTensors(f.read())\n  tensors = [string, x_shape, y_shape]\n  arrays = packed.unpack(tensors)\n\n  # Instantiate model.\n  entropy_bottleneck = tfc.EntropyBottleneck(dtype=tf.float32)\n  synthesis_transform = SynthesisTransform(args.num_filters)\n\n  # Decompress and transform the image back.\n  y_shape = tf.concat([y_shape, [args.num_filters]], axis=0)\n  y_hat = entropy_bottleneck.decompress(\n      string, y_shape, channels=args.num_filters)\n  x_hat = synthesis_transform(y_hat)\n\n  # Remove batch dimension, and crop away any extraneous padding on the bottom\n  # or right boundaries.\n  x_hat = x_hat[0, :x_shape[0], :x_shape[1], :]\n\n  # Write reconstructed image out as a PNG file.\n  op = write_png(args.output_file, x_hat)\n\n  # Load the latest model checkpoint, and perform the above actions.\n  with tf.Session() as sess:\n    latest = tf.train.latest_checkpoint(checkpoint_dir=args.checkpoint_dir)\n    tf.train.Saver().restore(sess, save_path=latest)\n    sess.run(op, feed_dict=dict(zip(tensors, arrays)))\n\n\ndef parse_args(argv):\n  """"""Parses command line arguments.""""""\n  parser = argparse_flags.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  # High-level options.\n  parser.add_argument(\n      ""--verbose"", ""-V"", action=""store_true"",\n      help=""Report bitrate and distortion when training or compressing."")\n  parser.add_argument(\n      ""--num_filters"", type=int, default=128,\n      help=""Number of filters per layer."")\n  parser.add_argument(\n      ""--checkpoint_dir"", default=""train"",\n      help=""Directory where to save/load model checkpoints."")\n  subparsers = parser.add_subparsers(\n      title=""commands"", dest=""command"",\n      help=""What to do: \'train\' loads training data and trains (or continues ""\n           ""to train) a new model. \'compress\' reads an image file (lossless ""\n           ""PNG format) and writes a compressed binary file. \'decompress\' ""\n           ""reads a binary file and reconstructs the image (in PNG format). ""\n           ""input and output filenames need to be provided for the latter ""\n           ""two options. Invoke \'<command> -h\' for more information."")\n\n  # \'train\' subcommand.\n  train_cmd = subparsers.add_parser(\n      ""train"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Trains (or continues to train) a new model."")\n  train_cmd.add_argument(\n      ""--train_glob"", default=""images/*.png"",\n      help=""Glob pattern identifying training data. This pattern must expand ""\n           ""to a list of RGB images in PNG format."")\n  train_cmd.add_argument(\n      ""--batchsize"", type=int, default=8,\n      help=""Batch size for training."")\n  train_cmd.add_argument(\n      ""--patchsize"", type=int, default=256,\n      help=""Size of image patches for training."")\n  train_cmd.add_argument(\n      ""--lambda"", type=float, default=0.01, dest=""lmbda"",\n      help=""Lambda for rate-distortion tradeoff."")\n  train_cmd.add_argument(\n      ""--last_step"", type=int, default=1000000,\n      help=""Train up to this number of steps."")\n  train_cmd.add_argument(\n      ""--preprocess_threads"", type=int, default=16,\n      help=""Number of CPU threads to use for parallel decoding of training ""\n           ""images."")\n\n  # \'compress\' subcommand.\n  compress_cmd = subparsers.add_parser(\n      ""compress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a PNG file, compresses it, and writes a TFCI file."")\n\n  # \'decompress\' subcommand.\n  decompress_cmd = subparsers.add_parser(\n      ""decompress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a TFCI file, reconstructs the image, and writes back ""\n                  ""a PNG file."")\n\n  # Arguments for both \'compress\' and \'decompress\'.\n  for cmd, ext in ((compress_cmd, "".tfci""), (decompress_cmd, "".png"")):\n    cmd.add_argument(\n        ""input_file"",\n        help=""Input filename."")\n    cmd.add_argument(\n        ""output_file"", nargs=""?"",\n        help=""Output filename (optional). If not provided, appends \'{}\' to ""\n             ""the input filename."".format(ext))\n\n  # Parse arguments.\n  args = parser.parse_args(argv[1:])\n  if args.command is None:\n    parser.print_usage()\n    sys.exit(2)\n  return args\n\n\ndef main(args):\n  # Invoke subcommand.\n  if args.command == ""train"":\n    train(args)\n  elif args.command == ""compress"":\n    if not args.output_file:\n      args.output_file = args.input_file + "".tfci""\n    compress(args)\n  elif args.command == ""decompress"":\n    if not args.output_file:\n      args.output_file = args.input_file + "".png""\n    decompress(args)\n\n\nif __name__ == ""__main__"":\n  app.run(main, flags_parser=parse_args)\n'"
examples/bmshj2018.py,60,"b'# Lint as: python3\n# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Nonlinear transform coder with hyperprior for RGB images.\n\nThis is the image compression model published in:\nJ. Ball\xc3\xa9, D. Minnen, S. Singh, S.J. Hwang, N. Johnston:\n""Variational Image Compression with a Scale Hyperprior""\nInt. Conf. on Learning Representations (ICLR), 2018\nhttps://arxiv.org/abs/1802.01436\n\nThis is meant as \'educational\' code - you can use this to get started with your\nown experiments. To reproduce the exact results from the paper, tuning of hyper-\nparameters may be necessary. To compress images with published models, see\n`tfci.py`.\n""""""\n\nimport argparse\nimport glob\nimport sys\n\nfrom absl import app\nfrom absl.flags import argparse_flags\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nimport tensorflow_compression as tfc\n\n\nSCALES_MIN = 0.11\nSCALES_MAX = 256\nSCALES_LEVELS = 64\n\n\ndef read_png(filename):\n  """"""Loads a PNG image file.""""""\n  string = tf.read_file(filename)\n  image = tf.image.decode_image(string, channels=3)\n  image = tf.cast(image, tf.float32)\n  image /= 255\n  return image\n\n\ndef quantize_image(image):\n  image = tf.round(image * 255)\n  image = tf.saturate_cast(image, tf.uint8)\n  return image\n\n\ndef write_png(filename, image):\n  """"""Saves an image to a PNG file.""""""\n  image = quantize_image(image)\n  string = tf.image.encode_png(image)\n  return tf.write_file(filename, string)\n\n\nclass AnalysisTransform(tf.keras.layers.Layer):\n  """"""The analysis transform.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(AnalysisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_0"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""gdn_0"")),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""gdn_1"")),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_2"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""gdn_2"")),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_3"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=None),\n    ]\n    super(AnalysisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\nclass SynthesisTransform(tf.keras.layers.Layer):\n  """"""The synthesis transform.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(SynthesisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_0"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""igdn_0"", inverse=True)),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""igdn_1"", inverse=True)),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_2"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tfc.GDN(name=""igdn_2"", inverse=True)),\n        tfc.SignalConv2D(\n            3, (5, 5), name=""layer_3"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=None),\n    ]\n    super(SynthesisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\nclass HyperAnalysisTransform(tf.keras.layers.Layer):\n  """"""The analysis transform for the entropy model parameters.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(HyperAnalysisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (3, 3), name=""layer_0"", corr=True, strides_down=1,\n            padding=""same_zeros"", use_bias=True,\n            activation=tf.nn.relu),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=True,\n            activation=tf.nn.relu),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_2"", corr=True, strides_down=2,\n            padding=""same_zeros"", use_bias=False,\n            activation=None),\n    ]\n    super(HyperAnalysisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\nclass HyperSynthesisTransform(tf.keras.layers.Layer):\n  """"""The synthesis transform for the entropy model parameters.""""""\n\n  def __init__(self, num_filters, *args, **kwargs):\n    self.num_filters = num_filters\n    super(HyperSynthesisTransform, self).__init__(*args, **kwargs)\n\n  def build(self, input_shape):\n    self._layers = [\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_0"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True, kernel_parameterizer=None,\n            activation=tf.nn.relu),\n        tfc.SignalConv2D(\n            self.num_filters, (5, 5), name=""layer_1"", corr=False, strides_up=2,\n            padding=""same_zeros"", use_bias=True, kernel_parameterizer=None,\n            activation=tf.nn.relu),\n        tfc.SignalConv2D(\n            self.num_filters, (3, 3), name=""layer_2"", corr=False, strides_up=1,\n            padding=""same_zeros"", use_bias=True, kernel_parameterizer=None,\n            activation=None),\n    ]\n    super(HyperSynthesisTransform, self).build(input_shape)\n\n  def call(self, tensor):\n    for layer in self._layers:\n      tensor = layer(tensor)\n    return tensor\n\n\ndef train(args):\n  """"""Trains the model.""""""\n\n  if args.verbose:\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n  # Create input data pipeline.\n  with tf.device(""/cpu:0""):\n    train_files = glob.glob(args.train_glob)\n    if not train_files:\n      raise RuntimeError(\n          ""No training images found with glob \'{}\'."".format(args.train_glob))\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n    train_dataset = train_dataset.shuffle(buffer_size=len(train_files)).repeat()\n    train_dataset = train_dataset.map(\n        read_png, num_parallel_calls=args.preprocess_threads)\n    train_dataset = train_dataset.map(\n        lambda x: tf.random_crop(x, (args.patchsize, args.patchsize, 3)))\n    train_dataset = train_dataset.batch(args.batchsize)\n    train_dataset = train_dataset.prefetch(32)\n\n  num_pixels = args.batchsize * args.patchsize ** 2\n\n  # Get training patch from dataset.\n  x = train_dataset.make_one_shot_iterator().get_next()\n\n  # Instantiate model.\n  analysis_transform = AnalysisTransform(args.num_filters)\n  synthesis_transform = SynthesisTransform(args.num_filters)\n  hyper_analysis_transform = HyperAnalysisTransform(args.num_filters)\n  hyper_synthesis_transform = HyperSynthesisTransform(args.num_filters)\n  entropy_bottleneck = tfc.EntropyBottleneck()\n\n  # Build autoencoder and hyperprior.\n  y = analysis_transform(x)\n  z = hyper_analysis_transform(abs(y))\n  z_tilde, z_likelihoods = entropy_bottleneck(z, training=True)\n  sigma = hyper_synthesis_transform(z_tilde)\n  scale_table = np.exp(np.linspace(\n      np.log(SCALES_MIN), np.log(SCALES_MAX), SCALES_LEVELS))\n  conditional_bottleneck = tfc.GaussianConditional(sigma, scale_table)\n  y_tilde, y_likelihoods = conditional_bottleneck(y, training=True)\n  x_tilde = synthesis_transform(y_tilde)\n\n  # Total number of bits divided by number of pixels.\n  train_bpp = (tf.reduce_sum(tf.log(y_likelihoods)) +\n               tf.reduce_sum(tf.log(z_likelihoods))) / (-np.log(2) * num_pixels)\n\n  # Mean squared error across pixels.\n  train_mse = tf.reduce_mean(tf.squared_difference(x, x_tilde))\n  # Multiply by 255^2 to correct for rescaling.\n  train_mse *= 255 ** 2\n\n  # The rate-distortion cost.\n  train_loss = args.lmbda * train_mse + train_bpp\n\n  # Minimize loss and auxiliary loss, and execute update op.\n  step = tf.train.create_global_step()\n  main_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n  main_step = main_optimizer.minimize(train_loss, global_step=step)\n\n  aux_optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n  aux_step = aux_optimizer.minimize(entropy_bottleneck.losses[0])\n\n  train_op = tf.group(main_step, aux_step, entropy_bottleneck.updates[0])\n\n  tf.summary.scalar(""loss"", train_loss)\n  tf.summary.scalar(""bpp"", train_bpp)\n  tf.summary.scalar(""mse"", train_mse)\n\n  tf.summary.image(""original"", quantize_image(x))\n  tf.summary.image(""reconstruction"", quantize_image(x_tilde))\n\n  hooks = [\n      tf.train.StopAtStepHook(last_step=args.last_step),\n      tf.train.NanTensorHook(train_loss),\n  ]\n  with tf.train.MonitoredTrainingSession(\n      hooks=hooks, checkpoint_dir=args.checkpoint_dir,\n      save_checkpoint_secs=300, save_summaries_secs=60) as sess:\n    while not sess.should_stop():\n      sess.run(train_op)\n\n\ndef compress(args):\n  """"""Compresses an image.""""""\n\n  # Load input image and add batch dimension.\n  x = read_png(args.input_file)\n  x = tf.expand_dims(x, 0)\n  x.set_shape([1, None, None, 3])\n  x_shape = tf.shape(x)\n\n  # Instantiate model.\n  analysis_transform = AnalysisTransform(args.num_filters)\n  synthesis_transform = SynthesisTransform(args.num_filters)\n  hyper_analysis_transform = HyperAnalysisTransform(args.num_filters)\n  hyper_synthesis_transform = HyperSynthesisTransform(args.num_filters)\n  entropy_bottleneck = tfc.EntropyBottleneck()\n\n  # Transform and compress the image.\n  y = analysis_transform(x)\n  y_shape = tf.shape(y)\n  z = hyper_analysis_transform(abs(y))\n  z_hat, z_likelihoods = entropy_bottleneck(z, training=False)\n  sigma = hyper_synthesis_transform(z_hat)\n  sigma = sigma[:, :y_shape[1], :y_shape[2], :]\n  scale_table = np.exp(np.linspace(\n      np.log(SCALES_MIN), np.log(SCALES_MAX), SCALES_LEVELS))\n  conditional_bottleneck = tfc.GaussianConditional(sigma, scale_table)\n  side_string = entropy_bottleneck.compress(z)\n  string = conditional_bottleneck.compress(y)\n\n  # Transform the quantized image back (if requested).\n  y_hat, y_likelihoods = conditional_bottleneck(y, training=False)\n  x_hat = synthesis_transform(y_hat)\n  x_hat = x_hat[:, :x_shape[1], :x_shape[2], :]\n\n  num_pixels = tf.cast(tf.reduce_prod(tf.shape(x)[:-1]), dtype=tf.float32)\n\n  # Total number of bits divided by number of pixels.\n  eval_bpp = (tf.reduce_sum(tf.log(y_likelihoods)) +\n              tf.reduce_sum(tf.log(z_likelihoods))) / (-np.log(2) * num_pixels)\n\n  # Bring both images back to 0..255 range.\n  x *= 255\n  x_hat = tf.clip_by_value(x_hat, 0, 1)\n  x_hat = tf.round(x_hat * 255)\n\n  mse = tf.reduce_mean(tf.squared_difference(x, x_hat))\n  psnr = tf.squeeze(tf.image.psnr(x_hat, x, 255))\n  msssim = tf.squeeze(tf.image.ssim_multiscale(x_hat, x, 255))\n\n  with tf.Session() as sess:\n    # Load the latest model checkpoint, get the compressed string and the tensor\n    # shapes.\n    latest = tf.train.latest_checkpoint(checkpoint_dir=args.checkpoint_dir)\n    tf.train.Saver().restore(sess, save_path=latest)\n    tensors = [string, side_string,\n               tf.shape(x)[1:-1], tf.shape(y)[1:-1], tf.shape(z)[1:-1]]\n    arrays = sess.run(tensors)\n\n    # Write a binary file with the shape information and the compressed string.\n    packed = tfc.PackedTensors()\n    packed.pack(tensors, arrays)\n    with open(args.output_file, ""wb"") as f:\n      f.write(packed.string)\n\n    # If requested, transform the quantized image back and measure performance.\n    if args.verbose:\n      eval_bpp, mse, psnr, msssim, num_pixels = sess.run(\n          [eval_bpp, mse, psnr, msssim, num_pixels])\n\n      # The actual bits per pixel including overhead.\n      bpp = len(packed.string) * 8 / num_pixels\n\n      print(""Mean squared error: {:0.4f}"".format(mse))\n      print(""PSNR (dB): {:0.2f}"".format(psnr))\n      print(""Multiscale SSIM: {:0.4f}"".format(msssim))\n      print(""Multiscale SSIM (dB): {:0.2f}"".format(-10 * np.log10(1 - msssim)))\n      print(""Information content in bpp: {:0.4f}"".format(eval_bpp))\n      print(""Actual bits per pixel: {:0.4f}"".format(bpp))\n\n\ndef decompress(args):\n  """"""Decompresses an image.""""""\n\n  # Read the shape information and compressed string from the binary file.\n  string = tf.placeholder(tf.string, [1])\n  side_string = tf.placeholder(tf.string, [1])\n  x_shape = tf.placeholder(tf.int32, [2])\n  y_shape = tf.placeholder(tf.int32, [2])\n  z_shape = tf.placeholder(tf.int32, [2])\n  with open(args.input_file, ""rb"") as f:\n    packed = tfc.PackedTensors(f.read())\n  tensors = [string, side_string, x_shape, y_shape, z_shape]\n  arrays = packed.unpack(tensors)\n\n  # Instantiate model.\n  synthesis_transform = SynthesisTransform(args.num_filters)\n  hyper_synthesis_transform = HyperSynthesisTransform(args.num_filters)\n  entropy_bottleneck = tfc.EntropyBottleneck(dtype=tf.float32)\n\n  # Decompress and transform the image back.\n  z_shape = tf.concat([z_shape, [args.num_filters]], axis=0)\n  z_hat = entropy_bottleneck.decompress(\n      side_string, z_shape, channels=args.num_filters)\n  sigma = hyper_synthesis_transform(z_hat)\n  sigma = sigma[:, :y_shape[0], :y_shape[1], :]\n  scale_table = np.exp(np.linspace(\n      np.log(SCALES_MIN), np.log(SCALES_MAX), SCALES_LEVELS))\n  conditional_bottleneck = tfc.GaussianConditional(\n      sigma, scale_table, dtype=tf.float32)\n  y_hat = conditional_bottleneck.decompress(string)\n  x_hat = synthesis_transform(y_hat)\n\n  # Remove batch dimension, and crop away any extraneous padding on the bottom\n  # or right boundaries.\n  x_hat = x_hat[0, :x_shape[0], :x_shape[1], :]\n\n  # Write reconstructed image out as a PNG file.\n  op = write_png(args.output_file, x_hat)\n\n  # Load the latest model checkpoint, and perform the above actions.\n  with tf.Session() as sess:\n    latest = tf.train.latest_checkpoint(checkpoint_dir=args.checkpoint_dir)\n    tf.train.Saver().restore(sess, save_path=latest)\n    sess.run(op, feed_dict=dict(zip(tensors, arrays)))\n\n\ndef parse_args(argv):\n  """"""Parses command line arguments.""""""\n  parser = argparse_flags.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  # High-level options.\n  parser.add_argument(\n      ""--verbose"", ""-V"", action=""store_true"",\n      help=""Report bitrate and distortion when training or compressing."")\n  parser.add_argument(\n      ""--num_filters"", type=int, default=192,\n      help=""Number of filters per layer."")\n  parser.add_argument(\n      ""--checkpoint_dir"", default=""train"",\n      help=""Directory where to save/load model checkpoints."")\n  subparsers = parser.add_subparsers(\n      title=""commands"", dest=""command"",\n      help=""What to do: \'train\' loads training data and trains (or continues ""\n           ""to train) a new model. \'compress\' reads an image file (lossless ""\n           ""PNG format) and writes a compressed binary file. \'decompress\' ""\n           ""reads a binary file and reconstructs the image (in PNG format). ""\n           ""input and output filenames need to be provided for the latter ""\n           ""two options. Invoke \'<command> -h\' for more information."")\n\n  # \'train\' subcommand.\n  train_cmd = subparsers.add_parser(\n      ""train"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Trains (or continues to train) a new model."")\n  train_cmd.add_argument(\n      ""--train_glob"", default=""images/*.png"",\n      help=""Glob pattern identifying training data. This pattern must expand ""\n           ""to a list of RGB images in PNG format."")\n  train_cmd.add_argument(\n      ""--batchsize"", type=int, default=8,\n      help=""Batch size for training."")\n  train_cmd.add_argument(\n      ""--patchsize"", type=int, default=256,\n      help=""Size of image patches for training."")\n  train_cmd.add_argument(\n      ""--lambda"", type=float, default=0.01, dest=""lmbda"",\n      help=""Lambda for rate-distortion tradeoff."")\n  train_cmd.add_argument(\n      ""--last_step"", type=int, default=1000000,\n      help=""Train up to this number of steps."")\n  train_cmd.add_argument(\n      ""--preprocess_threads"", type=int, default=16,\n      help=""Number of CPU threads to use for parallel decoding of training ""\n           ""images."")\n\n  # \'compress\' subcommand.\n  compress_cmd = subparsers.add_parser(\n      ""compress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a PNG file, compresses it, and writes a TFCI file."")\n\n  # \'decompress\' subcommand.\n  decompress_cmd = subparsers.add_parser(\n      ""decompress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a TFCI file, reconstructs the image, and writes back ""\n                  ""a PNG file."")\n\n  # Arguments for both \'compress\' and \'decompress\'.\n  for cmd, ext in ((compress_cmd, "".tfci""), (decompress_cmd, "".png"")):\n    cmd.add_argument(\n        ""input_file"",\n        help=""Input filename."")\n    cmd.add_argument(\n        ""output_file"", nargs=""?"",\n        help=""Output filename (optional). If not provided, appends \'{}\' to ""\n             ""the input filename."".format(ext))\n\n  # Parse arguments.\n  args = parser.parse_args(argv[1:])\n  if args.command is None:\n    parser.print_usage()\n    sys.exit(2)\n  return args\n\n\ndef main(args):\n  # Invoke subcommand.\n  if args.command == ""train"":\n    train(args)\n  elif args.command == ""compress"":\n    if not args.output_file:\n      args.output_file = args.input_file + "".tfci""\n    compress(args)\n  elif args.command == ""decompress"":\n    if not args.output_file:\n      args.output_file = args.input_file + "".png""\n    decompress(args)\n\n\nif __name__ == ""__main__"":\n  app.run(main, flags_parser=parse_args)\n'"
examples/tfci.py,24,"b'# Lint as: python3\n# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts an image between PNG and TFCI formats.\n\nUse this script to compress images with pre-trained models as published. See the\n\'models\' subcommand for a list of available models.\n""""""\n\nimport argparse\nimport os\nimport sys\nimport urllib\n\nfrom absl import app\nfrom absl.flags import argparse_flags\nimport tensorflow.compat.v1 as tf\n\nimport tensorflow_compression as tfc  # pylint:disable=unused-import\n\n# Default URL to fetch metagraphs from.\nURL_PREFIX = ""https://storage.googleapis.com/tensorflow_compression/metagraphs""\n# Default location to store cached metagraphs.\nMETAGRAPH_CACHE = ""/tmp/tfc_metagraphs""\n\n\ndef read_png(filename):\n  """"""Creates graph to load a PNG image file.""""""\n  string = tf.io.read_file(filename)\n  image = tf.image.decode_image(string)\n  image = tf.expand_dims(image, 0)\n  return image\n\n\ndef write_png(filename, image):\n  """"""Creates graph to write a PNG image file.""""""\n  image = tf.squeeze(image, 0)\n  if image.dtype.is_floating:\n    image = tf.round(image)\n  if image.dtype != tf.uint8:\n    image = tf.saturate_cast(image, tf.uint8)\n  string = tf.image.encode_png(image)\n  return tf.io.write_file(filename, string)\n\n\ndef load_cached(filename):\n  """"""Downloads and caches files from web storage.""""""\n  pathname = os.path.join(METAGRAPH_CACHE, filename)\n  try:\n    with tf.io.gfile.GFile(pathname, ""rb"") as f:\n      string = f.read()\n  except tf.errors.NotFoundError:\n    url = URL_PREFIX + ""/"" + filename\n    try:\n      request = urllib.request.urlopen(url)\n      string = request.read()\n    finally:\n      request.close()\n    tf.io.gfile.makedirs(os.path.dirname(pathname))\n    with tf.io.gfile.GFile(pathname, ""wb"") as f:\n      f.write(string)\n  return string\n\n\ndef import_metagraph(model):\n  """"""Imports a trained model metagraph into the current graph.""""""\n  string = load_cached(model + "".metagraph"")\n  metagraph = tf.MetaGraphDef()\n  metagraph.ParseFromString(string)\n  tf.train.import_meta_graph(metagraph)\n  return metagraph.signature_def\n\n\ndef instantiate_signature(signature_def):\n  """"""Fetches tensors defined in a signature from the graph.""""""\n  graph = tf.get_default_graph()\n  inputs = {\n      k: graph.get_tensor_by_name(v.name)\n      for k, v in signature_def.inputs.items()\n  }\n  outputs = {\n      k: graph.get_tensor_by_name(v.name)\n      for k, v in signature_def.outputs.items()\n  }\n  return inputs, outputs\n\n\ndef compress_image(model, input_image):\n  """"""Compresses an image array into a bitstring.""""""\n  with tf.Graph().as_default():\n    # Load model metagraph.\n    signature_defs = import_metagraph(model)\n    inputs, outputs = instantiate_signature(signature_defs[""sender""])\n\n    # Just one input tensor.\n    inputs = inputs[""input_image""]\n    # Multiple output tensors, ordered alphabetically, without names.\n    outputs = [outputs[k] for k in sorted(outputs) if k.startswith(""channel:"")]\n\n    # Run encoder.\n    with tf.Session() as sess:\n      arrays = sess.run(outputs, feed_dict={inputs: input_image})\n\n    # Pack data into bitstring.\n    packed = tfc.PackedTensors()\n    packed.model = model\n    packed.pack(outputs, arrays)\n    return packed.string\n\n\ndef compress(model, input_file, output_file, target_bpp=None, bpp_strict=False):\n  """"""Compresses a PNG file to a TFCI file.""""""\n  if not output_file:\n    output_file = input_file + "".tfci""\n\n  # Load image.\n  with tf.Graph().as_default():\n    with tf.Session() as sess:\n      input_image = sess.run(read_png(input_file))\n      num_pixels = input_image.shape[-2] * input_image.shape[-3]\n\n  if not target_bpp:\n    # Just compress with a specific model.\n    bitstring = compress_image(model, input_image)\n  else:\n    # Get model list.\n    models = load_cached(model + "".models"")\n    models = models.decode(""ascii"").split()\n\n    # Do a binary search over all RD points.\n    lower = -1\n    upper = len(models)\n    bpp = None\n    best_bitstring = None\n    best_bpp = None\n    while bpp != target_bpp and upper - lower > 1:\n      i = (upper + lower) // 2\n      bitstring = compress_image(models[i], input_image)\n      bpp = 8 * len(bitstring) / num_pixels\n      is_admissible = bpp <= target_bpp or not bpp_strict\n      is_better = (best_bpp is None or\n                   abs(bpp - target_bpp) < abs(best_bpp - target_bpp))\n      if is_admissible and is_better:\n        best_bitstring = bitstring\n        best_bpp = bpp\n      if bpp < target_bpp:\n        lower = i\n      if bpp > target_bpp:\n        upper = i\n    if best_bpp is None:\n      assert bpp_strict\n      raise RuntimeError(\n          ""Could not compress image to less than {} bpp."".format(target_bpp))\n    bitstring = best_bitstring\n\n  # Write bitstring to disk.\n  with tf.io.gfile.GFile(output_file, ""wb"") as f:\n    f.write(bitstring)\n\n\ndef decompress(input_file, output_file):\n  """"""Decompresses a TFCI file and writes a PNG file.""""""\n  if not output_file:\n    output_file = input_file + "".png""\n\n  with tf.Graph().as_default():\n    # Unserialize packed data from disk.\n    with tf.io.gfile.GFile(input_file, ""rb"") as f:\n      packed = tfc.PackedTensors(f.read())\n\n    # Load model metagraph.\n    signature_defs = import_metagraph(packed.model)\n    inputs, outputs = instantiate_signature(signature_defs[""receiver""])\n\n    # Multiple input tensors, ordered alphabetically, without names.\n    inputs = [inputs[k] for k in sorted(inputs) if k.startswith(""channel:"")]\n    # Just one output operation.\n    outputs = write_png(output_file, outputs[""output_image""])\n\n    # Unpack data.\n    arrays = packed.unpack(inputs)\n\n    # Run decoder.\n    with tf.Session() as sess:\n      sess.run(outputs, feed_dict=dict(zip(inputs, arrays)))\n\n\ndef list_models():\n  url = URL_PREFIX + ""/models.txt""\n  try:\n    request = urllib.request.urlopen(url)\n    print(request.read().decode(""utf-8""))\n  finally:\n    request.close()\n\n\ndef parse_args(argv):\n  """"""Parses command line arguments.""""""\n  parser = argparse_flags.ArgumentParser(\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  # High-level options.\n  parser.add_argument(\n      ""--url_prefix"",\n      default=URL_PREFIX,\n      help=""URL prefix for downloading model metagraphs."")\n  parser.add_argument(\n      ""--metagraph_cache"",\n      default=METAGRAPH_CACHE,\n      help=""Directory where to cache model metagraphs."")\n  subparsers = parser.add_subparsers(\n      title=""commands"", dest=""command"",\n      help=""Invoke \'<command> -h\' for more information."")\n\n  # \'compress\' subcommand.\n  compress_cmd = subparsers.add_parser(\n      ""compress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a PNG file, compresses it using the given model, and ""\n                  ""writes a TFCI file."")\n  compress_cmd.add_argument(\n      ""model"",\n      help=""Unique model identifier. See \'models\' command for options. If ""\n           ""\'target_bpp\' is provided, don\'t specify the index at the end of ""\n           ""the model identifier."")\n  compress_cmd.add_argument(\n      ""--target_bpp"", type=float,\n      help=""Target bits per pixel. If provided, a binary search is used to try ""\n           ""to match the given bpp as close as possible. In this case, don\'t ""\n           ""specify the index at the end of the model identifier. It will be ""\n           ""automatically determined."")\n  compress_cmd.add_argument(\n      ""--bpp_strict"", action=""store_true"",\n      help=""Try never to exceed \'target_bpp\'. Ignored if \'target_bpp\' is not ""\n           ""set."")\n\n  # \'decompress\' subcommand.\n  decompress_cmd = subparsers.add_parser(\n      ""decompress"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Reads a TFCI file, reconstructs the image using the model ""\n                  ""it was compressed with, and writes back a PNG file."")\n\n  # Arguments for both \'compress\' and \'decompress\'.\n  for cmd, ext in ((compress_cmd, "".tfci""), (decompress_cmd, "".png"")):\n    cmd.add_argument(\n        ""input_file"",\n        help=""Input filename."")\n    cmd.add_argument(\n        ""output_file"", nargs=""?"",\n        help=""Output filename (optional). If not provided, appends \'{}\' to ""\n             ""the input filename."".format(ext))\n\n  # \'models\' subcommand.\n  subparsers.add_parser(\n      ""models"",\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n      description=""Lists available trained models. Requires an internet ""\n                  ""connection."")\n\n  # Parse arguments.\n  args = parser.parse_args(argv[1:])\n  if args.command is None:\n    parser.print_usage()\n    sys.exit(2)\n  return args\n\n\ndef main(args):\n  # Command line can override these defaults.\n  global URL_PREFIX, METAGRAPH_CACHE\n  URL_PREFIX = args.url_prefix\n  METAGRAPH_CACHE = args.metagraph_cache\n\n  # Invoke subcommand.\n  if args.command == ""compress"":\n    compress(args.model, args.input_file, args.output_file,\n             args.target_bpp, args.bpp_strict)\n  if args.command == ""decompress"":\n    decompress(args.input_file, args.output_file)\n  if args.command == ""models"":\n    list_models()\n\n\nif __name__ == ""__main__"":\n  app.run(main, flags_parser=parse_args)\n'"
tensorflow_compression/__init__.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Data compression tools.""""""\n\ntry:\n  import tensorflow as _tensorflow\n  _tf_version = [int(v) for v in _tensorflow.version.VERSION.split(""."")]\n  assert _tf_version[0] == 2 and _tf_version[1] == 1\nexcept (ImportError, AssertionError):\n  raise RuntimeError(\n      ""For tensorflow_compression, please install TensorFlow 2.1."")\n\n\n# pylint: disable=wildcard-import\nfrom tensorflow_compression.python.distributions.deep_factorized import *\nfrom tensorflow_compression.python.distributions.helpers import *\nfrom tensorflow_compression.python.distributions.uniform_noise import *\nfrom tensorflow_compression.python.entropy_models.continuous_batched import *\nfrom tensorflow_compression.python.entropy_models.continuous_indexed import *\nfrom tensorflow_compression.python.layers.entropy_models import *\nfrom tensorflow_compression.python.layers.gdn import *\nfrom tensorflow_compression.python.layers.initializers import *\nfrom tensorflow_compression.python.layers.parameterizers import *\nfrom tensorflow_compression.python.layers.signal_conv import *\nfrom tensorflow_compression.python.ops.math_ops import *\nfrom tensorflow_compression.python.ops.padding_ops import *\nfrom tensorflow_compression.python.ops.range_coding_ops import *\nfrom tensorflow_compression.python.ops.spectral_ops import *\nfrom tensorflow_compression.python.util.packed_tensors import *\n# pylint: enable=wildcard-import\n'"
tools/generate_docs.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generates docs for the TensorFlow compression library.""""""\n\nimport os\nimport sys\n\nfrom absl import app\nfrom absl import flags\n\nfrom tensorflow_docs.api_generator import generate_lib\n\nimport tensorflow_compression as tfc\n\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n  doc_generator = generate_lib.DocGenerator(\n      root_title=""TensorFlow/compression"",\n      py_modules=[(""tfc"", tfc)],\n      base_dir=os.path.dirname(tfc.__file__),\n      private_map={\n          ""tfc.python.ops"": [""gen_range_coding_ops"", ""namespace_helper""],\n      },\n      code_url_prefix=""https://github.com/tensorflow/compression/tree/master/""\n                      ""tensorflow_compression"",\n      api_cache=False,\n  )\n  sys.exit(doc_generator.build(FLAGS.output_dir))\n\n\nif __name__ == ""__main__"":\n  flags.DEFINE_string(\n      ""output_dir"", ""/tmp/tensorflow_compression/api_docs/python"",\n      ""Output directory."")\n\n  app.run(main)\n'"
tensorflow_compression/python/__init__.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/all_test.py,1,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_compression/python.\n\nThis is a convenience file to be included in PIP package.\nNo BUILD entry exists for this file on purpose.\n""""""\n\nimport tensorflow.compat.v1 as tf\n\n# pylint: disable=wildcard-import\nfrom tensorflow_compression.python.layers.entropy_models_test import *\nfrom tensorflow_compression.python.layers.gdn_test import *\nfrom tensorflow_compression.python.layers.parameterizers_test import *\nfrom tensorflow_compression.python.layers.signal_conv_test import *\n\nfrom tensorflow_compression.python.ops.math_ops_test import *\nfrom tensorflow_compression.python.ops.padding_ops_test import *\nfrom tensorflow_compression.python.ops.range_coding_ops_test import *\nfrom tensorflow_compression.python.ops.spectral_ops_test import *\n\nfrom tensorflow_compression.python.util.packed_tensors_test import *\n# pylint: enable=wildcard-import\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/distributions/__init__.py,0,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/distributions/deep_factorized.py,37,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deep fully factorized distribution based on cumulative.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_compression.python.distributions import helpers\n\n\n__all__ = [""DeepFactorized""]\n\n\nclass DeepFactorized(tfp.distributions.Distribution):\n  """"""Fully factorized distribution based on neural network cumulative.\n\n  This is a flexible, nonparametric probability density model, described in\n  appendix 6.1 of the paper:\n\n  > ""Variational image compression with a scale hyperprior""<br />\n  > J. Ball\xc3\xa9, D. Minnen, S. Singh, S. J. Hwang, N. Johnston<br />\n  > https://openreview.net/forum?id=rkcQFMZRb\n\n  This implementation already includes convolution with a unit-width uniform\n  density, as described in appendix 6.2 of the same paper. Please cite the paper\n  if you use this code for scientific work.\n\n  This is a scalar distribution (i.e., its `event_shape` is always length 0),\n  and the density object always creates its own `tf.Variable`s representing the\n  trainable distribution parameters.\n  """"""\n\n  def __init__(self, batch_shape=(), num_filters=(3, 3), init_scale=10,\n               allow_nan_stats=False, dtype=tf.float32, name=""DeepFactorized""):\n    """"""Initializer.\n\n    Arguments:\n      batch_shape: Iterable of integers. The desired batch shape for the\n        `Distribution` (rightmost dimensions which are assumed independent, but\n        not identically distributed).\n      num_filters: Iterable of integers. The number of filters for each of the\n        hidden layers. The first and last layer of the network implementing the\n        cumulative distribution are not included (they are assumed to be 1).\n      init_scale: Float. Scale factor for the density at initialization. It is\n        recommended to choose a large enough scale factor such that most values\n        initially lie within a region of high likelihood. This improves\n        training.\n      allow_nan_stats: Boolean. Whether to allow `NaN`s to be returned when\n        querying distribution statistics.\n      dtype: A floating point `tf.dtypes.DType`. Computations relating to this\n        distribution will be performed at this precision.\n      name: String. A name for this distribution.\n    """"""\n    parameters = dict(locals())\n    self._batch_shape_tuple = tuple(int(s) for s in batch_shape)\n    self._num_filters = tuple(int(f) for f in num_filters)\n    self._init_scale = float(init_scale)\n    self._estimated_tail_mass = None\n    super().__init__(\n        dtype=dtype,\n        reparameterization_type=tfp.distributions.NOT_REPARAMETERIZED,\n        validate_args=False,\n        allow_nan_stats=allow_nan_stats,\n        parameters=parameters,\n        name=name,\n    )\n    self._make_variables()\n\n  @property\n  def num_filters(self):\n    return self._num_filters\n\n  @property\n  def init_scale(self):\n    return self._init_scale\n\n  def _make_variables(self):\n    """"""Creates the variables representing the parameters of the distribution.""""""\n    channels = self.batch_shape.num_elements()\n    filters = (1,) + self.num_filters + (1,)\n    scale = self.init_scale ** (1 / (len(self.num_filters) + 1))\n    self._matrices = []\n    self._biases = []\n    self._factors = []\n\n    for i in range(len(self.num_filters) + 1):\n      init = tf.math.log(tf.math.expm1(1 / scale / filters[i + 1]))\n      init = tf.cast(init, dtype=self.dtype)\n      init = tf.broadcast_to(init, (channels, filters[i + 1], filters[i]))\n      matrix = tf.Variable(init, name=""matrix_{}"".format(i))\n      self._matrices.append(matrix)\n\n      bias = tf.Variable(\n          tf.random.uniform(\n              (channels, filters[i + 1], 1), -.5, .5, dtype=self.dtype),\n          name=""bias_{}"".format(i))\n      self._biases.append(bias)\n\n      if i < len(self.num_filters):\n        factor = tf.Variable(\n            tf.zeros((channels, filters[i + 1], 1), dtype=self.dtype),\n            name=""factor_{}"".format(i))\n        self._factors.append(factor)\n\n  def _batch_shape_tensor(self):\n    return tf.constant(self._batch_shape_tuple, dtype=int)\n\n  def _batch_shape(self):\n    return tf.TensorShape(self._batch_shape_tuple)\n\n  def _event_shape_tensor(self):\n    return tf.constant((), dtype=int)\n\n  def _event_shape(self):\n    return tf.TensorShape(())\n\n  def _logits_cumulative(self, inputs):\n    """"""Evaluate logits of the cumulative densities.\n\n    Arguments:\n      inputs: The values at which to evaluate the cumulative densities, expected\n        to be a `tf.Tensor` of shape `(channels, 1, batch)`.\n\n    Returns:\n      A `tf.Tensor` of the same shape as `inputs`, containing the logits of the\n      cumulative densities evaluated at the given inputs.\n    """"""\n    logits = inputs\n    for i in range(len(self.num_filters) + 1):\n      matrix = tf.nn.softplus(self._matrices[i])\n      logits = tf.linalg.matmul(matrix, logits)\n      logits += self._biases[i]\n      if i < len(self.num_filters):\n        factor = tf.math.tanh(self._factors[i])\n        logits += factor * tf.math.tanh(logits)\n    return logits\n\n  def _prob(self, y):\n    """"""Called by the base class to compute likelihoods.""""""\n    # Convert to (channels, 1, batch) format by collapsing dimensions and then\n    # commuting channels to front.\n    y = tf.broadcast_to(\n        y, tf.broadcast_dynamic_shape(tf.shape(y), self.batch_shape_tensor()))\n    shape = tf.shape(y)\n    y = tf.reshape(y, (-1, 1, self.batch_shape.num_elements()))\n    y = tf.transpose(y, (2, 1, 0))\n\n    # Evaluate densities.\n    # We can use the special rule below to only compute differences in the left\n    # tail of the sigmoid. This increases numerical stability: sigmoid(x) is 1\n    # for large x, 0 for small x. Subtracting two numbers close to 0 can be done\n    # with much higher precision than subtracting two numbers close to 1.\n    lower = self._logits_cumulative(y - .5)\n    upper = self._logits_cumulative(y + .5)\n    # Flip signs if we can move more towards the left tail of the sigmoid.\n    sign = tf.stop_gradient(-tf.math.sign(lower + upper))\n    p = abs(tf.sigmoid(sign * upper) - tf.sigmoid(sign * lower))\n\n    # Convert back to (broadcasted) input tensor shape.\n    p = tf.transpose(p, (2, 1, 0))\n    p = tf.reshape(p, shape)\n    return p\n\n  def _quantization_offset(self):\n    return tf.constant(0, dtype=self.dtype)\n\n  def _lower_tail(self, tail_mass):\n    tail = helpers.estimate_tails(\n        self._logits_cumulative, -tf.math.log(2 / tail_mass - 1),\n        tf.constant([self.batch_shape.num_elements(), 1, 1], tf.int32),\n        self.dtype)\n    return tf.reshape(tail, self.batch_shape_tensor())\n\n  def _upper_tail(self, tail_mass):\n    tail = helpers.estimate_tails(\n        self._logits_cumulative, tf.math.log(2 / tail_mass - 1),\n        tf.constant([self.batch_shape.num_elements(), 1, 1], tf.int32),\n        self.dtype)\n    return tf.reshape(tail, self.batch_shape_tensor())\n'"
tensorflow_compression/python/distributions/deep_factorized_test.py,7,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of deep factorized distribution.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_compression.python.distributions import deep_factorized\nfrom tensorflow_compression.python.distributions import helpers\n\n\nclass DeepFactorizedTest(tf.test.TestCase):\n\n  def test_can_instantiate_scalar(self):\n    df = deep_factorized.DeepFactorized()\n    self.assertEqual(df.batch_shape, ())\n    self.assertEqual(df.event_shape, ())\n    self.assertEqual(df.num_filters, (3, 3))\n    self.assertEqual(df.init_scale, 10)\n\n  def test_can_instantiate_batched(self):\n    df = deep_factorized.DeepFactorized(batch_shape=(4, 3))\n    self.assertEqual(df.batch_shape, (4, 3))\n    self.assertEqual(df.event_shape, ())\n    self.assertEqual(df.num_filters, (3, 3))\n    self.assertEqual(df.init_scale, 10)\n\n  def test_variables_receive_gradients(self):\n    df = deep_factorized.DeepFactorized()\n    with tf.GradientTape() as tape:\n      x = tf.random.normal([20])\n      loss = -tf.reduce_mean(df.log_prob(x))\n    grads = tape.gradient(loss, df.trainable_variables)\n    self.assertLen(grads, 8)\n    self.assertNotIn(None, grads)\n\n  def test_logistic_is_special_case(self):\n    # With no hidden units, the density should collapse to a logistic\n    # distribution convolved with a standard uniform distribution.\n    df = deep_factorized.DeepFactorized(num_filters=(), init_scale=1)\n    logistic = tfp.distributions.Logistic(loc=-df._biases[0][0, 0], scale=1.)\n    x = tf.linspace(-5., 5., 20)\n    prob_df = df.prob(x)\n    prob_log = logistic.cdf(x + .5) - logistic.cdf(x - .5)\n    self.assertAllClose(prob_df, prob_log)\n\n  def test_uniform_is_special_case(self):\n    # With the scale parameter going to zero, the density should approach a\n    # unit-width uniform distribution.\n    df = deep_factorized.DeepFactorized(init_scale=1e-3)\n    x = tf.linspace(-1., 1., 10)\n    self.assertAllClose(df.prob(x), [0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n\n  def test_quantization_offset_is_zero(self):\n    df = deep_factorized.DeepFactorized()\n    self.assertEqual(helpers.quantization_offset(df), 0)\n\n  def test_tails_and_offset_are_in_order(self):\n    df = deep_factorized.DeepFactorized()\n    offset = helpers.quantization_offset(df)\n    lower_tail = helpers.lower_tail(df, 2**-8)\n    upper_tail = helpers.upper_tail(df, 2**-8)\n    self.assertGreater(upper_tail, offset)\n    self.assertGreater(offset, lower_tail)\n\n  def test_stats_throw_error(self):\n    df = deep_factorized.DeepFactorized()\n    with self.assertRaises(NotImplementedError):\n      df.mode()\n    with self.assertRaises(NotImplementedError):\n      df.mean()\n    with self.assertRaises(NotImplementedError):\n      df.quantile(.5)\n    with self.assertRaises(NotImplementedError):\n      df.survival_function(.5)\n    with self.assertRaises(NotImplementedError):\n      df.sample()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/distributions/helpers.py,29,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Extensions to `tfp.distributions.Distribution` to handle range coding.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\n__all__ = [\n    ""estimate_tails"",\n    ""quantization_offset"",\n    ""lower_tail"",\n    ""upper_tail"",\n]\n\n\n# TODO(jonycgn): Consider wrapping in tf.function.\ndef estimate_tails(func, target, shape, dtype):\n  """"""Estimates approximate tail quantiles.\n\n  This runs a simple Adam iteration to determine tail quantiles. The\n  objective is to find an `x` such that:\n  ```\n  func(x) == target\n  ```\n  For instance, if `func` is a CDF and the target is a quantile value, this\n  would find the approximate location of that quantile. Note that `func` is\n  assumed to be monotonic. When each tail estimate has passed the optimal value\n  of `x`, the algorithm does 10 additional iterations and then stops.\n\n  This operation is vectorized. The tensor shape of `x` is given by `shape`, and\n  `target` must have a shape that is broadcastable to the output of `func(x)`.\n\n  Arguments:\n    func: A callable that computes cumulative distribution function, survival\n      function, or similar.\n    target: The desired target value.\n    shape: The shape of the `tf.Tensor` representing `x`.\n    dtype: The `tf.dtypes.Dtype` of the computation (and the return value).\n\n  Returns:\n    A `tf.Tensor` representing the solution (`x`).\n  """"""\n  with tf.name_scope(""estimate_tails""):\n    dtype = tf.as_dtype(dtype)\n    shape = tf.convert_to_tensor(shape, tf.int32)\n    target = tf.convert_to_tensor(target, dtype)\n\n    def loop_cond(tails, m, v, count):\n      del tails, m, v  # unused\n      return tf.reduce_min(count) < 10\n\n    def loop_body(tails, m, v, count):\n      with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(tails)\n        loss = abs(func(tails) - target)\n      grad = tape.gradient(loss, tails)\n      m = .5 * m + .5 * grad  # Adam mean estimate.\n      v = .9 * v + .1 * tf.square(grad)  # Adam variance estimate.\n      tails -= .5 * m / (tf.sqrt(v) + 1e-7)\n      # Start counting when the gradient flips sign (note that this assumes\n      # `tails` is initialized to zero).\n      count = tf.where(\n          tf.math.logical_or(count > 0, tails * grad > 0),\n          count + 1, count)\n      return tails, m, v, count\n\n    init_tails = tf.zeros(shape, dtype=dtype)\n    init_m = tf.zeros(shape, dtype=dtype)\n    init_v = tf.ones(shape, dtype=dtype)\n    init_count = tf.zeros(shape, dtype=tf.int32)\n    return tf.while_loop(\n        loop_cond, loop_body, (init_tails, init_m, init_v, init_count),\n        back_prop=False)[0]\n\n\ndef quantization_offset(distribution):\n  """"""Computes distribution-dependent quantization offset.\n\n  For range coding of continuous random variables, the values need to be\n  quantized first. Typically, it is beneficial for compression performance to\n  align the centers of the quantization bins such that one of them coincides\n  with the mode of the distribution. With `offset` being the mode of the\n  distribution, for instance, this can be achieved simply by computing:\n  ```\n  x_hat = tf.round(x - offset) + offset\n  ```\n\n  This method tries to determine the offset in a best-effort fashion, based on\n  which statistics the `Distribution` implements. First, a method\n  `self._quantization_offset()` is tried. If that isn\'t defined, it tries in\n  turn: `self.mode()`, `self.quantile(.5)`, then `self.mean()`. If none of\n  these are implemented, it falls back on quantizing to integer values (i.e.,\n  an offset of zero).\n\n  Arguments:\n    distribution: A `tfp.distributions.Distribution` object.\n\n  Returns:\n    A `tf.Tensor` broadcastable to shape `self.batch_shape`, containing\n    the determined quantization offsets. No gradients are allowed to flow\n    through the return value.\n  """"""\n  try:\n    offset = distribution._quantization_offset()  # pylint:disable=protected-access\n  except (AttributeError, NotImplementedError):\n    try:\n      offset = distribution.mode()\n    except NotImplementedError:\n      try:\n        offset = distribution.quantile(.5)\n      except NotImplementedError:\n        try:\n          offset = distribution.mean()\n        except NotImplementedError:\n          offset = tf.constant(0, dtype=distribution.dtype)\n  return tf.stop_gradient(offset)\n\n\ndef lower_tail(distribution, tail_mass):\n  """"""Approximates lower tail quantile for range coding.\n\n  For range coding of random variables, the distribution tails need special\n  handling, because range coding can only handle alphabets with a finite\n  number of symbols. This method returns a cut-off location for the lower\n  tail, such that approximately `tail_mass` probability mass is contained in\n  the tails (together). The tails are then handled by using the \'overflow\'\n  functionality of the range coder implementation (using a Golomb-like\n  universal code).\n\n  Arguments:\n    distribution: A `tfp.distributions.Distribution` object.\n    tail_mass: Float between 0 and 1. Desired probability mass for the tails.\n\n  Returns:\n    A `tf.Tensor` broadcastable to shape `self.batch_shape` containing the\n    approximate lower tail quantiles for each scalar distribution.\n  """"""\n  try:\n    tail = distribution._lower_tail(tail_mass)  # pylint:disable=protected-access\n  except (AttributeError, NotImplementedError):\n    try:\n      tail = distribution.quantile(tail_mass / 2)\n    except NotImplementedError:\n      try:\n        tail = estimate_tails(\n            distribution.log_cdf, tf.math.log(tail_mass / 2),\n            distribution.batch_shape_tensor(), distribution.dtype)\n      except NotImplementedError:\n        raise NotImplementedError(\n            ""`distribution` must implement `_lower_tail()`, `quantile()`, or ""\n            ""`log_cdf()` so that lower tail can be located."")\n  return tf.stop_gradient(tail)\n\n\ndef upper_tail(distribution, tail_mass):\n  """"""Approximates upper tail quantile for range coding.\n\n  For range coding of random variables, the distribution tails need special\n  handling, because range coding can only handle alphabets with a finite\n  number of symbols. This method returns a cut-off location for the upper\n  tail, such that approximately `tail_mass` probability mass is contained in\n  the tails (together). The tails are then handled by using the \'overflow\'\n  functionality of the range coder implementation (using a Golomb-like\n  universal code).\n\n  Arguments:\n    distribution: A `tfp.distributions.Distribution` object.\n    tail_mass: Float between 0 and 1. Desired probability mass for the tails.\n\n  Returns:\n    A `tf.Tensor` broadcastable to shape `self.batch_shape` containing the\n    approximate upper tail quantiles for each scalar distribution.\n  """"""\n  try:\n    tail = distribution._upper_tail(tail_mass)  # pylint:disable=protected-access\n  except (AttributeError, NotImplementedError):\n    try:\n      tail = distribution.quantile(1 - tail_mass / 2)\n    except NotImplementedError:\n      try:\n        tail = estimate_tails(\n            distribution.log_survival_function, tf.math.log(tail_mass / 2),\n            distribution.batch_shape_tensor(), distribution.dtype)\n      except NotImplementedError:\n        raise NotImplementedError(\n            ""`distribution` must implement `_upper_tail()`, `quantile()`, or ""\n            ""`log_survival_function()` so that upper tail can be located."")\n  return tf.stop_gradient(tail)\n'"
tensorflow_compression/python/distributions/helpers_test.py,3,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of uniform noise adapter distribution.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_compression.python.distributions import deep_factorized\nfrom tensorflow_compression.python.distributions import helpers\n\n\nclass HelpersTest(tf.test.TestCase):\n\n  def test_cauchy_quantizes_to_mode(self):\n    dist = tfp.distributions.Cauchy(loc=1.5, scale=3.)\n    self.assertEqual(helpers.quantization_offset(dist), 1.5)\n\n  def test_gamma_quantizes_to_mode(self):\n    dist = tfp.distributions.Gamma(concentration=5., rate=1.)\n    self.assertEqual(helpers.quantization_offset(dist), 4.)\n\n  def test_laplace_quantizes_to_mode(self):\n    dist = tfp.distributions.Laplace(loc=-2., scale=5.)\n    self.assertEqual(helpers.quantization_offset(dist), -2.)\n\n  def test_logistic_quantizes_to_mode(self):\n    dist = tfp.distributions.Logistic(loc=-3., scale=1.)\n    self.assertEqual(helpers.quantization_offset(dist), -3.)\n\n  def test_lognormal_quantizes_to_mode(self):\n    dist = tfp.distributions.LogNormal(loc=4., scale=1.)\n    self.assertEqual(helpers.quantization_offset(dist), tf.exp(3.))\n\n  def test_normal_quantizes_to_mode(self):\n    dist = tfp.distributions.Normal(loc=3., scale=5.)\n    self.assertEqual(helpers.quantization_offset(dist), 3.)\n\n  def test_cauchy_tails_are_in_order(self):\n    dist = tfp.distributions.Cauchy(loc=1.5, scale=3.)\n    self.assertGreater(\n        helpers.upper_tail(dist, 2**-8), helpers.lower_tail(dist, 2**-8))\n\n  def test_laplace_tails_are_in_order(self):\n    dist = tfp.distributions.Laplace(loc=-2., scale=5.)\n    self.assertGreater(\n        helpers.upper_tail(dist, 2**-8), helpers.lower_tail(dist, 2**-8))\n\n  def test_logistic_tails_are_in_order(self):\n    dist = tfp.distributions.Logistic(loc=-3., scale=1.)\n    self.assertGreater(\n        helpers.upper_tail(dist, 2**-8), helpers.lower_tail(dist, 2**-8))\n\n  def test_lognormal_tails_are_in_order(self):\n    dist = tfp.distributions.LogNormal(loc=4., scale=1.)\n    self.assertGreater(\n        helpers.upper_tail(dist, 2**-8), helpers.lower_tail(dist, 2**-8))\n\n  def test_normal_tails_are_in_order(self):\n    dist = tfp.distributions.Normal(loc=3., scale=5.)\n    self.assertGreater(\n        helpers.upper_tail(dist, 2**-8), helpers.lower_tail(dist, 2**-8))\n\n  def test_deep_factorized_tails_are_in_order(self):\n    dist = deep_factorized.DeepFactorized(batch_shape=[10])\n    self.assertAllGreater(\n        helpers.upper_tail(dist, 2**-8) - helpers.lower_tail(dist, 2**-8), 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/distributions/uniform_noise.py,15,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Uniform noise adapter distribution.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_compression.python.distributions import helpers\n\n\n__all__ = [\n    ""UniformNoiseAdapter"",\n    ""NoisyMixtureSameFamily"",\n    ""NoisyNormal"",\n    ""NoisyLogistic"",\n    ""NoisyNormalMixture"",\n    ""NoisyLogisticMixture"",\n]\n\n\ndef _logsum_expbig_minus_expsmall(big, small):\n  """"""Numerically stable evaluation of `log(exp(big) - exp(small))`.\n\n  This assumes `small <= big` and arguments that can be broadcast against each\n  other.\n\n  Arguments:\n    big: Floating-point `tf.Tensor`.\n    small: Floating-point `tf.Tensor`.\n\n  Returns:\n    `tf.Tensor` containing the result.\n  """"""\n  with tf.name_scope(""logsum_expbig_minus_expsmall""):\n    return tf.math.log1p(-tf.exp(small - big)) + big\n\n\nclass UniformNoiseAdapter(tfp.distributions.Distribution):\n  """"""Additive i.i.d. uniform noise adapter distribution.\n\n  Given a base `tfp.distributions.Distribution` object, this distribution\n  models the base distribution after addition of independent uniform noise.\n\n  Effectively, the base density function is convolved with a box kernel of width\n  one. The resulting density can be efficiently evaluated via the relation:\n  ```\n  (p * u)(x) = c(x + .5) - c(x - .5)\n  ```\n  where `p` and `u` are the base density and the unit-width uniform density,\n  respectively, and `c` is the cumulative distribution function (CDF)\n  corresponding to `p`. This is described in appendix 6.2 of the paper:\n\n  > ""Variational image compression with a scale hyperprior""<br />\n  > J. Ball\xc3\xa9, D. Minnen, S. Singh, S. J. Hwang, N. Johnston<br />\n  > https://openreview.net/forum?id=rkcQFMZRb\n\n  For best numerical stability, the base `Distribution` should implement both\n  `cdf()` and `survival_function()` and/or their `log_*` equivalents.\n  """"""\n\n  def __init__(self, base, name=""UniformNoiseAdapter""):\n    """"""Initializer.\n\n    Arguments:\n      base: A `tfp.distributions.Distribution` object representing a\n        continuous-valued random variable.\n      name: String. A name for this distribution.\n    """"""\n    parameters = dict(locals())\n    self._base = base\n    super().__init__(\n        dtype=base.dtype,\n        reparameterization_type=base.reparameterization_type,\n        validate_args=base.validate_args,\n        allow_nan_stats=base.allow_nan_stats,\n        parameters=parameters,\n        name=name,\n    )\n\n  @property\n  def base(self):\n    """"""The base distribution (without uniform noise).""""""\n    return self._base\n\n  def _batch_shape_tensor(self):\n    return self.base.batch_shape_tensor()\n\n  def _batch_shape(self):\n    return self.base.batch_shape\n\n  def _event_shape_tensor(self):\n    return self.base.event_shape_tensor()\n\n  def _event_shape(self):\n    return self.base.event_shape\n\n  def _sample_n(self, n, seed=None):\n    with tf.name_scope(""transform""):\n      n = tf.convert_to_tensor(n, name=""n"")\n      samples = self.base.sample(n, seed=seed)\n      return samples + tf.random.uniform(\n          tf.shape(samples), minval=-.5, maxval=.5, dtype=samples.dtype)\n\n  def _log_prob(self, y):\n    if not hasattr(self.base, ""_log_cdf""):\n      raise NotImplementedError(\n          ""`log_prob()` is not implemented unless the base distribution ""\n          ""implements `log_cdf()`."")\n    try:\n      return self._log_prob_with_logsf_and_logcdf(y)\n    except NotImplementedError:\n      return self._log_prob_with_logcdf(y)\n\n  def _log_prob_with_logcdf(self, y):\n    return _logsum_expbig_minus_expsmall(\n        self.base.log_cdf(y + .5), self.base.log_cdf(y - .5))\n\n  def _log_prob_with_logsf_and_logcdf(self, y):\n    """"""Compute log_prob(y) using log survival_function and cdf together.""""""\n    # There are two options that would be equal if we had infinite precision:\n    # Log[ sf(y - .5) - sf(y + .5) ]\n    #   = Log[ exp{logsf(y - .5)} - exp{logsf(y + .5)} ]\n    # Log[ cdf(y + .5) - cdf(y - .5) ]\n    #   = Log[ exp{logcdf(y + .5)} - exp{logcdf(y - .5)} ]\n    logsf_y_plus = self.base.log_survival_function(y + .5)\n    logsf_y_minus = self.base.log_survival_function(y - .5)\n    logcdf_y_plus = self.base.log_cdf(y + .5)\n    logcdf_y_minus = self.base.log_cdf(y - .5)\n\n    # Important:  Here we use select in a way such that no input is inf, this\n    # prevents the troublesome case where the output of select can be finite,\n    # but the output of grad(select) will be NaN.\n\n    # In either case, we are doing Log[ exp{big} - exp{small} ]\n    # We want to use the sf items precisely when we are on the right side of the\n    # median, which occurs when logsf_y < logcdf_y.\n    condition = logsf_y_plus < logcdf_y_plus\n    big = tf.where(condition, logsf_y_minus, logcdf_y_plus)\n    small = tf.where(condition, logsf_y_plus, logcdf_y_minus)\n    return _logsum_expbig_minus_expsmall(big, small)\n\n  def _prob(self, y):\n    if not hasattr(self.base, ""_cdf""):\n      raise NotImplementedError(\n          ""`prob()` is not implemented unless the base distribution implements ""\n          ""`cdf()`."")\n    try:\n      return self._prob_with_sf_and_cdf(y)\n    except NotImplementedError:\n      return self._prob_with_cdf(y)\n\n  def _prob_with_cdf(self, y):\n    return self.cdf(y + .5) - self.cdf(y - .5)\n\n  def _prob_with_sf_and_cdf(self, y):\n    # There are two options that would be equal if we had infinite precision:\n    # sf(y - .5) - sf(y + .5)\n    # cdf(y + .5) - cdf(y - .5)\n    sf_y_plus = self.base.survival_function(y + .5)\n    sf_y_minus = self.base.survival_function(y - .5)\n    cdf_y_plus = self.base.cdf(y + .5)\n    cdf_y_minus = self.base.cdf(y - .5)\n\n    # sf_prob has greater precision iff we\'re on the right side of the median.\n    return tf.where(\n        sf_y_plus < cdf_y_plus,\n        sf_y_minus - sf_y_plus, cdf_y_plus - cdf_y_minus)\n\n  def _mean(self):\n    return self.base.mean()\n\n  def _quantization_offset(self):\n    return helpers.quantization_offset(self.base)\n\n  def _lower_tail(self, tail_mass):\n    return helpers.lower_tail(self.base, tail_mass)\n\n  def _upper_tail(self, tail_mass):\n    return helpers.upper_tail(self.base, tail_mass)\n\n\nclass NoisyMixtureSameFamily(tfp.distributions.MixtureSameFamily):\n  """"""Mixture of distributions with additive i.i.d. uniform noise.""""""\n\n  def __init__(self, mixture_distribution, components_distribution,\n               name=""NoisyMixtureSameFamily""):\n    super().__init__(\n        mixture_distribution=mixture_distribution,\n        components_distribution=UniformNoiseAdapter(components_distribution),\n        name=name,\n    )\n    self._base = tfp.distributions.MixtureSameFamily(\n        mixture_distribution=mixture_distribution,\n        components_distribution=components_distribution,\n        name=name + ""Base"",\n    )\n\n  @property\n  def base(self):\n    """"""The base distribution (without uniform noise).""""""\n    return self._base\n\n  def _quantization_offset(self):\n    # Picks the ""peakiest"" of the component quantization offsets.\n    offsets = helpers.quantization_offset(self.components_distribution)\n    rank = self.batch_shape.rank\n    transposed_offsets = tf.transpose(offsets, [rank] + list(range(rank)))\n    component = tf.argmax(self.log_prob(transposed_offsets), axis=0)\n    return tf.gather(offsets, component, axis=-1, batch_dims=rank)\n\n  def _lower_tail(self, tail_mass):\n    return helpers.lower_tail(self.base, tail_mass)\n\n  def _upper_tail(self, tail_mass):\n    return helpers.upper_tail(self.base, tail_mass)\n\n\nclass NoisyNormal(UniformNoiseAdapter):\n  """"""Gaussian distribution with additive i.i.d. uniform noise.""""""\n\n  def __init__(self, name=""NoisyNormal"", **kwargs):\n    super().__init__(tfp.distributions.Normal(**kwargs), name=name)\n\n\nclass NoisyLogistic(UniformNoiseAdapter):\n  """"""Logistic distribution with additive i.i.d. uniform noise.""""""\n\n  def __init__(self, name=""NoisyLogistic"", **kwargs):\n    super().__init__(tfp.distributions.Logistic(**kwargs), name=name)\n\n\nclass NoisyNormalMixture(NoisyMixtureSameFamily):\n  """"""Mixture of normal distributions with additive i.i.d. uniform noise.""""""\n\n  def __init__(self, loc, scale, weight, name=""NoisyNormalMixture""):\n    super().__init__(\n        mixture_distribution=tfp.distributions.Categorical(probs=weight),\n        components_distribution=tfp.distributions.Normal(loc=loc, scale=scale),\n        name=name,\n    )\n\n\nclass NoisyLogisticMixture(NoisyMixtureSameFamily):\n  """"""Mixture of logistic distributions with additive i.i.d. uniform noise.""""""\n\n  def __init__(self, loc, scale, weight, name=""NoisyLogisticMixture""):\n    super().__init__(\n        mixture_distribution=tfp.distributions.Categorical(probs=weight),\n        components_distribution=tfp.distributions.Logistic(\n            loc=loc, scale=scale),\n        name=name,\n    )\n'"
tensorflow_compression/python/distributions/uniform_noise_test.py,21,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of uniform noise adapter distribution.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.distributions import helpers\nfrom tensorflow_compression.python.distributions import uniform_noise\n\n\nclass LocationScaleTest(object):\n  """"""Common tests for noisy location-scale family of distributions.""""""\n\n  def test_can_instantiate_scalar(self):\n    dist = self.dist_cls(loc=3., scale=5.)\n    self.assertEqual(dist.batch_shape, ())\n    self.assertEqual(dist.event_shape, ())\n\n  def test_can_instantiate_batched(self):\n    dist = self.dist_cls(loc=[3., 2.], scale=5.)\n    self.assertEqual(dist.batch_shape, (2,))\n    self.assertEqual(dist.event_shape, ())\n\n  def test_variables_receive_gradients(self):\n    loc = tf.Variable(1., dtype=tf.float32)\n    log_scale = tf.Variable(0., dtype=tf.float32)\n    with tf.GradientTape() as tape:\n      dist = self.dist_cls(loc=loc, scale=tf.exp(log_scale))\n      x = tf.random.normal([20])\n      loss = -tf.reduce_mean(dist.log_prob(x))\n    grads = tape.gradient(loss, [loc, log_scale])\n    self.assertLen(grads, 2)\n    self.assertNotIn(None, grads)\n\n  def test_uniform_is_special_case(self):\n    # With the scale parameter going to zero, the adapted distribution should\n    # approach a unit-width uniform distribution. As a side effect, this tests\n    # that `mean()` is defined (because not for all distributions the mean\n    # coincides with the location parameter).\n    dist = self.dist_cls(loc=10, scale=1e-7)\n    mean = dist.mean()\n    x = tf.linspace(mean - 1, mean + 1, 10)\n    self.assertAllClose(dist.prob(x), [0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\n\n  def test_sampling_works(self):\n    dist = self.dist_cls(loc=0, scale=[3, 5])\n    sample = dist.sample((5, 4))\n    self.assertEqual(sample.shape, (5, 4, 2))\n\n  def test_tails_and_offset_are_in_order(self):\n    dist = self.dist_cls(loc=10, scale=1.5)\n    offset = helpers.quantization_offset(dist)\n    lower_tail = helpers.lower_tail(dist, 2**-8)\n    upper_tail = helpers.upper_tail(dist, 2**-8)\n    self.assertGreater(upper_tail, offset)\n    self.assertGreater(offset, lower_tail)\n\n  def test_stats_throw_error(self):\n    dist = self.dist_cls(loc=1, scale=2)\n    with self.assertRaises(NotImplementedError):\n      dist.mode()\n    with self.assertRaises(NotImplementedError):\n      dist.quantile(.5)\n    with self.assertRaises(NotImplementedError):\n      dist.survival_function(.5)\n\n\nclass NoisyNormalTest(LocationScaleTest, tf.test.TestCase):\n\n  dist_cls = uniform_noise.NoisyNormal\n\n\nclass NoisyLogisticTest(LocationScaleTest, tf.test.TestCase):\n\n  dist_cls = uniform_noise.NoisyLogistic\n\n\nclass MixtureTest(object):\n  """"""Common tests for noisy mixture distributions.""""""\n\n  def test_can_instantiate_scalar(self):\n    dist = self.dist_cls(loc=[3., -3.], scale=[5., 2.5], weight=[.3, .7])\n    self.assertEqual(dist.batch_shape, ())\n    self.assertEqual(dist.event_shape, ())\n\n  def test_can_instantiate_batched(self):\n    dist = self.dist_cls(\n        loc=[[3., -3.], [2., -2.]], scale=[5., 2.5], weight=[.3, .7])\n    self.assertEqual(dist.batch_shape, (2,))\n    self.assertEqual(dist.event_shape, ())\n\n  def test_variables_receive_gradients(self):\n    loc = tf.Variable(tf.ones([2], dtype=tf.float32))\n    log_scale = tf.Variable(tf.zeros([2], dtype=tf.float32))\n    logit_weight = tf.Variable(tf.constant([.3, .7], dtype=tf.float32))\n    with tf.GradientTape() as tape:\n      dist = self.dist_cls(\n          loc=loc, scale=tf.exp(log_scale), weight=tf.nn.softmax(logit_weight))\n      x = tf.random.normal([20])\n      loss = -tf.reduce_mean(dist.log_prob(x))\n    grads = tape.gradient(loss, [loc, log_scale, logit_weight])\n    self.assertLen(grads, 3)\n    self.assertNotIn(None, grads)\n\n  def test_uniform_is_special_case(self):\n    # With the scale parameters going to zero, the adapted distribution should\n    # approach a mixture of unit-width uniform distributions.\n    dist = self.dist_cls(loc=[2.5, -1.], scale=[1e-7, 1e-7], weight=[.3, .7])\n    mean = dist.components_distribution.mean()\n    x = tf.linspace(mean[0] - 1, mean[0] + 1, 10)\n    self.assertAllClose(dist.prob(x), [0, 0, 0, .3, .3, .3, .3, 0, 0, 0])\n    x = tf.linspace(mean[1] - 1, mean[1] + 1, 10)\n    self.assertAllClose(dist.prob(x), [0, 0, 0, .7, .7, .7, .7, 0, 0, 0])\n\n  def test_sampling_works(self):\n    dist = self.dist_cls(loc=[[0]], scale=[3, 5], weight=[.2, .8])\n    sample = dist.sample((5, 4))\n    self.assertEqual(sample.shape, (5, 4, 1))\n\n  def test_tails_and_offset_are_in_order(self):\n    dist = self.dist_cls(loc=10, scale=[1.5, 2], weight=[.5, .5])\n    offset = helpers.quantization_offset(dist)\n    lower_tail = helpers.lower_tail(dist, 2**-8)\n    upper_tail = helpers.upper_tail(dist, 2**-8)\n    self.assertGreater(upper_tail, offset)\n    self.assertGreater(offset, lower_tail)\n\n  def test_stats_throw_error(self):\n    dist = self.dist_cls(loc=[1, 0], scale=2, weight=[.1, .9])\n    with self.assertRaises(NotImplementedError):\n      dist.mode()\n    with self.assertRaises(NotImplementedError):\n      dist.quantile(.5)\n    with self.assertRaises(NotImplementedError):\n      dist.survival_function(.5)\n\n\nclass NoisyNormalMixtureTest(MixtureTest, tf.test.TestCase):\n\n  dist_cls = uniform_noise.NoisyNormalMixture\n\n\nclass NoisyLogisticMixtureTest(MixtureTest, tf.test.TestCase):\n\n  dist_cls = uniform_noise.NoisyLogisticMixture\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/entropy_models/__init__.py,0,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/entropy_models/continuous_base.py,38,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base class for continuous entropy models.""""""\n\nimport abc\nimport functools\n\nfrom absl import logging\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.distributions import helpers\nfrom tensorflow_compression.python.ops import range_coding_ops\n\n\n__all__ = [""ContinuousEntropyModelBase""]\n\n\nclass ContinuousEntropyModelBase(tf.Module, metaclass=abc.ABCMeta):\n  """"""Base class for continuous entropy models.\n\n  The basic functionality of this class is to pre-compute integer probability\n  tables based on the provided `tfp.distributions.Distribution` object, which\n  can then be used reliably across different platforms by the range coder and\n  decoder.\n  """"""\n\n  @abc.abstractmethod\n  def __init__(self, prior, coding_rank, compression=False,\n               likelihood_bound=1e-9, tail_mass=2**-8,\n               range_coder_precision=12):\n    """"""Initializer.\n\n    Arguments:\n      prior: A `tfp.distributions.Distribution` object. A density model fitting\n        the marginal distribution of the bottleneck data with additive uniform\n        noise, which is shared a priori between the sender and the receiver. For\n        best results, the distribution should be flexible enough to have a\n        unit-width uniform distribution as a special case, since this is the\n        marginal distribution for bottleneck dimensions that are constant.\n      coding_rank: Integer. Number of innermost dimensions considered a coding\n        unit. Each coding unit is compressed to its own bit string, and the\n        `bits()` method sums over each coding unit.\n      compression: Boolean. If set to `True`, the range coding tables used by\n        `compress()` and `decompress()` will be built on instantiation. If set\n        to `False`, these two methods will not be accessible.\n      likelihood_bound: Float. Lower bound for likelihood values, to prevent\n        training instabilities.\n      tail_mass: Float. Approximate probability mass which is range encoded with\n        less precision, by using a Golomb-like code.\n      range_coder_precision: Integer. Precision passed to the range coding op.\n\n    Raises:\n      RuntimeError: when attempting to instantiate an entropy model with\n        `compression=True` and not in eager execution mode.\n    """"""\n    if prior.event_shape.rank:\n      raise ValueError(""`prior` must be a (batch of) scalar distribution(s)."")\n    super().__init__()\n    with self.name_scope:\n      self._prior = prior\n      self._dtype = tf.as_dtype(prior.dtype)\n      self._prior_shape = tuple(int(s) for s in prior.batch_shape)\n      self._coding_rank = int(coding_rank)\n      self._compression = bool(compression)\n      self._likelihood_bound = float(likelihood_bound)\n      self._tail_mass = float(tail_mass)\n      self._range_coder_precision = int(range_coder_precision)\n      if self.compression:\n        self._build_tables(prior)\n\n  @property\n  def prior(self):\n    """"""Prior distribution, used for range coding.""""""\n    if not hasattr(self, ""_prior""):\n      raise RuntimeError(\n          ""This entropy model doesn\'t hold a reference to its prior ""\n          ""distribution. This can happen when it is unserialized, because ""\n          ""the prior is generally not serializable."")\n    return self._prior\n\n  def _check_compression(self):\n    if not self.compression:\n      raise RuntimeError(\n          ""For range coding, the entropy model must be instantiated with ""\n          ""`compression=True`."")\n\n  @property\n  def cdf(self):\n    self._check_compression()\n    return self._cdf.value()\n\n  @property\n  def cdf_offset(self):\n    self._check_compression()\n    return self._cdf_offset.value()\n\n  @property\n  def cdf_length(self):\n    self._check_compression()\n    return self._cdf_length.value()\n\n  @property\n  def dtype(self):\n    """"""Data type of this entropy model.""""""\n    return self._dtype\n\n  @property\n  def prior_shape(self):\n    """"""Batch shape of `prior` (dimensions which are not assumed i.i.d.).""""""\n    return self._prior_shape\n\n  @property\n  def coding_rank(self):\n    """"""Number of innermost dimensions considered a coding unit.""""""\n    return self._coding_rank\n\n  @property\n  def compression(self):\n    """"""Whether this entropy model is prepared for compression.""""""\n    return self._compression\n\n  @property\n  def likelihood_bound(self):\n    """"""Lower bound for likelihood values.""""""\n    return self._likelihood_bound\n\n  @property\n  def tail_mass(self):\n    """"""Approximate probability mass which is range encoded with overflow.""""""\n    return self._tail_mass\n\n  @property\n  def range_coder_precision(self):\n    """"""Precision passed to range coding op.""""""\n    return self._range_coder_precision\n\n  @tf.custom_gradient\n  def _quantize_no_offset(self, inputs):\n    return tf.round(inputs), lambda x: x\n\n  @tf.custom_gradient\n  def _quantize_offset(self, inputs, offset):\n    return tf.round(inputs - offset) + offset, lambda x: (x, None)\n\n  def _quantize(self, inputs, offset=None):\n    if offset is None:\n      outputs = self._quantize_no_offset(inputs)\n    else:\n      outputs = self._quantize_offset(inputs, offset)\n    return outputs\n\n  def _build_tables(self, prior):\n    """"""Computes integer-valued probability tables used by the range coder.\n\n    These tables must not be re-generated independently on the sending and\n    receiving side, since small numerical discrepancies between both sides can\n    occur in this process. If the tables differ slightly, this in turn would\n    very likely cause catastrophic error propagation during range decoding. For\n    a more in-depth discussion of this, see:\n\n    > ""Integer Networks for Data Compression with Latent-Variable Models""<br />\n    > J. Ball\xc3\xa9, N. Johnston, D. Minnen<br />\n    > https://openreview.net/forum?id=S1zz2i0cY7\n\n    The tables are stored in `tf.Variable`s as attributes of this object. The\n    recommended way is to train the model, instantiate an entropy model with\n    `compression=True`, and then distribute the model to a sender and a\n    receiver.\n\n    Arguments:\n      prior: The `tfp.distributions.Distribution` object (see initializer).\n    """"""\n    offset = helpers.quantization_offset(prior)\n    lower_tail = helpers.lower_tail(prior, self.tail_mass)\n    upper_tail = helpers.upper_tail(prior, self.tail_mass)\n\n    # Largest distance observed between lower tail and median, and between\n    # median and upper tail.\n    minima = offset - lower_tail\n    minima = tf.cast(tf.math.ceil(minima), tf.int32)\n    minima = tf.math.maximum(minima, 0)\n    maxima = upper_tail - offset\n    maxima = tf.cast(tf.math.ceil(maxima), tf.int32)\n    maxima = tf.math.maximum(maxima, 0)\n\n    # PMF starting positions and lengths.\n    pmf_start = offset - tf.cast(minima, self.dtype)\n    pmf_length = maxima + minima + 1\n\n    # Sample the densities in the computed ranges, possibly computing more\n    # samples than necessary at the upper end.\n    max_length = tf.math.reduce_max(pmf_length)\n    if tf.executing_eagerly() and max_length > 2048:\n      logging.warning(\n          ""Very wide PMF with %d elements may lead to out of memory issues. ""\n          ""Consider priors with smaller dispersion or increasing `tail_mass` ""\n          ""parameter."", int(max_length))\n    samples = tf.range(tf.cast(max_length, self.dtype), dtype=self.dtype)\n    samples = tf.reshape(samples, [-1] + len(self.prior_shape) * [1])\n    samples += pmf_start\n    pmf = prior.prob(samples)\n\n    # Collapse batch dimensions of distribution.\n    pmf = tf.reshape(pmf, [max_length, -1])\n    pmf = tf.transpose(pmf)\n\n    pmf_length = tf.broadcast_to(pmf_length, self.prior_shape)\n    pmf_length = tf.reshape(pmf_length, [-1])\n    cdf_length = pmf_length + 2\n    cdf_offset = tf.broadcast_to(-minima, self.prior_shape)\n    cdf_offset = tf.reshape(cdf_offset, [-1])\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      def loop_body(args):\n        prob, length = args\n        prob = prob[:length]\n        prob = tf.concat([prob, 1 - tf.reduce_sum(prob, keepdims=True)], axis=0)\n        cdf = range_coding_ops.pmf_to_quantized_cdf(\n            prob, precision=self.range_coder_precision)\n        return tf.pad(\n            cdf, [[0, max_length - length]], mode=""CONSTANT"", constant_values=0)\n\n      # TODO(jonycgn,ssjhv): Consider switching to Python control flow.\n      cdf = tf.map_fn(\n          loop_body, (pmf, pmf_length), dtype=tf.int32, name=""pmf_to_cdf"")\n\n    self._cdf = tf.Variable(cdf, trainable=False, name=""cdf"")\n    self._cdf_offset = tf.Variable(\n        cdf_offset, trainable=False, name=""cdf_offset"")\n    self._cdf_length = tf.Variable(\n        cdf_length, trainable=False, name=""cdf_length"")\n\n  @abc.abstractmethod\n  def get_config(self):\n    """"""Returns the configuration of the entropy model.\n\n    Returns:\n      A JSON-serializable Python dict.\n\n    Raises:\n      NotImplementedError: on attempting to call this method on an entropy model\n        with `compression=False`.\n    """"""\n    if not self.compression:\n      raise NotImplementedError(\n          ""Serializing entropy models with `compression=False` is currently ""\n          ""not supported."")\n    return dict(\n        dtype=self._dtype.name,\n        prior_shape=self._prior_shape,\n        coding_rank=self._coding_rank,\n        likelihood_bound=self._likelihood_bound,\n        tail_mass=self._tail_mass,\n        range_coder_precision=self._range_coder_precision,\n        cdf_width=self._cdf.shape.as_list()[1],\n    )\n\n  @classmethod\n  @abc.abstractmethod\n  def from_config(cls, config):\n    """"""Instantiates an entropy model from a configuration dictionary.\n\n    Arguments:\n      config: A `dict`, typically the output of `get_config`.\n\n    Returns:\n      An entropy model.\n    """"""\n    # Instantiate new object without calling initializers, and call superclass\n    # (tf.Module) initializer manually. Note: `cls` is child class of this one.\n    self = cls.__new__(cls)  # pylint:disable=no-value-for-parameter\n    super().__init__(self)\n\n    # What follows is the alternative initializer.\n    with self.name_scope:\n      # pylint:disable=protected-access\n      self._dtype = tf.as_dtype(config[""dtype""])\n      self._prior_shape = tuple(int(s) for s in config[""prior_shape""])\n      self._coding_rank = int(config[""coding_rank""])\n      self._compression = True\n      self._likelihood_bound = float(config[""likelihood_bound""])\n      self._tail_mass = float(config[""tail_mass""])\n      self._range_coder_precision = int(config[""range_coder_precision""])\n\n      prior_size = functools.reduce(lambda x, y: x * y, self.prior_shape, 1)\n      cdf_width = int(config[""cdf_width""])\n      zeros = tf.zeros([prior_size, cdf_width], dtype=tf.int32)\n      self._cdf = tf.Variable(zeros, trainable=False, name=""cdf"")\n      self._cdf_offset = tf.Variable(\n          zeros[:, 0], trainable=False, name=""cdf_offset"")\n      self._cdf_length = tf.Variable(\n          zeros[:, 0], trainable=False, name=""cdf_length"")\n      # pylint:enable=protected-access\n\n    return self\n\n  def get_weights(self):\n    return tf.keras.backend.batch_get_value(self.variables)\n\n  def set_weights(self, weights):\n    if len(weights) != len(self.variables):\n      raise ValueError(\n          ""`set_weights` expects a list of {} arrays, received {}.""\n          """".format(len(self.variables), len(weights)))\n    tf.keras.backend.batch_set_value(zip(self.variables, weights))\n'"
tensorflow_compression/python/entropy_models/continuous_batched.py,45,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Batched entropy model for continuous random variables.""""""\n\nimport functools\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.distributions import helpers\nfrom tensorflow_compression.python.entropy_models import continuous_base\nfrom tensorflow_compression.python.ops import math_ops\nfrom tensorflow_compression.python.ops import range_coding_ops\n\n\n__all__ = [\n    ""ContinuousBatchedEntropyModel"",\n]\n\n\n@tf.keras.utils.register_keras_serializable(package=""tensorflow_compression"")\nclass ContinuousBatchedEntropyModel(continuous_base.ContinuousEntropyModelBase):\n  """"""Batched entropy model for continuous random variables.\n\n  This entropy model handles quantization of a bottleneck tensor and helps with\n  training of the parameters of the probability distribution modeling the\n  tensor (a shared ""prior"" between sender and receiver). It also pre-computes\n  integer probability tables, which can then be used to compress and decompress\n  bottleneck tensors reliably across different platforms.\n\n  A typical workflow looks like this:\n\n  - Train a model using an instance of this entropy model as a bottleneck,\n    passing the bottleneck tensor through `quantize()` while optimizing\n    compressibility of the tensor using `bits()`. `bits(training=True)` computes\n    a differentiable upper bound on the number of bits needed to compress the\n    bottleneck tensor.\n  - For evaluation, get a closer estimate of the number of compressed bits\n    using `bits(training=False)`.\n  - Instantiate an entropy model with `compression=True` (and the same\n    parameters as during training), and share the model between a sender and a\n    receiver.\n  - On the sender side, compute the bottleneck tensor and call `compress()` on\n    it. The output is a compressed string representation of the tensor. Transmit\n    the string to the receiver, and call `decompress()` there. The output is the\n    quantized bottleneck tensor. Continue processing the tensor on the receiving\n    side.\n\n  This class assumes that all scalar elements of the encoded tensor are\n  statistically independent, and that the parameters of their scalar\n  distributions do not depend on data. The innermost dimensions of the\n  bottleneck tensor must be broadcastable to the batch shape of `prior`. Any\n  dimensions to the left of the batch shape are assumed to be i.i.d., i.e. the\n  likelihoods are broadcast to the bottleneck tensor accordingly.\n\n  A more detailed description (and motivation) of this way of performing\n  quantization and range coding can be found in the following paper. Please cite\n  the paper when using this code for derivative work.\n\n  > ""End-to-end Optimized Image Compression""<br />\n  > J. Ball\xc3\xa9, V. Laparra, E.P. Simoncelli<br />\n  > https://openreview.net/forum?id=rJxdQ3jeg\n  """"""\n\n  def __init__(self, prior, coding_rank, compression=False,\n               likelihood_bound=1e-9, tail_mass=2**-8,\n               range_coder_precision=12):\n    """"""Initializer.\n\n    Arguments:\n      prior: A `tfp.distributions.Distribution` object. A density model fitting\n        the marginal distribution of the bottleneck data with additive uniform\n        noise, which is shared a priori between the sender and the receiver. For\n        best results, the distribution should be flexible enough to have a\n        unit-width uniform distribution as a special case, since this is the\n        marginal distribution for bottleneck dimensions that are constant. The\n        distribution parameters may not depend on data (they must be either\n        variables or constants).\n      coding_rank: Integer. Number of innermost dimensions considered a coding\n        unit. Each coding unit is compressed to its own bit string, and the\n        `bits()` method sums over each coding unit.\n      compression: Boolean. If set to `True`, the range coding tables used by\n        `compress()` and `decompress()` will be built on instantiation. If set\n        to `False`, these two methods will not be accessible.\n      likelihood_bound: Float. Lower bound for likelihood values, to prevent\n        training instabilities.\n      tail_mass: Float. Approximate probability mass which is range encoded with\n        less precision, by using a Golomb-like code.\n      range_coder_precision: Integer. Precision passed to the range coding op.\n\n    Raises:\n      RuntimeError: when attempting to instantiate an entropy model with\n        `compression=True` and not in eager execution mode.\n    """"""\n    if coding_rank < prior.batch_shape.rank:\n      raise ValueError(\n          ""`coding_rank` can\'t be smaller than batch rank of prior."")\n    super().__init__(\n        prior, coding_rank, compression=compression,\n        likelihood_bound=likelihood_bound, tail_mass=tail_mass,\n        range_coder_precision=range_coder_precision)\n\n    quantization_offset = helpers.quantization_offset(prior)\n    if self.compression:\n      # Optimization: if the quantization offset is zero, we don\'t need to\n      # subtract/add it when quantizing, and we don\'t need to serialize its\n      # value. Note that this code will only work in eager mode.\n      # TODO(jonycgn): Reconsider if this optimization is worth keeping once\n      # the implementation is stable.\n      if tf.executing_eagerly() and tf.reduce_all(\n          tf.equal(quantization_offset, 0.)):\n        quantization_offset = None\n      else:\n        quantization_offset = tf.broadcast_to(\n            quantization_offset, self.prior_shape)\n        quantization_offset = tf.Variable(\n            quantization_offset, trainable=False, name=""quantization_offset"")\n    self._quantization_offset = quantization_offset\n\n  def _compute_indexes(self, broadcast_shape):\n    # TODO(jonycgn, ssjhv): Investigate broadcasting in range coding op.\n    prior_size = functools.reduce(lambda x, y: x * y, self.prior_shape, 1)\n    indexes = tf.range(prior_size, dtype=tf.int32)\n    indexes = tf.reshape(indexes, self.prior_shape)\n    indexes = tf.broadcast_to(\n        indexes, tf.concat([broadcast_shape, self.prior_shape], 0))\n    return indexes\n\n  @tf.Module.with_name_scope\n  def bits(self, bottleneck, training=True):\n    """"""Estimates the number of bits needed to compress a tensor.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed. Must have at\n        least `self.coding_rank` dimensions, and the innermost dimensions must\n        be broadcastable to `self.prior_shape`.\n      training: Boolean. If `False`, computes the Shannon information of\n        `bottleneck` under the distribution `self.prior`, which is a\n        non-differentiable, tight *lower* bound on the number of bits needed to\n        compress `bottleneck` using `compress()`. If `True`, returns a somewhat\n        looser, but differentiable *upper* bound on this quantity.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing the number of bits.\n    """"""\n    if training:\n      quantized = bottleneck + tf.random.uniform(\n          tf.shape(bottleneck), minval=-.5, maxval=.5, dtype=bottleneck.dtype)\n    else:\n      quantized = self.quantize(bottleneck)\n    probs = self.prior.prob(quantized)\n    probs = math_ops.lower_bound(probs, self.likelihood_bound)\n    axes = tuple(range(-self.coding_rank, 0))\n    bits = tf.reduce_sum(tf.math.log(probs), axis=axes) / -tf.math.log(2.)\n    return bits\n\n  @tf.Module.with_name_scope\n  def quantize(self, bottleneck):\n    """"""Quantizes a floating-point tensor.\n\n    To use this entropy model as an information bottleneck during training, pass\n    a tensor through this function. The tensor is rounded to integer values\n    shifted by an offset, which depends on `self.prior`. For instance, for a\n    Gaussian distribution, the returned values are rounded to the location of\n    the mode of the distribution plus or minus an integer.\n\n    The gradient of this rounding operation is overridden with the identity\n    (straight-through gradient estimator).\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be quantized. The innermost\n        dimensions must be broadcastable to `self.prior_shape`.\n\n    Returns:\n      A `tf.Tensor` containing the quantized values.\n    """"""\n    return self._quantize(bottleneck, self._quantization_offset)\n\n  @tf.Module.with_name_scope\n  def compress(self, bottleneck):\n    """"""Compresses a floating-point tensor.\n\n    Compresses the tensor to bit strings. `bottleneck` is first quantized\n    as in `quantize()`, and then compressed using the probability tables derived\n    from `self.prior`. The quantized tensor can later be recovered by\n    calling `decompress()`.\n\n    The innermost `self.coding_rank` dimensions are treated as one coding unit,\n    i.e. are compressed into one string each. Any additional dimensions to the\n    left are treated as batch dimensions.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed. Must have at\n        least `self.coding_rank` dimensions, and the innermost dimensions must\n        be broadcastable to `self.prior_shape`.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing a string for each\n      coding unit.\n    """"""\n    input_shape = tf.shape(bottleneck)\n    input_rank = tf.shape(input_shape)[0]\n    batch_shape, coding_shape = tf.split(\n        input_shape, [input_rank - self.coding_rank, self.coding_rank])\n    broadcast_shape = coding_shape[\n        :self.coding_rank - len(self.prior_shape)]\n\n    indexes = self._compute_indexes(broadcast_shape)\n    if self._quantization_offset is not None:\n      bottleneck -= self._quantization_offset\n    symbols = tf.cast(tf.round(bottleneck), tf.int32)\n    symbols = tf.reshape(symbols, tf.concat([[-1], coding_shape], 0))\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      cdf = self.cdf\n      cdf_length = self.cdf_length\n      cdf_offset = self.cdf_offset\n      def loop_body(symbols):\n        return range_coding_ops.unbounded_index_range_encode(\n            symbols, indexes, cdf, cdf_length, cdf_offset,\n            precision=self.range_coder_precision,\n            overflow_width=4, debug_level=1)\n\n      # TODO(jonycgn,ssjhv): Consider switching to Python control flow.\n      strings = tf.map_fn(\n          loop_body, symbols, dtype=tf.string, name=""compress"")\n\n    strings = tf.reshape(strings, batch_shape)\n    return strings\n\n  @tf.Module.with_name_scope\n  def decompress(self, strings, broadcast_shape):\n    """"""Decompresses a tensor.\n\n    Reconstructs the quantized tensor from bit strings produced by `compress()`.\n    It is necessary to provide a part of the output shape in `broadcast_shape`.\n\n    Arguments:\n      strings: `tf.Tensor` containing the compressed bit strings.\n      broadcast_shape: Iterable of ints. The part of the output tensor shape\n        between the shape of `strings` on the left and\n        `self.prior_shape` on the right. This must match the shape\n        of the input to `compress()`.\n\n    Returns:\n      A `tf.Tensor` of shape `strings.shape + broadcast_shape +\n      self.prior_shape`.\n    """"""\n    strings = tf.convert_to_tensor(strings, dtype=tf.string)\n    broadcast_shape = tf.convert_to_tensor(broadcast_shape, dtype=tf.int32)\n    batch_shape = tf.shape(strings)\n    symbols_shape = tf.concat(\n        [batch_shape, broadcast_shape, self.prior_shape], 0)\n\n    indexes = self._compute_indexes(broadcast_shape)\n    strings = tf.reshape(strings, [-1])\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      cdf = self.cdf\n      cdf_length = self.cdf_length\n      cdf_offset = self.cdf_offset\n      def loop_body(string):\n        return range_coding_ops.unbounded_index_range_decode(\n            string, indexes, cdf, cdf_length, cdf_offset,\n            precision=self.range_coder_precision,\n            overflow_width=4, debug_level=1)\n\n      # TODO(jonycgn,ssjhv): Consider switching to Python control flow.\n      symbols = tf.map_fn(\n          loop_body, strings, dtype=tf.int32, name=""decompress"")\n\n    symbols = tf.reshape(symbols, symbols_shape)\n    outputs = tf.cast(symbols, self.dtype)\n    if self._quantization_offset is not None:\n      outputs += self._quantization_offset\n    return outputs\n\n  def get_config(self):\n    """"""Returns the configuration of the entropy model.\n\n    Returns:\n      A JSON-serializable Python dict.\n\n    Raises:\n      NotImplementedError: on attempting to call this method on an entropy model\n        with `compression=False`.\n    """"""\n    config = super().get_config()\n    config.update(\n        quantization_offset=self._quantization_offset is not None,\n    )\n    return config\n\n  @classmethod\n  def from_config(cls, config):\n    """"""Instantiates an entropy model from a configuration dictionary.\n\n    Arguments:\n      config: A `dict`, typically the output of `get_config`.\n\n    Returns:\n      An entropy model.\n    """"""\n    self = super().from_config(config)\n    with self.name_scope:\n      # pylint:disable=protected-access\n      if config[""quantization_offset""]:\n        zeros = tf.zeros(self.prior_shape, dtype=self.dtype)\n        self._quantization_offset = tf.Variable(\n            zeros, name=""quantization_offset"")\n      else:\n        self._quantization_offset = None\n      # pylint:enable=protected-access\n    return self\n'"
tensorflow_compression/python/entropy_models/continuous_batched_test.py,17,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of batched continuous entropy model.""""""\n\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\n\nfrom tensorflow_compression.python.distributions import uniform_noise\nfrom tensorflow_compression.python.entropy_models.continuous_batched import ContinuousBatchedEntropyModel\n\n\nclass ContinuousBatchedEntropyModelTest(tf.test.TestCase):\n\n  def test_can_instantiate(self):\n    noisy = uniform_noise.NoisyNormal(loc=0., scale=1.)\n    em = ContinuousBatchedEntropyModel(noisy, 1)\n    self.assertIs(em.prior, noisy)\n    self.assertEqual(em.coding_rank, 1)\n    self.assertEqual(em.likelihood_bound, 1e-9)\n    self.assertEqual(em.tail_mass, 2**-8)\n    self.assertEqual(em.range_coder_precision, 12)\n    self.assertEqual(em.dtype, noisy.dtype)\n\n  def test_requires_scalar_distributions(self):\n    noisy = uniform_noise.UniformNoiseAdapter(\n        tfp.distributions.MultivariateNormalDiag(\n            loc=[-3, .2], scale_diag=[1, 2]))\n    with self.assertRaises(ValueError):\n      ContinuousBatchedEntropyModel(noisy, 1)\n\n  def test_requires_coding_rank_bigger_than_prior_batch_rank(self):\n    noisy = uniform_noise.NoisyLogistic(loc=0, scale=[[1], [2]])\n    with self.assertRaises(ValueError):\n      ContinuousBatchedEntropyModel(noisy, 0)\n    with self.assertRaises(ValueError):\n      ContinuousBatchedEntropyModel(noisy, 1)\n    ContinuousBatchedEntropyModel(noisy, 2)\n    ContinuousBatchedEntropyModel(noisy, 3)\n\n  def test_quantizes_to_integers_modulo_offset(self):\n    noisy = uniform_noise.NoisyNormal(loc=.25, scale=10.)\n    em = ContinuousBatchedEntropyModel(noisy, 1)\n    x = tf.range(-20., 20.) + .25\n    x_perturbed = x + tf.random.uniform(x.shape, -.49, .49)\n    x_quantized = em.quantize(x_perturbed)\n    self.assertAllEqual(x, x_quantized)\n\n  def test_gradients_are_straight_through(self):\n    noisy = uniform_noise.NoisyNormal(loc=0, scale=1)\n    em = ContinuousBatchedEntropyModel(noisy, 1)\n    x = tf.range(-20., 20.)\n    x_perturbed = x + tf.random.uniform(x.shape, -.49, .49)\n    with tf.GradientTape() as tape:\n      tape.watch(x_perturbed)\n      x_quantized = em.quantize(x_perturbed)\n    gradients = tape.gradient(x_quantized, x_perturbed)\n    self.assertAllEqual(gradients, tf.ones_like(gradients))\n\n  def test_default_kwargs_throw_error_on_compression(self):\n    noisy = uniform_noise.NoisyNormal(loc=.25, scale=10.)\n    em = ContinuousBatchedEntropyModel(noisy, 1)\n    x = tf.zeros(10)\n    with self.assertRaises(RuntimeError):\n      em.compress(x)\n    s = tf.zeros(10, dtype=tf.string)\n    with self.assertRaises(RuntimeError):\n      em.decompress(s, [10])\n\n  def test_compression_consistent_with_quantization(self):\n    noisy = uniform_noise.NoisyNormal(loc=.25, scale=10.)\n    em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n    x = noisy.base.sample([100])\n    x_quantized = em.quantize(x)\n    x_decompressed = em.decompress(em.compress(x), [100])\n    self.assertAllEqual(x_decompressed, x_quantized)\n\n  def test_information_bounds(self):\n    # `bits(training=True)` should be greater than `bits(training=False)`\n    # because it is defined as an upper bound (albeit for infinite data). The\n    # actual length of the bit string should always be greater than\n    # `bits(training=False)` because range coding is only asymptotically\n    # optimal, and because it operates on quantized probabilities.\n    for scale in 2 ** tf.linspace(-2., 7., 10):\n      noisy = uniform_noise.NoisyNormal(loc=0., scale=scale)\n      em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n      x = noisy.base.sample([10000])\n      bits_eval = em.bits(x, training=False)\n      bits_training = em.bits(x, training=True)\n      bits_compressed = 8 * len(em.compress(x).numpy())\n      self.assertGreater(bits_training, .9975 * bits_eval)\n      self.assertGreater(bits_compressed, bits_eval)\n\n  def test_low_entropy_bounds(self):\n    # For low entropy distributions, the training bound should be very loose,\n    # and the overhead of range coding manageable.\n    noisy = uniform_noise.NoisyNormal(loc=0., scale=.25)\n    em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n    x = noisy.base.sample([10000])\n    bits_eval = em.bits(x, training=False)\n    bits_training = em.bits(x, training=True)\n    bits_compressed = 8 * len(em.compress(x).numpy())\n    self.assertAllClose(bits_training, bits_eval, atol=0, rtol=1.25)\n    self.assertAllClose(bits_compressed, bits_eval, atol=0, rtol=5e-3)\n\n  def test_high_entropy_bounds(self):\n    # For high entropy distributions, the training bound should be very tight,\n    # and the overhead of range coding manageable.\n    noisy = uniform_noise.NoisyNormal(loc=0., scale=100.)\n    em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n    x = noisy.base.sample([10000])\n    bits_eval = em.bits(x, training=False)\n    bits_training = em.bits(x, training=True)\n    bits_compressed = 8 * len(em.compress(x).numpy())\n    self.assertAllClose(bits_training, bits_eval, atol=0, rtol=5e-5)\n    self.assertAllClose(bits_compressed, bits_eval, atol=0, rtol=5e-3)\n\n  def test_compression_works_after_serialization(self):\n    noisy = uniform_noise.NoisyNormal(loc=.5, scale=8.)\n    em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n    self.assertIsNot(em._quantization_offset, None)\n    json = tf.keras.utils.serialize_keras_object(em)\n    weights = em.get_weights()\n    x = noisy.base.sample([100])\n    x_quantized = em.quantize(x)\n    x_compressed = em.compress(x)\n    em = tf.keras.utils.deserialize_keras_object(json)\n    em.set_weights(weights)\n    self.assertAllEqual(em.compress(x), x_compressed)\n    self.assertAllEqual(em.decompress(x_compressed, [100]), x_quantized)\n\n  def test_compression_works_after_serialization_no_offset(self):\n    noisy = uniform_noise.NoisyNormal(loc=0, scale=5.)\n    em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n    self.assertIs(em._quantization_offset, None)\n    json = tf.keras.utils.serialize_keras_object(em)\n    weights = em.get_weights()\n    x = noisy.base.sample([100])\n    x_quantized = em.quantize(x)\n    x_compressed = em.compress(x)\n    em = tf.keras.utils.deserialize_keras_object(json)\n    em.set_weights(weights)\n    self.assertAllEqual(em.compress(x), x_compressed)\n    self.assertAllEqual(em.decompress(x_compressed, [100]), x_quantized)\n\n  def test_compression_works_in_tf_function(self):\n    noisy = uniform_noise.NoisyNormal(loc=0, scale=5.)\n    sample = noisy.base.sample([100])\n\n    # Since tf.function traces each function twice, and only allows variable\n    # creation in the first call, we need to have a stateful object in which we\n    # create the entropy model only the first time the function is called, and\n    # store it for the second time.\n\n    class Compressor(object):\n\n      def compress(self, values):\n        if not hasattr(self, ""em""):\n          self.em = ContinuousBatchedEntropyModel(noisy, 1, compression=True)\n        compressed = self.em.compress(values)\n        decompressed = self.em.decompress(compressed, [])\n        return decompressed\n\n    values_eager = Compressor().compress(sample)\n    values_function = tf.function(Compressor().compress)(sample)\n    self.assertAllEqual(values_eager, values_function)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/entropy_models/continuous_indexed.py,74,"b'# Lint as: python3\n# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Indexed entropy model for continuous random variables.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.distributions import helpers\nfrom tensorflow_compression.python.entropy_models import continuous_base\nfrom tensorflow_compression.python.ops import math_ops\nfrom tensorflow_compression.python.ops import range_coding_ops\n\n\n__all__ = [\n    ""ContinuousIndexedEntropyModel"",\n    ""LocationScaleIndexedEntropyModel"",\n]\n\n\nclass ContinuousIndexedEntropyModel(continuous_base.ContinuousEntropyModelBase):\n  """"""Indexed entropy model for continuous random variables.\n\n  This entropy model handles quantization of a bottleneck tensor and helps with\n  training of the parameters of the probability distribution modeling the\n  tensor (a shared ""prior"" between sender and receiver). It also pre-computes\n  integer probability tables, which can then be used to compress and decompress\n  bottleneck tensors reliably across different platforms.\n\n  A typical workflow looks like this:\n\n  - Train a model using an instance of this entropy model as a bottleneck,\n    passing the bottleneck tensor through `quantize()` while optimizing\n    compressibility of the tensor using `bits()`. `bits(training=True)` computes\n    a differentiable upper bound on the number of bits needed to compress the\n    bottleneck tensor.\n  - For evaluation, get a closer estimate of the number of compressed bits\n    using `bits(training=False)`.\n  - Instantiate an entropy model with `compression=True` (and the same\n    parameters as during training), and share the model between a sender and a\n    receiver.\n  - On the sender side, compute the bottleneck tensor and call `compress()` on\n    it. The output is a compressed string representation of the tensor. Transmit\n    the string to the receiver, and call `decompress()` there. The output is the\n    quantized bottleneck tensor. Continue processing the tensor on the receiving\n    side.\n\n  This class assumes that all scalar elements of the encoded tensor are\n  conditionally independent given some other random variable, possibly depending\n  on data. All dependencies must be represented by the `indexes` tensor. For\n  each bottleneck tensor element, it selects the appropriate scalar\n  distribution.\n\n  The `indexes` tensor must contain only integer values (but may have\n  floating-point type for purposes of backpropagation) in a pre-specified range.\n  If `index_ranges` is a single integer, the index values must be in the range\n  `[0, index_ranges)` and `indexes` must have the same shape as the bottleneck\n  tensor. This only allows a one-dimensional conditional dependency. To make the\n  distribution conditional on `n`-dimensional indexes, `index_ranges` must be\n  specified as an iterable of `n` integers. Then, `indexes` must have the same\n  shape as the bottleneck tensor with an additional channel dimension of length\n  `n`. The position of the channel dimension is given by `channel_axis`. The\n  index values in the `n`th channel must be in the range `[0, index_ranges[n])`.\n\n  The implied distribution for the bottleneck tensor is determined as:\n  ```\n  prior_fn(**{k: f(indexes) for k, f in parameter_fns.items()})\n  ```\n\n  A more detailed description (and motivation) of this indexing scheme can be\n  found in the following paper. Please cite the paper when using this code for\n  derivative work.\n\n  > ""Integer Networks for Data Compression with Latent-Variable Models""<br />\n  > J. Ball\xc3\xa9, N. Johnston, D. Minnen<br />\n  > https://openreview.net/forum?id=S1zz2i0cY7\n\n  Examples:\n\n  To make a parameterized zero-mean normal distribution, one could use:\n  ```\n  tfc.ContinuousIndexedEntropyModel(\n      prior_fn=tfc.NoisyNormal,\n      index_ranges=64,\n      parameter_fns=dict(\n          loc=lambda _: 0.,\n          scale=lambda i: tf.exp(i / 8 - 5),\n      ),\n      coding_rank=1,\n  )\n  ```\n  Then, each element of `indexes` in the range `[0, 64)` would indicate that the\n  corresponding element in `bottleneck` is normally distributed with zero mean\n  and a standard deviation between `exp(-5)` and `exp(2.875)`, inclusive.\n\n  To make a parameterized logistic mixture distribution, one could use:\n  ```\n  tfc.ContinuousIndexedEntropyModel(\n      prior_fn=tfc.NoisyLogisticMixture,\n      index_ranges=(10, 10, 5),\n      parameter_fns=dict(\n          loc=lambda i: i[..., 0:2] - 5,\n          scale=lambda _: 1,\n          weight=lambda i: tf.nn.softmax((i[..., 2:3] - 2) * [-1, 1]),\n      ),\n      coding_rank=1,\n      channel_axis=-1,\n  )\n  ```\n  Then, the last dimension of `indexes` would consist of triples of elements in\n  the ranges `[0, 10)`, `[0, 10)`, and `[0, 5)`, respectively. Each triple\n  would indicate that the element in `bottleneck` corresponding to the other\n  dimensions is distributed with a mixture of two logistic distributions, where\n  the components each have one of 10 location parameters between `-5` and `+5`,\n  inclusive, unit scale parameters, and one of five different mixture\n  weightings.\n  """"""\n\n  def __init__(self, prior_fn, index_ranges, parameter_fns, coding_rank,\n               compression=False, channel_axis=-1, dtype=tf.float32,\n               likelihood_bound=1e-9, tail_mass=2**-8,\n               range_coder_precision=12):\n    """"""Initializer.\n\n    Arguments:\n      prior_fn: A callable returning a `tfp.distributions.Distribution` object,\n        typically a `Distribution` class or factory function. This is a density\n        model fitting the marginal distribution of the bottleneck data with\n        additive uniform noise, which is shared a priori between the sender and\n        the receiver. For best results, the distributions should be flexible\n        enough to have a unit-width uniform distribution as a special case,\n        since this is the marginal distribution for bottleneck dimensions that\n        are constant. The callable will receive keyword arguments as determined\n        by `parameter_fns`.\n      index_ranges: Integer or iterable of integers. If a single integer,\n        `indexes` must have the same shape as `bottleneck`, and `channel_axis`\n        is ignored. Its values must be in the range `[0, index_ranges)`. If an\n        iterable of integers, `indexes` must have an additional dimension at\n        position `channel_axis`, and the values of the `n`th channel must be in\n        the range `[0, index_ranges[n])`.\n      parameter_fns: Dict of strings to callables. Functions mapping `indexes`\n        to each distribution parameter. For each item, `indexes` is passed to\n        the callable, and the string key and return value make up one keyword\n        argument to `prior_fn`.\n      coding_rank: Integer. Number of innermost dimensions considered a coding\n        unit. Each coding unit is compressed to its own bit string, and the\n        `bits()` method sums over each coding unit.\n      compression: Boolean. If set to `True`, the range coding tables used by\n        `compress()` and `decompress()` will be built on instantiation. This\n        assumes eager mode (throws an error if in graph mode or inside a\n        `tf.function` call). If set to `False`, these two methods will not be\n        accessible.\n      channel_axis: Integer. For iterable `index_ranges`, determines the\n        position of the channel axis in `indexes`. Defaults to the last\n        dimension.\n      dtype: `tf.dtypes.DType`. The data type of all floating-point\n        computations carried out in this class.\n      likelihood_bound: Float. Lower bound for likelihood values, to prevent\n        training instabilities.\n      tail_mass: Float. Approximate probability mass which is range encoded with\n        less precision, by using a Golomb-like code.\n      range_coder_precision: Integer. Precision passed to the range coding op.\n\n    Raises:\n      RuntimeError: when attempting to instantiate an entropy model with\n        `compression=True` and not in eager execution mode.\n    """"""\n    if coding_rank <= 0:\n      raise ValueError(""`coding_rank` must be larger than 0."")\n\n    self._prior_fn = prior_fn\n    if not callable(self.prior_fn):\n      raise TypeError(""`prior_fn` must be a class or factory function."")\n    try:\n      self._index_ranges = int(index_ranges)\n    except TypeError:\n      self._index_ranges = tuple(int(r) for r in index_ranges)  # pytype:disable=attribute-error\n    self._parameter_fns = dict(parameter_fns)\n    for name, fn in self.parameter_fns.items():\n      if not isinstance(name, str):\n        raise TypeError(""`parameter_fns` must have string keys."")\n      if not callable(fn):\n        raise TypeError(""`parameter_fns[\'{}\']` must be callable."".format(name))\n    self._channel_axis = int(channel_axis)\n    dtype = tf.as_dtype(dtype)\n\n    if isinstance(self.index_ranges, int):\n      indexes = tf.range(self.index_ranges, dtype=dtype)\n    else:\n      indexes = [tf.range(r, dtype=dtype) for r in self.index_ranges]\n      indexes = tf.meshgrid(*indexes, indexing=""ij"")\n      indexes = tf.stack(indexes, axis=self.channel_axis)\n    parameters = {k: f(indexes) for k, f in self.parameter_fns.items()}\n    prior = self.prior_fn(**parameters)  # pylint:disable=not-callable\n\n    super().__init__(\n        prior, coding_rank, compression=compression,\n        likelihood_bound=likelihood_bound, tail_mass=tail_mass,\n        range_coder_precision=range_coder_precision)\n\n  @property\n  def index_ranges(self):\n    """"""Upper bound(s) on values allowed in `indexes` tensor.""""""\n    return self._index_ranges\n\n  @property\n  def parameter_fns(self):\n    """"""Functions mapping `indexes` to each distribution parameter.""""""\n    return self._parameter_fns\n\n  @property\n  def prior_fn(self):\n    """"""Class or factory function returning a `Distribution` object.""""""\n    return self._prior_fn\n\n  @property\n  def channel_axis(self):\n    """"""Position of channel axis in `indexes` tensor.""""""\n    return self._channel_axis\n\n  def _make_prior(self, indexes):\n    indexes = tf.cast(indexes, self.dtype)\n    parameters = {k: f(indexes) for k, f in self.parameter_fns.items()}\n    return self.prior_fn(**parameters)  # pylint:disable=not-callable\n\n  def _normalize_indexes(self, indexes):\n    indexes = math_ops.lower_bound(indexes, 0)\n    if isinstance(self.index_ranges, int):\n      indexes = math_ops.upper_bound(indexes, self.index_ranges - 1)\n    else:\n      axes = [1] * indexes.shape.rank\n      axes[self.channel_axis] = len(self.index_ranges)\n      bounds = tf.reshape([s - 1 for s in self.index_ranges], axes)\n      indexes = math_ops.upper_bound(indexes, bounds)\n    return indexes\n\n  def _flatten_indexes(self, indexes):\n    indexes = tf.cast(indexes, tf.int32)\n    if isinstance(self.index_ranges, int):\n      return indexes\n    else:\n      strides = tf.cumprod(self.index_ranges, exclusive=True, reverse=True)\n      return tf.linalg.tensordot(indexes, strides, [[self.channel_axis], [0]])\n\n  @tf.Module.with_name_scope\n  def bits(self, bottleneck, indexes, training=True):\n    """"""Estimates the number of bits needed to compress a tensor.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed.\n      indexes: `tf.Tensor` specifying the scalar distribution for each element\n        in `bottleneck`. See class docstring for examples.\n      training: Boolean. If `False`, computes the Shannon information of\n        `bottleneck` under the distribution computed by `self.prior_fn`,\n        which is a non-differentiable, tight *lower* bound on the number of bits\n        needed to compress `bottleneck` using `compress()`. If `True`, returns a\n        somewhat looser, but differentiable *upper* bound on this quantity.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing the number of bits.\n    """"""\n    indexes = self._normalize_indexes(indexes)\n    prior = self._make_prior(indexes)\n    if training:\n      quantized = bottleneck + tf.random.uniform(\n          tf.shape(bottleneck), minval=-.5, maxval=.5, dtype=bottleneck.dtype)\n    else:\n      offset = helpers.quantization_offset(prior)\n      quantized = self._quantize(bottleneck, offset)\n    probs = prior.prob(quantized)\n    probs = math_ops.lower_bound(probs, self.likelihood_bound)\n    axes = tuple(range(-self.coding_rank, 0))\n    bits = tf.reduce_sum(tf.math.log(probs), axis=axes) / -tf.math.log(2.)\n    return bits\n\n  @tf.Module.with_name_scope\n  def quantize(self, bottleneck, indexes):\n    """"""Quantizes a floating-point tensor.\n\n    To use this entropy model as an information bottleneck during training, pass\n    a tensor through this function. The tensor is rounded to integer values\n    modulo a quantization offset, which depends on `indexes`. For instance, for\n    Gaussian distributions, the returned values are rounded to the location of\n    the mode of the distributions plus or minus an integer.\n\n    The gradient of this rounding operation is overridden with the identity\n    (straight-through gradient estimator).\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be quantized.\n      indexes: `tf.Tensor` specifying the scalar distribution for each element\n        in `bottleneck`. See class docstring for examples.\n\n    Returns:\n      A `tf.Tensor` containing the quantized values.\n    """"""\n    indexes = self._normalize_indexes(indexes)\n    offset = helpers.quantization_offset(self._make_prior(indexes))\n    return self._quantize(bottleneck, offset)\n\n  @tf.Module.with_name_scope\n  def compress(self, bottleneck, indexes):\n    """"""Compresses a floating-point tensor.\n\n    Compresses the tensor to bit strings. `bottleneck` is first quantized\n    as in `quantize()`, and then compressed using the probability tables derived\n    from `indexes`. The quantized tensor can later be recovered by calling\n    `decompress()`.\n\n    The innermost `self.coding_rank` dimensions are treated as one coding unit,\n    i.e. are compressed into one string each. Any additional dimensions to the\n    left are treated as batch dimensions.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed.\n      indexes: `tf.Tensor` specifying the scalar distribution for each element\n        in `bottleneck`. See class docstring for examples.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing a string for each\n      coding unit.\n    """"""\n    indexes = self._normalize_indexes(indexes)\n    flat_indexes = self._flatten_indexes(indexes)\n\n    symbols_shape = tf.shape(flat_indexes)\n    batch_shape = symbols_shape[:-self.coding_rank]\n    flat_shape = tf.concat([[-1], symbols_shape[-self.coding_rank:]], 0)\n\n    flat_indexes = tf.reshape(flat_indexes, flat_shape)\n\n    offset = helpers.quantization_offset(self._make_prior(indexes))\n    symbols = tf.cast(tf.round(bottleneck - offset), tf.int32)\n    symbols = tf.reshape(symbols, flat_shape)\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      cdf = self.cdf\n      cdf_length = self.cdf_length\n      cdf_offset = self.cdf_offset\n      def loop_body(args):\n        return range_coding_ops.unbounded_index_range_encode(\n            args[0], args[1], cdf, cdf_length, cdf_offset,\n            precision=self.range_coder_precision,\n            overflow_width=4, debug_level=1)\n\n      # TODO(jonycgn,ssjhv): Consider switching to Python control flow.\n      strings = tf.map_fn(\n          loop_body, (symbols, flat_indexes), dtype=tf.string, name=""compress"")\n\n    strings = tf.reshape(strings, batch_shape)\n    return strings\n\n  @tf.Module.with_name_scope\n  def decompress(self, strings, indexes):\n    """"""Decompresses a tensor.\n\n    Reconstructs the quantized tensor from bit strings produced by `compress()`.\n\n    Arguments:\n      strings: `tf.Tensor` containing the compressed bit strings.\n      indexes: `tf.Tensor` specifying the scalar distribution for each output\n        element. See class docstring for examples.\n\n    Returns:\n      A `tf.Tensor` of the same shape as `indexes` (without the optional channel\n      dimension).\n    """"""\n    indexes = self._normalize_indexes(indexes)\n    flat_indexes = self._flatten_indexes(indexes)\n\n    symbols_shape = tf.shape(flat_indexes)\n    flat_shape = tf.concat([[-1], symbols_shape[-self.coding_rank:]], 0)\n\n    flat_indexes = tf.reshape(flat_indexes, flat_shape)\n\n    strings = tf.reshape(strings, [-1])\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      cdf = self.cdf\n      cdf_length = self.cdf_length\n      cdf_offset = self.cdf_offset\n      def loop_body(args):\n        return range_coding_ops.unbounded_index_range_decode(\n            args[0], args[1], cdf, cdf_length, cdf_offset,\n            precision=self.range_coder_precision,\n            overflow_width=4, debug_level=1)\n\n      # TODO(jonycgn,ssjhv): Consider switching to Python control flow.\n      symbols = tf.map_fn(\n          loop_body, (strings, flat_indexes), dtype=tf.int32, name=""decompress"")\n\n    symbols = tf.reshape(symbols, symbols_shape)\n    offset = helpers.quantization_offset(self._make_prior(indexes))\n    return tf.cast(symbols, self.dtype) + offset\n\n  def get_config(self):\n    """"""Returns the configuration of the entropy model.""""""\n    raise NotImplementedError(\n        ""Serializing indexed entropy models is currently not supported."")\n\n  @classmethod\n  def from_config(cls, config):\n    """"""Instantiates an entropy model from a configuration dictionary.""""""\n    raise NotImplementedError(\n        ""Serializing indexed entropy models is currently not supported."")\n\n\nclass LocationScaleIndexedEntropyModel(ContinuousIndexedEntropyModel):\n  """"""Indexed entropy model for location-scale family of random variables.\n\n  This class is a common special case of `ContinuousIndexedEntropyModel`. The\n  specified distribution is parameterized with `num_scales` values of scale\n  parameters. An element-wise location parameter is handled by shifting the\n  distributions to zero. Note: this only works for shift-invariant\n  distributions, where the `loc` parameter really denotes a translation (i.e.,\n  not for the log-normal distribution).\n  """"""\n\n  def __init__(self, prior_fn, num_scales, scale_fn, coding_rank,\n               compression=False, dtype=tf.float32, likelihood_bound=1e-9,\n               tail_mass=2**-8, range_coder_precision=12):\n    """"""Initializer.\n\n    Arguments:\n      prior_fn: A callable returning a `tfp.distributions.Distribution` object,\n        typically a `Distribution` class or factory function. This is a density\n        model fitting the marginal distribution of the bottleneck data with\n        additive uniform noise, which is shared a priori between the sender and\n        the receiver. For best results, the distributions should be flexible\n        enough to have a unit-width uniform distribution as a special case,\n        since this is the marginal distribution for bottleneck dimensions that\n        are constant. The callable will receive keyword arguments as determined\n        by `parameter_fns`.\n      num_scales: Integer. Values in `indexes` must be in the range\n        `[0, num_scales)`.\n      scale_fn: Callable. `indexes` is passed to the callable, and the return\n        value is given as `scale` keyword argument to `prior_fn`.\n      coding_rank: Integer. Number of innermost dimensions considered a coding\n        unit. Each coding unit is compressed to its own bit string, and the\n        `bits()` method sums over each coding unit.\n      compression: Boolean. If set to `True`, the range coding tables used by\n        `compress()` and `decompress()` will be built on instantiation.\n        Otherwise, some computation can be saved, but these two methods will not\n        be accessible.\n      dtype: `tf.dtypes.DType`. The data type of all floating-point\n        computations carried out in this class.\n      likelihood_bound: Float. Lower bound for likelihood values, to prevent\n        training instabilities.\n      tail_mass: Float. Approximate probability mass which is range encoded with\n        less precision, by using a Golomb-like code.\n      range_coder_precision: Integer. Precision passed to the range coding op.\n    """"""\n    num_scales = int(num_scales)\n    super().__init__(\n        prior_fn=prior_fn,\n        index_ranges=num_scales,\n        parameter_fns=dict(\n            loc=lambda _: 0.,\n            scale=scale_fn,\n        ),\n        coding_rank=coding_rank,\n        compression=compression,\n        dtype=dtype,\n        likelihood_bound=likelihood_bound,\n        tail_mass=tail_mass,\n        range_coder_precision=range_coder_precision,\n    )\n\n  @tf.Module.with_name_scope\n  def bits(self, bottleneck, scale_indexes, loc=None, training=True):\n    """"""Estimates the number of bits needed to compress a tensor.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed.\n      scale_indexes: `tf.Tensor` indexing the scale parameter for each element\n        in `bottleneck`. Must have the same shape as `bottleneck`.\n      loc: `None` or `tf.Tensor`. If `None`, the location parameter for all\n        elements is assumed to be zero. Otherwise, specifies the location\n        parameter for each element in `bottleneck`. Must have the same shape as\n        `bottleneck`.\n      training: Boolean. If `False`, computes the Shannon information of\n        `bottleneck` under the distribution computed by `self.prior_fn`,\n        which is a non-differentiable, tight *lower* bound on the number of bits\n        needed to compress `bottleneck` using `compress()`. If `True`, returns a\n        somewhat looser, but differentiable *upper* bound on this quantity.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing the number of bits.\n    """"""\n    if loc is not None:\n      bottleneck -= loc\n    return super().bits(bottleneck, scale_indexes, training=training)\n\n  @tf.Module.with_name_scope\n  def quantize(self, bottleneck, scale_indexes, loc=None):\n    """"""Quantizes a floating-point tensor.\n\n    To use this entropy model as an information bottleneck during training, pass\n    a tensor through this function. The tensor is rounded to integer values\n    modulo a quantization offset, which depends on `indexes`. For instance, for\n    Gaussian distributions, the returned values are rounded to the location of\n    the mode of the distributions plus or minus an integer.\n\n    The gradient of this rounding operation is overridden with the identity\n    (straight-through gradient estimator).\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be quantized.\n      scale_indexes: `tf.Tensor` indexing the scale parameter for each element\n        in `bottleneck`. Must have the same shape as `bottleneck`.\n      loc: `None` or `tf.Tensor`. If `None`, the location parameter for all\n        elements is assumed to be zero. Otherwise, specifies the location\n        parameter for each element in `bottleneck`. Must have the same shape as\n        `bottleneck`.\n\n    Returns:\n      A `tf.Tensor` containing the quantized values.\n    """"""\n    if loc is None:\n      return super().quantize(bottleneck, scale_indexes)\n    else:\n      return super().quantize(bottleneck - loc, scale_indexes) + loc\n\n  @tf.Module.with_name_scope\n  def compress(self, bottleneck, scale_indexes, loc=None):\n    """"""Compresses a floating-point tensor.\n\n    Compresses the tensor to bit strings. `bottleneck` is first quantized\n    as in `quantize()`, and then compressed using the probability tables derived\n    from `indexes`. The quantized tensor can later be recovered by calling\n    `decompress()`.\n\n    The innermost `self.coding_rank` dimensions are treated as one coding unit,\n    i.e. are compressed into one string each. Any additional dimensions to the\n    left are treated as batch dimensions.\n\n    Arguments:\n      bottleneck: `tf.Tensor` containing the data to be compressed.\n      scale_indexes: `tf.Tensor` indexing the scale parameter for each element\n        in `bottleneck`. Must have the same shape as `bottleneck`.\n      loc: `None` or `tf.Tensor`. If `None`, the location parameter for all\n        elements is assumed to be zero. Otherwise, specifies the location\n        parameter for each element in `bottleneck`. Must have the same shape as\n        `bottleneck`.\n\n    Returns:\n      A `tf.Tensor` having the same shape as `bottleneck` without the\n      `self.coding_rank` innermost dimensions, containing a string for each\n      coding unit.\n    """"""\n    if loc is not None:\n      bottleneck -= loc\n    return super().compress(bottleneck, scale_indexes)\n\n  @tf.Module.with_name_scope\n  def decompress(self, strings, scale_indexes, loc=None):\n    """"""Decompresses a tensor.\n\n    Reconstructs the quantized tensor from bit strings produced by `compress()`.\n\n    Arguments:\n      strings: `tf.Tensor` containing the compressed bit strings.\n      scale_indexes: `tf.Tensor` indexing the scale parameter for each output\n        element.\n      loc: `None` or `tf.Tensor`. If `None`, the location parameter for all\n        output elements is assumed to be zero. Otherwise, specifies the location\n        parameter for each output element. Must have the same shape as\n        `scale_indexes`.\n\n    Returns:\n      A `tf.Tensor` of the same shape as `scale_indexes`.\n    """"""\n    values = super().decompress(strings, scale_indexes)\n    if loc is not None:\n      values += loc\n    return values\n'"
tensorflow_compression/python/entropy_models/continuous_indexed_test.py,9,"b'# Copyright 2020 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of indexed continuous entropy model.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.distributions import uniform_noise\nfrom tensorflow_compression.python.entropy_models import continuous_indexed\n\n\n# TODO(jonycgn): add further unit tests.\n\n\nclass ContinuousIndexedEntropyModelTest(tf.test.TestCase):\n\n  def test_can_instantiate_one_dimensional(self):\n    em = continuous_indexed.ContinuousIndexedEntropyModel(\n        uniform_noise.NoisyNormal, 64,\n        dict(loc=lambda _: 0, scale=lambda i: tf.exp(i / 8 - 5)), 1)\n    self.assertIsInstance(em.prior, uniform_noise.NoisyNormal)\n    self.assertEqual(em.coding_rank, 1)\n    self.assertEqual(em.likelihood_bound, 1e-9)\n    self.assertEqual(em.tail_mass, 2**-8)\n    self.assertEqual(em.range_coder_precision, 12)\n    self.assertEqual(em.dtype, tf.float32)\n\n  def test_can_instantiate_n_dimensional(self):\n    em = continuous_indexed.ContinuousIndexedEntropyModel(\n        uniform_noise.NoisyLogisticMixture,\n        (10, 10, 5),\n        dict(\n            loc=lambda i: i[..., 0:2] - 5,\n            scale=lambda _: 1,\n            weight=lambda i: tf.nn.softmax((i[..., 2:3] - 2) * [-1, 1]),\n        ),\n        1,\n    )\n    self.assertIsInstance(em.prior, uniform_noise.NoisyLogisticMixture)\n    self.assertEqual(em.coding_rank, 1)\n    self.assertEqual(em.channel_axis, -1)\n    self.assertEqual(em.likelihood_bound, 1e-9)\n    self.assertEqual(em.tail_mass, 2**-8)\n    self.assertEqual(em.range_coder_precision, 12)\n    self.assertEqual(em.dtype, tf.float32)\n\n\nclass LocationScaleIndexedEntropyModelTest(tf.test.TestCase):\n\n  def test_can_instantiate(self):\n    em = continuous_indexed.LocationScaleIndexedEntropyModel(\n        uniform_noise.NoisyNormal, 64, lambda i: tf.exp(i / 8 - 5), 1)\n    self.assertIsInstance(em.prior, uniform_noise.NoisyNormal)\n    self.assertEqual(em.coding_rank, 1)\n    self.assertEqual(em.likelihood_bound, 1e-9)\n    self.assertEqual(em.tail_mass, 2**-8)\n    self.assertEqual(em.range_coder_precision, 12)\n    self.assertEqual(em.dtype, tf.float32)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/layers/__init__.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/layers/entropy_models.py,130,"b'# Lint as: python3\n# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Entropy model layers.""""""\n\nimport numpy as np\nimport scipy.stats\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.keras.engine import input_spec\nfrom tensorflow_compression.python.ops import math_ops\nfrom tensorflow_compression.python.ops import range_coding_ops\n\n\n__all__ = [\n    ""EntropyModel"",\n    ""EntropyBottleneck"",\n    ""SymmetricConditional"",\n    ""GaussianConditional"",\n    ""LogisticConditional"",\n    ""LaplacianConditional"",\n]\n\n\nclass EntropyModel(tf.keras.layers.Layer):\n  """"""Entropy model (base class).""""""\n\n  _setattr_tracking = False\n\n  def __init__(self, tail_mass=2 ** -8, likelihood_bound=1e-9,\n               range_coder_precision=16, **kwargs):\n    """"""Initializer.\n\n    Arguments:\n      tail_mass: Float, between 0 and 1. The bottleneck layer automatically\n        determines the range of input values based on their frequency of\n        occurrence. Values occurring in the tails of the distributions will not\n        be encoded with range coding, but using a Golomb-like code. `tail_mass`\n        determines the amount of probability mass in the tails which will be\n        Golomb-coded. For example, the default value of `2 ** -8` means that on\n        average, one 256th of all values will use the Golomb code.\n      likelihood_bound: Float. If positive, the returned likelihood values are\n        ensured to be greater than or equal to this value. This prevents very\n        large gradients with a typical entropy loss (defaults to 1e-9).\n      range_coder_precision: Integer, between 1 and 16. The precision of the\n        range coder used for compression and decompression. This trades off\n        computation speed with compression efficiency, where 16 is the slowest\n        but most efficient setting. Choosing lower values may increase the\n        average codelength slightly compared to the estimated entropies.\n      **kwargs: Other keyword arguments passed to superclass (`Layer`).\n    """"""\n    super(EntropyModel, self).__init__(**kwargs)\n    self._tail_mass = float(tail_mass)\n    if not 0 < self.tail_mass < 1:\n      raise ValueError(\n          ""`tail_mass` must be between 0 and 1, got {}."".format(self.tail_mass))\n    self._likelihood_bound = float(likelihood_bound)\n    self._range_coder_precision = int(range_coder_precision)\n    if tf.executing_eagerly():\n      raise NotImplementedError(\n          ""Keras layer implementations of entropy models are not supported in ""\n          ""eager mode."")\n\n  @property\n  def tail_mass(self):\n    return self._tail_mass\n\n  @property\n  def likelihood_bound(self):\n    return self._likelihood_bound\n\n  @property\n  def range_coder_precision(self):\n    return self._range_coder_precision\n\n  def _quantize(self, inputs, mode):\n    """"""Perturb or quantize a `Tensor` and optionally dequantize.\n\n    Arguments:\n      inputs: `Tensor`. The input values.\n      mode: String. Can take on one of three values: `\'noise\'` (adds uniform\n        noise), `\'dequantize\'` (quantizes and dequantizes), and `\'symbols\'`\n        (quantizes and produces integer symbols for range coder).\n\n    Returns:\n      The quantized/perturbed `inputs`. The returned `Tensor` should have type\n      `self.dtype` if mode is `\'noise\'`, `\'dequantize\'`; `tf.int32` if mode is\n      `\'symbols\'`.\n    """"""\n    raise NotImplementedError(""Must inherit from EntropyModel."")\n\n  def _dequantize(self, inputs, mode):\n    """"""Dequantize a `Tensor`.\n\n    The opposite to `_quantize(inputs, mode=\'symbols\')`.\n\n    Arguments:\n      inputs: `Tensor`. The range coder symbols.\n      mode: String. Must be `\'dequantize\'`.\n\n    Returns:\n      The dequantized `inputs`. The returned `Tensor` should have type\n      `self.dtype`.\n    """"""\n    raise NotImplementedError(""Must inherit from EntropyModel."")\n\n  def _likelihood(self, inputs):\n    """"""Compute the likelihood of the inputs under the model.\n\n    Arguments:\n      inputs: `Tensor`. The input values.\n\n    Returns:\n      `Tensor` of same shape and type as `inputs`, giving the likelihoods\n      evaluated at `inputs`.\n    """"""\n    raise NotImplementedError(""Must inherit from EntropyModel."")\n\n  def _pmf_to_cdf(self, pmf, tail_mass, pmf_length, max_length):\n    """"""Helper function for computing the CDF from the PMF.""""""\n\n    # Prevent tensors from bouncing back and forth between host and GPU.\n    with tf.device(""/cpu:0""):\n      def loop_body(args):\n        prob, length, tail = args\n        prob = tf.concat([prob[:length], tail], axis=0)\n        cdf = range_coding_ops.pmf_to_quantized_cdf(\n            prob, precision=self.range_coder_precision)\n        return tf.pad(\n            cdf, [[0, max_length - length]], mode=""CONSTANT"", constant_values=0)\n\n      return tf.map_fn(\n          loop_body, (pmf, pmf_length, tail_mass),\n          dtype=tf.int32, back_prop=False, name=""pmf_to_cdf"")\n\n  def call(self, inputs, training):\n    """"""Pass a tensor through the bottleneck.\n\n    Arguments:\n      inputs: The tensor to be passed through the bottleneck.\n      training: Boolean. If `True`, returns a differentiable approximation of\n        the inputs, and their likelihoods under the modeled probability\n        densities. If `False`, returns the quantized inputs and their\n        likelihoods under the corresponding probability mass function. These\n        quantities can\'t be used for training, as they are not differentiable,\n        but represent actual compression more closely.\n\n    Returns:\n      values: `Tensor` with the same shape as `inputs` containing the perturbed\n        or quantized input values.\n      likelihood: `Tensor` with the same shape as `inputs` containing the\n        likelihood of `values` under the modeled probability distributions.\n\n    Raises:\n      ValueError: if `inputs` has an integral or inconsistent `DType`, or\n        inconsistent number of channels.\n    """"""\n    inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n    if inputs.dtype.is_integer:\n      raise ValueError(\n          ""{} can\'t take integer inputs."".format(type(self).__name__))\n\n    outputs = self._quantize(inputs, ""noise"" if training else ""dequantize"")\n    assert outputs.dtype == self.dtype\n    likelihood = self._likelihood(outputs)\n    if self.likelihood_bound > 0:\n      likelihood_bound = tf.constant(self.likelihood_bound, dtype=self.dtype)\n      likelihood = math_ops.lower_bound(likelihood, likelihood_bound)\n\n    if not tf.executing_eagerly():\n      outputs_shape, likelihood_shape = self.compute_output_shape(inputs.shape)\n      outputs.set_shape(outputs_shape)\n      likelihood.set_shape(likelihood_shape)\n\n    return outputs, likelihood\n\n  def compress(self, inputs):\n    """"""Compress inputs and store their binary representations into strings.\n\n    Arguments:\n      inputs: `Tensor` with values to be compressed.\n\n    Returns:\n      compressed: String `Tensor` vector containing the compressed\n        representation of each batch element of `inputs`.\n\n    Raises:\n      ValueError: if `inputs` has an integral or inconsistent `DType`, or\n        inconsistent number of channels.\n    """"""\n    with tf.name_scope(self._name_scope()):\n      inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n      if not self.built:\n        # Check input assumptions set before layer building, e.g. input rank.\n        input_spec.assert_input_compatibility(\n            self.input_spec, inputs, self.name)\n        if self.dtype is None:\n          self._dtype = inputs.dtype.base_dtype.name\n        self.build(inputs.shape)\n\n      # Check input assumptions set after layer building, e.g. input shape.\n      if not tf.executing_eagerly():\n        input_spec.assert_input_compatibility(\n            self.input_spec, inputs, self.name)\n        if inputs.dtype.is_integer:\n          raise ValueError(\n              ""{} can\'t take integer inputs."".format(type(self).__name__))\n\n      symbols = self._quantize(inputs, ""symbols"")\n      assert symbols.dtype == tf.int32\n\n      ndim = self.input_spec.ndim\n      indexes = self._prepare_indexes(shape=tf.shape(symbols)[1:])\n      broadcast_indexes = (indexes.shape.ndims != ndim)\n      if broadcast_indexes:\n        # We can\'t currently broadcast over anything else but the batch axis.\n        assert indexes.shape.ndims == ndim - 1\n        args = (symbols,)\n      else:\n        args = (symbols, indexes)\n\n      def loop_body(args):\n        string = range_coding_ops.unbounded_index_range_encode(\n            args[0], indexes if broadcast_indexes else args[1],\n            self._quantized_cdf, self._cdf_length, self._offset,\n            precision=self.range_coder_precision, overflow_width=4,\n            debug_level=0)\n        return string\n\n      strings = tf.map_fn(\n          loop_body, args, dtype=tf.string,\n          back_prop=False, name=""compress"")\n\n      if not tf.executing_eagerly():\n        strings.set_shape(inputs.shape[:1])\n\n      return strings\n\n  def decompress(self, strings, **kwargs):\n    """"""Decompress values from their compressed string representations.\n\n    Arguments:\n      strings: A string `Tensor` vector containing the compressed data.\n      **kwargs: Model-specific keyword arguments.\n\n    Returns:\n      The decompressed `Tensor`.\n    """"""\n    with tf.name_scope(self._name_scope()):\n      strings = tf.convert_to_tensor(strings, dtype=tf.string)\n\n      indexes = self._prepare_indexes(**kwargs)\n      ndim = self.input_spec.ndim\n      broadcast_indexes = (indexes.shape.ndims != ndim)\n      if broadcast_indexes:\n        # We can\'t currently broadcast over anything else but the batch axis.\n        assert indexes.shape.ndims == ndim - 1\n        args = (strings,)\n      else:\n        args = (strings, indexes)\n\n      def loop_body(args):\n        symbols = range_coding_ops.unbounded_index_range_decode(\n            args[0], indexes if broadcast_indexes else args[1],\n            self._quantized_cdf, self._cdf_length, self._offset,\n            precision=self.range_coder_precision, overflow_width=4,\n            debug_level=0)\n        return symbols\n\n      symbols = tf.map_fn(\n          loop_body, args, dtype=tf.int32, back_prop=False, name=""decompress"")\n\n      outputs = self._dequantize(symbols, ""dequantize"")\n      assert outputs.dtype == self.dtype\n\n      if not tf.executing_eagerly():\n        outputs.set_shape(self.input_spec.shape)\n\n      return outputs\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    return input_shape, input_shape\n\n\nclass EntropyBottleneck(EntropyModel):\n  """"""Entropy bottleneck layer.\n\n  This layer models the entropy of the tensor passing through it. During\n  training, this can be used to impose a (soft) entropy constraint on its\n  activations, limiting the amount of information flowing through the layer.\n  After training, the layer can be used to compress any input tensor to a\n  string, which may be written to a file, and to decompress a file which it\n  previously generated back to a reconstructed tensor. The entropies estimated\n  during training or evaluation are approximately equal to the average length of\n  the strings in bits.\n\n  The layer implements a flexible probability density model to estimate entropy\n  of its input tensor, which is described in the appendix of the paper (please\n  cite the paper if you use this code for scientific work):\n\n  > ""Variational image compression with a scale hyperprior""<br />\n  > J. Ball\xc3\xa9, D. Minnen, S. Singh, S. J. Hwang, N. Johnston<br />\n  > https://arxiv.org/abs/1802.01436\n\n  The layer assumes that the input tensor is at least 2D, with a batch dimension\n  at the beginning and a channel dimension as specified by `data_format`. The\n  layer trains an independent probability density model for each channel, but\n  assumes that across all other dimensions, the inputs are i.i.d. (independent\n  and identically distributed).\n\n  Because data compression always involves discretization, the outputs of the\n  layer are generally only approximations of its inputs. During training,\n  discretization is modeled using additive uniform noise to ensure\n  differentiability. The entropies computed during training are differential\n  entropies. During evaluation, the data is actually quantized, and the\n  entropies are discrete (Shannon entropies). To make sure the approximated\n  tensor values are good enough for practical purposes, the training phase must\n  be used to balance the quality of the approximation with the entropy, by\n  adding an entropy term to the training loss. See the example in the package\n  documentation to get started.\n\n  Note: the layer always produces exactly one auxiliary loss and one update op,\n  which are only significant for compression and decompression. To use the\n  compression feature, the auxiliary loss must be minimized during or after\n  training. After that, the update op must be executed at least once.\n  """"""\n\n  def __init__(self, init_scale=10, filters=(3, 3, 3),\n               data_format=""channels_last"", **kwargs):\n    """"""Initializer.\n\n    Arguments:\n      init_scale: Float. A scaling factor determining the initial width of the\n        probability densities. This should be chosen big enough so that the\n        range of values of the layer inputs roughly falls within the interval\n        [`-init_scale`, `init_scale`] at the beginning of training.\n      filters: An iterable of ints, giving the number of filters at each layer\n        of the density model. Generally, the more filters and layers, the more\n        expressive is the density model in terms of modeling more complicated\n        distributions of the layer inputs. For details, refer to the paper\n        referenced above. The default is `[3, 3, 3]`, which should be sufficient\n        for most practical purposes.\n      data_format: Either `\'channels_first\'` or `\'channels_last\'` (default).\n      **kwargs: Other keyword arguments passed to superclass (`EntropyModel`).\n    """"""\n    super(EntropyBottleneck, self).__init__(**kwargs)\n    self._init_scale = float(init_scale)\n    self._filters = tuple(int(f) for f in filters)\n    self._data_format = str(data_format)\n    self.input_spec = tf.keras.layers.InputSpec(min_ndim=2)\n\n    if self.data_format not in (""channels_first"", ""channels_last""):\n      raise ValueError(""Unknown data format: \'{}\'."".format(self.data_format))\n\n  @property\n  def init_scale(self):\n    return self._init_scale\n\n  @property\n  def filters(self):\n    return self._filters\n\n  @property\n  def data_format(self):\n    return self._data_format\n\n  def _channel_axis(self, ndim):\n    return {""channels_first"": 1, ""channels_last"": ndim - 1}[self.data_format]\n\n  def _get_input_dims(self):\n    """"""Returns a few useful numbers related to input dimensionality.\n\n    Returns:\n      ndim: Integer. Number of input dimensions including batch.\n      channel_axis: Integer. Index of dimension that enumerates channels.\n      channels: Integer. Number of channels in inputs.\n      input_slices: Tuple of slices. Can be used as an index to expand a vector\n        to input dimensions, where the vector now runs across channels.\n    """"""\n    ndim = self.input_spec.ndim\n    channel_axis = self._channel_axis(ndim)\n    channels = self.input_spec.axes[channel_axis]\n    # Tuple of slices for expanding tensors to input shape.\n    input_slices = ndim * [None]\n    input_slices[channel_axis] = slice(None)\n    input_slices = tuple(input_slices)\n    return ndim, channel_axis, channels, input_slices\n\n  def _logits_cumulative(self, inputs, stop_gradient):\n    """"""Evaluate logits of the cumulative densities.\n\n    Arguments:\n      inputs: The values at which to evaluate the cumulative densities, expected\n        to be a `Tensor` of shape `(channels, 1, batch)`.\n      stop_gradient: Boolean. Whether to add `tf.stop_gradient` calls so\n        that the gradient of the output with respect to the density model\n        parameters is disconnected (the gradient with respect to `inputs` is\n        left untouched).\n\n    Returns:\n      A `Tensor` of the same shape as `inputs`, containing the logits of the\n      cumulative densities evaluated at the given inputs.\n    """"""\n    logits = inputs\n\n    for i in range(len(self.filters) + 1):\n      matrix = self._matrices[i]\n      if stop_gradient:\n        matrix = tf.stop_gradient(matrix)\n      logits = tf.linalg.matmul(matrix, logits)\n\n      bias = self._biases[i]\n      if stop_gradient:\n        bias = tf.stop_gradient(bias)\n      logits += bias\n\n      if i < len(self._factors):\n        factor = self._factors[i]\n        if stop_gradient:\n          factor = tf.stop_gradient(factor)\n        logits += factor * tf.math.tanh(logits)\n\n    return logits\n\n  def build(self, input_shape):\n    """"""Builds the entropy model.\n\n    Creates the variables for the network modeling the densities, creates the\n    auxiliary loss estimating the median and tail quantiles of the densities,\n    and then uses that to create the probability mass functions and the discrete\n    cumulative density functions used by the range coder.\n\n    Arguments:\n      input_shape: Shape of the input tensor, used to get the number of\n        channels.\n\n    Raises:\n      ValueError: if `input_shape` doesn\'t specify the length of the channel\n        dimension.\n    """"""\n    input_shape = tf.TensorShape(input_shape)\n    channel_axis = self._channel_axis(input_shape.ndims)\n    channels = input_shape.as_list()[channel_axis]\n    if channels is None:\n      raise ValueError(""The channel dimension of the inputs must be defined."")\n    self.input_spec = tf.keras.layers.InputSpec(\n        ndim=input_shape.ndims, axes={channel_axis: channels})\n    filters = (1,) + self.filters + (1,)\n    scale = self.init_scale ** (1 / (len(self.filters) + 1))\n\n    # Create variables.\n    self._matrices = []\n    self._biases = []\n    self._factors = []\n    for i in range(len(self.filters) + 1):\n      init = np.log(np.expm1(1 / scale / filters[i + 1]))\n      matrix = self.add_weight(\n          ""matrix_{}"".format(i), dtype=self.dtype,\n          shape=(channels, filters[i + 1], filters[i]),\n          initializer=tf.initializers.constant(init))\n      matrix = tf.nn.softplus(matrix)\n      self._matrices.append(matrix)\n\n      bias = self.add_weight(\n          ""bias_{}"".format(i), dtype=self.dtype,\n          shape=(channels, filters[i + 1], 1),\n          initializer=tf.initializers.random_uniform(-.5, .5))\n      self._biases.append(bias)\n\n      if i < len(self.filters):\n        factor = self.add_weight(\n            ""factor_{}"".format(i), dtype=self.dtype,\n            shape=(channels, filters[i + 1], 1),\n            initializer=tf.initializers.zeros())\n        factor = tf.math.tanh(factor)\n        self._factors.append(factor)\n\n    # To figure out what range of the densities to sample, we need to compute\n    # the quantiles given by `tail_mass / 2` and `1 - tail_mass / 2`. Since we\n    # can\'t take inverses of the cumulative directly, we make it an optimization\n    # problem:\n    # `quantiles = argmin(|logit(cumulative) - target|)`\n    # where `target` is `logit(tail_mass / 2)` or `logit(1 - tail_mass / 2)`.\n    # Taking the logit (inverse of sigmoid) of the cumulative makes the\n    # representation of the right target more numerically stable.\n\n    # Numerically stable way of computing logits of `tail_mass / 2`\n    # and `1 - tail_mass / 2`.\n    target = np.log(2 / self.tail_mass - 1)\n    # Compute lower and upper tail quantile as well as median.\n    target = tf.constant([-target, 0, target], dtype=self.dtype)\n\n    def quantiles_initializer(shape, dtype=None, partition_info=None):\n      del partition_info  # unused\n      assert tuple(shape[1:]) == (1, 3)\n      init = tf.constant(\n          [[[-self.init_scale, 0, self.init_scale]]], dtype=dtype)\n      return tf.tile(init, (shape[0], 1, 1))\n\n    quantiles = self.add_weight(\n        ""quantiles"", shape=(channels, 1, 3), dtype=self.dtype,\n        initializer=quantiles_initializer,\n        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    logits = self._logits_cumulative(quantiles, stop_gradient=True)\n    loss = tf.math.reduce_sum(abs(logits - target))\n    self.add_loss(loss, inputs=None)\n\n    # Quantize such that the median coincides with the center of a bin.\n    medians = quantiles[:, 0, 1]\n    self._medians = tf.stop_gradient(medians)\n\n    # Largest distance observed between lower tail quantile and median, and\n    # between median and upper tail quantile.\n    minima = medians - quantiles[:, 0, 0]\n    minima = tf.cast(tf.math.ceil(minima), tf.int32)\n    minima = tf.math.maximum(minima, 0)\n    maxima = quantiles[:, 0, 2] - medians\n    maxima = tf.cast(tf.math.ceil(maxima), tf.int32)\n    maxima = tf.math.maximum(maxima, 0)\n\n    # PMF starting positions and lengths.\n    self._offset = -minima\n    pmf_start = medians - tf.cast(minima, self.dtype)\n    pmf_length = maxima + minima + 1\n\n    # Sample the densities in the computed ranges, possibly computing more\n    # samples than necessary at the upper end.\n    max_length = tf.math.reduce_max(pmf_length)\n    samples = tf.range(tf.cast(max_length, self.dtype), dtype=self.dtype)\n    samples += pmf_start[:, None, None]\n\n    half = tf.constant(.5, dtype=self.dtype)\n    # We strip the sigmoid from the end here, so we can use the special rule\n    # below to only compute differences in the left tail of the sigmoid.\n    # This increases numerical stability (see explanation in `call`).\n    lower = self._logits_cumulative(samples - half, stop_gradient=True)\n    upper = self._logits_cumulative(samples + half, stop_gradient=True)\n    # Flip signs if we can move more towards the left tail of the sigmoid.\n    sign = -tf.math.sign(tf.math.add_n([lower, upper]))\n    pmf = abs(tf.math.sigmoid(sign * upper) - tf.math.sigmoid(sign * lower))\n    pmf = pmf[:, 0, :]\n\n    # Compute out-of-range (tail) masses.\n    tail_mass = tf.math.add_n([\n        tf.math.sigmoid(lower[:, 0, :1]),\n        tf.math.sigmoid(-upper[:, 0, -1:]),\n    ])\n\n    # Construct a valid CDF initializer, so that we can run the model without\n    # error even on the zeroth training step.\n    def cdf_initializer(shape, dtype=None, partition_info=None):\n      del shape, partition_info  # unused\n      assert dtype == tf.int32\n      fill = tf.constant(.5, dtype=self.dtype)\n      prob = tf.fill((channels, 2), fill)\n      cdf = range_coding_ops.pmf_to_quantized_cdf(\n          prob, precision=self.range_coder_precision)\n      return tf.placeholder_with_default(cdf, shape=(channels, None))\n\n    # We need to supply an initializer without fully defined static shape\n    # here, or the variable will return the wrong dynamic shape later. A\n    # placeholder with default gets the trick done (see initializer above).\n    quantized_cdf = self.add_weight(\n        ""quantized_cdf"",\n        shape=(channels, None),\n        dtype=tf.int32,\n        trainable=False,\n        initializer=cdf_initializer,\n        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    cdf_length = self.add_weight(\n        ""cdf_length"", shape=(channels,), dtype=tf.int32, trainable=False,\n        initializer=tf.initializers.constant(3),\n        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    # Works around a weird TF issue with reading variables inside a loop.\n    self._quantized_cdf = tf.identity(quantized_cdf)\n    self._cdf_length = tf.identity(cdf_length)\n\n    update_cdf = tf.assign(\n        quantized_cdf,\n        self._pmf_to_cdf(pmf, tail_mass, pmf_length, max_length),\n        validate_shape=False)\n    update_length = tf.assign(\n        cdf_length,\n        pmf_length + 2)\n    update_op = tf.group(update_cdf, update_length)\n    self.add_update(update_op)\n\n    super(EntropyBottleneck, self).build(input_shape)\n\n  def _quantize(self, inputs, mode):\n    # Add noise or quantize (and optionally dequantize in one step).\n    half = tf.constant(.5, dtype=self.dtype)\n    _, _, _, input_slices = self._get_input_dims()\n\n    if mode == ""noise"":\n      noise = tf.random.uniform(tf.shape(inputs), -half, half)\n      return tf.math.add_n([inputs, noise])\n\n    medians = self._medians[input_slices]\n    outputs = tf.math.floor(inputs + (half - medians))\n\n    if mode == ""dequantize"":\n      outputs = tf.cast(outputs, self.dtype)\n      return outputs + medians\n    else:\n      assert mode == ""symbols"", mode\n      outputs = tf.cast(outputs, tf.int32)\n      return outputs\n\n  def _dequantize(self, inputs, mode):\n    _, _, _, input_slices = self._get_input_dims()\n    medians = self._medians[input_slices]\n    outputs = tf.cast(inputs, self.dtype)\n    return outputs + medians\n\n  def _likelihood(self, inputs):\n    ndim, channel_axis, _, _ = self._get_input_dims()\n    half = tf.constant(.5, dtype=self.dtype)\n\n    # Convert to (channels, 1, batch) format by commuting channels to front\n    # and then collapsing.\n    order = list(range(ndim))\n    order.pop(channel_axis)\n    order.insert(0, channel_axis)\n    inputs = tf.transpose(inputs, order)\n    shape = tf.shape(inputs)\n    inputs = tf.reshape(inputs, (shape[0], 1, -1))\n\n    # Evaluate densities.\n    # We can use the special rule below to only compute differences in the left\n    # tail of the sigmoid. This increases numerical stability: sigmoid(x) is 1\n    # for large x, 0 for small x. Subtracting two numbers close to 0 can be done\n    # with much higher precision than subtracting two numbers close to 1.\n    lower = self._logits_cumulative(inputs - half, stop_gradient=False)\n    upper = self._logits_cumulative(inputs + half, stop_gradient=False)\n    # Flip signs if we can move more towards the left tail of the sigmoid.\n    sign = -tf.math.sign(tf.math.add_n([lower, upper]))\n    sign = tf.stop_gradient(sign)\n    likelihood = abs(\n        tf.math.sigmoid(sign * upper) - tf.math.sigmoid(sign * lower))\n\n    # Convert back to input tensor shape.\n    order = list(range(1, ndim))\n    order.insert(channel_axis, 0)\n    likelihood = tf.reshape(likelihood, shape)\n    likelihood = tf.transpose(likelihood, order)\n\n    return likelihood\n\n  def _prepare_indexes(self, shape, channels=None):\n    shape = tf.convert_to_tensor(shape)\n\n    if not self.built:\n      if not (shape.shape.is_fully_defined() and shape.shape.ndims == 1):\n        raise ValueError(""`shape` must be a vector with known length."")\n      ndim = shape.shape.as_list()[0] + 1\n      channel_axis = self._channel_axis(ndim)\n      input_shape = ndim * [None]\n      input_shape[channel_axis] = channels\n      self.build(input_shape)\n\n    _, channel_axis, channels, input_slices = self._get_input_dims()\n\n    # TODO(jonycgn, ssjhv): Investigate broadcasting.\n    indexes = tf.range(channels, dtype=tf.int32)\n    indexes = tf.cast(indexes, tf.int32)\n    tiles = tf.concat(\n        [shape[:channel_axis - 1], [1], shape[channel_axis:]], axis=0)\n    indexes = tf.tile(indexes[input_slices[1:]], tiles)\n\n    return indexes\n\n  # Just giving a more useful docstring.\n  def decompress(self, strings, shape, channels=None):\n    """"""Decompress values from their compressed string representations.\n\n    Arguments:\n      strings: A string `Tensor` vector containing the compressed data.\n      shape: A `Tensor` vector of int32 type. Contains the shape of the tensor\n        to be decompressed, excluding the batch dimension.\n      channels: Integer. Specifies the number of channels statically. Needs only\n        be set if the layer hasn\'t been built yet (i.e., this is the first input\n        it receives).\n\n    Returns:\n      The decompressed `Tensor`. Its shape will be equal to `shape` prepended\n      with the batch dimension from `strings`.\n\n    Raises:\n      ValueError: If the length of `shape` isn\'t available at graph construction\n        time.\n    """"""\n    return super(EntropyBottleneck, self).decompress(\n        strings, shape=shape, channels=channels)\n\n\nclass SymmetricConditional(EntropyModel):\n  """"""Symmetric conditional entropy model (base class).""""""\n\n  def __init__(self, scale, scale_table,\n               scale_bound=None, mean=None, indexes=None, **kwargs):\n    """"""Initializer.\n\n    Arguments:\n      scale: `Tensor`, the scale parameters for the conditional distributions.\n      scale_table: Iterable of positive floats. For range coding, the scale\n        parameters in `scale` can\'t be used, because the probability tables need\n        to be constructed statically. Only the values given in this table will\n        actually be used for range coding. For each predicted scale, the next\n        greater entry in the table is selected. It\'s optimal to choose the\n        scales provided here in a logarithmic way.\n      scale_bound: Float. Lower bound for scales. Any values in `scale` smaller\n        than this value are set to this value to prevent non-positive scales. By\n        default (or when set to `None`), uses the smallest value in\n        `scale_table`. To disable, set to 0.\n      mean: `Tensor`, the mean parameters for the conditional distributions. If\n        `None`, the mean is assumed to be zero.\n      indexes: `Tensor` of type `int32` or `None`. Can be used to override the\n        selection of scale table indexes based on the predicted values in\n        `scale`. Only affects compression and decompression.\n      **kwargs: Other keyword arguments passed to superclass (`EntropyModel`).\n    """"""\n    super(SymmetricConditional, self).__init__(**kwargs)\n    self._scale = tf.convert_to_tensor(scale)\n    input_shape = self.scale.shape\n    self._scale_table = tuple(sorted(float(s) for s in scale_table))\n    if any(s <= 0 for s in self.scale_table):\n      raise ValueError(""`scale_table` must be an iterable of positive numbers."")\n    self._scale_bound = None if scale_bound is None else float(scale_bound)\n    self._mean = None if mean is None else tf.convert_to_tensor(mean)\n    if indexes is not None:\n      self._indexes = tf.convert_to_tensor(indexes)\n      if self.indexes.dtype != tf.int32:\n        raise ValueError(""`indexes` must have `int32` dtype."")\n      input_shape = input_shape.merge_with(self.indexes.shape)\n    if input_shape.ndims is None:\n      raise ValueError(\n          ""Number of dimensions of `scale` or `indexes` must be known."")\n    self.input_spec = tf.keras.layers.InputSpec(shape=input_shape)\n\n  @property\n  def scale(self):\n    return self._scale\n\n  @property\n  def scale_table(self):\n    return self._scale_table\n\n  @property\n  def scale_bound(self):\n    return self._scale_bound\n\n  @property\n  def mean(self):\n    return self._mean\n\n  @property\n  def indexes(self):\n    return self._indexes\n\n  def _standardized_cumulative(self, inputs):\n    """"""Evaluate the standardized cumulative density.\n\n    Note: This function should be optimized to give the best possible numerical\n    accuracy for negative input values.\n\n    Arguments:\n      inputs: `Tensor`. The values at which to evaluate the cumulative density.\n\n    Returns:\n      A `Tensor` of the same shape as `inputs`, containing the cumulative\n      density evaluated at the given inputs.\n    """"""\n    raise NotImplementedError(""Must inherit from SymmetricConditional."")\n\n  def _standardized_quantile(self, quantile):\n    """"""Evaluate the standardized quantile function.\n\n    This returns the inverse of the standardized cumulative function for a\n    scalar.\n\n    Arguments:\n      quantile: Float. The values at which to evaluate the quantile function.\n\n    Returns:\n      A float giving the inverse CDF value.\n    """"""\n    raise NotImplementedError(""Must inherit from SymmetricConditional."")\n\n  def build(self, input_shape):\n    """"""Builds the entropy model.\n\n    This function precomputes the quantized CDF table based on the scale table.\n    This can be done at graph construction time. Then, it creates the graph for\n    computing the indexes into that table based on the scale tensor, and then\n    uses this index tensor to determine the starting positions of the PMFs for\n    each scale.\n\n    Arguments:\n      input_shape: Shape of the input tensor.\n\n    Raises:\n      ValueError: If `input_shape` doesn\'t specify number of input dimensions.\n    """"""\n    input_shape = tf.TensorShape(input_shape)\n    input_shape.assert_is_compatible_with(self.input_spec.shape)\n\n    scale_table = tf.constant(self.scale_table, dtype=self.dtype)\n\n    # Lower bound scales. We need to do this here, and not in __init__, because\n    # the dtype may not yet be known there.\n    if self.scale_bound is None:\n      self._scale = math_ops.lower_bound(self._scale, scale_table[0])\n    elif self.scale_bound > 0:\n      self._scale = math_ops.lower_bound(self._scale, self.scale_bound)\n\n    multiplier = -self._standardized_quantile(self.tail_mass / 2)\n    pmf_center = np.ceil(np.array(self.scale_table) * multiplier).astype(int)\n    pmf_length = 2 * pmf_center + 1\n    max_length = np.max(pmf_length)\n\n    # This assumes that the standardized cumulative has the property\n    # 1 - c(x) = c(-x), which means we can compute differences equivalently in\n    # the left or right tail of the cumulative. The point is to only compute\n    # differences in the left tail. This increases numerical stability: c(x) is\n    # 1 for large x, 0 for small x. Subtracting two numbers close to 0 can be\n    # done with much higher precision than subtracting two numbers close to 1.\n    samples = abs(np.arange(max_length, dtype=int) - pmf_center[:, None])\n    samples = tf.constant(samples, dtype=self.dtype)\n    samples_scale = tf.expand_dims(scale_table, 1)\n    upper = self._standardized_cumulative((.5 - samples) / samples_scale)\n    lower = self._standardized_cumulative((-.5 - samples) / samples_scale)\n    pmf = upper - lower\n\n    # Compute out-of-range (tail) masses.\n    tail_mass = 2 * lower[:, :1]\n\n    def cdf_initializer(shape, dtype=None, partition_info=None):\n      del partition_info  # unused\n      assert tuple(shape) == (len(pmf_length), max_length + 2)\n      assert dtype == tf.int32\n      return self._pmf_to_cdf(\n          pmf, tail_mass,\n          tf.constant(pmf_length, dtype=tf.int32), max_length)\n\n    quantized_cdf = self.add_weight(\n        ""quantized_cdf"", shape=(len(pmf_length), max_length + 2),\n        initializer=cdf_initializer, dtype=tf.int32, trainable=False,\n        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    cdf_length = self.add_weight(\n        ""cdf_length"", shape=(len(pmf_length),),\n        initializer=tf.initializers.constant(pmf_length + 2),\n        dtype=tf.int32, trainable=False,\n        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    # Works around a weird TF issue with reading variables inside a loop.\n    self._quantized_cdf = tf.identity(quantized_cdf)\n    self._cdf_length = tf.identity(cdf_length)\n\n    # Now, if they haven\'t been overridden, compute the indexes into the table\n    # for each of the passed-in scales.\n    if not hasattr(self, ""_indexes""):\n      # Prevent tensors from bouncing back and forth between host and GPU.\n      with tf.device(""/cpu:0""):\n        fill = tf.constant(\n            len(self.scale_table) - 1, dtype=tf.int32)\n        initializer = tf.fill(tf.shape(self.scale), fill)\n\n        def loop_body(indexes, scale):\n          return indexes - tf.cast(self.scale <= scale, tf.int32)\n\n        self._indexes = tf.foldr(\n            loop_body, scale_table[:-1],\n            initializer=initializer, back_prop=False, name=""compute_indexes"")\n\n    self._offset = tf.constant(-pmf_center, dtype=tf.int32)\n\n    super(SymmetricConditional, self).build(input_shape)\n\n  def _quantize(self, inputs, mode):\n    # Add noise or quantize (and optionally dequantize in one step).\n    half = tf.constant(.5, dtype=self.dtype)\n\n    if mode == ""noise"":\n      noise = tf.random.uniform(tf.shape(inputs), -half, half)\n      return tf.math.add_n([inputs, noise])\n\n    outputs = inputs\n    if self.mean is not None:\n      outputs -= self.mean\n    outputs = tf.math.floor(outputs + half)\n\n    if mode == ""dequantize"":\n      if self.mean is not None:\n        outputs += self.mean\n      return outputs\n    else:\n      assert mode == ""symbols"", mode\n      outputs = tf.cast(outputs, tf.int32)\n      return outputs\n\n  def _dequantize(self, inputs, mode):\n    assert mode == ""dequantize""\n    outputs = tf.cast(inputs, self.dtype)\n    if self.mean is not None:\n      outputs += self.mean\n    return outputs\n\n  def _likelihood(self, inputs):\n    values = inputs\n    if self.mean is not None:\n      values -= self.mean\n\n    # This assumes that the standardized cumulative has the property\n    # 1 - c(x) = c(-x), which means we can compute differences equivalently in\n    # the left or right tail of the cumulative. The point is to only compute\n    # differences in the left tail. This increases numerical stability: c(x) is\n    # 1 for large x, 0 for small x. Subtracting two numbers close to 0 can be\n    # done with much higher precision than subtracting two numbers close to 1.\n    values = abs(values)\n    upper = self._standardized_cumulative((.5 - values) / self.scale)\n    lower = self._standardized_cumulative((-.5 - values) / self.scale)\n    likelihood = upper - lower\n\n    return likelihood\n\n  def _prepare_indexes(self, shape=None):\n    del shape  # unused\n    if not self.built:\n      self.build(self.input_spec.shape)\n    return self.indexes\n\n  # Just giving a more useful docstring.\n  def decompress(self, strings):  # pylint:disable=useless-super-delegation\n    """"""Decompress values from their compressed string representations.\n\n    Arguments:\n      strings: A string `Tensor` vector containing the compressed data.\n\n    Returns:\n      The decompressed `Tensor`.\n    """"""\n    return super(SymmetricConditional, self).decompress(strings)\n\n\nclass GaussianConditional(SymmetricConditional):\n  """"""Conditional Gaussian entropy model.\n\n  The layer implements a conditionally Gaussian probability density model to\n  estimate entropy of its input tensor, which is described in the paper (please\n  cite the paper if you use this code for scientific work):\n\n  > ""Variational image compression with a scale hyperprior""<br />\n  > J. Ball\xc3\xa9, D. Minnen, S. Singh, S. J. Hwang, N. Johnston<br />\n  > https://arxiv.org/abs/1802.01436\n  """"""\n\n  def _standardized_cumulative(self, inputs):\n    half = tf.constant(.5, dtype=self.dtype)\n    const = tf.constant(-(2 ** -0.5), dtype=self.dtype)\n    # Using the complementary error function maximizes numerical precision.\n    return half * tf.math.erfc(const * inputs)\n\n  def _standardized_quantile(self, quantile):\n    return scipy.stats.norm.ppf(quantile)\n\n\nclass LogisticConditional(SymmetricConditional):\n  """"""Conditional logistic entropy model.\n\n  This is a conditionally Logistic entropy model, analogous to\n  `GaussianConditional`.\n  """"""\n\n  def _standardized_cumulative(self, inputs):\n    return tf.math.sigmoid(inputs)\n\n  def _standardized_quantile(self, quantile):\n    return scipy.stats.logistic.ppf(quantile)\n\n\nclass LaplacianConditional(SymmetricConditional):\n  """"""Conditional Laplacian entropy model.\n\n  This is a conditionally Laplacian entropy model, analogous to\n  `GaussianConditional`.\n  """"""\n\n  def _standardized_cumulative(self, inputs):\n    exp = tf.math.exp(-abs(inputs))\n    return tf.where(inputs > 0, 2 - exp, exp) / 2\n\n  def _standardized_quantile(self, quantile):\n    return scipy.stats.laplace.ppf(quantile)\n'"
tensorflow_compression/python/layers/entropy_models_test.py,87,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of entropy models.""""""\n\nimport numpy as np\nimport scipy.stats\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.layers import entropy_models\n\n\n@test_util.deprecated_graph_mode_only\nclass EntropyBottleneckTest(tf.test.TestCase):\n\n  def test_noise(self):\n    # Tests that the noise added is uniform noise between -0.5 and 0.5.\n    inputs = tf.placeholder(tf.float32, (None, 1))\n    layer = entropy_models.EntropyBottleneck()\n    noisy, _ = layer(inputs, training=True)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)[:, None]\n      noisy, = sess.run([noisy], {inputs: values})\n    self.assertFalse(np.allclose(values, noisy, rtol=0, atol=.45))\n    self.assertAllClose(values, noisy, rtol=0, atol=.5)\n\n  def test_quantization_init(self):\n    # Tests that inputs are quantized to full integer values right after\n    # initialization.\n    inputs = tf.placeholder(tf.float32, (None, 1))\n    layer = entropy_models.EntropyBottleneck()\n    quantized, _ = layer(inputs, training=False)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)[:, None]\n      quantized, = sess.run([quantized], {inputs: values})\n    self.assertAllClose(np.around(values), quantized, rtol=0, atol=1e-6)\n\n  def test_quantization(self):\n    # Tests that inputs are not quantized to full integer values after quantiles\n    # have been updated. However, the difference between input and output should\n    # be between -0.5 and 0.5, and the offset must be consistent.\n    inputs = tf.placeholder(tf.float32, (None, 1))\n    layer = entropy_models.EntropyBottleneck()\n    quantized, _ = layer(inputs, training=False)\n    opt = tf.train.GradientDescentOptimizer(learning_rate=1)\n    self.assertEqual(1, len(layer.losses))\n    step = opt.minimize(layer.losses[0])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(step)\n      values = np.linspace(-50, 50, 100)[:, None]\n      quantized, = sess.run([quantized], {inputs: values})\n    self.assertAllClose(values, quantized, rtol=0, atol=.5)\n    diff = np.ravel(np.around(values) - quantized) % 1\n    self.assertAllClose(diff, np.full_like(diff, diff[0]), rtol=0, atol=5e-6)\n    self.assertNotEqual(diff[0], 0)\n\n  def test_codec_init(self):\n    # Tests that inputs are compressed and decompressed correctly, and quantized\n    # to full integer values right after initialization.\n    inputs = tf.placeholder(tf.float32, (1, None, 1))\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_last"", init_scale=30)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings, tf.shape(inputs)[1:])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)[None, :, None]\n      decoded, = sess.run([decoded], {inputs: values})\n    self.assertAllClose(np.around(values), decoded, rtol=0, atol=1e-6)\n\n  def test_codec(self):\n    # Tests that inputs are compressed and decompressed correctly, and not\n    # quantized to full integer values after quantiles have been updated.\n    # However, the difference between input and output should be between -0.5\n    # and 0.5, and the offset must be consistent.\n    inputs = tf.placeholder(tf.float32, (1, None, 1))\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_last"", init_scale=40)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings, tf.shape(inputs)[1:])\n    opt = tf.train.GradientDescentOptimizer(learning_rate=1)\n    self.assertEqual(1, len(layer.losses))\n    step = opt.minimize(layer.losses[0])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(step)\n      self.assertEqual(1, len(layer.updates))\n      sess.run(layer.updates[0])\n      values = np.linspace(-50, 50, 100)[None, :, None]\n      decoded, = sess.run([decoded], {inputs: values})\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n    diff = np.ravel(np.around(values) - decoded) % 1\n    self.assertAllClose(diff, np.full_like(diff, diff[0]), rtol=0, atol=5e-6)\n    self.assertNotEqual(diff[0], 0)\n\n  def test_channels_last(self):\n    # Test the layer with more than one channel and multiple input dimensions,\n    # with the channels in the last dimension.\n    inputs = tf.placeholder(tf.float32, (None, None, None, 2))\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_last"", init_scale=20)\n    noisy, _ = layer(inputs, training=True)\n    quantized, _ = layer(inputs, training=False)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings, tf.shape(inputs)[1:])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertEqual(1, len(layer.updates))\n      sess.run(layer.updates[0])\n      values = 5 * np.random.normal(size=(7, 5, 3, 2))\n      noisy, quantized, decoded = sess.run(\n          [noisy, quantized, decoded], {inputs: values})\n    self.assertAllClose(values, noisy, rtol=0, atol=.5)\n    self.assertAllClose(values, quantized, rtol=0, atol=.5)\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n\n  def test_channels_first(self):\n    # Test the layer with more than one channel and multiple input dimensions,\n    # with the channel dimension right after the batch dimension.\n    inputs = tf.placeholder(tf.float32, (None, 3, None, None))\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_first"", init_scale=10)\n    noisy, _ = layer(inputs, training=True)\n    quantized, _ = layer(inputs, training=False)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings, tf.shape(inputs)[1:])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertEqual(1, len(layer.updates))\n      sess.run(layer.updates[0])\n      values = 2.5 * np.random.normal(size=(2, 3, 5, 7))\n      noisy, quantized, decoded = sess.run(\n          [noisy, quantized, decoded], {inputs: values})\n    self.assertAllClose(values, noisy, rtol=0, atol=.5)\n    self.assertAllClose(values, quantized, rtol=0, atol=.5)\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n\n  def test_compress(self):\n    # Test compression and decompression, and produce test data for\n    # `test_decompress`. If you set the constant at the end to `True`, this test\n    # will fail and the log will contain the new test data.\n    inputs = tf.placeholder(tf.float32, (2, 3, 9))\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_first"", filters=(), init_scale=2)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings, tf.shape(inputs)[1:])\n    with self.cached_session() as sess:\n      values = 8 * np.random.uniform(size=(2, 3, 9)) - 4\n      sess.run(tf.global_variables_initializer())\n      self.assertEqual(1, len(layer.updates))\n      sess.run(layer.updates[0])\n      bitstrings, quantized_cdf, cdf_length, decoded = sess.run(\n          [bitstrings, layer._quantized_cdf, layer._cdf_length, decoded],\n          {inputs: values})\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n    # Set this constant to `True` to log new test data for `test_decompress`.\n    if False:  # pylint:disable=using-constant-test\n      assert False, (bitstrings, quantized_cdf, cdf_length, decoded)\n\n  # Data generated by `test_compress`.\n  # pylint:disable=bad-whitespace,bad-continuation\n  bitstrings = np.array([\n      b""\\x91\\xf4\\xdan2\\xd3q\\x97\\xd0\\x91N1~\\xc4\\xb0;\\xd38\\xa8\\x90"",\n      b""?\\xc7\\xf9\\x17\\xa8\\xcfu\\x99\\x1e4\\xfe\\xe0\\xd3U`z\\x15v"",\n  ], dtype=object)\n\n  quantized_cdf = np.array([\n      [    0,  5170, 11858, 19679, 27812, 35302, 65536],\n      [    0,  6100, 13546, 21671, 29523, 36269, 65536],\n      [    0,  6444, 14120, 22270, 29929, 36346, 65536],\n  ], dtype=np.int32)\n\n  cdf_length = np.array([7, 7, 7], dtype=np.int32)\n\n  expected = np.array([\n      [[-3.,  2.,  1., -3., -1., -3., -4., -2.,  2.],\n       [-2.,  2.,  4.,  1.,  0., -3., -3.,  2.,  4.],\n       [ 1.,  2.,  4., -1., -3.,  4.,  0., -2., -3.]],\n      [[ 0.,  4.,  0.,  2.,  4.,  1., -2.,  1.,  4.],\n       [ 2.,  2.,  3., -3.,  4., -1., -1.,  0., -1.],\n       [ 3.,  0.,  3., -3.,  3.,  3., -3., -4., -1.]],\n  ], dtype=np.float32)\n  # pylint:enable=bad-whitespace,bad-continuation\n\n  def test_decompress(self):\n    # Test that decompression of values compressed with a previous version\n    # works, i.e. that the file format doesn\'t change across revisions.\n    bitstrings = tf.placeholder(tf.string)\n    input_shape = tf.placeholder(tf.int32)\n    quantized_cdf = tf.placeholder(tf.int32)\n    cdf_length = tf.placeholder(tf.int32)\n    layer = entropy_models.EntropyBottleneck(\n        data_format=""channels_first"", filters=(), init_scale=2,\n        dtype=tf.float32)\n    layer.build(self.expected.shape)\n    layer._quantized_cdf = quantized_cdf\n    layer._cdf_length = cdf_length\n    decoded = layer.decompress(bitstrings, input_shape[1:])\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      decoded, = sess.run([decoded], {\n          bitstrings: self.bitstrings, input_shape: self.expected.shape,\n          quantized_cdf: self.quantized_cdf, cdf_length: self.cdf_length})\n    self.assertAllClose(self.expected, decoded, rtol=0, atol=1e-6)\n\n  def test_build_decompress(self):\n    # Test that layer can be built when `decompress` is the first call to it.\n    bitstrings = tf.placeholder(tf.string)\n    input_shape = tf.placeholder(tf.int32, shape=[3])\n    layer = entropy_models.EntropyBottleneck(dtype=tf.float32)\n    layer.decompress(bitstrings, input_shape[1:], channels=5)\n    self.assertTrue(layer.built)\n\n  def test_normalization(self):\n    # Test that densities are normalized correctly.\n    inputs = tf.placeholder(tf.float32, (None, 1))\n    layer = entropy_models.EntropyBottleneck(filters=(2,))\n    _, likelihood = layer(inputs, training=True)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      x = np.repeat(np.arange(-200, 201), 2000)[:, None]\n      likelihood, = sess.run([likelihood], {inputs: x})\n    self.assertEqual(x.shape, likelihood.shape)\n    integral = np.sum(likelihood) * .0005\n    self.assertAllClose(1, integral, rtol=0, atol=2e-4)\n\n  def test_entropy_estimates(self):\n    # Test that entropy estimates match actual range coding.\n    inputs = tf.placeholder(tf.float32, (1, None, 1))\n    layer = entropy_models.EntropyBottleneck(\n        filters=(2, 3), data_format=""channels_last"")\n    _, likelihood = layer(inputs, training=True)\n    diff_entropy = tf.reduce_sum(tf.log(likelihood)) / -np.log(2)\n    _, likelihood = layer(inputs, training=False)\n    disc_entropy = tf.reduce_sum(tf.log(likelihood)) / -np.log(2)\n    bitstrings = layer.compress(inputs)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertEqual(1, len(layer.updates))\n      sess.run(layer.updates[0])\n      diff_entropy, disc_entropy, bitstrings = sess.run(\n          [diff_entropy, disc_entropy, bitstrings],\n          {inputs: np.random.normal(size=(1, 10000, 1))})\n    codelength = 8 * sum(len(s) for s in bitstrings)\n    self.assertAllClose(diff_entropy, disc_entropy, rtol=5e-3, atol=0)\n    self.assertAllClose(disc_entropy, codelength, rtol=5e-3, atol=0)\n\n\n@test_util.deprecated_graph_mode_only\nclass SymmetricConditionalTest(object):\n\n  def test_noise(self):\n    # Tests that the noise added is uniform noise between -0.5 and 0.5.\n    inputs = tf.placeholder(tf.float32, [None])\n    scale = tf.placeholder(tf.float32, [None])\n    layer = self.subclass(scale, [1])\n    noisy, _ = layer(inputs, training=True)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)\n      noisy, = sess.run([noisy], {\n          inputs: values,\n          scale: np.random.uniform(1, 10, size=values.shape),\n      })\n    self.assertFalse(np.allclose(values, noisy, rtol=0, atol=.45))\n    self.assertAllClose(values, noisy, rtol=0, atol=.5)\n\n  def test_quantization(self):\n    # Tests that inputs are quantized to full integer values.\n    inputs = tf.placeholder(tf.float32, [None])\n    scale = tf.placeholder(tf.float32, [None])\n    layer = self.subclass(scale, [1], mean=None)\n    quantized, _ = layer(inputs, training=False)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)\n      quantized, = sess.run([quantized], {\n          inputs: values,\n          scale: np.random.uniform(1, 10, size=values.shape),\n      })\n    self.assertAllClose(np.around(values), quantized, rtol=0, atol=1e-6)\n\n  def test_quantization_mean(self):\n    # Tests that inputs are quantized to integer values with a consistent offset\n    # to the mean.\n    inputs = tf.placeholder(tf.float32, [None])\n    scale = tf.placeholder(tf.float32, [None])\n    mean = tf.placeholder(tf.float32, [None])\n    layer = self.subclass(scale, [1], mean=mean)\n    quantized, _ = layer(inputs, training=False)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)\n      mean_values = np.random.normal(size=values.shape)\n      quantized, = sess.run([quantized], {\n          inputs: values,\n          scale: np.random.uniform(1, 10, size=values.shape),\n          mean: mean_values,\n      })\n    self.assertAllClose(\n        np.around(values - mean_values) + mean_values, quantized,\n        rtol=0, atol=1e-5)\n\n  def test_codec(self):\n    # Tests that inputs are compressed and decompressed correctly, and quantized\n    # to full integer values.\n    inputs = tf.placeholder(tf.float32, [None, None])\n    scale = tf.placeholder(tf.float32, [None, None])\n    layer = self.subclass(\n        scale, [2 ** x for x in range(-10, 10)], mean=None)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)[None]\n      decoded, = sess.run([decoded], {\n          inputs: values,\n          scale: np.random.uniform(25, 75, size=values.shape),\n      })\n    self.assertAllClose(np.around(values), decoded, rtol=0, atol=1e-6)\n\n  def test_codec_mean(self):\n    # Tests that inputs are compressed and decompressed correctly, and quantized\n    # to integer values with an offset consistent with the mean.\n    inputs = tf.placeholder(tf.float32, [None, None])\n    scale = tf.placeholder(tf.float32, [None, None])\n    mean = tf.placeholder(tf.float32, [None, None])\n    layer = self.subclass(\n        scale, [2 ** x for x in range(-10, 10)], mean=mean)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.linspace(-50, 50, 100)[None]\n      mean_values = np.random.normal(size=values.shape)\n      decoded, = sess.run([decoded], {\n          inputs: values,\n          scale: np.random.uniform(25, 75, size=values.shape),\n          mean: mean_values,\n      })\n    self.assertAllClose(\n        np.around(values - mean_values) + mean_values, decoded,\n        rtol=0, atol=1e-5)\n\n  def test_multiple_dimensions(self):\n    # Test the layer with more than one channel and multiple input dimensions.\n    inputs = tf.placeholder(tf.float32, [None, None, None, None])\n    scale = tf.placeholder(tf.float32, [None, None, None, None])\n    layer = self.subclass(\n        scale, [2 ** x for x in range(-10, 10)])\n    noisy, _ = layer(inputs, training=True)\n    quantized, _ = layer(inputs, training=False)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = 10 * np.random.normal(size=(2, 5, 3, 7))\n      noisy, quantized, decoded = sess.run(\n          [noisy, quantized, decoded],\n          {inputs: values, scale: np.random.uniform(5, 15, size=values.shape)})\n    self.assertAllClose(values, noisy, rtol=0, atol=.5)\n    self.assertAllClose(values, quantized, rtol=0, atol=.5)\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n\n  def test_compress(self):\n    # Test compression and decompression, and produce test data for\n    # `test_decompress`. If you set the constant at the end to `True`, this test\n    # will fail and the log will contain the new test data.\n    shape = (2, 7)\n    scale_table = [2 ** x for x in range(-5, 1)]\n    inputs = tf.placeholder(tf.float32, shape)\n    scale = tf.placeholder(tf.float32, shape)\n    indexes = tf.placeholder(tf.int32, shape)\n    layer = self.subclass(scale, scale_table, indexes=indexes)\n    bitstrings = layer.compress(inputs)\n    decoded = layer.decompress(bitstrings)\n    with self.cached_session() as sess:\n      values = 8 * np.random.uniform(size=shape) - 4\n      indexes = np.random.randint(\n          0, len(scale_table), size=shape, dtype=np.int32)\n      sess.run(tf.global_variables_initializer())\n      bitstrings, quantized_cdf, cdf_length, decoded = sess.run(\n          [bitstrings, layer._quantized_cdf, layer._cdf_length, decoded],\n          {inputs: values, layer.indexes: indexes})\n    self.assertAllClose(values, decoded, rtol=0, atol=.5)\n    # Set this constant to `True` to log new test data for `test_decompress`.\n    if False:  # pylint:disable=using-constant-test\n      assert False, (bitstrings, indexes, quantized_cdf, cdf_length, decoded)\n\n  def test_decompress(self):\n    # Test that decompression of values compressed with a previous version\n    # works, i.e. that the file format doesn\'t change across revisions.\n    shape = (2, 7)\n    scale_table = [2 ** x for x in range(-5, 1)]\n    bitstrings = tf.placeholder(tf.string)\n    scale = tf.placeholder(tf.float32, shape)\n    indexes = tf.placeholder(tf.int32, shape)\n    layer = self.subclass(\n        scale, scale_table, indexes=indexes, dtype=tf.float32)\n    decoded = layer.decompress(bitstrings)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      decoded, = sess.run([decoded], {\n          bitstrings: self.bitstrings,\n          layer.indexes: self.indexes,\n          layer._quantized_cdf: self.quantized_cdf,\n          layer._cdf_length: self.cdf_length})\n    self.assertAllClose(self.expected, decoded, rtol=0, atol=1e-6)\n\n  def test_build_decompress(self):\n    # Test that layer can be built when `decompress` is the first call to it.\n    bitstrings = tf.placeholder(tf.string)\n    scale = tf.placeholder(tf.float32, [None, None, None])\n    layer = self.subclass(\n        scale, [2 ** x for x in range(-10, 10)], dtype=tf.float32)\n    layer.decompress(bitstrings)\n    self.assertTrue(layer.built)\n\n  def test_quantile_function(self):\n    # Test that quantile function inverts cumulative.\n    scale = tf.placeholder(tf.float64, [None])\n    layer = self.subclass(scale, [1], dtype=tf.float64)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      quantiles = np.array([1e-5, 1e-2, .1, .5, .6, .8])\n      locations = layer._standardized_quantile(quantiles)\n      locations = tf.constant(locations, tf.float64)\n      values, = sess.run([layer._standardized_cumulative(locations)])\n    self.assertAllClose(quantiles, values, rtol=1e-12, atol=0)\n\n  def test_distribution(self):\n    # Tests that the model represents the underlying distribution convolved\n    # with a uniform.\n    inputs = tf.placeholder(tf.float32, [None, None])\n    scale = tf.placeholder(tf.float32, [None, None])\n    layer = self.subclass(scale, [1], scale_bound=0, mean=None)\n    _, likelihood = layer(inputs, training=False)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      values = np.arange(-5, 1)[:, None]  # must be integers due to quantization\n      scales = 2 ** np.linspace(-3, 3, 10)[None, :]\n      likelihoods, = sess.run([likelihood], {inputs: values, scale: scales})\n    expected = (\n        self.scipy_class.cdf(values + .5, scale=scales) -\n        self.scipy_class.cdf(values - .5, scale=scales))\n    self.assertAllClose(expected, likelihoods, rtol=1e-5, atol=1e-7)\n\n  def test_entropy_estimates(self):\n    # Test that analytical entropy, entropy estimates, and range coding match\n    # each other.\n    inputs = tf.placeholder(tf.float32, [None, None])\n    scale = tf.placeholder(tf.float32, [None, None])\n    layer = self.subclass(\n        scale, [2 ** -10, 1, 10], scale_bound=0, likelihood_bound=0)\n    _, likelihood = layer(inputs, training=True)\n    diff_entropy = tf.reduce_mean(tf.log(likelihood), axis=1)\n    diff_entropy /= -np.log(2)\n    _, likelihood = layer(inputs, training=False)\n    disc_entropy = tf.reduce_mean(tf.log(likelihood), axis=1)\n    disc_entropy /= -np.log(2)\n    bitstrings = layer.compress(inputs)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      scales = np.repeat([layer.scale_table], 10000, axis=0).T\n      values = self.scipy_class.rvs(scale=scales, size=scales.shape)\n      diff_entropy, disc_entropy, bitstrings = sess.run(\n          [diff_entropy, disc_entropy, bitstrings],\n          {inputs: values, scale: scales})\n    codelength = [8 * len(s) for s in bitstrings]\n    codelength = np.array(codelength) / values.shape[1]\n    # The analytical entropy is only going to match the empirical for larger\n    # scales because of the additive uniform noise. For scale values going to\n    # zero, the empirical entropy will converge to zero (the entropy of a\n    # standard uniform) instead of -infty. For large scale values, the additive\n    # noise is negligible.\n    theo_entropy = self.scipy_class.entropy(scale=10) / np.log(2)\n    self.assertAllClose(0, diff_entropy[0], rtol=1e-2, atol=1e-2)\n    self.assertAllClose(theo_entropy, diff_entropy[-1], rtol=1e-2, atol=1e-2)\n    self.assertAllClose(diff_entropy, disc_entropy, rtol=1e-2, atol=1e-2)\n    self.assertAllClose(disc_entropy, codelength, rtol=1e-2, atol=1e-2)\n    # The range coder should have some overhead.\n    self.assertTrue(all(codelength > disc_entropy))\n\n\nclass GaussianConditionalTest(tf.test.TestCase, SymmetricConditionalTest):\n\n  subclass = entropy_models.GaussianConditional\n  scipy_class = scipy.stats.norm\n\n  # Data generated by `test_compress`.\n  # pylint:disable=bad-whitespace,bad-continuation\n  bitstrings = np.array([\n      b""\\xff\\xff\\x13\\xff\\xff\\x0f\\xff\\xef\\xa9\\x000\\xb9\\xffT\\x87\\xffUB"",\n      b""\\x10\\xf1m-\\xf0r\\xac\\x97\\xb6\\xd5"",\n  ], dtype=object)\n\n  indexes = np.array([\n      [1, 2, 3, 4, 2, 2, 1],\n      [5, 5, 1, 5, 3, 2, 3],\n  ], dtype=np.int32)\n\n  quantized_cdf = np.array([\n      [    0,     1, 65534, 65535, 65536,     0,     0,     0,     0],\n      [    0,     1, 65534, 65535, 65536,     0,     0,     0,     0],\n      [    0,     2, 65533, 65535, 65536,     0,     0,     0,     0],\n      [    0,  1491, 64044, 65535, 65536,     0,     0,     0,     0],\n      [    0,    88, 10397, 55138, 65447, 65535, 65536,     0,     0],\n      [    0,   392,  4363, 20205, 45301, 61143, 65114, 65506, 65536],\n  ], dtype=np.int32)\n\n  cdf_length = np.array([5, 5, 5, 5, 7, 9], dtype=np.int32)\n\n  expected = np.array([\n      [-3.,  2.,  1., -3., -1., -3., -4.],\n      [-2.,  2., -2.,  2.,  4.,  1.,  0.],\n  ], dtype=np.float32)\n  # pylint:enable=bad-whitespace,bad-continuation\n\n\nclass LogisticConditionalTest(tf.test.TestCase, SymmetricConditionalTest):\n\n  subclass = entropy_models.LogisticConditional\n  scipy_class = scipy.stats.logistic\n\n  # Data generated by `test_compress`.\n  # pylint:disable=bad-whitespace,bad-continuation\n  bitstrings = np.array([\n      b""\\xff\\xff\\x13\\xff\\xff\\x0e\\x17\\xfd\\xb5B\\x03\\xff\\xf4\\x11"",\n      b"",yh\\x13)\\x12F\\xfb"",\n  ], dtype=object)\n\n  indexes = np.array([\n      [1, 2, 3, 4, 2, 2, 1],\n      [5, 5, 1, 5, 3, 2, 3]\n  ], dtype=np.int32)\n\n  quantized_cdf = np.array([\n      [    0,     1, 65534, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0],\n      [    0,    22, 65513, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0],\n      [    0,  1178, 64357, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0],\n      [    0,   159,  7809, 57721, 65371, 65530, 65536,     0,     0,\n           0,     0,     0,     0,     0,     0,     0,     0],\n      [    0,    52,   431,  3100, 17617, 47903, 62420, 65089, 65468,\n       65520, 65536,     0,     0,     0,     0,     0,     0],\n      [    0,    62,   230,   683,  1884,  4935, 11919, 24706, 40758,\n       53545, 60529, 63580, 64781, 65234, 65402, 65464, 65536],\n  ], dtype=np.int32)\n\n  cdf_length = np.array([ 5,  5,  5,  7, 11, 17], dtype=np.int32)\n\n  expected = np.array([\n      [-3.,  2.,  1., -3., -1., -3., -4.],\n      [-2.,  2., -2.,  2.,  4.,  1.,  0.],\n  ], dtype=np.float32)\n  # pylint:enable=bad-whitespace,bad-continuation\n\n\nclass LaplacianConditionalTest(tf.test.TestCase, SymmetricConditionalTest):\n\n  subclass = entropy_models.LaplacianConditional\n  scipy_class = scipy.stats.laplace\n\n  # Data generated by `test_compress`.\n  # pylint:disable=bad-whitespace,bad-continuation\n  bitstrings = np.array([\n      b""\\xff\\xff\\x13\\xff\\xff\\x0e\\xea\\xc1\\xd9n\'\\xff\\xfe*"",\n      b""\\x1b\\x9c\\xd3\\x06\\xde_\\xc0$"",\n  ], dtype=object)\n\n  indexes = np.array([\n      [1, 2, 3, 4, 2, 2, 1],\n      [5, 5, 1, 5, 3, 2, 3],\n  ], dtype=np.int32)\n\n  quantized_cdf = np.array([\n      [    0,     1, 65534, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0],\n      [    0,    11, 65524, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0],\n      [    0,   600, 64935, 65535, 65536,     0,     0,     0,     0,\n           0,     0,     0,     0,     0,     0],\n      [    0,    80,  4433, 61100, 65453, 65533, 65536,     0,     0,\n           0,     0,     0,     0,     0,     0],\n      [    0,   191,  1602, 12025, 53451, 63874, 65285, 65476, 65536,\n           0,     0,     0,     0,     0,     0],\n      [    0,    85,   315,   940,  2640,  7262, 19825, 45612, 58175,\n       62797, 64497, 65122, 65352, 65437, 65536],\n  ], dtype=np.int32)\n\n  cdf_length = np.array([ 5,  5,  5,  7,  9, 15], dtype=np.int32)\n\n  expected = np.array([\n      [-3.,  2.,  1., -3., -1., -3., -4.],\n      [-2.,  2., -2.,  2.,  4.,  1.,  0.],\n  ], dtype=np.float32)\n  # pylint:enable=bad-whitespace,bad-continuation\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/layers/gdn.py,20,"b'# Lint as: python3\n# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""GDN layer.""""""\n\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow_compression.python.layers import parameterizers\n\n\n_default_beta_param = parameterizers.NonnegativeParameterizer(\n    minimum=1e-6)\n_default_gamma_param = parameterizers.NonnegativeParameterizer()\n\n\n__all__ = [\n    ""GDN"",\n]\n\n\nclass GDN(tf.keras.layers.Layer):\n  """"""Generalized divisive normalization layer.\n\n  Based on the papers:\n\n  > ""Density modeling of images using a generalized normalization\n  > transformation""<br />\n  > J. Ball\xc3\xa9, V. Laparra, E.P. Simoncelli<br />\n  > https://arxiv.org/abs/1511.06281\n\n  > ""End-to-end optimized image compression""<br />\n  > J. Ball\xc3\xa9, V. Laparra, E.P. Simoncelli<br />\n  > https://arxiv.org/abs/1611.01704\n\n  Implements an activation function that is essentially a multivariate\n  generalization of a particular sigmoid-type function:\n\n  ```\n  y[i] = x[i] / sqrt(beta[i] + sum_j(gamma[j, i] * x[j]^2))\n  ```\n\n  where `i` and `j` run over channels. This implementation never sums across\n  spatial dimensions. It is similar to local response normalization, but much\n  more flexible, as `beta` and `gamma` are trainable parameters.\n  """"""\n\n  def __init__(self,\n               inverse=False,\n               rectify=False,\n               gamma_init=.1,\n               data_format=""channels_last"",\n               beta_parameterizer=_default_beta_param,\n               gamma_parameterizer=_default_gamma_param,\n               **kwargs):\n    """"""Initializer.\n\n    Arguments:\n      inverse: Boolean. If `False` (default), compute GDN response. If `True`,\n        compute IGDN response (one step of fixed point iteration to invert GDN;\n        the division is replaced by multiplication).\n      rectify: Boolean. If `True`, apply a `relu` nonlinearity to the inputs\n        before calculating GDN response.\n      gamma_init: Float. The gamma matrix will be initialized as the identity\n        matrix multiplied with this value. If set to zero, the layer is\n        effectively initialized to the identity operation, since beta is\n        initialized as one. A good default setting is somewhere between 0 and\n        0.5.\n      data_format: Format of input tensor. Currently supports `\'channels_first\'`\n        and `\'channels_last\'`.\n      beta_parameterizer: `Parameterizer` object for beta parameter. Defaults\n        to `NonnegativeParameterizer` with a minimum value of 1e-6.\n      gamma_parameterizer: `Parameterizer` object for gamma parameter.\n        Defaults to `NonnegativeParameterizer` with a minimum value of 0.\n      **kwargs: Other keyword arguments passed to superclass (`Layer`).\n    """"""\n    super().__init__(**kwargs)\n    self._inverse = bool(inverse)\n    self._rectify = bool(rectify)\n    self._gamma_init = float(gamma_init)\n    self._data_format = str(data_format)\n    self._beta_parameterizer = beta_parameterizer\n    self._gamma_parameterizer = gamma_parameterizer\n\n    if self.data_format not in (""channels_first"", ""channels_last""):\n      raise ValueError(""Unknown data format: \'{}\'."".format(self.data_format))\n\n    self.input_spec = tf.keras.layers.InputSpec(min_ndim=2)\n\n  @property\n  def inverse(self):\n    return self._inverse\n\n  @property\n  def rectify(self):\n    return self._rectify\n\n  @property\n  def gamma_init(self):\n    return self._gamma_init\n\n  @property\n  def data_format(self):\n    return self._data_format\n\n  @property\n  def beta_parameterizer(self):\n    return self._beta_parameterizer\n\n  @beta_parameterizer.setter\n  def beta_parameterizer(self, val):\n    if self.built:\n      raise RuntimeError(\n          ""Can\'t set `beta_parameterizer` once layer has been built."")\n    self._beta_parameterizer = val\n\n  @property\n  def gamma_parameterizer(self):\n    return self._gamma_parameterizer\n\n  @gamma_parameterizer.setter\n  def gamma_parameterizer(self, val):\n    if self.built:\n      raise RuntimeError(\n          ""Can\'t set `gamma_parameterizer` once layer has been built."")\n    self._gamma_parameterizer = val\n\n  @property\n  def beta(self):\n    return self._beta.value()\n\n  @property\n  def gamma(self):\n    return self._gamma.value()\n\n  def _channel_axis(self):\n    return {""channels_first"": 1, ""channels_last"": -1}[self.data_format]\n\n  def build(self, input_shape):\n    channel_axis = self._channel_axis()\n    input_shape = tf.TensorShape(input_shape)\n    num_channels = input_shape.as_list()[channel_axis]\n    if num_channels is None:\n      raise ValueError(""The channel dimension of the inputs to `GDN` ""\n                       ""must be defined."")\n    self._input_rank = input_shape.ndims\n    self.input_spec = tf.keras.layers.InputSpec(\n        ndim=input_shape.ndims, axes={channel_axis: num_channels})\n\n    # Sorry, lint, but these objects really are callable ...\n    # pylint:disable=not-callable\n    self._beta = self.beta_parameterizer(\n        name=""beta"", shape=[num_channels], dtype=self.dtype,\n        getter=self.add_weight, initializer=tf.initializers.ones())\n\n    self._gamma = self.gamma_parameterizer(\n        name=""gamma"", shape=[num_channels, num_channels], dtype=self.dtype,\n        getter=self.add_weight,\n        initializer=tf.initializers.identity(gain=self._gamma_init))\n    # pylint:enable=not-callable\n\n    super().build(input_shape)\n\n  def call(self, inputs):\n    inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n    ndim = self._input_rank\n\n    if self.rectify:\n      inputs = tf.nn.relu(inputs)\n\n    # Compute normalization pool.\n    if ndim == 2:\n      norm_pool = tf.linalg.matmul(tf.math.square(inputs), self.gamma)\n      norm_pool = tf.nn.bias_add(norm_pool, self.beta)\n    elif self.data_format == ""channels_last"" and ndim <= 4:\n      # TODO(unassigned): This branch should also work for ndim == 5, but\n      # currently triggers a bug in TF.\n      shape = self.gamma.shape.as_list()\n      gamma = tf.reshape(self.gamma, (ndim - 2) * [1] + shape)\n      norm_pool = tf.nn.convolution(tf.math.square(inputs), gamma, ""VALID"")\n      norm_pool = tf.nn.bias_add(norm_pool, self.beta)\n    else:  # generic implementation\n      # This puts channels in the last dimension regardless of input.\n      norm_pool = tf.linalg.tensordot(\n          tf.math.square(inputs), self.gamma, [[self._channel_axis()], [0]])\n      norm_pool += self.beta\n      if self.data_format == ""channels_first"":\n        # Return to channels_first format if necessary.\n        axes = list(range(ndim - 1))\n        axes.insert(1, ndim - 1)\n        norm_pool = tf.transpose(norm_pool, axes)\n\n    if self.inverse:\n      norm_pool = tf.math.sqrt(norm_pool)\n    else:\n      norm_pool = tf.math.rsqrt(norm_pool)\n    outputs = inputs * norm_pool\n\n    if not tf.executing_eagerly():\n      outputs.set_shape(self.compute_output_shape(inputs.shape))\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape(input_shape)\n'"
tensorflow_compression/python/layers/gdn_test.py,15,"b'# Lint as: python3\n# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of GDN layer.""""""\n\nimport tensorflow.compat.v2 as tf\n\nfrom tensorflow_compression.python.layers import gdn\n\n\nclass GDNTest(tf.test.TestCase):\n\n  def test_invalid_data_format_raises_error(self):\n    x = tf.random.uniform((1, 2, 3, 4), dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      gdn.GDN(inverse=False, rectify=False, data_format=""NHWC"")(x)\n\n  def test_vector_input_raises_error(self):\n    x = tf.random.uniform((3,), dtype=tf.float32)\n    with self.assertRaises(ValueError):\n      gdn.GDN(inverse=False, rectify=False, data_format=""channels_last"")(x)\n    with self.assertRaises(ValueError):\n      gdn.GDN(inverse=True, rectify=True, data_format=""channels_first"")(x)\n\n  def test_channels_last_has_correct_output(self):\n    # This tests that the layer produces the correct output for a number of\n    # different input dimensionalities with \'channels_last\' data format.\n    for ndim in [2, 3, 4, 5, 6]:\n      x = tf.random.uniform((1, 2, 3, 4, 5, 6)[:ndim], dtype=tf.float32)\n      y = gdn.GDN(inverse=False, rectify=False, data_format=""channels_last"")(x)\n      self.assertEqual(x.shape, y.shape)\n      self.assertAllClose(y, x / tf.sqrt(1 + .1 * (x ** 2)), rtol=0, atol=1e-6)\n\n  def test_channels_first_has_correct_output(self):\n    # This tests that the layer produces the correct output for a number of\n    # different input dimensionalities with \'channels_first\' data format.\n    for ndim in [2, 3, 4, 5, 6]:\n      x = tf.random.uniform((6, 5, 4, 3, 2, 1)[:ndim], dtype=tf.float32)\n      y = gdn.GDN(inverse=False, rectify=False, data_format=""channels_first"")(x)\n      self.assertEqual(x.shape, y.shape)\n      self.assertAllClose(y, x / tf.sqrt(1 + .1 * (x ** 2)), rtol=0, atol=1e-6)\n\n  def test_igdn_has_correct_output(self):\n    x = tf.random.uniform((1, 2, 3, 4), dtype=tf.float32)\n    y = gdn.GDN(inverse=True, rectify=False)(x)\n    self.assertEqual(x.shape, y.shape)\n    self.assertAllClose(y, x * tf.sqrt(1 + .1 * (x ** 2)), rtol=0, atol=1e-6)\n\n  def test_rgdn_has_correct_output(self):\n    x = tf.random.uniform((1, 2, 3, 4), -.5, .5, dtype=tf.float32)\n    y = gdn.GDN(inverse=False, rectify=True)(x)\n    self.assertEqual(x.shape, y.shape)\n    x = tf.maximum(x, 0)\n    self.assertAllClose(y, x / tf.sqrt(1 + .1 * (x ** 2)), rtol=0, atol=1e-6)\n\n  def test_variables_receive_gradients(self):\n    x = tf.random.uniform((1, 2), dtype=tf.float32)\n    layer = gdn.GDN(inverse=False, rectify=True)\n    with tf.GradientTape() as g:\n      y = layer(x)\n    grads = g.gradient(y, layer.trainable_variables)\n    self.assertLen(grads, 2)\n    self.assertNotIn(None, grads)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/layers/initializers.py,4,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Initializers for layer classes.""""""\n\nimport tensorflow.compat.v1 as tf\n\n\n__all__ = [\n    ""IdentityInitializer"",\n]\n\n\nclass IdentityInitializer(object):\n  """"""Initialize to the identity kernel with the given shape.\n\n  This creates an n-D kernel suitable for `SignalConv*` with the requested\n  support that produces an output identical to its input (except possibly at the\n  signal boundaries).\n\n  Note: The identity initializer in `tf.initializers` is only suitable for\n  matrices, not for n-D convolution kernels (i.e., no spatial support).\n  """"""\n\n  def __init__(self, gain=1):\n    self.gain = float(gain)\n\n  def __call__(self, shape, dtype=None, partition_info=None):\n    del partition_info  # unused\n    assert len(shape) > 2, shape\n\n    support = tuple(shape[:-2]) + (1, 1)\n    indices = [[s // 2 for s in support]]\n    updates = tf.constant([self.gain], dtype=dtype)\n    kernel = tf.scatter_nd(indices, updates, support)\n\n    assert shape[-2] == shape[-1], shape\n    if shape[-1] != 1:\n      kernel *= tf.eye(shape[-1], dtype=dtype)\n\n    return kernel\n'"
tensorflow_compression/python/layers/parameterizers.py,13,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Parameterizers for layer classes.""""""\n\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow_compression.python.ops import math_ops\nfrom tensorflow_compression.python.ops import spectral_ops\n\n\n__all__ = [\n    ""Parameter"",\n    ""Parameterizer"",\n    ""StaticParameterizer"",\n    ""RDFTParameterizer"",\n    ""NonnegativeParameterizer"",\n]\n\n\nclass Parameter(object):\n  """"""Reparameterized `Layer` variable.\n\n  This object represents a parameter of a `tf.keras.layer.Layer` object which\n  isn\'t directly stored in a `tf.Variable`. Instead, the value is computed\n  on-demand by calling its `value()` method.\n  """"""\n\n  def __init__(self, value):\n    if not callable(value):\n      raise TypeError(""`value` must be callable without arguments."")\n    self.value = value\n\n\nclass Parameterizer(object):\n  """"""Parameterization object (abstract base class).\n\n  `Parameterizer`s are immutable objects designed to facilitate\n  reparameterization of model parameters (tensor variables). They are called\n  just like `tf.get_variable` with an additional argument `getter` specifying\n  the actual function call to generate a variable (in many cases, `getter` would\n  be `tf.get_variable`).\n\n  To achieve reparameterization, a `Parameterizer` wraps the provided\n  initializer, regularizer, and the returned variable in its own TensorFlow\n  code.\n  """"""\n  pass\n\n\nclass StaticParameterizer(Parameterizer):\n  """"""A parameterizer that returns a non-variable.\n\n  No variables are created, and `getter` is ignored. If `value` is a `Tensor`,\n  the parameter can depend on some other computation. Otherwise, it never\n  changes.\n  """"""\n\n  def __init__(self, value):\n    """"""Initializer.\n\n    Arguments:\n      value: Either a constant or `Tensor` value, or a callable which returns\n        such a thing given a shape and dtype argument (for example, an\n        initializer).\n    """"""\n    self.value = value\n\n  def __call__(self, getter, name, shape, dtype, initializer, regularizer=None):\n    del getter, name, initializer  # unused\n    if regularizer is not None:\n      raise NotImplementedError(""Regularizers are not currently supported for ""\n                                ""static parameterizers."")\n    if callable(self.value):\n      # Treat value as initializer.\n      return Parameter(lambda: self.value(shape, dtype=dtype))\n    else:\n      return Parameter(lambda: self.value)\n\n\nclass RDFTParameterizer(Parameterizer):\n  """"""Object encapsulating RDFT reparameterization.\n\n  This uses the real-input discrete Fourier transform (RDFT) of a kernel as\n  its parameterization. The inverse RDFT is applied to the variable to produce\n  the parameter.\n\n  (see https://en.wikipedia.org/wiki/Discrete_Fourier_transform)\n  """"""\n\n  def __init__(self, dc=True):\n    """"""Initializer.\n\n    Arguments:\n      dc: Boolean. If `False`, the DC component of the kernel RDFTs is not\n        represented, forcing the filters to be highpass. Defaults to `True`.\n    """"""\n    self.dc = bool(dc)\n\n  def __call__(self, getter, name, shape, dtype, initializer, regularizer=None):\n    if all(s == 1 for s in shape[:-2]):\n      return getter(name=name, shape=shape, dtype=dtype,\n                    initializer=initializer, regularizer=regularizer)\n    var_shape = shape\n    var_dtype = dtype\n    size = var_shape[0]\n    for s in var_shape[1:-2]:\n      size *= s\n    if self.dc:\n      rdft_shape = (size, var_shape[-2] * var_shape[-1])\n    else:\n      rdft_shape = (size - 1, var_shape[-2] * var_shape[-1])\n    rdft_dtype = var_dtype\n    rdft_name = name + ""_rdft""\n\n    def rdft_initializer(shape, dtype=None, partition_info=None):\n      """"""Initializer wrapper.""""""\n      del partition_info  # Ignored for TF 1/2 compatibility.\n      assert tuple(shape) == rdft_shape, shape\n      assert dtype == rdft_dtype, dtype\n      init = initializer(var_shape, dtype=var_dtype)\n      init = tf.reshape(init, (-1, rdft_shape[-1]))\n      irdft_matrix = spectral_ops.irdft_matrix(var_shape[:-2], dtype=var_dtype)\n      if not self.dc:\n        irdft_matrix = irdft_matrix[:, 1:]\n      init = tf.linalg.matmul(irdft_matrix, init, transpose_a=True)\n      return init\n\n    def reparam(rdft):\n      irdft_matrix = spectral_ops.irdft_matrix(var_shape[:-2], dtype=var_dtype)\n      if not self.dc:\n        irdft_matrix = irdft_matrix[:, 1:]\n      var = tf.linalg.matmul(irdft_matrix, rdft)\n      var = tf.reshape(var, var_shape)\n      return var\n\n    reparam_regularizer = None\n    if regularizer is not None:\n      reparam_regularizer = lambda rdft: regularizer(reparam(rdft))\n\n    rdft = getter(\n        name=rdft_name, shape=rdft_shape, dtype=rdft_dtype,\n        initializer=rdft_initializer, regularizer=reparam_regularizer)\n    return Parameter(lambda: reparam(rdft))\n\n\nclass NonnegativeParameterizer(Parameterizer):\n  """"""Object encapsulating nonnegative parameterization as needed for GDN.\n\n  The variable is subjected to an invertible transformation that slows down the\n  learning rate for small values.\n  """"""\n\n  def __init__(self, minimum=0, reparam_offset=2 ** -18):\n    """"""Initializer.\n\n    Arguments:\n      minimum: Float. Lower bound for parameters (defaults to zero).\n      reparam_offset: Float. Offset added to the reparameterization of beta and\n        gamma. The parameterization of beta and gamma as their square roots lets\n        the training slow down when their values are close to zero, which is\n        desirable as small values in the denominator can lead to a situation\n        where gradient noise on beta/gamma leads to extreme amounts of noise in\n        the GDN activations. However, without the offset, we would get zero\n        gradients if any elements of beta or gamma were exactly zero, and thus\n        the training could get stuck. To prevent this, we add this small\n        constant. The default value was empirically determined as a good\n        starting point. Making it bigger potentially leads to more gradient\n        noise on the activations, making it too small may lead to numerical\n        precision issues.\n    """"""\n    self.minimum = float(minimum)\n    self.reparam_offset = float(reparam_offset)\n\n  def __call__(self, getter, name, shape, dtype, initializer, regularizer=None):\n    pedestal = tf.constant(self.reparam_offset ** 2, dtype=dtype)\n    bound = tf.constant(\n        (self.minimum + self.reparam_offset ** 2) ** .5, dtype=dtype)\n    reparam_name = ""reparam_"" + name\n\n    def reparam_initializer(shape, dtype=None, partition_info=None):\n      del partition_info  # Ignored for TF 1/2 compatibility.\n      # We recreate pedestal to decouple the initializer from the model graph.\n      pedestal = tf.constant(self.reparam_offset ** 2, dtype=dtype)\n      init = initializer(shape, dtype=dtype)\n      init = tf.math.sqrt(tf.math.maximum(init + pedestal, pedestal))\n      return init\n\n    def reparam(var):\n      var = math_ops.lower_bound(var, bound)\n      var = tf.math.square(var) - pedestal\n      return var\n\n    reparam_regularizer = None\n    if regularizer is not None:\n      reparam_regularizer = lambda var: regularizer(reparam(var))\n\n    var = getter(\n        name=reparam_name, shape=shape, dtype=dtype,\n        initializer=reparam_initializer, regularizer=reparam_regularizer)\n    return Parameter(lambda: reparam(var))\n'"
tensorflow_compression/python/layers/parameterizers_test.py,9,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of parameterizers.""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.layers import parameterizers\n\n\n@test_util.deprecated_graph_mode_only\nclass ParameterizersTest(tf.test.TestCase):\n\n  def _test_parameterizer(self, param, init, shape):\n    var = param(\n        getter=tf.get_variable, name=""test"", shape=shape, dtype=tf.float32,\n        initializer=init, regularizer=None).value()\n    with self.cached_session() as sess:\n      tf.global_variables_initializer().run()\n      var, = sess.run([var])\n    return var\n\n  def test_static_parameterizer(self):\n    shape = (1, 2, 3, 4)\n    var = self._test_parameterizer(\n        parameterizers.StaticParameterizer(tf.initializers.zeros()),\n        tf.initializers.random_uniform(), shape)\n    self.assertEqual(var.shape, shape)\n    self.assertAllClose(var, np.zeros(shape), rtol=0, atol=1e-7)\n\n  def test_rdft_parameterizer(self):\n    shape = (3, 4, 2, 1)\n    var = self._test_parameterizer(\n        parameterizers.RDFTParameterizer(),\n        tf.initializers.ones(), shape)\n    self.assertEqual(var.shape, shape)\n    self.assertAllClose(var, np.ones(shape), rtol=0, atol=1e-6)\n\n  def test_nonnegative_parameterizer(self):\n    shape = (1, 2, 3, 4)\n    var = self._test_parameterizer(\n        parameterizers.NonnegativeParameterizer(),\n        tf.initializers.random_uniform(), shape)\n    self.assertEqual(var.shape, shape)\n    self.assertTrue(np.all(var >= 0))\n\n  def test_positive_parameterizer(self):\n    shape = (1, 2, 3, 4)\n    var = self._test_parameterizer(\n        parameterizers.NonnegativeParameterizer(minimum=.1),\n        tf.initializers.random_uniform(), shape)\n    self.assertEqual(var.shape, shape)\n    self.assertTrue(np.all(var >= .1))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/layers/signal_conv.py,70,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Swiss army tool for convolutions.""""""\n\nimport functools\n\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow_compression.python.layers import parameterizers\nfrom tensorflow_compression.python.ops import padding_ops\n\n\n__all__ = [\n    ""SignalConv1D"",\n    ""SignalConv2D"",\n    ""SignalConv3D"",\n]\n\n\nclass _SignalConv(tf.keras.layers.Layer):\n  """"""{rank}D convolution layer.\n\n  This layer creates a filter kernel that is convolved or cross correlated with\n  the layer input to produce an output tensor. The main difference of this class\n  to `tf.layers.Conv{rank}D` is how padding, up- and downsampling, and alignment\n  is handled. It supports much more flexible options for structuring the linear\n  transform.\n\n  In general, the outputs are equivalent to a composition of:\n  1. an upsampling step (if `strides_up > 1`)\n  2. a convolution or cross correlation\n  3. a downsampling step (if `strides_down > 1`)\n  4. addition of a bias vector (if `use_bias == True`)\n  5. a pointwise nonlinearity (if `activation is not None`)\n\n  For more information on what the difference between convolution and cross\n  correlation is, see [this](https://en.wikipedia.org/wiki/Convolution) and\n  [this](https://en.wikipedia.org/wiki/Cross-correlation) Wikipedia article,\n  respectively. Note that the distinction between convolution and cross\n  correlation is occasionally blurred (one may use convolution as an umbrella\n  term for both). For a discussion of up-/downsampling, refer to the articles\n  about [upsampling](https://en.wikipedia.org/wiki/Upsampling) and\n  [decimation](https://en.wikipedia.org/wiki/Decimation_(signal_processing)). A\n  more in-depth treatment of all of these operations can be found in:\n\n  > ""Discrete-Time Signal Processing""<br />\n  > Oppenheim, Schafer, Buck (Prentice Hall)\n\n  For purposes of this class, the center position of a kernel is always\n  considered to be at `K // 2`, where `K` is the support length of the kernel.\n  This implies that in the `\'same_*\'` padding modes, all of the following\n  operations will produce the same result if applied to the same inputs, which\n  is not generally true for convolution operations as implemented by\n  `tf.nn.convolution` or `tf.layers.Conv?D` (numbers represent kernel\n  coefficient values):\n\n  - convolve with `[1, 2, 3]`\n  - convolve with `[0, 1, 2, 3, 0]`\n  - convolve with `[0, 1, 2, 3]`\n  - correlate with `[3, 2, 1]`\n  - correlate with `[0, 3, 2, 1, 0]`\n  - correlate with `[0, 3, 2, 1]`\n\n  Available padding (boundary handling) modes:\n\n  - `\'valid\'`: This always yields the maximum number of output samples that can\n    be computed without making any assumptions about the values outside of the\n    support of the input tensor. The padding semantics are always applied to the\n    inputs. In contrast, even though `tf.nn.conv2d_transpose` implements\n    upsampling, in `\'VALID\'` mode it will produce an output tensor with *larger*\n    support than the input tensor (because it is the transpose of a `\'VALID\'`\n    downsampled convolution).\n\n    Examples (numbers represent indexes into the respective tensors, periods\n    represent skipped spatial positions):\n\n    `kernel_support = 5` and `strides_down = 2`:\n    ```\n    inputs:  |0 1 2 3 4 5 6 7 8|\n    outputs: |    0 . 1 . 2    |\n    ```\n    ```\n    inputs:  |0 1 2 3 4 5 6 7|\n    outputs: |    0 . 1 .    |\n    ```\n\n    `kernel_support = 3`, `strides_up = 2`, and `extra_pad_end = True`:\n    ```\n    inputs:   |0 . 1 . 2 . 3 . 4 .|\n    outputs:  |  0 1 2 3 4 5 6 7  |\n    ```\n\n    `kernel_support = 3`, `strides_up = 2`, and `extra_pad_end = False`:\n    ```\n    inputs:   |0 . 1 . 2 . 3 . 4|\n    outputs:  |  0 1 2 3 4 5 6  |\n    ```\n\n  - `\'same_zeros\'`: Values outside of the input tensor support are assumed to be\n    zero. Similar to `\'SAME\'` in `tf.nn.convolution`, but with different\n    padding. In `\'SAME\'`, the spatial alignment of the output depends on the\n    input shape. Here, the output alignment depends only on the kernel support\n    and the strides, making alignment more predictable. The first sample in the\n    output is always spatially aligned with the first sample in the input.\n\n    Examples (numbers represent indexes into the respective tensors, periods\n    represent skipped spatial positions):\n\n    `kernel_support = 5` and `strides_down = 2`:\n    ```\n    inputs:  |0 1 2 3 4 5 6 7 8|\n    outputs: |0 . 1 . 2 . 3 . 4|\n    ```\n    ```\n    inputs:  |0 1 2 3 4 5 6 7|\n    outputs: |0 . 1 . 2 . 3 .|\n    ```\n\n    `kernel_support = 3`, `strides_up = 2`, and `extra_pad_end = True`:\n    ```\n    inputs:   |0 . 1 . 2 . 3 . 4 .|\n    outputs:  |0 1 2 3 4 5 6 7 8 9|\n    ```\n\n    `kernel_support = 3`, `strides_up = 2`, and `extra_pad_end = False`:\n    ```\n    inputs:   |0 . 1 . 2 . 3 . 4|\n    outputs:  |0 1 2 3 4 5 6 7 8|\n    ```\n\n  - `\'same_reflect\'`: Values outside of the input tensor support are assumed to\n    be reflections of the samples inside. Note that this is the same padding as\n    implemented by `tf.pad` in the `\'REFLECT\'` mode (i.e. with the symmetry axis\n    on the samples rather than between). The output alignment is identical to\n    the `\'same_zeros\'` mode.\n\n    Examples: see `\'same_zeros\'`.\n\n    When applying several convolutions with down- or upsampling in a sequence,\n    it can be helpful to keep the axis of symmetry for the reflections\n    consistent. To do this, set `extra_pad_end = False` and make sure that the\n    input has length `M`, such that `M % S == 1`, where `S` is the product of\n    stride lengths of all subsequent convolutions. Example for subsequent\n    downsampling (here, `M = 9`, `S = 4`, and `^` indicate the symmetry axes\n    for reflection):\n\n    ```\n    inputs:       |0 1 2 3 4 5 6 7 8|\n    intermediate: |0 . 1 . 2 . 3 . 4|\n    outputs:      |0 . . . 1 . . . 2|\n                   ^               ^\n    ```\n\n  Note that due to limitations of the underlying operations, not all\n  combinations of arguments are currently implemented. In this case, this class\n  will throw a `NotImplementedError` exception.\n\n  Speed tips:\n\n  - Prefer combining correlations with downsampling, and convolutions with\n    upsampling, as the underlying ops implement these combinations directly.\n  - If that isn\'t desirable, prefer using odd-length kernel supports, since\n    odd-length kernels can be flipped if necessary, to use the fastest\n    implementation available.\n  - Combining upsampling and downsampling (for rational resampling ratios)\n    is relatively slow, because no underlying ops exist for that use case.\n    Downsampling in this case is implemented by discarding computed output\n    values.\n  - Note that `channel_separable` is only implemented for 1D and 2D. Also,\n    upsampled channel-separable convolutions are currently only implemented for\n    `filters == 1`. When using `channel_separable`, prefer using identical\n    strides in all dimensions to maximize performance.\n  """"""\n\n  def __init__(self, filters, kernel_support,\n               corr=False, strides_down=1, strides_up=1, padding=""valid"",\n               extra_pad_end=True, channel_separable=False,\n               data_format=""channels_last"",\n               activation=None, use_bias=False, use_explicit=True,\n               kernel_initializer=tf.initializers.variance_scaling(),\n               bias_initializer=tf.initializers.zeros(),\n               kernel_regularizer=None, bias_regularizer=None,\n               kernel_parameterizer=parameterizers.RDFTParameterizer(),\n               bias_parameterizer=None,\n               **kwargs):\n    """"""Initializer.\n\n    Arguments:\n      filters: Integer. If `not channel_separable`, specifies the total number\n        of filters, which is equal to the number of output channels. Otherwise,\n        specifies the number of filters per channel, which makes the number of\n        output channels equal to `filters` times the number of input channels.\n      kernel_support: An integer or iterable of {rank} integers, specifying the\n        length of the convolution/correlation window in each dimension.\n      corr: Boolean. If True, compute cross correlation. If False, convolution.\n      strides_down: An integer or iterable of {rank} integers, specifying an\n        optional downsampling stride after the convolution/correlation.\n      strides_up: An integer or iterable of {rank} integers, specifying an\n        optional upsampling stride before the convolution/correlation.\n      padding: String. One of the supported padding modes (see above).\n      extra_pad_end: Boolean. When upsampling, use extra skipped samples at the\n        end of each dimension (default). For examples, refer to the discussion\n        of padding modes above.\n      channel_separable: Boolean. If `False` (default), each output channel is\n        computed by summing over all filtered input channels. If `True`, each\n        output channel is computed from only one input channel, and `filters`\n        specifies the number of filters per channel. The output channels are\n        ordered such that the first block of `filters` channels is computed from\n        the first input channel, the second block from the second input channel,\n        etc.\n      data_format: String, one of `channels_last` (default) or `channels_first`.\n        The ordering of the input dimensions. `channels_last` corresponds to\n        input tensors with shape `(batch, ..., channels)`, while\n        `channels_first` corresponds to input tensors with shape `(batch,\n        channels, ...)`.\n      activation: Activation function or `None`.\n      use_bias: Boolean, whether an additive constant will be applied to each\n        output channel.\n      use_explicit: Boolean, whether to use `EXPLICIT` padding mode (supported\n        in TensorFlow >1.14).\n      kernel_initializer: An initializer for the filter kernel.\n      bias_initializer: An initializer for the bias vector.\n      kernel_regularizer: Optional regularizer for the filter kernel.\n      bias_regularizer: Optional regularizer for the bias vector.\n      kernel_parameterizer: Reparameterization applied to filter kernel. If not\n        `None`, must be a `Parameterizer` object. Defaults to\n        `RDFTParameterizer`.\n      bias_parameterizer: Reparameterization applied to bias. If not `None`,\n        must be a `Parameterizer` object. Defaults to `None`.\n      **kwargs: Other keyword arguments passed to superclass (`Layer`).\n    """"""\n    super(_SignalConv, self).__init__(**kwargs)\n\n    self._filters = int(filters)\n    self._kernel_support = self._normalized_tuple(\n        kernel_support, ""kernel_support"")\n    self._corr = bool(corr)\n    self._strides_down = self._normalized_tuple(strides_down, ""strides_down"")\n    self._strides_up = self._normalized_tuple(strides_up, ""strides_up"")\n    self._padding = str(padding).lower()\n    try:\n      self._pad_mode = {\n          ""valid"": None,\n          ""same_zeros"": ""CONSTANT"",\n          ""same_reflect"": ""REFLECT"",\n      }[self.padding]\n    except KeyError:\n      raise ValueError(""Unsupported padding mode: \'{}\'"".format(padding))\n    self._extra_pad_end = bool(extra_pad_end)\n    self._channel_separable = bool(channel_separable)\n    self._data_format = str(data_format)\n    self._activation = activation\n    self._use_bias = bool(use_bias)\n    self._use_explicit = bool(use_explicit)\n    self._kernel_initializer = kernel_initializer\n    self._bias_initializer = bias_initializer\n    self._kernel_regularizer = kernel_regularizer\n    self._bias_regularizer = bias_regularizer\n    self._kernel_parameterizer = kernel_parameterizer\n    self._bias_parameterizer = bias_parameterizer\n\n    if self.data_format not in (""channels_first"", ""channels_last""):\n      raise ValueError(""Unknown data format: \'{}\'."".format(self.data_format))\n\n    self.input_spec = tf.keras.layers.InputSpec(ndim=self._rank + 2)\n\n  @property\n  def filters(self):\n    return self._filters\n\n  @property\n  def kernel_support(self):\n    return self._kernel_support\n\n  @property\n  def corr(self):\n    return self._corr\n\n  @property\n  def strides_down(self):\n    return self._strides_down\n\n  @property\n  def strides_up(self):\n    return self._strides_up\n\n  @property\n  def padding(self):\n    return self._padding\n\n  @property\n  def extra_pad_end(self):\n    return self._extra_pad_end\n\n  @property\n  def channel_separable(self):\n    return self._channel_separable\n\n  @property\n  def data_format(self):\n    return self._data_format\n\n  @property\n  def activation(self):\n    return self._activation\n\n  @property\n  def use_bias(self):\n    return self._use_bias\n\n  @property\n  def use_explicit(self):\n    return self._use_explicit\n\n  @property\n  def kernel_initializer(self):\n    return self._kernel_initializer\n\n  @property\n  def bias_initializer(self):\n    return self._bias_initializer\n\n  @property\n  def kernel_regularizer(self):\n    return self._kernel_regularizer\n\n  @property\n  def bias_regularizer(self):\n    return self._bias_regularizer\n\n  @property\n  def kernel_parameterizer(self):\n    return self._kernel_parameterizer\n\n  @property\n  def bias_parameterizer(self):\n    return self._bias_parameterizer\n\n  @property\n  def kernel(self):\n    return self._kernel.value()\n\n  @property\n  def bias(self):\n    return self._bias.value()\n\n  @property\n  def _op_data_format(self):\n    fmt = {""channels_first"": ""NC{}"", ""channels_last"": ""N{}C""}[self.data_format]\n    return fmt.format({1: ""W"", 2: ""HW"", 3: ""DHW""}[self._rank])\n\n  def _padded_tuple(self, iterable, fill):\n    if self.data_format == ""channels_first"":\n      return (fill, fill) + tuple(iterable)\n    else:\n      return (fill,) + tuple(iterable) + (fill,)\n\n  def _normalized_tuple(self, value, name):\n    try:\n      return self._rank * (int(value),)\n    except (ValueError, TypeError):\n      try:\n        value = tuple(int(v) for v in value)\n        assert len(value) == self._rank\n        return value\n      except (ValueError, TypeError, AssertionError):\n        raise ValueError(\n            ""`{}` must be an integer or an iterable of integers with length {}.""\n            .format(name, self._rank))\n\n  def _greatest_common_factor(self, iterable):\n    for f in range(max(iterable), 1, -1):\n      if all(i % f == 0 for i in iterable):\n        return f\n    return 1\n\n  def _raise_notimplemented(self):\n    raise NotImplementedError(\n        ""The provided combination of SignalConv{}D arguments is not currently ""\n        ""implemented (filters={}, kernel_support={}, corr={}, strides_down={}, ""\n        ""strides_up={}, channel_separable={}, data_format={}). Try using ""\n        ""odd-length kernels or turning off separability?"".format(\n            self._rank, self.filters, self.kernel_support, self.corr,\n            self.strides_down, self.strides_up, self.channel_separable,\n            self.data_format))\n\n  def build(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    channel_axis = {""channels_first"": 1, ""channels_last"": -1}[self.data_format]\n    input_channels = input_shape.as_list()[channel_axis]\n    if input_channels is None:\n      raise ValueError(""The channel dimension of the inputs must be defined."")\n    self.input_spec = tf.keras.layers.InputSpec(\n        ndim=self._rank + 2, axes={channel_axis: input_channels})\n\n    kernel_shape = self.kernel_support + (input_channels, self.filters)\n    if self.channel_separable:\n      output_channels = self.filters * input_channels\n    else:\n      output_channels = self.filters\n\n    kernel_parameterizer = self.kernel_parameterizer\n    if kernel_parameterizer is None:\n      getter = self.add_weight\n    else:\n      getter = functools.partial(\n          kernel_parameterizer, getter=self.add_weight)\n    self._kernel = getter(\n        name=""kernel"", shape=kernel_shape, dtype=self.dtype,\n        initializer=self.kernel_initializer,\n        regularizer=self.kernel_regularizer)\n\n    if self.use_bias:\n      bias_parameterizer = self.bias_parameterizer\n      if bias_parameterizer is None:\n        getter = self.add_weight\n      else:\n        getter = functools.partial(\n            bias_parameterizer, getter=self.add_weight)\n      self._bias = getter(\n          name=""bias"", shape=(output_channels,), dtype=self.dtype,\n          initializer=self.bias_initializer, regularizer=self.bias_regularizer)\n\n    super(_SignalConv, self).build(input_shape)\n\n  def _correlate_down_valid(self, inputs, kernel):\n    # Computes valid correlation followed by downsampling.\n\n    data_format = self._op_data_format\n    strides = self._padded_tuple(self.strides_down, 1)\n\n    if 1 <= self._rank <= 3 and not self.channel_separable:\n      # `tf.nn.convolution` computes correlation followed by optional\n      # downsampling.\n      outputs = tf.nn.convolution(\n          inputs, kernel,\n          strides=self.strides_down, padding=""VALID"", data_format=data_format)\n    elif self._rank == 1 and self.channel_separable:\n      # There is no 1D depthwise correlation op, so if that is requested we\n      # insert an extra dimension and use the 2D op.\n      extradim = {""channels_first"": 2, ""channels_last"": 1}[self.data_format]\n      strides = strides[:extradim] + (strides[extradim],) + strides[extradim:]\n      data_format = data_format.replace(""W"", ""HW"")\n      inputs = tf.expand_dims(inputs, extradim)\n      kernel = tf.expand_dims(kernel, 0)\n      outputs = tf.nn.depthwise_conv2d_native(\n          inputs, kernel,\n          strides=strides, padding=""VALID"", data_format=data_format)\n      outputs = tf.squeeze(outputs, [extradim])\n    elif self._rank == 2 and self.channel_separable:\n      # `tf.nn.depthwise_conv2d_native` performs channel-separable correlations\n      # followed by optional downsampling. All strides must be identical. If\n      # not, we downsample by the greatest common factor and then downsample\n      # the result further.\n      gcf = self._greatest_common_factor(self.strides_down)\n      strides = self._padded_tuple(self._rank * (gcf,), 1)\n      outputs = tf.nn.depthwise_conv2d_native(\n          inputs, kernel,\n          strides=strides, padding=""VALID"", data_format=data_format)\n      # Perform remaining downsampling.\n      slices = tuple(slice(None, None, s // gcf) for s in self.strides_down)\n      if any(s.step > 1 for s in slices):\n        outputs = outputs[self._padded_tuple(slices, slice(None))]\n    else:\n      self._raise_notimplemented()\n\n    return outputs\n\n  def _correlate_down_explicit(self, inputs, kernel, padding):\n    # Computes correlation followed by downsampling, with arbitrary zero\n    # padding.\n    data_format = self._op_data_format\n    strides = self._padded_tuple(self.strides_down, 1)\n    padding = self._padded_tuple(padding, (0, 0))\n    do_cast = inputs.dtype.is_integer\n\n    if self._rank == 1 and not self.channel_separable:\n      # The 1D correlation op can\'t do explicit padding, so if that is requested\n      # we insert an extra dimension and use the 2D op.\n      extradim = {""channels_first"": 2, ""channels_last"": 1}[self.data_format]\n      strides = strides[:extradim] + (strides[extradim],) + strides[extradim:]\n      padding = padding[:extradim] + ((0, 0),) + padding[extradim:]\n      data_format = data_format.replace(""W"", ""HW"")\n      inputs = tf.expand_dims(inputs, extradim)\n      kernel = tf.expand_dims(kernel, 0)\n      if do_cast:\n        inputs = tf.cast(inputs, tf.float32)\n      outputs = tf.nn.conv2d(\n          inputs, kernel,\n          strides=strides, padding=padding, data_format=data_format)\n      if do_cast:\n        outputs = tf.cast(tf.math.round(outputs), self.accum_dtype)\n      outputs = tf.squeeze(outputs, [extradim])\n    elif self._rank == 2 and not self.channel_separable:\n      # `tf.nn.conv2d` performs correlations followed by optional downsampling.\n      if do_cast:\n        inputs = tf.cast(inputs, tf.float32)\n      outputs = tf.nn.conv2d(\n          inputs, kernel,\n          strides=strides, padding=padding, data_format=data_format)\n      if do_cast:\n        outputs = tf.cast(tf.math.round(outputs), self.accum_dtype)\n    else:\n      self._raise_notimplemented()\n\n    return outputs\n\n  def _up_convolve_transpose_valid(self, inputs, kernel, prepadding):\n    # Computes upsampling followed by convolution, via transpose convolution ops\n    # in VALID mode. This is a relatively inefficient implementation of\n    # upsampled convolutions, where we need to crop away a lot of the values\n    # computed in the boundaries.\n\n    # Transpose convolutions expect the output and input channels in reversed\n    # order. We implement this by swapping those dimensions of the kernel.\n    # For channel separable convolutions, we can\'t currently perform anything\n    # other than one filter per channel, so the last dimension needs to be of\n    # length one. Since this happens to be the format that the op expects it,\n    # we can skip the transpose in that case.\n    if not self.channel_separable:\n      kernel = tf.transpose(\n          kernel, list(range(self._rank)) + [self._rank + 1, self._rank])\n\n    # Compute shape of temporary.\n    input_shape = tf.shape(inputs)\n    temp_shape = [input_shape[0]] + (self._rank + 1) * [None]\n    if self.data_format == ""channels_last"":\n      spatial_axes = range(1, self._rank + 1)\n      temp_shape[-1] = (\n          input_shape[-1] if self.channel_separable else self.filters)\n    else:\n      spatial_axes = range(2, self._rank + 2)\n      temp_shape[1] = input_shape[1] if self.channel_separable else self.filters\n    if self.extra_pad_end:\n      get_length = lambda l, s, k: l * s + (k - 1)\n    else:\n      get_length = lambda l, s, k: l * s + ((k - 1) - (s - 1))\n    for i, a in enumerate(spatial_axes):\n      temp_shape[a] = get_length(\n          input_shape[a], self.strides_up[i], self.kernel_support[i])\n\n    data_format = self._op_data_format\n    strides = self._padded_tuple(self.strides_up, 1)\n\n    # Compute convolution.\n    if self._rank == 1 and not self.channel_separable:\n      # There\'s no 1D equivalent to conv2d_backprop_input, so we insert an\n      # extra dimension and use the 2D op.\n      extradim = {""channels_first"": 2, ""channels_last"": 1}[self.data_format]\n      data_format = data_format.replace(""W"", ""HW"")\n      strides = strides[:extradim] + (strides[extradim],) + strides[extradim:]\n      temp_shape = temp_shape[:extradim] + [1] + temp_shape[extradim:]\n      kernel = tf.expand_dims(kernel, 0)\n      inputs = tf.expand_dims(inputs, extradim)\n      outputs = tf.nn.conv2d_backprop_input(\n          temp_shape, kernel, inputs,\n          strides=strides, padding=""VALID"", data_format=data_format)\n      outputs = tf.squeeze(outputs, [extradim])\n    elif self._rank == 1 and self.channel_separable and self.filters == 1:\n      # There\'s no 1D equivalent to depthwise_conv2d_native_backprop_input, so\n      # we insert an extra dimension and use the 2D op.\n      extradim = {""channels_first"": 2, ""channels_last"": 1}[self.data_format]\n      data_format = data_format.replace(""W"", ""HW"")\n      strides = strides[:extradim] + (strides[extradim],) + strides[extradim:]\n      temp_shape = temp_shape[:extradim] + [1] + temp_shape[extradim:]\n      kernel = tf.expand_dims(kernel, 0)\n      inputs = tf.expand_dims(inputs, extradim)\n      outputs = tf.nn.depthwise_conv2d_native_backprop_input(\n          temp_shape, kernel, inputs,\n          strides=strides, padding=""VALID"", data_format=data_format)\n      outputs = tf.squeeze(outputs, [extradim])\n    elif self._rank == 2 and not self.channel_separable:\n      outputs = tf.nn.conv2d_backprop_input(\n          temp_shape, kernel, inputs,\n          strides=strides, padding=""VALID"", data_format=data_format)\n    elif (self._rank == 2 and self.channel_separable and\n          self.filters == 1 and self.strides_up[0] == self.strides_up[1]):\n      outputs = tf.nn.depthwise_conv2d_native_backprop_input(\n          temp_shape, kernel, inputs,\n          strides=strides, padding=""VALID"", data_format=data_format)\n    elif self._rank == 3 and not self.channel_separable:\n      outputs = tf.nn.conv3d_transpose(\n          inputs, kernel, temp_shape,\n          strides=strides, padding=""VALID"", data_format=data_format)\n    else:\n      self._raise_notimplemented()\n\n    # Perform crop, taking into account any pre-padding that was applied.\n    slices = (self._rank + 2) * [slice(None)]\n    for i, a in enumerate(spatial_axes):\n      if self.padding == ""valid"":\n        # Take `kernel_support - 1` samples away from both sides. This leaves\n        # just samples computed without any padding.\n        start = stop = self.kernel_support[i] - 1\n      else:  # same\n        # Take half of kernel sizes plus the pre-padding away from each side.\n        start = prepadding[i][0] * self.strides_up[i]\n        start += self.kernel_support[i] // 2\n        stop = prepadding[i][1] * self.strides_up[i]\n        stop += (self.kernel_support[i] - 1) // 2\n      step = self.strides_down[i]\n      start = start if start > 0 else None\n      stop = -stop if stop > 0 else None\n      step = step if step > 1 else None\n      slices[a] = slice(start, stop, step)\n    if not all(s.start is s.stop is s.step is None for s in slices):\n      outputs = outputs[tuple(slices)]\n\n    return outputs\n\n  def _up_convolve_transpose_explicit(self, inputs, kernel, prepadding):\n    # Computes upsampling followed by convolution, via transpose convolution ops\n    # in EXPLICIT mode. This is an efficient implementation of upsampled\n    # convolutions, where we only compute values that are necessary.\n    do_cast = inputs.dtype.is_integer\n\n    # conv2d_backprop_input expects the output and input channels in reversed\n    # order. We implement this by swapping those dimensions of the kernel.\n    kernel = tf.transpose(\n        kernel, list(range(self._rank)) + [self._rank + 1, self._rank])\n\n    # Compute explicit padding corresponding to the equivalent conv2d call,\n    # and the shape of the output, taking into account any pre-padding.\n    input_shape = tf.shape(inputs)\n    padding = (self._rank + 2) * [(0, 0)]\n    output_shape = [input_shape[0]] + (self._rank + 1) * [None]\n    if self.data_format == ""channels_last"":\n      spatial_axes = range(1, self._rank + 1)\n      output_shape[-1] = self.filters\n    else:\n      spatial_axes = range(2, self._rank + 2)\n      output_shape[1] = self.filters\n    if self.extra_pad_end:\n      get_length = lambda l, s, k, p: l * s + ((k - 1) - p)\n    else:\n      get_length = lambda l, s, k, p: l * s + ((k - 1) - (s - 1) - p)\n    for i, a in enumerate(spatial_axes):\n      if self.padding == ""valid"":\n        padding[a] = 2 * (self.kernel_support[i] - 1,)\n      else:  # same\n        padding[a] = (\n            prepadding[i][0] * self.strides_up[i] + self.kernel_support[i] // 2,\n            prepadding[i][1] * self.strides_up[i] + (\n                self.kernel_support[i] - 1) // 2,\n        )\n      output_shape[a] = get_length(\n          input_shape[a], self.strides_up[i], self.kernel_support[i],\n          sum(padding[a]))\n\n    data_format = self._op_data_format\n    strides = self._padded_tuple(self.strides_up, 1)\n\n    # Compute convolution.\n    if self._rank == 1 and not self.channel_separable:\n      # There\'s no 1D equivalent to conv2d_backprop_input, so we insert an\n      # extra dimension and use the 2D op.\n      extradim = {""channels_first"": 2, ""channels_last"": 1}[self.data_format]\n      data_format = data_format.replace(""W"", ""HW"")\n      strides = strides[:extradim] + (strides[extradim],) + strides[extradim:]\n      padding = padding[:extradim] + [(0, 0)] + padding[extradim:]\n      output_shape = output_shape[:extradim] + [1] + output_shape[extradim:]\n      kernel = tf.expand_dims(kernel, 0)\n      inputs = tf.expand_dims(inputs, extradim)\n      if do_cast:\n        inputs = tf.cast(inputs, tf.float32)\n      outputs = tf.nn.conv2d_backprop_input(\n          output_shape, kernel, inputs,\n          strides=strides, padding=padding, data_format=data_format)\n      if do_cast:\n        outputs = tf.cast(tf.math.round(outputs), self.accum_dtype)\n      outputs = tf.squeeze(outputs, [extradim])\n    elif self._rank == 2 and not self.channel_separable:\n      if do_cast:\n        inputs = tf.cast(inputs, tf.float32)\n      outputs = tf.nn.conv2d_backprop_input(\n          output_shape, kernel, inputs,\n          strides=strides, padding=padding, data_format=data_format)\n      if do_cast:\n        outputs = tf.cast(tf.math.round(outputs), self.accum_dtype)\n    else:\n      self._raise_notimplemented()\n\n    # Perform downsampling if it is requested.\n    if any(s > 1 for s in self.strides_down):\n      slices = tuple(slice(None, None, s) for s in self.strides_down)\n      slices = self._padded_tuple(slices, slice(None))\n      outputs = outputs[slices]\n\n    return outputs\n\n  def call(self, inputs):\n    inputs = tf.convert_to_tensor(inputs)\n    outputs = inputs\n\n    # Not for all possible combinations of (`kernel_support`, `corr`,\n    # `strides_up`, `strides_down`) TF ops exist. We implement some additional\n    # combinations by manipulating the kernels and toggling `corr`.\n    kernel = self.kernel\n    corr = self.corr\n\n    # If a convolution with no upsampling is desired, we flip the kernels and\n    # use cross correlation to implement it, provided the kernels are odd-length\n    # in every dimension (with even-length kernels, the boundary handling\n    # would have to change).\n    if (not corr and\n        all(s == 1 for s in self.strides_up) and\n        all(s % 2 == 1 for s in self.kernel_support)):\n      corr = True\n      slices = self._rank * (slice(None, None, -1),) + 2 * (slice(None),)\n      kernel = kernel[slices]\n\n    # Similarly, we can implement a cross correlation using convolutions.\n    # However, we do this only if upsampling is requested, as we are potentially\n    # wasting computation in the boundaries whenever we call the transpose ops.\n    elif (corr and\n          any(s != 1 for s in self.strides_up) and\n          all(s % 2 == 1 for s in self.kernel_support)):\n      corr = False\n      slices = self._rank * (slice(None, None, -1),) + 2 * (slice(None),)\n      kernel = kernel[slices]\n\n    # Compute amount of necessary padding, and determine whether to use built-in\n    # padding or to pre-pad with a separate op.\n    if self.padding == ""valid"":\n      padding = prepadding = self._rank * ((0, 0),)\n    else:  # same_*\n      padding = padding_ops.same_padding_for_kernel(\n          self.kernel_support, corr, self.strides_up)\n      if (self.padding == ""same_zeros"" and\n          not self.channel_separable and\n          1 <= self._rank <= 2 and\n          self.use_explicit):\n        # Don\'t pre-pad and use built-in EXPLICIT mode.\n        prepadding = self._rank * ((0, 0),)\n      else:\n        # Pre-pad and then use built-in valid padding mode.\n        outputs = tf.pad(\n            outputs, self._padded_tuple(padding, (0, 0)), self._pad_mode)\n        prepadding = padding\n        padding = self._rank * ((0, 0),)\n\n    # Compute the convolution/correlation. Prefer EXPLICIT padding ops where\n    # possible, but don\'t use them to implement VALID padding.\n    if (corr and\n        all(s == 1 for s in self.strides_up) and\n        not self.channel_separable and\n        1 <= self._rank <= 2 and\n        not all(p[0] == p[1] == 0 for p in padding)):\n      outputs = self._correlate_down_explicit(outputs, kernel, padding)\n    elif (corr and\n          all(s == 1 for s in self.strides_up) and\n          all(p[0] == p[1] == 0 for p in padding)):\n      outputs = self._correlate_down_valid(outputs, kernel)\n    elif (not corr and\n          not self.channel_separable and\n          1 <= self._rank <= 2 and\n          self.use_explicit):\n      outputs = self._up_convolve_transpose_explicit(\n          outputs, kernel, prepadding)\n    elif not corr:\n      outputs = self._up_convolve_transpose_valid(\n          outputs, kernel, prepadding)\n    else:\n      self._raise_notimplemented()\n\n    # Now, add bias if requested.\n    if self.use_bias:\n      bias = self.bias\n      if self.data_format == ""channels_first"":\n        # As of Mar 2017, direct addition is significantly slower than\n        # bias_add when computing gradients.\n        if self._rank == 1:\n          # tf.nn.bias_add does not accept a 1D input tensor.\n          outputs = tf.expand_dims(outputs, 2)\n          outputs = tf.nn.bias_add(outputs, bias, data_format=""NCHW"")\n          outputs = tf.squeeze(outputs, [2])\n        elif self._rank == 2:\n          outputs = tf.nn.bias_add(outputs, bias, data_format=""NCHW"")\n        elif self._rank >= 3:\n          shape = tf.shape(outputs)\n          outputs = tf.reshape(\n              outputs, tf.concat([shape[:3], [-1]], axis=0))\n          outputs = tf.nn.bias_add(outputs, bias, data_format=""NCHW"")\n          outputs = tf.reshape(outputs, shape)\n      else:\n        outputs = tf.nn.bias_add(outputs, bias)\n\n    # Finally, pass through activation function if requested.\n    if self.activation is not None:\n      outputs = self.activation(outputs)  # pylint:disable=not-callable\n\n    # Aid shape inference, for some reason shape info is not always available.\n    if not tf.executing_eagerly():\n      outputs.set_shape(self.compute_output_shape(inputs.shape))\n\n    return outputs\n\n  def compute_output_shape(self, input_shape):\n    input_shape = tf.TensorShape(input_shape)\n    input_shape = input_shape.with_rank(self._rank + 2)\n    batch = input_shape[0]\n    if self.data_format == ""channels_first"":\n      spatial = input_shape[2:].dims\n      channels = input_shape[1]\n    else:\n      spatial = input_shape[1:-1].dims\n      channels = input_shape[-1]\n\n    for i, s in enumerate(spatial):\n      if self.extra_pad_end:\n        s *= self.strides_up[i]\n      else:\n        s = (s - 1) * self.strides_up[i] + 1\n      if self.padding == ""valid"":\n        s -= self.kernel_support[i] - 1\n      s = (s - 1) // self.strides_down[i] + 1\n      spatial[i] = s\n\n    if self.channel_separable:\n      channels *= self.filters\n    else:\n      channels = self.filters\n\n    if self.data_format == ""channels_first"":\n      return tf.TensorShape([batch, channels] + spatial)\n    else:\n      return tf.TensorShape([batch] + spatial + [channels])\n\n\ndef _conv_class_factory(name, rank):\n  """"""Subclass from _SignalConv, fixing convolution rank.""""""\n  clsdict = {""_rank"": rank,\n             ""__doc__"": _SignalConv.__doc__.format(rank=rank)}\n  return type(name, (_SignalConv,), clsdict)\n\n\n# pylint:disable=invalid-name\n# Subclass _SignalConv for each dimensionality.\nSignalConv1D = _conv_class_factory(""SignalConv1D"", 1)\nSignalConv2D = _conv_class_factory(""SignalConv2D"", 2)\nSignalConv3D = _conv_class_factory(""SignalConv3D"", 3)\n# pylint:enable=invalid-name\n'"
tensorflow_compression/python/layers/signal_conv_test.py,13,"b'# Lint as: python3\n# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of signal processing convolution layers.""""""\n\nimport numpy as np\nimport scipy.signal\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.layers import initializers\nfrom tensorflow_compression.python.layers import parameterizers\nfrom tensorflow_compression.python.layers import signal_conv\n\n\n@test_util.deprecated_graph_mode_only\nclass SignalTest(tf.test.TestCase):\n\n  def numpy_upsample(self, inputs, strides_up, extra_pad_end):\n    """"""Upsample a numpy array.""""""\n    input_shape = np.array(inputs.shape, dtype=int)\n    strides_up = np.array(strides_up, dtype=int)\n\n    input_shape[2:] *= strides_up\n    if not extra_pad_end:\n      input_shape[2:] -= strides_up - 1\n    inputs_up = np.zeros(input_shape, dtype=np.float32)\n    slices = [slice(None), slice(None)]\n    slices += [slice(None, None, s) for s in strides_up]\n    inputs_up[tuple(slices)] = inputs\n\n    return inputs_up\n\n  def scipy_convolve_valid(self, corr, inputs, kernel, strides_down, strides_up,\n                           extra_pad_end, channel_separable):\n    """"""Convolve/correlate using SciPy.""""""\n    convolve = scipy.signal.correlate if corr else scipy.signal.convolve\n    slices = tuple(slice(None, None, s) for s in strides_down)\n\n    # Upsample input.\n    if not all(s == 1 for s in strides_up):\n      inputs = self.numpy_upsample(inputs, strides_up, extra_pad_end)\n\n    channels_out = kernel.shape[-1]\n    if channel_separable:\n      channels_out *= inputs.shape[1]\n\n    # Get size of output and initialize with zeros.\n    outputs = convolve(inputs[0, 0], kernel[..., 0, 0], mode=""valid"")\n    outputs = outputs[slices]\n    outputs = np.zeros((1, channels_out) + outputs.shape, dtype=np.float32)\n\n    # Iterate over channels, downsampling outputs as we go.\n    for batch in range(inputs.shape[0]):\n      for filter_out in range(kernel.shape[-1]):\n        for channel_in in range(inputs.shape[1]):\n          if channel_separable:\n            channel_out = channel_in * kernel.shape[-1] + filter_out\n          else:\n            channel_out = filter_out\n          outputs[batch, channel_out] += convolve(\n              inputs[batch, channel_in],\n              kernel[..., channel_in, filter_out],\n              mode=""valid"")[slices]\n\n    return outputs\n\n  def run_valid(self, batch, input_support, channels, filters, kernel_support,\n                corr, strides_down, strides_up, padding, extra_pad_end,\n                channel_separable, data_format, activation, use_bias):\n    assert padding == ""valid""\n\n    # Create input array.\n    inputs = np.random.randint(32, size=(batch, channels) + input_support)\n    inputs = inputs.astype(np.float32)\n    if data_format != ""channels_first"":\n      tf_inputs = tf.constant(np.moveaxis(inputs, 1, -1))\n    else:\n      tf_inputs = tf.constant(inputs)\n\n    # Create kernel array.\n    kernel = np.random.randint(16, size=kernel_support + (channels, filters))\n    kernel = kernel.astype(np.float32)\n    tf_kernel = parameterizers.StaticParameterizer(\n        tf.constant_initializer(kernel))\n\n    # Run SignalConv* layer.\n    layer_class = {\n        3: signal_conv.SignalConv1D,\n        4: signal_conv.SignalConv2D,\n        5: signal_conv.SignalConv3D,\n    }[inputs.ndim]\n    layer = layer_class(\n        filters, kernel_support, corr=corr, strides_down=strides_down,\n        strides_up=strides_up, padding=""valid"", extra_pad_end=extra_pad_end,\n        channel_separable=channel_separable, data_format=data_format,\n        activation=activation, use_bias=use_bias,\n        kernel_parameterizer=tf_kernel)\n    tf_outputs = layer(tf_inputs)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      outputs = sess.run(tf_outputs)\n\n    # Check that SignalConv* computes the correct output size.\n    predicted_shape = layer.compute_output_shape(tf_inputs.shape)\n    self.assertEqual(outputs.shape, tuple(predicted_shape.as_list()))\n\n    # If not using channels_first, convert back to it to compare to SciPy.\n    if data_format != ""channels_first"":\n      outputs = np.moveaxis(outputs, -1, 1)\n\n    # Compute the equivalent result using SciPy and compare.\n    expected = self.scipy_convolve_valid(\n        corr, inputs, kernel, strides_down, strides_up, extra_pad_end,\n        channel_separable)\n    self.assertAllClose(expected, outputs, rtol=0, atol=1e-3)\n\n  def run_same(self, batch, input_support, channels, filters, kernel_support,\n               corr, strides_down, strides_up, padding, extra_pad_end,\n               channel_separable, data_format, activation, use_bias):\n    assert channels == filters == 1\n\n    # Create input array.\n    input_shape = (batch, 1) + input_support\n    inputs = np.arange(np.prod(input_shape))\n    inputs = inputs.reshape(input_shape).astype(np.float32)\n    if data_format != ""channels_first"":\n      tf_inputs = tf.constant(np.moveaxis(inputs, 1, -1))\n    else:\n      tf_inputs = tf.constant(inputs)\n\n    # Create kernel array. This is an identity kernel, so the outputs should\n    # be equal to the inputs except for up- and downsampling.\n    tf_kernel = parameterizers.StaticParameterizer(\n        initializers.IdentityInitializer())\n\n    # Run SignalConv* layer.\n    layer_class = {\n        3: signal_conv.SignalConv1D,\n        4: signal_conv.SignalConv2D,\n        5: signal_conv.SignalConv3D,\n    }[inputs.ndim]\n    layer = layer_class(\n        1, kernel_support, corr=corr, strides_down=strides_down,\n        strides_up=strides_up, padding=padding, extra_pad_end=extra_pad_end,\n        channel_separable=channel_separable, data_format=data_format,\n        activation=activation, use_bias=use_bias,\n        kernel_parameterizer=tf_kernel)\n    tf_outputs = layer(tf_inputs)\n    with self.cached_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      outputs = sess.run(tf_outputs)\n\n    # Check that SignalConv* computes the correct output size.\n    predicted_shape = layer.compute_output_shape(tf_inputs.shape)\n    self.assertEqual(outputs.shape, tuple(predicted_shape.as_list()))\n\n    # If not using channels_first, convert back to it to compare to input.\n    if data_format != ""channels_first"":\n      outputs = np.moveaxis(outputs, -1, 1)\n\n    # Upsample and then downsample inputs.\n    expected = inputs\n    if not all(s == 1 for s in strides_up):\n      expected = self.numpy_upsample(expected, strides_up, extra_pad_end)\n    slices = (slice(None), slice(None))\n    slices += tuple(slice(None, None, s) for s in strides_down)\n    expected = expected[slices]\n\n    self.assertAllClose(expected, outputs, rtol=0, atol=1e-3)\n\n  def is_implemented(self, batch, input_support, channels, filters,\n                     kernel_support, corr, strides_down, strides_up, padding,\n                     extra_pad_end, channel_separable, data_format, activation,\n                     use_bias):\n    """"""Determine if SignalConv* implements the given arguments.""""""\n\n    # If convolution is requested, or kernels can be flipped, we can use the\n    # transpose ops.\n    can_use_transpose = (\n        not corr or all(s % 2 == 1 for s in kernel_support))\n\n    # If upsampling is requested, or convolution and kernels can\'t be flipped,\n    # we must use the transpose ops.\n    must_use_transpose = (\n        any(s != 1 for s in strides_up) or\n        (not corr and any(s % 2 != 1 for s in kernel_support)))\n\n    # If we must use transpose ops but can\'t, we fail.\n    if must_use_transpose and not can_use_transpose:\n      return False\n\n    # Channel-separable is only implemented for 1D and 2D.\n    if channel_separable and len(input_support) > 2:\n      return False\n\n    # Channel-separable with upsampling is only implemented for homogeneous\n    # strides.\n    if channel_separable and any(s != strides_up[0] for s in strides_up):\n      return False\n\n    # If we have to use the depthwise backprop op, we can\'t use filters > 1.\n    if channel_separable and must_use_transpose and filters != 1:\n      return False\n\n    return True\n\n  @property\n  def data_formats(self):\n    # On CPU, many ops don\'t support the channels first data format. Hence, if\n    # no GPU is available, we skip these tests.\n    if tf.config.experimental.list_physical_devices(""GPU""):\n      return (""channels_first"", ""channels_last"")\n    else:\n      return (""channels_last"",)\n\n  def run_or_fail(self, method,\n                  batch, input_support, channels, filters, kernel_support,\n                  corr, strides_down, strides_up, padding, extra_pad_end,\n                  channel_separable, data_format, activation, use_bias):\n    args = dict(locals())\n    del args[""self""]\n    del args[""method""]\n    if self.is_implemented(**args):\n      try:\n        method(**args)\n      except:\n        msg = []\n        for k in sorted(args):\n          msg.append(""{}={}"".format(k, args[k]))\n        print(""Failed when it shouldn\'t have: "" + "", "".join(msg))\n        raise\n    else:\n      try:\n        with self.assertRaisesRegexp(NotImplementedError, ""SignalConv""):\n          method(**args)\n      except:\n        msg = []\n        for k in sorted(args):\n          msg.append(""{}={}"".format(k, args[k]))\n        print(""Did not fail when it should have: "" + "", "".join(msg))\n        raise\n\n  def test_1d_valid_spatial(self):\n    """"""Test 1D valid convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""valid""\n    channels = 1\n    filters = 1\n    activation = None\n    use_bias = False\n    for channel_separable in [False, True]:\n      for input_support in [(12,), (7,)]:\n        for kernel_support in [(1,), (2,), (7,)]:\n          for corr in [False, True]:\n            for strides_down, strides_up, extra_pad_end in zip(\n                [(1,), (1,), (1,), (1,), (1,), (2,), (5,), (2,)],\n                [(1,), (2,), (2,), (3,), (3,), (1,), (1,), (3,)],\n                [True, False, True, False, True, True, True, True]):\n              for data_format in self.data_formats:\n                self.run_or_fail(\n                    self.run_valid,\n                    batch, input_support, channels, filters,\n                    kernel_support, corr, strides_down, strides_up,\n                    padding, extra_pad_end, channel_separable,\n                    data_format, activation, use_bias)\n\n  def test_1d_valid_channels(self):\n    """"""Test 1D valid convolutions with multiple channels/filters.""""""\n    batch = 1\n    padding = ""valid""\n    input_support = (9,)\n    kernel_support = (3,)\n    corr = True\n    strides_down = (1,)\n    extra_pad_end = True\n    activation = None\n    use_bias = False\n    for channel_separable in [False, True]:\n      for channels, filters in zip([1, 2], [2, 1]):\n        for strides_up in [(1,), (2,)]:\n          for data_format in self.data_formats:\n            self.run_or_fail(\n                self.run_valid,\n                batch, input_support, channels, filters,\n                kernel_support, corr, strides_down, strides_up,\n                padding, extra_pad_end, channel_separable,\n                data_format, activation, use_bias)\n\n  def test_1d_same_zeros_spatial(self):\n    """"""Test 1D same_zeros convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""same_zeros""\n    channels = 1\n    filters = 1\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for input_support in [(12,), (7,)]:\n      for kernel_support in [(1,), (2,), (3,), (7,)]:\n        for corr in [False, True]:\n          for strides_down, strides_up, extra_pad_end in zip(\n              [(1,), (1,), (1,), (2,), (5,), (2,)],\n              [(1,), (2,), (3,), (1,), (1,), (3,)],\n              [True, False, True, True, True, True]):\n            for data_format in self.data_formats:\n              self.run_or_fail(\n                  self.run_same,\n                  batch, input_support, channels, filters,\n                  kernel_support, corr, strides_down, strides_up,\n                  padding, extra_pad_end, channel_separable,\n                  data_format, activation, use_bias)\n\n  def test_1d_same_padding(self):\n    """"""Test 1D same convolutions with different padding modes.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (8,)\n    kernel_support = (3,)\n    corr = True\n    strides_up = (1,)\n    strides_down = (1,)\n    extra_pad_end = True\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for padding in [""same_reflect""]:\n      for data_format in self.data_formats:\n        self.run_or_fail(\n            self.run_same,\n            batch, input_support, channels, filters,\n            kernel_support, corr, strides_down, strides_up,\n            padding, extra_pad_end, channel_separable,\n            data_format, activation, use_bias)\n\n  def test_1d_bias_activation(self):\n    """"""Test 1D convolutions with bias and activation.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (6,)\n    kernel_support = (3,)\n    corr = True\n    strides_up = (1,)\n    strides_down = (1,)\n    extra_pad_end = True\n    channel_separable = False\n    activation = tf.identity\n    use_bias = True\n    padding = ""valid""\n    for data_format in self.data_formats:\n      self.run_or_fail(\n          self.run_valid,\n          batch, input_support, channels, filters,\n          kernel_support, corr, strides_down, strides_up,\n          padding, extra_pad_end, channel_separable,\n          data_format, activation, use_bias)\n\n  def test_2d_valid_spatial(self):\n    """"""Test 2D valid convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""valid""\n    channels = 1\n    filters = 1\n    activation = None\n    use_bias = False\n    for channel_separable in [False, True]:\n      for input_support in [(10, 7), (5, 8)]:\n        for kernel_support in [(5, 2), (2, 3), (3, 3)]:\n          for corr in [False, True]:\n            for strides_down, strides_up, extra_pad_end in zip(\n                [(1, 1), (2, 2), (1, 1), (1, 1), (3, 5)],\n                [(1, 1), (1, 1), (2, 2), (4, 3), (1, 1)],\n                [True, True, False, True, True]):\n              for data_format in self.data_formats:\n                self.run_or_fail(\n                    self.run_valid,\n                    batch, input_support, channels, filters,\n                    kernel_support, corr, strides_down, strides_up,\n                    padding, extra_pad_end, channel_separable,\n                    data_format, activation, use_bias)\n\n  def test_2d_valid_channels(self):\n    """"""Test 2D valid convolutions with multiple channels/filters.""""""\n    batch = 1\n    padding = ""valid""\n    input_support = (8, 7)\n    kernel_support = (3, 3)\n    corr = False\n    strides_down = (1, 1)\n    extra_pad_end = False\n    activation = None\n    use_bias = False\n    for channel_separable in [False, True]:\n      for channels, filters in zip([1, 2], [2, 1]):\n        for strides_up in [(1, 1), (2, 2)]:\n          for data_format in self.data_formats:\n            self.run_or_fail(\n                self.run_valid,\n                batch, input_support, channels, filters,\n                kernel_support, corr, strides_down, strides_up,\n                padding, extra_pad_end, channel_separable,\n                data_format, activation, use_bias)\n\n  def test_2d_same_zeros_spatial(self):\n    """"""Test 2D same_zeros convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""same_zeros""\n    channels = 1\n    filters = 1\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for input_support in [(4, 7), (5, 6)]:\n      for kernel_support in [(3, 2), (2, 6), (3, 3)]:\n        for corr in [False, True]:\n          for strides_down, strides_up, extra_pad_end in zip(\n              [(1, 1), (1, 1), (1, 1), (3, 5), (2, 3)],\n              [(1, 1), (2, 3), (5, 2), (1, 1), (3, 2)],\n              [True, False, True, True, False]):\n            for data_format in self.data_formats:\n              self.run_or_fail(\n                  self.run_same,\n                  batch, input_support, channels, filters,\n                  kernel_support, corr, strides_down, strides_up,\n                  padding, extra_pad_end, channel_separable,\n                  data_format, activation, use_bias)\n\n  def test_2d_same_padding(self):\n    """"""Test 2D same convolutions with different padding modes.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (4, 5)\n    kernel_support = (3, 2)\n    corr = True\n    strides_up = (1, 1)\n    strides_down = (1, 1)\n    extra_pad_end = True\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for padding in [""same_reflect""]:\n      for data_format in self.data_formats:\n        self.run_or_fail(\n            self.run_same,\n            batch, input_support, channels, filters,\n            kernel_support, corr, strides_down, strides_up,\n            padding, extra_pad_end, channel_separable,\n            data_format, activation, use_bias)\n\n  def test_2d_bias_activation(self):\n    """"""Test 2D convolutions with bias and activation.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (4, 6)\n    kernel_support = (2, 2)\n    corr = True\n    strides_up = (1, 1)\n    strides_down = (1, 1)\n    extra_pad_end = True\n    channel_separable = False\n    activation = tf.identity\n    use_bias = True\n    padding = ""valid""\n    for data_format in self.data_formats:\n      self.run_or_fail(\n          self.run_valid,\n          batch, input_support, channels, filters,\n          kernel_support, corr, strides_down, strides_up,\n          padding, extra_pad_end, channel_separable,\n          data_format, activation, use_bias)\n\n  def test_3d_valid_spatial(self):\n    """"""Test 3D valid convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""valid""\n    channels = 1\n    filters = 1\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for input_support in [(8, 7, 3), (5, 6, 4)]:\n      for kernel_support in [(1, 2, 3), (2, 1, 2), (3, 3, 3)]:\n        for corr in [False, True]:\n          for strides_down, strides_up, extra_pad_end in zip(\n              [(1, 1, 1), (1, 1, 1), (1, 1, 1), (3, 5, 4), (2, 1, 1)],\n              [(1, 1, 1), (1, 3, 2), (2, 4, 1), (1, 1, 1), (1, 1, 2)],\n              [True, False, True, True]):\n            for data_format in self.data_formats:\n              self.run_or_fail(\n                  self.run_valid,\n                  batch, input_support, channels, filters,\n                  kernel_support, corr, strides_down, strides_up,\n                  padding, extra_pad_end, channel_separable,\n                  data_format, activation, use_bias)\n\n  def test_3d_valid_channels(self):\n    """"""Test 3D valid convolutions with multiple channels/filters.""""""\n    batch = 1\n    padding = ""valid""\n    input_support = (7, 5, 4)\n    kernel_support = (2, 3, 2)\n    corr = False\n    strides_down = (1, 1, 1)\n    extra_pad_end = False\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for channels, filters in zip([1, 2], [2, 1]):\n      for strides_up in [(1, 1, 1), (1, 2, 2)]:\n        for data_format in self.data_formats:\n          self.run_or_fail(\n              self.run_valid,\n              batch, input_support, channels, filters,\n              kernel_support, corr, strides_down, strides_up,\n              padding, extra_pad_end, channel_separable,\n              data_format, activation, use_bias)\n\n  def test_3d_same_zeros_spatial(self):\n    """"""Test 3D same_zeros convolutions with different supports/strides.""""""\n    batch = 1\n    padding = ""same_zeros""\n    channels = 1\n    filters = 1\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for input_support in [(4, 5, 4), (5, 6, 3)]:\n      for kernel_support in [(1, 2, 3), (2, 1, 2), (3, 3, 3)]:\n        for corr in [False, True]:\n          for strides_down, strides_up, extra_pad_end in zip(\n              [(1, 1, 1), (1, 1, 1), (1, 1, 1), (3, 5, 4)],\n              [(1, 1, 1), (4, 3, 2), (2, 1, 3), (1, 1, 1)],\n              [True, False, True, True]):\n            for data_format in self.data_formats:\n              self.run_or_fail(\n                  self.run_same,\n                  batch, input_support, channels, filters,\n                  kernel_support, corr, strides_down, strides_up,\n                  padding, extra_pad_end, channel_separable,\n                  data_format, activation, use_bias)\n\n  def test_3d_same_padding(self):\n    """"""Test 3D same convolutions with different padding modes.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (6, 6, 5)\n    kernel_support = (3, 2, 2)\n    corr = True\n    strides_up = (1, 1, 1)\n    strides_down = (1, 1, 1)\n    extra_pad_end = True\n    channel_separable = False\n    activation = None\n    use_bias = False\n    for padding in [""same_reflect""]:\n      for data_format in self.data_formats:\n        self.run_or_fail(\n            self.run_same,\n            batch, input_support, channels, filters,\n            kernel_support, corr, strides_down, strides_up,\n            padding, extra_pad_end, channel_separable,\n            data_format, activation, use_bias)\n\n  def test_3d_bias_activation(self):\n    """"""Test 3D convolutions with bias and activation.""""""\n    batch = 1\n    channels = 1\n    filters = 1\n    input_support = (7, 2, 4)\n    kernel_support = (1, 2, 3)\n    corr = True\n    strides_up = (1, 1, 1)\n    strides_down = (1, 1, 1)\n    extra_pad_end = True\n    channel_separable = False\n    activation = tf.identity\n    use_bias = True\n    padding = ""valid""\n    for data_format in self.data_formats:\n      self.run_or_fail(\n          self.run_valid,\n          batch, input_support, channels, filters,\n          kernel_support, corr, strides_down, strides_up,\n          padding, extra_pad_end, channel_separable,\n          data_format, activation, use_bias)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/ops/__init__.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/ops/math_ops.py,27,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Math operations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\n\n\n__all__ = [\n    ""upper_bound"",\n    ""lower_bound"",\n]\n\n\n@tf.RegisterGradient(""IdentityFirstOfTwoInputs"")\ndef _identity_first_of_two_inputs_grad(op, grad):\n  """"""Gradient for `lower_bound` or `upper_bound` if `gradient == \'identity\'`.\n\n  Args:\n    op: The op for which to calculate a gradient.\n    grad: Gradient with respect to the output of the op.\n\n  Returns:\n    Gradient with respect to the inputs of the op.\n  """"""\n  del op  # unused\n  return [grad, None]\n\n\n@tf.RegisterGradient(""UpperBound"")\ndef _upper_bound_grad(op, grad):\n  """"""Gradient for `upper_bound` if `gradient == \'identity_if_towards\'`.\n\n  Args:\n    op: The op for which to calculate a gradient.\n    grad: Gradient with respect to the output of the op.\n\n  Returns:\n    Gradient with respect to the inputs of the op.\n  """"""\n  inputs, bound = op.inputs\n  pass_through_if = tf.logical_or(inputs <= bound, grad > 0)\n  return [tf.cast(pass_through_if, grad.dtype) * grad, None]\n\n\n@tf.RegisterGradient(""LowerBound"")\ndef _lower_bound_grad(op, grad):\n  """"""Gradient for `lower_bound` if `gradient == \'identity_if_towards\'`.\n\n  Args:\n    op: The op for which to calculate a gradient.\n    grad: Gradient with respect to the output of the op.\n\n  Returns:\n    Gradient with respect to the inputs of the op.\n  """"""\n  inputs, bound = op.inputs\n  pass_through_if = tf.logical_or(inputs >= bound, grad < 0)\n  return [tf.cast(pass_through_if, grad.dtype) * grad, None]\n\n\ndef upper_bound(inputs, bound, gradient=""identity_if_towards"", name=None):\n  """"""Same as `tf.minimum`, but with helpful gradient for `inputs > bound`.\n\n  This function behaves just like `tf.minimum`, but the behavior of the gradient\n  with respect to `inputs` for input values that hit the bound depends on\n  `gradient`:\n\n  If set to `\'disconnected\'`, the returned gradient is zero for values that hit\n  the bound. This is identical to the behavior of `tf.minimum`.\n\n  If set to `\'identity\'`, the gradient is unconditionally replaced with the\n  identity function (i.e., pretending this function does not exist).\n\n  If set to `\'identity_if_towards\'`, the gradient is replaced with the identity\n  function, but only if applying gradient descent would push the values of\n  `inputs` towards the bound. For gradient values that push away from the bound,\n  the returned gradient is still zero.\n\n  Note: In the latter two cases, no gradient is returned for `bound`.\n  Also, the implementation of `gradient == \'identity_if_towards\'` currently\n  assumes that the shape of `inputs` is the same as the shape of the output. It\n  won\'t work reliably for all possible broadcasting scenarios.\n\n  Args:\n    inputs: Input tensor.\n    bound: Upper bound for the input tensor.\n    gradient: \'disconnected\', \'identity\', or \'identity_if_towards\' (default).\n    name: Name for this op.\n\n  Returns:\n    `tf.minimum(inputs, bound)`\n\n  Raises:\n    ValueError: for invalid value of `gradient`.\n  """"""\n  try:\n    gradient = {\n        ""identity_if_towards"": ""UpperBound"",\n        ""identity"": ""IdentityFirstOfTwoInputs"",\n        ""disconnected"": None,\n    }[gradient]\n  except KeyError:\n    raise ValueError(""Invalid value for `gradient`: \'{}\'."".format(gradient))\n\n  with tf.name_scope(name, ""UpperBound"", [inputs, bound]) as scope:\n    inputs = tf.convert_to_tensor(inputs, name=""inputs"")\n    bound = tf.convert_to_tensor(\n        bound, name=""bound"", dtype=inputs.dtype)\n    if gradient:\n      with tf.get_default_graph().gradient_override_map({""Minimum"": gradient}):\n        return tf.minimum(inputs, bound, name=scope)\n    else:\n      return tf.minimum(inputs, bound, name=scope)\n\n\ndef lower_bound(inputs, bound, gradient=""identity_if_towards"", name=None):\n  """"""Same as `tf.maximum`, but with helpful gradient for `inputs < bound`.\n\n  This function behaves just like `tf.maximum`, but the behavior of the gradient\n  with respect to `inputs` for input values that hit the bound depends on\n  `gradient`:\n\n  If set to `\'disconnected\'`, the returned gradient is zero for values that hit\n  the bound. This is identical to the behavior of `tf.maximum`.\n\n  If set to `\'identity\'`, the gradient is unconditionally replaced with the\n  identity function (i.e., pretending this function does not exist).\n\n  If set to `\'identity_if_towards\'`, the gradient is replaced with the identity\n  function, but only if applying gradient descent would push the values of\n  `inputs` towards the bound. For gradient values that push away from the bound,\n  the returned gradient is still zero.\n\n  Note: In the latter two cases, no gradient is returned for `bound`.\n  Also, the implementation of `gradient == \'identity_if_towards\'` currently\n  assumes that the shape of `inputs` is the same as the shape of the output. It\n  won\'t work reliably for all possible broadcasting scenarios.\n\n  Args:\n    inputs: Input tensor.\n    bound: Lower bound for the input tensor.\n    gradient: \'disconnected\', \'identity\', or \'identity_if_towards\' (default).\n    name: Name for this op.\n\n  Returns:\n    `tf.maximum(inputs, bound)`\n\n  Raises:\n    ValueError: for invalid value of `gradient`.\n  """"""\n  try:\n    gradient = {\n        ""identity_if_towards"": ""LowerBound"",\n        ""identity"": ""IdentityFirstOfTwoInputs"",\n        ""disconnected"": None,\n    }[gradient]\n  except KeyError:\n    raise ValueError(""Invalid value for `gradient`: \'{}\'."".format(gradient))\n\n  with tf.name_scope(name, ""LowerBound"", [inputs, bound]) as scope:\n    inputs = tf.convert_to_tensor(inputs, name=""inputs"")\n    bound = tf.convert_to_tensor(\n        bound, name=""bound"", dtype=inputs.dtype)\n    if gradient:\n      with tf.get_default_graph().gradient_override_map({""Maximum"": gradient}):\n        return tf.maximum(inputs, bound, name=scope)\n    else:\n      return tf.maximum(inputs, bound, name=scope)\n'"
tensorflow_compression/python/ops/math_ops_test.py,8,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for the math operations.""""""\n\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.ops import math_ops\n\n\n@test_util.deprecated_graph_mode_only\nclass MathTest(tf.test.TestCase):\n\n  def _test_upper_bound(self, gradient):\n    inputs = tf.placeholder(dtype=tf.float32)\n    outputs = math_ops.upper_bound(inputs, 0, gradient=gradient)\n    pgrads, = tf.gradients([outputs], [inputs], [tf.ones_like(inputs)])\n    ngrads, = tf.gradients([outputs], [inputs], [-tf.ones_like(inputs)])\n\n    inputs_feed = [-1, 1]\n    outputs_expected = [-1, 0]\n    if gradient == ""disconnected"":\n      pgrads_expected = [1, 0]\n      ngrads_expected = [-1, 0]\n    elif gradient == ""identity"":\n      pgrads_expected = [1, 1]\n      ngrads_expected = [-1, -1]\n    else:\n      pgrads_expected = [1, 1]\n      ngrads_expected = [-1, 0]\n\n    with self.cached_session() as sess:\n      outputs, pgrads, ngrads = sess.run(\n          [outputs, pgrads, ngrads], {inputs: inputs_feed})\n      self.assertAllEqual(outputs, outputs_expected)\n      self.assertAllEqual(pgrads, pgrads_expected)\n      self.assertAllEqual(ngrads, ngrads_expected)\n\n  def test_upper_bound_disconnected(self):\n    self._test_upper_bound(""disconnected"")\n\n  def test_upper_bound_identity(self):\n    self._test_upper_bound(""identity"")\n\n  def test_upper_bound_identity_if_towards(self):\n    self._test_upper_bound(""identity_if_towards"")\n\n  def test_upper_bound_invalid(self):\n    with self.assertRaises(ValueError):\n      self._test_upper_bound(""invalid"")\n\n  def _test_lower_bound(self, gradient):\n    inputs = tf.placeholder(dtype=tf.float32)\n    outputs = math_ops.lower_bound(inputs, 0, gradient=gradient)\n    pgrads, = tf.gradients([outputs], [inputs], [tf.ones_like(inputs)])\n    ngrads, = tf.gradients([outputs], [inputs], [-tf.ones_like(inputs)])\n\n    inputs_feed = [-1, 1]\n    outputs_expected = [0, 1]\n    if gradient == ""disconnected"":\n      pgrads_expected = [0, 1]\n      ngrads_expected = [0, -1]\n    elif gradient == ""identity"":\n      pgrads_expected = [1, 1]\n      ngrads_expected = [-1, -1]\n    else:\n      pgrads_expected = [0, 1]\n      ngrads_expected = [-1, -1]\n\n    with self.cached_session() as sess:\n      outputs, pgrads, ngrads = sess.run(\n          [outputs, pgrads, ngrads], {inputs: inputs_feed})\n      self.assertAllEqual(outputs, outputs_expected)\n      self.assertAllEqual(pgrads, pgrads_expected)\n      self.assertAllEqual(ngrads, ngrads_expected)\n\n  def test_lower_bound_disconnected(self):\n    self._test_lower_bound(""disconnected"")\n\n  def test_lower_bound_identity(self):\n    self._test_lower_bound(""identity"")\n\n  def test_lower_bound_identity_if_towards(self):\n    self._test_lower_bound(""identity_if_towards"")\n\n  def test_lower_bound_invalid(self):\n    with self.assertRaises(ValueError):\n      self._test_lower_bound(""invalid"")\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/ops/namespace_helper.py,0,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helps importing C ops with a clean namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\ndef get_ops(module):\n  """"""Returns a dict of ops defined in a module by blacklisting internals.""""""\n  ops = dict()\n  for name in dir(module):\n    if name.startswith(""_""):\n      continue\n    if name.endswith(""_eager_fallback""):\n      continue\n    if name in (""LIB_HANDLE"", ""OP_LIST"", ""deprecated_endpoints"", ""tf_export""):\n      continue\n    ops[name] = getattr(module, name)\n  return ops\n'"
tensorflow_compression/python/ops/padding_ops.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Padding ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n__all__ = [\n    ""same_padding_for_kernel"",\n]\n\n\ndef same_padding_for_kernel(shape, corr, strides_up=None):\n  """"""Determine correct amount of padding for `same` convolution.\n\n  To implement `\'same\'` convolutions, we first pad the image, and then perform a\n  `\'valid\'` convolution or correlation. Given the kernel shape, this function\n  determines the correct amount of padding so that the output of the convolution\n  or correlation is the same size as the pre-padded input.\n\n  Args:\n    shape: Shape of the convolution kernel (without the channel dimensions).\n    corr: Boolean. If `True`, assume cross correlation, if `False`, convolution.\n    strides_up: If this is used for an upsampled convolution, specify the\n      strides here. (For downsampled convolutions, specify `(1, 1)`: in that\n      case, the strides don\'t matter.)\n\n  Returns:\n    The amount of padding at the beginning and end for each dimension.\n  """"""\n  rank = len(shape)\n  if strides_up is None:\n    strides_up = rank * (1,)\n\n  if corr:\n    padding = [(s // 2, (s - 1) // 2) for s in shape]\n  else:\n    padding = [((s - 1) // 2, s // 2) for s in shape]\n\n  padding = [((padding[i][0] - 1) // strides_up[i] + 1,\n              (padding[i][1] - 1) // strides_up[i] + 1) for i in range(rank)]\n  return padding\n'"
tensorflow_compression/python/ops/padding_ops_test.py,8,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of padding ops.""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.ops import padding_ops\n\n\n@test_util.deprecated_graph_mode_only\nclass PaddingOpsTest(tf.test.TestCase):\n\n  def test_same_padding_corr(self):\n    for ishape in [[10], [11]]:\n      inputs = np.zeros(ishape, dtype=np.float32)\n      inputs[len(inputs) // 2] = 1\n      for kshape in [[4], [5]]:\n        kernel = np.zeros(kshape, dtype=np.float32)\n        kernel[len(kernel) // 2] = 1\n        outputs = tf.nn.convolution(\n            tf.reshape(inputs, (1, 1, -1, 1)),\n            tf.reshape(kernel, (1, -1, 1, 1)),\n            padding=""VALID"", data_format=""NHWC"")\n        with self.cached_session() as sess:\n          outputs = np.squeeze(sess.run(outputs))\n        pos_inp = np.squeeze(np.nonzero(inputs))\n        pos_out = np.squeeze(np.nonzero(outputs))\n        padding = padding_ops.same_padding_for_kernel(kshape, True)\n        self.assertEqual(padding[0][0], pos_inp - pos_out)\n\n  def test_same_padding_conv(self):\n    for ishape in [[10], [11]]:\n      inputs = np.zeros(ishape, dtype=np.float32)\n      inputs[len(inputs) // 2] = 1\n      for kshape in [[4], [5]]:\n        kernel = np.zeros(kshape, dtype=np.float32)\n        kernel[len(kernel) // 2] = 1\n        outputs = tf.nn.conv2d_transpose(\n            tf.reshape(inputs, (1, 1, -1, 1)),\n            tf.reshape(kernel, (1, -1, 1, 1)),\n            (1, 1, ishape[0] + kshape[0] - 1, 1),\n            strides=(1, 1, 1, 1), padding=""VALID"", data_format=""NHWC"")\n        outputs = outputs[:, :, (kshape[0] - 1):-(kshape[0] - 1), :]\n        with self.cached_session() as sess:\n          outputs = np.squeeze(sess.run(outputs))\n        pos_inp = np.squeeze(np.nonzero(inputs))\n        pos_out = np.squeeze(np.nonzero(outputs))\n        padding = padding_ops.same_padding_for_kernel(kshape, False)\n        self.assertEqual(padding[0][0], pos_inp - pos_out)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/ops/range_coding_ops.py,0,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Range coding operations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.python.framework import load_library\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow_compression.python.ops import namespace_helper\n\n\nops = namespace_helper.get_ops(load_library.load_op_library(\n    resource_loader.get_path_to_datafile(\n        ""../../cc/libtensorflow_compression.so"")))\n\nglobals().update(ops)\n__all__ = list(ops)\n'"
tensorflow_compression/python/ops/range_coding_ops_test.py,9,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Range coding tests.""""""\n\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.ops import range_coding_ops\n\n\n@test_util.deprecated_graph_mode_only\nclass RangeCodingOpsTest(tf.test.TestCase):\n  """"""Python test for range coding ops.\n\n  Coding ops have C++ tests. This Python test just ensures that the Python\n  binding is not broken.\n  """"""\n\n  def test_readme_example(self):\n    data = tf.random.uniform((128, 128), 0, 10, dtype=tf.int32)\n    histogram = tf.bincount(data, minlength=10, maxlength=10)\n    cdf = tf.cumsum(histogram, exclusive=False)\n    cdf = tf.pad(cdf, [[1, 0]])\n    cdf = tf.reshape(cdf, [1, 1, -1])\n\n    data = tf.cast(data, tf.int16)\n    encoded = range_coding_ops.range_encode(data, cdf, precision=14)\n    decoded = range_coding_ops.range_decode(\n        encoded, tf.shape(data), cdf, precision=14)\n\n    with self.cached_session() as sess:\n      self.assertAllEqual(*sess.run((data, decoded)))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/ops/spectral_ops.py,9,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Parameterizations for layer classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom scipy import fftpack\nimport tensorflow.compat.v1 as tf\n\n\n__all__ = [\n    ""irdft_matrix"",\n]\n\n\ndef irdft_matrix(shape, dtype=tf.float32):\n  """"""Matrix for implementing kernel reparameterization with `tf.matmul`.\n\n  This can be used to represent a kernel with the provided shape in the RDFT\n  domain.\n\n  Example code for kernel creation, assuming 2D kernels:\n\n  ```\n  def create_kernel(init):\n    shape = init.shape.as_list()\n    matrix = irdft_matrix(shape[:2])\n    init = tf.reshape(init, (shape[0] * shape[1], shape[2] * shape[3]))\n    init = tf.matmul(tf.transpose(matrix), init)\n    kernel = tf.Variable(init)\n    kernel = tf.matmul(matrix, kernel)\n    kernel = tf.reshape(kernel, shape)\n    return kernel\n  ```\n\n  Args:\n    shape: Iterable of integers. Shape of kernel to apply this matrix to.\n    dtype: `dtype` of returned matrix.\n\n  Returns:\n    `Tensor` of shape `(prod(shape), prod(shape))` and dtype `dtype`.\n  """"""\n  shape = tuple(int(s) for s in shape)\n  dtype = tf.as_dtype(dtype)\n  size = np.prod(shape)\n  rank = len(shape)\n  matrix = np.identity(size, dtype=np.float64).reshape((size,) + shape)\n  for axis in range(rank):\n    matrix = fftpack.rfft(matrix, axis=axis + 1)\n    slices = (rank + 1) * [slice(None)]\n    if shape[axis] % 2 == 1:\n      slices[axis + 1] = slice(1, None)\n    else:\n      slices[axis + 1] = slice(1, -1)\n    matrix[tuple(slices)] *= np.sqrt(2)\n  matrix /= np.sqrt(size)\n  matrix = np.reshape(matrix, (size, size))\n  return tf.constant(\n      matrix, dtype=dtype, name=""irdft_"" + ""x"".join([str(s) for s in shape]))\n'"
tensorflow_compression/python/ops/spectral_ops_test.py,5,"b'# Copyright 2018 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of spectral_ops.""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.ops import spectral_ops\n\n\n@test_util.deprecated_graph_mode_only\nclass SpectralOpsTest(tf.test.TestCase):\n\n  def test_irdft1_matrix(self):\n    for shape in [(4,), (3,)]:\n      size = shape[0]\n      matrix = spectral_ops.irdft_matrix(shape)\n      # Test that the matrix is orthonormal.\n      result = tf.matmul(matrix, tf.transpose(matrix))\n      with self.cached_session() as sess:\n        result, = sess.run([result])\n        self.assertAllClose(result, np.identity(size))\n\n  def test_irdft2_matrix(self):\n    for shape in [(7, 4), (8, 9)]:\n      size = shape[0] * shape[1]\n      matrix = spectral_ops.irdft_matrix(shape)\n      # Test that the matrix is orthonormal.\n      result = tf.matmul(matrix, tf.transpose(matrix))\n      with self.cached_session() as sess:\n        result, = sess.run([result])\n        self.assertAllClose(result, np.identity(size))\n\n  def test_irdft3_matrix(self):\n    for shape in [(3, 4, 2), (6, 3, 1)]:\n      size = shape[0] * shape[1] * shape[2]\n      matrix = spectral_ops.irdft_matrix(shape)\n      # Test that the matrix is orthonormal.\n      result = tf.matmul(matrix, tf.transpose(matrix))\n      with self.cached_session() as sess:\n        result, = sess.run([result])\n        self.assertAllClose(result, np.identity(size))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
tensorflow_compression/python/util/__init__.py,0,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tensorflow_compression/python/util/packed_tensors.py,3,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Packed tensors in bit sequences.""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\n\n__all__ = [\n    ""PackedTensors"",\n]\n\n\nclass PackedTensors(object):\n  """"""Packed representation of compressed tensors.\n\n  This class can pack and unpack several tensor values into a single string. It\n  can also optionally store a model identifier.\n\n  The tensors currently must be rank 1 (vectors) and either have integer or\n  string type.\n  """"""\n\n  def __init__(self, string=None):\n    self._example = tf.train.Example()\n    if string:\n      self.string = string\n\n  @property\n  def model(self):\n    """"""A model identifier.""""""\n    buf = self._example.features.feature[""MD""].bytes_list.value[0]\n    return buf.decode(""ascii"")\n\n  @model.setter\n  def model(self, value):\n    self._example.features.feature[""MD""].bytes_list.value[:] = [\n        value.encode(""ascii"")]\n\n  @model.deleter\n  def model(self):\n    del self._example.features.feature[""MD""]\n\n  @property\n  def string(self):\n    """"""The string representation of this object.""""""\n    return self._example.SerializeToString()\n\n  @string.setter\n  def string(self, value):\n    self._example.ParseFromString(value)\n\n  def pack(self, tensors, arrays):\n    """"""Packs `Tensor` values into this object.""""""\n    if len(tensors) != len(arrays):\n      raise ValueError(""`tensors` and `arrays` must have same length."")\n    i = 1\n    for tensor, array in zip(tensors, arrays):\n      feature = self._example.features.feature[chr(i)]\n      feature.Clear()\n      if array.ndim != 1:\n        raise RuntimeError(""Unexpected tensor rank: {}."".format(array.ndim))\n      if tensor.dtype.is_integer:\n        feature.int64_list.value[:] = array\n      elif tensor.dtype == tf.string:\n        feature.bytes_list.value[:] = array\n      else:\n        raise RuntimeError(\n            ""Unexpected tensor dtype: \'{}\'."".format(tensor.dtype))\n      i += 1\n    # Delete any remaining, previously set arrays.\n    while chr(i) in self._example.features.feature:\n      del self._example.features.feature[chr(i)]\n      i += 1\n\n  def unpack(self, tensors):\n    """"""Unpacks `Tensor` values from this object.""""""\n    arrays = []\n    for i, tensor in enumerate(tensors):\n      feature = self._example.features.feature[chr(i + 1)]\n      np_dtype = tensor.dtype.as_numpy_dtype\n      if tensor.dtype.is_integer:\n        arrays.append(np.array(feature.int64_list.value, dtype=np_dtype))\n      elif tensor.dtype == tf.string:\n        arrays.append(np.array(feature.bytes_list.value, dtype=np_dtype))\n      else:\n        raise RuntimeError(\n            ""Unexpected tensor dtype: \'{}\'."".format(tensor.dtype))\n    return arrays\n'"
tensorflow_compression/python/util/packed_tensors_test.py,4,"b'# Copyright 2019 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests of PackedTensors class.""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow_compression.python.util import packed_tensors\n\n\n@test_util.deprecated_graph_mode_only\nclass PackedTensorsTest(tf.test.TestCase):\n\n  def test_pack_unpack(self):\n    """"""Tests packing and unpacking tensors.""""""\n    string = np.array([""xyz"".encode(""ascii"")], dtype=object)\n    shape = np.array([1, 3], dtype=np.int32)\n    arrays = [string, shape]\n\n    string_t = tf.placeholder(tf.string, [1])\n    shape_t = tf.placeholder(tf.int32, [2])\n    tensors = [string_t, shape_t]\n\n    packed = packed_tensors.PackedTensors()\n    packed.pack(tensors, arrays)\n    packed = packed_tensors.PackedTensors(packed.string)\n    string_u, shape_u = packed.unpack(tensors)\n\n    self.assertAllEqual(string_u, string)\n    self.assertAllEqual(shape_u, shape)\n\n  def test_model(self):\n    """"""Tests setting and getting model.""""""\n    packed = packed_tensors.PackedTensors()\n    packed.model = ""xyz""\n    packed = packed_tensors.PackedTensors(packed.string)\n    self.assertEqual(packed.model, ""xyz"")\n    del packed.model\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
