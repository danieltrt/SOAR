file_path,api_count,code
setup.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""tensorflow_docs is a package for generating python api-reference docs.""""""\nimport subprocess\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nproject_name = \'tensorflow-docs\'\nversion = \'0.0.0\'\n\ntry:\n  version += subprocess.check_output([\'git\', \'rev-parse\',\n                                      \'HEAD\']).decode(\'utf-8\')\nexcept subprocess.CalledProcessError:\n  pass\n\nDOCLINES = __doc__.split(\'\\n\')\n\nREQUIRED_PKGS = [\n    \'astor\',\n    \'absl-py\',\n    \'protobuf\',\n    \'pyyaml\',\n]\n\nVIS_REQURE = [\n    \'numpy\',\n    \'PILLOW\',\n    \'webp\',\n]\n\n# https://setuptools.readthedocs.io/en/latest/setuptools.html#new-and-changed-setup-keywords\nsetup(\n    name=project_name,\n    version=version,\n    description=DOCLINES[0],\n    long_description=\'\\n\'.join(DOCLINES[2:]),\n    author=\'Google Inc.\',\n    author_email=\'packages@tensorflow.org\',\n    url=\'http://github.com/tensorflow/docs\',\n    download_url=\'https://github.com/tensorflow/docs/tags\',\n    license=\'Apache 2.0\',\n    packages=find_packages(\'tools\'),\n    package_dir={\'\': \'tools\'},\n    scripts=[],\n    install_requires=REQUIRED_PKGS,\n    extras_require={\'vis\': VIS_REQURE},\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    ],\n    keywords=\'tensorflow api reference\',\n    # Include_package_data is required for setup.py to recognize the MAINFEST.in\n    #   https://python-packaging.readthedocs.io/en/latest/non-code-files.html\n    include_package_data=True,\n)\n'"
tools/nbfmt.py,0,"b'#!/usr/bin/env python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Format notebooks using the TensorFlow docs style.\n\nInstall dependencies:\n$ pip3 install -U [--user] absl-py\n\nUsage:\n$ nbfmt.py [options] notebook.ipynb [...]\n$ find . -name ""*\\.ipynb"" | xargs ./tools/nbfmt.py\n\nSee the TensorFlow notebook template:\nhttps://github.com/tensorflow/docs/blob/master/tools/templates/notebook.ipynb\nAnd the TensorFlow docs contributor guide:\nhttps://www.tensorflow.org/community/contribute/docs\n""""""\nimport json\nimport os\nimport pathlib\nimport re\nimport sys\nimport textwrap\n\nfrom typing import Any, Dict, Optional\n\nfrom absl import app\nfrom absl import flags\n\nOSS = True\n\nflags.DEFINE_bool(\n    ""preserve_outputs"", None,\n    ""Configures the notebook to either preserve, or clear outputs.""\n    ""- If `True`: Set the notebook to keep outputs.\\n""\n    ""- If `False`: Set the notebook to clear outputs.\\n""\n    ""- If unset (`None`): keep the notebook\'s current setting.\\n""\n    ""  - If it\'s not set in the notebook, set the notebook to preserve outputs ""\n    ""(to match the colab default).\\n""\n    ""If a notebook is configured to clear outputs, this script will clear them ""\n    ""when run.\\n""\n    ""Colab respects this setting. Jupyter does not."")\nflags.DEFINE_integer(\n    ""indent"", 2, ""Indention level for pretty-printed JSON."", lower_bound=0)\nflags.DEFINE_bool(""test"", False,\n                  ""Test if the notebook is formatted (useful for CI)."")\n\nFLAGS = flags.FLAGS\n\n\ndef warn(msg: str) -> None:\n  """"""Print highlighted warning message to stderr.\n\n  Args:\n    msg: String to print to console.\n  """"""\n  # Use terminal codes to print color output to console.\n  print(f"" \\033[33m {msg}\\033[00m"", file=sys.stderr)\n\n\ndef remove_extra_fields(data) -> None:\n  """"""Deletes extra notebook fields.\n\n  Jupyter format spec:\n  https://nbformat.readthedocs.io/en/latest/format_description.html\n\n  Args:\n    data: object representing a parsed JSON notebook.\n  """"""\n\n  def filter_keys(data, keep_list) -> None:\n    to_delete = set(data.keys()) - frozenset(keep_list)\n    for key in to_delete:\n      del data[key]\n\n  # These top-level fields are required:\n  filter_keys(data, [""cells"", ""metadata"", ""nbformat_minor"", ""nbformat""])\n  # All metadata is optional according to spec, but we use some of it.\n  # For example, this removes ""language_info"" and other editor-specific fields.\n  filter_keys(data[""metadata""], [""accelerator"", ""colab"", ""kernelspec""])\n\n\ndef clean_cells(data) -> None:\n  """"""Remove empty cells and strip outputs from `data` object.\n\n  Args:\n    data: object representing a parsed JSON notebook.\n  """"""\n  remove_extra_fields(data)\n\n  # Clear leading and trailing newlines.\n  for cell in data[""cells""]:\n    source = cell[""source""]\n    while source and source[0] == ""\\n"":\n      source.pop(0)\n    while source and source[-1] == ""\\n"":\n      source.pop()\n    cell[""source""] = source\n\n  # remove empty cells\n  data[""cells""] = [cell for cell in data[""cells""] if any(cell[""source""])]\n\n  # `private_outputs` will default to False. If `private_outputs` is True, then\n  # the output will be removed from the notebook.\n  private_outputs = (\n      data.get(""metadata"", {}).get(""colab"", {}).get(""private_outputs"", False))\n\n  has_outputs = False\n  for cell in data[""cells""]:\n    if cell[""cell_type""] != ""code"":\n      continue\n\n    # Clean cell metadata: remove the ""executionInfo"" block, this is what adds\n    # the little user photos in Colab.\n    cell_meta = cell.get(""metadata"", {})\n    cell_meta.pop(""executionInfo"", None)\n    cell[""metadata""] = cell_meta\n\n    # Clear any code cell outputs if notebook set to private outputs.\n    if private_outputs and cell.get(""outputs""):\n      has_outputs = True\n      # Clear code outputs\n      cell[""execution_count""] = 0\n      cell[""outputs""] = []\n\n    # Ensure outputs field exists since part of the nbformat spec.\n    if cell.get(""outputs"", None) is None:\n      cell[""outputs""] = []\n\n  if has_outputs:\n    warn(""Removed the existing output cells."")\n\n\ndef update_metadata(data: Dict[str, Any],\n                    filepath: Optional[pathlib.Path] = None) -> None:\n  """"""Set notebook metadata on `data` object using TF docs style.\n\n  Args:\n    data: object representing a parsed JSON notebook.\n    filepath: String of notebook filepath passed to the command-line.\n  """"""\n  metadata = data.get(""metadata"", {})\n  colab = metadata.get(""colab"", {})\n  # Set preferred metadata for notebook docs.\n  if filepath is not None:\n    colab[""name""] = os.path.basename(filepath)\n\n  colab[""provenance""] = []\n  colab[""toc_visible""] = True\n  # Always remove ""last_runtime"".\n  colab.pop(""last_runtime"", None)\n\n  metadata[""colab""] = colab\n  data[""metadata""] = metadata\n\n\ndef update_license_cell(data: Dict[str, Any]) -> None:\n  """"""Format license cell to hide code pane from the Colab form.\n\n  Args:\n    data: object representing a parsed JSON notebook.\n  """"""\n  # This pattern in Apache and MIT license boilerplate.\n  license_re = re.compile(r""#@title.*License"")\n\n  for idx, cell in enumerate(data[""cells""]):\n    src_text = """".join(cell[""source""])\n\n    if license_re.search(src_text):\n      # Hide code pane from license form\n      metadata = cell.get(""metadata"", {})\n      metadata[""cellView""] = ""form""\n      data[""cells""][idx][""metadata""] = metadata\n\n\ndef main(argv):\n  if len(argv) <= 1:\n    raise app.UsageError(""Missing arguments."")\n\n  found_error = False  # Track errors for final return code.\n  test_fail_notebooks = []\n\n  files = []\n  for path in argv[1:]:\n    path = pathlib.Path(path)\n    if path.is_file():\n      files.append(path)\n    elif path.is_dir():\n      files.extend(path.rglob(""*.ipynb""))\n    else:\n      found_error = True\n      warn(f""Bad arg: {str(path)}"")\n\n  for fp in files:\n    print(f""Notebook: {fp}"", file=sys.stderr)\n\n    if fp.suffix != "".ipynb"":\n      warn(""Not an \'.ipynb\' file, skipping."")\n      found_error = True\n      test_fail_notebooks.append(fp)\n      continue\n\n    with open(fp, ""r"", encoding=""utf-8"") as f:\n      try:\n        data = json.load(f)\n      except ValueError as err:\n        print(f""  {err.__class__.__name__}: {err}"", file=sys.stderr)\n        warn(""Unable to load JSON, skipping."")\n        found_error = True\n        test_fail_notebooks.append(fp)\n        continue\n\n    if FLAGS.preserve_outputs is not None:\n      # Overwrite the value of `private_outputs`\n      colab = data.get(""metadata"", {}).get(""colab"", {})\n      colab[""private_outputs""] = not FLAGS.preserve_outputs\n      data[""metadata""][""colab""] = colab\n\n    # Set top-level notebook defaults.\n    data[""nbformat""] = 4\n    data[""nbformat_minor""] = 0\n\n    clean_cells(data)\n    update_metadata(data, filepath=fp)\n    update_license_cell(data)\n\n    nbjson = json.dumps(\n        data, sort_keys=True, ensure_ascii=False, indent=FLAGS.indent)\n    if not OSS:\n      nbjson = nbjson.replace(""<"", r""\\u003c"").replace("">"", r""\\u003e"")\n    expected_output = (nbjson + ""\\n"").encode(""utf-8"")\n\n    if FLAGS.test:\n      # Compare formatted contents with original file contents.\n      src_bytes = fp.read_bytes()\n      if expected_output != src_bytes:\n        test_fail_notebooks.append(fp)\n    else:\n      fp.write_bytes(expected_output)\n\n  if FLAGS.test:\n    if test_fail_notebooks:\n      error_template = textwrap.dedent(""""""\n      [test] The following notebooks are not formatted:\n      {notebooks}\n      Format with: nbfmt.py notebook.ipynb [...]\n      """""")\n      notebooks = ""\\n"".join([f""- {str(fp)}"" for fp in test_fail_notebooks])\n      print(error_template.format(notebooks=notebooks), file=sys.stderr)\n      sys.exit(1)\n    else:\n      print(""[test] Notebooks are formatted"", file=sys.stderr)\n      sys.exit(0)\n\n  if found_error:\n    sys.exit(1)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
tools/release_tools/update_versions.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Update tensorflow version in docs. Run this from the repo-root.""""""\nimport argparse\nimport re\n\ntry:\n  import pathlib2 as pathlib\nexcept ImportError:\n  import pathlib\n\n\nEXTS = ["".ipynb"","".md"","".yaml"","".html""]\nEXPAND_TABLES = [\n    ""source_windows.md"",\n    ""source.md"",]\n\nclass Version(object):\n  def __init__(self, in_string):\n    self.major, self.minor, self.patch = in_string.split(""."")\n    assert self.major.isdigit()\n    assert self.minor.isdigit()\n    assert self.patch.isdigit()\n\n  def full(self):\n    return ""."".join([self.major, self.minor, self.patch])\n\n  def short(self):\n    return ""."".join([self.major, self.minor])\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--old_version"", type=Version, required=True,\n                    help=""The old version to replace"")\nparser.add_argument(""--new_version"", type=Version, required=True,\n                    help=""The new version to replace it with"")\n\nif __name__==""__main__"":\n  args = parser.parse_args()\n\n  for ext in EXTS:\n    for file_path in pathlib.Path(""."").rglob(""*""+ext):\n      content = file_path.read_text()\n      if file_path.name in EXPAND_TABLES:\n        content = re.sub(""(<tr>.*?){}(.*?</tr>)"".format(re.escape(args.old_version.short())),\n                         r""\\g<1>{}\\g<2>\\n\\g<0>"".format(args.new_version.short()), content)\n        file_path.write_text(content)\n        continue\n\n      content = file_path.read_text()\n\n      content = content.replace(args.old_version.full(), args.new_version.full())\n      content = content.replace(""github.com/tensorflow/tensorflow/blob/r""+args.old_version.short(),\n                                ""github.com/tensorflow/tensorflow/blob/r""+args.old_version.short())\n      file_path.write_text(content)\n'"
tools/templates/build_docs.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Example api reference docs generation script.\n\nThis script generates API reference docs for the reference doc generator.\n\n$> pip install -U git+https://github.com/tensorflow/docs\n$> python build_docs.py\n""""""\n\nimport os\n\nfrom absl import app\nfrom absl import flags\n\nimport tensorflow_docs\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nPROJECT_SHORT_NAME = \'tfdocs\'\nPROJECT_FULL_NAME = \'TensorFlow Docs\'\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \'output_dir\',\n    default=\'/tmp/generated_docs\',\n    help=\'Where to write the resulting docs to.\')\nflags.DEFINE_string(\'code_url_prefix\',\n                    (\'https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs\'),\n                    \'The url prefix for links to code.\')\n\nflags.DEFINE_bool(\'search_hints\', True,\n                  \'Include metadata search hints in the generated files\')\n\nflags.DEFINE_string(\'site_path\', \'/api_docs/python\',\n                    \'Path prefix in the _toc.yaml\')\n\n\ndef gen_api_docs():\n  """"""Generates api docs for the tensorflow docs package.""""""\n\n  # The below `del`\'s are to avoid the api_gen_test to not document these.\n  # Please remove these lines from your build_docs.py files when you create\n  # them.\n  del tensorflow_docs.google\n\n  doc_generator = generate_lib.DocGenerator(\n      root_title=PROJECT_FULL_NAME,\n      # Replace `tensorflow_docs` with your module, here.\n      py_modules=[(PROJECT_SHORT_NAME, tensorflow_docs)],\n      # Replace `tensorflow_docs` with your module, here.\n      base_dir=os.path.dirname(tensorflow_docs.__file__),\n      code_url_prefix=FLAGS.code_url_prefix,\n      search_hints=FLAGS.search_hints,\n      site_path=FLAGS.site_path,\n      private_map={},\n      table_view=True,\n      # This callback cleans up a lot of aliases caused by internal imports.\n      callbacks=[public_api.local_definitions_filter])\n\n  doc_generator.build(FLAGS.output_dir)\n\n  print(\'Output docs to: \', FLAGS.output_dir)\n\n\ndef main(argv):\n  if argv[1:]:\n    raise ValueError(\'Unrecognized arguments: {}\'.format(argv[1:]))\n\n  gen_api_docs()\n\n\nif __name__ == \'__main__\':\n  app.run(main)\n'"
tools/tensorflow_docs/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""tensorflow_docs is a package for generating python api-reference docs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow_docs import api_generator\n'"
tools/tensorflow_docs/api_generator/__init__.py,0,"b'# Lint as: python3\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tools for building tensorflow api reference docs.""""""\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import doc_generator_visitor\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import parser\nfrom tensorflow_docs.api_generator import pretty_docs\nfrom tensorflow_docs.api_generator import public_api\nfrom tensorflow_docs.api_generator import traverse\nfrom tensorflow_docs.api_generator import utils\n'"
tools/tensorflow_docs/api_generator/doc_controls.py,1,"b'# Lint as: python3\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Documentation control decorators.""""""\n\n_NO_SEARCH_HINTS = ""_tf_docs_no_search_hints""\n\n\ndef hide_from_search(obj):\n  """"""Marks an object so metadata search hints will not be included on it\'s page.\n\n  The page is set to ""noindex"" to hide it from search.\n\n  Note: This only makes sense to apply to functions, classes and modules.\n  Constants, and methods do not get their own pages.\n\n  Args:\n    obj: the object to hide.\n\n  Returns:\n    The object.\n  """"""\n  setattr(obj, _NO_SEARCH_HINTS, None)\n  return obj\n\n\ndef should_hide_from_search(obj):\n  """"""Returns true if metadata search hints should not be included.""""""\n  return hasattr(obj, _NO_SEARCH_HINTS)\n\n\n_CUSTOM_PAGE_CONTENT = ""_tf_docs_custom_page_content""\n\n\ndef set_custom_page_content(obj, content):\n  """"""Replace most of the generated page with custom content.""""""\n  setattr(obj, _CUSTOM_PAGE_CONTENT, content)\n\n\ndef get_custom_page_content(obj):\n  """"""Gets custom page content if available.""""""\n  return getattr(obj, _CUSTOM_PAGE_CONTENT, None)\n\n\n_DO_NOT_DOC = ""_tf_docs_do_not_document""\n\n\ndef do_not_generate_docs(obj):\n  """"""A decorator: Do not generate docs for this object.\n\n  For example the following classes:\n\n  ```\n  class Parent(object):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n\n  class Child(Parent):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n  ```\n\n  Produce the following api_docs:\n\n  ```\n  /Parent.md\n    # method1\n    # method2\n  /Child.md\n    # method1\n    # method2\n  ```\n\n  This decorator allows you to skip classes or methods:\n\n  ```\n  @do_not_generate_docs\n  class Parent(object):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n\n  class Child(Parent):\n    @do_not_generate_docs\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n  ```\n\n  This will only produce the following docs:\n\n  ```\n  /Child.md\n    # method2\n  ```\n\n  Note: This is implemented by adding a hidden attribute on the object, so it\n  cannot be used on objects which do not allow new attributes to be added. So\n  this decorator must go *below* `@property`, `@classmethod`,\n  or `@staticmethod`:\n\n  ```\n  class Example(object):\n    @property\n    @do_not_generate_docs\n    def x(self):\n      return self._x\n  ```\n\n  Args:\n    obj: The object to hide from the generated docs.\n\n  Returns:\n    obj\n  """"""\n  setattr(obj, _DO_NOT_DOC, None)\n  return obj\n\n\n_DO_NOT_DOC_INHERITABLE = ""_tf_docs_do_not_doc_inheritable""\n\n\ndef do_not_doc_inheritable(obj):\n  """"""A decorator: Do not generate docs for this method.\n\n  This version of the decorator is ""inherited"" by subclasses. No docs will be\n  generated for the decorated method in any subclass. Even if the sub-class\n  overrides the method.\n\n  For example, to ensure that `method1` is **never documented** use this\n  decorator on the base-class:\n\n  ```\n  class Parent(object):\n    @do_not_doc_inheritable\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n\n  class Child(Parent):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n  ```\n  This will produce the following docs:\n\n  ```\n  /Parent.md\n    # method2\n  /Child.md\n    # method2\n  ```\n\n  When generating docs for a class\'s arributes, the `__mro__` is searched and\n  the attribute will be skipped if this decorator is detected on the attribute\n  on any class in the `__mro__`.\n\n  Note: This is implemented by adding a hidden attribute on the object, so it\n  cannot be used on objects which do not allow new attributes to be added. So\n  this decorator must go *below* `@property`, `@classmethod`,\n  or `@staticmethod`:\n\n  ```\n  class Example(object):\n    @property\n    @do_not_doc_inheritable\n    def x(self):\n      return self._x\n  ```\n\n  Args:\n    obj: The class-attribute to hide from the generated docs.\n\n  Returns:\n    obj\n  """"""\n  setattr(obj, _DO_NOT_DOC_INHERITABLE, None)\n  return obj\n\n\n_FOR_SUBCLASS_IMPLEMENTERS = ""_tf_docs_tools_for_subclass_implementers""\n\n\ndef for_subclass_implementers(obj):\n  """"""A decorator: Only generate docs for this method in the defining class.\n\n  Also group this method\'s docs with and `@abstractmethod` in the class\'s docs.\n\n  No docs will generated for this class attribute in sub-classes.\n\n  The canonical use case for this is `tf.keras.layers.Layer.call`: It\'s a\n  public method, essential for anyone implementing a subclass, but it should\n  never be called directly.\n\n  Works on method, or other class-attributes.\n\n  When generating docs for a class\'s arributes, the `__mro__` is searched and\n  the attribute will be skipped if this decorator is detected on the attribute\n  on any **parent** class in the `__mro__`.\n\n  For example:\n\n  ```\n  class Parent(object):\n    @for_subclass_implementers\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n\n  class Child1(Parent):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n\n  class Child2(Parent):\n    def method1(self):\n      pass\n    def method2(self):\n      pass\n  ```\n\n  This will produce the following docs:\n\n  ```\n  /Parent.md\n    # method1\n    # method2\n  /Child1.md\n    # method2\n  /Child2.md\n    # method2\n  ```\n\n  Note: This is implemented by adding a hidden attribute on the object, so it\n  cannot be used on objects which do not allow new attributes to be added. So\n  this decorator must go *below* `@property`, `@classmethod`,\n  or `@staticmethod`:\n\n  ```\n  class Example(object):\n    @property\n    @for_subclass_implementers\n    def x(self):\n      return self._x\n  ```\n\n  Args:\n    obj: The class-attribute to hide from the generated docs.\n\n  Returns:\n    obj\n  """"""\n  setattr(obj, _FOR_SUBCLASS_IMPLEMENTERS, None)\n  return obj\n\n\ndo_not_doc_in_subclasses = for_subclass_implementers\n\n\ndef should_skip(obj):\n  """"""Returns true if docs generation should be skipped for this object.\n\n  checks for the `do_not_generate_docs` or `do_not_doc_inheritable` decorators.\n\n  Args:\n    obj: The object to document, or skip.\n\n  Returns:\n    True if the object should be skipped\n  """"""\n  if isinstance(obj, type):\n    # For classes, only skip if the attribute is set on _this_ class.\n    if _DO_NOT_DOC in obj.__dict__:\n      return True\n    else:\n      return False\n\n  # Unwrap fget if the object is a property\n  if isinstance(obj, property):\n    obj = obj.fget\n\n  return hasattr(obj, _DO_NOT_DOC) or hasattr(obj, _DO_NOT_DOC_INHERITABLE)\n\n\ndef _unwrap_func(obj):\n  # Unwrap fget if the object is a property or static method or classmethod.\n  if isinstance(obj, property):\n    return obj.fget\n\n  if isinstance(obj, (classmethod, staticmethod)):\n    return obj.__func__\n\n  return obj\n\n\ndef should_skip_class_attr(cls, name):\n  """"""Returns true if docs should be skipped for this class attribute.\n\n  Args:\n    cls: The class the attribute belongs to.\n    name: The name of the attribute.\n\n  Returns:\n    True if the attribute should be skipped.\n  """"""\n  # Get the object with standard lookup, from the nearest\n  # defining parent.\n  try:\n    obj = getattr(cls, name)\n  except AttributeError:\n    # Avoid error caused by enum metaclasses in python3\n    if name in (""name"", ""value""):\n      return True\n    raise\n\n  # Unwrap fget if the object is a property\n  obj = _unwrap_func(obj)\n\n  # Skip if the object is decorated with `do_not_generate_docs` or\n  # `do_not_doc_inheritable`\n  if should_skip(obj):\n    return True\n\n  # Use __dict__ lookup to get the version defined in *this* class.\n  obj = cls.__dict__.get(name, None)\n  obj = _unwrap_func(obj)\n\n  if obj is not None:\n    # If not none, the object is defined in *this* class.\n    # Do not skip if decorated with `for_subclass_implementers`.\n    if hasattr(obj, _FOR_SUBCLASS_IMPLEMENTERS):\n      return False\n\n  # for each parent class\n  for parent in getattr(cls, ""__mro__"", [])[1:]:\n    obj = getattr(parent, name, None)\n\n    if obj is None:\n      continue\n\n    obj = _unwrap_func(obj)\n\n    # Skip if the parent\'s definition is decorated with `do_not_doc_inheritable`\n    # or `for_subclass_implementers`\n    if hasattr(obj, _DO_NOT_DOC_INHERITABLE):\n      return True\n\n    if hasattr(obj, _FOR_SUBCLASS_IMPLEMENTERS):\n      return True\n\n  # No blockng decorators --> don\'t skip\n  return False\n'"
tools/tensorflow_docs/api_generator/doc_controls_test.py,0,"b'# Lint as: python3\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for documentation control decorators.""""""\n\nfrom absl.testing import absltest\n\nfrom tensorflow_docs.api_generator import doc_controls\n\n\nclass DocControlsTest(absltest.TestCase):\n\n  def test_do_not_generate_docs(self):\n\n    @doc_controls.do_not_generate_docs\n    def dummy_function():\n      pass\n\n    self.assertTrue(doc_controls.should_skip(dummy_function))\n\n  def test_do_not_doc_on_method(self):\n    """"""The simple decorator is not aware of inheritance.""""""\n\n    class Parent(object):\n\n      @doc_controls.do_not_generate_docs\n      def my_method(self):\n        pass\n\n    class Child(Parent):\n\n      def my_method(self):\n        pass\n\n    class GrandChild(Child):\n      pass\n\n    self.assertTrue(doc_controls.should_skip(Parent.my_method))\n    self.assertFalse(doc_controls.should_skip(Child.my_method))\n    self.assertFalse(doc_controls.should_skip(GrandChild.my_method))\n\n    self.assertTrue(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertFalse(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n\n  def test_do_not_doc_inheritable(self):\n\n    class Parent(object):\n\n      @doc_controls.do_not_doc_inheritable\n      def my_method(self):\n        pass\n\n    class Child(Parent):\n\n      def my_method(self):\n        pass\n\n    class GrandChild(Child):\n      pass\n\n    self.assertTrue(doc_controls.should_skip(Parent.my_method))\n    self.assertFalse(doc_controls.should_skip(Child.my_method))\n    self.assertFalse(doc_controls.should_skip(GrandChild.my_method))\n\n    self.assertTrue(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n\n  def test_do_not_doc_inheritable_property(self):\n\n    class Parent(object):\n\n      @property\n      @doc_controls.do_not_doc_inheritable\n      def my_method(self):\n        pass\n\n    class Child(Parent):\n\n      @property\n      def my_method(self):\n        pass\n\n    class GrandChild(Child):\n      pass\n\n    self.assertTrue(doc_controls.should_skip(Parent.my_method))\n    self.assertFalse(doc_controls.should_skip(Child.my_method))\n    self.assertFalse(doc_controls.should_skip(GrandChild.my_method))\n\n    self.assertTrue(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n\n  def test_do_not_doc_inheritable_staticmethod(self):\n\n    class GrandParent(object):\n\n      def my_method(self):\n        pass\n\n    class Parent(GrandParent):\n\n      @staticmethod\n      @doc_controls.do_not_doc_inheritable\n      def my_method():\n        pass\n\n    class Child(Parent):\n\n      @staticmethod\n      def my_method():\n        pass\n\n    class GrandChild(Child):\n      pass\n\n    self.assertFalse(doc_controls.should_skip(GrandParent.my_method))\n    self.assertTrue(doc_controls.should_skip(Parent.my_method))\n    self.assertFalse(doc_controls.should_skip(Child.my_method))\n    self.assertFalse(doc_controls.should_skip(GrandChild.my_method))\n\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandParent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n\n  def test_for_subclass_implementers(self):\n\n    class GrandParent(object):\n\n      def my_method(self):\n        pass\n\n    class Parent(GrandParent):\n\n      @doc_controls.for_subclass_implementers\n      def my_method(self):\n        pass\n\n    class Child(Parent):\n      pass\n\n    class GrandChild(Child):\n\n      def my_method(self):\n        pass\n\n    class Grand2Child(Child):\n      pass\n\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandParent, \'my_method\'))\n    self.assertFalse(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(Grand2Child, \'my_method\'))\n\n  def test_for_subclass_implementers_short_circuit(self):\n\n    class GrandParent(object):\n\n      @doc_controls.for_subclass_implementers\n      def my_method(self):\n        pass\n\n    class Parent(GrandParent):\n\n      def my_method(self):\n        pass\n\n    class Child(Parent):\n\n      @doc_controls.do_not_doc_inheritable\n      def my_method(self):\n        pass\n\n    class GrandChild(Child):\n\n      @doc_controls.for_subclass_implementers\n      def my_method(self):\n        pass\n\n    class Grand2Child(Child):\n      pass\n\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandParent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertTrue(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandChild, \'my_method\'))\n    self.assertTrue(\n        doc_controls.should_skip_class_attr(Grand2Child, \'my_method\'))\n\n  def test_skip_class_short_circuit(self):\n\n    class GrandParent(object):\n\n      def my_method(self):\n        pass\n\n    @doc_controls.do_not_generate_docs\n    class Parent(GrandParent):\n      pass\n\n    class Child(Parent):\n      pass\n\n    self.assertFalse(doc_controls.should_skip(Child))\n    self.assertTrue(doc_controls.should_skip(Parent))\n    self.assertFalse(doc_controls.should_skip(GrandParent))\n\n    self.assertFalse(\n        doc_controls.should_skip_class_attr(GrandParent, \'my_method\'))\n    self.assertFalse(doc_controls.should_skip_class_attr(Parent, \'my_method\'))\n    self.assertFalse(doc_controls.should_skip_class_attr(Child, \'my_method\'))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/doc_generator_visitor.py,4,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A `traverse` visitor for processing documentation.""""""\n\nimport collections\nimport inspect\n\n\ndef maybe_singleton(py_object):\n  """"""Returns `True` if `py_object` might be a singleton value .\n\n  Many immutable values in python act like singletons: small ints, some strings,\n  Bools, None, the empty tuple.\n\n  We can\'t rely on looking these up by their `id()` to find their name or\n  duplicates.\n\n  This function checks if the object is one of those maybe singleton values.\n\n  Args:\n    py_object: the object to check.\n\n  Returns:\n    A bool, True if the object might be a singleton.\n  """"""\n  # isinstance accepts nested tuples of types.\n  immutable_types = (int, str, bytes, float, complex, bool, type(None))\n  is_immutable_type = isinstance(py_object, immutable_types)\n\n  # Check if the object is the empty tuple.\n  return is_immutable_type or py_object is ()  # pylint: disable=literal-comparison\n\n\nclass ApiTreeNode(object):\n  """"""Represents a single API end-point.\n\n  Attributes:\n    path: A tuple of strings containing the path to the object from the root\n      like `(\'tf\', \'losses\', \'hinge\')`\n    obj: The python object.\n    children: A dictionary from short name to `ApiTreeNode`, including the\n      children nodes.\n    parent: The parent node.\n    short_name: The last path component\n    full_name: All path components joined with "".""\n  """"""\n\n  def __init__(self, path, obj, parent):\n    self.path = path\n    self.obj = obj\n    self.children = {}\n    self.parent = parent\n\n  @property\n  def short_name(self):\n    return self.path[-1]\n\n  @property\n  def full_name(self):\n    return \'.\'.join(self.path)\n\n\nclass ApiTree(object):\n  """"""Represents all api end-points as a tree.\n\n  Items must be inserted in order, from root to leaf.\n\n  Attributes:\n    index: A dict, mapping from path tuples to `ApiTreeNode`.\n    aliases: A dict, mapping from object ids to a list of all `ApiTreeNode` that\n      refer to the object.\n    root: The root `ApiTreeNode`\n  """"""\n\n  def __init__(self):\n    root = ApiTreeNode((), None, None)\n    self.index = {(): root}\n    self.aliases = collections.defaultdict(list)\n    self.root = root\n\n  def __contains__(self, path):\n    """"""Returns `True` if path exists in the tree.\n\n    Args:\n      path: A tuple of strings, the api path to the object.\n\n    Returns:\n      True if `path` exists in the tree.\n    """"""\n    return path in self.index\n\n  def __getitem__(self, path):\n    """"""Fetch an item from the tree.\n\n    Args:\n      path: A tuple of strings, the api path to the object.\n\n    Returns:\n      An `ApiTreeNode`.\n\n    Raises:\n      KeyError: If no node can be found at that path.\n    """"""\n    return self.index[path]\n\n  def __setitem__(self, path, obj):\n    """"""Add an object to the tree.\n\n    Args:\n      path: A tuple of strings.\n      obj: The python object.\n    """"""\n    parent_path = path[:-1]\n    parent = self.index[parent_path]\n\n    node = ApiTreeNode(path=path, obj=obj, parent=parent)\n\n    self.index[path] = node\n    if not maybe_singleton(obj):\n      # We cannot use the duplicate mechanism for some constants, since e.g.,\n      # id(c1) == id(c2) with c1=1, c2=1. This isn\'t problematic since constants\n      # have no usable docstring and won\'t be documented automatically.\n      self.aliases[id(obj)].append(node)\n    parent.children[node.short_name] = node\n\n\nclass DocGeneratorVisitor(object):\n  """"""A visitor that generates docs for a python object when __call__ed.""""""\n\n  def __init__(self):\n    """"""Make a visitor.\n\n    This visitor expects to be called on each node in the api. It is passed the\n    path to an object, the object, and the filtered list of the object\'s\n    children. (see the `__call__` method for details.\n\n    This object accumulates the various data-structures necessary to build the\n    docs, including (see the property definitions for details.):\n\n    In the decsription below ""master name"" is the object\'s preferred fully\n    qualified name.\n\n    Params:\n      index: A mapping from master names to python python objects.\n      tree: A mapping from master names to a list if attribute names.\n      reverse_index: Mapping from python object ids to master names.\n        Note that this doesn\'t work for python numbers, strings or tuples.\n      duplicate_of: A mapping from a fully qualified names to the object\'s\n        master name. The master names are not included as keys.\n      duplicates: A mapping from master names to lists of other fully qualified\n        names for the object.\n    """"""\n    self._index = {}\n    self._tree = {}\n    self._reverse_index = None\n    self._duplicates = None\n    self._duplicate_of = None\n\n    self._api_tree = ApiTree()\n\n  @property\n  def index(self):\n    """"""A map from fully qualified names to objects to be documented.\n\n    The index is filled when the visitor is passed to `traverse`.\n\n    Returns:\n      The index filled by traversal.\n    """"""\n    return self._index\n\n  @property\n  def tree(self):\n    """"""A map from fully qualified names to all its child names for traversal.\n\n    The full name to member names map is filled when the visitor is passed to\n    `traverse`.\n\n    Returns:\n      The full name to member name map filled by traversal.\n    """"""\n    return self._tree\n\n  @property\n  def reverse_index(self):\n    """"""A map from `id(object)` to the preferred fully qualified name.\n\n    This map only contains non-primitive objects (no numbers or strings) present\n    in `index` (for primitive objects, `id()` doesn\'t quite do the right thing).\n\n    It is computed when it, `duplicate_of`, or `duplicates` are first accessed.\n\n    Returns:\n      The `id(object)` to full name map.\n    """"""\n    self._maybe_find_duplicates()\n    return self._reverse_index\n\n  @property\n  def duplicate_of(self):\n    """"""A map from duplicate full names to a preferred fully qualified name.\n\n    This map only contains names that are not themself a preferred name.\n\n    It is computed when it, `reverse_index`, or `duplicates` are first accessed.\n\n    Returns:\n      The map from duplicate name to preferred name.\n    """"""\n    self._maybe_find_duplicates()\n    return self._duplicate_of\n\n  @property\n  def duplicates(self):\n    """"""A map from preferred full names to a list of all names for this symbol.\n\n    This function returns a map from preferred (master) name for a symbol to a\n    lexicographically sorted list of all aliases for that name (incl. the master\n    name). Symbols without duplicate names do not appear in this map.\n\n    It is computed when it, `reverse_index`, or `duplicate_of` are first\n    accessed.\n\n    Returns:\n      The map from master name to list of all duplicate names.\n    """"""\n    self._maybe_find_duplicates()\n    return self._duplicates\n\n  def __call__(self, parent_path, parent, children):\n    """"""Visitor interface, see `tensorflow/tools/common:traverse` for details.\n\n    This method is called for each symbol found in a traversal using\n    `tensorflow/tools/common:traverse`. It should not be called directly in\n    user code.\n\n    Args:\n      parent_path: A tuple of strings. The fully qualified path to a symbol\n        found during traversal.\n      parent: The Python object referenced by `parent_name`.\n      children: A list of `(name, py_object)` pairs enumerating, in alphabetical\n        order, the children (as determined by `inspect.getmembers`) of\n          `parent`. `name` is the local name of `py_object` in `parent`.\n\n    Returns:\n      The list of children, with any __metaclass__ removed.\n\n    Raises:\n      RuntimeError: If this visitor is called with a `parent` that is not a\n        class or module.\n    """"""\n    parent_name = \'.\'.join(parent_path)\n    self._index[parent_name] = parent\n    self._tree[parent_name] = []\n    if parent_path not in self._api_tree:\n      self._api_tree[parent_path] = parent\n\n    if not (inspect.ismodule(parent) or inspect.isclass(parent)):\n      raise RuntimeError(\'Unexpected type in visitor -- \'\n                         f\'{parent_name}: {parent!r}\')\n\n    for name, child in children:\n      self._api_tree[parent_path + (name,)] = child\n\n      full_name = \'.\'.join([parent_name, name]) if parent_name else name\n      self._index[full_name] = child\n      self._tree[parent_name].append(name)\n\n    return children\n\n  def _score_name(self, name):\n    """"""Return a tuple of scores indicating how to sort for the best name.\n\n    This function is meant to be used as the `key` to the `sorted` function.\n\n    This returns a score tuple composed of the following scores:\n      defining_class: Prefers method names pointing into the defining class,\n        over a subclass (`ParentClass.method` over `Subclass.method`, if it\n        referrs to the same method implementation).\n      experimental: Prefers names that are not in ""contrib"" or ""experimental"".\n      keras: Prefers keras names to non-keras names.\n      module_length: Prefers submodules (tf.sub.thing) over the root namespace\n        (tf.thing) over deeply nested paths (tf.a.b.c.thing)\n      name: Fallback, sorts lexicographically on the full_name.\n\n    Args:\n      name: the full name to score, for example `tf.estimator.Estimator`\n\n    Returns:\n      A tuple of scores. When sorted the preferred name will have the lowest\n      value.\n    """"""\n    parts = name.split(\'.\')\n    short_name = parts[-1]\n    if len(parts) == 1:\n      return (-99, -99, -99, -99, short_name)\n\n    container = self._index[\'.\'.join(parts[:-1])]\n\n    defining_class_score = 1\n    if inspect.isclass(container):\n      if short_name in container.__dict__:\n        # prefer the defining class\n        defining_class_score = -1\n\n    experimental_score = -1\n    if \'contrib\' in parts or any(\'experimental\' in part for part in parts):\n      experimental_score = 1\n\n    keras_score = 1\n    if \'keras\' in parts:\n      keras_score = -1\n\n    while parts:\n      container = self._index[\'.\'.join(parts)]\n      if inspect.ismodule(container):\n        break\n      parts.pop()\n\n    module_length = len(parts)\n\n    if len(parts) == 2:\n      # `tf.submodule.thing` is better than `tf.thing`\n      module_length_score = -1\n    else:\n      # shorter is better\n      module_length_score = module_length\n\n    return (defining_class_score, experimental_score, keras_score,\n            module_length_score, name)\n\n  def _maybe_find_duplicates(self):\n    """"""Compute data structures containing information about duplicates.\n\n    Find duplicates in `index` and decide on one to be the ""master"" name.\n\n    Computes a reverse_index mapping each object id to its master name.\n\n    Also computes a map `duplicate_of` from aliases to their master name (the\n    master name itself has no entry in this map), and a map `duplicates` from\n    master names to a lexicographically sorted list of all aliases for that name\n    (incl. the master name).\n\n    All these are computed and set as fields if they haven\'t already.\n    """"""\n    if self._reverse_index is not None:\n      return\n\n    # Maps the id of a symbol to its fully qualified name. For symbols that have\n    # several aliases, this map contains the first one found.\n    # We use id(py_object) to get a hashable value for py_object. Note all\n    # objects in _index are in memory at the same time so this is safe.\n    reverse_index = {}\n\n    # Decide on master names, rewire duplicates and make a duplicate_of map\n    # mapping all non-master duplicates to the master name. The master symbol\n    # does not have an entry in this map.\n    duplicate_of = {}\n\n    # Duplicates maps the main symbols to the set of all duplicates of that\n    # symbol (incl. itself).\n    duplicates = {}\n\n    for path, node in self._api_tree.index.items():\n      if not path:\n        continue\n      full_name = node.full_name\n      py_object = node.obj\n      object_id = id(py_object)\n      if full_name in duplicates:\n        continue\n\n      aliases = self._api_tree.aliases[object_id]\n      if not aliases:\n        aliases = [node]\n\n      names = [alias.full_name for alias in aliases]\n\n      names = sorted(names)\n      # Choose the master name with a lexical sort on the tuples returned by\n      # by _score_name.\n      master_name = min(names, key=self._score_name)\n\n      if names:\n        duplicates[master_name] = list(names)\n\n      names.remove(master_name)\n      for name in names:\n        duplicate_of[name] = master_name\n\n      # Set the reverse index to the canonical name.\n      if not maybe_singleton(py_object):\n        reverse_index[object_id] = master_name\n\n    self._duplicate_of = duplicate_of\n    self._duplicates = duplicates\n    self._reverse_index = reverse_index\n'"
tools/tensorflow_docs/api_generator/doc_generator_visitor_test.py,89,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tools.docs.doc_generator_visitor.""""""\n\nimport argparse\nimport os\nimport types\n\nfrom absl.testing import absltest\n\nfrom tensorflow_docs.api_generator import doc_generator_visitor\nfrom tensorflow_docs.api_generator import generate_lib\n\n\nclass NoDunderVisitor(doc_generator_visitor.DocGeneratorVisitor):\n\n  def __call__(self, parent_name, parent, children):\n    """"""Drop all the dunder methods to make testing easier.""""""\n    children = [\n        (name, obj) for (name, obj) in children if not name.startswith(\'_\')\n    ]\n    return super(NoDunderVisitor, self).__call__(parent_name, parent, children)\n\n\nclass DocGeneratorVisitorTest(absltest.TestCase):\n\n  def test_call_module(self):\n    visitor = doc_generator_visitor.DocGeneratorVisitor()\n    visitor(\n        (\'doc_generator_visitor\',), doc_generator_visitor,\n        [(\'DocGeneratorVisitor\', doc_generator_visitor.DocGeneratorVisitor)])\n\n    self.assertEqual({\'doc_generator_visitor\': [\'DocGeneratorVisitor\']},\n                     visitor.tree)\n    self.assertEqual({\n        \'doc_generator_visitor\': doc_generator_visitor,\n        \'doc_generator_visitor.DocGeneratorVisitor\':\n        doc_generator_visitor.DocGeneratorVisitor,\n    }, visitor.index)\n\n  def test_call_class(self):\n    visitor = doc_generator_visitor.DocGeneratorVisitor()\n    visitor(\n        (\'DocGeneratorVisitor\',), doc_generator_visitor.DocGeneratorVisitor,\n        [(\'index\', doc_generator_visitor.DocGeneratorVisitor.reverse_index)])\n\n    self.assertEqual({\'DocGeneratorVisitor\': [\'index\']},\n                     visitor.tree)\n    self.assertEqual({\n        \'DocGeneratorVisitor\':\n            doc_generator_visitor.DocGeneratorVisitor,\n        \'DocGeneratorVisitor.index\':\n            doc_generator_visitor.DocGeneratorVisitor.reverse_index\n    }, visitor.index)\n\n  def test_call_raises(self):\n    visitor = doc_generator_visitor.DocGeneratorVisitor()\n    with self.assertRaises(RuntimeError):\n      visitor((\'non_class_or_module\',), \'non_class_or_module_object\', [])\n\n  def test_duplicates_module_class_depth(self):\n\n    class Parent(object):\n\n      class Nested(object):\n        pass\n\n    tf = types.ModuleType(\'tf\')\n    tf.__file__ = \'/tmp/tf/__init__.py\'\n    tf.Parent = Parent\n    tf.submodule = types.ModuleType(\'submodule\')\n    tf.submodule.Parent = Parent\n\n    visitor = generate_lib.extract(\n        [(\'tf\', tf)],\n        base_dir=os.path.dirname(tf.__file__),\n        private_map={},\n        do_not_descend_map={},\n        visitor_cls=NoDunderVisitor)\n\n    self.assertEqual(\n        {\n            \'tf.submodule.Parent\':\n                sorted([\n                    \'tf.Parent\',\n                    \'tf.submodule.Parent\',\n                ]),\n            \'tf.submodule.Parent.Nested\':\n                sorted([\n                    \'tf.Parent.Nested\',\n                    \'tf.submodule.Parent.Nested\',\n                ]),\n            \'tf\': [\'tf\'],\n            \'tf.submodule\': [\'tf.submodule\']\n        }, visitor.duplicates)\n\n    self.assertEqual({\n        \'tf.Parent.Nested\': \'tf.submodule.Parent.Nested\',\n        \'tf.Parent\': \'tf.submodule.Parent\',\n    }, visitor.duplicate_of)\n\n    self.assertEqual({\n        id(Parent): \'tf.submodule.Parent\',\n        id(Parent.Nested): \'tf.submodule.Parent.Nested\',\n        id(tf): \'tf\',\n        id(tf.submodule): \'tf.submodule\',\n    }, visitor.reverse_index)\n\n  def test_duplicates_contrib(self):\n\n    class Parent(object):\n      pass\n\n    tf = types.ModuleType(\'tf\')\n    tf.__file__ = \'/tmp/tf/__init__.py\'\n    tf.contrib = types.ModuleType(\'contrib\')\n    tf.submodule = types.ModuleType(\'submodule\')\n    tf.contrib.Parent = Parent\n    tf.submodule.Parent = Parent\n\n    visitor = generate_lib.extract(\n        [(\'tf\', tf)],\n        base_dir=os.path.dirname(tf.__file__),\n        private_map={},\n        do_not_descend_map={},\n        visitor_cls=NoDunderVisitor)\n\n    self.assertEqual(\n        sorted([\'tf.contrib.Parent\', \'tf.submodule.Parent\']),\n        visitor.duplicates[\'tf.submodule.Parent\'])\n\n    self.assertEqual({\n        \'tf.contrib.Parent\': \'tf.submodule.Parent\',\n    }, visitor.duplicate_of)\n\n    self.assertEqual({\n        id(tf): \'tf\',\n        id(tf.submodule): \'tf.submodule\',\n        id(Parent): \'tf.submodule.Parent\',\n        id(tf.contrib): \'tf.contrib\',\n    }, visitor.reverse_index)\n\n  def test_duplicates_defining_class(self):\n\n    class Parent(object):\n      obj1 = object()\n\n    class Child(Parent):\n      pass\n\n    tf = types.ModuleType(\'tf\')\n    tf.__file__ = \'/tmp/tf/__init__.py\'\n    tf.Parent = Parent\n    tf.Child = Child\n\n    visitor = generate_lib.extract(\n        [(\'tf\', tf)],\n        base_dir=os.path.dirname(tf.__file__),\n        private_map={},\n        do_not_descend_map={},\n        visitor_cls=NoDunderVisitor)\n\n    self.assertEqual(\n        sorted([\n            \'tf.Parent.obj1\',\n            \'tf.Child.obj1\',\n        ]), visitor.duplicates[\'tf.Parent.obj1\'])\n\n    self.assertEqual({\n        \'tf.Child.obj1\': \'tf.Parent.obj1\',\n    }, visitor.duplicate_of)\n\n    self.assertEqual({\n        id(tf): \'tf\',\n        id(Parent): \'tf.Parent\',\n        id(Child): \'tf.Child\',\n        id(Parent.obj1): \'tf.Parent.obj1\',\n    }, visitor.reverse_index)\n\n  def test_duplicates_module_depth(self):\n\n    class Parent(object):\n      pass\n\n    tf = types.ModuleType(\'tf\')\n    tf.__file__ = \'/tmp/tf/__init__.py\'\n    tf.submodule = types.ModuleType(\'submodule\')\n    tf.submodule.submodule2 = types.ModuleType(\'submodule2\')\n    tf.Parent = Parent\n    tf.submodule.submodule2.Parent = Parent\n\n    visitor = generate_lib.extract(\n        [(\'tf\', tf)],\n        base_dir=os.path.dirname(tf.__file__),\n        private_map={},\n        do_not_descend_map={},\n        visitor_cls=NoDunderVisitor)\n\n    self.assertEqual(\n        sorted([\'tf.Parent\', \'tf.submodule.submodule2.Parent\']),\n        visitor.duplicates[\'tf.Parent\'])\n\n    self.assertEqual({\n        \'tf.submodule.submodule2.Parent\': \'tf.Parent\'\n    }, visitor.duplicate_of)\n\n    self.assertEqual({\n        id(tf): \'tf\',\n        id(tf.submodule): \'tf.submodule\',\n        id(tf.submodule.submodule2): \'tf.submodule.submodule2\',\n        id(Parent): \'tf.Parent\',\n    }, visitor.reverse_index)\n\n  def test_duplicates_name(self):\n\n    class Parent(object):\n      obj1 = object()\n\n    Parent.obj2 = Parent.obj1\n\n    tf = types.ModuleType(\'tf\')\n    tf.__file__ = \'/tmp/tf/__init__.py\'\n    tf.submodule = types.ModuleType(\'submodule\')\n    tf.submodule.Parent = Parent\n\n    visitor = generate_lib.extract(\n        [(\'tf\', tf)],\n        base_dir=os.path.dirname(tf.__file__),\n        private_map={},\n        do_not_descend_map={},\n        visitor_cls=NoDunderVisitor)\n    self.assertEqual(\n        sorted([\n            \'tf.submodule.Parent.obj1\',\n            \'tf.submodule.Parent.obj2\',\n        ]), visitor.duplicates[\'tf.submodule.Parent.obj1\'])\n\n    self.assertEqual({\n        \'tf.submodule.Parent.obj2\': \'tf.submodule.Parent.obj1\',\n    }, visitor.duplicate_of)\n\n    self.assertEqual({\n        id(tf): \'tf\',\n        id(tf.submodule): \'tf.submodule\',\n        id(Parent): \'tf.submodule.Parent\',\n        id(Parent.obj1): \'tf.submodule.Parent.obj1\',\n    }, visitor.reverse_index)\n\n\nclass ApiTreeTest(absltest.TestCase):\n\n  def test_contains(self):\n    tf = argparse.Namespace()\n    tf.sub = argparse.Namespace()\n\n    tree = doc_generator_visitor.ApiTree()\n    tree[(\'tf\',)] = tf\n    tree[(\'tf\', \'sub\')] = tf.sub\n\n    self.assertIn((\'tf\',), tree)\n    self.assertIn((\'tf\', \'sub\'), tree)\n\n  def test_node_insertion(self):\n    tf = argparse.Namespace()\n    tf.sub = argparse.Namespace()\n    tf.sub.object = object()\n\n    tree = doc_generator_visitor.ApiTree()\n    tree[(\'tf\',)] = tf\n    tree[(\'tf\', \'sub\')] = tf.sub\n    tree[(\'tf\', \'sub\', \'thing\')] = tf.sub.object\n\n    node = tree[(\'tf\', \'sub\')]\n    self.assertEqual(node.full_name, \'tf.sub\')\n    self.assertIs(node.obj, tf.sub)\n    self.assertIs(node.parent, tree[(\'tf\',)])\n    self.assertLen(node.children, 1)\n    self.assertIs(node.children[\'thing\'], tree[(\'tf\', \'sub\', \'thing\')])\n\n  def test_duplicate(self):\n    tf = argparse.Namespace()\n    tf.sub = argparse.Namespace()\n    tf.sub.thing = object()\n    tf.sub2 = argparse.Namespace()\n    tf.sub2.thing = tf.sub.thing\n\n    tree = doc_generator_visitor.ApiTree()\n    tree[(\'tf\',)] = tf\n    tree[(\'tf\', \'sub\')] = tf.sub\n    tree[(\'tf\', \'sub\', \'thing\')] = tf.sub.thing\n    tree[(\'tf\', \'sub2\')] = tf.sub2\n    tree[(\'tf\', \'sub2\', \'thing\')] = tf.sub2.thing\n\n    self.assertCountEqual(\n        tree.aliases[id(tf.sub.thing)],\n        [tree[(\'tf\', \'sub\', \'thing\')], tree[(\'tf\', \'sub2\', \'thing\')]])\n\n  def test_duplicate_singleton(self):\n    tf = argparse.Namespace()\n    tf.sub = argparse.Namespace()\n    tf.sub.thing = 999\n    tf.sub2 = argparse.Namespace()\n    tf.sub2.thing = tf.sub.thing\n\n    tree = doc_generator_visitor.ApiTree()\n    tree[(\'tf\',)] = tf\n    tree[(\'tf\', \'sub\')] = tf.sub\n    tree[(\'tf\', \'sub\', \'thing\')] = tf.sub.thing\n    tree[(\'tf\', \'sub2\')] = tf.sub2\n    tree[(\'tf\', \'sub2\', \'thing\')] = tf.sub2.thing\n\n    self.assertEmpty(tree.aliases[tf.sub.thing], [])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/generate_lib.py,11,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generate tensorflow.org style API Reference docs for a Python module.""""""\n\nimport collections\nimport fnmatch\nimport inspect\nimport os\nimport pathlib\nimport shutil\nimport tempfile\n\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import doc_generator_visitor\nfrom tensorflow_docs.api_generator import parser\nfrom tensorflow_docs.api_generator import pretty_docs\nfrom tensorflow_docs.api_generator import public_api\nfrom tensorflow_docs.api_generator import py_guide_parser\nfrom tensorflow_docs.api_generator import traverse\n\nimport yaml\n\n# Used to add a collections.OrderedDict representer to yaml so that the\n# dump doesn\'t contain !!OrderedDict yaml tags.\n# Reference: https://stackoverflow.com/a/21048064\n# Using a normal dict doesn\'t preserve the order of the input dictionary.\n_mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n\ndef dict_representer(dumper, data):\n  return dumper.represent_dict(data.items())\n\n\ndef dict_constructor(loader, node):\n  return collections.OrderedDict(loader.construct_pairs(node))\n\n\nyaml.add_representer(collections.OrderedDict, dict_representer)\nyaml.add_constructor(_mapping_tag, dict_constructor)\n\n\nclass TocNode(object):\n  """"""Represents a node in the TOC.\n\n  Attributes:\n    full_name: Name of the module.\n    short_name: The last path component.\n    py_object: Python object of the module.\n    path: Path to the module\'s page on tensorflow.org relative to\n      tensorflow.org.\n    experimental: Whether the module is experimental or not.\n    deprecated: Whether the module is deprecated or not.\n  """"""\n\n  def __init__(self, module, py_object, path):\n    self._module = module\n    self._py_object = py_object\n    self._path = path\n\n  @property\n  def full_name(self):\n    return self._module\n\n  @property\n  def short_name(self):\n    return self.full_name.split(\'.\')[-1]\n\n  @property\n  def py_object(self):\n    return self._py_object\n\n  @property\n  def path(self):\n    return self._path\n\n  @property\n  def experimental(self):\n    return \'experimental\' in self.short_name\n\n  _DEPRECATED_STRING = \'THIS FUNCTION IS DEPRECATED\'\n\n  @property\n  def deprecated(self):\n    """"""Checks if the module is deprecated or not.\n\n    Special case is `tf.contrib`. It doesn\'t have the _tf_decorator attribute\n    but that module should be marked as deprecated.\n\n    Each deprecated function has a `_tf_decorator.decorator_name` attribute.\n    Check the docstring of that function to confirm if the function was\n    indeed deprecated. If a different deprecation setting was used on the\n    function, then ""THIS FUNCTION IS DEPRECATED"" substring won\'t be inserted\n    into the docstring of that function by the decorator.\n\n    Returns:\n      True if depreacted else False.\n    """"""\n\n    if \'tf.contrib\' in self.full_name:\n      return True\n\n    try:\n      # Instead of only checking the docstring, checking for the decorator\n      # provides an additional level of certainty about the correctness of the\n      # the application of `status: deprecated`.\n      decorator_list = parser.extract_decorators(self.py_object)\n      if any(\'deprecat\' in dec for dec in decorator_list):\n        return self._check_docstring()\n    except AttributeError:\n      pass\n\n    return False\n\n  def _check_docstring(self):\n    # Only add the deprecated status if the function is deprecated. There are\n    # other settings that should be ignored like deprecate_args, etc.\n    docstring = self.py_object.__doc__\n    return docstring is not None and self._DEPRECATED_STRING in docstring\n\n\nclass Module(TocNode):\n  """"""Represents a single module and its children and submodules.\n\n  Attributes:\n    full_name: Name of the module.\n    short_name: The last path component.\n    py_object: Python object of the module.\n    title: Title of the module in _toc.yaml\n    path: Path to the module\'s page on tensorflow.org relative to\n      tensorflow.org.\n    children: List of attributes on the module.\n    submodules: List of submodules in the module.\n    experimental: Whether the module is experimental or not.\n    deprecated: Whether the module is deprecated or not.\n  """"""\n\n  def __init__(self, module, py_object, path):\n    super(Module, self).__init__(module, py_object, path)\n\n    self._children = []\n    self._submodules = []\n\n  @property\n  def title(self):\n    if self.full_name.count(\'.\') > 1:\n      title = self.full_name.split(\'.\')[-1]\n    else:\n      title = self.full_name\n    return title\n\n  @property\n  def children(self):\n    return sorted(\n        self._children, key=lambda x: (x.full_name.upper(), x.full_name))\n\n  @property\n  def submodules(self):\n    return self._submodules\n\n  def add_children(self, children):\n    if not isinstance(children, list):\n      children = [children]\n\n    self._children.extend(children)\n\n  def add_submodule(self, sub_mod):\n    self._submodules.append(sub_mod)\n\n\nclass ModuleChild(TocNode):\n  """"""Represents a child of a module.\n\n  Attributes:\n    full_name: Name of the child.\n    short_name: The last path component.\n    py_object: Python object of the child.\n    title: Title of the module in _toc.yaml\n    path: Path to the module\'s page on tensorflow.org relative to\n      tensorflow.org.\n    experimental: Whether the module child is experimental or not.\n    deprecated: Whether the module is deprecated or not.\n  """"""\n\n  def __init__(self, name, py_object, parent, path):\n    self._parent = parent\n    super(ModuleChild, self).__init__(name, py_object, path)\n\n  @property\n  def title(self):\n    return self.full_name[len(self._parent) + 1:]\n\n\nclass GenerateToc(object):\n  """"""Generates a data structure that defines the structure of _toc.yaml.""""""\n\n  def __init__(self, modules):\n    self._modules = modules\n\n  def _create_graph(self):\n    """"""Creates a graph to allow a dfs traversal on it to generate the toc.\n\n    Each graph key contains a module and its value is an object of `Module`\n    class. That module object contains a list of submodules.\n\n    Example low-level structure of the graph is as follows:\n\n    {\n      \'module1\': [submodule1, submodule2],\n      \'submodule1\': [sub1-submodule1],\n      \'sub1-submodule1\': [],\n      \'submodule2\': [],\n      \'module2\': [],\n      \'module3\': [submodule4],\n      \'submodule4\': [sub1-submodule4],\n      \'sub1-submodule4\': [sub1-sub1-submodule4],\n      \'sub1-sub1-submodule4\': []\n    }\n\n    Returns:\n      A tuple of (graph, base_modules). Base modules is returned because while\n      creating a nested list of dictionaries, the top level should only contain\n      the base modules.\n    """"""\n\n    # Sort the modules in case-insensitive alphabetical order.\n    sorted_modules = sorted(self._modules.keys(), key=lambda a: a.lower())\n    toc_base_modules = []\n\n    toc_graph = {}\n    for module in sorted_modules:\n      mod = self._modules[module]\n\n      # Add the module to the graph.\n      toc_graph[module] = mod\n\n      # If the module\'s name contains more than one dot, it is not a base level\n      # module. Hence, add it to its parents submodules list.\n      if module.count(\'.\') > 1:\n        # For example, if module is `tf.keras.applications.densenet` then its\n        # parent is `tf.keras.applications`.\n        parent_module = \'.\'.join(module.split(\'.\')[:-1])\n        parent_mod_obj = toc_graph.get(parent_module, None)\n        if parent_mod_obj is not None:\n          parent_mod_obj.add_submodule(mod)\n      else:\n        toc_base_modules.append(module)\n\n    return toc_graph, toc_base_modules\n\n  def _generate_children(self, mod, is_parent_deprecated):\n    """"""Creates a list of dictionaries containing child\'s title and path.\n\n    For example: The dictionary created will look this this in _toc.yaml.\n\n    ```\n    children_list = [{\'title\': \'Overview\', \'path\': \'/tf/app\'},\n                     {\'title\': \'run\', \'path\': \'/tf/app/run\'}]\n    ```\n\n    The above list will get converted to the following yaml syntax.\n\n    ```\n    - title: Overview\n      path: /tf/app\n    - title: run\n      path: /tf/app/run\n    ```\n\n    Args:\n      mod: A module object.\n      is_parent_deprecated: Bool, Whether the parent is deprecated or not.\n\n    Returns:\n      A list of dictionaries containing child\'s title and path.\n    """"""\n\n    children_list = []\n    children_list.append(\n        collections.OrderedDict([(\'title\', \'Overview\'), (\'path\', mod.path)]))\n\n    for child in mod.children:\n      child_yaml_content = [(\'title\', child.title), (\'path\', child.path)]\n\n      # Set `status: deprecated` only if the parent\'s status is not\n      # deprecated.\n      if child.deprecated and not is_parent_deprecated:\n        child_yaml_content.insert(1, (\'status\', \'deprecated\'))\n      elif child.experimental:\n        child_yaml_content.insert(1, (\'status\', \'experimental\'))\n\n      children_list.append(collections.OrderedDict(child_yaml_content))\n\n    return children_list\n\n  def _dfs(self, mod, visited, is_parent_deprecated):\n    """"""Does a dfs traversal on the graph generated.\n\n    This creates a nested dictionary structure which is then dumped as .yaml\n    file. Each submodule\'s dictionary of title and path is nested under its\n    parent module.\n\n    For example, `tf.keras.app.net` will be nested under `tf.keras.app` which\n    will be nested under `tf.keras`. Here\'s how the nested dictionaries will\n    look when its dumped as .yaml.\n\n    ```\n    - title: tf.keras\n      section:\n      - title: Overview\n        path: /tf/keras\n      - title: app\n        section:\n        - title: Overview\n          path: /tf/keras/app\n        - title: net\n          section:\n          - title: Overview\n            path: /tf/keras/app/net\n    ```\n\n    The above nested structure is what the dfs traversal will create in form\n    of lists of dictionaries.\n\n    Args:\n      mod: A module object.\n      visited: A dictionary of modules visited by the dfs traversal.\n      is_parent_deprecated: Bool, Whether any parent is deprecated or not.\n\n    Returns:\n      A dictionary containing the nested data structure.\n    """"""\n\n    visited[mod.full_name] = True\n\n    # parent_exp is set to the current module because the current module is\n    # the parent for its children.\n    children_list = self._generate_children(\n        mod, is_parent_deprecated or mod.deprecated)\n\n    # generate for submodules within the submodule.\n    for submod in mod.submodules:\n      if not visited[submod.full_name]:\n        sub_mod_dict = self._dfs(submod, visited, is_parent_deprecated or\n                                 mod.deprecated)\n        children_list.append(sub_mod_dict)\n\n    # If the parent module is not experimental, then add the experimental\n    # status to the submodule.\n    submod_yaml_content = [(\'title\', mod.title), (\'section\', children_list)]\n\n    # If the parent module is not deprecated, then add the deprecated\n    # status to the submodule. If the parent is deprecated, then setting its\n    # status to deprecated in _toc.yaml propagates to all its children and\n    # submodules.\n    if mod.deprecated and not is_parent_deprecated:\n      submod_yaml_content.insert(1, (\'status\', \'deprecated\'))\n    elif mod.experimental:\n      submod_yaml_content.insert(1, (\'status\', \'experimental\'))\n\n    return collections.OrderedDict(submod_yaml_content)\n\n  def generate(self):\n    """"""Generates the final toc.\n\n    Returns:\n      A list of dictionaries which will be dumped into .yaml file.\n    """"""\n\n    toc = []\n    toc_graph, toc_base_modules = self._create_graph()\n    visited = {node: False for node in toc_graph.keys()}\n\n    # Sort in alphabetical case-insensitive order.\n    toc_base_modules = sorted(toc_base_modules, key=lambda a: a.lower())\n    for module in toc_base_modules:\n      module_obj = toc_graph[module]\n      # Generate children of the base module.\n      section = self._generate_children(\n          module_obj, is_parent_deprecated=module_obj.deprecated)\n\n      # DFS traversal on the submodules.\n      for sub_mod in module_obj.submodules:\n        sub_mod_list = self._dfs(\n            sub_mod, visited, is_parent_deprecated=module_obj.deprecated)\n        section.append(sub_mod_list)\n\n      module_yaml_content = [(\'title\', module_obj.title), (\'section\', section)]\n      if module_obj.deprecated:\n        module_yaml_content.insert(1, (\'status\', \'deprecated\'))\n      elif module_obj.experimental:\n        module_yaml_content.insert(1, (\'status\', \'experimental\'))\n\n      toc.append(collections.OrderedDict(module_yaml_content))\n\n    return {\'toc\': toc}\n\n\ndef write_docs(output_dir,\n               parser_config,\n               yaml_toc,\n               root_title=\'TensorFlow\',\n               search_hints=True,\n               site_path=\'api_docs/python\',\n               gen_redirects=True,\n               table_view=True):\n  """"""Write previously extracted docs to disk.\n\n  Write a docs page for each symbol included in the indices of parser_config to\n  a tree of docs at `output_dir`.\n\n  Symbols with multiple aliases will have only one page written about\n  them, which is referenced for all aliases.\n\n  Args:\n    output_dir: Directory to write documentation markdown files to. Will be\n      created if it doesn\'t exist.\n    parser_config: A `parser.ParserConfig` object, containing all the necessary\n      indices.\n    yaml_toc: Set to `True` to generate a ""_toc.yaml"" file.\n    root_title: The title name for the root level index.md.\n    search_hints: (bool) include meta-data search hints at the top of each\n      output file.\n    site_path: The output path relative to the site root. Used in the\n      `_toc.yaml` and `_redirects.yaml` files.\n    gen_redirects: Bool which decides whether to generate _redirects.yaml\n        file or not.\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n        converted to a tabular format while generating markdown.\n        If False, they will be converted to a markdown List view.\n\n  Raises:\n    ValueError: if `output_dir` is not an absolute path\n  """"""\n  output_dir = pathlib.Path(output_dir)\n  site_path = pathlib.Path(\'/\', site_path)\n\n  # Make output_dir.\n  if not output_dir.is_absolute():\n    raise ValueError(""\'output_dir\' must be an absolute path.\\n""\n                     f""    output_dir=\'{output_dir}\'"")\n  output_dir.mkdir(parents=True, exist_ok=True)\n\n  # These dictionaries are used for table-of-contents generation below\n  # They will contain, after the for-loop below::\n  #  - module name(string):classes and functions the module contains(list)\n  module_children = {}\n\n  # Collect redirects for an api _redirects.yaml file.\n  redirects = []\n\n  # Parse and write Markdown pages, resolving cross-links (`tf.symbol`).\n  for full_name in sorted(parser_config.index.keys(), key=lambda k: k.lower()):\n    py_object = parser_config.index[full_name]\n\n    if full_name in parser_config.duplicate_of:\n      continue\n\n    # Methods constants are only documented only as part of their parent\'s page.\n    if parser_config.reference_resolver.is_fragment(full_name):\n      continue\n\n    # Remove the extension from the path.\n    docpath, _ = os.path.splitext(parser.documentation_path(full_name))\n\n    # For a module, remember the module for the table-of-contents\n    if inspect.ismodule(py_object):\n      if full_name in parser_config.tree:\n        mod_obj = Module(\n            module=full_name,\n            py_object=py_object,\n            path=str(site_path / docpath))\n        module_children[full_name] = mod_obj\n    # For something else that\'s documented,\n    # figure out what module it lives in\n    else:\n      subname = str(full_name)\n      while True:\n        subname = subname[:subname.rindex(\'.\')]\n        if inspect.ismodule(parser_config.index[subname]):\n          module_name = parser_config.duplicate_of.get(subname, subname)\n          child_mod = ModuleChild(\n              name=full_name,\n              py_object=py_object,\n              parent=module_name,\n              path=str(site_path / docpath))\n          module_children[module_name].add_children(child_mod)\n          break\n\n    # Generate docs for `py_object`, resolving references.\n    try:\n      page_info = parser.docs_for_object(full_name, py_object, parser_config)\n    except:\n      raise ValueError(f\'Failed to generate docs for symbol: `{full_name}`\')\n\n    path = output_dir / parser.documentation_path(full_name)\n    try:\n      path.parent.mkdir(exist_ok=True, parents=True)\n      # This function returns unicode in PY3.\n      hidden = doc_controls.should_hide_from_search(page_info.py_object)\n      if search_hints and not hidden:\n        content = [page_info.get_metadata_html()]\n      else:\n        content = [\'robots: noindex\\n\']\n\n      content.append(pretty_docs.build_md_page(page_info, table_view))\n      text = \'\\n\'.join(content)\n      path.write_text(text, encoding=\'utf-8\')\n    except OSError:\n      raise OSError(\'Cannot write documentation for \'\n                    f\'{full_name} to {path.parent}\')\n\n    duplicates = parser_config.duplicates.get(full_name, [])\n    if not duplicates:\n      continue\n\n    duplicates = [item for item in duplicates if item != full_name]\n\n    if gen_redirects:\n      for dup in duplicates:\n        from_path = site_path / dup.replace(\'.\', \'/\')\n        to_path = site_path / full_name.replace(\'.\', \'/\')\n        redirects.append({\'from\': str(from_path), \'to\': str(to_path)})\n\n  if yaml_toc:\n    toc_gen = GenerateToc(module_children)\n    toc_dict = toc_gen.generate()\n\n    # Replace the overview path *only* for \'TensorFlow\' to\n    # `/api_docs/python/tf_overview`. This will be redirected to\n    # `/api_docs/python/tf`.\n    toc_values = toc_dict[\'toc\'][0]\n    if toc_values[\'title\'] == \'tf\':\n      section = toc_values[\'section\'][0]\n      section[\'path\'] = str(site_path / \'tf_overview\')\n\n    leftnav_toc = output_dir / \'_toc.yaml\'\n    with open(leftnav_toc, \'w\') as toc_file:\n      yaml.dump(toc_dict, toc_file, default_flow_style=False)\n\n  if redirects and gen_redirects:\n    if yaml_toc and toc_values[\'title\'] == \'tf\':\n      redirects.append({\n          \'from\': str(site_path / \'tf_overview\'),\n          \'to\': str(site_path / \'tf\'),\n      })\n    redirects_dict = {\n        \'redirects\': sorted(redirects, key=lambda redirect: redirect[\'from\'])\n    }\n\n    api_redirects_path = output_dir / \'_redirects.yaml\'\n    with open(api_redirects_path, \'w\') as redirect_file:\n      yaml.dump(redirects_dict, redirect_file, default_flow_style=False)\n\n  # Write a global index containing all full names with links.\n  with open(output_dir / \'index.md\', \'w\') as f:\n    global_index = parser.generate_global_index(\n        root_title, parser_config.index, parser_config.reference_resolver)\n    if not search_hints:\n      global_index = \'robots: noindex\\n\' + global_index\n    f.write(global_index)\n\n\ndef add_dict_to_dict(add_from, add_to):\n  for key in add_from:\n    if key in add_to:\n      add_to[key].extend(add_from[key])\n    else:\n      add_to[key] = add_from[key]\n\n\ndef extract(py_modules,\n            base_dir,\n            private_map,\n            do_not_descend_map,\n            visitor_cls=doc_generator_visitor.DocGeneratorVisitor,\n            callbacks=None):\n  """"""Walks the module contents, returns an index of all visited objects.\n\n  The return value is an instance of `self._visitor_cls`, usually:\n  `doc_generator_visitor.DocGeneratorVisitor`\n\n  Args:\n    py_modules: A list containing a single (short_name, module_object) pair.\n      like `[(\'tf\',tf)]`.\n    base_dir: The package root directory. Nothing defined outside of this\n      directory is documented.\n    private_map: A {\'path\':[""name""]} dictionary listing particular object\n      locations that should be ignored in the doc generator.\n    do_not_descend_map: A {\'path\':[""name""]} dictionary listing particular object\n      locations where the children should not be listed.\n    visitor_cls: A class, typically a subclass of\n      `doc_generator_visitor.DocGeneratorVisitor` that acumulates the indexes of\n      objects to document.\n    callbacks: Additional callbacks passed to `traverse`. Executed between the\n      `PublicApiFilter` and the accumulator (`DocGeneratorVisitor`). The\n      primary use case for these is to filter the listy of children (see:\n        `public_api.local_definitions_filter`)\n\n  Returns:\n    The accumulator (`DocGeneratorVisitor`)\n  """"""\n  if callbacks is None:\n    callbacks = []\n\n  if len(py_modules) != 1:\n    raise ValueError(""only pass one [(\'name\',module)] pair in py_modules"")\n  short_name, py_module = py_modules[0]\n\n  api_filter = public_api.PublicAPIFilter(\n      base_dir=base_dir,\n      do_not_descend_map=do_not_descend_map,\n      private_map=private_map)\n\n  accumulator = visitor_cls()\n\n  # The objects found during traversal, and their children are passed to each\n  # of these visitors in sequence. Each visitor returns the list of children\n  # to be passed to the next visitor.\n  visitors = [api_filter, public_api.ignore_typing] + callbacks + [accumulator]\n\n  traverse.traverse(py_module, visitors, short_name)\n\n  return accumulator\n\n\nclass _GetMarkdownTitle(py_guide_parser.PyGuideParser):\n  """"""Extract the title from a .md file.""""""\n\n  def __init__(self):\n    self.title = None\n    py_guide_parser.PyGuideParser.__init__(self)\n\n  def process_title(self, _, title):\n    if self.title is None:  # only use the first title\n      self.title = title\n\n\nEXCLUDED = set([\'__init__.py\', \'OWNERS\', \'README.txt\'])\n\n\ndef replace_refs(src_dir,\n                 output_dir,\n                 reference_resolver,\n                 file_pattern=\'*.md\',\n                 api_docs_relpath=\'api_docs\'):\n  """"""Link `tf.symbol` references found in files matching `file_pattern`.\n\n  A matching directory structure, with the modified files is\n  written to `output_dir`.\n\n  `{""__init__.py"",""OWNERS"",""README.txt""}` are skipped.\n\n  Files not matching `file_pattern` (using `fnmatch`) are copied with no change.\n\n  Also, files in the `api_guides/python` directory get explicit ids set on all\n  heading-2s to ensure back-links work.\n\n  Args:\n    src_dir: The directory to convert files from.\n    output_dir: The root directory to write the resulting files to.\n    reference_resolver: A `parser.ReferenceResolver` to make the replacements.\n    file_pattern: Only replace references in files matching file_patters, using\n      `fnmatch`. Non-matching files are copied unchanged.\n    api_docs_relpath: Relative-path string to the api_docs, from the src_dir.\n  """"""\n  # Iterate through all the source files and process them.\n  for dirpath, _, filenames in os.walk(src_dir):\n    depth = os.path.relpath(src_dir, start=dirpath)\n    # How to get from `dirpath` to api_docs/python/\n    relative_path_to_root = os.path.join(depth, api_docs_relpath, \'python\')\n\n    # Make the directory under output_dir.\n    new_dir = os.path.join(output_dir,\n                           os.path.relpath(path=dirpath, start=src_dir))\n    if not os.path.exists(new_dir):\n      os.makedirs(new_dir)\n\n    for base_name in filenames:\n      if base_name in EXCLUDED:\n        continue\n      full_in_path = os.path.join(dirpath, base_name)\n\n      suffix = os.path.relpath(path=full_in_path, start=src_dir)\n      full_out_path = os.path.join(output_dir, suffix)\n      # Copy files that do not match the file_pattern, unmodified.\n      if not fnmatch.fnmatch(base_name, file_pattern):\n        if full_in_path != full_out_path:\n          shutil.copyfile(full_in_path, full_out_path)\n        continue\n\n      with open(full_in_path, \'rb\') as f:\n        content = f.read().decode(\'utf-8\')\n\n      content = reference_resolver.replace_references(content,\n                                                      relative_path_to_root)\n      with open(full_out_path, \'wb\') as f:\n        f.write(content.encode(\'utf-8\'))\n\n\nclass DocGenerator(object):\n  """"""Main entry point for generating docs.""""""\n\n  def __init__(self,\n               root_title,\n               py_modules,\n               base_dir=None,\n               code_url_prefix=(),\n               search_hints=True,\n               site_path=\'api_docs/python\',\n               private_map=None,\n               do_not_descend_map=None,\n               visitor_cls=doc_generator_visitor.DocGeneratorVisitor,\n               api_cache=True,\n               callbacks=None,\n               yaml_toc=True,\n               gen_redirects=True,\n               table_view=True):\n    """"""Creates a doc-generator.\n\n    Args:\n      root_title: A string. The main title for the project. Like ""TensorFlow""\n      py_modules: The python module to document.\n      base_dir: String or tuple of strings. Directories that ""Defined in"" links\n        are generated relative to. Modules outside one of these directories are\n        not documented. No `base_dir` should be inside another.\n      code_url_prefix: String or tuple of strings. The prefix to add to ""Defined\n        in"" paths. These are zipped with `base-dir`, to set the `defined_in`\n        path for each file. The defined in link for `{base_dir}/path/to/file` is\n        set to `{code_url_prefix}/path/to/file`.\n      search_hints: Bool. Include metadata search hints at the top of each file.\n      site_path: Path prefix in the ""_toc.yaml""\n      private_map: A {""module.path.to.object"": [""names""]} dictionary. Specific\n        aliases that should not be shown in the resulting docs.\n      do_not_descend_map: A {""module.path.to.object"": [""names""]} dictionary.\n        Specific aliases that will be shown, but not expanded.\n      visitor_cls: An option to override the default visitor class\n        `doc_generator_visitor.DocGeneratorVisitor`.\n      api_cache: Bool. Generate an api_cache file. This is used to easily add\n        api links for backticked symbols (like `tf.add`) in other docs.\n      callbacks: Additional callbacks passed to `traverse`. Executed between the\n        `PublicApiFilter` and the accumulator (`DocGeneratorVisitor`). The\n        primary use case for these is to filter the listy of children (see:\n          `public_api.local_definitions_filter`)\n      yaml_toc: Bool which decides whether to generate _toc.yaml file or not.\n      gen_redirects: Bool which decides whether to generate _redirects.yaml\n        file or not.\n      table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n        converted to a tabular format while generating markdown.\n        If False, they will be converted to a markdown List view.\n    """"""\n    self._root_title = root_title\n    self._py_modules = py_modules\n    self._short_name = py_modules[0][0]\n    self._py_module = py_modules[0][1]\n\n    if base_dir is None:\n      # If the user passes a single-file module, only document code defined in\n      # that file.\n      base_dir = self._py_module.__file__\n      if base_dir.endswith(\'__init__.py\'):\n        # If they passed a package, document anything defined in that directory.\n        base_dir = os.path.dirname(base_dir)\n    if isinstance(base_dir, str):\n      base_dir = (base_dir,)\n    self._base_dir = tuple(base_dir)\n    assert self._base_dir, \'`base_dir` cannot be empty\'\n\n    if isinstance(code_url_prefix, str):\n      code_url_prefix = (code_url_prefix,)\n    self._code_url_prefix = tuple(code_url_prefix)\n    if not self._code_url_prefix:\n      raise ValueError(\'`code_url_prefix` cannot be empty\')\n\n    if len(self._code_url_prefix) != len(base_dir):\n      raise ValueError(\'The `base_dir` list should have the same number of \'\n                       \'elements as the `code_url_prefix` list (they get \'\n                       \'zipped together).\')\n\n    self._search_hints = search_hints\n    self._site_path = site_path\n    self._private_map = private_map or {}\n    self._do_not_descend_map = do_not_descend_map or {}\n    self._visitor_cls = visitor_cls\n    self.api_cache = api_cache\n    if callbacks is None:\n      callbacks = []\n    self._callbacks = callbacks\n    self._yaml_toc = yaml_toc\n    self._gen_redirects = gen_redirects\n    self._table_view = table_view\n\n  def make_reference_resolver(self, visitor):\n    return parser.ReferenceResolver.from_visitor(\n        visitor, py_module_names=[self._short_name])\n\n  def make_parser_config(self, visitor, reference_resolver):\n    return parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates=visitor.duplicates,\n        duplicate_of=visitor.duplicate_of,\n        tree=visitor.tree,\n        index=visitor.index,\n        reverse_index=visitor.reverse_index,\n        base_dir=self._base_dir,\n        code_url_prefix=self._code_url_prefix)\n\n  def run_extraction(self):\n    """"""Walks the module contents, returns an index of all visited objects.\n\n    The return value is an instance of `self._visitor_cls`, usually:\n    `doc_generator_visitor.DocGeneratorVisitor`\n\n    Returns:\n    """"""\n    return extract(\n        py_modules=self._py_modules,\n        base_dir=self._base_dir,\n        private_map=self._private_map,\n        do_not_descend_map=self._do_not_descend_map,\n        visitor_cls=self._visitor_cls,\n        callbacks=self._callbacks)\n\n  def build(self, output_dir):\n    """"""Build all the docs.\n\n    This produces python api docs:\n      * generated from `py_module`.\n      * written to \'{output_dir}/api_docs/python/\'\n\n    Args:\n      output_dir: Where to write the resulting docs.\n    """"""\n    workdir = pathlib.Path(tempfile.mkdtemp())\n\n    # Extract the python api from the _py_modules\n    visitor = self.run_extraction()\n    reference_resolver = self.make_reference_resolver(visitor)\n    # Replace all the `tf.symbol` references in the workdir.\n    replace_refs(\n        str(workdir), str(workdir), reference_resolver, file_pattern=\'*.md\')\n\n    # Write the api docs.\n    parser_config = self.make_parser_config(visitor, reference_resolver)\n    work_py_dir = workdir / \'api_docs/python\'\n    write_docs(\n        output_dir=str(work_py_dir),\n        parser_config=parser_config,\n        yaml_toc=self._yaml_toc,\n        root_title=self._root_title,\n        search_hints=self._search_hints,\n        site_path=self._site_path,\n        gen_redirects=self._gen_redirects,\n        table_view=self._table_view)\n\n    if self.api_cache:\n      reference_resolver.to_json_file(\n          str(work_py_dir / self._short_name / \'_api_cache.json\'))\n\n    try:\n      os.makedirs(output_dir)\n    except OSError as e:\n      if e.strerror != \'File exists\':\n        raise\n\n    # Typical results are something like:\n    #\n    # out_dir/\n    #    {short_name}/\n    #    _redirects.yaml\n    #    _toc.yaml\n    #    index.md\n    #    {short_name}.md\n    #\n    # Copy the top level files to the `{output_dir}/`, delete and replace the\n    # `{output_dir}/{short_name}/` directory.\n\n    for work_path in work_py_dir.glob(\'*\'):\n      out_path = pathlib.Path(output_dir) / work_path.name\n      out_path.parent.mkdir(exist_ok=True, parents=True)\n\n      if work_path.is_file():\n        shutil.copy2(work_path, out_path)\n      elif work_path.is_dir():\n        shutil.rmtree(out_path, ignore_errors=True)\n        shutil.copytree(work_path, out_path)\n'"
tools/tensorflow_docs/api_generator/generate_lib_test.py,21,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for doc generator traversal.""""""\n\nimport os\nimport pathlib\nimport sys\nimport tempfile\n\nfrom absl import flags\nfrom absl.testing import absltest\n\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import parser\n\nimport yaml\n\nFLAGS = flags.FLAGS\n\n\ndef deprecated(func):\n  return func\n\n\n@deprecated\ndef test_function():\n  """"""Docstring for test_function.\n\n  THIS FUNCTION IS DEPRECATED and will be removed after some time.\n  """"""\n  pass\n\n\nclass TestClass(object):\n  """"""Docstring for TestClass itself.""""""\n\n  class ChildClass(object):\n    """"""Docstring for a child class.""""""\n\n    class GrandChildClass(object):\n      """"""Docstring for a child of a child class.""""""\n      pass\n\n\nclass DummyVisitor(object):\n\n  def __init__(self, index, duplicate_of):\n    self.index = index\n    self.duplicate_of = duplicate_of\n\n\nclass GenerateTest(absltest.TestCase):\n  _BASE_DIR = tempfile.mkdtemp()\n\n  def setUp(self):\n    super(GenerateTest, self).setUp()\n    self.workdir = os.path.join(self._BASE_DIR, self.id())\n    os.makedirs(self.workdir)\n\n  def get_test_objects(self):\n    # These are all mutable objects, so rebuild them for each test.\n    # Don\'t cache the objects.\n    module = sys.modules[__name__]\n\n    index = {\n        \'tf\': sys,  # Can be any module, this test doesn\'t care about content.\n        \'tf.TestModule\': module,\n        \'tf.test_function\': test_function,\n        \'tf.TestModule.test_function\': test_function,\n        \'tf.TestModule.TestClass\': TestClass,\n        \'tf.TestModule.TestClass.ChildClass\': TestClass.ChildClass,\n        \'tf.TestModule.TestClass.ChildClass.GrandChildClass\':\n        TestClass.ChildClass.GrandChildClass,\n    }\n\n    tree = {\n        \'tf\': [\'TestModule\', \'test_function\'],\n        \'tf.TestModule\': [\'test_function\', \'TestClass\'],\n        \'tf.TestModule.TestClass\': [\'ChildClass\'],\n        \'tf.TestModule.TestClass.ChildClass\': [\'GrandChildClass\'],\n        \'tf.TestModule.TestClass.ChildClass.GrandChildClass\': []\n    }\n\n    duplicate_of = {\'tf.test_function\': \'tf.TestModule.test_function\'}\n\n    duplicates = {\n        \'tf.TestModule.test_function\': [\n            \'tf.test_function\', \'tf.TestModule.test_function\'\n        ]\n    }\n\n    base_dir = os.path.dirname(__file__)\n\n    visitor = DummyVisitor(index, duplicate_of)\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates=duplicates,\n        duplicate_of=duplicate_of,\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=base_dir,\n        code_url_prefix=\'/\')\n\n    return reference_resolver, parser_config\n\n  def test_write(self):\n    _, parser_config = self.get_test_objects()\n\n    output_dir = pathlib.Path(self.workdir)\n\n    generate_lib.write_docs(output_dir, parser_config, yaml_toc=True)\n\n    # Check redirects\n    redirects_file = output_dir / \'_redirects.yaml\'\n    self.assertTrue(redirects_file.exists())\n    redirects = yaml.safe_load(redirects_file.read_text())\n    self.assertEqual(\n        redirects, {\n            \'redirects\': [{\n                \'from\': \'/api_docs/python/tf/test_function\',\n                \'to\': \'/api_docs/python/tf/TestModule/test_function\'\n            }, {\n                \'from\': \'/api_docs/python/tf_overview\',\n                \'to\': \'/api_docs/python/tf\'\n            }]\n        })\n\n    toc_file = output_dir / \'_toc.yaml\'\n    self.assertTrue(toc_file.exists())\n    toc_list = yaml.safe_load(toc_file.read_text())[\'toc\']\n\n    # Number of sections in the toc should be 2.\n    self.assertLen([item for item in toc_list if \'section\' in item], 2)\n\n    # The last path in the TOC must be the ground truth below.\n    # This will check if the symbols are being sorted in case-insensitive\n    # alphabetical order too, spanning across submodules and children.\n    test_function_toc = toc_list[1][\'section\'][-1]\n    self.assertEqual(test_function_toc[\'path\'],\n                     \'/api_docs/python/tf/TestModule/test_function\')\n    self.assertEqual(test_function_toc[\'status\'], \'deprecated\')\n\n    # Make sure that the right files are written to disk.\n    self.assertTrue(os.path.exists(os.path.join(output_dir, \'index.md\')))\n    self.assertTrue(os.path.exists(os.path.join(output_dir, \'tf.md\')))\n    self.assertTrue(os.path.exists(os.path.join(output_dir, \'_toc.yaml\')))\n    self.assertTrue(\n        os.path.exists(os.path.join(output_dir, \'tf/TestModule.md\')))\n    self.assertFalse(\n        os.path.exists(os.path.join(output_dir, \'tf/test_function.md\')))\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(output_dir, \'tf/TestModule/TestClass.md\')))\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(output_dir,\n                         \'tf/TestModule/TestClass/ChildClass.md\')))\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(\n                output_dir,\n                \'tf/TestModule/TestClass/ChildClass/GrandChildClass.md\')))\n    # Make sure that duplicates are not written\n    self.assertTrue(\n        os.path.exists(\n            os.path.join(output_dir, \'tf/TestModule/test_function.md\')))\n\n  def test_replace_refes(self):\n    test_dir = self.workdir\n    test_in_dir = os.path.join(test_dir, \'in\')\n    test_in_dir_a = os.path.join(test_dir, \'in/a\')\n    test_in_dir_b = os.path.join(test_dir, \'in/b\')\n    os.makedirs(test_in_dir)\n    os.makedirs(test_in_dir_a)\n    os.makedirs(test_in_dir_b)\n\n    test_out_dir = os.path.join(test_dir, \'out\')\n    os.makedirs(test_out_dir)\n\n    test_path1 = os.path.join(test_in_dir_a, \'file1.md\')\n    test_path2 = os.path.join(test_in_dir_b, \'file2.md\')\n    test_path3 = os.path.join(test_in_dir_b, \'file3.notmd\')\n    test_path4 = os.path.join(test_in_dir_b, \'OWNERS\')\n\n    with open(test_path1, \'w\') as f:\n      f.write(\'Use `tf.test_function` to test things.\')\n\n    with open(test_path2, \'w\') as f:\n      f.write(\'Use `tf.TestModule.TestClass.ChildClass` to test things.\\n\'\n              ""`tf.whatever` doesn\'t exist"")\n\n    with open(test_path3, \'w\') as f:\n      file3_content = (\n          \'Not a .md file. Should be copied unchanged:\'\n          \'`tf.TestModule.TestClass.ChildClass`, `tf.test_function`\')\n      f.write(file3_content)\n\n    with open(test_path4, \'w\') as f:\n      f.write(\'\')\n\n    reference_resolver, _ = self.get_test_objects()\n    generate_lib.replace_refs(test_in_dir, test_out_dir, reference_resolver,\n                              \'*.md\')\n\n    with open(os.path.join(test_out_dir, \'a/file1.md\')) as f:\n      content = f.read()\n      self.assertEqual(\n          content,\n          \'Use <a href=""../api_docs/python/tf/TestModule/test_function.md"">\'\n          \'<code>tf.test_function</code></a> to test things.\')\n\n    with open(os.path.join(test_out_dir, \'b/file2.md\')) as f:\n      content = f.read()\n      self.assertEqual(\n          content,\n          \'Use \'\n          \'<a href=""../api_docs/python/tf/TestModule/TestClass/ChildClass.md"">\'\n          \'<code>tf.TestModule.TestClass.ChildClass</code></a> \'\n          \'to test things.\\n\'\n          \'`tf.whatever` doesn\\\'t exist\')\n\n    with open(os.path.join(test_out_dir, \'b/file3.notmd\')) as f:\n      content = f.read()\n      self.assertEqual(content, file3_content)\n\n    with self.assertRaises(IOError):\n      # This should fail. The OWNERS file should not be copied\n      with open(os.path.join(test_out_dir, \'b/OWNERS\')) as f:\n        content = f.read()\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/parser.py,33,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Turn Python docstrings into Markdown for TensorFlow documentation.""""""\n\nimport ast\nimport collections\nimport enum\nimport functools\nimport inspect\nimport itertools\nimport json\nimport os\nimport re\nimport textwrap\nimport typing\n\nfrom typing import Any, Dict, List, Tuple, Iterable, NamedTuple, Optional, Union\n\nimport astor\n\nfrom tensorflow_docs.api_generator import doc_controls\n\nfrom google.protobuf.message import Message as ProtoMessage\n\n\nclass ObjType(enum.Enum):\n  """"""Enum to standardize object type checks.""""""\n  TYPE_ALIAS = \'type_alias\'\n  MODULE = \'module\'\n  CLASS = \'class\'\n  CALLABLE = \'callable\'\n  PROPERTY = \'property\'\n  OTHER = \'other\'\n\n\ndef get_obj_type(py_obj: Any) -> ObjType:\n  """"""Get the `ObjType` for the `py_object`.""""""\n  if hasattr(py_obj, \'__args__\') and hasattr(py_obj, \'__origin__\'):\n    return ObjType.TYPE_ALIAS\n  elif inspect.ismodule(py_obj):\n    return ObjType.MODULE\n  elif inspect.isclass(py_obj):\n    return ObjType.CLASS\n  elif callable(py_obj):\n    return ObjType.CALLABLE\n  elif isinstance(py_obj, property):\n    return ObjType.PROPERTY\n  else:\n    return ObjType.OTHER\n\n\nclass ParserConfig(object):\n  """"""Stores all indexes required to parse the docs.""""""\n\n  def __init__(self, reference_resolver, duplicates, duplicate_of, tree, index,\n               reverse_index, base_dir, code_url_prefix):\n    """"""Object with the common config for docs_for_object() calls.\n\n    Args:\n      reference_resolver: An instance of ReferenceResolver.\n      duplicates: A `dict` mapping fully qualified names to a set of all aliases\n        of this name. This is used to automatically generate a list of all\n        aliases for each name.\n      duplicate_of: A map from duplicate names to preferred names of API\n        symbols.\n      tree: A `dict` mapping a fully qualified name to the names of all its\n        members. Used to populate the members section of a class or module page.\n      index: A `dict` mapping full names to objects.\n      reverse_index: A `dict` mapping object ids to full names.\n      base_dir: A base path that is stripped from file locations written to the\n        docs.\n      code_url_prefix: A Url to pre-pend to the links to file locations.\n    """"""\n    self.reference_resolver = reference_resolver\n    self.duplicates = duplicates\n    self.duplicate_of = duplicate_of\n    self.tree = tree\n    self.reverse_index = reverse_index\n    self.index = index\n    self.base_dir = base_dir\n    self.code_url_prefix = code_url_prefix\n\n  def py_name_to_object(self, full_name):\n    """"""Return the Python object for a Python symbol name.""""""\n    return self.index[full_name]\n\n\nclass _FileLocation(object):\n  """"""This class indicates that the object is defined in a regular file.\n\n  This can be used for the `defined_in` slot of the `PageInfo` objects.\n  """"""\n  GITHUB_LINE_NUMBER_TEMPLATE = \'#L{start_line:d}-L{end_line:d}\'\n\n  def __init__(self, rel_path, url=None, start_line=None, end_line=None):\n    self.rel_path = rel_path\n    self.url = url\n    self.start_line = start_line\n    self.end_line = end_line\n\n    github_master_re = \'github.com.*?(blob|tree)/master\'\n    suffix = \'\'\n    # Only attach a line number for github URLs that are not using ""master""\n    if self.start_line and not re.search(github_master_re, self.url):\n      if \'github.com\' in self.url:\n        suffix = self.GITHUB_LINE_NUMBER_TEMPLATE.format(\n            start_line=self.start_line, end_line=self.end_line)\n\n        self.url = self.url + suffix\n\n\ndef is_class_attr(full_name, index):\n  """"""Check if the object\'s parent is a class.\n\n  Args:\n    full_name: The full name of the object, like `tf.module.symbol`.\n    index: The {full_name:py_object} dictionary for the public API.\n\n  Returns:\n    True if the object is a class attribute.\n  """"""\n  parent_name = full_name.rsplit(\'.\', 1)[0]\n  if inspect.isclass(index[parent_name]):\n    return True\n\n  return False\n\n\nclass TFDocsError(Exception):\n  pass\n\n\ndef documentation_path(full_name, is_fragment=False):\n  """"""Returns the file path for the documentation for the given API symbol.\n\n  Given the fully qualified name of a library symbol, compute the path to which\n  to write the documentation for that symbol (relative to a base directory).\n  Documentation files are organized into directories that mirror the python\n  module/class structure.\n\n  Args:\n    full_name: Fully qualified name of a library symbol.\n    is_fragment: If `False` produce a direct markdown link (`tf.a.b.c` -->\n      `tf/a/b/c.md`). If `True` produce fragment link, `tf.a.b.c` -->\n      `tf/a/b.md#c`\n\n  Returns:\n    The file path to which to write the documentation for `full_name`.\n  """"""\n  parts = full_name.split(\'.\')\n  if is_fragment:\n    parts, fragment = parts[:-1], parts[-1]\n\n  result = os.path.join(*parts) + \'.md\'\n\n  if is_fragment:\n    result = result + \'#\' + fragment\n\n  return result\n\n\ndef _get_raw_docstring(py_object):\n  """"""Get the docs for a given python object.\n\n  Args:\n    py_object: A python object to retrieve the docs for (class, function/method,\n      or module).\n\n  Returns:\n    The docstring, or the empty string if no docstring was found.\n  """"""\n\n  # For object instances, inspect.getdoc does give us the docstring of their\n  # type, which is not what we want. Only return the docstring if it is useful.\n  if get_obj_type(py_object) not in (ObjType.TYPE_ALIAS, ObjType.OTHER):\n    result = inspect.getdoc(py_object) or \'\'\n  else:\n    result = \'\'\n\n  result = _StripTODOs()(result)\n  result = _StripPylints()(result)\n  result = _AddDoctestFences()(result + \'\\n\')\n  return result\n\n\nclass _AddDoctestFences(object):\n  """"""Adds ``` fences around doctest caret blocks >>> that don\'t have them.""""""\n  CARET_BLOCK_RE = re.compile(\n      r""""""\n    (?<=\\n)\\ *\\n                           # After a blank line.\n    (?P<indent>\\ *)(?P<content>\\>\\>\\>.*?)  # Whitespace and a triple caret.\n    \\n\\s*?(?=\\n|$)                         # Followed by a blank line"""""",\n      re.VERBOSE | re.DOTALL)\n\n  def _sub(self, match):\n    groups = match.groupdict()\n    fence = f""\\n{groups[\'indent\']}```\\n""\n\n    content = groups[\'indent\'] + groups[\'content\']\n    return \'\'.join([fence, content, fence])\n\n  def __call__(self, content):\n    return self.CARET_BLOCK_RE.sub(self._sub, content)\n\n\nclass _StripTODOs(object):\n  TODO_RE = re.compile(\'#? *TODO.*\')\n\n  def __call__(self, content: str) -> str:\n    return self.TODO_RE.sub(\'\', content)\n\n\nclass _StripPylints(object):\n  PYLINT_RE = re.compile(\'# *?pylint:.*\')\n\n  def __call__(self, content: str) -> str:\n    return self.PYLINT_RE.sub(\'\', content)\n\n\nclass IgnoreLineInBlock(object):\n  """"""Ignores the lines in a block.\n\n  Attributes:\n    block_start: Contains the start string of a block to ignore.\n    block_end: Contains the end string of a block to ignore.\n  """"""\n\n  def __init__(self, block_start, block_end):\n    self._block_start = block_start\n    self._block_end = block_end\n    self._in_block = False\n\n    self._start_end_regex = re.escape(self._block_start) + r\'.*?\' + re.escape(\n        self._block_end)\n\n  def __call__(self, line):\n    # If start and end block are on the same line, return True.\n    if re.match(self._start_end_regex, line):\n      return True\n\n    if not self._in_block:\n      if self._block_start in line:\n        self._in_block = True\n\n    elif self._block_end in line:\n      self._in_block = False\n      # True is being returned here because the last line in the block should\n      # also be ignored.\n      return True\n\n    return self._in_block\n\n\n# ?P<...> helps to find the match by entering the group name instead of the\n# index. For example, instead of doing match.group(1) we can do\n# match.group(\'brackets\')\nAUTO_REFERENCE_RE = re.compile(\n    r""""""\n    (?P<brackets>\\[.*?\\])                    # find characters inside \'[]\'\n    |\n    `(?P<backticks>[\\w\\(\\[\\)\\]\\{\\}.,=\\s]+?)` # or find characters inside \'``\'\n    """""",\n    flags=re.VERBOSE)\n\n\nclass ReferenceResolver(object):\n  """"""Class for replacing `tf.symbol` references with Markdown links.""""""\n\n  def __init__(self, duplicate_of, is_fragment, py_module_names):\n    """"""Initializes a Reference Resolver.\n\n    Args:\n      duplicate_of: A map from duplicate names to preferred names of API\n        symbols.\n      is_fragment: A map from full names to bool for each symbol. If True the\n        object lives at a page fragment `tf.a.b.c` --> `tf/a/b#c`. If False\n        object has a page to itself: `tf.a.b.c` --> `tf/a/b/c`.\n      py_module_names: A list of string names of Python modules.\n    """"""\n    self._duplicate_of = duplicate_of\n    self._is_fragment = is_fragment\n    self._all_names = set(is_fragment.keys())\n    self._py_module_names = py_module_names\n    self._partial_symbols_dict = self._create_partial_symbols_dict()\n\n  @classmethod\n  def from_visitor(cls, visitor, **kwargs):\n    """"""A factory function for building a ReferenceResolver from a visitor.\n\n    Args:\n      visitor: an instance of `DocGeneratorVisitor`\n      **kwargs: all remaining args are passed to the constructor\n\n    Returns:\n      an instance of `ReferenceResolver` ()\n    """"""\n    is_fragment = {}\n    for full_name, obj in visitor.index.items():\n      obj_type = get_obj_type(obj)\n      if obj_type in (ObjType.CLASS, ObjType.MODULE):\n        is_fragment[full_name] = False\n      elif obj_type in (ObjType.CALLABLE, ObjType.TYPE_ALIAS):\n        if is_class_attr(full_name, visitor.index):\n          is_fragment[full_name] = True\n        else:\n          is_fragment[full_name] = False\n      else:\n        is_fragment[full_name] = True\n\n    return cls(\n        duplicate_of=visitor.duplicate_of, is_fragment=is_fragment, **kwargs)\n\n  def is_fragment(self, full_name: str):\n    """"""Returns True if the object\'s doc is a subsection of another page.""""""\n    return self._is_fragment[full_name]\n\n  @classmethod\n  def from_json_file(cls, filepath):\n    """"""Initialize the reference resolver via _api_cache.json.""""""\n    with open(filepath) as f:\n      json_dict = json.load(f)\n\n    return cls(**json_dict)\n\n  def _partial_symbols(self, symbol):\n    """"""Finds the partial symbols given the true symbol.\n\n    For example, symbol: `tf.keras.layers.Conv2D`, then the partial dictionary\n    returned will be:\n\n    partials = [""tf.keras.layers.Conv2D"",""keras.layers.Conv2D"",""layers.Conv2D""]\n\n    There should at least be one \'.\' in the partial symbol generated so as to\n    avoid guessing for the true symbol.\n\n    Args:\n      symbol: String, representing the true symbol.\n\n    Returns:\n      A list of partial symbol names\n    """"""\n\n    split_symbol = symbol.split(\'.\')\n    partials = [\n        \'.\'.join(split_symbol[i:]) for i in range(1,\n                                                  len(split_symbol) - 1)\n    ]\n    return partials\n\n  def _create_partial_symbols_dict(self):\n    """"""Creates a partial symbols dictionary for all the symbols in TensorFlow.\n\n    Returns:\n      A dictionary mapping {partial_symbol: real_symbol}\n    """"""\n    partial_symbols_dict = collections.defaultdict(list)\n\n    for name in sorted(self._all_names):\n      if \'tf.compat.v\' in name or \'tf.contrib\' in name:\n        continue\n      partials = self._partial_symbols(name)\n      for partial in partials:\n        partial_symbols_dict[partial].append(name)\n\n    new_partial_dict = {}\n    for partial, full_names in partial_symbols_dict.items():\n      if not full_names:\n        continue\n\n      full_names = [\n          self._duplicate_of.get(full_name, full_name)\n          for full_name in full_names\n      ]\n\n      new_partial_dict[partial] = full_names[0]\n\n    return new_partial_dict\n\n  def to_json_file(self, filepath):\n    """"""Converts the RefenceResolver to json and writes it to the specified file.\n\n    Args:\n      filepath: The file path to write the json to.\n    """"""\n\n    try:\n      os.makedirs(os.path.dirname(filepath))\n    except OSError:\n      pass\n\n    json_dict = {}\n    for key, value in self.__dict__.items():\n      # Drop these fields, they are generated by the constructor.\n      if key == \'_all_names\' or key == \'_partial_symbols_dict\':\n        continue\n\n      # Strip off any leading underscores on field names as these are not\n      # recognized by the constructor.\n      json_dict[key.lstrip(\'_\')] = value\n\n    with open(filepath, \'w\') as f:\n      json.dump(json_dict, f, indent=2, sort_keys=True)\n      f.write(\'\\n\')\n\n  def replace_references(self, string, relative_path_to_root, full_name=None):\n    """"""Replace `tf.symbol` references with links to symbol\'s documentation page.\n\n    This function finds all occurrences of ""`tf.symbol`"" in `string`\n    and replaces them with markdown links to the documentation page\n    for ""symbol"".\n\n    `relative_path_to_root` is the relative path from the document\n    that contains the ""`tf.symbol`"" reference to the root of the API\n    documentation that is linked to. If the containing page is part of\n    the same API docset, `relative_path_to_root` can be set to\n    `os.path.dirname(documentation_path(name))`, where `name` is the\n    python name of the object whose documentation page the reference\n    lives on.\n\n    Args:\n      string: A string in which ""`tf.symbol`"" references should be replaced.\n      relative_path_to_root: The relative path from the containing document to\n        the root of the API documentation that is being linked to.\n      full_name: (optional) The full name of current object, so replacements can\n        depend on context.\n\n    Returns:\n      `string`, with ""`tf.symbol`"" references replaced by Markdown links.\n    """"""\n\n    def one_ref(match):\n      return self._one_ref(match, relative_path_to_root, full_name)\n\n    fixed_lines = []\n\n    filters = [\n        IgnoreLineInBlock(\'<pre class=""tfo-notebook-code-cell-output"">\',\n                          \'</pre>\'),\n        IgnoreLineInBlock(\'```\', \'```\')\n    ]\n\n    for line in string.splitlines():\n      if not any(filter_block(line) for filter_block in filters):\n        line = re.sub(AUTO_REFERENCE_RE, one_ref, line)\n      fixed_lines.append(line)\n\n    return \'\\n\'.join(fixed_lines)\n\n  def python_link(self,\n                  link_text,\n                  ref_full_name,\n                  relative_path_to_root,\n                  code_ref=True):\n    """"""Resolve a ""`tf.symbol`"" reference to a Markdown link.\n\n    This will pick the canonical location for duplicate symbols.  The\n    input to this function should already be stripped of the \'@\' and\n    \'{}\'.  This function returns a Markdown link. If `code_ref` is\n    true, it is assumed that this is a code reference, so the link\n    text will be rendered as code (using backticks).\n    `link_text` should refer to a library symbol, starting with \'tf.\'.\n\n    Args:\n      link_text: The text of the Markdown link.\n      ref_full_name: The fully qualified name of the symbol to link to.\n      relative_path_to_root: The relative path from the location of the current\n        document to the root of the API documentation.\n      code_ref: If true (the default), put `link_text` in `...`.\n\n    Returns:\n      A markdown link to the documentation page of `ref_full_name`.\n    """"""\n    url = self.reference_to_url(ref_full_name, relative_path_to_root)\n\n    if code_ref:\n      link_text = link_text.join([\'<code>\', \'</code>\'])\n    else:\n      link_text = self._link_text_to_html(link_text)\n\n    return f\'<a href=""{url}"">{link_text}</a>\'\n\n  @staticmethod\n  def _link_text_to_html(link_text):\n    code_re = \'`(.*?)`\'\n    return re.sub(code_re, r\'<code>\\1</code>\', link_text)\n\n  def py_master_name(self, full_name):\n    """"""Return the master name for a Python symbol name.""""""\n    return self._duplicate_of.get(full_name, full_name)\n\n  def reference_to_url(self, ref_full_name, relative_path_to_root):\n    """"""Resolve a ""`tf.symbol`"" reference to a relative path.\n\n    The input to this function should already be stripped of the \'@\'\n    and \'{}\', and its output is only the link, not the full Markdown.\n\n    If `ref_full_name` is the name of a class member, method, or property, the\n    link will point to the page of the containing class, and it will include the\n    method name as an anchor. For example, `tf.module.MyClass.my_method` will be\n    translated into a link to\n    `os.join.path(relative_path_to_root, \'tf/module/MyClass.md#my_method\')`.\n\n    Args:\n      ref_full_name: The fully qualified name of the symbol to link to.\n      relative_path_to_root: The relative path from the location of the current\n        document to the root of the API documentation.\n\n    Returns:\n      A relative path that links from the documentation page of `from_full_name`\n      to the documentation page of `ref_full_name`.\n\n    Raises:\n      TFDocsError: If the symbol is not found.\n    """"""\n    if self._is_fragment.get(ref_full_name, False):\n      # methods and constants get duplicated. And that\'s okay.\n      # Use the master name of their parent.\n      parent_name, short_name = ref_full_name.rsplit(\'.\', 1)\n      parent_master_name = self._duplicate_of.get(parent_name, parent_name)\n      master_name = \'.\'.join([parent_master_name, short_name])\n    else:\n      master_name = self._duplicate_of.get(ref_full_name, ref_full_name)\n\n    # Check whether this link exists\n    if master_name not in self._all_names:\n      raise TFDocsError(f\'Cannot make link to {master_name!r}: Not in index.\')\n\n    ref_path = documentation_path(master_name, self._is_fragment[master_name])\n    return os.path.join(relative_path_to_root, ref_path)\n\n  def _one_ref(self, match, relative_path_to_root, full_name=None):\n    """"""Return a link for a single ""`tf.symbol`"" reference.""""""\n\n    if match.group(1):\n      # Found a \'[]\' group, return it unmodified.\n      return match.group(\'brackets\')\n\n    # Found a \'``\' group.\n    string = match.group(\'backticks\')\n\n    link_text = string\n\n    string = re.sub(r\'(.*)[\\(\\[].*\', r\'\\1\', string)\n\n    if string.startswith(\'compat.v1\') or string.startswith(\'compat.v2\'):\n      string = \'tf.\' + string\n    elif string.startswith(\'v1\') or string.startswith(\'v2\'):\n      string = \'tf.compat.\' + string\n\n    elif full_name is None or (\'tf.compat.v\' not in full_name and\n                               \'tf.contrib\' not in full_name):\n      string = self._partial_symbols_dict.get(string, string)\n\n    try:\n      if string.startswith(\'tensorflow::\'):\n        # C++ symbol\n        return self._cc_link(string, link_text, relative_path_to_root)\n\n      is_python = False\n      for py_module_name in self._py_module_names:\n        if string == py_module_name or string.startswith(py_module_name + \'.\'):\n          is_python = True\n          break\n\n      if is_python:  # Python symbol\n        return self.python_link(\n            link_text, string, relative_path_to_root, code_ref=True)\n    except TFDocsError:\n      pass\n\n    return match.group(0)\n\n  def _cc_link(self, string, link_text, relative_path_to_root):\n    """"""Generate a link for a `tensorflow::...` reference.""""""\n    # TODO(joshl): Fix this hard-coding of paths.\n    if string == \'tensorflow::ClientSession\':\n      ret = \'class/tensorflow/client-session.md\'\n    elif string == \'tensorflow::Scope\':\n      ret = \'class/tensorflow/scope.md\'\n    elif string == \'tensorflow::Status\':\n      ret = \'class/tensorflow/status.md\'\n    elif string == \'tensorflow::Tensor\':\n      ret = \'class/tensorflow/tensor.md\'\n    elif string == \'tensorflow::ops::Const\':\n      ret = \'namespace/tensorflow/ops.md#const\'\n    else:\n      raise TFDocsError(f\'C++ reference not understood: ""{string}""\')\n\n    # relative_path_to_root gets you to api_docs/python, we go from there\n    # to api_docs/cc, and then add ret.\n    cc_relative_path = os.path.normpath(\n        os.path.join(relative_path_to_root, \'../cc\', ret))\n\n    return f\'<a href=""{cc_relative_path}""><code>{link_text}</code></a>\'\n\n\ndef _handle_compatibility(doc) -> Tuple[str, Dict[str, str]]:\n  """"""Parse and remove compatibility blocks from the main docstring.\n\n  Args:\n    doc: The docstring that contains compatibility notes.\n\n  Returns:\n    A tuple of the modified doc string and a hash that maps from compatibility\n    note type to the text of the note.\n  """"""\n  compatibility_notes = {}\n  match_compatibility = re.compile(r\'[ \\t]*@compatibility\\((\\w+)\\)\\s*\\n\'\n                                   r\'((?:[^@\\n]*\\n)+)\'\n                                   r\'\\s*@end_compatibility\')\n  for f in match_compatibility.finditer(doc):\n    compatibility_notes[f.group(1)] = f.group(2)\n  return match_compatibility.subn(r\'\', doc)[0], compatibility_notes\n\n\ndef _pairs(items):\n  """"""Given an list of items [a,b,a,b...], generate pairs [(a,b),(a,b)...].\n\n  Args:\n    items: A list of items (length must be even)\n\n  Returns:\n    A list of pairs.\n  """"""\n  assert len(items) % 2 == 0\n  return list(zip(items[::2], items[1::2]))\n\n\nclass TitleBlock(object):\n  """"""A class to parse title blocks (like `Args:`) and convert them to markdown.\n\n  This handles the ""Args/Returns/Raises"" blocks and anything similar.\n\n  These are used to extract metadata (argument descriptions, etc), and upgrade\n  This `TitleBlock` to markdown.\n\n  These blocks are delimited by indentation. There must be a blank line before\n  the first `TitleBlock` in a series.\n\n  The expected format is:\n\n  ```\n  Title:\n    Freeform text\n    arg1: value1\n    arg2: value1\n  ```\n\n  These are represented as:\n\n  ```\n  TitleBlock(\n    title = ""Arguments"",\n    text = ""Freeform text"",\n    items=[(\'arg1\', \'value1\'), (\'arg2\', \'value2\')])\n  ```\n\n  The ""text"" and ""items"" fields may be empty. When both are empty the generated\n  markdown only serves to upgrade the title to a <h4>.\n\n  Attributes:\n    title: The title line, without the colon.\n    text: Freeform text. Anything between the `title`, and the `items`.\n    items: A list of (name, value) string pairs. All items must have the same\n      indentation.\n  """"""\n\n  _INDENTATION_REMOVAL_RE = re.compile(r\'( *)(.+)\')\n\n  # Don\'t change the width=""214px"" without consulting with the devsite-team.\n  _TABLE_TEMPLATE = textwrap.dedent(""""""\n    <!-- Tabular view -->\n     <table class=""responsive fixed orange"">\n    <colgroup><col width=""214px""><col></colgroup>\n    <tr><th colspan=""2"">{title}</th></tr>\n    {text}\n    {items}\n    </table>\n    """""")\n\n  _ITEMS_TEMPLATE = textwrap.dedent(""""""\\\n    <tr>\n    <td>\n    {name}\n    </td>\n    <td>\n    {description}\n    </td>\n    </tr>"""""")\n\n  _TEXT_TEMPLATE = textwrap.dedent(""""""\\\n    <tr class=""alt"">\n    <td colspan=""2"">\n    {text}\n    </td>\n    </tr>"""""")\n\n  def __init__(self,\n               *,\n               title: Optional[str] = None,\n               text: str,\n               items: Iterable[Tuple[str, str]]):\n    self.title = title\n    self.text = text\n    self.items = items\n\n  def table_view(self, title_template: Optional[str] = None) -> str:\n    """"""Returns a tabular markdown version of the TitleBlock.\n\n    Tabular view is only for `Args`, `Returns`, `Raises` and `Attributes`. If\n    anything else is encountered, redirect to list view.\n\n    Args:\n      title_template: Template for title detailing how to display it.\n\n    Returns:\n      Table containing the content to display.\n    """"""\n\n    if title_template is not None:\n      title = title_template.format(title=self.title)\n    else:\n      title = self.title\n\n    text = self.text.strip()\n    if text:\n      text = self._TEXT_TEMPLATE.format(text=text)\n      text = self._INDENTATION_REMOVAL_RE.sub(r\'\\2\', text)\n\n    items = []\n    for name, description in self.items:\n      if not description:\n        description = \'\'\n      else:\n        description = description.strip()\n      item_table = self._ITEMS_TEMPLATE.format(\n          name=f\'`{name}`\', description=description)\n      item_table = self._INDENTATION_REMOVAL_RE.sub(r\'\\2\', item_table)\n      items.append(item_table)\n\n    return \'\\n\' + self._TABLE_TEMPLATE.format(\n        title=title, text=text, items=\'\'.join(items)) + \'\\n\'\n\n  def list_view(self, title_template: str) -> str:\n    """"""Returns a List markdown version of the TitleBlock.\n\n    Args:\n      title_template: Template for title detailing how to display it.\n\n    Returns:\n      Markdown list containing the content to display.\n    """"""\n\n    sub = []\n    sub.append(title_template.format(title=self.title))\n    sub.append(textwrap.dedent(self.text))\n    sub.append(\'\\n\')\n\n    for name, description in self.items:\n      description = description.strip()\n      if not description:\n        sub.append(f\'* <b>`{name}`</b>\\n\')\n      else:\n        sub.append(f\'* <b>`{name}`</b>: {description}\\n\')\n\n    return \'\'.join(sub)\n\n  # This regex matches an entire title-block.\n  BLOCK_RE = re.compile(\n      r""""""\n      (?:^|^\\n|\\n\\n)                  # After a blank line (non-capturing):\n        (?P<title>[A-Z][\\s\\w]{0,20})  # Find a sentence case title, followed by\n          \\s*:\\s*?(?=\\n)              # whitespace, a colon and a new line.\n      (?P<content>.*?)                # Then take everything until\n        (?=\\n\\S|$)                    # look ahead finds a non-indented line\n                                      # (a new-line followed by non-whitespace)\n    """""", re.VERBOSE | re.DOTALL)\n\n  # This\n  ITEM_RE = re.compile(\n      r""""""\n      ^(\\*?\\*?          # Capture optional *s to allow *args, **kwargs.\n          \\w[\\w.]*?     # Capture a word character followed by word characters\n                        # or "".""s.\n      )\\s*:\\s           # Allow any whitespace around the colon."""""",\n      re.MULTILINE | re.VERBOSE)\n\n  @classmethod\n  def split_string(cls, docstring: str):\n    r""""""Given a docstring split it into a list of `str` or `TitleBlock` chunks.\n\n    For example the docstring of `tf.nn.relu`:\n\n    \'\'\'\n    Computes `max(features, 0)`.\n\n    Args:\n      features: A `Tensor`. Must be one of the following types: `float32`,\n        `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `half`.\n      name: A name for the operation (optional).\n\n    More freeform markdown text.\n    \'\'\'\n\n    This is parsed, and returned as:\n\n    ```\n    [\n        ""Computes rectified linear: `max(features, 0)`."",\n        TitleBlock(\n          title=\'Args\',\n          text=\'\',\n          items=[\n            (\'features\', \' A `Tensor`. Must be...\'),\n            (\'name\', \' A name for the operation (optional).\\n\\n\')]\n        ),\n        ""More freeform markdown text.""\n    ]\n    ```\n    Args:\n      docstring: The docstring to parse\n\n    Returns:\n      The docstring split into chunks. Each chunk produces valid markdown when\n      `str` is called on it (each chunk is a python `str`, or a `TitleBlock`).\n    """"""\n    parts = []\n    while docstring:\n      split = re.split(cls.BLOCK_RE, docstring, maxsplit=1)\n      # The first chunk in split is all text before the TitleBlock.\n      before = split.pop(0)\n      parts.append(before)\n\n      # If `split` is empty, there were no other matches, and we\'re done.\n      if not split:\n        break\n\n      # If there was a match,  split contains three items. The two capturing\n      # groups in the RE, and the remainder.\n      title, content, docstring = split\n\n      # Now `content` contains the text and the name-value item pairs.\n      # separate these two parts.\n      content = textwrap.dedent(content)\n      split = cls.ITEM_RE.split(content)\n      text = split.pop(0)\n      items = _pairs(split)\n\n      title_block = cls(title=title, text=text, items=items)\n      parts.append(title_block)\n\n    return parts\n\n\nclass _DocstringInfo(typing.NamedTuple):\n  brief: str\n  docstring_parts: List[Union[TitleBlock, str]]\n  compatibility: Dict[str, str]\n\n\ndef _parse_md_docstring(py_object, relative_path_to_root, full_name,\n                        reference_resolver) -> _DocstringInfo:\n  """"""Parse the object\'s docstring and return a `_DocstringInfo`.\n\n  This function clears @@\'s from the docstring, and replaces `` references\n  with markdown links.\n\n  For links within the same set of docs, the `relative_path_to_root` for a\n  docstring on the page for `full_name` can be set to:\n\n  ```python\n  relative_path_to_root = os.path.relpath(\n    path=\'.\', start=os.path.dirname(documentation_path(full_name)) or \'.\')\n  ```\n\n  Args:\n    py_object: A python object to retrieve the docs for (class, function/method,\n      or module).\n    relative_path_to_root: The relative path from the location of the current\n      document to the root of the Python API documentation. This is used to\n      compute links for ""`tf.symbol`"" references.\n    full_name: (optional) The api path to the current object, so replacements\n      can depend on context.\n    reference_resolver: An instance of ReferenceResolver.\n\n  Returns:\n    A _DocstringInfo object, all fields will be empty if no docstring was found.\n  """"""\n  # TODO(wicke): If this is a partial, use the .func docstring and add a note.\n  raw_docstring = _get_raw_docstring(py_object)\n\n  raw_docstring = reference_resolver.replace_references(raw_docstring,\n                                                        relative_path_to_root,\n                                                        full_name)\n\n  atat_re = re.compile(r\' *@@[a-zA-Z_.0-9]+ *$\')\n  raw_docstring = \'\\n\'.join(\n      line for line in raw_docstring.split(\'\\n\') if not atat_re.match(line))\n\n  docstring, compatibility = _handle_compatibility(raw_docstring)\n\n  if \'Generated by: tensorflow/tools/api/generator\' in docstring:\n    docstring = \'\'\n\n  # Remove the first-line ""brief"" docstring.\n  lines = docstring.split(\'\\n\')\n  brief = lines.pop(0)\n\n  docstring = \'\\n\'.join(lines)\n\n  docstring_parts = TitleBlock.split_string(docstring)\n\n  return _DocstringInfo(brief, docstring_parts, compatibility)\n\n\nclass TypeAnnotationExtractor(ast.NodeVisitor):\n  """"""Extracts the type annotations by parsing the AST of a function.""""""\n\n  def __init__(self):\n    self.annotation_dict = {}\n    self.arguments_typehint_exists = False\n    self.return_typehint_exists = False\n\n  def visit_FunctionDef(self, node) -> None:  # pylint: disable=invalid-name\n    """"""Visits the `FunctionDef` node in AST tree and extracts the typehints.""""""\n\n    # Capture the return type annotation.\n    if node.returns:\n      self.annotation_dict[\'return\'] = astor.to_source(\n          node.returns).strip().replace(\'""""""\', \'""\')\n      self.return_typehint_exists = True\n\n    # Capture the args type annotation.\n    for arg in node.args.args:\n      if arg.annotation:\n        self.annotation_dict[arg.arg] = astor.to_source(\n            arg.annotation).strip().replace(\'""""""\', \'""\')\n        self.arguments_typehint_exists = True\n\n    # Capture the kwarg only args type annotation.\n    for kwarg in node.args.kwonlyargs:\n      if kwarg.annotation:\n        self.annotation_dict[kwarg.arg] = astor.to_source(\n            kwarg.annotation).strip().replace(\'""""""\', \'""\')\n        self.arguments_typehint_exists = True\n\n\nclass ASTDefaultValueExtractor(ast.NodeVisitor):\n  """"""Extracts the default values by parsing the AST of a function.""""""\n\n  _PAREN_NUMBER_RE = re.compile(r\'^\\(([0-9.e-]+)\\)\')\n\n  def __init__(self):\n    self.ast_args_defaults = []\n    self.ast_kw_only_defaults = []\n\n  def _preprocess(self, val: str) -> str:\n    text_default_val = astor.to_source(val).strip().replace(\n        \'\\t\', \'\\\\t\').replace(\'\\n\', \'\\\\n\').replace(\'""""""\', ""\'"")\n    text_default_val = self._PAREN_NUMBER_RE.sub(\'\\\\1\', text_default_val)\n    return text_default_val\n\n  def visit_FunctionDef(self, node) -> None:  # pylint: disable=invalid-name\n    """"""Visits the `FunctionDef` node and extracts the default values.""""""\n\n    for default_val in node.args.defaults:\n      if default_val is not None:\n        text_default_val = self._preprocess(default_val)\n        self.ast_args_defaults.append(text_default_val)\n\n    for default_val in node.args.kw_defaults:\n      if default_val is not None:\n        text_default_val = self._preprocess(default_val)\n        self.ast_kw_only_defaults.append(text_default_val)\n\n\nclass FormatArguments(object):\n  """"""Formats the arguments and adds type annotations if they exist.""""""\n\n  _INTERNAL_NAMES = {\n      \'ops.GraphKeys\': \'tf.GraphKeys\',\n      \'_ops.GraphKeys\': \'tf.GraphKeys\',\n      \'init_ops.zeros_initializer\': \'tf.zeros_initializer\',\n      \'init_ops.ones_initializer\': \'tf.ones_initializer\',\n      \'saver_pb2.SaverDef\': \'tf.train.SaverDef\',\n  }\n\n  _OBJECT_MEMORY_ADDRESS_RE = re.compile(r\'<(?P<type>.+) object at 0x[\\da-f]+>\')\n\n  # A regular expression capturing a python identifier.\n  _IDENTIFIER_RE = r\'[a-zA-Z_]\\w*\'\n\n  _INDIVIDUAL_TYPES_RE = re.compile(\n      r""""""\n        (?P<single_type>\n          ([\\w.]*)\n          (?=$|,| |\\]|\\[)\n        )\n      """""", re.IGNORECASE | re.VERBOSE)\n\n  _TYPING = frozenset(\n      list(typing.__dict__.keys()) +\n      [\'int\', \'str\', \'bytes\', \'float\', \'complex\', \'bool\', \'None\'])\n\n  _IMMUTABLE_TYPES = frozenset(\n      [int, str, bytes, float, complex, bool,\n       type(None), tuple, frozenset])\n\n  def __init__(\n      self,\n      type_annotations: Dict[str, str],\n      parser_config: ParserConfig,\n      func_full_name: str,\n  ) -> None:\n    self._type_annotations = type_annotations\n    self._reverse_index = parser_config.reverse_index\n    self._reference_resolver = parser_config.reference_resolver\n    # func_full_name is used to calculate the relative path.\n    self._func_full_name = func_full_name\n\n    self._is_fragment = self._reference_resolver._is_fragment.get(\n        self._func_full_name, None)\n\n  def _calc_relative_path(self, single_type: str) -> str:\n    """"""Calculates the relative path of the type from the function.\n\n    The number of `..` are counted from `os.path.relpath` and adjusted based\n    on if the function (for which signature is being generated) is a fragment\n    or not.\n\n    Args:\n      single_type: The type for which the relative path is calculated.\n\n    Returns:\n      Relative path consisting of only `..` as a path.\n    """"""\n\n    func_full_path = self._func_full_name.replace(\'.\', \'/\')\n    single_type = single_type.replace(\'.\', \'/\')\n\n    dot_count = os.path.relpath(single_type, func_full_path).count(\'..\')\n    # Methods are fragments, stand-alone functions are not.\n    if self._is_fragment:\n      dot_count -= 2\n    else:\n      dot_count -= 1\n\n    dot_list = [\'..\'] * dot_count\n    return os.path.join(*dot_list)  # pylint: disable=no-value-for-parameter\n\n  def _get_link(self, full_name: str, ast_typehint: str) -> str:\n    full_name = self._reference_resolver._duplicate_of.get(full_name, full_name)  # pylint: disable=protected-access\n    relative_path = self._calc_relative_path(ast_typehint)\n    url = os.path.join(relative_path, full_name.replace(\'.\', \'/\')) + \'.md\'\n    # Use `full_name` for the text in the link since its available over\n    # `ast_typehint`.\n    return f\'<a href=""{url}""><code>{full_name}</code></a>\'\n\n  def _extract_non_builtin_types(self, arg_obj: Any,\n                                 non_builtin_types: List[Any]) -> List[Any]:\n    """"""Extracts the non-builtin types from a type annotations object.\n\n    Recurses if an object contains `__args__` attribute. If an object is\n    an inbuilt object or an `Ellipsis` then its skipped.\n\n    Args:\n      arg_obj: Type annotation object.\n      non_builtin_types: List to keep track of the non-builtin types extracted.\n\n    Returns:\n      List of non-builtin types.\n    """"""\n\n    annotations = getattr(arg_obj, \'__args__\', [arg_obj])\n    if annotations is None:\n      annotations = [arg_obj]\n\n    for anno in annotations:\n      if self._reverse_index.get(id(anno), None):\n        non_builtin_types.append(anno)\n      elif (anno in self._IMMUTABLE_TYPES or anno in typing.__dict__.values() or\n            anno is Ellipsis):\n        continue\n      elif hasattr(anno, \'__args__\'):\n        self._extract_non_builtin_types(anno, non_builtin_types)\n      else:\n        non_builtin_types.append(anno)\n    return non_builtin_types\n\n  def _get_non_builtin_ast_types(self, ast_typehint: str) -> List[str]:\n    """"""Extracts non-builtin types from a string AST type annotation.\n\n    If the type is an inbuilt type or an `...`(Ellipsis) then its skipped.\n\n    Args:\n      ast_typehint: AST extracted type annotation.\n\n    Returns:\n      List of non-builtin ast types.\n    """"""\n\n    non_builtin_ast_types = []\n    for single_type, _ in self._INDIVIDUAL_TYPES_RE.findall(ast_typehint):\n      if (not single_type or single_type in self._TYPING or\n          single_type == \'...\'):\n        continue\n      non_builtin_ast_types.append(single_type)\n    return non_builtin_ast_types\n\n  def _linkify(self, non_builtin_map: Dict[str, Any], match) -> str:\n    """"""Links off to types that can be linked.\n\n    Args:\n      non_builtin_map: Dictionary mapping non-builtin_ast_types to\n        non_builtin_type_objs\n      match: Match object returned by `re.sub`.\n\n    Returns:\n      Linked type annotation if the type annotation object exists.\n    """"""\n\n    group = match.groupdict()\n    ast_single_typehint = group[\'single_type\']\n\n    # If the AST type hint is a built-in type hint or an `Ellipsis`,\n    # return it as is.\n    if ast_single_typehint not in non_builtin_map:\n      return ast_single_typehint\n\n    if not non_builtin_map:\n      return ast_single_typehint\n\n    # Get the type object from the ast_single_typehint and lookup the object\n    # in reverse_index to get its full name.\n    obj_full_name = self._reverse_index.get(\n        id(non_builtin_map[ast_single_typehint]), None)\n    if obj_full_name is None:\n      return ast_single_typehint\n\n    return self._get_link(obj_full_name, ast_single_typehint)\n\n  def _preprocess(self, ast_typehint: str, obj_anno: Any) -> str:\n    """"""Links type annotations to its page if it exists.\n\n    Args:\n      ast_typehint: AST extracted type annotation.\n      obj_anno: Type annotation object.\n\n    Returns:\n      Linked type annotation if the type annotation object exists.\n    """"""\n    # If the object annotations exists in the reverse_index, get the link\n    # directly for the entire annotation.\n    obj_anno_full_name = self._reverse_index.get(id(obj_anno), None)\n    if obj_anno_full_name is not None:\n      return self._get_link(obj_anno_full_name, ast_typehint)\n\n    non_builtin_ast_types = self._get_non_builtin_ast_types(ast_typehint)\n    try:\n      non_builtin_type_objs = self._extract_non_builtin_types(obj_anno, [])\n    except RecursionError:\n      non_builtin_type_objs = {}\n\n    # If the length doesn\'t match then don\'t linkify any type annotation. This\n    # is done to avoid linking to wrong pages instead of guessing.\n    if len(non_builtin_type_objs) != len(non_builtin_ast_types):\n      non_builtin_map = {}\n    else:\n      non_builtin_map = dict(zip(non_builtin_ast_types, non_builtin_type_objs))\n\n    partial_func = functools.partial(self._linkify, non_builtin_map)\n    return self._INDIVIDUAL_TYPES_RE.sub(partial_func, ast_typehint)\n\n  def _replace_internal_names(self, default_text: str) -> str:\n    full_name_re = f\'^{self._IDENTIFIER_RE}(.{self._IDENTIFIER_RE})+\'\n    match = re.match(full_name_re, default_text)\n    if match:\n      for internal_name, public_name in self._INTERNAL_NAMES.items():\n        if match.group(0).startswith(internal_name):\n          return public_name + default_text[len(internal_name):]\n    return default_text\n\n  def format_return(self, return_anno: Any) -> str:\n    return self._preprocess(self._type_annotations[\'return\'], return_anno)\n\n  def format_args(self, args: List[inspect.Parameter]) -> List[str]:\n    """"""Creates a text representation of the args in a method/function.\n\n    Args:\n      args: List of args to format.\n\n    Returns:\n      Formatted args with type annotations if they exist.\n    """"""\n\n    args_text_repr = []\n\n    for arg in args:\n      arg_name = arg.name\n      if arg_name in self._type_annotations:\n        typeanno = self._preprocess(self._type_annotations[arg_name],\n                                    arg.annotation)\n        args_text_repr.append(f\'{arg_name}: {typeanno}\')\n      else:\n        args_text_repr.append(f\'{arg_name}\')\n\n    return args_text_repr\n\n  def format_kwargs(self, kwargs: List[inspect.Parameter],\n                    ast_defaults: List[str]) -> List[str]:\n    """"""Creates a text representation of the kwargs in a method/function.\n\n    Args:\n      kwargs: List of kwargs to format.\n      ast_defaults: Default values extracted from the function\'s AST tree.\n\n    Returns:\n      Formatted kwargs with type annotations if they exist.\n    """"""\n\n    kwargs_text_repr = []\n\n    if len(ast_defaults) < len(kwargs):\n      ast_defaults.extend([None] * (len(kwargs) - len(ast_defaults)))\n\n    for kwarg, ast_default in zip(kwargs, ast_defaults):\n      kname = kwarg.name\n      default_val = kwarg.default\n\n      if id(default_val) in self._reverse_index:\n        default_text = self._reverse_index[id(default_val)]\n      elif ast_default is not None:\n        default_text = ast_default\n        if default_text != repr(default_val):\n          default_text = self._replace_internal_names(default_text)\n      # Kwarg without default value.\n      elif default_val is kwarg.empty:\n        kwargs_text_repr.extend(self.format_args([kwarg]))\n        continue\n      else:\n        # Strip object memory addresses to avoid unnecessary doc churn.\n        default_text = self._OBJECT_MEMORY_ADDRESS_RE.sub(\n            r\'<\\g<type>>\', repr(default_val))\n\n      # Format the kwargs to add the type annotation and default values.\n      if kname in self._type_annotations:\n        typeanno = self._preprocess(self._type_annotations[kname],\n                                    kwarg.annotation)\n        kwargs_text_repr.append(f\'{kname}: {typeanno} = {default_text}\')\n      else:\n        kwargs_text_repr.append(f\'{kname}={default_text}\')\n\n    return kwargs_text_repr\n\n\nclass _SignatureComponents(NamedTuple):\n  """"""Contains the components that make up the signature of a function/method.""""""\n\n  arguments: List[str]\n  arguments_typehint_exists: bool\n  return_typehint_exists: bool\n  return_type: Optional[str] = None\n\n  def __str__(self):\n    arguments_signature = \'\'\n    if self.arguments:\n      str_signature = \',\\n\'.join(self.arguments)\n      # If there is no type annotation on arguments, then wrap the entire\n      # signature to width 80.\n      if not self.arguments_typehint_exists:\n        str_signature = textwrap.fill(str_signature, width=80)\n      arguments_signature = \'\\n\' + textwrap.indent(\n          str_signature, prefix=\'    \') + \'\\n\'\n\n    full_signature = f\'({arguments_signature})\'\n    if self.return_typehint_exists:\n      full_signature += f\' -> {self.return_type}\'\n\n    return full_signature\n\n\ndef generate_signature(func: Any, parser_config: ParserConfig,\n                       func_full_name: str) -> _SignatureComponents:\n  """"""Given a function, returns a list of strings representing its args.\n\n  This function uses `__name__` for callables if it is available. This can lead\n  to poor results for functools.partial and other callable objects.\n\n  The returned string is Python code, so if it is included in a Markdown\n  document, it should be typeset as code (using backticks), or escaped.\n\n  Args:\n    func: A function, method, or functools.partial to extract the signature for.\n    parser_config: `ParserConfig` for the method/function whose signature is\n      generated.\n    func_full_name: The full name of a function whose signature is generated.\n\n  Returns:\n    A `_SignatureComponents` NamedTuple.\n  """"""\n\n  all_args_list = []\n\n  try:\n    sig = inspect.signature(func)\n    sig_values = sig.parameters.values()\n    return_anno = sig.return_annotation\n  except (ValueError, TypeError):\n    sig_values = []\n    return_anno = None\n\n  type_annotation_visitor = TypeAnnotationExtractor()\n  ast_defaults_visitor = ASTDefaultValueExtractor()\n\n  try:\n    func_source = textwrap.dedent(inspect.getsource(func))\n    func_ast = ast.parse(func_source)\n    # Extract the type annotation from the parsed ast.\n    type_annotation_visitor.visit(func_ast)\n    ast_defaults_visitor.visit(func_ast)\n  except Exception:  # pylint: disable=broad-except\n    # A wide-variety of errors can be thrown here.\n    pass\n\n  type_annotations = type_annotation_visitor.annotation_dict\n  arguments_typehint_exists = type_annotation_visitor.arguments_typehint_exists\n  return_typehint_exists = type_annotation_visitor.return_typehint_exists\n\n  #############################################################################\n  # Process the information about the func.\n  #############################################################################\n\n  pos_only_args = []\n  args = []\n  kwargs = []\n  only_kwargs = []\n  varargs = None\n  varkwargs = None\n  skip_self_cls = True\n\n  for index, param in enumerate(sig_values):\n    kind = param.kind\n    default = param.default\n\n    if skip_self_cls and (param.name == \'self\' or param.name == \'cls\'):\n      # Only skip the first parameter. If the function contains both\n      # `self` and `cls`, skip only the first one.\n      skip_self_cls = False\n    elif kind == param.POSITIONAL_ONLY:\n      pos_only_args.append(param)\n    elif default is param.empty and kind == param.POSITIONAL_OR_KEYWORD:\n      args.append(param)\n    elif default is not param.empty and kind == param.POSITIONAL_OR_KEYWORD:\n      kwargs.append(param)\n    elif kind == param.VAR_POSITIONAL:\n      varargs = (index, param)\n    elif kind == param.KEYWORD_ONLY:\n      only_kwargs.append(param)\n    elif kind == param.VAR_KEYWORD:\n      varkwargs = param\n\n  #############################################################################\n  # Build the text representation of Args and Kwargs.\n  #############################################################################\n\n  formatter = FormatArguments(\n      type_annotations, parser_config, func_full_name=func_full_name)\n\n  if pos_only_args:\n    all_args_list.extend(formatter.format_args(pos_only_args))\n    all_args_list.append(\'/\')\n\n  if args:\n    all_args_list.extend(formatter.format_args(args))\n\n  if kwargs:\n    all_args_list.extend(\n        formatter.format_kwargs(kwargs, ast_defaults_visitor.ast_args_defaults))\n\n  if only_kwargs:\n    if varargs is None:\n      all_args_list.append(\'*\')\n    all_args_list.extend(\n        formatter.format_kwargs(only_kwargs,\n                                ast_defaults_visitor.ast_kw_only_defaults))\n\n  if varargs is not None:\n    all_args_list.insert(varargs[0], \'*\' + varargs[1].name)\n\n  if varkwargs is not None:\n    all_args_list.append(\'**\' + varkwargs.name)\n\n  if return_anno and return_anno is not sig.empty and type_annotations.get(\n      \'return\', None):\n    return_type = formatter.format_return(return_anno)\n  else:\n    return_type = \'None\'\n\n  return _SignatureComponents(\n      arguments=all_args_list,\n      arguments_typehint_exists=arguments_typehint_exists,\n      return_typehint_exists=return_typehint_exists,\n      return_type=return_type)\n\n\ndef _get_defining_class(py_class, name):\n  for cls in inspect.getmro(py_class):\n    if name in cls.__dict__:\n      return cls\n  return None\n\n\nclass MemberInfo(NamedTuple):\n  """"""Describes an attribute of a class or module.""""""\n  short_name: str\n  full_name: str\n  obj: Any\n  doc: _DocstringInfo\n  url: str\n\n\nclass MethodInfo(NamedTuple):\n  """"""Described a method.""""""\n  short_name: str\n  full_name: str\n  obj: Any\n  doc: _DocstringInfo\n  url: str\n  signature: _SignatureComponents\n  decorators: List[str]\n  defined_in: Optional[_FileLocation]\n\n  @classmethod\n  def from_member_info(cls, method_info: MemberInfo,\n                       signature: _SignatureComponents, decorators: List[str],\n                       defined_in: Optional[_FileLocation]):\n    """"""Upgrades a `MemberInfo` to a `MethodInfo`.""""""\n    return cls(\n        **method_info._asdict(),\n        signature=signature,\n        decorators=decorators,\n        defined_in=defined_in)\n\n\ndef extract_decorators(func: Any) -> List[str]:\n  """"""Extracts the decorators on top of functions/methods.\n\n  Args:\n    func: The function to extract the decorators from.\n\n  Returns:\n    A List of decorators.\n  """"""\n\n  class ASTDecoratorExtractor(ast.NodeVisitor):\n\n    def __init__(self):\n      self.decorator_list = []\n\n    def visit_FunctionDef(self, node):  # pylint: disable=invalid-name\n      for dec in node.decorator_list:\n        self.decorator_list.append(astor.to_source(dec).strip())\n\n  visitor = ASTDecoratorExtractor()\n\n  try:\n    func_source = textwrap.dedent(inspect.getsource(func))\n    func_ast = ast.parse(func_source)\n    visitor.visit(func_ast)\n  except Exception:  # pylint: disable=broad-except\n    # A wide-variety of errors can be thrown here.\n    pass\n\n  return visitor.decorator_list\n\n\nclass PageInfo(object):\n  """"""Base-class for api_pages objects.\n\n  Converted to markdown by pretty_docs.py.\n\n  Attributes:\n    full_name: The full, master name, of the object being documented.\n    short_name: The last part of the full name.\n    py_object: The object being documented.\n    defined_in: A _FileLocation describing where the object was defined.\n    aliases: A list of full-name for all aliases for this object.\n    doc: A list of objects representing the docstring. These can all be\n      converted to markdown using str().\n  """"""\n\n  def __init__(self, full_name, py_object):\n    """"""Initialize a PageInfo.\n\n    Args:\n      full_name: The full, master name, of the object being documented.\n      py_object: The object being documented.\n    """"""\n    self.full_name = full_name\n    self.py_object = py_object\n\n    self._defined_in = None\n    self._aliases = None\n    self._doc = None\n\n  @property\n  def short_name(self):\n    """"""Returns the documented object\'s short name.""""""\n    return self.full_name.split(\'.\')[-1]\n\n  @property\n  def defined_in(self):\n    """"""Returns the path to the file where the documented object is defined.""""""\n    return self._defined_in\n\n  def set_defined_in(self, defined_in):\n    """"""Sets the `defined_in` path.""""""\n    assert self.defined_in is None\n    self._defined_in = defined_in\n\n  @property\n  def aliases(self):\n    """"""Returns a list of all full names for the documented object.""""""\n    return self._aliases\n\n  def set_aliases(self, aliases):\n    """"""Sets the `aliases` list.\n\n    Args:\n      aliases: A list of strings. Containing all the object\'s full names.\n    """"""\n    assert self.aliases is None\n    self._aliases = aliases\n\n  @property\n  def doc(self):\n    """"""Returns a `_DocstringInfo` created from the object\'s docstring.""""""\n    return self._doc\n\n  def set_doc(self, doc):\n    """"""Sets the `doc` field.\n\n    Args:\n      doc: An instance of `_DocstringInfo`.\n    """"""\n    assert self.doc is None\n    self._doc = doc\n\n\nclass FunctionPageInfo(PageInfo):\n  """"""Collects docs For a function Page.\n\n  Attributes:\n    full_name: The full, master name, of the object being documented.\n    short_name: The last part of the full name.\n    py_object: The object being documented.\n    defined_in: A _FileLocation describing where the object was defined.\n    aliases: A list of full-name for all aliases for this object.\n    doc: A list of objects representing the docstring. These can all be\n      converted to markdown using str().\n    signature: the parsed signature (see: generate_signature)\n    decorators: A list of decorator names.\n  """"""\n\n  def __init__(self, full_name, py_object):\n    """"""Initialize a FunctionPageInfo.\n\n    Args:\n      full_name: The full, master name, of the object being documented.\n      py_object: The object being documented.\n    """"""\n    super(FunctionPageInfo, self).__init__(full_name, py_object)\n\n    self._signature = None\n    self._decorators = []\n\n  @property\n  def signature(self):\n    return self._signature\n\n  def collect_docs(self, parser_config):\n    """"""Collect all information necessary to genertate the function page.\n\n    Mainly this is details about the function signature.\n\n    Args:\n      parser_config: The ParserConfig for the module being documented.\n    """"""\n\n    assert self.signature is None\n    self._signature = generate_signature(self.py_object, parser_config,\n                                         self.full_name)\n    self._decorators = extract_decorators(self.py_object)\n\n  @property\n  def decorators(self):\n    return list(self._decorators)\n\n  def add_decorator(self, dec):\n    self._decorators.append(dec)\n\n  def get_metadata_html(self):\n    return Metadata(self.full_name).build_html()\n\n\nclass TypeAliasPageInfo(PageInfo):\n  """"""Collects docs For a type alias page.\n\n  Attributes:\n    full_name: The full, master name, of the object being documented.\n    short_name: The last part of the full name.\n    py_object: The object being documented.\n    defined_in: A _FileLocation describing where the object was defined.\n    aliases: A list of full-name for all aliases for this object.\n    doc: A list of objects representing the docstring. These can all be\n      converted to markdown using str().\n    signature: the parsed signature (see: generate_signature)\n    decorators: A list of decorator names.\n  """"""\n\n  def __init__(self, full_name: str, py_object: Any) -> None:\n    """"""Initialize a `TypeAliasPageInfo`.\n\n    Args:\n      full_name: The full, master name, of the object being documented.\n      py_object: The object being documented.\n    """"""\n\n    super().__init__(full_name, py_object)\n    self._signature = None\n\n  @property\n  def signature(self) -> None:\n    return self._signature\n\n  def set_doc(self, doc: _DocstringInfo) -> None:\n    """"""Overrides base class\'s method and sets the `doc` field.""""""\n    self._doc = _DocstringInfo(\n        brief=\'This symbol is a Type Alias.\',\n        docstring_parts=[],\n        compatibility={})\n\n  def collect_docs(self, parser_config) -> None:\n    """"""Collect all information necessary to genertate the function page.\n\n    Mainly this is details about the function signature.\n\n    Args:\n      parser_config: The ParserConfig for the module being documented.\n    """"""\n    del parser_config\n\n    assert self.signature is None\n    wrapped_sig = textwrap.fill(\n        repr(self.py_object).replace(\'typing.\', \'\'), width=80)\n    self._signature = textwrap.indent(wrapped_sig, \'    \').strip()\n\n  def get_metadata_html(self) -> str:\n    return Metadata(self.full_name).build_html()\n\n\nclass ClassPageInfo(PageInfo):\n  """"""Collects docs for a class page.\n\n  Attributes:\n    full_name: The full, master name, of the object being documented.\n    short_name: The last part of the full name.\n    py_object: The object being documented.\n    defined_in: A _FileLocation describing where the object was defined.\n    aliases: A list of full-name for all aliases for this object.\n    doc: A list of objects representing the docstring. These can all be\n      converted to markdown using str().\n    attributes: A dict mapping from ""name"" to a docstring\n    bases: A list of `MemberInfo` objects pointing to the docs for the parent\n      classes.\n    methods: A list of `MethodInfo` objects documenting the class\' methods.\n    classes: A list of `MemberInfo` objects pointing to docs for any nested\n      classes.\n    other_members: A list of `MemberInfo` objects documenting any other object\'s\n      defined inside the class object (mostly enum style fields).\n    attr_block: A `TitleBlock` containing information about the Attributes of\n      the class.\n  """"""\n\n  def __init__(self, full_name, py_object):\n    """"""Initialize a ClassPageInfo.\n\n    Args:\n      full_name: The full, master name, of the object being documented.\n      py_object: The object being documented.\n    """"""\n    super(ClassPageInfo, self).__init__(full_name, py_object)\n\n    self._namedtuplefields = collections.OrderedDict()\n    if issubclass(py_object, tuple):\n      namedtuple_attrs = (\'_asdict\', \'_fields\', \'_make\', \'_replace\')\n      if all(hasattr(py_object, attr) for attr in namedtuple_attrs):\n        for name in py_object._fields:\n          self._namedtuplefields[name] = None\n\n    self._properties = collections.OrderedDict()\n    self._bases = None\n    self._methods = []\n    self._classes = []\n    self._other_members = []\n    self.attr_block = None\n\n  @property\n  def bases(self):\n    """"""Returns a list of `MemberInfo` objects pointing to the class\' parents.""""""\n    return self._bases\n\n  def set_attr_block(self, attr_block):\n    assert self.attr_block is None\n    self.attr_block = attr_block\n\n  def _set_bases(self, relative_path, parser_config):\n    """"""Builds the `bases` attribute, to document this class\' parent-classes.\n\n    This method sets the `bases` to a list of `MemberInfo` objects point to the\n    doc pages for the class\' parents.\n\n    Args:\n      relative_path: The relative path from the doc this object describes to the\n        documentation root.\n      parser_config: An instance of `ParserConfig`.\n    """"""\n    bases = []\n    obj = parser_config.py_name_to_object(self.full_name)\n    for base in obj.__bases__:\n      base_full_name = parser_config.reverse_index.get(id(base), None)\n      if base_full_name is None:\n        continue\n      base_doc = _parse_md_docstring(base, relative_path, self.full_name,\n                                     parser_config.reference_resolver)\n      base_url = parser_config.reference_resolver.reference_to_url(\n          base_full_name, relative_path)\n\n      link_info = MemberInfo(\n          short_name=base_full_name.split(\'.\')[-1],\n          full_name=base_full_name,\n          obj=base,\n          doc=base_doc,\n          url=base_url)\n      bases.append(link_info)\n\n    self._bases = bases\n\n  def _add_property(self, member_info: MemberInfo):\n    """"""Adds an entry to the `properties` list.\n\n    Args:\n      member_info: a `MemberInfo` describing the property.\n    """"""\n    doc = member_info.doc\n    # Hide useless namedtuple docs-trings.\n    if re.match(\'Alias for field number [0-9]+\', doc.brief):\n      doc = doc._replace(docstring_parts=[], brief=\'\')\n\n    new_parts = [doc.brief]\n    # Strip args/returns/raises from property\n    new_parts.extend([\n        str(part)\n        for part in doc.docstring_parts\n        if not isinstance(part, TitleBlock)\n    ])\n    new_parts = [textwrap.indent(part, \'  \') for part in new_parts]\n    new_parts.append(\'\')\n    desc = \'\\n\'.join(new_parts)\n\n    if member_info.short_name in self._namedtuplefields:\n      self._namedtuplefields[member_info.short_name] = desc\n    else:\n      self._properties[member_info.short_name] = desc\n\n  @property\n  def methods(self):\n    """"""Returns a list of `MethodInfo` describing the class\' methods.""""""\n    return self._methods\n\n  def _add_method(\n      self,\n      member_info: MemberInfo,\n      defining_class: Optional[type],  # pylint: disable=g-bare-generic\n      parser_config: ParserConfig) -> None:\n    """"""Adds a `MethodInfo` entry to the `methods` list.\n\n    Args:\n      member_info: a `MemberInfo` describing the method.\n      defining_class: The `type` object where this method is defined.\n      parser_config: A `ParserConfig`.\n    """"""\n    if defining_class is None:\n      return\n\n    # Omit methods defined by namedtuple.\n    original_method = defining_class.__dict__[member_info.short_name]\n    if (hasattr(original_method, \'__module__\') and\n        (original_method.__module__ or \'\').startswith(\'namedtuple\')):\n      return\n\n    # Some methods are often overridden without documentation. Because it\'s\n    # obvious what they do, don\'t include them in the docs if there\'s no\n    # docstring.\n    if (not member_info.doc.brief.strip() and\n        member_info.short_name in [\'__del__\', \'__copy__\']):\n      return\n\n    signature = generate_signature(member_info.obj, parser_config,\n                                   member_info.full_name)\n\n    decorators = extract_decorators(member_info.obj)\n\n    defined_in = _get_defined_in(member_info.obj, parser_config)\n\n    method_info = MethodInfo.from_member_info(member_info, signature,\n                                              decorators, defined_in)\n    self._methods.append(method_info)\n\n  @property\n  def classes(self):\n    """"""Returns a list of `MemberInfo` pointing to any nested classes.""""""\n    return self._classes\n\n  def get_metadata_html(self) -> str:\n    meta_data = Metadata(self.full_name)\n    for item in itertools.chain(self.classes, self.methods, self.other_members):\n      meta_data.append(item)\n\n    return meta_data.build_html()\n\n  def _add_class(self, member_info):\n    """"""Adds a `MemberInfo` for a nested class to `classes` list.\n\n    Args:\n      member_info: a `MemberInfo` describing the class.\n    """"""\n    self._classes.append(member_info)\n\n  @property\n  def other_members(self):\n    """"""Returns a list of `MemberInfo` describing any other contents.""""""\n    return self._other_members\n\n  def _add_other_member(self, member_info: MemberInfo):\n    """"""Adds an `MemberInfo` entry to the `other_members` list.\n\n    Args:\n      member_info: a `MemberInfo` describing the object.\n    """"""\n    self._other_members.append(member_info)\n\n  def _add_member(\n      self,\n      member_info: MemberInfo,\n      defining_class: Optional[type],  # pylint: disable=g-bare-generic\n      parser_config: ParserConfig,\n  ) -> None:\n    """"""Adds a member to the class page.""""""\n    obj_type = get_obj_type(member_info.obj)\n\n    if obj_type is ObjType.PROPERTY:\n      self._add_property(member_info)\n    elif obj_type is ObjType.CLASS:\n      if defining_class is None:\n        return\n      self._add_class(member_info)\n    elif obj_type is ObjType.CALLABLE:\n      self._add_method(member_info, defining_class, parser_config)\n    else:\n      # Exclude members defined by protobuf that are useless\n      if issubclass(self.py_object, ProtoMessage):\n        if (member_info.short_name.endswith(\'_FIELD_NUMBER\') or\n            member_info.short_name in [\'__slots__\', \'DESCRIPTOR\']):\n          return\n\n      self._add_other_member(member_info)\n\n  def collect_docs(self, parser_config):\n    """"""Collects information necessary specifically for a class\'s doc page.\n\n    Mainly, this is details about the class\'s members.\n\n    Args:\n      parser_config: An instance of ParserConfig.\n    """"""\n    py_class = self.py_object\n    doc_path = documentation_path(self.full_name)\n    relative_path = os.path.relpath(\n        path=\'.\', start=os.path.dirname(doc_path) or \'.\')\n\n    self._set_bases(relative_path, parser_config)\n\n    for child_short_name in parser_config.tree[self.full_name]:\n      child_full_name = \'.\'.join([self.full_name, child_short_name])\n      child = parser_config.py_name_to_object(child_full_name)\n\n      # Don\'t document anything that is defined in object or by protobuf.\n      defining_class = _get_defining_class(py_class, child_short_name)\n      if defining_class in [object, type, tuple, BaseException, Exception]:\n        continue\n\n      # The following condition excludes most protobuf-defined symbols.\n      if (defining_class and\n          defining_class.__name__ in [\'CMessage\', \'Message\', \'MessageMeta\']):\n        continue\n\n      if doc_controls.should_skip_class_attr(py_class, child_short_name):\n        continue\n\n      child_doc = _parse_md_docstring(child, relative_path, self.full_name,\n                                      parser_config.reference_resolver)\n\n      child_url = parser_config.reference_resolver.reference_to_url(\n          child_full_name, relative_path)\n\n      member_info = MemberInfo(child_short_name, child_full_name, child,\n                               child_doc, child_url)\n      self._add_member(member_info, defining_class, parser_config)\n\n    self.set_attr_block(self._augment_attributes(self.doc.docstring_parts))\n\n  def _augment_attributes(self,\n                          docstring_parts: List[Any]) -> Optional[TitleBlock]:\n    """"""Augments and deletes the ""Attr"" block of the docstring.\n\n    The augmented block is returned and then added to the markdown page by\n    pretty_docs.py. The existing Attribute block is deleted from the docstring.\n\n    Merges `namedtuple` fields and properties into the attrs block.\n\n    + `namedtuple` fields first, in order.\n    + Then the docstring `Attr:` block.\n    + Then any `properties` not mentioned above.\n\n    Args:\n      docstring_parts: A list of docstring parts.\n\n    Returns:\n      Augmented ""Attr"" block.\n    """"""\n\n    attribute_block = None\n\n    for attr_block_index, part in enumerate(docstring_parts):\n      if isinstance(part, TitleBlock) and part.title.startswith(\'Attr\'):\n        raw_attrs = collections.OrderedDict(part.items)\n        break\n    else:\n      # Didn\'t find the attributes block, there may still be attributes so\n      # add a placeholder for them at the end.\n      raw_attrs = collections.OrderedDict()\n      attr_block_index = len(docstring_parts)\n      docstring_parts.append(None)\n\n    attrs = collections.OrderedDict()\n    # namedtuple fields first.\n    attrs.update(self._namedtuplefields)\n    # the contents of the `Attrs:` block from the docstring\n    attrs.update(raw_attrs)\n    # properties last.\n    for name, desc in self._properties.items():\n      # Don\'t overwrite existing items\n      attrs.setdefault(name, desc)\n\n    if attrs:\n      attribute_block = TitleBlock(\n          title=\'Attributes\', text=\'\', items=attrs.items())\n\n    # Delete the Attrs block if it exists or delete the placeholder.\n    del docstring_parts[attr_block_index]\n\n    return attribute_block\n\n\nclass ModulePageInfo(PageInfo):\n  """"""Collects docs for a module page.\n\n  Attributes:\n    full_name: The full, master name, of the object being documented.\n    short_name: The last part of the full name.\n    py_object: The object being documented.\n    defined_in: A _FileLocation describing where the object was defined.\n    aliases: A list of full-name for all aliases for this object.\n    doc: A list of objects representing the docstring. These can all be\n      converted to markdown using str().\n    classes: A list of `MemberInfo` objects pointing to docs for the classes in\n      this module.\n    functions: A list of `MemberInfo` objects pointing to docs for the functions\n      in this module\n    modules: A list of `MemberInfo` objects pointing to docs for the modules in\n      this module.\n    type_alias: A list of `MemberInfo` objects pointing to docs for the type\n      aliases in this module.\n    other_members: A list of `MemberInfo` objects documenting any other object\'s\n      defined on the module object (mostly enum style fields).\n  """"""\n\n  def __init__(self, full_name, py_object):\n    """"""Initialize a `ModulePageInfo`.\n\n    Args:\n      full_name: The full, master name, of the object being documented.\n      py_object: The object being documented.\n    """"""\n    super(ModulePageInfo, self).__init__(full_name, py_object)\n\n    self._modules = []\n    self._classes = []\n    self._functions = []\n    self._other_members = []\n    self._type_alias = []\n\n  @property\n  def modules(self):\n    return self._modules\n\n  @property\n  def functions(self):\n    return self._functions\n\n  @property\n  def classes(self):\n    return self._classes\n\n  @property\n  def type_alias(self):\n    return self._type_alias\n\n  @property\n  def other_members(self):\n    return self._other_members\n\n  def _add_module(self, member_info: MemberInfo):\n    self._modules.append(member_info)\n\n  def _add_class(self, member_info: MemberInfo):\n    self._classes.append(member_info)\n\n  def _add_function(self, member_info: MemberInfo):\n    self._functions.append(member_info)\n\n  def _add_type_alias(self, member_info: MemberInfo):\n    self._type_alias.append(member_info)\n\n  def _add_other_member(self, member_info: MemberInfo):\n    self._other_members.append(member_info)\n\n  def get_metadata_html(self):\n    meta_data = Metadata(self.full_name)\n\n    # Objects with their own pages are not added to the metadata list for the\n    # module, the module only has a link to the object page. No docs.\n    for item in self.other_members:\n      meta_data.append(item)\n\n    return meta_data.build_html()\n\n  def _add_member(self, member_info: MemberInfo) -> None:\n    """"""Adds members of the modules to the respective lists.""""""\n    obj_type = get_obj_type(member_info.obj)\n    if obj_type is ObjType.MODULE:\n      self._add_module(member_info)\n    elif obj_type is ObjType.CLASS:\n      self._add_class(member_info)\n    elif obj_type is ObjType.CALLABLE:\n      self._add_function(member_info)\n    elif obj_type is ObjType.TYPE_ALIAS:\n      self._add_type_alias(member_info)\n    else:\n      self._add_other_member(member_info)\n\n  def collect_docs(self, parser_config):\n    """"""Collect information necessary specifically for a module\'s doc page.\n\n    Mainly this is information about the members of the module.\n\n    Args:\n      parser_config: An instance of ParserConfig.\n    """"""\n    relative_path = os.path.relpath(\n        path=\'.\',\n        start=os.path.dirname(documentation_path(self.full_name)) or \'.\')\n\n    member_names = parser_config.tree.get(self.full_name, [])\n    for member_short_name in member_names:\n\n      if member_short_name in [\n          \'__builtins__\', \'__doc__\', \'__file__\', \'__name__\', \'__path__\',\n          \'__package__\', \'__cached__\', \'__loader__\', \'__spec__\',\n          \'absolute_import\', \'division\', \'print_function\', \'unicode_literals\'\n      ]:\n        continue\n\n      if self.full_name:\n        member_full_name = self.full_name + \'.\' + member_short_name\n      else:\n        member_full_name = member_short_name\n\n      member = parser_config.py_name_to_object(member_full_name)\n\n      member_doc = _parse_md_docstring(member, relative_path, self.full_name,\n                                       parser_config.reference_resolver)\n\n      url = parser_config.reference_resolver.reference_to_url(\n          member_full_name, relative_path)\n\n      member_info = MemberInfo(member_short_name, member_full_name, member,\n                               member_doc, url)\n      self._add_member(member_info)\n\n\ndef docs_for_object(full_name, py_object, parser_config):\n  """"""Return a PageInfo object describing a given object from the TF API.\n\n  This function uses _parse_md_docstring to parse the docs pertaining to\n  `object`.\n\n  This function resolves \'`tf.symbol`\' references in the docstrings into links\n  to the appropriate location. It also adds a list of alternative names for the\n  symbol automatically.\n\n  It assumes that the docs for each object live in a file given by\n  `documentation_path`, and that relative links to files within the\n  documentation are resolvable.\n\n  Args:\n    full_name: The fully qualified name of the symbol to be documented.\n    py_object: The Python object to be documented. Its documentation is sourced\n      from `py_object`\'s docstring.\n    parser_config: A ParserConfig object.\n\n  Returns:\n    Either a `FunctionPageInfo`, `ClassPageInfo`, or a `ModulePageInfo`\n    depending on the type of the python object being documented.\n\n  Raises:\n    RuntimeError: If an object is encountered for which we don\'t know how\n      to make docs.\n  """"""\n\n  # Which other aliases exist for the object referenced by full_name?\n  master_name = parser_config.reference_resolver.py_master_name(full_name)\n  duplicate_names = parser_config.duplicates.get(master_name, [])\n  if master_name in duplicate_names:\n    duplicate_names.remove(master_name)\n\n  obj_type = get_obj_type(py_object)\n  if obj_type is ObjType.CLASS:\n    page_info = ClassPageInfo(master_name, py_object)\n  elif obj_type is ObjType.CALLABLE:\n    page_info = FunctionPageInfo(master_name, py_object)\n  elif obj_type is ObjType.MODULE:\n    page_info = ModulePageInfo(master_name, py_object)\n  elif obj_type is ObjType.TYPE_ALIAS:\n    page_info = TypeAliasPageInfo(master_name, py_object)\n  else:\n    raise RuntimeError(\'Cannot make docs for object {full_name}: {py_object!r}\')\n\n  relative_path = os.path.relpath(\n      path=\'.\', start=os.path.dirname(documentation_path(full_name)) or \'.\')\n\n  page_info.set_doc(\n      _parse_md_docstring(py_object, relative_path, full_name,\n                          parser_config.reference_resolver))\n\n  page_info.collect_docs(parser_config)\n\n  page_info.set_aliases(duplicate_names)\n\n  page_info.set_defined_in(_get_defined_in(py_object, parser_config))\n\n  return page_info\n\n\ndef _unwrap_obj(obj):\n  while True:\n    unwrapped_obj = getattr(obj, \'__wrapped__\', None)\n    if unwrapped_obj is None:\n      break\n    obj = unwrapped_obj\n  return obj\n\n\ndef _get_defined_in(py_object: Any,\n                    parser_config: ParserConfig) -> Optional[_FileLocation]:\n  """"""Returns a description of where the passed in python object was defined.\n\n  Args:\n    py_object: The Python object.\n    parser_config: A ParserConfig object.\n\n  Returns:\n    A `_FileLocation`\n  """"""\n  # Every page gets a note about where this object is defined\n  base_dirs_and_prefixes = zip(parser_config.base_dir,\n                               parser_config.code_url_prefix)\n  try:\n    obj_path = inspect.getfile(_unwrap_obj(py_object))\n  except TypeError:  # getfile throws TypeError if py_object is a builtin.\n    return None\n\n  code_url_prefix = None\n  for base_dir, temp_prefix in base_dirs_and_prefixes:\n\n    rel_path = os.path.relpath(path=obj_path, start=base_dir)\n    # A leading "".."" indicates that the file is not inside `base_dir`, and\n    # the search should continue.\n    if rel_path.startswith(\'..\'):\n      continue\n    else:\n      code_url_prefix = temp_prefix\n      break\n\n  # No link if the file was not found in a `base_dir`, or the prefix is None.\n  if code_url_prefix is None:\n    return None\n\n  try:\n    lines, start_line = inspect.getsourcelines(py_object)\n    end_line = start_line + len(lines) - 1\n  except (IOError, TypeError, IndexError):\n    start_line = None\n    end_line = None\n\n  # TODO(wicke): If this is a generated file, link to the source instead.\n  # TODO(wicke): Move all generated files to a generated/ directory.\n  # TODO(wicke): And make their source file predictable from the file name.\n\n  # In case this is compiled, point to the original\n  if rel_path.endswith(\'.pyc\'):\n    # If a PY3 __pycache__/ subdir is being used, omit it.\n    rel_path = rel_path.replace(\'__pycache__\' + os.sep, \'\')\n    # Strip everything after the first . so that variants such as .pyc and\n    # .cpython-3x.pyc or similar are all handled.\n    rel_path = rel_path.partition(\'.\')[0] + \'.py\'\n\n  # Never include links outside this code base.\n  if re.search(r\'\\b_api\\b\', rel_path):\n    return None\n  if re.search(r\'\\bapi/(_v2|_v1)\\b\', rel_path):\n    return None\n  if re.search(r\'<[\\w\\s]+>\', rel_path):\n    # Built-ins emit paths like <embedded stdlib>, <string>, etc.\n    return None\n  if \'<attrs generated\' in rel_path:\n    return None\n\n  if re.match(r\'.*/gen_[^/]*\\.py$\', rel_path):\n    return _FileLocation(rel_path)\n  if \'genfiles\' in rel_path:\n    return _FileLocation(rel_path)\n  elif re.match(r\'.*_pb2\\.py$\', rel_path):\n    # The _pb2.py files all appear right next to their defining .proto file.\n\n    rel_path = rel_path[:-7] + \'.proto\'\n    return _FileLocation(\n        rel_path=rel_path, url=os.path.join(code_url_prefix, rel_path))  # pylint: disable=undefined-loop-variable\n  else:\n    return _FileLocation(\n        rel_path=rel_path,\n        url=os.path.join(code_url_prefix, rel_path),\n        start_line=start_line,\n        end_line=end_line)  # pylint: disable=undefined-loop-variable\n\n\n# TODO(markdaoust): This should just parse, pretty_docs should generate the md.\ndef generate_global_index(library_name, index, reference_resolver):\n  """"""Given a dict of full names to python objects, generate an index page.\n\n  The index page generated contains a list of links for all symbols in `index`\n  that have their own documentation page.\n\n  Args:\n    library_name: The name for the documented library to use in the title.\n    index: A dict mapping full names to python objects.\n    reference_resolver: An instance of ReferenceResolver.\n\n  Returns:\n    A string containing an index page as Markdown.\n  """"""\n  symbol_links = []\n  for full_name, py_object in index.items():\n    obj_type = get_obj_type(py_object)\n    if obj_type in (ObjType.OTHER, ObjType.PROPERTY):\n      continue\n    # In Python 3, unbound methods are functions, so eliminate those.\n    if obj_type is ObjType.CALLABLE:\n      if is_class_attr(full_name, index):\n        continue\n    symbol_links.append(\n        (full_name, reference_resolver.python_link(full_name, full_name, \'.\')))\n\n  lines = [f\'# All symbols in {library_name}\', \'\']\n\n  # Sort all the symbols once, so that the ordering is preserved when its broken\n  # up into master symbols and compat symbols and sorting the sublists is not\n  # required.\n  symbol_links = sorted(symbol_links, key=lambda x: x[0])\n\n  compat_v1_symbol_links = []\n  compat_v2_symbol_links = []\n  primary_symbol_links = []\n\n  for symbol, link in symbol_links:\n    if symbol.startswith(\'tf.compat.v1\'):\n      if \'raw_ops\' not in symbol:\n        compat_v1_symbol_links.append(link)\n    elif symbol.startswith(\'tf.compat.v2\'):\n      compat_v2_symbol_links.append(link)\n    else:\n      primary_symbol_links.append(link)\n\n  lines.append(\'## Primary symbols\')\n  for link in primary_symbol_links:\n    lines.append(f\'*  {link}\')\n\n  if compat_v2_symbol_links:\n    lines.append(\'\\n## Compat v2 symbols\\n\')\n    for link in compat_v2_symbol_links:\n      lines.append(f\'*  {link}\')\n\n  if compat_v1_symbol_links:\n    lines.append(\'\\n## Compat v1 symbols\\n\')\n    for link in compat_v1_symbol_links:\n      lines.append(f\'*  {link}\')\n\n  # TODO(markdaoust): use a _ModulePageInfo -> prety_docs.build_md_page()\n  return \'\\n\'.join(lines)\n\n\nclass Metadata(object):\n  """"""A class for building a page\'s Metadata block.\n\n  Attributes:\n    name: The name of the page being described by the Metadata block.\n    version: The source version.\n  """"""\n\n  def __init__(self, name, version=None, content=None):\n    """"""Creates a Metadata builder.\n\n    Args:\n      name: The name of the page being described by the Metadata block.\n      version: The source version.\n      content: Content to create the metadata from.\n    """"""\n\n    self.name = name\n\n    self.version = version\n    if self.version is None:\n      self.version = \'Stable\'\n\n    self._content = content\n    if self._content is None:\n      self._content = []\n\n  def append(self, item):\n    """"""Adds an item from the page to the Metadata block.\n\n    Args:\n      item: The parsed page section to add.\n    """"""\n    self._content.append(item.short_name)\n\n  def build_html(self):\n    """"""Returns the Metadata block as an Html string.""""""\n    # Note: A schema is not a URL. It is defined with http: but doesn\'t resolve.\n    schema = \'http://developers.google.com/ReferenceObject\'\n    parts = [f\'<div itemscope itemtype=""{schema}"">\']\n\n    parts.append(f\'<meta itemprop=""name"" content=""{self.name}"" />\')\n    parts.append(f\'<meta itemprop=""path"" content=""{self.version}"" />\')\n    for item in self._content:\n      parts.append(f\'<meta itemprop=""property"" content=""{item}""/>\')\n\n    parts.extend([\'</div>\', \'\'])\n\n    return \'\\n\'.join(parts)\n'"
tools/tensorflow_docs/api_generator/parser_test.py,59,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for documentation parser.""""""\n\nimport collections\nimport inspect\nimport os\nimport tempfile\nimport textwrap\n\nfrom typing import Union, List\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport attr\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import parser\n\n# The test needs a real module. `types.ModuleType()` doesn\'t work, as the result\n# is a `builtin` module. Using ""parser"" here is arbitraty. The tests don\'t\n# depend on the module contents. At this point in the process the public api\n# has already been extracted.\ntest_module = parser\n\n\ndef test_function(unused_arg, unused_kwarg=\'default\'):\n  """"""Docstring for test function.""""""\n  pass\n\n\ndef test_function_with_args_kwargs(unused_arg, *unused_args, **unused_kwargs):\n  """"""Docstring for second test function.""""""\n  pass\n\n\nclass ParentClass(object):\n\n  @doc_controls.do_not_doc_inheritable\n  def hidden_method(self):\n    pass\n\n\nclass TestClass(ParentClass):\n  """"""Docstring for TestClass itself.\n\n  Attributes:\n    hello: hello\n  """"""\n\n  def a_method(self, arg=\'default\'):\n    """"""Docstring for a method.""""""\n    pass\n\n  def hidden_method(self):\n    pass\n\n  @doc_controls.do_not_generate_docs\n  def hidden_method2(self):\n    pass\n\n  class ChildClass(object):\n    """"""Docstring for a child class.""""""\n    pass\n\n  @property\n  def a_property(self):\n    """"""Docstring for a property.""""""\n    pass\n\n  @staticmethod\n  def static_method(arg):\n    pass\n\n  @classmethod\n  def class_method(cls):\n    pass\n\n  CLASS_MEMBER = \'a class member\'\n\n\nclass DummyVisitor(object):\n\n  def __init__(self, index, duplicate_of):\n    self.index = index\n    self.duplicate_of = duplicate_of\n\n\nclass ConcreteMutableMapping(collections.MutableMapping):\n  """"""MutableMapping subclass to repro inspect.getsource() IndexError.""""""\n\n  def __init__(self):\n    self._map = {}\n\n  def __getitem__(self, key):\n    return self._map[key]\n\n  def __setitem__(self, key, value):\n    self._map[key] = value\n\n  def __delitem__(self, key):\n    del self._map[key]\n\n  def __iter__(self):\n    return self._map.__iter__()\n\n  def __len__(self):\n    return len(self._map)\n\n\nConcreteNamedTuple = collections.namedtuple(\'ConcreteNamedTuple\', [\'a\', \'b\'])\n\n\n@attr.s\nclass ClassUsingAttrs(object):\n  member = attr.ib(type=int)\n\n\nclass ParserTest(parameterized.TestCase):\n\n  def test_documentation_path(self):\n    self.assertEqual(\'test.md\', parser.documentation_path(\'test\'))\n    self.assertEqual(\'test/module.md\', parser.documentation_path(\'test.module\'))\n\n  def test_replace_references(self):\n\n    class HasOneMember(object):\n\n      def foo(self):\n        pass\n\n    string = (\n        \'A `tf.reference`, a member `tf.reference.foo`, and a `tf.third`. \'\n        \'This is `not a symbol`, and this is `tf.not.a.real.symbol`\')\n\n    duplicate_of = {\'tf.third\': \'tf.fourth\'}\n    index = {\n        \'tf.reference\': HasOneMember,\n        \'tf.reference.foo\': HasOneMember.foo,\n        \'tf.third\': HasOneMember,\n        \'tf.fourth\': HasOneMember\n    }\n\n    visitor = DummyVisitor(index, duplicate_of)\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    result = reference_resolver.replace_references(string, \'../..\')\n    self.assertEqual(\n        \'A <a href=""../../tf/reference.md"">\'\n        \'<code>tf.reference</code></a>, \'\n        \'a member <a href=""../../tf/reference.md#foo"">\'\n        \'<code>tf.reference.foo</code></a>, \'\n        \'and a <a href=""../../tf/fourth.md"">\'\n        \'<code>tf.third</code></a>. \'\n        \'This is `not a symbol`, and this is \'\n        \'`tf.not.a.real.symbol`\', result)\n\n  def test_docs_for_class(self):\n\n    index = {\n        \'TestClass\': TestClass,\n        \'TestClass.a_method\': TestClass.a_method,\n        \'TestClass.a_property\': TestClass.a_property,\n        \'TestClass.ChildClass\': TestClass.ChildClass,\n        \'TestClass.static_method\': TestClass.static_method,\n        \'TestClass.class_method\': TestClass.class_method,\n        \'TestClass.CLASS_MEMBER\': TestClass.CLASS_MEMBER,\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\n        \'TestClass\': [\n            \'a_method\', \'class_method\', \'static_method\', \'a_property\',\n            \'ChildClass\', \'CLASS_MEMBER\'\n        ]\n    }\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'TestClass\', py_object=TestClass, parser_config=parser_config)\n\n    # Make sure the brief docstring is present\n    self.assertEqual(\n        inspect.getdoc(TestClass).split(\'\\n\')[0], page_info.doc.brief)\n\n    # Make sure the method is present\n    method_infos = {\n        method_info.short_name: method_info for method_info in page_info.methods\n    }\n\n    self.assertIs(method_infos[\'a_method\'].obj, TestClass.a_method)\n\n    # Make sure that the signature is extracted properly and omits self.\n    self.assertEqual([""arg=\'default\'""],\n                     method_infos[\'a_method\'].signature.arguments)\n\n    self.assertEqual(method_infos[\'static_method\'].decorators, [\'staticmethod\'])\n    self.assertEqual(method_infos[\'class_method\'].decorators, [\'classmethod\'])\n\n    # Make sure the property is present\n    attrs = page_info.attr_block\n    self.assertIsInstance(attrs, parser.TitleBlock)\n    self.assertIn(\'a_property\', [name for name, desc in attrs.items])\n\n    # Make sure there is a link to the child class and it points the right way.\n    self.assertIs(TestClass.ChildClass, page_info.classes[0].obj)\n\n    # Make sure this file is contained as the definition location.\n    self.assertEqual(\n        os.path.relpath(__file__, \'/\'), page_info.defined_in.rel_path)\n\n  def test_namedtuple_field_order(self):\n    namedtupleclass = collections.namedtuple(\'namedtupleclass\',\n                                             {\'z\', \'y\', \'x\', \'w\', \'v\', \'u\'})\n\n    index = {\n        \'namedtupleclass\': namedtupleclass,\n        \'namedtupleclass.u\': namedtupleclass.u,\n        \'namedtupleclass.v\': namedtupleclass.v,\n        \'namedtupleclass.w\': namedtupleclass.w,\n        \'namedtupleclass.x\': namedtupleclass.x,\n        \'namedtupleclass.y\': namedtupleclass.y,\n        \'namedtupleclass.z\': namedtupleclass.z,\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\'namedtupleclass\': {\'u\', \'v\', \'w\', \'x\', \'y\', \'z\'}}\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'namedtupleclass\',\n        py_object=namedtupleclass,\n        parser_config=parser_config)\n\n    # Each namedtiple field has a docstring of the form:\n    #   \'Alias for field number ##\'. These props are returned sorted.\n\n    def sort_key(prop_info):\n      return int(prop_info.obj.__doc__.split(\' \')[-1])\n\n    self.assertSequenceEqual(page_info._properties,\n                             sorted(page_info._properties, key=sort_key))\n\n  def test_docs_for_class_should_skip(self):\n\n    class Parent(object):\n\n      @doc_controls.do_not_doc_inheritable\n      def a_method(self, arg=\'default\'):\n        pass\n\n    class Child(Parent):\n\n      def a_method(self, arg=\'default\'):\n        pass\n\n    index = {\n        \'Child\': Child,\n        \'Child.a_method\': Child.a_method,\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\n        \'Child\': [\'a_method\'],\n    }\n\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'Child\', py_object=Child, parser_config=parser_config)\n\n    # Make sure the `a_method` is not present\n    self.assertEmpty(page_info.methods)\n\n  def test_docs_for_message_class(self):\n\n    class CMessage(object):\n\n      def hidden(self):\n        pass\n\n    class Message(object):\n\n      def hidden2(self):\n        pass\n\n    class MessageMeta(object):\n\n      def hidden3(self):\n        pass\n\n    class ChildMessage(CMessage, Message, MessageMeta):\n\n      def my_method(self):\n        pass\n\n    index = {\n        \'ChildMessage\': ChildMessage,\n        \'ChildMessage.hidden\': ChildMessage.hidden,\n        \'ChildMessage.hidden2\': ChildMessage.hidden2,\n        \'ChildMessage.hidden3\': ChildMessage.hidden3,\n        \'ChildMessage.my_method\': ChildMessage.my_method,\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\'ChildMessage\': [\'hidden\', \'hidden2\', \'hidden3\', \'my_method\']}\n\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'ChildMessage\',\n        py_object=ChildMessage,\n        parser_config=parser_config)\n\n    self.assertLen(page_info.methods, 1)\n    self.assertEqual(\'my_method\', page_info.methods[0].short_name)\n\n  def test_docs_for_module(self):\n\n    index = {\n        \'TestModule\':\n            test_module,\n        \'TestModule.test_function\':\n            test_function,\n        \'TestModule.test_function_with_args_kwargs\':\n            test_function_with_args_kwargs,\n        \'TestModule.TestClass\':\n            TestClass,\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\n        \'TestModule\': [\n            \'TestClass\', \'test_function\', \'test_function_with_args_kwargs\'\n        ]\n    }\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'TestModule\',\n        py_object=test_module,\n        parser_config=parser_config)\n\n    # Make sure the brief docstring is present\n    self.assertEqual(\n        inspect.getdoc(test_module).split(\'\\n\')[0], page_info.doc.brief)\n\n    # Make sure that the members are there\n    funcs = {f_info.obj for f_info in page_info.functions}\n    self.assertEqual({test_function, test_function_with_args_kwargs}, funcs)\n\n    classes = {cls_info.obj for cls_info in page_info.classes}\n    self.assertEqual({TestClass}, classes)\n\n    # Make sure the module\'s file is contained as the definition location.\n    self.assertEqual(\n        os.path.relpath(test_module.__file__.rstrip(\'c\'), \'/\'),\n        page_info.defined_in.rel_path)\n\n  def test_docs_for_function(self):\n    index = {\'test_function\': test_function}\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\'\': [\'test_function\']}\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'test_function\',\n        py_object=test_function,\n        parser_config=parser_config)\n\n    # Make sure the brief docstring is present\n    self.assertEqual(\n        inspect.getdoc(test_function).split(\'\\n\')[0], page_info.doc.brief)\n\n    # Make sure the extracted signature is good.\n    self.assertEqual([\'unused_arg\', ""unused_kwarg=\'default\'""],\n                     page_info.signature.arguments)\n\n    # Make sure this file is contained as the definition location.\n    self.assertEqual(\n        os.path.relpath(__file__, \'/\'), page_info.defined_in.rel_path)\n\n  def test_docs_for_function_with_kwargs(self):\n    index = {\'test_function_with_args_kwargs\': test_function_with_args_kwargs}\n\n    visitor = DummyVisitor(index=index, duplicate_of={})\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\'\': [\'test_function_with_args_kwargs\']}\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'test_function_with_args_kwargs\',\n        py_object=test_function_with_args_kwargs,\n        parser_config=parser_config)\n\n    # Make sure the brief docstring is present\n    self.assertEqual(\n        inspect.getdoc(test_function_with_args_kwargs).split(\'\\n\')[0],\n        page_info.doc.brief)\n\n    # Make sure the extracted signature is good.\n    self.assertEqual([\'unused_arg\', \'*unused_args\', \'**unused_kwargs\'],\n                     page_info.signature.arguments)\n\n  def test_parse_md_docstring(self):\n\n    def test_function_with_fancy_docstring(arg):\n      """"""Function with a fancy docstring.\n\n      And a bunch of references: `tf.reference`, another `tf.reference`,\n          a member `tf.reference.foo`, and a `tf.third`.\n\n      Args:\n        arg: An argument.\n\n      Raises:\n        an exception\n\n      Returns:\n        arg: the input, and\n        arg: the input, again.\n\n      @compatibility(numpy)\n      NumPy has nothing as awesome as this function.\n      @end_compatibility\n\n      @compatibility(theano)\n      Theano has nothing as awesome as this function.\n\n      Check it out.\n      @end_compatibility\n\n      """"""\n      return arg, arg\n\n    class HasOneMember(object):\n\n      def foo(self):\n        pass\n\n    duplicate_of = {\'tf.third\': \'tf.fourth\'}\n    index = {\n        \'tf\': test_module,\n        \'tf.fancy\': test_function_with_fancy_docstring,\n        \'tf.reference\': HasOneMember,\n        \'tf.reference.foo\': HasOneMember.foo,\n        \'tf.third\': HasOneMember,\n        \'tf.fourth\': HasOneMember\n    }\n\n    visitor = DummyVisitor(index=index, duplicate_of=duplicate_of)\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    doc_info = parser._parse_md_docstring(\n        test_function_with_fancy_docstring,\n        relative_path_to_root=\'../..\',\n        full_name=None,\n        reference_resolver=reference_resolver)\n\n    freeform_docstring = \'\\n\'.join(\n        part for part in doc_info.docstring_parts if isinstance(part, str))\n    self.assertNotIn(\'@\', freeform_docstring)\n    self.assertNotIn(\'compatibility\', freeform_docstring)\n    self.assertNotIn(\'Raises:\', freeform_docstring)\n\n    title_blocks = [\n        part for part in doc_info.docstring_parts if not isinstance(part, str)\n    ]\n\n    self.assertLen(title_blocks, 3)\n\n    self.assertCountEqual(doc_info.compatibility.keys(), {\'numpy\', \'theano\'})\n\n    self.assertEqual(doc_info.compatibility[\'numpy\'],\n                     \'NumPy has nothing as awesome as this function.\\n\')\n\n  def test_generate_index(self):\n\n    index = {\n        \'tf\': test_module,\n        \'tf.TestModule\': test_module,\n        \'tf.test_function\': test_function,\n        \'tf.TestModule.test_function\': test_function,\n        \'tf.TestModule.TestClass\': TestClass,\n        \'tf.TestModule.TestClass.a_method\': TestClass.a_method,\n        \'tf.TestModule.TestClass.a_property\': TestClass.a_property,\n        \'tf.TestModule.TestClass.ChildClass\': TestClass.ChildClass,\n    }\n    duplicate_of = {\'tf.TestModule.test_function\': \'tf.test_function\'}\n\n    visitor = DummyVisitor(index=index, duplicate_of=duplicate_of)\n\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    docs = parser.generate_global_index(\n        \'TestLibrary\', index=index, reference_resolver=reference_resolver)\n\n    # Make sure duplicates and non-top-level symbols are in the index, but\n    # methods and properties are not.\n    self.assertNotIn(\'a_method\', docs)\n    self.assertNotIn(\'a_property\', docs)\n    self.assertIn(\'TestModule.TestClass\', docs)\n    self.assertIn(\'TestModule.TestClass.ChildClass\', docs)\n    self.assertIn(\'TestModule.test_function\', docs)\n    # Leading backtick to make sure it\'s included top-level.\n    # This depends on formatting, but should be stable.\n    self.assertIn(\'<code>tf.test_function\', docs)\n\n  def test_getsource_indexerror_resilience(self):\n    """"""Validates that parser gracefully handles IndexErrors.\n\n    inspect.getsource() can raise an IndexError in some cases. It\'s unclear\n    why this happens, but it consistently repros on the `get` method of\n    collections.MutableMapping subclasses.\n    """"""\n\n    # This isn\'t the full set of APIs from MutableMapping, but sufficient for\n    # testing.\n    index = {\n        \'ConcreteMutableMapping\':\n            ConcreteMutableMapping,\n        \'ConcreteMutableMapping.__init__\':\n            ConcreteMutableMapping.__init__,\n        \'ConcreteMutableMapping.__getitem__\':\n            ConcreteMutableMapping.__getitem__,\n        \'ConcreteMutableMapping.__setitem__\':\n            ConcreteMutableMapping.__setitem__,\n        \'ConcreteMutableMapping.values\':\n            ConcreteMutableMapping.values,\n        \'ConcreteMutableMapping.get\':\n            ConcreteMutableMapping.get\n    }\n    visitor = DummyVisitor(index=index, duplicate_of={})\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\n        \'ConcreteMutableMapping\': [\n            \'__init__\', \'__getitem__\', \'__setitem__\', \'values\', \'get\'\n        ]\n    }\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'ConcreteMutableMapping\',\n        py_object=ConcreteMutableMapping,\n        parser_config=parser_config)\n\n    self.assertIn(ConcreteMutableMapping.get,\n                  [m.obj for m in page_info.methods])\n\n  def test_strips_default_arg_memory_address(self):\n    """"""Validates that parser strips memory addresses out out default argspecs.\n\n     argspec.defaults can contain object memory addresses, which can change\n     between invocations. It\'s desirable to strip these out to reduce churn.\n\n     See: `help(collections.MutableMapping.pop)`\n    """"""\n    index = {\n        \'ConcreteMutableMapping\': ConcreteMutableMapping,\n        \'ConcreteMutableMapping.pop\': ConcreteMutableMapping.pop\n    }\n    visitor = DummyVisitor(index=index, duplicate_of={})\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {\'ConcreteMutableMapping\': [\'pop\']}\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index=index,\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    page_info = parser.docs_for_object(\n        full_name=\'ConcreteMutableMapping\',\n        py_object=ConcreteMutableMapping,\n        parser_config=parser_config)\n\n    pop_default_arg = page_info.methods[0].signature.arguments[1]\n    self.assertNotIn(\'object at 0x\', pop_default_arg)\n    self.assertIn(\'<object>\', pop_default_arg)\n\n  @parameterized.named_parameters(\n      (\'mutable_mapping\', \'ConcreteMutableMapping\', \'__contains__\',\n       ConcreteMutableMapping.__contains__),\n      (\'namedtuple\', \'ConcreteNamedTuple\', \'__new__\',\n       ConcreteNamedTuple.__new__),\n      (\'ClassUsingAttrs_eq\', \'ClassUsingAttrs\', \'__eq__\',\n       ClassUsingAttrs.__eq__),\n      (\'ClassUsingAttrs_init\', \'ClassUsingAttrs\', \'__init__\',\n       ClassUsingAttrs.__init__),\n  )\n  def test_empty_defined_in(self, cls, method, py_object):\n    """"""Validates that the parser omits the defined_in location properly.\n\n    This test covers two cases where the parser should omit the defined_in\n    location:\n      1. built-ins.\n      2. methods automatically generated by python attr library.\n\n    For built-ins, if without special handling, the defined-in URL ends up like:\n      http://prefix/<embedded stdlib>/_collections_abc.py\n\n    For methods automatically generated by python attr library, if without\n    special handling, the defined-in URL ends up like:\n      http://prefix/<attrs generated eq ...>\n\n    Args:\n      cls: The class name to generate docs for.\n      method: The class method name to generate docs for.\n      py_object: The python object for the specified cls.method.\n    """"""\n\n    visitor = DummyVisitor(index={}, duplicate_of={})\n    reference_resolver = parser.ReferenceResolver.from_visitor(\n        visitor=visitor, py_module_names=[\'tf\'])\n\n    tree = {cls: [method]}\n    parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree=tree,\n        index={},\n        reverse_index={},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n    function_info = parser.docs_for_object(\n        full_name=\'%s.%s\' % (cls, method),\n        py_object=py_object,\n        parser_config=parser_config)\n\n    self.assertIsNone(function_info.defined_in)\n\n\nclass TestReferenceResolver(absltest.TestCase):\n  _BASE_DIR = tempfile.mkdtemp()\n\n  def setUp(self):\n    super(TestReferenceResolver, self).setUp()\n    self.workdir = os.path.join(self._BASE_DIR, self.id())\n    os.makedirs(self.workdir)\n\n  def testSaveReferenceResolver(self):\n    duplicate_of = {\'AClass\': [\'AClass2\']}\n    is_fragment = {\n        \'tf\': False,\n        \'tf.VERSION\': True,\n        \'tf.AClass\': False,\n        \'tf.AClass.method\': True,\n        \'tf.AClass2\': False,\n        \'tf.function\': False\n    }\n    py_module_names = [\'tf\', \'tfdbg\']\n\n    resolver = parser.ReferenceResolver(duplicate_of, is_fragment,\n                                        py_module_names)\n\n    outdir = self.workdir\n\n    filepath = os.path.join(outdir, \'resolver.json\')\n\n    resolver.to_json_file(filepath)\n    resolver2 = parser.ReferenceResolver.from_json_file(filepath)\n\n    # There are no __slots__, so all fields are visible in __dict__.\n    self.assertEqual(resolver.__dict__, resolver2.__dict__)\n\n  def testIsClasssAttr(self):\n    result = parser.is_class_attr(\'test_module.test_function\',\n                                  {\'test_module\': test_module})\n    self.assertFalse(result)\n\n    result = parser.is_class_attr(\'TestClass.test_function\',\n                                  {\'TestClass\': TestClass})\n    self.assertTrue(result)\n\n  def test_duplicate_fragment(self):\n    duplicate_of = {\n        \'tf.Class2.method\': \'tf.Class1.method\',\n        \'tf.sub.Class2.method\': \'tf.Class1.method\',\n        \'tf.sub.Class2\': \'tf.Class2\'\n    }\n    is_fragment = {\n        \'tf.Class1.method\': True,\n        \'tf.Class2.method\': True,\n        \'tf.sub.Class2.method\': True,\n        \'tf.Class1\': False,\n        \'tf.Class2\': False,\n        \'tf.sub.Class2\': False\n    }\n    py_module_names = [\'tf\']\n\n    reference_resolver = parser.ReferenceResolver(duplicate_of, is_fragment,\n                                                  py_module_names)\n\n    # Method references point to the method, in the canonical class alias.\n    result = reference_resolver.reference_to_url(\'tf.Class1.method\', \'\')\n    self.assertEqual(\'tf/Class1.md#method\', result)\n    result = reference_resolver.reference_to_url(\'tf.Class2.method\', \'\')\n    self.assertEqual(\'tf/Class2.md#method\', result)\n    result = reference_resolver.reference_to_url(\'tf.sub.Class2.method\', \'\')\n    self.assertEqual(\'tf/Class2.md#method\', result)\n\n    # Class references point to the canonical class alias\n    result = reference_resolver.reference_to_url(\'tf.Class1\', \'\')\n    self.assertEqual(\'tf/Class1.md\', result)\n    result = reference_resolver.reference_to_url(\'tf.Class2\', \'\')\n    self.assertEqual(\'tf/Class2.md\', result)\n    result = reference_resolver.reference_to_url(\'tf.sub.Class2\', \'\')\n    self.assertEqual(\'tf/Class2.md\', result)\n\n\nRELU_DOC = """"""Computes rectified linear: `max(features, 0)`\n\nRELU is an activation\n\nArgs:\n  features: A `Tensor`. Must be one of the following types: `float32`,\n    `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`,\n    `half`.\n  name: A name for the operation (optional)\n    Note: this is a note, not another parameter.\n\nExamples:\n\n  ```\n  a+b=c\n  ```\n\nReturns:\n  Some tensors, with the same type as the input.\n  first: is the something\n  second: is the something else\n""""""\n\n\nclass TestParseDocstring(absltest.TestCase):\n\n  def test_split_title_blocks(self):\n    docstring_parts = parser.TitleBlock.split_string(RELU_DOC)\n\n    self.assertLen(docstring_parts, 7)\n\n    args = docstring_parts[1]\n    self.assertEqual(args.title, \'Args\')\n    self.assertEqual(args.text, \'\\n\')\n    self.assertLen(args.items, 2)\n    self.assertEqual(args.items[0][0], \'features\')\n    self.assertEqual(\n        args.items[0][1],\n        \'A `Tensor`. Must be one of the following types: `float32`,\\n\'\n        \'  `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`,\\n\'\n        \'  `half`.\\n\')\n    self.assertEqual(args.items[1][0], \'name\')\n    self.assertEqual(\n        args.items[1][1], \'A name for the operation (optional)\\n\'\n        \'  Note: this is a note, not another parameter.\\n\')\n\n    returns = [item for item in docstring_parts if not isinstance(item, str)\n              ][-1]\n    self.assertEqual(returns.title, \'Returns\')\n    self.assertEqual(returns.text,\n                     \'\\nSome tensors, with the same type as the input.\\n\')\n    self.assertLen(returns.items, 2)\n\n\nclass TestPartialSymbolAutoRef(parameterized.TestCase):\n  REF_TEMPLATE = \'<a href=""{link}""><code>{text}</code></a>\'\n\n  @parameterized.named_parameters(\n      (\'basic1\', \'keras.Model.fit\', \'../tf/keras/Model.md#fit\'),\n      (\'duplicate_object\', \'layers.Conv2D\', \'../tf/keras/layers/Conv2D.md\'),\n      (\'parens\', \'Model.fit(x, y, epochs=5)\', \'../tf/keras/Model.md#fit\'),\n      (\'duplicate_name\', \'tf.matmul\', \'../tf/linalg/matmul.md\'),\n      (\'full_name\', \'tf.concat\', \'../tf/concat.md\'),\n      (\'normal_and_compat\', \'linalg.matmul\', \'../tf/linalg/matmul.md\'),\n      (\'compat_only\', \'math.deprecated\', None),\n      (\'contrib_only\', \'y.z\', None),\n  )\n  def test_partial_symbol_references(self, string, link):\n    duplicate_of = {\n        \'tf.matmul\': \'tf.linalg.matmul\',\n        \'tf.layers.Conv2d\': \'tf.keras.layers.Conv2D\',\n    }\n\n    is_fragment = {\n        \'tf.keras.Model.fit\': True,\n        \'tf.concat\': False,\n        \'tf.keras.layers.Conv2D\': False,\n        \'tf.linalg.matmul\': False,\n        \'tf.compat.v1.math.deprecated\': False,\n        \'tf.compat.v1.linalg.matmul\': False,\n        \'tf.contrib.y.z\': False,\n    }\n\n    py_module_names = [\'tf\']\n\n    resolver = parser.ReferenceResolver(duplicate_of, is_fragment,\n                                        py_module_names)\n    input_string = string.join(\'``\')\n    ref_string = resolver.replace_references(input_string, \'..\')\n\n    if link is None:\n      expected = input_string\n    else:\n      expected = self.REF_TEMPLATE.format(link=link, text=string)\n\n    self.assertEqual(expected, ref_string)\n\n\nclass TestIgnoreLineInBlock(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (\'ignore_backticks\', [\'```\'], [\'```\'],\n       \'```\\nFiller\\n```\\n```Same line```\\n```python\\nDowner\\n```\'),\n      (\'ignore_code_cell_output\', [\'<pre>{% html %}\'], [\'{% endhtml %}</pre>\'],\n       \'<pre>{% html %}\\nOutput\\nmultiline{% endhtml %}</pre>\'),\n      (\'ignore_backticks_and_cell_output\', [\'<pre>{% html %}\', \'```\'\n                                           ], [\'{% endhtml %}</pre>\', \'```\'],\n       (\'```\\nFiller\\n```\\n```Same line```\\n<pre>{% html %}\\nOutput\\nmultiline\'\n        \'{% endhtml %}</pre>\\n```python\\nDowner\\n```\')))\n  def test_ignore_lines(self, block_start, block_end, expected_ignored_lines):\n\n    text = textwrap.dedent(""""""\\\n    ```\n    Filler\n    ```\n\n    ```Same line```\n\n    <pre>{% html %}\n    Output\n    multiline{% endhtml %}</pre>\n\n    ```python\n    Downer\n    ```\n    """""")\n\n    filters = [\n        parser.IgnoreLineInBlock(start, end)\n        for start, end in zip(block_start, block_end)\n    ]\n\n    ignored_lines = []\n    for line in text.splitlines():\n      if any(filter_block(line) for filter_block in filters):\n        ignored_lines.append(line)\n\n    self.assertEqual(\'\\n\'.join(ignored_lines), expected_ignored_lines)\n\n  def test_clean_text(self):\n    text = textwrap.dedent(""""""\\\n    ```\n    Ignore lines here.\n    ```\n    Useful information.\n    Don\'t ignore.\n    ```python\n    Ignore here too.\n    ```\n    Stuff.\n    ```Not useful.```\n    """""")\n\n    filters = [parser.IgnoreLineInBlock(\'```\', \'```\')]\n\n    clean_text = []\n    for line in text.splitlines():\n      if not any(filter_block(line) for filter_block in filters):\n        clean_text.append(line)\n\n    expected_clean_text = \'Useful information.\\nDon\\\'t ignore.\\nStuff.\'\n\n    self.assertEqual(\'\\n\'.join(clean_text), expected_clean_text)\n\n  def test_strip_todos(self):\n    input_str = (""""""#  TODO(blah) blah\n\n        hello    TODO: more stuff\n        middle\n        goodbye  TODO\n        """""")\n\n    expected = (""""""\n\n        hello\n        middle\n        goodbye\n        """""")\n    strip_todos = parser._StripTODOs()\n    self.assertEqual(expected, strip_todos(input_str))\n\n\nclass TestGenerateSignature(absltest.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.known_object = object()\n    reference_resolver = parser.ReferenceResolver(\n        duplicate_of={}, is_fragment={}, py_module_names=[\'\'])\n    self.parser_config = parser.ParserConfig(\n        reference_resolver=reference_resolver,\n        duplicates={},\n        duplicate_of={},\n        tree={},\n        index={},\n        reverse_index={id(self.known_object): \'location.of.object.in.api\'},\n        base_dir=\'/\',\n        code_url_prefix=\'/\')\n\n  def test_known_object(self):\n\n    def example_fun(arg=self.known_object):  # pylint: disable=unused-argument\n      pass\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(sig.arguments, [\'arg=location.of.object.in.api\'])\n\n  def test_literals(self):\n\n    def example_fun(a=5, b=5.0, c=None, d=True, e=\'hello\', f=(1, (2, 3))):  # pylint: disable=g-bad-name, unused-argument\n      pass\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(\n        sig.arguments,\n        [\'a=5\', \'b=5.0\', \'c=None\', \'d=True\', ""e=\'hello\'"", \'f=(1, (2, 3))\'])\n\n  def test_dotted_name(self):\n    # pylint: disable=g-bad-name\n\n    class a(object):\n\n      class b(object):\n\n        class c(object):\n\n          class d(object):\n\n            def __init__(self, *args):\n              pass\n\n    # pylint: enable=g-bad-name\n\n    e = {\'f\': 1}\n\n    def example_fun(arg1=a.b.c.d, arg2=a.b.c.d(1, 2), arg3=e[\'f\']):  # pylint: disable=unused-argument\n      pass\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(sig.arguments,\n                     [\'arg1=a.b.c.d\', \'arg2=a.b.c.d(1, 2)\', ""arg3=e[\'f\']""])\n\n  def test_compulsory_kwargs_without_defaults(self):\n\n    def example_fun(x, z, a=True, b=\'test\', *, y=None, c, **kwargs) -> bool:  # pylint: disable=unused-argument\n      return True\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(\n        sig.arguments,\n        [\'x\', \'z\', \'a=True\', ""b=\'test\'"", \'*\', \'y=None\', \'c\', \'**kwargs\'])\n    self.assertEqual(sig.return_type, \'bool\')\n    self.assertEqual(sig.arguments_typehint_exists, False)\n    self.assertEqual(sig.return_typehint_exists, True)\n\n  def test_compulsory_kwargs_without_defaults_with_args(self):\n\n    def example_fun(x, z, *args, a=True, b=\'test\', y=None, c, **kwargs):  # pylint: disable=unused-argument\n      return True\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(\n        sig.arguments,\n        [\'x\', \'z\', \'*args\', \'a=True\', ""b=\'test\'"", \'y=None\', \'c\', \'**kwargs\'])\n    self.assertEqual(sig.arguments_typehint_exists, False)\n    self.assertEqual(sig.return_typehint_exists, False)\n\n  def test_type_annotations(self):\n    # pylint: disable=unused-argument\n\n    def example_fun(self,\n                    cls,\n                    x: List[str],\n                    z: int,\n                    a: Union[List[str], str, int] = None,\n                    b: str = \'test\',\n                    *,\n                    y: bool = False,\n                    c: int,\n                    **kwargs) -> None:\n      pass\n\n    # pylint: enable=unused-argument\n\n    sig = parser.generate_signature(\n        example_fun, parser_config=self.parser_config, func_full_name=\'\')\n    self.assertEqual(sig.arguments, [\n        \'cls\', \'x: List[str]\', \'z: int\', \'a: Union[List[str], str, int] = None\',\n        ""b: str = \'test\'"", \'*\', \'y: bool = False\', \'c: int\', \'**kwargs\'\n    ])\n    self.assertEqual(sig.return_type, \'None\')\n    self.assertEqual(sig.arguments_typehint_exists, True)\n    self.assertEqual(sig.return_typehint_exists, True)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/pretty_docs.py,6,"b'# Lint as: python3\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A module for converting parsed doc content into markdown pages.\n\nThe adjacent `parser` module creates `PageInfo` objects, containing all data\nnecessary to document an element of the TensorFlow API.\n\nThis module contains one public function, which handels the conversion of these\n`PageInfo` objects into a markdown string:\n\n    md_page = build_md_page(page_info)\n""""""\n\nimport textwrap\n\nfrom typing import Dict, List, Optional, NamedTuple\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import doc_generator_visitor\nfrom tensorflow_docs.api_generator import parser\n\n_TABLE_ITEMS = (\'arg\', \'return\', \'raise\', \'attr\')\n\n\ndef build_md_page(page_info: parser.PageInfo, table_view: bool) -> str:\n  """"""Given a PageInfo object, return markdown for the page.\n\n  Args:\n    page_info: must be a `parser.FunctionPageInfo`, `parser.ClassPageInfo`, or\n      `parser.ModulePageInfo`\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n\n  Returns:\n    Markdown for the page\n\n  Raises:\n    ValueError: if `page_info` is an instance of an unrecognized class\n  """"""\n  if isinstance(page_info, parser.ClassPageInfo):\n    return _build_class_page(page_info, table_view)\n\n  if isinstance(page_info, parser.FunctionPageInfo):\n    return _build_function_page(page_info, table_view)\n\n  if isinstance(page_info, parser.ModulePageInfo):\n    return _build_module_page(page_info, table_view)\n\n  if isinstance(page_info, parser.TypeAliasPageInfo):\n    return _build_type_alias_page(page_info, table_view)\n\n  raise ValueError(f\'Unknown Page Info Type: {type(page_info)}\')\n\n\ndef _format_docstring(item,\n                      table_view: bool,\n                      *,\n                      table_title_template: Optional[str] = None) -> str:\n  """"""Formats TitleBlock into a table or list or a normal string.\n\n  Args:\n    item: A TitleBlock instance or a normal string.\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n    table_title_template: Template for title detailing how to display it in the\n      table.\n\n  Returns:\n    A formatted docstring.\n  """"""\n\n  if isinstance(item, parser.TitleBlock):\n    lower_title = item.title.lower()\n    if table_view and lower_title.startswith(_TABLE_ITEMS):\n      return item.table_view(title_template=table_title_template)\n    else:\n      if \'attr\' in lower_title:\n        return item.list_view(title_template=\'\\n\\n## {title}\\n\')\n      else:\n        return item.list_view(title_template=\'\\n\\n#### {title}:\\n\')\n  else:\n    return str(item)\n\n\ndef _build_function_page(page_info: parser.FunctionPageInfo,\n                         table_view: bool) -> str:\n  """"""Constructs a markdown page given a `FunctionPageInfo` object.\n\n  Args:\n    page_info: A `FunctionPageInfo` object containing information that\'s used to\n      create a function page.\n      For example, see https://www.tensorflow.org/api_docs/python/tf/concat\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n\n  Returns:\n    The function markdown page.\n  """"""\n\n  parts = [f\'# {page_info.full_name}\\n\\n\']\n\n  parts.append(\'<!-- Insert buttons and diff -->\\n\')\n\n  parts.append(_top_source_link(page_info.defined_in))\n  parts.append(\'\\n\\n\')\n\n  parts.append(page_info.doc.brief + \'\\n\\n\')\n\n  parts.append(_build_collapsable_aliases(page_info.aliases))\n\n  if page_info.signature is not None:\n    parts.append(_build_signature(page_info, obj_name=page_info.full_name))\n    parts.append(\'\\n\\n\')\n\n  # This will be replaced by the ""Used in: <notebooks>"" whenever it is run.\n  parts.append(\'<!-- Placeholder for ""Used in"" -->\\n\')\n\n  for item in page_info.doc.docstring_parts:\n    parts.append(\n        _format_docstring(\n            item,\n            table_view,\n            table_title_template=\'<h2 class=""add-link"">{title}</h2>\'))\n\n  parts.append(_build_compatibility(page_info.doc.compatibility))\n\n  custom_content = doc_controls.get_custom_page_content(page_info.py_object)\n  if custom_content is not None:\n    parts.append(custom_content)\n\n  return \'\'.join(parts)\n\n\ndef _build_type_alias_page(page_info: parser.TypeAliasPageInfo,\n                           table_view: bool) -> str:\n  """"""Constructs a markdown page given a `TypeAliasPageInfo` object.\n\n  Args:\n    page_info: A `TypeAliasPageInfo` object containing information that\'s used\n      to create a type alias page.\n    table_view: Unused for this function.\n\n  Returns:\n    The type alias\'s markdown page.\n  """"""\n\n  del table_view\n\n  parts = []\n  parts.append(page_info.doc.brief)\n  parts.append(\'\\n\\n\')\n\n  if page_info.signature is not None:\n    parts.append(\'Source:\\n\\n\')\n    parts.append(\n        _build_signature(\n            page_info, obj_name=page_info.short_name, type_alias=True))\n    parts.append(\'\\n\\n\')\n\n  return \'\\n\'.join(parts)\n\n\nclass _Methods(NamedTuple):\n  info_dict: Dict[str, parser.MethodInfo]\n  constructor: parser.MethodInfo\n\n\ndef _split_methods(methods: List[parser.MethodInfo]) -> _Methods:\n  """"""Splits the given methods list into constructors and the remaining methods.\n\n  If both `__init__` and `__new__` exist on the class, then prefer `__init__`\n  as the constructor over `__new__` to document.\n\n  Args:\n    methods: List of all the methods on the `ClassPageInfo` object.\n\n  Returns:\n    A `DocumentMethods` object containing a {method_name: method object}\n    dictionary and a constructor object.\n  """"""\n\n  # Create a method_name to methods object dictionary.\n  method_info_dict = {method.short_name: method for method in methods}\n\n  # Pop the constructors from the dictionary.\n  init_constructor = method_info_dict.pop(\'__init__\', None)\n  new_constructor = method_info_dict.pop(\'__new__\', None)\n\n  constructor = None\n  # Prefers `__init__` over `__new__` as the constructor to document.\n  if init_constructor is not None:\n    constructor = init_constructor\n  elif new_constructor is not None:\n    constructor = new_constructor\n\n  return _Methods(info_dict=method_info_dict, constructor=constructor)\n\n\ndef _merge_class_and_constructor_docstring(\n    class_page_info: parser.ClassPageInfo, ctor_info: parser.MethodInfo,\n    table_view: bool) -> List[str]:\n  """"""Merges the class and the constructor docstrings.\n\n  While merging, the following rules are followed:\n\n  * Only `Arguments` and `Raises` blocks from constructor are uplifted to the\n    class docstring. Rest of the stuff is ignored since it doesn\'t add much\n    value and in some cases the information is repeated.\n\n  * The `Raises` block is added to the end of the classes docstring.\n\n  * The `Arguments` or `Args` block is inserted before the `Attributes` section.\n    If `Attributes` section does not exist in the class docstring then add it\n    to the end.\n\n  * If the constructor does not exist on the class, then the class docstring\n    is returned as it is.\n\n  Args:\n    class_page_info: Object containing information about the class.\n    ctor_info: Object containing information about the constructor of the class.\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n\n  Returns:\n    A list of strings containing the merged docstring.\n  """"""\n\n  def _create_class_doc(doc):\n    updated_doc = []\n    for item in doc:\n      updated_doc.append(\n          _format_docstring(\n              item,\n              table_view,\n              table_title_template=\'<h2 class=""add-link"">{title}</h2>\'))\n    return updated_doc\n\n  # Get the class docstring. `.doc.docstring_parts` contain the entire\n  # docstring except for the one-line docstring that\'s compulsory.\n  class_doc = class_page_info.doc.docstring_parts\n\n  # If constructor doesn\'t exist, return the class docstring as it is.\n  if ctor_info is None:\n    return _create_class_doc(class_doc)\n\n  # Get the constructor\'s docstring parts.\n  constructor_doc = ctor_info.doc.docstring_parts\n\n  # Extract the `Arguments`/`Args` from the constructor\'s docstring.\n  # A constructor won\'t contain `Args` and `Arguments` section at once.\n  # It can contain either one of these so check for both.\n  for block in constructor_doc:\n    if isinstance(block, parser.TitleBlock):\n      if block.title.startswith((\'Args\', \'Arguments\', \'Raises\')):\n        class_doc.append(block)\n\n  return _create_class_doc(class_doc)\n\n\ndef _build_class_page(page_info: parser.ClassPageInfo, table_view: bool) -> str:\n  """"""Constructs a markdown page given a `ClassPageInfo` object.\n\n  Args:\n    page_info: A `ClassPageInfo` object containing information that\'s used to\n      create a class page. For example, see\n      https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n\n  Returns:\n    The class markdown page.\n  """"""\n\n  # Add the full_name of the symbol to the page.\n  parts = [\'# {page_info.full_name}\\n\\n\'.format(page_info=page_info)]\n\n  # This is used as a marker to initiate the diffing process later down in the\n  # pipeline.\n  parts.append(\'<!-- Insert buttons and diff -->\\n\')\n\n  # Add the github button.\n  parts.append(_top_source_link(page_info.defined_in))\n  parts.append(\'\\n\\n\')\n\n  # Add the one line docstring of the class.\n  parts.append(page_info.doc.brief + \'\\n\\n\')\n\n  # If a class is a child class, add which classes it inherits from.\n  if page_info.bases:\n    parts.append(\'Inherits From: \')\n\n    link_template = \'[`{short_name}`]({url})\'\n    parts.append(\', \'.join(\n        link_template.format(**base._asdict()) for base in page_info.bases))\n    parts.append(\'\\n\\n\')\n\n  # Build the aliases section and keep it collapses by default.\n  parts.append(_build_collapsable_aliases(page_info.aliases))\n\n  # Split the methods into constructor and other methods.\n  methods = _split_methods(page_info.methods)\n\n  # If the class has a constructor, build its signature.\n  # The signature will contain the class name followed by the arguments it\n  # takes.\n  if methods.constructor is not None:\n    parts.append(\n        _build_signature(methods.constructor, obj_name=page_info.full_name))\n    parts.append(\'\\n\\n\')\n\n  # This will be replaced by the ""Used in: <notebooks>"" later in the pipeline.\n  parts.append(\'<!-- Placeholder for ""Used in"" -->\\n\')\n\n  # Merge the class and constructor docstring.\n  parts.extend(\n      _merge_class_and_constructor_docstring(page_info, methods.constructor,\n                                             table_view))\n\n  # Add the compatibility section to the page.\n  parts.append(_build_compatibility(page_info.doc.compatibility))\n  parts.append(\'\\n\\n\')\n\n  custom_content = doc_controls.get_custom_page_content(page_info.py_object)\n  if custom_content is not None:\n    parts.append(custom_content)\n    return \'\'.join(parts)\n\n  if page_info.attr_block is not None:\n    parts.append(\n        _format_docstring(\n            page_info.attr_block,\n            table_view,\n            table_title_template=\'<h2 class=""add-link"">{title}</h2>\'))\n    parts.append(\'\\n\\n\')\n\n  # If the class has child classes, add that information to the page.\n  if page_info.classes:\n    parts.append(\'## Child Classes\\n\')\n\n    link_template = (\'[`class {class_info.short_name}`]\'\n                     \'({class_info.url})\\n\\n\')\n    class_links = sorted(\n        link_template.format(class_info=class_info)\n        for class_info in page_info.classes)\n\n    parts.extend(class_links)\n\n  # If the class contains methods other than the constructor, then add them\n  # to the page.\n  if methods.info_dict:\n    parts.append(\'## Methods\\n\\n\')\n    for method_name in sorted(methods.info_dict, key=_method_sort):\n      parts.append(\n          _build_method_section(methods.info_dict[method_name], table_view))\n    parts.append(\'\\n\\n\')\n\n  # Add class variables/members if they exist to the page.\n  if page_info.other_members:\n    parts.append(\'## Class Variables\\n\\n\')\n    parts.append(_other_members(page_info.other_members))\n\n  return \'\'.join(parts)\n\n\ndef _method_sort(method_name):\n  # All dunder methods will be at the end of the list in an alphabetically\n  # sorted order. Rest of the methods will be promoted to the top in an\n  # alphabetically sorted order.\n  if method_name.startswith(\'__\'):\n    return (1, method_name)\n  return (-1, method_name)\n\n\ndef _other_members(other_members):\n  """"""Returns ""other_members"" rendered to markdown.\n\n  `other_members` is used for anything that is not a class, function, module,\n  or method.\n\n  Args:\n    other_members: a list of (name, object) pairs.\n\n  Returns:\n    A markdown string\n  """"""\n  parts = []\n  list_item = \'* `{short_name}` <a id=""{short_name}""></a>\\n\'\n  list_item_with_value = (\'* `{short_name} = {obj!r}` \'\n                          \'<a id=""{short_name}""></a>\\n\')\n  for other_member in other_members:\n    if doc_generator_visitor.maybe_singleton(other_member.obj):\n      part = list_item_with_value.format(**other_member._asdict())\n    else:\n      part = list_item.format(**other_member._asdict())\n    parts.append(part)\n\n  return \'\'.join(parts)\n\n\ndef _build_method_section(method_info, table_view, heading_level=3):\n  """"""Generates a markdown section for a method.\n\n  Args:\n    method_info: A `MethodInfo` object.\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n    heading_level: An Int, which HTML heading level to use.\n\n  Returns:\n    A markdown string.\n  """"""\n  parts = []\n  heading = (\'<h{heading_level} id=""{short_name}"">\'\n             \'<code>{short_name}</code>\'\n             \'</h{heading_level}>\\n\\n\')\n  parts.append(\n      heading.format(heading_level=heading_level, **method_info._asdict()))\n\n  if method_info.defined_in:\n    parts.append(_small_source_link(method_info.defined_in))\n\n  if method_info.signature is not None:\n    parts.append(_build_signature(method_info, obj_name=method_info.short_name))\n\n  parts.append(method_info.doc.brief + \'\\n\')\n\n  for item in method_info.doc.docstring_parts:\n    parts.append(_format_docstring(item, table_view, table_title_template=None))\n\n  parts.append(_build_compatibility(method_info.doc.compatibility))\n  parts.append(\'\\n\\n\')\n  return \'\'.join(parts)\n\n\ndef _build_module_parts(module_parts: List[parser.MemberInfo],\n                        template: str) -> List[str]:\n  mod_str_parts = []\n  for item in module_parts:\n    mod_str_parts.append(template.format(**item._asdict()))\n    if item.doc.brief:\n      mod_str_parts.append(\': \' + item.doc.brief)\n    mod_str_parts.append(\'\\n\\n\')\n  return mod_str_parts\n\n\ndef _build_module_page(page_info: parser.ModulePageInfo,\n                       table_view: bool) -> str:\n  """"""Constructs a markdown page given a `ModulePageInfo` object.\n\n  Args:\n    page_info: A `ModulePageInfo` object containing information that\'s used to\n      create a module page.\n      For example, see https://www.tensorflow.org/api_docs/python/tf/data\n    table_view: If True, `Args`, `Returns`, `Raises` or `Attributes` will be\n      converted to a tabular format while generating markdown. If False, they\n      will be converted to a markdown List view.\n\n  Returns:\n    The module markdown page.\n  """"""\n\n  parts = [f\'# Module: {page_info.full_name}\\n\\n\']\n\n  parts.append(\'<!-- Insert buttons and diff -->\\n\')\n\n  parts.append(_top_source_link(page_info.defined_in))\n  parts.append(\'\\n\\n\')\n\n  # First line of the docstring i.e. a brief introduction about the symbol.\n  parts.append(page_info.doc.brief + \'\\n\\n\')\n\n  parts.append(_build_collapsable_aliases(page_info.aliases))\n\n  # All lines in the docstring, expect the brief introduction.\n  for item in page_info.doc.docstring_parts:\n    parts.append(_format_docstring(item, table_view, table_title_template=None))\n\n  parts.append(_build_compatibility(page_info.doc.compatibility))\n\n  parts.append(\'\\n\\n\')\n\n  custom_content = doc_controls.get_custom_page_content(page_info.py_object)\n  if custom_content is not None:\n    parts.append(custom_content)\n    return \'\'.join(parts)\n\n  if page_info.modules:\n    parts.append(\'## Modules\\n\\n\')\n    parts.extend(\n        _build_module_parts(\n            module_parts=page_info.modules,\n            template=\'[`{short_name}`]({url}) module\'))\n\n  if page_info.classes:\n    parts.append(\'## Classes\\n\\n\')\n    parts.extend(\n        _build_module_parts(\n            module_parts=page_info.classes,\n            template=\'[`class {short_name}`]({url})\'))\n\n  if page_info.functions:\n    parts.append(\'## Functions\\n\\n\')\n    parts.extend(\n        _build_module_parts(\n            module_parts=page_info.functions,\n            template=\'[`{short_name}(...)`]({url})\'))\n\n  if page_info.type_alias:\n    parts.append(\'## Type Aliases\\n\\n\')\n    parts.extend(\n        _build_module_parts(\n            module_parts=page_info.type_alias,\n            template=\'[`{short_name}`]({url})\'))\n\n  if page_info.other_members:\n    # TODO(markdaoust): Document the value of the members, for basic types.\n    parts.append(\'## Other Members\\n\\n\')\n    parts.append(_other_members(page_info.other_members))\n\n  return \'\'.join(parts)\n\n\nDECORATOR_WHITELIST = {\n    \'classmethod\',\n    \'staticmethod\',\n    \'tf_contextlib.contextmanager\',\n    \'contextlib.contextmanager\',\n    \'tf.function\',\n    \'types.method\',\n    \'abc.abstractmethod\',\n}\n\n\ndef _build_signature(obj_info: parser.PageInfo,\n                     obj_name: str,\n                     type_alias: bool = False) -> str:\n  """"""Returns a markdown code block containing the function signature.\n\n  Wraps the signature and limits it to 80 characters.\n\n  Args:\n    obj_info: Object containing information about the class/method/function for\n      which a signature will be created.\n    obj_name: The name to use to build the signature.\n    type_alias: If True, uses an `=` instead of `()` for the signature.\n      For example: `TensorLike = (Union[str, tf.Tensor, int])`.\n      Defaults to `False`.\n\n  Returns:\n    The signature of the object.\n  """"""\n\n  # Special case tf.range, since it has an optional first argument\n  if obj_info.full_name == \'tf.range\':\n    return textwrap.dedent(""""""\n      ```python\n      tf.range(limit, delta=1, dtype=None, name=\'range\')\n      tf.range(start, limit, delta=1, dtype=None, name=\'range\')\n      ```\n      """""")\n\n  full_signature = str(obj_info.signature)\n\n  parts = [\n      \'<pre class=""devsite-click-to-copy prettyprint lang-py \'\n      \'tfo-signature-link"">\'\n  ]\n\n  if hasattr(obj_info, \'decorators\'):\n    parts.extend([\n        f\'<code>@{dec}</code>\' for dec in obj_info.decorators\n        if dec in DECORATOR_WHITELIST\n    ])\n\n  if type_alias:\n    parts.append(f\'<code>{obj_name} = {full_signature}\')\n  else:\n    parts.append(f\'<code>{obj_name}{full_signature}\')\n  parts.append(\'</code></pre>\\n\\n\')\n\n  return \'\\n\'.join(parts)\n\n\ndef _build_compatibility(compatibility):\n  """"""Return the compatibility section as an md string.""""""\n  parts = []\n  sorted_keys = sorted(compatibility.keys())\n  for key in sorted_keys:\n\n    value = compatibility[key]\n    # Dedent so that it does not trigger markdown code formatting.\n    value = textwrap.dedent(value)\n    parts.append(f\'\\n\\n#### {key.title()} Compatibility\\n{value}\\n\')\n\n  return \'\'.join(parts)\n\n\ndef _top_source_link(location):\n  """"""Retrns a source link with Github image, like the notebook butons.""""""\n  table_template = textwrap.dedent(""""""\n    <table class=""tfo-notebook-buttons tfo-api"" align=""left"">\n    {}\n    </table>\n\n    """""")\n\n  link_template = textwrap.dedent(""""""\n    <td>\n      <a target=""_blank"" href=""{url}"">\n        <img src=""https://www.tensorflow.org/images/GitHub-Mark-32px.png"" />\n        View source on GitHub\n      </a>\n    </td>"""""")\n\n  if location is None or not location.url:\n    return table_template.format(\'\')\n\n  if \'github.com\' not in location.url:\n    return table_template.format(\'\') + _small_source_link(location)\n\n  link = link_template.format(url=location.url)\n  table = table_template.format(link)\n  return table\n\n\ndef _small_source_link(location):\n  """"""Returns a small source link.""""""\n  template = \'<a target=""_blank"" href=""{url}"">View source</a>\\n\\n\'\n\n  if not location.url:\n    return \'\'\n\n  return template.format(url=location.url)\n\n\ndef _build_collapsable_aliases(aliases: List[str]) -> str:\n  """"""Returns the top ""Aliases"" line.""""""\n\n  def join_aliases(aliases: List[str]) -> str:\n    return \', \'.join(\'`{}`\'.format(name) for name in aliases)\n\n  collapsable_template = textwrap.dedent(""""""\\\n    <section class=""expandable"">\n      <h4 class=""showalways"">View aliases</h4>\n      <p>{content}</p>\n    </section>\n    """""")\n\n  main_alias_template = textwrap.dedent(""""""\n    <b>Main aliases</b>\n    <p>{content}</p>\n    """""")\n\n  compat_alias_template = textwrap.dedent(""""""\n    <b>Compat aliases for migration</b>\n    <p>See\n    <a href=""https://www.tensorflow.org/guide/migrate"">Migration guide</a> for\n    more details.</p>\n    <p>{content}</p>\n    """""")\n\n  main_aliases = []\n  compat_aliases = []\n\n  for alias in aliases:\n    if \'__\' in alias:\n      continue\n    elif \'compat.v\' in alias:\n      compat_aliases.append(alias)\n    else:\n      main_aliases.append(alias)\n\n  alias_content = \'\'\n  if main_aliases:\n    alias_content += main_alias_template.format(\n        content=join_aliases(main_aliases))\n  if compat_aliases:\n    alias_content += compat_alias_template.format(\n        content=join_aliases(compat_aliases))\n\n  if alias_content:\n    return collapsable_template.format(content=alias_content) + \'\\n\'\n\n  return alias_content\n'"
tools/tensorflow_docs/api_generator/public_api.py,11,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Visitor restricting traversal to only the public tensorflow API.""""""\n\nimport ast\nimport inspect\nimport typing\n\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import doc_generator_visitor\n\n_TYPING = frozenset(\n    id(obj)\n    for obj in typing.__dict__.values()\n    if not doc_generator_visitor.maybe_singleton(obj))\n\n\ndef ignore_typing(path, parent, children):\n  """"""Removes all children that are members of the typing module.\n\n  Arguments:\n    path: A tuple or name parts forming the attribute-lookup path to this\n      object. For `tf.keras.layers.Dense` path is:\n        (""tf"",""keras"",""layers"",""Dense"")\n    parent: The parent object.\n    children: A list of (name, value) pairs. The attributes of the patent.\n\n  Returns:\n    A filtered list of children `(name, value)` pairs.\n  """"""\n  del path\n  del parent\n\n  children = [(name, child_obj)\n              for (name, child_obj) in children\n              if id(child_obj) not in _TYPING]\n\n  return children\n\n\ndef local_definitions_filter(path, parent, children):\n  """"""Filters children recursively.\n\n  Only returns those defined in this package.\n\n  This follows the API for visitors defined by `traverse.traverse`.\n\n  This is a much tighter constraint than the default ""base_dir"" filter which\n  ensures that only objects defined within the package root are documented.\n  This applies a similar constraint, to each submodule.\n\n  in the api-tree below, `Dense` is defined in `tf.keras.layers`, but imported\n  into `tf.layers` and `tf.keras`.\n\n  ```\n  tf\n    keras\n      from tf.keras.layers import Dense\n      layers\n         class Dense(...)\n    layers\n      from tf.keras.layers import Dense\n  ```\n\n  This filter hides the `tf.layers.Dense` reference as `Dense` is not defined\n  within the `tf.layers` package. The reference at `tf.keras.Dense` is still\n  visible because dense is defined inside the `tf.keras` tree.\n\n  One issue/feature to be aware of with this approach is that if you hide\n  the defining module, the object will not be visible in the docs at all. For\n  example, if used with the package below, `util_1` and `util_2` are never\n  documented.\n\n  ```\n  my_package\n    _utils    # leading `_` means private.\n      def util_1\n      def util_2\n    subpackage\n      from my_package._utils import *\n  ```\n  Arguments:\n    path: A tuple or name parts forming the attribute-lookup path to this\n      object. For `tf.keras.layers.Dense` path is:\n        (""tf"",""keras"",""layers"",""Dense"")\n    parent: The parent object.\n    children: A list of (name, value) pairs. The attributes of the patent.\n\n  Returns:\n    A filtered list of children `(name, value)` pairs.\n  """"""\n  del path\n\n  if not inspect.ismodule(parent):\n    return children\n\n  filtered_children = []\n  for pair in children:\n    # Find the name of the module the child was defined in.\n    _, child = pair\n    if inspect.ismodule(child):\n      maybe_submodule = child.__name__\n    elif inspect.isfunction(child) or inspect.isclass(child):\n      maybe_submodule = child.__module__\n    else:\n      maybe_submodule = parent.__name__\n\n    # Skip if child was not defined within the parent module\n    in_submodule = maybe_submodule.startswith(parent.__name__)\n    if not in_submodule:\n      continue\n\n    filtered_children.append(pair)\n\n  return filtered_children\n\n\ndef _get_imported_symbols(obj):\n  """"""Returns a list of symbol names imported by the given `obj`.""""""\n\n  class ImportNodeVisitor(ast.NodeVisitor):\n    """"""An `ast.Visitor` that collects the names of imported symbols.""""""\n\n    def __init__(self):\n      self.imported_symbols = []\n\n    def _add_imported_symbol(self, node):\n      self.imported_symbols.extend([alias.name for alias in node.names])\n\n    def visit_Import(self, node):  # pylint: disable=invalid-name\n      self._add_imported_symbol(node)\n\n    def visit_ImportFrom(self, node):  # pylint: disable=invalid-name\n      self._add_imported_symbol(node)\n\n  source = inspect.getsource(obj)\n  tree = ast.parse(source)\n  visitor = ImportNodeVisitor()\n  visitor.visit(tree)\n  return visitor.imported_symbols\n\n\ndef explicit_package_contents_filter(path, parent, children):\n  """"""Filter modules to only include explicit contents.\n\n  This function returns the children explicitly included by this module, meaning\n  that it will exclude:\n\n  *   Modules in a package not explicitly imported by the package (submodules\n      are implicitly injected into their parent\'s namespace).\n  *   Modules imported by a module that is not a package.\n\n  This filter is useful if you explicitly define your API in the packages of\n  your library, but do not expliticly define that API in the `__all__` variable\n  of each module. The purpose is to make it easier to maintain that API.\n\n  Note: This filter does work with wildcard imports, however it is generally not\n  recommended to use wildcard imports.\n\n  Args:\n    path: A tuple of names forming the path to the object.\n    parent: The parent object.\n    children: A list of (name, value) tuples describing the attributes of the\n      patent.\n\n  Returns:\n    A filtered list of children `(name, value)` pairs.\n  """"""\n  del path  # Unused\n  is_parent_module = inspect.ismodule(parent)\n  is_parent_package = is_parent_module and hasattr(parent, \'__path__\')\n  if is_parent_package:\n    imported_symbols = _get_imported_symbols(parent)\n  filtered_children = []\n  for child in children:\n    name, obj = child\n    if inspect.ismodule(obj):\n      # Do not include modules in a package not explicitly imported by the\n      # package.\n      if is_parent_package and name not in imported_symbols:\n        continue\n      # Do not include modules imported by a module that is not a package.\n      if is_parent_module and not is_parent_package:\n        continue\n    filtered_children.append(child)\n  return filtered_children\n\n\nclass PublicAPIFilter(object):\n  """"""Visitor to use with `traverse` to filter just the public API.""""""\n\n  def __init__(self, base_dir, do_not_descend_map=None, private_map=None):\n    """"""Constructor.\n\n    Args:\n      base_dir: The directory to take source file paths relative to.\n      do_not_descend_map: A mapping from dotted path like ""tf.symbol"" to a list\n        of names. Included names will have no children listed.\n      private_map: A mapping from dotted path like ""tf.symbol"" to a list of\n        names. Included names will not be listed at that location.\n    """"""\n    self._base_dir = base_dir\n    self._do_not_descend_map = do_not_descend_map or {}\n    self._private_map = private_map or {}\n\n  ALLOWED_PRIVATES = frozenset([\n      \'__abs__\', \'__add__\', \'__and__\', \'__bool__\', \'__call__\', \'__concat__\',\n      \'__contains__\', \'__div__\', \'__enter__\', \'__eq__\', \'__exit__\',\n      \'__floordiv__\', \'__ge__\', \'__getitem__\', \'__gt__\', \'__init__\',\n      \'__invert__\', \'__iter__\', \'__le__\', \'__len__\', \'__lt__\', \'__matmul__\',\n      \'__mod__\', \'__mul__\', \'__new__\', \'__ne__\', \'__neg__\', \'__pos__\',\n      \'__nonzero__\', \'__or__\', \'__pow__\', \'__radd__\', \'__rand__\', \'__rdiv__\',\n      \'__rfloordiv__\', \'__rmatmul__\', \'__rmod__\', \'__rmul__\', \'__ror__\',\n      \'__rpow__\', \'__rsub__\', \'__rtruediv__\', \'__rxor__\', \'__sub__\',\n      \'__truediv__\', \'__xor__\', \'__version__\'\n  ])\n\n  def _is_private(self, path, name, obj):\n    """"""Return whether a name is private.""""""\n    # Skip objects blocked by doc_controls.\n    if doc_controls.should_skip(obj):\n      return True\n\n    # Skip modules outside of the package root.\n    if inspect.ismodule(obj):\n      if hasattr(obj, \'__file__\'):\n        # `startswith` will match any item in a tuple if a tuple of base_dir\n        # are passed.\n        # It\'s important that this is a string comparison not a `relpath`,\n        # because in some cases `base_dir` may not a directory but a single\n        # file path.\n        if not obj.__file__.startswith(self._base_dir):\n          return True\n\n    # Skip objects blocked by the private_map\n    if name in self._private_map.get(\'.\'.join(path), []):\n      return True\n\n    # Skip ""_"" hidden attributes\n    if name.startswith(\'_\') and name not in self.ALLOWED_PRIVATES:\n      return True\n\n    return False\n\n  def __call__(self, path, parent, children):\n    """"""Visitor interface, see `traverse` for details.""""""\n\n    # Avoid long waits in cases of pretty unambiguous failure.\n    if inspect.ismodule(parent) and len(path) > 10:\n      raise RuntimeError(\'Modules nested too deep:\\n\\n{}\\n\\nThis is likely a \'\n                         \'problem with an accidental public import.\'.format(\n                             \'.\'.join(path)))\n\n    # No children if ""do_not_descend"" is set.\n    parent_path = \'.\'.join(path[:-1])\n    name = path[-1]\n    if name in self._do_not_descend_map.get(parent_path, []):\n      del children[:]\n\n    # Remove things that are not visible.\n    children = [(child_name, child_obj)\n                for child_name, child_obj in list(children)\n                if not self._is_private(path, child_name, child_obj)]\n\n    return children\n'"
tools/tensorflow_docs/api_generator/public_api_test.py,13,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow.tools.common.public_api.""""""\n\nimport inspect\nimport types\n\nimport typing\n\nfrom absl.testing import absltest\n# This import is using to test\nfrom tensorflow_docs import api_generator\nfrom tensorflow_docs.api_generator import public_api\n\n\nclass PublicApiTest(absltest.TestCase):\n\n  class TestVisitor(object):\n\n    def __init__(self):\n      self.symbols = set()\n      self.last_parent = None\n      self.last_children = None\n\n    def __call__(self, path, parent, children):\n      self.symbols.add(path)\n      self.last_parent = parent\n      self.last_children = list(children)  # Make a copy to preserve state.\n      return children\n\n  def test_call_forward(self):\n    visitor = self.TestVisitor()\n\n    api_visitors = [public_api.PublicAPIFilter(base_dir=\'/\'), visitor]\n\n    path = (\'tf\', \'test\')\n    parent = \'dummy\'\n    children = [(\'name1\', \'thing1\'), (\'name2\', \'thing2\')]\n\n    for api_visitor in api_visitors:\n      children = api_visitor(path, parent, children)\n\n    self.assertEqual(set([(\n        \'tf\',\n        \'test\',\n    )]), visitor.symbols)\n    self.assertEqual(\'dummy\', visitor.last_parent)\n    self.assertEqual([(\'name1\', \'thing1\'), (\'name2\', \'thing2\')],\n                     visitor.last_children)\n\n  def test_private_child_removal(self):\n    visitor = self.TestVisitor()\n    api_visitors = [\n        public_api.PublicAPIFilter(base_dir=\'/\'),\n        visitor,\n    ]\n\n    children = [(\'name1\', \'thing1\'), (\'_name2\', \'thing2\')]\n    path = (\'tf\', \'test\')\n    parent = \'dummy\'\n    for api_visitor in api_visitors:\n      children = api_visitor(path, parent, children)\n\n    # Make sure the private symbols are removed before the visitor is called.\n    self.assertEqual([(\'name1\', \'thing1\')], visitor.last_children)\n    self.assertEqual([(\'name1\', \'thing1\')], children)\n\n  def test_no_descent_child_removal(self):\n    visitor = self.TestVisitor()\n\n    api_visitors = [\n        public_api.PublicAPIFilter(\n            base_dir=\'/\', do_not_descend_map={\'tf.test\': [\'mock\']}), visitor\n    ]\n\n    children = [(\'name1\', \'thing1\'), (\'name2\', \'thing2\')]\n    path = (\'tf\', \'test\', \'mock\')\n    parent = \'dummy\'\n\n    for api_visitor in api_visitors:\n      children = api_visitor(path, parent, children)\n\n    # Make sure not-to-be-descended-into symbols\'s children are removed.\n    self.assertEqual([], visitor.last_children)\n    self.assertEqual([], children)\n\n  def test_private_map_child_removal(self):\n    visitor = self.TestVisitor()\n\n    api_visitors = [\n        public_api.PublicAPIFilter(\n            base_dir=\'/\', private_map={\'tf.test\': [\'mock\']}), visitor\n    ]\n\n    children = [(\'name1\', \'thing1\'), (\'mock\', \'thing2\')]\n    path = (\'tf\', \'test\')\n    parent = \'dummy\'\n\n    for api_visitor in api_visitors:\n      children = api_visitor(path, parent, children)\n    # Make sure private aliases are removed.\n    self.assertEqual([(\'name1\', \'thing1\')], visitor.last_children)\n    self.assertEqual([(\'name1\', \'thing1\')], children)\n\n  def test_local_definitions_filter(self):\n    tf = types.ModuleType(\'tf\')\n    tf.keras = types.ModuleType(\'tf.keras\')\n    tf.keras.layers = types.ModuleType(\'tf.keras.layers\')\n    tf.keras.layers.Dense = lambda: None\n    tf.keras.layers.Dense.__module__ = \'tf.keras.layers\'\n\n    tf.keras.Dense = tf.keras.layers.Dense\n\n    tf.layers = types.ModuleType(\'tf.layers\')\n    tf.layers.Dense = tf.keras.layers.Dense\n\n    def public_members(obj):\n      members = inspect.getmembers(obj)\n      return [\n          (name, value) for name, value in members if not name.startswith(\'_\')\n      ]\n\n    filtered_children = public_api.local_definitions_filter(\n        (\'tf\', \'keras\', \'layers\'), tf.keras.layers,\n        public_members(tf.keras.layers))\n    filtered_names = [name for name, _ in filtered_children]\n\n    self.assertCountEqual([\'Dense\'], filtered_names)\n\n    filtered_children = public_api.local_definitions_filter(\n        (\'tf\', \'keras\'), tf.keras, public_members(tf.keras))\n    filtered_names = [name for name, _ in filtered_children]\n\n    self.assertCountEqual([\'layers\', \'Dense\'], filtered_names)\n\n    filtered_children = public_api.local_definitions_filter(\n        (\'tf\', \'layers\'), tf.layers, public_members(tf.layers))\n    filtered_names = [name for name, _ in filtered_children]\n\n    self.assertCountEqual([], filtered_names)\n\n  def test_explicit_package_contents_filter_removes_modules_not_explicitly_imported(\n      self):\n    path = (\'tensorflow_docs\', \'api_generator\')\n    parent = api_generator\n    members = inspect.getmembers(parent)\n    members.append((\'inspect\', inspect))\n\n    # Assert that parent is a module and is a package, and that the members of\n    # parent include a module named `inspect`.\n    self.assertTrue(inspect.ismodule(parent))\n    self.assertTrue(hasattr(parent, \'__path__\'))\n    self.assertIn(\'inspect\', [name for name, _ in members])\n    self.assertTrue(inspect.ismodule(inspect))\n\n    filtered_members = public_api.explicit_package_contents_filter(\n        path, parent, members)\n\n    # Assert that the filtered_members do not include a module named `inspect`.\n    self.assertNotIn(\'inspect\', [name for name, _ in filtered_members])\n\n  def test_explicit_package_contents_filter_removes_modules_imported_by_modules(\n      self):\n    path = (\'tensorflow_docs\', \'api_generator\', \'public_api\')\n    parent = public_api\n    members = inspect.getmembers(parent)\n\n    # Assert that parent is a module and not a package, and that the members of\n    # parent include a module named `inspect`.\n    self.assertTrue(inspect.ismodule(parent))\n    self.assertFalse(hasattr(parent, \'__path__\'))\n    self.assertIn(\'inspect\', [name for name, _ in members])\n    self.assertTrue(inspect.ismodule(inspect))\n\n    filtered_members = public_api.explicit_package_contents_filter(\n        path, parent, members)\n\n    # Assert that the filtered_members do not include a module named `inspect`.\n    self.assertNotIn(\'inspect\', [name for name, _ in filtered_members])\n\n  def test_ignore_typing(self):\n    children_before = [(\'a\', 1), (\'b\', 3), (\'c\', typing.List)]\n    children_after = public_api.ignore_typing(\'ignored\', \'ignored\',\n                                              children_before)\n    self.assertEqual(children_after, children_before[:-1])\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/py_guide_parser.py,0,"b'# Lint as: python3\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Library for operating on Python API Guide files.""""""\n\nimport os\nimport re\n\n\ndef md_files_in_dir(py_guide_src_dir):\n  """"""Returns a list of filename (full_path, base) pairs for guide files.""""""\n  all_in_dir = [(os.path.join(py_guide_src_dir, f), f)\n                for f in os.listdir(py_guide_src_dir)]\n  return [(full, f) for full, f in all_in_dir\n          if os.path.isfile(full) and f.endswith(\'.md\')]\n\n\nclass PyGuideParser(object):\n  """"""Simple parsing of a guide .md file.\n\n  Descendants can override the process_*() functions (called by process())\n  to either record information from the guide, or call replace_line()\n  to affect the return value of process().\n  """"""\n\n  def __init__(self):\n    self._lines = None\n\n  def process(self, full_path):\n    """"""Read and process the file at `full_path`.""""""\n    with open(full_path, \'rb\') as f:\n      md_string = f.read().decode(\'utf-8\')\n    self._lines = md_string.split(\'\\n\')\n    seen = set()\n\n    in_blockquote = False\n    for i, line in enumerate(self._lines):\n      if \'```\' in line:\n        in_blockquote = not in_blockquote\n\n      if not in_blockquote and line.startswith(\'# \'):\n        self.process_title(i, line[2:])\n      elif not in_blockquote and line.startswith(\'## \'):\n        section_title = line.strip()[3:]\n        existing_tag = re.search(\' {([^}]+)} *$\', line)\n        if existing_tag:\n          tag = existing_tag.group(1)\n        else:\n          tag = re.sub(\'[^a-zA-Z0-9]+\', \'_\', section_title)\n          if tag in seen:\n            suffix = 0\n            while True:\n              candidate = f\'{tag}_{suffix}\'\n              if candidate not in seen:\n                tag = candidate\n                break\n        seen.add(tag)\n        self.process_section(i, section_title, tag)\n\n      elif in_blockquote:\n        self.process_in_blockquote(i, line)\n      else:\n        self.process_line(i, line)\n\n    ret = \'\\n\'.join(self._lines)\n    self._lines = None\n    return ret\n\n  def replace_line(self, line_number, line):\n    """"""Replace the contents of line numbered `line_number` with `line`.""""""\n    self._lines[line_number] = line\n\n  def process_title(self, line_number, title):\n    pass\n\n  def process_section(self, line_number, section_title, tag):\n    pass\n\n  def process_in_blockquote(self, line_number, line):\n    pass\n\n  def process_line(self, line_number, line):\n    pass\n'"
tools/tensorflow_docs/api_generator/py_guide_parser_test.py,0,"b'# Lint as: python3\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for py_guide_parser.""""""\n\nimport os\nimport tempfile\nimport textwrap\n\nfrom absl.testing import absltest\n\nfrom tensorflow_docs.api_generator import py_guide_parser\n\n\nclass TestPyGuideParser(py_guide_parser.PyGuideParser):\n\n  def __init__(self):\n    self.calls = []\n    py_guide_parser.PyGuideParser.__init__(self)\n\n  def process_title(self, line_number, title):\n    self.calls.append((line_number, \'title\', title))\n\n  def process_section(self, line_number, section_title, tag):\n    self.calls.append((line_number, \'section\',\n                       \'%s : %s\' % (section_title, tag)))\n\n  def process_in_blockquote(self, line_number, line):\n    self.calls.append((line_number, \'blockquote\', line))\n    self.replace_line(line_number, line + \' BQ\')\n\n  def process_line(self, line_number, line):\n    self.calls.append((line_number, \'line\', line))\n\n\nclass PyGuideParserTest(absltest.TestCase):\n  _BASE_DIR = tempfile.mkdtemp()\n\n  def setUp(self):\n    super().setUp()\n    self.workdir = os.path.join(self._BASE_DIR, self.id())\n    os.makedirs(self.workdir)\n\n  def testBasics(self):\n    tmp = os.path.join(self.workdir, \'py_guide_parser_test.md\')\n    f = open(tmp, \'w\')\n    f.write(\n        textwrap.dedent(""""""\n        # a title\n        a line\n        ## a section\n        ```shell\n        in a blockquote\n        ```\n        out of blockquote\n        """""")[1:])\n    f.close()\n    parser = TestPyGuideParser()\n    result = parser.process(tmp)\n    expected = textwrap.dedent(""""""\n        # a title\n        a line\n        ## a section\n        ```shell BQ\n        in a blockquote BQ\n        ```\n        out of blockquote\n        """""")[1:]\n    self.assertEqual(expected, result)\n    expected = [(0, \'title\', \'a title\'), (1, \'line\', \'a line\'),\n                (2, \'section\', \'a section : a_section\'),\n                (3, \'blockquote\', \'```shell\'),\n                (4, \'blockquote\', \'in a blockquote\'), (5, \'line\', \'```\'),\n                (6, \'line\', \'out of blockquote\'), (7, \'line\', \'\')]\n    self.assertEqual(expected, parser.calls)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/test_module1.py,0,"b'# Lint as: python3\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A module target for TraverseTest.test_module.""""""\nfrom tensorflow_docs.api_generator import test_module2\n\n\nclass ModuleClass1(object):\n\n  def __init__(self):\n    self._m2 = test_module2.ModuleClass2()\n\n  def __model_class1_method__(self):\n    pass\n'"
tools/tensorflow_docs/api_generator/test_module2.py,0,"b'# Lint as: python3\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A module target for TraverseTest.test_module.""""""\n\n\nclass ModuleClass2(object):\n\n  def __init__(self):\n    pass\n\n  def __model_class1_method__(self):\n    pass\n\n\nclass Hidden(object):\n  pass\n\n\n__all__ = [\'ModuleClass2\']\n'"
tools/tensorflow_docs/api_generator/traverse.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Traversing Python modules and classes.""""""\nimport inspect\nimport sys\n\nfrom google.protobuf.message import Message as ProtoMessage\n\n__all__ = [\'traverse\']\n\n\ndef _filter_module_all(path, root, children):\n  """"""Filters module children based on the ""__all__"" arrtibute.\n\n  Args:\n    path: API to this symbol\n    root: The object\n    children: A list of (name, object) pairs.\n\n  Returns:\n    `children` filtered to respect __all__\n  """"""\n  del path\n  if not (inspect.ismodule(root) and hasattr(root, \'__all__\')):\n    return children\n  module_all = set(root.__all__)\n  children = [(name, value) for (name, value) in children if name in module_all]\n\n  return children\n\n\ndef _add_proto_fields(path, root, children):\n  """"""Add properties to Proto classes, so they can be documented.\n\n  Warning: This inserts the Properties into the class so the rest of the system\n  is unaffected. This patching is acceptable because there is never a reason to\n  run other tensorflow code in the same process as the doc generator.\n\n  Args:\n    path: API to this symbol\n    root: The object\n    children: A list of (name, object) pairs.\n\n  Returns:\n    `children` with proto fields added as properties.\n  """"""\n  del path\n  if not inspect.isclass(root) or not issubclass(root, ProtoMessage):\n    return children\n\n  descriptor = getattr(root, \'DESCRIPTOR\', None)\n  if descriptor is None:\n    return children\n  fields = descriptor.fields\n  if not fields:\n    return children\n\n  field = fields[0]\n  # Make the dictionaries mapping from int types and labels to type and\n  # label names.\n  types = {\n      getattr(field, name): name\n      for name in dir(field)\n      if name.startswith(\'TYPE\')\n  }\n\n  labels = {\n      getattr(field, name): name\n      for name in dir(field)\n      if name.startswith(\'LABEL\')\n  }\n\n  field_properties = {}\n\n  for field in fields:\n    name = field.name\n    doc_parts = []\n\n    label = labels[field.label].lower().replace(\'label_\', \'\')\n    if label != \'optional\':\n      doc_parts.append(label)\n\n    type_name = types[field.type]\n    if type_name == \'TYPE_MESSAGE\':\n      type_name = field.message_type.name\n    elif type_name == \'TYPE_ENUM\':\n      type_name = field.enum_type.name\n    else:\n      type_name = type_name.lower().replace(\'type_\', \'\')\n\n    doc_parts.append(type_name)\n    doc_parts.append(name)\n    doc = \'`{}`\'.format(\' \'.join(doc_parts))\n    prop = property(fget=lambda x: x, doc=doc)\n    field_properties[name] = prop\n\n  for name, prop in field_properties.items():\n    setattr(root, name, prop)\n\n  children = dict(children)\n  children.update(field_properties)\n  children = sorted(children.items(), key=lambda item: item[0])\n\n  return children\n\n\ndef _filter_builtin_modules(path, root, children):\n  """"""Filters module children to remove builtin modules.\n\n  Args:\n    path: API to this symbol\n    root: The object\n    children: A list of (name, object) pairs.\n\n  Returns:\n    `children` with all builtin modules removed.\n  """"""\n  del path\n  del root\n  # filter out \'builtin\' modules\n  filtered_children = []\n  for name, child in children:\n    # Do not descend into built-in modules\n    if inspect.ismodule(child) and child.__name__ in sys.builtin_module_names:\n      continue\n    filtered_children.append((name, child))\n  return filtered_children\n\n\ndef _traverse_internal(root, visitors, stack, path):\n  """"""Internal helper for traverse.""""""\n  new_stack = stack + [root]\n\n  # Only traverse modules and classes\n  if not inspect.isclass(root) and not inspect.ismodule(root):\n    return\n\n  try:\n    children = inspect.getmembers(root)\n  except ImportError:\n    # On some Python installations, some modules do not support enumerating\n    # members (six in particular), leading to import errors.\n    children = []\n\n  # Break cycles.\n  filtered_children = []\n  for name, child in children:\n    if any(child is item for item in new_stack):  # `in`, but using `is`\n      continue\n    filtered_children.append((name, child))\n  children = filtered_children\n\n  # Apply all callbacks, allowing each to filter the children\n  for visitor in visitors:\n    children = visitor(path, root, list(children))\n\n  for name, child in children:\n    # Break cycles\n    child_path = path + (name,)\n    _traverse_internal(child, visitors, new_stack, child_path)\n\n\ndef traverse(root, visitors, root_name):\n  """"""Recursively enumerate all members of `root`.\n\n  Similar to the Python library function `os.path.walk`.\n\n  Traverses the tree of Python objects starting with `root`, depth first.\n  Parent-child relationships in the tree are defined by membership in modules or\n  classes. The function `visit` is called with arguments\n  `(path, parent, children)` for each module or class `parent` found in the tree\n  of python objects starting with `root`. `path` is a string containing the name\n  with which `parent` is reachable from the current context. For example, if\n  `root` is a local class called `X` which contains a class `Y`, `visit` will be\n  called with `(\'Y\', X.Y, children)`).\n\n  If `root` is not a module or class, `visit` is never called. `traverse`\n  never descends into built-in modules.\n\n  `children`, a list of `(name, object)` pairs are determined by\n  `inspect.getmembers`. To avoid visiting parts of the tree, `children` can\n  be modified in place, using `del` or slice assignment.\n\n  Cycles (determined by reference equality, `is`) stop the traversal. A stack of\n  objects is kept to find cycles. Objects forming cycles may appear in\n  `children`, but `visit` will not be called with any object as `parent` which\n  is already in the stack.\n\n  Traversing system modules can take a long time, it is advisable to pass a\n  `visit` callable which blacklists such modules.\n\n  Args:\n    root: A python object with which to start the traversal.\n    visitors: A list of callables. Each taking `(path, parent, children)` as\n      arguments, and returns a list of accepted children.\n    root_name: The short-name of the root module.\n  """"""\n  base_visitors = [\n      _filter_module_all,\n      _add_proto_fields,\n      _filter_builtin_modules\n  ]\n  _traverse_internal(root, base_visitors + visitors, [], (root_name,))\n'"
tools/tensorflow_docs/api_generator/traverse_test.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for Python module traversal.""""""\nfrom absl.testing import absltest\n\nfrom tensorflow_docs.api_generator import test_module1\nfrom tensorflow_docs.api_generator import test_module2\n\nfrom tensorflow_docs.api_generator import traverse\n\n\nclass TestVisitor(object):\n\n  def __init__(self):\n    self.call_log = []\n\n  def __call__(self, path, parent, children):\n    self.call_log += [(path, parent, children)]\n    return children\n\n\nclass TraverseTest(absltest.TestCase):\n\n  def test_cycle(self):\n\n    class Cyclist(object):\n      pass\n    Cyclist.cycle = Cyclist\n\n    visitor = TestVisitor()\n    traverse.traverse(Cyclist, [visitor], root_name=\'root_name\')\n    # We simply want to make sure we terminate.\n\n  def test_module(self):\n    visitor = TestVisitor()\n    traverse.traverse(test_module1, [visitor], root_name=\'root_name\')\n\n    called = [parent for _, parent, _ in visitor.call_log]\n\n    self.assertIn(test_module1.ModuleClass1, called)\n    self.assertIn(test_module2.ModuleClass2, called)\n    self.assertNotIn(test_module2.Hidden, called)\n\n  def test_class(self):\n    visitor = TestVisitor()\n    traverse.traverse(TestVisitor, [visitor], root_name=\'root_name\')\n    self.assertEqual(TestVisitor,\n                     visitor.call_log[0][1])\n    # There are a bunch of other members, but make sure that the ones we know\n    # about are there.\n    self.assertIn(\'__init__\', [name for name, _ in visitor.call_log[0][2]])\n    self.assertIn(\'__call__\', [name for name, _ in visitor.call_log[0][2]])\n\n    # There are more classes descended into, at least __class__ and\n    # __class__.__base__, neither of which are interesting to us, and which may\n    # change as part of Python version etc., so we don\'t test for them.\n\n  def test_non_class(self):\n    integer = 5\n    visitor = TestVisitor()\n    traverse.traverse(integer, [visitor], root_name=\'root_name\')\n    self.assertEqual([], visitor.call_log)\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/utils.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utility functions.""""""\n\nimport importlib\nimport logging\nimport pkgutil\nimport sys\n\n_ALLOWED_EXCEPTIONS = (ImportError, AttributeError, NotImplementedError)\n\n\ndef _onerror(name):\n  logging.exception(f\'Failed to load package: {name!r}\')\n  errortype, error, _ = sys.exc_info()\n\n  if not issubclass(errortype, _ALLOWED_EXCEPTIONS):\n    raise error\n\n\ndef recursive_import(root, strict=False):\n  """"""Recursively imports all the modules under a root package.\n\n  Args:\n    root: A python package.\n    strict: Bool, if `True` raise exceptions, else just log them.\n\n  Returns:\n    A list of all imported modules.\n  """"""\n\n  modules = []\n\n  kwargs = {}\n  # If strict=False, ignore errors during `pkgutil.walk_packages`.\n  if not strict:\n    kwargs = {\'onerror\': _onerror}\n\n  for _, name, _ in pkgutil.walk_packages(\n      root.__path__, prefix=root.__name__ + \'.\', **kwargs):\n    try:\n      modules.append(importlib.import_module(name))\n    # And ignore the same set of errors if import_module fails, these are not\n    # triggered by walk_packages.\n    except _ALLOWED_EXCEPTIONS:\n      if strict:\n        raise\n      else:\n        logging.exception(f\'Failed to load module: {name!r}\')\n\n  return modules\n'"
tools/tensorflow_docs/modeling/__init__.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow.compat.v2 as tf\n\n\nclass EpochDots(tf.keras.callbacks.Callback):\n  """"""A simple callback that prints a ""."" every epoch, with occasional reports.\n\n  Args:\n    report_every: How many epochs between full reports\n    dot_every: How many epochs between dots.\n  """"""\n\n  def __init__(self, report_every=100, dot_every=1):\n    self.report_every = report_every\n    self.dot_every = dot_every\n\n  def on_epoch_end(self, epoch, logs):\n    if epoch % self.report_every == 0:\n      print()\n      print(\'Epoch: {:d}, \'.format(epoch), end=\'\')\n      for name, value in sorted(logs.items()):\n        print(\'{}:{:0.4f}\'.format(name, value), end=\',  \')\n      print()\n\n    if epoch % self.dot_every == 0:\n      print(\'.\', end=\'\')\n'"
tools/tensorflow_docs/plots/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Visualization tools for tensorflow_docs.\n\nUse this module for plotting and visualization code that is too long to inline\ninto a notebook.\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprop_cycle = plt.rcParams[\'axes.prop_cycle\']\nCOLOR_CYCLE = prop_cycle.by_key()[\'color\']\n\n\ndef _smooth(values, std):\n  """"""Smooths a list of values by convolving with a gussian.\n\n  Assumes equal spacing.\n\n  Args:\n    values: A 1D array of values to smooth.\n    std: The standard devistion of the gussian. The units are array elements.\n\n  Returns:\n    The smoothed array.\n  """"""\n  width = std * 4\n  x = np.linspace(-width, width, 2 * width + 1)\n  kernel = np.exp(-(x / 5)**2)\n\n  values = np.array(values)\n  weights = np.ones_like(values)\n\n  smoothed_values = np.convolve(values, kernel, mode=\'same\')\n  smoothed_weights = np.convolve(weights, kernel, mode=\'same\')\n\n  return smoothed_values / smoothed_weights\n\n\nclass HistoryPlotter(object):\n  """"""A class for plotting named set of keras-histories.\n\n  The class maintains colors for each key from plot to plot.\n  """"""\n\n  def __init__(self, metric=None, smoothing_std=None):\n    self.color_table = {}\n    self.metric = metric\n    self.smoothing_std = smoothing_std\n\n  def plot(self, histories, metric=None, smoothing_std=None):\n    """"""Plots a {name: history} dictionary of keras histories.\n\n    Colors are assigned to the name-key, and maintained from call to call.\n    Training metrics are shown as a solid line, validation metrics dashed.\n\n    Args:\n      histories: {name: history} dictionary of keras histories.\n      metric: which metric to plot from all the histories.\n      smoothing_std: the standard-deviaation of the smoothing kernel applied\n        before plotting. The units are in array-indices.\n    """"""\n    if metric is None:\n      metric = self.metric\n    if smoothing_std is None:\n      smoothing_std = self.smoothing_std\n\n    for name, history in histories.items():\n      # Remember name->color asociations.\n      if name in self.color_table:\n        color = self.color_table[name]\n      else:\n        color = COLOR_CYCLE[len(self.color_table) % len(COLOR_CYCLE)]\n        self.color_table[name] = color\n\n      train_value = history.history[metric]\n      val_value = history.history[\'val_\' + metric]\n      if smoothing_std is not None:\n        train_value = _smooth(train_value, std=smoothing_std)\n        val_value = _smooth(val_value, std=smoothing_std)\n\n      plt.plot(\n          history.epoch,\n          train_value,\n          color=color,\n          label=name.title() + \' Train\')\n      plt.plot(\n          history.epoch,\n          val_value,\n          \'--\',\n          label=name.title() + \' Val\',\n          color=color)\n\n    plt.xlabel(\'Epochs\')\n    plt.ylabel(metric.replace(\'_\', \' \').title())\n    plt.legend()\n\n    plt.xlim(\n        [0, max([history.epoch[-1] for name, history in histories.items()])])\n    plt.grid(True)\n'"
tools/tensorflow_docs/tools/__init__.py,0,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tools/tensorflow_docs/vis/__init__.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Visualization tools for tensorflow_docs.\n\nUse this module for plotting and visualization code that is too long to inline\ninto a notebook.\n""""""\n\n'"
tools/tensorflow_docs/vis/embed.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Simple functions for embedding data in a notebook file.""""""\n\nimport base64\nimport mimetypes\nimport os\nimport pathlib\nimport textwrap\n\nimport IPython.display\n\n\ndef embed_data(mime: str, data: bytes) -> IPython.display.HTML:\n  """"""Embeds data as an html tag with a data-url.""""""\n  b64 = base64.b64encode(data).decode()\n  if mime.startswith(\'image\'):\n    tag = f\'<img src=""data:{mime};base64,{b64}""/>\'\n  elif mime.startswith(\'video\'):\n    tag = textwrap.dedent(f""""""\n        <video width=""640"" height=""480"" controls>\n          <source src=""data:{mime};base64,{b64}"" type=""video/mp4"">\n          Your browser does not support the video tag.\n        </video>\n        """""")\n  else:\n    raise ValueError(\'Images and Video only.\')\n  return IPython.display.HTML(tag)\n\n\ndef embed_file(path: os.PathLike) -> IPython.display.HTML:\n  """"""Embeds a file in the notebook as an html tag with a data-url.""""""\n  path = pathlib.Path(path)\n  mime, unused_encoding = mimetypes.guess_type(str(path))\n  data = path.read_bytes()\n\n  return embed_data(mime, data)\n'"
tools/tensorflow_docs/vis/webp_animation.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Easy notebook embedded webp animations.\n\n```\nimport tensorflow_docs.vis.webp_animation as webp_animation\n\nenv = gym.make(\'SpaceInvaders-v0\')\nobs = env.reset()\ndone = False\nn = 0\n\nanim = webp_animation.Webp()\n\nwhile not done:\n  img = env.render(mode = \'rgb_array\')\n  anim.append(img)\n  act = env.action_space.sample() # take a random action\n  obs, reward, done, info = env.step(act)\n  n += 1\n\nanim.save(""test.webp"")\nanim\n```\n""""""\n\nimport numpy as np\nimport PIL.Image\n\nfrom tensorflow_docs.vis import embed\nimport webp\n\n\nclass Webp(object):\n  """"""Builds a webp animation.\n\n  Attributes:\n    frame_rate: The default frame rate for appended images.\n    shape: The shape of the animation frames. Will default to the size of the\n      first image if not set.\n    result: The binary image data string. Once the animation has been used, it\n      can no longer updated. And the result field contains the webp encoded\n      data.\n  """"""\n\n  def __init__(self, shape=None, frame_rate=60.0, **options):\n    """"""A notebook-embedable webp animation.\n\n    Args:\n      shape: Optional. The image_shape of the animation. Defaults to the shape\n        of the first image if unset.\n      frame_rate: The default frame rate for the animation.\n      **options: Additional arguments passed to `WebPAnimEncoderOptions.new`.\n    """"""\n    self.frame_rate = frame_rate\n    self._timestamp_ms = 0\n    self._empty = True\n\n    if options is None:\n      options = {}\n\n    self._options = webp.WebPAnimEncoderOptions.new(**options)\n    self._encoder = None\n    self._shape = shape\n    self._result = None\n\n  def append(self, img, dt_ms=None):\n    """"""Append an image to the animation.\n\n    Args:\n      img: The image to add.\n      dt_ms: override the animation frame rate for this frame with a frame\n        length in ms.\n\n    Raises:\n      ValueError:\n        * if the video has already been ""assembled"" (used).\n        * if `img` does not match the shape of the animation.\n    """"""\n    if self._result is not None:\n      raise ValueError(\n          ""Can\'t append to an animation after it has been \\""assembled\\"" (used).""\n      )\n    self._empty = False\n\n    if not isinstance(img, PIL.Image.Image):\n      img = np.asarray(img)\n      img = PIL.Image.fromarray(img)\n\n    if self._shape is None:\n      self._shape = img.size\n\n    if self._encoder is None:\n      self._encoder = webp.WebPAnimEncoder.new(self.shape[0], self.shape[1],\n                                               self._options)\n\n    if img.size != self.shape:\n      raise ValueError(""Image shape does not match video shape"")\n\n    img = webp.WebPPicture.from_pil(img)\n\n    self._encoder.encode_frame(img, int(self._timestamp_ms))\n\n    if dt_ms is None:\n      self._timestamp_ms += 1000 * (1.0 / self.frame_rate)\n    else:\n      self._timestamp_ms += dt_ms\n\n  def extend(self, imgs, dt_ms=None):\n    """"""Extend tha animation with an iterable if images.\n\n    Args:\n      imgs: An iterable of images, to pass to `.append`.\n      dt_ms: Override the animation frame rate for these frames with a frame\n        length in ms.\n    """"""\n    for img in imgs:\n      self.append(img, dt_ms=dt_ms)\n\n  @property\n  def result(self):\n    result = self._result\n    if result is None:\n      anim_data = self._encoder.assemble(int(self._timestamp_ms))\n      result = anim_data.buffer()\n      self._result = result\n    return result\n\n  @property\n  def shape(self):\n    """"""The shape of the animation. Read only once set.""""""\n    return self._shape\n\n  def _repr_html_(self):\n    """"""Notebook display hook, embed the image in an <img> tag.""""""\n    if self._empty:\n      return ""Empty Animation""\n\n    return embed.embed_data(""image/webp"", self.result)._repr_html_()  # pylint: disable=protected-access,\n\n  def save(self, filename):\n    """"""Write the webp data to a file.""""""\n    with open(filename, ""wb"") as f:\n      f.write(self.result)\n'"
tools/tensorflow_docs/vis/webp_test.py,0,"b'# Lint as: python3\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_docs.vis.webp.""""""\n\nimport os\n\nfrom absl.testing import absltest\n\nimport numpy as np\nimport PIL.Image\n\nfrom tensorflow_docs.vis import webp_animation\n\n\nclass WebpTest(absltest.TestCase):\n\n  def test_smoke(self):\n    workdir = self.create_tempdir().full_path\n\n    img = PIL.Image.fromarray(np.zeros([10, 12, 3], dtype=np.uint8))\n    anim = webp_animation.Webp()\n\n    anim.append(img)\n    anim.extend([img])\n    anim.save(os.path.join(workdir, \'test.webp\'))\n\n\nif __name__ == \'__main__\':\n  absltest.main()\n'"
tools/tensorflow_docs/api_generator/gen_java/__init__.py,1,"b'# Lint as: python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generate javadoc-doclava reference docs for tensorflow.org.""""""\n\nimport contextlib\nimport os\nimport pathlib\nimport subprocess\n\n# __file__ is the path to this file\nGEN_JAVA_DIR = pathlib.Path(__file__).resolve().parent\n\nTEMPLATES = GEN_JAVA_DIR / \'templates\'\nDOCLAVA_FOR_TF = GEN_JAVA_DIR / \'run-javadoc-for-tf.sh\'\n\n\ndef gen_java_docs(package: str, source_path: pathlib.Path,\n                  output_dir: pathlib.Path, site_path: pathlib.Path) -> None:\n  os.environ[\'PACKAGE\'] = package\n  os.environ[\'SOURCE_PATH\'] = str(source_path)\n  os.environ[\'OUTPUT_DIR\'] = str(output_dir)\n  os.environ[\'SITE_PATH\'] = str(pathlib.Path(\'/\') / site_path)\n  os.environ[\'TEMPLATES\'] = str(TEMPLATES)\n  subprocess.check_call([\'bash\', DOCLAVA_FOR_TF], cwd=GEN_JAVA_DIR)\n'"
tools/tensorflow_docs/tools/nblint/__init__.py,0,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tools/tensorflow_docs/tools/nblint/__main__.py,0,"b'# Lint as: python3\n# pylint: disable=invalid-name\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Check notebook conformance with documentation styles.\n\nInstall the tensorflow-docs package:\n$ python3 -m pip install -U [--user] git+https://github.com/tensorflow/docs\n\nUsage:\n$ python3 -m tensorflow_docs.tools.nblint [options] notebook.ipynb [...]\n$ python3 -m tensorflow_docs.tools.nblint --verbose \\\n    [--styles=google,tensorflow] notebook.ipynb [...]\n\nSee the TensorFlow notebook template:\nhttps://github.com/tensorflow/docs/blob/master/tools/templates/notebook.ipynb\nAnd the TensorFlow docs contributor guide:\nhttps://www.tensorflow.org/community/contribute/docs\n""""""\n\nimport importlib\nimport inspect\nimport pathlib\nimport sys\nimport textwrap\n\nfrom absl import app\nfrom absl import flags\n\nfrom tensorflow_docs.tools.nblint import decorator\nfrom tensorflow_docs.tools.nblint import linter\n\nflags.DEFINE_list(""styles"", [""google"", ""tensorflow""], ""Lint styles to include."")\nflags.DEFINE_boolean(""verbose"", False, ""Display verbose output."")\n\nFLAGS = flags.FLAGS\n\n\ndef _collect_notebook_paths(filepaths):\n  """"""Return list of `Path`s for (recursive) notebook filepaths.\n\n  Skips any file that\'s not an .ipynb notebook file.\n\n  Args:\n    filepaths: List file path strings passed at command-line.\n\n  Returns:\n    A list of Path objects for all notebook files.\n  """"""\n  paths = []\n  for fp in filepaths:\n    path = pathlib.Path(fp)\n    if path.is_dir():\n      paths.extend(path.rglob(""*.ipynb""))\n    elif path.is_file():\n      if path.suffix == "".ipynb"":\n        paths.append(path)\n      else:\n        print(f""Not an \'.ipynb\' file, skipping: {path}"", file=sys.stderr)\n    else:\n      print(f""Invalid file, skipping: {path}"", file=sys.stderr)\n  return paths\n\n\ndef _print_fails(path_list):\n  """"""Format notebooks that failed lint and print to console.\n\n  Args:\n    path_list: A list of Path objects.\n  """"""\n  template = textwrap.dedent(""""""\\\n    The following notebook{plural} failed lint:\n    {filepaths}"""""")\n  paths = ""\\n"".join([f"" {str(fp)}"" for fp in path_list])\n  plural = ""s"" if len(paths) > 1 else """"\n  print(template.format(filepaths=paths, plural=plural), file=sys.stderr)\n\n\ndef _is_user_defined_lint(mod_name):\n  """"""Return a function that tests if a module member is a user-defined lint.\n\n  Args:\n    mod_name: THe string name of the module file containing the lint.\n\n  Returns:\n    Function: This returns a Boolean: True if the module member is a lint.\n  """"""\n\n  def is_lint(member):\n    return (inspect.isfunction(member) and member.__module__ == mod_name and\n            hasattr(member, ""_lint""))\n\n  return is_lint\n\n\ndef add_styles(styles, verbose):\n  """"""Import lint assertions from style modules.\n\n  Style modules must exist in the `style/` directory of this package.\n\n  Args:\n    styles: A list of short names for style modules to import.\n    verbose: Bool, to print more details to console. Default is False.\n\n  Returns:\n    A dictionary containing all the lint styles.\n  """"""\n\n  lint_dict = {\n      decorator.Options.Scope.CODE: {\n          decorator.Options.Cond.ALL: [],\n          decorator.Options.Cond.ANY: []\n      },\n      decorator.Options.Scope.TEXT: {\n          decorator.Options.Cond.ALL: [],\n          decorator.Options.Cond.ANY: []\n      },\n      decorator.Options.Scope.CELLS: {\n          decorator.Options.Cond.ALL: [],\n          decorator.Options.Cond.ANY: []\n      },\n      decorator.Options.Scope.FILE: {\n          decorator.Options.Cond.ANY: []  # Only one queue is relevant.\n      }\n  }\n\n  for style in styles:\n    mod_name = f""tensorflow_docs.tools.nblint.style.{style}""\n    mod = importlib.import_module(mod_name)\n    is_lint = _is_user_defined_lint(mod_name)\n\n    # Extract Lint instance attached to function object by decorator.\n    lints = [\n        getattr(mem[1], ""_lint"") for mem in inspect.getmembers(mod, is_lint)\n    ]\n\n    if verbose:\n      lint_names = "", "".join([lint.name for lint in lints])\n      print(f""From style \'{mod_name}\' import lints: {lint_names}\\n"")\n\n    for lint in lints:\n      lint.style = style\n      lint_dict[lint.scope][lint.cond].append(lint)\n\n  return lint_dict\n\n\ndef main(argv):\n  if len(argv) <= 1:\n    raise app.UsageError(""Missing arguments."", 1)\n  if not FLAGS.styles:\n    raise app.UsageError(""Missing styles."", 1)\n\n  nb_linter = linter.Linter(verbose=FLAGS.verbose)\n  lint_dict = add_styles(FLAGS.styles, FLAGS.verbose)\n\n  linter_fails = []  # Track failed notebooks for final return code.\n\n  for path in _collect_notebook_paths(argv[1:]):\n    print(f""Lint notebook: {path}"")\n\n    status = nb_linter.run(path, lint_dict)\n    if not status.is_success:\n      linter_fails.append(path)\n\n    print(status)\n\n  if linter_fails:\n    _print_fails(linter_fails)\n    sys.exit(1)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
tools/tensorflow_docs/tools/nblint/decorator.py,0,"b'# Lint as: python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Defines the @lint API used in style files to create lint tests.\n\nUsers define lints by adding the @lint decorator to test functions. A collection\nof lint functions are grouped in a style module. Style modules can be\nenabled/disabled at the command-line.\n\nLint functions must return a Boolean value: True for pass, False for fail.\nNegative assertions must follow this logic, always pass True if the lint test is\nconsidered a success.\n\nWhether a lint assertion passes or fails depends on its scope and condition\nparameters:\n\n- The *condition* determines if a test is considered a success if it passes for\n  *all* cells in the notebook, or *any* cells (just one).\n- The *scope* determines where the assertion should be executed: for all\nnotebook\n  cells, only text cells, only code, cells, or just run once per notebook.\n\nImplementation-wise, the @lint decorator creates a `Lint` instance that is\nattached to the underlying function object within the style module. When the\nmodule is imported into the `Linter`, the `Lint` object is extracted.\n\n""""""\nimport enum\nimport functools\n\n\nclass Options:\n  """"""Options to define the condition and scope of a @lint defined assertion.""""""\n\n  class Cond(enum.Enum):\n    """"""Determines if a test is considered a success by which cells it passes.\n\n    Attributes:\n      ALL: Success if all cells pass.\n      ANY: Success if any cells pass (just one). [Default]\n    """"""\n    ALL = enum.auto()\n    ANY = enum.auto()\n\n  class Scope(enum.Enum):\n    """"""Determines where a function test is executed.\n\n    Attributes:\n      CELLS: Code and text cells. [Default]\n      CODE: Code cells only.\n      TEXT: Text cells only.\n      FILE: Run assertion function once per file.\n    """"""\n    CELLS = enum.auto()\n    CODE = enum.auto()\n    TEXT = enum.auto()\n    FILE = enum.auto()\n\n\nclass Lint:\n  """"""Contains the function and properties defined by the @lint decorator.\n\n  Attributes:\n    run: User-defined assertion callback that returns a Boolean.\n    scope: `Options.Scope` to determine where an assertion is executed.\n    cond: `Options.Cond` to determine if an assertion is considered a success.\n    name: Optional string name for assertion function in reports.\n    message: String message to include in status report.\n    style: String name of style module that defines the Lint. (Added on load.)\n  """"""\n\n  def __init__(self, fn, scope, cond, message=None, name=None):\n    self.run = fn\n    self.scope = scope\n    self.cond = cond\n    self.name = name if name else fn.__name__\n    self.message = message.strip() if message else """"\n    self.style = None  # Added on style load.\n\n\n# Define a decorator with optional arguments.\ndef lint(fn=None, *, message=None, scope=None, cond=None):\n  """"""Function decorator for user-defined lint assertions.\n\n  Args:\n    fn: User-defined assertion callback that returns a Boolean. See `Linter.run`\n      for args passed to callback, depending on scope:\n        * For cell-scope: callback(source, cell_data, path).\n        * For file-scope: callback(source, all_data, path)\n    message: Optional string message to include in status report.\n    scope: Determines where the function should be executed, options in\n      `Options.Scope`.\n    cond: Determines how an assertion is considered a success, if it passes all\n      cells or any cells. Options available in `Options.Cond`.\n\n  Returns:\n    Function: A wrapper around the user-defined `fn` function.\n  """"""\n  if fn is None:\n    return functools.partial(lint, message=message, scope=scope, cond=cond)\n\n  scope = scope if scope else Options.Scope.CELLS\n  cond = cond if cond else Options.Cond.ANY\n\n  @functools.wraps(fn)\n  def wrapper(*args, **kwargs):\n    return fn(*args, **kwargs)\n\n  # Attach to function object to access when importing the style module.\n  setattr(wrapper, ""_lint"", Lint(fn, scope, cond, message))\n  return wrapper\n'"
tools/tensorflow_docs/tools/nblint/linter.py,0,"b'# Lint as: python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Import lint tests, run lints, and report status.\n\nA `Linter` instance imports lint tests from a style file into a structured queue\nthat are then run on notebook files. Depending on condition and scope options,\nthese are executed for the entire notebook or for each text and code cell.\n\nA `LinterStatus` instance is returned when lint tests are run on a file. While\na single `Linter` instance can be run on multiple files, a `LinterStatus` is\nassociated with a single notebook file. It maintains the pass/fail state for\neach lint test on the file. Additionally, `LinterStatus` implements the\nformatting required to print the report that the console.\n""""""\n\nimport json\nimport sys\nimport textwrap\nimport typing\n\nfrom tensorflow_docs.tools.nblint import decorator\n\n\nclass Linter:\n  """"""Manages the collection of lints to execute on a notebook.\n\n  Lint assertions are imported by style modules and dispatched by condition and\n  scope. A Linter can be run on multiple notebooks.\n\n  Attributes:\n    verbose: Boolean to print more details to console. Default is False.\n  """"""\n\n  def __init__(self, verbose=False):\n    self.verbose = verbose\n\n  def _load_notebook(self, path):\n    """"""Load and parse JSON data from a notebook file.\n\n    Args:\n      path: A `pathlib.Path` of a Jupyter notebook.\n\n    Returns:\n      Dict: Contains data of the parsed JSON notebook.\n      String: The entire JSON source code of the notebook.\n    """"""\n    source = path.read_text(encoding=""utf-8"")\n    try:\n      data = json.loads(source)\n      if not isinstance(data.get(""cells""), list):\n        print(\n            ""Error: Invalid notebook, unable to find list of cells."",\n            file=sys.stderr)\n        sys.exit(1)\n    except (json.JSONDecodeError, ValueError) as err:\n      print(\n          textwrap.dedent(f""""""\n        Unable to load JSON for {path}:\n        {err.__class__.__name__}: {err}\n        """"""),\n          file=sys.stderr)\n      data = None\n\n    return data, source\n\n  def _run_lint_group(self, lint, data, status, path):\n    """"""Run lint over all cells with scope and return cumulative pass/fail.\n\n    Args:\n      lint: `decorator.Lint` containg the assertion, scope, and condition.\n      data: `dict` containing data of entire parse notebook.\n      status: The `LinterStatus` to add individual entries for group members.\n      path: `pathlib.Path` of notebook to pass to @lint defined callback.\n\n    Returns:\n      Boolean: True if lint passes for all/any cells, otherwise False.\n\n    Raises:\n      Exception: Unsupported lint condition in `decorator.Options.Cond`.\n    """"""\n    scope = lint.scope\n    is_success_list = []  # Collect for each (scoped) cell in notebook.\n\n    for cell_idx, cell in enumerate(data.get(""cells"")):\n      # Evict notebook cells outside of scope.\n      cell_type = cell.get(""cell_type"")\n      if scope is decorator.Options.Scope.TEXT and cell_type != ""markdown"":\n        continue\n      elif scope is decorator.Options.Scope.CODE and cell_type != ""code"":\n        continue\n\n      # Execute lint on cell and collect result.\n      source = """".join(cell[""source""])\n      is_success = lint.run(source, cell, path)\n      is_success_list.append(is_success)\n\n      # All lint runs get a status entry. Group success is a separate entry.\n      name = f""{lint.name}__cell_{cell_idx}""\n      status.add_entry(\n          lint, is_success, name=name, group=lint.name, is_group_entry=True)\n\n    # Return True/False success for entire cell group.\n    if lint.cond is decorator.Options.Cond.ANY:\n      return any(is_success_list)\n    elif lint.cond is decorator.Options.Cond.ALL:\n      return all(is_success_list)\n    else:\n      raise Exception(""Unsupported lint condition."")\n\n  def run(self, path, lint_dict):\n    """"""Multiple hooks provided to run tests at specific points.\n\n    Args:\n      path: `pathlib.Path` of notebook to run lints against.\n      lint_dict: A dictionary containing the lint styles.\n\n    Returns:\n      LinterStatus: Provides status and reporting of lint tests for a notebook.\n    """"""\n    data, source = self._load_notebook(path)\n    if not data:\n      return False\n\n    status = LinterStatus(path, verbose=self.verbose)\n\n    # File-level scope.\n    # Lint run once for the file.\n    for lint in lint_dict[decorator.Options.Scope.FILE][\n        decorator.Options.Cond.ANY]:\n      is_success = lint.run(source, data, path)\n      status.add_entry(lint, is_success)\n\n    # Cell-level scope.\n    # These lints run on each cell, then return a cumulative result.\n    for scope in [\n        decorator.Options.Scope.CELLS, decorator.Options.Scope.CODE,\n        decorator.Options.Scope.TEXT\n    ]:\n      for cond in decorator.Options.Cond:\n        lints = lint_dict[scope][cond]\n        for lint in lints:\n          is_success = self._run_lint_group(lint, data, status, path)\n          status.add_entry(lint, is_success, group=lint.name)\n\n    return status\n\n\nclass LinterStatus:\n  """"""Provides status and reporting of lint tests for a notebook.\n\n  A new `LinterStatus` object is returned when `Linter.run` is executed on a\n  given notebook. A `LinterStatus` object represents a run of all lints for a\n  single notebook file. Multiple notebook files require multiple `LinterStatus`\n  objects. Though multiple status objects can be created by the same `Linter`.\n\n  The `LinterStatus` instance manages `LintStatusEntry` objects. These are added\n  in the `Linter.run` for each lint test. Some entries may be a part of a larger\n  lint group that represents a collective pass/fail status.\n\n  A `LinterStatus` instance is also reponsible for printing status reports for\n  entries to the console to display to the user.\n\n  Attributes:\n    path: `pathlib.Path` of notebook that lints were run against.\n    verbose: Boolean to print more details to console. Default is False.\n    is_success: Boolean status of entire lint report: True if all tests pass,\n      otherwise False.\n  """"""\n\n  class LintStatusEntry(typing.NamedTuple):\n    """"""Represents the status of a lint tested against a single section.\n\n    Depending on the scope of the lint, one lint can create multiple\n    `LintStatusEntry` objects. For example, if tested against all notebook\n    cells, one status entry would be created for each cell it is run on. This\n    would also create a group entry representing the cumulative conditional\n    test: any/all.\n\n    Groups are determined by a shared a group name. If an entry is designed with\n    True for `is_group_entry`, that means it\'s a member (child) of the group.\n    The cumulative status is the one member of the group that is set to False\n    for `is_group_entry`.\n\n    Attributes:\n      lint: `decorator.Lint` associated with this status.\n      is_success: Boolean\n      name: Optional name of the status entry for reports. Default to lint name.\n      group: Optional string of shared group name for multiple entries.\n      is_group_entry: Boolean. If in group, True if entry is memmber/child of\n        group, and Falsw if it represents the collective status of a group.\n    """"""\n    lint: decorator.Lint\n    is_success: bool\n    name: str\n    group: typing.Optional[str]\n    is_group_entry: bool\n\n  def __init__(self, path, verbose=False):\n    self.path = path\n    self.verbose = verbose\n    self._status_list = []  # Contains all status entries.\n\n  def add_entry(self,\n                lint,\n                is_success,\n                name=None,\n                group=None,\n                is_group_entry=False):\n    """"""Add a new `LintStatusEntry` to report.\n\n    Args:\n      lint: `decorator.Lint` associated with this status.\n      is_success: Boolean\n      name: Optional name of the status entry for reports. Default to lint name.\n      group: Optional string of shared group name for multiple entries.\n      is_group_entry: Boolean. If in group, True if entry is memmber/child of\n        group, and Falsw if it represents the collective status of a group.\n    """"""\n    if not isinstance(is_success, bool):\n      raise TypeError(f""Lint status must return Boolean, got: {is_success}"")\n    name = name if name else lint.name\n    entry = self.LintStatusEntry(lint, is_success, name, group, is_group_entry)\n    self._status_list.append(entry)\n\n  @property\n  def is_success(self):\n    """"""Represents the status of entire lint report.\n\n    Returns:\n      Boolean: True if all top-level status entries pass, otherwise False.\n    """"""\n    status = True\n    for entry in self._status_list:\n      if not entry.is_group_entry and not entry.is_success:\n        status = False\n        break\n    return status\n\n  def _format_status(self, entry):\n    """"""Pretty-print an entry status for console (with color).\n\n    Args:\n      entry: `LintStatusEntry` with status.\n\n    Returns:\n      String: \'Pass\' or \'Fail\' with terminal color codes.\n    """"""\n    if entry.is_success:\n      msg = ""\\033[32mPass\\033[00m""  # Green\n    else:\n      if entry.is_group_entry:\n        msg = ""\\033[33mFail\\033[00m""  # Yellow: group entry\n      else:\n        msg = ""\\033[91mFail\\033[00m""  # Light red: root entry\n    return msg\n\n  def __str__(self):\n    """"""Print the entire status report of all entries to console.\n\n    Arrange and format entries for reporting to console. If\n    `LinterStatus.verbose` is True, display group member entries in addition to\n    the cumulative group status. Called as: `print(linter_status)`.\n\n    Returns:\n      String containing the entire lint report.\n    """"""\n    # Sort group entries to display nested underneath parent.\n    groups = {}\n    # Can skip if not displaying.\n    if self.verbose:\n      for entry in self._status_list:\n        if entry.is_group_entry:\n          if entry.group in groups:\n            groups[entry.group].append(entry)\n          else:\n            groups[entry.group] = [entry]\n\n    # Filter top-level entries.\n    root_entries = [obj for obj in self._status_list if not obj.is_group_entry]\n    output = """"\n\n    for entry in root_entries:\n      # Print top-level entry.\n      status = self._format_status(entry)\n      name = f""{entry.lint.style}::{entry.name}""\n      msg = f"" | {entry.lint.message}"" if entry.lint.message else """"\n      output += f""{status} | {name}{msg}\\n""\n\n      # Print child entries, if applicable.\n      if self.verbose and entry.group in groups:\n        output += ""[All results]\\n""\n        for child in groups[entry.group]:\n          output += f""- {self._format_status(child)} | {child.name}\\n""\n\n        output += ""\\n""\n\n    return output\n'"
tools/tensorflow_docs/tools/nblint/style/__init__.py,0,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
tools/tensorflow_docs/tools/nblint/style/google.py,0,"b'# Lint as: python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Lint assertions that adhere to the Google dev docs style guide.\n\nThis style module is a non-exhaustive implemention of style rules found in the\nGoogle developer documentation style guide: https://developers.google.com/style\n\nWhen adding lints, please link to the URL of the relevant style rule.\n""""""\nimport re\n\nfrom tensorflow_docs.tools.nblint.decorator import lint\nfrom tensorflow_docs.tools.nblint.decorator import Options\n\nsecond_person_re = re.compile(r""\\bwe\\b"", re.IGNORECASE)\n\n\n@lint(\n    message=""Prefer second person \'you\' instead of \'we\': https://developers.google.com/style/person"",\n    cond=Options.Cond.ALL)\ndef second_person(source, cell, filepath):\n  del cell, filepath  # Unused by callback\n  if not second_person_re.search(source):\n    return True\n  else:\n    return False\n'"
tools/tensorflow_docs/tools/nblint/style/tensorflow.py,0,"b'# Lint as: python3\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Lint assertions that adhere to the TensorFlow documentation guide.\n\nThis style module is a non-exhaustive implemention of style rules found in the\nTensorFlow documentation and style guides. See:\n\n- https://www.tensorflow.org/community/contribute/docs\n- https://www.tensorflow.org/community/contribute/docs_style\n\nWhen adding lints, please link to the URL of the relevant style rule, if\napplicable.\n""""""\nimport re\n\nfrom tensorflow_docs.tools.nblint.decorator import lint\nfrom tensorflow_docs.tools.nblint.decorator import Options\n\ncopyright_re = re.compile(\n    r""Copyright 20[1-9][0-9] The TensorFlow\\s.*?\\s?Authors"")\n\n\n@lint(message=""TensorFlow copyright is required"", scope=Options.Scope.TEXT)\ndef copyright_check(source, cell, filepath):\n  del cell, filepath  # Unused by callback\n  if copyright_re.search(source):\n    return True\n  else:\n    return False\n\n\nlicense_re = re.compile(""#@title Licensed under the Apache License"")\n\n\n@lint(\n    message=""Apache license is required"",\n    scope=Options.Scope.CODE,\n    cond=Options.Cond.ANY)\ndef license_check(source, cell, filepath):\n  del cell, filepath  # Unused by callback\n  if license_re.search(source):\n    return True\n  else:\n    return False\n\n\n@lint(scope=Options.Scope.FILE)\ndef not_translation(source, data, filepath):\n  del source, data  # Unused by callback\n  if ""site"" not in filepath.parents:\n    return True\n  else:\n    return ""site/en"" in filepath.parents\n'"
